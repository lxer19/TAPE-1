URL: http://polaris.cs.uiuc.edu/reports/1408.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: A FAMILY OF PRECONDITIONED ITERATIVE SOLVERS FOR SPARSE LINEAR SYSTEMS  
Author: BY ULRIKE MEIER YANG 
Degree: Dipl., Ruhruniversitat Bochum, 1983 THESIS Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science in the Graduate College of the  
Address: 1995 Urbana, Illinois  
Affiliation: University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> I. Bogle, J. Perkins, </author> <title> A new sparsity preserving Quasi-Newton update for solving nonlinear equations, </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <pages> 11(1990) 621-630. </pages>
Reference-contexts: For these reasons, these versions are not investigated further in this thesis. The problem of scaling-invariance has been considered in the context of quasi-Newton methods for nonlinear systems. Since Newton's method is scaling-invariant, some authors were interested in developing scaling-invariant quasi-Newton methods <ref> [1] </ref> [34]. The definition they use is more general than the one above. Note that none of the scaling-invariant algorithms above is scaling-invariant according to the new definition below. <p> are obtained through a standard five point finite difference discretization of the following two-dimensional partial differential equation, which is taken from [48], u xx u yy + fi (u x + u y ) = f on (6:1) with Dirichlet boundary conditions u = c on @; (6:2) where = <ref> [0; 1] </ref> fi [0; 1]. We choose step size h = 0:01, which leads to a matrix of order 9801. 6.1 Problem 1 For our first problem we choose fi = 1. This is an example for which CGS and BiCGSTAB converge fairly quickly. <p> a standard five point finite difference discretization of the following two-dimensional partial differential equation, which is taken from [48], u xx u yy + fi (u x + u y ) = f on (6:1) with Dirichlet boundary conditions u = c on @; (6:2) where = <ref> [0; 1] </ref> fi [0; 1]. We choose step size h = 0:01, which leads to a matrix of order 9801. 6.1 Problem 1 For our first problem we choose fi = 1. This is an example for which CGS and BiCGSTAB converge fairly quickly.
Reference: [2] <author> C. </author> <title> Broyden, A class of methods for solving nonlinear simultaneous equations, </title> <journal> Math. Comp., </journal> <note> 19(1965) 577-593. </note>
Reference: [3] <author> C. </author> <title> Broyden, Quasi-Newton methods and their application to function minimization, </title> <journal> Math. Comp., </journal> <pages> 21(1967) 368-381. </pages>
Reference: [4] <author> C. </author> <title> Broyden, A new method of solving nonlinear simultaneous equations, </title> <journal> Computer J., </journal> <pages> 12(1969) 94-99. </pages>
Reference-contexts: Two methods that have not received much attention for solving linear systems are Broyden's method <ref> [4] </ref> and the EN method [13], which was developed fairly recently. The family of Broyden methods has suffered from a bad reputation for solving linear systems, but recent efforts [8] have shown that different line search strategies lead to versions that are competitive with GMRES. <p> These methods assume symmetric (and often positive definite) matrices, and we do not consider them here, since our goal is to solve nonsymmetric linear systems. Instead, we focus on variants of Broyden's method, a quasi-Newton method that is suitable for solving nonsymmetric linear systems <ref> [4] </ref>. 10 If F is defined by F (x) = Ax b, then its Jacobian equals A, and Broyden's method, in its most general form, is given by Algorithm 2. <p> Based on Algorithm 2, Broyden suggested using f k = H H k p k <ref> [4] </ref>. The resulting algorithm is known in the literature as the `good' Broyden method (GBM). <p> However, the convergence behavior of GBM and BBM for case (1) of our test problem in Figure 2.2 shows that this is not always a desirable choice despite its finite termination property. Broyden suggests in <ref> [4] </ref> choosing ff k so that kr k+1 k &lt; kr k k. He also states that this choice of ff k can lead to worse results than choosing an ff k that does not necessarily fulfill kr k+1 k &lt; kr k k, e.g., ff k = 1.
Reference: [5] <author> C. </author> <title> Broyden, The convergence of single-rank quasi-Newton methods, </title> <journal> Math. Comp., </journal> <pages> 24(1970) 365-382. </pages>
Reference-contexts: Lemmas 4.9 and 4.10 relate the norms of the two error matrices to the choice of f k and the auxiliary vector q k . Lemma 4.8 For x; y 2 C n with x H y = 1, kI xy H k = kxkkyk: (4:24) Proof: See <ref> [5] </ref>. 2 We can now prove the following two lemmas.
Reference: [6] <author> C. Broyden, J. Dennis, J. </author> <title> More, On the local and superlinear convergence of Quasi-Newton methods, </title> <editor> J. </editor> <publisher> Inst. </publisher> <address> Maths Applics 12(1973) 223-245. </address>
Reference-contexts: The second part follows from the first part using induction. 2 The results above all assumed the use of the 2-norm. Similar results can be developed for the Frobenius norm (k:k F ) of E k and ~ E k using a lemma from <ref> [6] </ref>. Lemma 4.15 If A is an n fi n matrix and x, y are two vectors of order n, then kA + xy H k F = kAk 2 Proof: See [6]. 2 We can now prove the following lemmas. <p> for the Frobenius norm (k:k F ) of E k and ~ E k using a lemma from <ref> [6] </ref>. Lemma 4.15 If A is an n fi n matrix and x, y are two vectors of order n, then kA + xy H k F = kAk 2 Proof: See [6]. 2 We can now prove the following lemmas. These lemmas are not only interesting in and of themselves, but their proofs also require the development of identities that are required later to prove the convergence results for certain Broyden methods. <p> fl )e k )k fl ke k k 2 : Combining these equations with (7.59) leads to lim ke k+1 k = 0: (7:65) Just as Gay [24] showed local 2n-step q-quadratic convergence for Broyden's method, using the finite termination property of Broyden's method and the local convergence properties in <ref> [6] </ref>, we show local n-step q-quadratic convergence for nonlinear GEN and BEN in Theorems 7.12 and 7.13, respectively. Theorem 7.12 Let F : R n ! R n , F 2 C 1 (D), D R n be open and convex.
Reference: [7] <author> T. Chan, E. Gallopoulos, V. Simoncini, T. Szeto, C. Tong QMRCGSTAB: </author> <title> A quasi-minimal residual variant of the BI-CGSTAB algorithm for nonsymmetric systems, </title> <journal> SIAM J. Sci. Stat. Comp. </journal> <month> 15 </month> <year> (1994) </year> <month> 338-347. </month>
Reference-contexts: Freund [18] developed a transpose-free QMR-algorithm (TFQMR), which is related to the CGS-algorithm. Based on these algorithms and the ones mentioned above, a variety of other similar algorithms have been and are being developed that are combinations of the above ideas, such as QMRS [20] or QMRCGSTAB <ref> [7] </ref>, which behave similarly to some of the methods considered above [45]. Two methods that have not received much attention for solving linear systems are Broyden's method [4] and the EN method [13], which was developed fairly recently.
Reference: [8] <author> P.Deuflhard, R. Freund, A. Walter, </author> <title> Fast secant methods for the iterative solution of large nonsymmetric linear systems, </title> <booktitle> IMPACT of Computing in Science and Engineering, </booktitle> <pages> 2(1990) 244-276. </pages>
Reference-contexts: Two methods that have not received much attention for solving linear systems are Broyden's method [4] and the EN method [13], which was developed fairly recently. The family of Broyden methods has suffered from a bad reputation for solving linear systems, but recent efforts <ref> [8] </ref> have shown that different line search strategies lead to versions that are competitive with GMRES. The EN method can be related to GMRES [51]. A closer look at the EN method and Broyden methods shows that they are also related. <p> For case (1), the damped version converges faster while the unmodified version performs better for the other two cases, with case (3) yielding the best results. Deuflhard et al. <ref> [8] </ref> showed that the best ff k for a method depends on the choice of f k and proposed ff k = k r k k q k 16 k r k =q H (dashed curves) methods that are competitive with GMRES. 2.3 The Family of EN-like Methods We have seen <p> k Proof: The proof is straightforward. 2 21 Lemma 2.3 shows that one step of an EN-like method can be considered as an iteration step of the corresponding Broyden method with ff k = 1 followed by an iteration step of Broyden's method using the optimal line search principle of <ref> [8] </ref> without updating the approximation to A 1 . 2.4 Scaling-Invariance As noted earlier, it is possible for scaling of the linear system to affect an iterative solver's performance significantly. This can be made more formal by considering the concept of scaling-invariance. <p> in a form with fewer operations per iteration step than GCR-BM and PBM, we do not include GCR-BM and PBM in our experiments in Chapter 6. 2.6.2 Richardson's Method and the Galerkin Method Another interesting relationship of Broyden's method with Richardson's method and the Galerkin method is pointed out in <ref> [8] </ref>. <p> k+1 = x k + k r k k q k followed by one step of Richardson's method of the first order applied to the preconditioned linear system H k Ax = H k b x k+1 = ~x k+1 + H k (b Ax k+1 ): (2:35) Proof: See <ref> [8] </ref>. 2 Due to the relationship between Broyden's method and the EN-like method given in Lemma 2.3, a similar theorem can be derived for the EN-like method. 30 Theorem 2.5 One step of the EN-like method consists of one Galerkin step followed by one step of Richardson's method. <p> One can develop more efficient versions for all of these methods by avoiding the actual computation of H k+1 . Such an approach can also be found in <ref> [8] </ref> and [15] for Broyden's method and in [13] for the EN method. <p> One would expect such an approach to lead to better convergence, since more information is being kept. We see in Chapter 4 and Chapter 6, that this not always true. In fact, Deuflhard, Freund and Walter <ref> [8] </ref> saw in their experiments for Broyden's methods that this approach in general is worse than restarting. Our experiments show, however, that this result does not transfer to the EN-like methods (see Chapter 6). <p> For Broyden's methods, only the operation counts for optimal line search (where ff k = f H k q k ) according to <ref> [8] </ref> are given. Those for GBM or BBM with ff k = 1 are slightly 43 lower. We use `dmv' to denote dense matrix-vector multiplication. <p> are derived in Section 4.2.3. 4.2.1 Previous Results for Broyden's Method The convergence of the two best known members of the family of Broyden's methods, BBM and GBM, can be characterized in terms of the reduction of the residual and error vectors respectively by the following theorems from the literature <ref> [8] </ref>. Our goal is to demonstrate that the contraction coefficients in the correponding expressions for the EN-like methods are approximately squared, and therefore the desired behavior of a convergence rate double that of Broyden's methods can be expected. <p> Additionally, the convergence is q-superlinear; i.e., there exists a sequence c k with 0 c k &lt; 1 and lim k!1 c k = 0, so that kr k+1 k c k kr k k: (4:17) Proof: See <ref> [8] </ref>. 2 Theorem 4.4 Assume one of the following choices of ff k : (i) ff k = 1, k p k =p H (iii) ff k = p H k H H for GBM (Broyden's method with f k = H H k p k ). <p> Additionally, the convergence is q-superlinear; i.e., there exists a sequence c k with 0 c k &lt; 1 and lim k!1 c k = 0, so that ke k+1 k c k ke k k: (4:20) Proof: See <ref> [8] </ref>. 2 4.2.2 Basic Relationships and Lemmas In order to investigate the convergence for the EN-like methods and certain Broyden methods, we must develop some basic lemmas. The progession of the lemmas has the following general scheme. <p> These are summarized in Lemmas 4.6 and 4.7, respectively. Lemma 4.6 For Broyden's method, r k+1 = [(1 ff k )I + ff k E k ]r k ; (4:21) which for ff k = 1 reduces to r k+1 = E k r k : (4:22) Proof: See <ref> [8] </ref>. 2 Lemma 4.7 For the EN-like method, r k+1 = E k+1 E k r k : (4:23) Proof: Using E k+1 q k = 0, r k+1 = E k+1 r k = E k+1 E k r k The lemmas above and the fact that the behavior of <p> For ff k = q H k r k =kq k k 2 , the inequality is obtained, according to <ref> [8] </ref>, through r k+1 = r k ff k q k q k q H q H ! = I k k q k E k r k : The second part follows from the first part using induction. 2 Lemma 4.14 For BEN (f k = flq k ), EN <p> Then, lim j1 ff k j = 0: (4:49) Proof: Consider kE k k ffi &lt; 1. The proof for choice (i) is given in <ref> [8] </ref>. <p> Then, lim j1 ff k j = 0: (4:51) Proof: Consider k ~ E k k ffi &lt; 1. A proof of (4.50) for choice (i) is given in <ref> [8] </ref>. But, since it is not completely correct, we repeat it here with the necessary correction. <p> k = kH k q k k j1 *=cj kH k q k k 63 since jcj j1 c*j jc *j = jcj 2 + * 2 2*Real (c) 1 + jcj 2 * 2 2*Real (c) ! 1 1: The proof of (4.50) for choice (ii) is given in <ref> [8] </ref>. <p> For ff k = q H k q k , the proof for GCR-BM and PBM is similar to that for GBM in <ref> [8] </ref>. <p> any of the choices above for ff k (except ff k = 1), one obtains using Lemma 4.21 kI ff k H k Ak = k (1 ff k )I + ff k (I H k A)k ffi + (1 + ffi) 1 ffi 2ffi : (4.67) Now, (see also <ref> [8] </ref>) we obtain ke k+1 k = ke k + ff k p k k kI ff k H k Akke k k; 67 which, together with Lemma 4.21, leads to (4.62) and (4.63). <p> Certainly, the finite termination property is lost for these methods. The local convergence behavior of the restarted versions can be explored using the results derived above. The convergence behavior for restarted BBM and GBM with the choices of ff k of Theorems 4.3 and 4.4 is considered in <ref> [8] </ref>. We investigate the convergence of restarted versions of PBM, GCR-BM, the dual PBM, and the dual GCR-BM for various ff k and add two choices for ff k to the results for restarted GBM of [8]. <p> with the choices of ff k of Theorems 4.3 and 4.4 is considered in <ref> [8] </ref>. We investigate the convergence of restarted versions of PBM, GCR-BM, the dual PBM, and the dual GCR-BM for various ff k and add two choices for ff k to the results for restarted GBM of [8]. According to the definition of these variants, one restarts the method after m + 1 steps with H m+1 = H 0 . <p> Unlike the full or restarted method, we are not able to find an upper bound on kE (t) k k, and it is possible that an increase in kE k k prevents convergence. This supports reports in <ref> [8] </ref> that truncated Broyden methods performed worse than restarted methods. There is a convergence result in the literature for a truncated method, ORTHOMIN, which is truncated GCR [12] [14]. A more thorough investigation shows that GCR and ORTHOMIN are related to SEN and tSEN.
Reference: [9] <author> J. Demmel, M. Heath, H. van der Vorst, </author> <title> Parallel numerical linear algebra, </title> <booktitle> Acta Numerica (1993), </booktitle> <pages> 111-197. </pages>
Reference-contexts: In this case its operation count, as given in Table 3.1, is identical to that of eGCR. Since it also can lead to instability, however, it has been suggested that the classical Gram-Schmidt algorithm be used twice <ref> [9] </ref>. In Tables 3.2 and 3.3 the number of floating point operations per iteration step for the truncated and restarted versions are given. We use the notation `tMethod (m)' for the truncated versions and `Method (m)' for the restarted versions. Note that tGCR (m) is better known as ORTHOMIN (m).
Reference: [10] <author> J. Dennis, Jr., J. </author> <title> More, Quasi-Newton methods, motivation and theory, </title> <journal> SIAM Review 1(1977), </journal> <pages> 46-89. </pages>
Reference-contexts: We develop scaling-invariant versions of the EN method in Section 2.4. 2.2 The Family of Broyden Methods An important class of methods based on rank-k updates is the class of quasi-Newton methods <ref> [10] </ref>. The purpose of quasi-Newton methods is to determine a zero of a function F or minimize a function G using approximations of the Jacobian of F , or the Hessian of G, or their inverses. There are several effective quasi-Newton methods, including the Davidon-Fletcher-Powell method and the BFGS method [10]. <p> <ref> [10] </ref>. The purpose of quasi-Newton methods is to determine a zero of a function F or minimize a function G using approximations of the Jacobian of F , or the Hessian of G, or their inverses. There are several effective quasi-Newton methods, including the Davidon-Fletcher-Powell method and the BFGS method [10]. These methods assume symmetric (and often positive definite) matrices, and we do not consider them here, since our goal is to solve nonsymmetric linear systems.
Reference: [11] <author> J. Dennis, Jr., R. Schnabel, </author> <title> Numerical methods for unconstrained optimization and nonlinear equations, </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1983. </year>
Reference-contexts: The k-dimensional Krylov subspace is defined by K k (A; x) := spanfx; Ax; :::; A k1 xg. For the convergence theory in Chapters 4 and 7, we need the following definitions (see also <ref> [11] </ref>): Definition 1.1 Let x 2 C n (or R n ), x k 2 C n (or R n ), k = 0; 1; :::. <p> (x k ) F (x k1 ) The one-dimensional nonlinear EN-like method is p k = b k b k+1 = p k x k+1 = x k b k+1 It is a special case of the finite-difference Newton's method, which allows a more general choice of p k (see <ref> [11, p. 29] </ref>). The following theorem, which states that the secant method has 2-step q-quadratic convergence, can be found in [11]. Theorem 7.2 Let F : (a; b) ! R, F 2 C 1 ((a; b)), F 0 2 Lip fl ((a; b)). <p> The following theorem, which states that the secant method has 2-step q-quadratic convergence, can be found in <ref> [11] </ref>. Theorem 7.2 Let F : (a; b) ! R, F 2 C 1 ((a; b)), F 0 2 Lip fl ((a; b)). Assume jF 0 (x)j ff for some positive ff; , and for all x 2 (a; b). <p> there exists &gt; 0 with jx 0 x fl j &lt; , so that the sequence fx k g, generated by the secant method, is defined, x k 2 (a; b), k = 0; 1; :::, and fx k g converges two-step q-quadratically to x fl ; i.e., Proof: See <ref> [11] </ref>. 2 Interestingly enough, in the one-dimensional case, the EN-like method has q-quadratic con vergence just like Newton's method. 107 Theorem 7.3 Let F : (a; b) ! R, F 2 C 1 ((a; b)), F 0 2 Lip fl ((a; b)). <p> Proof: This theorem follows from the fact that the one-dimensional EN-like method is a special case of the finite difference Newton's method. The proof for the latter method can be found in <ref> [11] </ref>. 2 Considering the complexity of the two methods, the secant method requires one function evaluation and 5 operations per iteration step, whereas the nonlinear EN-like method requires two function evaluations and 6 operations per iteration step, and is therefore, in view of the fact that it converges twice as fast, <p> One can, however, show superlinear convergence for the nonlinear Broyden's `good' method with f k = H T k p k (GBM) and the nonlinear Broyden's `bad' method with f k = q k (BBM) <ref> [11] </ref>. Theorem 7.4 Let F : R n ! R n , F 2 C 1 (D), D R n be open and convex. <p> One shows specifically kB k J (x fl )k (2 t k )ffi; k = 0; 1; :::; (7.6) (7.8) with the following conditions for * and ffi: fiffi t ; (7.9) 1 t ffi: (7.10) The complete proof is given in <ref> [11] </ref> for the special case t = 1 2 . 2 Theorem 7.5 Let F : R n ! R n , F 2 C 1 (D), D R n be open and convex. <p> 1 k (2 t k )ffi; k 0; (7.12) (7.14) with the following conditions for * and ffi: fl* (1 t 2 )fiffi + (1 + t )fi 2 ; (7.15) 2ffffi + ( fi + ffi)fl* t: (7.16) The proof is similar to the proof of Theorem 7.4 (see <ref> [11] </ref>) and the proof for nonlinear BEN, which is given below. 2 In [24], Gay went a step further and showed local 2n-step q-quadratic convergence for nonlinear GBM, using the finite termination property of Broyden's method. <p> Lemma 7.1 Let F : R n ! R n , F 2 C 1 (D), D R n open and convex, J (x) 2 Lip fl (D). Then, for any u; v 2 D, kF (v) F (u) J (x)(v u)k fl 2 110 Proof: See <ref> [11] </ref>, p. 77. 2 Lemma 7.2 Let F : R n ! R n , F 2 C 1 (D), D R n open and convex, J (x) 2 Lip fl (D). <p> If additionally J (x) 1 , then there exists * &gt; 0, 0 &lt; ff &lt; fi, such that ffkv uk kF (v) F (u)k fikv uk (7:20) for all u; v 2 D for which max (kv xk; ku xk) *. Proof: See <ref> [11] </ref>, p. 77. 2 The following lemma gives an estimate of the norm of the difference between the approximation B k of the Jacobian of F at the solution, J (x fl ), and J (x fl ) itself, and the corresponding inverses. <p> Lemma 7.3 For f k = H T k p k : kq k J (xfl)p k k : (7:21) kH k+1 J (x fl ) 1 k kH k J (x fl ) 1 k + kq k k Proof: See <ref> [11] </ref>, p. 176. 2 Note that an increase in the norms of the differences as k increases is possible (as opposed to the linear case where it is not). <p> Under the assumptions of Theorem 7.8, for f k = H T k p k : k!1 kp k k Under the assumptions of Theorem 7.9, for f k = q k : lim k (H k J (x fl ) 1 )q k k = 0: (7:49) Proof: See <ref> [11] </ref>, pp. 183-184. 2 Theorems 7.10 and 7.11 demonstrate the superlinear convergence for nonlinear GEN and BEN, respectively. Theorem 7.10 Let F : R n ! R n , F 2 C 1 (D), D R n be open and convex.
Reference: [12] <author> S. Eisenstat, H. Elman, M. Schultz, </author> <title> Variational iterative methods for nonsymmetric systems of linear equations, </title> <journal> SIAM J. Numer. Anal. </journal> <year> (1983) </year> <month> 345-357. </month>
Reference-contexts: See Figure 2.3 for their convergence behavior when applied to our test problem. There is another choice of f k that leads to a version of Broyden's method that is equivalent to the generalized conjugate residual method (GCR) <ref> [12] </ref> or GMRES with the initial vector ~x 0 = x 0 + H 0 r 0 and consequently terminates within n steps [51]. For this method we choose f k = (I AH k ) H (I AH k )q k . <p> either too costly or inferior to the SEN and the `s'-methods. 2.5 The Generalized Conjugate Residual Algorithm (GCR) Another well-known algorithm that is related in several ways to the methods considered in this thesis and plays a major role in the following sections is the generalized conjugate residual algorithm (GCR) <ref> [12] </ref> [14]. The algorithm is given below: Algorithm 6. <p> This supports reports in [8] that truncated Broyden methods performed worse than restarted methods. There is a convergence result in the literature for a truncated method, ORTHOMIN, which is truncated GCR <ref> [12] </ref> [14]. A more thorough investigation shows that GCR and ORTHOMIN are related to SEN and tSEN. Lemma 4.23 If the scaling parameter fl k in (3.7) equals 0, the SEN iteration step is reduced to a GCR iteration step. <p> k ) 2 = (1 i=(m) r T kr k k 2 k AH 0 r k (1 i=(m) 2 max (H T )kr k k 2 : The second inequality can be proved similarly using the following inequality, which is valid for all x 6= 0 and proved in <ref> [12] </ref>, x T AH 0 x min (M ) 2 In the same way, we can prove convergence for the full SEN method and the restarted method SEN (m) and obtain similar estimates for the norm of residual with a slight change in the lower sum index. <p> Addition ally, Inequalities (4.85) and (4.86) are valid, with (m) = 0 for SEN and SEN (m). If we recall the equivalent estimate for ORTHOMIN or GCR <ref> [12] </ref> [14], kr k+1 k (1 min (AH 0 ) 0 A T AH 0 ) we see that the main difference between the two estimates is the factor 1 i=(m) which equals 1 only when AH 0 r k is orthogonal to all c i ; i = (m); :::;
Reference: [13] <author> T. Eirola, O. Nevanlinna, </author> <title> Accelerating with rank-one updates, </title> <journal> Lin. Alg. and its Appl., </journal> <volume> 121(1989), </volume> <pages> 511-520. </pages>
Reference-contexts: Two methods that have not received much attention for solving linear systems are Broyden's method [4] and the EN method <ref> [13] </ref>, which was developed fairly recently. The family of Broyden methods has suffered from a bad reputation for solving linear systems, but recent efforts [8] have shown that different line search strategies lead to versions that are competitive with GMRES. The EN method can be related to GMRES [51]. <p> In Section 2.6, relationships of Broyden and EN-like methods to other methods are presented. Finally, Section 2.7 contains a summary of the methods. 2.1 The EN Method The EN method was first proposed by Eirola and Nevanlinna <ref> [13] </ref>. The main idea is to improve an approximation to A 1 , H k , via a rank-one update on each iteration of the method while simultaneously improving an approximation x k to the solution of the linear system. <p> Theorem 2.1 summarizes a few of the 7 basic characteristics of the EN method that have been noted in the literature (see <ref> [13] </ref> and [51]) but not proved and presented in a unified fashion. <p> k = 0; 1; : : :: ~c H i ~u k = H 0 r k i=0 ~c k = AH 0 r k i=0 k = k~c k k 2 ~c H k r k+1 = r k ffi k ~c k As mentioned by Eirola and Nevanlinna <ref> [13] </ref> and proved by Vuik and van der Vorst [51], this algorithm can also be obtained by replacing the evaluation of ~u k in the EN method with ~u k = H k r k : (2:29) Consequently, GCR can also be considered as a method based on rank-one updates. <p> One can develop more efficient versions for all of these methods by avoiding the actual computation of H k+1 . Such an approach can also be found in [8] and [15] for Broyden's method and in <ref> [13] </ref> for the EN method. <p> we use ~u k = H k E k r k : Due to the orthogonality of the ~c i (see (ii) in Theorem 2.1), we have q H i E i x = q H i E 0 x: This leads to the following version for SEN (see also <ref> [13] </ref> [51]): Algorithm 11. <p> As above, we conclude lim kE H kE k q k k As shown in <ref> [13] </ref>, kE H kE k q k k kxk=1 k kE k q l k kq k k E H E k q k = kq k k Consequently, (4.40) follows also for f k = flE H k E k q k .
Reference: [14] <author> H. Elman, </author> <title> Iterative methods for large, sparse, nonsymmetric systems of linear equations, </title> <type> Research Report 229, </type> <institution> Yale University, </institution> <year> 1982. </year> <month> 129 </month>
Reference-contexts: One can maintain both properties by considering the normal equations instead of the original system. The new matrices A T A (CGNR) or AA T (CGNE) are symmetric positive definite, and the CG algorithm can therefore be applied to them. This approach is considered in <ref> [14] </ref>. The disadvantage of using the normal equations is that the condition number, which plays a role in the convergence rate of CG, increases significantly. One can maintain the minimization property by choosing the direction vector as a linear combination of the residual vector and k previous direction vectors. <p> One can maintain the minimization property by choosing the direction vector as a linear combination of the residual vector and k previous direction vectors. Such an approach is used, e.g., for the generalized conjugate residual method (GCR) <ref> [14] </ref> or the generalized minimal residual method (GMRES) [39]. The disadvantage of these methods is that they require substantial storage for the previous direction vectors. <p> too costly or inferior to the SEN and the `s'-methods. 2.5 The Generalized Conjugate Residual Algorithm (GCR) Another well-known algorithm that is related in several ways to the methods considered in this thesis and plays a major role in the following sections is the generalized conjugate residual algorithm (GCR) [12] <ref> [14] </ref>. The algorithm is given below: Algorithm 6. <p> In fact, Theorem 1.1 is also valid for GCR using the same proof and yields the following properties. (Properties (ii) through (iv) can also be found in <ref> [14] </ref>.) Corollary 2.1 Define the following vectors: u k := kA~u k k Then, GCR has the following properties (i) E k+1 = (I c k c H (ii) c H i c j = ffi ij (orthogonality), 27 (iii) u H i A H Au j = ffi ij (A <p> This supports reports in [8] that truncated Broyden methods performed worse than restarted methods. There is a convergence result in the literature for a truncated method, ORTHOMIN, which is truncated GCR [12] <ref> [14] </ref>. A more thorough investigation shows that GCR and ORTHOMIN are related to SEN and tSEN. Lemma 4.23 If the scaling parameter fl k in (3.7) equals 0, the SEN iteration step is reduced to a GCR iteration step. <p> Addition ally, Inequalities (4.85) and (4.86) are valid, with (m) = 0 for SEN and SEN (m). If we recall the equivalent estimate for ORTHOMIN or GCR [12] <ref> [14] </ref>, kr k+1 k (1 min (AH 0 ) 0 A T AH 0 ) we see that the main difference between the two estimates is the factor 1 i=(m) which equals 1 only when AH 0 r k is orthogonal to all c i ; i = (m); :::; k
Reference: [15] <author> M. Engleman, G. Strang, K.-J. Bathe, </author> <title> The application of quasi-Newton meth-ods in fluid mechanics, </title> <booktitle> International Journal for Numerical Methods in Engineering, </booktitle> <pages> 17(1981) 707-718. </pages>
Reference-contexts: One can develop more efficient versions for all of these methods by avoiding the actual computation of H k+1 . Such an approach can also be found in [8] and <ref> [15] </ref> for Broyden's method and in [13] for the EN method.
Reference: [16] <author> V. Faber, T. Manteuffel, </author> <title> Necessary and sufficient conditions for the existence of a conjugate gradient method, </title> <journal> SIAM J. Numer. Anal., </journal> <pages> 21(1984) 352-362. </pages>
Reference-contexts: CG has two fundamental properties: it minimizes the residual vector along each iteration step, and the residual vectors satisfy a three-term recurrence. It is not possible to maintain both properties for a nonsymmetric linear system <ref> [16] </ref>, [49]. There are several ways to deal with this situation. One can maintain both properties by considering the normal equations instead of the original system.
Reference: [17] <author> R. Fletcher, </author> <title> Conjugate gradient methods for indefinite systems, in Numerical Analysis Dundee 1975, </title> <editor> G. Watson, ed., </editor> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: The possibility of increasing k gives these methods robustness compared to those considered next, but at the cost of requiring more storage. One can also choose to maintain the three-term recurrence property, as is done, e.g., in the biconjugate gradient algorithm (BICG) <ref> [17] </ref>, conjugate gradient squared (CGS) [41], and the stabilized version of CGS (BICGSTAB) [47]. This approach leads to a low memory requirement, but also to a lack of robustness. This can be mitigated by the use of a preconditioner, but the methods often fail completely [28].
Reference: [18] <author> R. Freund, </author> <title> A transpose-free quasi-minimal residual algorithm for non-Hermitian linear systems, </title> <note> SIAM J. on Scientific Computing 14 (1993) 470ff. </note>
Reference-contexts: Another approach is the quasi-minimal residual algorithm (QMR) [19], which is based on a quasi-minimization of the residual. The original QMR-algorithm involves the, possibly costly, multiplication of the transpose of the matrix with a vector. Freund <ref> [18] </ref> developed a transpose-free QMR-algorithm (TFQMR), which is related to the CGS-algorithm.
Reference: [19] <author> R. Freund, N. Nachtigal, </author> <title> QMR: A quasi-minimal residual method for non-Hermitian linear systems, </title> <type> Numerische Mathematik 60(1991) 315-339. </type>
Reference-contexts: This approach leads to a low memory requirement, but also to a lack of robustness. This can be mitigated by the use of a preconditioner, but the methods often fail completely [28]. Another approach is the quasi-minimal residual algorithm (QMR) <ref> [19] </ref>, which is based on a quasi-minimization of the residual. The original QMR-algorithm involves the, possibly costly, multiplication of the transpose of the matrix with a vector. Freund [18] developed a transpose-free QMR-algorithm (TFQMR), which is related to the CGS-algorithm.
Reference: [20] <author> R. Freund, T. Szeto, </author> <title> A quasi-minimal residual squared algorithm for non-Hermitian linear systems Technical Report, </title> <type> RIACS, </type> <institution> NASA Ames Res. Center, </institution> <year> (1991). </year>
Reference-contexts: Freund [18] developed a transpose-free QMR-algorithm (TFQMR), which is related to the CGS-algorithm. Based on these algorithms and the ones mentioned above, a variety of other similar algorithms have been and are being developed that are combinations of the above ideas, such as QMRS <ref> [20] </ref> or QMRCGSTAB [7], which behave similarly to some of the methods considered above [45]. Two methods that have not received much attention for solving linear systems are Broyden's method [4] and the EN method [13], which was developed fairly recently.
Reference: [21] <author> R. Freund, G. Golub, N. Nachtigal, </author> <title> Iterative solution of linear systems, </title> <journal> Acta Numerica, </journal> <year> (1992) </year> <month> 57-100. </month>
Reference-contexts: INTRODUCTION 1.1 Motivation There still is a great need to find a robust efficient iterative solver for general sparse linear systems, Ax = b, where A is an n fi n-matrix. Such systems arise in many applications. A large number of efficient iterative methods <ref> [21] </ref> have been developed. Many of these methods often fail, however, but the more robust methods available tend to converge too slowly [28].
Reference: [22] <author> K. Gallivan, A. Sameh, Z. Zlatev, </author> <title> A parallel hybrid sparse linear system solver, </title> <booktitle> Computing Systems in Engineering, </booktitle> <pages> 1(1990) 183-195. </pages>
Reference-contexts: For the former type of preconditioning we assess the new algorithms as inner, as well as outer, method. The latter preconditioner is taken from PARASPAR, a robust parallel software package based on Y12M <ref> [22] </ref>, which has many other interesting features. The new family of methods appears to be very suitable for the strategy used in PARASPAR that gives it its robustness. <p> This can be of advantage on an architecture with very fast dense matrix vector operations or in the case of an expensive sparse matrix vector multiplication and/or preconditioning step. 5.3 PARASPAR A hybrid software package called PARASPAR, which is based on Y12M <ref> [22] </ref> [52] and combines both iterative and direct methods, has been used as a framework for some of our experiments. Although direct methods (sparse Gaussian elimination schemes) generally achieve high accuracy, they are often too time consuming and can have difficulty exploiting parallelism.
Reference: [23] <author> D. Gay, R. Schnabel, </author> <title> Solving systems of nonlinear equations by Broyden's method with projected updates, </title> <editor> in O. L. Mangasarian, R. Meyer, S. Robinson (eds.), </editor> <booktitle> Nonlinear Programming 3, </booktitle> <pages> pp. 245-281, </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Both methods were developed independently by Gay and Schnabel <ref> [23] </ref> who call them Broy-den's method with projected updates. We call the first method with projected updates `PBM', and the second one, which was inspired by Broyden's direct method `dual PBM'. See Figure 2.3 for their convergence behavior when applied to our test problem. <p> equals its maximal singular value. 57 The proof of (4.29) for f k = fl (q k P k1 i q k ~q i ) is similar to that of (4.32) for f k = flH H P k1 i p k ~p i ) and can be found in <ref> [23] </ref>. <p> EN-like tSEN (m) (2) kr k+1 k C (1 2 max (H T methods (3) kr k+1 k C (1 2 min (M) max (M)+j max (R)j 2 ) 1=2 kr k k where C = (1 P k1 Table 4.2 Summary of convergence results for EN-like methods 76 systems <ref> [23] </ref>, it has not been presented in this form before. The new convergence results for the full Broyden methods are summarized in Table 4.5. We have not summarized those for the restarted versions here, which are found in Theorems 4.9 and 4.10, since they are very similar.
Reference: [24] <author> D. Gay, </author> <title> Some convergence properties of Broyden's method, </title> <journal> SIAM J. Numer. Anal. </journal> <volume> 16(1979), </volume> <pages> 623-630. </pages>
Reference-contexts: The most obvious choice is ff k = 1. One can show that for this case Broyden's method terminates within at most 2n steps <ref> [24] </ref> (see also Chapter 4). However, the convergence behavior of GBM and BBM for case (1) of our test problem in Figure 2.2 shows that this is not always a desirable choice despite its finite termination property. <p> One of the amazing and unexpected properties of Broyden's method is its finite termination property when ff k = 1. This is shown by Gay in <ref> [24] </ref>. Theorem 4.1 For ff k = 1, f H k q k 6= 0, Broyden's method converges within at most 2n steps. Proof: See [24]. 2 Gay also shows that this upper bound on the number of iterations required for convergence is tight; i.e., convergence can take a full 2n <p> This is shown by Gay in <ref> [24] </ref>. Theorem 4.1 For ff k = 1, f H k q k 6= 0, Broyden's method converges within at most 2n steps. Proof: See [24]. 2 Gay also shows that this upper bound on the number of iterations required for convergence is tight; i.e., convergence can take a full 2n steps. <p> Lemma 4.1 If k 1, q k 6= 0, v H k q k1 6= 0, rank (E k )=n 1, then, rank (E k+1 ) = n 1 and E k+1 q k = 0. Proof: See <ref> [24] </ref>. 2 One interesting consequence of Lemma 4.1 is that, under its assumptions, the matrices H k agree with the inverse of A only on a subspace of dimension 1. <p> conditions for * and ffi: fl* (1 t 2 )fiffi + (1 + t )fi 2 ; (7.15) 2ffffi + ( fi + ffi)fl* t: (7.16) The proof is similar to the proof of Theorem 7.4 (see [11]) and the proof for nonlinear BEN, which is given below. 2 In <ref> [24] </ref>, Gay went a step further and showed local 2n-step q-quadratic convergence for nonlinear GBM, using the finite termination property of Broyden's method. Theorem 7.6 Let F : R n ! R n , F 2 C 1 (D), D R n be open and convex. <p> there exist *; ffi &gt; 0, so that for kx 0 x fl k * and kB 0 J (x fl )k ffi, nonlinear GBM is well defined and converges 2n-step q-quadratically; i.e., kx k+2n x fl k C 1 kx k x fl k 2 : (7:17) Proof: See <ref> [24] </ref>. 2 Theorem 7.7 Let F : R n ! R n , F 2 C 1 (D), D R n be open and convex. <p> *; ffi &gt; 0, so that for kx 0 x fl k * and kH 0 J (x fl ) 1 k ffi, nonlinear BBM is well defined and converges 2n-step q-quadratically, i.e. kx k+2n x fl k C 1 kx k x fl k 2 : (7:18) Proof: See <ref> [24] </ref>. 2 For each of the two choices of f k in Broyden's method there is a corresponding EN-like method: nonlinear GEN uses f k = H T k p k and corresponds to nonlinear GBM; nonlinear BEN uses f k = q k and corresponds to nonlinear BBM. <p> fl ))e k k = ke k + p k + H k (F (x k ) F (x fl ) J (x fl )e k )k fl ke k k 2 : Combining these equations with (7.59) leads to lim ke k+1 k = 0: (7:65) Just as Gay <ref> [24] </ref> showed local 2n-step q-quadratic convergence for Broyden's method, using the finite termination property of Broyden's method and the local convergence properties in [6], we show local n-step q-quadratic convergence for nonlinear GEN and BEN in Theorems 7.12 and 7.13, respectively.
Reference: [25] <author> R. Horn, C. Johnson, </author> <title> Topics in matrix analysis, </title> <publisher> Cambridge University Press, </publisher> <year> 1991. </year>
Reference-contexts: Properties (ii) and (iv) imply that the EN method terminates within n steps (see Chapter 4). Property (v) implies that the 2-norm and the Frobenius norm (or any other unitarily invariant norm <ref> [25] </ref>) of E k do not increase with k, a fact that is used for the convergence theory in Chapter 4. To illustrate the convergence behavior of the methods presented in this chapter we use a test problem from [51].
Reference: [26] <author> D. Liu, J. Nocedal, </author> <title> On the limited memory BFGS method for large scale optimization, </title> <booktitle> Mathematical Programming 45(1989), </booktitle> <pages> 503-528. </pages>
Reference-contexts: Such versions have been considered before in the context of quasi-Newton methods <ref> [26] </ref>, [31]. Additionally, it will be interesting to investigate these new nonlinear methods in an application specific context, e.g., the integration of differential-algebraic systems for power systems analysis involves nonlinear/linear systems, which are very difficult to solve. 128
Reference: [27] <author> D. Luenberger, </author> <title> Linear and nonlinear programming, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1984. </year>
Reference-contexts: Consequently, this method terminates within at most n steps (see also <ref> [27] </ref>), since the algorithm constructs H k , a better approximation to A 1 , on the k-th iteration, until finally H n = A 1 , if f H i q i 6= 0; i = 0; :::; n1.
Reference: [28] <author> U. Meier Yang, </author> <title> Preconditioned conjugate gradient-like methods for nonsymmetric linear systems, </title> <type> CSRD Tech. Report 1210, </type> <institution> Center for Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: Such systems arise in many applications. A large number of efficient iterative methods [21] have been developed. Many of these methods often fail, however, but the more robust methods available tend to converge too slowly <ref> [28] </ref>. The most popular solvers are influenced by the conjugate gradient method (CG), which is a very effective method for solving symmetric positive definite sparse linear systems but is not directly applicable to nonsymmetric or indefinite linear systems. <p> This approach leads to a low memory requirement, but also to a lack of robustness. This can be mitigated by the use of a preconditioner, but the methods often fail completely <ref> [28] </ref>. Another approach is the quasi-minimal residual algorithm (QMR) [19], which is based on a quasi-minimization of the residual. The original QMR-algorithm involves the, possibly costly, multiplication of the transpose of the matrix with a vector. Freund [18] developed a transpose-free QMR-algorithm (TFQMR), which is related to the CGS-algorithm.
Reference: [29] <author> J. Meijerink, H. van der Vorst, </author> <title> An iterative solution method for linear systems for which the coefficient matrix is a symmetric M-matrix, </title> <journal> Math. Comp. </journal> <pages> 31(1977) 148-162. </pages>
Reference: [30] <author> J. More, J. Traugenstein, </author> <title> On the global convergence of Broyden's method, </title> <note> Mathematics of Computation 30(1976) 523-540. 130 </note>
Reference-contexts: then lim kE k q k k = 0; (4:40) k E k q k , kE k q k k 6= 0, additionally, lim kE H kE k q k k Proof: The proof of (4.46) for f k = flH H k p k can be found in <ref> [30] </ref>. Using the same strategy, one can prove the rest of the lemma. Let us first consider f k = flq k .
Reference: [31] <author> J. Nocedal, </author> <title> Updating Quasi-Newton matrices with limited storage, </title> <note> Mathematics of Computation 35(1980) 773-782. </note>
Reference-contexts: Such versions have been considered before in the context of quasi-Newton methods [26], <ref> [31] </ref>. Additionally, it will be interesting to investigate these new nonlinear methods in an application specific context, e.g., the integration of differential-algebraic systems for power systems analysis involves nonlinear/linear systems, which are very difficult to solve. 128
Reference: [32] <author> D. O'Leary, </author> <title> Why Broyden's nonsymmetric method terminates on linear equations, </title> <type> Tech. Report CS-TR-3045, </type> <institution> University of Maryland, </institution> <year> 1993, </year> <note> to appear in SIAM J. on Optimization. </note>
Reference-contexts: Gay's proof of finite termination, while significant, does not offer an insight into the operation of Broyden's method. Recently, O'Leary <ref> [32] </ref> characterized the subspace that Broyden's method implicitly constructs. The subspace dimension increases every other step and is orthogonal to the last residual. The orthogonality property guarantees finite termination. Lemma 4.2 Assume v H j q j1 6= 0; j = 1; :::; k, and q k 6= 0. <p> Additionally, z H (0) z H Proof: See <ref> [32] </ref>. 2 O' Leary's approach can be used along with the relationship between Broyden's method and the general EN-like method to prove finite termination for the EN-like methods. Lemmas 4.3, 4.4, and 4.5 apply this approach and characterize the behavior of the rank of the error matrix E k .
Reference: [33] <author> J. Ortega, W. Rheinboldt, </author> <title> Iterative solution of nonlinear equations in several variables, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: For its proof we need the Perturbation Lemma from <ref> [33] </ref>. Lemma 4.22 (Perturbation Lemma) Assume that A and C are matrices of order n, A is nonsingular with kA 1 k ff, kA Ck fi, and fffi &lt; 1. C is then also nonsingular, and kC 1 k 1 fffi : (4:61) Proof: See [33]. 2 The following theorem overlaps <p> need the Perturbation Lemma from <ref> [33] </ref>. Lemma 4.22 (Perturbation Lemma) Assume that A and C are matrices of order n, A is nonsingular with kA 1 k ff, kA Ck fi, and fffi &lt; 1. C is then also nonsingular, and kC 1 k 1 fffi : (4:61) Proof: See [33]. 2 The following theorem overlaps with Theorem 4.4, which states the results for GBM with the first three choices for ff k . We introduce two new choices for ff k and consider also the dual PBM and the dual GCR-BM. <p> According to (7.23), (7.26), and the Perturbation Lemma <ref> [33, p. 45] </ref>, B 1 k exists, and kB 1 fi 1 2fiffi (7.35) 2 t 2 )fi: (7.36) Assume that the assertion is true, for i = 1; :::; k.
Reference: [34] <author> J. Paloschi, J. Perkins, </author> <title> Scale invariant Quasi-Newton methods for the solution of nonlinear equations, </title> <institution> Comput. Chem. </institution> <address> Engineering 12(1988) 91-97. </address>
Reference-contexts: For these reasons, these versions are not investigated further in this thesis. The problem of scaling-invariance has been considered in the context of quasi-Newton methods for nonlinear systems. Since Newton's method is scaling-invariant, some authors were interested in developing scaling-invariant quasi-Newton methods [1] <ref> [34] </ref>. The definition they use is more general than the one above. Note that none of the scaling-invariant algorithms above is scaling-invariant according to the new definition below. <p> In <ref> [34] </ref>, Paloschi and Perkins show circumstances under which Broyden's method is scaling-invariant according to their definition. The same result is true for the EN-like method and can be shown accordingly.
Reference: [35] <author> V. Pan, R. Schreiber, </author> <title> An Improved Newton Iteration for the Generalized Inverse of a Matrix, with Applications, </title> <type> Tech. Report. </type>
Reference-contexts: In this case, H k+1 acts on r k in a manner similar to the Newton iteration. Using this knowledge, it is possible to try to improve the convergence by using an acceleration scheme as has been used for Newton's method. Such a scheme has been proposed in <ref> [35] </ref>, and has the form H N where ff k is evaluated through the following recursions: ff 0 = 2 n 0 = ff 0 2 k = ff k1 (2 k1 ) k1 ; 2 ; where k 1, and 1 and n are the smallest and the largest singular
Reference: [36] <author> H. </author> <title> Rutishauser, Theory of gradient methods, </title> <institution> Mitteilungen aus dem Institut fuer Angewandte Mathematik 8(1959) 24-29. </institution>
Reference-contexts: therefore presented in this chapter. 5.1 Preconditioning with Iterative Methods The idea of using an iterative method as a preconditioner can be found in the CGT-method introduced by Rutishauser, who used the Chebyshev method as inner method to precondition the conjugate gradient method, an approach equivalent to Chebyshev polynomial preconditioning <ref> [36] </ref>. Methods have also been proposed for the nonsymmetric case. These include FGMRES [37] with GMRES as the outer method, GMRESR [48] with GCR as the outer method and GMRES as the inner method, and others considered further in this and the following section.
Reference: [37] <author> Y. Saad, </author> <title> A flexible inner-outer preconditioned GMRES algorithm, </title> <type> Tech. Report, </type> <institution> Computer Science Department and MSI, University of Minnesota, </institution> <year> 1991. </year>
Reference-contexts: There are different ways to precondition iterative methods. We will consider here two different types of preconditioners, the use of an inner iterative method as a preconditioner similar to GMRESR [48] or FGMRES <ref> [37] </ref> as well as an Incomplete LU factorization based on numerical dropping. For the former type of preconditioning we assess the new algorithms as inner, as well as outer, method. <p> Methods have also been proposed for the nonsymmetric case. These include FGMRES <ref> [37] </ref> with GMRES as the outer method, GMRESR [48] with GCR as the outer method and GMRES as the inner method, and others considered further in this and the following section.
Reference: [38] <author> Y. Saad, M. Schultz, </author> <title> Conjugate gradient-like algorithms for solving nonsymmetric linear systems, </title> <journal> Math. Comp, </journal> <pages> 44(1985) 417-424. </pages>
Reference-contexts: Therefore, we consider restarted and truncated versions. An overview of restarted and truncated algorithms can be found in <ref> [38] </ref>. The restarted version of any of the methods can be described as follows: Algorithm 14. Restarted method (m). Do, until convergence, 1.
Reference: [39] <author> Y. Saad, M. Schultz, </author> <title> GMRES: a generalized minimal residual algorithm for solving nonsymmetric linear systems, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <pages> 7(1986) 856-869. </pages>
Reference-contexts: One can maintain the minimization property by choosing the direction vector as a linear combination of the residual vector and k previous direction vectors. Such an approach is used, e.g., for the generalized conjugate residual method (GCR) [14] or the generalized minimal residual method (GMRES) <ref> [39] </ref>. The disadvantage of these methods is that they require substantial storage for the previous direction vectors. In order to reduce the storage, restarted or truncated versions of the above algorithms are used, which involve a fixed number k of previous 1 direction vectors. <p> In Table 3.1 the types and number of operations are given for a variety of methods. We have included GCR and the equivalent GMRES <ref> [39] </ref> here, since they are related to the methods considered (see next section), and CGS [41] and BiCGSTAB [47], since they are very popular solvers.
Reference: [40] <author> G. Schultz, </author> <title> Iterative Berechnung der reziproken Matrix, </title> <journal> Z. Angew. Math. Mech. </journal> <pages> 13(1933) 57-59. </pages>
Reference-contexts: Such a method can be found in <ref> [40] </ref>.
Reference: [41] <author> P. Sonneveld, </author> <title> CGS, a fast Lanczos-type solver for nonsymmetric linear systems, </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <pages> 10(1989) 36-52. </pages>
Reference-contexts: The possibility of increasing k gives these methods robustness compared to those considered next, but at the cost of requiring more storage. One can also choose to maintain the three-term recurrence property, as is done, e.g., in the biconjugate gradient algorithm (BICG) [17], conjugate gradient squared (CGS) <ref> [41] </ref>, and the stabilized version of CGS (BICGSTAB) [47]. This approach leads to a low memory requirement, but also to a lack of robustness. This can be mitigated by the use of a preconditioner, but the methods often fail completely [28]. <p> In Table 3.1 the types and number of operations are given for a variety of methods. We have included GCR and the equivalent GMRES [39] here, since they are related to the methods considered (see next section), and CGS <ref> [41] </ref> and BiCGSTAB [47], since they are very popular solvers. For Broyden's methods, only the operation counts for optimal line search (where ff k = f H k q k ) according to [8] are given. Those for GBM or BBM with ff k = 1 are slightly 43 lower.
Reference: [42] <author> G. W. Stewart, </author> <title> Introduction to Matrix Computations, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: H = q H j E i q j (iii) follows from (ii) since c i = Au i . (iv) Using (i), we have c H i E i+1 r i i (I c i c H = 0: (v) can be proved using (i) and Theorem 5.9 in <ref> [42] </ref>.
Reference: [43] <author> E. de Sturler, D. Fokkema, </author> <title> Nested Krylov methods and preserving the orthogonality, </title> <editor> in T. Manteuffel and S. McCormick, eds., </editor> <booktitle> Proceedings of the Sixth Copper Mountain Multigrid Conference on Multigrid Methods, </booktitle> <address> VA, </address> <year> 1993. </year>
Reference-contexts: Our experiments show, however, that this result does not transfer to the EN-like methods (see Chapter 6). It is also possible to use more sophisticated truncation schemes, which can lead to better convergence, see <ref> [43] </ref> and [50]. The application of these to the family of EN-like methods is left as future work. The truncation of Algorithms 12 and 13 is far more complicated than the other algorithms. De Sturler and Fokkema describe in [43] how to truncate Algorithm 12. <p> more sophisticated truncation schemes, which can lead to better convergence, see <ref> [43] </ref> and [50]. The application of these to the family of EN-like methods is left as future work. The truncation of Algorithms 12 and 13 is far more complicated than the other algorithms. De Sturler and Fokkema describe in [43] how to truncate Algorithm 12. The difficulty is that for this version x k is not updated in each iteration step. However due to the truncation, the basis of the Krylov subspace and the parameters change at each iteration step and need to be 42 updated. <p> This was observed by de Sturler and Fokkema for the case of GMRESR <ref> [43] </ref>. Here, the outer method generates a minimal residual polynomial that is completely ignored by the inner method, which searches for a new minimal residual polynomial. <p> inverse preconditioners might yield an effective iterative method/preconditioner combination with less of the difficulties associated with maintaining sparsity in the approximate inverses. * Since truncation has shown to be promising for many problems, but also has its difficulties for other problems, we would like to consider more sophisticated truncation schemes <ref> [43] </ref>, [50]. Since for some problems restarted methods perform far better than truncated methods, the development of hybrids of restarted and truncated methods appears to be a promising approach. 127 * Orthogonality preserving methods are trading sparse for dense matrix vector operations.
Reference: [44] <author> E. de Sturler, </author> <title> Nested Krylov methods based on GCR, </title> <type> Technical Report 93-50, </type> <institution> Faculty of Technical Mathematics and Informatics, Delft University of Technology, Delft, </institution> <address> Holland, </address> <year> 1993, </year> <note> to appear in Journal of Computational and Applied Mathematics. </note>
Reference-contexts: If we do not require the evaluation of x k+1 at each iteration step, it is possible to achieve further savings in EN and SEN by avoiding the evaluation of u k . Only the coefficients need to be saved. Such an approach has been used in <ref> [44] </ref>, to improve the implementation for GCR given in Algorithm 6.
Reference: [45] <author> C. Tong, </author> <title> A comparative study of preconditioned Lanczos methods for nonsymmetric linear systems, </title> <type> Tech. Report, </type> <institution> Center for Computational Engineering, Sandia National Laboratories, Livermore, </institution> <address> CA, </address> <year> 1992. </year> <month> 131 </month>
Reference-contexts: Based on these algorithms and the ones mentioned above, a variety of other similar algorithms have been and are being developed that are combinations of the above ideas, such as QMRS [20] or QMRCGSTAB [7], which behave similarly to some of the methods considered above <ref> [45] </ref>. Two methods that have not received much attention for solving linear systems are Broyden's method [4] and the EN method [13], which was developed fairly recently.
Reference: [46] <author> P. Vinsome, Orthomin, </author> <title> an iterative method for solving sparse sets of simultaneous linear equations, </title> <booktitle> in Proc. Fourth Symp. on Reservoir Simulation, </booktitle> <institution> Society of Petr. Eng. of AIME, </institution> <year> 1976. </year>
Reference: [47] <author> H. van der Vorst, </author> <title> BI-CGSTAB: a fast and smoothly converging variant of BI-CG for the solution of nonsymmetric linear systems, </title> <journal> SIAM J. Sci. Stat. Comp. </journal> <month> 13 </month> <year> (1992) </year> <month> 631-644. </month>
Reference-contexts: One can also choose to maintain the three-term recurrence property, as is done, e.g., in the biconjugate gradient algorithm (BICG) [17], conjugate gradient squared (CGS) [41], and the stabilized version of CGS (BICGSTAB) <ref> [47] </ref>. This approach leads to a low memory requirement, but also to a lack of robustness. This can be mitigated by the use of a preconditioner, but the methods often fail completely [28]. <p> In Table 3.1 the types and number of operations are given for a variety of methods. We have included GCR and the equivalent GMRES [39] here, since they are related to the methods considered (see next section), and CGS [41] and BiCGSTAB <ref> [47] </ref>, since they are very popular solvers. For Broyden's methods, only the operation counts for optimal line search (where ff k = f H k q k ) according to [8] are given. Those for GBM or BBM with ff k = 1 are slightly 43 lower.
Reference: [48] <author> H. van der Vorst, C. Vuik, GMRESR: </author> <title> A family of nested GMRES methods, </title> <note> to appear in Numerical Linear Algebra and its Applications. </note>
Reference-contexts: Methods in both families require, like most other iterative methods, a good preconditioner in order to be robust. There are different ways to precondition iterative methods. We will consider here two different types of preconditioners, the use of an inner iterative method as a preconditioner similar to GMRESR <ref> [48] </ref> or FGMRES [37] as well as an Incomplete LU factorization based on numerical dropping. For the former type of preconditioning we assess the new algorithms as inner, as well as outer, method. <p> Methods have also been proposed for the nonsymmetric case. These include FGMRES [37] with GMRES as the outer method, GMRESR <ref> [48] </ref> with GCR as the outer method and GMRES as the inner method, and others considered further in this and the following section. <p> For the test problems considered, the use of classical Gram-Schmidt does not effect the stability. The first three matrices are obtained through a standard five point finite difference discretization of the following two-dimensional partial differential equation, which is taken from <ref> [48] </ref>, u xx u yy + fi (u x + u y ) = f on (6:1) with Dirichlet boundary conditions u = c on @; (6:2) where = [0; 1] fi [0; 1].
Reference: [49] <author> V. Voevodin, </author> <title> The problem of a non-selfadjoint generalization of the conjugate gradient method has been closed, </title> <institution> USSR Comput. Math. and Math. Phys. </institution> <month> 23 </month> <year> (1983) </year> <month> 143-144. </month>
Reference-contexts: CG has two fundamental properties: it minimizes the residual vector along each iteration step, and the residual vectors satisfy a three-term recurrence. It is not possible to maintain both properties for a nonsymmetric linear system [16], <ref> [49] </ref>. There are several ways to deal with this situation. One can maintain both properties by considering the normal equations instead of the original system. The new matrices A T A (CGNR) or AA T (CGNE) are symmetric positive definite, and the CG algorithm can therefore be applied to them.
Reference: [50] <author> C. Vuik, </author> <title> Further experiences with GMRESR, </title> <type> Technical Report 92-12, </type> <institution> Delft University of Technology, </institution> <year> 1992. </year>
Reference-contexts: Our experiments show, however, that this result does not transfer to the EN-like methods (see Chapter 6). It is also possible to use more sophisticated truncation schemes, which can lead to better convergence, see [43] and <ref> [50] </ref>. The application of these to the family of EN-like methods is left as future work. The truncation of Algorithms 12 and 13 is far more complicated than the other algorithms. De Sturler and Fokkema describe in [43] how to truncate Algorithm 12. <p> preconditioners might yield an effective iterative method/preconditioner combination with less of the difficulties associated with maintaining sparsity in the approximate inverses. * Since truncation has shown to be promising for many problems, but also has its difficulties for other problems, we would like to consider more sophisticated truncation schemes [43], <ref> [50] </ref>. Since for some problems restarted methods perform far better than truncated methods, the development of hybrids of restarted and truncated methods appears to be a promising approach. 127 * Orthogonality preserving methods are trading sparse for dense matrix vector operations.
Reference: [51] <author> C. Vuik, H. van der Vorst, </author> <title> A comparison of some GMRES-like methods, Linear Algebra and its Applications, </title> <month> 160 </month> <year> (1992) </year> <month> 131-162. </month>
Reference-contexts: The family of Broyden methods has suffered from a bad reputation for solving linear systems, but recent efforts [8] have shown that different line search strategies lead to versions that are competitive with GMRES. The EN method can be related to GMRES <ref> [51] </ref>. A closer look at the EN method and Broyden methods shows that they are also related. Using this relationship, one can define a new family of methods, the EN-like methods, which includes the EN method as a special case. In this thesis, we consider both families. <p> Theorem 2.1 summarizes a few of the 7 basic characteristics of the EN method that have been noted in the literature (see [13] and <ref> [51] </ref>) but not proved and presented in a unified fashion. <p> To illustrate the convergence behavior of the methods presented in this chapter we use a test problem from <ref> [51] </ref>. <p> The sensitivity of the convergence to changes in H 0 is an effect of the scaling-dependence of the EN method (which is also considered in <ref> [51] </ref>). We develop scaling-invariant versions of the EN method in Section 2.4. 2.2 The Family of Broyden Methods An important class of methods based on rank-k updates is the class of quasi-Newton methods [10]. <p> There is another choice of f k that leads to a version of Broyden's method that is equivalent to the generalized conjugate residual method (GCR) [12] or GMRES with the initial vector ~x 0 = x 0 + H 0 r 0 and consequently terminates within n steps <ref> [51] </ref>. For this method we choose f k = (I AH k ) H (I AH k )q k . Its convergence behavior is similar to that of GMRES or GCR, which is considered in further detail later. We refer to this method as `GCR-BM'. <p> For the last case fl = 1, and consequently we see the same behavior as for the scaling-dependent version of the method. Vuik and van der Vorst <ref> [51] </ref> suggested another scaling-invariant version of the EN method, which we denote by `SEN'. <p> i ~u k = H 0 r k i=0 ~c k = AH 0 r k i=0 k = k~c k k 2 ~c H k r k+1 = r k ffi k ~c k As mentioned by Eirola and Nevanlinna [13] and proved by Vuik and van der Vorst <ref> [51] </ref>, this algorithm can also be obtained by replacing the evaluation of ~u k in the EN method with ~u k = H k r k : (2:29) Consequently, GCR can also be considered as a method based on rank-one updates. <p> Finally, we consider the relationship of the EN-like methods to Newton's method for approximating the inverse of a matrix. 28 2.6.1 GCR and Broyden's Method As mentioned in <ref> [51] </ref> and in Section 2.2, GCR-BM (Broyden's method with f k = E H k E k q k ) is equivalent to GCR using the initial vector x 0 + H 0 r 0 . <p> use ~u k = H k E k r k : Due to the orthogonality of the ~c i (see (ii) in Theorem 2.1), we have q H i E i x = q H i E 0 x: This leads to the following version for SEN (see also [13] <ref> [51] </ref>): Algorithm 11. <p> The vectors z 0 ; :::; z k1 (or c 0 ; :::; c k1 for EN) span a k-dimensional subspace of the 2k-dimensional Krylov subspace K 2k (AH 0 ; AH 0 r 0 ). (This was shown for the original EN method in <ref> [51] </ref>.) Future work characterizing the relationship between the k-dimensional space produced by the EN-like methods to the (typically different) Krylov subspace produced by GMRES may be a key factor in determining when EN-like methods outperform GMRES (which they are seen to do in the results in Chapter 6).

References-found: 51

