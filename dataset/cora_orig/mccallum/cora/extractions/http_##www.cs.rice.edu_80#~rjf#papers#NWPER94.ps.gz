URL: http://www.cs.rice.edu:80/~rjf/papers/NWPER94.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~rjf/pubs.html
Root-URL: 
Title: Carlsberg: A Distributed Execution Environment Providing Coherent Shared Memory and Integrated Message Passing  
Author: Povl T. Koch Robert J. Fowler 
Address: Sweden,  Universitetsparken 1, 2100 Copenhagen, Denmark  
Affiliation: Lund,  Department of Computer Science, University of Copenhagen (DIKU)  
Note: A Position/Work-in-Progress Paper presented at Nordic Workshop on Programming Environment Research, NWPER'94,  
Email: E-mail: koch,fowler@diku.dk  
Phone: Tel: +45 35 32 14 18 Fax: +45 35 32 14 01  
Date: June, 1994  
Abstract: The Carlsberg prototype is a distributed operating system designed to provide efficient support for distributed-parallel applications on a cluster of high-performance workstations. A unique feature of Carlsberg is the integration of coherent shared memory, multithreading, and message passing in one system. In this paper we discuss the motivation for the Carlsberg system and we present aspects of its design. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve and M. D. Hill. </author> <title> A Unified Formalization of Four Shared-Memory Models. </title> <type> Technical Report CS-1051, </type> <institution> University of Wisconsin, Madison, </institution> <month> September </month> <year> 1991. </year>
Reference: [2] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference: [3] <author> J.K. Bennett, S. Dwarkadas, J.A. Greenwood, and E. Speight. </author> <title> Willow: A scalable shared memory multiprocessor. </title> <booktitle> In Proceedings of Supercomputing '92, </booktitle> <pages> pages 336-345, </pages> <month> November </month> <year> 1992. </year>
Reference: [4] <author> John K. Bennett, John B. Carter, and Willy Zwaenepoel. Munin: </author> <title> Distributed shared memory based on type-specific memory coherence. </title> <booktitle> In Proceedings of the 2nd Annual Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> March </month> <year> 1990. </year> <note> In SIG-PLAN Notices 25(3). </note>
Reference: [5] <author> B. N. Bershad, M. J. Zekauskas, and W. A. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In Proceedings of the '93 CompCon Conference, </booktitle> <pages> pages 528-537, </pages> <month> Febru-ary </month> <year> 1993. </year>
Reference-contexts: For example, a collection of data might be associated with a lock or other synchronization object. In that case, data transfer and synchronization can be profitably combined into a single operation [21]. The entry consistency model of Midway <ref> [5] </ref> exploits this binding. * "Lazy" implementations of shared memory try to not transfer data values between processors until some process tries to read them. Unfortunately, this makes read latency very difficult to hide. Latency reduction techniques such as memory instructions scheduling and prefetching, have met with limited success [17,28].
Reference: [6] <author> John B. Carter. </author> <title> Efficient Distributed Shared Memory Based On Multi-Protocol Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> September </month> <year> 1993. </year>
Reference: [7] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Techniques for reducing consistency-related communication in distributed shared memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <month> August </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: The Coherent shared memory region contains user-allocated data which coherency is maintained automatically using lazy release consistency [19,20]. The implementation mechanisms are described below. A Non-coherent (system) shared memory region is necessary because synchronization objects must be implemented outside coherent shared memory <ref> [7] </ref>. The runtime system can also use this region for data structures such as tasks and stacks. Language runtime libraries will also use the Non-coherent regions to hold data objects that are better kept consistent with explicit, language-level protocols.
Reference: [8] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS directories: A scalable cache coherence scheme. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference: [9] <author> Rohit Chandra, Kourosh Gharachorloo, Viajayaraghaan Soundararajan, and Annop Gupta. </author> <title> Performance evaluation of hybrid hardware and software distributed shared memory protocols. </title> <note> Submitted for publication. </note>
Reference: [10] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Littlefield. </author> <title> The Amber system: parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Private memory regions are used by only one processor. Non-coherent regions can be shared by multiple processors, but any coherence is the responsibility of user code, preferably provided by the implementor of the programming language. Distributed object-oriented systems such as Emerald [18] and Amber <ref> [10] </ref> could use a collection of non-coherent regions as an explicitly-managed object store. Coherent regions are managed by Carlsberg using the DRF1 model and a combination of lazy invalidation and eager update. There will be a message-passing component alongside the memory system.
Reference: [11] <author> G.S. Delp, </author> <title> A.S. Sethi, and D.J. Farber. An analysis of Memnet: An experiment in high-speed shared-memory local networking. </title> <booktitle> In Proceedings of the Sigcomm '88 Symposium, </booktitle> <pages> pages 165-174, </pages> <month> August </month> <year> 1988. </year>
Reference: [12] <author> M. Dubois, C. Scheurich, and F.A. Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: This problem is at its worst worst in software DSM systems. 1 There are too many to cite. * Relaxed models of memory consistency, such as the weak <ref> [12] </ref>, release [16], and "Data Race Free 1" (DRF1) [1,20] models, offer higher performance than more strict models, but these models require that synchronization operations be identified to the memory coherence mechanism.
Reference: [13] <author> Sandhay Dwarkadas, Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 244-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: prefixed with the identifier of a system function to invoke immediately and to which the rest of the body of the message is to be passed as 4 The consistency information is composed of invalidation and/or update information depending what lazy release consistency protocol is used (invalidate, update, or hybrid <ref> [13] </ref>). an argument. Typically the invoked operation accesses a system data structure, en--queues a task for later execution, or copies data directly to its final destination in user memory. This design is chosen because a good implementation can avoid overhead associated with buffering, data copying, and scheduling.
Reference: [14] <author> B. Fleisch and G. Popek. </author> <title> Mirage: A coherent distributed shared memory design. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 211-223, </pages> <month> December </month> <year> 1989. </year>
Reference: [15] <author> Robert J. Fowler and Leonedis I. Kontothanassis. </author> <title> Object-affinity scheduling and continuation passing on multiprocessors. </title> <booktitle> In Proceedings of PARLE 1994, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Additionally, lightweight tasks created on one node to be executed on another will be the basis for remote invocation, fork-join parallelism, and other forms of parallel and/or distributed computation. Our experiences with a continuation-passing style of lightweight tasking in the Mercury runtime system <ref> [15] </ref> have encouraged us to use the same strategy in Carlsberg. This mechanism is similar in principles to active messages [32] in that the basic message-passing operation specifies an action to be taken immediately by the receiver.
Reference: [16] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposim on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: This problem is at its worst worst in software DSM systems. 1 There are too many to cite. * Relaxed models of memory consistency, such as the weak [12], release <ref> [16] </ref>, and "Data Race Free 1" (DRF1) [1,20] models, offer higher performance than more strict models, but these models require that synchronization operations be identified to the memory coherence mechanism.
Reference: [17] <author> A. Gupta, J. Hennessy, K. Gharachorloo, T. Mowry, and W.-D. Weber. </author> <title> Comparative evaluation of latency reducing and tolerating techniques. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> May </month> <year> 1991. </year>
Reference: [18] <author> E. Jul, H. Levy, N. Hutchinson, and A. Black. </author> <title> Fine-grained mobility in the Emerald system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 109-133, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Private memory regions are used by only one processor. Non-coherent regions can be shared by multiple processors, but any coherence is the responsibility of user code, preferably provided by the implementor of the programming language. Distributed object-oriented systems such as Emerald <ref> [18] </ref> and Amber [10] could use a collection of non-coherent regions as an explicitly-managed object store. Coherent regions are managed by Carlsberg using the DRF1 model and a combination of lazy invalidation and eager update. There will be a message-passing component alongside the memory system. <p> The Carlsberg runtime system contains library routines for fine-grained parallelism, scheduling, synchronization, and user-messages that interface to the Carlsberg kernel. The runtime services will not be described in this paper but we expect that many different runtime systems (e.g., for C, C++, Emerald <ref> [18] </ref>, and ML) can be implemented using the Carlsberg kernel services. The Carlsberg kernel provides shared memory and integrated message passing. This kernel is independent of any particular language or compiler.
Reference: [19] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter USENIX Conference, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: investigation of the different tradeoffs that will arise in low latency networks good for short messages, such as ATM [13,31], versus implementations appropriate for networks optimized for high bandwidth on large messages, such as FDDI. 4 Empirical Justification In this section, we examine the performance of the TreadMarks DSM system <ref> [19] </ref> to provide supporting evidence for the Carlsberg strategy. We also try to to provide a rough estimate of the potential benefits we can expect from Carlsberg. <p> We call this the AXP cluster. We compare our measurements with published measurements gathered on a cluster of DECstation 5000/240s with 40 MHz MIPS R3000 processors (approx. 30 SPECmarks) running Ultrix version 4.3 and connected by a 100 Mbit/sec ATM network <ref> [19] </ref>. Messages are sent using ATM adaption layer AAL3/4. We call this the MIPS cluster. Table 1 summarizes the costs of some operations used by TreadMarks. There are three main factors that differentiate the two systems. First and most obvious is the difference in network hardware. <p> Table 2 summarizes the performance of three applications run on TreadMarks on our AXP cluster. Water is a modified version of a molecular dynamics simulation 2 A page is initially marked unaccessible with mprotect. When a write occurs, a segment violation is reported using a UNIX signal. See <ref> [19] </ref> or [27] for details. 3 Switching delay is not included in this cost but it is on the order of 6 or 7 sec. from the SPLASH suite [30]. Water uses locks and barriers for synchronization and 343 molecules for 5 steps are simulated. <p> For multi-programming to be really worthwhile, however, it will be necessary to keep the overhead of communication as low as possible, hence our emphasis on low overhead network interfaces and protocols. In Table 3 the column labelled MIPS is a breakdown of the measured execution time <ref> [19] </ref> (in seconds) of Water on a 8-node, Ethernet-based cluster of MIPS-based workstations running TreadMarks. <p> Other researchers [5,19] have concluded that the limited bandwidth and high latency of Ethernet no longer makes it a suitable network technology for high-perfor mance DSM systems. Keleher et al. <ref> [19] </ref> also points out that the software overhead from standard UNIX interfaces limit performance. In contrast, we believe that it is possible to achieve reasonable speedups on Ethernet clusters as long as the applications have enough parallelism and can use a low-overhead network interface. <p> Each node allocates memory out of its own section. This means that nodes can create and reference shared data objects efficiently, while guaranteeing that there will be no collisions in the virtual address space. Lazy release consistency <ref> [19] </ref> is a relaxed memory model that allows processors to delay and buffer updates to shared objects until synchronization events occur. Two types of synchronization events are distinguished: acquires and releases, corresponding to acquiring and releasing a lock, respectively.
Reference: [20] <author> Peter Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference: [21] <author> D. Kranz, K. Johnson, A. Agarwal, J.Kubiatowicz, and B-H. Lim. </author> <title> Integrating message-passing and shared-memory: Early experience. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <address> San Diego, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: For example, a collection of data might be associated with a lock or other synchronization object. In that case, data transfer and synchronization can be profitably combined into a single operation <ref> [21] </ref>. The entry consistency model of Midway [5] exploits this binding. * "Lazy" implementations of shared memory try to not transfer data values between processors until some process tries to read them. Unfortunately, this makes read latency very difficult to hide.
Reference: [22] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a message in the Alewife multiprocessor. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <pages> pages 195-206, </pages> <month> July </month> <year> 1993. </year>
Reference: [23] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin ad David Nakahira, Joel Baxter, Mark Horowitz, Annop Gupta, Mendel Rosenbaum, and John Hennessy. </author> <title> The stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Conference on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference: [24] <author> L. Lamport. </author> <title> How to make a multiprocesor computer that executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Thus, eager memory can be useful, but unless the demands on bandwidth can be controlled, it is not the solution to all of our problems. * Common (sequential <ref> [24] </ref>, weak, release and DRF1) models of memory consistency are conceptually based on Lamport's happens-before relation, which captures the notion of potential causality between events. A system based on potential causality will incur more communication than one that is based only on the actual communication requirements of the user program.
Reference: [25] <author> J. Lee and U. Ramachandran. </author> <title> Architectural primitives for a scalable shared memory multiprocessor. </title> <booktitle> In Proceedings of the 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 103-114, </pages> <address> Hilton Head, South Carolina, </address> <month> July </month> <year> 1991. </year>
Reference: [26] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference: [27] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Water is a modified version of a molecular dynamics simulation 2 A page is initially marked unaccessible with mprotect. When a write occurs, a segment violation is reported using a UNIX signal. See [19] or <ref> [27] </ref> for details. 3 Switching delay is not included in this cost but it is on the order of 6 or 7 sec. from the SPLASH suite [30]. Water uses locks and barriers for synchronization and 343 molecules for 5 steps are simulated.
Reference: [28] <author> T. Mowry and A. Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12 </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference: [29] <author> U. Ramachandran, M. Yousef, and A. Khalidi. </author> <title> An implementation of distributed shared memory. </title> <journal> Software|Practice and Experience, </journal> <volume> 21 </volume> <pages> 443-464, </pages> <month> May </month> <year> 1991. </year>
Reference: [30] <author> J. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: When a write occurs, a segment violation is reported using a UNIX signal. See [19] or [27] for details. 3 Switching delay is not included in this cost but it is on the order of 6 or 7 sec. from the SPLASH suite <ref> [30] </ref>. Water uses locks and barriers for synchronization and 343 molecules for 5 steps are simulated. Jacobi a successive over-relaxation computation on a grid of 2000 by 1000 floating point values using barriers between iterations for synchronization.
Reference: [31] <author> Chandramohan A. Thekkath and Henry M. Levy. </author> <title> Limits to low-latency communication on high speed networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Operation AXP MIPS Minimal packet r/t hardware cost only <ref> [31] </ref> 0.25 0.07 3 Minimal packet r/t blocking recv. 0.99 0.50 Minimal packet r/t w/signal on recv. 1.96 0.67 Remote page fault 10.7 (8 Kb) 2.8 (4 Kb) Table 1: The costs of basic operations in milliseconds.
Reference: [32] <author> Thomas von Eicken, David E. Culler, Seth Copen Goldstein, and Karl Erik Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Our experiences with a continuation-passing style of lightweight tasking in the Mercury runtime system [15] have encouraged us to use the same strategy in Carlsberg. This mechanism is similar in principles to active messages <ref> [32] </ref> in that the basic message-passing operation specifies an action to be taken immediately by the receiver. For all user-level messages, the specified action will be an operation by the lightweight task scheduler. <p> The coordination annotations of the Carlsberg message interface is used to notify the kernel that synchronization should take place between the sending and receiving node. The necessary consistency information piggybacked on the the original message. Carlsberg kernel messages (also called system messages) are structured as active messages <ref> [32] </ref>; each message is prefixed with the identifier of a system function to invoke immediately and to which the rest of the body of the message is to be passed as 4 The consistency information is composed of invalidation and/or update information depending what lazy release consistency protocol is used (invalidate,
Reference: [33] <author> Larry D. Wittie, Gudjun Hermannsson, and Ai Li. </author> <title> Eager sharing for efficient massive parallelism. </title> <booktitle> In 1992 International Conference on Parallel Processing, </booktitle> <pages> pages 251-255, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: codes with simple static communication patterns or for scientific visualization and multimedia systems that pipe streams of audio, graphical, and video data. * "Eager", or update-based, implementations of shared memory can avoid read latencies, but at the price of consuming large amounts of bandwidth and incurring other forms of overhead <ref> [33] </ref>.
Reference: [34] <author> S. Zhou, M. Stumm, and T. McInerney. </author> <title> Extending distributed shared memory to heterogeneous environments. </title> <booktitle> In Proceedings of the 10th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 30-37, </pages> <month> May </month> <year> 1990. </year>
References-found: 34

