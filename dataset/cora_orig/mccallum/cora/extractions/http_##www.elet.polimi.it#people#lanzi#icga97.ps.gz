URL: http://www.elet.polimi.it/people/lanzi/icga97.ps.gz
Refering-URL: http://www.elet.polimi.it/people/lanzi/listpub.html
Root-URL: 
Email: lanzi@elet.polimi.it  
Title: A Study of the Generalization Capabilities of XCS  
Author: Pier Luca Lanzi 
Address: P.zza Leonardo da Vinci, 32, I-20133 Milano Italia  
Affiliation: Artificial Intelligence and Robotics Project Dipartimento di Elettronica e Informazione Politecnico di Milano  
Abstract: We analyze the generalization behavior of the XCS classifier system in environments in which only a few generalizations can be done. Experimental results presented in the paper evidence that the generalization mechanism of XCS can prevent it from learning even simple tasks in such environments. We present a new operator, named Specify, which contributes to the solution of this problem. XCS with the Specify operator, named XCSS, is compared to XCS in terms of performance and generalization capabilities in different types of environments. Experimental results show that XCSS can deal with a greater variety of environments and that it is more robust than XCS with respect to population size.
Abstract-found: 1
Intro-found: 1
Reference: <author> Dorigo, M. </author> <year> (1993). </year> <title> Genetic and non-genetic operators in alecsys. </title> <booktitle> Evolutionary Computation 1 (2), </booktitle> <pages> 151-164. </pages>
Reference-contexts: Section 6 isolates the primitive components of generalization, discusses the Mutespec operator proposed by Dorigo <ref> (Dorigo 1993) </ref>, and finally defines the Specify operator. Experimental results on the Maze4 environment for the proposed XCSS and XCS systems are compared in Section 7. The generalization capabilities of the two systems are evaluated and compared in Section 8. <p> The first two cases can be regarded as initial conditions of the system, while mutation is a main component of generalization in XCS. Thus we devised a mechanism to contrast mutation in situations that do not allow much generalization. In literature Dorigo <ref> (Dorigo 1993) </ref> already presented an operator, named Mutespec, for this kind of problems. 800 classifiers. The curve is averaged on ten experiments. 6.1 THE MUTESPEC OPERATOR Dorigo (Dorigo 1993) presents the Mutespec operator to reduce reward variance in oscillating classifiers. <p> Thus we devised a mechanism to contrast mutation in situations that do not allow much generalization. In literature Dorigo <ref> (Dorigo 1993) </ref> already presented an operator, named Mutespec, for this kind of problems. 800 classifiers. The curve is averaged on ten experiments. 6.1 THE MUTESPEC OPERATOR Dorigo (Dorigo 1993) presents the Mutespec operator to reduce reward variance in oscillating classifiers. These are classifiers that, due to the presence of some don't care symbols, match different conditions with different rewards.
Reference: <author> Holland, J. H. </author> <year> (1986). </year> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In M. Michalski, Carbonell (Ed.), </editor> <booktitle> Machine Learning, </booktitle> <volume> Volume 2, Chapter 20, </volume> <pages> pp. 593-623. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Generalization in XCS is achieved mainly by the combination of two factors. First, classifier fitness in XCS is based on the accuracy of the classifier prediction instead of the prediction itself as in traditional classifier systems <ref> (Holland 1986) </ref>. Second, the genetic algorithm in XCS acts in environmental niches, as opposed to the traditional panmictical GA. XCS evolves populations of accurate and maximally general classifiers. Because general classifiers match more niches, they reproduce more. <p> The generalization capabilities of the two systems are evaluated and compared in Section 8. A conclusions section ends the paper. 2 OVERVIEW OF THE XCS CLASSIFIER SYSTEM XCS is a classifier system developed by Wilson which differs from the traditional one defined by Holland <ref> (Holland 1986) </ref> mainly because (i) it has a very simple architecture, (ii) there is no message list, and most important (iii) the traditional strength is replaced by three different parameters. In the following we briefly review XCS in its most recent version (Wilson 1996).
Reference: <author> Kovacs, T. </author> <year> (1996). </year> <title> Evolving optimal populations with XCS classifier systems. </title> <institution> Technical Report CSR-96-17 and CSRP-96-17, School of Computer Science, University of Birmingham, Birmingham, U.K. </institution> <note> Avaiable from the technical report archive at http://www/system/tech-reports/tr.html. </note>
Reference-contexts: In the following we briefly review XCS in its most recent version (Wilson 1996). The original XCS description can be found in (Wilson 1995) or in Kovacs's report <ref> (Kovacs 1996) </ref> where some original results are duplicated and extended for more complex environments. Classifier Parameters. Classifiers in XCS have three main parameters: the prediction p j , the prediction error " j and the fitness F j . <p> Macroclas-sifiers are essentially a programming technique that speeds up the learning process reducing the number of real, macro, classifiers XCS has to deal with. Experimental results reported in <ref> (Kovacs 1996) </ref> evidence that macroclassifiers do not affect the population of micro-classifiers since every procedure is written to take into account the numerosity parameter. The number of macroclassifiers is a useful statistic to measure the degree of generalization obtained by the system.
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning from delayed reward. </title> <type> PhD Thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, England. </address>
Reference-contexts: 1 INTRODUCTION XCS is a classifier system recently proposed by Wilson (Wilson 1995) which has a strong tendency to evolve near-minimal populations of accurate and maximally general classifiers. Experimental results reported in the literature show that XCS can learn a more compact representation than that learned by tabular Q-learning <ref> (Watkins 1989) </ref>. Generalization in XCS is achieved mainly by the combination of two factors. First, classifier fitness in XCS is based on the accuracy of the classifier prediction instead of the prediction itself as in traditional classifier systems (Holland 1986).
Reference: <author> Widrow, B. and M. </author> <title> Hoff (1960). Adaptive switching circuits. </title> <booktitle> In Western Electronic Show and Convention, </booktitle> <volume> Volume 4, </volume> <pages> pp. 96-104. </pages> <booktitle> Institute of Radio Engineers (now IEEE). </booktitle>
Reference-contexts: First, the maximum system prediction is discounted by a factor fl (0 fl &lt; 1) and added to the reward returned in the previous time step. The resulting quantity, simply named P, is used to update the prediction p j by the Widrow-Hoff delta rule <ref> (Widrow and Hoff 1960) </ref> with learning rate fi (0 &lt; fi 1): p j p j + fi (P p j ). Then the prediction error " j is adjusted using the delta rule technique: " j " j + fi (jP p j j " j ).
Reference: <author> Wilson, S. W. </author> <year> (1995). </year> <title> Classifier fitness based on accuracy. </title> <booktitle> Evolutionary Computation 3 (2), </booktitle> <pages> 149-175. </pages>
Reference-contexts: 1 INTRODUCTION XCS is a classifier system recently proposed by Wilson <ref> (Wilson 1995) </ref> which has a strong tendency to evolve near-minimal populations of accurate and maximally general classifiers. Experimental results reported in the literature show that XCS can learn a more compact representation than that learned by tabular Q-learning (Watkins 1989). <p> In the following we briefly review XCS in its most recent version (Wilson 1996). The original XCS description can be found in <ref> (Wilson 1995) </ref> or in Kovacs's report (Kovacs 1996) where some original results are duplicated and extended for more complex environments. Classifier Parameters. Classifiers in XCS have three main parameters: the prediction p j , the prediction error " j and the fitness F j . <p> In fact, as XCS converges to a population of accurate and maximally general classifiers, the number of macroclassi-fiers decreases while the number of microclassifiers is kept constant by the delete/insert procedures. 3 DESIGN OF EXPERIMENTS Experiments with XCS were conducted in the well-known "woods" environments following the methodology employed in <ref> (Wilson 1995) </ref>. In the next we give a brief overview of the woods environments. Then the design of experiments discussed in the rest of the paper will be introduced. The Woods Environments. <p> When in exploration mode, the system selects actions randomly with a probability proportional to their predicted reward 1 . When in exploitation, XCS selects the action with the highest predicted reward. This strategy is simply referred to as 50/50 exploration/exploitation strategy <ref> (Wilson 1995) </ref>. Two types of statistics are collected for each environment: the performance and the population size in macroclassifiers. Performance is computed as the average number of steps to food in the last 50 exploitation problems. <p> XCS IN THE MAZE4 ENVIRONMENT Results proposed in literature for the XCS classifier system in the woods environments are limited to very regular and periodic environments that is, built repeating a certain pattern indefinitely in the horizontal and vertical directions, such as in Woods1 and 1 XCS as proposed in <ref> (Wilson 1995) </ref> selects actions randomly when in exploration mode. Our experiments show no significant difference between the two criteria. Woods2. Thus, after having duplicated Wilson's re-sults, we experimented the system on a series of non periodic environments. <p> Thus Maze4 does not allow much generalization as other traditional environments do. last 50 exploitation problems. The XCS system parameters were set as by Wilson for the Woods2 environment in <ref> (Wilson 1995) </ref>: N = 800, fi=0.2, fl=0.71, = 25, " 0 =0.01, ff=0.1, =0.8, =0.01, ffi=0.1, =0.5, As it can be noticed, XCS fails to learn an optimal path to food. <p> Unfortunately, the larger the population, the more the time to learn, and therefore 2 Some of these parameters have not been presented in the XCS overview but are reported here for completeness. We refer the reader to <ref> (Wilson 1995) </ref> for a complete discussion of those parameters. Maze4 environment with 800 classifiers. Optimal performance is represented by the horizontal dashed line. <p> We first experimented with a version of XCS in which the tendency to generalize was reduced. This was obtained as follows: (i) the GA acted in the match set as in the first Wilson's proposal <ref> (Wilson 1995) </ref>; (ii) classifiers were selected for reproduction with probability proportional to the product of the prediction and the fitness (p j fi F j ) instead of the fitness alone; (iii) during exploration problems actions were randomly selected using the Boltzmann distribution. <p> It is worth noting the oscillating performance of XCSS, that evidences the presence of the contrasting generalization/specification operators. 8 XCSS IN THE WOODS2 ENVIRONMENT The Woods2 environment, shown in Figure 7, has been introduced by Wilson in <ref> (Wilson 1995) </ref> to study the generalization mechanism in XCS. It contains two types of food cells (G and F) and two types of rocks (Q and O).
Reference: <author> Wilson, S. W. </author> <year> (1996). </year> <title> Generalization in XCS. Unpublished contribution to the ICML '96 Workshop on Evolutionary Computing and Machine Learning. </title> <note> Avaiable at http://netq.rowland.org. </note>
Reference-contexts: Because general classifiers match more niches, they reproduce more. But, since the GA bases the fitness upon classifiers accuracy, overgeneral classifiers, that are inaccurate, tend to reproduce less. Evolved classifiers are as general as possible while still being accurate. Recently, subsumption deletion, has been introduced by Wilson <ref> (Wilson 1996) </ref> to improve generalization. Subsumption deletion acts in the GA and replaces offspring classifiers with clones of their parents if the parents subsume, that is are generalization of, the offspring. XCS with subsumption deletion has a strong tendency to generalization. <p> The rest of the paper is organized as follows. Section 2 gives a brief overview of XCS according to the most recent presentation by Wilson <ref> (Wilson 1996) </ref>. Section 3 presents the woods environments, and the design of experiments for the results presented in the paper. <p> In the following we briefly review XCS in its most recent version <ref> (Wilson 1996) </ref>. The original XCS description can be found in (Wilson 1995) or in Kovacs's report (Kovacs 1996) where some original results are duplicated and extended for more complex environments. Classifier Parameters. <p> We apply XCSS and XCS to this environment and compare the results. The goal is to evaluate the generalization capabilities of XCSS with respect to XCS or equivalently how much generalization is lost when XCSS is used in environments which allow generalization. XCS parameters are set as in <ref> (Wilson 1996) </ref> for the same experiment: N = 800, fi=0.2, fl=0.71, = 25, " 0 =0.01, ff=0.1, =0.8, =0.01, ffi=0.1, =0.5, P # =0.5, P I =10.0, " I =0.0, F I =10.0. Parameters for the Specify operator are set as for the Maze4 environment.
References-found: 7

