URL: http://www.cs.msu.edu/~kaligotl/layer.ps.gz
Refering-URL: http://www.cs.msu.edu/~kaligotl/papers.html
Root-URL: http://www.cs.msu.edu
Title: Layered Classification of Parallel Computers  
Author: Chakrapani Kaligotla Instructor: Professor Lionel M. Ni 
Date: CPS 822 (Spring 1995):  February 2, 1995  
Affiliation: Department of Computer Science Michigan State University  Parallel Processing Computer Systems  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. J. Flynn, </author> <title> "Some computer organizations and their effectiveness," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-21, </volume> <pages> pp. 948-960, </pages> <month> Sept. </month> <year> 1972. </year>
Reference-contexts: The relationships between the large number of parallel architectures, design choices available, pros and cons of various approaches are difficult to appreciate because of the lack of a sufficiently rich taxonomy for parallel computer systems. One most frequently used taxonomy by Flynn <ref> [1] </ref>, classifies von Neumann machines 1 into four categories based on the number of instruction and data streams that they can process simultaneously. * SISD : Single Instruction and Single Data. * MISD : Multiple Instruction and Single Data. * SIMD : Single Instruction and Multiple Data. * MIMD : Multiple
Reference: [2] <author> G. Bell, </author> <title> "Ultracomputer: A teraflop before its time," </title> <journal> Communications of the ACM, </journal> <volume> vol. 35, </volume> <pages> pp. 26 - 47, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: Any processor can access any location in the memories with about the same latency. Communication and synchronization among processes is acheived by the use of shared variables <ref> [2] </ref>. A loosely coupled system consists of a number of processors each with its own local memory and processors communicate through explicit message passing over a network [3]. Some other frequently used system classification terminologies include shared memory multiprocessors, message passing multiprocessors, distributed memory multiprocessors, multicomputers, symmetric/asymmetric multiprocessors, etc.
Reference: [3] <author> W. Athas and C. Seitz, </author> <title> "Multicomputers: Message-passing concurrent computers," </title> <booktitle> Computer, </booktitle> <pages> pp. 9-25, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Communication and synchronization among processes is acheived by the use of shared variables [2]. A loosely coupled system consists of a number of processors each with its own local memory and processors communicate through explicit message passing over a network <ref> [3] </ref>. Some other frequently used system classification terminologies include shared memory multiprocessors, message passing multiprocessors, distributed memory multiprocessors, multicomputers, symmetric/asymmetric multiprocessors, etc. Lacking of uniformity in classifying parallel computer systems has given rise to a lot of confusion to users.
Reference: [4] <institution> BBN Advanced Computers Inc., Cambridge, Massachusetts, Inside the TC2000 Computer, </institution> <year> 1990. </year>
Reference-contexts: Lacking of uniformity in classifying parallel computer systems has given rise to a lot of confusion to users. For example, the BBN's Butterfly TC2000 is considered a tightly coupled system even though its underlying desing has a distributed memory architecture <ref> [4] </ref>. Elxsi 6400 has a globally shared memory through a common bus, but it provides a message-passing computing environment [5]. <p> There are no commercial UMA computers with separate address space at the time of this writing. NUMA architectures can also provide single address space using either static or dynamic binding. The BBN Butterfly GP1000/TC2000 has a NUMA architecture and provides a single address space with static binding <ref> [11, 4] </ref>. There is no software involved in providing a single adress space. Processor can access any memory location and hardware generates required messages to fetch the required memory location.
Reference: [5] <author> R. A. Olson, B. Kumar, and L. E. Shar, </author> <title> "Message and multiprocessing in the ELXSI system 6400," </title> <booktitle> in Proceedings of Spring COMPCON, </booktitle> <pages> pp. 21-24, </pages> <year> 1983. </year>
Reference-contexts: For example, the BBN's Butterfly TC2000 is considered a tightly coupled system even though its underlying desing has a distributed memory architecture [4]. Elxsi 6400 has a globally shared memory through a common bus, but it provides a message-passing computing environment <ref> [5] </ref>.
Reference: [6] <author> L. M. Ni, </author> <title> "A layered classification of parallel computer systems," </title> <booktitle> in Proceedings of the 1991 International Conference for Young Computer Scientists, </booktitle> <address> (Beijing, China), </address> <month> July </month> <year> 1991. </year> <type> (invited paper). </type>
Reference-contexts: Elxsi 6400 has a globally shared memory through a common bus, but it provides a message-passing computing environment [5]. The five-layer classification of parallel computer is based on an earlier work of Ni <ref> [6] </ref>. 2 According to Flynn, the pipeline in MISD is visible to programmers and the machine he had in mind was early cryptographic machines such as the IBM Harvest attachment to the 7030 (Stretch) computer. 2 2 Layered Approach A parallel computer system consists of not merely its underlying hardware components
Reference: [7] <author> D. B. Skillicorn, </author> <title> "A taxonomy for computer architectures," </title> <journal> IEEE Computer, </journal> <volume> vol. 21, </volume> <pages> pp. 46-57, </pages> <month> Nov. </month> <year> 1988. </year>
Reference-contexts: The integration of hardware organization, operating system and the parallel programming languages provide a real computing environment for users. Therefore, a layered classification fits well in classifying parallel computers. Some of the reasons for classifying parallel computers are quoted below <ref> [7] </ref>: * Allows easy understanding of what has been already achieved.
Reference: [8] <author> A. Garcia, D. Foster, and R. Freitas, </author> <title> "The Advanced Computing Environment Multiprocessor Workstation," </title> <type> tech. rep., </type> <institution> IBM T.J. Watson Research Center, </institution> <month> Mar. </month> <year> 1989. </year>
Reference-contexts: There are other different ways to interconnect the processors and memory modules. Figure - reff:ace illustrates the ACE research prototype of IBM <ref> [8] </ref>. In additional to local memory, a global memory is shared by all nodes. Thus, depending on the memory locations, there are three different 5 memory access times from each processor: local, remote, and global. This is also considered as a NUMA architecture. There are some other hierarchical organizations.
Reference: [9] <author> J. Ramanathan and L. M. Ni, </author> <title> "Critical factors in numa memory management," </title> <booktitle> in Proc. of the 11th International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: Designing parallel computers with different memory architectures is not difficult for manufactures. The most challenging and difficult issue is in the efficient use of complicated memory hierarchy <ref> [9] </ref>. An efficient and optimal placement of data over different memory modules is still an open research issue. 4 Memory Address Space The address space of a processor in a computer system varies among different architectures.
Reference: [10] <institution> Thinking Machines Corporation, Cambridge, MA, </institution> <type> CM-5: Technical Summary, </type> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Computers with NUMA architectures and separate address space communicate by using messaging passing software primitives explicitly. The Intel Paragon XP/s, CM-5 use this architecture <ref> [10] </ref>. There are no commercial UMA computers with separate address space at the time of this writing. NUMA architectures can also provide single address space using either static or dynamic binding. The BBN Butterfly GP1000/TC2000 has a NUMA architecture and provides a single address space with static binding [11, 4].
Reference: [11] <institution> BBN Laboratories, Cambridge, MA, Overview of THE BUTTERFLY GP1000, </institution> <month> November </month> <year> 1988. </year>
Reference-contexts: There are no commercial UMA computers with separate address space at the time of this writing. NUMA architectures can also provide single address space using either static or dynamic binding. The BBN Butterfly GP1000/TC2000 has a NUMA architecture and provides a single address space with static binding <ref> [11, 4] </ref>. There is no software involved in providing a single adress space. Processor can access any memory location and hardware generates required messages to fetch the required memory location.
Reference: [12] <author> B. Nitzberg and V. Lo, </author> <title> "Distributed shared memory: A survey of issues and algorithms," </title> <booktitle> IEEE Computer, </booktitle> <pages> pp. 52-60, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Servers from SUN, SGI and HP are examples of machines providing shared variable model over single address space with UMA memory architecture. Shared variable model can also be provided over separate address space <ref> [12] </ref>. Ivy is an example machine which provides shared variable model over separate address sapce with NUMA architecture.
Reference: [13] <author> K. Li and R. Schaefer, </author> <title> "A hypercube shared virtual memory," </title> <booktitle> in Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <volume> vol. I, </volume> <pages> pp. 125-132, </pages> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: Shared variable model can also be provided over separate address space [12]. Ivy is an example machine which provides shared variable model over separate address sapce with NUMA architecture. Shared variable model implemented over distributed memory architectures using software may lead to large overheads and are not highly scalable <ref> [13] </ref>. 5.2 Message Passing Processes communicate with each other by using message passing primitives like send and receive explicitly (Figure 5 (b)). Various message passing operations like blocking vs. non-blocking, selective vs. non-selective, single destination vs. multiple destinations can be provided.
Reference: [14] <author> R. Olson, </author> <title> "Parallel processing in a message-based operating system," </title> <journal> IEEE Software, </journal> <volume> vol. C-36, </volume> <pages> pp. 39 - 49, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Workstation clusters is an example architecture providing message passing over separate address space with NUMA memory architecture. Message passing can also be provided over single address space. Elxsi Message Based Operating System (EMBOS) of Elxsi 6400 machines provides message passing over single address space with UMA memory architecture <ref> [14] </ref>. Princetin Shiva is an example machine 8 Memory Address Syn. and Comm.
Reference: [15] <author> N. Carriero and D. Gelernter, </author> <title> "Linda on hypercube multicomputers," in Hypercube Multiprocessors 1986 (M. </title> <editor> T. Heath, Ed.), </editor> <title> pp. </title> <type> 45 - 56, </type> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1986. </year>
Reference-contexts: Programmer has a single thread view but language supports parallelism by using intrinsic functions and libraries to hide synchronization and communication from the user. TMC paralle FORTRAN, MASPAR Parallel FORTRAN and HPF are examples parallel languages supporting SIMD parallel programming paradigm. LINDA <ref> [15, 16] </ref> and HPF are examples languages supporting single thread SPMD programming paradigm. SIMD programming paradigm can also be multithreaded. In this approach, the programmer is aware of multiple threads. All threads run the same program with explicit synchronizatin between threads.
Reference: [16] <author> N. Carriers and D. Gelernter, </author> <title> "Linda in context," </title> <journal> Communications of the ACM, </journal> <volume> vol. 32, </volume> <pages> pp. 444 - 458, </pages> <month> Apr. </month> <year> 1989. </year>
Reference-contexts: Programmer has a single thread view but language supports parallelism by using intrinsic functions and libraries to hide synchronization and communication from the user. TMC paralle FORTRAN, MASPAR Parallel FORTRAN and HPF are examples parallel languages supporting SIMD parallel programming paradigm. LINDA <ref> [15, 16] </ref> and HPF are examples languages supporting single thread SPMD programming paradigm. SIMD programming paradigm can also be multithreaded. In this approach, the programmer is aware of multiple threads. All threads run the same program with explicit synchronizatin between threads.
Reference: [17] <author> K. M. Chandy, R. Manohar, B. L. Massingill, and D. I. Meiron, </author> <title> "Integrating task and data parallelism with the collective communication archetype," </title> <booktitle> in Proceedings of the 1995 International Parallel Processing Symposium, </booktitle> <year> 1995. </year>
Reference-contexts: In some application, each task in a task parallel program can be a data parallel program, known as MSPMD programming model. For example, the simulation of a climate model consists of two coupled simulations: one is ocean simulation and the other is atmosphere simulation <ref> [17] </ref>. Such a simulation consists of two concurrent tasks, and each task is a data parallel program. The two SPMD programs will exchange boundary data in order to solve the global climate simulation (see Fig. 7).
Reference: [18] <author> D. M. Pase, T. MacDonald, and A. Meltzer, </author> <title> MPP Fortran Programming Model. </title> <institution> Cray Research, Inc., Eagan, Minnesota, </institution> <month> Feb. </month> <year> 1993. </year> <month> 15 </month>
Reference-contexts: The MSPMD programs can run on a heterogenous computing environment, where each SPMD program can run on a homogeneous computing environment. These multiple SPMDs require to communicate and synchronize with each other. The MPP parallel programming model proposed in Cray combined all of the programming models <ref> [18] </ref>. Table 2 shows some example parallel languages that fit the top two layers of the classification. 8 Conclusions Lacking of uniformity in classifying parallel computers has given rise to a lot of confusion.
References-found: 18

