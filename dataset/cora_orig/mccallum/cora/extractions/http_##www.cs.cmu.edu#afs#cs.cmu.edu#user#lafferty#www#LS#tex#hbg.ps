URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/lafferty/www/LS/tex/hbg.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/lafferty/www/LS/syllabus.html
Root-URL: 
Title: Towards History-based Grammars: Using Richer Models for Probabilistic Parsing  
Author: Ezra Black Fred Jelinek John Lafferty David M. Magerman Robert Mercer Salim Roukos 
Affiliation: IBM T. J. Watson Research Center  
Abstract: We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Tree-bank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Baker, J. K., </author> <year> 1975. </year> <title> Stochastic Modeling for Automatic Speech Understanding. In Speech Recognition, edited by Raj Reddy, </title> <publisher> Academic Press, </publisher> <pages> pp. 521-542. </pages>
Reference-contexts: Previous work on disambiguation and probabilistic parsing has offered partial answers to this question. Hidden Markov models of words and their tags, introduced in <ref> [1] </ref> and [11] and popularized in the natural language community by Church [5], demonstrate the power of short-term n-gram statistics to deal with lexical ambiguity. Hindle and Rooth [8] use a statistical measure of lexical associations to resolve structural ambiguities.
Reference: 2. <author> Brent, M. R. </author> <year> 1991. </year> <title> Automatic Acquisition of Subcategorization Frames from Untagged Free-text Corpora. </title> <booktitle> In Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics. </booktitle> <address> Berkeley, California. </address>
Reference-contexts: Hindle and Rooth [8] use a statistical measure of lexical associations to resolve structural ambiguities. Brent <ref> [2] </ref> acquires likely verb subcategorization patterns using the fl Thanks to Philip Resnik and Stanley Chen for their valued input. frequencies of verb-object-preposition triples. Mager-man and Marcus [10] propose a model of context that combines the n-gram model with information from dominating constituents.
Reference: 3. <author> Brill, E., Magerman, D., Marcus, M., and Santorini, B. </author> <year> 1990. </year> <title> Deducing Linguistic Structure from the Statistics of Large Corpora. </title> <booktitle> In Proceedings of the June 1990 DARPA Speech and Natural Language Workshop. </booktitle> <address> Hidden Valley, Pennsylvania. </address>
Reference: 4. <author> Brown, P. F., Della Pietra, V. J., deSouza, P. V., Lai, J. C., and Mercer, R. L. </author> <title> Class-based n-gram Models of Natural Language. </title> <booktitle> In Proceedings of the IBM Natural Language ITL, </booktitle> <address> March, 1990. Paris, France. </address>
Reference-contexts: Our intention is that bit strings differing in the least significant bit positions correspond to categories of non-terminals or rules that are similar. We also have assigned bitstrings for the words in the vocabulary (the lexical heads) using automatic clustering algorithms using the bigram mutual information clustering algorithm (see <ref> [4] </ref>). Given the bitsting of a history, we then designed a decision tree for modeling the probability that a rule will be used for rewriting a node in the parse tree.
Reference: 5. <author> Church, K. </author> <year> 1988. </year> <title> A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing. </booktitle> <address> Austin, Texas. </address>
Reference-contexts: Previous work on disambiguation and probabilistic parsing has offered partial answers to this question. Hidden Markov models of words and their tags, introduced in [1] and [11] and popularized in the natural language community by Church <ref> [5] </ref>, demonstrate the power of short-term n-gram statistics to deal with lexical ambiguity. Hindle and Rooth [8] use a statistical measure of lexical associations to resolve structural ambiguities.
Reference: 6. <author> Gale, W. A. and Church, K. </author> <year> 1990. </year> <title> Poor Estimates of Context are Worse than None. </title> <booktitle> In Proceedings of the June 1990 DARPA Speech and Natural Language Workshop. </booktitle> <address> Hidden Valley, Pennsylvania. </address>
Reference: 7. <author> Harrison, M. A. </author> <year> 1978. </year> <title> Introduction to Formal Language Theory. </title> <publisher> Addison-Wesley Publishing Company. </publisher>
Reference-contexts: Is the subject of the previous sentence animate? Was the previous sentence a question? etc.) can be incorporated into the language model. 3. The History-based Grammar Model The history-based grammar model defines context of a parse tree in terms of the leftmost derivation of the tree. Following <ref> [7] </ref>, we show in Figure 1 a context-free grammar (CFG) for a n b n and the parse tree for the sentence aabb.
Reference: 8. <author> Hindle, D. and Rooth, M. </author> <year> 1990. </year> <title> Structural Ambiguity and Lexical Relations. </title> <booktitle> In Proceedings of the June 1990 DARPA Speech and Natural Language Workshop. </booktitle> <address> Hidden Valley, Pennsylvania. </address>
Reference-contexts: Hidden Markov models of words and their tags, introduced in [1] and [11] and popularized in the natural language community by Church [5], demonstrate the power of short-term n-gram statistics to deal with lexical ambiguity. Hindle and Rooth <ref> [8] </ref> use a statistical measure of lexical associations to resolve structural ambiguities. Brent [2] acquires likely verb subcategorization patterns using the fl Thanks to Philip Resnik and Stanley Chen for their valued input. frequencies of verb-object-preposition triples.
Reference: 9. <author> Jelinek, F. </author> <year> 1985. </year> <title> Self-organizing Language Modeling for Speech Recognition. </title> <type> IBM Report. </type>
Reference: 10. <author> Magerman, D. M. and Marcus, M. P. </author> <year> 1991. </year> <title> Pearl: A Probabilistic Chart Parser. </title> <booktitle> In Proceedings of the Febru-ary 1991 DARPA Speech and Natural Language Workshop. Asilomar, </booktitle> <address> California. </address>
Reference-contexts: Hindle and Rooth [8] use a statistical measure of lexical associations to resolve structural ambiguities. Brent [2] acquires likely verb subcategorization patterns using the fl Thanks to Philip Resnik and Stanley Chen for their valued input. frequencies of verb-object-preposition triples. Mager-man and Marcus <ref> [10] </ref> propose a model of context that combines the n-gram model with information from dominating constituents. All of these aspects of context are necessary for disambiguation, yet none is sufficient. <p> Most previous probabilistic models of parsing assume the probabilities of sentences in a discourse are independent of other sentences. In fact, previous works have made much stronger independence assumptions. The P-CFG model considers the probability of each constituent rule independent of all other constituents in the sentence. The Pearl <ref> [10] </ref> model includes a slightly richer model of context, allowing the probability of a constituent rule to depend upon the immediate parent of the rule and a part-of-speech trigram from the input sentence. But none of these models come close to incorporating enough context to disambiguate many cases of ambiguity.
Reference: 11. <author> Derouault, A., and Merialdo, B., </author> <year> 1985. </year> <title> Probabilistic Grammar for Phonetic to French Transcription. </title> <booktitle> ICASSP 85 Proceedings. </booktitle> <address> Tampa, Florida, </address> <pages> pp. 1577-1580. </pages>
Reference-contexts: Previous work on disambiguation and probabilistic parsing has offered partial answers to this question. Hidden Markov models of words and their tags, introduced in [1] and <ref> [11] </ref> and popularized in the natural language community by Church [5], demonstrate the power of short-term n-gram statistics to deal with lexical ambiguity. Hindle and Rooth [8] use a statistical measure of lexical associations to resolve structural ambiguities.
Reference: 12. <author> Sharman, R. A., Jelinek, F., and Mercer, R. </author> <year> 1990. </year> <title> Generating a Grammar for Statistical Training. </title> <booktitle> In Proceedings of the June 1990 DARPA Speech and Natural Language Workshop. </booktitle> <address> Hidden Valley, Pennsylvania. </address>
References-found: 12

