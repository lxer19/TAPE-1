URL: file://ftp.cs.ucsd.edu/pub/rich/papers/sched.ps.gz
Refering-URL: http://www.cs.ucsd.edu/users/rich/publications.html
Root-URL: http://www.cs.ucsd.edu
Title: Static Scheduling of Hierarchical Program Graphs  
Author: Rich Wolski 
Keyword: Scheduling, Partitioning, Compilation, Dataflow  
Address: La Jolla, CA 92093-0114, USA  
Affiliation: Department of Computer Science and Engineering, University of California, San Diego,  
Note: Parallel Processing Letters c World Scientific Publishing Company  Received (received date) Revised (revised date) Communicated by (Name of Editor)  
Abstract: Many parallel compilation systems represent programs internally as Directed Acyclic Graphs (DAGs). However, the storage of these DAGs becomes prohibitive when the program being compiled is large. In this paper we describe a compile-time scheduling methodology for hierarchical DAG programs represented in the IFX intermediate form. The method we present is itself hierarchical reducing the storage that would otherwise be required by a single flat DAG representation. We describe the scheduling model and demonstrate the method using the Optimizing Sisal Compiler and two scientific applications. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Cann, </author> <title> Retire Fortran? A Debate Rekindled, </title> <journal> Communications of the ACM, </journal> <volume> 35, </volume> <month> 8 (August </month> <year> 1992), </year> <pages> 81-89. </pages>
Reference-contexts: By developing a hierarchy of schedules instead of one single global schedule, we are able to drastically reduce the amount of storage necessary to hold the program during code generation. We have implemented this methodology as a static scheduler for OSC (the Optimizing Sisal Compiler) <ref> [1] </ref> which uses IFX as an intermediate program representation.
Reference: 2. <author> W. Clark, </author> <title> The Gantt Chart, 3rd edition, </title> <publisher> Pitman and Sons, </publisher> <address> London, </address> <year> 1952. </year>
Reference-contexts: Compile time scheduling methods are advantageous because they incur no runtime overhead, but they can be difficult to implement due to large storage requirements. Our method forms a hierarchy of schedules (Gantt charts <ref> [2] </ref>) that specify the processor assignment for each computation within a program. By developing a hierarchy of schedules instead of one single global schedule, we are able to drastically reduce the amount of storage necessary to hold the program during code generation.
Reference: 3. <author> B. Gorda and R. Wolski, </author> <title> Timesharing Massively Parallel Machines, </title> <booktitle> submitted to 12 Proc. of 1995 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: In Figure 10a we depict predicted execution time in processor clock cycles as a function of machine size in processors. The estimates depicted in the graph are consistent with other experiments conducted using the TC2000 <ref> [3, 12] </ref>. Note that most of the exploitable parallelism is exhausted by 50 processors. The difference between the improvement gained between 50 processors and 100 processors is small compared to the gain achieved in the range between 1 and 50. <p> The scheduler elects to exploit no more than two parallel threads in each loop body due to the granularity of the TC2000. Given the communication to computation ratio of the TC2000, neither code can effectively use more than approximately 50 processors. In <ref> [3] </ref>, a study is made of the parallel programs executed on the BBN TC2000 during three years of "production" parallel computing. Over the course of that time, greater than 70 % of the parallel jobs used 30 processors or less out of a possible 128.
Reference: 4. <author> S. J. Kim, </author> <title> A General Approach to Multiprocessor Scheduling, </title> <type> Ph. D. Thesis, </type> <institution> University of Texas Tech. Report TR-88-04, Austin, TX, </institution> <month> February </month> <year> 1988. </year>
Reference-contexts: In Section 6, we give scheduling results in terms of estimated program execution times for two scientific applications. We conclude with several observations in Section 7. 2. Background and Previous Research Previous researchers have studied the problem of parallel program scheduling in considerable detail <ref> [11, 9, 10, 4, 5] </ref>. The problem of determining a schedule that minimizes program execution is NP-complete [9], so the emphasis as been on heuristic-based approaches. <p> Background and Previous Research Previous researchers have studied the problem of parallel program scheduling in considerable detail [11, 9, 10, 4, 5]. The problem of determining a schedule that minimizes program execution is NP-complete [9], so the emphasis as been on heuristic-based approaches. Sarkar [9], Kim and Browne <ref> [4] </ref>, and Yang and Gerasoulis [10] provide methods for statically partitioning and scheduling program graphs into a set of non-strict communicating sequential threads assuming an infinite number of processors.
Reference: 5. <author> B. Kruatrachue and T. Lewis, </author> <title> Grain Size Determination for Parallel Processing, </title> <journal> IEEE Software, </journal> <month> January, </month> <year> 1988, </year> <pages> 23-32. </pages>
Reference-contexts: In Section 6, we give scheduling results in terms of estimated program execution times for two scientific applications. We conclude with several observations in Section 7. 2. Background and Previous Research Previous researchers have studied the problem of parallel program scheduling in considerable detail <ref> [11, 9, 10, 4, 5] </ref>. The problem of determining a schedule that minimizes program execution is NP-complete [9], so the emphasis as been on heuristic-based approaches.
Reference: 6. <author> W. </author> <title> Press, Numerical Recipes, </title> <publisher> Cambridge University Press, </publisher> <year> 1986. </year>
Reference-contexts: GJ is a Gauss-Jordon linear systems solver found in <ref> [6] </ref>. The flat DAG representation corresponds to the solution of 100 equations in 100 variables. PIC is a particle-in-cell transport code developed at the Los Alamos National Laboratory. It simulates the behavior of charged particles within a cylinder under the influence of only electromagnetic forces [11].
Reference: 7. <author> J. E. Raneletti, </author> <title> Graph Transformation Algorithms for Array Memory Optimization in Applicative Languages, </title> <type> Ph. D. Thesis, </type> <institution> Lawrence Livermore National Laboratory Tech. Report UCRL-53832, Livermore, </institution> <address> CA, </address> <month> November </month> <year> 1987. </year>
Reference-contexts: 1. Introduction Effective program scheduling is critical to good execution performance on parallel machines. In this work, we investigate a compile-time methodology for scheduling parallel programs represented using the IFX intermediate form <ref> [8, 7] </ref>. Compile time scheduling methods are advantageous because they incur no runtime overhead, but they can be difficult to implement due to large storage requirements. Our method forms a hierarchy of schedules (Gantt charts [2]) that specify the processor assignment for each computation within a program. <p> Our method avoids the formation of a single DAG or equivalent single Gantt chart by working within the hierarchical structure of IFX. 3. IFX IF1 [8] and IF2 <ref> [7] </ref> are graphical program representations in which programs are represented as a hierarchy of DAGs. IF1 specifies the basic language semantics in terms of dataflow, and IF2 extends IF1 to include side-effects in the form of memory management operations. Together, we refer to the combination as IFX.
Reference: 8. <author> S. K. Skedzielewski and J. Glauert, </author> <title> IF1 An Intermediate Form for Applicative Languages, </title> <institution> Lawrence Livermore National Laboratory Manual M-170, Livermore, </institution> <address> CA, </address> <month> July </month> <year> 1985. </year>
Reference-contexts: 1. Introduction Effective program scheduling is critical to good execution performance on parallel machines. In this work, we investigate a compile-time methodology for scheduling parallel programs represented using the IFX intermediate form <ref> [8, 7] </ref>. Compile time scheduling methods are advantageous because they incur no runtime overhead, but they can be difficult to implement due to large storage requirements. Our method forms a hierarchy of schedules (Gantt charts [2]) that specify the processor assignment for each computation within a program. <p> Our method avoids the formation of a single DAG or equivalent single Gantt chart by working within the hierarchical structure of IFX. 3. IFX IF1 <ref> [8] </ref> and IF2 [7] are graphical program representations in which programs are represented as a hierarchy of DAGs. IF1 specifies the basic language semantics in terms of dataflow, and IF2 extends IF1 to include side-effects in the form of memory management operations. Together, we refer to the combination as IFX. <p> Nodes within each IFX DAG specify computations, and edges describe the data dependencies between computations. Since program semantics can be defined strictly on the basis of data dependence, parallelism is explicit in the representation. Independent nodes may be executed concurrently since computations within IFX are functional <ref> [8] </ref>. That is, their outputs are defined strictly in terms of their 2 input there is no globally shared program state. Each directed edge specifies the conveyance of data from a source computation (producing the data as an output) and a destination computation (consuming the data as an input). <p> Notice that in an IFX program, a IFX also represents conditional execution using a compound node structure. See <ref> [8] </ref> for a full description of IFX compound node semantics. 4 each thread has a unique parent compound node that most directly encloses it. The scheduler will insert a thread image only in the Gantt chart associated with its parent.
Reference: 9. <author> V. Sarkar, </author> <title> Partitioning and Scheduling of Parallel Programs for Multiprocessors, </title> <booktitle> Research Monographs in Parallel Computing, </booktitle> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: In Section 6, we give scheduling results in terms of estimated program execution times for two scientific applications. We conclude with several observations in Section 7. 2. Background and Previous Research Previous researchers have studied the problem of parallel program scheduling in considerable detail <ref> [11, 9, 10, 4, 5] </ref>. The problem of determining a schedule that minimizes program execution is NP-complete [9], so the emphasis as been on heuristic-based approaches. <p> We conclude with several observations in Section 7. 2. Background and Previous Research Previous researchers have studied the problem of parallel program scheduling in considerable detail [11, 9, 10, 4, 5]. The problem of determining a schedule that minimizes program execution is NP-complete <ref> [9] </ref>, so the emphasis as been on heuristic-based approaches. Sarkar [9], Kim and Browne [4], and Yang and Gerasoulis [10] provide methods for statically partitioning and scheduling program graphs into a set of non-strict communicating sequential threads assuming an infinite number of processors. <p> Background and Previous Research Previous researchers have studied the problem of parallel program scheduling in considerable detail [11, 9, 10, 4, 5]. The problem of determining a schedule that minimizes program execution is NP-complete <ref> [9] </ref>, so the emphasis as been on heuristic-based approaches. Sarkar [9], Kim and Browne [4], and Yang and Gerasoulis [10] provide methods for statically partitioning and scheduling program graphs into a set of non-strict communicating sequential threads assuming an infinite number of processors.
Reference: 10. <author> T. Yang and A. Gerasoulis, </author> <title> A Fast Static Scheduling Algorithm for DAGs on an Unbounded Number of Processors, </title> <booktitle> Proc. of Supercomputing '91, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: In Section 6, we give scheduling results in terms of estimated program execution times for two scientific applications. We conclude with several observations in Section 7. 2. Background and Previous Research Previous researchers have studied the problem of parallel program scheduling in considerable detail <ref> [11, 9, 10, 4, 5] </ref>. The problem of determining a schedule that minimizes program execution is NP-complete [9], so the emphasis as been on heuristic-based approaches. <p> The problem of determining a schedule that minimizes program execution is NP-complete [9], so the emphasis as been on heuristic-based approaches. Sarkar [9], Kim and Browne [4], and Yang and Gerasoulis <ref> [10] </ref> provide methods for statically partitioning and scheduling program graphs into a set of non-strict communicating sequential threads assuming an infinite number of processors. Sarkar, in particular, outlines a static scheduling methodology in which the components of a hierarchical program are flattened into a single graph.
Reference: 11. <author> R. M. Wolski, </author> <title> Program Partitioning for NUMA Computer Architectures, </title> <type> Ph. D. Thesis, </type> <institution> Lawrence Livermore National Laboratory Tech. Report UCRL-LR-117760, Lawrence Livermore National Laboratory, Livermore, </institution> <address> CA, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: In Section 6, we give scheduling results in terms of estimated program execution times for two scientific applications. We conclude with several observations in Section 7. 2. Background and Previous Research Previous researchers have studied the problem of parallel program scheduling in considerable detail <ref> [11, 9, 10, 4, 5] </ref>. The problem of determining a schedule that minimizes program execution is NP-complete [9], so the emphasis as been on heuristic-based approaches. <p> Note that the thread image may not be added to the end of a slot, but may be interleaved with previously assigned images. If interleaving occurs, the ST and CT values for all images in the slot must be adjusted. In <ref> [11] </ref> we provide an algorithm for interleaving thread images which we omit here for the purpose of brevity. The scheduling algorithm considers each Gantt chart to form a context in which enclosed threads and compounds must be scheduled in the form of their respective images. <p> The flat DAG representation corresponds to the solution of 100 equations in 100 variables. PIC is a particle-in-cell transport code developed at the Los Alamos National Laboratory. It simulates the behavior of charged particles within a cylinder under the influence of only electromagnetic forces <ref> [11] </ref>. Clearly, forming a flat DAG from either program is not feasible as the number of nodes in each program is prohibitively large. In Figure 10a we depict predicted execution time in processor clock cycles as a function of machine size in processors.
Reference: 12. <author> R. Wolski, J. Feo, and D. Cann, </author> <title> A Prototype Functional Language Implementation for Hierarchical-Memory Architectures, </title> <booktitle> Proceedings of the 25th Annual Hawaii International Conference on System Sciences, </booktitle> <address> Kauai, Hawaii, </address> <month> January, </month> <year> 1992. </year> <month> 13 </month>
Reference-contexts: We demonstrate the effectiveness of the scheduler using two scientific programs: a Gauss-Jordon linear systems solver, and a particle-in-cell particle dynamics simulation. We compare the amount of exploitable parallelism found by the scheduler to the results reported in <ref> [12] </ref>. In the next section, we discuss previous static partitioning and scheduling research and its relationship to our methodology. Section 3 briefly describes IFX, Section 4 describes the architecture model used by the scheduler, and Section 5 details the scheduling model itself. <p> In Figure 10a we depict predicted execution time in processor clock cycles as a function of machine size in processors. The estimates depicted in the graph are consistent with other experiments conducted using the TC2000 <ref> [3, 12] </ref>. Note that most of the exploitable parallelism is exhausted by 50 processors. The difference between the improvement gained between 50 processors and 100 processors is small compared to the gain achieved in the range between 1 and 50. <p> Note that most of the exploitable parallelism is exhausted by 50 processors. The difference between the improvement gained between 50 processors and 100 processors is small compared to the gain achieved in the range between 1 and 50. In <ref> [12] </ref>, the performance of GJ is measured on the machine using various problem sizes and processor ments them as such. 11 counts. Both these results and the results reported in [12] indicate that the the exploitable parallelism is largely exhausted in the 50 range. <p> In <ref> [12] </ref>, the performance of GJ is measured on the machine using various problem sizes and processor ments them as such. 11 counts. Both these results and the results reported in [12] indicate that the the exploitable parallelism is largely exhausted in the 50 range. In Figure 10b, we show similar scheduling results for PIC. The scheduler elects to exploit no more than two parallel threads in each loop body due to the granularity of the TC2000.
References-found: 12

