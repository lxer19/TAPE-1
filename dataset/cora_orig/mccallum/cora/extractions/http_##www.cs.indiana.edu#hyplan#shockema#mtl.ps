URL: http://www.cs.indiana.edu/hyplan/shockema/mtl.ps
Refering-URL: http://www.cs.indiana.edu/hyplan/shockema/resume.html
Root-URL: http://www.cs.indiana.edu
Email: hockema@ecn.purdue.edu  
Title: Multi-task Learning for Software Agents  
Author: Stephen A. Hockema 
Address: West Lafayette, IN 47907  
Affiliation: Purdue University, School of Electrical and Computer Engineering  
Abstract: This paper describes experiments done to demonstrate the effectiveness of Multi-task Learning (M T L) for software agents. The experiments were carried out on an agent for processing electronic mail (email). M T L is shown to slightly improve the learning rate in this domain over Single-task Learning (ST L) in a k-nearest-neighbor implementation. We then introduce a new method of lazy learning we call Neural Neighbor which lends itself much better to the incorporation of M T L and outperforms both our ST L and M T L k-nearest-neighbor implementations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. S. Abu-Mostafa. </author> <title> Learning from hints in neural networks. </title> <journal> Journal of Complexity, </journal> <volume> 6(2):192198, </volume> <year> 1989. </year>
Reference-contexts: Caruana has shown many potential uses for M T L [7]. This paper explores M T L's effectiveness for software agents. M T L was first studied for back-propagation neural networks [6], [4], <ref> [1] </ref>, but Caruana has since introduced a method for incorporating the technique into other learning algorithms such as k-nearest-neighbor (kNN) and decision trees [7]. This paper first describes results obtained using the technique he suggested for kNN. <p> All feature-weights are started at 0:5 and constrained to remain in the interval <ref> [0; 1] </ref>. For F () we tried both a linear function and a cubic function. (The effects of this will be discussed in Section 3.4.) 0 f refers to the normalized feature differences between the newly predicted instance and the nearest-neighbor instance that was used to make the prediction.
Reference: [2] <author> D. Aha, S. Kasif, J. Rachlin, S. Salzberg, and D. Waltz. </author> <title> Towards a framework for memory-based reasoning. </title> <note> Submitted for publication. Copy found online at URL: http://external.nj.nec.com/homepages/waltz.html, 1995. </note>
Reference: [3] <author> D. Aha and D. Kibler. </author> <title> Noise-tolerant instance-based learning algorithms. </title> <booktitle> Machine Learning, </booktitle> <address> 8:794799, </address> <year> 1991. </year>
Reference-contexts: The kNN algorithm was modified to reduce memory usage and handle noisy instances by 1) associating a confidence metric with each entry in the stored instance database, and 2) only saving incorrectly predicted instances (as suggested in <ref> [3] </ref>). Instances whose confidence falls below a certain threshold are forgotten. A correctly classified instance is consolidated with the saved instance to which it was closest. By this, we mean that the correct, saved instance is moved slightly in the direction of the new instance.
Reference: [4] <author> Rich Caruana. </author> <title> Multitask learning: A knowledge-based source of inductive bias. </title> <booktitle> In Proceedings of the 10th International Conference on Machine Learning, </booktitle> <pages> pages 4148, </pages> <institution> University of Massechusetts, Amherst, </institution> <year> 1993. </year>
Reference-contexts: Caruana has shown many potential uses for M T L [7]. This paper explores M T L's effectiveness for software agents. M T L was first studied for back-propagation neural networks [6], <ref> [4] </ref>, [1], but Caruana has since introduced a method for incorporating the technique into other learning algorithms such as k-nearest-neighbor (kNN) and decision trees [7]. This paper first describes results obtained using the technique he suggested for kNN.
Reference: [5] <author> Rich Caruana. </author> <title> Multitask connectionist learning. </title> <booktitle> In Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <pages> pages 372379, </pages> <year> 1994. </year>
Reference: [6] <author> Rich Caruana. </author> <title> Learning many related tasks at the same time with backpropagation. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 656664. </pages> <address> NIPS94, </address> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Caruana has shown many potential uses for M T L [7]. This paper explores M T L's effectiveness for software agents. M T L was first studied for back-propagation neural networks <ref> [6] </ref>, [4], [1], but Caruana has since introduced a method for incorporating the technique into other learning algorithms such as k-nearest-neighbor (kNN) and decision trees [7]. This paper first describes results obtained using the technique he suggested for kNN.
Reference: [7] <author> Rich Caruana. </author> <title> Algorithms and applications for multitask learning. </title> <booktitle> Machine Learning, </booktitle> <address> 8:8795, </address> <year> 1996. </year>
Reference-contexts: Multi-task Learning (M T L) is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias [8]. Caruana has shown many potential uses for M T L <ref> [7] </ref>. This paper explores M T L's effectiveness for software agents. M T L was first studied for back-propagation neural networks [6], [4], [1], but Caruana has since introduced a method for incorporating the technique into other learning algorithms such as k-nearest-neighbor (kNN) and decision trees [7]. <p> for M T L <ref> [7] </ref>. This paper explores M T L's effectiveness for software agents. M T L was first studied for back-propagation neural networks [6], [4], [1], but Caruana has since introduced a method for incorporating the technique into other learning algorithms such as k-nearest-neighbor (kNN) and decision trees [7]. This paper first describes results obtained using the technique he suggested for kNN. Then we will suggest an improvement to the kNN algorithm that seems very promising, especially for M T L. M T L is very effective when given sets of related tasks to learn. <p> between 0 and 1 which denotes the relative importance of task T to the current task. (Note that when task-weight T =0, no M T L is being done, and when task-weight T =1 the other task is judged to be equally important as the primary task.) Caruana suggests in <ref> [7] </ref> that optimization be performed for these task-weights so that they reflect the relative importance of the additional tasks to the main task. We instead chose to fix them at an experimentally determined value of 1=#f eatures. <p> In other domains, this trade-off might not have to be made. It should be noted that the way we incorporate MTL for the feature-weight updates is not quite the same as the way suggested by Caruana. In <ref> [7] </ref>, he suggests searching for optimal feature-weights by training on instances that have been labeled with outputs for several related tasks. The attribute weights are optimized to yield good performance across these tasks (implying a shared set of feature-weights for all tasks). <p> This implied a modification to Equation 5 of: nn-weight nft += j 0 f g 0 (I tn ) fl (T n f eat-weights tn ) fl task-weight T Note that this is not quite M T L for neural networks as Caruana has described it in <ref> [7] </ref>. Ideally, each node would be making a prediction for all of the related tasks. Then, they could obtain feedback simultaneously on each of these predictions and weight them accordingly. However, we face the same problem here as discussed in Section 2.2.
Reference: [8] <author> Rich Caruana. </author> <title> Algorithms and applications for multitask learning. </title> <booktitle> Machine Learning, </booktitle> <address> 28:4175, </address> <year> 1997. </year>
Reference-contexts: Multi-task Learning (M T L) is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias <ref> [8] </ref>. Caruana has shown many potential uses for M T L [7]. This paper explores M T L's effectiveness for software agents. <p> Then we will suggest an improvement to the kNN algorithm that seems very promising, especially for M T L. M T L is very effective when given sets of related tasks to learn. It is also effective when multiple outputs are functions of the same inputs <ref> [8] </ref>. In this respect, the software agent domain appears to lend itself ideally to M T L. <p> However, it is something that people might be using without knowing it, especially in neural networks. (Caruana points out the example of NETtalk in <ref> [8] </ref>.) The method of M T L query was suggested by Caruana in [8]. It is similar to work being done on collaborative agents. These are systems where multiple agents exist and agents can query each other about unknown or new situations [10][16]. <p> However, it is something that people might be using without knowing it, especially in neural networks. (Caruana points out the example of NETtalk in <ref> [8] </ref>.) The method of M T L query was suggested by Caruana in [8]. It is similar to work being done on collaborative agents. These are systems where multiple agents exist and agents can query each other about unknown or new situations [10][16]. Principles being applied there may be applicable using M T L in single user agents as well.
Reference: [9] <author> Rich Caruana, Summet Baluja, and Tom Mitchell. </author> <title> Using the future to sort out the present: Rankprop and multitask learning for medical risk prediction. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 959965. </pages> <address> NIPS95, </address> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference: [10] <author> Hsinchun Chen. </author> <title> Collaborative systems: Solving the vocabulary problem. </title> <booktitle> Computer, </booktitle> <address> 27(5):5866, </address> <month> May </month> <year> 1994. </year>
Reference: [11] <author> Hsinchun Chen. </author> <title> Internet categorization and search: A self-organizing approach. Journal of Visual Communication and Image Representation, </title> <address> 7(1):88102, </address> <month> March </month> <year> 1996. </year>
Reference: [12] <author> S. Cost and S. Salzberg. </author> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <booktitle> Machine Learning, </booktitle> <pages> pages 5778, </pages> <year> 1993. </year>
Reference-contexts: In addition, they used a Memory Based Reasoning (MBR) technique which is similar to Neural Neighbors. ([2] gives an overview of MBR.) MBR uses a Modified Value Difference Metric (MVDM) in the distance calculation which is calculated using Bayesian probabilities <ref> [12] </ref>. Instead of calculating the difference among the feature values directly, MBR calculates the difference between the conditional probabilities of output classes given the feature values. We made an attempt to incorporate this into MLElm as well. <p> This was especially troublesome for the continuous valued features (like time spent reading). Thus, we eventually abandonded the MBR approach in favor of Neural Neighbors. Cost and Salzberg have tested a technique which involves weighting instances and then using these weights to modify the distance metric <ref> [12] </ref>. This is very similar to what was done with the prediction confidence in Equation 3.
Reference: [13] <author> O. Etzioni and D. Weld. </author> <title> Intelligent agents on the internet: Fact, fiction, and forecast. </title> <journal> IEEE Expert, </journal> <volume> 10(4):4449, </volume> <month> August </month> <year> 1995. </year>
Reference-contexts: These are systems where multiple agents exist and agents can query each other about unknown or new situations [10][16]. Principles being applied there may be applicable using M T L in single user agents as well. Many groups have done work on software agents of various types, such as <ref> [13] </ref>, [14], [22], [21], [24], [25], and [27]. Our email agent is probably the most similar to the one described by Maes and Kozierok in [22], employing similar schemes to make it user friendly.
Reference: [14] <author> L. Hermens and J. Schlimmer. </author> <title> A machine learning apprentice for the completion of repetitive forms. </title> <journal> IEEE Expert, </journal> <volume> 9(1):2833, </volume> <month> February </month> <year> 1994. </year>
Reference-contexts: That is, a software agent learns and modifies its behavior by observing the way it is being used by a human. Many types of software agents have been developed, including agents to maintain personal calendars [24], predict interest in web-pages [25], automate the completion of forms <ref> [14] </ref>, and assist in the handling of electronic mail [22]. It should be noted that in many of these domains, the agent takes on a secretarial role, freeing the human user from having to spend time on repetitive tasks. <p> Principles being applied there may be applicable using M T L in single user agents as well. Many groups have done work on software agents of various types, such as [13], <ref> [14] </ref>, [22], [21], [24], [25], and [27]. Our email agent is probably the most similar to the one described by Maes and Kozierok in [22], employing similar schemes to make it user friendly.
Reference: [15] <author> A. K. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, USA, </address> <year> 1988. </year>
Reference: [16] <editor> Yezdi Lashkari, Max Metral, and Pattie Maes. </editor> <booktitle> Collaborative interface agents. In Proceedings of the 12th National Conference of the American Association of Artificial Intelligence, Part 1 (of 2), </booktitle> <pages> pages 444449, </pages> <address> Menlo Park, CA, USA, 1994. </address> <publisher> AAAI. </publisher>
Reference: [17] <author> Changhwan Lee. </author> <title> An information theoretic similarity-based learning method for databases. </title> <booktitle> In Proceedings of the 10th Conference on Artificial Intelligence, </booktitle> <pages> pages 99105, </pages> <address> Piscat-away, NJ, USA, 1994. </address> <publisher> IEEE. </publisher>
Reference: [18] <author> David D. Lewis. </author> <title> Evaluation of phrasal and cluster representations on a text categorization task. </title> <booktitle> In Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 3750, </pages> <institution> Fort Collins Computer Center, </institution> <address> Fort Collins, CO, USA, 1992. </address> <publisher> ACM. </publisher>
Reference: [19] <author> David D. Lewis. </author> <title> Evaluating and optimizing autonomous text classification systems. </title> <booktitle> In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 246254, </pages> <address> Fort Collins, CO, USA, 1995. </address> <publisher> ACM. </publisher>
Reference: [20] <author> David D. Lewis and Karen Sparck Jones. </author> <title> Natural language processing for information retrieval. </title> <journal> Communications of the ACM, </journal> <volume> 39(1):92101, </volume> <year> 1996. </year>
Reference: [21] <editor> Pattie Maes. </editor> <title> Agents that reduce work and information overload. </title> <journal> Communications of the ACM, </journal> <volume> 37(7):3040, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: Principles being applied there may be applicable using M T L in single user agents as well. Many groups have done work on software agents of various types, such as [13], [14], [22], <ref> [21] </ref>, [24], [25], and [27]. Our email agent is probably the most similar to the one described by Maes and Kozierok in [22], employing similar schemes to make it user friendly.
Reference: [22] <editor> Pattie Maes and Robyn Kozierok. </editor> <booktitle> Learning interface agents. In Proceedings of the 11th National Conference of the American Association of Artificial Intelligence, </booktitle> <pages> pages 459 465, </pages> <address> Menlo Park, CA, USA, 1993. </address> <publisher> AAAI. </publisher>
Reference-contexts: Many types of software agents have been developed, including agents to maintain personal calendars [24], predict interest in web-pages [25], automate the completion of forms [14], and assist in the handling of electronic mail <ref> [22] </ref>. It should be noted that in many of these domains, the agent takes on a secretarial role, freeing the human user from having to spend time on repetitive tasks. <p> We associate with each prediction a confidence level similar to <ref> [22] </ref>. <p> Principles being applied there may be applicable using M T L in single user agents as well. Many groups have done work on software agents of various types, such as [13], [14], <ref> [22] </ref>, [21], [24], [25], and [27]. Our email agent is probably the most similar to the one described by Maes and Kozierok in [22], employing similar schemes to make it user friendly. <p> Many groups have done work on software agents of various types, such as [13], [14], <ref> [22] </ref>, [21], [24], [25], and [27]. Our email agent is probably the most similar to the one described by Maes and Kozierok in [22], employing similar schemes to make it user friendly. In addition, they used a Memory Based Reasoning (MBR) technique which is similar to Neural Neighbors. ([2] gives an overview of MBR.) MBR uses a Modified Value Difference Metric (MVDM) in the distance calculation which is calculated using Bayesian probabilities [12].
Reference: [23] <author> N. McGough. </author> <title> Filtering mail FAQ. </title> <note> Found at URL: http://www.cis.ohio-state.edu/hypertext/faq/usenet/mail/filtering-faq/faq.html, Copyright 1994-1996. </note>
Reference: [24] <author> Tom Mitchell, Rich Caruana, Dayne Freitag, John McDermott, and David Zabowski. </author> <title> Experience with a learning personal assistant. </title> <journal> Communications of the ACM, </journal> <volume> 37(7):8191, </volume> <month> July </month> <year> 1994. </year>
Reference-contexts: That is, a software agent learns and modifies its behavior by observing the way it is being used by a human. Many types of software agents have been developed, including agents to maintain personal calendars <ref> [24] </ref>, predict interest in web-pages [25], automate the completion of forms [14], and assist in the handling of electronic mail [22]. It should be noted that in many of these domains, the agent takes on a secretarial role, freeing the human user from having to spend time on repetitive tasks. <p> Mitchell etal. has already had some success with M T L in an agent <ref> [24] </ref>. Their Calendar Apprentice Program (CAP) is an agent that learns user scheduling preferences for four related meeting attributes. In this domain, M T L was able to take advantage of overlapping features, yielding better performance than ST L. <p> Principles being applied there may be applicable using M T L in single user agents as well. Many groups have done work on software agents of various types, such as [13], [14], [22], [21], <ref> [24] </ref>, [25], and [27]. Our email agent is probably the most similar to the one described by Maes and Kozierok in [22], employing similar schemes to make it user friendly.
Reference: [25] <author> Michael Pazzani, Larry Nguyen, and Stefanus Mantik. </author> <title> Learning from hotlists and coldlists: Towards a WWW information filtering and seeking agent. </title> <booktitle> In Proceedings of the 1995 IEEE 7th International Conference on Tools with Artificial Intelligence, </booktitle> <pages> pages 492495, </pages> <address> Piscataway, NJ, USA, 1995. </address> <publisher> IEEE. </publisher>
Reference-contexts: That is, a software agent learns and modifies its behavior by observing the way it is being used by a human. Many types of software agents have been developed, including agents to maintain personal calendars [24], predict interest in web-pages <ref> [25] </ref>, automate the completion of forms [14], and assist in the handling of electronic mail [22]. It should be noted that in many of these domains, the agent takes on a secretarial role, freeing the human user from having to spend time on repetitive tasks. <p> For indeterminant (uncountable) discrete textual features (e.g. subject line and message con-tent), 0 f is difficult to calculate. For these features, we use a keyword set difference, using an information-based measure to incrementally select relevant keywords as described in <ref> [25] </ref>. <p> Furthermore, we found that randomly re-ordering messages (during the automated portion of the testing) over multiple trials is also unrealistic (and unfair). Because of the lazy style of incremental learning employed and the fact that we used an information-based statistic for selecting keywords from messages <ref> [25] </ref>, randomly ordering messages greatly affected the results, adding a wide variance to the error rates. (The information content of a keyword depends upon the other previously seen words.) Thus, messages were processed in the order in which they had been received, as if the user immediately read each message when <p> Principles being applied there may be applicable using M T L in single user agents as well. Many groups have done work on software agents of various types, such as [13], [14], [22], [21], [24], <ref> [25] </ref>, and [27]. Our email agent is probably the most similar to the one described by Maes and Kozierok in [22], employing similar schemes to make it user friendly.
Reference: [26] <author> Gerard Salton, James Allan, and Amit Singhal. </author> <title> Automatic text decomposition and structuring. </title> <booktitle> Information Processing and Management, </booktitle> <address> 32(2):127138, </address> <month> March </month> <year> 1996. </year>
Reference: [27] <author> Jeffrey A. Schlimmer and Leonard A. Hermens. </author> <title> Software agents: Completing patterns and constructing user interfaces. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1:6189, </volume> <pages> 1993-94. </pages>
Reference-contexts: Principles being applied there may be applicable using M T L in single user agents as well. Many groups have done work on software agents of various types, such as [13], [14], [22], [21], [24], [25], and <ref> [27] </ref>. Our email agent is probably the most similar to the one described by Maes and Kozierok in [22], employing similar schemes to make it user friendly.
References-found: 27

