URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-90-956/CS-TR-90-956.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/ncstrl.uwmadison/CS-TR-90-956/
Root-URL: http://www.cs.wisc.edu
Title: Cricket: A Mapped, Persistent Object Store  
Author: Eugene Shekita Michael Zwilling 
Note: This research was partially supported by DARPA under contracts N00014-88-K-0303 and NAG 2-618, NSF under grant IRI-8657323, and donations from Texas Instruments and Digital Equipment Corporation.  
Address: Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin  
Abstract-found: 0
Intro-found: 1
Reference: [Acce86] <author> M. Accetta et al., </author> <title> "Mach: A New Kernel Foundation for UNIX Development," </title> <booktitle> Proc. of the Summer Usenix Conf., </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: Towards this goal, we have designed a new database storage system called Cricket. 1 Cricket uses the memory management primitives of Mach <ref> [Acce86] </ref> to provide the abstraction of a shared, transactional, single-level store. One advantage of a single-level store is that it provides applications with a uniform view of volatile and non-volatile (i.e., persistent) memory.
Reference: [Agra89] <author> R. Agrawal and N. Gehani, </author> <title> "ODE (Object Database and Environment): The Language and the Data Model," </title> <booktitle> Proc. of the 1989 ACM SIGMOD Conf., </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: This is done to reduce the cost of traversing objects. Unfortunately, swizzling is not as simple as it sounds. There are the issues of what identifiers to swizzle, when to swizzle them, and how to unswizzle them. And in persistent languages based on C <ref> [Agra89, Rich89] </ref>, it is often difficult to know where identifiers are located, when they change, and when they need to be reswizzled [Schu90]. With a single-level store, object identifiers become virtual memory addresses, so all this effort (and its associated cost) can be eliminated.
Reference: [Appe86] <author> A. Appel et al., </author> <title> "Garbage Collection Can be Faster Than Stack Allocation," </title> <institution> Computer Science Tech. </institution> <type> Report 045-86, </type> <institution> Princeton Univ., </institution> <month> June </month> <year> 1986. </year> <month> 19 </month>
Reference-contexts: This required special modifications to the operating system. Address exceptions have also been used by Li [Li86] to implement memory coherency in a distributed virtual memory system and in the language ML to trigger garbage collection <ref> [Appe86] </ref>. 5.1.4. Disk Allocation Cricket uses an extent-based scheme for managing disk space. A disk is partitioned into extents, with each extent containing the same number of pages usually at least 16 Kbytes worth.
Reference: [Atki87] <author> M. Atkinson and P. Buneman, </author> <title> "Types and Persistence in Database Programming Languages," </title> <journal> ACM Computing Surveys, </journal> <volume> 19(2), </volume> <year> 1987. </year>
Reference-contexts: While these storage systems will undoubtedly meet the performance demands of many new applications, our view is that for some applications there is still considerable room for improvement. In particular, we feel that for design environments [Katz87, Chan89], persistent programming languages <ref> [Cock84, Atki87] </ref>, and other applications in which response time rather than throughput is often the key concern, different storage techniques that those currently in use can provide better performance. <p> With a single-level store, object identifiers become virtual memory addresses, so all this effort (and its associated cost) can be eliminated. Yet another advantage of using a single-level store is that persistence and type can be kept orthogonal <ref> [Atki87] </ref>. That is, application code can be written without concern for whether it is operating on non-persistent or persistent data.
Reference: [Bako90] <author> H. Bakoglu et al., </author> <title> "The IBM RISC System/6000 Processor: Hardware Overview," </title> <journal> IBM Journal of Research and Development, </journal> <volume> 34(1), </volume> <year> 1990. </year>
Reference-contexts: More will be said about this shortly. g For many emerging database applications, a 32-bit address space is sufficient. Moreover, with the rapid increase in memory sizes and with shared-memory multiprocessors becoming more commonplace, processors with large virtual address spaces may soon become available. In fact, IBM's RS/6000 <ref> [Bako90] </ref> already supports a 52-bit address space, and HP's Precision Architecture [Maho86] supports a 64-bit address space (although strictly speaking, these are both segmented architectures). g As the cost of memory decreases, large page tables will become less of a concern.
Reference: [Bens72] <editor> A. Bensoussan et al., </editor> <title> "The Multics Virtual Memory: Concepts and Design," </title> <journal> CACM, </journal>
Reference-contexts: eliminating the need for applications to distinguish and convert between non-persistent and persistent data formats [Cope90]. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 As the reader shall see, the flow of control really hops around in our storage system! Although storage systems based on a single-level store have been proposed as far back as Multics <ref> [Bens72] </ref>, Cricket offers several features that have not been combined in one system before. One of Cricket's key features is the ability to let applications directly access persistent data, but at the same time maintain the applications in separate (and potentially distributed) protection domains. <p> This is in contrast to a conventional two-level store, where access to persistent data is less direct and a user-level buffer pool is typically maintained to cache disk pages. Single-level stores are nothing new, of course. Their origins can be traced back almost 20 years to Multics <ref> [Bens72] </ref>, and many operating systems provide mapped file facilities that effectively implement a single-level store. But more importantly, database implementors have repeatedly rejected the idea of using the mapped file facilities offered by operating systems and instead have chosen to manage buffering and disk storage themselves.
Reference: [Bora90] <author> H. Boral et al., </author> <title> "Prototyping Bubba, A Highly Parallel Database System" IEEE Trans. </title> <journal> on Data and Knowledge Eng., </journal> <volume> 2(1), </volume> <year> 1990. </year> <month> 15(5), May </month> <year> 1972. </year>
Reference-contexts: Because of RPC costs, this turned out to be more expensive than the design we have chosen. 9 It is important to note that using address exceptions to trigger locking is not a new idea. Exceptions were also used in the Bubba database system <ref> [Bora90] </ref> to set locks. Our scheme differs from theirs in that we perform lock management in a user-level task, whereas locking was performed by the operating system in Bubba. This required special modifications to the operating system. <p> To us, they suggest that for its intended applications, Cricket can provide better performance than a general-purpose database storage system. 7. RELATED WORK The work most closely related to ours is that done by the implementors of the Bubba database system at MCC <ref> [Bora90, Cope90] </ref>. In Bubba, the kernel of an AT&T UNIX System V kernel was modified to provide a single-level store with automatic, two-phase, page-level locking. Although we have borrowed a number of ideas from Bubba, several differences distinguish Cricket from the approach taken in Bubba.
Reference: [Blac88] <author> D. Black et al., </author> <title> "The Mach Exception Handling Facility," </title> <institution> Computer Science Tech. </institution> <type> Report 88-129, </type> <institution> Car-negie Mellon Univ., </institution> <month> April </month> <year> 1988. </year>
Reference-contexts: This would involve added complexity, copying costs, extra buffering (possibly leading to double-paging [Bric76]), and would also destroy the abstraction of a single-level store. 5.1.3. Concurrency Control By default, Cricket provides transparent, two-phase, page-level locking for client access to the database. This is done using Mach's exception handling facility <ref> [Blac88] </ref>, which allows the exceptions of one task to be caught and handled by another task. In our case, Cricket handles exceptions for client tasks. When a client first connects to Cricket, its exception handler is set to be the Cricket server.
Reference: [Bric76] <author> P. Brice and S. Sherman, </author> <title> "An Extension of the Performance of a Database Manager in a Virtual Memory System using Partially Locked Virtual Buffers," </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 6(1), </volume> <year> 1976. </year>
Reference-contexts: Without direct access to the database, a client application would have to make an explicit request to read data into its address space, and it would have to take analogous steps to have it written back. This would involve added complexity, copying costs, extra buffering (possibly leading to double-paging <ref> [Bric76] </ref>), and would also destroy the abstraction of a single-level store. 5.1.3. Concurrency Control By default, Cricket provides transparent, two-phase, page-level locking for client access to the database.
Reference: [Care86] <author> M. Carey et al., </author> <title> "Object and File Management in the EXODUS Extensible Database System," </title> <booktitle> Proc. of the 12th Intl. Conf. on Very Large Databases, </booktitle> <month> Sept. </month> <year> 1986. </year>
Reference-contexts: Out of this research has come a variety of new storage systems that attempt to provide more functionality as well as improved performance for these emerging applications. Examples of such systems include <ref> [Care86, Horn87, Lind87, Moss88, Sche90, Ston90] </ref>. While these storage systems will undoubtedly meet the performance demands of many new applications, our view is that for some applications there is still considerable room for improvement. <p> This is followed by Section 4, where we review Mach's external pager facilities [Youn87], which play a central role in Cricket's design. Cricket's system architecture is then described in Section 5, and in Section 6, we provide some preliminary performance results that compare Cricket to the EXODUS Storage Manager <ref> [Care86] </ref>. These preliminary results show that for its intended applications, Cricket can provide better performance than a general-purpose database storage system. Finally, related work is mentioned in Section 7, and conclusions are drawn in Section 8. 2. <p> Implementors of persistent languages have already run up against many of the same problems [Rich89, Schu90]. More recently, a number of new database storage systems have been proposed to address some of these issues, e.g., <ref> [Care86, Horn87, Lind87, Moss88, Sche90, Ston90] </ref>. But our feeling is that for design environments and persistent languages, many of these systems will still fall short of the mark. This is due to the fact that many of them still use a procedure-based interface to access persistent data. <p> This is due to the fact that many of them still use a procedure-based interface to access persistent data. Moreover, many of them still use fairly traditional recovery techniques based on write-ahead logging [Moh89a]. It was these observations and also our experiences with the EXODUS Storage Manager <ref> [Care86] </ref> and the persistent language E [Rich89, Schu90] that motivated us to design Cricket. 3. THE ARGUMENT FOR A SINGLE-LEVEL STORE As mentioned earlier, Cricket provides the abstraction of a single-level store to applications, and we advertise this as one of its key features. <p> Because a single-level store makes use of MMU hardware, multi-page objects can be made to appear in memory as though they were contiguous without actually requiring physical contiguity. This is in contrast to the EXODUS Storage Manager <ref> [Care86] </ref>, where considerable effort was required to implement contiguous buffering of multi-page objects. 4. EXTERNAL PAGERS IN MACH In the next section, we will describe Cricket's system architecture. <p> Comparing Cricket to a General-Purpose Database Storage System To determine how Cricket's performance compares to a general-purpose database storage system, we ran a treesearch benchmark on Cricket and also on the single-user version of the EXODUS Storage Manager <ref> [Care86] </ref>. In this benchmark, a persistent tree is searched in a depth-first manner, and the number of times the tree is searched can be varied. No processing is done on a node other than to follow its edges to neighboring nodes.
Reference: [Chan88] <author> A. Chang and M. Mergen, </author> <title> "801 Storage: Architecture and Programming," </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 6(1), </volume> <year> 1988. </year>
Reference-contexts: Cricket also offers transparent concurrency control and recovery, and since it runs as a user-level process on Mach, it is easily ported to a variety of machines. We believe that these features distinguish Cricket from other recent proposals based on a single-level store <ref> [Chan88, Ford88, Spec88, Cope90] </ref> and make it an attractive platform for design environments and persistent languages. The remainder of this paper provides a detailed description of Cricket. <p> In contrast to Cricket, Camelot also provides fairly conventional locking and recovery services that must be explicitly invoked by its clients. 18 The last related work that we need to mention is that done in IBM's 801 prototype hardware architecture <ref> [Chan88] </ref>. In the 801 prototype, the operating system essentially provided mapped files with automatic concurrency control and recovery. Special hardware was added for both locking and logging. While this is an interesting approach, our view is that it suffers from being too inflexible.
Reference: [Chan89] <author> E. Chang and R. Katz, </author> <title> "Exploiting Inheritance and Structure Semantics for Effective Clustering and Buffering in an Object-Oriented DBMS", </title> <booktitle> Proc. of the 1989 ACM SIGMOD Conf., </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Examples of such systems include [Care86, Horn87, Lind87, Moss88, Sche90, Ston90]. While these storage systems will undoubtedly meet the performance demands of many new applications, our view is that for some applications there is still considerable room for improvement. In particular, we feel that for design environments <ref> [Katz87, Chan89] </ref>, persistent programming languages [Cock84, Atki87], and other applications in which response time rather than throughput is often the key concern, different storage techniques that those currently in use can provide better performance.
Reference: [Chou85] <author> H-T. Chou and D. Dewitt, </author> <title> "An Evaluation of Buffer Management Strategies for Relational Database Systems," </title> <booktitle> Proc. of the 1985 VLDB Conf., </booktitle> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: Among the most notable are: g Operating systems typically provide no control over when the data pages of a mapped file are written to disk, which makes it impossible to use recovery protocols like write-ahead logging [Moh89a] and sophisticated buffer management <ref> [Chou85] </ref>. g The virtual address space provided by mapped files, usually limited to 32 bits, is too small to represent a large database. g Page tables associated with mapped files can become excessively large.
Reference: [Cock84] <author> W. Cockshott et al., </author> <title> "Persistent Object Management Systems," </title> <journal> Software-Practice and Experience, </journal> <volume> vol. 14, </volume> <year> 1984. </year>
Reference-contexts: While these storage systems will undoubtedly meet the performance demands of many new applications, our view is that for some applications there is still considerable room for improvement. In particular, we feel that for design environments [Katz87, Chan89], persistent programming languages <ref> [Cock84, Atki87] </ref>, and other applications in which response time rather than throughput is often the key concern, different storage techniques that those currently in use can provide better performance. <p> To reduce the cost of accessing persistent data, persistent languages often use "pointer swizzling" <ref> [Cock84, Moss90, Schu90] </ref>. In pointer swizzling, the embedded object identifiers (i.e., pointers) that are stored in persistent objects are typically converted to virtual addresses while they are in memory. This is done to reduce the cost of traversing objects. Unfortunately, swizzling is not as simple as it sounds.
Reference: [Cope90] <author> G. Copeland et al., </author> <title> "Uniform Object Management," </title> <booktitle> Proc. of the Intl. Conf. on Extending Database Technology, </booktitle> <month> March </month> <year> 1990. </year>
Reference-contexts: One advantage of a single-level store is that it provides applications with a uniform view of volatile and non-volatile (i.e., persistent) memory. This in turn can lead to improved performance by eliminating the need for applications to distinguish and convert between non-persistent and persistent data formats <ref> [Cope90] </ref>. hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 As the reader shall see, the flow of control really hops around in our storage system! Although storage systems based on a single-level store have been proposed as far back as Multics [Bens72], Cricket offers several features that have not been combined in one system before. <p> Cricket also offers transparent concurrency control and recovery, and since it runs as a user-level process on Mach, it is easily ported to a variety of machines. We believe that these features distinguish Cricket from other recent proposals based on a single-level store <ref> [Chan88, Ford88, Spec88, Cope90] </ref> and make it an attractive platform for design environments and persistent languages. The remainder of this paper provides a detailed description of Cricket. <p> As pointed out in [Eppi89] and <ref> [Cope90] </ref>, however, these criticisms may no longer be as valid as they once were. The above items can be countered by arguing that: 3 g With the right operating system hooks, it is possible to control when the data pages of a mapped file are writ-ten to disk. <p> As a result, applications often convert persistent data to a more efficient in-memory format before operating on it. Unfortunately, this can involve copying costs, added buffering requirements, and format conversions. With a single-level store, non-persistent and persistent data can have a uniform representation and these costs can be eliminated <ref> [Cope90] </ref>. This has obvious benefits in applications like design environments, where the real-time cost of accessing and updating persistent data is a key concern. 4 For similar reasons, we feel that a single-level store will also simplify the job of implementing a persistent language. <p> To us, they suggest that for its intended applications, Cricket can provide better performance than a general-purpose database storage system. 7. RELATED WORK The work most closely related to ours is that done by the implementors of the Bubba database system at MCC <ref> [Bora90, Cope90] </ref>. In Bubba, the kernel of an AT&T UNIX System V kernel was modified to provide a single-level store with automatic, two-phase, page-level locking. Although we have borrowed a number of ideas from Bubba, several differences distinguish Cricket from the approach taken in Bubba.
Reference: [Date86] <author> C. Date, </author> <title> "An Introduction to Database Systems," </title> <journal> Ch. </journal> <volume> 3., pg. 56, </volume> <publisher> Addison-Wesley, </publisher> <address> Reading Mass. </address> <year> 1986. </year>
Reference: [DBE87] <institution> Database Engineering, </institution> <note> Special Issue on Extensible Database Systems, </note> <editor> M. Carey ed., </editor> <volume> 10(2), </volume> <month> June </month> <year> 1987. </year>
Reference-contexts: 1. INTRODUCTION In recent years, there has been a great deal of research in extending database technology to meet the needs of emerging database applications such as text management and multi-media office systems (see <ref> [DBE87] </ref> for a good survey). Out of this research has come a variety of new storage systems that attempt to provide more functionality as well as improved performance for these emerging applications. Examples of such systems include [Care86, Horn87, Lind87, Moss88, Sche90, Ston90].
Reference: [DeWi90] <author> D. DeWitt et al., </author> <title> "A Study of Three Alternative Workstation-Server Architectures for Object-Oriented Database Systems," </title> <institution> Computer Science Tech. </institution> <type> Report 907, </type> <month> Jan. </month> <year> 1990. </year> <institution> Univ. of Wisconsin, </institution>
Reference-contexts: Although we could use the algorithms described by Li [Li86] to maintain memory coherency across machines, transaction semantics open up the possibility for us to use more efficient algorithms. There has been some work done in this area (see <ref> [Wilk90, Dewi90] </ref>), but not in the context of a single-level store. One of Cricket's designers is actively working on this problem already [Fran90]. 13 Client Task Client Task . . . database Cricket Front-End Workstation Client Task Client Task . . .
Reference: [Drav88] <author> R. Draves and E. Cooper, </author> <title> "C Threads," </title> <institution> Computer Science Tech. </institution> <type> Report 88-154, </type> <institution> Carnegie Mellon Univ., </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: More will be said about this shortly. The Cricket server is multi-threaded to permit true parallelism on multiprocessors and also to improve throughput by permitting threads to run even when others are blocked on synchronous events like I/O. The Mach C-Threads package <ref> [Drav88] </ref> is used to create and manage threads. When Cricket starts up, it creates a pool of threads which all line up on the same central message queue waiting to service client or kernel requests. A given thread is not tied to any particular function or transaction.
Reference: [Duch89] <author> D. Duchamp, </author> <title> "Analysis of Transaction Management Performance," </title> <booktitle> Proc. of the 11th Symposium on Operating System Principles, </booktitle> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: Despite these compelling arguments (see [Eppi89] for several more), the jury is still out on whether a single-level store offers any advantages for traditional database applications. In fact, the performance results presented in Eppinger's Ph.D thesis and also in <ref> [Duch89] </ref> seem to argue that it may not be a good idea for transaction processing. Interestingly enough, the real problem with a using single-level store for transaction processing appears to be the high cost of handling page faults for persistent data rather than the criticisms mentioned above. 3.1.
Reference: [Eppi89] <author> J. Eppinger, </author> <title> "Virtual Memory Management for Transaction Processing Systems," </title> <type> Ph.D thesis, </type> <institution> Computer Science Tech. </institution> <type> Report 89-115, </type> <institution> Carnegie Mellon Univ., </institution> <month> Feb. </month> <year> 1989. </year>
Reference-contexts: As pointed out in <ref> [Eppi89] </ref> and [Cope90], however, these criticisms may no longer be as valid as they once were. The above items can be countered by arguing that: 3 g With the right operating system hooks, it is possible to control when the data pages of a mapped file are writ-ten to disk. <p> Inverted page tables exhibit the desirable property of growing in pro portion to the size of physical memory rather than the size of virtual memory. Despite these compelling arguments (see <ref> [Eppi89] </ref> for several more), the jury is still out on whether a single-level store offers any advantages for traditional database applications. In fact, the performance results presented in Eppinger's Ph.D thesis and also in [Duch89] seem to argue that it may not be a good idea for transaction processing. <p> Consequently, an LRU replacement policy is used by default. For the types of applications we have in mind, where the working set of an application will typically fit in memory, this is expected to be adequate. As noted in <ref> [Eppi89] </ref>, the beauty of letting Mach buffer regular data is that it effectively provides a buffer pool that dynamically changes its size in response to other system activity. We examined two alternatives for managing system meta-data such as the page-allocation bitmaps.
Reference: [Ford88] <author> S. Ford et al., "ZEITGEIST: </author> <title> Database Support for Object-Oriented Programming," </title> <booktitle> The 2nd Workshop on Object-Oriented Database Systems, </booktitle> <year> 1988. </year>
Reference-contexts: Cricket also offers transparent concurrency control and recovery, and since it runs as a user-level process on Mach, it is easily ported to a variety of machines. We believe that these features distinguish Cricket from other recent proposals based on a single-level store <ref> [Chan88, Ford88, Spec88, Cope90] </ref> and make it an attractive platform for design environments and persistent languages. The remainder of this paper provides a detailed description of Cricket.
Reference: [Fran90] <author> M. Franklin, et al. </author> <title> Paper in progress on algorithms for maintaining cache coherency in a client/server hardware environment. </title> <institution> Univ. of Wisconsin. </institution>
Reference-contexts: There has been some work done in this area (see [Wilk90, Dewi90]), but not in the context of a single-level store. One of Cricket's designers is actively working on this problem already <ref> [Fran90] </ref>. 13 Client Task Client Task . . . database Cricket Front-End Workstation Client Task Client Task . . . Cricket Front-End Workstation . . . global functions: local functionslocal functions cache coherency disk allocation - I/O, etc.
Reference: [Horn87] <author> M. Hornick and S. Zdonik, </author> <title> "A Shared, Segmented Memory System for an Object-Oriented Database," </title> <journal> ACM Trans. on Office Information Systems, </journal> <volume> 5(1), </volume> <year> 1987. </year>
Reference-contexts: Out of this research has come a variety of new storage systems that attempt to provide more functionality as well as improved performance for these emerging applications. Examples of such systems include <ref> [Care86, Horn87, Lind87, Moss88, Sche90, Ston90] </ref>. While these storage systems will undoubtedly meet the performance demands of many new applications, our view is that for some applications there is still considerable room for improvement. <p> Implementors of persistent languages have already run up against many of the same problems [Rich89, Schu90]. More recently, a number of new database storage systems have been proposed to address some of these issues, e.g., <ref> [Care86, Horn87, Lind87, Moss88, Sche90, Ston90] </ref>. But our feeling is that for design environments and persistent languages, many of these systems will still fall short of the mark. This is due to the fact that many of them still use a procedure-based interface to access persistent data.
Reference: [Katz87] <author> R. Katz and E. Chang, </author> <title> "Managing Change in a Computer-Aided Design Database," </title> <booktitle> Proc. of the 1987 VLDB Conf., </booktitle> <month> Sept., </month> <year> 1987 </year>
Reference-contexts: Examples of such systems include [Care86, Horn87, Lind87, Moss88, Sche90, Ston90]. While these storage systems will undoubtedly meet the performance demands of many new applications, our view is that for some applications there is still considerable room for improvement. In particular, we feel that for design environments <ref> [Katz87, Chan89] </ref>, persistent programming languages [Cock84, Atki87], and other applications in which response time rather than throughput is often the key concern, different storage techniques that those currently in use can provide better performance. <p> To illustrate why, it is useful to step through the execution of a design transaction in a CAD/CAM system <ref> [Katz87] </ref>. There, transactions can be broken down into three basic phases: 1) a loading phase, when the design is loaded into memory from disk, 2) a work phase, during which the design is repeatedly changed, and 3) a saving phase, when design changes are committed.
Reference: [Kern78] <author> B. Kernighan and D. Ritchie, </author> <title> "The C Programming Language," </title> <publisher> Prentice-Hall, </publisher> <year> 1978. </year>
Reference-contexts: Because of its widespread use, our view is that nobody will take us seriously if we are unable to support applications written in C <ref> [Kern78] </ref> or its derivatives.
Reference: [Li86] <author> K. Li and P. Hudak, </author> <title> "Memory Coherence in Shared Virtual Memory Systems," </title> <booktitle> Proc. of the 5th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: Our scheme differs from theirs in that we perform lock management in a user-level task, whereas locking was performed by the operating system in Bubba. This required special modifications to the operating system. Address exceptions have also been used by Li <ref> [Li86] </ref> to implement memory coherency in a distributed virtual memory system and in the language ML to trigger garbage collection [Appe86]. 5.1.4. Disk Allocation Cricket uses an extent-based scheme for managing disk space. <p> Note that because we provide a single-level store to clients, this architecture supports what amounts to distributed, transactional, shared, persistent virtual memory (phew!). Although we could use the algorithms described by Li <ref> [Li86] </ref> to maintain memory coherency across machines, transaction semantics open up the possibility for us to use more efficient algorithms. There has been some work done in this area (see [Wilk90, Dewi90]), but not in the context of a single-level store.
Reference: [Litw80] <author> W. Litwin, </author> <title> "Linear Hashing: A New Tool for File and Table Addressing," </title> <booktitle> Proc. of the 1980 VLDB Conf., </booktitle> <month> Aug. </month> <year> 1980. </year>
Reference-contexts: A disk is partitioned into extents, with each extent containing the same number of pages usually at least 16 Kbytes worth. Extents and the pages within an extent are allocated in a lazy manner, much like in Camelot. Linear hashing <ref> [Litw80] </ref> is used to map a virtual address to a physical extent on disk, allowing us to efficiently handle sparse databases. Because hashing is done on an extent basis, the hash table will generally consume very little space.
Reference: [Lind87] <author> B. Lindsay et al., </author> <title> "A Data Management Extension Architecture," </title> <booktitle> Proc. of the 1987 ACM SIGMOD Conf., </booktitle> <month> May </month> <year> 1987. </year>
Reference-contexts: Out of this research has come a variety of new storage systems that attempt to provide more functionality as well as improved performance for these emerging applications. Examples of such systems include <ref> [Care86, Horn87, Lind87, Moss88, Sche90, Ston90] </ref>. While these storage systems will undoubtedly meet the performance demands of many new applications, our view is that for some applications there is still considerable room for improvement. <p> Implementors of persistent languages have already run up against many of the same problems [Rich89, Schu90]. More recently, a number of new database storage systems have been proposed to address some of these issues, e.g., <ref> [Care86, Horn87, Lind87, Moss88, Sche90, Ston90] </ref>. But our feeling is that for design environments and persistent languages, many of these systems will still fall short of the mark. This is due to the fact that many of them still use a procedure-based interface to access persistent data.
Reference: [Maho86] <author> M. Mahon et al., </author> <title> "Hewlett-Packard Precision Architecture: The Processor," </title> <journal> Hewlett-Packard Journal, </journal> <month> August </month> <year> 1986, </year> <pages> pp. 4-22. 20 </pages>
Reference-contexts: Moreover, with the rapid increase in memory sizes and with shared-memory multiprocessors becoming more commonplace, processors with large virtual address spaces may soon become available. In fact, IBM's RS/6000 [Bako90] already supports a 52-bit address space, and HP's Precision Architecture <ref> [Maho86] </ref> supports a 64-bit address space (although strictly speaking, these are both segmented architectures). g As the cost of memory decreases, large page tables will become less of a concern.
Reference: [Maie89] <author> D. Maier, </author> <title> "Making Database Systems Fast Enough for CAD Applications," in Object-Oriented Con--cepts, Database and Applications, </title> <editor> W. Kim and F. Lochovsky, eds., </editor> <publisher> Addison-Wesley, </publisher> <year> 1987, </year> <pages> pp. 573-581. </pages>
Reference-contexts: There, transactions can be broken down into three basic phases: 1) a loading phase, when the design is loaded into memory from disk, 2) a work phase, during which the design is repeatedly changed, and 3) a saving phase, when design changes are committed. As noted in <ref> [Maie89] </ref>, this load/work/save paradigm is substantially different from a traditional database workload. During the work phase, accesses are unpredictable and fast response time is the key performance criteria rather than system throughput. <p> Unfortunately, traditional database storage systems are not geared for these sort of access patterns. Among other things, the procedure-based interface that must typically be used to traverse and update persistent objects is too slow [Moss90]. And as noted in <ref> [Maie89] </ref>, the recovery protocols are often inappropriate. For example, generating a log record for each update in the work phase of a design transaction would obviously have a negative impact on response time (not to mention the volumes of log data that could be generated).
Reference: [Moh89a] <author> C. Mohan et al., </author> <title> "ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking and Partial Rollbacks Using Write-Ahead Logging," </title> <institution> IBM Research Report RJ6649, </institution> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: This is due to the fact that many of them still use a procedure-based interface to access persistent data. Moreover, many of them still use fairly traditional recovery techniques based on write-ahead logging <ref> [Moh89a] </ref>. It was these observations and also our experiences with the EXODUS Storage Manager [Care86] and the persistent language E [Rich89, Schu90] that motivated us to design Cricket. 3. <p> Among the most notable are: g Operating systems typically provide no control over when the data pages of a mapped file are written to disk, which makes it impossible to use recovery protocols like write-ahead logging <ref> [Moh89a] </ref> and sophisticated buffer management [Chou85]. g The virtual address space provided by mapped files, usually limited to 32 bits, is too small to represent a large database. g Page tables associated with mapped files can become excessively large. <p> And, as our preliminary results will show, exception handling in Mach is not drastically more expensive than sending an RPC. 2 As mentioned earlier, we are also experimenting with different concurrency control options other than simple two-phase, page-level locking. Among other things, we eventually intend to support dirty reads <ref> [Moh89a] </ref> and also design- or file-level locks. The latter would be used by design transactions, where aborting a long-running transaction due to a deadlock makes little sense. <p> In such an environment, traditional old-value/new-value logging is clearly inappropriate. At this point, we have decided that for disk allocation data, indexes, and all other meta-data, we will use the ARIES recovery algorithm <ref> [Moh89a] </ref>, which is based on operation logging.
Reference: [Moh89b] <author> C. Mohan and F. Levine, "ARIES/IM: </author> <title> An Efficient and High Concurrency Index Management Method Using Write-Ahead Logging," </title> <institution> IBM Research Report RJ6846, </institution> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: Consequently, index pages cannot be treated as regular data. Obtaining adequate system performance usually requires fairly complex concurrency control and recovery algorithms to be used on indexes <ref> [Moh89b] </ref>. (In general, the same holds true for all meta-data structures.) Index management presents something of a dilemma because on the one hand we would like to protect indexes from being damaged by client applications, but on the other hand the cost of sending an RPC to the Cricket server for
Reference: [Moss88] <author> J. Moss and S. Sinofsky, </author> <title> "Managing Persistent Data with Mneme: Designing a Reliable, Shared Object Interface," </title> <booktitle> in Advances in Object-Oriented Database Systems, vol. 334 of Lecture Notes in Computer Science, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1988, </year> <pages> pp. 298-316. </pages>
Reference-contexts: Out of this research has come a variety of new storage systems that attempt to provide more functionality as well as improved performance for these emerging applications. Examples of such systems include <ref> [Care86, Horn87, Lind87, Moss88, Sche90, Ston90] </ref>. While these storage systems will undoubtedly meet the performance demands of many new applications, our view is that for some applications there is still considerable room for improvement. <p> Implementors of persistent languages have already run up against many of the same problems [Rich89, Schu90]. More recently, a number of new database storage systems have been proposed to address some of these issues, e.g., <ref> [Care86, Horn87, Lind87, Moss88, Sche90, Ston90] </ref>. But our feeling is that for design environments and persistent languages, many of these systems will still fall short of the mark. This is due to the fact that many of them still use a procedure-based interface to access persistent data.
Reference: [Moss90] <author> J. Moss, </author> <title> "Working with Persistent Objects: To Swizzle or Not to Swizzle," </title> <institution> Computer Science Tech. </institution> <type> Report 90-38, </type> <institution> Univ. of Massachusetts, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: Unfortunately, traditional database storage systems are not geared for these sort of access patterns. Among other things, the procedure-based interface that must typically be used to traverse and update persistent objects is too slow <ref> [Moss90] </ref>. And as noted in [Maie89], the recovery protocols are often inappropriate. For example, generating a log record for each update in the work phase of a design transaction would obviously have a negative impact on response time (not to mention the volumes of log data that could be generated). <p> In most database storage systems, the format of persistent data and the access to it usually differs from that of non-persistent data. Moreover, the cost of accessing persistent data is generally more expensive, even after it has been brought into memory <ref> [Moss90, Schu90] </ref>. As a result, applications often convert persistent data to a more efficient in-memory format before operating on it. Unfortunately, this can involve copying costs, added buffering requirements, and format conversions. <p> To reduce the cost of accessing persistent data, persistent languages often use "pointer swizzling" <ref> [Cock84, Moss90, Schu90] </ref>. In pointer swizzling, the embedded object identifiers (i.e., pointers) that are stored in persistent objects are typically converted to virtual addresses while they are in memory. This is done to reduce the cost of traversing objects. Unfortunately, swizzling is not as simple as it sounds.
Reference: [Rich89] <author> J. Richardson, E: </author> <title> A Persistent Systems Implementation Language, </title> <type> Ph.D thesis, </type> <institution> Computer Science Tech. </institution> <type> Report 868, </type> <institution> Univ. of Wisconsin, </institution> <month> August </month> <year> 1989. </year>
Reference-contexts: In general, we would argue that these problems are not just limited to design environments. Implementors of persistent languages have already run up against many of the same problems <ref> [Rich89, Schu90] </ref>. More recently, a number of new database storage systems have been proposed to address some of these issues, e.g., [Care86, Horn87, Lind87, Moss88, Sche90, Ston90]. But our feeling is that for design environments and persistent languages, many of these systems will still fall short of the mark. <p> Moreover, many of them still use fairly traditional recovery techniques based on write-ahead logging [Moh89a]. It was these observations and also our experiences with the EXODUS Storage Manager [Care86] and the persistent language E <ref> [Rich89, Schu90] </ref> that motivated us to design Cricket. 3. THE ARGUMENT FOR A SINGLE-LEVEL STORE As mentioned earlier, Cricket provides the abstraction of a single-level store to applications, and we advertise this as one of its key features. <p> This is done to reduce the cost of traversing objects. Unfortunately, swizzling is not as simple as it sounds. There are the issues of what identifiers to swizzle, when to swizzle them, and how to unswizzle them. And in persistent languages based on C <ref> [Agra89, Rich89] </ref>, it is often difficult to know where identifiers are located, when they change, and when they need to be reswizzled [Schu90]. With a single-level store, object identifiers become virtual memory addresses, so all this effort (and its associated cost) can be eliminated.
Reference: [Sche90] <editor> H. Schek et al., </editor> <title> "The DASDBS Project: Objectives, Experiences, and Future Perspectives," </title> <journal> IEEE Trans. on Data and Knowledge Eng., </journal> <volume> 2(1), </volume> <year> 1990. </year>
Reference-contexts: Out of this research has come a variety of new storage systems that attempt to provide more functionality as well as improved performance for these emerging applications. Examples of such systems include <ref> [Care86, Horn87, Lind87, Moss88, Sche90, Ston90] </ref>. While these storage systems will undoubtedly meet the performance demands of many new applications, our view is that for some applications there is still considerable room for improvement. <p> Implementors of persistent languages have already run up against many of the same problems [Rich89, Schu90]. More recently, a number of new database storage systems have been proposed to address some of these issues, e.g., <ref> [Care86, Horn87, Lind87, Moss88, Sche90, Ston90] </ref>. But our feeling is that for design environments and persistent languages, many of these systems will still fall short of the mark. This is due to the fact that many of them still use a procedure-based interface to access persistent data.
Reference: [Schu90] <author> D. Schuh et al., </author> <title> "Persistence in E Revisited Implementation Experiences," </title> <booktitle> Proc. of the 4th Intl. Workshop on Persistent Object Systems Design, Implementation and Use, </booktitle> <month> Sept. </month> <year> 1990. </year> <title> [Spec88] "The Guide to the Camelot Distributed Transaction Facility: Release 1," </title> <editor> A. Spector and K. Swedlow eds., </editor> <publisher> Carnegie Mellon Univ., </publisher> <year> 1988. </year>
Reference-contexts: In general, we would argue that these problems are not just limited to design environments. Implementors of persistent languages have already run up against many of the same problems <ref> [Rich89, Schu90] </ref>. More recently, a number of new database storage systems have been proposed to address some of these issues, e.g., [Care86, Horn87, Lind87, Moss88, Sche90, Ston90]. But our feeling is that for design environments and persistent languages, many of these systems will still fall short of the mark. <p> Moreover, many of them still use fairly traditional recovery techniques based on write-ahead logging [Moh89a]. It was these observations and also our experiences with the EXODUS Storage Manager [Care86] and the persistent language E <ref> [Rich89, Schu90] </ref> that motivated us to design Cricket. 3. THE ARGUMENT FOR A SINGLE-LEVEL STORE As mentioned earlier, Cricket provides the abstraction of a single-level store to applications, and we advertise this as one of its key features. <p> In most database storage systems, the format of persistent data and the access to it usually differs from that of non-persistent data. Moreover, the cost of accessing persistent data is generally more expensive, even after it has been brought into memory <ref> [Moss90, Schu90] </ref>. As a result, applications often convert persistent data to a more efficient in-memory format before operating on it. Unfortunately, this can involve copying costs, added buffering requirements, and format conversions. <p> To reduce the cost of accessing persistent data, persistent languages often use "pointer swizzling" <ref> [Cock84, Moss90, Schu90] </ref>. In pointer swizzling, the embedded object identifiers (i.e., pointers) that are stored in persistent objects are typically converted to virtual addresses while they are in memory. This is done to reduce the cost of traversing objects. Unfortunately, swizzling is not as simple as it sounds. <p> There are the issues of what identifiers to swizzle, when to swizzle them, and how to unswizzle them. And in persistent languages based on C [Agra89, Rich89], it is often difficult to know where identifiers are located, when they change, and when they need to be reswizzled <ref> [Schu90] </ref>. With a single-level store, object identifiers become virtual memory addresses, so all this effort (and its associated cost) can be eliminated. Yet another advantage of using a single-level store is that persistence and type can be kept orthogonal [Atki87].
Reference: [Ston81] <author> M. Stonebraker, </author> <title> "Operating System Support for Database Management," </title> <journal> CACM, </journal> <volume> 24(7), </volume> <year> 1981. </year>
Reference-contexts: But more importantly, database implementors have repeatedly rejected the idea of using the mapped file facilities offered by operating systems and instead have chosen to manage buffering and disk storage themselves. There are a variety of reasons given why this is so (see <ref> [Ston81, Trai82, Ston84] </ref>).
Reference: [Ston84] <author> M. Stonebraker, </author> <title> "Virtual Memory Transaction Management," </title> <journal> ACM Operating Systems Review, </journal> <volume> 18(2), </volume> <year> 1984. </year>
Reference-contexts: But more importantly, database implementors have repeatedly rejected the idea of using the mapped file facilities offered by operating systems and instead have chosen to manage buffering and disk storage themselves. There are a variety of reasons given why this is so (see <ref> [Ston81, Trai82, Ston84] </ref>).
Reference: [Ston90] <author> M. Stonebraker et al., </author> <title> "The Implementation of POSTGRES," </title> <journal> IEEE Trans. on Data and Knowledge Eng., </journal> <volume> 2(1), </volume> <year> 1990. </year>
Reference-contexts: Out of this research has come a variety of new storage systems that attempt to provide more functionality as well as improved performance for these emerging applications. Examples of such systems include <ref> [Care86, Horn87, Lind87, Moss88, Sche90, Ston90] </ref>. While these storage systems will undoubtedly meet the performance demands of many new applications, our view is that for some applications there is still considerable room for improvement. <p> Implementors of persistent languages have already run up against many of the same problems [Rich89, Schu90]. More recently, a number of new database storage systems have been proposed to address some of these issues, e.g., <ref> [Care86, Horn87, Lind87, Moss88, Sche90, Ston90] </ref>. But our feeling is that for design environments and persistent languages, many of these systems will still fall short of the mark. This is due to the fact that many of them still use a procedure-based interface to access persistent data.
Reference: [Trai82] <author> I. Traiger, </author> <title> "Virtual Memory Management for Database Systems," </title> <journal> ACM Operating Systems Review, </journal> <volume> 16(4), </volume> <year> 1982. </year>
Reference-contexts: But more importantly, database implementors have repeatedly rejected the idea of using the mapped file facilities offered by operating systems and instead have chosen to manage buffering and disk storage themselves. There are a variety of reasons given why this is so (see <ref> [Ston81, Trai82, Ston84] </ref>).
Reference: [Wilk90] <author> K. Wilkinson and M. Neimat, </author> <title> "Maintaining Consistency of Client-Cached Data," </title> <booktitle> Proc. of the 1990 VLDB Conf., </booktitle> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: Although we could use the algorithms described by Li [Li86] to maintain memory coherency across machines, transaction semantics open up the possibility for us to use more efficient algorithms. There has been some work done in this area (see <ref> [Wilk90, Dewi90] </ref>), but not in the context of a single-level store. One of Cricket's designers is actively working on this problem already [Fran90]. 13 Client Task Client Task . . . database Cricket Front-End Workstation Client Task Client Task . . .
Reference: [Youn87] <author> M. Young et al., </author> <title> "The Duality of Memory and Communication in the Implementation of a Multiprocessor Operating System," </title> <booktitle> Proc. of the 11th Symposium on Operating System Principles, </booktitle> <month> Nov. </month> <year> 1987. </year> <month> 21 </month>
Reference-contexts: The remainder of this paper provides a detailed description of Cricket. In the next section, we present the motivation for Cricket, and then in Section 3, we argue why a single-level store is the right approach. This is followed by Section 4, where we review Mach's external pager facilities <ref> [Youn87] </ref>, which play a central role in Cricket's design. Cricket's system architecture is then described in Section 5, and in Section 6, we provide some preliminary performance results that compare Cricket to the EXODUS Storage Manager [Care86]. <p> The above items can be countered by arguing that: 3 g With the right operating system hooks, it is possible to control when the data pages of a mapped file are writ-ten to disk. Mach, for example, provides many of the necessary hooks with its notion of memory objects <ref> [Youn87] </ref>. More will be said about this shortly. g For many emerging database applications, a 32-bit address space is sufficient. Moreover, with the rapid increase in memory sizes and with shared-memory multiprocessors becoming more commonplace, processors with large virtual address spaces may soon become available. <p> EXTERNAL PAGERS IN MACH In the next section, we will describe Cricket's system architecture. Before we can do that, however, we need to briefly go over Mach's external pager interface <ref> [Youn87] </ref>, since it plays a central role in Cricket's design. Among other things, Mach provides a number of facilities that allow user-level tasks (i.e., processes) to exercise control over virtual memory management.
References-found: 44

