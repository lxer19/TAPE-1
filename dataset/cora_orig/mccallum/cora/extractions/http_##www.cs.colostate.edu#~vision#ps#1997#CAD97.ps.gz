URL: http://www.cs.colostate.edu/~vision/ps/1997/CAD97.ps.gz
Refering-URL: http://www.cs.colostate.edu/~vision/html/publications.html
Root-URL: 
Email: ross@cs.colostate.edu  
Title: CAD-based Target Identification in Range, IR and Color Imagery Using On-Line Rendering and Feature Prediction
Author: J. Ross Beveridge Mark R. Stevens J. Ross Beveridge 
Degree: Assistant Professor in the  
Address: Howes Street  Fort Collins, Colorado 80523  
Affiliation: Computer Science Department at Colorado State University  601 South  Colorado State University Computer Science Department  
Note: Affiliation of Authors Dr. J. Ross Beveridge received his B.S. degree in Applied Mechanics and Engineering Science from the University of California at San Diego in 1980 and his M.S. and Ph.D. degrees in Computer Science from the University of Massachusetts in 1987 and 1993 respectively. He has been  since 1993. His present interests include  Contact Information  This work was sponsored by the Defense Advanced Research Projects Agency (DARPA) Image Understanding Program under grants DAAH04-93-G-422 and DAAH04-95-1-0447, monitored by the U. S. Army Research Office, and the National Science Foundation under grants CDA-9422007 and IRI-9503366  
Abstract: Mark R. Stevens received his B.S. degree in Computer Science from the University of Maine at Orono in 1993, and his M.S. in Computer Science from Colorado State University in 1995. He is currently working on his Ph.D. under Dr. Beveridge in the area of computer vision. His present interests include object recognition, multi-sensor visualization, solid-modeling and neural computing. He is a member of IEEE, AAAI and ACM. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. K. Aggarwal. </author> <title> MultiSensor Fusion for Automatic Scene Interpretation. </title> <editor> In Ramesh C. Jain and Anil K. Jain, editors, </editor> <title> Analysis and Interpretation of Range Images, chapter 8. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Typically these CAD systems rely on either the 3D or 2D geometry of the model to constrain the location and appearance of that object in the imagery [25, 19, 20]. With respect to sensor fusion, Aggarwal <ref> [1] </ref> nicely summarizes past work and notes that sensor fusion has tended to emphasize single modality sensors. There is comparatively little work focusing on different sensor modalities. He states that relating data from different modalities is more difficult, in part because of issues of sensor alignment and registration. <p> Projection is possible because both the intrinsic sensor parameters and the pose of the target are known. The gradient under each line is then estimated and converted to an error normalized to the range <ref> [0; 1] </ref>. Lines with weak gradient estimates are omitted. The range fitness error represents how well the predicted 3D sampled surface model points fit the actual range data. The error is based on the average distance from each model point to the corresponding nearest Euclidean neighbor.
Reference: [2] <author> Alexander Akerman and Ronald Patton and Walter Delashmit and Robert Hummel. </author> <title> Target Identification Using Geometric Hashing and FLIR/LADAR fusion. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 595 - 618, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Each ROI is then analyzed and a list of most likely target types and poses is generated. A number of algorithms might fill this role, including perhaps geometric hashing techniques <ref> [2] </ref>. In our work, an existing boundary probing algorithm [10] developed by Alliant Techsystems has been adapted to this hypothesis generation task. The LARS suite uses a non-segmenting model-based approach, which efficiently exploits the 2-D (boundary matching) shape information contained in range signatures.
Reference: [3] <author> F. Arman and J.K. Aggarwal. </author> <title> Cad-based vision: Object recognition in cluttered range images using recognition strategies. </title> <booktitle> Image Understanding, </booktitle> <volume> 58 </volume> <pages> 33-48, </pages> <year> 1993. </year>
Reference-contexts: Most uses of CAD-based recognition focus upon a single sensor. For example, CAD models have been used for matching to 2D imagery [14, 35, 9], 3D range data <ref> [3, 4] </ref>, as well as multispectral imagery such as IR [29] and SAR [11]. Typically these CAD systems rely on either the 3D or 2D geometry of the model to constrain the location and appearance of that object in the imagery [25, 19, 20].
Reference: [4] <author> P. J. Besl and R. C. Jain. </author> <title> Invariant surface characteristics for 3D object recognition from range data. </title> <journal> cvgip, </journal> <volume> 33:33 - 80, </volume> <year> 1986. </year>
Reference-contexts: Most uses of CAD-based recognition focus upon a single sensor. For example, CAD models have been used for matching to 2D imagery [14, 35, 9], 3D range data <ref> [3, 4] </ref>, as well as multispectral imagery such as IR [29] and SAR [11]. Typically these CAD systems rely on either the 3D or 2D geometry of the model to constrain the location and appearance of that object in the imagery [25, 19, 20].
Reference: [5] <author> Paul J. Besl and Ramesh C. Jain. </author> <title> Three-dimensional object recognition. </title> <journal> ACM Computing Surveys, 17(1):75 -145, </journal> <month> March </month> <year> 1985. </year>
Reference-contexts: Our current system employs such constraints within a closed loop rendering system for dynamic feature prediction. Our use of rendering to perform feature prediction within a matching loop exemplifies an approach advocated by Besl <ref> [5] </ref> when he stressed the potential value of graphical rendering to support object verification. Others have recognized the value of graphical rendering in support of object recognition. Wells has used graphics hardware for the computation of model points for use in tracking faces in video 5 sequences [22].
Reference: [6] <author> J. Ross Beveridge. </author> <title> Local Search Algorithms for Geometric Obejct Recognition: Optimal Correspondence and Pose. </title> <type> PhD thesis, </type> <institution> University of Massachuesetts at Amherst, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Omission introduces a bias in favor of accounting for as many model features as possible <ref> [6] </ref>. The fitness error values are summarized below (see [28] for a through discussion). The optical fitness error represents the fidelity of match between the 3D edge features and the underlying image. The process of determining the error begins by projecting the predicted 3D model edges into the optical imagery.
Reference: [7] <author> J. Ross Beveridge, Allen Hanson, and Durga Panda. </author> <title> Integrated color ccd, flir & ladar based object modeling and recognition. </title> <type> Technical report, </type> <institution> Colorado State University and Alliant Techsystems and University of Massachusetts, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: While Aggarwal [26] and others [31, 21, 16, 36] have examples of successful mixed-modality fusion, this is still a young research area. One solution to the problem of imperfectly aligned sensors is to use the CAD model geometry to suggest how image registration needs to be adjusted <ref> [7] </ref>. Our current system employs such constraints within a closed loop rendering system for dynamic feature prediction. Our use of rendering to perform feature prediction within a matching loop exemplifies an approach advocated by Besl [5] when he stressed the potential value of graphical rendering to support object verification.
Reference: [8] <author> J. Ross Beveridge, Durga P. Panda, and Theodore Yachik. </author> <title> November 1993 Fort Carson RSTA Data Collection Final Report. </title> <type> Technical Report CSS-94-118, </type> <institution> Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: To adapt the best features that CAD-based recognition has to offer to the task of identifying vehicles in multisensor data, we have developed a model-based ATR system [33] and tested it against what we consider to be a demanding and realistic dataset <ref> [8] </ref>. The system is based on the conviction that a CAD-based recognition system should utilize 3D models dynamically during recognition to make predictions about what features will be visible. <p> Over 400 range, IR and color images were collected and this imagery has been cleared for unlimited public distribution and Colorado State maintains a data distribution homepage (http://www.cs.colostate.edu/~vision). This homepage also includes a complete data browser for the color imagery. A 50 page report <ref> [8] </ref> describes each image, vehicles present, and ancillary information such as time of day and weather conditions. <p> Table 1 presents a confusion matrix summarizing how well the multisensor identification system performed on the 35 test cases. A detailed case-by-case breakdown is presented in Table 2. The second column indicates the vehicle shot number and vehicle array as identified in the Fort Carson data collection report <ref> [8] </ref>. The third column indicates the true target. The next five columns show the performance of the probing system, with the first four being the number of vehicle types returned out of 15 possible trials run. The fifth column shows the best probing output.
Reference: [9] <author> J. Ross Beveridge and Edward M. Riseman. </author> <title> Optimal Geometric Model Matching Under Full 3D Perspective. </title> <booktitle> In Second CAD-Based Vision Workshop, </booktitle> <pages> pages 54 - 63. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> February </month> <year> 1994. </year> <note> (Submitted to CVGIP-IU). </note>
Reference-contexts: Most uses of CAD-based recognition focus upon a single sensor. For example, CAD models have been used for matching to 2D imagery <ref> [14, 35, 9] </ref>, 3D range data [3, 4], as well as multispectral imagery such as IR [29] and SAR [11]. Typically these CAD systems rely on either the 3D or 2D geometry of the model to constrain the location and appearance of that object in the imagery [25, 19, 20].
Reference: [10] <author> James E. Bevington. </author> <title> Laser Radar ATR Algorithms: Phase III Final Report. </title> <type> Technical report, </type> <institution> Alliant Techsystems, Inc., </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: Each ROI is then analyzed and a list of most likely target types and poses is generated. A number of algorithms might fill this role, including perhaps geometric hashing techniques [2]. In our work, an existing boundary probing algorithm <ref> [10] </ref> developed by Alliant Techsystems has been adapted to this hypothesis generation task. The LARS suite uses a non-segmenting model-based approach, which efficiently exploits the 2-D (boundary matching) shape information contained in range signatures.
Reference: [11] <author> Bir Bhanu and Grinnell Jones and Joon Ahn and Ming Li and June Yi. </author> <title> Recognition of Articulated Objects in SAR Imagery. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 1237-1250, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Most uses of CAD-based recognition focus upon a single sensor. For example, CAD models have been used for matching to 2D imagery [14, 35, 9], 3D range data [3, 4], as well as multispectral imagery such as IR [29] and SAR <ref> [11] </ref>. Typically these CAD systems rely on either the 3D or 2D geometry of the model to constrain the location and appearance of that object in the imagery [25, 19, 20].
Reference: [12] <author> C. E. Brodley and P. E. Utgoff. </author> <title> Goal-directed Classification Using Linear Machine Decision Trees. </title> <journal> Machine Learning, </journal> <note> page (to appear), </note> <year> 1994. </year>
Reference-contexts: We have developed two upstream processes which together provide this cuing information. To provide context, these are briefly summarized. 4.1 Cuing Step 1: Color Detection Targets are first detected using a new machine learning algorithm <ref> [12, 15] </ref> geared towards finding camouflaged targets in multi-spectral (RGB) images.
Reference: [13] <author> Major Tom Burns. </author> <title> Moving and stationary target acquisition (mstar). In Image Understanding Technology Programs, </title> <booktitle> Fort Belvoir, 1996. DARPA. </booktitle> <pages> 21 </pages>
Reference-contexts: 1 Introduction The utility of CAD-based recognition techniques has long been recognized for industrial domains where detailed geometric models are available. It has also shown promise for Automatic Target Recognition <ref> [29, 13] </ref>. However, ATR is a highly challenging object recognition domain: targets typically appear at low resolution, sensor modalities other than visible light are typically most important, and targets are viewed against cluttered backgrounds. So far, many of the common techniques in CAD-based vision have adapted poorly to this domain.
Reference: [14] <author> Jin-Long Chen, George C. Stockman, and Kashi Rao. </author> <title> Recovering and tracking pose of curved 3d objects from 2d images. </title> <booktitle> In Proceedings Computer Vision and Pattern Recognition, </booktitle> <pages> pages 233-239, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Most uses of CAD-based recognition focus upon a single sensor. For example, CAD models have been used for matching to 2D imagery <ref> [14, 35, 9] </ref>, 3D range data [3, 4], as well as multispectral imagery such as IR [29] and SAR [11]. Typically these CAD systems rely on either the 3D or 2D geometry of the model to constrain the location and appearance of that object in the imagery [25, 19, 20].
Reference: [15] <author> B. Draper, C. E. Brodley, and P. Utgoff. </author> <title> Goal-directed Classification Using Linear Machine Decision Trees. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <note> 16(9):(to appear), </note> <month> September </month> <year> 1994. </year>
Reference-contexts: We have developed two upstream processes which together provide this cuing information. To provide context, these are briefly summarized. 4.1 Cuing Step 1: Color Detection Targets are first detected using a new machine learning algorithm <ref> [12, 15] </ref> geared towards finding camouflaged targets in multi-spectral (RGB) images.
Reference: [16] <author> R. O. Eason and R. C. Gonzalez. </author> <title> Least-Squares Fusion of Multisensory Data. </title> <editor> In Mongi A. Abidi and Rafael C. Gonzalez, editors, </editor> <booktitle> Data Fusion in Robotics and Machine Intelligence, chapter 9. </booktitle> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: There is comparatively little work focusing on different sensor modalities. He states that relating data from different modalities is more difficult, in part because of issues of sensor alignment and registration. While Aggarwal [26] and others <ref> [31, 21, 16, 36] </ref> have examples of successful mixed-modality fusion, this is still a young research area. One solution to the problem of imperfectly aligned sensors is to use the CAD model geometry to suggest how image registration needs to be adjusted [7].
Reference: [17] <author> T.L. Gandhi and O.I. </author> <title> Camps. Robust feature selection for object recognition using uncertain 2d image data. </title> <journal> Computer Vision and Pattern Recognition, </journal> <pages> pages 281-287, </pages> <year> 1994. </year>
Reference-contexts: Wells has used graphics hardware for the computation of model points for use in tracking faces in video 5 sequences [22]. Sato has used computer graphics to recover reflectance models of objects spinning on a turntable [30]. Others have used rendering for the generation of imagery for statistical modeling <ref> [17, 39] </ref>. Alternatively, we advocate an on-line prediction capability which performs the mapping from stored model to predicted features dynamically as part of the recognition process [28, 33].
Reference: [18] <author> F. Glover. </author> <title> Tabu search part i. </title> <journal> ORSA Journal on Computing, </journal> <volume> 1(3):190 - 206, </volume> <year> 1989. </year>
Reference-contexts: The initial scaling of the sampling interval is determined automatically, based upon moment analysis applied to the current model and sensor data sets. A variant on local search, called tabu search, is used to escape from some local optima <ref> [18] </ref>. Tabu search keeps a limited history and will explore `uphill' for a short duration to climb out of local optima. In this problem, it turns out that the regeneration of predicted target features changes the error landscape after each move.
Reference: [19] <author> W. Eric L. </author> <title> Grimson. Object Recognition by Computer: The Role of Geometric Constraints. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Typically these CAD systems rely on either the 3D or 2D geometry of the model to constrain the location and appearance of that object in the imagery <ref> [25, 19, 20] </ref>. With respect to sensor fusion, Aggarwal [1] nicely summarizes past work and notes that sensor fusion has tended to emphasize single modality sensors. There is comparatively little work focusing on different sensor modalities.
Reference: [20] <author> Daniel P. Huttenlocher and Shimon Ullman. </author> <title> Object Recognition Using Alignment. </title> <booktitle> In Proc. First International Conference on Computer Vision, </booktitle> <pages> pages 102-111, </pages> <address> London, England, June 1987. </address> <publisher> Computer Society Press of the IEEE. </publisher>
Reference-contexts: Typically these CAD systems rely on either the 3D or 2D geometry of the model to constrain the location and appearance of that object in the imagery <ref> [25, 19, 20] </ref>. With respect to sensor fusion, Aggarwal [1] nicely summarizes past work and notes that sensor fusion has tended to emphasize single modality sensors. There is comparatively little work focusing on different sensor modalities.
Reference: [21] <author> Alexander Akerman III, Ronald Patton, Walter H. Delashmit, and Robert Hummel. </author> <title> Multisensor fusion using FLIR and LADAR identification. </title> <type> Technical Report NRC-TR-94-052, </type> <institution> Nichols Research Corporation, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: There is comparatively little work focusing on different sensor modalities. He states that relating data from different modalities is more difficult, in part because of issues of sensor alignment and registration. While Aggarwal [26] and others <ref> [31, 21, 16, 36] </ref> have examples of successful mixed-modality fusion, this is still a young research area. One solution to the problem of imperfectly aligned sensors is to use the CAD model geometry to suggest how image registration needs to be adjusted [7].
Reference: [22] <author> William Wells III, Michael Halle, Ron Kikinis, and Paul Viola. </author> <title> Alignment and tracking using graphics hardware. </title> <booktitle> In Image Understanding Workshop, </booktitle> <pages> pages 837-842. DARPA, </pages> <year> 1996. </year>
Reference-contexts: Others have recognized the value of graphical rendering in support of object recognition. Wells has used graphics hardware for the computation of model points for use in tracking faces in video 5 sequences <ref> [22] </ref>. Sato has used computer graphics to recover reflectance models of objects spinning on a turntable [30]. Others have used rendering for the generation of imagery for statistical modeling [17, 39].
Reference: [23] <author> J. Ross Beveridge and Mark R. Stevens and Zhongfei Zhang and Mike Goss. </author> <title> Approximate Image Mappings Between Nearly Boresight Aligned Optical and Range Sensors. </title> <type> Technical Report CS-96-112, </type> <institution> Computer Science, Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Put simply, they are likely to be positioned quite close to each other and to be looking in the same direction. We have prepared a detailed study of different sources of uncertainty in sensor-to-sensor alignment for near-boresight-aligned sensors <ref> [23] </ref>. A useful heuristic from this study is that over small rotations and restricted depth ranges, sensor-to-sensor rotation may be approximated with simpler co-planar translation. Together the sensors are free to rotate and translate relative to the object, but are constrained to permit only translation in a common image plane. <p> This homepage also includes a complete data browser for the color imagery. A 50 page report [8] describes each image, vehicles present, and ancillary information such as time of day and weather conditions. Additional information on the sensor calibration may be found in <ref> [23] </ref>. 6.2 How Difficult is the Fort Carson Dataset? The Fort Carson dataset was designed to contain challenging target identification problems requiring advancements to the state-of-the-art in ATR. We believe this goal has been met.
Reference: [24] <author> Jacques G. Verly and Richard T. Lacoss. </author> <title> Automatic Target Recognition for LADAR imagery Using Functional Templates Derived From 3-D CAD Models. In Reconnaissance, Surveilance, and Target Acquisition (RSTA) for the Unmanned Ground Vehicle. </title> <note> Morgan Kaufmann (to appear), </note> <year> 1997. </year>
Reference-contexts: We believe this goal has been met. To our knowledge, only one other organization has carried out target identification on this data, and that is the group from MIT Lincoln Laboratory. The Fort Carson dataset has been used in part of the evaluation of their own range-only ATR system <ref> [24] </ref>. The MIT group has also developed a set of correct-recognition performance curves that allow them to predict the best performance they can expect to achieve for given operating parameters (range, depression angle, noise, etc).
Reference: [25] <author> David G. Lowe. </author> <title> Three-dimensional Object Recognition from Single Two-dimensional Images. </title> <journal> Artificial Intelligence, </journal> <volume> 31, </volume> <year> 1987. </year>
Reference-contexts: Typically these CAD systems rely on either the 3D or 2D geometry of the model to constrain the location and appearance of that object in the imagery <ref> [25, 19, 20] </ref>. With respect to sensor fusion, Aggarwal [1] nicely summarizes past work and notes that sensor fusion has tended to emphasize single modality sensors. There is comparatively little work focusing on different sensor modalities.
Reference: [26] <author> M. J. Magee, B. A. Boyter, C. H. Chien, and J. K. Aggarwal. </author> <title> Experiments in Intensity Guided Range Sensing Recognition of Three-Dimensional Objects. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(6):629 - 637, </volume> <month> November </month> <year> 1985. </year>
Reference-contexts: There is comparatively little work focusing on different sensor modalities. He states that relating data from different modalities is more difficult, in part because of issues of sensor alignment and registration. While Aggarwal <ref> [26] </ref> and others [31, 21, 16, 36] have examples of successful mixed-modality fusion, this is still a young research area. One solution to the problem of imperfectly aligned sensors is to use the CAD model geometry to suggest how image registration needs to be adjusted [7].
Reference: [27] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Interleaving 3D Model Feature Prediction and Matching to Support Multi-Sensor Object Recognition. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 699-706, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: We have already developed algorithms to convert these models to a level of detail more appropriate for matching to the given sensor data [34, 32]. Another system, summarized here and fully described in <ref> [27] </ref>, has been developed to extract edge and surface information from these models. The feature prediction algorithm renders the vehicle using the current pose and lighting estimates to infer which 3D components of the target will generate detectable features in the specific scene.
Reference: [28] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Precise Matching of 3-D Target Models to Multisensor Data. </title> <journal> IEEE Transactions on Image Processing, </journal> <note> page (to appear), </note> <month> January </month> <year> 1997. </year>
Reference-contexts: One such constraint is the sun angle, which is needed to predict what aspects of an objects internal structure will stand out in visible light imagery. In a prior paper <ref> [28] </ref>, we described a system which uses the steps just described to precisely recover the pose of a known 3D object given near-boresight-aligned range, IR and color imagery. <p> Others have used rendering for the generation of imagery for statistical modeling [17, 39]. Alternatively, we advocate an on-line prediction capability which performs the mapping from stored model to predicted features dynamically as part of the recognition process <ref> [28, 33] </ref>. A key to making this approach feasible is the development of algorithms which run many, if not all, computations in parallel on standard graphics acceleration hardware. <p> This section will first review the overall approach as presented in <ref> [28] </ref>. <p> Omission introduces a bias in favor of accounting for as many model features as possible [6]. The fitness error values are summarized below (see <ref> [28] </ref> for a through discussion). The optical fitness error represents the fidelity of match between the 3D edge features and the underlying image. The process of determining the error begins by projecting the predicted 3D model edges into the optical imagery.
Reference: [29] <author> N. Nandhakumar and J. K. Aggarwal. </author> <title> Integrated analysis of thermal and visual images for scene interpretation. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 10(4) </volume> <pages> 469-481, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: 1 Introduction The utility of CAD-based recognition techniques has long been recognized for industrial domains where detailed geometric models are available. It has also shown promise for Automatic Target Recognition <ref> [29, 13] </ref>. However, ATR is a highly challenging object recognition domain: targets typically appear at low resolution, sensor modalities other than visible light are typically most important, and targets are viewed against cluttered backgrounds. So far, many of the common techniques in CAD-based vision have adapted poorly to this domain. <p> Most uses of CAD-based recognition focus upon a single sensor. For example, CAD models have been used for matching to 2D imagery [14, 35, 9], 3D range data [3, 4], as well as multispectral imagery such as IR <ref> [29] </ref> and SAR [11]. Typically these CAD systems rely on either the 3D or 2D geometry of the model to constrain the location and appearance of that object in the imagery [25, 19, 20].
Reference: [30] <author> Yoichi Sato and Katsushi Ikeuchi. </author> <title> Refectance analysis for 3d computer graphics model generation. </title> <type> Technical Report CMU-CS-95-146, </type> <institution> Carnegie Mellon University, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: Others have recognized the value of graphical rendering in support of object recognition. Wells has used graphics hardware for the computation of model points for use in tracking faces in video 5 sequences [22]. Sato has used computer graphics to recover reflectance models of objects spinning on a turntable <ref> [30] </ref>. Others have used rendering for the generation of imagery for statistical modeling [17, 39]. Alternatively, we advocate an on-line prediction capability which performs the mapping from stored model to predicted features dynamically as part of the recognition process [28, 33].
Reference: [31] <author> A. Stentz and Y. </author> <title> Goto. </title> <booktitle> The CMU Navigational Architecture. In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 440-446, </pages> <address> Los Angeles, CA, </address> <month> February </month> <year> 1987. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: There is comparatively little work focusing on different sensor modalities. He states that relating data from different modalities is more difficult, in part because of issues of sensor alignment and registration. While Aggarwal [26] and others <ref> [31, 21, 16, 36] </ref> have examples of successful mixed-modality fusion, this is still a young research area. One solution to the problem of imperfectly aligned sensors is to use the CAD model geometry to suggest how image registration needs to be adjusted [7].
Reference: [32] <author> Mark R. Stevens. </author> <title> Obtaining 3D Shilhouettes and Sampled Surfaces from Solid Models for use in Computer Vision. </title> <type> Master's thesis, </type> <institution> Colorado State Univeristy, Fort Collins, Colorado, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: We have already developed algorithms to convert these models to a level of detail more appropriate for matching to the given sensor data <ref> [34, 32] </ref>. Another system, summarized here and fully described in [27], has been developed to extract edge and surface information from these models.
Reference: [33] <author> Mark R. Stevens and J. Ross Beveridge. </author> <title> Interleaving 3d model feature prediction and matching to support multi-sensor object recognition. </title> <booktitle> In International Conference on Pattern Recognition, volume 13, </booktitle> <address> Austria, </address> <month> August </month> <year> 1996. </year> <journal> Internation Association of Pattern Recognition. </journal>
Reference-contexts: So far, many of the common techniques in CAD-based vision have adapted poorly to this domain. To adapt the best features that CAD-based recognition has to offer to the task of identifying vehicles in multisensor data, we have developed a model-based ATR system <ref> [33] </ref> and tested it against what we consider to be a demanding and realistic dataset [8]. The system is based on the conviction that a CAD-based recognition system should utilize 3D models dynamically during recognition to make predictions about what features will be visible. <p> Others have used rendering for the generation of imagery for statistical modeling [17, 39]. Alternatively, we advocate an on-line prediction capability which performs the mapping from stored model to predicted features dynamically as part of the recognition process <ref> [28, 33] </ref>. A key to making this approach feasible is the development of algorithms which run many, if not all, computations in parallel on standard graphics acceleration hardware.
Reference: [34] <author> Mark R. Stevens, J. Ross Beveridge, and Michael E. Goss. </author> <title> Reduction of BRL/CAD Models and Their Use in Automatic Target Recognition Algorithms. </title> <booktitle> In Proceedings: BRL-CAD Symposium. </booktitle> <institution> Army Research Labs, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: We have already developed algorithms to convert these models to a level of detail more appropriate for matching to the given sensor data <ref> [34, 32] </ref>. Another system, summarized here and fully described in [27], has been developed to extract edge and surface information from these models.
Reference: [35] <author> G.D Sullivan, A.D. Worrall, and J.M Ferryman. </author> <title> Visual Object Recognition Using Deformable Models of Vehicles. </title> <booktitle> In Workshop on Context-Based Vision, </booktitle> <pages> pages 75-86, </pages> <month> june </month> <year> 1995. </year>
Reference-contexts: Most uses of CAD-based recognition focus upon a single sensor. For example, CAD models have been used for matching to 2D imagery <ref> [14, 35, 9] </ref>, 3D range data [3, 4], as well as multispectral imagery such as IR [29] and SAR [11]. Typically these CAD systems rely on either the 3D or 2D geometry of the model to constrain the location and appearance of that object in the imagery [25, 19, 20].
Reference: [36] <author> C.W. Tong, S.K. Rodgers, J.P. Mills, </author> <title> and M.K. Kabrinsky. Multisensor data fusion of laser radar and forward looking infared for target segmentation and enhancement. </title> <editor> In R.G. Buser and F.B. Warren, editors, </editor> <title> Infared Sensors and Sensor Fusion. </title> <booktitle> SPIE, </booktitle> <year> 1987. </year> <month> 22 </month>
Reference-contexts: There is comparatively little work focusing on different sensor modalities. He states that relating data from different modalities is more difficult, in part because of issues of sensor alignment and registration. While Aggarwal [26] and others <ref> [31, 21, 16, 36] </ref> have examples of successful mixed-modality fusion, this is still a young research area. One solution to the problem of imperfectly aligned sensors is to use the CAD model geometry to suggest how image registration needs to be adjusted [7].
Reference: [37] <author> U. S. </author> <note> Army Ballistic Research Laboratory. BRL-CAD User's Manual, release 4.0 edition, </note> <month> December </month> <year> 1991. </year>
Reference-contexts: The three key elements in this process are: feature prediction, match evaluation, and local search. Each of these elements is described below. 5.1.1 On-line Model Feature Prediction Highly detailed Constructive Solid Geometry (CSG) models of target vehicles are available in BRL-CAD format <ref> [37] </ref>. We have already developed algorithms to convert these models to a level of detail more appropriate for matching to the given sensor data [34, 32]. Another system, summarized here and fully described in [27], has been developed to extract edge and surface information from these models.
Reference: [38] <author> Jacques G. Verly, Dan E. Dudgeon, and Richard T. Lacoss. </author> <title> Model-Based Automatic Target Recognition System for the UGV/RSTA Ladar: Status at Demo C. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 549-583. ARPA, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: From the study we have learned that the system is performing identification quite well, 77% correct, across the complete test set. Moreover, based on the conclusions of MIT Lincoln Laboratory <ref> [38] </ref>, this level of performance is unlikely to be reached using only traditional ATR image-space representations (templates). The paper is broken down into six distinct sections. Section 2 contains a literature review describing the works of other researchers as they pertain to our work.
Reference: [39] <author> Mark D. Wheeler and Katsushi Ikeuchi. </author> <title> Sensor modeling, probabilistic hypothesis generation, and robust localization for object recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(3) </volume> <pages> 252-265, </pages> <year> 1995. </year> <month> 23 </month>
Reference-contexts: Wells has used graphics hardware for the computation of model points for use in tracking faces in video 5 sequences [22]. Sato has used computer graphics to recover reflectance models of objects spinning on a turntable [30]. Others have used rendering for the generation of imagery for statistical modeling <ref> [17, 39] </ref>. Alternatively, we advocate an on-line prediction capability which performs the mapping from stored model to predicted features dynamically as part of the recognition process [28, 33].
References-found: 39

