URL: ftp://ftp.mcs.anl.gov/pub/tech_reports/ip/mcenter.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/otc/InteriorPoint/abstracts/Gondzio-2.html
Root-URL: http://www.mcs.anl.gov
Email: e-mail: gondzio@divsun.unige.ch  
Title: Multiple Centrality Corrections in a Primal-Dual Method for Linear Programming  
Author: Jacek Gondzio 
Keyword: Key words. Centrality correctors, higher order method, linear programming, in terior point methods.  
Note: Supported by the Fonds National de la Recherche Scientifique Suisse, grant #12-34002.92. on leave from the Systems Research  
Address: 102 Bd Carl Vogt, CH-1211 Geneva 4, Switzerland  Newelska 6, 01-447 Warsaw, Poland  
Affiliation: Logilab, HEC Geneva, Section of Management Studies, University of Geneva,  Institute, Polish Academy of Sciences,  
Abstract: Technical Report 1994.20 November 11, 1994, revised May 6, 1995 Abstract A modification of (infeasible) primal-dual interior point method is developed. The method uses multiple corrections to improve the centrality of the current iterate. The maximum number of corrections the algorithm is encouraged to make depends on the ratio of the efforts to solve and to factorize the KKT systems. For any LP problem it is determined right after preprocessing KKT system and prior to optimization process. The harder the factorization, the more advantageous higher order corrections might prove. Computational performance of the method is studied on more difficult Netlib problems as well as on tougher and larger real-life LP models arising from applications. The use of multiple centrality corrections gives in the average 25% to 40% reduction in the number of iterations compared with widely used second order predictor-corrector method. This translates into 20% to 30% savings in CPU time. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Altman, A. and Gondzio, J. </author> <title> (1993) An Efficient Implementation of a Higher Order Primal-Dual Interior Point Method for Large Sparse Linear Programs, </title> <booktitle> Archives of Control Sciences 2 1/2, </booktitle> <pages> 23-40. </pages>
Reference-contexts: Additionally, to save on useless corrections, the process is prematurely terminated if it does not sufficiently improve the quality of the new trial point. The multiple centrality correction technique has been incorporated into our experimental HOPDM 2.0 code (Higher Order Primal Dual Method, version 2.0) <ref> [1, 7] </ref>. Its use results in a significant reduction in the number of iterations compared with the widely used predictor-corrector methods. This translates into 20% to 30% savings in CPU time on the most difficult problems of our test set. The paper is organized as follows.
Reference: [2] <author> Bixby, R. E. </author> <year> (1994). </year> <title> Progress in Linear Programming, </title> <journal> ORSA Journal on Computing 6, </journal> <pages> 15-22. </pages>
Reference-contexts: To give the reader some idea on the efficiency of our HOPDM 2.1 primal-dual implementation, Table 1 includes results of running CPLEX 3.0 simplex (SM) and predictor-corrector primal-dual barrier (PD) codes <ref> [2] </ref>. Let us observe that in terms of iterations to reach optimality our predictor-corrector code compares 1 to get the problems, send a request to the author: gondzio@divsun.unige.ch 14 favourably with CPLEX barrier.
Reference: [3] <author> Domich P.D., P.T. Boggs, J.E. Rogers and Ch. </author> <title> Witzgall (1991). Optimizing over three-dimensional subspaces in an interior-point method for linear programming, </title> <booktitle> Linear Algebra and its Applications 152, </booktitle> <pages> pp. 315-342 </pages>
Reference-contexts: This method builds a higher order Taylor approximation of the (infeasible) central trajectory and pushes an iterate towards an optimum along such an approximation. The paper [21] gives a rigorous presentation of this approach and rich computational results as well. Another approach, due to Domich, Boggs, Rogers and Witzgall <ref> [3] </ref> used three independent directions and solved an auxilliary linear program in a three dimensional subspace to find a search direction.
Reference: [4] <author> Carpenter T.J., I. J. Lustig,, J. M. Mulvey, and D. F. </author> <title> Shanno (1993) Higher Order Predictor-corrector Interior Point Methods with Application to Quadratic Programming, </title> <journal> SIAM Journal on Optimization 3, </journal> <pages> pp. 696-725. </pages>
Reference-contexts: The discouraging results on the use of the higher order predictor-corrector method were later confirmed in <ref> [4] </ref>. We suppose that there are at least two reasons for that. The first is a consequence of the sometimes complicated shape of the central trajectory and of large errors of its approximation 2 based on local information available at a current point. <p> We want to stress here that the primary aim of corrections is improving centrality, and it is not getting reduction in the complementarity gap as suggested in Section 6 of [18] and in <ref> [4] </ref>. Of course, if there are large outliers in the complementarity products (larger than fi max ), then the corrector term is supposed to reduce them; consequently the complementarity gap may also decrease.
Reference: [5] <author> Duff, </author> <title> I.S., Erisman, A.M. and Reid J.K. (1989) Direct Methods for Sparse Matrices, </title> <publisher> Oxford University Press, </publisher> <address> New York. </address> <month> 16 </month>
Reference-contexts: to the normal equations system (AD 2 A T )y = AD 2 r + h; (8) where D 2 = (X 1 Z + S 1 W ) 1 ; h = b : Computing (x; y) from (7) or y from (8) is usually (when a direct approach <ref> [5] </ref> is applied) divided into two phases: factorization of the matrix to some easily invertible form and the following solve that exploits this factorization.
Reference: [6] <author> Gay D.M. </author> <title> (1985) Electronic mail distribution of linear programming test problems, </title> <journal> Mathematical Programming Society COAL Newsletter 13, </journal> <pages> pp. 10-12. </pages>
Reference-contexts: Unfortunately, some of them are proprietary. To avoid an unacceptable practice 13 of publishing (unverifiable) results obtained on problems that are not available to the whole LP community, we limited ourselves to reporting them only for problems from the Netlib collection <ref> [6] </ref> and some large scale industry models that we are allowed (or that we hope soon to be allowed) to distribute 1 . 4.2 Numerical results We start from a demonstration of the advantages of the use of multiple centrality correctors on two large Netlib problems: dense, PILOT87 and sparse, STOCFOR3 <p> Note that the heuristic of Section 3.3 proved very successful in the choice of the "optimal" order of corrections appropriate for a given problem. In the following experiment, we run HOPDM 2.1 on problems from the Netlib collection <ref> [6] </ref>. Centrality correctors were allowed only in the case of 31 problems (of the whole 90 LP tests). Table 2 summarizes our experience for 39 larger Netlib problems for which the ratio r f=s exceeds 5:0.
Reference: [7] <author> Gondzio, J. </author> <title> (1994) Presolve Analysis of Linear Programs Prior to Applying the Interior Point Method, </title> <type> Technical Report 1994.3, </type> <institution> Department of Management Studies, University of Geneva, Switzerland. </institution>
Reference-contexts: Additionally, to save on useless corrections, the process is prematurely terminated if it does not sufficiently improve the quality of the new trial point. The multiple centrality correction technique has been incorporated into our experimental HOPDM 2.0 code (Higher Order Primal Dual Method, version 2.0) <ref> [1, 7] </ref>. Its use results in a significant reduction in the number of iterations compared with the widely used predictor-corrector methods. This translates into 20% to 30% savings in CPU time on the most difficult problems of our test set. The paper is organized as follows. <p> We run the method on a collection of about 200 LP problems (including Netlib set of 90 problems) for different numbers of corrections and we compared the iteration numbers with those obtained by the predictor-corrector version 2.0 of HOPDM code <ref> [7] </ref>. The results have shown that an addition of one centrality corrector term to a predictor-corrector direction reduces the number of iterations from 10% to 30%. <p> The estimates (23) correspond to the choice made in our implementation. 4 Computational experience The method of multiple centrality corrections has been incorporated into our experimental primal-dual code HOPDM (its version 2.0 <ref> [7] </ref> used Mehrotra's predictor-corrector technique while the newest version 2.1 applies multiple correctors). HOPDM is written in FORTRAN hence it is easily portable to any platform. The results reported here were obtained by running the code on an IBM Power PC workstation (type 7011, model 25T). <p> These savings vary for different problems but they reach, in the average, 10% to 30% and show a tendency to increase for more difficult problems. In the last experiment, we used more difficult, large scale industry problems. Table 3 gives their statistics in original form and after presolve reductions <ref> [7] </ref>, the ratios of factorization and solve efforts, r f=s and the maximum number of correctors chosen automatically by the heuristic of Section 3.3, K. Problems BL through UK are MARKAL economical models for different countries [25, 28, 16, 24].
Reference: [8] <author> Gondzio J. and T. </author> <title> Terlaky (1995) A computational view of interior point methods for linear programming, in: Advances in Linear and Integer Programming, </title> <editor> J. Beasley ed., </editor> <publisher> Oxford University Press, Oxford, </publisher> <address> England, </address> <note> 1995 (to appear). </note>
Reference-contexts: 1 Introduction It is now widely accepted [18] that an infeasible primal-dual interior point method is a powerfull tool to solve very large linear programs. Most issues of its efficient implementation seem now to be deeply understood (see, e.g., the survey <ref> [8] </ref>). All modern codes based on primal-dual method proceed in a more or less similar way: at every iteration they apply some direct method to factorize the Karush-Kuhn-Tucker (KKT) equations and later solve them twice for predictor and corrector terms in the Newton step [20]. <p> The first theoretical results for the primal-dual algorithm come from Megiddo [19] and Kojima, Mizuno and Yoshise [15]. Descriptions of its efficient implementations can be found in [17, 18, 20, 21] and in the survey <ref> [8] </ref>. 3 2.1 Fundamentals of the primal-dual method Let us consider a primal linear programming problem minimize c T x; subject to Ax = b; (1) x + s = u; where c; x; s; u 2 R n ; b 2 R m ; A 2 R mfln and its <p> The KKT systems are in the HOPDM code reduced to the normal equations (8). Hence, after neglecting the operations of multiplying the LP constraint matrix, or its transpose, with a vector and that of building AD 2 A T , we use the following measures <ref> [8] </ref> for factorization and solve efforts E f = i=1 i and E s = 2 fi i=1 where l i is the number of off-diagonal nonzero entries in column i of the Cholesky matrix and m; n are the LP problem sizes. <p> The results reported here were obtained by running the code on an IBM Power PC workstation (type 7011, model 25T). The program was compiled with options -O -qarch=ppc. All problems are solved to an 8-digit accurate optimum (see e.g., <ref> [8] </ref>, Section 3.6). All solution times are in seconds. They include the whole processing time except problem input and solution output. 4.1 Test problems As stated before, we run the multiple centrality correction approach on a large set of about 200 LP models. Unfortunately, some of them are proprietary.
Reference: [9] <author> Gonzaga, C. C. </author> <title> (1992) Path-Following Methods for Linear Programming, </title> <journal> SIAM Review 34, </journal> <pages> 167-224. </pages>
Reference-contexts: The predictor term is responsible for "optimization" (reducing the primal and dual infeasibil-ities and the duality gap). The corrector term keeps current iterate away from the boundary of feasible region (ideally, would be to keep it close to central path <ref> [9] </ref>) and gives more chance for a long step to be made in the next iteration. The factorization of KKT equations involves almost always significantly more computational effort than the following solves for the predictor and corrector direction terms.
Reference: [10] <author> Gonzaga, C. C. </author> <title> (1994) Private communication, </title> <month> August, </month> <year> 1994. </year>
Reference-contexts: This approach required feasibility and was tested only in the context of MATLAB implementation on small and dense problems. Let us also mention that Hung and Ye [11] studied theoretically higher order predictor-corrector techniques incorporated in a homogenous self-dual algorithm and Gonzaga <ref> [10] </ref> anal-ysed their use in a primal-dual method. The most widely used higher order method is the second order variant of Mehrotra's predictor-corrector.
Reference: [11] <author> Hung, P.-F. and Ye, Y. </author> <title> (1994) An Asymptotical O( p nL)-Iteration Path-Following Linear Programming Algorithm That Uses Long Steps, </title> <type> Technical Report, </type> <institution> Department of Mathematics, University of Iowa, Iowa City, USA, </institution> <month> March </month> <year> 1994, </year> <note> to appear in SIAM Journal on Optimization. </note>
Reference-contexts: This was later followed by one (or more) centering steps to drive the next iterate sufficiently close to the central path. This approach required feasibility and was tested only in the context of MATLAB implementation on small and dense problems. Let us also mention that Hung and Ye <ref> [11] </ref> studied theoretically higher order predictor-corrector techniques incorporated in a homogenous self-dual algorithm and Gonzaga [10] anal-ysed their use in a primal-dual method. The most widely used higher order method is the second order variant of Mehrotra's predictor-corrector.
Reference: [12] <author> Jansen, B., Roos, C., Terlaky, T. and Vial, J.-P. </author> <year> (1993), </year> <title> Primal-Dual Target Following Algorithms for Linear Programming, </title> <type> Technical Report 93-107, </type> <institution> Faculty of Technical Mathematics and Infor-matics, Technical University Delft, Delft, The Netherlands, </institution> <note> to appear in the special issue of Annals of Operation Research, </note> <author> K. Anstreicher and R. </author> <type> Freund (eds.). </type>
Reference-contexts: Note that we shall cross, in general, the boundary of the feasible region. Next, we shall define a corrector direction that drives from this trial point towards some better centered target <ref> [12] </ref>. An important issue of the approach is that the target is not an (usually unachievable) analytic center but it is some point in a large neighbourhood of the central path that, as we hope, can be easier to reach. <p> There exists a theoretical justification that large discrepancy between complementarity products causes an iterate to be far from the region where the Newton method for centering converges fast. This in practice manifests in small steps allowed in the primal and dual spaces. Jansen, Roos, Terlaky and Vial <ref> [12] </ref> suggest a remedy to this: they define a sequence of traceable weighted analytic centers, targets. Such a sequence goes from an arbitrary interior point to a point close to the central path. The algorithm follows these targets and continuously improves the centrality of subsequent iterates.
Reference: [13] <author> Karmarkar, N. K. </author> <title> (1984) A New Polynomial-Time Algorithm for Linear Programming, </title> <booktitle> Combina-torica 4, </booktitle> <pages> 373-395. </pages>
Reference: [14] <author> Karmarkar, N. K., Lagarias, J. C., Slutsman, L. and Wang, P. </author> <title> (1989) Power Series Variants of Karmarkar-Type Algorithms, </title> <journal> AT&T Technical Journal, </journal> <volume> 68, </volume> <pages> 20-36. </pages>
Reference-contexts: This gave rise to introducing higher order terms when computing directions. The first such approach was proposed by Karmarkar, Lagarias, Slutsman and Wang <ref> [14] </ref> who constructed parametrized representation of the (feasible) trajectory motivated from the use of differential equations. A successfull technique that incorporates higher order information in the primal-dual algorithm comes from Mehrotra [20, 21].
Reference: [15] <author> Kojima, M., Mizuno, S. and Yoshise, A. </author> <title> (1989) A Primal-Dual Interior Point Algorithm for Linear Programming, </title> <editor> in N. Megiddo, ed., </editor> <title> Progress in Mathematical Programming: Interior Point and Related Methods, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <pages> 29-48. </pages>
Reference-contexts: In Section 4 we present computational results and, finally, in Section 5 we give our conclusions. 2 Primal-dual algorithm In this section we very briefly discuss an (infeasible) primal-dual method. The first theoretical results for the primal-dual algorithm come from Megiddo [19] and Kojima, Mizuno and Yoshise <ref> [15] </ref>.
Reference: [16] <author> Loulou R. </author> <year> (1994). </year> <title> Cost minimization of Quebec's and Ontario's Energy System in the Long Term, </title> <type> Technical Report, </type> <institution> GERAD, McGill University, Montreal, Canada, </institution> <year> 1994. </year>
Reference-contexts: Table 3 gives their statistics in original form and after presolve reductions [7], the ratios of factorization and solve efforts, r f=s and the maximum number of correctors chosen automatically by the heuristic of Section 3.3, K. Problems BL through UK are MARKAL economical models for different countries <ref> [25, 28, 16, 24] </ref>. The remaining problems come from the Kennington's collection available via Netlib. Table 4 reports results of solving them with the HOPDM 2.1 code.
Reference: [17] <author> Lustig I.J., Marsten, R.E. and Shanno, D.F. </author> <title> (1992) On Implementing Mehrotra's Predictor-Corrector Interior Point Method for Linear Programming, </title> <journal> SIAM Journal on Optimization 2, </journal> <pages> 435-449. </pages>
Reference-contexts: The first theoretical results for the primal-dual algorithm come from Megiddo [19] and Kojima, Mizuno and Yoshise [15]. Descriptions of its efficient implementations can be found in <ref> [17, 18, 20, 21] </ref> and in the survey [8]. 3 2.1 Fundamentals of the primal-dual method Let us consider a primal linear programming problem minimize c T x; subject to Ax = b; (1) x + s = u; where c; x; s; u 2 R n ; b 2 R <p> The use of higher (than two) order terms in the direction further reduces the number of iterations; however it does not, in general, translate into computational time savings. The second order method became thus the computational state-of-the-art <ref> [17, 18] </ref> for the following couple of years. This variant of the predictor-corrector method used to be implemented in a simplified way that abused rigorous mathematics but it proved particularly efficient in computations. Our presentation of it will follow the implementational practice.
Reference: [18] <author> Lustig, I. J., Marsten, R. E. and Shanno, D. F. </author> <title> (1994) Interior Point Methods for Linear Programming: Computational State of the Art, </title> <journal> ORSA Journal on Computing 6, </journal> <pages> 1-14. </pages>
Reference-contexts: 1 Introduction It is now widely accepted <ref> [18] </ref> that an infeasible primal-dual interior point method is a powerfull tool to solve very large linear programs. Most issues of its efficient implementation seem now to be deeply understood (see, e.g., the survey [8]). <p> The first theoretical results for the primal-dual algorithm come from Megiddo [19] and Kojima, Mizuno and Yoshise [15]. Descriptions of its efficient implementations can be found in <ref> [17, 18, 20, 21] </ref> and in the survey [8]. 3 2.1 Fundamentals of the primal-dual method Let us consider a primal linear programming problem minimize c T x; subject to Ax = b; (1) x + s = u; where c; x; s; u 2 R n ; b 2 R <p> The use of higher (than two) order terms in the direction further reduces the number of iterations; however it does not, in general, translate into computational time savings. The second order method became thus the computational state-of-the-art <ref> [17, 18] </ref> for the following couple of years. This variant of the predictor-corrector method used to be implemented in a simplified way that abused rigorous mathematics but it proved particularly efficient in computations. Our presentation of it will follow the implementational practice. <p> We want to stress here that the primary aim of corrections is improving centrality, and it is not getting reduction in the complementarity gap as suggested in Section 6 of <ref> [18] </ref> and in [4]. Of course, if there are large outliers in the complementarity products (larger than fi max ), then the corrector term is supposed to reduce them; consequently the complementarity gap may also decrease.
Reference: [19] <author> Megiddo, N. </author> <title> (1989) Pathways to the Optimal Set in Linear Programming, </title> <editor> in N. Megiddo, ed., </editor> <title> Progress in Mathematical Programming: Interior Point and Related Methods, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <pages> 131-158. </pages>
Reference-contexts: In Section 3 we discuss multiple centrality corrections. In Section 4 we present computational results and, finally, in Section 5 we give our conclusions. 2 Primal-dual algorithm In this section we very briefly discuss an (infeasible) primal-dual method. The first theoretical results for the primal-dual algorithm come from Megiddo <ref> [19] </ref> and Kojima, Mizuno and Yoshise [15].
Reference: [20] <author> Mehrotra, S. </author> <title> (1992) On the Implementation of a Primal-Dual Interior Point Method, </title> <journal> SIAM Journal on Optimization 2, </journal> <pages> 575-601. </pages>
Reference-contexts: All modern codes based on primal-dual method proceed in a more or less similar way: at every iteration they apply some direct method to factorize the Karush-Kuhn-Tucker (KKT) equations and later solve them twice for predictor and corrector terms in the Newton step <ref> [20] </ref>. The predictor term is responsible for "optimization" (reducing the primal and dual infeasibil-ities and the duality gap). <p> The first such approach was proposed by Karmarkar, Lagarias, Slutsman and Wang [14] who constructed parametrized representation of the (feasible) trajectory motivated from the use of differential equations. A successfull technique that incorporates higher order information in the primal-dual algorithm comes from Mehrotra <ref> [20, 21] </ref>. This method builds a higher order Taylor approximation of the (infeasible) central trajectory and pushes an iterate towards an optimum along such an approximation. The paper [21] gives a rigorous presentation of this approach and rich computational results as well. <p> The first theoretical results for the primal-dual algorithm come from Megiddo [19] and Kojima, Mizuno and Yoshise [15]. Descriptions of its efficient implementations can be found in <ref> [17, 18, 20, 21] </ref> and in the survey [8]. 3 2.1 Fundamentals of the primal-dual method Let us consider a primal linear programming problem minimize c T x; subject to Ax = b; (1) x + s = u; where c; x; s; u 2 R n ; b 2 R <p> It is thus natural to look for the possibility of reducing their number to the necessary minimum, even at the expense of some increase of a cost of a single iteration. Mehrotra's predictor-corrector technique <ref> [20, 21] </ref> incorporates higher order information when approximating central trajectory and computing direction step. Formally, this method looks for for a parametric representation of the trajectory that guides from a given (infeasible) point in primal and dual spaces to a solution of (1)-(2).
Reference: [21] <author> Mehrotra, S. </author> <title> (1991) Higher Order Methods and their Performance, </title> <type> Technical Report 90-16R1, </type> <institution> Department of Industrial Engineering and Management Sciences, Northwestern University, </institution> <address> Evanston, USA. </address> <month> 17 </month>
Reference-contexts: The first such approach was proposed by Karmarkar, Lagarias, Slutsman and Wang [14] who constructed parametrized representation of the (feasible) trajectory motivated from the use of differential equations. A successfull technique that incorporates higher order information in the primal-dual algorithm comes from Mehrotra <ref> [20, 21] </ref>. This method builds a higher order Taylor approximation of the (infeasible) central trajectory and pushes an iterate towards an optimum along such an approximation. The paper [21] gives a rigorous presentation of this approach and rich computational results as well. <p> A successfull technique that incorporates higher order information in the primal-dual algorithm comes from Mehrotra [20, 21]. This method builds a higher order Taylor approximation of the (infeasible) central trajectory and pushes an iterate towards an optimum along such an approximation. The paper <ref> [21] </ref> gives a rigorous presentation of this approach and rich computational results as well. Another approach, due to Domich, Boggs, Rogers and Witzgall [3] used three independent directions and solved an auxilliary linear program in a three dimensional subspace to find a search direction. <p> The most widely used higher order method is the second order variant of Mehrotra's predictor-corrector. The computational results of <ref> [21] </ref> confirmed the advantages of using a second order method over the use of a pure | first order | primal-dual algorithm but at the same time showed that introducing higher (than two) order terms had hardly predictable consequences for the overall efficiency measured with CPU time. <p> The first theoretical results for the primal-dual algorithm come from Megiddo [19] and Kojima, Mizuno and Yoshise [15]. Descriptions of its efficient implementations can be found in <ref> [17, 18, 20, 21] </ref> and in the survey [8]. 3 2.1 Fundamentals of the primal-dual method Let us consider a primal linear programming problem minimize c T x; subject to Ax = b; (1) x + s = u; where c; x; s; u 2 R n ; b 2 R <p> It is thus natural to look for the possibility of reducing their number to the necessary minimum, even at the expense of some increase of a cost of a single iteration. Mehrotra's predictor-corrector technique <ref> [20, 21] </ref> incorporates higher order information when approximating central trajectory and computing direction step. Formally, this method looks for for a parametric representation of the trajectory that guides from a given (infeasible) point in primal and dual spaces to a solution of (1)-(2). <p> Formally, this method looks for for a parametric representation of the trajectory that guides from a given (infeasible) point in primal and dual spaces to a solution of (1)-(2). It then builds an approximation of this trajectory using Taylor polynomial expansion derived for the abovementioned representation. Computational results <ref> [21] </ref> showed that the use of second order method (with one correction term) gave evident savings over the basic first order variant of the algorithm. <p> Constructing the Taylor or parametric higher (than two) order approximation of the central trajectory seems to be a natural choice. However, extensive computational experience <ref> [21] </ref> showed that its influence on the efficiency of the code was questionable: the method of order two (one corrector term) was shown to be the fastest in the average. <p> This is in contrast to the approach of <ref> [21] </ref> in which (apart from the abovementioned operations) computing kth order corrector requires additionally the solution of o (n) polynomial inequalities of order k. We end this section with a formal presentation of a prototype algorithm of multiple centrality corrections.
Reference: [22] <author> Messner S. </author> <title> (1984) User's guide for the matrix genertor of MESSAGE II, </title> <institution> IIASA WP-84-71a, Inter--national Institute for Applied Systems Analysis, Laxenburg, Austria, </institution> <year> 1984. </year>
Reference-contexts: are allowed (or that we hope soon to be allowed) to distribute 1 . 4.2 Numerical results We start from a demonstration of the advantages of the use of multiple centrality correctors on two large Netlib problems: dense, PILOT87 and sparse, STOCFOR3 and two energy planning models developed at IIASA, <ref> [22, 27, 23] </ref> MOD2 and WORLD. Their original sizes (the number of rows, columns and nonzero entries in A) are: (m; n; nz) = (2030; 4883; 73804), (m; n; nz)= (16676; 15695; 74004), (m; n; nz)=(35664; 31728; 220116) and (m; n; nz)=(35510; 32734; 220748), respectively.
Reference: [23] <author> Nakicenovic N., A. Gruebler, A. Inaba, S. Messner, S. Nilsson, Y. Nishimura, H-H. Rogner, A. Schaefer, L. Schrattenholzer, M. Strubegger, J. Swisher, D. Victor and D. </author> <title> Wilson (1993) Long-Term Strategies for Mitigating Global Warming. </title> <journal> Special Issue, Energy The International Journal 18, </journal> <volume> No 5, </volume> <pages> pp. 409-601. </pages>
Reference-contexts: are allowed (or that we hope soon to be allowed) to distribute 1 . 4.2 Numerical results We start from a demonstration of the advantages of the use of multiple centrality correctors on two large Netlib problems: dense, PILOT87 and sparse, STOCFOR3 and two energy planning models developed at IIASA, <ref> [22, 27, 23] </ref> MOD2 and WORLD. Their original sizes (the number of rows, columns and nonzero entries in A) are: (m; n; nz) = (2030; 4883; 73804), (m; n; nz)= (16676; 15695; 74004), (m; n; nz)=(35664; 31728; 220116) and (m; n; nz)=(35510; 32734; 220748), respectively.
Reference: [24] <institution> Okken P.A. et al. </institution> <year> (1994). </year> <title> Energy Systems and CO 2 Constraints, </title> <type> Technical Report ECN-C-93-014, </type> <institution> Energieonderzoek Centrum Nederland, Petten, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Table 3 gives their statistics in original form and after presolve reductions [7], the ratios of factorization and solve efforts, r f=s and the maximum number of correctors chosen automatically by the heuristic of Section 3.3, K. Problems BL through UK are MARKAL economical models for different countries <ref> [25, 28, 16, 24] </ref>. The remaining problems come from the Kennington's collection available via Netlib. Table 4 reports results of solving them with the HOPDM 2.1 code.
Reference: [25] <author> Schaumann P. et al. </author> <year> (1994). </year> <institution> Integrierte Gesamtstrategien der Minderung energiebedingter Treib-hausgasemissionen (2005/2020). Studie im Auftrag der Enquete-Kommission "Schutz der Erdat-mosphaere" des 12. Deutschen Budnestages, Stuttgart, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: Table 3 gives their statistics in original form and after presolve reductions [7], the ratios of factorization and solve efforts, r f=s and the maximum number of correctors chosen automatically by the heuristic of Section 3.3, K. Problems BL through UK are MARKAL economical models for different countries <ref> [25, 28, 16, 24] </ref>. The remaining problems come from the Kennington's collection available via Netlib. Table 4 reports results of solving them with the HOPDM 2.1 code.
Reference: [26] <author> Sonnevend G., Stoer J. Zhao G. </author> <year> (1994). </year> <title> Subspace Methods for Solving Linear Programming Problems, </title> <type> Technical Report, </type> <institution> Institut fur Angewandte Mathematik und Statistic, Universitat Wurzburg, Wurzburg, Germany, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Another approach, due to Domich, Boggs, Rogers and Witzgall [3] used three independent directions and solved an auxilliary linear program in a three dimensional subspace to find a search direction. The method of Sonnevend, Stoer and Zhao <ref> [26] </ref> used subspaces spanned by directions generated by higher order derivatives of the central path, or earlier computed points of it as a predictor step. This was later followed by one (or more) centering steps to drive the next iterate sufficiently close to the central path.
Reference: [27] <author> Strubegger M. </author> <title> (1984) User's guide for the post-processor of MESSAGE II, </title> <booktitle> IIASA WP-84-71b, International Institute for Applied Systems Analysis, </booktitle> <address> Laxenburg, Austria, </address> <year> 1984. </year>
Reference-contexts: are allowed (or that we hope soon to be allowed) to distribute 1 . 4.2 Numerical results We start from a demonstration of the advantages of the use of multiple centrality correctors on two large Netlib problems: dense, PILOT87 and sparse, STOCFOR3 and two energy planning models developed at IIASA, <ref> [22, 27, 23] </ref> MOD2 and WORLD. Their original sizes (the number of rows, columns and nonzero entries in A) are: (m; n; nz) = (2030; 4883; 73804), (m; n; nz)= (16676; 15695; 74004), (m; n; nz)=(35664; 31728; 220116) and (m; n; nz)=(35510; 32734; 220748), respectively.

References-found: 27

