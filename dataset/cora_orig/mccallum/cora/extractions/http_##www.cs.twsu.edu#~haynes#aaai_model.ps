URL: http://www.cs.twsu.edu/~haynes/aaai_model.ps
Refering-URL: http://adept.cs.twsu.edu/~thomas/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail: [haynes,sandip]@euler.mcs.utulsa.edu  
Title: Learning Cases to Resolve Conflicts and Improve Group Behavior  
Author: Thomas Haynes and Sandip Sen 
Address: 600 South College Avenue Tulsa, OK 74104-3189  
Affiliation: Department of Mathematical Computer Sciences The University of Tulsa  
Abstract: Groups of agents following fixed behavioral rules can be limited in performance and efficiency. Adaptability and flexibility are key components of intelligent behavior which allow agent groups to improve performance in a given domain using prior problem solving experience. We motivate the usefulness of individual learning by group members in the context of overall group behavior. In particular, we propose a framework in which individual group members learn cases to improve their model of other group members. We use a testbed problem from the distributed AI literature to show that simultaneous learning by group members can lead to significant improvement in group performance and efficiency over agent groups following static behavioral rules. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W.; Kibler, D.; and Albert, M. K. </author> <year> 1991. </year> <title> Instance-based learning algorithms. </title> <booktitle> Machine Learning 6(1) </booktitle> <pages> 37-66. </pages>
Reference-contexts: The set of actions corresponding to the most relevant case is then adapted to fit the current situation. Cardie (Cardie 1993) defined case-based learning (CBL) as a machine learning technique used to extend instance-based learning (IBL) <ref> (Aha, Kibler, & Albert 1991) </ref>. The IBL algorithm retrieves the nearest instance (for our purposes, an instance can be thought of a case) to a state, and performs the suggested actions. There is no case adaptation if the retrieved instance is not a direct match to the current state.
Reference: <author> Cardie, C. </author> <year> 1993. </year> <title> Using decision trees to improve case-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> 25-32. </pages> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: If there is no such match, then cases that are similar to the current state are retrieved from the case library. The set of actions corresponding to the most relevant case is then adapted to fit the current situation. Cardie <ref> (Cardie 1993) </ref> defined case-based learning (CBL) as a machine learning technique used to extend instance-based learning (IBL) (Aha, Kibler, & Albert 1991). The IBL algorithm retrieves the nearest instance (for our purposes, an instance can be thought of a case) to a state, and performs the suggested actions.
Reference: <author> Garland, A., and Alterman, R. </author> <year> 1995. </year> <title> Preparation of multi-agent knowledge for reuse. </title> <editor> In Aha, D. W., and Ram, A., eds., </editor> <booktitle> Working Notes for the AAAI Symposium on Adaptation of Knowldege for Reuse. </booktitle> <address> Cam-bridge, MA: </address> <publisher> AAAI. </publisher>
Reference: <author> Gmytrasiewicz, P. J., and Durfee, E. H. </author> <year> 1995. </year> <title> A rigorous, operational formalization of recursive modeling. </title>
Reference-contexts: A problem in multiagent systems is that the best action for Agent A i might be in conflict with that for another Agent A j . Agent A i , then, should try to model the behavior of A j , and incorporate that into its expected utility calculations <ref> (Gmytrasiewicz & Durfee 1995) </ref>. The optimal action for an individual agent might not be the optimal action for its group. Thus an agent can evaluate the utility of its actions on two levels: individual and group.
Reference: <editor> In Lesser, V., ed., </editor> <booktitle> Proceedings of the First International Conference on Multi-Agent Systems, </booktitle> <pages> 125-132. </pages> <address> San Francisco, CA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Golding, A. R., and Rosenbloom, P. S. </author> <year> 1991. </year> <title> Improving rule-based systems through case-based reasoning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> 22-27. </pages>
Reference: <author> Halpern, J., and Moses, Y. </author> <year> 1990. </year> <title> Knowledge and common knowledge in a distributed environment. </title> <journal> Journal of the ACM 37(3) </journal> <pages> 549-587. </pages> <note> A preliminary version appeared in Proc. 3rd ACM Symposium on Principles of Distributed Computing, </note> <year> 1984. </year>
Reference-contexts: Even if agents are allowed to communicate, communication delays, improper use of language, different underlying assumptions, etc. can prevent agents from developing a shared common body of knowledge <ref> (Halpern & Moses 1990) </ref>. For example, even communicating intentions and negotiating to avoid conflict situations may prove to be too time consuming and impractical in some domains (Lesser 1995).
Reference: <author> Hammond, K.; Converse, T.; and Marks, M. </author> <year> 1990. </year> <title> Towards a theory of agency. </title> <booktitle> In Proceedings of the Workshop on Innovative Approaches to Planning, Scheduling and Control, </booktitle> <pages> 354-365. </pages> <address> San Diego: </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Haynes, T., and Sen, S. </author> <year> 1996. </year> <title> Evolving behavioral strategies in predators and prey. </title> <editor> In Wei, G., and Sen, S., eds., </editor> <booktitle> Adaptation and Learning in Multiagent Systems, Lecture Notes in Artificial Intelligence. </booktitle> <address> Ber-lin: </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: There are two problems with this setup: the number of cases is too large, and the agents do not act independently. This case window and others are analyzed and rejected in <ref> (Haynes, Lau, & Sen 1996) </ref>. Unless the entire world is used as a case, any narrowing of the case window is going to suffer from the above points of the "effective" case window presented above. The same case can represent several actual configurations of the domain being modeled. <p> The case window employed is that depicted in Figure 2. We have also identified two enhancements to break ties caused by the default rules employed in the MD metric: look ahead and least conflict <ref> (Haynes, Lau, & Sen 1996) </ref>. Look ahead breaks ties in which two moves are equidistant via MD, the one which is potentially closer in two moves is selected. <p> Initially we were interested in the ability of predator behavioral rules to effectively capture the Still prey. We tested three behavioral strategies: MD the basic MD algorithm, MD-EDR the MD modified with the enhancements discussed in <ref> (Haynes, Lau, & Sen 1996) </ref>, and MD-CBL which is MD-EDR utilizing a case base learned from training on 100 random simulations. The results of applying these strategies on 100 test cases are shown in Table 1.
Reference: <author> Haynes, T.; Sen, S.; Schoenefeld, D.; and Wainwright, R. </author> <year> 1995. </year> <title> Evolving multiagent coordination strategies with genetic programming. </title> <journal> Artificial Intelligence. </journal> <note> (submitted for review). </note>
Reference-contexts: The goal is for four predator agents to capture a prey agent. In spite of its apparent simplicity, it has been shown that the domain provides for complex interactions between agents and no hand-coded coordination strategy is very effective <ref> (Haynes et al. 1995) </ref>. Simple greedy strategies for the predators have long been postulated to efficiently capture the prey (Korf 1992). The underlying assumption that the prey moves first, then the predators move in order simplifies the domain such that efficient capture is possible. <p> Relaxing the assumption leads to a more natural model in which all agents move at once. This model has been shown to create deadlock situations for simple prey algorithms of moving in a straight line (Linear) or even not moving at all (Still) <ref> (Haynes et al. 1995) </ref>! Two possible solutions have been identified: allowing communication and adding state information. We investigate a learning system that utilizes past expectations to reduce deadlock situations. The predator agents have to capture the prey agent by blocking its orthogonal movement. <p> Any ties are broken randomly. He claims this addition to the prey movements makes the problem considerably more difficult. The MD strategy is more successful than the MN in capturing a Linear prey (22% vs 0%) <ref> (Haynes et al. 1995) </ref>. Despite the fact that it can often block the forward motion of the prey, its success is still very low. The MD metric algorithms are very susceptible to deadlock situations, such as in Figure 1.
Reference: <author> Haynes, T.; Lau, K.; and Sen, S. </author> <year> 1996. </year> <title> Learning cases to compliment rules for conflict resolution in multia-gent systems. </title> <editor> In Sen, S., ed., </editor> <booktitle> Working Notes for the AAAI Symposium on Adaptation, Co-evolution and Learning in Multiagent Systems, </booktitle> <pages> 51-56. </pages>
Reference-contexts: There are two problems with this setup: the number of cases is too large, and the agents do not act independently. This case window and others are analyzed and rejected in <ref> (Haynes, Lau, & Sen 1996) </ref>. Unless the entire world is used as a case, any narrowing of the case window is going to suffer from the above points of the "effective" case window presented above. The same case can represent several actual configurations of the domain being modeled. <p> The case window employed is that depicted in Figure 2. We have also identified two enhancements to break ties caused by the default rules employed in the MD metric: look ahead and least conflict <ref> (Haynes, Lau, & Sen 1996) </ref>. Look ahead breaks ties in which two moves are equidistant via MD, the one which is potentially closer in two moves is selected. <p> Initially we were interested in the ability of predator behavioral rules to effectively capture the Still prey. We tested three behavioral strategies: MD the basic MD algorithm, MD-EDR the MD modified with the enhancements discussed in <ref> (Haynes, Lau, & Sen 1996) </ref>, and MD-CBL which is MD-EDR utilizing a case base learned from training on 100 random simulations. The results of applying these strategies on 100 test cases are shown in Table 1.
Reference: <author> Kolodner, J. L. </author> <year> 1993. </year> <title> Case-Based Reasoning. </title> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: The multiagent case-based learning (MCBL) algorithm utilizes exceptions to a default ruleset, which describes the behavior of an agent. These exceptions form a case library. The agent does not reason with these cases, as in CBR <ref> (Kolodner 1993) </ref>, but rather modifies an inaccurate individual model to approximate a group model. <p> The question that arises from these findings is how should the agents manage conflict resolution? An answer can be found in the ways we as humans manage conflict resolution, with cases <ref> (Kolodner 1993) </ref>. In the simplest sense, if predator 1 senses that if predator 2 is in its Northeast cell, and it has determined to move North, then if the other agent moves West there will be a conflict with predator 2.
Reference: <author> Korf, R. E. </author> <year> 1992. </year> <title> A simple solution to pursuit games. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> 183-194. </pages>
Reference-contexts: In spite of its apparent simplicity, it has been shown that the domain provides for complex interactions between agents and no hand-coded coordination strategy is very effective (Haynes et al. 1995). Simple greedy strategies for the predators have long been postulated to efficiently capture the prey <ref> (Korf 1992) </ref>. The underlying assumption that the prey moves first, then the predators move in order simplifies the domain such that efficient capture is possible. Relaxing the assumption leads to a more natural model in which all agents move at once. <p> Both algorithms examine the metrics from the set of possible moves, i.e. moving in one of the four orthogonal directions or staying still, and select a move corresponding to the minimal distance metric. All ties are randomly broken. Korf <ref> (Korf 1992) </ref> claims in his research that a discret-ization of the continuous world that allows only horizontal and vertical movements is a poor approximation. He calls this the orthogonal game. Korf developed several greedy solutions to problems where eight predators are allowed to move orthogonally as well as diagonally.
Reference: <author> Lesser, V. R. </author> <year> 1995. </year> <title> Multiagent systems: An emerging subdiscipline of AI. </title> <journal> ACM Computing Surveys 27(3) </journal> <pages> 340-342. </pages>
Reference-contexts: For example, even communicating intentions and negotiating to avoid conflict situations may prove to be too time consuming and impractical in some domains <ref> (Lesser 1995) </ref>. These and other problems combine to confound an individual in its attempt to predict the behavior of other members of its group. We investigate a method for allowing agents to improve their models of other members of the group.
Reference: <author> Prasad, M. V. N.; Lesser, V. R.; and Lander, S. </author> <year> 1995. </year> <title> Reasoning and retrieval in distributed case bases. Journal of Visual Communication and Image Representation, </title> <note> Special Issue on Digital Libraries. Also as UMASS CS Technical Report 95-27, </note> <year> 1995. </year>
Reference: <editor> Sen, S., ed. </editor> <booktitle> 1995. Working Notes of the IJCAI-95 Workshop on Adaptation and Learning in Multiagent Systems. </booktitle>
Reference-contexts: Adaptation and learning are key mechanisms by which agents can modify their behavior on-line to maintain a viable performance profile in such scenarios. A number of researchers have recently started investigating learning approaches targeted for multia-gent systems <ref> (Sen 1995) </ref>.
Reference: <author> Stephens, L. M., and Merx, M. B. </author> <year> 1990. </year> <title> The effect of agent control strategy on the performance of a DAI pursuit problem. </title> <booktitle> In Proceedings of the 1990 Distributed AI Workshop. </booktitle>
Reference-contexts: We investigate a learning system that utilizes past expectations to reduce deadlock situations. The predator agents have to capture the prey agent by blocking its orthogonal movement. The game is typically played on a 30 by 30 grid world, which is toroidal <ref> (Stephens & Merx 1990) </ref>. The behavioral strategies of the predators use one of two distance met-rics: Manhattan distance (MD) and max norm (MN). The MD metric is the sum of the differences of the x and y coordinates between two agents.
Reference: <author> Sycara, K. </author> <year> 1987. </year> <title> Planning for negotiation: A case-based approach. </title> <booktitle> In DARPA Knowledge-Based Planning Workshop, 11.1-11.10. </booktitle> <pages> 7 </pages>
References-found: 18

