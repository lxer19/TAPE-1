URL: http://www.rpi.edu/~bennek/mr98100.ps
Refering-URL: http://www.rpi.edu/~bennek/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: bennek@rpi.edu, wud2@rpi.edu  
Phone: (518)276-6899  
Title: On Support Vector Decision Trees for Database Marketing  
Author: Kristin P. Bennett Leonardo Auslender Donghui Wu Sagamore Ave. 
Date: March 1998  
Address: Edison, NJ 08820  leodavinci@worldnet.att.net Troy, NY 12180  
Affiliation: Department of Mathematical Sciences  Rensselaer Polytechnic Institute  R.P.I. Math  
Pubnum: Report 98-100  
Abstract: We introduce a support vector decision tree method for customer targeting in the framework of large databases (database marketing). The goal is to provide a tool to identify the best customers based on historical data. Then this tool is used to forecast the best potential customers among a pool of prospects. We begin by recursively constructing a decision tree. Each decision consists of a linear combination of independent attributes. A linear program motivated by the support vector machine method from Vap-nik's Statistical Learning Theory is used to construct each decision. This linear program automatically selects the relevant subset of attributes for each decision. Each customer is scored based on the decision tree. A gain-schart table is used to verify the goodness of fit of the targeting, to determine the likely prospects and the expected utility or profit. Successful results are given for three industrial problems. The method consistently pro duced trees with a very small number of decision nodes. Each decision consisted of a relatively small number of attributes, which evinces the method's power of dimensionality reduction. The largest training dataset tested contained 15,700 points with 866 attributes. The commercial optimization package used, CPLEX, is capable of solving even larger problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Leonardo Auslender. </author> <title> Missing value imputation methods for large databases. </title> <booktitle> In Proceedings of the NorthEast SAS Users Group, </booktitle> <publisher> Inc., </publisher> <year> 1997. </year>
Reference-contexts: The first step is the creation of the database, which could be specific to an event of interest. In this paper we examine three problems from two different databases. All continuous attributes were rescaled between 0 and 100 or 0 and 1. Binary discrete attributes were mapped to <ref> [0, 1] </ref>. All missing values were imputed using the algorithms in [1]. The next step is development a classification model and rank ordering of the customers. Diagnostics and model assessment are presented in a gainschart table that determines the likely prospects and the expected customer utility or business profit [14]. <p> In this paper we examine three problems from two different databases. All continuous attributes were rescaled between 0 and 100 or 0 and 1. Binary discrete attributes were mapped to [0, 1]. All missing values were imputed using the algorithms in <ref> [1] </ref>. The next step is development a classification model and rank ordering of the customers. Diagnostics and model assessment are presented in a gainschart table that determines the likely prospects and the expected customer utility or business profit [14].
Reference: [2] <author> K. P. Bennett. </author> <title> Decision tree construction via linear programming. </title> <editor> In M. Evans, editor, </editor> <booktitle> Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference, </booktitle> <pages> pages 97-101, </pages> <address> Utica, Illinois, </address> <year> 1992. </year>
Reference-contexts: SVDT performs top down induction of decision trees (TDIDT) like many other decision tree algorithms including CHAID, CART, MSMT, C4.5, and OC1 <ref> [8, 2, 18, 15] </ref>. The primary distinguishing factors between SVDT and other TDIDT algorithms are the type of decisions used (linear SVM) and the method of constructing the decisions (linear programming). The basic TDIDT algorithm works as follows: Algorithm 2.1 (Basic TDIDT ) Start with the root node.
Reference: [3] <author> K. P. Bennett and E. J. Bredensteiner. </author> <title> Geometry in learning. </title> <editor> In C. Gorini, E. Hart, W. Meyer, and T. Phillips, editors, </editor> <booktitle> Geometry at Work, </booktitle> <address> Washington, D.C., </address> <year> 1997. </year> <journal> Mathematical Association of America. </journal> <note> To appear. </note>
Reference-contexts: The variance is controlled by the "margin of separation". Constructing decision trees with maximal margins of separation at each decision leads to better generalization, both theoretically and in practice [19]. Each decision is constructed using robust linear programming (RLP) for classification with margin maximization <ref> [4, 3, 7] </ref>. RLP is 3 PSfrag replacements Class 1 Class -1 w T x = fl 1 w T x = fl kwk 2 o a minor but critical variation of the generalized optimal plane (GOP) method for support vector machines [21].
Reference: [4] <author> K. P. Bennett and O. L. Mangasarian. </author> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1 </volume> <pages> 23-34, </pages> <year> 1992. </year>
Reference-contexts: The variance is controlled by the "margin of separation". Constructing decision trees with maximal margins of separation at each decision leads to better generalization, both theoretically and in practice [19]. Each decision is constructed using robust linear programming (RLP) for classification with margin maximization <ref> [4, 3, 7] </ref>. RLP is 3 PSfrag replacements Class 1 Class -1 w T x = fl 1 w T x = fl kwk 2 o a minor but critical variation of the generalized optimal plane (GOP) method for support vector machines [21]. <p> Then if t i = 1, then ffi i = 1=jClass 1j. Sim ilarly, if t i = 1, then ffi i = 1=jClass 1j. Historically, these were the misclassification costs used in the first version of RLP proposed <ref> [4] </ref> and we continue to use them because they help scale the problem consistently for different training set sizes and class proportions. In order to choose we create a validation set consisting of 1/7 of the training data. Using the validation set, we choose the appropriate setting for .
Reference: [5] <author> K. P. Bennett and O. L. Mangasar-ian. </author> <title> Serial and parallel multicategory discrimination. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4(4) </volume> <pages> 722-734, </pages> <year> 1994. </year>
Reference-contexts: The actual problem size possible under the current limitation depends on the total number of nonzero data entries and the amount of memory available. SVDT can be used on problems that exceed memory limitations. The decomposition methods used in parallel optimization <ref> [6, 5] </ref> and those developed for general SVM [17] could be applied to SVDT. The last dataset, Age, is from a different source than the previous two datasets. The problem is to target customers under the age of 25.
Reference: [6] <author> D.P. Bertsekas and J.N. Tsitsiklis. </author> <title> Parallel and Distributed Computation. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: The actual problem size possible under the current limitation depends on the total number of nonzero data entries and the amount of memory available. SVDT can be used on problems that exceed memory limitations. The decomposition methods used in parallel optimization <ref> [6, 5] </ref> and those developed for general SVM [17] could be applied to SVDT. The last dataset, Age, is from a different source than the previous two datasets. The problem is to target customers under the age of 25.
Reference: [7] <author> E. Bredensteiner and K. Bennett. </author> <title> Feature minimization within decision trees. R.P.I. Math Report No. </title> <type> 218, </type> <institution> Rensselaer Polytechnic Institute, </institution> <address> Troy, NY, </address> <year> 1995. </year> <note> To appear in Computational Optimization and Applications. </note>
Reference-contexts: The variance is controlled by the "margin of separation". Constructing decision trees with maximal margins of separation at each decision leads to better generalization, both theoretically and in practice [19]. Each decision is constructed using robust linear programming (RLP) for classification with margin maximization <ref> [4, 3, 7] </ref>. RLP is 3 PSfrag replacements Class 1 Class -1 w T x = fl 1 w T x = fl kwk 2 o a minor but critical variation of the generalized optimal plane (GOP) method for support vector machines [21].
Reference: [8] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International, </booktitle> <address> Califor-nia, </address> <year> 1984. </year>
Reference-contexts: SVDT performs top down induction of decision trees (TDIDT) like many other decision tree algorithms including CHAID, CART, MSMT, C4.5, and OC1 <ref> [8, 2, 18, 15] </ref>. The primary distinguishing factors between SVDT and other TDIDT algorithms are the type of decisions used (linear SVM) and the method of constructing the decisions (linear programming). The basic TDIDT algorithm works as follows: Algorithm 2.1 (Basic TDIDT ) Start with the root node.
Reference: [9] <author> C. J. C Burges. </author> <title> A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery, </title> <note> 1998. to appear. </note>
Reference-contexts: The quadratic program (3) is solvable in polynomial time using interior point methods . By incorporating kernels or basis functions, the method can readily be used to construct nonlinear classifications functions such as neural networks and radial basis functions. See <ref> [9] </ref> for a tutorial on SVM. RLP differs from GOP in the objective. The margin maximization term is changed from the 2-norm kwk 2 to the 1-norm, kwk 1 = P n j=1 jw j j.
Reference: [10] <author> C. Cortes and V. N. Vapnik. </author> <title> Support vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 273-297, </pages> <year> 1995. </year>
Reference-contexts: A slack term i is added for each point such that if the point is misclassified, i 1. The final GOP formulation is: GOP Problem min m X i i 0; i = 1; : : : ; m This is a minor variant of the formulations given in <ref> [10, 21] </ref>. The problem objective maximizes the margin with weight and minimizes a measure of the classification error with weight (1 ). The user must select 2 (0; 1) to control the trade-off between the misclassification rate and the margin of separation.
Reference: [11] <institution> CPLEX Optimization Incorporated, </institution> <address> Incline Village, Nevada. Using the CPLEX Callable Library, </address> <year> 1994. </year>
Reference-contexts: State-of-the-art linear programming solvers are more efficient, more robust, and capable of solving larger problems than are quadratic programming solvers. GOP has the advantage that it can easily be generalized to create nonlinear SVMs. In practice we use the commercial package CPLEX <ref> [11] </ref> to solve the more efficient dual of the RLP using the simplex method: Dual RLP Problem min m X ff i m X t i ff i x i (1 )e (5) where e is an n-dimensional vector of ones.
Reference: [12] <author> R. D'Agostino, J. Griffith, C. Schmid, and N. Terrin. </author> <title> Measures for evaluating model performance. </title> <journal> American Statistical Association, </journal> <volume> manuscript, </volume> <year> 1997. </year>
Reference-contexts: Given randomness, it is very unlikely that every possible "good" customer can be identified. Given misclassification costs (embedded in the gain-schart), the analyst can decide the targeting thresholds. Classifiers with the best accuracy do not necessarily generate the best gainscharts <ref> [12] </ref>. There are additional criteria to consider. Dimensionality reduction is important. It is preferable to minimize the number of attributes required to produce the desired results. Minimizing the number of attributes used helps to reduce the cost of obtaining data. Further, minimization usually enhances model interpretability.
Reference: [13] <author> M. Golea, P. L. Bartlett, W. S. Lee, and L. Mason. </author> <title> Generalization in decision trees and DNF: </title> <booktitle> Does size matter? In Advances in Neural Information Processing Systems 10, </booktitle> <year> 1998. </year>
Reference-contexts: Recent results on applying learning theory to decision trees show that there is a tradeoff between the structural complexity of a tree, i.e. the depth and number of nodes, and the complexity of the decisions that are used <ref> [13, 19] </ref>. If we want a tree with a simpler structure we can use more powerful decisions. The limiting case is a tree consisting of one very powerful decision function such as a neural network. But then all the benefits of the decision trees are lost.
Reference: [14] <author> A. M. Hughes. </author> <title> The Complete Database Marketer. </title> <publisher> Irwin Prof. Publishing, </publisher> <address> Chicago, </address> <year> 1996. </year>
Reference-contexts: All missing values were imputed using the algorithms in [1]. The next step is development a classification model and rank ordering of the customers. Diagnostics and model assessment are presented in a gainschart table that determines the likely prospects and the expected customer utility or business profit <ref> [14] </ref>. In Section 2 we describe the recursive SVDT algorithm for constructing the classification model. In this paper, each decision is a linear discriminant. <p> We reached 77% of the Class 1 population at the fifth decile. The testing gainschart combined with a model of expected profitability can be used to determine thresholds for scoring. In the scoring process, potential customers are selected based on the model and the selected threshold <ref> [20, 14] </ref>. 7 PSfrag replacements D0 D2 L3 L0 28 attr 487 margin 13 attr 1058 margin 12 attr 37 margin Class 1 Resp Rate: 83.0% Targ Class: 67.8% Total Pop: 37.4% Class -1 Resp Rate: 21.0% Targ Class: 10.4% Total Pop: 24.9% Class -1 Resp Rate: 27.9% Targ Class: 15.8%
Reference: [15] <author> S. Murthy, S. Kasif, and S. Salzberg. </author> <title> A system for induction of oblique decision trees. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 1-32, </pages> <year> 1994. </year> <month> 10 </month>
Reference-contexts: SVDT performs top down induction of decision trees (TDIDT) like many other decision tree algorithms including CHAID, CART, MSMT, C4.5, and OC1 <ref> [8, 2, 18, 15] </ref>. The primary distinguishing factors between SVDT and other TDIDT algorithms are the type of decisions used (linear SVM) and the method of constructing the decisions (linear programming). The basic TDIDT algorithm works as follows: Algorithm 2.1 (Basic TDIDT ) Start with the root node.
Reference: [16] <author> S. G. Nash and A. Sofer. </author> <title> Linear and Nonlinear Programming. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1996. </year>
Reference-contexts: Larger problems would be possible on our machine. For massive datasets, CPLEX is primarily memory constrained. For the simplex method, we can expect on average the number of pivots to be one to three times the number of attributes in the problem <ref> [16] </ref>. Polynomial time interior point methods may also be used. CPLEX and other linear programming packages typically can exploit any sparsity of the data. The actual problem size possible under the current limitation depends on the total number of nonzero data entries and the amount of memory available.
Reference: [17] <author> E. Osuna, R. Freund, and F. Girosi. </author> <title> Support vector machines: Training and applications. </title> <type> AI Memo 1602, </type> <institution> Maas-sachusets Institute of Technology, </institution> <year> 1997. </year>
Reference-contexts: The actual problem size possible under the current limitation depends on the total number of nonzero data entries and the amount of memory available. SVDT can be used on problems that exceed memory limitations. The decomposition methods used in parallel optimization [6, 5] and those developed for general SVM <ref> [17] </ref> could be applied to SVDT. The last dataset, Age, is from a different source than the previous two datasets. The problem is to target customers under the age of 25.
Reference: [18] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: SVDT performs top down induction of decision trees (TDIDT) like many other decision tree algorithms including CHAID, CART, MSMT, C4.5, and OC1 <ref> [8, 2, 18, 15] </ref>. The primary distinguishing factors between SVDT and other TDIDT algorithms are the type of decisions used (linear SVM) and the method of constructing the decisions (linear programming). The basic TDIDT algorithm works as follows: Algorithm 2.1 (Basic TDIDT ) Start with the root node. <p> The historical data consists of a set of 3,364 points with 612 attributes each. Approximately 49% belong in the target class. The data was divided into a training set of 2,358 points and a testing set of 1,006 points. The C4.5 algorithm <ref> [18] </ref>, a univariate TDIDT method, constructed a tree with 251 univariate decisions for this data. The tree is too large to readily interpret. Since the training set accuracy was high (88%) and the testing set accuracy is fairly low (66%) it appears that overfitting is occurring.
Reference: [19] <author> J. Shawe-Taylor and N. Cristianini. </author> <title> Data-dependent structural risk minimi-sation for perceptron decision trees. </title> <booktitle> In Advances in Neural Information Processing Systems 10, </booktitle> <year> 1998. </year>
Reference-contexts: Recent results on applying learning theory to decision trees show that there is a tradeoff between the structural complexity of a tree, i.e. the depth and number of nodes, and the complexity of the decisions that are used <ref> [13, 19] </ref>. If we want a tree with a simpler structure we can use more powerful decisions. The limiting case is a tree consisting of one very powerful decision function such as a neural network. But then all the benefits of the decision trees are lost. <p> But then all the benefits of the decision trees are lost. Hence, we restrict the tree to simple linear decisions or perceptrons. For the splitting criterion for each decision we use structural risk minimization (SRM) principle from statistical learning theory <ref> [21, 19] </ref>. By SRM, the classifier must both reduce the expected classification error and the variance in the error to ensure good generalization. The variance is controlled by the "margin of separation". <p> The variance is controlled by the "margin of separation". Constructing decision trees with maximal margins of separation at each decision leads to better generalization, both theoretically and in practice <ref> [19] </ref>. Each decision is constructed using robust linear programming (RLP) for classification with margin maximization [4, 3, 7]. <p> By using larger values of that require greater margins of separation, we both avoid overfitting and bias the answer toward decisions with few attributes. An added bonus is that learning theory results for decision trees predict that decisions with large margins should produce better generalization <ref> [19] </ref>. Once again we reserve 1/7 of the training data as a validation set. Using the results on the validation and training sets as a guide, the modeler controls how the tree is grown and pruned. We do not use the testing set to construct the tree.
Reference: [20] <author> Ward Thomas. </author> <title> Database marketing: Dual approach outdoes response modeling. Database Marketing News, </title> <type> page 26, </type> <month> June </month> <year> 1996. </year>
Reference-contexts: The gainschart is used to ascertain the discriminating power of the model, by measuring the percentage of targeted events "captured" at each decile. In conjunction with the profit parameters, the gainschart provides profit estimates and thus serves to guide final business decisions <ref> [20] </ref>. Each customer can be ranked by the response rate at the leaf of the decision tree that is used to classify that customer. Then the question arises as to how to order all the customers reaching a leaf. <p> We reached 77% of the Class 1 population at the fifth decile. The testing gainschart combined with a model of expected profitability can be used to determine thresholds for scoring. In the scoring process, potential customers are selected based on the model and the selected threshold <ref> [20, 14] </ref>. 7 PSfrag replacements D0 D2 L3 L0 28 attr 487 margin 13 attr 1058 margin 12 attr 37 margin Class 1 Resp Rate: 83.0% Targ Class: 67.8% Total Pop: 37.4% Class -1 Resp Rate: 21.0% Targ Class: 10.4% Total Pop: 24.9% Class -1 Resp Rate: 27.9% Targ Class: 15.8%
Reference: [21] <author> V. N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1996. </year> <month> 11 </month>
Reference-contexts: In Section 2 we describe the recursive SVDT algorithm for constructing the classification model. In this paper, each decision is a linear discriminant. We treat each decision as a support vector machine (SVM) and use linear programming to construct the decision according to the principles of Statistical Learning Theory <ref> [21] </ref>. Linear discriminants are a special case of the more general SVM. Nonlinear SVMs, (e.g. polynomials, neural networks, and radial basis functions) could also be used for the decisions in SVDT. The classification model is used to score or rank order a pool of potential customers. <p> But then all the benefits of the decision trees are lost. Hence, we restrict the tree to simple linear decisions or perceptrons. For the splitting criterion for each decision we use structural risk minimization (SRM) principle from statistical learning theory <ref> [21, 19] </ref>. By SRM, the classifier must both reduce the expected classification error and the variance in the error to ensure good generalization. The variance is controlled by the "margin of separation". <p> RLP is 3 PSfrag replacements Class 1 Class -1 w T x = fl 1 w T x = fl kwk 2 o a minor but critical variation of the generalized optimal plane (GOP) method for support vector machines <ref> [21] </ref>. <p> A slack term i is added for each point such that if the point is misclassified, i 1. The final GOP formulation is: GOP Problem min m X i i 0; i = 1; : : : ; m This is a minor variant of the formulations given in <ref> [10, 21] </ref>. The problem objective maximizes the margin with weight and minimizes a measure of the classification error with weight (1 ). The user must select 2 (0; 1) to control the trade-off between the misclassification rate and the margin of separation.
References-found: 21

