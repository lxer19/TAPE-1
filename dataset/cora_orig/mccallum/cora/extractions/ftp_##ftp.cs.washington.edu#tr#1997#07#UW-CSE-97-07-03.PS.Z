URL: ftp://ftp.cs.washington.edu/tr/1997/07/UW-CSE-97-07-03.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: Parallel Prefetching and Caching  
Author: by Tracy Kimbrel 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Approved by (Co-Chairperson of Supervisory Committee) (Co-Chairperson of Supervisory Committee)  
Note: Program Authorized to Offer Degree Date  
Date: 1997  
Affiliation: University of Washington  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> H.M. Abdel-Wahab and T. Kameda. </author> <title> Scheduling to Minimize Maximum Cumulative Cost Subject to Series-Parallel Precedence Constraints. </title> <journal> Operations Research, </journal> <volume> 26(1) </volume> <pages> 141-158, </pages> <year> 1978. </year>
Reference-contexts: The algorithm requires O (n log m) arithmetic operations, where n is the total number of transactions in all of the m sequences. An algorithm that solves this problem and has the same running time was given by Abdel-Wahab and Kameda <ref> [1] </ref>. <p> ) = fI 2 I (W ) : D (I) = D fl (W )g where D fl (W ) = max D (I): An algorithm that solves this problem and is very similar to the one described here, with the same running time, was given by Abdel-Wahab and Kameda <ref> [1] </ref>. However, their algorithm is fully o*ine; that is, it considers its entire input before producing any of its output.
Reference: [2] <author> A. Acharya, M. Uysal, R. Bennett, A. Mendelson, M. Beynon, J. Hollingsworth, J. Saltz and A. Sussman. </author> <title> Tuning the Performance of I/O-Intensive Parallel Applications. </title> <booktitle> Proceedings of the Fourth Annual Workshop on I/O in Parallel and Distributed Systems, </booktitle> <pages> pages 15-27, </pages> <month> May, </month> <year> 1996. </year>
Reference: [3] <author> Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. </author> <title> Data Structures and Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983, </year> <pages> pp. 135-145. </pages>
Reference-contexts: let T 0 i + j fl F , and the conditions for the induction step on the phase index i are met. 2 3.3 The algorithms' running times In this section we consider the time required to determine a prefetching schedule in the uniform-cost RAM model (see, for example, <ref> [3] </ref>). This is distinct from the time required to serve the sequence in the model described in Chapter 2, which is the primary measure we are trying to optimize. First, consider the single-disk case. <p> Note Cache will never contain more than K keys. Each operation on Cache thus requires O (log K) time (see, for example, <ref> [3] </ref>). Note that the maximum element in Cache, the value of N extHole, and the position of the cursor provide the information needed by aggressive, fixed horizon, and reverse aggressive to decide when and what to prefetch, and what to evict. <p> The second loop can be handled similarly. Producing the output has a total cost of O (n). Thus, the most expensive operations are the O (n) insert and delete maximum (or minimum) operations on the priority queue, each with a cost of O (log m) (see, for example, <ref> [3] </ref>). Thus we have the following.
Reference: [4] <author> L.A. Belady. </author> <title> A Study of Replacement Algorithms for Virtual Storage Computers. </title> <journal> IBM Systems Journal, </journal> <volume> 5(2) </volume> <pages> 78-101, </pages> <year> 1966. </year>
Reference-contexts: This idea has been applied at the level of the processor cache to avoid main memory accesses [37]. 1.4 Relation to previous work Caching and prefetching have been known techniques to improve performance of storage hierarchies for many years <ref> [4, 13] </ref>. In computer architecture, work on caching and prefetching has focused on bridging the performance gap between processors and main memory [41]. Research using caching and prefetching in database systems [10, 33, 11] showed that it is important to use applications' knowledge to perform caching and prefetching. <p> Indeed, one principle for prefetching (the optimal eviction rule described in Section 2.3) is derived from Belady's optimal longest forward distance paging algorithm <ref> [4] </ref>. As we will see, however, the application of this rule alone is insufficient to guarantee good prefetching performance. We know of no prior theoretical analysis of the integration of prefetching and caching in the presence of multiple disks. <p> In addition, the design and analysis of an optimal o*ine algorithm is an important step towards understanding and evaluating more practical limited-lookahead algorithms. We can perhaps draw an analogy with the impact of the optimal o*ine paging algorithm <ref> [4] </ref> on the design, implementation and evaluation of online paging algorithms [40]. Our model is read-only. The algorithms we consider can improve the performance of read-only and read-mostly applications (i.e., those for which the performance impact of write traffic is negligible). <p> Conservative is the algorithm that refuses to fetch until it can evict the block that would be evicted by Belady's optimal longest forward distance <ref> [4] </ref> algorithm in the classical paging model. Belady's algorithm suffers the fewest page faults among all paging algorithms. It does this by evicting the page not needed for the longest time among all blocks in the cache whenever the next request is missing from the cache. <p> The goal is to construct, on input request sequences fR i g, an ordering of the sequences such that the number of cache misses is minimized. (The number of misses is easily determined using Belady's algorithm <ref> [4] </ref> for an associative cache, and by an even simpler method for a direct-mapped cache, once the individual sequences are ordered). We reduce the Directed Hamiltonian Path problem to the sequence ordering problem. <p> As before, each of the private blocks will cause a miss. The second reference to each p i;j ensures that none of the private blocks will be evicted before all K of them have been brought into the cache, 1 by Belady's <ref> [4] </ref> longest forward distance page replacement rule, since each must be served again before any other blocks.
Reference: [5] <author> Pei Cao, Edward Felten, and Kai Li. </author> <title> Application-Controlled File Caching Policies. </title> <booktitle> In USENIX Summer 1994 Technical Conference, </booktitle> <pages> pages 171-182, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Research using caching and prefetching in database systems [10, 33, 11] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [13, 28, 20, 31, 42, 5, 18, 6, 36] </ref>. The most common prefetching approach is 6 to perform sequential readahead, i.e., to detect when an application accesses a file sequentially, and to prefetch in order the blocks of the files that are so used [13, 28, 29]. <p> Another body of work has been on predicting future access patterns [12, 42, 33, 11, 18]. A recent trend is to use applications' knowledge about their access patterns to perform more effective caching and prefetching <ref> [5, 6, 35, 36] </ref>. Patterson et al. [35] describe a mechanism by which an application's request sequence can be made known in advance. They use a hinting interface through which an application can be explicitly programmed to diclose its future file accesses to the file system. <p> The algorithms described above determine prefetching and caching schedules for a single application process. Cao et al. and Patterson et al. propose different policies to allocate cache resources to multiple, competing processes. Cao et al. <ref> [5, 6, 8] </ref> describe LRU-SP, which determines cache allocations based on those that would be obtained using the least recently used (LRU) replacement policy, applied globally to all processes' interleaved reference streams.
Reference: [6] <author> Pei Cao, Edward W. Felten, and Kai Li. </author> <title> Implementation and Performance of Application-Controlled File Caching. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <pages> pages 165-178, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Research using caching and prefetching in database systems [10, 33, 11] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [13, 28, 20, 31, 42, 5, 18, 6, 36] </ref>. The most common prefetching approach is 6 to perform sequential readahead, i.e., to detect when an application accesses a file sequentially, and to prefetch in order the blocks of the files that are so used [13, 28, 29]. <p> Another body of work has been on predicting future access patterns [12, 42, 33, 11, 18]. A recent trend is to use applications' knowledge about their access patterns to perform more effective caching and prefetching <ref> [5, 6, 35, 36] </ref>. Patterson et al. [35] describe a mechanism by which an application's request sequence can be made known in advance. They use a hinting interface through which an application can be explicitly programmed to diclose its future file accesses to the file system. <p> The algorithms described above determine prefetching and caching schedules for a single application process. Cao et al. and Patterson et al. propose different policies to allocate cache resources to multiple, competing processes. Cao et al. <ref> [5, 6, 8] </ref> describe LRU-SP, which determines cache allocations based on those that would be obtained using the least recently used (LRU) replacement policy, applied globally to all processes' interleaved reference streams.
Reference: [7] <author> Pei Cao, Edward W. Felten, Anna Karlin, and Kai Li. </author> <title> A Study of Integrated Prefetching and Caching Strategies. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> pages , May </month> <year> 1995. </year>
Reference-contexts: Blocks A, C, E, and F reside on one disk; blocks b and d on a different disk. A straightforward approach is to use the aggressive algorithm <ref> [7] </ref>: always fetch the missing block that will be referenced soonest; evict the block whose next reference is furthest in the future; but do not fetch if the evicted block will be referenced before the fetched block. <p> However, these studies concentrated only on the problem of determining which blocks to fetch, and did not address the problem of determining which blocks to replace. This thesis builds on recent studies by Cao, Felten, Karlin, and Li of the single-disk prefetching and caching problem <ref> [7, 8] </ref>. They showed that it is important to integrate prefetching, caching and disk scheduling and that a properly integrated strategy can perform much better than a naive strategy, both theoretically and in practice. <p> Nonetheless, Cao et al. <ref> [7] </ref> were able to show that a simple and natural algorithm called aggressive, which prefetches as early as is reasonable, has performance that is provably close to optimal in the single disk case. <p> The first two are natural extensions of the two single disk prefetching strategies described by Cao et al. <ref> [7] </ref>. They lie at opposite ends of the spectrum in terms of the total number of fetches performed: Conservative performs the minimum possible number of fetches, at the expense of a worse elapsed time in the worst case; Aggressive prefetches as aggressively as possible without being foolish. <p> We give an intuitive explanation of reverse aggressive's advantages in Section 2.4.3. Detailed proofs are contained in Chapter 3. 2.3 Prefetching with a single disk Before proceeding, we review the results of Cao et al. <ref> [7] </ref> for prefetching and caching in the single-disk case. They described four properties that can be assumed of any optimal strategy in the single-disk case. <p> Full details are given in Section 3.2. 3.1.1 Performance of conservative, aggressive, fixed horizon, and forestall The key concept in the upper bound of Theorem 2 is the notion of domination from the work on prefetching and caching in the single-disk case <ref> [7] </ref>. This allows us to bound the cost of aggressive's prefetching schedule in terms of the progress of the optimal schedule at intermediate points during the processing of the request sequence. <p> Theorem 9 Reverse aggressive requires less than 1 + dF=K times the optimal elapsed time to service any request sequence, plus an additive term dF independent of the length of the sequence. Proof: For d = 1, the theorem follows directly from the result of <ref> [7] </ref>. Thus we may assume d 2. <p> i+1 T 0 Thus if we take time T i+1 to be T i + (T 0 i+1 T 0 i ) + dF 1, the invariants are restored. 2 3.2.3 Reverse aggressive: lower bound We have been unable to strengthen the lower bound of Cao, Felten, Karlin, and Li <ref> [7] </ref>, which showed that aggressive can perform (1 + (F 1)=K) times worse than optimal in the single-disk case. This bound applies directly to reverse aggressive, since there is no asymmetry between the reverse and forward problems in the single-disk case. <p> This result was referred to as the domination lemma <ref> [7] </ref>. The proof of this is similar to but simpler than that of Lemma 5 for algorithms working with the reverse sequence. <p> Table 4.8: Utilization of disks by forestall on the postgres-select trace. disks 1 2 3 4 5 6 util. .99 .92 .87 .81 .68 .63 disks 7 8 10 12 16 util. .62 .54 .39 .30 .22 Chapter 5 AN EXACT SOLUTION TO A RESTRICTED PROBLEM Cao et al. <ref> [7] </ref> studied the problem of integrated prefetching and caching with a single backing store. They left open the problem of finding an optimal schedule in time polynomial in both the input length and the cache size. We make partial progress on this problem in this chapter. <p> The set of holes resulting from the eviction at time i F dominates that resulting from the eviction at time i F 1, by the Domination Lemma of <ref> [7] </ref>. Thus c 0 i;0 is at least as good as c i;0 .
Reference: [8] <author> Pei Cao, Edward W. Felten, Anna Karlin, and Kai Li. </author> <title> Implementation and Performance of Integrated Application-Controlled Caching, Prefetching and Disk Scheduling. </title> <journal> ACM Transactions on Computer Systems (TOCS) 14(4) </journal> <pages> 311-343, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: However, these studies concentrated only on the problem of determining which blocks to fetch, and did not address the problem of determining which blocks to replace. This thesis builds on recent studies by Cao, Felten, Karlin, and Li of the single-disk prefetching and caching problem <ref> [7, 8] </ref>. They showed that it is important to integrate prefetching, caching and disk scheduling and that a properly integrated strategy can perform much better than a naive strategy, both theoretically and in practice. <p> The algorithms described above determine prefetching and caching schedules for a single application process. Cao et al. and Patterson et al. propose different policies to allocate cache resources to multiple, competing processes. Cao et al. <ref> [5, 6, 8] </ref> describe LRU-SP, which determines cache allocations based on those that would be obtained using the least recently used (LRU) replacement policy, applied globally to all processes' interleaved reference streams. <p> Prefetching and caching algorithms must deal effectively with missing or incorrect hints, as well as multiple simultaneously executing processes. Fixed horizon, aggressive and forestall can all be adapted to deal with these more general situations <ref> [8, 36] </ref>. 4.2 Simulation model Our theoretical model described and analyzed in Chapters 2 and 3 simplifies the real situation by assuming that the CPU time between every two file references is the same, that all disk accesses take the same amount of time, and that there is no CPU overhead <p> Chapter 6 INTEGRATING PREFETCHING WITH PROCESSOR AND DISK SCHEDULING Integrated prefetching and caching policies, augmented by mechanisms to allocate cache space among multiple processes, have been shown empirically to improve the performance of multi-programmed workloads <ref> [8, 36, 43] </ref>. These studies used standard multi-programming scheduling mechanisms to arbitrate the processes' competing prefetch requests and processing demands. The goal was to improve average I/O response time, leaving the scheduling policies unchanged.
Reference: [9] <author> P.M. Chen and D.A. Patterson. </author> <title> Maximizing Performance in a Striped Disk Array. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 322-331, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Much research on parallel I/O has concentrated on techniques for striping and distributing error-correction codes among redundant disk arrays or other devices. These techniques are used to achieve high bandwidth by exploiting parallelism and to tolerate failures <ref> [21, 39, 9, 34, 17] </ref>. For purposes of this thesis, striping will refer to a data layout in which block i of a file resides on disk (i mod d), where d is the number of disks. <p> This simulation models fine architectural details to provide a very accurate simulation of the HP 97560 disk drive. Table 4.2 lists several characteristics of the HP 97560 (taken from [38]). The CMU simulator uses the Berkeley RaidSim <ref> [9] </ref> simulator, as modified at CMU, to simulate 0661 IBM Lightning disk drives. The simulators were cross-validated on a common set of traces. The CMU simulator does not implement reverse aggressive. We obtained good agreement between the simulators on the results for aggressive and fixed horizon for several traces.
Reference: [10] <author> H.T. Chou and D.J. DeWitt. </author> <title> An Evaluation of Buffer Management Strategies for Relational Database Systems. </title> <booktitle> In Proceedings of the 19th International Conference on Very Large Data Bases, </booktitle> <pages> pages 127-141, </pages> <address> Dublin, Ireland, </address> <year> 1993. </year> <month> 157 </month>
Reference-contexts: In computer architecture, work on caching and prefetching has focused on bridging the performance gap between processors and main memory [41]. Research using caching and prefetching in database systems <ref> [10, 33, 11] </ref> showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems [13, 28, 20, 31, 42, 5, 18, 6, 36].
Reference: [11] <author> Kenneth M. Curewitz, P. Krishnan, and Jeffrey S. Vitter. </author> <title> Practical Prefetching via Data Compression. </title> <booktitle> In Proceedings of the 1993 ACM Conference on Management of Data (SIGMOD), </booktitle> <pages> pages 257-266, </pages> <address> Washington, DC, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In computer architecture, work on caching and prefetching has focused on bridging the performance gap between processors and main memory [41]. Research using caching and prefetching in database systems <ref> [10, 33, 11] </ref> showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems [13, 28, 20, 31, 42, 5, 18, 6, 36]. <p> The limitation of this approach is that it benefits only applications that make sequential references to large files. Recently, caching and prefetching have also been studied for parallel file systems [12, 25, 35]. Another body of work has been on predicting future access patterns <ref> [12, 42, 33, 11, 18] </ref>. A recent trend is to use applications' knowledge about their access patterns to perform more effective caching and prefetching [5, 6, 35, 36]. Patterson et al. [35] describe a mechanism by which an application's request sequence can be made known in advance.
Reference: [12] <author> Carla Schlatter Ellis and David Kotz. </author> <title> Prefetching in File System for MIMD Multiprocessors. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages 306-314, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: At the same time, it has been observed that many of these applications have largely predictable access patterns. This has enabled the use of prefetching and informed cache replacement (e.g., <ref> [12, 25, 35, 36] </ref>) as techniques for reducing I/O overhead in such systems. Typical disk drive response times are in the 5-30 millisecond range [38]. This large cache miss cost makes it worthwhile to spend a large amount of effort to avoid cache misses. <p> The limitation of this approach is that it benefits only applications that make sequential references to large files. Recently, caching and prefetching have also been studied for parallel file systems <ref> [12, 25, 35] </ref>. Another body of work has been on predicting future access patterns [12, 42, 33, 11, 18]. A recent trend is to use applications' knowledge about their access patterns to perform more effective caching and prefetching [5, 6, 35, 36]. <p> The limitation of this approach is that it benefits only applications that make sequential references to large files. Recently, caching and prefetching have also been studied for parallel file systems [12, 25, 35]. Another body of work has been on predicting future access patterns <ref> [12, 42, 33, 11, 18] </ref>. A recent trend is to use applications' knowledge about their access patterns to perform more effective caching and prefetching [5, 6, 35, 36]. Patterson et al. [35] describe a mechanism by which an application's request sequence can be made known in advance.
Reference: [13] <editor> R.J. Feiertag and E.I. Organisk. </editor> <booktitle> The Multics Input/Ouput System. In Proceedings of the 3rd Symposium on Operating Systems Principles, </booktitle> <pages> pages 35-41, </pages> <year> 1971. </year>
Reference-contexts: This idea has been applied at the level of the processor cache to avoid main memory accesses [37]. 1.4 Relation to previous work Caching and prefetching have been known techniques to improve performance of storage hierarchies for many years <ref> [4, 13] </ref>. In computer architecture, work on caching and prefetching has focused on bridging the performance gap between processors and main memory [41]. Research using caching and prefetching in database systems [10, 33, 11] showed that it is important to use applications' knowledge to perform caching and prefetching. <p> Research using caching and prefetching in database systems [10, 33, 11] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [13, 28, 20, 31, 42, 5, 18, 6, 36] </ref>. The most common prefetching approach is 6 to perform sequential readahead, i.e., to detect when an application accesses a file sequentially, and to prefetch in order the blocks of the files that are so used [13, 28, 29]. <p> The most common prefetching approach is 6 to perform sequential readahead, i.e., to detect when an application accesses a file sequentially, and to prefetch in order the blocks of the files that are so used <ref> [13, 28, 29] </ref>. The limitation of this approach is that it benefits only applications that make sequential references to large files. Recently, caching and prefetching have also been studied for parallel file systems [12, 25, 35].
Reference: [14] <author> Michael R. Garey and David S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. W.H. </title> <publisher> Freeman and Company, </publisher> <year> 1979, </year> <pages> pp. 199-200. </pages>
Reference-contexts: We reduce the Directed Hamiltonian Path problem to the sequence ordering problem. This problem remains NP-complete for graphs in which no vertex is incident to more than three edges <ref> [14] </ref>. A simple transformation allows us to assume that no vertex has outdegree or indegree greater than two. This will allow us to take the cache size to be a constant (K 4 is enough) rather than to depend on the input size.
Reference: [15] <author> Garth A. Gibson. </author> <type> Personal communication. </type>
Reference-contexts: Aggressive can lose its 1 F=K is typically less than 0.02 as described in Chapter 2. Small disk arrays with at most 5-10 disks are most common in practicem and are likely to continue to be so <ref> [15, 45] </ref>.
Reference: [16] <author> Jim Gray. </author> <title> The Benchmark Handbook. </title> <address> Morgan-Kaufman, San Mateo, CA. </address> <year> 1991. </year>
Reference-contexts: The relations are those used in the Wisconsin Benchmark <ref> [16] </ref>. Since the result relation is small, most of the file accesses are reads. Here, the index blocks are accessed much more frequently than the data blocks. postgres-select: the Postgres relational database system executing a selection query of choosing 2% of the tuples from an indexed 32MB relation. <p> Here, the index blocks are accessed much more frequently than the data blocks. postgres-select: the Postgres relational database system executing a selection query of choosing 2% of the tuples from an indexed 32MB relation. The selection query is part of the Wisconsin Benchmark suite <ref> [16] </ref> and uses indexed search. ld: the Ultrix link-editor, building the Ultrix 4.3 kernel from about 25MB of object files. xds: a 3-D data visualization program, XDataSlice, generating 25 planar slice images at random orientations from a 64MB data file.
Reference: [17] <author> James Gray and et al. </author> <title> Parity Striping of Disk Arrays: Low-Cost Reliable Storage with Acceptable Throughput. </title> <booktitle> In 16th International Conference on VLDB, </booktitle> <address> Australia, </address> <year> 1990. </year>
Reference-contexts: Much research on parallel I/O has concentrated on techniques for striping and distributing error-correction codes among redundant disk arrays or other devices. These techniques are used to achieve high bandwidth by exploiting parallelism and to tolerate failures <ref> [21, 39, 9, 34, 17] </ref>. For purposes of this thesis, striping will refer to a data layout in which block i of a file resides on disk (i mod d), where d is the number of disks.
Reference: [18] <author> Jim Griffioen and Randy Appleton. </author> <title> Reducing File System Latency using a Predictive Approach. </title> <booktitle> In USENIX Summer 1994 Technical Conference, </booktitle> <pages> pages 197-208, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Research using caching and prefetching in database systems [10, 33, 11] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [13, 28, 20, 31, 42, 5, 18, 6, 36] </ref>. The most common prefetching approach is 6 to perform sequential readahead, i.e., to detect when an application accesses a file sequentially, and to prefetch in order the blocks of the files that are so used [13, 28, 29]. <p> The limitation of this approach is that it benefits only applications that make sequential references to large files. Recently, caching and prefetching have also been studied for parallel file systems [12, 25, 35]. Another body of work has been on predicting future access patterns <ref> [12, 42, 33, 11, 18] </ref>. A recent trend is to use applications' knowledge about their access patterns to perform more effective caching and prefetching [5, 6, 35, 36]. Patterson et al. [35] describe a mechanism by which an application's request sequence can be made known in advance.
Reference: [19] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach, Second Edition. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: Moreover, technological trends are such that F=K will only get smaller with time: disk and memory speeds (which dominate F ) change slowly, while memory size increases exponentially <ref> [19] </ref>. 28 advantage, and lose time to opt, by prefetching more aggressively than opt; this will become clear as the details are presented.
Reference: [20] <author> John H. Howard, Michael Kazar, Sherri G. Menees, David A. Nichols, M. Satya-narayanan, Robert N. Sidebotham, and Michael J. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Research using caching and prefetching in database systems [10, 33, 11] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [13, 28, 20, 31, 42, 5, 18, 6, 36] </ref>. The most common prefetching approach is 6 to perform sequential readahead, i.e., to detect when an application accesses a file sequentially, and to prefetch in order the blocks of the files that are so used [13, 28, 29].
Reference: [21] <author> M. Kim. </author> <title> Synchronized Disk Interleaving. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 35(11) </volume> <pages> 978-988, </pages> <year> 1986. </year>
Reference-contexts: Much research on parallel I/O has concentrated on techniques for striping and distributing error-correction codes among redundant disk arrays or other devices. These techniques are used to achieve high bandwidth by exploiting parallelism and to tolerate failures <ref> [21, 39, 9, 34, 17] </ref>. For purposes of this thesis, striping will refer to a data layout in which block i of a file resides on disk (i mod d), where d is the number of disks.
Reference: [22] <author> Tracy Kimbrel, Pei Cao, Edward W. Felten, Anna Karlin, and Kai Li. </author> <title> Integrated Parallel Prefetching and Caching. </title> <institution> Princeton University Computer Science Department Tech Report TR-502-95, </institution> <month> November </month> <year> 1995. </year> <booktitle> Short version in 1996 ACM SIG-METRICS Conference on Measurement and Modeling of Computer Systems. </booktitle>
Reference-contexts: Thus we need to append the current cache contents to the (reversed) lookahead chunk before running reverse aggressive. An implementation of this approach was developed during the early stages of the simulations reported in Chapter 4 <ref> [22] </ref>. The modifications described to make the other algorithms deal with other forms of imperfect lookahead can be applied to reverse aggressive, now that we have a mechanism for producing a schedule given some amount of partial lookahead information.
Reference: [23] <author> Tracy Kimbrel and Anna R. Karlin. </author> <title> Near-optimal Parallel Prefetching and Caching. </title> <booktitle> In Proceedings of the 1996 IEEE Symposium on Foundations of Computer Science, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: The problem of finding an optimal solution is shown to be NP-hard. Chapter 8 summarizes the thesis and presents conclusions and directions for future study. Preliminary versions of the results presented in Chapter 3 were presented in <ref> [23] </ref>. The results presented in Chapter 4 appeared previously in [24]. Chapter 2 THE PARALLEL PREFETCHING AND CACHING PROBLEM In this chapter, we describe a theoretical model that captures the important characteristics of a system for prefetching and caching with multiple backing stores. <p> Chapter 3 THEORETICAL ANALYSIS 3.1 Overview of results This chapter presents the results of joint work with Anna Karlin <ref> [23] </ref>. All the algorithms are shown to perform nearly d times worse than optimal in the worst case, with the exception of reverse aggressive. Reverse aggressive is shown to perform within 1 + F=K of optimal in the worst case.
Reference: [24] <author> Tracy Kimbrel, Andrew Tomkins, R. Hugo Patterson, Brian Bershad, Pei Cao, Ed-ward W. Felten, Garth A. Gibson, Anna R. Karlin, and Kai Li. </author> <title> A Trace-Driven Comparison of Algorithms for Parallel Prefetching and Caching. </title> <booktitle> In Proceedings of 158 the ACM SIGOPS/USENIX Association Symposium on Operating System Design and Implementation (OSDI), </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: The problem of finding an optimal solution is shown to be NP-hard. Chapter 8 summarizes the thesis and presents conclusions and directions for future study. Preliminary versions of the results presented in Chapter 3 were presented in [23]. The results presented in Chapter 4 appeared previously in <ref> [24] </ref>. Chapter 2 THE PARALLEL PREFETCHING AND CACHING PROBLEM In this chapter, we describe a theoretical model that captures the important characteristics of a system for prefetching and caching with multiple backing stores. We also describe several prefetching and caching algorithms. <p> A trivial bound on T (n; m) is O (nm), yielding O (jRjjBj) for the running time of forestall. Chapter 4 EXPERIMENTAL ANALYSIS This chapter presents the results of joint work with Tomkins, Patterson, Bershad, Cao, Felten, Gibson, Karlin, and Li <ref> [24] </ref>. The presentation follows the chronological development of the results. An assessment was made of the practical algorithms aggressive and fixed horizon, using reverse aggressive as a benchmark against which to evaluate their performance.
Reference: [25] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Practical Prefetching Techniques for Multiprocessor File Systems. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 1(1) </volume> <pages> 33-51, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: At the same time, it has been observed that many of these applications have largely predictable access patterns. This has enabled the use of prefetching and informed cache replacement (e.g., <ref> [12, 25, 35, 36] </ref>) as techniques for reducing I/O overhead in such systems. Typical disk drive response times are in the 5-30 millisecond range [38]. This large cache miss cost makes it worthwhile to spend a large amount of effort to avoid cache misses. <p> The limitation of this approach is that it benefits only applications that make sequential references to large files. Recently, caching and prefetching have also been studied for parallel file systems <ref> [12, 25, 35] </ref>. Another body of work has been on predicting future access patterns [12, 42, 33, 11, 18]. A recent trend is to use applications' knowledge about their access patterns to perform more effective caching and prefetching [5, 6, 35, 36].
Reference: [26] <author> David Kotz, Song Bac Toh, and Sriram Radhakrishnan. </author> <title> A Detailed Simulation Model of the HP 97560 Disk Drive. </title> <type> Technical Report PCS-TR94-220, </type> <institution> Department of Computer Science, Datmouth College, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: However, the simulators do not model serialization of memory bus transactions. 2 Two separate simulators were developed, one at Washington (UW) and one at Carnegie Mellon (CMU). The UW simulator uses the disk drive simulation of Kotz et al. <ref> [26] </ref> (which is based on that of Ruemmler and Wilkes [38]) to accurately model I/O costs. This simulation models fine architectural details to provide a very accurate simulation of the HP 97560 disk drive. Table 4.2 lists several characteristics of the HP 97560 (taken from [38]).
Reference: [27] <author> P. Krishnan and Jeffrey S. Vitter. </author> <title> Optimal Prediction for Prefetching in the Worst Case. </title> <booktitle> In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA), </booktitle> <year> 1994. </year>
Reference-contexts: We know of no prior theoretical analysis of the integration of prefetching and caching in the presence of multiple disks. There have been some interesting results on the use of data compression for the design of optimal prefetching strategies <ref> [27, 44] </ref>, and work on prefetching strategies for external merging under a probabilistic model of request sequences [32]. However, these studies concentrated only on the problem of determining which blocks to fetch, and did not address the problem of determining which blocks to replace.
Reference: [28] <author> Marshall K. McKusick, William N. Joy, Samuel J. Le*er, and Robert S. Fabry. </author> <title> A Fast File System for UNIX. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(3) </volume> <pages> 181-197, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: Research using caching and prefetching in database systems [10, 33, 11] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [13, 28, 20, 31, 42, 5, 18, 6, 36] </ref>. The most common prefetching approach is 6 to perform sequential readahead, i.e., to detect when an application accesses a file sequentially, and to prefetch in order the blocks of the files that are so used [13, 28, 29]. <p> The most common prefetching approach is 6 to perform sequential readahead, i.e., to detect when an application accesses a file sequentially, and to prefetch in order the blocks of the files that are so used <ref> [13, 28, 29] </ref>. The limitation of this approach is that it benefits only applications that make sequential references to large files. Recently, caching and prefetching have also been studied for parallel file systems [12, 25, 35].
Reference: [29] <author> L. W. McVoy and S. R. Kleiman. </author> <title> Extent-like Performance from a UNIX File System. </title> <booktitle> In Proceedings of the 1991 Winter USENIX Conference, </booktitle> <pages> pages 33-43, </pages> <year> 1991. </year>
Reference-contexts: The most common prefetching approach is 6 to perform sequential readahead, i.e., to detect when an application accesses a file sequentially, and to prefetch in order the blocks of the files that are so used <ref> [13, 28, 29] </ref>. The limitation of this approach is that it benefits only applications that make sequential references to large files. Recently, caching and prefetching have also been studied for parallel file systems [12, 25, 35].
Reference: [30] <author> Todd C. Mowry, Angela K. Demke, and Orran Krieger. </author> <title> Automatic Compiler-inserted I/O Prefetching for Out-of-core Applications. </title> <booktitle> In Proceedings of the 1996 Symposium on Operating System Design and Implementation, </booktitle> <month> October </month> <year> 1996, </year> <pages> pp. 3-17. </pages>
Reference-contexts: Patterson et al. [35] describe a mechanism by which an application's request sequence can be made known in advance. They use a hinting interface through which an application can be explicitly programmed to diclose its future file accesses to the file system. Mowry et al. <ref> [30] </ref> use a different mechanism to make an application's demands known to the system. There, compiler techniques are applied to regularly-structured computations to predict applications' virtual memory page faults. <p> Another closely related line of research is the work of Patterson, Tomkins, Gibson, Ginting, Stodolski, and Zalenka [35, 36, 43]. Patterson et al. proposed the fixed horizon prefetching and caching algorithm [36]. Fixed horizon is described in detail in Chapter 2. Mowry et al. <ref> [30] </ref> do not separate the generation of lookahead information from its use, as do the other works mentioned. Their compiler inserts prefetch requests in the code it generates.
Reference: [31] <author> Michael N. Nelson, Brent B. Welch, and John K. Ousterhout. </author> <title> Caching in the Sprite File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Research using caching and prefetching in database systems [10, 33, 11] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [13, 28, 20, 31, 42, 5, 18, 6, 36] </ref>. The most common prefetching approach is 6 to perform sequential readahead, i.e., to detect when an application accesses a file sequentially, and to prefetch in order the blocks of the files that are so used [13, 28, 29].
Reference: [32] <author> Vinay S. Pai, Alejandro A. Schaffer, and Peter J. Varman. </author> <title> Markov Analysis of Multiple-Disk Prefetching Strategies for External Merging. </title> <booktitle> Theoretical Computer Science, v. </booktitle> <volume> 128, </volume> <year> 1994. </year>
Reference-contexts: There have been some interesting results on the use of data compression for the design of optimal prefetching strategies [27, 44], and work on prefetching strategies for external merging under a probabilistic model of request sequences <ref> [32] </ref>. However, these studies concentrated only on the problem of determining which blocks to fetch, and did not address the problem of determining which blocks to replace. This thesis builds on recent studies by Cao, Felten, Karlin, and Li of the single-disk prefetching and caching problem [7, 8].
Reference: [33] <author> Mark Palmer and Stanley B. Zdonik. </author> <title> Fido: A Cache That Learns to Fetch. </title> <booktitle> In Proceedings of the 17th International Conference on Very Large Data Bases, </booktitle> <pages> pages 255-264, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: In computer architecture, work on caching and prefetching has focused on bridging the performance gap between processors and main memory [41]. Research using caching and prefetching in database systems <ref> [10, 33, 11] </ref> showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems [13, 28, 20, 31, 42, 5, 18, 6, 36]. <p> The limitation of this approach is that it benefits only applications that make sequential references to large files. Recently, caching and prefetching have also been studied for parallel file systems [12, 25, 35]. Another body of work has been on predicting future access patterns <ref> [12, 42, 33, 11, 18] </ref>. A recent trend is to use applications' knowledge about their access patterns to perform more effective caching and prefetching [5, 6, 35, 36]. Patterson et al. [35] describe a mechanism by which an application's request sequence can be made known in advance.
Reference: [34] <author> D.A. Patterson, G. Gibson, and R.H. Katz. </author> <title> A Case for Redundant Arrays for Inexpensive Disks (RAID). </title> <booktitle> In Proceedings of ACM SIGMOD Conference, </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Much research on parallel I/O has concentrated on techniques for striping and distributing error-correction codes among redundant disk arrays or other devices. These techniques are used to achieve high bandwidth by exploiting parallelism and to tolerate failures <ref> [21, 39, 9, 34, 17] </ref>. For purposes of this thesis, striping will refer to a data layout in which block i of a file resides on disk (i mod d), where d is the number of disks.
Reference: [35] <author> R. Hugo Patterson and Garth A. Gibson. </author> <title> Exposing I/O Concurrency with Informed Prefetching. </title> <booktitle> In Proceedings of the Third International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 7-16, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: At the same time, it has been observed that many of these applications have largely predictable access patterns. This has enabled the use of prefetching and informed cache replacement (e.g., <ref> [12, 25, 35, 36] </ref>) as techniques for reducing I/O overhead in such systems. Typical disk drive response times are in the 5-30 millisecond range [38]. This large cache miss cost makes it worthwhile to spend a large amount of effort to avoid cache misses. <p> The limitation of this approach is that it benefits only applications that make sequential references to large files. Recently, caching and prefetching have also been studied for parallel file systems <ref> [12, 25, 35] </ref>. Another body of work has been on predicting future access patterns [12, 42, 33, 11, 18]. A recent trend is to use applications' knowledge about their access patterns to perform more effective caching and prefetching [5, 6, 35, 36]. <p> Another body of work has been on predicting future access patterns [12, 42, 33, 11, 18]. A recent trend is to use applications' knowledge about their access patterns to perform more effective caching and prefetching <ref> [5, 6, 35, 36] </ref>. Patterson et al. [35] describe a mechanism by which an application's request sequence can be made known in advance. They use a hinting interface through which an application can be explicitly programmed to diclose its future file accesses to the file system. <p> Another body of work has been on predicting future access patterns [12, 42, 33, 11, 18]. A recent trend is to use applications' knowledge about their access patterns to perform more effective caching and prefetching [5, 6, 35, 36]. Patterson et al. <ref> [35] </ref> describe a mechanism by which an application's request sequence can be made known in advance. They use a hinting interface through which an application can be explicitly programmed to diclose its future file accesses to the file system. <p> Cao et al. proposed the aggressive prefetching and caching algorithm, which is described in detail in Chapter 2. Another closely related line of research is the work of Patterson, Tomkins, Gibson, Ginting, Stodolski, and Zalenka <ref> [35, 36, 43] </ref>. Patterson et al. proposed the fixed horizon prefetching and caching algorithm [36]. Fixed horizon is described in detail in Chapter 2. Mowry et al. [30] do not separate the generation of lookahead information from its use, as do the other works mentioned.
Reference: [36] <author> R.H. Patterson, G.A. Gibson, E. Ginting, D. Stodolsky, and J. Zelenka. </author> <title> Informed Prefetching and Caching. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year> <month> 159 </month>
Reference-contexts: At the same time, it has been observed that many of these applications have largely predictable access patterns. This has enabled the use of prefetching and informed cache replacement (e.g., <ref> [12, 25, 35, 36] </ref>) as techniques for reducing I/O overhead in such systems. Typical disk drive response times are in the 5-30 millisecond range [38]. This large cache miss cost makes it worthwhile to spend a large amount of effort to avoid cache misses. <p> Research using caching and prefetching in database systems [10, 33, 11] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [13, 28, 20, 31, 42, 5, 18, 6, 36] </ref>. The most common prefetching approach is 6 to perform sequential readahead, i.e., to detect when an application accesses a file sequentially, and to prefetch in order the blocks of the files that are so used [13, 28, 29]. <p> Another body of work has been on predicting future access patterns [12, 42, 33, 11, 18]. A recent trend is to use applications' knowledge about their access patterns to perform more effective caching and prefetching <ref> [5, 6, 35, 36] </ref>. Patterson et al. [35] describe a mechanism by which an application's request sequence can be made known in advance. They use a hinting interface through which an application can be explicitly programmed to diclose its future file accesses to the file system. <p> Cao et al. proposed the aggressive prefetching and caching algorithm, which is described in detail in Chapter 2. Another closely related line of research is the work of Patterson, Tomkins, Gibson, Ginting, Stodolski, and Zalenka <ref> [35, 36, 43] </ref>. Patterson et al. proposed the fixed horizon prefetching and caching algorithm [36]. Fixed horizon is described in detail in Chapter 2. Mowry et al. [30] do not separate the generation of lookahead information from its use, as do the other works mentioned. <p> Cao et al. proposed the aggressive prefetching and caching algorithm, which is described in detail in Chapter 2. Another closely related line of research is the work of Patterson, Tomkins, Gibson, Ginting, Stodolski, and Zalenka [35, 36, 43]. Patterson et al. proposed the fixed horizon prefetching and caching algorithm <ref> [36] </ref>. Fixed horizon is described in detail in Chapter 2. Mowry et al. [30] do not separate the generation of lookahead information from its use, as do the other works mentioned. Their compiler inserts prefetch requests in the code it generates. <p> Cao et al. [5, 6, 8] describe LRU-SP, which determines cache allocations based on those that would be obtained using the least recently used (LRU) replacement policy, applied globally to all processes' interleaved reference streams. The cost-benefit analysis of Patterson et al. <ref> [36] </ref> compares the cost of one process giving up a cache buffer to the benefit of reallocating that buffer to another process. These are measured in terms of the time saved or lost by a process divided by the amount of time the buffer is used or given up. <p> Implementations of aggressive, fixed horizon, and forestall that incorporate these features have been developed by Patterson, Tomkins, et al. <ref> [36, 43] </ref>. Modifying reverse aggressive is more complicated. Before describing an implementation that copes with imperfect lookahead, we describe a simple trick needed to implement the algorithm even in the case of perfect lookahead. <p> Prefetching and caching algorithms must deal effectively with missing or incorrect hints, as well as multiple simultaneously executing processes. Fixed horizon, aggressive and forestall can all be adapted to deal with these more general situations <ref> [8, 36] </ref>. 4.2 Simulation model Our theoretical model described and analyzed in Chapters 2 and 3 simplifies the real situation by assuming that the CPU time between every two file references is the same, that all disk accesses take the same amount of time, and that there is no CPU overhead <p> Assuming an average disk response time of 15ms (which is usually an overestimate in our simulations) and 243s to read a block from the cache (which was measured on the implemented TIP2 system of Patterson et al. <ref> [36] </ref>) yields a value of H = 62; we used this value in all our simulations, except where noted otherwise. 4.3 Implementations of the algorithms In the context of the considerations of the previous section, we summarize the implementations we compared. <p> Chapter 6 INTEGRATING PREFETCHING WITH PROCESSOR AND DISK SCHEDULING Integrated prefetching and caching policies, augmented by mechanisms to allocate cache space among multiple processes, have been shown empirically to improve the performance of multi-programmed workloads <ref> [8, 36, 43] </ref>. These studies used standard multi-programming scheduling mechanisms to arbitrate the processes' competing prefetch requests and processing demands. The goal was to improve average I/O response time, leaving the scheduling policies unchanged.
Reference: [37] <author> James Philbin, Jan Edler, Otto J. Anshus, Craig C. Douglas, and Kai Li. </author> <title> Thread Scheduling for Cache Locality. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 60-71, </pages> <month> October, </month> <year> 1996. </year>
Reference-contexts: The problem considered in Chapter 7 is one of increasing locality of reference by carefully constructing request sequences. This idea has been applied at the level of the processor cache to avoid main memory accesses <ref> [37] </ref>. 1.4 Relation to previous work Caching and prefetching have been known techniques to improve performance of storage hierarchies for many years [4, 13]. In computer architecture, work on caching and prefetching has focused on bridging the performance gap between processors and main memory [41]. <p> Chapter 7 describes a mechanism that has been proposed previously by which performance can be improved by increasing a program's locality of reference <ref> [37] </ref>. The problem of finding an optimal solution is shown to be NP-hard. Chapter 8 summarizes the thesis and presents conclusions and directions for future study. Preliminary versions of the results presented in Chapter 3 were presented in [23]. The results presented in Chapter 4 appeared previously in [24]. <p> Chapter 7 HARDNESS OF ORDERING REQUEST SEQUENCES TO MINIMIZE CACHE MISSES In this chapter, we consider a generalization of the classic paging problem raised by Philbin et al. <ref> [37] </ref>. Suppose we are given m distinct sequences of references, and must process the sequences in succession. That is, each sequence must be served in its entirety before serving another sequence. However, the ordering of the individual sequences is arbitrary. <p> The development of approximation algorithms for this problem and/or lower bounds on its approximability are very interesting problems, as is that of determining bounds on the quality of the approximation produced by the heuristic of Philbin et al. <ref> [37] </ref>.
Reference: [38] <author> Chris Ruemmler and John Wilkes. </author> <title> An Introduction to Disk Drive Modelling. </title> <journal> In IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-28, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: This has enabled the use of prefetching and informed cache replacement (e.g., [12, 25, 35, 36]) as techniques for reducing I/O overhead in such systems. Typical disk drive response times are in the 5-30 millisecond range <ref> [38] </ref>. This large cache miss cost makes it worthwhile to spend a large amount of effort to avoid cache misses. The techniques in this thesis require nontrivial amounts of computation to make available and maintain lookahead information and its relation to the cache state. <p> However, the simulators do not model serialization of memory bus transactions. 2 Two separate simulators were developed, one at Washington (UW) and one at Carnegie Mellon (CMU). The UW simulator uses the disk drive simulation of Kotz et al. [26] (which is based on that of Ruemmler and Wilkes <ref> [38] </ref>) to accurately model I/O costs. This simulation models fine architectural details to provide a very accurate simulation of the HP 97560 disk drive. Table 4.2 lists several characteristics of the HP 97560 (taken from [38]). <p> simulation of Kotz et al. [26] (which is based on that of Ruemmler and Wilkes <ref> [38] </ref>) to accurately model I/O costs. This simulation models fine architectural details to provide a very accurate simulation of the HP 97560 disk drive. Table 4.2 lists several characteristics of the HP 97560 (taken from [38]). The CMU simulator uses the Berkeley RaidSim [9] simulator, as modified at CMU, to simulate 0661 IBM Lightning disk drives. The simulators were cross-validated on a common set of traces. The CMU simulator does not implement reverse aggressive.
Reference: [39] <author> K. Salem and H. Garcia-Molina. </author> <title> Disk Striping. </title> <booktitle> In the 2nd IEEE Conference on Data Engineering, </booktitle> <year> 1986. </year>
Reference-contexts: Much research on parallel I/O has concentrated on techniques for striping and distributing error-correction codes among redundant disk arrays or other devices. These techniques are used to achieve high bandwidth by exploiting parallelism and to tolerate failures <ref> [21, 39, 9, 34, 17] </ref>. For purposes of this thesis, striping will refer to a data layout in which block i of a file resides on disk (i mod d), where d is the number of disks.
Reference: [40] <author> Abraham Silberschatz and Peter B. Galvin. </author> <title> Operating System Concepts, Fourth Edition. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1994, </year> <pages> pp. 312-325. </pages>
Reference-contexts: In addition, the design and analysis of an optimal o*ine algorithm is an important step towards understanding and evaluating more practical limited-lookahead algorithms. We can perhaps draw an analogy with the impact of the optimal o*ine paging algorithm [4] on the design, implementation and evaluation of online paging algorithms <ref> [40] </ref>. Our model is read-only. The algorithms we consider can improve the performance of read-only and read-mostly applications (i.e., those for which the performance impact of write traffic is negligible). An interesting open problem is the integration of the algorithms considered here with techniques to improve write performance. <p> Because of the significance of the disk scheduling effect, we modify the definitions of aggressive, reverse aggressive, and forestall to submit disk requests in 62 batches. We have found that the performance of all the algorithms benefits from the CSCAN disk scheduling algorithm (see, for example, <ref> [40] </ref>). Reverse aggressive also benefits from batching of requests during its construction of its prefetching schedule (the reverse pass over the request sequence).
Reference: [41] <author> Alan J. Smith. </author> <title> Second Bibliography on Cache Memories. </title> <journal> Computer Architecture News, </journal> <volume> 19(4) </volume> <pages> 154-182, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In computer architecture, work on caching and prefetching has focused on bridging the performance gap between processors and main memory <ref> [41] </ref>. Research using caching and prefetching in database systems [10, 33, 11] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems [13, 28, 20, 31, 42, 5, 18, 6, 36].
Reference: [42] <author> C. Tait and D. Duchamp. </author> <title> Service Interface and Replica Management Algorithm for Mobile File System Clients. </title> <booktitle> In Proceedings of Parallel and Distributed Information Systems, </booktitle> <pages> pages 190-196. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: Research using caching and prefetching in database systems [10, 33, 11] showed that it is important to use applications' knowledge to perform caching and prefetching. File caching and prefetching have become standard techniques for sequential file systems <ref> [13, 28, 20, 31, 42, 5, 18, 6, 36] </ref>. The most common prefetching approach is 6 to perform sequential readahead, i.e., to detect when an application accesses a file sequentially, and to prefetch in order the blocks of the files that are so used [13, 28, 29]. <p> The limitation of this approach is that it benefits only applications that make sequential references to large files. Recently, caching and prefetching have also been studied for parallel file systems [12, 25, 35]. Another body of work has been on predicting future access patterns <ref> [12, 42, 33, 11, 18] </ref>. A recent trend is to use applications' knowledge about their access patterns to perform more effective caching and prefetching [5, 6, 35, 36]. Patterson et al. [35] describe a mechanism by which an application's request sequence can be made known in advance.
Reference: [43] <author> Andrew Tomkins, R. Hugo Patterson, and Garth A. Gibson. </author> <title> Informed Multi-process Prefetching and Caching. </title> <booktitle> In Proceedings of the 1997 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Cao et al. proposed the aggressive prefetching and caching algorithm, which is described in detail in Chapter 2. Another closely related line of research is the work of Patterson, Tomkins, Gibson, Ginting, Stodolski, and Zalenka <ref> [35, 36, 43] </ref>. Patterson et al. proposed the fixed horizon prefetching and caching algorithm [36]. Fixed horizon is described in detail in Chapter 2. Mowry et al. [30] do not separate the generation of lookahead information from its use, as do the other works mentioned. <p> Tomkins et al. have gone on to evaluate forestall experimentally in conjunction with each of the cost-benefit and LRU-SP allocation mechanisms, as well as aggressive in conjunction with LRU-SP and fixed horizon in conjunction with cost-benefit <ref> [43] </ref> . In addition to the single-process parallel prefetching and caching problem, this thesis also addresses three related issues. The first is the development of an efficient algorithm to find optimal prefetching and caching schedules. <p> Implementations of aggressive, fixed horizon, and forestall that incorporate these features have been developed by Patterson, Tomkins, et al. <ref> [36, 43] </ref>. Modifying reverse aggressive is more complicated. Before describing an implementation that copes with imperfect lookahead, we describe a simple trick needed to implement the algorithm even in the case of perfect lookahead. <p> Chapter 6 INTEGRATING PREFETCHING WITH PROCESSOR AND DISK SCHEDULING Integrated prefetching and caching policies, augmented by mechanisms to allocate cache space among multiple processes, have been shown empirically to improve the performance of multi-programmed workloads <ref> [8, 36, 43] </ref>. These studies used standard multi-programming scheduling mechanisms to arbitrate the processes' competing prefetch requests and processing demands. The goal was to improve average I/O response time, leaving the scheduling policies unchanged.
Reference: [44] <author> Jeffrey S. Vitter and P. Krishnan. </author> <title> Optimal Prefetching via Data Compression. </title> <booktitle> In Proceedings of the IEEE Symposium on Foundations of Computer Science (FOCS), </booktitle> <pages> pages 121-130, </pages> <year> 1991. </year>
Reference-contexts: We know of no prior theoretical analysis of the integration of prefetching and caching in the presence of multiple disks. There have been some interesting results on the use of data compression for the design of optimal prefetching strategies <ref> [27, 44] </ref>, and work on prefetching strategies for external merging under a probabilistic model of request sequences [32]. However, these studies concentrated only on the problem of determining which blocks to fetch, and did not address the problem of determining which blocks to replace.
Reference: [45] <author> John Wilkes. </author> <type> Personal communication. </type>
Reference-contexts: Aggressive can lose its 1 F=K is typically less than 0.02 as described in Chapter 2. Small disk arrays with at most 5-10 disks are most common in practicem and are likely to continue to be so <ref> [15, 45] </ref>.
References-found: 45

