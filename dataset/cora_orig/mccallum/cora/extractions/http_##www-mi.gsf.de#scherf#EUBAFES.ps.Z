URL: http://www-mi.gsf.de/scherf/EUBAFES.ps.Z
Refering-URL: http://wwwbrauer.informatik.tu-muenchen.de/~scherf/
Root-URL: 
Email: e-mail: scherf@gsf.de  e-mail: brauer@informatik.tu-muenchen.de  
Phone: 2  
Title: Feature Selection by Means of a Feature Weighting Approach  
Author: M. Scherf W. Brauer 
Address: D-85754 Neuherberg,  D-80290 Munchen,  
Affiliation: 1 GSF National Research Center for Environment and Health, medis Institute  Institut fur Informatik Technische Universitat Munchen,  Forschungsberichte Kunstliche Intelligenz, Institut fur Informatik, Technische Universitat Munchen  
Pubnum: Technical Report No. FKI-221-97,  
Abstract: Selecting a set of features which is optimal for a given classification task is one of the central problems in machine learning. We address the problem using the flexible and robust filter technique EUBAFES. EUBAFES is based on a feature weighting approach which computes binary feature weights and therefore a solution in the feature selection sense and also gives detailed information about feature relevance by continuous weights. Moreover the user gets not only one but several potentially optimal feature subsets which is important for filter-based feature selection algorithms since it gives the flexibility to use even complex classifiers by the application of a combined filter/wrapper approach. We applied EUBAFES on a number of artificial and real world data sets and used radial basis function networks to examine the impact of the feature subsets to classifier accuracy and complexity.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Bankert R.L. Aha D.W. </author> <title> Feature selection for case-based classification of cloud types: An empirical comparison. </title> <booktitle> In Proceedings of the 1994 AAAI Workshop on Case-Based Reasoning, </booktitle> <pages> pages 106-112, </pages> <year> 1994. </year>
Reference-contexts: Therefore we propose one of the following standardization techniques as data pre-processing step: - <ref> [0; 1] </ref> Scaling Let x min q ); i = 1; 2; :::; N and x max q ); i = 1; 2; :::; N . <p> Regularization function. function causes a deformation of the criterion-function shape. The degree of deformation is controlled by parameter . The first derivative of the extended criterion function is defined as: @ ~ J = @w q Consequently, supposing the restriction of the feature weights to the hypercube <ref> [0; 1] </ref> Q , i.e. w q 2 [0; 1], the additional function causes a shift of the local minimum from the edges to the center of the hypercube. <p> The degree of deformation is controlled by parameter . The first derivative of the extended criterion function is defined as: @ ~ J = @w q Consequently, supposing the restriction of the feature weights to the hypercube <ref> [0; 1] </ref> Q , i.e. w q 2 [0; 1], the additional function causes a shift of the local minimum from the edges to the center of the hypercube.
Reference: 2. <author> Bishop C.M. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1996. </year>
Reference-contexts: The scaled feature values are given by: ~x n x n q q x min (20) Linear scaling This scaling method is introduced in <ref> [2] </ref>. Let x q = N n=1 q (21) be the mean of the q'th feature value and 2 1 N X (x n its variance.
Reference: 3. <author> Breiman L. Friedman J.M. Olshen R.A. Stone C.J. </author> <title> Classification and regression trees. </title> <booktitle> Wadsworth International Group: </booktitle> <address> Belmont, California, </address> <year> 1984. </year>
Reference-contexts: RELIEF-F are able to detect those features necessary to describe a given target concept. The necessary features were known a priory in every case. In the following sections we describe the data sets and compare the results of EUBAFES and RELIEF-F. Waveform-40 The Waveform-40 data set was introduced in <ref> [3] </ref> and applied in [25] to examine how well a feature selection algorithm works in the presence of a high number of irrelevant features. The data set enfolds 300 instances with every instance assigned to one of three classes. An instance X i is defined by 40 continuous feature values.
Reference: 4. <author> Cardie C. </author> <title> Using decision trees to improve case-based learning. </title> <booktitle> Proceedings of the tenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 25-32, </pages> <year> 1993. </year>
Reference-contexts: Therefore our approach belongs to the group of filter approaches. Further filter approaches are the FOCUS algorithm [7,8], the Decision Tree Based approach by Cardie <ref> [4] </ref> and the LVF algorithm [20].
Reference: 5. <author> Creecy R.H. Masand B.M. Smith S.J. </author> <title> Waltz D.J. Trading mips and memory for knowledge engeneering. </title> <journal> Communications of the ACM, </journal> <volume> 35, </volume> <pages> 48-64, </pages> <year> 1992. </year>
Reference-contexts: They distinguish between approaches, based on conditional probabilities like PCF <ref> [5] </ref>, class projection like the value-difference metric (VDM) [26] and mutual information, introduced by Shannon [21] to assign continuous weights to features, giving a measure of relevance. Nevertheless none of the feature weighting approaches is able to give automatically a set of binary feature weights.
Reference: 6. <author> Diamond J. </author> <title> Berthold M.R. Boosting the performance of rbf networks with dynamic decay adjustment. </title> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <address> vol.7, </address> <year> 1995. </year>
Reference-contexts: The features, necessary do describe a given target concept were all known a priori in the case of artificial data sets. The real world data sets were drawn from the UCI Repository of machine learning [17]. Table 1 gives an overview. We used a RBF-DDA network <ref> [6] </ref> throughout our experiments with real world data sets.
Reference: 7. <author> Dietterich T.G. Almuallim A. </author> <title> Learning with many irrelevant features. </title> <booktitle> AAAI-91 proceedings of the 9th National Conference on Artificial Inteligence, </booktitle> <year> 1991. </year>
Reference: 8. <author> Dietterich T.G. Almuallim A. </author> <title> Learning boolean concepts in the presence of many irrelevant features. </title> <booktitle> Artificial Intelligence 69(1-2), </booktitle> <pages> 279-306, </pages> <year> 1994. </year>
Reference: 9. <author> Freitag D. Caruana R. </author> <title> Greedy attribute selection. </title> <editor> In Haym Hirsh William W. Cohne, editor, </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 28-36, </pages> <year> 1994. </year>
Reference-contexts: This could be a problem if features have nearly equal feature weights. Are all of them relevant or irrelevant ? Or is just a subset of them relevant ? The difficulty of determining a convenient threshold value is also reported in <ref> [9] </ref>. In [18] and [12], RELIEF is yet described as a method which is only able to detect features which are either strongly or weakly relevant since the authors propose to choose = 0.
Reference: 10. <author> Fukunaga K. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> ACADEMIC PRESS INC, Harcourt Brace Jovanovich, </publisher> <address> Boston San Diego New York London Sydney Tokyo Toronto, </address> <year> 1990. </year>
Reference-contexts: Our result is also confirmed by the donor of the bupa data set, who states, that "drinks &gt; 5 is some sort of a selector on this data base". 7 Related Work The feature subset selection problem has been investigated for decades by researchers in pattern recognition [13] and statistics <ref> [10] </ref>, with concentration mainly on subset selection using linear regression and with assumptions that do not apply to most learning algorithms [18]. In the last years, feature selection has received considerable attention by researchers from artificial intelligence and knowledge discovery.
Reference: 11. <author> John G. Kohavi R. </author> <title> Feature subset selection using the wrapper model: Overfitting and dynamic search space topology. </title> <booktitle> Proceedings of the First International Conference on Knowledge Discovery and Data Mining, </booktitle> <year> 1995. </year>
Reference-contexts: This would be a hard problem if features have approximately the same feature weights are all of these features relevant or just a subset of them ? For applying RELIEF-F, [18], <ref> [11] </ref> and [12] propose the selection of features with negative weights, which would lead to the selection of all features in every of our real world examples and therefore to suboptimal results.
Reference: 12. <author> John G. Kohavi R. </author> <title> Wrappers for feature subset selection. </title> <note> AIJ issue on relevance (to appear), </note> <year> 1995. </year>
Reference-contexts: A serious disadvantage of this optimization technique is, that every optimization step is based on a small number of distances, thus the algorithm have to be run for an infinite amount of time to avoid suboptimal local minima of the criterion function. This assumption is confirmed by <ref> [12] </ref> who found significant variance due to the choice of a small number of nearest hits and misses in the rankings given by RELIEF. They propose to use all instances and all nearest hits and misses of each instance. <p> This could be a problem if features have nearly equal feature weights. Are all of them relevant or irrelevant ? Or is just a subset of them relevant ? The difficulty of determining a convenient threshold value is also reported in [9]. In [18] and <ref> [12] </ref>, RELIEF is yet described as a method which is only able to detect features which are either strongly or weakly relevant since the authors propose to choose = 0. <p> strategies in artificial intelligence, we will first give a brief categorization: As already stated in the introduction of our paper, feature selection approaches can be grouped into two categories: Wrapper or filter methods [18], where filter methods "attempt to asses the merits of features from data, ignoring the induction algorithm" <ref> [12] </ref> whereas wrapper methods use the accuracy of a certain classifier as criterion function. In [12], RELIEF-F is categorized as filter approach, which from our point of view is discussible according to their definition of the filter approach, because RELIEF-F as well as EUBAFES have a criterion function which is related <p> the introduction of our paper, feature selection approaches can be grouped into two categories: Wrapper or filter methods [18], where filter methods "attempt to asses the merits of features from data, ignoring the induction algorithm" <ref> [12] </ref> whereas wrapper methods use the accuracy of a certain classifier as criterion function. In [12], RELIEF-F is categorized as filter approach, which from our point of view is discussible according to their definition of the filter approach, because RELIEF-F as well as EUBAFES have a criterion function which is related to the k-nearest neighbor classifier. <p> This would be a hard problem if features have approximately the same feature weights are all of these features relevant or just a subset of them ? For applying RELIEF-F, [18], [11] and <ref> [12] </ref> propose the selection of features with negative weights, which would lead to the selection of all features in every of our real world examples and therefore to suboptimal results.
Reference: 13. <author> Kittler J. Devijever P. </author> <title> Pattern Recognition: A Statistical Approach. </title> <address> Prentice/Hall, </address> <year> 1982. </year>
Reference-contexts: Our result is also confirmed by the donor of the bupa data set, who states, that "drinks &gt; 5 is some sort of a selector on this data base". 7 Related Work The feature subset selection problem has been investigated for decades by researchers in pattern recognition <ref> [13] </ref> and statistics [10], with concentration mainly on subset selection using linear regression and with assumptions that do not apply to most learning algorithms [18]. In the last years, feature selection has received considerable attention by researchers from artificial intelligence and knowledge discovery.
Reference: 14. <author> Kononenko I. </author> <title> Estimation attributes: Analysis and extensions of relief. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, Catana, Italy, </booktitle> <pages> pages 171-182. </pages> <publisher> Springer Verlag, </publisher> <year> 1994. </year>
Reference-contexts: RELIEF was originally introduced by [19] and extended to RELIEF-F for handling noisy, incomplete and multi-class data sets by <ref> [14] </ref>. Analog to our method, RELIEF pursues the reinforcement of similarities between instances in the same class and deterioration of similarities between instances in different classes by a feature weighting approach. RELIEF differs from our approach concerning the similarity metric, optimization strategy and the determination of feature subsets.
Reference: 15. <author> Lee M.S. Moore A.W. </author> <title> Efficient algorithms for minimizing cross validation error. </title> <editor> In William W. Cohne and Haym Hirsh, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 190-198, </pages> <year> 1994. </year>
Reference: 16. <author> Moody J. Darken C.J. </author> <title> Fast learning in networks with locally-tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 281-294, </pages> <year> 1989. </year>
Reference: 17. <author> Murphy P. </author> <title> Uci repository of machine learning databases. </title> <address> Irvine, CA: </address> <institution> University of California. Department of Information and Computer Science, </institution> <year> 1995. </year>
Reference-contexts: The features, necessary do describe a given target concept were all known a priori in the case of artificial data sets. The real world data sets were drawn from the UCI Repository of machine learning <ref> [17] </ref>. Table 1 gives an overview. We used a RBF-DDA network [6] throughout our experiments with real world data sets. <p> RELIEF-F did not find any relevant features in any of the monk-data bases with any number of k-nearest neighbors. 6.3 Real World Data Sets We applied EUBAFES and RELIEF-F on real world data sets from the UCI Repository of machine learning <ref> [17] </ref>. In every experiment we applied EUBAFES in a crossvalidation manner, i.e. we partitioned every data set in ten segments, containing all except ten randomly selected instances and applied EUBAFES to every subset. <p> A general demand in context with our work would be the composition of benchmark data sets for feature selection methods. It is very hard to find appropriate examples, showing the advantages or disadvantages of a method. The examples of the UCI Repository of machine learning data bases <ref> [17] </ref> are often not applicative, because they already contain optimal feature subsets.
Reference: 18. <author> Pfleger K. John G., Kohavi R. </author> <title> Irrelevant features and the subset selection problem. </title> <editor> In William W. Cohne and Haym Hirsh, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 121-129, </pages> <year> 1994. </year>
Reference-contexts: This could be a problem if features have nearly equal feature weights. Are all of them relevant or irrelevant ? Or is just a subset of them relevant ? The difficulty of determining a convenient threshold value is also reported in [9]. In <ref> [18] </ref> and [12], RELIEF is yet described as a method which is only able to detect features which are either strongly or weakly relevant since the authors propose to choose = 0. <p> sort of a selector on this data base". 7 Related Work The feature subset selection problem has been investigated for decades by researchers in pattern recognition [13] and statistics [10], with concentration mainly on subset selection using linear regression and with assumptions that do not apply to most learning algorithms <ref> [18] </ref>. In the last years, feature selection has received considerable attention by researchers from artificial intelligence and knowledge discovery. <p> To give a comparison of our approach with existing feature selection strategies in artificial intelligence, we will first give a brief categorization: As already stated in the introduction of our paper, feature selection approaches can be grouped into two categories: Wrapper or filter methods <ref> [18] </ref>, where filter methods "attempt to asses the merits of features from data, ignoring the induction algorithm" [12] whereas wrapper methods use the accuracy of a certain classifier as criterion function. <p> This would be a hard problem if features have approximately the same feature weights are all of these features relevant or just a subset of them ? For applying RELIEF-F, <ref> [18] </ref>, [11] and [12] propose the selection of features with negative weights, which would lead to the selection of all features in every of our real world examples and therefore to suboptimal results.
Reference: 19. <author> Rendell L. Kira K. </author> <title> A practical approach to feature selection. </title> <editor> In Edwards P. Sleeman D., editor, </editor> <booktitle> Proceedings of the International Conference on Machine Learning, Aberdeen, </booktitle> <pages> pages 249-256. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: In contrast to this a flat gradient indicates, that a feature has weak discrimination properties. 5 RELIEF A Related Feature Weighting Approach In this section we briefly introduce RELIEF, a feature weighting technique which is related to EUBAFES. RELIEF was originally introduced by <ref> [19] </ref> and extended to RELIEF-F for handling noisy, incomplete and multi-class data sets by [14]. Analog to our method, RELIEF pursues the reinforcement of similarities between instances in the same class and deterioration of similarities between instances in different classes by a feature weighting approach. <p> to weight optimization by gradient descent with learning constant 1 kflm and criterion function J R k X Q X w q R (x j q hit ) i=1 q=1 q ; x j i q miss ) (29) 5.3 RELIEF Feature Selection Strategy To use RELIEF for feature selection, <ref> [19] </ref> propose the introduction of a user defined threshold . Feature q is selected if w q &gt; holds, otherwise it is neglected. 5.4 RELIEF vs. EUBAFES In the following sections we will compare EUBAFES and RELIEF in more detail. Batch vs. <p> In RELIEF, a feature weight update bases only on the discrimination property of the feature, ignoring the instance similarity. RELIEF cannot be used for determining binary feature weights like it is done in EUBAFES, since all features would be selected which have discrimination property. This assertion is underlined by <ref> [19] </ref> who state, that "Relief does not help with redundant features. If most of the given features are relevant to the concept, it would select most of them even though only a fraction are necessary for concept description". Binary vs.
Reference: 20. <author> Setiono R. Huan L. </author> <title> A probabilistic approach to feature selection a filter solution. </title> <booktitle> 13th International Conference on Machine Learning (ICML'97), </booktitle> <month> July </month> <year> 1996, </year> <institution> pp.319-327 Bary, Italy, </institution> <year> 1996. </year>
Reference-contexts: Therefore our approach belongs to the group of filter approaches. Further filter approaches are the FOCUS algorithm [7,8], the Decision Tree Based approach by Cardie [4] and the LVF algorithm <ref> [20] </ref>.
Reference: 21. <author> Shannon C.E. </author> <title> A mathematical theory of communication. </title> <journal> Bell Systems Technology Journal, </journal> <volume> 27, </volume> <pages> 379-423, </pages> <year> 1948. </year>
Reference-contexts: They distinguish between approaches, based on conditional probabilities like PCF [5], class projection like the value-difference metric (VDM) [26] and mutual information, introduced by Shannon <ref> [21] </ref> to assign continuous weights to features, giving a measure of relevance. Nevertheless none of the feature weighting approaches is able to give automatically a set of binary feature weights.
Reference: 22. <author> Skalak D. </author> <title> Prototype and feature selection by sampling and random mutation hill climbing algorithms. </title> <editor> In William W. Cohne and Haym Hirsh, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pages 293-301, </pages> <year> 1994. </year>
Reference: 23. <editor> Thrun S.B. et al. </editor> <title> The monk's problems: A performance comparison of different learning algorithms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnon Mellon University, </institution> <year> 1991. </year>
Reference-contexts: RELIEF-F was not able to detect the relevant features for any number of nearest neighbors. We assume, that the reason for this result is substantiated by the more robust, batch oriented approach. The Monks Problem The three data sets Monk1, Monk2 and Monk3 were taken from <ref> [23] </ref>. The instances of each data set are defined by six discrete features. The Monk1 and Monk3 data base need only three, whereas the Monk2 data base needs all six features to describe the target concept. The Monk3 data base additionally contains some noise.
Reference: 24. <author> Wettschereck D. </author> <title> A study of distance based machine learning algorithms. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Oregon State University, </institution> <address> Corvalis, OR, </address> <year> 1994. </year>
Reference: 25. <author> Wettschereck D. Mohori T. Aha D.W. </author> <title> A review and empirical evaluation of feature weighting methods for a class of lazy learning algorithms. </title> <note> Artificial Intelligence Review (to appear), </note> <year> 1997. </year>
Reference-contexts: The necessary features were known a priory in every case. In the following sections we describe the data sets and compare the results of EUBAFES and RELIEF-F. Waveform-40 The Waveform-40 data set was introduced in [3] and applied in <ref> [25] </ref> to examine how well a feature selection algorithm works in the presence of a high number of irrelevant features. The data set enfolds 300 instances with every instance assigned to one of three classes. An instance X i is defined by 40 continuous feature values. <p> In [12], RELIEF-F is categorized as filter approach, which from our point of view is discussible according to their definition of the filter approach, because RELIEF-F as well as EUBAFES have a criterion function which is related to the k-nearest neighbor classifier. Our argumentation is supported by <ref> [25] </ref>, where RELIEF-F is itemized as a wrapper approach for the k-nearest neighbor algorithm. Thus wrapper methods can be seen as techniques, where the judgment, whether a feature subset is superior over another is done by the accuracy of a certain classifier. <p> group of techniques are feature weighting approaches: Since continuos feature weights include binary feature weights, these methods could always be used for feature selection by mapping the continuous feature weights to binary feature weights by an arbitrary criterion like a threshold value as it is proposed in context with RELIEF. <ref> [25] </ref> gives an overview of filter methods, using continuous feature weights. They distinguish between approaches, based on conditional probabilities like PCF [5], class projection like the value-difference metric (VDM) [26] and mutual information, introduced by Shannon [21] to assign continuous weights to features, giving a measure of relevance.
Reference: 26. <author> Waltz D. Stanfill C. </author> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29, </volume> <pages> 1213-1228, </pages> <year> 1992. </year>
Reference-contexts: They distinguish between approaches, based on conditional probabilities like PCF [5], class projection like the value-difference metric (VDM) <ref> [26] </ref> and mutual information, introduced by Shannon [21] to assign continuous weights to features, giving a measure of relevance. Nevertheless none of the feature weighting approaches is able to give automatically a set of binary feature weights.
References-found: 26

