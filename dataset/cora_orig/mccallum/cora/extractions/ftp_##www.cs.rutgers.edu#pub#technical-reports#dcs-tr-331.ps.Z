URL: ftp://www.cs.rutgers.edu/pub/technical-reports/dcs-tr-331.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: 
Title: Automated Classification of DNA Structure from Sequence Information Keywords: DNA Backbone Conformation, Entropy Estimation, DNA
Author: David M. Loewenstern and Helen M. Berman Haym Hirsh 
Date: June 26, 1997  
Address: Piscataway, NJ 08855  Whippany, NJ 07981  Piscataway, NJ 08855  Piscataway, NJ 08855  
Affiliation: Department of Computer Science Rutgers University  Bell Laboratories  Department of Chemistry and Waksman Institute Rutgers University  Department of Computer Science Rutgers University  
Abstract: We introduce an algorithm, lllama, which combines simple pattern recognizers into a general method for estimating the entropy of a sequence. Each pattern recognizer exploits a partial match between subsequences to build a model of the sequence. Since the primary features of interest in biological sequence domains are subsequences with small variations in exact composition, lllama is particularly suited to such domains. We describe two methods, lllama-length and lllama-alone, which use this entropy estimate to perform maximum a posteriori classification. We apply these methods to several problems in three-dimensional structure classification of short DNA sequences. The results include a fl Email: loewenst@paul.rutgers.edu. Phone: 201-761-5949. Fax: 908-445-0537. y Email: berman@adenine.rutgers.edu. z Email: hirsh@cs.rutgers.edu. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. E. Baum and J. E. Eagon, </author> <title> An inequality with application to statistical estimation for probabalistic functions of a Markov process and to models for ecology, </title> <journal> Bull. AMS, </journal> <volume> 73 (1967), </volume> <pages> pp. 360-363. </pages>
Reference-contexts: The learning task before us is to estimate the parameters Pr (h = ijf = j; w = k) and Pr (w = k) by examining the training set T . Our algorithm is an application of the Baum-Welch algorithm for Hidden Markov Models <ref> [1] </ref>, and may also be viewed as an instance of Expectation Maximization (EM), a later rediscovery [5] of essentially the same algorithm and underlying information theoretic inequality.
Reference: [2] <author> H. M. Berman, A. Gelbin, and J. Westbrook, </author> <title> Nucleic acid crystallography: A view from the nucleic acid database, </title> <journal> Prog. Biophys. Mol. Biol., </journal> <note> (1997). In Press. </note>
Reference-contexts: The second task is to predict the crystal type, which is to say the crystallographic unit cell and space group. The third task is prediction of packing motif: a group of crystal types belong to the same motif if the molecular interactions within the crystal are similar <ref> [2] </ref>. We would like to solve these tasks for short DNA sequences (fewer than 13 nucleotides). For our purposes, it is sufficient to label the sequence with exactly one helical conformational class, one crystal type, and one packing motif. <p> B-DNA structures had 16 crystal types, and Z-DNA structures had 5 crystal types. Packing motifs are defined according to the way in which molecules interact in the crystal. For B-DNA there were three motifs, for Z-DNA there were two and for A-DNA there was one <ref> [2] </ref>. 2.2 Data Representation For testing purposes, the data for each of the three tasks were considered separately. In addition, for each task, we constructed three different data sets which were labeled by the structure class of each sequence.
Reference: [3] <author> H. M. Berman, W. K. Olson, D. L. Beveridge, J. Westbrook, A. Gelbin, T. Demeny, S.-H. Hsieh, A. R. Srinivasan, and B. Schneider, </author> <title> The nucleic acid database a comprehensive relational database of three-dimensional structures of nucleic acids, </title> <journal> Biophys. J., </journal> <volume> 63 (1992), </volume> <pages> pp. 751-759. </pages>
Reference-contexts: Furthermore, we wish to use general machine learning techniques which can be easily applied to a range of DNA classification tasks. To this end, we extracted a training corpus from the Nucleic Acid Database (NDB) <ref> [3] </ref> composed of all nucleotide sequences with exactly one known helical conformational class, crystal type, and packing motif. The corpus (unlike the NDB) contains only symbols representing the sequence itself, and does not contain three-dimensional coordinate information.
Reference: [4] <author> L. Cardon and G. Stormo, </author> <title> Expectation maximization algorithm for identifying protein-binding sites with variable lengths from unaligned DNA fragments, </title> <journal> Journal of Molecular Biology, </journal> <volume> 223 (1992), </volume> <pages> pp. 159-170. </pages>
Reference-contexts: and Expectation Maximization to the problem of modeling DNA sequences, but do so in a task-specific way which does not permit broader application of the methods. [9], for example, is not intended to be a general modeler, but a specific model for a specific task, finding genes in E. coli. <ref> [4] </ref> applies Expectation Maximization to a Markov Chain modeling E. coli promoters to find the most likely promoter starting position in a training set. [7] is a method for modeling DNA sequences, in particular prokaryotic promoters, by scanning for mismatches in a manner somewhat like lllama.
Reference: [5] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin, </author> <title> Maximum-likelihood from incomplete data via the EM algorithm, </title> <journal> J. Royal Statistical Society Ser. B (methodological), </journal> <volume> 39 (1977), </volume> <pages> pp. 1-38. </pages>
Reference-contexts: Our algorithm is an application of the Baum-Welch algorithm for Hidden Markov Models [1], and may also be viewed as an instance of Expectation Maximization (EM), a later rediscovery <ref> [5] </ref> of essentially the same algorithm and underlying information theoretic inequality. This method iteratively updates the above parameters, with the guaranteed result that the probability of the training set Q b2T Pr (bjpast b ) increases or remains the same with each iteration.
Reference: [6] <author> M. Farach, M. Noordewier, S. Savari, L. Shepp, A. Wyner, and J. Ziv, </author> <title> On the entropy of DNA: Algorithms and measurements based on memory and rapid convergence, </title> <booktitle> in Proceedings of the Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1994. </year>
Reference-contexts: either the building or application of the models, which is one of the key ideas presented in this paper. [13, 12] use such methods to compare the probability that the sequence was generated from the model to a null hypothesis that the sequence was randomly generated from a flat distribution. <ref> [6] </ref> estimates the entropy near splice junctions in eukaryotic genes, and proposes using this method to detect splice junctions.
Reference: [7] <author> D. Galas, M. Eggert, and M. Waterman, </author> <title> Rigorous pattern-recognition methods for DNA sequences, </title> <journal> Journal of Molecular Biology, </journal> <year> (1985), </year> <pages> pp. 117-128. </pages>
Reference-contexts: the methods. [9], for example, is not intended to be a general modeler, but a specific model for a specific task, finding genes in E. coli. [4] applies Expectation Maximization to a Markov Chain modeling E. coli promoters to find the most likely promoter starting position in a training set. <ref> [7] </ref> is a method for modeling DNA sequences, in particular prokaryotic promoters, by scanning for mismatches in a manner somewhat like lllama. Unlike lllama, the method the authors describe does not generate a Bayesian model for predicting the next character, but a score for detecting near-repeats.
Reference: [8] <author> H. Hirsh and M. O. Noordewier, </author> <title> Using background knowledge to improve inductive learning of DNA sequences, </title> <journal> IEEE Expert, </journal> <year> (1994). </year>
Reference-contexts: When there is little prior knowledge available to build the attributes, a naive feature representation (such as the sequence of nucleotides itself) must be used, which is generally not very effective <ref> [8] </ref> (Figure 2). This method works well when attributes represent more complex, independent features such as length, %CG, or charge (Figure 3). However, this representation requires a substantial amount of biological knowledge to create, and must be created anew for each new classification problem.
Reference: [9] <author> A. Krogh, I. Mian, and D. Haussler, </author> <title> A hidden Markov model that finds genes in Escheria coli DNA, </title> <journal> Nucleic Acids Research supplement, </journal> <volume> 22 (1994), </volume> <pages> pp. 4768-4778. </pages>
Reference-contexts: Several methods apply Bayesian techniques and Expectation Maximization to the problem of modeling DNA sequences, but do so in a task-specific way which does not permit broader application of the methods. <ref> [9] </ref>, for example, is not intended to be a general modeler, but a specific model for a specific task, finding genes in E. coli. [4] applies Expectation Maximization to a Markov Chain modeling E. coli promoters to find the most likely promoter starting position in a training set. [7] is a
Reference: [10] <author> D. M. Loewenstern and P. N. Yianilos, </author> <title> Significantly lower entropy estimates for natural DNA seqeuences, </title> <type> Tech. Rep. 96-51, </type> <institution> DIMACS, </institution> <year> 1996. </year> <title> Extended version of [11]. [11] , Significantly lower entropy estimates for natural DNA seqeuences, </title> <booktitle> in Proceedings of the Data Compression Conference, </booktitle> <month> March </month> <year> 1997. </year>
Reference-contexts: In addition, the lllama models may be used directly, without retraining, for more than classification. For example, to find the degree of local A-helical conformational propensity (or propensity for any other class) of each nucleotide of a long sequence such as a gene or chromosome (see <ref> [10] </ref>, Figure 11 for an example). Lllama has been applied to large biological sequence problems as well, such as comparing coding and non-coding regions in an entire chromosome [11].
Reference: [12] <author> A. Milosavljevi c, </author> <title> Discovering sequence similarity by the algorithmic significance method, </title> <booktitle> in International Conference on Intelligent Systems for Molecular Biology, </booktitle> <year> 1993, </year> <pages> pp. </pages> <month> 284-291. </month> <title> [13] , Sequence comparisons via algorithmic mutual information, </title> <booktitle> in International Conference on Intelligent Systems for Molecular Biology, </booktitle> <year> 1994. </year> <month> 21 </month>
Reference-contexts: In general, these methods do not permit the use of mismatches in either the building or application of the models, which is one of the key ideas presented in this paper. <ref> [13, 12] </ref> use such methods to compare the probability that the sequence was generated from the model to a null hypothesis that the sequence was randomly generated from a flat distribution. [6] estimates the entropy near splice junctions in eukaryotic genes, and proposes using this method to detect splice junctions.
Reference: [14] <author> O. Nash, </author> <title> The Lama, in Selected Poetry of Ogden Nash, Little, </title> <publisher> Brown, </publisher> <year> 1995, </year> <note> p. 310. </note>
Reference-contexts: Section 3.1 provides a brief introduction to C4.5 and k-nearest-neighbor methods. Section 3.2 then describes the lllama-alone and lllama-length methods, and Section 3.3 the underlying lllama algorithm. Finally, this paper presents experimental results comparing 1 Lllama Looks Like A Meaningful Acronym, with apologies to Ogden Nash <ref> [14] </ref>. 6 the classification methods in Section 4, discusses related work in Section 5, and draws conclusions in Section 6. 2 Data 2.1 DNA sequences The data used for these analyses were those contained in the Nucleic Acid Database (NDB). The NDB contains all three-dimensional structures determined using x-ray crystallography.
Reference: [15] <author> J. R. Quinlan, </author> <title> Induction of decision trees, </title> <booktitle> Machine Learning, 1 (1986), </booktitle> <pages> pp. 81-106. </pages>
Reference-contexts: GGGATCCC ? Acid Database (NDB). The boxed sequence is to be classified. structure classification. To illustrate these views, refer to Figures 2-6. The first of these methods, represented by C4.5 <ref> [15] </ref>, treats individual features separately, as a set of attribute/value pairs, also known as features. It then tries to build a classifier from training examples that examines groups of features to make a structure prediction. <p> The first of these methods was C4.5 <ref> [15] </ref>. For this method, the data was encoded as a twelve-position feature vector with feature 1 corresponding to the 3'-most nucleotide in the sequence, feature 2 its 5' neighbor, and so forth. If the sequence was shorter than twelve nucleotides, the last several features were given the placeholder value .
Reference: [16] <author> E. S. Ristad and P. N. Yianilos, </author> <title> Learning string edit distance, </title> <type> Tech. Rep. </type> <institution> TR-532-96, Princeton University, </institution> <year> 1996. </year>
Reference-contexts: It also shares with lllama the advantages of extensibility to long sequences and ability to recognize subsequence features. It also has the advantage of much shorter training time. Extending the method by tuning the Smith-Waterman matching parameters (for example, see <ref> [16] </ref>) could yield significant improvements in classification accuracy with only minimal cost in training time. 20
Reference: [17] <author> T. F. Smith and M. Waterman, </author> <title> Identification of common molecular subsequences, </title> <journal> Journal of Molecular Biology, </journal> <volume> 147 (1981), </volume> <pages> pp. 195-197. </pages>
Reference-contexts: It should be noted that the representation making each feature correspond to a single nucleotide is sensitive to the alignment of the sequences. The second method was a k-nearest-neighbor algorithm [18], using the Smith-Waterman edit distance function <ref> [17] </ref> to estimate the distance between sequences. Again, two different representations of the alphabet were used, an simplified alphabet a; c; g; t and an alphabet including modified and unmodified variants a; c; g; t; I; A; C; G; T; U.
Reference: [18] <author> S. Weiss and C. </author> <title> Kulikowski, Computer Systems that Learn, </title> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA, </address> <year> 1991. </year>
Reference-contexts: This method works well when attributes represent more complex, independent features such as length, %CG, or charge (Figure 3). However, this representation requires a substantial amount of biological knowledge to create, and must be created anew for each new classification problem. A second view, represented by a k-nearest-neighbor method <ref> [18] </ref>, looks for similarities between a sequence and each individual training example, seen as a whole (Figure 4). Here, some biological knowledge must be used to judge similarity. <p> In addition, for each task, we constructed three different data sets which were labeled by the structure class of each sequence. For each of the three data sets, training and test sets were constructed repeatedly using a leaving-one-out method <ref> [18] </ref>. The training sets were then used to train each of the various classification methods, which were compared on their average accuracy on the test sets as discussed in Section 4. Two different data representations were tried for the data. <p> In this example, GGGATCCC has features pos10= and pos8=C, which alone place it into a class labeled A-DNA. It should be noted that the representation making each feature correspond to a single nucleotide is sensitive to the alignment of the sequences. The second method was a k-nearest-neighbor algorithm <ref> [18] </ref>, using the Smith-Waterman edit distance function [17] to estimate the distance between sequences. Again, two different representations of the alphabet were used, an simplified alphabet a; c; g; t and an alphabet including modified and unmodified variants a; c; g; t; I; A; C; G; T; U. <p> To provide a point of reference, we compare our model directly against the methods described in Section 3.1. Each method and representation is shown in order of increasing error rate, as calculated by the leaving-one-out method <ref> [18] </ref>. In these figures, k-nearest-neighbor is only reported for k = 1, and labeled "1NN". To make the bar graphs readable, the bars representing the error for the most frequent class method were sometimes clipped. classification tasks on the full data set.
Reference: [19] <author> J. Westbrook, T. Demeny, and S.-H. Hsieh, NDBQuery, v4.0, </author> <title> A simplified user interface to the Nucleic Acid Database, </title> <type> Tech. Rep. </type> <institution> NDB99, Rutgers University, </institution> <year> 1996. </year> <month> 22 </month>
Reference-contexts: The NDB contains all three-dimensional structures determined using x-ray crystallography. The data are organized in a relational database which is queried using the program NDBQuery <ref> [19] </ref>. Constraints were applied so that the resulting reports contained sequences sorted according to conformation type, crystal type, and packing type. The conformation types for DNA helices are the two right-handed forms, A-DNA and B-DNA, and the left-handed form Z-DNA.
References-found: 17

