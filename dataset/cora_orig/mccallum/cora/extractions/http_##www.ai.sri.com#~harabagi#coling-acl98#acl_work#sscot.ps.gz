URL: http://www.ai.sri.com/~harabagi/coling-acl98/acl_work/sscot.ps.gz
Refering-URL: http://www.ai.sri.com/~harabagi/coling-acl98/acl_work/acl_work.html
Root-URL: 
Email: sscott@csi.uottawa.ca  stan@csi.uottawa.ca  
Title: Text Classification Using WordNet Hypernyms  
Author: Sam Scott Stan Matwin 
Address: Ottawa, ON K1N 6N5 (Canada)  Ottawa, ON K1N 6N5 (Canada)  
Affiliation: Computer Science Dept. University of Ottawa  Computer Science Dept. University of Ottawa  
Abstract: This paper describes experiments in Machine Learning for text classification using a new representation of text based on WordNet hypernyms. Six binary classification tasks of varying difficulty are defined, and the Ripper system is used to produce discrimination rules for each task using the new hypernym density representation. Rules are also produced with the commonly used bag-of-words representation, incorporating no knowledge from WordNet. Experiments show that for some of the more difficult tasks the hypernym density representation leads to significantly more accurate and more comprehensible rules.
Abstract-found: 1
Intro-found: 1
Reference: [Brill 92] <author> Eric Brill. </author> <title> A simple rule-based part of speech tagger. </title> <booktitle> In Proceedings of the Third Conference on Applied Natural Language Processing, ACL, </booktitle> <year> 1992. </year>
Reference-contexts: This paper explores the hypothesis that incorporating linguistic knowledge into text representation can lead to improvements in classification accuracy. Specifically, we use part of speech information from the Brill tagger <ref> [ Brill 92] </ref> and the synonymy and hypernymy relations from WordNet [Miller 90] to change the representation of the text from bag-of-words to hypernym density. We report results from an ongoing study in which the hypernym density representation at different heights of generalization is compared to the old bag-of-words model. <p> Clearly the background knowledge and linguistic competence humans bring to a classification task enables us to overcome the difficulties posed by the text itself. 3. The Hypernym Density Representation The algorithm for computing hypernym density requires three passes through the corpus. a) During the first pass, the Brill tagger <ref> [ Brill 92] </ref> assigns a part of speech tag to each word in the corpus. b) During the second pass, all nouns and verbs are looked up in WordNet and a global list of all synonym and hypernym synsets is assembled.
Reference: [Cohen 95] <author> William W. Cohen. </author> <title> Fast Effective Rule Induction. </title> <booktitle> In Proc. </booktitle> <address> ICML-95. Lake Tahoe, California, </address> <year> 1995. </year>
Reference-contexts: In all cases the classes were made completely disjoint by removing any overlapping examples. 1 The machine learning algorithm chosen for this study was Ripper, a rule-based learner developed by William Cohen <ref> [Cohen 95] </ref>. Ripper was specifically designed to handle the high dimensionality of bag-of-words text classification by being fast and using set-valued features [Cohen 96]. Table 1 shows that our intuitions about the difficulty of the three corpora for bag-of-words classification are valid in the case of the Ripper algorithm.
Reference: [Cohen 96] <author> William W. Cohen. </author> <title> Learning Trees and Rules with Set-valued Features. </title> <booktitle> In Proc. AAAI-96 , 1996 </booktitle>
Reference-contexts: Ripper was specifically designed to handle the high dimensionality of bag-of-words text classification by being fast and using set-valued features <ref> [Cohen 96] </ref>. Table 1 shows that our intuitions about the difficulty of the three corpora for bag-of-words classification are valid in the case of the Ripper algorithm.
Reference: [Greenhaus 96] <author> Dick Greenhaus. </author> <title> About the Digital Tradition. </title> ( <note> www.mudcat.org/DigiTrad-blurb.html), 1996. 4 </note>
Reference-contexts: In keeping with previous studies, we used topic headings as the basis for the Reuters classification tasks and newsgroup names as the basis for the USENET tasks. The third corpus, DigiTrad is a public domain collection of 6500 folk song lyrics <ref> [ Greenhaus 96] </ref>. To aid searching, the owners of DigiTrad have assigned to each song one or more key words from a fixed list.
Reference: [Joachims 97] <author> T. Joachims. </author> <title> A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization. </title> <booktitle> In Proc. ICML-97, </booktitle> <pages> 143-146, </pages> <year> 1997. </year>
Reference-contexts: This text representation, referred to as the bag-of-words, is used in most typical approaches to text classification (for recent work see [ Lang 95], <ref> [Joachims 97] </ref>, and [ Koller & Sahami 97]). In these approaches, no linguistic processing (other than a stop list of most frequent words) is applied to the original text. This paper explores the hypothesis that incorporating linguistic knowledge into text representation can lead to improvements in classification accuracy.
Reference: [Kohavi & John 95] <author> Ron Kohavi and George H. John. </author> <title> Automatic Parameter Selection by Minimizing Estimated Error. </title> <booktitle> In Proc. </booktitle> <address> ICML-95, </address> <year> 1995. </year>
Reference-contexts: All currently available machine learning systems contain a large number of parameters. The only difference is that h modifies the feature set rather than the learning algorithm itself. Nevertheless, it is worth addressing the question of how this parameter could be set in practice. <ref> [Kohavi & John 95] </ref> describe a wrapper method for learning algorithms that automatically selects appropriate parameters. In their system, the set of parameters is treated as a vector space that can be searched for an optimal setting.
Reference: [Kohavi & John 97] <author> Ron Kohavi and George H. John. </author> <title> Wrapers for Feature Subset Selection. </title> <journal> In Artificial Intelligence Journal, special issue on relevance, </journal> <month> May 20, </month> <year> 1997. </year>
Reference-contexts: This system 3 Some of these terms do appear in WordNet 1.6 could easily be adapted to include a parameter such as h that modifies the feature set. Indeed <ref> [ Kohavi & John 97] </ref> have already extended their method to the related problem of finding optimal feature subsets for learning. 5. Conclusions and future work.
Reference: [Koller & Sahami 96] <author> D. Koller and M. Sahami. </author> <title> Hierarchically Classifying Documents Using Very Few Words. </title> <booktitle> In Proc. ICML-97, </booktitle> <pages> 170-176, </pages> <year> 1997. </year>
Reference: [Lang 95] <author> K. Lang. NewsWeeder: </author> <title> Learning to Filter News. </title> <booktitle> In Proc. ICML-95, </booktitle> <pages> 331-336, </pages> <year> 1995. </year>
Reference-contexts: This text representation, referred to as the bag-of-words, is used in most typical approaches to text classification (for recent work see <ref> [ Lang 95] </ref>, [Joachims 97], and [ Koller & Sahami 97]). In these approaches, no linguistic processing (other than a stop list of most frequent words) is applied to the original text.
Reference: [Li et al. 95] <author> Xiaobin Li, </author> <title> Stan Szpakowicz and Stan 4 The DigiTrad database itself is at www.deltablues.com/folksearch.html Matwin. A WordNet-based Algorithm for Word Sense Disambiguation. </title> <booktitle> In Proc. IJCAI-95, </booktitle> <address> Montral, Canada, </address> <year> 1995. </year>
Reference-contexts: This approach was feasible in the context of that study because of the small number of words involved. In the current study, the words number in the tens of thousands, making manual disambiguation unfeasible. Automatic disambiguation is possible and often obtains good results as in [ Yarowski 95] or <ref> [ Li et al. 95] </ref>, but this is left as future work.) Clearly the change of representation process leaves a lot of room for inaccuracies to be introduced to the feature set.
Reference: [Mitchell 97] <author> Tom Mitchell, </author> <title> Machine Learning , McGraw Hill, </title> <year> 1997. </year>
Reference-contexts: set of classification labels C, and set of training examples E, each of which has been assigned one of the class labels from C, the system must use E to form a hypothesis that can be used to predict the class labels of previously unseen examples of the same type <ref> [Mitchell 97] </ref>. In machine learning systems that classify text, E is a set of labeled documents from a corpus such as Reuters-21578. The labels can signify topic headings, writing styles, or judgements as to the documents relevance.
Reference: [Miller 90] <author> George A. Miller. </author> <title> WordNet: an Online Lexical Database. </title> <journal> International Journal of Lexicography 3(4), </journal> <year> 1990. </year>
Reference-contexts: This paper explores the hypothesis that incorporating linguistic knowledge into text representation can lead to improvements in classification accuracy. Specifically, we use part of speech information from the Brill tagger [ Brill 92] and the synonymy and hypernymy relations from WordNet <ref> [Miller 90] </ref> to change the representation of the text from bag-of-words to hypernym density. We report results from an ongoing study in which the hypernym density representation at different heights of generalization is compared to the old bag-of-words model.
Reference: [NLM 98] <institution> National Library of Medicine. </institution> <note> Unified Medical Language System Overview . www.nlm.nih.gov/research/umls/UMLSDOC.HTML, February, </note> <year> 1998. </year>
Reference-contexts: This will give the change of representation an even more semantic character. More sophisticated word sense disambiguation could produce more accurate hypernym density features. The use of other linguistic resources available in the public domain, such as the Unified Medical Language System Metathesaurus <ref> [NLM 98] </ref>, could improve classifier performance in knowledge domains that are semantically close and highly expert. Finally, there is the task of testing whether the improvements noted in this study generalize to machine learning systems other than Ripper.
Reference: [Rodrguez et al. 97] <author> Manuel de Buenaga Rodrguez, Jos Mara Gmez-Hidalgo and Beln Daz-Agudo. </author> <title> Using WordNet to Complement Training Information in Text Categorization. </title> <booktitle> In Proc. </booktitle> <address> RANLP-97 , Stanford, March 25-27, </address> <year> 1997 </year>
Reference-contexts: The issue of whether our results will generalize to other machine learning systems is left as future work. The only published study comparable to this one is <ref> [Rodrguez et al. 97] </ref>. Their study used WordNet to enhance neural network learning algorithms for significant improvements in classification accuracy on the Reuters-21578 corpus. <p> It has been observed that the topic headings in Reuters tend to consist of words that appear frequently in the text, and this observation has been exploited to help improve classification accuracy <ref> [ Rodrguez et al. 97] </ref>. DigiTrad and USENET are good examples of the opposite extreme. The texts in DigiTrad make heavy use of metaphoric, rhyming, unusual and archaic language. Often the lyrics do not explicitly state what a song is about. <p> In other words, a relatively low value for a feature indicates that little evidence was found for the meaningfulness of that synset to the document. (In the <ref> [ Rodrguez et al. 97] </ref> text classification paper, word sense disambiguation was performed by manual inspection. This approach was feasible in the context of that study because of the small number of words involved. In the current study, the words number in the tens of thousands, making manual disambiguation unfeasible.
Reference: [Weiss et al. 96] <author> Scott A. Weiss, Simon Kasif, and Eric Brill. </author> <title> Text Classification in USENET Newsgroups: A Progress Report. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Machine Learning in Information Access , Bulgaria, </booktitle> <month> September 11-13, </month> <year> 1996. </year>
Reference-contexts: Both Reuters and USENET have been the subject of previous studies in machine learning (see [ Koller & Sahami 97] for a study of Reuters and <ref> [Weiss et al. 96] </ref> for a study of USENET). In keeping with previous studies, we used topic headings as the basis for the Reuters classification tasks and newsgroup names as the basis for the USENET tasks.
Reference: [Yarowski 95] <author> David Yarowski. </author> <title> Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. </title> <booktitle> In Proceedings of the 33 rd Meeting of the ACL , Cambridge, </booktitle> <month> June 26-30, </month> <year> 1995. </year>
Reference-contexts: This approach was feasible in the context of that study because of the small number of words involved. In the current study, the words number in the tens of thousands, making manual disambiguation unfeasible. Automatic disambiguation is possible and often obtains good results as in <ref> [ Yarowski 95] </ref> or [ Li et al. 95], but this is left as future work.) Clearly the change of representation process leaves a lot of room for inaccuracies to be introduced to the feature set.
References-found: 16

