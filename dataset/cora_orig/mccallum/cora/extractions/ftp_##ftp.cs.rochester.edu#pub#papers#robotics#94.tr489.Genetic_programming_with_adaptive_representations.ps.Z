URL: ftp://ftp.cs.rochester.edu/pub/papers/robotics/94.tr489.Genetic_programming_with_adaptive_representations.ps.Z
Refering-URL: http://www.cs.indiana.edu/cstr/search/?Knowledge+discovery+MINK%3D2
Root-URL: 
Title: Genetic Programming with Adaptive Representations  
Author: Justinian P. Rosca, Dana H. Ballard 
Date: February 1994  
Address: Rochester, New York 14627  
Affiliation: The University of Rochester Computer Science Department  
Pubnum: Technical Report 489  
Abstract: Machine learning aims towards the acquisition of knowledge based on either experience from the interaction with the external environment or by analyzing the internal problem-solving traces. Both approaches can be implemented in the Genetic Programming (GP) paradigm. [Hillis, 1990] proves in an ingenious way how the first approach can work. There have not been any significant tests to prove that GP can take advantage of its own search traces. This paper presents an approach to automatic discovery of functions in GP based on the ideas of discovery of useful building blocks by analyzing the evolution trace, generalizing of blocks to define new functions and finally adapting of the problem representation on-the-fly. Adaptation of the representation determines a hierarchical organization of the extended function set which enables a restructuring of the search space so that solutions can be found more easily. Complexity measures of solution trees are defined for an adaptive representation framework and empirical results are presented. This material is based on work supported by the National Science Foundation under Grant numbered IRI-8903582 by NIH/PHS research grant numbered 1 R24 RR06853-02 and by a Human Science Frontiers Program research grant. The government has certain rights in this material. 
Abstract-found: 1
Intro-found: 1
Reference: [Angeline, 1994] <author> Peter J. Angeline, </author> <title> "Genetic Programming and Emergent Intelligence," </title> <editor> In Kim Kinnear, editor, </editor> <booktitle> Advances in Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The epistasis idea can not be easily applied to GP approaches, where the interaction of parts of a program is very hard to decipher. Moreover, no precise definition of a GP schema exists, and no alternative schemata theorem has been proven for GP. One more difficulty is outlined in <ref> [Angeline, 1994] </ref>. GP individuals represent code, as opposed to the GA representation. There exists a complicated many-to-many mapping between genotypic and phenotypic features. <p> The problem is that the converse of the GA schemata theorem is not true. Poor blocks, i.e. blocks that are identities or add no functionality, may be frequent and should not be considered as candidates, although they may have a role of preserving recessive features (introns in <ref> [Angeline, 1994] </ref>). In general, if a building block is not discovered early enough, and population diversity is not preserved due to biased operators or poor tuning of the application parameters, other blocks will account for high frequency counts and the search problem will get stuck in a local minimum.
Reference: [Angeline and Pollack, 1993a] <author> Peter J. Angeline and Jordan B. Pollack, </author> <title> "Competitive Environments Evolve Better Solutions for Complex Tasks," </title> <booktitle> In Proceedings of the Fifth International Conference on Genetic Algorithms. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <year> 1993. </year>
Reference-contexts: Normalized fitness can be also defined based on ranking [Winston, 1992] or other ad-hoc methods. Two selection methods are currently used. A first one, tournament selection, is described in <ref> [Angeline and Pollack, 1993a] </ref>. We randomly choose a small group of individuals and let them play a "tournament". The winner represents the individual selected.
Reference: [Angeline and Pollack, 1993b] <author> Peter J. Angeline and Jordan B. Pollack, </author> <title> "Evolutionary Module Acquisition," </title> <booktitle> In Proceedings of the Second Annual Conference on Evolutionary Programming, </booktitle> <year> 1993. </year>
Reference-contexts: ADF-GP automatically discovers how to decompose a problem into subproblems how to solve the subproblems (ADFs) and how to combine the solutions to subproblems (in higher level ADFs and program body). 3.2 Module acquisition The module acquisition approach <ref> [Angeline and Pollack, 1993b] </ref> is applied more generally to evolutionary algorithms (an umbrella term covering GP and Evolutionary Programming). The ideas implemented within the genetic programming paradigm are illustrative and will be described shortly here. <p> Expand, implements the inverse function of compress. It simply expands a selected module name with the module itself back into the program tree where it was created. Compression freezes possibly useful genetic material, by protecting it from the destructive effect of other genetic operators <ref> [Angeline and Pollack, 1993b] </ref>. It also increases the expressiveness of the base language and helps in decreasing the average size of individuals 6 in the population, while maintaining the same power. As a side effect, if used alone, it may generate a loss of diversity in the population.
Reference: [Angeline and Pollack, 1994] <author> Peter J. Angeline and Jordan B. Pollack, </author> <title> "Coevolving High Level Representations," </title> <booktitle> In The Proceedings of Artificial Life III (to appear), </booktitle> <year> 1994. </year>
Reference-contexts: However changing individual program instructions is most often unfruitful, so extensions to GP have tried to randomly define and use larger components such as new functions (automatic function definition or ADF in [Koza, 1992]) or modules (in <ref> [Angeline and Pollack, 1994] </ref>). This paper presents an alternative approach to structure synthesis different from either ADF or modules in several respects. We show that efficient structure synthesis can be obtained by discovering useful genetic material and using it globally to adapt the problem representation. <p> The question is then how can we improve the performance of GP for very sparse solution spaces? 4 3 Related Work In the next two sections we summarize two extensions to GP that point out the possibility of evolution of new functions [Koza, 1992] or emergence of modules <ref> [Angeline and Pollack, 1994] </ref> as possible ways to cope with complex problems. 3.1 Automatic definition of functions Automatic definition of functions (ADF) is an extension of the GP paradigm to cope with the automatic decomposition of a solution function [Koza, 1992]. <p> The ideas implemented within the genetic programming paradigm are illustrative and will be described shortly here. This approach is based on the creation and administration of a library of modules which extend the problem representation and on the use of two new genetic operators, compress and expand <ref> [Angeline and Pollack, 1994] </ref>, that control the process of modifying population individuals. A module is a function with a unique name defined by selecting and chopping off branches of a subtree selected randomly from an individual. The function's parameters are determined by the places where the tree is trimmed. <p> As a side effect, if used alone, it may generate a loss of diversity in the population. The expansion operator overcomes this problem (see figure 4.) It is interesting that in <ref> [Angeline and Pollack, 1994] </ref>, the authors talk about the worth of a module, but they attribute to it a rather passive role. The module's worth is the number of times the module has been used since its birth, in subsequent generations. <p> We define blocks as entire subtrees of a given maximum height from population individuals. This represents the bottom-up approach in figure 5. (In contrast, the module approach <ref> [Angeline and Pollack, 1994] </ref> starts with subtrees of any depth and chops off all branches at a given depth.) We will show that the bottom-up approach enables the definition of blocks whose usefulness can be evaluated. <p> Extraordinary improvements of the effort needed to find a solution have been observed for problems with regularity in their solutions. This work distinguishes from other approaches to understanding what building blocks are in several major respects. As opposed to <ref> [Angeline and Pollack, 1994] </ref> where the functional role of a module is identified and the evolution of the representation language is suggested, we select, generalize and use blocks in a different way. Selection is based on 25 estimated block usefulness determined by an analysis of the GP trace. <p> The discovered functions evolve into a hierarchy of functions of increasing complexity that solve subproblems. We have not experimented with deletion of functions previously added to the function set, but our remarks and <ref> [Angeline and Pollack, 1994] </ref> argue for using the appearance frequency of these functions to decide upon their usefulness. Developing robust criteria for discovering useful building blocks in general could help in the design of GP applications for complex problems.
Reference: [Antonisse, 1989] <author> Jim Antonisse, </author> <title> "A New Interpretation of Schema Notation that Overturns the Binary Encoding Constraint," </title> <booktitle> In Proceedings of the Third International Conference on Genetic Algorithms. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <year> 1989. </year>
Reference-contexts: The design of a good representation should reconcile two conflicting goals: allow easy exchange and variation of genetic material and define operators that preserve as much genetic material form the parent strings as possible [Deugo and Oppacher, 1990]. <ref> [Antonisse, 1989] </ref> argues for the idea that a more expressive alphabet carries more power in an informationally equivalent environment. He uses a different interpretation of schema notation, in which don't care symbols quantify over subsets of symbols that can occupy a given string position.
Reference: [Beckenbach and Bellman, 1965] <author> Edwin F. Beckenbach and Richard Bellman, </author> <title> Inequalities, </title> <publisher> Springer Verlag, </publisher> <year> 1965. </year>
Reference: [Davidor, 1989] <author> Yuval Davidor, </author> <title> "Epistasis Variance Suitability of a Representation to Genetic Algorithms," </title> <institution> Department of Applied Mathematics and Computer Science CS89-25, The Weizmann Institute of Science, </institution> <year> 1989. </year>
Reference-contexts: Individuals in the population provide a partial evaluation of schemata they match. GAs work best when the internal representation encourages the emergence of useful building blocks that can subsequently be combined with each other to improve performance. <ref> [Davidor, 1989] </ref> outlines that choosing a representation for a GA is more of an art. The schemata theorem provides only general rules, it does not assess the suitability of a representation.
Reference: [DeJong, 1988] <author> Kenneth DeJong, </author> <title> "Learning with Genetic Algorithms: An Overview," </title> <journal> Machine Learning, </journal> 3(2/3):121-138, 1988. 
Reference-contexts: 1 Introduction The defining characteristic of a learning system is considered to be the capability of adaptation under the implicit goal of performance task improvement <ref> [DeJong, 1988] </ref>. The adaptation process subsumes structural changes, discovery and use of new concepts. Our focus is the Genetic Programming paradigm, as defined in [Koza, 1992]. In GP, evolution can be viewed as structure synthesis, where the format of the structure is in terms of programs.
Reference: [Deugo and Oppacher, 1990] <author> Dwight Deugo and Franz Oppacher, </author> <title> "Explicitely Schema Based Genetic Algorithms," </title> <booktitle> In Proceedings of the Canadian Conference on Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference-contexts: The design of a good representation should reconcile two conflicting goals: allow easy exchange and variation of genetic material and define operators that preserve as much genetic material form the parent strings as possible <ref> [Deugo and Oppacher, 1990] </ref>. [Antonisse, 1989] argues for the idea that a more expressive alphabet carries more power in an informationally equivalent environment. He uses a different interpretation of schema notation, in which don't care symbols quantify over subsets of symbols that can occupy a given string position.
Reference: [Goldberg, 1989] <author> David E. Goldberg, </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: GP can take advantage of its own evolutionary trace in order to discover good genetic material and use it to adapt the search process. 4.1 Foundations In Genetic Algorithms (GA), the schemata theorem [Holland, 1992], <ref> [Goldberg, 1989] </ref> summarizes the effect of fitness-proportionate reproduction, crossover and mutation. Schemata are template strings representing sets of individuals in the search space. Schemata are defined by strings over the (usually binary) alphabet extended with a don't care symbol.
Reference: [Hillis, 1990] <author> W. Daniel Hillis, </author> <title> "Co-evolving Parasites Improve Simulated Evolution as an Optimization Procedure," </title> <editor> In Stephanie Forrest, editor, </editor> <booktitle> Proceedings of the Ninth Annual International Conference of the Center for Nonlinear Studies on Self-organizing, Collective, and Cooperative Phenomena in Natural and Artificial Computing Networks, </booktitle> <pages> pages 228-234. </pages> <publisher> North Holland, </publisher> <year> 1990. </year>
Reference-contexts: E can represent an evolving population too, as in the parasites metaphor (see <ref> [Hillis, 1990] </ref>); (3) define the control parameters (population size M , maximum number of generations G, selection method, fractions of individuals on which each genetic operator is applied, etc.); (4) define a termination criterion (new generations of individuals are created until this criterion is satisfied).
Reference: [Hinton and Nowlan, 1987] <author> Geoffrey E. Hinton and Steven J. Nowlan, </author> <title> "How Learning Can Guide Evolution," </title> <journal> Complex Systems, </journal> <pages> pages 495-502, </pages> <year> 1987. </year>
Reference-contexts: In the even-parity examples, functions correspond to subproblems of the same type (parity problems of a lower dimension). 22 the EVEN-8-PARITY example * Functions can be reused, so it becomes easier to incorporate and reuse good fragments of genetic material. The effect is a restructuring of the search space <ref> [Hinton and Nowlan, 1987] </ref>. Many programs that are far from optimal do not become candidates any more simply because it is less probable to generate unfruitful combinations of initial primitives (compare the "NewFunctions" and "Primitives" plots in the figures from section 7.3). <p> Automatic adaptation of the representation is implemented by extending the function set with fit building blocks discovered. The effect of our approach is a dynamical reshaping of the search space and exemplifies the effect learning, in the form of acquisition of new functions, in the evolutionary process <ref> [Hinton and Nowlan, 1987] </ref>. Extraordinary improvements of the effort needed to find a solution have been observed for problems with regularity in their solutions. This work distinguishes from other approaches to understanding what building blocks are in several major respects.
Reference: [Holland, 1992] <author> John H. Holland, </author> <title> Adaptation in Natural and Artificial Systems, An Introductory Analysis with Applications to Biology, </title> <booktitle> Control and Artificial Intelligence, </booktitle> <publisher> MIT Press, </publisher> <address> second edition, </address> <year> 1992. </year> <month> 27 </month>
Reference-contexts: Most importantly we will give a clear interpretation of what good fragments of genetic material are. GP can take advantage of its own evolutionary trace in order to discover good genetic material and use it to adapt the search process. 4.1 Foundations In Genetic Algorithms (GA), the schemata theorem <ref> [Holland, 1992] </ref>, [Goldberg, 1989] summarizes the effect of fitness-proportionate reproduction, crossover and mutation. Schemata are template strings representing sets of individuals in the search space. Schemata are defined by strings over the (usually binary) alphabet extended with a don't care symbol.
Reference: [Kinnear, 1994] <author> Kim Kinnear, </author> <title> "Alternatives in Automatic Function Definition," </title> <editor> In Kim Kinnear, editor, </editor> <booktitle> Advances in Genetic Programming. </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Each branch has a set of arguments A (only for ADFs), a function set F and a terminal set T which are established in the problem definition The main advantages of the approach are its generality, flexibility, and performance as proved in many examples ([Koza, 1994], <ref> [Kinnear, 1994] </ref>). <p> Moreover, a library of modules has to keep track of all these modules, increasing memory requirements and overhead. Problem specific modularizations emerge, but they might have no meaning from the application's point of view. A performance comparison of ADF and module acquisition is presented in a recent paper <ref> [Kinnear, 1994] </ref>. The author analyzes the performance of the two methods and many other variations of the methods and attributes the better performance of ADF to the repeated use of calls to automatically defined functions and to the multiple use of parameters. <p> For such an organization, the size of individuals and discovered functions is kept within reasonable bounds while the structural complexity of individuals is much bigger. The power of GP using automatically discovered functions to solve problems is significantly increased too ([Koza, 1992], [Koza, 1994], <ref> [Kinnear, 1994] </ref>) (see also section 7). Moreover, the descriptional complexity can be used as a measure for the fitness of an individual T that would drive GP towards discovering solutions with a smaller descriptional complexity.
Reference: [Koza, 1992] <author> John R. Koza, </author> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction The defining characteristic of a learning system is considered to be the capability of adaptation under the implicit goal of performance task improvement [DeJong, 1988]. The adaptation process subsumes structural changes, discovery and use of new concepts. Our focus is the Genetic Programming paradigm, as defined in <ref> [Koza, 1992] </ref>. In GP, evolution can be viewed as structure synthesis, where the format of the structure is in terms of programs. <p> However changing individual program instructions is most often unfruitful, so extensions to GP have tried to randomly define and use larger components such as new functions (automatic function definition or ADF in <ref> [Koza, 1992] </ref>) or modules (in [Angeline and Pollack, 1994]). This paper presents an alternative approach to structure synthesis different from either ADF or modules in several respects. We show that efficient structure synthesis can be obtained by discovering useful genetic material and using it globally to adapt the problem representation. <p> The ideas introduced are analyzed in section 7 using even-parity as a test case. Conclusions and future work directions are finally presented. 2 Overview of the GP Paradigm A concise description of the paradigm, its main parameters and discussions of some advanced topics are presented in <ref> [Koza, 1992] </ref>, [Koza, 1994]. Here we just overview some basic concepts and notations used throughout this paper. In GP, problem solving is formulated as a search in the space of computer programs, structures of dynamically varying size and shape. <p> This function appears to be difficult to learn in a GP implementation especially for values of n greater than 5 <ref> [Koza, 1992] </ref>. Formally, the even-n-parity problem is to find a functional program representing a logical composition of primitive boolean functions that computes the sum of input bits over the field of integers modulo 2. <p> The question is then how can we improve the performance of GP for very sparse solution spaces? 4 3 Related Work In the next two sections we summarize two extensions to GP that point out the possibility of evolution of new functions <ref> [Koza, 1992] </ref> or emergence of modules [Angeline and Pollack, 1994] as possible ways to cope with complex problems. 3.1 Automatic definition of functions Automatic definition of functions (ADF) is an extension of the GP paradigm to cope with the automatic decomposition of a solution function [Koza, 1992]. <p> of evolution of new functions <ref> [Koza, 1992] </ref> or emergence of modules [Angeline and Pollack, 1994] as possible ways to cope with complex problems. 3.1 Automatic definition of functions Automatic definition of functions (ADF) is an extension of the GP paradigm to cope with the automatic decomposition of a solution function [Koza, 1992]. In ADF-based GP each individual has a fixed number of components: functions to be automatically evolved (having a fixed number of parameters) and result producing branches. <p> A graphical interface for representing program trees and structure trees has been implemented in Common Lisp Object System (CLOS) and Common Lisp Interface Manager v2.0 (CLIM) for Sun Systems. The AR kernel has been built built on top of the standard GP LISP code described in <ref> [Koza, 1992] </ref>. The even-n-parity problem (introduced in section 2.1) is solvable by problem decomposition into simpler subproblems and thus represents a good test bench for the discovery of more and more complex building blocks. 16 We used the following parameter settings. Population size was M = 4000. <p> The fitness proportionate reproduction fraction was 0.1. We have not experimented with other values of these parameters, but rather have used the values reported in <ref> [Koza, 1992] </ref> for result comparability reasons. The block fitness function was the same as the fitness function, with one slight change. Hits are evaluated on a subset of the set of fitness cases, determined by fixing the values of variables not used in the block to arbitrary values. <p> All functions on a given level are independent (discovered over one or more generations), and higher levels correspond to higher generation numbers. For higher order even-parity problems it becomes harder and harder to find solutions without using the adaptive framework. The computational effort increases considerably as reported in <ref> [Koza, 1992] </ref>. The adaptive representation method discovers solutions to harder and harder problems, creating a hierarchy of functions like the one in figure 12 and enabling a problem decomposition into subproblems. 7.3 Evolution of complexity and the average values over the population.
Reference: [Koza, 1994] <editor> John R. Koza, </editor> <booktitle> Genetic Programming II, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1994. </year>
Reference-contexts: The ideas introduced are analyzed in section 7 using even-parity as a test case. Conclusions and future work directions are finally presented. 2 Overview of the GP Paradigm A concise description of the paradigm, its main parameters and discussions of some advanced topics are presented in [Koza, 1992], <ref> [Koza, 1994] </ref>. Here we just overview some basic concepts and notations used throughout this paper. In GP, problem solving is formulated as a search in the space of computer programs, structures of dynamically varying size and shape. <p> In addition the sparseness in solutions of the search space increases as the problem is scaled up, as is the case with the results for many problems reported in <ref> [Koza, 1994] </ref>. <p> This has been done mostly in an empirical way. For example, individuals with too big a size, number of state-changing operations (as in the lawn mower or artificial ant problems described in <ref> [Koza, 1994] </ref>) or "running time" are penalized. It is very important how we measure size or running time. This section outlines the differences between different measures that can be used. Suppose an individual is represented by a program which calls discovered functions which may call older discovered functions. <p> For such an organization, the size of individuals and discovered functions is kept within reasonable bounds while the structural complexity of individuals is much bigger. The power of GP using automatically discovered functions to solve problems is significantly increased too ([Koza, 1992], <ref> [Koza, 1994] </ref>, [Kinnear, 1994]) (see also section 7). Moreover, the descriptional complexity can be used as a measure for the fitness of an individual T that would drive GP towards discovering solutions with a smaller descriptional complexity. <p> 3 brings quantitative experimental evidence. 24 EVEN-3 EVEN-4 EVEN-5 EVEN-8 Method gen SC gen SC gen SC gen SC AR-GP 2 17 3 15 5 32 10 41 ADF-GP # 3 48 10 60 28 157 24 186 Table 3: Comparison of results (rounded figures): AR-GP vs. results reported in <ref> [Koza, 1994] </ref>, marked ( # ). <p> Rows 2 and 3 present some comparative results taken from <ref> [Koza, 1994] </ref> for sample runs of GP with similar parameter values, but M = 16000. 8 Conclusions Automatic discovery of problem representation primitives is a challenging goal that has been tackled in recent research in Machine Learning in general, and Genetic Programming in particular. <p> Selection is based on 25 estimated block usefulness determined by an analysis of the GP trace. Generalization iden-tifies block leaves labeled identically. Use is based on the adaptation of the function set. No need for more genetic operators appears. The difference from <ref> [Koza, 1994] </ref> consists in the knowledge-driven strategy of defining functions. Functions are created based on the usefulness criteria and thus have a clear semantics. They can be called from any member of the population.
Reference: [Li and Vitanyi, 1993] <author> Ming Li and Paul Vitanyi, </author> <title> An Introduction to Kolmogorov Complexity and its Applications, </title> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: of a decision tree that best explains a set of examples [Quinlan and Rivest, 1989], the construction of a finite automaton or the inference of a boolean function that satisfies a set of constraints are all problems that match the described pattern and can be solved using the MDL principle <ref> [Li and Vitanyi, 1993] </ref>. MDL is also called stochastic complexity. The MDL principle advocates in favor of a hierarchical representation of evolved programs (see appendix A). A more complex behavior can be obtained based on a more complex, hierarchical organization.
Reference: [Michalski, 1983] <author> Ryszard S. Michalski, </author> <title> "A Theory and Methodology of Inductive Learning," </title> <editor> In Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell, editors, </editor> <booktitle> Machine Learning, An Artificial Intelligence Approach, </booktitle> <pages> pages 83-129. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference-contexts: Furthermore useful blocks tend to be small and the process can be applied recursively to discover more and more complex blocks. A block can be generalized by substituting each occurrence of a terminal of the block by a variable (descriptive generalization in <ref> [Michalski, 1983] </ref>). This process transforms a block into a parameterized function whose parameters are given by the block variables. Figure 5 presents an example of a function F b generated in this approach. Steps 3a and 3b from figure 6 implement this idea (see section 6).
Reference: [Quinlan and Rivest, 1989] <author> J. Ross Quinlan and Ronald L. Rivest, </author> <title> "Inferring Decision Trees Using the Minimum Description Length Principle," </title> <booktitle> Information and Computation, </booktitle> <pages> pages 227-248, </pages> <year> 1989. </year>
Reference-contexts: It states that the best theory to explain a set of data is one which minimizes the length of the data description together with the hypothesis description. In general, problems such as the inference of a decision tree that best explains a set of examples <ref> [Quinlan and Rivest, 1989] </ref>, the construction of a finite automaton or the inference of a boolean function that satisfies a set of constraints are all problems that match the described pattern and can be solved using the MDL principle [Li and Vitanyi, 1993]. MDL is also called stochastic complexity.
Reference: [Simon, 1973] <author> Herbert A. Simon, </author> <title> "The Organization of Complex Systems," </title> <editor> In G. Braziller Howard H. Pattee, editor, </editor> <booktitle> Hierarchy Theory; The Challenge of Complex Systems, </booktitle> <pages> pages 3-27. </pages> <address> New York, </address> <year> 1973. </year>
Reference-contexts: The discovery of stable components when solving a problem could speed the search process, as the time needed for the complex system to evolve based on its subsystems is much shorter than if the system evolves from its elementary parts <ref> [Simon, 1973] </ref>. We claim that in GP genotypic features (building blocks) emerge that correspond to such stable components. They can be discovered in the population using a process of generalization of substructures (blocks) already incorporated in the individuals.
Reference: [Tackett, 1993] <author> Walter Alden Tackett, </author> <title> "Genetic Programming for Feature Discovery and Image Discrimination," </title> <booktitle> In Proceedings of the Fifth International Conference on Genetic Algorithms. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <year> 1993. </year>
Reference-contexts: An attempt to explain the course of evolution in GP based on an understanding of what building blocks are appears in <ref> [Tackett, 1993] </ref>. The idea that frequent subtrees in one individual correspond to synthesized features suggests the conclusion that those subtrees comprise "building blocks". 7 4 Building Blocks We will cope with some of the problems of these earlier approaches.
Reference: [Winston, 1992] <author> Patrick Henry Winston, </author> <booktitle> Artificial Intelligence, 3rd edition, </booktitle> <publisher> Addison-Wesley Pub. Co., </publisher> <year> 1992. </year> <month> 28 </month>
Reference-contexts: A useful measure is normalized fitness. If performance of individual i is c (i) 0 (also called standardized fitness) then a normalized 1 fitness measure of individual i is c (i) P i c (i) . Normalized fitness can be also defined based on ranking <ref> [Winston, 1992] </ref> or other ad-hoc methods. Two selection methods are currently used. A first one, tournament selection, is described in [Angeline and Pollack, 1993a]. We randomly choose a small group of individuals and let them play a "tournament". The winner represents the individual selected. <p> The hierarchical approach makes the problems tractable. A shift in representation really transforms a very difficult problem into an easy one. * The the principle of diversity "being different can be as good as being fit" <ref> [Winston, 1992] </ref> has a higher probability of being satisfied when using an adaptive representation. With ADF or AR the scalability of the even parity problem improves significantly. We have argued from a theoretical point of view that hierarchical structures are more powerful than structures based on the initial function set.
References-found: 22

