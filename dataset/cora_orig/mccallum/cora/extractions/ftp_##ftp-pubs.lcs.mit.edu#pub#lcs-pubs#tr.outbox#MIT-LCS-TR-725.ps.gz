URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-725.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/tr700.html
Root-URL: 
Title: Managing Scheduled Routing With A High-Level Communications Language  
Author: by Christopher D. Metcalf Stephen A. Ward Arthur C. Smith 
Degree: 1988 Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  All rights reserved. Author  Certified by  Professor of Electrical Engineering and Computer Science Thesis Supervisor Accepted by  Chairman, Departmental Committee on Graduate Students  
Date: SEPTEMBER 1997  August 19, 1997  
Affiliation: B.S., M.S., Computer Science Yale University,  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology 1997.  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Anant Agarwal et al. </author> <title> The MIT Alewife machine: Architecture and performance. </title> <booktitle> In Annual International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: More modern machines (e.g. recent Cray machines [36]) have much higher-speed interfaces and networks; furthermore, recent work in communication techniques is helping to lower or mask latencies further (e.g. active messages [66] and rapid context switching <ref> [1, 61] </ref>). Communications, however, can still be a serious performance constraint. Two approaches to reducing latency and increasing bandwidth are possible. One is to reduce cycle times in the network, thus moving data through the network more quickly, and directly improving both latency and bandwidth.
Reference: [2] <author> Kazuhiro Aoyama and Andrew A. Chien. </author> <title> The cost of adaptivity and virtual lanes in a wormhole router. </title> <journal> Journal of VLSI Design, </journal> <volume> 2(4):315333, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: This allows scheduled routing to perform flow control within a single inter-node transfer time. Larger buffers and asynchronous flow-control signals can be used to avoid round-trip handshakes for dynamic routers as well, but this is costly in terms of pin resources and/or buffer management. In <ref> [2] </ref>, a comparison of dynamic routers is given, breaking down all the components of the cost. A simple deterministic router's cycle time is quoted at 9.8 ns, with a simple planar-adaptive router taking 11.4 ns.
Reference: [3] <author> Jonathan Babb et al. </author> <title> The RAW benchmark suite: Computation structures for general purpose computing. </title> <booktitle> In IEEE Symposium on Field-Programmable Custom Computing Machines, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: Most of the discussion is also applicable to the case where the scheduled router has dynamic-routing hardware available. The current implementation of COP relies on an interrupt-driven system that provides buffering in processor RAM for messages. A future implementation of NuMesh (or a similar system, such as RAW <ref> [3, 67] </ref>) could include traditional online-routing functionality in hardware. Online routing is handled by providing one-hop streams between all adjacent nodes involved. One VFSM reads an interface address dedicated to, e.g., sending data to +x, and routes the data out the +x port.
Reference: [4] <author> Jonathan Babb, Matthew Frank, and Anant Agarwal. </author> <title> Solving graph problems with dynamic computation structures. </title> <booktitle> In SPIE's Intl. Symp. on Voice, Video and Data Communications, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: model is inadequate, but it does have the advantage that it is relatively straightforward to implement and requires no close coupling between the stream router and the high-level language (HLL) compiler. * Create the necessary schedules by hand for each application (or perhaps by an application specific compiler, similar to <ref> [4] </ref>), allowing for any necessary data-dependency and schedule 25 switching by a close knowledge of the particular communications schedules created for that application. This solution, while allowing the necessary power for the given appli cation, is insufficiently general. Instead, let us consider the more general compilation possibilities.
Reference: [5] <author> John Beetem, Monty Denneau, and Don Weingarten. </author> <title> The GF11 supercomputer. </title> <booktitle> In The 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 108115, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: NuMesh extends upon their work in several ways, as discussed more extensively in [56]; in particular, NuMesh allows faster communications, more precise control over stream bandwidths, and faster phase changing and other router modifications. 3.2.2 GF11 The GF11 parallel computer from Yorktown <ref> [5] </ref> is a SIMD architecture with 576 processors and an unusual interconnect. The processors are connected through a three-stage Benes network, capable of supporting any permutation of the processors, and reconfigurable among 1024 distinct configurations in a single cycle.
Reference: [6] <author> Francine Berman. </author> <title> Experience with an automatic solution to the mapping problem. </title> <booktitle> In The Characteristics of Parallel Algorithms, </booktitle> <pages> pages 307334. </pages> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: This is fairly restricted, since it neither captures multiple phases of communi cation nor allows a way of specifying stream usage information. * The Prep-P automatic mapping system <ref> [6] </ref> includes a graph description language GDL, closely tied to their programming language XX.
Reference: [7] <author> Ronald P. Bianchini, Jr. and John Paul Shen. </author> <title> Interprocessor traffic scheduling algorithm for multiple-processor networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(4):396409, </volume> <month> April </month> <year> 1987. </year>
Reference-contexts: They found that their scheme was able to guarantee consistent message arrival times under most loads, whereas dynamic routing incurred frequent output inconsistency, causing stalling of the entire application. 50 3.3.2 Bianchini-Shen Bianchini and Shen <ref> [7] </ref> presented a methodology for generating routing schedules for static problems, using an optimal polynomial-time algorithm. They examine `traffic compilation' once data operators are mapped onto nodes, optimizing for overall bandwidth in the system. Their network traffic scheduler iterates through the following four steps.
Reference: [8] <author> Jeffrey C. Bier, Edward A. Lee, et al. Gabriel: </author> <title> A design environment for DSP. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 2845, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: A good example of this is the Gabriel project <ref> [8, 27] </ref>, now the Ptolemy project [12, 13]. Their system uses block diagrams and a flexible signal-flow model to support multirate and asynchronous systems. It uses a synchronous dataflow, or Sdf model for DSP applications.
Reference: [9] <author> S. Wayne Bollinger and Scott F. Midkiff. </author> <title> Processor and link assignment in multicomput-ers using simulated annealing. </title> <booktitle> International Conference on Parallel Processing, </booktitle> <pages> pages 17, </pages> <year> 1988. </year>
Reference-contexts: D. Dahl showed an algorithm for scheduled routing on the CM-2 using a swap primitive [17]. His algorithm does no backtracking to figure the schedule and assumes unlim ited buffering for data in the nodes. * Bollinger and Midkiff presented a simulated annealing algorithm for link assignment <ref> [9] </ref>. Their algorithm essentially precomputes adaptive routes but assumes online routing (presumably non-adaptive) at runtime. * Shin and Kandlur present a virtual cut-through route assignment algorithm [32]. As for Bollinger and Midkiff, they attempt to precompute adaptive routes for a known traffic distribution.
Reference: [10] <author> Shekhar Borkar, Robert Cohn, George Cox, et al. </author> <title> Supporting systolic and memory communication in iWarp. </title> <type> Technical Report 90-197, </type> <institution> CMU-CS, </institution> <month> September </month> <year> 1990. </year> <note> See also Proceedings 17th SIGARCH. </note>
Reference-contexts: Languages in this category include, for example, Linda [14], Split-C [16], and CSP [29]. 3.2 Scheduled Routing Architectures There are a number of existing scheduled-routing architectures discussed in the literature. Two of them are presented here, iWarp and GF11. 3.2.1 iWarp The iWarp architecture <ref> [54, 10, 11] </ref> is CMU and Intel's follow-on project to the Warp architecture. iWarp integrates the processing and routing units on a single chip, targeted to DSP, scientific, and image processing.
Reference: [11] <author> Shekhar Borkar et al. </author> <title> iWarp: An integrated solution to high-speed parallel computing. </title> <booktitle> In Proceedings of Supercomputing '88, </booktitle> <month> November </month> <year> 1988. </year> <month> 152 </month>
Reference-contexts: Languages in this category include, for example, Linda [14], Split-C [16], and CSP [29]. 3.2 Scheduled Routing Architectures There are a number of existing scheduled-routing architectures discussed in the literature. Two of them are presented here, iWarp and GF11. 3.2.1 iWarp The iWarp architecture <ref> [54, 10, 11] </ref> is CMU and Intel's follow-on project to the Warp architecture. iWarp integrates the processing and routing units on a single chip, targeted to DSP, scientific, and image processing.
Reference: [12] <author> Joseph Buck, Soonhoi Ha, Edward A. Lee, and David G. Messerschmitt. </author> <title> Multirate signal processing in Ptolemy. </title> <booktitle> In Proceedings of ICASSP, </booktitle> <month> April </month> <year> 1991. </year> <institution> Toronto, Canada. </institution>
Reference-contexts: A good example of this is the Gabriel project [8, 27], now the Ptolemy project <ref> [12, 13] </ref>. Their system uses block diagrams and a flexible signal-flow model to support multirate and asynchronous systems. It uses a synchronous dataflow, or Sdf model for DSP applications.
Reference: [13] <author> Joseph Buck, Soonhoi Ha, Edward A. Lee, and David G. Messerschmitt. Ptolemy: </author> <title> A platform for heterogeneous simulation and prototyping. </title> <booktitle> In Proceedings of 1991 European Simulation Conference, </booktitle> <month> June </month> <year> 1991. </year> <institution> Copenhagen, Denmark. </institution>
Reference-contexts: A good example of this is the Gabriel project [8, 27], now the Ptolemy project <ref> [12, 13] </ref>. Their system uses block diagrams and a flexible signal-flow model to support multirate and asynchronous systems. It uses a synchronous dataflow, or Sdf model for DSP applications.
Reference: [14] <author> Nicholas Carriero and David Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4):444458, </volume> <month> April </month> <year> 1989. </year>
Reference-contexts: Some languages, such as HPF [46] or Crystal [15], provide high-level communications operators; these operators easily allow for suitable communication schedules to be generated. Other languages, such as Mul-T [38] and Linda <ref> [14] </ref>, have less emphasis on high-level operations and are harder to extract structured communications from. For languages with lower-level primitives, the burden falls to the compiler to extract the necessary structured communications. <p> Such languages are largely outside the realm of related work, since they meet neither of the two criteria for communications languages. Languages in this category include, for example, Linda <ref> [14] </ref>, Split-C [16], and CSP [29]. 3.2 Scheduled Routing Architectures There are a number of existing scheduled-routing architectures discussed in the literature.
Reference: [15] <author> Marina C. Chen. </author> <title> A design methodology for synthesizing parallel algorithms and architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 461491, </pages> <year> 1986. </year>
Reference-contexts: While the thesis explicitly ignores the issue of converting high-level language inputs into structured communications, this process can be either helped or hindered by the choice of high-level language. Some languages, such as HPF [46] or Crystal <ref> [15] </ref>, provide high-level communications operators; these operators easily allow for suitable communication schedules to be generated. Other languages, such as Mul-T [38] and Linda [14], have less emphasis on high-level operations and are harder to extract structured communications from.
Reference: [16] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumeta, and T. von Eicken. </author> <title> Introduction to Split-C. </title> <booktitle> In Proceedings of Supercomputing, </booktitle> <year> 1993. </year>
Reference-contexts: Such languages are largely outside the realm of related work, since they meet neither of the two criteria for communications languages. Languages in this category include, for example, Linda [14], Split-C <ref> [16] </ref>, and CSP [29]. 3.2 Scheduled Routing Architectures There are a number of existing scheduled-routing architectures discussed in the literature.
Reference: [17] <author> E. Denning Dahl. </author> <title> Mapping and compiled communication on the Connection Machine system. </title> <booktitle> In The Fifth Distributed Memory Computing Conference, </booktitle> <pages> pages 756766, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: D. Dahl showed an algorithm for scheduled routing on the CM-2 using a swap primitive <ref> [17] </ref>. His algorithm does no backtracking to figure the schedule and assumes unlim ited buffering for data in the nodes. * Bollinger and Midkiff presented a simulated annealing algorithm for link assignment [9].
Reference: [18] <author> W. J. Dally and C. L. Seitz. </author> <title> Deadlock-free message routing in multiprocessor interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(5):547553, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: compromising the ability of high-bandwidth streams to run at their full designated bandwidth. 1.2.5 Header Traffic All online routing algorithms require some form of overhead associated with each message, typically in the form of a message header of one or two words holding the destination address, length, and so forth <ref> [18] </ref>. Offline-routed messages generally require no packet headers, since the message is identified by the scheduled time of transmission rather than by any data that it carries with it.
Reference: [19] <author> William J. Dally. </author> <title> Express cubes: Improving the performance of k-ary n-cube interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(9):10161023, </volume> <month> September </month> <year> 1991. </year>
Reference-contexts: One extension not currently implemented in the COP compiler is to allow non-nearest-neighbor streams. One simple version of this would be express channels <ref> [19] </ref>. Some nodes would have an additional set of streams available to them beyond the basic nearest-neighbor sets; these streams would connect to other nodes distant in the mesh.
Reference: [20] <author> William J. Dally. </author> <title> Virtual-channel flow control. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2):194205, </volume> <month> March </month> <year> 1991. </year>
Reference-contexts: The simulator handles standard Cartesian and diamond-lattice dynamic routing methods, as well as a precomputed shortest-path technique that can be used for arbitrary networks. Routing protocols currently supported include standard E-cube (and variants for diamond-lattice and arbitrary networks) and the dynamic adaptive routing protocol discussed in <ref> [20] </ref>. 7.2.1 Disambiguating Messages The implementation of COP for the chosen dynamic router model allows different messages to be delivered to different interface addresses. This technique provides a more level playing ground for comparing dynamic routing to scheduled routing.
Reference: [21] <author> William J. Dally and Hiromichi Aoki. </author> <title> Deadlock-free adaptive routing in multicomputer networks using virtual channels. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(4), </volume> <month> April </month> <year> 1993. </year>
Reference-contexts: With scheduled routing, good paths can be found for all the communications; techniques. 512 data words are transferred between each pair of nodes with bitreversed addresses. Regular e-cube dynamic routing is shown, as is a simple adaptive routing technique from <ref> [21] </ref>, and a scheduled-routing algorithm. The scheduled-routing algorithm schedules a route for all the streams at compile time by rerouting streams one at a time until it finds a good path, using only per-link bandwidths to determine legal routes. <p> Transpose This experiment compares transpose performance rotating a 3-dimensional array from (x; y; z) to (y; z; x). The scheduled-routing backend is contrasted to two dynamic routing algorithms both the traditional oblivious algorithm, as well as a somewhat more sophisticated adaptive algorithm (Dally's adaptive dimension-reversal router) <ref> [21] </ref>. A small benchmark case, with 2KB of data on each node, is examined. The dynamic router uses 8 words (32 bytes) of buffering on each link, with five virtual channels on each link for the adaptive algorithm.
Reference: [22] <author> Dawson R. Engler, M. Frans Kaashoek, and James O'Toole, Jr. Exokernel: </author> <title> An operating system architecture for application-level resource management. </title> <booktitle> In Proceedings of the Fifteenth Symposium on Operating System Principles, </booktitle> <year> 1995. </year>
Reference-contexts: Direct access to the physical hardware would be mediated by some combination of hardware support (as in FUGU [47]) and operating-system support (as is done in the Exokernel <ref> [22] </ref>).
Reference: [23] <author> J. Flower and A. Kolawa. </author> <title> Express is not just a message-passing system: Current and future directions in Express. </title> <booktitle> Parallel Computing, </booktitle> <address> 20(4):497614, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: They include support for a synchronization primitive to allow changing phases during execution of the dataflow graph. * Express <ref> [23] </ref> and PVM [62] both provide some notion of connections, but they are only set up at runtime, and provide a layer of buffering that reduces performance. Their library primitives provide similar functionality to COP's operators.
Reference: [24] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: With the overlay model, PCS allows the mesh to be subdivided along different hierarchical lines in each phase, thus adding a degree of flexibility to the phases. 46 3.1.4 The CrOS Project The CrOS project from CalTech <ref> [24] </ref> uses a crystalline programming framework. The authors present an MIMD model with nodes calling library functions to communicate with each other. The CrOS target hardware did not have a separate router, and accordingly the processors themselves performed the communication over dedicated point-to-point links.
Reference: [25] <author> T. Gross, A. Hasegawa, S. Hinrichs, D. O'Hallaron, and T. Stricker. </author> <title> The impact of communication style on machine resource usage for the iWarp parallel processor. </title> <type> Technical Report 92-215, </type> <institution> CMU-CS, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Similarly, a node can write out a message, then strip the header from an incoming message so as to leave the two messages merged as one. Systolic communication was found to perform very well on applications for which it could be used <ref> [25] </ref>. In general, applications performed better when more systolic paths were established, since communication latencies were lower. iWarp's system support includes a number of compilers for C and FORTRAN, as well as compilers for image processing and the like.
Reference: [26] <author> David B. Gustavson. </author> <title> The Scalable Coherent Interface and related standards projects. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 1022, </pages> <month> February </month> <year> 1992. </year> <month> 153 </month>
Reference-contexts: For the sake of fixed units, this chapter assumes a dynamic router at 1 GHz (as is currently the case, for example, for SCI <ref> [26] </ref>), and a scheduled router at 2 GHz.
Reference: [27] <author> Soonhoi Ha and Edward Ashford Lee. </author> <title> Compile-time scheduling and assignment of dataflow program graphs with data-dependent iteration. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(11), </volume> <month> November </month> <year> 1991. </year>
Reference-contexts: A good example of this is the Gabriel project <ref> [8, 27] </ref>, now the Ptolemy project [12, 13]. Their system uses block diagrams and a flexible signal-flow model to support multirate and asynchronous systems. It uses a synchronous dataflow, or Sdf model for DSP applications.
Reference: [28] <author> Susan Hinrichs. </author> <title> Simplifying connection-based communication. </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <address> 3(1):2536, </address> <month> Spring </month> <year> 1995. </year>
Reference-contexts: the application can define suitable ring_edge and chordal_edge functions in Lisp, then create the desired phase information by using Lisp's basic control primitives, or one of COP's iterating constructs (such as all). 3.1.3 The PCS Tool Chain A somewhat similar approach is used in the Programmed Communication Service tool chain <ref> [28] </ref>, developed as part of the iWarp effort. The authors share the opinion of the NuMesh project, that long-term communication connections can both be beneficial to performance as well as prove usable to programmers.
Reference: [29] <author> C. A. R. Hoare. </author> <title> Communicating sequential processes. </title> <journal> Communications of the ACM, </journal> <volume> 21(8):666677, </volume> <month> August </month> <year> 1978. </year>
Reference-contexts: Such languages are largely outside the realm of related work, since they meet neither of the two criteria for communications languages. Languages in this category include, for example, Linda [14], Split-C [16], and CSP <ref> [29] </ref>. 3.2 Scheduled Routing Architectures There are a number of existing scheduled-routing architectures discussed in the literature.
Reference: [30] <author> Yuan-Shin Hwang, Raja Das, Joel Saltz, Bernard Brooks, and Milan Hodoscek. </author> <title> Paral-lelizing molecular dynamics programs for distributed memory machines: an application of the CHAOS runtime support library. </title> <type> Technical Report CS-TR-3374, </type> <institution> University of Maryland, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Without the ability to extract information at compile-time, however, they do an `inspector'-style step where routing information is collected in the first phase, then subsequent phases use the collected information to avoid having to examine all possible streams for input. 3.1.5 Chaos and PARTI The Chaos <ref> [55, 30] </ref> and PARTI [64] work from the University of Maryland includes some of the same conceptual framework that COP does. The goal is to support irregular distributions of data in applications, unlike the standard block and cyclic distributions applied to regular data.
Reference: [31] <author> Kirk Johnson. </author> <title> High-Performance All-Software Distributed Shared Memory. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: More sophisticated memory semantics (such as Fetch-and-Op) could also be included at relatively low incremental cost in the handler. This implementation of shared memory could easily be used under a software shared memory implementation such as CRL <ref> [31] </ref>. Online-routing hardware could also be included directly on the router. Online-routing timeslots would be scheduled just like any other data transfer.
Reference: [32] <author> Dilip D. Kandlur and Kang G. Shin. </author> <title> Traffic routing for multicomputer networks with virtual cut-through capability. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(10):12571270, </volume> <month> Oc-tober </month> <year> 1992. </year>
Reference-contexts: Their algorithm essentially precomputes adaptive routes but assumes online routing (presumably non-adaptive) at runtime. * Shin and Kandlur present a virtual cut-through route assignment algorithm <ref> [32] </ref>. As for Bollinger and Midkiff, they attempt to precompute adaptive routes for a known traffic distribution. The algorithm used is an incremental greedy path router. 51 Chapter 4 Managing Data Dependency This chapter describes the techniques used to manage data dependency in a reprogrammable scheduled router.
Reference: [33] <author> Simon M. Kaplan and Gail E. Kaiser. Garp: </author> <title> graph abstractions for concurrent programming. </title> <booktitle> In The 2nd European Symposium on Programming, </booktitle> <pages> pages 191205, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Its capabilities are similar to those of Conic. * The Assign parallel program generator [53] handles partitioning, mapping and routing for static coarse-grained flowgraphs, similar to those used for Gabriel. * GARP (graph abstractions for concurrent programming <ref> [33] </ref>) is a graph-rewriting language representing connections among agents. Communications occur only along graph edges. The language, while elegant, lacks notions of communication phases, runtime streams, and stream usage specifiers. * Lee and Bier [39] discuss mapping a limited class of dataflow program graphs to a scheduled routing environment.
Reference: [34] <author> Philip Klein, Ajit Agrawal, R. Ravi, and Satish Rao. </author> <title> Approximation through multicom-modity flow. </title> <booktitle> In 31st Symposium on the Foundations of Computer Science, </booktitle> <volume> volume II, </volume> <pages> pages 726737, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: A new schedule is presented, by rerouting traffic from saturated to unsaturated links. 4. The previous two steps are iterated until an optimal traffic schedule results. The traffic-smoothing technique they present is a version of multi-commodity flow, much discussed in the literature <ref> [41, 34, 35, 40] </ref>. The general form of the problem is to model a network graph as fluid pipes, where distinct communication source/destination pairs correspond to distinct commodities that must push through the pipes to reach their destination.
Reference: [35] <author> Philip Klein, Clifford Stein, and Eva Tardos. </author> <title> Leighton-Rao might be practical: faster approximation algorithms for concurrent flow with uniform capacities. </title> <booktitle> In 22nd ACM Symposium on Theory of Computing, </booktitle> <pages> pages 310321, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: A new schedule is presented, by rerouting traffic from saturated to unsaturated links. 4. The previous two steps are iterated until an optimal traffic schedule results. The traffic-smoothing technique they present is a version of multi-commodity flow, much discussed in the literature <ref> [41, 34, 35, 40] </ref>. The general form of the problem is to model a network graph as fluid pipes, where distinct communication source/destination pairs correspond to distinct commodities that must push through the pipes to reach their destination.
Reference: [36] <author> R. K. Koeninger, M. Furtney, and M. Walker. </author> <title> A shared MPP from Cray Research. </title> <journal> Digital Technical Journal, </journal> <volume> 6(2):821, </volume> <month> spring </month> <year> 1994. </year>
Reference-contexts: Introduction One of the most critical issues in multiprocessors today is managing the communications among processors. Traditionally, communication in multiprocessors was a major bottleneck, as was the case for the early Intel multiprocessors [37], for example. More modern machines (e.g. recent Cray machines <ref> [36] </ref>) have much higher-speed interfaces and networks; furthermore, recent work in communication techniques is helping to lower or mask latencies further (e.g. active messages [66] and rapid context switching [1, 61]). Communications, however, can still be a serious performance constraint. Two approaches to reducing latency and increasing bandwidth are possible.
Reference: [37] <author> A. Kolawa and S. W. Otto. </author> <title> Performance of the Mark II and Intel hypercubes. </title> <booktitle> In Hypercube Multiprocessors, </booktitle> <year> 1986. </year>
Reference-contexts: Introduction One of the most critical issues in multiprocessors today is managing the communications among processors. Traditionally, communication in multiprocessors was a major bottleneck, as was the case for the early Intel multiprocessors <ref> [37] </ref>, for example. More modern machines (e.g. recent Cray machines [36]) have much higher-speed interfaces and networks; furthermore, recent work in communication techniques is helping to lower or mask latencies further (e.g. active messages [66] and rapid context switching [1, 61]). Communications, however, can still be a serious performance constraint.
Reference: [38] <author> David A. Kranz, Robert H. Halstead, Jr., and Eric Mohr. Mul-T: </author> <title> a high-performance parallel Lisp. </title> <booktitle> In Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 8190, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Some languages, such as HPF [46] or Crystal [15], provide high-level communications operators; these operators easily allow for suitable communication schedules to be generated. Other languages, such as Mul-T <ref> [38] </ref> and Linda [14], have less emphasis on high-level operations and are harder to extract structured communications from. For languages with lower-level primitives, the burden falls to the compiler to extract the necessary structured communications.
Reference: [39] <author> Edward Ashford Lee and Jeffery C. Bier. </author> <title> Architectures for statically scheduled dataflow. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10:333348, </volume> <year> 1990. </year>
Reference-contexts: Communications occur only along graph edges. The language, while elegant, lacks notions of communication phases, runtime streams, and stream usage specifiers. * Lee and Bier <ref> [39] </ref> discuss mapping a limited class of dataflow program graphs to a scheduled routing environment.
Reference: [40] <author> Tom Leighton, Fillia Makedon, Serge Plotkin, Clifford Stein, Eva Tardos, and Spyros Tragoudas. </author> <title> Fast approximation algorithms for multicommodity flow problems. </title> <booktitle> In 23rd ACM Symposium on Theory of Computing, </booktitle> <pages> pages 101111, </pages> <month> May </month> <year> 1991. </year> <month> 154 </month>
Reference-contexts: A new schedule is presented, by rerouting traffic from saturated to unsaturated links. 4. The previous two steps are iterated until an optimal traffic schedule results. The traffic-smoothing technique they present is a version of multi-commodity flow, much discussed in the literature <ref> [41, 34, 35, 40] </ref>. The general form of the problem is to model a network graph as fluid pipes, where distinct communication source/destination pairs correspond to distinct commodities that must push through the pipes to reach their destination.
Reference: [41] <author> Tom Leighton and Satish Rao. </author> <title> An approximate max-flow min-cut theorem for uniform multicommodity flow problems with applications to approximation algorithms. </title> <booktitle> In 29th Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 422431, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: A new schedule is presented, by rerouting traffic from saturated to unsaturated links. 4. The previous two steps are iterated until an optimal traffic schedule results. The traffic-smoothing technique they present is a version of multi-commodity flow, much discussed in the literature <ref> [41, 34, 35, 40] </ref>. The general form of the problem is to model a network graph as fluid pipes, where distinct communication source/destination pairs correspond to distinct commodities that must push through the pipes to reach their destination.
Reference: [42] <author> Virginia M. Lo, Sanjay Rajopadhye, et al. LaRCS: </author> <title> A language for describing parallel computations. </title> <institution> Department of Computer and Information Science, Univesity of Oregon. </institution>
Reference-contexts: The inputs and outputs are specified as indivisible messages of size order and 2fiorder. Internal state, an initialization function, and the main processing function are all named at the end. 3.1.2 OREGAMI/LaRCS One language close in spirit to COP is LaRCS <ref> [42] </ref>, the language component of the University of Oregon's OREGAMI system [43, 44]. The OREGAMI system is a set of software tools for automatic mapping of parallel computations to parallel architectures. LaRCS (the Language for Regular Communication Structures) is a description language in the spirit of COP.
Reference: [43] <author> Virginia M. Lo, Sanjay Rajopadhye, et al. OREGAMI: </author> <title> Software tools for mapping parallel computations to parallel architectures. </title> <type> Technical Report 89-18, </type> <institution> Department of Computer and Information Science, University of Oregon, </institution> <year> 1989. </year>
Reference-contexts: Internal state, an initialization function, and the main processing function are all named at the end. 3.1.2 OREGAMI/LaRCS One language close in spirit to COP is LaRCS [42], the language component of the University of Oregon's OREGAMI system <ref> [43, 44] </ref>. The OREGAMI system is a set of software tools for automatic mapping of parallel computations to parallel architectures. LaRCS (the Language for Regular Communication Structures) is a description language in the spirit of COP.
Reference: [44] <author> Virginia M. Lo, Sanjay Rajopadhye, Samik Gupta, et al. OREGAMI: </author> <title> tools for mapping parallel computations to parallel architectures. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(3):237270, </volume> <month> June </month> <year> 1991. </year>
Reference-contexts: Internal state, an initialization function, and the main processing function are all named at the end. 3.1.2 OREGAMI/LaRCS One language close in spirit to COP is LaRCS [42], the language component of the University of Oregon's OREGAMI system <ref> [43, 44] </ref>. The OREGAMI system is a set of software tools for automatic mapping of parallel computations to parallel architectures. LaRCS (the Language for Regular Communication Structures) is a description language in the spirit of COP.
Reference: [45] <author> Pat LoPresti. Tadpole: </author> <title> An off-line router for the NuMesh system. </title> <type> Master's thesis, </type> <institution> MIT, </institution> <month> February </month> <year> 1997. </year> <title> NuMesh group, </title> <publisher> MIT LCS. </publisher>
Reference-contexts: Phases containing a single schedule-generator implementation use the implementation's code to generate their schedules; all the 101 other phases must use a stream router. A separate stream router is assumed, such as Tadpole <ref> [45] </ref> for NuMesh; such a router takes static, fixed streams, and finds a schedule for them over a given set of nodes and a given schedule length. 6.3.1 Basic Stream Router Interface The stream router is provided with a list of nodes and streams, and instructed to route the streams using <p> However, as long as one access is required always to succeed, or both accesses always to fail, this transient condition will be resolved in the following pass through the schedule. 7.1.3 The NuMesh Stream Router The stream router for the NuMesh backend is Tadpole <ref> [45] </ref>. Tadpole handles single-source, multiple-destination streams, and allows a fixed bandwidth to be requested for each stream. It is linked into the COP compiler as a C library.
Reference: [46] <author> David B. Loveman. </author> <title> High Performance Fortran. </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <address> 1(1):2542, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: While the thesis explicitly ignores the issue of converting high-level language inputs into structured communications, this process can be either helped or hindered by the choice of high-level language. Some languages, such as HPF <ref> [46] </ref> or Crystal [15], provide high-level communications operators; these operators easily allow for suitable communication schedules to be generated. Other languages, such as Mul-T [38] and Linda [14], have less emphasis on high-level operations and are harder to extract structured communications from.
Reference: [47] <author> Kenneth Mackenzie, John Kubiatowicz, Anant Agarwal, and Frans Kaashoek. FUGU: </author> <title> Implementing translation and protection in a multiuser, multimodel multiprocessor. </title> <type> Technical Report LCS/TM-503, </type> <institution> MIT, </institution> <year> 1994. </year>
Reference-contexts: The network will be timesliced along with the applications themselves, with a context switch on the processor accompanied by a context switch of the scheduled router. Direct access to the physical hardware would be mediated by some combination of hardware support (as in FUGU <ref> [47] </ref>) and operating-system support (as is done in the Exokernel [22]).
Reference: [48] <author> Jeff Magee, Jeff Kramer, and Morris Sloman. </author> <title> Constructing distributed systems in Conic. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(6):663675, </volume> <month> June </month> <year> 1989. </year>
Reference-contexts: While not all of the languages listed below fully meet the two criteria for a communications language mentioned in Section 2, they all have, at least, some characteristics of such a language. * The Conic distributed programming environment <ref> [48] </ref> is a Pascal-based language with ports defined in each module, a uni-directional link command, and a broadcast primitive.
Reference: [49] <author> Bruce M. Maggs. </author> <title> A critical look at three of parallel computing's maxims. </title> <booktitle> In International Symposium on Parallel Architectures, Algorithms and Networks, </booktitle> <pages> pages 17, </pages> <year> 1996. </year>
Reference-contexts: The number of cycles between each slot where the network can accept a data word is known as the gap <ref> [65, 49] </ref>. Before attempting to optimize this number to a lower level, it is important to make sure that no other bottleneck exists: in particular, the processor may be unable to inject messages faster than one cycle in two (as is the case for the current NuMesh/SPARC prototype).
Reference: [50] <author> Chris Metcalf. </author> <title> The NuMesh simulator, </title> <publisher> nsim. MIT LCS NuMesh memo 24, </publisher> <address> ftp://cag.lcs.mit.edu/pub/numesh/memos/nsim.ps.Z, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: The scheduled-routing and dynamic-routing platforms are tested using the same C and COP code. 8.1.2 Simulation The compiled output from the C/COP environment was run on the the NuMesh simulator, nsim <ref> [50] </ref>. The simulator is event-driven, with a modular mechanism for attaching routing elements, CPU elements, and interface elements between the two.
Reference: [51] <author> Chris Metcalf. </author> <title> Parallel prefix on the NuMesh. </title> <publisher> MIT LCS NuMesh memo 25, </publisher> <address> ftp://cag.lcs.mit.edu/pub/numesh/memos/parpref.ps.Z, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: The reduction tree routine is used, forcing distance to one for all connections, and setting the maximum tree degree to three to match the restrictions in the schedule generator. A more detailed discussion of this is available in an internal NuMesh memo, <ref> [51] </ref>. str prefix. The same tree is created as for sch prefix, but without the restrictions on neighbor distance. The broadcast is then performed by processor forwarding, and all the streams run in the same phase. dyn prefix. As for str prefix, but with dynamic routing. barrier repl allreduce.
Reference: [52] <author> John Nguyen, John Pezaris, Gill Pratt, and Steve Ward. </author> <title> Three-dimensional network topologies. </title> <booktitle> In Proceedings of the First International Parallel Computer Routing and Communication Workshop, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Such networks are extremely difficult to handle using online routing algorithms, since the route from one node to another may not lead through any obvious path. Even regular topologies may be hard to get maximum performance from with online algorithms. For example, the diamond lattice network <ref> [63, 52] </ref> is an appealing network topology, with isomorphic 3D connectivity but only four neighbors for each node; this allows a simpler crossbar and more pins per I/O port for pin-limited packages.
Reference: [53] <author> David R. O'Hallaron. </author> <title> The Assign parallel program generator. </title> <booktitle> In The 6th Distributed Memory Computing Conference, </booktitle> <pages> pages 178185, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Its capabilities are similar to those of Conic. * The Assign parallel program generator <ref> [53] </ref> handles partitioning, mapping and routing for static coarse-grained flowgraphs, similar to those used for Gabriel. * GARP (graph abstractions for concurrent programming [33]) is a graph-rewriting language representing connections among agents. Communications occur only along graph edges.
Reference: [54] <author> Craig Peterson, James Sutton, and Paul Wiley. </author> <title> iWarp: A 100-MOPS, LIW microprocessor for multicomputers. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 2629, 8187, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Languages in this category include, for example, Linda [14], Split-C [16], and CSP [29]. 3.2 Scheduled Routing Architectures There are a number of existing scheduled-routing architectures discussed in the literature. Two of them are presented here, iWarp and GF11. 3.2.1 iWarp The iWarp architecture <ref> [54, 10, 11] </ref> is CMU and Intel's follow-on project to the Warp architecture. iWarp integrates the processing and routing units on a single chip, targeted to DSP, scientific, and image processing.
Reference: [55] <author> Ravi Ponnusamy, Yuan-Shin Hwang, Raja Das, Joel Saltz, Alok Choudhary, and Geoffrey Fox. </author> <title> Supporting irregular distributions in FORTRAN 90D/HPF compilers. </title> <type> Technical Report CS-TR-3258.1, </type> <institution> University of Maryland, </institution> <month> November </month> <year> 1994. </year> <month> 155 </month>
Reference-contexts: Without the ability to extract information at compile-time, however, they do an `inspector'-style step where routing information is collected in the first phase, then subsequent phases use the collected information to avoid having to examine all possible streams for input. 3.1.5 Chaos and PARTI The Chaos <ref> [55, 30] </ref> and PARTI [64] work from the University of Maryland includes some of the same conceptual framework that COP does. The goal is to support irregular distributions of data in applications, unlike the standard block and cyclic distributions applied to regular data.
Reference: [56] <author> David Shoemaker. </author> <title> An Optimized Hardware Architecture and Communication Protocol for Scheduled Communication. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: In [2], a comparison of dynamic routers is given, breaking down all the components of the cost. A simple deterministic router's cycle time is quoted at 9.8 ns, with a simple planar-adaptive router taking 11.4 ns. In <ref> [56] </ref>, Shoemaker uses these component estimates to derive an estimate for the cycle time of a scheduled router using the same technology. He finds that the NuMesh scheduled router's cycle time is constrained only by the time for a single cross-node transfer, 4.9 ns. <p> Overall, iWarp is the existing system closest to the reprogrammable scheduled routing system targeted by COP in this thesis, and their results were very promising. NuMesh extends upon their work in several ways, as discussed more extensively in <ref> [56] </ref>; in particular, NuMesh allows faster communications, more precise control over stream bandwidths, and faster phase changing and other router modifications. 3.2.2 GF11 The GF11 parallel computer from Yorktown [5] is a SIMD architecture with 576 processors and an unusual interconnect. <p> The VFSM annotation has three fields: one for the type of data move and one each for source and destination. The data-move type used by COP is always a flow-controlled move; other options are a blind move (no flow control), and a `copy move', discussed in detail in <ref> [56] </ref>. The possible sources and destinations are given in Table 7.1; some are not used by COP and are not discussed here. The current implementation supports up to 32 concurrent VFSMs in each pipeline. <p> The current implementation supports up to 32 concurrent VFSMs in each pipeline. Value Source/Destination 05 Neighbor nodes (x; +x; y; +y; z; +z) 6 MEMADR destination; JTAG TDO bit source (see <ref> [56] </ref>) 7 special buffer handling (see [56]) 8 NOP (invalid source; unsuccessful destination) 13 NULL (always-valid source; bit-bucket destination) 1631 processor interface registers 3263 VFSM buffer registers Table 7.1: NuMesh VFSM sources and destinations The third, or read stage, decodes the source and fetches the appropriate input word. <p> The current implementation supports up to 32 concurrent VFSMs in each pipeline. Value Source/Destination 05 Neighbor nodes (x; +x; y; +y; z; +z) 6 MEMADR destination; JTAG TDO bit source (see <ref> [56] </ref>) 7 special buffer handling (see [56]) 8 NOP (invalid source; unsuccessful destination) 13 NULL (always-valid source; bit-bucket destination) 1631 processor interface registers 3263 VFSM buffer registers Table 7.1: NuMesh VFSM sources and destinations The third, or read stage, decodes the source and fetches the appropriate input word. <p> While some figures present raw cycle counts, others present results in units of wall-clock time, attempting to factor in the difference in cycle speeds between two equivalent implementations. As discussed in Section 1.2.1, and in more detail in <ref> [56] </ref>, a 1:2 cycle speed difference is used between scheduled and deterministic dynamic routers. No additional speed penalty is accrued to adaptive routers, though [56] suggests that the speed penalty would be greater than 1:2 for such routers. <p> As discussed in Section 1.2.1, and in more detail in <ref> [56] </ref>, a 1:2 cycle speed difference is used between scheduled and deterministic dynamic routers. No additional speed penalty is accrued to adaptive routers, though [56] suggests that the speed penalty would be greater than 1:2 for such routers. While this ratio is admittedly purely theoretical, it seems likely to reflect the approximate difference in possible speeds likely with the two architectures.
Reference: [57] <author> David Shoemaker, Frank Honore, Pat LoPresti, Chris Metcalf, and Steve Ward. </author> <title> A unified system for scheduled communication. </title> <booktitle> In International Conference on Parallel and Distributed Processing Techniques and Applications, </booktitle> <month> July </month> <year> 1997. </year>
Reference-contexts: In this thesis, this technique is referred to as reprogrammable scheduled routing; this class of routers is represented by the NuMesh router <ref> [68, 58, 59, 57] </ref>. Essentially, routing decisions are made offline (when the application is compiled), and at run time the routers simply execute repetitive schedules that handle all data transfers. Pre-scheduling communications through the network has several clear advantages.
Reference: [58] <author> David Shoemaker, Frank Honore, Chris Metcalf, and Steve Ward. NuMesh: </author> <title> an architecture optimized for scheduled communication. </title> <journal> The Journal of Supercomputing, </journal> <volume> 10:285 302, </volume> <year> 1996. </year>
Reference-contexts: In this thesis, this technique is referred to as reprogrammable scheduled routing; this class of routers is represented by the NuMesh router <ref> [68, 58, 59, 57] </ref>. Essentially, routing decisions are made offline (when the application is compiled), and at run time the routers simply execute repetitive schedules that handle all data transfers. Pre-scheduling communications through the network has several clear advantages. <p> hardware's constraints (discussed below), and generates NuMesh code as its output format for scheduled routing; however, if the specified architectural parameters exceed the limits of the current NuMesh hardware (and simulator) the compiler will emit a warning message and mark the generated program as non-executable. 7.1.1 The NuMesh Hardware NuMesh <ref> [58, 59] </ref> is the canonical example of a reprogrammable scheduled router. This section adds implementation-specific detail to the high-level description presented in Section 1.4. Pipeline Stages The high-level discussion implicitly assumed two pipeline stages, with each VFSM reading on one cycle, then writing on the next.
Reference: [59] <author> David Shoemaker, Chris Metcalf, and Steve Ward. NuMesh: </author> <title> A communication architecture for static routing. </title> <booktitle> In International Conference on Parallel and Distributed Processing Techniques and Applications, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: In this thesis, this technique is referred to as reprogrammable scheduled routing; this class of routers is represented by the NuMesh router <ref> [68, 58, 59, 57] </ref>. Essentially, routing decisions are made offline (when the application is compiled), and at run time the routers simply execute repetitive schedules that handle all data transfers. Pre-scheduling communications through the network has several clear advantages. <p> hardware's constraints (discussed below), and generates NuMesh code as its output format for scheduled routing; however, if the specified architectural parameters exceed the limits of the current NuMesh hardware (and simulator) the compiler will emit a warning message and mark the generated program as non-executable. 7.1.1 The NuMesh Hardware NuMesh <ref> [58, 59] </ref> is the canonical example of a reprogrammable scheduled router. This section adds implementation-specific detail to the high-level description presented in Section 1.4. Pipeline Stages The high-level discussion implicitly assumed two pipeline stages, with each VFSM reading on one cycle, then writing on the next.
Reference: [60] <author> Shridhar B. Shukla and Dharma P. Agrawal. </author> <title> Scheduling pipelined communication in distributed memory multiprocessors for real-time applications. </title> <booktitle> In Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 222231, </pages> <year> 1991. </year>
Reference-contexts: to give the reader a sense of how scheduled routing is carried out at the low level, a brief description of some of the relevant work is presented here. 3.3.1 Agrawal-Shukla Agrawal and Shukla at NCSU did a good initial analysis and simulation of scheduled routing for arbitrary compile-time streams <ref> [60] </ref>. Their work focussed on the notion of output consistency, a definition for pipelined computation that requires that data always be received by a node by the time the node is ready to consume it.
Reference: [61] <author> B. J. Smith. </author> <title> Architecture and applications of the HEP multiprocessor computer system. </title> <booktitle> In Real-Time Signal Processing IV, </booktitle> <pages> pages 241248, </pages> <month> August </month> <year> 1981. </year>
Reference-contexts: More modern machines (e.g. recent Cray machines [36]) have much higher-speed interfaces and networks; furthermore, recent work in communication techniques is helping to lower or mask latencies further (e.g. active messages [66] and rapid context switching <ref> [1, 61] </ref>). Communications, however, can still be a serious performance constraint. Two approaches to reducing latency and increasing bandwidth are possible. One is to reduce cycle times in the network, thus moving data through the network more quickly, and directly improving both latency and bandwidth.
Reference: [62] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4):315339, </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: They include support for a synchronization primitive to allow changing phases during execution of the dataflow graph. * Express [23] and PVM <ref> [62] </ref> both provide some notion of connections, but they are only set up at runtime, and provide a layer of buffering that reduces performance. Their library primitives provide similar functionality to COP's operators.
Reference: [63] <institution> The Supercomputing Research Center. </institution> <note> A five year review, </note> <month> March </month> <year> 1991. </year>
Reference-contexts: Such networks are extremely difficult to handle using online routing algorithms, since the route from one node to another may not lead through any obvious path. Even regular topologies may be hard to get maximum performance from with online algorithms. For example, the diamond lattice network <ref> [63, 52] </ref> is an appealing network topology, with isomorphic 3D connectivity but only four neighbors for each node; this allows a simpler crossbar and more pins per I/O port for pin-limited packages.
Reference: [64] <author> Alan Sussman, Gagan Agrawal, and Joel Saltz. </author> <title> A manual for the multiblock PARTI runtime primitives. </title> <type> Technical Report CS-TR-3070.1, </type> <institution> University of Maryland, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Without the ability to extract information at compile-time, however, they do an `inspector'-style step where routing information is collected in the first phase, then subsequent phases use the collected information to avoid having to examine all possible streams for input. 3.1.5 Chaos and PARTI The Chaos [55, 30] and PARTI <ref> [64] </ref> work from the University of Maryland includes some of the same conceptual framework that COP does. The goal is to support irregular distributions of data in applications, unlike the standard block and cyclic distributions applied to regular data.
Reference: [65] <author> L. G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(9):103111, </volume> <month> August </month> <year> 1990. </year>
Reference-contexts: The number of cycles between each slot where the network can accept a data word is known as the gap <ref> [65, 49] </ref>. Before attempting to optimize this number to a lower level, it is important to make sure that no other bottleneck exists: in particular, the processor may be unable to inject messages faster than one cycle in two (as is the case for the current NuMesh/SPARC prototype).
Reference: [66] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: a mechanism for integrated communication and computation. </title> <type> Technical Report 92/675, </type> <institution> UCB/CSD, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: More modern machines (e.g. recent Cray machines [36]) have much higher-speed interfaces and networks; furthermore, recent work in communication techniques is helping to lower or mask latencies further (e.g. active messages <ref> [66] </ref> and rapid context switching [1, 61]). Communications, however, can still be a serious performance constraint. Two approaches to reducing latency and increasing bandwidth are possible. One is to reduce cycle times in the network, thus moving data through the network more quickly, and directly improving both latency and bandwidth. <p> A shared-memory handler could then be built on top of this by the application, using interrupts to access the memory and return data to the requesting node (similar arguments could be made, for example, for Active Messages <ref> [66] </ref>). Where COP becomes useful is when the compiler can extract phase and communication information from the application (e.g., when a given section of the application performs a transpose using nested for loops), and convert it into COP operators.
Reference: [67] <author> Elliot Waingold et al. </author> <title> Baring it all to software: The Raw machine. </title> <type> Technical Report LCS TR-709, </type> <institution> MIT, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: Most of the discussion is also applicable to the case where the scheduled router has dynamic-routing hardware available. The current implementation of COP relies on an interrupt-driven system that provides buffering in processor RAM for messages. A future implementation of NuMesh (or a similar system, such as RAW <ref> [3, 67] </ref>) could include traditional online-routing functionality in hardware. Online routing is handled by providing one-hop streams between all adjacent nodes involved. One VFSM reads an interface address dedicated to, e.g., sending data to +x, and routes the data out the +x port.
Reference: [68] <author> S. Ward, K. Abdalla, R. Dujari, M. Fetterman, F. Honore, R. Jenez, P. Laffont, K. Macken-zie, C. Metcalf, Milan Minsky, J. Nguyen, J. Pezaris, G. Pratt, and R. Tessier. </author> <title> The NuMesh: A modular, scalable communications substrate. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year> <month> 156 </month>
Reference-contexts: In this thesis, this technique is referred to as reprogrammable scheduled routing; this class of routers is represented by the NuMesh router <ref> [68, 58, 59, 57] </ref>. Essentially, routing decisions are made offline (when the application is compiled), and at run time the routers simply execute repetitive schedules that handle all data transfers. Pre-scheduling communications through the network has several clear advantages.
References-found: 68

