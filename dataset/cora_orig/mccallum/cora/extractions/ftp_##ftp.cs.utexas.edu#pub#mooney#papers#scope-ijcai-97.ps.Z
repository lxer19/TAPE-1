URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/scope-ijcai-97.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: festlin,mooneyg@cs.utexas.edu  
Title: Learning to Improve both Efficiency and Quality of Planning  
Author: Tara A. Estlin and Raymond J. Mooney 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas at Austin  
Note: To appear in the Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence (IJCAI-97)  
Abstract: Most research in learning for planning has concentrated on efficiency gains. Another important goal is improving the quality of final plans. Learning to improve plan quality has been examined by a few researchers, however, little research has been done learning to improve both efficiency and quality. This paper explores this problem by using the Scope learning system to acquire control knowledge that improves on both of these metrics. Since Scope uses a very flexible training approach, we can easily focus its learning algorithm to prefer search paths that are better for particular evaluation met- rics. Experimental results show that Scope can significantly improve both the quality of final plans and overall planning efficiency.
Abstract-found: 1
Intro-found: 1
Reference: [ Barrett et al., 1995 ] <author> Anthony Barrett, Dave Christianson, Marc Friedman, Chung Kwok, Keith Golden, Scott Penberthy, and Daniel Weld. UCPOP: </author> <note> User's manual (version 4.0). Technical Report 93-09-06d, </note> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> November </month> <year> 1995. </year>
Reference: [ Bhatnagar and Mostow, 1994 ] <author> Neeraj Bhatnagar and Jack Mostow. </author> <title> On-line learning from search failure. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 69-117, </pages> <year> 1994. </year>
Reference: [ Carbonell and et al., 1992 ] <editor> J. Carbonell and et al. PRODIGY4.0: </editor> <title> The manual and tutorial. </title> <type> Technical Report CMU-CS-92-150, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburg,PA, </institution> <year> 1992. </year>
Reference-contexts: Very little research has been done in learning rules that improve plan quality. One learning mechanism for improving the quality of plans was introduced by [ Perez, 1996 ] and is built on top of the PRODIGY nonlinear planner <ref> [ Carbonell and et al., 1992 ] </ref> . It uses EBL and an input quality evaluation function to explain why a higher quality solution is better than a lower quality one and converts this information into control knowledge.
Reference: [ Chien et al., 1996 ] <author> Steve Chien, Randall Hill, XueMei Wang, Tara Estlin, Kristina Fayyad, and Helen Mortensen. </author> <title> Why real-world planning is difficult: A tale of two appli-cations. </title> <editor> In M. Ghallab and A. Milani, editors, </editor> <booktitle> New Directions in AI Planning, </booktitle> <pages> pages 287-298. </pages> <publisher> IOS Press, </publisher> <address> Amsterdam, </address> <year> 1996. </year>
Reference-contexts: In this type of learning, control rules guide the planner towards better solutions. Generating high-quality plans is an essential feature for many real-world planning systems, as is generating these plans efficiently <ref> [ Chien et al., 1996 ] </ref> . Im- proving plan quality and planner efficiency can often be accomplished by using similar learning methods.
Reference: [ Cohen, 1990 ] <author> W. W. Cohen. </author> <title> Learning approximate control rules of high utility. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 268-276, </pages> <address> Austin, TX, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Scope is implemented in Prolog, which provides an excellent framework for learning control rules. Search algorithms can be implemented in Prolog in such a way that allows control information to be easily incorporated in the form of clause-selection rules <ref> [ Cohen, 1990 ] </ref> .
Reference: [ DeJong and Mooney, 1986 ] <author> G. F. DeJong and R. J. Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year> <note> Reprinted in Readings in Machine Learning, </note> <editor> J. W. Shavlik and T. G. Dietterich (eds.), </editor> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Then, Scope uses the proof trees to extract examples of correct and incorrect refinement-selection decisions. The second output of this phase is a set of generalized proof trees. Standard explanation-based generalization (EBG) <ref> [ Mitchell et al., 1986; DeJong and Mooney, 1986 ] </ref> techniques are used to generalize each training example proof tree.
Reference: [ Estlin and Mooney, 1996 ] <author> Tara A. Estlin and Raymond J. Mooney. </author> <title> Multi-strategy learning of search control for partial-order planning. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: To investigate this issue, we employ the Scope 1 learning system, which uses a combination of machine learning techniques to acquire control rules for a partial-order planner. Scope has previously been shown to significantly improve efficiency of a version of the well-known UCPOP planner <ref> [ Estlin and Mooney, 1996 ] </ref> . Scope employs a flexible learning algorithm that can be trained to focus on different evaluation metrics. The primary focus of previous experiments was to avoid backtracking, and thus, improve planning efficiency. <p> (T,Loc) if there does not exist another action in the plan drive-truck (T,Loc,Loc2) which moves the truck to a new location. 4 Focusing Scope on Plan Quality By learning rules which avoid search paths that lead to backtracking, Scope has been shown to significantly improving the efficiency of a planner <ref> [ Estlin and Mooney, 1996 ] </ref> . However, by modifying the method used to collect training data, it can easily improve other planning metrics as well. In order to improve plan quality, we trained Scope on only high-quality solutions. <p> In contrast to Scope, which uses a combination of EBL and induction, UCPOP+EBL uses a purely explanation- based approach to construct control rules in response to planning failures. This system has been shown to improve planning efficiency, however, Scope has outperformed UCPOP+EBL in previous experiments using the blocksworld domain <ref> [ Estlin and Mooney, 1996 ] </ref> . Most other research on learning control rules to improve planning efficiency has been conducted on linear, state-based planners. [ Minton, 1989; Etzioni, 1993; Leckie and Zuckerman, 1993; Bhatnagar and Mostow, 1994 ] .
Reference: [ Estlin, 1996 ] <author> Tara A. Estlin. </author> <title> Integrating explanation-based and inductive learning techniques to acquire search-control for planning. </title> <type> Technical Report AI96-250, </type> <institution> Department of Computer Sciences, University of Texas, Austin, TX, </institution> <year> 1996. </year>
Reference-contexts: As shown in Figure 2, Scope's algorithm has three main phases which are are briefly presented in the next few sections. A more detailed description can be found in <ref> [ Estlin, 1996 ] </ref> . 3.1 Example Analysis In the example analysis phase, training examples are solved using the original planner and two main outputs are produced: a set of selection-decision examples and a set of generalized proof trees.
Reference: [ Etzioni, 1993 ] <author> O. Etzioni. </author> <title> Acquiring search control knowl-edge via static analysis. </title> <journal> Artificial Intelligence, </journal> <volume> 60(2), </volume> <year> 1993. </year>
Reference: [ Foulser et al., 1992 ] <author> D. Foulser, M. Li, and Q. Yang. </author> <title> Theory and algorithms for plan merging. </title> <journal> Artificial Intelligence, </journal> <volume> 52 </volume> <pages> 143-181, </pages> <year> 1992. </year>
Reference-contexts: Most other work in plan quality has concentrated on adding features to the plan algorithm itself that prefer least-cost plans [ Hayes, 1990; Williamson and Hanks, 1994 ] or on examining goal interactions and how they relate to solution quality <ref> [ Wilensky, 1983; Foulser et al., 1992 ] </ref> . 7 Future Directions There are several issues we hope to address in future research. First, we would like to experiment with different types of quality metrics.
Reference: [ Gil, 1991 ] <author> Y. Gil. </author> <title> A specification of manufacturing processes for planning. </title> <type> Technical Report CMU-CS-91-179, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1991. </year>
Reference-contexts: It uses EBL and an input quality evaluation function to explain why a higher quality solution is better than a lower quality one and converts this information into control knowledge. This method has been successfully used to improve solution quality in the process planning domain <ref> [ Gil, 1991 ] </ref> . In contrast, Scope uses a combination of learning techniques to learn control rules that cover good planning decisions and rule out bad ones. Also, Scope has been focused on improving both quality and efficiency. <p> In this way, we can measure plan quality in terms of plan execution cost as well as plan length. We would also like to experiment with the Truckworld domain utilized by [ Williamson and Hanks, 1994 ] where resource consumption is important, and the process planning domain <ref> [ Gil, 1991 ] </ref> , where a number of quality metrics would be applicable. Finally, we would like to devise a method for incremental learning and training for Scope. Scope could use learned control information for smaller problems to help generate training examples for more complex problems.
Reference: [ Hayes, 1990 ] <author> Caroline Hayes. </author> <title> Machining Planning: A Model of an Expert Level Planning Process. </title> <type> PhD thesis, </type> <institution> The Robotics Institue, Carnegie Mellon University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: This method is similar to the one employed by Perez, however it does not make use of the quality evaluation function to build the explanation. Most other work in plan quality has concentrated on adding features to the plan algorithm itself that prefer least-cost plans <ref> [ Hayes, 1990; Williamson and Hanks, 1994 ] </ref> or on examining goal interactions and how they relate to solution quality [ Wilensky, 1983; Foulser et al., 1992 ] . 7 Future Directions There are several issues we hope to address in future research.
Reference: [ Iwamoto, 1994 ] <author> M. Iwamoto. </author> <title> A planner with quality goal and its speedup learning for optimization problem. </title> <booktitle> In Proceedings of the Second International Conference of AI Planning Systems, </booktitle> <address> Chicago, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: These systems construct domain-specific control rules that enable a planner to find solutions more quickly. Another aim of planning and learning research, which has received much less attention, is to improve the quality of plans produced by a planner <ref> [ Perez, 1996; Iwamoto, 1994 ] </ref> . In this type of learning, control rules guide the planner towards better solutions. Generating high-quality plans is an essential feature for many real-world planning systems, as is generating these plans efficiently [ Chien et al., 1996 ] . <p> Also, Scope has been focused on improving both quality and efficiency. Another learning method for improving quality, which was also runs on the PRODIGY nonlinear planner, was developed by <ref> [ Iwamoto, 1994 ] </ref> . This technique uses EBL to acquire control rules for near-optimal solutions in LSI design. This method is similar to the one employed by Perez, however it does not make use of the quality evaluation function to build the explanation.
Reference: [ Kambhampati et al., 1996 ] <author> Subbarao Kambhampati, Suresh Katukam, and Yong Qu. </author> <title> Failure driven search control for partial order planners: An expla-nation based approach. </title> <journal> Artificial Intelligence, </journal> <volume> 88, </volume> <year> 1996. </year>
Reference-contexts: In this regard, the most closely, related system to Scope is UCPOP+EBL <ref> [ Kambhampati et al., 1996 ] </ref> . In contrast to Scope, which uses a combination of EBL and induction, UCPOP+EBL uses a purely explanation- based approach to construct control rules in response to planning failures.
Reference: [ Korf, 1985 ] <author> R. Korf. </author> <title> Depth-first iterative-deepening: An op-timal admissable tree search. </title> <journal> Artificial Intelligence, </journal> <volume> 27(1), </volume> <year> 1985. </year>
Reference-contexts: Thus, it learns rules that not only avoid dead-end paths, but also that avoid paths that lead to sub-optimal solutions. In order to train Scope on high-quality solutions, the search method of depth-first iterative deepening (DFID) <ref> [ Korf, 1985 ] </ref> was employed to solve the training problems. This method ensured that the shortest possible solutions were always returned.
Reference: [ Leckie and Zuckerman, 1993 ] <author> Chistopher Leckie and Ingrid Zuckerman. </author> <title> An inductive approach to learning search con-trol rules for planning. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1100-1105, </pages> <address> Chamberry,France, </address> <month> August </month> <year> 1993. </year>
Reference: [ Minton, 1989 ] <author> S. Minton. </author> <title> Explanation-based learning: A problem solving perspective. </title> <journal> Artificial Intelligence, </journal> <volume> 40:63118, </volume> <year> 1989. </year>
Reference: [ Mitchell et al., 1986 ] <author> Tom M. Mitchell, Richard M. Keller, and Smadar T. Kedar-Cabelli. </author> <title> Explanation-based gener-alization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: Then, Scope uses the proof trees to extract examples of correct and incorrect refinement-selection decisions. The second output of this phase is a set of generalized proof trees. Standard explanation-based generalization (EBG) <ref> [ Mitchell et al., 1986; DeJong and Mooney, 1986 ] </ref> techniques are used to generalize each training example proof tree.
Reference: [ Muggleton, 1992 ] <editor> S. H. Muggleton, editor. </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference: [ Perez, 1996 ] <author> M. Alicia Perez. </author> <title> Representing and learning quality-improving search control knowledge. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 382-390, </pages> <address> Bari,Italy, </address> <month> July </month> <year> 1996. </year>
Reference-contexts: These systems construct domain-specific control rules that enable a planner to find solutions more quickly. Another aim of planning and learning research, which has received much less attention, is to improve the quality of plans produced by a planner <ref> [ Perez, 1996; Iwamoto, 1994 ] </ref> . In this type of learning, control rules guide the planner towards better solutions. Generating high-quality plans is an essential feature for many real-world planning systems, as is generating these plans efficiently [ Chien et al., 1996 ] . <p> Very little research has been done in learning rules that improve plan quality. One learning mechanism for improving the quality of plans was introduced by <ref> [ Perez, 1996 ] </ref> and is built on top of the PRODIGY nonlinear planner [ Carbonell and et al., 1992 ] .
Reference: [ Quinlan, 1990 ] <author> J.R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: In- formation from these trees is used in the next phase to construct control rules. 3.2 Control Rule Induction The goal of the induction phase is to produce an operational definition of when it is useful to apply a planning refinement. Scope employs a version of the Foil algorithm <ref> [ Quinlan, 1990 ] </ref> to learn control rules through induction. Foil attempts to learn a control definition that is composed of a set of Horn clauses. This definition covers all of the positive examples of when to apply a refinement, and none of the negatives.
Reference: [ Silverstein and Pazzani, 1991 ] <author> Glenn Silverstein and Michael J. Pazzani. </author> <title> Relational cliches: Constraining constructive induction during relational learning. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> pages 203-207, </pages> <address> Evanston, IL, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Scope also considers several other types of control rule antecedents during induction. These include negated proof tree literals, determinate literals [ Muggle- ton, 1992 ] , variable codesignation constraints, and relational cliches <ref> [ Silverstein and Pazzani, 1991 ] </ref> . 3.3 Program Specialization Phase Once refinement selection rules have been learned, they are passed to the program specialization phase which adds this control information into the original plan <p>- 3 When selecting each new clause antecedent, Foil tries all possible variable combinations for all
Reference: [ Veloso, 1992 ] <author> Manuela M. Veloso. </author> <title> Learning by Analogical Reasoning in General Problem Solving. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: Figure 1 illustrates an example from the logistics transportation domain <ref> [ Veloso, 1992 ] </ref> where control knowledge could be useful. Here, there are two possible refinement candidates for adding a new action to achieve the goal at-obj (pkg1,city2-airport). <p> This method ensured that the shortest possible solutions were always returned. To improve upon other quality metrics, different training methods may have to be employed that return optimal (or near-optimal) solutions based on other evaluation functions. 5 Evaluation 5.1 Experimental Setup The logistics transportation domain <ref> [ Veloso, 1992 ] </ref> was used to evaluate Scope's ability to improve both quality and efficiency. In this domain, packages must be delivered to different locations in several cities. Trucks are used to transport packages within a city, and planes are used to transport packages between different cities.
Reference: [ Wilensky, 1983 ] <author> Robert W. Wilensky. </author> <title> Planning and Understanding: A Computational Approach to Human Reasoning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: Most other work in plan quality has concentrated on adding features to the plan algorithm itself that prefer least-cost plans [ Hayes, 1990; Williamson and Hanks, 1994 ] or on examining goal interactions and how they relate to solution quality <ref> [ Wilensky, 1983; Foulser et al., 1992 ] </ref> . 7 Future Directions There are several issues we hope to address in future research. First, we would like to experiment with different types of quality metrics.
Reference: [ Williamson and Hanks, 1994 ] <author> Mike Williamson and Steve Hanks. </author> <title> Optimal planning with a goal-directed utility model. </title> <booktitle> In Proceedings of the Second International Conference of AI Planning Systems, </booktitle> <pages> pages 176-181, </pages> <address> Chicago, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: This method is similar to the one employed by Perez, however it does not make use of the quality evaluation function to build the explanation. Most other work in plan quality has concentrated on adding features to the plan algorithm itself that prefer least-cost plans <ref> [ Hayes, 1990; Williamson and Hanks, 1994 ] </ref> or on examining goal interactions and how they relate to solution quality [ Wilensky, 1983; Foulser et al., 1992 ] . 7 Future Directions There are several issues we hope to address in future research. <p> We are currently working on implementing a version of the logistics domain where actions have different execution costs. In this way, we can measure plan quality in terms of plan execution cost as well as plan length. We would also like to experiment with the Truckworld domain utilized by <ref> [ Williamson and Hanks, 1994 ] </ref> where resource consumption is important, and the process planning domain [ Gil, 1991 ] , where a number of quality metrics would be applicable. Finally, we would like to devise a method for incremental learning and training for Scope.
References-found: 25

