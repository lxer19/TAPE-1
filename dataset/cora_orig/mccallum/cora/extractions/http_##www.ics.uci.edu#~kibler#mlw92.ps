URL: http://www.ics.uci.edu/~kibler/mlw92.ps
Refering-URL: http://www.ics.uci.edu/~kibler/
Root-URL: 
Email: pdatta@ics.uci.edu  
Title: Utilizing Prior Concepts for Learning  
Author: Piew Datta and Dennis Kibler 
Address: Irvine, CA 92717-3425  
Affiliation: Information and Computer Science Department University of California, Irvine  
Abstract: The inductive learning problem consists of learning a concept given examples and non-examples of the concept. To perform this learning task, inductive learning algorithms bias their learning method. Here we discuss biasing the learning method to use previously learned concepts from the same domain. These learned concepts highlight useful information for other concepts in the domain. We describe a transference bias and present M-FOCL, a Horn clause relational learning algorithm, that utilizes this bias to learn multiple concepts. We provide preliminary empirical evaluation to show the effects of biasing previous information on noise-free and noisy data.
Abstract-found: 1
Intro-found: 1
Reference: <author> Brunk, C. A., Pazzani, M. </author> <year> (1991). </year> <title> An investigation of noise-tolerant relational concept learning algorithms. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Thus learning systems must operate somewhat accurately. In propositional domains, classification and attribute noise are two types of noise that have been studied (Quinlan, 1986). The definitions of classification and attribute noise for relational domains are similar to those used in Brunk and Pazzani <ref> (Brunk & Pazzani, 1991) </ref>. We measure the amount of noise for a relation in the following manner. Associated with each relation is the set of tuples which satisfy the relation. We can alter the class of a tuple or an element of the tuple.
Reference: <author> De Raedt, L., Bruynooghe, M. </author> <year> (1992). </year> <title> Interactive concept-learning and constructive induction by analogy. </title> <journal> Machine Learning, </journal> <volume> volume 8, no. </volume> <pages> 2. </pages> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Other dynamic biasing systems modify their inductive algorithm by using a table to store the best parameter settings for various types of problems (Rendell, Seshu, & Tcheng, 1987). M-FOCL differs from most of the systems utilizing a representation bias to aid learning. Unlike STABB (Utgoff, 1986), Clint <ref> (De Raedt & Bruynooghe, 1992) </ref>, STAGGER (Schlimmer, 1987) and VBMS (Rendell, Seshu, & Tcheng, 1987), M-FOCL does not have sets of predefined languages that the learning system can use if its current language is unsuitable for learning. Instead M-FOCL transfers relevant domain information obtained from previous concepts to subsequent concepts. <p> Since M-FOCL transfers knowledge from one concept to another it can be compared to systems using analogy to learn. Utilizing a transference bias is related to using analogy for reasoning. Cia <ref> (De Raedt & Bruynooghe, 1992) </ref>, Clint's component for performing analogical reasoning, uses second order schemata to aid concept learning. It mainly relies on the syntax for the problems and solutions to find other solutions for the target domain.
Reference: <author> Falkenhainer, B., Forbus, K., Gentner D. </author> <year> (1986). </year> <title> The structure-mapping engine. </title> <type> (Technical Report no. </type> <institution> UIUCDCS-R-86-1275). Urbana, Illinois: University of Illinois at Urbana-Champaign, Department of Computer Science. </institution>
Reference: <author> Gordon, D. </author> <year> (1989). </year> <title> Screening hypotheses with explicit bias. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Markman, E. </author> <year> (1989). </year> <title> Categorization and naming in children. </title> <address> Cambridge, Massachusetts: </address> <publisher> The MIT Press. </publisher>
Reference: <author> Michalski, R. S. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <booktitle> Machine Learning: An Artificial Intelligence Approach, volume 2. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Inductive learning systems have focused on these biases for learning single concepts. Although these static biases were identified, they could not be changed since they were designed into systems. In later work, parametric algorithms allowed the user to manually change the bias before learning <ref> (Michalski, 1983) </ref>. More recent work (Utgoff, 1986; Schlimmer, 1987; Rendell, Seshu, & Tcheng, 1987; Gordon, 1989; De Raedt & Bruynooghe 1992) uses biases that change the representation language or the search algorithm dynamically at execution time to accommodate the characteristics of the domain.
Reference: <author> Minton, S., Carbonell, J. G. </author> <year> (1987). </year> <title> Strategies for learning search control rules: an explanation-based approach. </title> <booktitle> Proceedings of the Tenth International Joint Conference on Artificial Intelligence. </booktitle> <address> Milan, Italy. </address>
Reference-contexts: Creating intensional predicates often reduces the depth of the search when a good selection is chosen, but it also increases the branching factor of the search. Therefore, the trade-offs for generating intensional predicates with these rules must be analyzed. This is analogous to the utility problem <ref> (Minton & Carbonell, 1987) </ref> while learning macros for planning. In accordance with the transference bias, we developed the entirety, full conjunct, and partial conjunct rules. These rules facilitate M-FOCL's use of prior concepts to learn subsequent concepts.
Reference: <author> Mitchell, T. M. </author> <year> (1980). </year> <title> The need for biases in learning generalizations. </title> <type> (Technical Report CBM-TR-117). </type> <institution> New Brunswick, NJ: Rutgers University, Department of Computer Science. </institution>
Reference: <author> Pagallo, G., Haussler, D. </author> <year> (1990). </year> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> volume 5, no. </volume> <pages> 1. </pages> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: In addition, it only uses previous knowledge or subcon-cepts that have proven to be helpful in more than one situation. FRINGE <ref> (Pagallo & Haussler, 1990) </ref> learns Boolean functions using decision trees, but iteratively modifies its tree by adding Boolean combinations of features found at the fringe of the tree as a new feature to its representation language. <p> Since FRINGE learns only one concept, it transfers information, in the form of new features, from one part of the concept to another, avoiding the replication problem <ref> (Pagallo & Haussler, 1990) </ref>. Although M-FOCL also has this self-transference ability, it focuses on transferring information from several concepts to other concepts. In addition, FRINGE creates new features without knowing their utility for learning, whereas M-FOCL only creates predicates using partial concepts if they have shown to be useful.
Reference: <author> Pazzani, M. J., Kibler, D. </author> <year> (1990). </year> <title> The utility of knowledge in inductive learning. </title> <type> (Technical Report no. 90-18). </type> <institution> Irvine, CA: University of California, Irvine, Department of Information and Computer Science. </institution>
Reference-contexts: We contrast our transference bias with other types of dynamic biases in section 5. We conclude this paper by summarizing our contributions and discussing future work. 2 Relational Horn clause learning algorithms Before describing Multiple concept FOCL (M-FOCL) and FOCL <ref> (Pazzani & Kibler, 1990) </ref>, we will describe FOIL (Quinlan, 1990) to explain the core learning method applied in these systems. Given a set of extensional predicates, positive examples, and negative examples, FOIL learns a relational Horn clause concept using these predefined predicates.
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> The effect of noise on concept learning. </title> <booktitle> Machine Learning: An Artificial Intelligence Approach, volume 2. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Thus learning systems must operate somewhat accurately. In propositional domains, classification and attribute noise are two types of noise that have been studied <ref> (Quinlan, 1986) </ref>. The definitions of classification and attribute noise for relational domains are similar to those used in Brunk and Pazzani (Brunk & Pazzani, 1991). We measure the amount of noise for a relation in the following manner.
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> volume 5 no. </volume> <pages> 3. </pages> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: We contrast our transference bias with other types of dynamic biases in section 5. We conclude this paper by summarizing our contributions and discussing future work. 2 Relational Horn clause learning algorithms Before describing Multiple concept FOCL (M-FOCL) and FOCL (Pazzani & Kibler, 1990), we will describe FOIL <ref> (Quinlan, 1990) </ref> to explain the core learning method applied in these systems. Given a set of extensional predicates, positive examples, and negative examples, FOIL learns a relational Horn clause concept using these predefined predicates. This concept covers all of the positive examples and excludes all negative ones.
Reference: <author> Rendell, L., Seshu, R., Tcheng, D. </author> <year> (1987). </year> <title> More robust concept learning using dynamically-variable bias. </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Other dynamic biasing systems modify their inductive algorithm by using a table to store the best parameter settings for various types of problems <ref> (Rendell, Seshu, & Tcheng, 1987) </ref>. M-FOCL differs from most of the systems utilizing a representation bias to aid learning. Unlike STABB (Utgoff, 1986), Clint (De Raedt & Bruynooghe, 1992), STAGGER (Schlimmer, 1987) and VBMS (Rendell, Seshu, & Tcheng, 1987), M-FOCL does not have sets of predefined languages that the learning system <p> algorithm by using a table to store the best parameter settings for various types of problems <ref> (Rendell, Seshu, & Tcheng, 1987) </ref>. M-FOCL differs from most of the systems utilizing a representation bias to aid learning. Unlike STABB (Utgoff, 1986), Clint (De Raedt & Bruynooghe, 1992), STAGGER (Schlimmer, 1987) and VBMS (Rendell, Seshu, & Tcheng, 1987), M-FOCL does not have sets of predefined languages that the learning system can use if its current language is unsuitable for learning. Instead M-FOCL transfers relevant domain information obtained from previous concepts to subsequent concepts.
Reference: <author> Ruby, D., Kibler, D. </author> <year> (1991). </year> <title> SteppingStone: An empirical and analytical evaluation. </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: To aid the explanation of the rules, an analogy can be drawn between learning new predicates in M-FOCL and learning macros in planning systems. Macros are learned from previous successful sequences of operators <ref> (Ruby & Kibler, 1991) </ref>, whereas the entirety, full, and partial conjunct rules facilitate learning new predicates from previous concepts.
Reference: <author> Schlimmer, J. C. </author> <year> (1987). </year> <title> Incremental adjustment of representations. </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: M-FOCL differs from most of the systems utilizing a representation bias to aid learning. Unlike STABB (Utgoff, 1986), Clint (De Raedt & Bruynooghe, 1992), STAGGER <ref> (Schlimmer, 1987) </ref> and VBMS (Rendell, Seshu, & Tcheng, 1987), M-FOCL does not have sets of predefined languages that the learning system can use if its current language is unsuitable for learning. Instead M-FOCL transfers relevant domain information obtained from previous concepts to subsequent concepts.
Reference: <author> Utgoff, P. </author> <year> (1986). </year> <title> Shift of bias for inductive concept learning. </title> <booktitle> Machine Learning: An Artificial Intelligence Approach, volume 2. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Other dynamic biasing systems modify their inductive algorithm by using a table to store the best parameter settings for various types of problems (Rendell, Seshu, & Tcheng, 1987). M-FOCL differs from most of the systems utilizing a representation bias to aid learning. Unlike STABB <ref> (Utgoff, 1986) </ref>, Clint (De Raedt & Bruynooghe, 1992), STAGGER (Schlimmer, 1987) and VBMS (Rendell, Seshu, & Tcheng, 1987), M-FOCL does not have sets of predefined languages that the learning system can use if its current language is unsuitable for learning.
Reference: <author> Vere, S. A. </author> <year> (1977). </year> <title> Induction of relational productions in the presence of background information. </title> <booktitle> Fifth International Joint Conference on Artificial Intelligence. </booktitle> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 17

