URL: http://www.cs.colorado.edu/~zorn/cs7135/Fall-1996/projects96/oswald.ps
Refering-URL: http://www.cs.colorado.edu/~zorn/cs7135/Fall-1996/fall1996.html
Root-URL: http://www.cs.colorado.edu
Keyword: Carrie Oswald CSCI7135  
Note: Some of the issues of information retrieval on the Web can be summarized by the following questions:  
Abstract: 1 Oh, What a Tangled Web We Weave Abstract Oh, what a tangled Web we weave. The World Wide Web (WWW, Web) is a distributed information system in which documents, software, sounds, images, and other file system data are located on different computers dispersed around the Internet. The Web has been growing exponentially and includes information that is decentralized, dynamic, and diverse, often not organized, and of both poor and high quality. Locating information of interest has become unorderly and chaotic. Navigation on the Web can be difficult and finding desired information can be a real challenge. How does one find the information that he or she is looking for? What information retrieval techniques are available? What are their advantages and disadvantages? How does one effectively state what he or she wants to find? How is the stuff that is not applicable to the search eliminated? These issues and others keep researchers busy trying to find the best ways to harness the proliferation of information on the WWW. Information retrieval systems must support scalable means of organizing, browsing, and searching while collecting and correlating the information from the many incomplete, inconsistent, and heterogeneous repositories. There exists a variety of tools and techniques for discovering and retrieving information on the Web. Some of the more traditional methods include full text scanning, signature files, inversion, and clustering. More recent methods attempt to improve query precision and capture more semantic information about each document to achieve better performance. These include methods that use concept-based searching, query expansion, artificial intelligence, natural language processing, the Latent Semantic Indexing method, probabilistic logic, query by example, and methods using neural networks. Many comparisons of various search engines have been done, particularly of the more popular ones such as Alta Vista, Excite, Harvest, InfoSeek, Lycos, Magellan, and Point. This paper does not compare search engines, but describes what the underlying techniques are for information retrieval. In doing so, it also attempts to answer the questions outlined above. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Belkin, Nicholas J. and Croft, W. Bruce, </author> <title> Information Filtering and Information Retrieval: Two Sides of the Same Coin?, </title> <journal> Communications of the ACM, Dec. 1992, </journal> <volume> Vol. 35, No. </volume> <pages> 12. </pages>
Reference: [2] <author> Bender, Laura and Eagan, Ann, Spiders and Worms and Crawlers, Oh My: </author> <title> Searching on the World Wide Web, </title> <address> http://www.library.ucsb.edu/untangle/eagan.html. </address>
Reference: [3] <author> Bowman, C. Mic, Danzig, Peter B., Manber, Udi, Schwartz, Michael F., </author> <title> Scaleable Internet Resource Discovery: Research Problems and Approaches, </title> <journal> Communications of the ACM, August 1994, </journal> <volume> Vol. 37, No. </volume> <pages> 8. </pages>
Reference-contexts: Information retrieval systems must support scalable means of organizing, browsing, and searching while collecting and correlating the information from many incomplete, inconsistent, and heterogeneous repositories <ref> [3] </ref>. There exists a variety of tools and techniques for discovering and retrieving information on the Web. Some of the more traditional methods include full text scanning, signature files, inversion, and clustering. <p> Ironically, this can also be a strength of organized subject trees, because people prefer a fixed system they can get used to, even if it is not the most efficient <ref> [3] </ref>. Browsing subject catalogs can also lead to navigation problems, and users can get disoriented. To some extent this problem can be alleviated by systems that support multiple views of information. <p> On the other hand, users are less prone to disorientation. The searching paradigm can handle change much better than subject indexes; plus different services can be connected with searching more easily than by interfacing their organizations <ref> [3] </ref>. Many search engines work on the principle that the information content of a document can be summarized by extracting words from the title or text, by taking information from the first few lines, or by locating key statements from within the document. <p> Search engines can be placed on a spectrum of indexing space vs. representativeness. There is usually a time-space tradeoff; i.e. systems that use less space require more time for searching <ref> [3] </ref>. Decisions regarding the best search engine to start with depends partly upon knowing and understanding what elements in a document have been indexed by each engine. Different search engines offer different search options. <p> Reasonably smallsized descriptor objects should be sufficient to point anyone to this data. Only these descriptor objects will need to be searched an indexed. Sound, video, and many other types of nontextual data can also be treated in the same way <ref> [3] </ref>. Information scientists continue to find ways to improve techniques for addressing the issues of information retrieval on the rapidly growing Internet. Areas of focus include harvesting and indexing methods, natural language and linguistic methods, ranking algorithms, relevance feedback, and performance. <p> User interfaces and search strategies should be highly customizable in order to support a wide range of user expertise and needs. Plus the Internet will need a hierarchically structured, extensible, object-caching service through which clients can retrieve data objects, once they are discovered <ref> [3] </ref>. Some of the conventional information retrieval techniques include full-text scanning, signature files, inversion, and clustering. <p> However, the current separate local indexes do not present a general enough view of much of the available information. Often, users need to perform a lengthy browsing session to find the right indexes, and they often miss. This problem will only get worse as the Internet grows <ref> [3] </ref>. Another disadvantage is that it is not possible to achieve an efficient adaptation of an 10 inverted file to deal with the matching of more elaborate document and query descriptions such as weighted keywords [26]. Clustering Clustering is the traditional approach used in library science.
Reference: [4] <author> Bowman, C. Mic, Danzig, Peter B., Hardy, Darren R., Manber, Udi, Schwartz, Michael F., and Wessels, Duane P., Harvest: </author> <title> A Scalable, Customizable Discovery and Access System, </title> <publisher> ftp:/ftp.cs.colorado.edu/pub/cs/techreports/schwartz/Harvest.ps.Z. </publisher>
Reference: [5] <author> Cahill, Tony, Hinchey, Michael G., and Relihan, Liam, </author> <title> Untangling the WorldWide Web, </title> <address> http://itdsrv1.ul.ie/Research/WWW/utwww.ps. </address>
Reference: [6] <author> Dumais, Susan T. and Foltz, Peter W., </author> <title> Personalized Information Delivery: An Analysis of Information Filtering Methods, </title> <journal> Communications of the ACM, Dec. 1992, </journal> <volume> Vol. 35, No. </volume> <pages> 12. </pages>
Reference-contexts: In a standard keyword-matching vector system, the similarity between two documents is computed as the inner product or cosine of the corresponding two columns of the word-by-document matrix <ref> [6] </ref>. Queries can also be represented as vectors of the words and thus compared against all document columns returning the best matches. <p> Advantages of the LSI technique are that queries can retrieve documents even if they have no words in common, plus the technique captures deeper associative structure than simple term-to-term correlations and clusters and it is completely automatic <ref> [6] </ref>. The LSI method has been applied to several standard information retrieval collections with favorable results. LSI has equaled or outperformed standard vector methods and other variants in every case, with improvement of as much as 30%. <p> LSI has equaled or outperformed standard vector methods and other variants in every case, with improvement of as much as 30%. As with the standard vector method, differential term weighting and relevance feedback can improve LSI performance substantially <ref> [6] </ref>. Probabilistic Logic Probability theory has been used to explain chance. Answers to queries rely on a factual information as history coupled with probability. The same is true in information retrieval.
Reference: [7] <author> Emtage, Alan, Kahle, Brewster, Neuman, B. Clifford, and Schwartz, Michael F., </author> <title> A Comparison of Internet Resource Discovery Approaches, </title> <publisher> ftp:/ftp.cs.colorado.edu/pub/cs/techreports/schwartz/RD.Comparison.ps.Z. </publisher>
Reference: [8] <author> Faloutsos, Christos, and Oard, Douglas, </author> <title> A Survey of Information Retrieval and Filtering Methods, </title> <address> http://www.enee.umd.edu/medlab/filter/papers/survey.ps. </address>
Reference-contexts: Traditional information retrieval methods use only a small amount of the information associated with a document as the basis for relevance decisions. Despite this inherent limitation, they often achieve acceptable precision because the full text of a document contains a significant amount of redundancy <ref> [8] </ref>. Many search engines use Boolean logic and keyword logic to assist users in more effectively stating what they want to find by making the query more precise. <p> A primary disadvantage is the bad response time, especially for large databases. To help alleviate bad response times, full text scanning is often carried out by special purpose hardware or used in cooperation with another access method (e.g. inversion) that would restrict the scope of searching <ref> [8] </ref>. Signature Files In the signature file approach, hashing is used on each documents words and superimposed coding, yielding a bit string or signature. The resulting document signatures are stored sequentially in a separate file called a signature file. <p> In addition, the method is easily parallelizable. The primary disadvantage is the bad response time when the files are large <ref> [8] </ref>. Inversion In the inversion method, each document is represented by a list of keywords, which describes the contents of the document for retrieval purposes. Fast retrieval can be achieved if those keywords are inverted. <p> Disadvantages are the amount of storage needed, which can reach up to 300% of the original file size, the cost of updating and reorganizing the index, if the environment is dynamic, and the cost of merging the lists, if they are to long or too many <ref> [8] </ref>. Because of these costs, maintaining an inverted index of say, the whole Internet FTP space may not be feasible. However, the current separate local indexes do not present a general enough view of much of the available information. <p> Clustering techniques group similar documents together to form clusters. Queries are matched with a cluster representative, which represents a cluster with some kind of profile that attempts to summarize and characterize the cluster of documents. Grouping similar documents accelerates the searching <ref> [8] </ref> and gives relevance feedback plus the ability to provide ranked output. The cluster representative is expected to discriminate the relevant from the nonrelevant documents when matched against any query. <p> The first step in a more complete natural language processing information retrieval system might be automatic syntactic analysis. Considerable advances have been made in recent years in syntactic modeling of natural language, and efficient parsers with a broad domain have recently become available <ref> [8] </ref>. Latent Semantic Indexing In information retrieval, a textual database can be represented by a word-by-document matrix whose integer entries represent the number of occurrences of a word in a specific document. <p> Documents can then be ranked in order of decreasing similarity to a query by using normalized inner products on these vectors to compute the cosine similarity measure <ref> [8] </ref>. The assumption is that there is some underlying or latent structure in the pattern of a word usage across documents and that statistical techniques can be used to estimate this latent structure. <p> The main idea in the neural network class of methods is to use spreading activation methods. The usual technique is to construct a thesaurus, either manually or automatically, and then create one node in a hidden layer to correspond to each concept in the thesaurus <ref> [8] </ref>. Conclusion Oh, what a tangled Web we weave. The World Wide Web has been growing exponentially and includes information that is decentralized, dynamic, and diverse, often not organized, and of both poor and high quality. Locating information of interest has become unorderly and chaotic.
Reference: [9] <author> Gershon, Nahum and Winstead, Joel, </author> <title> The WorldWide Web: A Global Source of Data and Information, The Information Revolution: </title> <booktitle> Impact on Science and Technology, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference: [10] <author> Gilster, Paul, </author> <title> Finding it on the Internet: the Internet Navigators Guide to Search Tools and Techniques, </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1996. </year>
Reference: [11] <institution> IRTF Research Group on Resource Discovery, </institution> <note> http://www.cs.colorado.edu/homes/schwartz/public_html/IRTF.html. </note>
Reference-contexts: IRTF-RD is investigating a number of research problems concerned with information discovery and access, focusing on growth in data volume, user base, and data diversity <ref> [11] </ref>. Other related work on Internet information retrieval has come from universities, such as the Untangling the Web conference, which was held April 26, 1996, at the University of California, Santa Barbara (UCSB). It was sponsored by the universitys Librarians Association and friends of the UCSB Library [25].
Reference: [12] <author> Kass, Robert and Stadnyk, Irene, </author> <title> Modeling Users Interests in Information Filters, </title> <journal> Communications of the ACM, Dec. 1992, </journal> <volume> Vol. 35, No. 12. </volume> <pages> 16 </pages>
Reference-contexts: Further, users may encounter a conceptualization problem, where the concepts used to represent available information are different from the concepts the user has for the domain because the same concept can be described by many different words (e.g. information retrieval, resource discovery, searching) <ref> [12] </ref>. Checking for patterns of 8 keywords is not enough to model interests. Semantic and contextual information should also be used. Recent research in artificial intelligence, and in natural language understanding in particular, has resulted in technologies that can help in the design of more intelligent information retrieval systems.
Reference: [13] <author> Koch, Traugott, </author> <title> Improving Resource Discovery and Retrieval: existing approaches and the EU project DESIRE, </title> <address> http://www.ub2.lu.se/tk/demos/BFD9602-en.html. </address>
Reference: [14] <author> Lager, Mark, </author> <title> Spinning a Web Search, </title> <address> http://www.library.ucsb.edu/untangle/lager.html. </address>
Reference-contexts: Precision focuses on the relevant, most useful items retrieved in the search. Recall with high precision is the ultimate goal. The goal of information retrieval scientists is to provide the most precise or relevant documents in the midst of the recalled search results <ref> [14] </ref>. There are several other factors that also determine the success of search engines. These factors include the size, content, and currency of the database, the availability of search options and features, the speed of searching, the relevancy and presentation of the result sets, and the overall ease of use. <p> Searching this list identifies cataloged items. In this manner, users do not have to know the exact words to use to retrieve relevant documents. And, instead of reinitiating the search based on confidence or weighting, the search engine automatically includes the like terms <ref> [14] </ref>. An advantage of concept-based searching is enhanced recall. Query Expansion Once a search has been completed, it often tends to need to be enhanced or changed. Users may not be able to express their information needs with just one query. <p> Artificial intelligence refers to computers that can think and reason and it focuses on finding a logical, mathematical way to represent knowledge. The computer can be programmed with this mathematical model to assist in decision 11 making, information retrieval, and analysis <ref> [14] </ref>. Artificial intelligence systems can represent, reason about, and effectively manage their own informationseeking behavior. A key requirement in such systems is their ability to build and reason about explicit representations of the desired informationtheir knowledge goals. <p> Not only can language be processed by exact match using keywords, but natural language can be processed using a set of concepts to sort out the interrelationships of words. Sentences are broken apart into its semantic parts: nouns, verbs, adjectives, etc., and links are created <ref> [14] </ref>. Since language can be ambiguous, vague, or metaphorical, natural language processing seeks to compute the relationships between words, giving each a correlation to the words around it. Put into a formula, the computer then makes assumptions based on its logic. <p> If a document uses the key terms often, it is ranked more highly than one that seldom uses that particular term. Plus words from the query that are found next to each other in the document score higher <ref> [14] </ref>. 13 One method of explaining possibilities was created by a mathematician from the 18 th century, Rev. Thomas Bayes. His theorem tried to apply a mathematical, logical representation to various factors using case-based reasoning. His mathematical model of probability involves probability, hypothesis, evidence, and context [14]. <p> the document score higher <ref> [14] </ref>. 13 One method of explaining possibilities was created by a mathematician from the 18 th century, Rev. Thomas Bayes. His theorem tried to apply a mathematical, logical representation to various factors using case-based reasoning. His mathematical model of probability involves probability, hypothesis, evidence, and context [14]. Query By Example Probabilistic logic allows query by example, which is the concept of providing the search engine with an example and then using that example, the system returns other similar documents. This technique sets up queries to find like pages or files. <p> The search is reinitiated using the example as the new source for the query. This interactive searching gives the user more control over the search process. Users can find more documents like the one selected. The results returned are more focused because of the qualified terms <ref> [14] </ref>. Neural Networks Neural networks provide a convenient knowledge representation for information retrieval applications in which nodes typically represent objects such as keywords, authors, and citations and bidirectional links represent their weighted associations of relevance.
Reference: [15] <author> Lewis, David D., Jones, Karen Sparck, </author> <title> Natural Language Processing for Information Retrieval, </title> <journal> Communications of the ACM, January 1996, </journal> <volume> Vol. 39, No. </volume> <pages> 1. </pages>
Reference: [16] <author> Liu, Jian, </author> <title> Understanding WWW Search Tools, </title> <address> http://www.indiana.edu/~librcsd/search. </address>
Reference-contexts: A good search engine should provide results that are easy to understand and with enough information to make a decision about the usefulness of the results <ref> [16] </ref>. Search engine evaluation is an ongoing challenge. Not only is the rate of Internet publishing expanding the indexes and subject hierarchies at an exponential rate, but the search algorithms and the search interfaces are being enhanced and refined to improve the performance of these tools.
Reference: [17] <author> Manber, Udi, </author> <note> Udi Manber Home Page, http://glimpse.cs.arizona.edu:1994/udi.html. </note>
Reference: [18] <author> McBryan, Oliver A., </author> <title> GENVL and WWWW: Tools for Taming the Web, </title> <address> http://www.cs.colorado.edu/home/mcbryan/mypapers/www94.ps. </address>
Reference: [19] <author> Moffat, Alistair and Zobel, Justin, </author> <title> Information Retrieval Systems for Large Document Collections, </title> <address> http:/potomac.ncsl.nist.gov:80/TREC/trec3.papers/moffat-zobel.ps. </address>
Reference: [20] <author> Paul, Kathryn and Webster, Kathleen, </author> <title> Beyond Surfing: Tools and Techniques for Searching the Web, </title> <address> http://magi.com/~mmelick/it96jan.html. </address>
Reference-contexts: Some of the more well-known subject indexes that provide significant added value to each link with commentaries and ratings provided by skilled reviewers include GNNs Whole Internet Catalogue, Magellan, and Point Communications <ref> [20] </ref> . The subject-oriented organization of various subject catalogs, while logical, can make it difficult to second guess topics that are not easily categorized. There exists no controlled vocabulary within and between different subject catalogs.
Reference: [21] <author> Ram, Ashwin, </author> <title> Natural Language Understanding for Information-Filtering Systems, </title> <journal> Communications of the ACM, Dec. 1992, </journal> <volume> Vol. 35, No. </volume> <pages> 12. </pages>
Reference-contexts: find interesting and update user models automatically; choose to filter and extract information, not because a user explicitly asked them to, but to learn more about some domain in support of their own problemsolving actions; or use multiple strategies to search, retrieve, filter, or infer information that might be relevant <ref> [21] </ref>. Natural Language Processing Natural language processing is the act and science of getting computers to understand natural language. Not only can language be processed by exact match using keywords, but natural language can be processed using a set of concepts to sort out the interrelationships of words.
Reference: [22] <institution> The Center for Intelligent Information Retrieval (CIIR), </institution> <note> http://ciir.cs.umass.edu. </note>
Reference-contexts: Their main goal is to develop software that will support accurate and efficient access to, and analysis of, the enormous and growing amount of information that is stored in text databases throughout businesses and government. Areas of research include information retrieval, natural language processing, database systems, and case-based reasoning <ref> [22] </ref>. The IRTF-RD is supported primarily by the Advanced Research Projects Agency (ARPA), with additional support from the Air Force Office of Scientific Research, the National Science Foundation, Hughes Aircraft Company under a NASA EOSDIS project subcontract, and Sun Microsystems Collaborative Research Program.
Reference: [23] <institution> The Search Page, </institution> <note> http://www.accesscom.com/~ziegler/search.html#search_reviews. </note>
Reference: [24] <institution> The Text Retrieval Conference (TREC), </institution> <note> http://potomac.ncsl.nist.gov:80/TREC. </note>
Reference-contexts: They use a common evaluation package to allow comparisons of the effectiveness of different techniques. They also discuss how differences between the various systems affect performance <ref> [24] </ref>. The CIIR is funded by the National Science Foundation (NSF) to carry out basic research and technology transfer in the area of text-based information systems.
Reference: [25] <institution> Untangling the Web Conference, </institution> <note> http://www.library.ucsb.edu/untangle. [26] van Rijsbergen, C. J., Information Retrieval, http://www.dcs.gla.ac.uk/Keith/Preface.html. </note>
Reference-contexts: Other related work on Internet information retrieval has come from universities, such as the Untangling the Web conference, which was held April 26, 1996, at the University of California, Santa Barbara (UCSB). It was sponsored by the universitys Librarians Association and friends of the UCSB Library <ref> [25] </ref>. There also exists work on information retrieval from others from various organizations or whose affiliations may not be apparent [2,5,8,16,17,20,26,27,28].
Reference: [27] <author> Web Matrix: </author> <title> Whats the Difference?, </title> <address> http://www.si.umich.edu/~fprefect/matrix/answer.html. </address>
Reference-contexts: Effective use of 6 keyword controls, such as Boolean and proximity operators, can focus or expand the results of a search <ref> [27] </ref>. Search engines vary according to the size of the index, the frequency of updating the index, the search options, the speed of returning a result set, the result set presentation, the relevancy of the items included in a result set, and the user interface design.
Reference: [28] <author> Weiss, Scott, </author> <title> Survey of Information Retrieval, </title> <address> http://www.cs.jhu.edu/~weiss/ir.html. </address>
References-found: 27

