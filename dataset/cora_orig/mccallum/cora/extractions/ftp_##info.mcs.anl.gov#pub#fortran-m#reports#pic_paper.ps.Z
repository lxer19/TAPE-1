URL: ftp://info.mcs.anl.gov/pub/fortran-m/reports/pic_paper.ps.Z
Refering-URL: http://www.mcs.anl.gov/fortran-m/applications.html
Root-URL: http://www.mcs.anl.gov
Note: 6. CONCLUSIONS 7. REFERENCES  
Abstract: ous way to write a PIC application in Orca is to distribute the particles nonspatially, as in our Fortran M implementation, and to divide the grid into shared subgrids. To clarify the issues that arise, consider the extreme case in which there is just a single shared subgrid namely, the entire grid. Since operations on shared objects are atomic, having just one shared grid essentially serializes the scatter phase. Memory constraints may also preclude sharing the entire grid. Increasing the number of shared subgrids increases the parallelism in the scatter phase, but having too many subgrids may place too great a burden on the Orca runtime system. As in the Fortran M implementation, improved performance may result if the granularity of communication is increased, for example, by scattering to all corners of a cell in a single operation, rather than scattering to each corner in separate operations. The granularity may be increased further by maintaining local copies of grid point information that are accessed and updated locally, and used to periodically update the shared subgrids. A drawback of this approach is that the data structures for storing the local copies are dynamic since as particles move the set of grid points with which they interact changes. A simpler approach is to maintain a local buffer for each shared subgrid and to store deferred remote operations in these buffers. This is essentially the same as buffering messages in the scatter and router processes in our Fortran M implementation. We also intend to develop a version of the PIC application in the Split-C language, which is a parallel extension to C. The extensions that Split-C provides are few in number, but form the basis for a rich parallel programming environment. A shared memory programming style is supported through the use of global pointers. Split-C also provides split-phase access to remote data, allowing remote accesses to be overlapped with useful computation. The scatter phase of the PIC algorithm can be implemented using the split-phase put assignment operation, while the gather phase can be implemented using a get assignment operation. The signaling store operation, which stores to a remote location and signals the processor containing the location that the store has occurred, may also be used in the scatter phase. Finally, Split-C provides bulk data operations that can be used to increase the granularity of communication. Here again the idea is to maintain local copies of grid point information that can be used periodically to update grid points on remote processors. We have described our progress in implementing the scatter and gather phases of the PIC algorithm in Fortran M, and discussed strategies for optimization. We have also sketched ideas for implementing the PIC algorithm using the Orca shared object language, and the Split-C parallel language. In all three of these programming methodologies we expect increasing the granularity of communication to be important for improving performance. The use of split-phase communication is also likely to prove a useful means of optimizing codes. Our future work in this area will be directed at investigating which optimization strategies are the most effective, and how they impact the programmability and maintainability of the code. Bal, H. E. 1991. Programming Distributed Systems. Pren-tice Hall International, Hemel Hempstead, England. Bal, H. E.; Kaashoek, M. F.; and Tanenbaum, A. S. 1992. "Orca: A Language for Parallel Programming of Distributed Systems." IEEE Trans. Software Engineering 18, no. 3:190-205. Birdsall, C. K. and Langdon, A. B. 1985. Plasma Physics Via Computer Simulation. McGraw-Hill, New York, NY. Culler, D. E.; Dusseau, A.; Goldstein, S. C.; Krishna-murthy, A.; Lumetta, S.; von Eicken, T.; and Yelick, K. 1993. "Parallel Programming in Split-C." In Proceedings of Supercomputing '93 (Portland, OR, Nov. 15-19). IEEE Computer Soc. Press, Los Alamitos, CA, 262-273. Foster, I. T. and Chandy, K. M.. 1992. "Fortran M: A Language for Modular Parallel Programming." Technical Report, Argonne National Laboratory. Foster, I. T.; Olson, R.; and Tuecke, S. 1993. "Programming in Fortran M." Technical Report, Argonne National Laboratory. Hockney, R. W. and Eastwood, J. W. 1988. Computer Simulation Using Particles. Adam Hilger, Bristol, Eng-land. Li, K. and Hudak, P. 1989. "Memory Coherence in Shared Virtual Memory Systems." ACM Trans. Computers Systems 7, no. 4 (Nov.): 321-359. MPI. 1993. "Document for a Standard Message-Passing Interface." Technical Report CS-93-214, Department of Computer Science, University of Tennessee, Knoxville, TN. Otto, S. W. 1991. "MetaMP: A Higher Level Abstraction for Message Passing Programming." Technical Report CS/E 91-003, Department of Computer Science and Engineering, Oregon Graduate Institute. Otto, S. W. 1994. "Parallel Array Classes and Lightweight Sharing Mechanisms." Scientific Computing 2, no. 4, 203-216. Parallel Computing. 1994. Special Issue on Message Passing. Walker, D. W. 1990. "Characterizing the Parallel Performance of a Large Scale, Particle-In-Cell Plasma Simulation Code." Concurrency: Practice and Experience 2, no. 4 (Dec.): 257-288. Walker, D. W. 1993. "The Design of a Standard Message Passing Interface for Distributed Memory Concurrent Computers." Technical Report TM-12512, Oak Ridge National Laboratory, Oak Ridge, TN. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bal, H. E. </author> <year> 1991. </year> <title> Programming Distributed Systems. </title> <publisher> Pren-tice Hall International, </publisher> <address> Hemel Hempstead, England. </address>
Reference: <author> Bal, H. E.; Kaashoek, M. F.; and Tanenbaum, A. S. </author> <year> 1992. </year> <title> "Orca: A Language for Parallel Programming of Distributed Systems." </title> <journal> IEEE Trans. Software Engineering 18, </journal> <volume> no. 3 </volume> <pages> 190-205. </pages>
Reference: <author> Birdsall, C. K. and Langdon, A. B. </author> <year> 1985. </year> <title> Plasma Physics Via Computer Simulation. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY. </address>
Reference: <author> Culler, D. E.; Dusseau, A.; Goldstein, S. C.; Krishna-murthy, A.; Lumetta, S.; von Eicken, T.; and Yelick, K. </author> <year> 1993. </year> <title> "Parallel Programming in Split-C." </title> <booktitle> In Proceedings of Supercomputing '93 (Portland, </booktitle> <address> OR, </address> <month> Nov. </month> <pages> 15-19). </pages> <publisher> IEEE Computer Soc. Press, Los Alamitos, CA, </publisher> <pages> 262-273. </pages>
Reference: <author> Foster, I. T. and Chandy, K. M.. </author> <year> 1992. </year> <title> "Fortran M: A Language for Modular Parallel Programming." </title> <type> Technical Report, </type> <institution> Argonne National Laboratory. </institution>
Reference: <author> Foster, I. T.; Olson, R.; and Tuecke, S. </author> <year> 1993. </year> <title> "Programming in Fortran M." </title> <type> Technical Report, </type> <institution> Argonne National Laboratory. </institution>
Reference: <author> Hockney, R. W. and Eastwood, J. W. </author> <year> 1988. </year> <title> Computer Simulation Using Particles. Adam Hilger, </title> <address> Bristol, Eng-land. </address>
Reference: <author> Li, K. and Hudak, P. </author> <year> 1989. </year> <title> "Memory Coherence in Shared Virtual Memory Systems." </title> <journal> ACM Trans. Computers Systems 7, </journal> <volume> no. 4 (Nov.): </volume> <pages> 321-359. </pages> <publisher> MPI. </publisher> <year> 1993. </year> <title> "Document for a Standard Message-Passing Interface." </title> <type> Technical Report CS-93-214, </type> <institution> Department of Computer Science, University of Tennessee, Knoxville, TN. </institution>
Reference: <author> Otto, S. W. </author> <year> 1991. </year> <title> "MetaMP: A Higher Level Abstraction for Message Passing Programming." </title> <type> Technical Report CS/E 91-003, </type> <institution> Department of Computer Science and Engineering, Oregon Graduate Institute. </institution>
Reference: <author> Otto, S. W. </author> <year> 1994. </year> <title> "Parallel Array Classes and Lightweight Sharing Mechanisms." </title> <journal> Scientific Computing 2, </journal> <volume> no. 4, </volume> <pages> 203-216. </pages> <note> Parallel Computing. 1994. Special Issue on Message Passing. </note>
Reference: <author> Walker, D. W. </author> <year> 1990. </year> <title> "Characterizing the Parallel Performance of a Large Scale, Particle-In-Cell Plasma Simulation Code." </title> <journal> Concurrency: Practice and Experience 2, </journal> <volume> no. </volume> <month> 4 (Dec.): </month> <pages> 257-288. </pages>


References-found: 11

