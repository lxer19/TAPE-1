URL: http://www.cs.princeton.edu/~ristad/papers/esca98.ps.gz
Refering-URL: http://www.cs.princeton.edu/~ristad/papers/esca98.html
Root-URL: http://www.cs.princeton.edu
Phone: 2  
Title: A SURFICIAL PRONUNCIATION MODEL  
Author: Eric Sven Ristad Peter N. Yianilos 
Address: Princeton, NJ, USA  Princeton, NJ, USA  
Affiliation: 1 Mnemonic Technology, Inc.,  NEC Research Institute,  
Abstract: We argue for a surficial pronunciation model: a model without underlying forms. The surficial model outperforms a traditional generative model by a significant margin on conversational speech (Switchboard) as well as on read speech (TIMIT). Our results suggest that the true mapping from underlying forms to surface forms is too complex to be accurately modeled using current techniques, and that we would be best served to model the surface forms directly. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bahl, L. R., and Jelinek, F. </author> <title> Decoding for channels with insertions, deletions, and substitutions with applications to speech recognition. </title> <journal> IEEE Trans. Inform. Theory IT-21, </journal> <volume> 4 (1975), </volume> <pages> 404-411. </pages>
Reference-contexts: 1 INTRODUCTION Following the information-theoretic approach pioneered by the IBM Speech Recognition group in the 1970's <ref> [7, 1] </ref>, and lead by the generative phonology revolution [2, 6], the pronunciation models in modern speech recognition systems typically consist of a phonological lexicon coupled with a statistical transducer. The phonological lexicon maps each syntactic word to a small set of underlying pronunciations; it is typically designed by hand.
Reference: [2] <author> Chomsky, N., and Halle, M. </author> <title> The Sound Pattern of English. </title> <publisher> Harper & Row, </publisher> <address> New York, </address> <year> 1968. </year>
Reference-contexts: 1 INTRODUCTION Following the information-theoretic approach pioneered by the IBM Speech Recognition group in the 1970's [7, 1], and lead by the generative phonology revolution <ref> [2, 6] </ref>, the pronunciation models in modern speech recognition systems typically consist of a phonological lexicon coupled with a statistical transducer. The phonological lexicon maps each syntactic word to a small set of underlying pronunciations; it is typically designed by hand.
Reference: [3] <author> Garofolo, J. S., Lamel, L. F., Fisher, W. M., Fiscus, J. G., Pallett, D. S., and Dahlgren, N. L. </author> <title> The darpa timit acoustic-phonetic continuous speech corpus. </title> <type> Tech. Rep. </type> <institution> Speech Disc CD1-1.1, NIST, Gaithersburg, MD, </institution> <year> 1986. </year>
Reference-contexts: Since the minimum error rate is 8.65%, the surficial approach reduces the error rate of the generative approach by a factor of 2.5. 4 TIMIT We have also tested our approach on TIMIT <ref> [3] </ref>. We partitioned the TIMIT transcripts into 30,132 training samples and 11,025 test samples according to the TIMIT instructions. Experiment E1 used the standard TIMIT lexicon, while experiment E3 used the training corpus for its lexicon.
Reference: [4] <author> Godfrey, J., Holliman, E., and McDaniel, J. </author> <title> Switchboard: telephone speech corpus for research and development. </title> <booktitle> In Proc. IEEE ICASSP (Detroit, </booktitle> <year> 1995), </year> <pages> pp. 517-520. </pages>
Reference-contexts: Fourthly, the surfi-cial approach offers the promise of instantaneous learning of new words and new pronunciations in real-time (ie., without batch-optimizing any parameters in the model), because the power of the model comes from the pronunciation lexicon rather than the transducer. 3 SWITCHBOARD We have tested our approach on Switchboard <ref> [4] </ref>. Over 200,000 words of Switchboard have been manually assigned phonetic transcripts at ICSI [5]. We partitioned the available transcripts 9:1 into 192,879 training samples and 21,431 test samples. We report recognition results for three experiments; see [10, 11] for additional details.
Reference: [5] <author> Greenberg, S., Hollenbach, J., and Ellis, D. </author> <title> Insights into spoken language gleaned from phonetic transcription of the Switchboard corpus. </title> <booktitle> In Proc. </booktitle> <address> IC-SLP (Philadelphia, </address> <month> October </month> <year> 1996). </year>
Reference-contexts: Over 200,000 words of Switchboard have been manually assigned phonetic transcripts at ICSI <ref> [5] </ref>. We partitioned the available transcripts 9:1 into 192,879 training samples and 21,431 test samples. We report recognition results for three experiments; see [10, 11] for additional details. In experiment E1, we used the standard Switchboard pronouncing lexicon.
Reference: [6] <author> Halle, M. </author> <title> Phonology in generative grammar. Word 18, </title> <month> 1-2 </month> <year> (1962), </year> <pages> 54-72. </pages>
Reference-contexts: 1 INTRODUCTION Following the information-theoretic approach pioneered by the IBM Speech Recognition group in the 1970's [7, 1], and lead by the generative phonology revolution <ref> [2, 6] </ref>, the pronunciation models in modern speech recognition systems typically consist of a phonological lexicon coupled with a statistical transducer. The phonological lexicon maps each syntactic word to a small set of underlying pronunciations; it is typically designed by hand.
Reference: [7] <author> Jelinek, F., Bahl, L. R., and Mercer, R. L. </author> <title> The design of a linguistic statistical decoder for the recognition of continuous speech. </title> <journal> IEEE Trans. Inform. Theory IT-21, </journal> <volume> 3 (1975), </volume> <pages> 250-256. </pages>
Reference-contexts: 1 INTRODUCTION Following the information-theoretic approach pioneered by the IBM Speech Recognition group in the 1970's <ref> [7, 1] </ref>, and lead by the generative phonology revolution [2, 6], the pronunciation models in modern speech recognition systems typically consist of a phonological lexicon coupled with a statistical transducer. The phonological lexicon maps each syntactic word to a small set of underlying pronunciations; it is typically designed by hand.
Reference: [8] <author> Riley, M., Ljolje, A., Hindle, D., and Pereira, F. </author> <title> The AT&T 60,000 word speech-to-text system. </title> <booktitle> In Eurospeech'95: ECSA 4th European Conference on Speech Communication and Technology (Madrid, </booktitle> <address> Spain, </address> <month> September </month> <year> 1995), </year> <editor> J. M. Pardo, E. Enrquez, J. Ortega, J. Ferreiros, J. Macas, and F.J.Valverde, Eds., </editor> <volume> vol. 1, </volume> <booktitle> European Speech Communication Association, </booktitle> <pages> pp. 207-210. </pages>
Reference-contexts: Little has changed in the intervening decades. Current speech technology still employs a sparse pronouncing lexicon of hand-crafted underlying forms. When the vocabulary is large or contains many proper nouns, then the pronouncing lexicon may be generated by a hand-crafted text-to-speech system <ref> [8] </ref>. In most systems, the mapping from underlying forms to surface forms is left to the acoustic models. In more advanced systems, underlying segments are mapped to their surface realizations using a statistical decision tree [9].
Reference: [9] <author> Riley, M. D., and Ljolje, A. </author> <title> Automatic generation of detailed pronunciation lexicons. In Automatic Speech and Speaker Recognition: Advanced Topics, </title> <editor> C.- H. Lee, F. K. Soong, and K. K. Paliwal, Eds. </editor> <publisher> Kluwer Academic, </publisher> <address> Boston, </address> <month> March </month> <year> 1996, </year> <note> ch. 12. </note>
Reference-contexts: In most systems, the mapping from underlying forms to surface forms is left to the acoustic models. In more advanced systems, underlying segments are mapped to their surface realizations using a statistical decision tree <ref> [9] </ref>. In this paper, we argue for a surficial pronunciation model: a model without underlying forms. We demonstrate that a surficial model outperforms a generative model by a significant margin, both on conversational speech (Switchboard) and on read speech (TIMIT).
Reference: [10] <author> Ristad, E. S., and Yianilos, P. N. </author> <title> Learning string edit distance. </title> <type> Tech. Rep. </type> <institution> CS-TR-532-96, Department of Computer Science, Princeton University, Princeton, NJ, </institution> <month> October </month> <year> 1996. </year> <month> Revised November </month> <year> 1997. </year>
Reference-contexts: When the stochastic transducer M is memoryless and independent of the syntactic word as it is for our experiments then this model has only O (jLj + k 2 ) parameters for a phonetic alphabet A of k symbols. The model parameters may be optimized using expectation-maximization <ref> [10, 11] </ref>. <p> Over 200,000 words of Switchboard have been manually assigned phonetic transcripts at ICSI [5]. We partitioned the available transcripts 9:1 into 192,879 training samples and 21,431 test samples. We report recognition results for three experiments; see <ref> [10, 11] </ref> for additional details. In experiment E1, we used the standard Switchboard pronouncing lexicon. In experiment E3, we built a pronunciation lexicon directly from the training corpus. The test corpus contains 512 samples whose truth value does not appear in the E3 lexicon.
Reference: [11] <author> Ristad, E. S., and Yianilos, P. N. </author> <title> Learning string edit distance. </title> <journal> IEEE Trans. </journal> <note> PAMI (to appear). Preliminary version as Princeton CS-TR-532-96. </note>
Reference-contexts: When the stochastic transducer M is memoryless and independent of the syntactic word as it is for our experiments then this model has only O (jLj + k 2 ) parameters for a phonetic alphabet A of k symbols. The model parameters may be optimized using expectation-maximization <ref> [10, 11] </ref>. <p> Over 200,000 words of Switchboard have been manually assigned phonetic transcripts at ICSI [5]. We partitioned the available transcripts 9:1 into 192,879 training samples and 21,431 test samples. We report recognition results for three experiments; see <ref> [10, 11] </ref> for additional details. In experiment E1, we used the standard Switchboard pronouncing lexicon. In experiment E3, we built a pronunciation lexicon directly from the training corpus. The test corpus contains 512 samples whose truth value does not appear in the E3 lexicon.
References-found: 11

