URL: http://www.cs.dartmouth.edu/~rus/papers/agents/charles.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/~rus/papers/agents/agents.html
Root-URL: http://www.cs.dartmouth.edu
Title: Information Retrieval, Information Structure, and Information Agents  
Author: Daniela Rus Devika Subramanian 
Date: November 19, 1995  
Abstract: This paper presents a customizable architecture for software agents that capture and access information in large, heterogeneous, distributed electronic repositories. The key idea is to exploit underlying structure at various levels of granularity to build high-level indices with task-specific interpretations. Information agents construct such indices and are configured as a network of reusable modules called structure detectors and segmenters. We illustrate our architecture with the design and implementation of smart information filters in two contexts: retrieving stock market data from Internet newsgroups, and retrieving technical reports from Internet ftp sites.
Abstract-found: 1
Intro-found: 1
Reference: [AS] <author> J. Allan and G. Salton, </author> <title> The identification of text relations using automatic hypertext linking, </title> <booktitle> in the Workshop on Intelligent Hypertext, the ACM Conference on Information Knowledge Management, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: We propose augmenting the set of recognizable structures to include higher level entities: tables, graphs, pictures, time 3 We use the information retrieval system SMART <ref> [AS] </ref> to cluster on this topic. 3 histories of patterns, etc. This permits us to handle heterogeneous forms of information (numbers, weather maps, simulation data).
Reference: [BDG] <author> J. L. Balcazar, J. D iaz, and J. Gabarro, </author> <title> Structural Complexity I, </title> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <title> 15 at the lowest common denominator over usages 31 </title>
Reference-contexts: Finally, the contents of each row have to be interpreted to obtain the precision recall measures. This last step requires knowledge about the form of precision-recall measures. 2.3.1 Formalizing agents We represent an agent in the language of circuits <ref> [BDG] </ref>. Asynchronous circuits are a useful engineering metaphor for assembling agents from detectors and segmenters.
Reference: [Bel] <author> N. Belkin and W. Croft, </author> <title> Information filtering and information retrieval: two sides of the same coin?, </title> <journal> Communications of the ACM, </journal> <volume> vol. 35(12), </volume> <pages> pp. 29-38, </pages> <year> 1992. </year>
Reference: [BK] <author> M. Blum and D. Kozen, </author> <title> On the power of the compass (or, why mazes are easier to search than graphs), </title> <booktitle> in Proceedings of the Symposium on Foundations of Computer Science, </booktitle> <pages> pp 132-142, </pages> <year> 1978. </year>
Reference-contexts: If t ij = t i 0 j 0 the data in row i and column j and the data in row i 0 and column j 0 are *-similar. The type matrix for the table in Figure 4 is given in Figure 7. A GCD algorithm <ref> [BK] </ref> can be used to determine the type, if any, of the overall matrix and thus to decide whether the matrix represents a table. We provide for error tolerance in the typing of each column by supplying an error parameter * r . <p> To do this, we need a formal framework for analyzing what information is necessary for performing a task. Such a framework, based on the notion of information invariants, has been discussed in the robotics context by [Don, DJR] and in the theoretical literature by <ref> [BK] </ref>. Our long-term goal is to computationally characterize methods such as statistics over character sequences [SM, PN], statistics over word occurrence, layout and geometry, and other notions of structure with respect to information content.
Reference: [Bro1] <author> R. Brooks, </author> <title> Elephants don't play chess, Design of Autonomous Agents, </title> <editor> ed. P. Maes, MIT/Elsevier, </editor> <year> 1990. </year>
Reference-contexts: Transportable agents can travel from machine to machine processing information locally, thus avoiding costly data transfers over congested networks. Agents, in addition to being transportable, can learn and can operate autonomously. We draw inspiration from robotics <ref> [Bro1, Bro2, Don, JR] </ref> to design information agents. The basic modules for physical robots are sensors and effectors. Our information agents are autonomous sensori-computational "circuits" comprised of a network of virtual sensors to detect, extract, and interpret structure in data and virtual effectors for transportation. <p> The major use of structure in their work is to summarize and present information to a user. * Mobile Robotics. The analogy between mobile robots in unstructured physical environments and information agents in a rich multi media data environments is not just metaphorical. We have observed <ref> [Bro1, Bro2] </ref> that the lessons learned in designing task-directed mobile robots can be imported to the problem of information capture and access. <p> Additionally, since our data is in a distributed, wide-area network, it is important that they detect and recover from network failures. This is typically accomplished by having each module be self-correcting|a lesson about organization of complex systems that was <ref> [Bro1, Bro2] </ref> discovered in the context of mobile robotics and insect intelligences. Our designs incorporate task-specific error detection and recovery schemes. A complex search is organized as a network of on-line computations. Each node in the network corresponds to a module; modules interact by sharing data.
Reference: [Bro2] <author> R. Brooks, </author> <title> A robust layered control system for a mobile robot, </title> <journal> IEEE Journal of Robotics and Automation, </journal> <year> 1986. </year>
Reference-contexts: Transportable agents can travel from machine to machine processing information locally, thus avoiding costly data transfers over congested networks. Agents, in addition to being transportable, can learn and can operate autonomously. We draw inspiration from robotics <ref> [Bro1, Bro2, Don, JR] </ref> to design information agents. The basic modules for physical robots are sensors and effectors. Our information agents are autonomous sensori-computational "circuits" comprised of a network of virtual sensors to detect, extract, and interpret structure in data and virtual effectors for transportation. <p> The major use of structure in their work is to summarize and present information to a user. * Mobile Robotics. The analogy between mobile robots in unstructured physical environments and information agents in a rich multi media data environments is not just metaphorical. We have observed <ref> [Bro1, Bro2] </ref> that the lessons learned in designing task-directed mobile robots can be imported to the problem of information capture and access. <p> Additionally, since our data is in a distributed, wide-area network, it is important that they detect and recover from network failures. This is typically accomplished by having each module be self-correcting|a lesson about organization of complex systems that was <ref> [Bro1, Bro2] </ref> discovered in the context of mobile robotics and insect intelligences. Our designs incorporate task-specific error detection and recovery schemes. A complex search is organized as a network of on-line computations. Each node in the network corresponds to a module; modules interact by sharing data.
Reference: [CG] <author> J. Canny and K. Goldberg, </author> <title> A "RISC" Paradigm for Industrial Robotics, to appear, </title> <booktitle> Proceedings of the International Conference on Robotics and Automation, </booktitle> <year> 1993. </year>
Reference: [Cate] <author> V. Cate, Alex: </author> <title> a global file system, </title> <booktitle> in Proceedings of the Usenix Conference on File Systems, </booktitle> <year> 1992. </year>
Reference-contexts: Of course, this amortization is not valid if data changes so frequently as to make the index out of date as soon as it is created. Bib Agent is built on top of the Alex <ref> [Cate] </ref> filesystem which provides users transparent access to files located in all the ftp-sites over the world. Bib Agent can thus use Unix commands like cd and ls to navigate to the directories and subdirectories accessible by anonymous ftp.
Reference: [CRD] <author> P. Crean, C. Russell, and M. V. Dellon, </author> <title> Overview and Programming Guide to the Mind Image Management Systems, </title> <type> Xerox Technical Report X9000627, </type> <year> 1991. </year>
Reference: [DL] <author> J. Davis and C. Lagoze, </author> <title> Drop-in publishing with the World-Wide Web, </title> <booktitle> in Proceedings of the Second International WWW Conference, </booktitle> <address> pg 749-758, </address> <year> 1994. </year>
Reference-contexts: We are currently working on an implementation of Bib Agent in Agent-Tcl with a World Wide Web interface. This will be provided to external users as a service of the Dienst Technical Report server <ref> [DL] </ref>. 5 Discussion Our goal is to develop and prototype a methodology for customizable information and access tools for large, distributed, heterogeneous information domains.
Reference: [Don] <author> B. Donald, </author> <title> Information Invariants in Robotics, </title> <note> to appear, Artificial Intelligence. </note>
Reference-contexts: Transportable agents can travel from machine to machine processing information locally, thus avoiding costly data transfers over congested networks. Agents, in addition to being transportable, can learn and can operate autonomously. We draw inspiration from robotics <ref> [Bro1, Bro2, Don, JR] </ref> to design information agents. The basic modules for physical robots are sensors and effectors. Our information agents are autonomous sensori-computational "circuits" comprised of a network of virtual sensors to detect, extract, and interpret structure in data and virtual effectors for transportation. <p> We have observed [Bro1, Bro2] that the lessons learned in designing task-directed mobile robots can be imported to the problem of information capture and access. We were influenced in defining topology-based segmenters and structure detectors by the work of <ref> [Don, JR] </ref>, who consider the problem of determining the information requirements to perform robot tasks using the concept of information invariants and perceptual equivalence classes. 2 Organizing Principles for Information Gathering Search En gines For any specific information access query, no matter how complicated, we can write a special purpose program <p> To do this, we need a formal framework for analyzing what information is necessary for performing a task. Such a framework, based on the notion of information invariants, has been discussed in the robotics context by <ref> [Don, DJR] </ref> and in the theoretical literature by [BK]. Our long-term goal is to computationally characterize methods such as statistics over character sequences [SM, PN], statistics over word occurrence, layout and geometry, and other notions of structure with respect to information content.
Reference: [DJ] <author> B. Donald and J. Jennings, </author> <title> Constructive recognizability for task-directed robot programming, </title> <journal> Journal of Robotics and Autonomous Systems, </journal> <volume> 9(1), </volume> <year> 1992. </year>
Reference: [DJR] <author> B. Donald, J. Jennings, and D. </author> <title> Rus, Information Invariants for Cooperating Autonomous Mobile Robots, </title> <booktitle> in Proceedings of the International Symposium on Robotics Research, </booktitle> <year> 1993. </year>
Reference-contexts: To do this, we need a formal framework for analyzing what information is necessary for performing a task. Such a framework, based on the notion of information invariants, has been discussed in the robotics context by <ref> [Don, DJR] </ref> and in the theoretical literature by [BK]. Our long-term goal is to computationally characterize methods such as statistics over character sequences [SM, PN], statistics over word occurrence, layout and geometry, and other notions of structure with respect to information content.
Reference: [EW] <author> O. Etzioni and D. Weld, </author> <title> A softbot-based interface to the Internet, </title> <journal> in Communications of the ACM, </journal> <volume> vol. 37, no. 7, </volume> <pages> pg 72-76, </pages> <year> 1994. </year>
Reference-contexts: There has been a recent flurry of activity in the area of designing intelligent task-directed agents that live and act within realistic software environments. These are called knowbots by [KC], softbots by <ref> [EW] </ref>, sodabots by [KSC], software agents by [GK], and personal assistants by [Mae, MCFMZ]. A knowledge-level specification of know-bots is provided in Chapter 4 of [KC], which outlines the required capabilities without committing to a specific implementation. <p> Our agent architecture can also be used to set up Personal Library Systems [KC] "that selectively view, organize, and update contents" of an electronic library for individual use. We are interested in the same class of tasks as <ref> [EW, Mae, MCFMZ, KSC] </ref>. Etzioni and Weld [EW] synthesize agents that are Unix shell scripts by using classical AI planning techniques. The focus of Mitchell and Maes's work is in the interaction between users and agents. <p> Our agent architecture can also be used to set up Personal Library Systems [KC] "that selectively view, organize, and update contents" of an electronic library for individual use. We are interested in the same class of tasks as [EW, Mae, MCFMZ, KSC]. Etzioni and Weld <ref> [EW] </ref> synthesize agents that are Unix shell scripts by using classical AI planning techniques. The focus of Mitchell and Maes's work is in the interaction between users and agents. They propose statistical and machine learning methods for building user 4 models to control the agent actions [Mae, MCFMZ].
Reference: [FNK] <author> H. Fujisawa, Y. Nakano, and K. Kurino, </author> <title> Segmentation methods for character recognition: from segmentation to document structure analysis, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 80, no. 7, </volume> <year> 1992. </year>
Reference-contexts: We draw on this work to define parametric segmenters for electronic document retrieval. Previous work on classifying and logically relating blocks includes <ref> [TA, TSKK, FNK, NSV, WCW, MT*, WS] </ref>. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [WS, FNK, TA, NSV]. <p> We draw on this work to define parametric segmenters for electronic document retrieval. Previous work on classifying and logically relating blocks includes [TA, TSKK, FNK, NSV, WCW, MT*, WS]. Methods for discriminating between text blocks and pictures in bitmapped images are presented in <ref> [WS, FNK, TA, NSV] </ref>. A language for representing the hierarchical structure of documents is given in [FNK]. [TA] and [MT*] extract the logical structure by a bottom-up approach that starts with the finest units of structure and then computes aggregates. * Agents. <p> Previous work on classifying and logically relating blocks includes [TA, TSKK, FNK, NSV, WCW, MT*, WS]. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [WS, FNK, TA, NSV]. A language for representing the hierarchical structure of documents is given in <ref> [FNK] </ref>. [TA] and [MT*] extract the logical structure by a bottom-up approach that starts with the finest units of structure and then computes aggregates. * Agents. There has been a recent flurry of activity in the area of designing intelligent task-directed agents that live and act within realistic software environments.
Reference: [GK] <author> M. Genesereth, S. Ketchpel, </author> <title> Software agents, </title> <journal> in Communications of the ACM, </journal> <volume> vol. 37, no. 7, </volume> <pages> pg 48-53, </pages> <year> 1994. </year>
Reference-contexts: There has been a recent flurry of activity in the area of designing intelligent task-directed agents that live and act within realistic software environments. These are called knowbots by [KC], softbots by [EW], sodabots by [KSC], software agents by <ref> [GK] </ref>, and personal assistants by [Mae, MCFMZ]. A knowledge-level specification of know-bots is provided in Chapter 4 of [KC], which outlines the required capabilities without committing to a specific implementation.
Reference: [GGT] <author> L. Gravano, H. Garcia-Molina, and A. Tomasic, </author> <title> The Efficacy of GlOSS for the Text Database Discovery Problem, </title> <type> Technical Report no. </type> <institution> STAN-CS-TN-93-01, Computer Science Department, Stanford University, </institution> <year> 1993. </year>
Reference: [Gra] <author> R. Gray, </author> <title> Transportable Agents, </title> <type> Technical Report PCS-TR95-261, </type> <institution> Department of Computer Science, Dartmouth College, </institution> <year> 1995. </year> <month> 32 </month>
Reference-contexts: This paper describes our agent architecture and the sensory modules required for processing information. These modules fit on a transportable platform called Agent-Tcl 2 developed by our student Bob Gray. A preliminary report of this work is in his PhD thesis proposal <ref> [Gra] </ref>. <p> However, they exploit only one type of structure in the data. 2 This is a transportable version of Tcl/Tk <ref> [Gra] </ref> that allows programs to suspend execution and move to a different machine running an Agent-Tcl server. The current capabilities include process migration, message passing, and communication. 2 appropriate for the detectors. The modules are efficient, reliable in the presence of uncertainties in data interpretation and extraction, and fault tolerant.
Reference: [HP] <author> M. Hearst and C. Plaunt, </author> <title> Subtopic Structuring for Full-Length Document Access, </title> <booktitle> in Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pg. </booktitle> <pages> 59-68, </pages> <year> 1993. </year>
Reference: [HU] <author> J. Hopcroft and J. Ullman, </author> <title> Introduction to Automata Theory, Languages, and Computation, </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year>
Reference: [HKR] <author> D. Huttenlocher, G. Klanderman, and W. Rucklidge, </author> <title> Comparing images using the Haus-dorff distance, to appear, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence. </journal>
Reference: [HNR] <author> D. Huttenlocher, J. Noh, and W. Rucklidge, </author> <title> Tracking non-rigid objects in complex scenes, </title> <institution> Cornell University Technical Report TR92-1320, </institution> <year> 1992. </year>
Reference: [JB] <author> A. Jain and S. Bhattacharjee, </author> <title> Address block location on envelopes using Gabor filters, </title> <journal> Pattern Recognition, </journal> <volume> vol. 25, no. 12, </volume> <year> 1992. </year>
Reference-contexts: Document structuring is usually done in two phases. In the first phase, the location of the blocks on the page is determined. In the second phase, the blocks are classified and the logical layout of the document is calculated. Previous work on block segmentation of documents include <ref> [JB, NSV, WS] </ref>. The methods are developed in the context of very specific applications; segmenting pages of technical journals [NSV, WS]and locating address blocks in letters [JB]. <p> Previous work on block segmentation of documents include [JB, NSV, WS]. The methods are developed in the context of very specific applications; segmenting pages of technical journals [NSV, WS]and locating address blocks in letters <ref> [JB] </ref>. The methods (the run-length smoothing algorithm [WCW] and the recursive XY cuts algorithm [NSV]) use area-based techniques; that is, every pixel in the document is examined at least once in the process of generating the segmentation. We draw on this work to define parametric segmenters for electronic document retrieval.
Reference: [JR] <author> J. Jennings and D. </author> <title> Rus, Active model acquisition for near-sensorless manipulation with mobile robots, </title> <booktitle> in Proceedings of the IASTED Conference on Robotics and Automation, </booktitle> <year> 1993. </year>
Reference-contexts: Transportable agents can travel from machine to machine processing information locally, thus avoiding costly data transfers over congested networks. Agents, in addition to being transportable, can learn and can operate autonomously. We draw inspiration from robotics <ref> [Bro1, Bro2, Don, JR] </ref> to design information agents. The basic modules for physical robots are sensors and effectors. Our information agents are autonomous sensori-computational "circuits" comprised of a network of virtual sensors to detect, extract, and interpret structure in data and virtual effectors for transportation. <p> We have observed [Bro1, Bro2] that the lessons learned in designing task-directed mobile robots can be imported to the problem of information capture and access. We were influenced in defining topology-based segmenters and structure detectors by the work of <ref> [Don, JR] </ref>, who consider the problem of determining the information requirements to perform robot tasks using the concept of information invariants and perceptual equivalence classes. 2 Organizing Principles for Information Gathering Search En gines For any specific information access query, no matter how complicated, we can write a special purpose program
Reference: [Kah] <author> B. Kahle, </author> <title> Overview of Wide Area Information Servers, WAIS on-line documentation, </title> <year> 1991. </year>
Reference-contexts: Our research goal in this paper is to develop methods for solving the ICA problem and to provide a computational paradigm for customizing this process in heterogeneous, distributed repositories. A diverse collection of tools like Wais, Smart, Gopher, Archie and Mosaic <ref> [Kah, SM, SEKN] </ref> have been developed for information capture and access. Information capture tools like Gopher and World Wide Web (WWW) provide hierarchical and networked organization of data, but they require substantial manual effort to build and maintain.
Reference: [KC] <author> R. Kahn and V. Cerf, </author> <title> The World of Knowbots, report to the Corporation for National Research Initiative, </title> <address> Arlington, VA 1988. </address>
Reference-contexts: We will show how smart filters can be composed to form customizable search engines. Throughout this paper, we will refer to the result of the composition interchangeably as information agents and as search engines. 1.1 Related work The proposal by Kahn <ref> [KC] </ref> for organizing architectures for retrieving information from electronic repositories provides the context for the problem addressed in this paper. We are inspired by research in several distinct areas: information retrieval and filtering, automated document structuring, agents, information visualization, and mobile robotics. * Information Retrieval and Filtering. <p> There has been a recent flurry of activity in the area of designing intelligent task-directed agents that live and act within realistic software environments. These are called knowbots by <ref> [KC] </ref>, softbots by [EW], sodabots by [KSC], software agents by [GK], and personal assistants by [Mae, MCFMZ]. A knowledge-level specification of know-bots is provided in Chapter 4 of [KC], which outlines the required capabilities without committing to a specific implementation. <p> These are called knowbots by <ref> [KC] </ref>, softbots by [EW], sodabots by [KSC], software agents by [GK], and personal assistants by [Mae, MCFMZ]. A knowledge-level specification of know-bots is provided in Chapter 4 of [KC], which outlines the required capabilities without committing to a specific implementation. The modular information agent architecture, organized around the idea of structure that is presented here, can be treated as a specific implementation proposal for this specification. <p> In common with Kahn and Cerf, our initial applications are "in the retrieval of documents for which a user may only be able to specify an imprecise description". Our agent architecture can also be used to set up Personal Library Systems <ref> [KC] </ref> "that selectively view, organize, and update contents" of an electronic library for individual use. We are interested in the same class of tasks as [EW, Mae, MCFMZ, KSC]. Etzioni and Weld [EW] synthesize agents that are Unix shell scripts by using classical AI planning techniques.
Reference: [KSC] <author> H. Kautz, B. Selman, and M. Coen, </author> <title> Bottom-up design of software agents, </title> <journal> in Communications of the ACM, </journal> <volume> vol 37, no. 7, </volume> <pages> pg 143-145, </pages> <year> 1994. </year>
Reference-contexts: There has been a recent flurry of activity in the area of designing intelligent task-directed agents that live and act within realistic software environments. These are called knowbots by [KC], softbots by [EW], sodabots by <ref> [KSC] </ref>, software agents by [GK], and personal assistants by [Mae, MCFMZ]. A knowledge-level specification of know-bots is provided in Chapter 4 of [KC], which outlines the required capabilities without committing to a specific implementation. <p> Our agent architecture can also be used to set up Personal Library Systems [KC] "that selectively view, organize, and update contents" of an electronic library for individual use. We are interested in the same class of tasks as <ref> [EW, Mae, MCFMZ, KSC] </ref>. Etzioni and Weld [EW] synthesize agents that are Unix shell scripts by using classical AI planning techniques. The focus of Mitchell and Maes's work is in the interaction between users and agents.
Reference: [KF] <author> H. Kucera and W. Francis, </author> <title> Computational Analysis of Present Day American English, </title> <publisher> Brown University Press, </publisher> <address> Providence, RI, </address> <year> 1967. </year>
Reference-contexts: The analysis makes the following assumptions: * The average word length that occurs in text is known. For English, <ref> [KF] </ref> have determined that the average word length of distinct words is 8.1 characters, but of word occurrences in written text, it is 4.7 characters. For simplicity, we assume that in basic text, the average word length is 4 characters. * The blank spaces in base text are distributed independently.
Reference: [Les] <author> M. Lesk, </author> <title> The CORE electronic chemistry library, </title> <booktitle> Proceedings of the SIGIR, </booktitle> <year> 1991. </year>
Reference: [LR] <author> C. Lewis and D. </author> <title> Rus, Robust table recognition and extraction, </title> <type> forthcoming technical report, </type> <institution> Cornell University. </institution>
Reference-contexts: The regular expression r 1 +: : : + r n is a trivial generalization of a given set of the elements r 1 ; : : : ; r n ; otherwise it is non-trivial. 6 This section is based on <ref> [LR] </ref>. 12 Definition 2.8 Horizontal structure: Consider the columns c 1 : : : c n of a block of text satisfying Definition 2.7, and consider the lexical descriptions r 1 : : : r n of these columns. <p> t 4 t 6 t 8 t 9 t 10 3 7 7 7 7 7 7 7 7 We have implemented this table detector that is robust with respect to layout imperfections and used it to build search engines for retrieval tasks whose answers are found in tabular form <ref> [LR] </ref>. 2.2.2 Experiments with the table detector We have tested the performance of the table detector on several thousand articles culled from a number of Usenet news groups 10 over a period of a few weeks. Each article was partitioned into paragraphs and then filtered through the table detector.
Reference: [Mae] <author> P. Maes, </author> <title> Agents that reduce work and information overload, </title> <journal> in Communications of the ACM, </journal> <volume> vol 37, no. 7, </volume> <pages> pg 31-40, </pages> <year> 1994. </year>
Reference-contexts: There has been a recent flurry of activity in the area of designing intelligent task-directed agents that live and act within realistic software environments. These are called knowbots by [KC], softbots by [EW], sodabots by [KSC], software agents by [GK], and personal assistants by <ref> [Mae, MCFMZ] </ref>. A knowledge-level specification of know-bots is provided in Chapter 4 of [KC], which outlines the required capabilities without committing to a specific implementation. <p> Our agent architecture can also be used to set up Personal Library Systems [KC] "that selectively view, organize, and update contents" of an electronic library for individual use. We are interested in the same class of tasks as <ref> [EW, Mae, MCFMZ, KSC] </ref>. Etzioni and Weld [EW] synthesize agents that are Unix shell scripts by using classical AI planning techniques. The focus of Mitchell and Maes's work is in the interaction between users and agents. <p> Etzioni and Weld [EW] synthesize agents that are Unix shell scripts by using classical AI planning techniques. The focus of Mitchell and Maes's work is in the interaction between users and agents. They propose statistical and machine learning methods for building user 4 models to control the agent actions <ref> [Mae, MCFMZ] </ref>. Genesereth and Ketchpel propose a declarative agent communication language. * Information Visualization. The graphical-user-interface community has developed a range of visual methods for interacting with large information sets [CACM93]. The work most relevant to our project is that of the information visualization group at Xerox Parc [RCM].
Reference: [MCFMZ] <author> T. Mitchell, R. Caruana, D. Freitag, J. McDermott, and D. Zabowski, </author> <title> Experience with a learning personal assistant, </title> <journal> in Communications of the ACM, </journal> <volume> vol 37, no. 7, </volume> <pages> pg 81-91, </pages> <year> 1994. </year>
Reference-contexts: There has been a recent flurry of activity in the area of designing intelligent task-directed agents that live and act within realistic software environments. These are called knowbots by [KC], softbots by [EW], sodabots by [KSC], software agents by [GK], and personal assistants by <ref> [Mae, MCFMZ] </ref>. A knowledge-level specification of know-bots is provided in Chapter 4 of [KC], which outlines the required capabilities without committing to a specific implementation. <p> Our agent architecture can also be used to set up Personal Library Systems [KC] "that selectively view, organize, and update contents" of an electronic library for individual use. We are interested in the same class of tasks as <ref> [EW, Mae, MCFMZ, KSC] </ref>. Etzioni and Weld [EW] synthesize agents that are Unix shell scripts by using classical AI planning techniques. The focus of Mitchell and Maes's work is in the interaction between users and agents. <p> Etzioni and Weld [EW] synthesize agents that are Unix shell scripts by using classical AI planning techniques. The focus of Mitchell and Maes's work is in the interaction between users and agents. They propose statistical and machine learning methods for building user 4 models to control the agent actions <ref> [Mae, MCFMZ] </ref>. Genesereth and Ketchpel propose a declarative agent communication language. * Information Visualization. The graphical-user-interface community has developed a range of visual methods for interacting with large information sets [CACM93]. The work most relevant to our project is that of the information visualization group at Xerox Parc [RCM].
Reference: [MT*] <author> M. Mizuno, Y. Tsuji, T. Tanaka, H. Tanaka, M. Iwashita, and T. Temma, </author> <title> Document recognition system with layout structure generator, </title> <journal> NEC Research and Development, </journal> <volume> vol. 32, no. 3, </volume> <year> 1991. </year>
Reference-contexts: We draw on this work to define parametric segmenters for electronic document retrieval. Previous work on classifying and logically relating blocks includes <ref> [TA, TSKK, FNK, NSV, WCW, MT*, WS] </ref>. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [WS, FNK, TA, NSV]. <p> Previous work on classifying and logically relating blocks includes [TA, TSKK, FNK, NSV, WCW, MT*, WS]. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [WS, FNK, TA, NSV]. A language for representing the hierarchical structure of documents is given in [FNK]. [TA] and <ref> [MT*] </ref> extract the logical structure by a bottom-up approach that starts with the finest units of structure and then computes aggregates. * Agents. There has been a recent flurry of activity in the area of designing intelligent task-directed agents that live and act within realistic software environments.
Reference: [Mun] <author> J. Munkres, </author> <title> Topology: A First Course, </title> <publisher> Prentice Hall, </publisher> <year> 1975. </year> <month> 33 </month>
Reference-contexts: The table detector expects information to be broken up into pieces at the paragraph level; this is generated by a paragraph segmenter described later in this section. We model granularity shifts in the descriptions of the data using concepts from topology <ref> [Mun] </ref>. A topology over a set S is a set of subsets of S that are collectively exhaustive | the union of the subsets yields S. A topology is closed under the operations of union and finite intersection.
Reference: [NSV] <author> G. Nagy, S. Seth, and M. Vishwanathan, </author> <title> A prototype document image analysis system for technical journals, </title> <journal> Computer, </journal> <volume> vol. 25, no. 7, </volume> <year> 1992. </year>
Reference-contexts: Document structuring is usually done in two phases. In the first phase, the location of the blocks on the page is determined. In the second phase, the blocks are classified and the logical layout of the document is calculated. Previous work on block segmentation of documents include <ref> [JB, NSV, WS] </ref>. The methods are developed in the context of very specific applications; segmenting pages of technical journals [NSV, WS]and locating address blocks in letters [JB]. <p> In the second phase, the blocks are classified and the logical layout of the document is calculated. Previous work on block segmentation of documents include [JB, NSV, WS]. The methods are developed in the context of very specific applications; segmenting pages of technical journals <ref> [NSV, WS] </ref>and locating address blocks in letters [JB]. The methods (the run-length smoothing algorithm [WCW] and the recursive XY cuts algorithm [NSV]) use area-based techniques; that is, every pixel in the document is examined at least once in the process of generating the segmentation. <p> Previous work on block segmentation of documents include [JB, NSV, WS]. The methods are developed in the context of very specific applications; segmenting pages of technical journals [NSV, WS]and locating address blocks in letters [JB]. The methods (the run-length smoothing algorithm [WCW] and the recursive XY cuts algorithm <ref> [NSV] </ref>) use area-based techniques; that is, every pixel in the document is examined at least once in the process of generating the segmentation. We draw on this work to define parametric segmenters for electronic document retrieval. <p> We draw on this work to define parametric segmenters for electronic document retrieval. Previous work on classifying and logically relating blocks includes <ref> [TA, TSKK, FNK, NSV, WCW, MT*, WS] </ref>. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [WS, FNK, TA, NSV]. <p> We draw on this work to define parametric segmenters for electronic document retrieval. Previous work on classifying and logically relating blocks includes [TA, TSKK, FNK, NSV, WCW, MT*, WS]. Methods for discriminating between text blocks and pictures in bitmapped images are presented in <ref> [WS, FNK, TA, NSV] </ref>. A language for representing the hierarchical structure of documents is given in [FNK]. [TA] and [MT*] extract the logical structure by a bottom-up approach that starts with the finest units of structure and then computes aggregates. * Agents. <p> Most documents have a rectangular layout produced with a finite set of fonts. Each font size has a characteristic spacing between lines and characters. Our algorithm relies on the following generic typesetting rules. A superset of these conventions are in <ref> [NSV] </ref>. 1. Printed lines are roughly horizontal. 2. The base lines of characters are aligned. 3. Word spaces are larger than character spaces. 4. Paragraphs are separated by wider spaces than lines within a paragraph, or by indentation. 5. Illustrations are confined to rectangular frames.
Reference: [PN] <author> C. Pearce and C. Nicholas, </author> <title> Generating a dynamic hypertext environment with n-gram analysis, </title> <booktitle> in Proceedings of the ACM Conference on Information Knowledge Management, </booktitle> <pages> pp. 148-153, </pages> <year> 1993. </year>
Reference-contexts: Such a framework, based on the notion of information invariants, has been discussed in the robotics context by [Don, DJR] and in the theoretical literature by [BK]. Our long-term goal is to computationally characterize methods such as statistics over character sequences <ref> [SM, PN] </ref>, statistics over word occurrence, layout and geometry, and other notions of structure with respect to information content.
Reference: [Rob] <author> S. Robertson, </author> <title> The methodology of information retrieval experiment, Information Retrieval Experiment, </title> <editor> in K. Sparck Jones, </editor> <publisher> Ed., </publisher> <pages> pp 9-31, </pages> <publisher> Butterworths, </publisher> <year> 1981. </year>
Reference: [RCM] <author> G. Robertson, S. Card, and J. Mackinlay, </author> <title> Information visualization using 3D interactive animation, </title> <journal> in Communications of the ACM, </journal> <volume> Vol. 36, No. 4, pg 57070, </volume> <year> 1993. </year>
Reference-contexts: Genesereth and Ketchpel propose a declarative agent communication language. * Information Visualization. The graphical-user-interface community has developed a range of visual methods for interacting with large information sets [CACM93]. The work most relevant to our project is that of the information visualization group at Xerox Parc <ref> [RCM] </ref>. They have developed an integrated architecture that allows a user to browse through a rich information space and visualize the results of retrievals in interesting three-dimensional views (e.g., walk-throughs in 3D rooms).
Reference: [RSa] <author> D. Rus and D. Subramanian, </author> <title> Multi-media RISSC Informatics: Retrieving Information with Simple Structural Components, </title> <booktitle> in Proceedings of the ACM Conference on Information and Knowledge Management, </booktitle> <month> Nov. </month> <year> 1993. </year>
Reference: [RSb] <author> D. Rus and K. Summers, </author> <title> Using whitespace for automated document structuring, </title> <editor> in eds. eds. N. Adam, B. Bhargava, and Y. </editor> <title> Yesha Advances in digital libraries, </title> <note> Springer-Verlag, Lecture Notes in Computer Science, to appear, </note> <year> 1995. </year>
Reference-contexts: We illustrate the idea of segmenters with a general algorithm that can partition two dimensional documents with arbitrary layout. A special case of the algorithm presented here has been implemented and tested in the context of the Cornell technical report collection <ref> [RSb] </ref>. The segmenter in [RSb] automatically synthesizes a logical view of a document by analyzing the geometry of the white spaces in the left and right margins. 2.1.1 An Example: Segmenting Documents Given a pixel array of a document, the segmenter's goal is to partition the document into regions that capture <p> We illustrate the idea of segmenters with a general algorithm that can partition two dimensional documents with arbitrary layout. A special case of the algorithm presented here has been implemented and tested in the context of the Cornell technical report collection <ref> [RSb] </ref>. The segmenter in [RSb] automatically synthesizes a logical view of a document by analyzing the geometry of the white spaces in the left and right margins. 2.1.1 An Example: Segmenting Documents Given a pixel array of a document, the segmenter's goal is to partition the document into regions that capture its layout.
Reference: [SM] <author> G. Salton and M. McGill, </author> <title> Introduction to Modern Information Retrieval, </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Our research goal in this paper is to develop methods for solving the ICA problem and to provide a computational paradigm for customizing this process in heterogeneous, distributed repositories. A diverse collection of tools like Wais, Smart, Gopher, Archie and Mosaic <ref> [Kah, SM, SEKN] </ref> have been developed for information capture and access. Information capture tools like Gopher and World Wide Web (WWW) provide hierarchical and networked organization of data, but they require substantial manual effort to build and maintain. <p> We are inspired by research in several distinct areas: information retrieval and filtering, automated document structuring, agents, information visualization, and mobile robotics. * Information Retrieval and Filtering. We can interpret the classical work <ref> [SM, SB] </ref> in information retrieval (IR) as follows: the data environment is text, the unit of structure is typically a word, the detectors constructed in the IR literature are pattern-matchers over words, the segmenters are word indexes over documents. <p> Such a framework, based on the notion of information invariants, has been discussed in the robotics context by [Don, DJR] and in the theoretical literature by [BK]. Our long-term goal is to computationally characterize methods such as statistics over character sequences <ref> [SM, PN] </ref>, statistics over word occurrence, layout and geometry, and other notions of structure with respect to information content.
Reference: [SB] <author> G. Salton and C. Buckley, </author> <title> Improving retrieval performance by relevance feedback, </title> <journal> Journal of American Society for Information Science, </journal> <volume> vol. 41(4), </volume> <pages> pp. 288-297, </pages> <year> 1990. </year>
Reference-contexts: We are inspired by research in several distinct areas: information retrieval and filtering, automated document structuring, agents, information visualization, and mobile robotics. * Information Retrieval and Filtering. We can interpret the classical work <ref> [SM, SB] </ref> in information retrieval (IR) as follows: the data environment is text, the unit of structure is typically a word, the detectors constructed in the IR literature are pattern-matchers over words, the segmenters are word indexes over documents.
Reference: [Sal] <author> G. Salton, </author> <title> Automatic Text Processing: the transformation, analysis, and retrieval of information by computer, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference: [SK] <author> D. Sankoff and J. Kruskal, </author> <title> Time warps, string edits, and macromolecules: the theory and practice of sequence comparison, </title> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: To express this more rigorously, let M be a metric for string comparison (we use the Levenshtein metric <ref> [SK] </ref>.) Given * &gt; 0, two strings a and b are *-similar if M (a; b) * h . We use * h -typings, defined below, of the regular expressions that correspond to the entries of a column in order to control the imperfections we allow in horizontal structure.
Reference: [ST] <author> M. Schwartz and P. Tsirigotis, </author> <title> Experience with a Semantically Cognizant Internet White Pages Directory Tool, </title> <journal> Journal of Internetworking Research and Experience, </journal> <month> March </month> <year> 1991. </year>
Reference-contexts: We distinguish between logical and physical segmenters. A segmenter that extracts sections of an article for table detection generates a logical segmentation of the data environment. In contrast, agents like Netfind <ref> [ST] </ref> employ segmenters that partition the nodes in the Internet into relevant and irrelevant sets. We illustrate the idea of segmenters with a general algorithm that can partition two dimensional documents with arbitrary layout.
Reference: [SEKN] <author> M. Schwartz, A. Emtage, B. Kahle, and B. Neuman, </author> <title> A comparison of Internet discovery approaches, </title> <journal> Computer Systems, </journal> <volume> 5(4), </volume> <year> 1992. </year>
Reference-contexts: Our research goal in this paper is to develop methods for solving the ICA problem and to provide a computational paradigm for customizing this process in heterogeneous, distributed repositories. A diverse collection of tools like Wais, Smart, Gopher, Archie and Mosaic <ref> [Kah, SM, SEKN] </ref> have been developed for information capture and access. Information capture tools like Gopher and World Wide Web (WWW) provide hierarchical and networked organization of data, but they require substantial manual effort to build and maintain.
Reference: [TSKK] <author> Y. Tanosaki, K. Suzuki, K. Kikuchi, and M. Kurihara, </author> <title> A logical structure analysis system for documents, </title> <booktitle> Proceedings of the second international symposium on interoperable information systems, </booktitle> <year> 1988. </year>
Reference-contexts: We draw on this work to define parametric segmenters for electronic document retrieval. Previous work on classifying and logically relating blocks includes <ref> [TA, TSKK, FNK, NSV, WCW, MT*, WS] </ref>. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [WS, FNK, TA, NSV].
Reference: [TA] <author> S. Tsujimoto and H. Asada, </author> <title> Major components of a complete text reading system, </title> <booktitle> in Proceedings of the IEEE, </booktitle> <volume> vol. 80, no. 7, </volume> <year> 1992. </year>
Reference-contexts: We draw on this work to define parametric segmenters for electronic document retrieval. Previous work on classifying and logically relating blocks includes <ref> [TA, TSKK, FNK, NSV, WCW, MT*, WS] </ref>. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [WS, FNK, TA, NSV]. <p> We draw on this work to define parametric segmenters for electronic document retrieval. Previous work on classifying and logically relating blocks includes [TA, TSKK, FNK, NSV, WCW, MT*, WS]. Methods for discriminating between text blocks and pictures in bitmapped images are presented in <ref> [WS, FNK, TA, NSV] </ref>. A language for representing the hierarchical structure of documents is given in [FNK]. [TA] and [MT*] extract the logical structure by a bottom-up approach that starts with the finest units of structure and then computes aggregates. * Agents. <p> Previous work on classifying and logically relating blocks includes [TA, TSKK, FNK, NSV, WCW, MT*, WS]. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [WS, FNK, TA, NSV]. A language for representing the hierarchical structure of documents is given in [FNK]. <ref> [TA] </ref> and [MT*] extract the logical structure by a bottom-up approach that starts with the finest units of structure and then computes aggregates. * Agents. There has been a recent flurry of activity in the area of designing intelligent task-directed agents that live and act within realistic software environments.
Reference: [WS] <author> D. Wang and S. Srihari, </author> <title> Classification of newspaper image blocks using texture analysis, Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> vol. 47, </volume> <year> 1989. </year>
Reference-contexts: Document structuring is usually done in two phases. In the first phase, the location of the blocks on the page is determined. In the second phase, the blocks are classified and the logical layout of the document is calculated. Previous work on block segmentation of documents include <ref> [JB, NSV, WS] </ref>. The methods are developed in the context of very specific applications; segmenting pages of technical journals [NSV, WS]and locating address blocks in letters [JB]. <p> In the second phase, the blocks are classified and the logical layout of the document is calculated. Previous work on block segmentation of documents include [JB, NSV, WS]. The methods are developed in the context of very specific applications; segmenting pages of technical journals <ref> [NSV, WS] </ref>and locating address blocks in letters [JB]. The methods (the run-length smoothing algorithm [WCW] and the recursive XY cuts algorithm [NSV]) use area-based techniques; that is, every pixel in the document is examined at least once in the process of generating the segmentation. <p> We draw on this work to define parametric segmenters for electronic document retrieval. Previous work on classifying and logically relating blocks includes <ref> [TA, TSKK, FNK, NSV, WCW, MT*, WS] </ref>. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [WS, FNK, TA, NSV]. <p> We draw on this work to define parametric segmenters for electronic document retrieval. Previous work on classifying and logically relating blocks includes [TA, TSKK, FNK, NSV, WCW, MT*, WS]. Methods for discriminating between text blocks and pictures in bitmapped images are presented in <ref> [WS, FNK, TA, NSV] </ref>. A language for representing the hierarchical structure of documents is given in [FNK]. [TA] and [MT*] extract the logical structure by a bottom-up approach that starts with the finest units of structure and then computes aggregates. * Agents.
Reference: [WCW] <author> K. Wong, R. Casey, and F. Wahl, </author> <title> Document Analysis System, </title> <journal> IBM Journal of Research and Development, </journal> <volume> vol. 26, no. 6, </volume> <year> 1982. </year> <month> 34 </month>
Reference-contexts: Previous work on block segmentation of documents include [JB, NSV, WS]. The methods are developed in the context of very specific applications; segmenting pages of technical journals [NSV, WS]and locating address blocks in letters [JB]. The methods (the run-length smoothing algorithm <ref> [WCW] </ref> and the recursive XY cuts algorithm [NSV]) use area-based techniques; that is, every pixel in the document is examined at least once in the process of generating the segmentation. We draw on this work to define parametric segmenters for electronic document retrieval. <p> We draw on this work to define parametric segmenters for electronic document retrieval. Previous work on classifying and logically relating blocks includes <ref> [TA, TSKK, FNK, NSV, WCW, MT*, WS] </ref>. Methods for discriminating between text blocks and pictures in bitmapped images are presented in [WS, FNK, TA, NSV].
Reference: [Splus] <author> User Manual, </author> <title> Splus Reference Manual, </title> <journal> Statistical Sciences,Inc.,Seattle,Washington, </journal> <year> 1991. </year>
Reference-contexts: This is due to the fact that the lengths of words and of the spacing between them are variable, and their occurrences in a line of text are random. We have tested the independence of the distribution of whitespace by extensive experiments with Splus <ref> [Splus] </ref>. This implies that the blank spaces of a line have a binomial distribution. 7 Let B be a block of text of n rows and m columns.
Reference: [CACM93] <editor> Communications of the ACM, </editor> <volume> vol. 36, no. 4, </volume> <year> 1994. </year> <month> 35 </month>
Reference-contexts: They propose statistical and machine learning methods for building user 4 models to control the agent actions [Mae, MCFMZ]. Genesereth and Ketchpel propose a declarative agent communication language. * Information Visualization. The graphical-user-interface community has developed a range of visual methods for interacting with large information sets <ref> [CACM93] </ref>. The work most relevant to our project is that of the information visualization group at Xerox Parc [RCM].
References-found: 52

