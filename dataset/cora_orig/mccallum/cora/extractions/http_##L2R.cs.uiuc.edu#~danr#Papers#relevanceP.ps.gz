URL: http://L2R.cs.uiuc.edu/~danr/Papers/relevanceP.ps.gz
Refering-URL: http://L2R.cs.uiuc.edu/~danr/publications.html
Root-URL: http://www.cs.uiuc.edu
Email: froni,danrg@das.harvard.edu  
Title: Exploiting Relevance through Model-Based Reasoning  
Author: Roni Khardon Dan Roth 
Address: Cambridge, MA 02138.  
Affiliation: Aiken Computation Laboratory, Harvard University,  
Date: November 1994.  
Note: To appear in AAAI Fall symposium on Relevance,  
Abstract-found: 0
Intro-found: 1
Reference: <author> Amsterdam, J. </author> <year> 1988. </year> <title> Extending the valiant learning model. </title> <booktitle> In Proceeding of the Fifth International Workshop on Machine Learning, </booktitle> <pages> 364-375. </pages>
Reference: <author> Cadoli, M. </author> <year> 1993. </year> <title> Semantical and computational aspects of Horn approximations. </title> <booktitle> In Proceedings of the International Joint Conference of Artificial Intelligence, </booktitle> <pages> 39-44. </pages>
Reference: <author> Cohen, W. W. </author> <year> 1994. </year> <title> Pac-learning nondeterminate clauses. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> 676-681. </pages>
Reference-contexts: The second issue concerns using learning procedures that output only "approximate" representations. For example, PAC learning has been accepted as a good measure of learning even when learning for the purpose of performing reasoning 4 , e.g. when learning logic programs <ref> (Cohen 1994) </ref>. As observed in (Khardon & Roth 1994a; Kearns 1992) learning algorithms with guaranteed PAC performance may yield erroneous reasoning behavior unless they have an additional property: the hypothesis KB must be a subset of the function W (or at least a subset of its least upper bound).
Reference: <author> Greiner, R., and Schuurmans, D. </author> <year> 1992. </year> <title> Learning useful Horn approximations. </title> <booktitle> In Proceedings of the International Conference on the Principles of Knowledge Representation and Reasoning, </booktitle> <pages> 383-392. </pages>
Reference: <author> Johnson-Laird, P. N., and Byrne, R. M. J. </author> <year> 1991. </year> <title> Deduction. </title> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Johnson-Laird, P. N. </author> <year> 1983. </year> <title> Mental Models. </title> <publisher> Harvard Press. </publisher>
Reference: <author> Kautz, H., and Selman, B. </author> <year> 1991. </year> <title> A general framework for knowledge compilation. </title> <booktitle> In Proceedings of the International Workshop on Processing Declarative Knowledge, </booktitle> <address> Kaiserlautern, Germany. </address>
Reference: <author> Kautz, H., and Selman, B. </author> <year> 1992. </year> <title> Forming concepts for fast inference. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> 786-793. </pages>
Reference: <author> Kautz, H.; Kearns, M.; and Selman, B. </author> <year> 1993. </year> <title> Reasoning with characteristic models. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> 34-39. </pages>
Reference: <author> Kautz, H.; Kearns, M.; and Selman, B. </author> <year> 1994. </year> <title> Horn approximations of empirical data. </title> <journal> Artificial Intelligence. Forthcoming. </journal>
Reference: <author> Kearns, M. </author> <year> 1992. </year> <title> Oblivious pac learning of concepts hierarchies. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> 215-222. </pages>
Reference: <author> Khardon, R., and Roth, D. </author> <year> 1994a. </year> <title> Learning to reason. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> 682-687. </pages> <note> Full version: Technical Report TR-2-94, </note> <institution> Aiken Computation Lab., Harvard University, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Stated in a different way, as in <ref> (Khardon & Roth 1994a) </ref>, the result of Theorem 4 provides exact deduction, that is, a solution to the problem KB j= ff with respect to all *-fair queries ff. (The query ff is called (KB; *)-fair if either KB j= ff or P D [KB n ff] &gt; *. <p> In <ref> (Khardon & Roth 1994a) </ref> we have defined a general framework, learning to reason, that incorporates the ideas above into the the study of reasoning. <p> In general, making the same restrictions on the tasks and performance does not help if one is using formula-based knowledge representations and theorem proving (even though some restricted results are still possible in the learning to reason framework <ref> (Khardon & Roth 1994a) </ref>). It is interesting to note that this approach is very similar (though not identical) to theories of reasoning developed by psychologists (Johnson-Laird 1983; Johnson-Laird & Byrne 1991; Kosslyn 1983) who allude to an intuitive notion of relevance.
Reference: <author> Khardon, R., and Roth, D. </author> <year> 1994b. </year> <title> Reasoning with models. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> 1148-1153. </pages> <note> Full version: Technical Report TR-1-94, </note> <institution> Aiken Computation Lab., Harvard University, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Let Q be a set of common queries. In particular, we assume here for simplicity of exposition that Q is the set of CNF formulas in which each clause is either Horn or has at most 2 log n literals. In <ref> (Khardon & Roth 1994b) </ref> we show that the model-based approach is feasible: Theorem 1 ((Khardon & Roth 1994b)) For any knowledge base KB there is a set of models KB , whose size is bounded by the DNF size of KB, such that model based reasoning with KB is correct for
Reference: <author> Kosslyn, S. M. </author> <year> 1983. </year> <title> Image and Mind. </title> <publisher> Harvard Press. </publisher>
Reference: <author> Levesque, H. </author> <year> 1986. </year> <title> Making believers out of computers. </title> <booktitle> Artificial Intelligence 30 </booktitle> <pages> 81-108. </pages>
Reference-contexts: Consider the deduction problem KB j= ff: It has been argued that in real life situations, one normally completes a lot of missing "context" information when answering queries <ref> (Levesque 1986) </ref>.
Reference: <author> Reiter, R. </author> <year> 1980. </year> <title> A logic for default reasoning. </title> <journal> Artificial Intelligence 13(1,2). </journal>
Reference-contexts: The connective "implies" (j=) used between boolean functions is equivalent to the connective "subset or equal" () used for subsets of f0; 1g n , that is, f j= g if and only if f g. is very similar to default reasoning <ref> (Reiter 1980) </ref> but here we assume that the context (i.e. the "correct extension") is known. A Model-Based Approach Consider the following model-based approach to the problem KB j= ff: Test Set: A set of assignments. <p> We note that this simple approach to dealing with context can be extended to handle a restricted case of default reasoning in the sense defined by Reiter <ref> (Reiter 1980) </ref>. This can be done using results on abduction with models (Kautz, Kearns, & Selman 1993; Khardon & Roth 1994b), and results on the relation between abduction and default reasoning (Reiter 1987; Selman 1990).
Reference: <author> Reiter, R. </author> <year> 1987. </year> <title> A theory of diagnosis from first principles. </title> <booktitle> Artificial Intelligence 32(1). </booktitle>
Reference: <author> Roth, D. </author> <year> 1993. </year> <title> On the hardness of approximate reasoning. </title> <booktitle> In Proceedings of the International Joint Conference of Artificial Intelligence, </booktitle> <pages> 613-618. </pages>
Reference-contexts: The conditional probability P r D (ffjKB) is the degree of belief in the query ff, given the knowledge base KB. Consider the problem of computing the conditional probability P r D (ffjKB) where KB and ff are some propositional CNF formulas. The following hardness result is proved in <ref> (Roth 1993) </ref> 2 for the case where D is the uniform distribution: Theorem 3 The problem of computing P r D (ffjKB) is #P-Complete. Approximating it is NP-hard.
Reference: <author> Selman, B., and Kautz, H. </author> <year> 1990. </year> <title> Model-preference default theories. </title> <booktitle> Artificial Intelligence 45 </booktitle> <pages> 287-322. </pages>
Reference-contexts: This corresponds to assigning the value "true" to the attribute "here" for the purpose of answering the question. Sometimes we need a more expressive language to describe our assumptions regarding the current context and assume, say, that some rule applies <ref> (Selman & Kautz 1990) </ref>. For example, we may assume (in our current context) that if someone has a car, then it is a rental car.
Reference: <author> Selman, B., and Kautz, H. </author> <year> 1991. </year> <title> Knowledge compilation using Horn approximations. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> 904-909. </pages>
Reference: <author> Selman, B. </author> <year> 1990. </year> <title> Tractable Default Reasoning. </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, University of Toronto. </institution>
Reference-contexts: This corresponds to assigning the value "true" to the attribute "here" for the purpose of answering the question. Sometimes we need a more expressive language to describe our assumptions regarding the current context and assume, say, that some rule applies <ref> (Selman & Kautz 1990) </ref>. For example, we may assume (in our current context) that if someone has a car, then it is a rental car.
Reference: <author> Valiant, L. G. </author> <year> 1984. </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM 27(11) </journal> <pages> 1134-1142. </pages>
Reference-contexts: In systems that learn, the world in which the performance criterion is applied is the same world that supplies the agent with the information it learns from, through some interface. This intuition is captured in the distribution free model of learning theory <ref> (Valiant 1984) </ref>. There, an agent first wanders around in the world observing examples drawn from some unknown distribution D which governs the occurrences of instances in the world, and then has to perform its task, namely classify instances. <p> That may be phrased as using algorithms with properties which are relevant to the task. Conclusions We have considered several situations where relevance can be exploited in order to make the computational 4 Originally <ref> (Valiant 1984) </ref>, the framework was suggested for the purpose of learning to classify instances. task of reasoning easier. First we considered using lim-ited information, describing a particular context, in order to reason within that context. This can be done if we have complete information and the information about the context.
References-found: 22

