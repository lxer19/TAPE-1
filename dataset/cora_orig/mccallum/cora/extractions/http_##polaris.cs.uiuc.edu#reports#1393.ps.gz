URL: http://polaris.cs.uiuc.edu/reports/1393.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Title: INDUCTION VARIABLE SUBSTITUTION AND REDUCTION RECOGNITION IN THE POLARIS PARALLELIZING COMPILER  
Author: BY WILLIAM MORTON POTTENGER 
Degree: B.A., Lehigh University, 1980 THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the Graduate College of the  
Date: 1989  
Address: Alaska, Fairbanks,  1995 Urbana, Illinois  
Affiliation: B.S., University of  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Zahira Ammarguellat and Luddy Harrison. </author> <title> Automatic Recognition of Induction & Recurrence Relations by Abstract Interpretation. </title> <booktitle> Proceedings of Sigplan 1990, </booktitle> <address> Yorktown Heights, </address> <month> 25(6) </month> <pages> 283-295, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Thus, although it appears that the framework is capable of supporting the solution of inductions in multiple loops, it has not been implemented. A second feature of <ref> [1] </ref> is the unification of recurrences with predefined templates containing closed forms.
Reference: [2] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: INTRODUCTION The parallelization of loops requires resolution of many types of data dependences ([5], [22], <ref> [2] </ref>). In particular, cross-iteration dependences caused by inductions may prohibit parallel execution. Induction variable substitution is an important technique for resolving certain classes of such dependences. In addition, induction solution provides the environment within which privatization of arrays can take place.
Reference: [3] <author> M. Berry, D. Chen, P. Koss, D. Kuck, L. Pointer, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, G. Swanson, R. Goodrum, and J. Martin. </author> <title> The Perfect Club Benchmarks: Effective Performance Evalution of Supercomputers. </title> <booktitle> Int'l. Journal of Supercomputer Applications, Fall 1989, </booktitle> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference: [4] <author> Bill Blume, Rudolf Eigenmann, Keith Faigin, John Grout, Jay Hoeflinger, David Padua, Paul Petersen, Bill Pottenger, Lawrence Rauchwerger, Peng Tu, and Stephen Weatherford. </author> <title> Polaris: The Next Generation in Parallelizing Compilers. </title> <booktitle> Proceedings of the Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, New York, pages 10.1 - 10.18, </address> <month> August </month> <year> 1994. </year>
Reference: [5] <author> William Blume and Rudolf Eigenmann. </author> <title> The Range Test: A Dependence Test for Symbolic, Nonlinear Expressions. </title> <booktitle> Proceedings of Supercomputing '94, November 1994, </booktitle> <address> Washington D.C., </address> <pages> pages 528-537. </pages>
Reference-contexts: Following this, a second, data-dependence test pass analyzes these candidate reductions to determine if they are indeed reductions <ref> [5] </ref>. In the case where the data-dependence test fails to determine independence, the algorithm conservatively assumes that a reduction actually exists so that, in effect, we are still able to parallelize the loop.
Reference: [6] <author> William Blume and Rudolf Eigenmann. </author> <title> Symbolic Range Propagation. </title> <type> Technical Report 1381, </type> <institution> Univ of Illinois at Urbana-Champaign, Cntr for Supercomputing Res & Dev, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: This is a non-trivial problem in symbolic analysis, and is currently handled by new techniques developed in <ref> [6] </ref> known as Symbolic Range Propagation 52 .
Reference: [7] <author> William Blume, Rudolf Eigenmann, Jay Hoeflinger, David Padua, Paul Petersen, Lawrence Rauchw-erger, and Peng Tu. </author> <title> Automatic Detection of Parallelism: A Grand Challenge for High-Performance Computing. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(3) </volume> <pages> 37-47, </pages> <month> Fall </month> <year> 1994. </year>
Reference: [8] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> An Efficient Method of Computing Static Single Assignment Form. </title> <booktitle> Proceedings of ACM Symposium on the Priniciples of Programming Languages, </booktitle> <year> 1989. </year>
Reference-contexts: For additional information, see <ref> [8] </ref> 24 280 LABEL 280 ENDDO LMIN_$3 = 1 LMAX_$3 = MK_$1+1 290 LABEL 290 ENDDO Let's trace the value of LM AX through to it's use as the upper bound of the M L loop 280 14 .
Reference: [9] <author> SGI Documentation. </author> <title> Fortran 77 Programmer's Guide and associated man pages. </title> <type> Technical report, </type> <institution> Silicon Graphics, Inc., </institution> <year> 1994. </year>
Reference-contexts: Parallel processes are executed on a processor set consisting of 2, 4 or 8 processors 5 , and 3 Flash mode sets the master processes' priority to a non-degrading maximum which is inherited by all spawned (slave) processes <ref> [9] </ref> 4 Timing was performed by the timex command which reports wall-clock, user, and system times to 1=100 second 5 For background on processor sets see [15] 55 all processes were set to spin-wait (rather than block) while waiting for a parallel section to begin execution 6 . <p> These were obtained by running on a two-processor processor set and disabling migration between processors (i.e., executing on only a single processor) 7 . For comparison purposes, the three benchmarks under discussion were compiled and executed using the commercially available SGI product Power Fortran Accelerator, PFA (see <ref> [9] </ref> for detail). Compilation commands were as follows: for the serial runs, "f77 -O2 -mips2"; for the Polaris parallel runs, "f77 -mp -O2 -mips2"; and for the PFA runs, "f77 -pfa keep -O2 -mips2" 8 . <p> The cause for this is found in the fact that several small trip count loops in do1000 which contain reductions were parallelized, incurring a large degree of overhead 12 . Heuristics to handle this situation better are under development. 10 accurate to 1=100 of a second <ref> [9] </ref> 11 On the order of 4, versus the 6.3 reported above 12 These inner loops are of course not parallelized when do1000 is parallelized 57 4.2.2 MDG-POTENG do2000 This loop accounts for less than 2% of the execution time, however it too has a very good speedup.
Reference: [10] <author> Rudolf Eigenmann, Jay Hoeflinger, Zhiyuan Li, and David Padua. </author> <title> Experience in the Automatic Parallelization of Four Perfect-Benchmark Programs. </title> <booktitle> Lecture Notes in Computer Science 589. Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <pages> pages 65-83, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: ADMAR C2D BDNAD YFESM FLO52MDG MG3DOCEANQCD SPEC77TRA CK TRFD induction substitution 8.3 a 12.7 parallel reductions b 3.3 2.1 1.1 21 15.2 3.4 c Table 1.1: Performance loss from disabling individual restructuring techniques in terms of factor increase of the best execution time a This is a speedup cited from <ref> [10] </ref> b ADM contains reductions in significant loops. Most of them can be parallelized using existing synchronization tech niques without excessive overhead c TRFD contains accumulation operations that would become important for parallelization if advanced induction variable substitution and array privatization were not available.
Reference: [11] <author> Rudolf Eigenmann, Jay Hoeflinger, and David Padua. </author> <title> On the Automatic Parallelization of the Perfect Benchmarks . Technical Report 1392, </title> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> December </month> <year> 1994. </year> <month> 65 </month>
Reference-contexts: Current compilers are able to solve simple scalar reductions and in some cases, single dimensional array reductions with 1 In triangular loop nests, inner loop bounds depend on outer loop indices 2 invariant indices. Based on work completed in <ref> [11] </ref>, however, two additional classes of reductions were determined important: single address reductions, which occur on a scalar or on an array of one or more dimensions with loop invariant indices; and histogram reductions, which occur on arrays with loop variant indices. <p> All references to the loop index within a given loop are adjusted accordingly 33 . Next we iterate through each do loop in a given ProgramUnit and each statement in the loop. The performance critical reductions identified in <ref> [11] </ref> are additive reductions. As a result, at this stage of the algorithm we look only at assignment statements of the form outlined in the algorithm above. <p> While not confirmed at this time, the reason for this result is most likely due to Polaris' greater success in identifying parallel loops. 59 CHAPTER 5 RELATED WORK In <ref> [11] </ref>, induction variable substitution and parallel reductions were recognized as two of four performance-critical techniques necessary for the parallelization of the Perfect Club Benchmark codes. This section deals with several alternative approaches to the solution of these two problems. <p> To that end we have focussed on two techniques which were clearly identified as important in <ref> [11] </ref> induction variable substitution and parallel reductions. The approaches discussed in the Section 5 solve recurrence relations in one degree or another, but none of the reported work has demonstrated actual speedups on real benchmark codes.
Reference: [12] <author> Keith A. Faigin, Jay P. Hoeflinger, David A. Padua, Paul M. Petersen, and Stephen A. Weatherford. </author> <title> The Polaris Internal Representation. </title> <type> Technical Report 1317, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. and Dev, </institution> <month> October </month> <year> 1993. </year>
Reference: [13] <author> Michael P. Gerlek, Eric Stoltz, and Michael Wolfe. </author> <title> Beyond induction variables: Detecting and classifying sequences using a demand-driver ssa form. </title> <note> To appear in TOPLAS, </note> <month> August, </month> <year> 1994. </year>
Reference: [14] <author> Ronald L. Graham, Donald E. Knuth, and Oren Patashnik. </author> <title> Concrete Mathematics. </title> <publisher> Addison-Wesley, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Given the coefficients B k , expressions are factored into monomial form and summed according to 3.1 above. 17 Material drawn from <ref> [14] </ref>, page 269 18 Normally denoted C (n; r) 27 3.1.9 Induction Variable Substitution in TRFD TRFD, one of the Perfect Club Benchmark codes, is a kernel simulating the computational aspects of a two-electron integral transformation. It's interest to us lies in the presence of some unusually complex induction variables.
Reference: [15] <author> Anoop Gupta, Andrew Tucker, and Luis Stevens. </author> <title> Making Effective Use of Shared-Memory Multiprocessors: The Process Control Approach. </title> <type> Technical Report CSL-TR-91-475A, </type> <institution> Stanford University, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: processors 5 , and 3 Flash mode sets the master processes' priority to a non-degrading maximum which is inherited by all spawned (slave) processes [9] 4 Timing was performed by the timex command which reports wall-clock, user, and system times to 1=100 second 5 For background on processor sets see <ref> [15] </ref> 55 all processes were set to spin-wait (rather than block) while waiting for a parallel section to begin execution 6 . In the above tests all parallel timing was conducted on an 8-processor processor set with the exception of the single processor times.
Reference: [16] <author> Mohammad Haghighat and Constantine Polychronopoulos. </author> <title> Symbolic Analysis: A Basis for Paral-leliziation, Optimization, and Scheduling of Programs. </title> <booktitle> Proceedings of the Sixth Annual Languages and Compilers for Parallelism Workshop, </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: In a prototype implementation, the induction pass used the symbolic algebra package Maple TM to solve recurrences. However, as has been pointed out in <ref> [16] </ref>, there are serious drawbacks to both Mathematica TM and Maple. Both of these packages are unable to handle the problem of zero-trip loops.
Reference: [17] <author> Mohammad R. Haghighat and Constantine D. Polychronopoulos. </author> <title> Symbolic Program Analysis and Optimization for Parallelizing Compilers. </title> <booktitle> Presented at the 5th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August 3-5, </month> <year> 1992. </year>
Reference: [18] <author> Luddy Harrison. </author> <title> Personal communication with author, </title> <year> 1994. </year>
Reference: [19] <author> Jay Hoeflinger. </author> <title> Automatic Parallelization and Manual Improvements of the Perfect Club Program TRFD for Cedar. </title> <type> Technical Report 1247, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: For example: K1 = 0 Do I = 1; N K1 = K1 + 1 EndDo K2 = K2 + K1 EndDo 1 E.g., in the Perfect Club Benchmark code TRFD, subroutine OLDA <ref> [19] </ref> 5 Here the induction variable K2 is dependent on the (triangular) induction variable K1.
Reference: [20] <author> Jay Hoeflinger. </author> <title> Automatic Parallelization and Manual Improvements of the Perfect Club Program OCEAN for Cedar. </title> <type> Technical Report 1246, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: 1; N K private = 2 fl 2 ((1)+I) A (K private ) = 0 EndDo Collectively, we have termed these three classes of inductions Generalized Induction Variables, or GIVs. 2.2 What is Reduction Recognition? Consider the following example code segment: 2 Important in the Perfect Club Benchmark code OCEAN <ref> [20] </ref> 6 EndDo Due to loop-carried flow, anti, and output dependences on sum, this loop cannot be executed in parallel (note that sum is not referenced elsewhere in the loop). However, it is possible to solve this reduction by a number of methods which take better advantage of parallel hardware. <p> IF (JL-1) 102,102,104 DO 103 JJ=JL,NPTS,I2KP DO 103 MM=1,MTRN JS=(JJ-1)*NSKIP+(MM-1)*MSKIP+1 H=DATA (JS)-DATA (JS+I2KS) DATA (JS)=DATA (JS)+DATA (JS+I2KS) DATA (JS+I2KS)=H 103 CONTINUE GO TO 109 C C INCREMENT JL-DEPENDENT EXPONENTIAL FACTOR C DO 106 JJ=JL,NPTS,I2KP DO 106 MM=1,MTRN JS=(JJ-1)*NSKIP+(MM-1)*MSKIP+1 H=DATA (JS)-DATA (JS+I2KS) DATA (JS)=DATA (JS)+DATA (JS+I2KS) DATA (JS+I2KS)=H*EXJ 25 See <ref> [20] </ref> 36 106 CONTINUE GO TO 109 107 EXJ=CMPLX (0.,SGN1) DO 108 JJ=JL,NPTS,I2KP DO 108 MM=1,MTRN JS=(JJ-1)*NSKIP+(MM-1)*MSKIP+1 H=DATA (JS)-DATA (JS+I2KS) DATA (JS)=DATA (JS)+DATA (JS+I2KS) DATA (JS+I2KS)=CMPLX (-SGN1*HH (2),SGN1*HH (1)) 108 CONTINUE 109 CONTINUE ... RETURN END After manual partitioning, the code becomes: SUBROUTINE FTRVMT (DATA,EX) ...
Reference: [21] <author> D. Padua and M. Wolfe. </author> <title> Advanced Compiler Optimization for Supercomputers. </title> <journal> CACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December, </month> <year> 1986. </year>
Reference: [22] <author> W. Pugh. </author> <title> The omega test: A fast and practical integer programming algorithm for dependence analysis. </title> <booktitle> Supercomputing '91, </booktitle> <year> 1991. </year>
Reference-contexts: INTRODUCTION The parallelization of loops requires resolution of many types of data dependences ([5], <ref> [22] </ref>, [2]). In particular, cross-iteration dependences caused by inductions may prohibit parallel execution. Induction variable substitution is an important technique for resolving certain classes of such dependences. In addition, induction solution provides the environment within which privatization of arrays can take place.
Reference: [23] <author> Peng Tu and David Padua. </author> <title> Automatic Array Privatization. </title> <editor> In Utpal BanerjeeDavid Gelern-terAlex NicolauDavid Padua, editor, </editor> <booktitle> Proc. Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR. </address> <booktitle> Lecture Notes in Computer Science., </booktitle> <volume> volume 768, </volume> <pages> pages 500-521, </pages> <month> August 12-14, </month> <year> 1993. </year>
Reference: [24] <author> Peng Tu and David Padua. </author> <title> Demand-Driven Symbolic Analysis. </title> <type> Technical Report 1336, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> Febraury </month> <year> 1994. </year>
Reference: [25] <author> Peng Tu and David Padua. </author> <title> Efficient Building and Placing of Gating Functions. </title> <type> Technical Report 1389, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: variables matched against the SCC under 3 The current implementation of their work does not do substitution [27] 4 This operates in a manner similar to that we have implemented for the detection of wrap-around loop bounds in Section 3.1.6 5 and functions are distinguished in a way similar to <ref> [25] </ref>. is a special case of that has an arity of two with reaching definitions coming from outside and inside the loop respectively 6 The induction variable types we have termed triangular and multiplicative fall into these two classes 7 Used in the solution of non-constant periodic sequences 62 investigation.
Reference: [26] <author> Stephen Weatherford. </author> <title> High-Level Pattern-Matching Extensions to C++ for Fortran Program Manipulation in Polaris. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: In this case, 2 is invariant so K remains an active candidate. Since no coupled inductions exist and flow control is limited to DO loops, K becomes a valid induction. The final step is to calculate ~: 11 Using Polaris FORBOL primitives <ref> [26] </ref> 19 k (j) = EndDo Y s=DOJ multipliers of K = 2 This completes the recognition phase. Phase 2 Calculating the closed form In this phase, processing begins with entry into the initial DO loop with index J . <p> For example, the pattern used to match definition-use pairs in the form: A (ff (i; j; k; :::)) = A (ff (i; j; k; :::)) + fi (l; m; n; :::) is implemented as 34 Possibly from among several alternatives 35 Part of the FORBOL environment mentioned above <ref> [26] </ref> 43 wildcard not (wildcard contains (id (symbol))). This translates to the creation of a Wildcard object which does not contain a reference to id (symbol), where id (symbol) returns a reference to the array name A shown above 36 . All structure matching is then done using these objects. <p> All structure matching is then done using these objects. For further detail on the implementation of the WildCard class structure, an excellent source can be found in <ref> [26] </ref>, which treats the FORBOL environment in Polaris. There is much of interest and importance to discuss in the reduction backend, and this is the topic of the next section. 3.2.4 Code Generation Overview As noted in the introduction to this section, reduction recognition is only the first step.
Reference: [27] <author> Michael Wolfe and Michael Gerlek. </author> <title> Personal communication with authors, </title> <year> 1994. </year>
Reference-contexts: The schemes employed to classify a given variable are based on a priori knowledge of classes of sequence variables matched against the SCC under 3 The current implementation of their work does not do substitution <ref> [27] </ref> 4 This operates in a manner similar to that we have implemented for the detection of wrap-around loop bounds in Section 3.1.6 5 and functions are distinguished in a way similar to [25]. is a special case of that has an arity of two with reaching definitions coming from outside
References-found: 27

