URL: http://www.ri.cmu.edu/afs/cs/usr/kdeng/www/paper/effloc.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs/usr/kdeng/www/papers.html
Root-URL: 
Title: Efficient Locally Weighted Polynomial Regression Predictions  
Author: Andrew W. Moore, Jeff Schneider, and Kan Deng 
Keyword: Locally Weighted Learning, Instance Based Learning, Memory Based Learning, Prediction  
Note: Email: awm@cs.cmu.edu, Phone: 412-268-7599  
Address: 5000 Forbes Ave, Pittsburgh, PA 15123  
Affiliation: The Robotics Institute, Carnegie Mellon University  
Abstract: This paper is currently submitted for publication) Abstract Locally weighted polynomial regression is a popular instance-based algorithm for learning continuous non-linear mappings. For more than two or three inputs and for more than a few thousand datapoints the computational expense of predictions is daunting. We discuss drawbacks with previous approaches to dealing with this problem, and present a new algorithm based on a multiresolution search of a quickly-constructible augmented kd-tree. Without needing to rebuild the tree, we can make fast predictions with arbitrary local weighting functions, arbitrary kernel widths and arbitrary queries. The paper begins with a new, faster, algorithm for exact LWPR predictions. Next we introduce an approximation that achieves up to a two-orders-of-magnitude speedup with negligible accuracy losses. Increasing a certain approximation parameter achieves greater speedups still, but with a correspondingly larger accuracy degradation. This is nevertheless useful during operations such as the early stages of model selection and locating optima of a fitted surface. We also show how the approximations can permit real-time query-specific optimization of the kernel width. We conclude with a brief discussion of potential extensions for tractable instance-based learning on datasets that are too large to fit in a com puter's main memory. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <year> 1984. </year>
Reference-contexts: And in turn they have children. This continues recursively until the leaf nodes, which each contain just one point. How do we decide which input attribute to split on and where? Unlike 5 decision trees <ref> [1, 9] </ref> for induction, the sole purpose of the splits are to increase computational efficiency|not to alter the inductive bias.
Reference: [2] <author> W. S. Cleveland and S. J. Delvin. </author> <title> Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 83(403) </volume> <pages> 596-610, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: It is particularly appropriate for learning complex highly non-linear functions of up to about 30 inputs from noisy data. Popularized in the statistics literature in the past decades <ref> [2, 5] </ref> it is enjoying increasing use in applications such as learning robot dynamics [7, 11] and learning process models.
Reference: [3] <author> K. Deng and A. W. Moore. </author> <title> Multiresolution Instance-based Learning. </title> <booktitle> In To appear in proceddings of IJCAI-95. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Much worse, for many datasets, the best kernel width is very broad, meaning that a significant fraction of the data (sometimes all the data) has non-zero weight. In that case, avoiding the zero-weight datapoints is not much help. In this paper we use the main idea from <ref> [3] </ref> in which a multiresolution data structure increased the speed of kernel regression (also known as Locally Weighted Averaging). Here, we extend that method to arbitrary locally weighted polynomials, and give a number of empirical evaluations of the resulting algorithms. <p> We do not believe that the choice between the numerous kd-tree splitting criteria is critical for this purpose, and so we choose the same "split in the middle of the dimension with the widest spread" method as <ref> [3] </ref>. Let ND be a node in the kd-tree.
Reference: [4] <author> J. H. Friedman, J. L. Bentley, and R. A. Finkel. </author> <title> An Algorithm for Finding Best Matches in Logarithmic Expected Time. </title> <journal> ACM Trans. on Mathematical Software, </journal> <volume> 3(3) </volume> <pages> 209-226, </pages> <month> September </month> <year> 1977. </year>
Reference-contexts: The scheme used in this paper retains all the information necessary to do a full local regression analysis, providing noise estimates and confidence intervals along with the prediction. A third solution, and one which does retain information, uses a technique called range-searching with kd-trees <ref> [4] </ref>. It is possible to arrange data in such a way that given a query point and a distance, all datapoints within the given distance of the query are returned without needing to search the entire dataset.
Reference: [5] <author> E. Grosse. LOESS: </author> <title> Multivariate Smoothing by Moving Least Squares. </title> <editor> In L. L. Schumaker C. K. Chul and J. D. Ward, editors, </editor> <title> Approximation Theory VI. </title> <publisher> Academic Press, </publisher> <year> 1989. </year>
Reference-contexts: It is particularly appropriate for learning complex highly non-linear functions of up to about 30 inputs from noisy data. Popularized in the statistics literature in the past decades <ref> [2, 5] </ref> it is enjoying increasing use in applications such as learning robot dynamics [7, 11] and learning process models. <p> X does not need to be constructed explicitly. 4 which a multivariate spline model is built when the data is first loaded. <ref> [5] </ref> do this with a variable resolution kd-tree structure and multilinear interpolation within tree leaves. Unfortunately, continuous interpolation above two dimensions is very expensive in computation and memory. [10] also uses a caching method but ignores continuity by storing separate discontinuous linear maps in the leaves.
Reference: [6] <author> T. J. Hastie and R. J. Tibshirani. </author> <title> Generalized additive models. </title> <publisher> Chapman and Hall, </publisher> <year> 1990. </year>
Reference-contexts: Popularized in the statistics literature in the past decades [2, 5] it is enjoying increasing use in applications such as learning robot dynamics [7, 11] and learning process models. Both classical and Bayesian linear regression analysis tools can be extended to work in the locally weighted framework <ref> [6] </ref>, providing confidence intervals on predictions, on gradient estimates and on noise estimates|all important when a learned mapping is to be used by a controller. Let's briefly review LWPR. We'll begin with linear regression on one input and one output.
Reference: [7] <author> A. W. Moore. </author> <title> Fast, Robust Adaptive Control by Learning only Forward Models. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippman, editors, </editor> <booktitle> Ad 16 vances in Neural Information Processing Systems 4. </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> April </month> <year> 1992. </year>
Reference-contexts: It is particularly appropriate for learning complex highly non-linear functions of up to about 30 inputs from noisy data. Popularized in the statistics literature in the past decades [2, 5] it is enjoying increasing use in applications such as learning robot dynamics <ref> [7, 11] </ref> and learning process models. Both classical and Bayesian linear regression analysis tools can be extended to work in the locally weighted framework [6], providing confidence intervals on predictions, on gradient estimates and on noise estimates|all important when a learned mapping is to be used by a controller.
Reference: [8] <author> S. M. Omohundro. </author> <title> Bumptrees for Efficient Function, Constraint, and Classification Learning. </title> <editor> In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: This strengthens the W SoFar bound. * Ball trees <ref> [8] </ref> play a similar role to a kd-tree used for range searching, but it is possible that a hierarchy of balls, each containing the sufficient statistics of datapoints they contain, could be used beneficially in place of the bounding boxes we used. * The algorithms have been modified to permit the
Reference: [9] <author> J. R. Quinlan. </author> <title> Learning Efficient Classification Procedures and their Application to Chess End Games. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning|An Artificial Intelligence Approach (I). </booktitle> <publisher> Tioga Publishing Company, </publisher> <address> Palo Alto, </address> <year> 1983. </year>
Reference-contexts: And in turn they have children. This continues recursively until the leaf nodes, which each contain just one point. How do we decide which input attribute to split on and where? Unlike 5 decision trees <ref> [1, 9] </ref> for induction, the sole purpose of the splits are to increase computational efficiency|not to alter the inductive bias.
Reference: [10] <author> J. R. Quinlan. </author> <title> Combining Instance-Based and Model-Based Learning. </title> <booktitle> In Machine Learning: Proceedings of the Tenth International Conference, </booktitle> <year> 1993. </year>
Reference-contexts: Unfortunately, continuous interpolation above two dimensions is very expensive in computation and memory. <ref> [10] </ref> also uses a caching method but ignores continuity by storing separate discontinuous linear maps in the leaves. Another downside to caching solutions is that they only record the fitted surface.
Reference: [11] <author> S. Schaal and C. Atkeson. </author> <title> Robot Juggling: An Implementation of Memory-based Learning. </title> <journal> Control Systems Magazine, </journal> <volume> 14, </volume> <year> 1994. </year> <month> 17 </month>
Reference-contexts: It is particularly appropriate for learning complex highly non-linear functions of up to about 30 inputs from noisy data. Popularized in the statistics literature in the past decades [2, 5] it is enjoying increasing use in applications such as learning robot dynamics <ref> [7, 11] </ref> and learning process models. Both classical and Bayesian linear regression analysis tools can be extended to work in the locally weighted framework [6], providing confidence intervals on predictions, on gradient estimates and on noise estimates|all important when a learned mapping is to be used by a controller. <p> With N datapoints and M terms, this means O (N M 2 ) multiplications to build the matrices and then another O (M 3 ) operations to compute the fi vector. Sometimes the computational expense isn't an inconvenience. The devil stick robot of <ref> [11] </ref> was able to make 5 predictions a second with 10 inputs, 5 outputs and a thousand data points using a DSP board. But in cases such as graphing, optimizing over the model, and performing test-set validation of a model, much faster predictions are desirable.
References-found: 11

