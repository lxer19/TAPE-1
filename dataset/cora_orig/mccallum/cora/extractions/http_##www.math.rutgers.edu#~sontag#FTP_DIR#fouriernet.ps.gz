URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/fouriernet.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Title: Using Fourier-Neural Recurrent Networks to Fit Sequential Input/Output Data  
Author: Renee Koplon Eduardo D. Sontag 
Date: March 1, 1995  
Address: Dayton, Ohio 45435  New Brunswick, New Jersey 08903  
Affiliation: Dept. of Mathematics and Statistics Wright State University  Dept. of Mathematics Rutgers University  
Abstract: This paper suggests the use of Fourier-type activation functions in fully recurrent neural networks. The main theoretical advantage is that, in principle, the problem of recovering internal coefficients from input/output data is solvable in closed form.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Albertini, Francesca., and Eduardo. D. Sontag, </author> <title> "For neural networks, function determines form," </title> <booktitle> Neural Networks 6 (1993), </booktitle> <pages> pp. 975-990. </pages> <note> Summarized version in Proc. IEEE Conf. </note> <institution> Decision and Control, </institution> <address> Tucson, Dec. 1992, </address> <publisher> IEEE Publications, </publisher> <year> 1992, </year> <pages> pp. 26-31. </pages>
Reference-contexts: It was shown that at least some algebraic issues such as testing for identifiability and observability of system parameters can be handled analogously to the case of linear systems (see <ref> [1, 2, 3] </ref>). In a related context, recurrent net models have been proposed, and used by many authors, for adaptive control and system identification applications (see e.g. [9, 11]). In this paper, we propose a control theory-based technique for the initial estimation of weights. <p> Mathematically justifying such potential applications is beyond the scope of the current theoretical results. The Approach Here The approach taken here is inspired by the major developments in <ref> [1] </ref> (for continuous-time) and [2] (for discrete-time) that internal coefficients or "weights" of a recurrent net can be determined from external dynamical behavior in an essentially unique manner.
Reference: [2] <author> Albertini, Francesca., and Eduardo. D. Sontag, </author> <title> "Identifiability of discrete-time neural networks," </title> <booktitle> Proc. European Control Conference, </booktitle> <address> Groningen, </address> <month> June </month> <year> 1993, </year> <pages> pp. 460-465. </pages>
Reference-contexts: It was shown that at least some algebraic issues such as testing for identifiability and observability of system parameters can be handled analogously to the case of linear systems (see <ref> [1, 2, 3] </ref>). In a related context, recurrent net models have been proposed, and used by many authors, for adaptive control and system identification applications (see e.g. [9, 11]). In this paper, we propose a control theory-based technique for the initial estimation of weights. <p> Mathematically justifying such potential applications is beyond the scope of the current theoretical results. The Approach Here The approach taken here is inspired by the major developments in [1] (for continuous-time) and <ref> [2] </ref> (for discrete-time) that internal coefficients or "weights" of a recurrent net can be determined from external dynamical behavior in an essentially unique manner. <p> For the computer implemention, of course, cannot actually be irrational. But if we assume r and s are reasonably small (which corresponds to the entries of B being reasonably small, since the determinations of log used by most computer systems have arguments in [; ] or <ref> [0; 2] </ref>), then a that is not close to being the quotient of two small integers will work. Using (7) we can, in principle, find the unique integer vectors r and s and recover the true B as follows.
Reference: [3] <author> Albertini, Francesca., and Eduardo. D. Sontag, </author> <title> "State observability in recurrent neural networks," </title> <booktitle> Proc. IEEE Conf. Decision and Control, </booktitle> <address> San Antonio, Dec. 1993, </address> <publisher> IEEE Publications, </publisher> <year> 1993, </year> <pages> pp. 3706-3707. </pages>
Reference-contexts: It was shown that at least some algebraic issues such as testing for identifiability and observability of system parameters can be handled analogously to the case of linear systems (see <ref> [1, 2, 3] </ref>). In a related context, recurrent net models have been proposed, and used by many authors, for adaptive control and system identification applications (see e.g. [9, 11]). In this paper, we propose a control theory-based technique for the initial estimation of weights.
Reference: [4] <author> Ben-Or, M., and P. Tiwari, </author> <title> "A deterministic algorithm for sparse multivariate polynomial interpolation," </title> <booktitle> in Proc. 29th IEEE Symp. Foundations of Comp. Sci., </booktitle> <year> 1988, </year> <pages> pp. 301-309. </pages>
Reference-contexts: The method appears in many other areas; for instance in coding theory for decoding BCH codes, and in learning theory for sparse polynomial interpolation (see section 3 in the paper <ref> [4] </ref>).
Reference: [5] <author> Koplon, Renee, </author> <title> Linear Systems with Constrained Outputs and Transitions, </title> <type> Ph.D. Dissertation, </type> <institution> Rutgers University, </institution> <year> 1994. </year>
Reference: [6] <author> Koplon, Renee, and Eduardo. D. Sontag, </author> <title> "Techniques for Parameter Reconstruction in Fourier-Neural Recurrent Networks," </title> <booktitle> Proc. IEEE Conf. Decision and Control, </booktitle> <address> Or-lando, Dec. 1994, </address> <publisher> IEEE Publications, </publisher> <year> 1994, </year> <pages> pp. 213-218. </pages>
Reference: [7] <author> Kuhn, Gary M., and Norman P. Herzberg, </author> <title> "Some Variations on Training of Recurrent Networks," Neural Networks: Theory and Applications, </title> <editor> (Richard J. Mammone and Yehoshua Zeevi, </editor> <booktitle> eds.) </booktitle> <pages> pp. 233-244. </pages>
Reference: [8] <author> Ljung, L., </author> <title> System Identification: Theory for the User, </title> <publisher> Prentice Hall, </publisher> <year> 1987. </year>
Reference-contexts: Comparison With Linear Systems Historically linear systems have been used to approximate various nonlinear systems since the parameters can be identified easily. Linear system identification is a well-established and widely used tool; see for instance <ref> [8] </ref>. In broad ranges of operation, linearity is a reasonable simplification, as the success of the linear theory attests to. However, linear systems are limited. Most real world processes are inherently nonlinear. In many areas, linear simplifications are not valid, which means that nonlinear approaches must be tried.
Reference: [9] <author> Narendra, K.S. and K. Parthasarathy, </author> <title> "Identification and control of dynamical systems using neural networks," </title> <journal> IEEE Trans. Neural Nets, </journal> <volume> 1 (1990), </volume> <pages> pp. 4-27. </pages>
Reference-contexts: In a related context, recurrent net models have been proposed, and used by many authors, for adaptive control and system identification applications (see e.g. <ref> [9, 11] </ref>). In this paper, we propose a control theory-based technique for the initial estimation of weights. Most implementations of recurrent network learning algorithms assume a random initialization of weights.
Reference: [10] <author> Nesterov, Y. and A. Nemirovskii, </author> <title> Interior-point Polynomial Algorithms in Convex Programming, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1994. </year>
Reference: [11] <author> Polycarpou, </author> <title> M.M., and P.A. Ioannou, "Neural networks and on-line approximators for adaptive control," </title> <booktitle> in Proc. Seventh Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pp. 93-798, </pages> <institution> Yale University, </institution> <year> 1992. </year>
Reference-contexts: In a related context, recurrent net models have been proposed, and used by many authors, for adaptive control and system identification applications (see e.g. <ref> [9, 11] </ref>). In this paper, we propose a control theory-based technique for the initial estimation of weights. Most implementations of recurrent network learning algorithms assume a random initialization of weights.
Reference: [12] <author> Siegelmann, Hava. T., and Eduardo. D. Sontag, </author> <title> "Some results on computing with `neural nets'," </title> <booktitle> Proc. IEEE Conf. Decision and Control, </booktitle> <address> Tucson, Dec. 1992, </address> <publisher> IEEE Publications, </publisher> <year> 1992, </year> <pages> pp. 1476-1481. 28 </pages>
Reference-contexts: With feedback, one may exploit context-sensitivity and memory, characteristics essential in sequence processing as well as in the modeling and control of processes involving dynamical elements. Recent theoretical results about neural networks have established their universality as models for systems approximation as well as analog computing devices (see e.g. <ref> [14, 12] </ref>). The use of recurrent networks has been proposed in areas as varied as the design of control laws for robotic manipulators, in speech recognition, speaker identification, formal language inference, and sequence extrapolation for time series prediction.
Reference: [13] <author> Sontag, Eduardo. D., </author> <title> Mathematical Control Theory: Deterministic Finite Dimensional Systems, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: This step of the algorithm is based on Hankel-matrix techniques that are classical in the context of linear recurrences and their multivariable extensions developed in control theory (see a detailed discussion in Chapter 5 of <ref> [13] </ref>). The method appears in many other areas; for instance in coding theory for decoding BCH codes, and in learning theory for sparse polynomial interpolation (see section 3 in the paper [4]).
Reference: [14] <author> Sontag, Eduardo. D., </author> <title> "Neural networks for control," in Essays on Control: Perspectives in the Theory and its Applications (H.L. </title> <editor> Trentelman and J.C. Willems, eds.), </editor> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1993, </year> <pages> pp. 339-380. 29 </pages>
Reference-contexts: With feedback, one may exploit context-sensitivity and memory, characteristics essential in sequence processing as well as in the modeling and control of processes involving dynamical elements. Recent theoretical results about neural networks have established their universality as models for systems approximation as well as analog computing devices (see e.g. <ref> [14, 12] </ref>). The use of recurrent networks has been proposed in areas as varied as the design of control laws for robotic manipulators, in speech recognition, speaker identification, formal language inference, and sequence extrapolation for time series prediction.
References-found: 14

