URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tm.outbox/MIT-LCS-TM-503.ps.gz
Refering-URL: http://www.cag.lcs.mit.edu/multiscale/fugu-abstract.html
Root-URL: 
Title: FUGU: Implementing Translation and Protection in a Multiuser, Multimodel Multiprocessor  
Author: Kenneth Mackenzie, John Kubiatowicz, Anant Agarwal and Frans Kaashoek 
Note: This research has been funded in part by NSF grant MIP-9012773, in part by DARPA contract N00014-91-J-1698, and in part by a NSF Presidential Young Investigator Award.  
Date: October 24, 1994  
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: Multimodel multiprocessors provide both shared memory and message passing primitives to the user for efficient communication. In a multiuser machine, translation permits machine resources to be virtualized and protection permits users to be isolated. The challenge in a multiuser multiprocessor is to provide translation and protection sufficient for general-purpose computing without compromising communication performance, particularly the performance of communication between parallel threads belonging to the same computation. FUGU is a proposed architecture that integrates translation and protection with a set of communication mechanisms originally designed for high performance on a single-user, physically-addressed, large-scale, multimodel multiprocessor. Communication in FUGU is based on the mechanisms of the Alewife machine [1]. The mechanisms are shared memory with hardware cache coherence, user-level message sends and receives, and a user-controlled DMA facility integrated with messages for bulk transfers. This paper presents a design that integrates translation and protection with these communication mechanisms. Three components of the design are novel. First, we propose maintaining TLB coherence as a side-effect of cache coherence on page table entries. Second, we describe how to permit user-launched and user-handled messages without dedicating physical memory for buffering at the sender or at the receiver. We propose to use a rudimentary, second, system-only network to avoid deadlock of the user-accessible network. Third, we show how to integrate user DMA with virtual memory to permit bulk transfers without prenegotiation and global locking of physical memory. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, David Chaiken, Godfrey D'Souza, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, Dan Nussbaum, Mike Parkin, and Donald Yeung. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper has been submitted for publication, and appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: Multiprocessors, for example Alewife <ref> [1] </ref>, achieve good performance by providing direct access to the underlying hardware. Shared-memory machines initiate communication with ordinary, user-level load and store instructions. User-level messaging interfaces permit user-level code to launch and to handle messages by directly manipulating the network interface hardware. <p> An application programmer generally will not write programs at this level of abstraction but will instead use higher-level abstractions provided by the compiler and runtime system [12]. The Alewife machine serves as our prototype of a single-user, multimodel multiprocessor <ref> [1, 14] </ref>. The machine consists of identical processing nodes connected via a message-passing network. FUGU (Figure 1) is arranged similarly, although with a second logical network. The Communications and Memory Management Unit (CMMU) serves to implement shared memory and serves as the network interface for message passing. <p> A minimal message can be launched from processor registers with three instructions: stio rheader, cmmu-output-registers [0] stio rvalue, cmmu-output-registers <ref> [1] </ref> launch At the receive side, the processor is notified of the arrival of a packet either via polling or via interrupts, at the option of the user code. An incoming message appears in a complementary set of 16 CMMU registers forming a window into the network input queue. <p> Descriptors are distinguished from values by using another colored store instruction, stdesc. Single blocks are restricted to one page in size for reasons explained in Section 5. A block of memory could be included in a message as follows: stio rheader, cmmu-output-registers [0] stio rvalue, cmmu-output-registers <ref> [1] </ref> stdesc rbegin, cmmu-output-registers [2] stdesc rend, cmmu-output-registers [3] launch At the receive side, a handler can invoke DMA to dispose of a message by writing descriptors for the destination blocks of memory into a third set of CMMU registers before invoking the storeback operation. <p> It focuses on hardware and software mechanisms for protection and translation in scalable multiprocessors that provide user-level messaging and coherent shared memory. The FUGU effort relates to other research projects as follows. Alewife <ref> [1] </ref> is a single-user, multimodel multiprocessor. It has user-level message handling integrated with coherent shared memory. It supports both short messages and bulk transfer (DMA) messages, with a unified packet interface. Bulk transfer messages require the receiving processor to determine the destination of the data.
Reference: [2] <author> Anant Agarwal, John Kubiatowicz, David Kranz, Beng-Hong Lim, Donald Yeung, Godfrey D'Souza, and Mike Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Multiprocessors. </title> <journal> IEEE Micro, </journal> <volume> 13(3) </volume> <pages> 48-61, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: An outgoing message is composed in the window and then launch atomically commits the message to the network. The first word of the message is distinguished 1 Such as provided by the SPARCLE processor <ref> [2] </ref> and the general SPARC architecture. 5 as the header and is interpreted by the hardware for routing. <p> Single blocks are restricted to one page in size for reasons explained in Section 5. A block of memory could be included in a message as follows: stio rheader, cmmu-output-registers [0] stio rvalue, cmmu-output-registers [1] stdesc rbegin, cmmu-output-registers <ref> [2] </ref> stdesc rend, cmmu-output-registers [3] launch At the receive side, a handler can invoke DMA to dispose of a message by writing descriptors for the destination blocks of memory into a third set of CMMU registers before invoking the storeback operation.
Reference: [3] <author> Thomas E. Anderson, Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. </author> <title> Scheduler Activations: Effective Kernel Support for the User-Level Management of Parallelism. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 95-109, </pages> <year> 1991. </year>
Reference-contexts: Single blocks are restricted to one page in size for reasons explained in Section 5. A block of memory could be included in a message as follows: stio rheader, cmmu-output-registers [0] stio rvalue, cmmu-output-registers [1] stdesc rbegin, cmmu-output-registers [2] stdesc rend, cmmu-output-registers <ref> [3] </ref> launch At the receive side, a handler can invoke DMA to dispose of a message by writing descriptors for the destination blocks of memory into a third set of CMMU registers before invoking the storeback operation. <p> Note that this 6 latter stipulation is merely for performance reasons; we do not advocate enforced space sharing or hard-partitioning of the machine and gang-scheduling of partitions. Scheduling processes together appears to be desirable in general for multiprocessing <ref> [3, 26, 7] </ref>. 2.3 Protection Model User process groups should act as if they are running on a private, virtual multiprocessor. Multiple groups can be multiplexed on the hardware without interference, except in terms of performance.
Reference: [4] <author> David L. Black, Richard F. Rashid, David B. Golub, Charles R. Hill, and Robert V. Baron. </author> <title> Translation Lookaside Buffer Consistency: A Software Approach. </title> <booktitle> In Third International Conference on Architectural Support for Programming Languages and Operating Systems., </booktitle> <pages> pages 113-122. </pages> <publisher> ACM, </publisher> <year> 1989. </year>
Reference-contexts: and potentially require accesses to global memory. 3 In essence, there are two levels of page faults reported: not-in-local-memory and not-in-any-memory. 3.3 Translation Coherence via PTE Coherence The problem of maintaining a coherent view of a global virtual address space across a multiprocessor is generally called the TLB consistency problem <ref> [4, 23, 24] </ref>. Translations for global pages are read from entries in the global page table (GPTEs). In FUGU, GPTEs may be stored in several places aside from the processors' TLBs, including the main caches and the LPTs. <p> PLATINUM maintains a directory for each GPTE. The standard TLB shootdown algorithm maintains one directory for an entire page table <ref> [4, 23] </ref>. Maintaining directories at a grain finer than a page table is important to scalability because it generally reduces the number of nodes that must receive invalidation messages for any given invalidation. FUGU maintains a directory for several GPTEs together on a cache line.
Reference: [5] <author> Matthias A. Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, Edward W. Felten, and Jonathan Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings 21st Annual International Symposium on Computer Architecture (ISCA'94), </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year> <month> 26 </month>
Reference-contexts: User-initiated message sends can be permitted to globally named, pre-negotiated areas of physical memory at the receiver, for instance as remote-write operations. Translation and protection are handled in analogy to virtual memory. SHRIMP <ref> [5] </ref> and bulk transfers in FLASH [15, 10] use remote-write. 2. User-level access to the network hardware can be preserved if the machine is rigidly partitioned and all hardware in the partition, including the network, is context switched. The CM-5 adopts this solution [17]. 3. <p> For instance, *T demultiplexes messages into several receive queues, and the receive queues are implemented as a part of the processor register set. SHRIMP <ref> [5] </ref> and Hamlyn [27] propose remote writes for protected user communication between pre-negotiated and pre-locked pages. Recent work on parallel processing on networks of workstations seeks to identify mechanisms to accelerate communication while retaining protection.
Reference: [6] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: Trapping to software at the memory side has been proposed for hybrid hardware/software implementations of cache coherence and for implementing special case coherence protocols <ref> [6] </ref>. Translation coherence is a case of an alternate cache coherence protocol: the trap is used to send alternate invalidation messages so that side effects may be added at the cache side.
Reference: [7] <author> Rohit Chandra, Scott Devine, Ben Verghese, Anoop Gupta, and Mendel Rosenblum. </author> <title> Scheduling and Page Migration for Multiprocessor Compute Servers. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <pages> pages 12-24. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: Note that this 6 latter stipulation is merely for performance reasons; we do not advocate enforced space sharing or hard-partitioning of the machine and gang-scheduling of partitions. Scheduling processes together appears to be desirable in general for multiprocessing <ref> [3, 26, 7] </ref>. 2.3 Protection Model User process groups should act as if they are running on a private, virtual multiprocessor. Multiple groups can be multiplexed on the hardware without interference, except in terms of performance. <p> We expect the operating system to make use of these coarse-grain sharing techniques to complement the fine-grain sharing provided by the native coherent caches. Various groups have experimented with tradeoffs between multiple shared memory mechanisms and found page-grain sharing beneficial on shared-memory machines without caching <ref> [22, 16, 7] </ref>. Second, translation at the processor is an implementation issue that provides a simplification and a complication. The simplification is that performing all translation at the processor permits the rest of the memory system, including the hardware shared memory system, to operate in terms of physical addresses.
Reference: [8] <author> A. Cox and R. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <month> December </month> <year> 1989. </year> <note> Also as a Univ. </note> <institution> Rochester TR-263, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: be supported by recording the active cache page number in the coherence directory associated with each memory line and permitting only one cache page per memory line to be active at a time. 8 FUGU uses a variation of the page table layout used by Cox and Fowler for PLATINUM <ref> [8] </ref>. The page tables are split into local page tables (LPTs), stored inverted in the private, physical memory of each processor, and a global page table (GPT), stored in distributed, shared memory. All pages that are physically resident on the node (whether private or shared) are listed in the LPT.
Reference: [9] <author> William J. Dally et al. </author> <title> The J-Machine: A Fine-Grain Concurrent Computer. </title> <booktitle> In Proceedings of the IFIP (International Federation for Information Processing), 11th World Congress, </booktitle> <pages> pages 1147-1153, </pages> <address> New York, 1989. </address> <publisher> Elsevier Science Publishing. </publisher>
Reference-contexts: Similarly, Hybrid Deposit [19] proposes hardware to interpret messages as operations on pre-negotiated buffer areas. FUGU's approach is to add protection while maintaining existing, well-defined user-level communication mechanisms and efficient, distributed shared memory. The J-machine multicomputer <ref> [9] </ref> provides two levels of network priorities, user-level access to the network hardware and the ability to relaunch incoming messages from memory transparently. The J-machine is a single-user machine with no support for shared memory or DMA on messages.
Reference: [10] <author> John Heinlein, Kourosh Gharachorloo, Scott Dresser, and Anoop Gupta. </author> <title> Integration of Message Passing and Shared Memory in the Stanford FLASH Multiprocessor. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 38-50. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: User-initiated message sends can be permitted to globally named, pre-negotiated areas of physical memory at the receiver, for instance as remote-write operations. Translation and protection are handled in analogy to virtual memory. SHRIMP [5] and bulk transfers in FLASH <ref> [15, 10] </ref> use remote-write. 2. User-level access to the network hardware can be preserved if the machine is rigidly partitioned and all hardware in the partition, including the network, is context switched. The CM-5 adopts this solution [17]. 3. <p> Bulk transfer messages require the receiving processor to determine the destination of the data. FUGU extends Alewife features for multiuser operation and uses an exokernel operating system. In FUGU, bulk transfers require translation at the receiving processor, but do not pre-negotiate for pages. FLASH <ref> [15, 10] </ref> is a multimodel machine with a microcoded, kernel-level coprocessor for message handling including shared-memory protocol messages. Bulk transfers in FLASH are in the form of remote-writes which avoid using the receiving processor, but require pre-negotiating the sending addresses in shared memory.
Reference: [11] <author> James C. Hoe. </author> <title> Network Interface for Message-Passing Parallel Computation on a Workstation Cluster. </title> <booktitle> In Proceedings of Hot Interconnects II, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: The CM-5 adopts this solution [17]. 3. User-initiated transfers between memories can use explicit acknowledgements to manage sender-side buffer space for each message. Software protocols such as IP typically use this approach. FLASH general messages and FUNet <ref> [11] </ref> provide the acknowledgements in hardware. 1.2 FUGU Overview FUGU provides hardware support for three communication mechanisms. A coherent shared memory system communicates implicitly via messages synthesized and interpreted by hardware.
Reference: [12] <author> Wilson C. Hsieh, Kirk L. Johnson, M. Frans Kaashoek, Deborah A. Wallach, and William E. Weihl. </author> <title> Efficient Implementation of High-Level Languages on User-Level Communication Architectures. </title> <publisher> MIT/LCS TR-616, MIT, </publisher> <address> Cambridge, MA, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: The term user-level in this context refers to the protection domain. An application programmer generally will not write programs at this level of abstraction but will instead use higher-level abstractions provided by the compiler and runtime system <ref> [12] </ref>. The Alewife machine serves as our prototype of a single-user, multimodel multiprocessor [1, 14]. The machine consists of identical processing nodes connected via a message-passing network. FUGU (Figure 1) is arranged similarly, although with a second logical network.
Reference: [13] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory; Early Experience. </title> <booktitle> In Practice and Principles of Parallel Programming (PPoPP) 1993, </booktitle> <pages> pages 54-63, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year> <note> ACM. Also as MIT/LCS TM-478, </note> <month> January </month> <year> 1993. </year>
Reference-contexts: This class of multimodel machines is of great interest because it combines the best of two worlds: the efficiency of message passing, for bulk data transfer and for combining data with synchronization, and the flexibility and convenience of shared memory for fine-grained and dynamic computations <ref> [13] </ref>. Multiprocessors, for example Alewife [1], achieve good performance by providing direct access to the underlying hardware. Shared-memory machines initiate communication with ordinary, user-level load and store instructions. User-level messaging interfaces permit user-level code to launch and to handle messages by directly manipulating the network interface hardware. <p> Local coherence is easier and more desirable to support in hardware for a number of reasons [14]. Furthermore, many uses of messages involve separate message buffers which are reserved exclusively for message traffic <ref> [13] </ref>. Thus, we propose to support locally-coherent DMA in hardware and to synthesize global coherence with software only when required, just as in Alewife. Block cleaning However, we also wish to support general paging, including page migration. Further, we wish to support explicit rearrangement of shared data by the user.
Reference: [14] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In Proceedings of the International Supercomputing Conference (ISC) 1993, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year> <note> IEEE. Also as MIT/LCS TM-498, </note> <month> December </month> <year> 1992. </year>
Reference-contexts: An application programmer generally will not write programs at this level of abstraction but will instead use higher-level abstractions provided by the compiler and runtime system [12]. The Alewife machine serves as our prototype of a single-user, multimodel multiprocessor <ref> [1, 14] </ref>. The machine consists of identical processing nodes connected via a message-passing network. FUGU (Figure 1) is arranged similarly, although with a second logical network. The Communications and Memory Management Unit (CMMU) serves to implement shared memory and serves as the network interface for message passing. <p> Second, the use of DMA in conjunction with cache-coherent shared memory raises questions about the level of memory coherence which can be expected from DMA-transferred data. This is the DMA-coherence problem introduced in <ref> [14] </ref>. Third, the use of DMA implies some form of asynchronous notification mechanism to indicate to the user that previously requested DMA operations have completed. <p> If so, we permit the migration, but schedule the (newly unmapped) local page frame for future merging with data. 5.2 DMA and Cache Coherence The use of DMA in a cache-coherent shared memory multiprocessor raises the specter of DMA coherence <ref> [14] </ref>. The most general possible DMA coherence model is that of global coherence, in which data in both the source and destination blocks of memory are fully coherent with respect to all processors. <p> Local coherence is easier and more desirable to support in hardware for a number of reasons <ref> [14] </ref>. Furthermore, many uses of messages involve separate message buffers which are reserved exclusively for message traffic [13]. Thus, we propose to support locally-coherent DMA in hardware and to synthesize global coherence with software only when required, just as in Alewife.
Reference: [15] <author> Jeffrey Kuskin, David Ofelt, and Mark Heinrich et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA) 1994, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: User-initiated message sends can be permitted to globally named, pre-negotiated areas of physical memory at the receiver, for instance as remote-write operations. Translation and protection are handled in analogy to virtual memory. SHRIMP [5] and bulk transfers in FLASH <ref> [15, 10] </ref> use remote-write. 2. User-level access to the network hardware can be preserved if the machine is rigidly partitioned and all hardware in the partition, including the network, is context switched. The CM-5 adopts this solution [17]. 3. <p> Bulk transfer messages require the receiving processor to determine the destination of the data. FUGU extends Alewife features for multiuser operation and uses an exokernel operating system. In FUGU, bulk transfers require translation at the receiving processor, but do not pre-negotiate for pages. FLASH <ref> [15, 10] </ref> is a multimodel machine with a microcoded, kernel-level coprocessor for message handling including shared-memory protocol messages. Bulk transfers in FLASH are in the form of remote-writes which avoid using the receiving processor, but require pre-negotiating the sending addresses in shared memory.
Reference: [16] <author> T. J. LeBlanc, B. D. Marsh, and M. L. Scott. </author> <title> Memory Management for Large-Scale NUMA Multiprocessors. </title> <type> TR 311, </type> <institution> Rochester, Rochester, </institution> <address> NY, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: We expect the operating system to make use of these coarse-grain sharing techniques to complement the fine-grain sharing provided by the native coherent caches. Various groups have experimented with tradeoffs between multiple shared memory mechanisms and found page-grain sharing beneficial on shared-memory machines without caching <ref> [22, 16, 7] </ref>. Second, translation at the processor is an implementation issue that provides a simplification and a complication. The simplification is that performing all translation at the processor permits the rest of the memory system, including the hardware shared memory system, to operate in terms of physical addresses.
Reference: [17] <author> Charles E. Leiserson, Aahil S. Abuhamdeh, and David C. Douglas et al. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In The Fourth Annual ACM Symposium on Parallel Algorithms and Architectures. ACM, </booktitle> <year> 1992. </year>
Reference-contexts: SHRIMP [5] and bulk transfers in FLASH [15, 10] use remote-write. 2. User-level access to the network hardware can be preserved if the machine is rigidly partitioned and all hardware in the partition, including the network, is context switched. The CM-5 adopts this solution <ref> [17] </ref>. 3. User-initiated transfers between memories can use explicit acknowledgements to manage sender-side buffer space for each message. Software protocols such as IP typically use this approach. FLASH general messages and FUNet [11] provide the acknowledgements in hardware. 1.2 FUGU Overview FUGU provides hardware support for three communication mechanisms. <p> The second processor lengthens the message latency in the best-case situation, although it provides concurrency in most other situations. Typhoon provides network protection by context-switching the network in the manner of the CM-5 <ref> [17] </ref>. Context-switching the network fails to support a client-server model of interprocess communication. The *T [18] processor uses a memory coprocessor model as well. *T does not include DMA facilities or coherent caches and thus does not address the interaction of these features with messaging. <p> The J-machine is a single-user machine with no support for shared memory or DMA on messages. The CM-5 multicomputer provides multiuser multiprocessing by rigidly partitioning its network and context switching entire partitions <ref> [17] </ref>.
Reference: [18] <author> R. S. Nikhil, G. M. Papadopoulos, and Arvind. </author> <title> *T: A Multithreaded Massively Parallel Architecture. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167. </pages> <publisher> ACM, </publisher> <year> 1992. </year>
Reference-contexts: The second processor lengthens the message latency in the best-case situation, although it provides concurrency in most other situations. Typhoon provides network protection by context-switching the network in the manner of the CM-5 [17]. Context-switching the network fails to support a client-server model of interprocess communication. The *T <ref> [18] </ref> processor uses a memory coprocessor model as well. *T does not include DMA facilities or coherent caches and thus does not address the interaction of these features with messaging.
Reference: [19] <author> Randy Osborne. </author> <title> A Hybrid Deposit Model for Low Overhead Communication in High Speed LANs. </title> <type> Technical Report 94-02v3, </type> <address> MERL, 201 Broadway, Cambridge, MA 02139, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Work by Thekkath and others at the University of Washington [25] advocates remote memory access to pre-negotiated buffers as a mechanism to 25 accelerate communication while retaining protection. Similarly, Hybrid Deposit <ref> [19] </ref> proposes hardware to interpret messages as operations on pre-negotiated buffer areas. FUGU's approach is to add protection while maintaining existing, well-defined user-level communication mechanisms and efficient, distributed shared memory.
Reference: [20] <author> Gregory M. Papadopoulos, G. Andy Boughton, Robert Greiner, and Michael J. Beckerle. </author> <title> *T: Integrated Building Blocks for Parallel Computing. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 624-635. </pages> <publisher> IEEE, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: Context-switching the network fails to support a client-server model of interprocess communication. The *T [18] processor uses a memory coprocessor model as well. *T does not include DMA facilities or coherent caches and thus does not address the interaction of these features with messaging. A recent *T paper <ref> [20] </ref> has independently proposed protection mechanisms similar to FUGU's for short messages and makes the same assumptions about separating performance from correctness in scheduling.
Reference: [21] <author> Steve K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA) 1994, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <journal> IEEE. </journal> <volume> 27 </volume>
Reference-contexts: Packets can be sorted into request and reply networks because the message protocols are controlled by the kernel-level coprocessor. FUGU mostly uses one network with mixed kernel and user traffic, but relies on a cost-effective, rudimentary network to avoid deadlock. Typhoon <ref> [21] </ref> offers user-level message handling and user-level cache coherence using a second processor dedicated to the network interface. The second processor lengthens the message latency in the best-case situation, although it provides concurrency in most other situations.
Reference: [22] <author> Richard P. LaRowe, Jr. and Carla Schlatter Ellis. </author> <title> Page Placement Policies for NUMA Multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11 </volume> <pages> 112-119, </pages> <year> 1991. </year>
Reference-contexts: We expect the operating system to make use of these coarse-grain sharing techniques to complement the fine-grain sharing provided by the native coherent caches. Various groups have experimented with tradeoffs between multiple shared memory mechanisms and found page-grain sharing beneficial on shared-memory machines without caching <ref> [22, 16, 7] </ref>. Second, translation at the processor is an implementation issue that provides a simplification and a complication. The simplification is that performing all translation at the processor permits the rest of the memory system, including the hardware shared memory system, to operate in terms of physical addresses.
Reference: [23] <author> Bryan S. Rosenburg. </author> <title> Low-Synchronization Translation Lookaside Buffer Consistency in Large-Scale Shared-Memory Multiprocessors. </title> <journal> ACM Operating Systems Review, </journal> <volume> 23(5) </volume> <pages> 137-146, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: and potentially require accesses to global memory. 3 In essence, there are two levels of page faults reported: not-in-local-memory and not-in-any-memory. 3.3 Translation Coherence via PTE Coherence The problem of maintaining a coherent view of a global virtual address space across a multiprocessor is generally called the TLB consistency problem <ref> [4, 23, 24] </ref>. Translations for global pages are read from entries in the global page table (GPTEs). In FUGU, GPTEs may be stored in several places aside from the processors' TLBs, including the main caches and the LPTs. <p> PLATINUM maintains a directory for each GPTE. The standard TLB shootdown algorithm maintains one directory for an entire page table <ref> [4, 23] </ref>. Maintaining directories at a grain finer than a page table is important to scalability because it generally reduces the number of nodes that must receive invalidation messages for any given invalidation. FUGU maintains a directory for several GPTEs together on a cache line.
Reference: [24] <author> Patricia Jane Teller. </author> <title> Translation-lookaside buffer consistency in highly-parallel shard-memory multiprocessors. </title> <type> RC 16858, </type> <institution> IBM, IBM Thomas J. Watson Research Center, Distribution Services F-11 Stormytown, </institution> <address> PO Bos 218, Yorktown Heights, NY 10598, </address> <month> May </month> <year> 1991. </year> <note> RC 16858 (no. 74685) 5/14/91. </note>
Reference-contexts: and potentially require accesses to global memory. 3 In essence, there are two levels of page faults reported: not-in-local-memory and not-in-any-memory. 3.3 Translation Coherence via PTE Coherence The problem of maintaining a coherent view of a global virtual address space across a multiprocessor is generally called the TLB consistency problem <ref> [4, 23, 24] </ref>. Translations for global pages are read from entries in the global page table (GPTEs). In FUGU, GPTEs may be stored in several places aside from the processors' TLBs, including the main caches and the LPTs.
Reference: [25] <author> Chandramohan A. Thekkath, Henry M. Levy, and Edward D. Lazowska. </author> <title> Efficient Support for Multicomputing on ATM Networks. </title> <institution> UW-CSE 93-04-03, University of Washington, </institution> <address> Seattle, WA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: SHRIMP [5] and Hamlyn [27] propose remote writes for protected user communication between pre-negotiated and pre-locked pages. Recent work on parallel processing on networks of workstations seeks to identify mechanisms to accelerate communication while retaining protection. Work by Thekkath and others at the University of Washington <ref> [25] </ref> advocates remote memory access to pre-negotiated buffers as a mechanism to 25 accelerate communication while retaining protection. Similarly, Hybrid Deposit [19] proposes hardware to interpret messages as operations on pre-negotiated buffer areas.
Reference: [26] <author> Andrew Tucker. </author> <title> Efficient Scheduling on Multiprogrammed Shared-Memory Multiprocessors. </title> <address> CSL-TR 94-601, Stanford, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Note that this 6 latter stipulation is merely for performance reasons; we do not advocate enforced space sharing or hard-partitioning of the machine and gang-scheduling of partitions. Scheduling processes together appears to be desirable in general for multiprocessing <ref> [3, 26, 7] </ref>. 2.3 Protection Model User process groups should act as if they are running on a private, virtual multiprocessor. Multiple groups can be multiplexed on the hardware without interference, except in terms of performance.
Reference: [27] <author> John Wilkes. </author> <title> Hamlyn an interface for sender-based communications. </title> <type> Department technical report HPL-OSR-92-13, </type> <institution> HP Labs OS Research, </institution> <month> November </month> <year> 1992. </year> <title> 28 Instruction Description ldio CMMU-Rs, Rd Load from CMMU register. stio Rs, CMMU-Rd Store to CMMU register, including output queue. stdesc Ra, Rl, CMMU-Dd Store to CMMU output queue. launch(i) Rd Launch packet; this ldio returns the head pointer for the DMA channel. storeback(i) Skip, Rd Discard/storeback input packet; ldio returns head pointer. uei Enable user traps. udi Disable user traps. Table 3: Network instructions and operations </title>
Reference-contexts: For instance, *T demultiplexes messages into several receive queues, and the receive queues are implemented as a part of the processor register set. SHRIMP [5] and Hamlyn <ref> [27] </ref> propose remote writes for protected user communication between pre-negotiated and pre-locked pages. Recent work on parallel processing on networks of workstations seeks to identify mechanisms to accelerate communication while retaining protection.
References-found: 27

