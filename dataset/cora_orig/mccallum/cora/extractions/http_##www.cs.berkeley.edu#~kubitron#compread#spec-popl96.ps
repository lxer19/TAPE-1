URL: http://www.cs.berkeley.edu/~kubitron/compread/spec-popl96.ps
Refering-URL: http://www.cs.berkeley.edu/~kubitron/compread/index.html
Root-URL: 
Email: fjdg,blellochg@cs.cmu.edu  
Title: A Provably Time-Efficient Parallel Implementation of Full Speculation  
Author: John Greiner and Guy E. Blelloch 
Affiliation: Carnegie Mellon University  
Abstract: Speculative evaluation, including leniency and futures, is often used to produce high degrees of parallelism. Existing speculative implementations, however, may serialize computation because of their implementation of queues of suspended threads. We give a provably efficient parallel implementation of a speculative functional language on various machine models. The implementation includes proper parallelization of the necessary queuing operations on suspended threads. Our target machine models are a butterfly network, hypercube, and PRAM. To prove the efficiency of our implementation, we provide a cost model using a profiling semantics and relate the cost model to implementations on the parallel machine models. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Guy Blelloch, Phil Gibbons, and Yossi Matias. </author> <title> Provably efficient scheduling for languages with fine-grained parallelism. </title> <booktitle> In ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: In particular, we schedule at most pT fetchadd (p) threads per step. Also, if we can choose the scheduled states appropriately, we might also be able to minimize the maximum number of active states on any step, for space efficiency <ref> [1] </ref>. But it seems unlikely that call-by-speculation allows an efficient implementation of a depth-first p-traversal of the computation DAG. Consider a modification of the FSAM model, called the Partially Speculative Abstract Machine (PSAM), which incorporates these changes. Before we can discard irrelevant threads, we must first detect them.
Reference: [2] <author> Guy Blelloch and John Greiner. </author> <title> Parallelism in sequential functional languages. </title> <booktitle> In Proceedings 7th International Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 226-237, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: In Section 2 we describe how most common language features can be added with only constant overheads. In the -calculus it is safe to evaluate the two expressions in a function application e 1 e 2 in parallel. In previous work <ref> [2] </ref> we considered call-by-value parallelism in which e 1 and e 2 are evaluated in parallel to return v 1 and v 2 . The processes then synchronize at which point v 1 is applied to v 2 . <p> As well as handling the queues the implementation also has to handle the scheduling of the threads. This is somewhat more complicated than in the in the call-by-value implementation since completing a thread can reactivate an arbitrary number of suspended threads rather than just creating pairs of threads <ref> [2] </ref>. 1.3 Structure of the Implementation Our implementation and bounds are based on simulating the PSL on the target machines. We stage this simulation into two parts to simplify the mapping. We define an intermediary model using an abstract machine called the Fully Speculative Abstract Machine (FSAM). <p> As an intermediate step, we introduce the Fully Speculative Abstract Machine (FSAM), based loosely on the P-ECD machine <ref> [2] </ref>. It executes a sequence of steps in parallel over a sets of states. Each thread of computation is represented by a series of states over time. <p> The other main FSAM data structure is an environment. Using balanced binary trees, we can bound the time for each environment access and update by v e , the logarithm of the number of variables in the program e <ref> [2] </ref>. Given these, implementing each of the FSAM substeps is straightforward.
Reference: [3] <author> Guy E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: To properly balance the work for copying the queues we allocate a number of processors proportional to the size of the old queue (jq a j) to each queue. Such allocation can be implemented with the fetch-and-add operation or with segmented operations <ref> [3] </ref>. Each processor then copies their specified portion of a queue or set of queues into the new arrays. The length q l of each new queue is set to the number of elements that were copied (jq a j of the old array).
Reference: [4] <author> Richard P. Brent. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> Journal of the ACM, </journal> <volume> 21(2) </volume> <pages> 201-206, </pages> <year> 1974. </year>
Reference-contexts: Analogous results hold for the other models. Proof: The proof uses Brent's scheduling principle <ref> [4] </ref>. We assume that step i of the FSAM processes a i active states. We know from Corollary 2 that P i&lt; ^ d i=0 a i = w .
Reference: [5] <author> David Callahan and Burton Smith. </author> <title> A future-based parallel language for a general-purpose highly-paralle computer. </title> <editor> In David Galernter, Alexander Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Research Monographs in Parallel and Distributed Computing, chapter 6, </booktitle> <pages> pages 95-113. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: To avoid these problems most implementations will suspend a thread by adding it to a queue associated with the variable y (some implementations will spin for a fixed amount of time and then suspend <ref> [5] </ref>). In many cases such suspension works well, but the problem with the given code is that a large number of threads will try to suspend on a single variable almost simultaneously.
Reference: [6] <author> Rohit Chandra, Anoop Gupta, and John L Hennessy. </author> <title> COOL: a language for parallel programming. </title> <editor> In David Galernter, Alexander Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, Research Monographs in Parallel and Distributed Computing, chapter 8, </booktitle> <pages> pages 126-148. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The results assume that the number of independent variable names in a program is constant. We assume the butterfly has p log 2 p switches, and the hypercube can communicate over all wires simultaneously (multiport version). all the threads waiting on the queue <ref> [22, 17, 27, 6, 20, 10, 7] </ref>. The problem is that all implementations we know of sequen-tialize these queues, which in turn can fully sequentialize programs that appear to be highly parallel. This will happen when all threads access the same value and get suspended on a single queue. <p> In many cases such suspension works well, but the problem with the given code is that a large number of threads will try to suspend on a single variable almost simultaneously. Current implementations sequentialize this process by using a linked list for the queue <ref> [22, 17, 6, 24, 7] </ref>, and therefore would sequentialize the code. We note that a call-by-value semantics would not have this problem since the value of y would be computed before executing the pmap.
Reference: [7] <author> Marc Feeley. </author> <title> An Efficient and General Implementation of Futures on Large Scale Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Brandeis University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: The results assume that the number of independent variable names in a program is constant. We assume the butterfly has p log 2 p switches, and the hypercube can communicate over all wires simultaneously (multiport version). all the threads waiting on the queue <ref> [22, 17, 27, 6, 20, 10, 7] </ref>. The problem is that all implementations we know of sequen-tialize these queues, which in turn can fully sequentialize programs that appear to be highly parallel. This will happen when all threads access the same value and get suspended on a single queue. <p> In many cases such suspension works well, but the problem with the given code is that a large number of threads will try to suspend on a single variable almost simultaneously. Current implementations sequentialize this process by using a linked list for the queue <ref> [22, 17, 6, 24, 7] </ref>, and therefore would sequentialize the code. We note that a call-by-value semantics would not have this problem since the value of y would be computed before executing the pmap.
Reference: [8] <author> Cormac Flanagan and Mattias Felleisen. </author> <title> The semantics of future and its use in program optimization. </title> <booktitle> In Proceedings 22nd ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 209-220, </pages> <year> 1995. </year>
Reference-contexts: While lists can be encoded, a more pragmatic implementation would need to include them. Flanagan and Felleisen also gave a cost-augmented opera-tional semantics for a language with futures, defining the total work and the mandatory work of a computation <ref> [8] </ref>. However, none of these related the costs of the modelled language to those in machine models. Nikhil introduce the P-RISC [24] abstract machine for implementing Id, a speculative language.
Reference: [9] <author> Joseph Gil and Yossi Matias. </author> <title> Fast and efficient simulations among CRCW PRAMs. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 23(2) </volume> <pages> 135-148, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: In parallel all processors can atomically fetch the value from the address while incrementing the value by i. This can be implemented in a butterfly or hypercube network by combining requests as they go through the network [31], and on a PRAM by various other techniques <ref> [21, 9] </ref>. The bounds for T fetchadd (p) for these machine models are given in Figure 8. These bounds assume the butterfly has p log 2 p switches which can do the combining, and the hypercube can communicate and combine over all wires simultaneously (multiport version).
Reference: [10] <author> Ron Goldman and Richard P. Gabriel. </author> <title> Qlisp: Experience and new directions. </title> <booktitle> In Proceedings ACM SIG-PLAN Symposium on Parallel Programming: Experience with Applications, Languages and Systems, </booktitle> <pages> pages 111-123, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: The results assume that the number of independent variable names in a program is constant. We assume the butterfly has p log 2 p switches, and the hypercube can communicate over all wires simultaneously (multiport version). all the threads waiting on the queue <ref> [22, 17, 27, 6, 20, 10, 7] </ref>. The problem is that all implementations we know of sequen-tialize these queues, which in turn can fully sequentialize programs that appear to be highly parallel. This will happen when all threads access the same value and get suspended on a single queue.
Reference: [11] <author> Allan Gottlieb, B. D. Lubachevsky, and Larry Rudolph. </author> <title> Basic techniques for the efficient coordination of very large numbers of cooperating sequential processors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(2), </volume> <month> April </month> <year> 1983. </year>
Reference-contexts: When the array overflows, we move the elements to a new array of twice the number of elements in the queue. Adding to the array, growing of the array, and dequeuing from the array can all be implemented in parallel using a fetch-and-add operation <ref> [11, 31] </ref>. To account for the cost of growing the array, we amortized it against the cost of originally inserting into the queue. As well as handling the queues the implementation also has to handle the scheduling of the threads. <p> Since speculative evaluation centers on the use of queues, the operations on queues are of particular importance. Our simulation bounds are parameterized by the asymptotic time T fetchadd (p) required to implement a fetch-and add operation <ref> [11] </ref> (also called a multiprefix [31]) on p processors. In a fetch-and-add operation, each processor has an address and an integer value i. In parallel all processors can atomically fetch the value from the address while incrementing the value by i.
Reference: [12] <author> Dale H. Grit and Rex L. </author> <title> Page. Deleting irrelevant tasks in an expression-oriented multiprocessor system. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 3(1) </volume> <pages> 49-59, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: First, instead of fully evaluating all threads, any threads irrelevant to computing the final result may be discarded during evaluation <ref> [12] </ref>. Clearly, this can reduce the asymptotic work and maximum depth of some computations. Second, instead of using all the currently active states on each FSAM step, we could schedule only a subset, leaving the unscheduled ones to be active on the next step.
Reference: [13] <author> Robert H. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: The evaluation of the body is then blocked if it references an argument that is not yet available and reactivated when the argument becomes available. With futures in languages such as Mul-tilisp <ref> [13, 14, 27] </ref> and MultiScheme [22], the programmer explicitly states what should be evaluated in parallel using the future annotation. In lenient languages, such as Id [25] and pH [26], by default all subexpressions can evaluate speculatively.
Reference: [14] <author> Robert H. Halstead, Jr. </author> <title> New ideas in parallel lisp: Language design, implementation, and programming tools. </title> <editor> In T. Ito and R. H. Halstead, Jr., editors, </editor> <booktitle> Parallel Lisp: Languages and Systems, US/Japan Workshop on Parallel Lisp, number 441 in Lecture Notes in Computer Science, </booktitle> <pages> pages 2-51. </pages> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: The evaluation of the body is then blocked if it references an argument that is not yet available and reactivated when the argument becomes available. With futures in languages such as Mul-tilisp <ref> [13, 14, 27] </ref> and MultiScheme [22], the programmer explicitly states what should be evaluated in parallel using the future annotation. In lenient languages, such as Id [25] and pH [26], by default all subexpressions can evaluate speculatively.
Reference: [15] <author> Paul Hudak and Steve Anderson. </author> <title> Pomset interpretations of parallel functional programs. </title> <booktitle> In Proceedings 3rd International Conference on Functional Programming Languages and Computer Architecture, number 274 in Lecture Notes in Computer Science, </booktitle> <pages> pages 234-256. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Futures, lenient languages, and several implementations of graph reduction for lazy languages all use speculative evaluation (call-by-speculation <ref> [15] </ref>) to expose parallelism. The basic idea of speculative evaluation, in this context, is that the evaluation of a function body can start in parallel with the evaluation of the function arguments.
Reference: [16] <author> Lorenz Huelsbergen, James R. Larus, and Alexander Aiken. </author> <title> Using the run-time sizes of data structures to guide parallel-thread creation. </title> <booktitle> In Proceedings ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 79-90, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: As long as they were grouped into clusters of constant size, this would not effect asymptotic time bounds, but could greatly reduce load-balancing costs. This is the same basic idea as work examining heuristics for building large sequential blocks of code, e.g., <ref> [16, 30] </ref>. Another way to reduce communication is to cache in the environment the results of fetching values from other threads. This is simple since the value of a thread never changes once computed. There are several approaches available to improve the space efficiency of the FSAM.
Reference: [17] <author> Takayasu Ito and Manabu Matsui. </author> <title> A parallel lisp language PaiLisp and its kernal specification. </title> <editor> In T. Ito and R. H. Halstead, Jr., editors, </editor> <booktitle> Parallel Lisp: Languages and Systems, US/Japan Workshop on Parallel Lisp, number 441 in Lecture Notes in Computer Science, </booktitle> <pages> pages 58-100. </pages> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: The results assume that the number of independent variable names in a program is constant. We assume the butterfly has p log 2 p switches, and the hypercube can communicate over all wires simultaneously (multiport version). all the threads waiting on the queue <ref> [22, 17, 27, 6, 20, 10, 7] </ref>. The problem is that all implementations we know of sequen-tialize these queues, which in turn can fully sequentialize programs that appear to be highly parallel. This will happen when all threads access the same value and get suspended on a single queue. <p> In many cases such suspension works well, but the problem with the given code is that a large number of threads will try to suspend on a single variable almost simultaneously. Current implementations sequentialize this process by using a linked list for the queue <ref> [22, 17, 6, 24, 7] </ref>, and therefore would sequentialize the code. We note that a call-by-value semantics would not have this problem since the value of y would be computed before executing the pmap.
Reference: [18] <author> Mike Joy and Tom Axford. </author> <title> Parallel combinator reduction: Some performance bounds. </title> <type> Technical Report RR210, </type> <institution> University of Warwick, </institution> <year> 1992. </year>
Reference-contexts: In lenient languages, such as Id [25] and pH [26], by default all subexpressions can evaluate speculatively. With parallel implementations of lazy graph reduction <ref> [30, 18] </ref> speculative evaluation is used to overcome the inherent lack of parallelism of laziness [19, 36].
Reference: [19] <author> Richard Kennaway. </author> <title> A conflict between call-by-need computation and parallelism (extended abstract). </title> <booktitle> In Proceedings Conditional Term Rewriting Systems-94, </booktitle> <month> February </month> <year> 1994. </year>
Reference-contexts: In lenient languages, such as Id [25] and pH [26], by default all subexpressions can evaluate speculatively. With parallel implementations of lazy graph reduction [30, 18] speculative evaluation is used to overcome the inherent lack of parallelism of laziness <ref> [19, 36] </ref>. Although call-by-speculation is a powerful mechanism to achieve high degrees of parallelism, with current implementations it can be hard to understand the performance characteristics of a program without a reasonably deep understanding of the implementation. An important cause of this problem is the handling of blocked threads.
Reference: [20] <author> David A. Kranz, Jr. Robert H. Halstead, and Eric Mohr. Mul-T: </author> <title> A high-performance parallel lisp. </title> <booktitle> In Proceedings ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 81-90, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: The results assume that the number of independent variable names in a program is constant. We assume the butterfly has p log 2 p switches, and the hypercube can communicate over all wires simultaneously (multiport version). all the threads waiting on the queue <ref> [22, 17, 27, 6, 20, 10, 7] </ref>. The problem is that all implementations we know of sequen-tialize these queues, which in turn can fully sequentialize programs that appear to be highly parallel. This will happen when all threads access the same value and get suspended on a single queue.
Reference: [21] <author> Yossi Matias and Uzi Vishkin. </author> <title> On parallel hashing and integer sorting. </title> <journal> Journal of Algorithms, </journal> <volume> 12(4) </volume> <pages> 573-606, </pages> <year> 1991. </year>
Reference-contexts: In parallel all processors can atomically fetch the value from the address while incrementing the value by i. This can be implemented in a butterfly or hypercube network by combining requests as they go through the network [31], and on a PRAM by various other techniques <ref> [21, 9] </ref>. The bounds for T fetchadd (p) for these machine models are given in Figure 8. These bounds assume the butterfly has p log 2 p switches which can do the combining, and the hypercube can communicate and combine over all wires simultaneously (multiport version).
Reference: [22] <author> James S. Miller. MultiScheme: </author> <title> A Parallel Processing System Based on MIT Scheme. </title> <type> PhD thesis, </type> <institution> Mas-sachusetts Institute of Technology, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: The evaluation of the body is then blocked if it references an argument that is not yet available and reactivated when the argument becomes available. With futures in languages such as Mul-tilisp [13, 14, 27] and MultiScheme <ref> [22] </ref>, the programmer explicitly states what should be evaluated in parallel using the future annotation. In lenient languages, such as Id [25] and pH [26], by default all subexpressions can evaluate speculatively. <p> The results assume that the number of independent variable names in a program is constant. We assume the butterfly has p log 2 p switches, and the hypercube can communicate over all wires simultaneously (multiport version). all the threads waiting on the queue <ref> [22, 17, 27, 6, 20, 10, 7] </ref>. The problem is that all implementations we know of sequen-tialize these queues, which in turn can fully sequentialize programs that appear to be highly parallel. This will happen when all threads access the same value and get suspended on a single queue. <p> In many cases such suspension works well, but the problem with the given code is that a large number of threads will try to suspend on a single variable almost simultaneously. Current implementations sequentialize this process by using a linked list for the queue <ref> [22, 17, 6, 24, 7] </ref>, and therefore would sequentialize the code. We note that a call-by-value semantics would not have this problem since the value of y would be computed before executing the pmap.
Reference: [23] <author> James S. Miller and Barbara S. Epstein. </author> <note> Garbage col-lection in MultiScheme (preliminary version). </note> <editor> In T. Ito and Jr. R. H. Halstead, editors, </editor> <booktitle> Parallel Lisp: Languages and Systems, US/Japan Workshop on Parallel Lisp, number 441 in Lecture Notes in Computer Science, </booktitle> <pages> pages 138-160. </pages> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: What we have just described is a form of garbage collection designed specifically not to increase our work bound. Some languages simply incorporate this into their usual garbage collection mechanism <ref> [27, 23] </ref>. For the purpose of minimizing the work spent evaluating irrelevant threads, scheduling a subset of the active states is only beneficial if we can effectively prioritize states and threads.
Reference: [24] <author> Rishiyur S. Nikhil. </author> <title> The parallel programming language Id and its compilation for parallel machines. </title> <type> Technical Report Computation Structures Group Memo 313, </type> <institution> Massachusetts Institute of Technology, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: In many cases such suspension works well, but the problem with the given code is that a large number of threads will try to suspend on a single variable almost simultaneously. Current implementations sequentialize this process by using a linked list for the queue <ref> [22, 17, 6, 24, 7] </ref>, and therefore would sequentialize the code. We note that a call-by-value semantics would not have this problem since the value of y would be computed before executing the pmap. <p> Flanagan and Felleisen also gave a cost-augmented opera-tional semantics for a language with futures, defining the total work and the mandatory work of a computation [8]. However, none of these related the costs of the modelled language to those in machine models. Nikhil introduce the P-RISC <ref> [24] </ref> abstract machine for implementing Id, a speculative language. The machine, however, is not meant as a formal model and does not fully define the interprocess communication and the selection of tasks to evaluate.
Reference: [25] <author> Rishiyur S. Nikhil. </author> <note> Id version 90.1 reference manual. Technical Report Computation Structures Group Memo 284-1, </note> <institution> Laboratory for Computer Science, Mas-sachusetts Institute of Technology, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: With futures in languages such as Mul-tilisp [13, 14, 27] and MultiScheme [22], the programmer explicitly states what should be evaluated in parallel using the future annotation. In lenient languages, such as Id <ref> [25] </ref> and pH [26], by default all subexpressions can evaluate speculatively. With parallel implementations of lazy graph reduction [30, 18] speculative evaluation is used to overcome the inherent lack of parallelism of laziness [19, 36].
Reference: [26] <author> Rishiyur S. Nikhil, Arvind, James Hicks, Shail Aditya, Lennar Augustsson, Jan-Willem Maessen, and Yuli Zhou. </author> <title> pH language reference manual, </title> <note> version 1.0| preliminary. Technical Report Computation Structures Group Memo 369, </note> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: With futures in languages such as Mul-tilisp [13, 14, 27] and MultiScheme [22], the programmer explicitly states what should be evaluated in parallel using the future annotation. In lenient languages, such as Id [25] and pH <ref> [26] </ref>, by default all subexpressions can evaluate speculatively. With parallel implementations of lazy graph reduction [30, 18] speculative evaluation is used to overcome the inherent lack of parallelism of laziness [19, 36].
Reference: [27] <author> Randy B. Osborne. </author> <title> Speculative Computation in Multil-isp. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: The evaluation of the body is then blocked if it references an argument that is not yet available and reactivated when the argument becomes available. With futures in languages such as Mul-tilisp <ref> [13, 14, 27] </ref> and MultiScheme [22], the programmer explicitly states what should be evaluated in parallel using the future annotation. In lenient languages, such as Id [25] and pH [26], by default all subexpressions can evaluate speculatively. <p> The results assume that the number of independent variable names in a program is constant. We assume the butterfly has p log 2 p switches, and the hypercube can communicate over all wires simultaneously (multiport version). all the threads waiting on the queue <ref> [22, 17, 27, 6, 20, 10, 7] </ref>. The problem is that all implementations we know of sequen-tialize these queues, which in turn can fully sequentialize programs that appear to be highly parallel. This will happen when all threads access the same value and get suspended on a single queue. <p> Only the appropriate branch, i.e., the body of the corresponding abstraction, is evaluated once the test has been evaluated. An encoding which does not wrap e 2 and e 3 in abstractions would lead to speculative evaluation of both branches, an option offered in some languages <ref> [27] </ref>. Similarly, the standard definition of local binding: let x = e 1 in e 2 (x .e 2 ) e 1 produces speculative evaluation of both expressions as in some languages [33]. <p> What we have just described is a form of garbage collection designed specifically not to increase our work bound. Some languages simply incorporate this into their usual garbage collection mechanism <ref> [27, 23] </ref>. For the purpose of minimizing the work spent evaluating irrelevant threads, scheduling a subset of the active states is only beneficial if we can effectively prioritize states and threads. <p> Note that at most one of those threads is active at a time. One approach is to prioritize the argument of an application lower than its function, since the argument may not be relevant even if the function is. While used in practice <ref> [27, 29, 28, 37] </ref>, it is unclear whether this approach can reduce work asymptotically without increasing depth asymptotically. The problem is updating thread priorities efficiently. During evaluation, as we detect which threads are relevant, we adjust priorities.
Reference: [28] <author> Andrew S. Partridge. </author> <title> Speculative Evaluation in Parallel Implementations of Lazy Functional Languages. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Tasmania, </institution> <year> 1991. </year>
Reference-contexts: Note that at most one of those threads is active at a time. One approach is to prioritize the argument of an application lower than its function, since the argument may not be relevant even if the function is. While used in practice <ref> [27, 29, 28, 37] </ref>, it is unclear whether this approach can reduce work asymptotically without increasing depth asymptotically. The problem is updating thread priorities efficiently. During evaluation, as we detect which threads are relevant, we adjust priorities.
Reference: [29] <author> Andrew S. Partridge and Anthony H. Dekker. </author> <title> Speculative parallelism in a distributed graph reduction machine. </title> <booktitle> In Proceedings Hawaii International Conference on System Sciences, </booktitle> <volume> volume 2, </volume> <pages> pages 771-779, </pages> <year> 1989. </year>
Reference-contexts: Note that at most one of those threads is active at a time. One approach is to prioritize the argument of an application lower than its function, since the argument may not be relevant even if the function is. While used in practice <ref> [27, 29, 28, 37] </ref>, it is unclear whether this approach can reduce work asymptotically without increasing depth asymptotically. The problem is updating thread priorities efficiently. During evaluation, as we detect which threads are relevant, we adjust priorities.
Reference: [30] <author> Simon L Peyton Jones. </author> <title> Parallel implementations of functional programming languages. </title> <journal> The Computer Journal, </journal> <volume> 32(2) </volume> <pages> 175-186, </pages> <year> 1989. </year>
Reference-contexts: In lenient languages, such as Id [25] and pH [26], by default all subexpressions can evaluate speculatively. With parallel implementations of lazy graph reduction <ref> [30, 18] </ref> speculative evaluation is used to overcome the inherent lack of parallelism of laziness [19, 36]. <p> Our implementation aggressively creates many threads to maximize parallelism, and it frequently synchronizes all threads to guarantee load-balancing. Since most expressions are relatively simple, and the cost of thread management is high, creating a thread for each subexpression involves too much overhead <ref> [30] </ref>. Furthermore, the substeps F body little computation between each load-balancing. One way to improve this would be to group sets of these substeps between each load-balancing. <p> As long as they were grouped into clusters of constant size, this would not effect asymptotic time bounds, but could greatly reduce load-balancing costs. This is the same basic idea as work examining heuristics for building large sequential blocks of code, e.g., <ref> [16, 30] </ref>. Another way to reduce communication is to cache in the environment the results of fetching values from other threads. This is simple since the value of a thread never changes once computed. There are several approaches available to improve the space efficiency of the FSAM.
Reference: [31] <author> Abhiram G. Ranade. </author> <title> Fluent Parallel Computation. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <year> 1989. </year>
Reference-contexts: When the array overflows, we move the elements to a new array of twice the number of elements in the queue. Adding to the array, growing of the array, and dequeuing from the array can all be implemented in parallel using a fetch-and-add operation <ref> [11, 31] </ref>. To account for the cost of growing the array, we amortized it against the cost of originally inserting into the queue. As well as handling the queues the implementation also has to handle the scheduling of the threads. <p> Since speculative evaluation centers on the use of queues, the operations on queues are of particular importance. Our simulation bounds are parameterized by the asymptotic time T fetchadd (p) required to implement a fetch-and add operation [11] (also called a multiprefix <ref> [31] </ref>) on p processors. In a fetch-and-add operation, each processor has an address and an integer value i. In parallel all processors can atomically fetch the value from the address while incrementing the value by i. <p> In parallel all processors can atomically fetch the value from the address while incrementing the value by i. This can be implemented in a butterfly or hypercube network by combining requests as they go through the network <ref> [31] </ref>, and on a PRAM by various other techniques [21, 9]. The bounds for T fetchadd (p) for these machine models are given in Figure 8.
Reference: [32] <author> Paul Roe. </author> <title> Calculating lenient programs' performance. </title> <editor> In Simon L Peyton Jones, Graham Hutton, and Carsten Kehler Holst, editors, </editor> <booktitle> Proceedings Functional Programming, Glasgow 1990, Workshops in computing, </booktitle> <pages> pages 227-236. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: i =p) + ^ dT fetchadd (p)) = 2kv e (w =p + ^ dT fetchadd (p)) 2 5 Related Work Several researchers have used cost-augmented semantics for automatic time analysis or definitional purposes, e.g., [34, 35, 38, 39], the most similar being that of Roe for a lenient language <ref> [32, 33] </ref>. The following are the primary differences of Roe's model as compared to ours: * Looking up a variable does not wait for the value as in VAR.
Reference: [33] <author> Paul Roe. </author> <title> Parallel Programming using Functional Languages. </title> <type> PhD thesis, </type> <institution> Department of Computing Science, University of Glasgow, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: To make the cost of the source language explicit we use a profiling semantics similar to that of Roe <ref> [33] </ref>. This semantics specifies the cost of a computation in terms of the total work it performs and its parallel depth (the length of the critical path of sequential dependences). We call this cost-augmented language the parallel speculative -calculus (PSL). <p> We augment a standard call-by-value semantics, written E ` e =) v , that defines the evaluation of an expression in an environment to a value. We add intensional information which describes the call-by-speculation of our model, defining when the result value of each subexpression is available <ref> [33] </ref>. To define the minimum depth of a variable, we must tag values in the environment with the depths at which they become available. <p> Similarly, the standard definition of local binding: let x = e 1 in e 2 (x .e 2 ) e 1 produces speculative evaluation of both expressions as in some languages <ref> [33] </ref>. A serialized equivalent can be encoded using a continuation passing style transform, CPS [e 1 ] x .e 2 . <p> i =p) + ^ dT fetchadd (p)) = 2kv e (w =p + ^ dT fetchadd (p)) 2 5 Related Work Several researchers have used cost-augmented semantics for automatic time analysis or definitional purposes, e.g., [34, 35, 38, 39], the most similar being that of Roe for a lenient language <ref> [32, 33] </ref>. The following are the primary differences of Roe's model as compared to ours: * Looking up a variable does not wait for the value as in VAR.
Reference: [34] <author> David Sands. </author> <title> Calculi for Time Analysis of Functional Programs. </title> <type> PhD thesis, </type> <institution> University of London, Imperial College, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: i + a i+1 )=p + T fetchadd (p) = 2kv e (( i=0 a i =p) + ^ dT fetchadd (p)) = 2kv e (w =p + ^ dT fetchadd (p)) 2 5 Related Work Several researchers have used cost-augmented semantics for automatic time analysis or definitional purposes, e.g., <ref> [34, 35, 38, 39] </ref>, the most similar being that of Roe for a lenient language [32, 33]. The following are the primary differences of Roe's model as compared to ours: * Looking up a variable does not wait for the value as in VAR.
Reference: [35] <author> David B. Skillicorn and W. Cai. </author> <title> A cost calculus for parallel functional programming. </title> <note> To appear in Journal of Parallel and Distributed Computing. </note>
Reference-contexts: i + a i+1 )=p + T fetchadd (p) = 2kv e (( i=0 a i =p) + ^ dT fetchadd (p)) = 2kv e (w =p + ^ dT fetchadd (p)) 2 5 Related Work Several researchers have used cost-augmented semantics for automatic time analysis or definitional purposes, e.g., <ref> [34, 35, 38, 39] </ref>, the most similar being that of Roe for a lenient language [32, 33]. The following are the primary differences of Roe's model as compared to ours: * Looking up a variable does not wait for the value as in VAR.
Reference: [36] <author> Guy Tremblay and G. R. Gao. </author> <title> The impact of laziness on parallelism and the limits of strictness analysis. </title> <editor> In A. P. Wim Bohm and John T. Feo, editors, </editor> <booktitle> Proceedings High Performance Functional Computing, </booktitle> <pages> pages 119-133, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: In lenient languages, such as Id [25] and pH [26], by default all subexpressions can evaluate speculatively. With parallel implementations of lazy graph reduction [30, 18] speculative evaluation is used to overcome the inherent lack of parallelism of laziness <ref> [19, 36] </ref>. Although call-by-speculation is a powerful mechanism to achieve high degrees of parallelism, with current implementations it can be hard to understand the performance characteristics of a program without a reasonably deep understanding of the implementation. An important cause of this problem is the handling of blocked threads.
Reference: [37] <author> C K Yuen, M D Feng, and J J Yee. </author> <title> Speculative parallelism in BaLinda Lisp. </title> <type> Technical Report TR31/92, </type> <institution> Department of Information Systems and Computer Science, National University of Singapore, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Note that at most one of those threads is active at a time. One approach is to prioritize the argument of an application lower than its function, since the argument may not be relevant even if the function is. While used in practice <ref> [27, 29, 28, 37] </ref>, it is unclear whether this approach can reduce work asymptotically without increasing depth asymptotically. The problem is updating thread priorities efficiently. During evaluation, as we detect which threads are relevant, we adjust priorities.
Reference: [38] <author> Wolf Zimmermann. </author> <title> Automatic worst case complexity analysis of parallel programs. </title> <type> Technical Report TR-90-066, </type> <institution> International Computer Science Institute, </institution> <month> Decem-ber </month> <year> 1990. </year>
Reference-contexts: i + a i+1 )=p + T fetchadd (p) = 2kv e (( i=0 a i =p) + ^ dT fetchadd (p)) = 2kv e (w =p + ^ dT fetchadd (p)) 2 5 Related Work Several researchers have used cost-augmented semantics for automatic time analysis or definitional purposes, e.g., <ref> [34, 35, 38, 39] </ref>, the most similar being that of Roe for a lenient language [32, 33]. The following are the primary differences of Roe's model as compared to ours: * Looking up a variable does not wait for the value as in VAR.
Reference: [39] <author> Wolf Zimmermann. </author> <title> Complexity issues in the design of functional languages with explicit parallelism. </title> <booktitle> In Proceedings International Conference on Computer Languages, </booktitle> <pages> pages 34-43, </pages> <year> 1992. </year>
Reference-contexts: i + a i+1 )=p + T fetchadd (p) = 2kv e (( i=0 a i =p) + ^ dT fetchadd (p)) = 2kv e (w =p + ^ dT fetchadd (p)) 2 5 Related Work Several researchers have used cost-augmented semantics for automatic time analysis or definitional purposes, e.g., <ref> [34, 35, 38, 39] </ref>, the most similar being that of Roe for a lenient language [32, 33]. The following are the primary differences of Roe's model as compared to ours: * Looking up a variable does not wait for the value as in VAR.
References-found: 39

