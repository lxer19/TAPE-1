URL: ftp://ftp.cs.washington.edu/tr/1998/11/UW-CSE-98-11-01.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-date.html
Root-URL: http://www.cs.washington.edu
Title: A Region-based Approach for Sparse Parallel Computing  
Author: Bradford L. Chamberlain E Christopher Lewis Lawrence Snyder 
Address: Seattle, WA 98195-2350  
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: This paper introduces a technique for parallel sparse computation by extending the array-language concept of regionsregular programmer-specified index sets used for specifying array computations. We introduce the notion of sparse regions which can represent an arbitrary set of indices. Sparse regions inherit the benefits of regular regions, including conciseness, a direct encapsulation of parallelism, and support for language performance models that highlight parallel overheads. We show that region-based array languages can benefit from the use of sparse regions, both in terms of the semantic richness available to the programmer and the execution times of the resulting program. We also demonstrate that regions result in efficient implementations as compared to array-based approachs, due to their role in amortizing sparse overheads and enabling optimizations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Guy E. Blelloch, Siddhartha Chatterjee, Jonathan C. Hardwick, Jay Sipelstein, and Marco Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1):414, </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: One such example is NESL <ref> [1] </ref>, a functional language that allows the construction of nested parallel data structures. These structures can be used to explicitly construct standard sparse array representations such as compressed column storage.
Reference: [2] <author> Bradford L. Chamberlain, Sung-Eun Choi, E Christopher Lewis, Calvin Lin, Lawrence Snyder, and W. Derrick Weathersby. </author> <title> ZPL's WYSIWYG performance model. </title> <booktitle> In Proceedings of the Third International Workshop on High-Level Parallel Programming Models and Supportive Environments, </booktitle> <pages> pages 5061. </pages> <publisher> IEEE, </publisher> <month> March </month> <year> 1998. </year>
Reference-contexts: Its features have proven popular with applications programmers, and ZPL is supported for most modern parallel architectures. ZPL has an associated performance model that allows programmers to reason about the parallel implementation of their code without forcing them to program at a per-processor level <ref> [2] </ref>. The ZPL compiler produces efficient code that is competitive with hand-coded C [3] and which tends to outperform High Performance Fortran (HPF) [11]. Given ZPL's successes in the domain of regular array computation, it seems only logical to consider extending it to support sparse computations. <p> This is the basis for ZPL's WYSIWYG performance model, which allows the parallel overheads associated with every statement to be easily detected and reasoned about by the programmer <ref> [2] </ref>. Our goal in this work is to extend ZPL's regular regions to support sparse index sets without sacrificing the syntactic, semantic, and performance benefits that were achieved in the non-sparse case. ZPL currently supports a notion of operating on arbitrary indices using masking. <p> We recognize that sparse computations are often more sensitive to load balancing than dense computations, and therefore expect to extend A-ZPL's distribution capabilities to handle more complex partitions. In doing this, we intend to preserve ZPL's region distribution invariant <ref> [2] </ref> so that the performance model 10 will extend transparently to the sparse domain.
Reference: [3] <author> Bradford L. Chamberlain, Sung-Eun Choi, E Christopher Lewis, Lawrence Snyder, W. Derrick Weathersby, and Calvin Lin. </author> <title> The case for high-level parallel programming in ZPL. </title> <journal> IEEE Computational Science and Engineering, </journal> <volume> 5(3):7685, </volume> <month> JulySeptember </month> <year> 1998. </year>
Reference-contexts: ZPL has an associated performance model that allows programmers to reason about the parallel implementation of their code without forcing them to program at a per-processor level [2]. The ZPL compiler produces efficient code that is competitive with hand-coded C <ref> [3] </ref> and which tends to outperform High Performance Fortran (HPF) [11]. Given ZPL's successes in the domain of regular array computation, it seems only logical to consider extending it to support sparse computations. We do this in the context of Advanced ZPL (A-ZPL), the successor language to ZPL.
Reference: [4] <author> Bradford L. Chamberlain, E Christopher Lewis, Calvin Lin, and Lawrence Snyder. </author> <title> Regions: An abstraction for expressing array computation. </title> <type> Technical Report UW-CSE-98-10-02, </type> <institution> University of Washington, </institution> <month> October </month> <year> 1998. </year>
Reference-contexts: (1; 1); (1; 2); : : : ; (n; n)g: region R = [1..n,1..n]; declare an n x n index set ZPL's support for regions includes a set of operators useful for describing boundary conditions, strided regions for use in hierarchical multigrid problems, and other common transformations on index sets <ref> [4] </ref>. 4 Regions serve two purposes in ZPL. The first is to declare parallel arrays. <p> The second is to emphasize index locality so that the relationship between the elements accessed by each array operand is clear and obvious to the programmer <ref> [4] </ref>. This is the basis for ZPL's WYSIWYG performance model, which allows the parallel overheads associated with every statement to be easily detected and reasoned about by the programmer [2]. <p> The base index set of each sparse region will be distributed across the processor set as with dense regions <ref> [4] </ref>. Each processor will then allocate the sparse structure described above to store its subset of the global index space. We recognize that sparse computations are often more sensitive to load balancing than dense computations, and therefore expect to extend A-ZPL's distribution capabilities to handle more complex partitions.
Reference: [5] <author> Sung-Eun Choi and Lawrence Snyder. </author> <title> Quantifying the effect of communication optimizations. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 218222, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: In addition to performing our traditional optimizations in a sparse context (most notably array contraction [9] and communication optimizations <ref> [5] </ref>), we expect that there will be many new opportunities for optimization exposed by language support for sparsity.
Reference: [6] <author> John R. Gilbert, Cleve Moler, and Robert Schreiber. </author> <title> Sparse matrices in MATLAB: Design and implementation. </title> <type> SIMAX, </type> <institution> 13(1):333356, </institution> <month> January </month> <year> 1992. </year>
Reference: [7] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Specification Version 2.0, </title> <month> January </month> <year> 1997. </year>
Reference-contexts: In contrast, our approach allows for the clear representation of a sparse computation at a global level, leaving details of representation and optimizations to the compiler. High-Performance Fortran <ref> [7] </ref> is perhaps the most prominent parallel language. Although it has no inherent support for sparse arrays, an extension to HPF has been proposed by Ujaldon et al., which allows for the declaration of sparse arrays using a variety of standard storage schemes [13].
Reference: [8] <author> S. A. Hutchinson, J. N. Shadid, and R. S. Tuminaro. </author> <note> Aztec User's Guide: Version 2.0. </note> <institution> Sandia National Laboratories, </institution> <month> September </month> <year> 1998. </year>
Reference-contexts: This is an unfortunate burden to place on users, obfuscating a code's meaning. In contrast, the sparse and dense versions of A-ZPL algorithms are quite similar in appearance. Although parallel libraries have been developed for representing and operating on sparse arrays (e.g., <ref> [8] </ref>), these have typically assumed a sparse matrix interpretation, providing support for linear algebra operations. We believe that although such libraries are valuable, support for sparse computation at the language level aids in the clear expression of the programmer's computation.
Reference: [9] <author> E Christopher Lewis, Calvin Lin, and Lawrence Snyder. </author> <title> The implementation and evaluation of fusion and contraction in array languages. </title> <booktitle> In SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 5059, </pages> <month> June </month> <year> 1998. </year>
Reference-contexts: In addition to performing our traditional optimizations in a sparse context (most notably array contraction <ref> [9] </ref> and communication optimizations [5]), we expect that there will be many new opportunities for optimization exposed by language support for sparsity.
Reference: [10] <author> Mathworks. </author> <title> MATLAB User's Guide, </title> <year> 1993. </year>
Reference: [11] <author> Ton A. Ngo, Lawrence Snyder, and Bradford L. Chamberlain. </author> <title> Portable performance of data parallel languages. </title> <booktitle> In SC97: High Performance Networking and Computing, </booktitle> <month> November </month> <year> 1997. </year>
Reference-contexts: The ZPL compiler produces efficient code that is competitive with hand-coded C [3] and which tends to outperform High Performance Fortran (HPF) <ref> [11] </ref>. Given ZPL's successes in the domain of regular array computation, it seems only logical to consider extending it to support sparse computations. We do this in the context of Advanced ZPL (A-ZPL), the successor language to ZPL. This section gives a brief introduction to some of ZPL's fundamental concepts.
Reference: [12] <author> Lawrence Snyder. </author> <title> Programming Guide to ZPL. </title> <publisher> MIT Press (in press), </publisher> <year> 1998. </year>
Reference-contexts: The use of sparse regions makes this trivial and gives programmers full control over the types of sparsity they require. 3 Introduction to ZPL ZPL is an array-based data-parallel language that contains support for dense arrays and regularly-strided sparse arrays <ref> [12] </ref>. Its features have proven popular with applications programmers, and ZPL is supported for most modern parallel architectures. ZPL has an associated performance model that allows programmers to reason about the parallel implementation of their code without forcing them to program at a per-processor level [2]. <p> We do this in the context of Advanced ZPL (A-ZPL), the successor language to ZPL. This section gives a brief introduction to some of ZPL's fundamental concepts. A more thorough presentation is available elsewhere <ref> [12] </ref>. In addition to standard support for constants and variables, ZPL provides configuration variables for defining runtime constantsvalues that are set at the outset of a program's execution and then remain constant for the duration of the program.
Reference: [13] <author> M. Ujaldon, E. L. Zapata, B. M. Chapman, and H. Zima. </author> <title> Vienna fortran/HPF extensions for sparse and irregular problems and their compilation. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 8(10):10681083, </volume> <month> October </month> <year> 1997. </year>
Reference-contexts: High-Performance Fortran [7] is perhaps the most prominent parallel language. Although it has no inherent support for sparse arrays, an extension to HPF has been proposed by Ujaldon et al., which allows for the declaration of sparse arrays using a variety of standard storage schemes <ref> [13] </ref>. Although this exposes the sparse array representation to the compiler, computations over the arrays still require users to directly refer to the underlying sparse data structure. This is an unfortunate burden to place on users, obfuscating a code's meaning.
Reference: [14] <author> Robert van de Geijn and Jerrell Watts. SUMMA: </author> <title> Scalable universal matrix multiplication algorithm. </title> <type> Technical Report TR-95-13, </type> <institution> University of Texas, Austin, Texas, </institution> <month> April </month> <year> 1995. </year> <month> 14 </month>
References-found: 14

