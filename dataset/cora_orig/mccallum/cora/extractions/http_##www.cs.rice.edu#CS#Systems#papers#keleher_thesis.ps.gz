URL: http://www.cs.rice.edu/CS/Systems/papers/keleher_thesis.ps.gz
Refering-URL: http://www.cs.rice.edu/~willy/TreadMarks/theses.html
Root-URL: 
Title: Lazy Release Consistency for Distributed Shared Memory  
Author: by Peter Keleher 
Degree: A Thesis Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Approved, Thesis Committee: Willy Zwaenepoel, Professor, Chairman Computer Science Alan Cox, Assistant Professor Computer Science John Bennett, Associate Professor  
Date: January, 1995  
Address: Houston, Texas  
Affiliation: RICE UNIVERSITY  Electrical and Computer Engineering  
Abstract-found: 0
Intro-found: 1
Reference: [ACDZ94] <author> S.V. Adve, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. </author> <title> Replacing locks by higher-level primitives. </title> <type> Technical Report TR94-237, </type> <institution> Rice University, </institution> <year> 1994. </year>
Reference-contexts: However, work with the hybrid protocol, as well as work by other researchers <ref> [KFJ94, ACDZ94] </ref>, has shown that the protocols can greatly benefit from being more closely integrated with the application. We expect this direction of research to include two key components: 1.
Reference: [Adv93] <author> S.V. Adve. </author> <title> Designing Memory Consistency Models for Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, </institution> <month> Decem-ber </month> <year> 1993. </year>
Reference-contexts: However, many studies have shown that sequential consistency poses serious problems for efficient distributed implementations of shared memory, primarily because sequential consistency imposes such strict requirements on system-wide ordering of accesses to shared memory <ref> [Adv93] </ref>. <p> The proof for LH is identical. This proof is based on a result by Adve <ref> [Adv93] </ref> that any system meeting a specific set of sufficient conditions guarantees to produce only sequentially consistency executions. The rest of this section re-caps some necessary definitions and then shows by inspection that LI meets the required sufficient conditions, and therefore guarantees SC executions. <p> Everything else is a data operation. Intuitively, a program is correct if enough operations are distinguished as releases or acquires. In this context, "enough" means that the resulting program has no data races, or is data-race-free (Definition 2.5). 33 Definition 2.6 and Condition 2.4.1 are from Adve's thesis <ref> [Adv93] </ref>. Definition 2.6 A system obeys the data-race-free-1 memory model if and only if the result of every execution of a data-race-free program on the hardware can be obtained by an execution of the program on sequentially consistent hardware. <p> To avoid confusion, hb1 ! is used in the rest of the text. See <ref> [Adv93] </ref> for the original definitions. 34 modifications are totally ordered by hb1 !, and diffs are always requested and applied in an order consistent with hb1 The "synchronization requirement" specifies that synchronization operations are sequentially consistent with one another.
Reference: [AH93] <author> S. V. Adve and M. D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(6) </volume> <pages> 613-624, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: P 2 to P 3 not only carries an invalidation for y, which was modified by P 2 , but also an invalidation for x, which was previously modified by P 1 . 2.2.4 Happened-Before-1 In order to support the memory model described in Definition 2.3, we use a happened-before-1 <ref> [AH93] </ref> partial ordering over all shared accesses: Definition 2.4 Shared memory accesses are partially ordered by happened before-1 , denoted hb1 !, defined as follows: * If a 1 and a 2 are accesses on the same process, and a 1 occurs before a 2 in program order, then a 1 <p> LRC protocols guarantee to support the same programming model as sequentially consistent protocols if programs are data-race-free. The following definitions are from Adve <ref> [AH93] </ref>: Definition 2.5 A data race in an execution is a pair of conflicting operations, at least one of which is to data, that is not ordered by the happened-before-1 relation defined for the execution. An execution is data-race-free if and only if it does not have any data races.
Reference: [ALKK90] <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Alewife <ref> [ALKK90, CA94] </ref> is a sequentially consistent system that uses directory-based cache coherence. Alewife uses a LimitLESS [CKA91] directory structure that supports a small number of directory pointers directly in hardware, and traps to a software handler when the pointers are exhausted.
Reference: [BBLS91] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report TR RNR-91-002, </type> <institution> NASA Ames, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: The programs come from several different sources. Barnes and Water come from the Stanford Parallel Applications for Shared Memory (SPLASH) benchmark suite [SWG91]. FFT and IS come from the NAS benchmark suite <ref> [BBLS91] </ref>. SOR and TSP were developed at Rice University. ILINK was derived from a sequential program used by geneticists worldwide [LLJO84, CIS93, DSC + 94]. MIP was developed by researchers from the Department of Computational and Applied Mathematics at Rice University.
Reference: [BCCR91] <author> R. Bryant, P. Carini, H.-Y. Chang, and B. Rosenburg. </author> <title> Supporting structured shared virtual memory under Mach. </title> <booktitle> In Proceedings of the 2nd Mach Usenix Symposium, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: Data must be written before it can be read and can be used to support efficient synchronization. TreadMarks handles synchronization separately from the memory system. Bryant et al. <ref> [BCCR91] </ref> implemented Structured Shared Virtual Memory (SSVM) on a star network of IBM RS-6000s running Mach 2.5. Two different implementation strategies were followed: one using the Mach external pager interface [YTR + 87], and one using the Mach exception interface [BGR + 88].
Reference: [BFS89] <author> W.J. Bolosky, R.P. Fitzgerald, </author> <title> and M.L. Scott. Simple but effective techniques for NUMA memory management. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 19-31, </pages> <month> Decem-ber </month> <year> 1989. </year>
Reference-contexts: If the relative orderings of sub-operations of competing memory operations do not agree, then SC is violated. This problem is especially severe in systems in which sub-operations take differing amounts of time to complete, such as ring architectures that support concurrent access [WHL92], or NUMA machines <ref> [BSF + 91, SJG92, BFS89, Cox92, CF89, LE91, LEK91] </ref> . 2.2.2 Release Consistency Release consistency (RC) [GLL + 90], hereafter referred to as eager release consistency, is a relaxed memory consistency model that permits a process to delay making its changes to shared data visible to other processes until certain synchronization
Reference: [BGR + 88] <author> D. Black, D. Golub, R. Rashid, A. Tevanian, and M. Young. </author> <title> The Mach exception handling facility. </title> <journal> SigPlan Notices, </journal> <volume> 24(1) </volume> <pages> 45-56, </pages> <month> May </month> <year> 1988. </year> <month> 105 </month>
Reference-contexts: Bryant et al. [BCCR91] implemented Structured Shared Virtual Memory (SSVM) on a star network of IBM RS-6000s running Mach 2.5. Two different implementation strategies were followed: one using the Mach external pager interface [YTR + 87], and one using the Mach exception interface <ref> [BGR + 88] </ref>. They report that the latter implementation|which is very similar to ours|is more efficient, because of the inability of Mach's external pager interface to asynchronously update a page in the user's address space. Emerald, Amber, and Orca are distributed object systems that share many similarities with DSM systems.
Reference: [BHJ + 87] <author> A. Black, N. Hutchinson, E. Jul, H. Levy, and L. Carter. </author> <title> Distribution and abstract types in Emerald. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1):65-74, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: Emerald, Amber, and Orca are distributed object systems that share many similarities with DSM systems. Distributed object systems distribute computations among threads executing in objects on different machines. Processes interact by invoking methods of shared objects. Emerald <ref> [BHJL86, BHJ + 87] </ref> implements local communication among threads and objects through shared memory, and remote communication through RPC. The system marshals RPC arguments into the message, accommodating pointer values by copying data pointed to by any pointer parameters.
Reference: [BHJL86] <author> A. Black, N. Hutchinson, E. Jul, and H. Levy. </author> <title> Object structure in the Emerald system. </title> <booktitle> In Proceedings of the ACM Conference on Object-Oriented Programming Systems, Languages and Applications, </booktitle> <pages> pages 78-86, </pages> <month> October </month> <year> 1986. </year> <journal> Special Issue of SIGPLAN Notices, </journal> <volume> Volume 21, Number 11, </volume> <month> November, </month> <year> 1986. </year>
Reference-contexts: Emerald, Amber, and Orca are distributed object systems that share many similarities with DSM systems. Distributed object systems distribute computations among threads executing in objects on different machines. Processes interact by invoking methods of shared objects. Emerald <ref> [BHJL86, BHJ + 87] </ref> implements local communication among threads and objects through shared memory, and remote communication through RPC. The system marshals RPC arguments into the message, accommodating pointer values by copying data pointed to by any pointer parameters.
Reference: [BKT92] <author> H.E. Bal, M.F. Kaashoek, </author> <title> and A.S. Tanenbaum. Orca: A language for parallel programming of distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 190-205, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Subsidiary objects could also be "attached" to other objects. All attached objects moved between nodes when any one of the objects moved. Orca <ref> [BKT92, Kaa92] </ref> is an object-based system that supports transparent replication by a fast, hardware-supported, multicast mechanism. Replication is not automatic, but initiated by heuristics.
Reference: [BL92] <author> T. Ball and J. Larus. </author> <title> Optimally profiling and tracing programs. </title> <booktitle> In POPL92, </booktitle> <pages> pages 59-70, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: For our current system, "wire time" is an insignificant contributor to overall performance. Therefore, the impact the protocols have on the amount of required communication is far more important than the direct protocol execution cost. Finally, we used a profiling tool, vt <ref> [BL92] </ref>, to rewrite our executables to add instrumentation code that estimates execution time in terms of processor cycles. <p> Almost seven molecules fit on a single page. Lazy diffing often allows LI to delay creating a diff of a page until multiple molecules have been modified, while LH creates diffs more frequently 3.3.2 Execution Time Breakdown We used qpt <ref> [BL92] </ref> to break down the applications' execution times. <p> Therefore, we chose a method that allowed the execution of the actual protocol code on the simulator. To meet our objectives, we use vt <ref> [BL92] </ref>, a profiling tool that rewrites an executable program to incorporate instrumentation code that produces an estimated processor cycle count.
Reference: [BLA + 93] <author> M.A. Blumrich, K. Li, R. Alpert, C. Dubnicki, E.W. Felten, and J. Sand-berg. </author> <title> Virtual memory mapped network interface for the SHRIMP mul-ticomputer. </title> <type> Technical Report CS-TR-487-93, </type> <institution> Department of Computer Science, Princeton University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: because the number of synchronization messages (and the wait time to acquire the locks) remains high (See Figure 4.5). 4.2.4 Reduced Software Overhead Message-passing systems with lower software overhead than Unix sockets are possible, either through optimizing the software structure, e.g., Peregrine [JZ93], or a user-level hardware interface, e.g., SHRIMP <ref> [BLA + 93] </ref>. In this section, we examine the effect of reducing both the fixed and per word overheads. <p> Sesame [WHL92] is a hardware network interface that implements eager sharing by selectively sending updates before they are requested. This approach is almost exactly opposite of LRC Their results indicate that eager sharing can perform an order of magnitude better than demand-driven systems under ideal conditions. The SHRIMP <ref> [BLA + 93] </ref> parallel machine consists of commodity workstations connected to a commodity backplanes via a custom network interface.
Reference: [BSF + 91] <author> W.J. Bolosky, M.L. Scott, R.P. Fitzgerald, R.J. Fowler, and A.L. Cox. </author> <title> NUMA policies and their relation to memory architecture. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 212-221, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: If the relative orderings of sub-operations of competing memory operations do not agree, then SC is violated. This problem is especially severe in systems in which sub-operations take differing amounts of time to complete, such as ring architectures that support concurrent access [WHL92], or NUMA machines <ref> [BSF + 91, SJG92, BFS89, Cox92, CF89, LE91, LEK91] </ref> . 2.2.2 Release Consistency Release consistency (RC) [GLL + 90], hereafter referred to as eager release consistency, is a relaxed memory consistency model that permits a process to delay making its changes to shared data visible to other processes until certain synchronization
Reference: [BT88] <author> H.E. Bal and A.S. Tanenbaum. </author> <title> Distributed programming with shared data. </title> <booktitle> In Proceedings of the 1988 International Conference on Computer Languages, </booktitle> <pages> pages 82-91, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Claim 1.2 DSM systems based on LRC can achieve better performance on a broader range of applications than systems based on eager protocols. Many previous systems either restricted their focus to coarse-grained programs [LH89, FP89], required user annotations [ZSB94, Lee94], or substantially changed the programming model <ref> [BT88, DCM + 90] </ref>.
Reference: [CA94] <author> D. Chaiken and A. Agarwal. </author> <title> Software-Extended Coherent Shared Memory. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 314-324, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Alewife <ref> [ALKK90, CA94] </ref> is a sequentially consistent system that uses directory-based cache coherence. Alewife uses a LimitLESS [CKA91] directory structure that supports a small number of directory pointers directly in hardware, and traps to a software handler when the pointers are exhausted.
Reference: [CAL + 89] <author> J.S. Chase, F.G. Amador, E.D. Lazowska, H.M. Levy, and R.J. Little-field. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year> <month> 106 </month>
Reference-contexts: The system marshals RPC arguments into the message, accommodating pointer values by copying data pointed to by any pointer parameters. Objects are not automatically migrated, and Emerald does not support object replication. Emerald does support programmer-directed object migration across machine boundaries. Amber <ref> [CAL + 89] </ref> eliminated Emerald's complicated RPC handling by implementing a single address space across all machines. Amber did not automatically move or replicate data, but provided special RPC variations that told the system whether to use RPC or to move the calling object to the destination site.
Reference: [Car93] <author> J.B. Carter. Munin: </author> <title> Efficient Distributed Shared Memory Using Multi-Protocol Release Consistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: Earlier work has shown that eager release consistent (ERC) systems outperform conventional DSM systems and can approach explicit message passing in communication requirements <ref> [Car93] </ref>. We therefore use ERC protocols as the yardstick with which to gauge the success of the LRC protocols. LRC protocols piggyback consistency information on top of synchronization messages, and only move data when needed. Eager protocols attempt to hide communication latency by moving data ahead of any need. <p> Claim 1.3 DSM systems can often obtain performance competitive with hardware shared memory systems. Claim 1.3 has been made before, but this work supersedes previous work in that (i) we show that the lazy protocols usually perform substantially better than the eager protocol <ref> [DKCZ93, Car93] </ref> that represents the previous state of the art, and (ii) we back up our contention by presenting a detailed performance comparison of our system with a hardware shared memory system that is based on the same processors and caches, as well as extensive simulations. 1.5 Contributions The primary contributions <p> LI and LH are new protocols that implement LRC. EI is a straightforward, invalidate-based implementation of eager release consistency (See Section 2.3.3). EI is used as the basis for our comparisons in Chapter 3 because studies <ref> [CBZ91, Car93] </ref> have shown that eager update protocols uniformly perform better than conventional protocols, and our own work [KCZ92, DKCZ93] has shown that eager invalidate protocols outperform eager update protocols. Section 2.1 describes the user interface of TreadMarks, the DSM system in which all three protocols are implemented. <p> An alternative to this static ownership scheme is to use an adaptive scheme that relies on guesses of the token's location, and follows successive guesses to the current owner of the token <ref> [Car93, LH89] </ref>. Since local guesses are updated to point to the requester as the request is forwarded along, every node in the chain of guesses ends 23 up knowing which process currently owns the token, and subsequent request chains are likely to be short. <p> Our evaluation of the LRC protocols has three components. The first is a comparison of the performance of the lazy protocols with the performance of an eager invalidate (EI) protocol. We use EI because studies [DKCZ93, KCZ92] have shown that EI consistently outperforms eager update protocols, and previous studies <ref> [CBZ91, Car93] </ref> showed that eager update protocols consistently outperform conventional DSM protocols. All of the protocols were implemented in the same system, running on the same hardware, and incur the same costs for communication and virtual memory primitives.
Reference: [CBZ91] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: LI and LH are new protocols that implement LRC. EI is a straightforward, invalidate-based implementation of eager release consistency (See Section 2.3.3). EI is used as the basis for our comparisons in Chapter 3 because studies <ref> [CBZ91, Car93] </ref> have shown that eager update protocols uniformly perform better than conventional protocols, and our own work [KCZ92, DKCZ93] has shown that eager invalidate protocols outperform eager update protocols. Section 2.1 describes the user interface of TreadMarks, the DSM system in which all three protocols are implemented. <p> This can result in many more diffs being created under LH than LI, as well as larger lock acquisition latencies. 2.3.3 Eager Invalidate We base our eager RC algorithms on Munin's multiple-writer protocol <ref> [CBZ91] </ref>. RC requires all prior ordinary accesses to be globally performed before a subsequent release is performed at any other process. The EI protocol calls a flush operation prior to any release. The flush operation sends invalidates to all other processes in the system that cache the affected pages. <p> Our evaluation of the LRC protocols has three components. The first is a comparison of the performance of the lazy protocols with the performance of an eager invalidate (EI) protocol. We use EI because studies [DKCZ93, KCZ92] have shown that EI consistently outperforms eager update protocols, and previous studies <ref> [CBZ91, Car93] </ref> showed that eager update protocols consistently outperform conventional DSM protocols. All of the protocols were implemented in the same system, running on the same hardware, and incur the same costs for communication and virtual memory primitives. <p> While 96 Orca's update mechanisms are indeed efficient, they require both language and hardware support. The language support is necessary to implement and enforce the object model, while the transparent replication relies on hardware multicast support. Munin <ref> [CBZ91] </ref> was the first DSMs to implement a relaxed consistency model.
Reference: [CDJ + 91] <author> R. G. Covington, S. Dwarkadas, J. R. Jump, S. Madala, and J. B. Sin-clair. </author> <title> The efficient simulation of parallel computer systems. </title> <journal> International Journal in Computer Simulation, </journal> <volume> 1 </volume> <pages> 31-58, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Compared to the AH approach, commodity parts can be used, reducing the cost and complexity of the design. In this section, we assess the performance of the HS approach compared to AS and AH. 4.2.1 Simulation Models We modeled the architectures and simulated the programs using an execution-driven simulator <ref> [CDJ + 91] </ref>. Instead of the DECstation-5000/240 and SGI 4D/480, we base our models on leading-edge technology.
Reference: [CF89] <author> A.L. Cox and R.J. Fowler. </author> <title> The implementation of a coherent memory abstraction on a NUMA multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: If the relative orderings of sub-operations of competing memory operations do not agree, then SC is violated. This problem is especially severe in systems in which sub-operations take differing amounts of time to complete, such as ring architectures that support concurrent access [WHL92], or NUMA machines <ref> [BSF + 91, SJG92, BFS89, Cox92, CF89, LE91, LEK91] </ref> . 2.2.2 Release Consistency Release consistency (RC) [GLL + 90], hereafter referred to as eager release consistency, is a relaxed memory consistency model that permits a process to delay making its changes to shared data visible to other processes until certain synchronization
Reference: [Che88] <author> D.R. Cheriton. </author> <title> The V distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: As with many of the systems discussed in this chapter, direct comparison between Munin and TreadMarks performance is difficult because of differences in the underlying systems (Munin was implemented on top of 16 Sun 3/60s connected by a 10MBit ethernet and running the V <ref> [Che88] </ref> operating system). 97 Midway [ZSB94] and Concord are software DSMs based on entry consistency (EC). Entry consistency requires each shared data object to be attached to a synchronization object.
Reference: [CIS93] <author> R. W. Cottingham Jr., R. M. Idury, and A. A. Schaffer. </author> <title> Faster sequential genetic linkage computations. </title> <journal> American Journal of Human Genetics, </journal> <volume> 53 </volume> <pages> 252-263, </pages> <year> 1993. </year>
Reference-contexts: Barnes and Water come from the Stanford Parallel Applications for Shared Memory (SPLASH) benchmark suite [SWG91]. FFT and IS come from the NAS benchmark suite [BBLS91]. SOR and TSP were developed at Rice University. ILINK was derived from a sequential program used by geneticists worldwide <ref> [LLJO84, CIS93, DSC + 94] </ref>. MIP was developed by researchers from the Department of Computational and Applied Mathematics at Rice University. Both ILINK and MIP are production codes used to solve real scientific and commercial problems that may require days or weeks of computation. <p> Since the array is often several megabytes or larger, the time spent on the transpose can be the limiting factor on overall performance. We ran the tests with array dimensions of 64x64x32. ILINK ILINK [DSC + 94] is a parallel version of FASTLINK 1.0 <ref> [CIS93] </ref>, which is an improved version of LINKAGE [LLJO84]. Genetic linkage analysis is a statistical technique that uses family pedigree information to map human genes and locate disease genes in the human genome.
Reference: [CKA91] <author> D. Chaiken, J. Kubiatowicz, and A. Agarwal. </author> <title> LimitLESS directories: A scalable cache coherence scheme. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Alewife [ALKK90, CA94] is a sequentially consistent system that uses directory-based cache coherence. Alewife uses a LimitLESS <ref> [CKA91] </ref> directory structure that supports a small number of directory pointers directly in hardware, and traps to a software handler when the pointers are exhausted. Simulations show that the LimitLESS directories achieve between 71% and 100% of the performance of full-map directory structures on a 256 node system.
Reference: [CLBHL93] <author> Jeff Chase, Hank Levy, Miche Baker-Harvey, and Ed Lazowska. Opal: </author> <title> A single address space system for 64-bit architectures. </title> <booktitle> In Proceedings of the Fourth Workshop on Workstation Operating Systems, </booktitle> <pages> pages 80-85, </pages> <year> 1993. </year>
Reference-contexts: Mether's primary innovation is allowing both intra- and inter-program sharing. None of the other systems discussed in this chapter explicitly address inter-program sharing, 95 but this sharing is one of the main rationales behind recent research in single address space systems <ref> [CLBHL93, SLM90] </ref>. Mether also divides shared space into regions that have different consistency mechanisms. "Demand-driven" space behaves as in a conventional system, while "data-driven" space has semantics somewhat like unix-pipes. Data must be written before it can be read and can be used to support efficient synchronization.
Reference: [Col91] <author> W.W. Collier. </author> <title> Reasoning about Parallel Architectures. </title> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: For instance, the example in to bypass writes in the buffers. Supporting SC in the presence of non-atomic memory transactions is even more difficult. An update in a distributed system can be logically decomposed into a series of sub-operations <ref> [Col91] </ref>, each of which applies to a single process. If the relative orderings of sub-operations of competing memory operations do not agree, then SC is violated.
Reference: [Cox92] <author> A.L. Cox. </author> <title> The Implementation and Evaluation of a Coherent Memory Abstraction for NUMA Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Rochester, Rochester, </institution> <address> NY, </address> <month> May </month> <year> 1992. </year> <month> 107 </month>
Reference-contexts: If the relative orderings of sub-operations of competing memory operations do not agree, then SC is violated. This problem is especially severe in systems in which sub-operations take differing amounts of time to complete, such as ring architectures that support concurrent access [WHL92], or NUMA machines <ref> [BSF + 91, SJG92, BFS89, Cox92, CF89, LE91, LEK91] </ref> . 2.2.2 Release Consistency Release consistency (RC) [GLL + 90], hereafter referred to as eager release consistency, is a relaxed memory consistency model that permits a process to delay making its changes to shared data visible to other processes until certain synchronization
Reference: [DCM + 90] <author> P. Dasgupta, R.C. Chen, S. Menon, M. Pearson, R. Ananthanarayanan, U. Ramachandran, M. Ahamad, R. LeBlanc Jr., W. Applebe, J.M. Bernabeu-Auban, P.W. Hutto, M.Y.A. Khalidi, and C.J. Wileknloh. </author> <title> The design and implementation of the Clouds distributed operating system. </title> <journal> Computing Systems Journal, </journal> <volume> 3, </volume> <month> Winter </month> <year> 1990. </year>
Reference-contexts: Claim 1.2 DSM systems based on LRC can achieve better performance on a broader range of applications than systems based on eager protocols. Many previous systems either restricted their focus to coarse-grained programs [LH89, FP89], required user annotations [ZSB94, Lee94], or substantially changed the programming model <ref> [BT88, DCM + 90] </ref>. <p> The duration of the pin is set at application startup and remains constant through the duration of the execution. The purpose of the pin is to prevent the "ping-pong" effect that can plague systems like Ivy. Clouds <ref> [DCM + 90] </ref> is another early sequentially consistent DSM. Clouds differs from Ivy in that it uses program-defined segments as the unit of memory granularity rather than pages, and in that it allows memory to be pinned as in Mirage. However, segments are pinned via explicit "pin" and "unpin" operations.
Reference: [DKCZ93] <author> S. Dwarkadas, P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 244-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Claim 1.3 DSM systems can often obtain performance competitive with hardware shared memory systems. Claim 1.3 has been made before, but this work supersedes previous work in that (i) we show that the lazy protocols usually perform substantially better than the eager protocol <ref> [DKCZ93, Car93] </ref> that represents the previous state of the art, and (ii) we back up our contention by presenting a detailed performance comparison of our system with a hardware shared memory system that is based on the same processors and caches, as well as extensive simulations. 1.5 Contributions The primary contributions <p> EI is a straightforward, invalidate-based implementation of eager release consistency (See Section 2.3.3). EI is used as the basis for our comparisons in Chapter 3 because studies [CBZ91, Car93] have shown that eager update protocols uniformly perform better than conventional protocols, and our own work <ref> [KCZ92, DKCZ93] </ref> has shown that eager invalidate protocols outperform eager update protocols. Section 2.1 describes the user interface of TreadMarks, the DSM system in which all three protocols are implemented. Section 2.3 describes the design and implementation of the three protocols. <p> Our evaluation of the LRC protocols has three components. The first is a comparison of the performance of the lazy protocols with the performance of an eager invalidate (EI) protocol. We use EI because studies <ref> [DKCZ93, KCZ92] </ref> have shown that EI consistently outperforms eager update protocols, and previous studies [CBZ91, Car93] showed that eager update protocols consistently outperform conventional DSM protocols.
Reference: [DSC + 94] <author> S. Dwarkadas, A.A. Schaffer, R.W. Cottingham Jr., A.L. Cox, P. Keleher, and W. Zwaenepoel. </author> <title> Parallelization of general linkage analysis problems. </title> <booktitle> Human Heredity, </booktitle> <volume> 44 </volume> <pages> 127-141, </pages> <year> 1994. </year>
Reference-contexts: Barnes and Water come from the Stanford Parallel Applications for Shared Memory (SPLASH) benchmark suite [SWG91]. FFT and IS come from the NAS benchmark suite [BBLS91]. SOR and TSP were developed at Rice University. ILINK was derived from a sequential program used by geneticists worldwide <ref> [LLJO84, CIS93, DSC + 94] </ref>. MIP was developed by researchers from the Department of Computational and Applied Mathematics at Rice University. Both ILINK and MIP are production codes used to solve real scientific and commercial problems that may require days or weeks of computation. <p> Since the array is often several megabytes or larger, the time spent on the transpose can be the limiting factor on overall performance. We ran the tests with array dimensions of 64x64x32. ILINK ILINK <ref> [DSC + 94] </ref> is a parallel version of FASTLINK 1.0 [CIS93], which is an improved version of LINKAGE [LLJO84]. Genetic linkage analysis is a statistical technique that uses family pedigree information to map human genes and locate disease genes in the human genome. <p> Given a fixed value of the recombination vector, the outer loops of the likelihood evaluation iterate over all the pedigrees and 41 each nuclear family (consisting of parents and child) within each pedigree to update the probabilities of each genotype (see <ref> [DSC + 94] </ref>) for each individual, which is stored in an array genarray. A straightforward method of parallelizing this program is to split the iteration space among the processes and surround each addition with a lock to do it in place. <p> ILINK The speedup for ILINK for all three protocols is 5.9. Overall, ILINK achieves less than linear speedup because several sections of code are executed sequentially rather than in parallel, and the algorithm has an inherent load balancing problem <ref> [DSC + 94] </ref>. It is not possible without significant computation and communication to predict in advance whether the set of iterations distributed to the processes will result in each process having an equal amount of work. <p> perform better than CLP and are more representative of linkage analysis in general. 80 ILINK achieves less than linear speedup on both the 4D/480 and TreadMarks because of a combination of load balancing problems and the cost of several sections of code that are implemented sequentially rather than in parallel <ref> [DSC + 94] </ref>. The 4D/480 outperforms TreadMarks because of the large amount of communication. The communication rate for the CLP input set is 157 Kbytes/second and 449 messages/second on 8 processors, compared to 526 Kbytes/second and 1,800 messages/second for the BAD input set, hence the better speedups achieved for CLP.
Reference: [EZ92a] <author> E.N. Elnozahy and W. Zwaenepoel. Manetho: </author> <title> Transparent rollback-recovery with low overhead, limited rollback, and fast output commit. </title> <journal> IEEE Transactions on Computers Special Issue On Fault-Tolerant Computing, </journal> <volume> 41(5) </volume> <pages> 526-531, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Lazy release consistency provides new opportunities for closer integration of fault tolerant substrates into the DSM system because much of the information needed by fault tolerant systems is already available. Systems such as Manetho <ref> [EZ92a, EZ92b] </ref> rely on detailed message ordering information that can be reconstructed by systems that maintain happened-before-1 information. Checkpointing 103 overhead can be reduced by maintaining invariants assuring that portions of shared memory can be reconstructed on failure.
Reference: [EZ92b] <author> E.N. Elnozahy and W. Zwaenepoel. </author> <title> Replicated distributed process in Manetho. </title> <booktitle> In Proceedings of the 22nd International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 18-27, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Lazy release consistency provides new opportunities for closer integration of fault tolerant substrates into the DSM system because much of the information needed by fault tolerant systems is already available. Systems such as Manetho <ref> [EZ92a, EZ92b] </ref> rely on detailed message ordering information that can be reconstructed by systems that maintain happened-before-1 information. Checkpointing 103 overhead can be reduced by maintaining invariants assuring that portions of shared memory can be reconstructed on failure.
Reference: [FP89] <author> B. Fleisch and G. Popek. </author> <title> Mirage: A coherent distributed shared memory design. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 211-223, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Claim 1.2 DSM systems based on LRC can achieve better performance on a broader range of applications than systems based on eager protocols. Many previous systems either restricted their focus to coarse-grained programs <ref> [LH89, FP89] </ref>, required user annotations [ZSB94, Lee94], or substantially changed the programming model [BT88, DCM + 90]. <p> Ivy used a dynamic, distributed page management and location scheme. Chains of "probable owners" are followed until arriving at the owner. At each intermediate site, the local "probable owner" is updated to point to the processor performing the search. Mirage <ref> [FP89] </ref> differs from Ivy in that newly mapped pages are "pinned" to a processor for an interval of time () before they can be invalidated or migrated to other processors. The duration of the pin is set at application startup and remains constant through the duration of the execution.
Reference: [GLL + 90] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: False sharing is more common on DSM systems than on HSM systems because DSMs track accesses at the granularity of virtual memory pages, while HSMs track accesses at the granularity of cache lines. 1.3 Lazy Release Consistency Lazy release consistency is based on release consistency (RC) <ref> [GLL + 90] </ref>, a relaxed memory consistency model that permits a processor to delay making its changes to shared data visible to other processors until subsequent synchronization accesses occur. Essentially, all shared accesses are divided into ordinary accesses, acquire synchronization accesses, and release synchronization accesses. <p> However, programs written for SC produce the same results on an RC memory, provided that (i) all synchronization operations use system-visible primitives, and (ii) there is a chain of synchronization between 5 every pair of conflicting ordinary accesses to the same memory location by different processors <ref> [GLL + 90] </ref>. In practice, most shared memory programs require little or no modification to meet these requirements. LRC is a refinement of RC that allows consistency action to be postponed until a synchronization variable released in a subsequent operation is acquired by another processor. <p> This problem is especially severe in systems in which sub-operations take differing amounts of time to complete, such as ring architectures that support concurrent access [WHL92], or NUMA machines [BSF + 91, SJG92, BFS89, Cox92, CF89, LE91, LEK91] . 2.2.2 Release Consistency Release consistency (RC) <ref> [GLL + 90] </ref>, hereafter referred to as eager release consistency, is a relaxed memory consistency model that permits a process to delay making its changes to shared data visible to other processes until certain synchronization accesses occur. <p> Eager release consistency addresses the performance issue by allowing write accesses to be pipelined or batched, and addresses the programming model issue by guaranteeing results equivalent to an SC system for properly-labeled <ref> [GLL + 90] </ref> programs. Informally, a program is properly-labeled if the program contains enough synchronization to avoid data races. This concept is similar to the notion of data-race-free, which will be discussed in Section 2.2.4. this categorization, we first explain the notion of competing accesses.
Reference: [HLRW92] <author> M. D. Hill, J. R. Larus, S. K. Reinhardt, and D. A. Wood. </author> <title> Cooperative shared memory: Software and hardware support for scaleable multiprocessors. </title> <booktitle> In Proceedings of the 5th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 262-273, </pages> <month> Oc-tober </month> <year> 1992. </year> <month> 108 </month>
Reference-contexts: Another of Alewife's innovations is the use of very fast context switching of threads to hide access latency. Architectures such as LimitLESS can benefit from the Check-In/Check-Out (CICO) <ref> [HLRW92] </ref> programming model. CICO allows programmers to pass performance directives directly to the memory system. In a CICO system, all accesses to shared memory are bracketed by check in and check out instructions.
Reference: [HWC + 93] <author> J. T. Hecht, Y. Wang, B. Connor, S. H. Blanton, and S. P. Daiger. Non-syndromic cleft lip and palate: </author> <title> No evidence of linkage to hla or factor 13a. </title> <journal> American Journal of Human Genetics, </journal> <volume> 52 </volume> <pages> 1230-1233, </pages> <year> 1993. </year>
Reference-contexts: This version uses a local copy of the genarray, called gene, to temporarily hold updates to the global array. They are eventually merged into the final copy after synchronization. ILINK's input consists of data on 12 families with autosomal dominant nonsyn-dromic cleft lip and palate (CLP) <ref> [HWC + 93] </ref>. Integer Sort (IS) This application ranks an unsorted sequence of N keys. The rank of a key in a sequence is the index value i that the key would have if the sequence of keys were sorted. <p> The 1000 fi 1000 run is included to assess the effect of changing the communication to computation ratio. Both 18- and 19-city problems were used as input to TSP. We ran ILINK with two different inputs, CLP <ref> [HWC + 93] </ref> and BAD [LRC + 92], both corresponding to real data sets used in disease gene location. 4.1.3 Results Table 4.1 presents the single processor execution times on both systems, as well as the single processor DECstation time without TreadMarks.
Reference: [JZ93] <author> D.B. Johnson and W. Zwaenepoel. </author> <title> The Peregrine high-performance RPC system. </title> <journal> Software: Practice and Experience, </journal> <volume> 23(2) </volume> <pages> 201-221, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: AS architecture, its performance does not match AH because the number of synchronization messages (and the wait time to acquire the locks) remains high (See Figure 4.5). 4.2.4 Reduced Software Overhead Message-passing systems with lower software overhead than Unix sockets are possible, either through optimizing the software structure, e.g., Peregrine <ref> [JZ93] </ref>, or a user-level hardware interface, e.g., SHRIMP [BLA + 93]. In this section, we examine the effect of reducing both the fixed and per word overheads.
Reference: [Kaa92] <author> M.F. Kaashoek. </author> <title> Group Communication in Distributed Computer Systems. </title> <type> PhD thesis, </type> <institution> Vrije Universiteit, </institution> <address> Amsterdam, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: Subsidiary objects could also be "attached" to other objects. All attached objects moved between nodes when any one of the objects moved. Orca <ref> [BKT92, Kaa92] </ref> is an object-based system that supports transparent replication by a fast, hardware-supported, multicast mechanism. Replication is not automatic, but initiated by heuristics.
Reference: [KCZ92] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: EI is a straightforward, invalidate-based implementation of eager release consistency (See Section 2.3.3). EI is used as the basis for our comparisons in Chapter 3 because studies [CBZ91, Car93] have shown that eager update protocols uniformly perform better than conventional protocols, and our own work <ref> [KCZ92, DKCZ93] </ref> has shown that eager invalidate protocols outperform eager update protocols. Section 2.1 describes the user interface of TreadMarks, the DSM system in which all three protocols are implemented. Section 2.3 describes the design and implementation of the three protocols. <p> Our evaluation of the LRC protocols has three components. The first is a comparison of the performance of the lazy protocols with the performance of an eager invalidate (EI) protocol. We use EI because studies <ref> [DKCZ93, KCZ92] </ref> have shown that EI consistently outperforms eager update protocols, and previous studies [CBZ91, Car93] showed that eager update protocols consistently outperform conventional DSM protocols.
Reference: [Kea94] <author> J. Kuskin and D. Ofelt et al. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Cache coherence is maintained using a directory-based protocol. A cache miss satisfied by remote memory takes 92 to 130 processor cycles, depending on the block's location and whether it is modified. These cycle counts are similar to the Stanford DASH [LLG + 92] and FLASH <ref> [Kea94] </ref> multiprocessors. In both the AS and the HS models, the general-purpose network is an ATM switch with a point-to-point bandwidth of 622 MBit/second and a switching latency of 1 sec. Memory consistency between the nodes is maintained using the TreadMarks LRC invalidate protocol (See Chapter 2.3.1). <p> Typhoon [RLW94] supports shared memory over a message-passing network by using a communication co-processor to connect each primary processor to the network. All communication and protocol code is handled by the communication co-processor, freeing the main processor to continue executing application code. Finally, Flash <ref> [Kea94] </ref> is an even more ambitious project that uses a custom designed node controller (MAGIC) to handle all communication both within and between nodes.
Reference: [KFJ94] <author> Povl T. Koch, Robert J. Fowler, and Eric Jul. </author> <title> Message-driven relaxed consistency in a software distributed shared memory. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 75-86, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Since the handler is provided by the application, it allows update behaviors to be tailored 98 very closely to specific applications. However, the use of handlers complicates the programming model even further. Finally, the Carlos system <ref> [KFJ94] </ref> integrates message passing into an LRC system by providing a message interface and requiring messages to be annotated as to their significance to memory consistency. The message passing interface can be used to build efficient high level synchronization constructs such as queues or heaps. <p> However, work with the hybrid protocol, as well as work by other researchers <ref> [KFJ94, ACDZ94] </ref>, has shown that the protocols can greatly benefit from being more closely integrated with the application. We expect this direction of research to include two key components: 1. <p> De-constructing LRC By making more of the system visible to users at the application level, we expect to achieve a better match of system mechanisms to application semantics. 2. Compiler Integration Compiler analysis can be useful in detecting specific access patterns and supplementing user annotations. Other researchers <ref> [KFJ94] </ref> have shown that lazy release consistency can be deconstructed into building blocks that can be used to create custom synchronization mechanisms. We envision extending this de-construction to expose data movement, consistency management, as well as synchronization flow.
Reference: [Lam79] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Essentially, all shared accesses are divided into ordinary accesses, acquire synchronization accesses, and release synchronization accesses. Release consistency allows the results of ordinary shared writes to be buffered locally until the next release operation. In contrast, sequential consistency (SC) <ref> [Lam79] </ref>, until recently the model implemented by most bus-based multiprocessors, requires all prior shared writes to complete before any subsequent shared accesses can be initiated. RC systems can achieve large performance gains over SC systems because they allow updates to be buffered. <p> The Figure 2.1 shows a complete, runnable program that fills an array in parallel. 2.2 Lazy Release Consistency 2.2.1 Motivation and Background Most previous DSM systems supported the canonical consistency model, sequential consistency (SC) <ref> [Lam79] </ref>. However, many studies have shown that sequential consistency poses serious problems for efficient distributed implementations of shared memory, primarily because sequential consistency imposes such strict requirements on system-wide ordering of accesses to shared memory [Adv93].
Reference: [LE91] <author> R. P. LaRowe and C. S. Ellis. </author> <title> Experimental comparison of memory management policies for NUMA multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(4) </volume> <pages> 319-363, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: If the relative orderings of sub-operations of competing memory operations do not agree, then SC is violated. This problem is especially severe in systems in which sub-operations take differing amounts of time to complete, such as ring architectures that support concurrent access [WHL92], or NUMA machines <ref> [BSF + 91, SJG92, BFS89, Cox92, CF89, LE91, LEK91] </ref> . 2.2.2 Release Consistency Release consistency (RC) [GLL + 90], hereafter referred to as eager release consistency, is a relaxed memory consistency model that permits a process to delay making its changes to shared data visible to other processes until certain synchronization
Reference: [Lea87] <editor> E. L. Lusk and R. A. Overbeek et al. </editor> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart and Winston, Inc, </publisher> <year> 1987. </year>
Reference-contexts: Section 2.4 presents a proof that LRC is indistinguishable from conventional memory models under most conditions, and Section 2.5 summarizes the chapter. 2.1 Application Program Interface (API) TreadMarks is entirely implemented as a C library, using an interface similar to the parmacs macros from Argonne National Laboratory <ref> [Lea87] </ref> for process and synchronization support. While the parmacs macros are implemented using m4 macros, our DSM library is implemented as a set of procedure calls. Nonetheless, properly synchronized programs using the parmacs macros can be ported to our system with only minor changes in naming and initialization.
Reference: [Lee94] <author> J. William Lee. </author> <title> Concord: Re-thinking the division of labor in a distributed shared memory system. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 585-592, </pages> <month> may </month> <year> 1994. </year> <month> 109 </month>
Reference-contexts: Claim 1.2 DSM systems based on LRC can achieve better performance on a broader range of applications than systems based on eager protocols. Many previous systems either restricted their focus to coarse-grained programs [LH89, FP89], required user annotations <ref> [ZSB94, Lee94] </ref>, or substantially changed the programming model [BT88, DCM + 90]. <p> Third, the current implementation by the Midway group relies heavily on several non-standard features of the DECStation's cache structure, and hence is not highly portable. Finally, the dirty bit approach requires significant compiler involvement, while the diff approach is language and compiler independent. Concord <ref> [Lee94] </ref> extends entry consistency by allowing handlers to be associated with synchronization operations, as well as data. A handler is a procedure that is run at synchronization time, and specifies exactly what data should be updated.
Reference: [LEK91] <author> R. P. LaRowe, C. S. Ellis, and L. S. Kaplan. </author> <title> The robustness of numa memory management. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 110-121, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: If the relative orderings of sub-operations of competing memory operations do not agree, then SC is violated. This problem is especially severe in systems in which sub-operations take differing amounts of time to complete, such as ring architectures that support concurrent access [WHL92], or NUMA machines <ref> [BSF + 91, SJG92, BFS89, Cox92, CF89, LE91, LEK91] </ref> . 2.2.2 Release Consistency Release consistency (RC) [GLL + 90], hereafter referred to as eager release consistency, is a relaxed memory consistency model that permits a process to delay making its changes to shared data visible to other processes until certain synchronization
Reference: [LH89] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Claim 1.2 DSM systems based on LRC can achieve better performance on a broader range of applications than systems based on eager protocols. Many previous systems either restricted their focus to coarse-grained programs <ref> [LH89, FP89] </ref>, required user annotations [ZSB94, Lee94], or substantially changed the programming model [BT88, DCM + 90]. <p> In Figure 2.10, process P 3 takes a coherence miss on page p, which contains data x, y, and z. Unlike conventional protocols <ref> [LH89] </ref>, lazy protocols allow processes to determine the location of needed data entirely on the basis of local information. P 3 is therefore able to determine that there have been two previous modifications that need to be applied locally before accessing z. <p> An alternative to this static ownership scheme is to use an adaptive scheme that relies on guesses of the token's location, and follows successive guesses to the current owner of the token <ref> [Car93, LH89] </ref>. Since local guesses are updated to point to the requester as the request is forwarded along, every node in the chain of guesses ends 23 up knowing which process currently owns the token, and subsequent request chains are likely to be short. <p> HS compared to AH. 92 93 94 Chapter 5 Related Work This section compares lazy release consistency and the TreadMarks system with a number of other software and hardware shared memory system, paying particular attention to differences in memory and programming models. 5.1 Software-Supported Shared Memory Sequentially Consistent Systems Ivy <ref> [LH89] </ref> was the first software-only implementation of distributed shared memory. Ivy implemented a single-writer, sequentially consistent, invalidate protocol that used virtual memory pages as the base unit of consistency. Ivy used a dynamic, distributed page management and location scheme. Chains of "probable owners" are followed until arriving at the owner.
Reference: [LLG + 90] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The message passing interface can be used to build efficient high level synchronization constructs such as queues or heaps. Early performance results of Carlos are encouraging, showing substantial gains over TreadMarks's performance for such lock-based programs as TSP, QS, and Water. 5.2 Hardware-Supported Shared Memory DASH <ref> [LLG + 90] </ref> is a cluster-based machine that uses a directory-based cache coherency protocol. DASH was the first system of any type to support release consistency. However, DASH uses a write invalidate protocol to maintain consistency. DASH does allow updates to be pipelined.
Reference: [LLG + 92] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Cache coherence is maintained using a directory-based protocol. A cache miss satisfied by remote memory takes 92 to 130 processor cycles, depending on the block's location and whether it is modified. These cycle counts are similar to the Stanford DASH <ref> [LLG + 92] </ref> and FLASH [Kea94] multiprocessors. In both the AS and the HS models, the general-purpose network is an ATM switch with a point-to-point bandwidth of 622 MBit/second and a switching latency of 1 sec.
Reference: [LLJO84] <author> G.M. Lathrop, J.M. Lalouel, C. Julier, and J. Ott. </author> <title> Strategies for mul-tilocus linkage analysis in humans. </title> <booktitle> Proceedings of National Academy of Science, </booktitle> <volume> 81 </volume> <pages> 3443-3446, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Barnes and Water come from the Stanford Parallel Applications for Shared Memory (SPLASH) benchmark suite [SWG91]. FFT and IS come from the NAS benchmark suite [BBLS91]. SOR and TSP were developed at Rice University. ILINK was derived from a sequential program used by geneticists worldwide <ref> [LLJO84, CIS93, DSC + 94] </ref>. MIP was developed by researchers from the Department of Computational and Applied Mathematics at Rice University. Both ILINK and MIP are production codes used to solve real scientific and commercial problems that may require days or weeks of computation. <p> We ran the tests with array dimensions of 64x64x32. ILINK ILINK [DSC + 94] is a parallel version of FASTLINK 1.0 [CIS93], which is an improved version of LINKAGE <ref> [LLJO84] </ref>. Genetic linkage analysis is a statistical technique that uses family pedigree information to map human genes and locate disease genes in the human genome. The fundamental goal in linkage analysis is to compute the recombination probability, which is the probability that a recombination occurs between two genes.
Reference: [LRC + 92] <author> A. Law, C. W. Richard III, R. W. Cottingham Jr., G. M. Lathrop, D. R. Cox, and R. M. Myers. </author> <title> Genetic linkage analysis of bipolar affective disorder in an old order amish pedigree. </title> <journal> Human Genetics, </journal> <volume> 88 </volume> <pages> 562-568, </pages> <year> 1992. </year>
Reference-contexts: The 1000 fi 1000 run is included to assess the effect of changing the communication to computation ratio. Both 18- and 19-city problems were used as input to TSP. We ran ILINK with two different inputs, CLP [HWC + 93] and BAD <ref> [LRC + 92] </ref>, both corresponding to real data sets used in disease gene location. 4.1.3 Results Table 4.1 presents the single processor execution times on both systems, as well as the single processor DECstation time without TreadMarks.
Reference: [LT88] <author> T. Lovett and S. Thakkar. </author> <title> The Symmetry multiprocessor system. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 303-310, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Our results show that efficiently supporting the abstraction of shared memory in software is possible for a broad class of applications, but only with the use of high-performance protocols that are specifically crafted to reduce communication requirements. Given appropriate protocols, DSM systems can have performance comparable to hardware systems <ref> [LT88] </ref> for small clusters, and significant speedup even on large clusters. 1.1 Programming Model The LRC protocols developed in this work provide a generic abstraction of shared memory to application programs. Programs are multi-threaded, and synchronize through locks and barriers. All data communication between threads is through globally shared memory. <p> Programs are multi-threaded, and synchronize through locks and barriers. All data communication between threads is through globally shared memory. For most programs, the memory abstraction supported by LRC protocols is indistinguishable from that of a multiprocessor that supports shared memory in hardware <ref> [LT88] </ref>. The protocols and the system described in this dissertation do not require user annotation or language support.
Reference: [Mat89] <author> F. Mattern. </author> <title> Virtual time and global states of distributed systems. In Parallel & Distributed Algorithms. </title> <publisher> Elsevier Science Publishers, </publisher> <year> 1989. </year>
Reference-contexts: An interval is said to be performed at a process if all the interval's accesses have been performed at that process. The lazy protocols track which intervals have been performed at a process by maintaining a per process vector timestamp <ref> [Mat89] </ref>. A vector timestamp consists of a set of interval indices, one per process in the system. Let vv i p be the vector timestamp of process p at interval i.
Reference: [MF89] <author> R.C. Minnich and D.J. Farber. </author> <title> The mether system: A distributed shared memory for SunOS 4.0. </title> <booktitle> In Proceedings of the 1989 Summer Usenix Conference, </booktitle> <pages> pages 51-60, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: However, segments are pinned via explicit "pin" and "unpin" operations. This method creates opportunities for savvy programmers to improve application performance, but also requires them to understand more of the details of the underlying DSM system. Mether <ref> [MF89, MF90] </ref> is another Ivy-like system that runs on top of SunOS. Mether's primary innovation is allowing both intra- and inter-program sharing.
Reference: [MF90] <author> R.C. Minnich and D.J. Farber. </author> <title> Reducing host load, network load, and latency in a distributed shared memory. </title> <booktitle> In Proceedings of the 10th Inter 110 national Conference on Distributed Computing Systems, </booktitle> <pages> pages 468-475, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: However, segments are pinned via explicit "pin" and "unpin" operations. This method creates opportunities for savvy programmers to improve application performance, but also requires them to understand more of the details of the underlying DSM system. Mether <ref> [MF89, MF90] </ref> is another Ivy-like system that runs on top of SunOS. Mether's primary innovation is allowing both intra- and inter-program sharing.
Reference: [RLW94] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: The annotations can also be used to increase cache reuse and to reduce data sharing. CICO was developed with a shared memory system implementing a conventional memory model in mind. However, information from the directives could benefit an LRC system as well. Typhoon <ref> [RLW94] </ref> supports shared memory over a message-passing network by using a communication co-processor to connect each primary processor to the network. All communication and protocol code is handled by the communication co-processor, freeing the main processor to continue executing application code.
Reference: [SJG92] <author> P. Stenstrom, T. Joe, and A. Gupta. </author> <title> Comparative performance evaluation of cache-coherent NUMA and COMA architectures. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 80-91, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: If the relative orderings of sub-operations of competing memory operations do not agree, then SC is violated. This problem is especially severe in systems in which sub-operations take differing amounts of time to complete, such as ring architectures that support concurrent access [WHL92], or NUMA machines <ref> [BSF + 91, SJG92, BFS89, Cox92, CF89, LE91, LEK91] </ref> . 2.2.2 Release Consistency Release consistency (RC) [GLL + 90], hereafter referred to as eager release consistency, is a relaxed memory consistency model that permits a process to delay making its changes to shared data visible to other processes until certain synchronization
Reference: [SLM90] <author> M.L. Scott, T.J. LeBlanc, </author> <title> and B.D. Marsh. </title> <booktitle> Multi-model parallel programming in Psyche. In Proceedings of the 1990 Conference on the Principles and Practice of Parallel Programming, </booktitle> <pages> pages 70-78, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Mether's primary innovation is allowing both intra- and inter-program sharing. None of the other systems discussed in this chapter explicitly address inter-program sharing, 95 but this sharing is one of the main rationales behind recent research in single address space systems <ref> [CLBHL93, SLM90] </ref>. Mether also divides shared space into regions that have different consistency mechanisms. "Demand-driven" space behaves as in a conventional system, while "data-driven" space has semantics somewhat like unix-pipes. Data must be written before it can be read and can be used to support efficient synchronization.
Reference: [SWG91] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stan-ford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: For example, SOR and TSP are relatively simple, while ILINK and MIP each consist of more than ten thousand lines of code. The programs come from several different sources. Barnes and Water come from the Stanford Parallel Applications for Shared Memory (SPLASH) benchmark suite <ref> [SWG91] </ref>. FFT and IS come from the NAS benchmark suite [BBLS91]. SOR and TSP were developed at Rice University. ILINK was derived from a sequential program used by geneticists worldwide [LLJO84, CIS93, DSC + 94]. <p> We ran TSP on a nineteen city input set. Water Water is a slightly modified version of the Water program from SPLASH <ref> [SWG91] </ref>. Our version differs from the standard by condensing multiple lock acquisitions into single acquisitions, thereby greatly reducing synchronization overhead. Only nine lines of code were affected by the change. Water is a molecular dynamics simulation. Each time-step, the intra- and inter-molecular forces incident on a molecule are computed.
Reference: [WHL92] <author> L.D. Wittie, G. Hermannsson, and A. Li. </author> <title> Eager sharing for efficient massive parallelism. </title> <booktitle> In 1992 International Conference on Parallel Processing, </booktitle> <pages> pages 251-255, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: If the relative orderings of sub-operations of competing memory operations do not agree, then SC is violated. This problem is especially severe in systems in which sub-operations take differing amounts of time to complete, such as ring architectures that support concurrent access <ref> [WHL92] </ref>, or NUMA machines [BSF + 91, SJG92, BFS89, Cox92, CF89, LE91, LEK91] . 2.2.2 Release Consistency Release consistency (RC) [GLL + 90], hereafter referred to as eager release consistency, is a relaxed memory consistency model that permits a process to delay making its changes to shared data visible to other <p> DASH was the first system of any type to support release consistency. However, DASH uses a write invalidate protocol to maintain consistency. DASH does allow updates to be pipelined. Sesame <ref> [WHL92] </ref> is a hardware network interface that implements eager sharing by selectively sending updates before they are requested. This approach is almost exactly opposite of LRC Their results indicate that eager sharing can perform an order of magnitude better than demand-driven systems under ideal conditions.
Reference: [YTR + 87] <author> M. Young, A. Tevanian, R. Rashid, D. Golub, J. Eppinger, J. Chew, W. Bolosky, D. Black, and R. Baron. </author> <title> The duality of memory and communication in the implementation of a multiprocessor operating system. </title> <booktitle> In Proceedings of the 11th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 63-76, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: TreadMarks handles synchronization separately from the memory system. Bryant et al. [BCCR91] implemented Structured Shared Virtual Memory (SSVM) on a star network of IBM RS-6000s running Mach 2.5. Two different implementation strategies were followed: one using the Mach external pager interface <ref> [YTR + 87] </ref>, and one using the Mach exception interface [BGR + 88]. They report that the latter implementation|which is very similar to ours|is more efficient, because of the inability of Mach's external pager interface to asynchronously update a page in the user's address space.
Reference: [ZSB94] <author> Mathew J. Zekauskas, Wayne A. Sawdon, and Brian N. Bershad. </author> <title> Software write detection for distributed shared memory. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 87-100, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Claim 1.2 DSM systems based on LRC can achieve better performance on a broader range of applications than systems based on eager protocols. Many previous systems either restricted their focus to coarse-grained programs [LH89, FP89], required user annotations <ref> [ZSB94, Lee94] </ref>, or substantially changed the programming model [BT88, DCM + 90]. <p> As with many of the systems discussed in this chapter, direct comparison between Munin and TreadMarks performance is difficult because of differences in the underlying systems (Munin was implemented on top of 16 Sun 3/60s connected by a 10MBit ethernet and running the V [Che88] operating system). 97 Midway <ref> [ZSB94] </ref> and Concord are software DSMs based on entry consistency (EC). Entry consistency requires each shared data object to be attached to a synchronization object.
References-found: 62

