URL: http://arch.cs.ucdavis.edu/~chong/250C/WRL-TR-95.7.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C.html
Root-URL: http://www.cs.ucdavis.edu
Title: Shared Memory Consistency Models: A Tutorial  
Author: S E P T E M B E R Sarita V. Adve Kourosh Gharachorloo 
Date: 1 9 9 5  95/7  
Pubnum: Research Report  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Sarita V. Adve. </author> <title> Designing Memory Consistency Models for Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> December </month> <year> 1993. </year> <note> Available as Technical Report #1198. </note>
Reference-contexts: This view describes models in terms of program behavior, rather than in terms of hardware or compiler optimizations. Readers interested in further pursuing a more formal treatment of both the system-centric and programmer-centric views may refer to our previous work <ref> [1, 6, 8] </ref>. The rest of this article is organized as follows. We begin with a short note on who should be concerned with the memory consistency model of a system. <p> Second, we assume all models enforce uniprocessor data and control dependences. Finally, models that relax the program order from reads to following write operations must also maintain a subtle form of multiprocessor data and control dependences <ref> [8, 1] </ref>; this latter constraint is inherently upheld by all processor designs we are aware of and can also be easily maintained by the compiler. 6.2 Relaxing the Write to Read Program Order The first set of models we discuss relax the program order constraints in the case of a write <p> The properly-labeled (PL) approach was provided along with the definition of release consistency (RCsc) as a simpler way to reason about the type of optimizations exploited by RCsc [13]. Programmer-centric approaches for exploiting more aggressive optimizations are described in our other work <ref> [7, 3, 1, 6] </ref>; a unified framework for designing programmer-centric models has also been developed and used to explore the design space of such models [1]. <p> Programmer-centric approaches for exploiting more aggressive optimizations are described in our other work [7, 3, 1, 6]; a unified framework for designing programmer-centric models has also been developed and used to explore the design space of such models <ref> [1] </ref>. To illustrate the programmer-centric approach more concretely, the next section describes the type of program-level information that may be provided by the programmer to enable optimizations similar to those exploited by the weak ordering model. <p> In particular, the optimizations enabled by the weak ordering model can be safely applied. Furthermore, the information also enables more aggressive optimizations than exploited by weak ordering <ref> [2, 13, 1] </ref>. As shown in Figure 13, the programmer-centric framework requires the programmer to identify all operations that may be involved in a race as synchronization operations.
Reference: [2] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering Anew definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Systems based on the model exploit the information to perform optimizations without violating sequential consistency. Our previous work has explored various programmer-centric approaches. For example, the data-race-free-0 (DRF0) approach explores the information that is required to allow optimizations similar to those enabled by weak ordering <ref> [2] </ref>. The properly-labeled (PL) approach was provided along with the definition of release consistency (RCsc) as a simpler way to reason about the type of optimizations exploited by RCsc [13]. <p> In particular, the optimizations enabled by the weak ordering model can be safely applied. Furthermore, the information also enables more aggressive optimizations than exploited by weak ordering <ref> [2, 13, 1] </ref>. As shown in Figure 13, the programmer-centric framework requires the programmer to identify all operations that may be involved in a race as synchronization operations.
Reference: [3] <author> Sarita V. Adve and Mark D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(6) </volume> <pages> 613-624, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The properly-labeled (PL) approach was provided along with the definition of release consistency (RCsc) as a simpler way to reason about the type of optimizations exploited by RCsc [13]. Programmer-centric approaches for exploiting more aggressive optimizations are described in our other work <ref> [7, 3, 1, 6] </ref>; a unified framework for designing programmer-centric models has also been developed and used to explore the design space of such models [1].
Reference: [4] <author> Francisco Corella, Janice M. Stone, and Charles M. Barton. </author> <title> A formal specification of the PowerPC shared memory architecture. </title> <type> Technical Report Computer Science Technical Report RC 18638(81566), </type> <institution> IBM Research Division, T.J. Watson Research Center, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: [14] serialization instructions TSO [20] RMW p p p PSO [20] RMW, STBAR WO [5] synchronization RCsc [13, 12] p p p p release, acquire, nsync, RMW RCpc [13, 12] p p p p p release, acquire, nsync, RMW Alpha [19] p p p p RMO [21] various MEMBAR's PowerPC <ref> [17, 4] </ref> p p p p p SYNC p indicates that the corresponding relaxation is allowed by straightforward implementations of the corresponding model. It also indicates that the relaxation can be detected by the programmer (by affecting the results of the program) except for the following cases.
Reference: [5] <author> Michel Dubois, Christoph Scheurich, and Faye Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Relaxation W ! R W ! W R ! RW Read Others' Read Own Safety net Order Order Order Write Early Write Early SC [16] IBM 370 [14] serialization instructions TSO [20] RMW p p p PSO [20] RMW, STBAR WO <ref> [5] </ref> synchronization RCsc [13, 12] p p p p release, acquire, nsync, RMW RCpc [13, 12] p p p p p release, acquire, nsync, RMW Alpha [19] p p p p RMO [21] various MEMBAR's PowerPC [17, 4] p p p p p SYNC p indicates that the corresponding relaxation is
Reference: [6] <author> Kourosh Gharachorloo. </author> <title> Memory Consistency Models for Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1995. </year>
Reference-contexts: This view describes models in terms of program behavior, rather than in terms of hardware or compiler optimizations. Readers interested in further pursuing a more formal treatment of both the system-centric and programmer-centric views may refer to our previous work <ref> [1, 6, 8] </ref>. The rest of this article is organized as follows. We begin with a short note on who should be concerned with the memory consistency model of a system. <p> to acknowledge the invalidation or update message as soon as it is received by a processing node and potentially before the actual cache copy is affected; such a design can still satisfy sequential consistency as long as certain ordering constraints are observed in processing the incoming messages to the cache <ref> [6] </ref>. 5.2.3 Maintaining the Illusion of Atomicity for Writes While sequential consistency requires memory operations to appear atomic or instantaneous, propagating changes to multiple cache copies is inherently a non-atomic operation. <p> A more formal and unified system-centric framework to describe both hardware and software based models, along with a formal description of several models within the framework, appears in our previous work <ref> [8, 6] </ref>. <p> where virtually all optimizations that are used for uniprocessor programs can be safely applied. 7 An Alternate Abstraction for Relaxed Memory Models The flexibility provided by the relaxed memory models described in the previous section enables a wide range of performance optimizations that have been shown to improve performance substantially <ref> [9, 11, 6] </ref>. However, the higher performance is accompanied by a higher level of complexity for programmers. Furthermore, the wide range of models supported by different systems requires programmers to deal with various semantics that differ in subtle ways and complicates the task of porting programs across these systems. <p> The properly-labeled (PL) approach was provided along with the definition of release consistency (RCsc) as a simpler way to reason about the type of optimizations exploited by RCsc [13]. Programmer-centric approaches for exploiting more aggressive optimizations are described in our other work <ref> [7, 3, 1, 6] </ref>; a unified framework for designing programmer-centric models has also been developed and used to explore the design space of such models [1]. <p> of weak ordering on hardware that supports Alpha-like memory barriers, the compiler can precede and follow every synchronization operation with a memory barrier. 8 Discussion There is strong evidence that relaxed memory consistency models provide better performance than is possible with sequential consistency by enabling a number of hardware optimizations <ref> [9, 11, 6] </ref>. The increase in processor speeds relative to memory and communication speeds will only increase the potential benefit from these models. In 20 addition to providing performance gains at the hardware level, relaxed memory consistency models also play a key role in enabling important compiler optimizations.
Reference: [7] <author> Kourosh Gharachorloo, Sarita V. Adve, Anoop Gupta, John L. Hennessy, and Mark D. Hill. </author> <title> Programming for different memory consistency models. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4) </volume> <pages> 399-407, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The reasoning for how read-modify-write operations ensure the required program order or atomicity in the above models is beyond the scope of this paper <ref> [7] </ref>. There are some disadvantages to relying on a read-modify-write as a safety net in models such as TSO and PC. First, a system may not implement a general read-modify-write that can be used to appropriately replace any read or write. <p> The properly-labeled (PL) approach was provided along with the definition of release consistency (RCsc) as a simpler way to reason about the type of optimizations exploited by RCsc [13]. Programmer-centric approaches for exploiting more aggressive optimizations are described in our other work <ref> [7, 3, 1, 6] </ref>; a unified framework for designing programmer-centric models has also been developed and used to explore the design space of such models [1].
Reference: [8] <author> Kourosh Gharachorloo, Sarita V. Adve, Anoop Gupta, John L. Hennessy, and Mark D. Hill. </author> <title> Specifying system requirements for memory consistency models. </title> <type> Technical Report CSL-TR-93-594, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1993. </year> <note> Also available as Computer Sciences Technical Report #1199, </note> <institution> University of Wisconsin - Madison. </institution>
Reference-contexts: This view describes models in terms of program behavior, rather than in terms of hardware or compiler optimizations. Readers interested in further pursuing a more formal treatment of both the system-centric and programmer-centric views may refer to our previous work <ref> [1, 6, 8] </ref>. The rest of this article is organized as follows. We begin with a short note on who should be concerned with the memory consistency model of a system. <p> A more formal and unified system-centric framework to describe both hardware and software based models, along with a formal description of several models within the framework, appears in our previous work <ref> [8, 6] </ref>. <p> For instance, this relaxation is allowed by sequential consistency as long as all other program order and atomicity requirements are maintained <ref> [8] </ref>, which is why we did not discuss it in the previous section. Furthermore, this relaxation can be safely applied to all except one of the models discussed in this section. with mechanisms for overriding such relaxations. For example, explicit fence instructions may be provided to override program order relaxations. <p> Second, we assume all models enforce uniprocessor data and control dependences. Finally, models that relax the program order from reads to following write operations must also maintain a subtle form of multiprocessor data and control dependences <ref> [8, 1] </ref>; this latter constraint is inherently upheld by all processor designs we are aware of and can also be easily maintained by the compiler. 6.2 Relaxing the Write to Read Program Order The first set of models we discuss relax the program order constraints in the case of a write
Reference: [9] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Performance evaluation of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 245-257, </pages> <month> April </month> <year> 1991. </year> <month> 21 </month>
Reference-contexts: Furthermore, most programs do not frequently depend on the write to read program order or write atomicity for correctness. Relaxing the program order from a write followed by a read can improve performance substantially at the hardware level by effectively hiding the latency of write operations <ref> [9] </ref>. For compiler optimizations, however, this relaxation alone is not beneficial in practice. The reason is that reads and writes are usually finely interleaved in a program; therefore, most reordering optimizations effectively result in reordering with respect to both reads and writes. <p> where virtually all optimizations that are used for uniprocessor programs can be safely applied. 7 An Alternate Abstraction for Relaxed Memory Models The flexibility provided by the relaxed memory models described in the previous section enables a wide range of performance optimizations that have been shown to improve performance substantially <ref> [9, 11, 6] </ref>. However, the higher performance is accompanied by a higher level of complexity for programmers. Furthermore, the wide range of models supported by different systems requires programmers to deal with various semantics that differ in subtle ways and complicates the task of porting programs across these systems. <p> of weak ordering on hardware that supports Alpha-like memory barriers, the compiler can precede and follow every synchronization operation with a memory barrier. 8 Discussion There is strong evidence that relaxed memory consistency models provide better performance than is possible with sequential consistency by enabling a number of hardware optimizations <ref> [9, 11, 6] </ref>. The increase in processor speeds relative to memory and communication speeds will only increase the potential benefit from these models. In 20 addition to providing performance gains at the hardware level, relaxed memory consistency models also play a key role in enabling important compiler optimizations.
Reference: [10] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Two techniques to enhance the performance of memory consistency models. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages I:355-364, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: We first discuss two hardware techniques applicable to sequentially consistent systems with hardware support for cache coherence <ref> [10] </ref>. The first technique automatically prefetches ownership for any write operations that are delayed due to the program order requirement (e.g., by issuing prefetch-exclusive requests for any writes delayed in the write buffer), thus partially overlapping the service of the delayed writes with the operations preceding them in program order.
Reference: [11] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Hiding memory latency using dynamic scheduling in shared-memory multiprocessors. </title> <booktitle> In Proceeding of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 22-33, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In hardware, this flexibility provides the possibility of hiding the latency of read operations by implementing true non-blocking reads in the context of either static (in-order) or dynamic (out-of-order) scheduling processors, supported by techniques such as non-blocking (lockup-free) caches and speculative execution <ref> [11] </ref>. All of the models in this group allow a processor to read its own write early. However, RCpc and PowerPC are the only models whose straightforward implementations allow a read to return the value of another processor's write early. <p> where virtually all optimizations that are used for uniprocessor programs can be safely applied. 7 An Alternate Abstraction for Relaxed Memory Models The flexibility provided by the relaxed memory models described in the previous section enables a wide range of performance optimizations that have been shown to improve performance substantially <ref> [9, 11, 6] </ref>. However, the higher performance is accompanied by a higher level of complexity for programmers. Furthermore, the wide range of models supported by different systems requires programmers to deal with various semantics that differ in subtle ways and complicates the task of porting programs across these systems. <p> of weak ordering on hardware that supports Alpha-like memory barriers, the compiler can precede and follow every synchronization operation with a memory barrier. 8 Discussion There is strong evidence that relaxed memory consistency models provide better performance than is possible with sequential consistency by enabling a number of hardware optimizations <ref> [9, 11, 6] </ref>. The increase in processor speeds relative to memory and communication speeds will only increase the potential benefit from these models. In 20 addition to providing performance gains at the hardware level, relaxed memory consistency models also play a key role in enabling important compiler optimizations.
Reference: [12] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Revision to Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <type> Technical Report CSL-TR-93-568, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Relaxation W ! R W ! W R ! RW Read Others' Read Own Safety net Order Order Order Write Early Write Early SC [16] IBM 370 [14] serialization instructions TSO [20] RMW p p p PSO [20] RMW, STBAR WO [5] synchronization RCsc <ref> [13, 12] </ref> p p p p release, acquire, nsync, RMW RCpc [13, 12] p p p p p release, acquire, nsync, RMW Alpha [19] p p p p RMO [21] various MEMBAR's PowerPC [17, 4] p p p p p SYNC p indicates that the corresponding relaxation is allowed by straightforward <p> ! W R ! RW Read Others' Read Own Safety net Order Order Order Write Early Write Early SC [16] IBM 370 [14] serialization instructions TSO [20] RMW p p p PSO [20] RMW, STBAR WO [5] synchronization RCsc <ref> [13, 12] </ref> p p p p release, acquire, nsync, RMW RCpc [13, 12] p p p p p release, acquire, nsync, RMW Alpha [19] p p p p RMO [21] various MEMBAR's PowerPC [17, 4] p p p p p SYNC p indicates that the corresponding relaxation is allowed by straightforward implementations of the corresponding model.
Reference: [13] <author> Kourosh Gharachorloo, Dan Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: set of conditions commonly associated with a cache coherence protocol are: (1) a write is eventually made visible to all processors, and (2) writes to the same location appear to be seen in the same order by all processors (also referred to as serialization of writes to the same location) <ref> [13] </ref>. <p> Relaxation W ! R W ! W R ! RW Read Others' Read Own Safety net Order Order Order Write Early Write Early SC [16] IBM 370 [14] serialization instructions TSO [20] RMW p p p PSO [20] RMW, STBAR WO [5] synchronization RCsc <ref> [13, 12] </ref> p p p p release, acquire, nsync, RMW RCpc [13, 12] p p p p p release, acquire, nsync, RMW Alpha [19] p p p p RMO [21] various MEMBAR's PowerPC [17, 4] p p p p p SYNC p indicates that the corresponding relaxation is allowed by straightforward <p> ! W R ! RW Read Others' Read Own Safety net Order Order Order Write Early Write Early SC [16] IBM 370 [14] serialization instructions TSO [20] RMW p p p PSO [20] RMW, STBAR WO [5] synchronization RCsc <ref> [13, 12] </ref> p p p p release, acquire, nsync, RMW RCpc [13, 12] p p p p p release, acquire, nsync, RMW Alpha [19] p p p p RMO [21] various MEMBAR's PowerPC [17, 4] p p p p p SYNC p indicates that the corresponding relaxation is allowed by straightforward implementations of the corresponding model. <p> We should note that the RCsc model is also 16 accompanied by a higher level abstraction (described in Section 7) that relieves the need for the programmer to directly reason with the lower level specification for a large class of programs <ref> [13] </ref>. 6.4.3 Alpha, RMO, and PowerPC The Alpha, RMO, and PowerPC models all provide explicit fence instructions as their safety nets. The Alpha model provides two different fence instructions, the memory barrier (MB) and the write memory barrier (WMB). <p> The properly-labeled (PL) approach was provided along with the definition of release consistency (RCsc) as a simpler way to reason about the type of optimizations exploited by RCsc <ref> [13] </ref>. Programmer-centric approaches for exploiting more aggressive optimizations are described in our other work [7, 3, 1, 6]; a unified framework for designing programmer-centric models has also been developed and used to explore the design space of such models [1]. <p> In particular, the optimizations enabled by the weak ordering model can be safely applied. Furthermore, the information also enables more aggressive optimizations than exploited by weak ordering <ref> [2, 13, 1] </ref>. As shown in Figure 13, the programmer-centric framework requires the programmer to identify all operations that may be involved in a race as synchronization operations.
Reference: [14] <institution> IBM System/370 Principles of Operation. IBM, </institution> <month> May </month> <year> 1983. </year> <note> Publication Number GA22-7000-9, File Number S370-01. </note>
Reference-contexts: Relaxation W ! R W ! W R ! RW Read Others' Read Own Safety net Order Order Order Write Early Write Early SC [16] IBM 370 <ref> [14] </ref> serialization instructions TSO [20] RMW p p p PSO [20] RMW, STBAR WO [5] synchronization RCsc [13, 12] p p p p release, acquire, nsync, RMW RCpc [13, 12] p p p p p release, acquire, nsync, RMW Alpha [19] p p p p RMO [21] various MEMBAR's PowerPC [17,
Reference: [15] <author> Arvind Krishnamurthy and Katherine Yelick. </author> <title> Optimizing parallel SPMD programs. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <year> 1994. </year>
Reference-contexts: Such an analysis can be used to implement both hardware and compiler optimizations by reordering only those operation pairs that have been analyzed to be safe for reordering by the compiler. The algorithm by Shasha and Snir has exponential complexity <ref> [15] </ref>; more recently, a new algorithm has been proposed for SPMD programs with polynomial complexity [15]. <p> The algorithm by Shasha and Snir has exponential complexity <ref> [15] </ref>; more recently, a new algorithm has been proposed for SPMD programs with polynomial complexity [15]. However, both algorithms require global dependence analysis to determine if two operations from different processors can conflict (similar to alias analysis); 10 this analysis is difficult and often leads to conservative information which can decrease the effectiveness of the algorithm.
Reference: [16] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: algorithms do not depend on sequential consistency for correctness. 3 with a simple and intuitive model and yet allow a wide range of efficient system designs. 4 Understanding Sequential Consistency The most commonly assumed memory consistency model for shared memory multiprocessors is sequential consistency, formally defined by Lamport as follows <ref> [16] </ref>. Definition: [A multiprocessor system is sequentially consistent if] the result of any execution is the same as if the operations of all the processors were executed in some sequential order, and the operations of each individual processor appear in this sequence in the order specified by its program. <p> Relaxation W ! R W ! W R ! RW Read Others' Read Own Safety net Order Order Order Write Early Write Early SC <ref> [16] </ref> IBM 370 [14] serialization instructions TSO [20] RMW p p p PSO [20] RMW, STBAR WO [5] synchronization RCsc [13, 12] p p p p release, acquire, nsync, RMW RCpc [13, 12] p p p p p release, acquire, nsync, RMW Alpha [19] p p p p RMO [21] various
Reference: [17] <editor> Cathy May, Ed Silha, Rick Simpson, and Hank Warren, editors. </editor> <title> The PowerPC Architecture: A Specification for a New Family of RISC Processors. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1994. </year>
Reference-contexts: [14] serialization instructions TSO [20] RMW p p p PSO [20] RMW, STBAR WO [5] synchronization RCsc [13, 12] p p p p release, acquire, nsync, RMW RCpc [13, 12] p p p p p release, acquire, nsync, RMW Alpha [19] p p p p RMO [21] various MEMBAR's PowerPC <ref> [17, 4] </ref> p p p p p SYNC p indicates that the corresponding relaxation is allowed by straightforward implementations of the corresponding model. It also indicates that the relaxation can be detected by the programmer (by affecting the results of the program) except for the following cases.
Reference: [18] <author> Dennis Shasha and Marc Snir. </author> <title> Efficient and correct execution of parallel programs that share memory. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(2) </volume> <pages> 282-312, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: However, the above techniques are also beneficial when used in conjunction with relaxed memory consistency. Finally, Shasha and Snir developed a compiler algorithm to detect when memory operations can be reordered without violating sequential consistency <ref> [18] </ref>. Such an analysis can be used to implement both hardware and compiler optimizations by reordering only those operation pairs that have been analyzed to be safe for reordering by the compiler.
Reference: [19] <author> Richard L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <year> 1992. </year>
Reference-contexts: Order Order Write Early Write Early SC [16] IBM 370 [14] serialization instructions TSO [20] RMW p p p PSO [20] RMW, STBAR WO [5] synchronization RCsc [13, 12] p p p p release, acquire, nsync, RMW RCpc [13, 12] p p p p p release, acquire, nsync, RMW Alpha <ref> [19] </ref> p p p p RMO [21] various MEMBAR's PowerPC [17, 4] p p p p p SYNC p indicates that the corresponding relaxation is allowed by straightforward implementations of the corresponding model.
Reference: [20] <institution> The SPARC Architecture Manual. Sun Microsystems Inc., </institution> <month> January </month> <year> 1991. </year> <note> No. 800-199-12, Version 8. </note>
Reference-contexts: Relaxation W ! R W ! W R ! RW Read Others' Read Own Safety net Order Order Order Write Early Write Early SC [16] IBM 370 [14] serialization instructions TSO <ref> [20] </ref> RMW p p p PSO [20] RMW, STBAR WO [5] synchronization RCsc [13, 12] p p p p release, acquire, nsync, RMW RCpc [13, 12] p p p p p release, acquire, nsync, RMW Alpha [19] p p p p RMO [21] various MEMBAR's PowerPC [17, 4] p p p <p> Relaxation W ! R W ! W R ! RW Read Others' Read Own Safety net Order Order Order Write Early Write Early SC [16] IBM 370 [14] serialization instructions TSO <ref> [20] </ref> RMW p p p PSO [20] RMW, STBAR WO [5] synchronization RCsc [13, 12] p p p p release, acquire, nsync, RMW RCpc [13, 12] p p p p p release, acquire, nsync, RMW Alpha [19] p p p p RMO [21] various MEMBAR's PowerPC [17, 4] p p p p p SYNC p indicates that

References-found: 20

