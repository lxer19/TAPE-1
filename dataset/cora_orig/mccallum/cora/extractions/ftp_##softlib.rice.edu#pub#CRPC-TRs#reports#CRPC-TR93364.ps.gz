URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR93364.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Compiler Technology for Machine-Independent Parallel Programming expressive enough to multiply the productivity of the typical
Author: Ken Kennedy 
Keyword: Fortran, which support machine-independent expression of data parallelism.  
Note: 1 of 21  Center for Research on Parallel Computation  1.0 Introduction Compiler writers originally hoped that sufficiently powerful compilers would lead to languages  
Date: March 23, 1994  
Abstract: Historically, the principal achievement of compiler technology has been to make it possible to program in a high-level, machine-independent style. The absence of compiler technology to provide such a style for parallel computers is the main reason these systems have not found widespread acceptance. This paper examines the prospects for machine-independent parallel programming, concentrating on Fortran D and High Performance One of the principal goals of compiler technology has been the support of a machine-independent programming interface. The substantial personnel resources required to develop a significant application program make it imperative that the programming investment be protected against the changes in target machine architecture that are a fixture of the evolving computer industry. Without this protection, the cost of application development would be prohibitive, because the entire application would need to be redeveloped every two to three years as newer parallel machines become available. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Backus. </author> <title> The history of FORTRAN I, II and III. </title> <journal> ACM SIGPLAN Notices 13(8): </journal> <volume> 165180, </volume> <month> August </month> <year> 1978. </year>
Reference-contexts: If the generated code was too inefficient, the user would abandon FORTRAN and the language would be a failure. In a retrospective article on the implementation of the original FORTRAN compiler, John Backus observed <ref> [1] </ref>: It was our belief that if FORTRAN, during its first months, were to translate any reasonable scientific source program into an object program only half as fast as its hand-coded counterpart, then acceptance of our system would be in serious danger...
Reference: [2] <author> D. J. Kuck, R. H. Kuhn, B. Leasure and M. Wolfe. </author> <title> The structure of an advanced vectorizer for pipelined processors. </title> <booktitle> Proceedings of COMPSAC 80, The 4th International Computer Software and Applications Conference, </booktitle> <pages> pages 709715, </pages> <address> Chicago, Illinois, </address> <month> October </month> <year> 1980. </year>
Reference-contexts: In the recurrence example above, the statement in the loop body depends upon itself because the loop provides a path to itself and it uses a result from its incarnation on the previous iteration. Kucks basic vectorization principle can be paraphrased as follows <ref> [2] </ref>: Vectorization Principle. A statement in a loop can be directly vectorized with respect to that loop if that statement does not depend upon itself, either directly or indirectly.
Reference: [3] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Trans. on Programming Languages and Systems 9(4): </journal> <volume> 491542, </volume> <month> October </month> <year> 1987. </year> <title> References Compiler Technology for Machine-Independent Parallel Programming 19 of 21 </title>
Reference-contexts: Seeking coarse-grain parallelism is difficult because of the limitations of program analysis techniques. The theory of dependence again plays a central role in parallelization. A dependence is said to be carried by a loop if the source and sink of the dependence are on different iterations of that loop <ref> [3] </ref>. This definition is used in the following analog of the vectorization principle [9]: Parallelization Principle. A loop can be parallelized without inter-iteration synchroniza tion only if the loop carries no dependence. When parallelizing loops in an existing program, a compiler must be conservative.
Reference: [4] <author> K. Kennedy, K. S. McKinley and C. Tseng. </author> <title> Analysis and transformation in the ParaScope Editor. </title> <booktitle> Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference: [5] <author> D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> Conference Record of the Eighth Annual ACM Symposium on the Principals of Programming Languages, pages 207218, </booktitle> <address> Williamsburg, Virginia, </address> <month> January </month> <year> 1981. </year>
Reference: [6] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1989. </year>
Reference: [7] <author> H. Zima and B. Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference: [8] <author> C.-W. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> Ph.D. Dissertation, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Programs in CM Fortran optimized for the CM-2 do not achieve high performance on the CM-5. Currently, the best way to get high performance on the CM-5 is to rewrite the program completely in message-passing Fortran <ref> [8] </ref>. The absence of a standard, machine-independent programming interface for parallel computer systems is the principal reason that these systems have seen so little success in the commercial marketplace.
Reference: [9] <author> J. R. Allen, D. Callahan and K. Kennedy. </author> <title> Automatic decomposition of scientific programs for parallel execution. </title> <booktitle> Proceedings of the 14th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <address> Munich, Germany, </address> <pages> pages 6376, </pages> <month> January, </month> <year> 1987. </year>
Reference-contexts: The theory of dependence again plays a central role in parallelization. A dependence is said to be carried by a loop if the source and sink of the dependence are on different iterations of that loop [3]. This definition is used in the following analog of the vectorization principle <ref> [9] </ref>: Parallelization Principle. A loop can be parallelized without inter-iteration synchroniza tion only if the loop carries no dependence. When parallelizing loops in an existing program, a compiler must be conservative.
Reference: [10] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon and D. Walker. </author> <title> Solving Problems on Concurrent Processors, Volume 1. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: If the target is a distributed-memory machine, the data arrays in the program must be distributed to the individual processors in a way that minimizes the communication required during execution of the program. This data distribution problem is widely viewed as the fundamental challenge to programming distributed memory machines <ref> [10] </ref>. Although a substantial amount of research has been conducted on automatic methods for selecting data distributions, this problem is still considered too hard for practical compilers.
Reference: [11] <author> Parallel Computing Forum. </author> <title> PCF: parallel Fortran extensions. Fortran Forum 10(3), </title> <address> Septem-ber, </address> <year> 1991. </year>
Reference-contexts: Since the Parallel Computing Forum (PCF), an informal standardization group, proposed a standard for this style of extension, this language is often referred to as PCF Fortran <ref> [11] </ref>. Historically, parallel loop Fortrans were developed for uniform-access shared-memory parallel computer systems that used a bus to connect processors to global memory. Issues of data distribution are much less crucial on these machines, because each processor experiences a uniform access time to global memory.
Reference: [12] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing 2: </journal> <volume> 151169, </volume> <month> October </month> <year> 1988. </year>
Reference: [13] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Trans. on Parallel and Distributed Systems 2(4), </journal> <month> October </month> <year> 1991. </year>
Reference: [14] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> Proceedings of the SIGPLAN 89 Conference on Programming Language Design and Implementation, </booktitle> <address> Portland, Oregon, </address> <month> June </month> <year> 1989. </year>
Reference: [15] <author> M. Rosing, R. Schnabel and R. Weaver. </author> <title> Expressing complex parallel algorithms in DINO. </title> <booktitle> Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, California, </address> <month> March </month> <year> 1989. </year>
Reference: [16] <author> H. Zima, H.-J. Bast and M. Gerndt. </author> <title> SUPERB: a tool for semi-automatic MIMD/SIMD par-allelization. </title>
Reference: [17] <author> S. Hiranandani, K. Kennedy and C. Tseng, </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> Proceedings of the ACM 1992 International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <pages> pages 1-14, </pages> <month> July </month> <year> 1992. </year> <title> References Compiler Technology for Machine-Independent Parallel Programming 20 of 21 </title>
Reference: [18] <author> S. Hiranandani, K. Kennedy and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM 35(8): </journal> <volume> 6680, </volume> <month> August </month> <year> 1992. </year>
Reference: [19] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December, </month> <year> 1990. </year>
Reference-contexts: An experimental compiler constructed at Rice University, along with a number of earlier experimental systems have demonstrated the feasibility of this approach, at least for regular-mesh problems in which all subscripted variables are simple linear expressions [17,18]. 4.5.5 High Performance Fortran The original technical report on Fortran D <ref> [19] </ref> was released in December of 1989 and received widespread attention. A number of manufacturers expressed interest in producing an informal standard for a language called High Performance Fortran (HPF) based in part on the notion of data distribution.
Reference: [20] <author> R. Mirchandaney, J. Saltz, R. Smith, D. Nicol and K. Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference: [21] <author> J. Wu, J. Saltz, S. Hiranandani and H. Berryman. </author> <title> Runtime compilation methods for multi-computers. </title> <booktitle> Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, Illinois, </address> <month> August </month> <year> 1991. </year>
Reference: [22] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> Comm. ACM, </journal> <month> April </month> <year> 1989. </year>
Reference-contexts: Research is being conducted on a number of approaches to this problem, which I shall briey survey: 1. In Linda, the user is able to share data through a global tuple space, which can be written to and read from any process via special primitives <ref> [22] </ref>. Process synchronization is also pro vided via read and write primitives, as one form of the read primitive blocks when a tuple with the specified key does not yet exist in the tuple space.
Reference: [23] <author> A. Beguelin, J. Dongarra, G. A. Geist, R. Manchek and V. S. Sunderam. </author> <booktitle> Graphical development tools for network-based concurrent supercomputing.Proceedings of Supercomputing 91, </booktitle> <pages> pages 435-444, </pages> <address> Albuquerque, New Mexico, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: HeNCE, developed by Dongarra and his colleagues at the University of Tennessee and Oak Ridge National Laboratory, is an interactive graphical programming system that allows the user to specify synchronization and communication between subroutines in a Fortran or C program <ref> [23] </ref>. In the HeNCE graph, a node represents a process, which is an instantiation of some procedure, and each directed edge represents a communication or synchronization constraint.
Reference: [24] <author> A. Beguelin, J. Dongarra, A. Geist, R. Manchek and V. Sunderam. </author> <title> Opening the Door to Heterogeneous Network Supercomputing. </title> <booktitle> Supercomputing Review 4(9): </booktitle> <pages> 44-45, </pages> <year> 1991. </year>
Reference-contexts: The HeNCE system then generates a C program with calls to PVM (Parallel Virtual Machine), a message passing library <ref> [24] </ref>, for synchronization and communication. 3. CODE (computationally-oriented display environment), developed by J.C. Browne and his group at the University of Texas is an interactive graphical programming system, similar to HeNCE.
Reference: [25] <author> J.C. Browne. </author> <title> Software engineering of parallel programs in a computationally oriented display environment. </title> <editor> In D. Gelernter, A. Nicolau and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 7594. </pages> <publisher> MIT Press, </publisher> <address> Cambridge Mass., </address> <year> 1990. </year>
Reference-contexts: It provides a rich collection of communication types and it is also hierarchical, in the sense that nodes in a graph can represent a subgraph, expandable on demand. CODE can be used to generate programs in any of a variety of task-parallel programming languages <ref> [25] </ref>. 4. Fortran M (for modular) and Concurrent C++ (CC++) are extended languages developed by Mani Chandys group in the Center for Research on Parallel Computation [26,27]. These languages provide typed message passing, so that it can be established at compile time that a program is deterministic.
Reference: [26] <author> K. M. Chandy and C. Kesselman. </author> <title> C++: A Declarative Concurrent Object Oriented Programming Language. </title> <type> Technical Report CS-TR-92-01, </type> <institution> California Institute of Technology, Pasa-dena, California. </institution>
Reference: [27] <author> I. Foster and K. M. Chandy. </author> <title> Fortran M: a language for modular parallel programming. </title> <type> Preprint MCS-P327-0992, </type> <institution> Argonne National Lab, </institution> <year> 1992. </year>
Reference: [28] <author> F. E. Allen, M. Burke, R. Cytron, J. Ferrante, V. Sarkar and W. Hseih. </author> <title> A framework for determining useful parallelism. </title> <booktitle> Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference: [29] <author> F. E. Allen, M. Burke, P. Charles and R. Cytron. </author> <title> An overview of the PTRAN analysis system for multiprocessing. </title> <booktitle> Parallel and Distributed Computing 5: </booktitle> <pages> 617640. </pages>
Reference: [30] <author> M. Girkar and C. D. Polychronopoulos. </author> <title> Automatic extraction of functional parallelism from ordinary programs. </title> <journal> IEEE Trans. on Parallel and Distributed Systems 3(2): </journal> <volume> 166178, </volume> <month> March </month> <year> 1992. </year>
Reference: [31] <author> C. D. Polychronopoulos. </author> <title> Toward auto-scheduling compilers. </title> <journal> Journal of Supercomputing 2: </journal> <volume> 297330, </volume> <year> 1988. </year> <title> References Compiler Technology for Machine-Independent Parallel Programming 21 of 21 </title>
Reference: [32] <author> C. D. Polychronopoulos. </author> <title> The hierarchical task graph and its use in auto-scheduling. </title> <booktitle> Proceedings of the 1991 ACM International Conference on Supercomputing, pages 252 264,Cologne, </booktitle> <address> Germany, </address> <month> June </month> <year> 1991. </year>
Reference: [33] <author> C. D. Polychronopoulos, M. B. Girkar, M. R. Haghighat, C. L. Lee, Bruce P. Leung and D. A. Schouten. </author> <title> The structure of Parafrase-2: an advanced parallelizing compiler for C and Fortran. </title> <editor> In D. Gelernter, A. Nicolau and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 114125. </pages> <publisher> MIT Press, </publisher> <address> Cambridge Mass., </address> <year> 1990. </year>
Reference: [34] <author> N. Carriero and D. Gelernter. </author> <title> Tuple analysis and partial evaluation strategies in the Linda precompiler. </title> <editor> In D. Gelernter, A. Nicolau and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 114125. </pages> <publisher> MIT Press, </publisher> <address> Cambridge Mass., </address> <year> 1990. </year>
Reference-contexts: Outside the automatic task parallelization community, not much work on compiler optimization for task parallelism has been done, since most of the other approaches rely almost exclusively on run-time libraries. There are two exceptions. Linda has a precompiler that can optimize calls to Linda primitives in context <ref> [34] </ref> and the Center for Research on Parallel Computation has begun a project to unify Fortran M and CC++ with Fortran D, making it possible to use the Fortran D compiler technology to optimize programs that intermix task and data parallelism.
Reference: [35] <author> K. Sridharan, M. McShea, C. Denton, B. Eventoff, J. C. Browne, P. Newton, M. Ellis, D. Grossbard, T. Wise, and D. Clemmer. </author> <title> An environment for parallel structuring for Fortran programs. </title> <booktitle> Proc. 1989 International Conference on Parallel Processing, Volume II: Software, </booktitle> <pages> pages 98106, </pages> <address> Chicago, Illinois, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: In addition, a few restructuring environments such as E/SP, a commercially-available system from SES that is based on Brownes work <ref> [35] </ref>, have dealt with task parallelism. Summary and Conclusions Compiler Technology for Machine-Independent Parallel Programming 18 of 21 5.0 Summary and Conclusions Progress in compiler technology over the past few years has brought within reach a solution to the problem of writing machine-independent data-parallel programs.
Reference: [36] <author> J. K. Lee and D. Gannon. </author> <title> Object oriented parallel programming: experiments and results. </title> <booktitle> Proceedings of Supercomputing 91, </booktitle> <pages> pages 273282, </pages> <address> Albuquerque, New Mexico, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: If successful, Fortran D and High Performance Fortran may make it possible to maintain a single image of a Fortran program for a variety of parallel machines. A research project led by Dennis Gannon at Indiana University is developing pC++, which addresses the same problem for C++ programs <ref> [36] </ref>. When combined with process-based methods for specifying task parallelism, these languages and their associated compiler technology may represent a first step toward a truly comprehensive solution to the portable parallel programming support. Nevertheless, there is still much to do.
References-found: 36

