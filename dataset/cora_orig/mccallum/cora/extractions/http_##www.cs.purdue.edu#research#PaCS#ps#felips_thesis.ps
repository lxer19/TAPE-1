URL: http://www.cs.purdue.edu/research/PaCS/ps/felips_thesis.ps
Refering-URL: http://www.cs.purdue.edu/research/PaCS/parasol.html
Root-URL: http://www.cs.purdue.edu
Title: SOFTWARE ARCHITECTURES FOR FAULT-TOLERANT REPLICATIONS AND MULTITHREADED DECOMPOSITIONS: EXPERIMENTS WITH PRACTICAL PARALLEL SIMULATION  
Author: by Felipe Knop 
Degree: A Thesis Submitted to the Faculty of  In Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy  
Date: August 1996  
Affiliation: Purdue University  
Abstract-found: 0
Intro-found: 1
Reference: <institution> 181 BIBLIOGRAPHY </institution>
Reference: [AD92] <author> H.H. Ammar and S. Deng. </author> <title> Time Warp simulation using time scale decomposition. </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> 2(2) </volume> <pages> 158-177, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: This approach is exemplified by the queuing system shown in Figure 2.1, which depicts the model of a uniprocessor time-sharing computer system. This example has been adapted from <ref> [AD92] </ref>. As is usually done for spatial decomposition, the system under study is decomposed into active entities called physical processes that interact with each other. Each physical process is modeled by a logical process (LP), and their interaction is modeled by timestamped messages between logical processes. <p> The method requires that the simulation problem be posed using recurrence relations, so that techniques such as parallel prefix computation, parallel merging, and iterative folding can be applied. Ammar and Deng <ref> [AD92] </ref> have observed that time warp faces a large rollback overhead in systems where the duration of activities differs by several orders of magnitude, such as the system shown in Figure 2.1 (the terminals are usually much slower than the CPU and the disks).
Reference: [Bag91] <author> R.L. Bagrodia. </author> <title> Iterative design of efficient simulations using Maisie. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 243-247, </pages> <year> 1991. </year>
Reference-contexts: SPEEDES [Ste92] is a direct descendant of TWOS. It is written in C++ and has an object-oriented view. Like TWOS, SPEEDES is even-based. Some of its interesting features are the support to multiple synchronization protocols [Ste92] and its delta exchange mechanism to support incremental state saving [Ste93]. Maisie <ref> [BL90, Bag91] </ref> differs from the above systems because it is a C-based language, instead of offering a function (or method) call interface. Maisie also differs from other systems because it uses a process-interaction world view. This system is described in more detail in Section 5.1. <p> Among the existing PDES systems, Maisie <ref> [BL90, Bag91] </ref> is one of the best known. Maisie takes a language approach, adding simulation and communication constructs to the C language. Entities (the same as LPs) are used to model the static components of the physical system. <p> Although powerful, the wait-invoke-hold mechanism has some limitations| inherent to the active-server approach|that make programming somewhat non-intuitive. For example, consider the modeling of a closed queueing network presented in <ref> [Bag91] </ref>. Following the active-server paradigm, the servers are modeled by Maisie entities and the jobs by messages. <p> This natural approach, however, cannot be applied, since arriving job messages cannot be handled while the server is inside hold. Instead, to prevent the server from holding, the implementation in <ref> [Bag91] </ref> creates two types of entities: (1) the server, which receives job messages and computes their service completion times (taking into account the messages' arrival times and the other jobs being served) and (2) the router, which receives job messages from its associate server entity, holds for the remaining time between
Reference: [BC85] <author> J. Banks and J.S. Carson. </author> <booktitle> Process-interaction simulation languages. Simulation, </booktitle> <volume> 44(5) </volume> <pages> 225-235, </pages> <month> May </month> <year> 1985. </year>
Reference-contexts: To make ParaSol useful to a larger community, we have embarked on a project to parallelize a major (sequential) simulation language which uses our system as a foundation. GPSS [BKP76] was the language of choice because of its widespread use <ref> [BC85] </ref>, which also implies a large base of existing simulation programs. Since GPSS was not developed with parallel execution in mind, it offers a good test of the suitability of ParaSol for parallelizing sequential simulation packages.
Reference: [BC91] <author> K. Birman and R. Cooper. </author> <title> The Isis project: Real experience with a fault tolerant programming system. </title> <booktitle> Operating Systems Review, </booktitle> <pages> pages 103-107, </pages> <year> 1991. </year>
Reference-contexts: The outline for the original sequential program is shown in Figure 3.1. By using one of the parallel virtual machine environments available <ref> [Sun90, GL92, BC91, Tur93] </ref>, we can create a program where samples are produced in parallel, exploiting the computing power of all machines in a network. In our case, for reasons explained on Section 3.3, we use the Conch [Top92] distributed system. The resulting program is shown in Figure 3.2.
Reference: [BCM94] <author> M. Berry, J. Comiskey, and K. Minser. </author> <title> Parallel cluster analysis in landscape ecology. </title> <journal> IEEE Computational Science and Engineering, </journal> <volume> 1(2) </volume> <pages> 24-38, </pages> <month> Summer </month> <year> 1994. </year>
Reference-contexts: To demonstrate this, we describe here an application in parallel cluster labeling [KR95, KR96]. Cluster algorithms have application in diverse areas, including statistical mechanics of polymer solutions [NRS95, RMN94], spin models in physics [Bin84], and the study of ecological systems <ref> [BCM94] </ref>. The statistical mechanics of polymer solutions [Flo69], for example, is often studied by simulating a self-avoiding walk (SAW) on a randomly diluted 3-D lattice.
Reference: [BDO85] <author> B. Biles, D. Daniels, and T. O'Donnell. </author> <title> Statistical considerations in simulation on a network of microcomputers. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 388-393, </pages> <year> 1985. </year>
Reference-contexts: Biles et al. <ref> [BDO85] </ref> were probably the first to propose that replication-based simulation be executed on a network of microcomputers. They proposed the use of a tree of processors, where the processors lower down in the tree are divided in groups, each group simulating a different variant of the model.
Reference: [BDS + 88] <author> B. Beckman, M. DiLoreto, K. Sturdevant, P. Hontalas, V. Warren, L. Blume, D. Jefferson, and S. Bellenot. </author> <title> Distributed simulation and Time Warp part 1: </title> <booktitle> Design of Colliding Pucks. In Distributed Simulation, </booktitle> <pages> pages 56-60, </pages> <year> 1988. </year>
Reference-contexts: The primitives just outlined can also be used for non-queueing applications. For example, in the "colliding pucks" <ref> [BDS + 88] </ref> problem, each puck can be modeled by a transaction. Pucks, created with trCreate (), compute the time until their first collision and then execute trHold () for that time.
Reference: [Bel89] <author> G. Bell. </author> <title> The future of high performance computers in science and engineering. </title> <journal> Communications of the ACM, </journal> <volume> 32 </volume> <pages> 1091-1101, </pages> <year> 1989. </year>
Reference-contexts: It has been extensively used to acquire information about systems that are either impossible or impractical to study directly, or that are too complex to be analyzed by mathematical methods. Nobel laureate Ken Wilson characterizes <ref> [Bel89] </ref> computer simulation as the third paradigm in science, supplementing theory and experimentation.
Reference: [Bel90] <author> S. Bellenot. </author> <title> Global virtual time algorithms. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> volume 22, </volume> <pages> pages 122-127, </pages> <year> 1990. </year>
Reference-contexts: Computation of GVT involves all processes in the simulation. The computation is not trivial because the needed data is scattered among all processes and because of messages in transit. Schemes for computing GVT are described in <ref> [Bel90, DFW94, LL89a, SR93] </ref>. The main performance problems associated with time warp are state saving overhead, significant if the LP's state is large, and rollback overhead, which may seriously 15 impact performance if rollbacks are too frequent or poorly implemented.
Reference: [Bin84] <author> K. Binder, </author> <title> editor. </title> <booktitle> Topics in Current Physics, </booktitle> <pages> pages 1-36. </pages> <address> Number 36. </address> <publisher> Springer Verlag, </publisher> <year> 1984. </year>
Reference-contexts: To demonstrate this, we describe here an application in parallel cluster labeling [KR95, KR96]. Cluster algorithms have application in diverse areas, including statistical mechanics of polymer solutions [NRS95, RMN94], spin models in physics <ref> [Bin84] </ref>, and the study of ecological systems [BCM94]. The statistical mechanics of polymer solutions [Flo69], for example, is often studied by simulating a self-avoiding walk (SAW) on a randomly diluted 3-D lattice.
Reference: [BKP76] <author> P. Bobillier, B. Kahan, and A. Probst. </author> <title> Simulation with GPSS and GPSS V. </title> <publisher> Prentice Hall, </publisher> <year> 1976. </year> <month> 182 </month>
Reference-contexts: We also show, using performance experiments, that using the active-transaction approach does not incur a substantial runtime penalty. Finally, to demonstrate that our approach to model decomposition can lead to simpler modeling, we used ParaSol to develop the first parallel implementation of the widely used GPSS <ref> [BKP76] </ref> simulation language. 5 ParaSol is a joint project developed at the Parallel Computation and Simulation laboratory (PaCS) at the Department of Computer Sciences, Purdue University. 1.2 Organization of the Thesis The remainder of the thesis is organized as follows. <p> Process-interaction models typically are easier to develop than event-scheduling models. Some existing simulation packages (or languages) support process-interaction, 9 such as GPSS <ref> [BKP76] </ref>, CSIM [Sch86], and SIMAN [PSS90], some support event-scheduling, such as SIMSCRIPT II.5 [Joh74], and some support both, such as SIM-SCRIPT II.5 and Sol [Chu93]. Among packages supporting process-interaction, some adopt the active transaction approach [BKP76, Sch86] and some the active server approach [SSRT91]. <p> Some existing simulation packages (or languages) support process-interaction, 9 such as GPSS [BKP76], CSIM [Sch86], and SIMAN [PSS90], some support event-scheduling, such as SIMSCRIPT II.5 [Joh74], and some support both, such as SIM-SCRIPT II.5 and Sol [Chu93]. Among packages supporting process-interaction, some adopt the active transaction approach <ref> [BKP76, Sch86] </ref> and some the active server approach [SSRT91]. The active-transaction approach is present among many of the commercial simulation languages. 2.3 Parallel Simulation with Model Decomposition Several different parallel processing approaches have been used to speed up the evaluation of simulation models. <p> In a sequential simulation setting, the active-transaction approach, where the system's behavior is focused on the dynamic (mobile) components of the model, is adopted by several simulation languages and systems <ref> [BKP76, Sch86, PSS90] </ref>, some of which are heavily used in real-world simulations. Active-transaction programs often are simple and elegant. <p> Si was layered upon a distributed threads system that is capable of migrating threads from one processor to another. 97 Given the widespread use of languages like GPSS <ref> [BKP76] </ref> and tools like CSIM [Sch86], and their suitability in modeling significant physical systems, the utility of an active-transaction PDES system is unquestionable. <p> To make ParaSol useful to a larger community, we have embarked on a project to parallelize a major (sequential) simulation language which uses our system as a foundation. GPSS <ref> [BKP76] </ref> was the language of choice because of its widespread use [BC85], which also implies a large base of existing simulation programs. Since GPSS was not developed with parallel execution in mind, it offers a good test of the suitability of ParaSol for parallelizing sequential simulation packages. <p> This example (taken from <ref> [BKP76] </ref>) models a barber shop: a barber, represented by facility BARB, can serve only one customer at a time. Customers, represented by transactions, enter the shop and wait for service if the barber is busy. Customers are served in order of arrival. <p> The existence of the CEC may severely impact performance, since it assumes that all 155 transactions it holds will have to be rescanned every time some "significant" event, such as the release of a facility, occurs. While the rescan in a sequential setting is already recognized as inefficient <ref> [BKP76] </ref>, doing it in a distributed environment would certainly lead to bottlenecks: if the same scheme is adopted in a distributed implementation, the "significant" events would have to be reported to all processes, resulting in unacceptable communication cost.
Reference: [BL90] <author> R.L. Bagrodia and W.-T. Liao. Maisie: </author> <title> A language and optimizing environment for distributed simulation. </title> <booktitle> In Distributed Simulation, </booktitle> <pages> pages 205-210, </pages> <year> 1990. </year>
Reference-contexts: SPEEDES [Ste92] is a direct descendant of TWOS. It is written in C++ and has an object-oriented view. Like TWOS, SPEEDES is even-based. Some of its interesting features are the support to multiple synchronization protocols [Ste92] and its delta exchange mechanism to support incremental state saving [Ste93]. Maisie <ref> [BL90, Bag91] </ref> differs from the above systems because it is a C-based language, instead of offering a function (or method) call interface. Maisie also differs from other systems because it uses a process-interaction world view. This system is described in more detail in Section 5.1. <p> The use of events is encouraged by the close mapping between events and messages, given the ease with which the former is packed into the latter. Recognizing that the process-interaction view allows for an easier modeling effort for many problems, some researchers have produced process-based distributed simulation systems <ref> [BL90, MB95] </ref>. Though process-based, these systems still use messages for communication between sub-models. <p> Among the existing PDES systems, Maisie <ref> [BL90, Bag91] </ref> is one of the best known. Maisie takes a language approach, adding simulation and communication constructs to the C language. Entities (the same as LPs) are used to model the static components of the physical system.
Reference: [Ble93] <author> G.E. Blelloch. </author> <title> Prefix sums and their applications. </title> <editor> In J.H. Reif, editor, </editor> <title> Synthesis of Parallel Algorithms. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: The search in fc f ; c f+1 ; : : : ; c l g is performed using a binary search strategy. 4.5 Parallel Prefix Tree combining can be easily extended to implement a parallel prefix operation <ref> [KRS85, Ble93] </ref>. While still being able to take advantage of the virtual tree topology, the parallel prefix computation can be applied in many different areas. Some examples include [Ble93] parallel sorting, lexical analysis, and solving recurrences. <p> While still being able to take advantage of the virtual tree topology, the parallel prefix computation can be applied in many different areas. Some examples include <ref> [Ble93] </ref> parallel sorting, lexical analysis, and solving recurrences.
Reference: [BM93] <author> C. Burdorf and J. Marti. </author> <title> Load balancing strategies for Time Warp on multi-user workstations. </title> <journal> The Computer Journal, </journal> <volume> 36(2) </volume> <pages> 168-176, </pages> <year> 1993. </year>
Reference-contexts: Several attempts have been made to improve time warp's performance: * Avoid saving state at every event, at the cost of increasing the rollback cost [Fuj90a, RA94], or save state incrementally [CGU + 94, Ste93]. * Migrate LPs to balance the workload <ref> [RJ90, BM93, GT93] </ref>. * Decrease the number of rollbacks by introducing some degree of LP blocking, therefore limiting time warp's optimism [HT93, LSW89, SBW88, RJ89]. * Reduce the number of anti-messages by not sending them immediately after a straggler is received. <p> The design challenge for ParaSol designers is how to optimize performance for shared-memory architectures while still allowing the system to be used in workstation clusters, distributed-memory multiprocessors, and a combination of these. * Implementation of LP migration. This technique may be used to achieve load balancing <ref> [BM93, GT93] </ref>: LPs are migrated from one processor to another to balance the computational load among processors or to put in a cluster the LPs that communicate heavily with each other.
Reference: [Boy95] <editor> BoyanTech, </editor> <publisher> Inc., </publisher> <address> McLean, VA 22102. </address> <note> CPSim 1.0 User's Guide and Reference Manual, </note> <year> 1995. </year>
Reference-contexts: Maisie [BL90, Bag91] differs from the above systems because it is a C-based language, instead of offering a function (or method) call interface. Maisie also differs from other systems because it uses a process-interaction world view. This system is described in more detail in Section 5.1. CPSim <ref> [Gro95, Boy95] </ref> is a commercially available tool based on events and using a conservative synchronization protocol. It has been ported to machines ranging from IBM-PC compatibles to distributed memory multiprocessors like the CM-5.
Reference: [CGU + 94] <author> J. Cleary, F. Gomes, B. Unger, X. Zhonge, and R. Thudt. </author> <title> Cost of state saving and rollback. </title> <booktitle> In Proceedings of the Eighth Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 94-101, </pages> <year> 1994. </year>
Reference-contexts: Several attempts have been made to improve time warp's performance: * Avoid saving state at every event, at the cost of increasing the rollback cost [Fuj90a, RA94], or save state incrementally <ref> [CGU + 94, Ste93] </ref>. * Migrate LPs to balance the workload [RJ90, BM93, GT93]. * Decrease the number of rollbacks by introducing some degree of LP blocking, therefore limiting time warp's optimism [HT93, LSW89, SBW88, RJ89]. * Reduce the number of anti-messages by not sending them immediately after a straggler is
Reference: [Chu93] <author> K. Chung. </author> <title> A Concurrent Composite Computational Model for Stochastic Simulation. </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, Purdue University, </institution> <year> 1993. </year>
Reference-contexts: Process-interaction models typically are easier to develop than event-scheduling models. Some existing simulation packages (or languages) support process-interaction, 9 such as GPSS [BKP76], CSIM [Sch86], and SIMAN [PSS90], some support event-scheduling, such as SIMSCRIPT II.5 [Joh74], and some support both, such as SIM-SCRIPT II.5 and Sol <ref> [Chu93] </ref>. Among packages supporting process-interaction, some adopt the active transaction approach [BKP76, Sch86] and some the active server approach [SSRT91]. <p> The Sol (Simulation Object Library) system, resident at the uppermost layer, facilitates the construction of simulation models and other parallel applications in a variety of domains <ref> [Chu93, CSR93] </ref>. Sol is a C++ based library based on the object-oriented paradigm, designed to support the event-scheduling and process-interaction views. Sol provides applications with a variety of simulation event calendars and other data structures for the rapid development of simulation models. <p> While a CSIM facility represents a server and its associated queue, a QDL facility is more flexible, encompassing a set of servers and queues, as shown in Figure 5.8. The separation of facilities into servers and queues is suggested in <ref> [Chu93] </ref>. The user can customize QDL facilities by adding to them queues and servers and by specifying (1) which queue to use to enqueue a customer refused by a server and (2) which queue to dequeue when a server becomes available.
Reference: [CM92] <author> H. Clark and B. McMillin. </author> <title> DAWGS A distributed compute server utilizing idle workstations. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14 </volume> <pages> 175-186, </pages> <year> 1992. </year>
Reference-contexts: Some network computing systems provide transparent checkpoint and recovery, in the sense that no programming effort is required <ref> [CM92, LFS93, Sho91] </ref>. The drawback of this approach is a heavy checkpoint overhead, since the complete address space of all processes involved must be saved at each checkpoint.
Reference: [CS89] <author> K.M. Chandy and R. Sherman. </author> <title> Space-time and simulation. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 53-57, </pages> <year> 1989. </year>
Reference-contexts: After computing ST (N ) for the fast-event submodels, we can simulate the slow-event 17 one process to another. submodel by using the ST (N )'s as the mean service time of the composite queues and by simulating those queues with a processor-sharing discipline. Chandy and Sherman <ref> [CS89] </ref> have noticed that discrete-event simulators deal with the progress of time and that all methods work by filling a space-time rectangle.
Reference: [CS92] <author> B.A. Cota and R.G. Sargent. </author> <title> A modification of the process interaction world view. </title> <booktitle> ACM Transaction on Modeling and Computer Simulation, </booktitle> <volume> 2(2) </volume> <pages> 109-129, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: When several processes are scheduled to be reactivated, the process having the smallest reactivation time is chosen to execute. Process-interaction models may be developed in two alternative ways <ref> [Hen81, CS92] </ref>. In the first method, called the active server approach, the simulation focuses on the resources or static components of the model. In the second method, called the active transaction approach, the simulation focuses on the transactions that flow through the system.
Reference: [CSR93] <author> K. Chung, J. Sang, and V. Rego. Sol-es: </author> <title> An object-oriented platform for event-scheduled simulations. </title> <booktitle> In Proceedings of the Summer Simulation Conference, </booktitle> <year> 1993. </year>
Reference-contexts: The Sol (Simulation Object Library) system, resident at the uppermost layer, facilitates the construction of simulation models and other parallel applications in a variety of domains <ref> [Chu93, CSR93] </ref>. Sol is a C++ based library based on the object-oriented paradigm, designed to support the event-scheduling and process-interaction views. Sol provides applications with a variety of simulation event calendars and other data structures for the rapid development of simulation models.
Reference: [DFW94] <author> L.M. D'Souza, X. Fan, </author> <title> and P.A. Wilsey. pGVT: An algorithm for accurate gvt estimation. </title> <booktitle> In Proceedings of the Eighth Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 102-109, </pages> <year> 1994. </year>
Reference-contexts: Computation of GVT involves all processes in the simulation. The computation is not trivial because the needed data is scattered among all processes and because of messages in transit. Schemes for computing GVT are described in <ref> [Bel90, DFW94, LL89a, SR93] </ref>. The main performance problems associated with time warp are state saving overhead, significant if the LP's state is large, and rollback overhead, which may seriously 15 impact performance if rollbacks are too frequent or poorly implemented.
Reference: [Flo69] <author> P. J. Flory. </author> <title> Statistical Mechanics of Chain Molecules. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, </address> <year> 1969. </year>
Reference-contexts: To demonstrate this, we describe here an application in parallel cluster labeling [KR95, KR96]. Cluster algorithms have application in diverse areas, including statistical mechanics of polymer solutions [NRS95, RMN94], spin models in physics [Bin84], and the study of ecological systems [BCM94]. The statistical mechanics of polymer solutions <ref> [Flo69] </ref>, for example, is often studied by simulating a self-avoiding walk (SAW) on a randomly diluted 3-D lattice. First, a percolation cluster [Sta85] is created on a finite grid by randomly diluting the grid (i.e., marking sites as inaccessible to the chain) with probability q = 1 p &gt; 0.
Reference: [Fuj90a] <author> R.M. Fujimoto. </author> <title> Optimistic approaches to parallel discrete event simulation. </title> <journal> Transactions of the Society for Computer Simulation, </journal> <volume> 7(2) </volume> <pages> 153-191, </pages> <year> 1990. </year>
Reference-contexts: One important advantage offered by optimistic methods is their independence from application-specific knowledge, because there is no need to know each process's lookahead capability <ref> [Fuj90a] </ref>. The time warp mechanism, based on the virtual time paradigm, is the most well-known optimistic protocol [JBW + 87, Jef85]. <p> Several attempts have been made to improve time warp's performance: * Avoid saving state at every event, at the cost of increasing the rollback cost <ref> [Fuj90a, RA94] </ref>, or save state incrementally [CGU + 94, Ste93]. * Migrate LPs to balance the workload [RJ90, BM93, GT93]. * Decrease the number of rollbacks by introducing some degree of LP blocking, therefore limiting time warp's optimism [HT93, LSW89, SBW88, RJ89]. * Reduce the number of anti-messages by not sending <p> In the second case, the function is called when fossil collection goes beyond that point. In the third, func () is called when addCallback () is executed in coast forwarding (see <ref> [Fuj90a] </ref> or Section 5.4.2.5 for definition of coast forwarding). In addition to making the kernel smaller (addCallback () is simpler than the "safe" versions of malloc ()/free ()), addCallback () also provides useful functionality that can be exploited by domain layers and even application programmers. <p> This re-execution phase is called coast forwarding. By carefully controlling the tradeoff between state saving cost and re-execution cost, designers have been able to obtain performance improvements. 112 As described in <ref> [Fuj90a] </ref>, coast forwarding calendar entries is not the same as blindly executing them.
Reference: [Fuj90b] <author> R.M. Fujimoto. </author> <title> Parallel discrete event simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 30-53, </pages> <month> October </month> <year> 1990. </year> <month> 183 </month>
Reference-contexts: In recent years, 2 researchers have suggested various techniques to improve execution times with the use of parallel processing <ref> [Fuj90b] </ref>, in an attempt to take advantage of new parallel machines and networked computers. <p> Parallel simulation based on model decomposition is a hard problem <ref> [Fuj90b] </ref>, because sub-models need to communicate often, and because causal dependencies among 4 sub-models severely limit the available speedup. The research aimed at extracting better performance from model-decomposed applications has produced some good results, but at the cost of making parallel simulations harder to design than sequential simulations. <p> While some of the efforts are directed towards par-allelizing classes of simulation models, others take advantage of peculiarities existing in each model. A considerable research effort aims at parallelizing the execution of discrete-event simulation models <ref> [Fuj90b] </ref> (the term PDES, Parallel Discrete-Event Simulation, is often used). The main commonly adopted idea is to decompose the modeled system spatially, and then execute each part of the model on a different processor. <p> proven [Mis86] that messages from one LP to another, that is, those traveling through the same link, always have non-decreasing timestamp in conservative protocols. 12 LP blocking may lead to poor performance no useful work is done by a blocked LP and may also cause deadlocks when all LPs block <ref> [Fuj90b] </ref>. Deadlocks may occur even when there exist "safe" events to execute, because the LPs do not have global knowledge of all events in the system. Deadlocks can be broken by observing that the smallest timestamped event in the whole system is always safe to process. <p> When the transaction resumes from trHold (), it will already be executing a new calendar entry, or "event". As in time-warp, already executed calendar entries are kept in the calendar. A fossil collection mechanism <ref> [Jef85, Fuj90b] </ref> is used to discard unneeded entries. 5.4.2.4 State and State Saving The simulation state must be saved periodically in optimistic systems, so that a previously saved state can be restored in the event of a rollback. <p> Then, if the destination LP is in the future of the arriving thread, in terms of virtual time, a rollback procedure is immediately initiated. The mechanism used for reclaiming the space-time memory is similar to that used in fossil collection: values saved before GVT (Global Virtual Time <ref> [Fuj90b] </ref>) may be discarded since they are no longer needed. Notice, however, that all manipulation of the space-time memory is done at the domain level, independently of the kernel. The kernel could have provided all support for space-time memory by incorporating it into the state saving mechanisms.
Reference: [Fuj90c] <author> R.M. Fujimoto. </author> <title> Time Warp on a shared memory multiprocessor. </title> <journal> Transactions of the Society for Computer Simulation, </journal> <volume> 6(3) </volume> <pages> 211-239, </pages> <year> 1990. </year>
Reference-contexts: Instead of sending a thread message, the kernel accesses the destination LP's calendar to schedule the transaction directly, similar to what was done in <ref> [Fuj90c] </ref>. A copy of the transaction's context must still be made for state saving purposes, as is done for incoming external migrations. A pointer, as Figure 5.5 shows, is kept from the calendar entry that originated the migration to that created as a result of the migration. <p> We would like to investigate additions to the kernel that would also provide efficient support for these kinds of time-stepped applications, without significantly affecting either ParaSol's pro gramming interface or its design. * Shared-memory optimizations. Several changes to existing PDES mechanisms have been proposed for shared-memory architectures <ref> [Fuj90c, WLB88] </ref>. The design challenge for ParaSol designers is how to optimize performance for shared-memory architectures while still allowing the system to be used in workstation clusters, distributed-memory multiprocessors, and a combination of these. * Implementation of LP migration.
Reference: [Fuj93] <author> R.M. Fujimoto. </author> <title> Parallel discrete event simulation: </title> <journal> Will the field survive? ORSA Journal of Computing, </journal> <volume> 5(3) </volume> <pages> 213-230, </pages> <year> 1993. </year>
Reference-contexts: As a result, the acceptance of model decomposition tools by the simulation community has been small <ref> [Fuj93] </ref>. Our goal is to produce a model-decomposed parallel simulation tool that makes programming an application similar to sequential programming.
Reference: [Gaf88] <author> A. Gafni. </author> <title> Rollback mechanisms for optimistic distributed simulation systems. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 61-67, </pages> <year> 1988. </year>
Reference-contexts: The LP waits to see if the re-execution of the computation causes any of the same messages to be re-generated; if the same message is recreated, there is no need to cancel the original. This scheme is called lazy can cellation, as opposed to the standard aggressive cancellation <ref> [Gaf88, RFSJ90] </ref>. 2.3.3 Other Mechanisms In addition to the conservative and optimistic approaches, and to some variants of both, several other methods to parallelize simulation models have been developed. <p> When a migration occurs, control is immediately passed to the simulation driver, and this marks the end of the execution of a calendar entry. 113 unexecuted. If there is an anti-thread message stored, it is sent to the destination LP. 5.4.2.7 Lazy Cancellation Lazy cancellation <ref> [RFSJ90, Gaf88] </ref> is a technique used to reduce the number, and consequently the impact, of anti-messages sent during a rollback. When a calendar entry is unexecuted, the anti-messages are not sent.
Reference: [GFU + 95] <author> F. Gomes, S. Franks, B. Unger, Z. Xiao, J. Cleary, and A. Covington. SimKit: </author> <title> A high performance logical process simulation class in C++. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 706-713, </pages> <month> Decem-ber </month> <year> 1995. </year>
Reference-contexts: This system is described in more detail in Section 5.1. CPSim [Gro95, Boy95] is a commercially available tool based on events and using a conservative synchronization protocol. It has been ported to machines ranging from IBM-PC compatibles to distributed memory multiprocessors like the CM-5. SimKit <ref> [GFU + 95] </ref> is a C++ simulation class library, also based on events and fined tuned for performance. SimKit is optimized for shared-memory execution. The system is built atop the WarpKit parallel executive, a C++ interface to the Shared Memory Time Warp system.
Reference: [GL92] <author> B. Gropp and E. Lusk. </author> <title> A test implementation of the MPI draft message-passing standard. </title> <type> Technical report, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: The outline for the original sequential program is shown in Figure 3.1. By using one of the parallel virtual machine environments available <ref> [Sun90, GL92, BC91, Tur93] </ref>, we can create a program where samples are produced in parallel, exploiting the computing power of all machines in a network. In our case, for reasons explained on Section 3.3, we use the Conch [Top92] distributed system. The resulting program is shown in Figure 3.2.
Reference: [GLM91] <author> A.G. Greenberg, B.D. Lubachesvsky, and I. Mitrani. </author> <title> Algorithms for un-boundedly parallel simulations. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(3) </volume> <pages> 201-221, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: While the state-variable phase must be executed sequentially by the several processes, the event-scheduling phase may be executed concurrently by taking advantage of a careful pending-event set implementation (see also [Jon89]). Greenberg, Lubachesvsky, and Mitrani <ref> [GLM91] </ref> have proposed a shared memory method allowing the use of a larger number of processors than the number of objects 16 in the system being simulated.
Reference: [Gro95] <author> B. Groselj. CPSim: </author> <title> A tool for creating scalable discrete event simulations. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 579-583, </pages> <year> 1995. </year>
Reference-contexts: Maisie [BL90, Bag91] differs from the above systems because it is a C-based language, instead of offering a function (or method) call interface. Maisie also differs from other systems because it uses a process-interaction world view. This system is described in more detail in Section 5.1. CPSim <ref> [Gro95, Boy95] </ref> is a commercially available tool based on events and using a conservative synchronization protocol. It has been ported to machines ranging from IBM-PC compatibles to distributed memory multiprocessors like the CM-5.
Reference: [GT93] <author> D.W. Glazer and C. Tropper. </author> <title> On process migration and load balancing in Time Warp. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(3) </volume> <pages> 318-327, </pages> <year> 1993. </year>
Reference-contexts: Several attempts have been made to improve time warp's performance: * Avoid saving state at every event, at the cost of increasing the rollback cost [Fuj90a, RA94], or save state incrementally [CGU + 94, Ste93]. * Migrate LPs to balance the workload <ref> [RJ90, BM93, GT93] </ref>. * Decrease the number of rollbacks by introducing some degree of LP blocking, therefore limiting time warp's optimism [HT93, LSW89, SBW88, RJ89]. * Reduce the number of anti-messages by not sending them immediately after a straggler is received. <p> The design challenge for ParaSol designers is how to optimize performance for shared-memory architectures while still allowing the system to be used in workstation clusters, distributed-memory multiprocessors, and a combination of these. * Implementation of LP migration. This technique may be used to achieve load balancing <ref> [BM93, GT93] </ref>: LPs are migrated from one processor to another to balance the computational load among processors or to put in a cluster the LPs that communicate heavily with each other.
Reference: [Hei86] <author> P. Heidelberger. </author> <title> Statistical analysis of parallel simulators. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 220-295, </pages> <year> 1986. </year>
Reference-contexts: Consider the intuitive policy where processes repeatedly compute samples, with a master process collecting samples as they are generated and then computing the confidence interval to determine whether the simulation should terminate. This policy tends to benefit samples that are generated faster, and Heidelberger <ref> [Hei86, Hei88a] </ref> proves that this leads to a biased estimator, usually caused by a correlation between the time to generate a sample and the sample's value. Biased estimators should be avoided, even at a cost of a smaller speedup. <p> An interesting feature of the mechanism is that, unlike common discrete-event simulation, no feedback from the state update mechanism to the clock is necessary. Some theoretical work on executing multiple replications in parallel was performed by Heidelberger <ref> [Hei86, Hei88b] </ref>. This work focuses on determining when it is advantageous to execute replications in parallel, proposing different estimators for parallel execution. 26 3. <p> We presented two alternative policies, wait and no wait, that can be used to enforce the requirement. 93 5. DISTRIBUTING MODELS: THE PARASOL APPROACH Although providing a potentially lower speedup than model replication (see analysis in <ref> [Hei86] </ref>), model decomposition is the only alternative when the model is too large to fit a single machine's memory. Examples in this category include large queuing networks and large battlefield simulations. In this chapter, we investigate issues pertaining to model-decomposed parallel simulation systems.
Reference: [Hei88a] <author> P. Heidelberger. </author> <title> Discrete event simulations and parallel processing: Statistical properties. </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 9(6) </volume> <pages> 1114-1132, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: Consider the intuitive policy where processes repeatedly compute samples, with a master process collecting samples as they are generated and then computing the confidence interval to determine whether the simulation should terminate. This policy tends to benefit samples that are generated faster, and Heidelberger <ref> [Hei86, Hei88a] </ref> proves that this leads to a biased estimator, usually caused by a correlation between the time to generate a sample and the sample's value. Biased estimators should be avoided, even at a cost of a smaller speedup. <p> If the policy described above is used to collect samples, there will be a tendency for proportionally more samples with smaller values of cnt to be generated, resulting in ^ &lt; = (1 p)=p. In <ref> [Hei88a] </ref>, Heidelberger studied several estimators. <p> For an n-sampler system, D [M C (j)] generally corresponds to a state in which the monitor has received a cumulative total of j i results from sampler i, 1 i n. In the special case of tree-combining for replicative simulation applications, bias-free estimation <ref> [Hei88a] </ref> requires that j i = j for 1 i n (see Table 2.1). In other words, tree-combining forces the monitor to obtain the same number of samples from each sampler at the end of each phase. <p> In a deterministic computation, such missing data will guarantee results that are different from the results that would have been generated by a failure-free computation. Even in a stochastic computation, such missing data is guaranteed to bias estimates or cause result inconsistency <ref> [Hei88a] </ref>. Similar problems may occur also in the absence of tree-combining. For a few applications this aberrant behavior may be acceptable. For example, results generated by different samplers may be statistically indistinguishable from one another and therefore replaceable. For most applications, however, and in the interests of bias prevention [Hei88a], such <p> inconsistency <ref> [Hei88a] </ref>. Similar problems may occur also in the absence of tree-combining. For a few applications this aberrant behavior may be acceptable. For example, results generated by different samplers may be statistically indistinguishable from one another and therefore replaceable. For most applications, however, and in the interests of bias prevention [Hei88a], such behavior is unacceptable. 52 To avoid result inconsistency, we provide a "data versioning" scheme in EcliPSe. Several versions of checkpoint data are saved and, in the event of a process failure, system state is restored to the most recent stable state.
Reference: [Hei88b] <author> P. Heidelberger. </author> <title> Discrete event simulations and parallel processing: Statistical properties. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(6) </volume> <pages> 1114-1132, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: An interesting feature of the mechanism is that, unlike common discrete-event simulation, no feedback from the state update mechanism to the clock is necessary. Some theoretical work on executing multiple replications in parallel was performed by Heidelberger <ref> [Hei86, Hei88b] </ref>. This work focuses on determining when it is advantageous to execute replications in parallel, proposing different estimators for parallel execution. 26 3.
Reference: [Hen81] <author> J.O. Henriksen. </author> <title> GPSS Finding the appropriate world-view. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 505-516, </pages> <year> 1981. </year>
Reference-contexts: When several processes are scheduled to be reactivated, the process having the smallest reactivation time is chosen to execute. Process-interaction models may be developed in two alternative ways <ref> [Hen81, CS92] </ref>. In the first method, called the active server approach, the simulation focuses on the resources or static components of the model. In the second method, called the active transaction approach, the simulation focuses on the transactions that flow through the system. <p> As an aside, motivated by concerns of execution efficiency, Henriksen <ref> [Hen81] </ref> advocates the use of the active-server approach even for GPSS. The resulting programs, however, are harder to understand and to design than equivalent programs that use active transactions.
Reference: [HK76] <author> J. Hoshen and R. Kopelman. </author> <title> Percolation and cluster distribution. </title> <journal> Physical Review, </journal> <volume> B14(3438), </volume> <year> 1976. </year> <month> 184 </month>
Reference-contexts: The cluster labeling problem is stated as follows. Given a lattice M containing both occupied and unoccupied sites, identify distinct clusters and give each a unique label, such as an integer id. 74 4.4.2 A Sequential Algorithm A well-known sequential solution <ref> [HK76] </ref> to the cluster labeling problem is as follows. Traverse M row by row, in increasing row-order, assigning to M [i; j] the label assigned to M [i 1; j] if both sites are occupied.
Reference: [HT93] <author> D.O. Hammes and A. Tripathi. </author> <title> Investigations in adaptive distributed simulation. </title> <type> Technical Report TR 93-79, </type> <institution> Computer Science Department, University of Minnesota, </institution> <year> 1993. </year>
Reference-contexts: state at every event, at the cost of increasing the rollback cost [Fuj90a, RA94], or save state incrementally [CGU + 94, Ste93]. * Migrate LPs to balance the workload [RJ90, BM93, GT93]. * Decrease the number of rollbacks by introducing some degree of LP blocking, therefore limiting time warp's optimism <ref> [HT93, LSW89, SBW88, RJ89] </ref>. * Reduce the number of anti-messages by not sending them immediately after a straggler is received.
Reference: [Int93] <author> Intel Corporation. </author> <title> Paragon User's Guide, </title> <month> October </month> <year> 1993. </year> <title> Order number: </title> <type> 312489-002. </type>
Reference-contexts: In an alternative view (active-transaction approach) the jobs could be modeled as active entities, while the terminals, CPU, and disks would be considered passive entities. 10 messages in the simulation. 11 If the execution environment provides no shared memory among processors, as in multicomputers such as the Intel Paragon <ref> [Int93] </ref> or a network of workstations, all interactions between logical processes are achieved through messages, since the use of shared variables is not practical. Without using shared variables, each logical process also ends up having its own simulation clock and proceeding at its own pace.
Reference: [JBW + 87] <author> D. Jefferson, B. Beckman, F. Wieland, L. Blume, M. DiLoreto, P. Hon-talas, P. Laroche, K. Sturdevant, J. Tupman, V. Warren, J. Wedel, H. Younger, and S. Bellenot. </author> <title> The time Warp operating system. </title> <booktitle> In Proceedings of the Eleventh Symposium on Operating Systems Principles, </booktitle> <pages> pages 77-93, </pages> <year> 1987. </year>
Reference-contexts: One important advantage offered by optimistic methods is their independence from application-specific knowledge, because there is no need to know each process's lookahead capability [Fuj90a]. The time warp mechanism, based on the virtual time paradigm, is the most well-known optimistic protocol <ref> [JBW + 87, Jef85] </ref>. <p> The algorithm terminates when a fixed point is reached. 18 2.3.4 Software Systems Supporting Model Decomposition While there is substantial research in several aspects of model-decomposed parallel simulation, few parallel simulation tools are available. We survey some of these tools. The Time Warp Operating System (TWOS) <ref> [JBW + 87] </ref> was one of the first major implementations of the time warp mechanism [Jef85]. TWOS is event based; logical processes exchange timestamped event messages. Unlike some other PDES systems, TWOS runs on top of the bare hardware. <p> The parallel implementation of the event list is obtained with mechanisms similar to those in time warp <ref> [JBW + 87] </ref>. As a case study, the paper presents the implementation of the SIMSCRIPT II.5 language [Joh74] for a shared-memory environment. In [NH94], Nicol and Heidelberger describe a different approach: because existing simulation tools are large and complex, the tools must be parallelized with little or no modification.
Reference: [JCRB89] <author> D.W. Jones, C.-C. Chou, D. Renk, </author> <title> and S.C. Bruell. Experience with concurrent simulation. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 756-763, </pages> <year> 1989. </year>
Reference-contexts: Jones has proposed a shared memory mechanism called concurrent simulation <ref> [Jon86, JCRB89] </ref>, that explores concurrent access to a shared pending event-set to obtain speed up in a parallel execution.
Reference: [Jef85] <author> D. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <year> 1985. </year>
Reference-contexts: One important advantage offered by optimistic methods is their independence from application-specific knowledge, because there is no need to know each process's lookahead capability [Fuj90a]. The time warp mechanism, based on the virtual time paradigm, is the most well-known optimistic protocol <ref> [JBW + 87, Jef85] </ref>. <p> We survey some of these tools. The Time Warp Operating System (TWOS) [JBW + 87] was one of the first major implementations of the time warp mechanism <ref> [Jef85] </ref>. TWOS is event based; logical processes exchange timestamped event messages. Unlike some other PDES systems, TWOS runs on top of the bare hardware. <p> It is still a matter of debate as to which is better suitable to applications, considering ease of modeling, modularity, and implementation efficiency. When PDES was first introduced <ref> [Jef85, Mis86] </ref>, the path taken to parallel execution of discrete-event simulations was: From sequential ! To parallel Shared state ! Distributed state (partitioned among processors) Single simulation clock ! Distributed simulation clock Single simulation calendar ! Distributed simulation calendar Schedule events [event type, time] in calendar ! Send timestamped messages; recipient <p> In a queueing simulation, for instance, LP A may send a "customer" message with timestamp t to LP B, indicating that a customer left server A 110 at simulated time t and then moved to server B. An anti-message <ref> [Jef85] </ref> is sent from LP A to LP B when computation in LP A is rolled back before time t. The equivalent to a message in the active transaction approach is the migration of a transaction, which at the lowest levels is implemented as a message. <p> When the transaction resumes from trHold (), it will already be executing a new calendar entry, or "event". As in time-warp, already executed calendar entries are kept in the calendar. A fossil collection mechanism <ref> [Jef85, Fuj90b] </ref> is used to discard unneeded entries. 5.4.2.4 State and State Saving The simulation state must be saved periodically in optimistic systems, so that a previously saved state can be restored in the event of a rollback.
Reference: [Joh74] <author> G. Johnson. </author> <title> SIMSCRIPT II.5 User's Manual (Release 8). </title> <publisher> CACI Inc., </publisher> <year> 1974. </year>
Reference-contexts: Process-interaction models typically are easier to develop than event-scheduling models. Some existing simulation packages (or languages) support process-interaction, 9 such as GPSS [BKP76], CSIM [Sch86], and SIMAN [PSS90], some support event-scheduling, such as SIMSCRIPT II.5 <ref> [Joh74] </ref>, and some support both, such as SIM-SCRIPT II.5 and Sol [Chu93]. Among packages supporting process-interaction, some adopt the active transaction approach [BKP76, Sch86] and some the active server approach [SSRT91]. <p> The parallel implementation of the event list is obtained with mechanisms similar to those in time warp [JBW + 87]. As a case study, the paper presents the implementation of the SIMSCRIPT II.5 language <ref> [Joh74] </ref> for a shared-memory environment. In [NH94], Nicol and Heidelberger describe a different approach: because existing simulation tools are large and complex, the tools must be parallelized with little or no modification.
Reference: [Jon86] <author> D.W. Jones. </author> <title> Concurrent simulation: An alternative to distributed simulation. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 417-423, </pages> <year> 1986. </year>
Reference-contexts: Jones has proposed a shared memory mechanism called concurrent simulation <ref> [Jon86, JCRB89] </ref>, that explores concurrent access to a shared pending event-set to obtain speed up in a parallel execution.
Reference: [Jon89] <author> D.W. Jones. </author> <title> Concurrent operations on priority queues. </title> <journal> Communications of the ACM, </journal> <volume> 32(1) </volume> <pages> 132-137, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: While the state-variable phase must be executed sequentially by the several processes, the event-scheduling phase may be executed concurrently by taking advantage of a careful pending-event set implementation (see also <ref> [Jon89] </ref>). Greenberg, Lubachesvsky, and Mitrani [GLM91] have proposed a shared memory method allowing the use of a larger number of processors than the number of objects 16 in the system being simulated.
Reference: [KMR96] <author> F. Knop, E. Mascarenhas, and V. Rego. </author> <title> A parallel GPSS based on the ParaSol simulation system. </title> <booktitle> In Proceedings of the Winter Simulation Conference (to apear), </booktitle> <year> 1996. </year>
Reference-contexts: This chapter presents details on the GPSS-to-ParaSol mapping, challenges involved in the task, and some of the solutions adopted to increase the available parallelism. Additional details can be found in <ref> [KMR96] </ref>. This chapter is organized as follows. Section 6.1 presents an overview of the GPSS language and highlights its main commands. Section 6.2 describes how programs in this language are mapped into ParaSol.
Reference: [KMRS94] <author> F. Knop, E. Mascarenhas, V. Rego, and V. Sunderam. </author> <title> An introduction to fault tolerant parallel simulation with EcliPSe. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 700-707, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Our work on replication-based parallel simulation has resulted in the EcliPSe system <ref> [KMRS94, KMRS95, KRS96, KRSF94] </ref>, a toolkit for fault-tolerant parallel simulation based on the replication paradigm. We use the toolkit as a testbed to evaluate our approach towards confronting the issues mentioned above. The chapter is organized as follows.
Reference: [KMRS95] <author> F. Knop, E. Mascarenhas, V. Rego, and V. Sunderam. </author> <title> Fail-safe concurrent simulation with EcliPSe: An introduction. </title> <journal> Simulation Practice and Theory, </journal> <volume> 3 </volume> <pages> 121-146, </pages> <year> 1995. </year>
Reference-contexts: Our work on replication-based parallel simulation has resulted in the EcliPSe system <ref> [KMRS94, KMRS95, KRS96, KRSF94] </ref>, a toolkit for fault-tolerant parallel simulation based on the replication paradigm. We use the toolkit as a testbed to evaluate our approach towards confronting the issues mentioned above. The chapter is organized as follows. <p> One example would be automation of the termination procedures based on unbiased estimators. Going in a slightly different direction, we could exploit graphical tools that help in the generation of EcliPSe applications, such as the work reported in <ref> [KMRS95] </ref>. 178 The following are some directions for continued work in model decomposition. * A vast area for future investigations with ParaSol is the development of new application domain libraries.
Reference: [KR95] <author> F. Knop and V. Rego. </author> <title> Parallel cluster labeling on a network of workstations. </title> <booktitle> In Proceedings of the Thirteenth Brazilian Symposium on Computer Networks, </booktitle> <pages> pages 533-548, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: The monitor, located at the root of the tree, receives the average and sample variances of the values generated by all nodes. 4.4 Cluster Labeling The tree-combining mechanism can also be applied with more complex operators. To demonstrate this, we describe here an application in parallel cluster labeling <ref> [KR95, KR96] </ref>. Cluster algorithms have application in diverse areas, including statistical mechanics of polymer solutions [NRS95, RMN94], spin models in physics [Bin84], and the study of ecological systems [BCM94]. <p> We investigated how the program scales with the number of machines used. Other experiments are reported in <ref> [KR95, KR96] </ref>. In generating clusters, we kept the value of p = 0:31 (p is the probability of filling each slot in the lattice) fixed throughout. We chose this number because it is near criticality and, according to percolation theory [Sta85], tends to yield a cluster spanning the entire grid.
Reference: [KR96] <author> F. Knop and V. Rego. </author> <title> Parallel labeling of three-dimensional clusters on networks of workstations. </title> <note> Submitted for publication, 1996. 185 </note>
Reference-contexts: The monitor, located at the root of the tree, receives the average and sample variances of the values generated by all nodes. 4.4 Cluster Labeling The tree-combining mechanism can also be applied with more complex operators. To demonstrate this, we describe here an application in parallel cluster labeling <ref> [KR95, KR96] </ref>. Cluster algorithms have application in diverse areas, including statistical mechanics of polymer solutions [NRS95, RMN94], spin models in physics [Bin84], and the study of ecological systems [BCM94]. <p> Details on how this is done, based on a depth-first search, can be found in [NRS95, NM92]. One important step of the algorithm is the identification of the clusters. We describe the problem in terms of 2-D grids. The implementation for 3-D grids is presented in <ref> [KR96] </ref>. 4.4.1 The Cluster Labeling Problem Let M be an N fi N 2-D lattice of sites, where each site M [i; j] (0 i; j &lt; N ) is either "occupied" or "unoccupied". <p> We investigated how the program scales with the number of machines used. Other experiments are reported in <ref> [KR95, KR96] </ref>. In generating clusters, we kept the value of p = 0:31 (p is the probability of filling each slot in the lattice) fixed throughout. We chose this number because it is near criticality and, according to percolation theory [Sta85], tends to yield a cluster spanning the entire grid.
Reference: [KRS85] <author> C.P. Kruskal, L. Rudolph, and M. Snir. </author> <title> The power of parallel prefix. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(10):965-968, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: The search in fc f ; c f+1 ; : : : ; c l g is performed using a binary search strategy. 4.5 Parallel Prefix Tree combining can be easily extended to implement a parallel prefix operation <ref> [KRS85, Ble93] </ref>. While still being able to take advantage of the virtual tree topology, the parallel prefix computation can be applied in many different areas. Some examples include [Ble93] parallel sorting, lexical analysis, and solving recurrences.
Reference: [KRS94] <author> F. Knop, V. Rego, and V. Sunderam. </author> <title> EcliPSe: A system for fault-tolerant replicative computations. </title> <booktitle> In Proceedings of the IEEE/USP International Symposium on High-Performance Computing, </booktitle> <pages> pages 17-34, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: The effectiveness of the features above is evaluated through performance experiments described in section 3.5. 3.1.3 Performance Monitoring and Optimization EcliPSe applications support a variety of computation structures (see <ref> [KRS94] </ref>) and execute on a number of machine environments. Bottlenecks in a distributed application, however, may impair execution performance, resulting in a waste of computational resources.
Reference: [KRS96] <author> F. Knop, V. Rego, and V. Sunderam. </author> <title> Fail-safe concurrency in the EcliPSe system. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 8(4) </volume> <pages> 283-312, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Our work on replication-based parallel simulation has resulted in the EcliPSe system <ref> [KMRS94, KMRS95, KRS96, KRSF94] </ref>, a toolkit for fault-tolerant parallel simulation based on the replication paradigm. We use the toolkit as a testbed to evaluate our approach towards confronting the issues mentioned above. The chapter is organized as follows. <p> Using a simple application as an example, we explain why there is generally no need to save much data at a checkpoint, and de scribe modifications required to a user program to make it fault tolerant. Fault tolerance issues in EcliPSe are further discussed in <ref> [KRSF94, KRS96] </ref>. 45 3.4.1 Basic Approach To allow recovery from a process or machine failure, a checkpoint-rollback mechanism is used: data from all processes is periodically saved and then later restored should a failure occur, with a new process being created to replace that which failed. <p> Details on how state-saving is accomplished and on the algorithm used to deter mine when old system state can be safely discarded are given in <ref> [KRS96] </ref>. 3.4.5 Replacement of a Failed Process We describe here the procedure used for recovering from a failed process. <p> The mechanism used for failure notification is actually based on normal data messages containing this new sequence number, sent from the monitor to the samplers. Details of the procedures adopted by all processes involved are presented in <ref> [KRS96] </ref>. 3.4.6 Heuristics for the Detection of Infinite Loops When the monitor finds a sampler silent (i.e., has not sent data to the monitor) for a period of time that is incompatible with its past pattern of data transmissions, the system assumes that the process has entered an infinite loop. <p> The constant work per sampler is primarily caused by the sending of checkpoint data to samplers (to allow their recovery), and to a lesser extent because of other tasks such as handling the data structures needed to implement the data-versioning mechanism <ref> [KRS96] </ref>.
Reference: [KRSF94] <author> F. Knop, V. Rego, V. Sunderam, and A. Ferrari. </author> <title> Failure-resilient computations in the EcliPSe system. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages III-184-III-187, </pages> <year> 1994. </year>
Reference-contexts: Our work on replication-based parallel simulation has resulted in the EcliPSe system <ref> [KMRS94, KMRS95, KRS96, KRSF94] </ref>, a toolkit for fault-tolerant parallel simulation based on the replication paradigm. We use the toolkit as a testbed to evaluate our approach towards confronting the issues mentioned above. The chapter is organized as follows. <p> Using a simple application as an example, we explain why there is generally no need to save much data at a checkpoint, and de scribe modifications required to a user program to make it fault tolerant. Fault tolerance issues in EcliPSe are further discussed in <ref> [KRSF94, KRS96] </ref>. 45 3.4.1 Basic Approach To allow recovery from a process or machine failure, a checkpoint-rollback mechanism is used: data from all processes is periodically saved and then later restored should a failure occur, with a new process being created to replace that which failed.
Reference: [LFS93] <author> J. Leon, A.L. Fisher, and P. Steenkiste. </author> <title> Fail-safe PVM: A portable package for distributed programming with transparent recovery. </title> <type> Technical Report CMU-CS-93-124, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: Some network computing systems provide transparent checkpoint and recovery, in the sense that no programming effort is required <ref> [CM92, LFS93, Sho91] </ref>. The drawback of this approach is a heavy checkpoint overhead, since the complete address space of all processes involved must be saved at each checkpoint.
Reference: [Lin94] <author> Y.-B. Lin. </author> <title> Parallel independent replicated simulation on a network of workstations. </title> <booktitle> In Proceedings of the Eighth Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 73-80, </pages> <year> 1994. </year>
Reference-contexts: Several experiments involving Monte Carlo and discrete-event simulations, performed in [RS92, SR91], suggested that an almost linear speedup can be obtained with the EcliPSe approach. Lin <ref> [Lin94] </ref> has done theoretical and experimental work on replicated simulation on a network of workstations. He used task migration to offset the performance penalty caused by slow processors or those presenting a heavy load because of "alien" processes sharing the CPU.
Reference: [LK91] <author> A.M. Law and W.D. </author> <title> Kelton. Simulation Modeling & Analysis. </title> <publisher> McGraw-Hill, </publisher> <year> 1991. </year>
Reference-contexts: Nobel laureate Ken Wilson characterizes [Bel89] computer simulation as the third paradigm in science, supplementing theory and experimentation. Law and Kelton <ref> [LK91] </ref> list the following kinds of problems for which simulation has been found to be a useful and powerful tool: * Designing and analyzing manufacturing systems * Evaluating hardware and software requirements for a computer system * Evaluating a new military weapons system or tactic * Determining ordering policies for an <p> In a discrete system, state variables change instantaneously at distinct points in time. In a continuous system, state variables change continuously with time. Few systems in practice are completely discrete or continuous, but usually one type of variable change predominates. Simulation models can be classified along three different dimensions <ref> [LK91] </ref>: 7 * Static vs. Dynamic simulation models: A static simulation model is a representation of a system at a particular time or a representation where time plays no role (e.g., classical Monte Carlo models [Sob74]). <p> Each replication produces a sample X i of some parameter having an unknown value . After computing N replications, we can estimate by ^ = ( P N strong law of large numbers <ref> [LK91] </ref> guarantees that ^ ! as N ! 1. That is, as the number of replications increases, we obtain more precise estimates of a parameter. <p> Biased estimators should be avoided, even at a cost of a smaller speedup. For an example of biased estimation, consider a program that generates samples from the Geometric distribution <ref> [LK91] </ref>. This distribution corresponds to the number of failures before the first success in a sequence of independent Bernoulli trials (that is, trials with only two possible outcomes: success or failure) with probability p of success on each trial.
Reference: [LL89a] <author> Y.-B. Lin and E.D. Lazowska. </author> <title> Determining the global virtual time in a distributed simulation. </title> <type> Technical Report 90-01-02, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Computation of GVT involves all processes in the simulation. The computation is not trivial because the needed data is scattered among all processes and because of messages in transit. Schemes for computing GVT are described in <ref> [Bel90, DFW94, LL89a, SR93] </ref>. The main performance problems associated with time warp are state saving overhead, significant if the LP's state is large, and rollback overhead, which may seriously 15 impact performance if rollbacks are too frequent or poorly implemented.
Reference: [LL89b] <author> Y.-B. Lin and E.D. Lazowska. </author> <title> Exploiting lookahead in parallel simulation. </title> <type> Technical Report 89-10-06, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: Lookahead information can also be used to avoid deadlocks. The major drawback of conservative methods is their reliance on knowledge of the application to determine which events are safe to process. Determining the lookahead capability of each application may be a complex task (see <ref> [LL89b] </ref>, for example). 2.3.2 Optimistic Synchronization Mechanisms Optimistic mechanisms avoid LP blocking and deadlock by using a detection and recovery paradigm: causality errors are initially allowed, but a rollback mechanism is triggered whenever they occur.
Reference: [LSW89] <author> B. Lubachevsky, A. Shwartz, and A. Weiss. </author> <title> Rollback sometimes works ... if filtered. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 630-639, </pages> <year> 1989. </year>
Reference-contexts: state at every event, at the cost of increasing the rollback cost [Fuj90a, RA94], or save state incrementally [CGU + 94, Ste93]. * Migrate LPs to balance the workload [RJ90, BM93, GT93]. * Decrease the number of rollbacks by introducing some degree of LP blocking, therefore limiting time warp's optimism <ref> [HT93, LSW89, SBW88, RJ89] </ref>. * Reduce the number of anti-messages by not sending them immediately after a straggler is received.
Reference: [Mac87] <author> M.H. MacDougall. </author> <title> Simulating Computer Systems Techniques and Tools. </title> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: When simulating a vacationing server, a token transaction that is resumed goes on "vacation" (that is, holds for some amount of virtual time), reactivates the server, and then suspends itself again. When simulating a token-ring network <ref> [Mac87] </ref>, the token transaction moves to another server and reactivates it. QDL. Some of these are explained in the next section. 5.5.2 Implementation Aspects: QDL Kernel Interface Like other domain-specific libraries, QDL is implemented using kernel primitives. Domain libraries can also serve as a basis for other domain libraries.
Reference: [Mas96] <author> E. Mascarenhas. </author> <title> A System for Multithreaded Parallel Simulation and Computation with Migrant Threads and Objects. </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, Purdue University, </institution> <year> 1996. </year>
Reference-contexts: Our approach is to pursue solutions leading to powerful, yet easy to use tools that can be adopted by the simulation community. To validate our solutions, we and colleagues have created the ParaSol parallel simulation system <ref> [MKR95, Mas96] </ref>. ParaSol uses migrating threads to build environments that can be used in various simulation domains. This chapter is organized as follows. Section 5.1 justifies our choice of the world view presented to the user. <p> QDL. Some of these are explained in the next section. 5.5.2 Implementation Aspects: QDL Kernel Interface Like other domain-specific libraries, QDL is implemented using kernel primitives. Domain libraries can also serve as a basis for other domain libraries. The synchronization library <ref> [Mas96] </ref>, for instance, provides synchronization objects such as events and mailboxes that can be used to build other domain-specific libraries. QDL's main class, Facility, is derived from class GlobalObject; this is needed to allow facilities to be globally located and their state to be saved. <p> We found EcliPSe to be useful not only for replicative applications, but also for many structured parallel programs with one-to-many and many-to-one communication patterns. General interprocess communication is also supported with help from Conch. In collaboration with Edward Mascarenhas <ref> [Mas96] </ref> and Vernon Rego, we built the ParaSol distributed simulation tool to facilitate the development of parallel discrete-event simulations. ParaSol offers a programming model different from other systems.
Reference: [MB95] <author> J.M. Martin and R. Bagrodia. COMPOSE: </author> <title> An object-oriented environment for parallel discrete-event simulations. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 763-767, </pages> <month> December </month> <year> 1995. </year> <month> 186 </month>
Reference-contexts: The use of events is encouraged by the close mapping between events and messages, given the ease with which the former is packed into the latter. Recognizing that the process-interaction view allows for an easier modeling effort for many problems, some researchers have produced process-based distributed simulation systems <ref> [BL90, MB95] </ref>. Though process-based, these systems still use messages for communication between sub-models.
Reference: [Mis86] <author> J. Misra. </author> <title> Distributed discrete-event simulation. </title> <journal> Computing Surveys, </journal> <volume> 18(1) </volume> <pages> 39-65, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: Causality errors cannot be allowed, since they may result in an incorrect simulation. The mechanisms created to avoid causality errors broadly fall into two categories: the conservative and the optimistic mechanisms, and these are described in the following sections. 2.3.1 Conservative Synchronization Mechanisms Conservative mechanisms <ref> [Mis86] </ref> work by prohibiting the occurrence of causality errors. An LP does not process an event until it determines that no other event (s) containing a smaller timestamp will arrive after this event is processed. <p> Storing the timestamp of the last message received from each LP (called the link clock) is enough to avoid causality errors, since it can be proven <ref> [Mis86] </ref> that messages from one LP to another, that is, those traveling through the same link, always have non-decreasing timestamp in conservative protocols. 12 LP blocking may lead to poor performance no useful work is done by a blocked LP and may also cause deadlocks when all LPs block [Fuj90b]. <p> It is still a matter of debate as to which is better suitable to applications, considering ease of modeling, modularity, and implementation efficiency. When PDES was first introduced <ref> [Jef85, Mis86] </ref>, the path taken to parallel execution of discrete-event simulations was: From sequential ! To parallel Shared state ! Distributed state (partitioned among processors) Single simulation clock ! Distributed simulation clock Single simulation calendar ! Distributed simulation calendar Schedule events [event type, time] in calendar ! Send timestamped messages; recipient
Reference: [MKR95] <author> E. Mascarenhas, F. Knop, and V. Rego. ParaSol: </author> <title> A multithreaded system for parallel simulation based on mobile threads. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 690-697, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Our approach is to pursue solutions leading to powerful, yet easy to use tools that can be adopted by the simulation community. To validate our solutions, we and colleagues have created the ParaSol parallel simulation system <ref> [MKR95, Mas96] </ref>. ParaSol uses migrating threads to build environments that can be used in various simulation domains. This chapter is organized as follows. Section 5.1 justifies our choice of the world view presented to the user.
Reference: [MR95] <author> E. Mascarenhas and V. Rego. </author> <title> Migrant threads on process farms: Parallel programming with Ariadne. </title> <type> Technical Report TR-95-081, </type> <institution> Purdue University, Department of Computer Science, </institution> <year> 1995. </year>
Reference-contexts: Instead of embedding a threads system in the simulation kernel, as done for example in CSIM [Sch86], we have opted to consider the threads system as a separate entity with a clean interface. This allows us to (1) use the threads system independently of ParaSol (as done in <ref> [MR95] </ref>) and (2) replace the threads system in ParaSol by another one if so desired. We are currently using Ariadne [MR96] as the threads system, because of it supports features needed by ParaSol. These features are detailed in Section 5.4.7.
Reference: [MR96] <author> E. Mascarenhas and V. Rego. Ariadne: </author> <title> Architecture of a portable threads system supporting thread migration. </title> <journal> Software Practice and Experience, </journal> <volume> 26(3) </volume> <pages> 327-356, </pages> <year> 1996. </year>
Reference-contexts: Also, being an experimental system, Conch could accommodate changes to improve its support of EcliPSe. The use of layering can be further extended, with EcliPSe itself being regarded as a foundation, on top of which complex replicative applications may be designed. Ariadne <ref> [MRS94, MR96] </ref> is an efficient lightweight process library designed to provide support for a variety of concurrency constructs at the Conch, EcliPSe, and 43 Sol layers. <p> This allows us to (1) use the threads system independently of ParaSol (as done in [MR95]) and (2) replace the threads system in ParaSol by another one if so desired. We are currently using Ariadne <ref> [MR96] </ref> as the threads system, because of it supports features needed by ParaSol. These features are detailed in Section 5.4.7. Through use of a communication system, different processes, perhaps located on different machines, may exchange data with one another. <p> In this interface, each system visualizes the other asymmetrically. While the kernel sees the threads system, currently Ariadne <ref> [MR96] </ref>, as a provider of thread primitives, the threads system treats the kernel as a simple thread scheduler: every time a transaction/thread suspends itself, control goes back to the threads system, which invokes the kernel's driver () function.
Reference: [MRS94] <author> E. Mascarenhas, V. Rego, and V. Sunderam. </author> <title> Ariadne user manual. </title> <type> Technical Report CSD-TR 94-081, </type> <institution> Purdue University, </institution> <year> 1994. </year>
Reference-contexts: Also, being an experimental system, Conch could accommodate changes to improve its support of EcliPSe. The use of layering can be further extended, with EcliPSe itself being regarded as a foundation, on top of which complex replicative applications may be designed. Ariadne <ref> [MRS94, MR96] </ref> is an efficient lightweight process library designed to provide support for a variety of concurrency constructs at the Conch, EcliPSe, and 43 Sol layers.
Reference: [MRS95] <author> E. Mascarenhas, V. Rego, and J. Sang. </author> <title> DISplay: A system for visual-interaction in distributed simulations. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 698-705, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Some bottlenecks are easily circumvented by a small change in the program or execution environment, provided the source of the bottleneck can be found. To address the problem of locating execution bottlenecks, we make use of the DISplay tool <ref> [MRS95] </ref> for the interactive display of graphical performance data collected and displayed on-the-fly during an application run. <p> The buttons shown under the title Interaction allow a user to interact with an ongoing EcliPSe application. The DISplay library provides a user with a simple interface, allowing for calls that can create application specific dialogs and receive input from the user <ref> [MRS95] </ref>. Using the Setup panel and clicking on options available, a user can select/deselect the plotting of histograms for CPU occupation-level, CPU load, in/out packet rate, etc. an M/GI/1 queue, using eight samplers and one monitor, distributed over a network of six machines (nodes). <p> A similar primitive, display (), sends data to the graphical display server <ref> [MRS95] </ref>. 5.3.6 User Callbacks There are situations where it may be useful for layers above the kernel to be aware of the underlying synchronization mechanism. As an example, consider memory allocation.
Reference: [Mur89] <author> T. Murata. </author> <title> Petri nets: Properties, analysis and applications. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(4) </volume> <pages> 541-580, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Not only will this fulfill ParaSol's promise to cater to different application domains, but it will also help us evaluate some of our design decisions, such as the kernel-domain libraries interface. Two examples of domain libraries that can be developed atop the kernel are Petri nets <ref> [Mur89] </ref> and circuit simulation. When ParaSol is used to simulate Petri nets, places and transitions are modeled by LPs, and tokens are modeled by transactions.
Reference: [NH94] <author> D. Nicol and P. Heidelberger. </author> <title> On extending parallelism to serial simulators. </title> <type> Technical Report ICASE Report No. 94-95, </type> <institution> Institute for Computer Applications in Science and Engineering - NASA Langley Research Center, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: The parallel implementation of the event list is obtained with mechanisms similar to those in time warp [JBW + 87]. As a case study, the paper presents the implementation of the SIMSCRIPT II.5 language [Joh74] for a shared-memory environment. In <ref> [NH94] </ref>, Nicol and Heidelberger describe a different approach: because existing simulation tools are large and complex, the tools must be parallelized with little or no modification. On the other hand, this implies that the model designed for parallel simulation may have to be changed from that used for sequential execution.
Reference: [NM92] <author> H. Nakanishi and J. Moon. </author> <title> Self-avoiding walks on critical percolation cluster. </title> <journal> Physica A, </journal> <volume> 191 </volume> <pages> 309-312, </pages> <year> 1992. </year>
Reference-contexts: Once this disordered cluster has been identified, a starting point of an SAW is chosen randomly, and all SAWs with a given number of steps N are generated. Details on how this is done, based on a depth-first search, can be found in <ref> [NRS95, NM92] </ref>. One important step of the algorithm is the identification of the clusters. We describe the problem in terms of 2-D grids.
Reference: [NRS95] <author> H. Nakanishi, V. Rego, and V. Sunderam. </author> <title> On the effectiveness of super-concurrent computations on heterogeneous networks. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 24 </volume> <pages> 177-190, </pages> <year> 1995. </year>
Reference-contexts: To demonstrate this, we describe here an application in parallel cluster labeling [KR95, KR96]. Cluster algorithms have application in diverse areas, including statistical mechanics of polymer solutions <ref> [NRS95, RMN94] </ref>, spin models in physics [Bin84], and the study of ecological systems [BCM94]. The statistical mechanics of polymer solutions [Flo69], for example, is often studied by simulating a self-avoiding walk (SAW) on a randomly diluted 3-D lattice. <p> Once this disordered cluster has been identified, a starting point of an SAW is chosen randomly, and all SAWs with a given number of steps N are generated. Details on how this is done, based on a depth-first search, can be found in <ref> [NRS95, NM92] </ref>. One important step of the algorithm is the identification of the clusters. We describe the problem in terms of 2-D grids.
Reference: [Pra91] <author> S. Prata. </author> <title> C++ Primer Plus. The Waite Group, </title> <year> 1991. </year>
Reference-contexts: An option exists that allows objects to work in an "always dirty" mode, when the optimization is turned off. The implementation of ParaSol's data structures was facilitated by the use of the template construct in C++ <ref> [Pra91] </ref>, which enabled the reuse of code for fundamental data structures such as linked lists and hash queues.
Reference: [PSS90] <author> C.D. Pegden, R.E. Shannon, and R.P. Sadowski. </author> <title> Introduction to Simulation Using SIMAN. </title> <publisher> McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: Process-interaction models typically are easier to develop than event-scheduling models. Some existing simulation packages (or languages) support process-interaction, 9 such as GPSS [BKP76], CSIM [Sch86], and SIMAN <ref> [PSS90] </ref>, some support event-scheduling, such as SIMSCRIPT II.5 [Joh74], and some support both, such as SIM-SCRIPT II.5 and Sol [Chu93]. Among packages supporting process-interaction, some adopt the active transaction approach [BKP76, Sch86] and some the active server approach [SSRT91]. <p> In a sequential simulation setting, the active-transaction approach, where the system's behavior is focused on the dynamic (mobile) components of the model, is adopted by several simulation languages and systems <ref> [BKP76, Sch86, PSS90] </ref>, some of which are heavily used in real-world simulations. Active-transaction programs often are simple and elegant.
Reference: [PYM94] <author> K. Pawlikowski, V. Yau, and D. McNickle. </author> <title> Distributed stochastic discrete-event simulation in parallel time streams. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 723-730, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Some theoretical work was also conducted to determine (a) the effect of slow processes, and when it is advantageous to use extra available machines, and (b) the effect of workload changes. Pawlikowski and others <ref> [PYM94] </ref> have developed the AKAROA system for simulating multiple replications in parallel. This work is based on the "Spectral Analysis in Parallel Time Streams" technique. Somewhat similar to the model replication approach, the Single Clock Multiple System (SCMS) [Vak92] technique allows simulation of coupled trajectories on SIMD machines.
Reference: [RA94] <author> R. Ronngran and R. Ayani. </author> <title> Adaptive checkpointing in Time Warp. </title> <booktitle> In Proceedings of the Eighth Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 110-117, </pages> <year> 1994. </year> <month> 187 </month>
Reference-contexts: Several attempts have been made to improve time warp's performance: * Avoid saving state at every event, at the cost of increasing the rollback cost <ref> [Fuj90a, RA94] </ref>, or save state incrementally [CGU + 94, Ste93]. * Migrate LPs to balance the workload [RJ90, BM93, GT93]. * Decrease the number of rollbacks by introducing some degree of LP blocking, therefore limiting time warp's optimism [HT93, LSW89, SBW88, RJ89]. * Reduce the number of anti-messages by not sending
Reference: [RFSJ90] <author> P.L. Reiher, R. Fujimoto, S.Bellenot, and D. Jefferson. </author> <title> Cancellation strategies in optimistic execution systems. </title> <booktitle> In Proceedings of the SCS Mul-ticonference on Distributed Simulation, </booktitle> <volume> volume 22, </volume> <pages> pages 112-121, </pages> <year> 1990. </year>
Reference-contexts: The LP waits to see if the re-execution of the computation causes any of the same messages to be re-generated; if the same message is recreated, there is no need to cancel the original. This scheme is called lazy can cellation, as opposed to the standard aggressive cancellation <ref> [Gaf88, RFSJ90] </ref>. 2.3.3 Other Mechanisms In addition to the conservative and optimistic approaches, and to some variants of both, several other methods to parallelize simulation models have been developed. <p> When a migration occurs, control is immediately passed to the simulation driver, and this marks the end of the execution of a calendar entry. 113 unexecuted. If there is an anti-thread message stored, it is sent to the destination LP. 5.4.2.7 Lazy Cancellation Lazy cancellation <ref> [RFSJ90, Gaf88] </ref> is a technique used to reduce the number, and consequently the impact, of anti-messages sent during a rollback. When a calendar entry is unexecuted, the anti-messages are not sent.
Reference: [Ric81] <author> J.R. Rice. </author> <title> Matrix Computations and Mathematical Software. </title> <publisher> McGraw-Hill, </publisher> <year> 1981. </year>
Reference-contexts: Figure 3.6 shows the core of an EcliPSe program used for solving a linear system Ax = b using the Jacobi iterative method <ref> [Ric81] </ref>. Although not particularly efficient as far as convergence characteristics are concerned, this method has the desirable property that processes are allowed to compute independently of each other.
Reference: [RJ89] <author> P.L. Reiher and D. Jefferson. </author> <title> Limitation of optimism in the Time Warp operating system. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 765-770, </pages> <year> 1989. </year>
Reference-contexts: state at every event, at the cost of increasing the rollback cost [Fuj90a, RA94], or save state incrementally [CGU + 94, Ste93]. * Migrate LPs to balance the workload [RJ90, BM93, GT93]. * Decrease the number of rollbacks by introducing some degree of LP blocking, therefore limiting time warp's optimism <ref> [HT93, LSW89, SBW88, RJ89] </ref>. * Reduce the number of anti-messages by not sending them immediately after a straggler is received.
Reference: [RJ90] <author> P.L. Reiher and D. Jefferson. </author> <title> Virtual time based dynamic load management in the Time Warp operating system. </title> <booktitle> In Proceedings of the SCS Mul-ticonference on Distributed Simulation, </booktitle> <volume> volume 22, </volume> <pages> pages 103-111, </pages> <year> 1990. </year>
Reference-contexts: Several attempts have been made to improve time warp's performance: * Avoid saving state at every event, at the cost of increasing the rollback cost [Fuj90a, RA94], or save state incrementally [CGU + 94, Ste93]. * Migrate LPs to balance the workload <ref> [RJ90, BM93, GT93] </ref>. * Decrease the number of rollbacks by introducing some degree of LP blocking, therefore limiting time warp's optimism [HT93, LSW89, SBW88, RJ89]. * Reduce the number of anti-messages by not sending them immediately after a straggler is received.
Reference: [RMN94] <author> M.D. Rintoul, J. Moon, and H. Nakanishi. </author> <title> Statistics of self-avoiding walks on randomly diluted lattice. </title> <journal> Physical Review E, </journal> <volume> 49 </volume> <pages> 2790-2803, </pages> <year> 1994. </year>
Reference-contexts: EcliPSe has proven to be a useful tool: simple replicative applications were par-allelized, made fault-tolerant, and executed on a workstation network in a matter of a few hours. Large scale applications such as that described in <ref> [RMN94] </ref> have also been successfully ported to EcliPSe. Experiments conducted on up to 128 workstations confirm EcliPSe's good performance, with its scalability-improving features being essential in large runs. <p> To demonstrate this, we describe here an application in parallel cluster labeling [KR95, KR96]. Cluster algorithms have application in diverse areas, including statistical mechanics of polymer solutions <ref> [NRS95, RMN94] </ref>, spin models in physics [Bin84], and the study of ecological systems [BCM94]. The statistical mechanics of polymer solutions [Flo69], for example, is often studied by simulating a self-avoiding walk (SAW) on a randomly diluted 3-D lattice.
Reference: [RS92] <author> V. J. Rego and V. S. Sunderam. </author> <title> Experiments in concurrent stochastic simulation: The EcliPSe paradigm. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14(1) </volume> <pages> 66-84, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Processes higher in the tree combine results produced by their descendents and also perform statistical and optimization functions. The root of the tree is responsible for controlling the whole procedure. The EcliPSe prototype <ref> [RS92, SR91] </ref> has been developed to support replicative simulation programs and also some applications based on the data-parallel paradigm. 23 Table 2.1 Main estimators for parallel execution and their bias. P is the number of processes and t is the time when the simulation is ordered to stop. <p> Several experiments involving Monte Carlo and discrete-event simulations, performed in <ref> [RS92, SR91] </ref>, suggested that an almost linear speedup can be obtained with the EcliPSe approach. Lin [Lin94] has done theoretical and experimental work on replicated simulation on a network of workstations. <p> Based on batch-means, the statistics sent to the monitor include 57 mean system delay and maximum number of customers found in the queue. The regenerative method may also be used (as was done in <ref> [RS92] </ref>) to estimate these quantities. The monitor applies results from independent parallel replications to build a confidence interval for both statistics. Though times between the reporting of samples may be large, the monitor can be overworked if a large number of processes is used.
Reference: [San94] <author> J. Sang. </author> <title> Multithreading in Distributed Memory Systems and Simulation: Design, Implementation, and Experiments. </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, Purdue University, </institution> <year> 1994. </year>
Reference-contexts: Using active transactions in distributed simulations is not trivial, however, since it requires the ability to move computations (the active transactions), rather than just data, across processor boundaries. Nevertheless, Sang and others <ref> [San94, SMR96] </ref> proved this approach to be viable by building Si, a conservative PDES system based on the active-transaction and process-interaction world view.
Reference: [SBW88] <author> L.M. Sokol, </author> <title> D.P. Briscoe, and A.P. Wieland. MTW: A strategy for scheduling discrete simulation events for concurrent execution. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 34-42, </pages> <year> 1988. </year>
Reference-contexts: state at every event, at the cost of increasing the rollback cost [Fuj90a, RA94], or save state incrementally [CGU + 94, Ste93]. * Migrate LPs to balance the workload [RJ90, BM93, GT93]. * Decrease the number of rollbacks by introducing some degree of LP blocking, therefore limiting time warp's optimism <ref> [HT93, LSW89, SBW88, RJ89] </ref>. * Reduce the number of anti-messages by not sending them immediately after a straggler is received.
Reference: [Sch86] <author> H.D. Schwetman. CSIM: </author> <title> A C-based, process-oriented simulation language. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 387-396, </pages> <year> 1986. </year>
Reference-contexts: Process-interaction models typically are easier to develop than event-scheduling models. Some existing simulation packages (or languages) support process-interaction, 9 such as GPSS [BKP76], CSIM <ref> [Sch86] </ref>, and SIMAN [PSS90], some support event-scheduling, such as SIMSCRIPT II.5 [Joh74], and some support both, such as SIM-SCRIPT II.5 and Sol [Chu93]. Among packages supporting process-interaction, some adopt the active transaction approach [BKP76, Sch86] and some the active server approach [SSRT91]. <p> Some existing simulation packages (or languages) support process-interaction, 9 such as GPSS [BKP76], CSIM [Sch86], and SIMAN [PSS90], some support event-scheduling, such as SIMSCRIPT II.5 [Joh74], and some support both, such as SIM-SCRIPT II.5 and Sol [Chu93]. Among packages supporting process-interaction, some adopt the active transaction approach <ref> [BKP76, Sch86] </ref> and some the active server approach [SSRT91]. The active-transaction approach is present among many of the commercial simulation languages. 2.3 Parallel Simulation with Model Decomposition Several different parallel processing approaches have been used to speed up the evaluation of simulation models. <p> This constrains the synchronization mechanism to be conservative. The computation and dissemination of lookahead information needed by conservative mechanisms to achieve speedup is encapsulated in the extensions. The proposed approach is applied to produce a library that extends the CSIM simulation package <ref> [Sch86] </ref>. <p> In a sequential simulation setting, the active-transaction approach, where the system's behavior is focused on the dynamic (mobile) components of the model, is adopted by several simulation languages and systems <ref> [BKP76, Sch86, PSS90] </ref>, some of which are heavily used in real-world simulations. Active-transaction programs often are simple and elegant. <p> Si was layered upon a distributed threads system that is capable of migrating threads from one processor to another. 97 Given the widespread use of languages like GPSS [BKP76] and tools like CSIM <ref> [Sch86] </ref>, and their suitability in modeling significant physical systems, the utility of an active-transaction PDES system is unquestionable. <p> The resulting architecture is depicted in Figure 5.2. Since ParaSol's transactions are implemented as threads (threads enable concurrent processes with small context switching overheads), ParaSol requires thread support. Instead of embedding a threads system in the simulation kernel, as done for example in CSIM <ref> [Sch86] </ref>, we have opted to consider the threads system as a separate entity with a clean interface. This allows us to (1) use the threads system independently of ParaSol (as done in [MR95]) and (2) replace the threads system in ParaSol by another one if so desired. <p> We have patterned the library's functionality after CSIM's <ref> [Sch86] </ref>, but have added an object-oriented view and also extra flexibility.
Reference: [Sho91] <author> G.C. Shoja. </author> <title> A distributed facility for load sharing and parallel processing among workstations. </title> <journal> Journal of Systems and Software, </journal> <volume> 14 </volume> <pages> 163-172, </pages> <year> 1991. </year>
Reference-contexts: Some network computing systems provide transparent checkpoint and recovery, in the sense that no programming effort is required <ref> [CM92, LFS93, Sho91] </ref>. The drawback of this approach is a heavy checkpoint overhead, since the complete address space of all processes involved must be saved at each checkpoint.
Reference: [SMR96] <author> J. Sang, E. Mascarenhas, and V. Rego. </author> <title> Mobile-process-based parallel simulation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 33 </volume> <pages> 12-23, </pages> <year> 1996. </year>
Reference-contexts: Using active transactions in distributed simulations is not trivial, however, since it requires the ability to move computations (the active transactions), rather than just data, across processor boundaries. Nevertheless, Sang and others <ref> [San94, SMR96] </ref> proved this approach to be viable by building Si, a conservative PDES system based on the active-transaction and process-interaction world view.
Reference: [Sob74] <author> I.M. Sobol'. </author> <title> The Monte Carlo Method. </title> <publisher> The University of Chicago Press, </publisher> <year> 1974. </year>
Reference-contexts: Simulation models can be classified along three different dimensions [LK91]: 7 * Static vs. Dynamic simulation models: A static simulation model is a representation of a system at a particular time or a representation where time plays no role (e.g., classical Monte Carlo models <ref> [Sob74] </ref>). On the other hand, a dynamic simulation model represents a system as it evolves over time. * Deterministic vs. Stochastic simulation models: A deterministic model does not contain any random components, and the simulation's output depends only on known inputs and on the model's relationships.
Reference: [SR91] <author> V. S. Sunderam and V. J. Rego. </author> <title> EcliPSe: A system for High Performance Concurrent Simulation. </title> <journal> Software Practice and Experience, </journal> <volume> 21(11) </volume> <pages> 1189-1219, </pages> <year> 1991. </year> <month> 188 </month>
Reference-contexts: Processes higher in the tree combine results produced by their descendents and also perform statistical and optimization functions. The root of the tree is responsible for controlling the whole procedure. The EcliPSe prototype <ref> [RS92, SR91] </ref> has been developed to support replicative simulation programs and also some applications based on the data-parallel paradigm. 23 Table 2.1 Main estimators for parallel execution and their bias. P is the number of processes and t is the time when the simulation is ordered to stop. <p> Several experiments involving Monte Carlo and discrete-event simulations, performed in <ref> [RS92, SR91] </ref>, suggested that an almost linear speedup can be obtained with the EcliPSe approach. Lin [Lin94] has done theoretical and experimental work on replicated simulation on a network of workstations.
Reference: [SR93] <author> S. Srinivasan and P.F. Reynolds, Jr. </author> <title> Non-interfering GVT computation via asynchronous global reductions. </title> <type> Technical Report Tr-93-17, </type> <institution> Department of Computer Science, University of Virginia, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Computation of GVT involves all processes in the simulation. The computation is not trivial because the needed data is scattered among all processes and because of messages in transit. Schemes for computing GVT are described in <ref> [Bel90, DFW94, LL89a, SR93] </ref>. The main performance problems associated with time warp are state saving overhead, significant if the LP's state is large, and rollback overhead, which may seriously 15 impact performance if rollbacks are too frequent or poorly implemented.
Reference: [SSRT91] <author> D.P. Sanderson, R. Sharma, R. Rosiz, and S. Treu. </author> <title> The hierarchical simulation language HSL: A versatile tool for process-oriented simulation. </title> <booktitle> ACM Transaction on Modeling and Computer Simulation, </booktitle> <volume> 1(2) </volume> <pages> 113-153, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Among packages supporting process-interaction, some adopt the active transaction approach [BKP76, Sch86] and some the active server approach <ref> [SSRT91] </ref>. The active-transaction approach is present among many of the commercial simulation languages. 2.3 Parallel Simulation with Model Decomposition Several different parallel processing approaches have been used to speed up the evaluation of simulation models.
Reference: [Sta85] <author> D. Stauffer. </author> <title> Introduction to Percolation Theory. </title> <publisher> Taylor & Francis, </publisher> <year> 1985. </year>
Reference-contexts: The statistical mechanics of polymer solutions [Flo69], for example, is often studied by simulating a self-avoiding walk (SAW) on a randomly diluted 3-D lattice. First, a percolation cluster <ref> [Sta85] </ref> is created on a finite grid by randomly diluting the grid (i.e., marking sites as inaccessible to the chain) with probability q = 1 p &gt; 0. The remaining sites then form connected components called clusters. <p> Other experiments are reported in [KR95, KR96]. In generating clusters, we kept the value of p = 0:31 (p is the probability of filling each slot in the lattice) fixed throughout. We chose this number because it is near criticality and, according to percolation theory <ref> [Sta85] </ref>, tends to yield a cluster spanning the entire grid. Unfortunately, since this makes the mapping tables peak in size, it is also the most taxing value of p for the proposed algorithm. Because of interest in criticality, however, this value of p is often used in clustering studies.
Reference: [Ste92] <author> J.S. Steinman. SPEEDES: </author> <title> A unified approach to parallel simulation. </title> <booktitle> In Proceedings of the Sixth Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 75-84, </pages> <year> 1992. </year>
Reference-contexts: Because of that, the system is structured in layers, with the bottom layer responsible for low-level tasks such as context management, message communication, and interrupt handling, and the top layer responsible for time warp mechanisms such as rollbacks and anti-messages. SPEEDES <ref> [Ste92] </ref> is a direct descendant of TWOS. It is written in C++ and has an object-oriented view. Like TWOS, SPEEDES is even-based. Some of its interesting features are the support to multiple synchronization protocols [Ste92] and its delta exchange mechanism to support incremental state saving [Ste93]. <p> SPEEDES <ref> [Ste92] </ref> is a direct descendant of TWOS. It is written in C++ and has an object-oriented view. Like TWOS, SPEEDES is even-based. Some of its interesting features are the support to multiple synchronization protocols [Ste92] and its delta exchange mechanism to support incremental state saving [Ste93]. Maisie [BL90, Bag91] differs from the above systems because it is a C-based language, instead of offering a function (or method) call interface. Maisie also differs from other systems because it uses a process-interaction world view.
Reference: [Ste93] <author> J.S. Steinman. </author> <title> Incremental state saving in SPEEDES using C++. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 1-10, </pages> <year> 1993. </year>
Reference-contexts: Several attempts have been made to improve time warp's performance: * Avoid saving state at every event, at the cost of increasing the rollback cost [Fuj90a, RA94], or save state incrementally <ref> [CGU + 94, Ste93] </ref>. * Migrate LPs to balance the workload [RJ90, BM93, GT93]. * Decrease the number of rollbacks by introducing some degree of LP blocking, therefore limiting time warp's optimism [HT93, LSW89, SBW88, RJ89]. * Reduce the number of anti-messages by not sending them immediately after a straggler is <p> SPEEDES [Ste92] is a direct descendant of TWOS. It is written in C++ and has an object-oriented view. Like TWOS, SPEEDES is even-based. Some of its interesting features are the support to multiple synchronization protocols [Ste92] and its delta exchange mechanism to support incremental state saving <ref> [Ste93] </ref>. Maisie [BL90, Bag91] differs from the above systems because it is a C-based language, instead of offering a function (or method) call interface. Maisie also differs from other systems because it uses a process-interaction world view. This system is described in more detail in Section 5.1.
Reference: [Sun90] <author> V.S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4), </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: The outline for the original sequential program is shown in Figure 3.1. By using one of the parallel virtual machine environments available <ref> [Sun90, GL92, BC91, Tur93] </ref>, we can create a program where samples are produced in parallel, exploiting the computing power of all machines in a network. In our case, for reasons explained on Section 3.3, we use the Conch [Top92] distributed system. The resulting program is shown in Figure 3.2. <p> As with the threads system, the communication system is made to present a clean interface to ParaSol. Its 99 implementation uses the executing environment to extract better performance: when running in a workstation cluster, a parallel virtual machine environment such as PVM <ref> [Sun90] </ref> or Conch [Top92] is used. On a distributed memory multiprocessor, the native interprocessor communication primitives, which exploit the underlying hardware interconnection network, are used. Finally, on a shared-memory multiprocessor, shared memory is used to hold message buffers.
Reference: [TF93] <author> J.-J. Tsai and R.M. Fujimoto. </author> <title> Automatic parallelization of discrete event simulation programs. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 697-705, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: SimKit is optimized for shared-memory execution. The system is built atop the WarpKit parallel executive, a C++ interface to the Shared Memory Time Warp system. Some research was directed towards the parallelization of existing simulation languages. In <ref> [TF93] </ref>, Tsai and Fujimoto describe a framework for parallelizing simulation languages. In their framework, a common layer implements a set of basic primitives that can be used to build compilers for existing languages. <p> Therefore, the following scheme may be adopted: * Variables that are frequently accessed in a read-only fashion, such as the queue size in the given example, are stored in a "space-time" manner, not unlike in <ref> [TF93] </ref>: not only is the most current value kept, but also all past values are stored.
Reference: [Top92] <author> B. Topol. Conch: </author> <title> Second generation heterogeneous computing. </title> <type> Technical report, Master thesis, </type> <institution> Department of Math and Computer Science, Emory University, </institution> <year> 1992. </year>
Reference-contexts: By using one of the parallel virtual machine environments available [Sun90, GL92, BC91, Tur93], we can create a program where samples are produced in parallel, exploiting the computing power of all machines in a network. In our case, for reasons explained on Section 3.3, we use the Conch <ref> [Top92] </ref> distributed system. The resulting program is shown in Figure 3.2. It executes on n processes, with n 1 generating samples in parallel (the "samplers"), and one central process (the "monitor") collecting samples, computing the confidence interval, and deciding when to stop. <p> While this may introduce additional overhead because of layering, it brings unquestionable advantages in terms of increased portability and decreased design complexity. Conch <ref> [Top92] </ref> was chosen as our basis because of its support of virtual machine topologies and fault tolerance. Also, being an experimental system, Conch could accommodate changes to improve its support of EcliPSe. <p> As with the threads system, the communication system is made to present a clean interface to ParaSol. Its 99 implementation uses the executing environment to extract better performance: when running in a workstation cluster, a parallel virtual machine environment such as PVM [Sun90] or Conch <ref> [Top92] </ref> is used. On a distributed memory multiprocessor, the native interprocessor communication primitives, which exploit the underlying hardware interconnection network, are used. Finally, on a shared-memory multiprocessor, shared memory is used to hold message buffers.
Reference: [Tur93] <author> L.H. Turcotte. </author> <title> A survey of software environments for exploiting networked computing resources. </title> <type> Technical report, </type> <institution> Engineering Research Center for Computational Field Simulation, Mississippi State University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Moreover, most applications currently using expensive supercomputers could be moved to smaller machines or to workstation clusters by using CPU cycles that would otherwise go unused. Turcotte <ref> [Tur93] </ref> surveys the growth of networked computing, including software being developed for such computing environments. Parallel simulation shares some of the main problems inherent to general parallel applications: * the attainable speedup is often limited by the problem itself, by the parallel algorithm used, or by the hardware architecture. <p> The outline for the original sequential program is shown in Figure 3.1. By using one of the parallel virtual machine environments available <ref> [Sun90, GL92, BC91, Tur93] </ref>, we can create a program where samples are produced in parallel, exploiting the computing power of all machines in a network. In our case, for reasons explained on Section 3.3, we use the Conch [Top92] distributed system. The resulting program is shown in Figure 3.2.
Reference: [Vak92] <author> P. Vakili. </author> <title> Massively parallel and distributed simulation of a class of discrete event systems: A different perspective. </title> <journal> ACM Transactions on Modeling and Computer Simulation, </journal> <volume> 2(3) </volume> <pages> 214-238, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Pawlikowski and others [PYM94] have developed the AKAROA system for simulating multiple replications in parallel. This work is based on the "Spectral Analysis in Parallel Time Streams" technique. Somewhat similar to the model replication approach, the Single Clock Multiple System (SCMS) <ref> [Vak92] </ref> technique allows simulation of coupled trajectories on SIMD machines. The technique is used in simulation of a large number of systems that are variants of a "nominal system" having different system parameter values or operation policies.
Reference: [Wil91] <author> R. Williams. </author> <title> Associations: A tool for parallel computing with meshes. </title> <type> Technical Report CCSF-9-91, </type> <institution> Caltech Concurrent Supercomputing Facilities, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: As another example, tree-combining is not useful when the result of an operation depends on the order of the operands. For the tree-combining mechanism to work, 70 a combining operation <ref> [Wil91] </ref> must be used.

References-found: 103

