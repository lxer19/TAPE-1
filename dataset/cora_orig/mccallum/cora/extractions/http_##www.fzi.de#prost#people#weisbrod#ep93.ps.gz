URL: http://www.fzi.de/prost/people/weisbrod/ep93.ps.gz
Refering-URL: http://www.fzi.de/prost/publications/overview.html
Root-URL: http://www.fzi.de
Email: braun@ira.uka.de weisbrod@ira.uka.de  
Title: Evolving Neural Networks for Application Oriented Problems  
Author: Heinrich Braun Joachim Weisbrod 
Address: Karlsruhe  
Affiliation: Institut f ur Logik, Komplexit at Institut f ur Programmstrukturen und Deduktionssysteme und Datenorganisation Universit at Karlsruhe Universit at  
Abstract: For many practical problem domains the use of neural networks has led to very satisfactory results. Nevertheless the choice of an appropriate, problem specific network architecture still remains a very poorly understood task. Given an actual problem, one can choose a few different architectures, train the chosen architectures a few times and finally select the architecture with the best behaviour. But, of course, there may exist totally different and much more suited topologies. In this paper we present a genetic algorithm driven network generator that evolves neural feedforward network architectures for specific problems. Our system ENZO 1 optimizes both the network topology and the connection weights at the same time, thereby saving an order of magnitude in necessary learning time. Together with our new concept to solve the crucial neural network problem of permuted internal representations this approach provides an efficient and successfull crossover operator. This makes ENZO very appropriate to manage the large networks needed in application oriented domains. In experiments with three different applications our system generated very successful networks. The generated topologies possess distinct improvements referring to network size, learning time, and generalization ability. 
Abstract-found: 1
Intro-found: 1
Reference: [Barna 90] <author> G.Barna and K.Kaski, </author> <title> Choosing optimal network structure, </title> <booktitle> Proc. of the Internat. Neural Network Conf. </booktitle> <address> (Paris, </address> <year> 1990) </year>
Reference-contexts: Besides a network property of ten desired is generalization ability. To be sufficiently obtainable this capability also demands sparsely con 2 F BP is another user specified parameter in [0; 1]. nected networks, because networks with too many connections are not forced to detect enough abstract internal features (e.g. <ref> [Barna 90] </ref>). On the other hand networks with relatively few connections do not learn very well. Therefore the best way to evolve sparsely connected networks is to start with a high connection density in order to thin out the given topologies.
Reference: [1] <author> R.K. Belew, J. McInerney, </author> <title> N.N. Schraudolph, Evolving networks: using the genetic algorithm with connectionist learning, </title> <institution> CSE Technical Report #CS90-174 (University of California, </institution> <address> San Diego) </address>
Reference-contexts: First of all less connections mean less arithmetic operations (software) or less wires (hardware). Besides a network property of ten desired is generalization ability. To be sufficiently obtainable this capability also demands sparsely con 2 F BP is another user specified parameter in <ref> [0; 1] </ref>. nected networks, because networks with too many connections are not forced to detect enough abstract internal features (e.g. [Barna 90]). On the other hand networks with relatively few connections do not learn very well.
Reference: [2] <author> H. </author> <type> Braun, </type> <institution> Massiv parallele Algorithmen fuer kom-binatorische Optimierungsprobleme und ihre Imple-mentierung auf einem Parallelrechner, Dissertation TH Karlsruhe, Fakultaet fuer Informatik, </institution> <year> 1990 </year>
Reference: [3] <author> H. Braun, </author> <title> On solving traveling salesman problems by genetic algor ithms, in: Parallel Problem Solving from Nature, </title> <publisher> LNCS 496 (Berlin, </publisher> <year> 1991) </year>
Reference-contexts: The single components of our algorithm, which are responsible for that optimization, will be introduced now. ENZO uses the same scheme as already proposed by Braun ([2], <ref> [3] </ref>) for finding the optimal solution of large travelling salesman problems (see figure 1).
Reference: [4] <author> H. Braun, J. Feulner, V. Ullrich, </author> <title> Learning strategies for solving the problem of planning using backpropagation, </title> <booktitle> in: Proc. Fourth Intern. Conf. Neural Networks (Nimes, </booktitle> <year> 1991) </year>
Reference-contexts: Using ENZO we tested our algorithm with three different applications: (1) a pattern recognition problem, (2) the emulation of the kinematics of a backing up truck derived from [13], and (3) the endgame of the two-player game Nine Men's Morris <ref> [4] </ref>, employing a (23-12-10-10)-topology, a (3-20-3)-topology and a (120-60-20-2-1)-topology, respectively. So at least the last one of them is much more complex than problems considered in publications on comparable topics ([1], [5], [6], [7], [8], [9], [11], [12], [14], [15], [17], [18], [19]). <p> We found that the number of training epochs is impressively reduced from 50 at the start to almost constantly 3 epochs in the evolution phase in both cases, as a consequence of weight transmission. In order to compare differently trained `Sokrates' nets in <ref> [4] </ref> the authors introduce the average improvement as a measure for the network's playing performance. Besides being used as fitness criterion this measure also allows a comparison between our best evolved net and the handcrafted original `Sokrates' net.
Reference: [5] <author> D.J. Chalmers, </author> <title> The evolution of learning: an experi-ment in genetic connectionism, </title> <booktitle> in: Proc. of the 1990 Connectionist Models Summer School (Morgan-Kaufmann, </booktitle> <address> San Mateo, CA, </address> <year> 1990) </year>
Reference-contexts: So at least the last one of them is much more complex than problems considered in publications on comparable topics ([1], <ref> [5] </ref>, [6], [7], [8], [9], [11], [12], [14], [15], [17], [18], [19]). Arguing with Miller et.al. [11] the search space of possible network topologies is infinitely large, not differentiable, complex, noisy, deceptive and multimodal. These attributes make random, enumerative, gradient descent or heuristic search methods unprac-ticable.
Reference: [6] <editor> D.B. Fogel, L.J. Fogel, V.W. Porto, </editor> <booktitle> Evolving neural networks, in: Biological Cybernetics 63 (Springer, </booktitle> <address> Berlin, </address> <year> 1990) </year>
Reference-contexts: So at least the last one of them is much more complex than problems considered in publications on comparable topics ([1], [5], <ref> [6] </ref>, [7], [8], [9], [11], [12], [14], [15], [17], [18], [19]). Arguing with Miller et.al. [11] the search space of possible network topologies is infinitely large, not differentiable, complex, noisy, deceptive and multimodal. These attributes make random, enumerative, gradient descent or heuristic search methods unprac-ticable.
Reference: [7] <author> P.J.B. Hancock, L.S. Smith, GANNET: </author> <title> Genetic design of a neural net for face recognition, in: Parallel Problem Solving from Nature (Springer, </title> <address> Berlin, </address> <year> 1990) </year>
Reference-contexts: So at least the last one of them is much more complex than problems considered in publications on comparable topics ([1], [5], [6], <ref> [7] </ref>, [8], [9], [11], [12], [14], [15], [17], [18], [19]). Arguing with Miller et.al. [11] the search space of possible network topologies is infinitely large, not differentiable, complex, noisy, deceptive and multimodal. These attributes make random, enumerative, gradient descent or heuristic search methods unprac-ticable. <p> So the length of the genstring is equivalent to the number of potential connections allowed by the represented architecture. In weak representation schemes the genes correspond to more abstract network properties. Examples for such weak encodings can be found in <ref> [7] </ref>, [8], or [19]. We agree with Miller et.al. [11], that weak schemes may be useful for `capturing the architectural regularities of large networks rather efficiently'. But their application also requires a much more detailed knowledge about both genetic and neural mechanisms.
Reference: [8] <author> S. Harp, T. Samad, A. Guha, </author> <title> Towards the genetic synthesis of neural networks, </title> <booktitle> in: Proc. Third In-ternat. Conf. Genetic Algorithms (Morgan Kauf-mann, </booktitle> <address> San Mateo, CA, </address> <year> 1989) </year>
Reference-contexts: So at least the last one of them is much more complex than problems considered in publications on comparable topics ([1], [5], [6], [7], <ref> [8] </ref>, [9], [11], [12], [14], [15], [17], [18], [19]). Arguing with Miller et.al. [11] the search space of possible network topologies is infinitely large, not differentiable, complex, noisy, deceptive and multimodal. These attributes make random, enumerative, gradient descent or heuristic search methods unprac-ticable. <p> So the length of the genstring is equivalent to the number of potential connections allowed by the represented architecture. In weak representation schemes the genes correspond to more abstract network properties. Examples for such weak encodings can be found in [7], <ref> [8] </ref>, or [19]. We agree with Miller et.al. [11], that weak schemes may be useful for `capturing the architectural regularities of large networks rather efficiently'. But their application also requires a much more detailed knowledge about both genetic and neural mechanisms.
Reference: [9] <author> K.U. Hoeffgen, H.P. Siemon, A. Ultsch, </author> <title> Genetic improvements of feedforward nets for approximating functions, in: Parallel Problem Solving from Nature (Springer, </title> <address> Berlin, </address> <year> 1990) </year>
Reference-contexts: So at least the last one of them is much more complex than problems considered in publications on comparable topics ([1], [5], [6], [7], [8], <ref> [9] </ref>, [11], [12], [14], [15], [17], [18], [19]). Arguing with Miller et.al. [11] the search space of possible network topologies is infinitely large, not differentiable, complex, noisy, deceptive and multimodal. These attributes make random, enumerative, gradient descent or heuristic search methods unprac-ticable. <p> But their application also requires a much more detailed knowledge about both genetic and neural mechanisms. For this reason in our work we preferred a strong encoding. Interesting explorations using strong representation schemes are described in <ref> [9] </ref>, [11], or [18], for instance.
Reference: [10] <author> J.H. Holland, </author> <booktitle> Adaption in natural and artificial systems, </booktitle> <address> (Ann Arbor: </address> <publisher> University of Michigan Press, </publisher> <year> 1975) </year>
Reference-contexts: These attributes make random, enumerative, gradient descent or heuristic search methods unprac-ticable. Genetic algorithms, however, represent a search method that is able to manage the demands of the examined search space <ref> [10] </ref>. Accordingly, there has been a lot of work concerning the use of genetic methods in order to evolve problem specific network architectures.
Reference: [11] <author> G. Miller, P. Todd, S. Hedge, </author> <title> Designing neural networks using genetic algorithms, </title> <booktitle> in: Proc. Third In-ternat. Conf. Genetic Algorithms (Morgan Kauf-mann, </booktitle> <address> San Mateo, CA, </address> <year> 1989) </year>
Reference-contexts: So at least the last one of them is much more complex than problems considered in publications on comparable topics ([1], [5], [6], [7], [8], [9], <ref> [11] </ref>, [12], [14], [15], [17], [18], [19]). Arguing with Miller et.al. [11] the search space of possible network topologies is infinitely large, not differentiable, complex, noisy, deceptive and multimodal. These attributes make random, enumerative, gradient descent or heuristic search methods unprac-ticable. <p> So at least the last one of them is much more complex than problems considered in publications on comparable topics ([1], [5], [6], [7], [8], [9], <ref> [11] </ref>, [12], [14], [15], [17], [18], [19]). Arguing with Miller et.al. [11] the search space of possible network topologies is infinitely large, not differentiable, complex, noisy, deceptive and multimodal. These attributes make random, enumerative, gradient descent or heuristic search methods unprac-ticable. Genetic algorithms, however, represent a search method that is able to manage the demands of the examined search space [10]. <p> So the length of the genstring is equivalent to the number of potential connections allowed by the represented architecture. In weak representation schemes the genes correspond to more abstract network properties. Examples for such weak encodings can be found in [7], [8], or [19]. We agree with Miller et.al. <ref> [11] </ref>, that weak schemes may be useful for `capturing the architectural regularities of large networks rather efficiently'. But their application also requires a much more detailed knowledge about both genetic and neural mechanisms. For this reason in our work we preferred a strong encoding. <p> But their application also requires a much more detailed knowledge about both genetic and neural mechanisms. For this reason in our work we preferred a strong encoding. Interesting explorations using strong representation schemes are described in [9], <ref> [11] </ref>, or [18], for instance.
Reference: [12] <author> H. Muehlenbein, </author> <title> Limitations of multi-layer perceptron networks steps towards genetic neural networks, </title> <booktitle> in: Parallel Computing 14 (1990) </booktitle>
Reference-contexts: So at least the last one of them is much more complex than problems considered in publications on comparable topics ([1], [5], [6], [7], [8], [9], [11], <ref> [12] </ref>, [14], [15], [17], [18], [19]). Arguing with Miller et.al. [11] the search space of possible network topologies is infinitely large, not differentiable, complex, noisy, deceptive and multimodal. These attributes make random, enumerative, gradient descent or heuristic search methods unprac-ticable.
Reference: [13] <author> D. Nguyen and B. Widrow, </author> <title> The truck backer-upper, </title> <booktitle> Proc. Internat. Network Conf. </booktitle> <publisher> (Kluwer Academic Publishers, </publisher> <address> Dordrecht, </address> <year> 1990) </year>
Reference-contexts: Our network generator ENZO represents such a system, that has been successfully employed to accomplish the needs of different tasks. Using ENZO we tested our algorithm with three different applications: (1) a pattern recognition problem, (2) the emulation of the kinematics of a backing up truck derived from <ref> [13] </ref>, and (3) the endgame of the two-player game Nine Men's Morris [4], employing a (23-12-10-10)-topology, a (3-20-3)-topology and a (120-60-20-2-1)-topology, respectively. <p> We found, that the optimum value for the parameter F BP depends very much on the complexity of the given application. Without weight transmission, i.e. F BP = 0, ENZO evolved no perfect network at all. (2) Kinematics of the truck backer-upper Our second application was derived from <ref> [13] </ref>. A (3-20-3)-topology was used to emulate the kinematics of a backing up truck within a delimited range of situations. Despite of being very easy to learn (about 20 training epochs) this application could confirm our conclusions drawn from the digit recognition task.
Reference: [14] <author> W. Schiffmann, M. Joost, R. Werner, </author> <title> Performance evaluation of evolutionary created neural network topologies, in: Parallel Problem Solving from Nature (Springer, </title> <address> Berlin, </address> <year> 1990) </year>
Reference-contexts: So at least the last one of them is much more complex than problems considered in publications on comparable topics ([1], [5], [6], [7], [8], [9], [11], [12], <ref> [14] </ref>, [15], [17], [18], [19]). Arguing with Miller et.al. [11] the search space of possible network topologies is infinitely large, not differentiable, complex, noisy, deceptive and multimodal. These attributes make random, enumerative, gradient descent or heuristic search methods unprac-ticable.
Reference: [15] <author> M. Scholz, </author> <title> A learning strategy for neural networks based on a modified evolutionary strategy , in: Parallel Problem Solving from Nature (Springer, </title> <address> Berlin, </address> <year> 1990) </year>
Reference-contexts: So at least the last one of them is much more complex than problems considered in publications on comparable topics ([1], [5], [6], [7], [8], [9], [11], [12], [14], <ref> [15] </ref>, [17], [18], [19]). Arguing with Miller et.al. [11] the search space of possible network topologies is infinitely large, not differentiable, complex, noisy, deceptive and multimodal. These attributes make random, enumerative, gradient descent or heuristic search methods unprac-ticable. <p> Most of the experiments examined with this application were run to find good choices for the newly established parameters. Moreover, our results sustained the often published opinion that in the given domain mutation alone is a powerful mechanism to achieve satisfying performance (cf. <ref> [15] </ref>, [18], or [19]). (3) Nine Men's Morris Finally we tested our system with a very hard problem requiring a very large network architecture. The task was to learn a scoring function for the endgame of the two-player game Nine Men's Morris.
Reference: [16] <author> V. </author> <type> Ullrich, </type> <institution> Erlernen von Spielstrategien f ur M uhle durch Neuronale Netze, Diplomarbeit TH Karl-sruhe, Fakult at f ur Informatik (1991) </institution>
Reference-contexts: The task was to learn a scoring function for the endgame of the two-player game Nine Men's Morris. The so called `Sokrates' net introduced in <ref> [16] </ref> has gained a high level performance, defeating most human opponents. In order to train the net the employed (60-30-10-1)-topology is doubled, leading to a (120-60-20-2-1)-topology with 4:425 weights, see figure 3.
Reference: [17] <author> D. Whitley, T. Hanson, </author> <title> Optimizing neural networks using faster, more accurate genetic search, </title> <booktitle> in: Proc. Third Internat. Conf. Genetic Algorithms (Mor-gan Kaufmann, </booktitle> <address> San Mateo, CA, </address> <year> 1989) </year>
Reference-contexts: So at least the last one of them is much more complex than problems considered in publications on comparable topics ([1], [5], [6], [7], [8], [9], [11], [12], [14], [15], <ref> [17] </ref>, [18], [19]). Arguing with Miller et.al. [11] the search space of possible network topologies is infinitely large, not differentiable, complex, noisy, deceptive and multimodal. These attributes make random, enumerative, gradient descent or heuristic search methods unprac-ticable. <p> Accordingly, there has been a lot of work concerning the use of genetic methods in order to evolve problem specific network architectures. On the other hand there have been attempts to achieve robuster learning techniques by using genetic methods ([15], <ref> [17] </ref>), the most promising of them be while evolution do (1) select one rsp. two parents selection (2) generate one offspring mutation rsp. crossover (3) tune offspring and learning evaluate its fitness & evaluation (4) insert offspring in population survival of the fittest and delete the last population element ing combinations
Reference: [18] <author> D. Whitley, T. Starkweather, C. Bogart, </author> <title> Genetic algorithms and neural networks: optimizing connections and connectivity, </title> <booktitle> in: Parallel Computing 14 (1990) </booktitle>
Reference-contexts: So at least the last one of them is much more complex than problems considered in publications on comparable topics ([1], [5], [6], [7], [8], [9], [11], [12], [14], [15], [17], <ref> [18] </ref>, [19]). Arguing with Miller et.al. [11] the search space of possible network topologies is infinitely large, not differentiable, complex, noisy, deceptive and multimodal. These attributes make random, enumerative, gradient descent or heuristic search methods unprac-ticable. <p> evolution do (1) select one rsp. two parents selection (2) generate one offspring mutation rsp. crossover (3) tune offspring and learning evaluate its fitness & evaluation (4) insert offspring in population survival of the fittest and delete the last population element ing combinations of standard backpropagation (BP) and GA ([1], <ref> [18] </ref>). But from our point of view there is no reason to strictly separate topology and weight optimization. Our network generator ENZO successfully hybridizes both optimization processes, additionally establishing powerful mechanisms to both improve the optimization process and to save learning time. <p> But their application also requires a much more detailed knowledge about both genetic and neural mechanisms. For this reason in our work we preferred a strong encoding. Interesting explorations using strong representation schemes are described in [9], [11], or <ref> [18] </ref>, for instance. <p> Most of the experiments examined with this application were run to find good choices for the newly established parameters. Moreover, our results sustained the often published opinion that in the given domain mutation alone is a powerful mechanism to achieve satisfying performance (cf. [15], <ref> [18] </ref>, or [19]). (3) Nine Men's Morris Finally we tested our system with a very hard problem requiring a very large network architecture. The task was to learn a scoring function for the endgame of the two-player game Nine Men's Morris.
Reference: [19] <author> D. Whitley, S. Dominic, R. Das, </author> <title> Genetic reinforcement learning with multilayer neural networks, </title> <booktitle> in: Proc. Fourth Internat. Conf. </booktitle> <publisher> Genetic Algorithms (Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991) </year>
Reference-contexts: So at least the last one of them is much more complex than problems considered in publications on comparable topics ([1], [5], [6], [7], [8], [9], [11], [12], [14], [15], [17], [18], <ref> [19] </ref>). Arguing with Miller et.al. [11] the search space of possible network topologies is infinitely large, not differentiable, complex, noisy, deceptive and multimodal. These attributes make random, enumerative, gradient descent or heuristic search methods unprac-ticable. <p> So the length of the genstring is equivalent to the number of potential connections allowed by the represented architecture. In weak representation schemes the genes correspond to more abstract network properties. Examples for such weak encodings can be found in [7], [8], or <ref> [19] </ref>. We agree with Miller et.al. [11], that weak schemes may be useful for `capturing the architectural regularities of large networks rather efficiently'. But their application also requires a much more detailed knowledge about both genetic and neural mechanisms. For this reason in our work we preferred a strong encoding. <p> Most of the experiments examined with this application were run to find good choices for the newly established parameters. Moreover, our results sustained the often published opinion that in the given domain mutation alone is a powerful mechanism to achieve satisfying performance (cf. [15], [18], or <ref> [19] </ref>). (3) Nine Men's Morris Finally we tested our system with a very hard problem requiring a very large network architecture. The task was to learn a scoring function for the endgame of the two-player game Nine Men's Morris.
Reference: [20] <author> J. Weisbrod, Untersuchung der Einsatzm oglichkei-ten Neuronaler Netze zur visuellen Mustererken-nung gestanzter Ziffern, </author> <title> Studienarbeit TH Karls-ruhe, </title> <institution> Fakult at f ur Elektrotechnik (1989) </institution>
Reference-contexts: The complete results can be found in [21]. (1) Digit recognition First we examined a digit recognition problem described in <ref> [20] </ref>. In order to classify seven distorted sets of the trained digits a (23-12-10-10)-topology was used. While in [20] no perfect network could be found 3 , ENZO evolved lots of them. <p> The complete results can be found in [21]. (1) Digit recognition First we examined a digit recognition problem described in <ref> [20] </ref>. In order to classify seven distorted sets of the trained digits a (23-12-10-10)-topology was used. While in [20] no perfect network could be found 3 , ENZO evolved lots of them. As optimization criterion we simply used the number of false classifications with the seven test sets, i.e. 70 distorted digits.
Reference: [21] <author> J. Weisbrod, </author> <title> Einsatz Genetischer Algorithmen zur Optimierung der Topologie mehrschichtiger Feedforward-Netzwerke, </title> <institution> Diplomarbeit TH Karls-ruhe, Fakult at f ur Informatik (1992) @INPROCEEDINGS-WB93b, </institution> <note> author = -Braun, </note> <editor> H." and Weisbrod, J.-, </editor> <booktitle> TITLE = -Evolving neural networks for application oriented problems-, BOOKTITLE = -Proceedings of the Second Annual Conference on Evolutionary Programming (EP'93)-, ADDRESS = -San Diego-, PUBLISHER = -Evolutionary Programming Society-, YEAR = -1993 </booktitle> - 
Reference-contexts: The complete results can be found in <ref> [21] </ref>. (1) Digit recognition First we examined a digit recognition problem described in [20]. In order to classify seven distorted sets of the trained digits a (23-12-10-10)-topology was used. While in [20] no perfect network could be found 3 , ENZO evolved lots of them.
References-found: 22

