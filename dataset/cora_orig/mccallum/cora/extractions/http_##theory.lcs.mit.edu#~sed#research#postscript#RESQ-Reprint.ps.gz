URL: http://theory.lcs.mit.edu/~sed/research/postscript/RESQ-Reprint.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~sed/research/RESQ-abs.html
Root-URL: 
Email: Email: jaa@das.harvard.edu  Email: sed@das.harvard.edu  
Title: Specification and Simulation of Statistical Query Algorithms for Efficiency and Noise Tolerance  
Author: Javed A. Aslam Scott E. Decatur 
Note: Supported by DARPA Air Force Contract F49620-92-J-0466. Part of this research was conducted while the author was at MIT and supported by DARPA Contract N00014-87-K-825 and by NSF Grant CCR-89-14428.  Supported by an NDSEG Fellowship and by NSF Grant CCR-92-00884.  
Address: Cambridge, MA 02138  
Affiliation: Aiken Computation Laboratory Harvard University  
Abstract: A recent innovation in computational learning theory is the statistical query (SQ) model. The advantage of specifying learning algorithms in this model is that SQ algorithms can be simulated in the PAC model, both in the absence and in the presence of noise. However, simulations of SQ algorithms in the PAC model have non-optimal time and sample complexities. In this paper, we introduce a new method for specifying statistical query algorithms based on a type of relative error and provide simulations in the noise-free and noise-tolerant PAC models which yield efficient algorithms. Requests for estimates of statistics in this new model take the form: "Return an estimate of the statistic within a 1 factor, or return `?', promising that the statistic is less than ." In addition to showing that this is a very natural language for specifying learning algorithms, we also show that this new specification is polynomially equivalent to standard SQ, and thus, known learnability and hardness results for statistical query learning are preserved. We then give highly efficient PAC simulations of relative error SQ algorithms. We show that the learning algorithms obtained by simulating efficient relative error SQ algorithms in both the absence of noise and in the presence of malicious noise have roughly optimal sample complexity. We also show that the simulation of efficient relative error SQ algorithms in the presence of classification noise yield learning algorithms at least as efficient as those obtained through standard methods, and in some cases improved, roughly optimal results are achieved. The sample complexities for all of these simulations are based on the d metric which is a type of relative error metric useful for quantities which are small or even zero. We show that uniform convergence with respect to the d metric yields "uniform convergence" with respect to (; ) accuracy. In Proceedings of the Eighth Annual ACM Conference on Computational Learning The ory, pages 437-446. ACM Press, July 1995. Finally, while we show that many specific learning algorithms can be written as highly efficient relative error SQ algorithms, we also show, in fact, that all SQ algorithms can be written efficiently by proving general upper bounds on the complexity of (; ) queries as a function of the accuracy parameter ". As a consequence of this result, we give general upper bounds on the complexity of learning algorithms achieved through the use of relative error SQ algorithms and the simulations described above. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Javed Aslam and Scott Decatur. </author> <title> General bounds on statistical query learning and PAC learning with noise via hypothesis boosting. </title> <booktitle> In Proceedings of the 34 th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 282-291, </pages> <month> November </month> <year> 1993. </year> <note> To appear in Information and Computation. </note>
Reference-contexts: For any query [; ; ], define as follows: If P &lt; , then = 1; else = =P . Thus, is the ratio between and P for queries with a "significant" dependence on the labels of examples, and note that 2 <ref> [; 1] </ref>. Finally, for notational convenience we say that a probability p is estimated with hff; i error if the estimate ^p satisfies d -(p; ^p) ff. <p> The probability is over the random draw of the sample. The proof of Lemma 4 is given in the appendix. Now, let fl be a lower bound on for every query [; ; ], fl 2 <ref> [ fl ; 1] </ref>, and note that the size and VC-dimension of our new query space (composed of queries of the form 1 , 2 , and ) are only increased by constant factors. We then have the following: Lemma 5 Let Q be a set of queries. <p> We do so by applying boosting techniques [9, 10, 16] and specifically, these techniques as applied in the statistical query model <ref> [1] </ref>. The proof of Theorem 8 is given in the appendix. Theorem 8 If a concept class F is SQ learnable by algorithm A, then F is SQ learnable with O queries each with threshold ative error ( 0 ).
Reference: [2] <author> Javed Aslam and Scott Decatur. </author> <title> On the sample complexity of noise-tolerant learning. </title> <journal> Information Processing Letters, </journal> <note> 1996. To appear. </note>
Reference-contexts: Therefore, the classification noise simulation of this algorithm uses only ~ O n ffi examples, while the simulation of the analogous additive error SQ algorithm uses ~ O n 2 ffi examples. Note that our sample complexity roughly matches the lower bound of "(12) 2 log 1 <ref> [2] </ref>. This is the first non-trivial class to be shown learnable in the presence of classification noise using a sample complexity whose dependence on " is o (1=" 2 ).
Reference: [3] <author> Avrim Blum, Merrick Furst, Jeffery Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich. </author> <title> Weakly 8 learning DNF and characterizing statistical query learn-ing using Fourier analysis. </title> <booktitle> In Proceedings of the 26 th Annual ACM Symposium on the Theory of Computing, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: This includes the results of Kearns [12] showing that almost all classes known to be PAC learnable are learnable by additive error statistical queries, the hardness result of Kearns [12] for learning parity functions, and the general hardness results of Blum et al. <ref> [3] </ref> based on Fourier analysis. The advantages of the new model are then demonstrated by the simulations of relative error SQ algorithms in the noise-free PAC model, the malicious error PAC model and the classification noise PAC model.
Reference: [4] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haus-sler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4) </volume> <pages> 929-865, </pages> <year> 1989. </year>
Reference-contexts: Since 1=t fl = (1=") for all SQ algorithms [12], this simulation effectively uses (1=" 2 ) examples. This is clearly suboptimal when compared to the basically tight general upper and lower bounds on the noise-free sample complexity whose dependence on " is ~ fi (1=") <ref> [4, 8] </ref>. 1 Thus, while there is an incentive for developing algorithms in the statistical query model due to the noise tolerance gained, there is also a disincentive towards doing so due to the inefficiency of the simulations: Algorithm designers must choose between writing algorithms in the SQ model to achieve
Reference: [5] <author> Scott Decatur. </author> <title> Statistical queries and faulty PAC oracles. </title> <booktitle> In Proceedings of the Sixth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 262-268. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1993. </year>
Reference-contexts: Specifically, an SQ algorithm can be simulated in the PAC model in the presence of classification noise, malicious errors, attribute noise and even hybrid models combining these different noises <ref> [12, 5, 7, 6] </ref>. <p> Decatur <ref> [5] </ref> showed that an SQ algorithm can be simulated in the presence of malicious errors with a maximum allowable error rate which depends on t fl , the smallest additive error required by the SQ algorithm. <p> is less than P . 2 Note that this implies that monotone conjunctions can be learned in the presence of malicious errors via relative error statistical queries using a sample of size O " log n , an n=" factor better than that obtainable via standard additive error statistical queries <ref> [5] </ref>. The maximum allowable malicious error rate is ("=n), iden tical to that achieved by the additive error simulation and optimal with respect to ". Another application of Theorem 5 is learning con junctions of at most k relevant variables.
Reference: [6] <author> Scott Decatur. </author> <title> Learning in hybrid noise environments using statistical queries. </title> <booktitle> In Proceedings of the Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 175-185, </pages> <month> January </month> <year> 1995. </year> <note> To appear in Springer-Verlag Lecture Notes in Statistics. </note>
Reference-contexts: Specifically, an SQ algorithm can be simulated in the PAC model in the presence of classification noise, malicious errors, attribute noise and even hybrid models combining these different noises <ref> [12, 5, 7, 6] </ref>.
Reference: [7] <author> Scott Decatur and Rosario Gennaro. </author> <title> On learning from noisy and incomplete examples. </title> <booktitle> In Proceedings of the Eighth Annual ACM Workshop on Computational Learning Theory. </booktitle> <publisher> ACM Press, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: Specifically, an SQ algorithm can be simulated in the PAC model in the presence of classification noise, malicious errors, attribute noise and even hybrid models combining these different noises <ref> [12, 5, 7, 6] </ref>.
Reference: [8] <author> Andrzej Ehrenfeucht, David Haussler, Michael Kearns, and Leslie Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82(3) </volume> <pages> 247-251, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Since 1=t fl = (1=") for all SQ algorithms [12], this simulation effectively uses (1=" 2 ) examples. This is clearly suboptimal when compared to the basically tight general upper and lower bounds on the noise-free sample complexity whose dependence on " is ~ fi (1=") <ref> [4, 8] </ref>. 1 Thus, while there is an incentive for developing algorithms in the statistical query model due to the noise tolerance gained, there is also a disincentive towards doing so due to the inefficiency of the simulations: Algorithm designers must choose between writing algorithms in the SQ model to achieve <p> In Corollary 3, these bounds are within logarithmic factors of both the O (") maximum allowable malicious error rate [13] and the (1=") lower bound on the sample complexity for noise-free PAC learning <ref> [8] </ref>. We also note that in this malicious-error tolerant PAC simulation, the sample, time, space and hypothesis size complexities are asymptotically identical to the corresponding complexities in our noise-free PAC simulation.
Reference: [9] <author> Yoav Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 202-216. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: We do so by applying boosting techniques <ref> [9, 10, 16] </ref> and specifically, these techniques as applied in the statistical query model [1]. The proof of Theorem 8 is given in the appendix.
Reference: [10] <author> Yoav Freund. </author> <title> An improved boosting algorithm and its implications on learning complexity. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 391-398. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: We do so by applying boosting techniques <ref> [9, 10, 16] </ref> and specifically, these techniques as applied in the statistical query model [1]. The proof of Theorem 8 is given in the appendix.
Reference: [11] <author> David Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Information and Computation, </journal> <volume> 100 </volume> <pages> 78-150, </pages> <year> 1992. </year>
Reference-contexts: In each case, we determine the complexity of the PAC algorithm as a function of the complexity of the SQ algorithm. In order to prove sample complexity bounds for these simulations, we make use of the d metric <ref> [15, 11] </ref>. Haussler gives sample complexity bounds sufficient for uniform convergence, with respect to the d metric, of probabilities and their estimates based on a sample. <p> We therefore look to take advantage of uniform convergence results based on the Vapnik-Chervonenkis dimension of the class of queries to be estimated. Consider the d metric defined over the non-negative reals as follows <ref> [15, 11] </ref>: d -(r; s) = - + r + s Haussler ([11]: Definition 3 and Theorems 1 and 7) effectively proves the following theorem on the sample size sufficient to ensure uniform convergence, with respect to the d metric, of empirical estimates to true probabilities.
Reference: [12] <author> Michael Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proceedings of the 25 th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 392-401, </pages> <address> San Diego, </address> <year> 1993. </year>
Reference-contexts: A recently developed tool for creating efficient, noise-tolerant, learning algorithms is the statistical query (SQ) model <ref> [12] </ref>. In this model, instead of using labelled examples, the algorithm asks for the estimates of the values of statistics defined over the distribution of labelled examples. <p> However, this restriction has been found to be quite mild in that most every class of concepts which is learnable in the PAC model is also learnable by statistical queries <ref> [12] </ref>. The most important use of the SQ model stems from the property that statistical queries can actually be simulated with the use of noisy example oracles. <p> Specifically, an SQ algorithm can be simulated in the PAC model in the presence of classification noise, malicious errors, attribute noise and even hybrid models combining these different noises <ref> [12, 5, 7, 6] </ref>. <p> The limitation of the additive model is evident in even the standard, noise-free simulation of additive error statistical query algorithms <ref> [12] </ref> which uses (1=t 2 fl ) examples. Since 1=t fl = (1=") for all SQ algorithms [12], this simulation effectively uses (1=" 2 ) examples. <p> The limitation of the additive model is evident in even the standard, noise-free simulation of additive error statistical query algorithms <ref> [12] </ref> which uses (1=t 2 fl ) examples. Since 1=t fl = (1=") for all SQ algorithms [12], this simulation effectively uses (1=" 2 ) examples. <p> Thus, known learnability and hardness results for additive error SQ learning are preserved in relative error SQ learning. This includes the results of Kearns <ref> [12] </ref> showing that almost all classes known to be PAC learnable are learnable by additive error statistical queries, the hardness result of Kearns [12] for learning parity functions, and the general hardness results of Blum et al. [3] based on Fourier analysis. <p> Thus, known learnability and hardness results for additive error SQ learning are preserved in relative error SQ learning. This includes the results of Kearns <ref> [12] </ref> showing that almost all classes known to be PAC learnable are learnable by additive error statistical queries, the hardness result of Kearns [12] for learning parity functions, and the general hardness results of Blum et al. [3] based on Fourier analysis. <p> This algorithm achieves the same goals as the additive error SQ algorithm described above, yet does so much more efficiently. Specifically, the sample complexity of the standard, noise-free PAC simulation of additive error SQ algorithms depends linearly on 1=t 2 fl <ref> [12] </ref>, while in Section 5, we show that the sample complexity of a noise-free PAC simulation of relative error SQ algorithms depends linearly on 1= 2 fl fl . <p> Another application of Theorem 5 is learning con junctions of at most k relevant variables. By appropriately modifying the additive error SQ algorithm for this problem <ref> [12] </ref>, one can obtain a relative error SQ algorithm where fl is a constant, fl = " log jQj = O k log n log (1=") . In the full paper, we give the details of this modification. <p> = fi (1="), while jHj corresponds to the number of noise rate guesses which is O fl fl log 1 Combining the sample complexities for estimating queries and hypothesis testing, and noting that fl fl = O (") (by combining the proof of Theorem 1 with a theorem of Kearns <ref> [12] </ref>), we obtain the following: Theorem 7 The total sample complexity required to sim ulate an SQ algorithm in the presence of classification noise is O "(12 b ) 2 + 2 fl fl fl (12 b ) 2 = log (jQj=ffi) when Q is finite, or O 2 fl fl
Reference: [13] <author> Michael Kearns and Ming Li. </author> <title> Learning in the presence of malicious errors. </title> <booktitle> In Proceedings of the 20 th Annual ACM Symposium on Theory of Computing, </booktitle> <address> Chicago, Illinois, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: In Corollary 3, these bounds are within logarithmic factors of both the O (") maximum allowable malicious error rate <ref> [13] </ref> and the (1=") lower bound on the sample complexity for noise-free PAC learning [8]. We also note that in this malicious-error tolerant PAC simulation, the sample, time, space and hypothesis size complexities are asymptotically identical to the corresponding complexities in our noise-free PAC simulation.
Reference: [14] <author> Philip D. Laird. </author> <title> Learning from Good and Bad Data. </title> <booktitle> Kluwer international series in engineering and computer science. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: Finally, we give a technique for hypothesis testing (required to de termine which noise rate guess was correct) which uses fewer example than the standard technique. This improved hypothesis testing is achieved by generalizing a result of Laird <ref> [14] </ref>. We begin be giving a new decomposition of P into quantities that may be "guessed" or estimated using the classification noise oracle. Let ^, and be the standard binary operators for AND, exclusive-OR and equivalence, respectively. <p> Assume that we have run our SQ algorithms with an accuracy parameter " 0 = "=2. Then we can find some "-good hypothesis through the use of the following generalization of a theorem due to Laird <ref> [14] </ref>. We give the proof of this lemma in the full paper. Lemma 6 Let H be a set of hypotheses at least one of which has true error rate at most e 1 , and let e 2 be any threshold strictly larger than e 1 .
Reference: [15] <author> D. Pollard. </author> <title> Rates of uniform almost-sure convergence for empirical processes indexed by unbounded classes of functions. </title> <type> Manuscript, </type> <year> 1986. </year>
Reference-contexts: In each case, we determine the complexity of the PAC algorithm as a function of the complexity of the SQ algorithm. In order to prove sample complexity bounds for these simulations, we make use of the d metric <ref> [15, 11] </ref>. Haussler gives sample complexity bounds sufficient for uniform convergence, with respect to the d metric, of probabilities and their estimates based on a sample. <p> We therefore look to take advantage of uniform convergence results based on the Vapnik-Chervonenkis dimension of the class of queries to be estimated. Consider the d metric defined over the non-negative reals as follows <ref> [15, 11] </ref>: d -(r; s) = - + r + s Haussler ([11]: Definition 3 and Theorems 1 and 7) effectively proves the following theorem on the sample size sufficient to ensure uniform convergence, with respect to the d metric, of empirical estimates to true probabilities.
Reference: [16] <author> Robert Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-226, </pages> <year> 1990. </year>
Reference-contexts: We do so by applying boosting techniques <ref> [9, 10, 16] </ref> and specifically, these techniques as applied in the statistical query model [1]. The proof of Theorem 8 is given in the appendix.
Reference: [17] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: 1 Introduction In this paper, we focus on the development of efficient, fault-tolerant algorithms for learning in the "Probably Approximately Correct" (PAC) model <ref> [17] </ref>. An algorithm for "PAC learning" a class of functions (concepts) uses examples drawn from an oracle (the environment) in order to approximate a hidden target function selected from the class. The examples are labelled according to the hidden function.
References-found: 17

