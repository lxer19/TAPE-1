URL: http://polaris.cs.uiuc.edu/reports/1428.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: EFFICIENT SCHEDULING OF PARALLEL TASKS IN A MULTIPROGRAMMING ENVIRONMENT  
Author: BY DALE ALLAN SCHOUTEN 
Degree: THESIS Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science in the Graduate College of the  
Date: 1990  
Address: 1986 M.S., University of Illinois,  1995 Urbana, Illinois  
Affiliation: B.S.C.E., University of Arizona,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [ABLL91] <author> Thomas E. Anderson, Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <booktitle> In 13th ACM Symposium on Operating Systems Principles, </booktitle> <volume> volume 25, </volume> <pages> pages 95-109. </pages> <publisher> ACM Sigops, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: The Process Control Approach [Tuc93] relies on periodic redistribution of processors between jobs. A user level scheduler schedules tasks within a process and gives up processors at safe points, at which no locks are held. Scheduler activations <ref> [ABLL91] </ref> provide an effective form of kernel support for user level threading, while minimizing the need for actual kernel calls. A scheduler activation contains both a user level and kernel level execution context. User level threads are executed in a scheduler activation. <p> This support should take the form of direct allocation and deallocation of processors to an autoscheduled program. An interface such as Scheduler Activations <ref> [ABLL91] </ref> becomes necessary. * Load balancing | Dynamic load balancing is an important feature of autosched uling. Any implementation of autoscheduled threads takes advantage of this. * Nested Parallelism | Many environments which support parallel loops only support parallelism at the outmost level. Any inner parallel loops are run serially. <p> In order to take this into account, a limited two-way interaction between the kernel and the user level is required. 4.1 Various Approaches To Multiprogramming Support First consider the approaches taken in two related schemes; scheduler activations and the process control approach. 57 4.1.1 Scheduler Activations Scheduler activations <ref> [ABLL91] </ref> provide an effective form of kernel support for user level threading, while minimizing the need for actual kernel calls. A scheduler activation contains both a user level and kernel level execution context. User level threads are executed in a scheduler activation, which provides the stack used by the thread.
Reference: [ALL89] <author> Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> The performance implications of thread management alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <month> De-cember </month> <year> 1989. </year>
Reference: [ASU86] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers : Principles, Techniques and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <month> March </month> <year> 1986. </year>
Reference: [Bec93] <author> Carl Josef Beckmann. </author> <title> Hardware and Software for Functional and Fine Grain Parallelism. </title> <type> PhD thesis, </type> <institution> UIUC, </institution> <month> Oct </month> <year> 1993. </year>
Reference-contexts: The worst imbalance is a function of the largest integer, so reducing the size of tasks promotes load balancing, albeit at the cost of increased overhead. 24 3.3 Issues In The Implementation Of An Autoscheduling Library Current implementations of autoscheduling <ref> [Mor95, Bec93] </ref> are done at the machine level. This requires rewriting a compiler to generate machine specific autoscheduling code. A library implementation of autoscheduling would allow a user, or a machine independent source-to-source code restructurer to write code to take advantage of au-toscheduling. <p> The counter algorithm <ref> [Bec93] </ref> is used to implement autoscheduling, and works as follows. For every edge e p!q , task function h p contains in its exit block an instruction that decrements the dependence counter for h q . If e p!q is a data dependence, the counter is decremented by one. <p> This information (about which edges need to be signaled) is available at compile time, so the signaling can be done efficiently at run-time. Beckmann <ref> [Bec93] </ref> describes techniques to minimize the number of actual decrements that are required. In order to transform a function, F , to run in parallel using autoscheduling, the following transformation is performed. A call to the function is replaced by a construction of a related class, F C .
Reference: [BMR94] <author> T. P. Baker, Framk Mueller, and Viresh Rustagi. </author> <title> Experience with a prototype of the posix "minimal realtime system profile". </title> <booktitle> In Proceedings of the 11th IEEE Workshop Systems and Software, </booktitle> <pages> pages 12-16, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: With user level threads, a user can obviate these problems. Threads have long been used as a paradigm for asynchronous execution of different instruction streams. Operating systems <ref> [CJR87, TRG + 87, GL91, BMR94] </ref> and simulation packages [Gru92, LWP93] have relied on threads as a model for concurrency. Threads are also useful as a model for parallel programming to take advantage of parallelization on multiprocessor machines.
Reference: [CD90] <author> Eric C. Cooper and Richard P. Draves. </author> <title> C threads. </title> <note> Draft of 11 Sept. </note> <year> 1990, </year> <month> September </month> <year> 1990. </year>
Reference-contexts: Chant [HCM94] is a threads model and implementation of `talking threads' for a distributed memory system. Talking threads communicate via point-to-point message passing. Solaris [Sol93] and Mach <ref> [CD90] </ref> are multithreaded versions of Unix. They supply kernel support for a user level threads package by allocating multiple kernel threads. In the simplest case, there is a one-to-one correspondence between kernel threads and user threads; one kernel thread is created for each user thread.
Reference: [CG72] <author> E. G. Jr. Coffman and R. L. Graham. </author> <title> Optimal scheduling on two processor systems. </title> <journal> Acta Informatica, </journal> <volume> 1(3), </volume> <year> 1972. </year>
Reference-contexts: An optimal schedule is one in which the total runtime of the schedule is less than or equal to the total runtime of every other schedule. Given the runtime for each task, finding an optimal schedule is in general NP-complete <ref> [CG72] </ref>. Of more importance in the context of this thesis are scheduling algorithms in which the run time of a particular task is unknown at the time it is scheduled.
Reference: [CGJ78] <author> E. G. Jr. Coffman, M. R. Garey, and D. S. Johnson. </author> <title> An application of bin-packing to multiprocessor scheduling. </title> <journal> SIAM J. of Computing, </journal> <volume> 7(1), </volume> <month> February 78. </month>
Reference-contexts: As a result, a significant overhead can destroy the benefits of exploiting fine grained parallelism. Autoscheduling can simplify many problems associated with overhead and synchronization through the use of compile-time dependence analysis and optimizations. 2 Scheduling, in its simplest form, can be modeled as a bin-packing problem <ref> [CGJ78] </ref>, packing integers into a collection of bins, where bins correspond to processors and integers represent the time required to complete a particular task.
Reference: [CH90] <author> Jyh-Herng Chow and Williams Ludwell Harrison. Switch-stacks: </author> <title> A scheme for microtasking nested parallel loops. </title> <booktitle> In Supercomputing 90, </booktitle> <pages> pages 190-199, </pages> <month> Nov. </month> <year> 1990. </year> <month> 131 </month>
Reference-contexts: The first two rules minimize the overhead and maximize locality in creating and running tasks. The last rule allows for stack allocation of PCB's, but creates load-balance problems as one processor potentially sits idle waiting for others to finish. 17 Switch Stacks <ref> [CH90] </ref> are a scheme to handle nested parallelism by actually swapping stacks between processors, so that no one processor is left waiting for others to finish.
Reference: [CJR87] <author> R. Campbell, G. Johnston, and V. Russo. </author> <title> Choices, (class hierarchical open interface for custom embedded systems). </title> <journal> Operating Systems Review, </journal> <volume> 21(3) </volume> <pages> 9-17, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: With user level threads, a user can obviate these problems. Threads have long been used as a paradigm for asynchronous execution of different instruction streams. Operating systems <ref> [CJR87, TRG + 87, GL91, BMR94] </ref> and simulation packages [Gru92, LWP93] have relied on threads as a model for concurrency. Threads are also useful as a model for parallel programming to take advantage of parallelization on multiprocessor machines.
Reference: [D. 94] <author> D. Bailey, et al. </author> <title> The nas parallel benchmarks. </title> <type> Technical Report RNR-95-007, </type> <institution> RNR, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Two Dimensional Fast Fourier Transform Fortran code that performs a two dimensional fast Fourier transform (FFT2) was obtained from CMU's "Task Parallel Suite" <ref> [D. 94] </ref> and hand-parallelized with the C++ nanoThreads library. There are two nested parallel loops that are exploited. There is also one routine performing matrix transpose that exploits functional parallelism. Perfect Benchmark TRFD A C++ version of the Perfect Club benchmark TRFD was implemented with the nanoThreads library.
Reference: [DBRD91] <author> Richard P. Draves, Brian N. Bershad, Richard F. Rashid, and Randall W. Dean. </author> <title> Using continuations to implement thread management and communication in operating systems. </title> <booktitle> In 13th ACM Symposium on Operating Systems Principles, </booktitle> <volume> volume 25, </volume> <pages> pages 122-136. </pages> <publisher> ACM Sigops, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: Furthermore, more synchronization is required as execution must stall at certain points in program (in the exit blocks of compound nodes, for example). In 38 the ideal model, or in a machine level implementation the exit code could be performed without need of stalling by using continuations <ref> [DBRD91] </ref>. Continuations are useful because they obviate the need for stacks. Traditional programming languages depend on the stack to determine return values when returning from subroutines. Continuations provide an alternate destination for tasks. This is more closely in line with the basic nanoThreads model.
Reference: [EAL93] <author> Dawson R. Engler, Gregory R. Andrews, and David K. Lowenthal. </author> <title> Efficient support for fine-grain parallelism. </title> <type> Technical Report TR 93-13, </type> <institution> University of Arizona, </institution> <year> 1993. </year>
Reference-contexts: This allows the structures containing the threads to be reused. It is especially useful for loop parallelism as it allows elegant and efficient facilities to specify regular dependences between iterations in an iteration space, and allows for self scheduling or chunk scheduling. Filaments were proposed in <ref> [EAL93] </ref> as a low overhead alternative to threads for very fine grain parallelism. Filaments eliminate overhead by minimizing the amount of state they store. The major overhead they eliminate is the stack, which leaves an inability to block. This still allows for run-to-completion threads, barrier synchronization and 13 fork/join synchronization.
Reference: [EZ93] <author> Derek L. Eager and John Zahorjan. Chores: </author> <title> Enhanced run-time support for shared-memory parallel computing. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> February 93. </month>
Reference-contexts: It is useful for simulating parallel systems and taking detailed measurements of such a system. For such threads systems, kernel support is not required as there is, at any given point, only one processor being used. Chores <ref> [EZ93] </ref> allow sets of threads (for example, multiple iterations of a given loop) to be grouped together, eliminating the overhead involved in creating each thread separately. This allows the structures containing the threads to be reused.
Reference: [FOW87] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The Control Dependence Graph is similar to the DDG, except that the edges represent Control Dependences. A node is control dependent on a particular branch if the taking of the branch determines whether the node will execute. The Program Dependence Graph <ref> [FOW87] </ref> combines the DDG and CDG in one structure that contains both Data Dependence edges and Control Dependence Edges. 2.2 Thread Models Threads have been widely used to exploit or simulate parallelism, and to enhance the utilization of resources. While the term is widely used, it is also widely defined.
Reference: [FR90] <author> Dror G. Feitelson and Larry Rudolph. </author> <title> Wasted resources in gang scheduling. </title> <booktitle> In Proceedings of the 5th Annual Jerusalem Conference on Information Technology, </booktitle> <pages> pages 127-136. </pages> <publisher> IEEE, </publisher> <month> October </month> <year> 1990. </year>
Reference-contexts: Kernel threads are scheduled by the kernel, without user influence, while user level threads are scheduled without consideration of the kernel. Gang scheduling <ref> [FR90] </ref> is a simple technique for allocating multiple processors to a parallel job. The basic idea is to simultaneously allocate a fixed number of processors to the job. This ensures that there are no problems related to tasks stalling while waiting for synchronizing signals from processors that are swapped out.
Reference: [Gir91] <author> Milind Baburao Girkar. </author> <title> Functional parallelism theoretical foundations and implementations. </title> <type> Technical Report 1182, </type> <institution> CSRD, </institution> <month> Dec </month> <year> 1991. </year>
Reference: [GL89] <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, MD, </address> <year> 1989. </year>
Reference-contexts: The two outermost loops are run in parallel as doall loops. The size of the parallel loops is regular, i.e. each loop iteration performs the same amount of work. Strassen's Matrix Multiply An implementation of Strassen's matrix multiply algorithm (SMM) <ref> [GL89] </ref> was also written in C++ using the nanoThreads library. Strassen's algorithm is a recursive algorithm that breaks down a matrix into four quadrants, recursively performs multiplies on the quadrants and performs a combination of matrix adds and subtracts to determine the result.
Reference: [GL91] <author> Bill O. Gallmeister and Chris Lanier. </author> <title> Early experience with posix 1003.4 and 1003.4a. </title> <booktitle> In Proceedings. Twelfth Real-Time Systems Symposium, </booktitle> <pages> pages 190-198. </pages> <publisher> IEEE, IEEE Comput. Soc. Press, </publisher> <month> December </month> <year> 1991. </year>
Reference-contexts: With user level threads, a user can obviate these problems. Threads have long been used as a paradigm for asynchronous execution of different instruction streams. Operating systems <ref> [CJR87, TRG + 87, GL91, BMR94] </ref> and simulation packages [Gru92, LWP93] have relied on threads as a model for concurrency. Threads are also useful as a model for parallel programming to take advantage of parallelization on multiprocessor machines.
Reference: [Gru92] <author> Dirk C. Grunwald. </author> <title> User's Guide to AWESIME-II A Widely Extensible Simulation Environment, </title> <year> 1992. </year>
Reference-contexts: With user level threads, a user can obviate these problems. Threads have long been used as a paradigm for asynchronous execution of different instruction streams. Operating systems [CJR87, TRG + 87, GL91, BMR94] and simulation packages <ref> [Gru92, LWP93] </ref> have relied on threads as a model for concurrency. Threads are also useful as a model for parallel programming to take advantage of parallelization on multiprocessor machines. Using threads to increase performance on multiprocessor machines involves unique considerations of efficiency, scheduling and resource sharing. <p> Generally, a memory space is associated with the process, so multiple threads share the same memory address space. The simplest threads implementations are not used for parallelism, but only for simulation or increased processor utilization <ref> [Gru92, LWP93] </ref>. Awesime [Gru92] is a simulation environment in which threads are the basic unit of simulated parallelism. It is useful for simulating parallel systems and taking detailed measurements of such a system. <p> Generally, a memory space is associated with the process, so multiple threads share the same memory address space. The simplest threads implementations are not used for parallelism, but only for simulation or increased processor utilization [Gru92, LWP93]. Awesime <ref> [Gru92] </ref> is a simulation environment in which threads are the basic unit of simulated parallelism. It is useful for simulating parallel systems and taking detailed measurements of such a system.
Reference: [HCM94] <author> Matthew Haines, David Cronk, and Piyush Mehrotra. </author> <title> On the design of chant: A talking threads package. </title> <type> Technical Report 94-25, </type> <institution> NASA ICASE, </institution> <year> 1994. </year>
Reference-contexts: This still allows for run-to-completion threads, barrier synchronization and 13 fork/join synchronization. To deal with more general blocking, they suggest implementing Filaments on top of a heavier-weight threads package, using its threads as servers for the filaments. Chant <ref> [HCM94] </ref> is a threads model and implementation of `talking threads' for a distributed memory system. Talking threads communicate via point-to-point message passing. Solaris [Sol93] and Mach [CD90] are multithreaded versions of Unix. They supply kernel support for a user level threads package by allocating multiple kernel threads.
Reference: [HS91] <author> S. F. Hummel and E. Schonberg. </author> <title> Low-overhead scheduling of nested parallelism. </title> <journal> IBM J. Res. Develp., </journal> 35(5/6):743-765, Sept/Nov 1991. <volume> 132 </volume>
Reference-contexts: The issue of nested parallelism is of particular importance in the presence of non-loop parallelism. The number of different parallel tasks at a given level is typically small so nested parallelism is needed to allow for a larger degree of parallelism. <ref> [HS91] </ref> considers many of the issues involved in dealing with nested loop parallelism.
Reference: [KDLS86] <author> D. J. Kuck, E. S. Davidson, D. H. Lawrie, and A. H. Sameh. </author> <title> Parallel supercomputing today and the cedar approach. </title> <journal> Science, </journal> <volume> 231(4740) </volume> <pages> 967-974, </pages> <month> February 28 </month> <year> 1986. </year>
Reference-contexts: The access times to different portions of memory, however, may vary. The time variation may be due to a hierarchical memory <ref> [KDLS86] </ref>, cache locality, or local and non-local memories, but the exact cause is transparent to the user. Given the nanoThreads model, there are several potential problems that can lead to inefficiencies in a NUMA system: Hot Spot Contention There are several potential hot spots in the standard nanoThreads model.
Reference: [KKP + 81] <author> D. J. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proceedings of the 8-th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 207-218, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: There are a variety of Parallel Program Graphs [Sar92, SS93] used towards this end. The Data Dependence Graph <ref> [KKP + 81, Wol82] </ref> is a graph in which the edges represent data dependences between nodes, which may represent statements, instructions or even blocks of code. Data dependences arise when two nodes reference the same memory location and at least one of them writes to that location.
Reference: [LHM91] <author> S. L. Lyons, T. J. Hanratty, and J. B. MacLaughlin. </author> <title> Large-scale computer simulation of fully developed channel flow with heat transfer. </title> <journal> International Journal of Numerical Methods for Fluids, </journal> <volume> 13 </volume> <pages> 999-1028, </pages> <year> 1991. </year>
Reference-contexts: Only three loops were parallelized. Two of the parallel loops are perfectly nested, with the inner one being triangular. 93 This is the kernel of a Fourier-Chebyshev spectral Computational Fluid Dynamics (CFD) code <ref> [LHM91] </ref>. The task graph is shown in Figure 5.6. It consists of 4 stages involving matrix operations.
Reference: [LWP93] <author> Sun 4 User's manual, </author> <year> 1993. </year>
Reference-contexts: With user level threads, a user can obviate these problems. Threads have long been used as a paradigm for asynchronous execution of different instruction streams. Operating systems [CJR87, TRG + 87, GL91, BMR94] and simulation packages <ref> [Gru92, LWP93] </ref> have relied on threads as a model for concurrency. Threads are also useful as a model for parallel programming to take advantage of parallelization on multiprocessor machines. Using threads to increase performance on multiprocessor machines involves unique considerations of efficiency, scheduling and resource sharing. <p> Generally, a memory space is associated with the process, so multiple threads share the same memory address space. The simplest threads implementations are not used for parallelism, but only for simulation or increased processor utilization <ref> [Gru92, LWP93] </ref>. Awesime [Gru92] is a simulation environment in which threads are the basic unit of simulated parallelism. It is useful for simulating parallel systems and taking detailed measurements of such a system.
Reference: [Mor95] <author> Jose Moreira. </author> <title> On the Implementation and Effectiveness of Autoscheduling for Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois, </institution> <year> 1995. </year>
Reference-contexts: The worst imbalance is a function of the largest integer, so reducing the size of tasks promotes load balancing, albeit at the cost of increased overhead. 24 3.3 Issues In The Implementation Of An Autoscheduling Library Current implementations of autoscheduling <ref> [Mor95, Bec93] </ref> are done at the machine level. This requires rewriting a compiler to generate machine specific autoscheduling code. A library implementation of autoscheduling would allow a user, or a machine independent source-to-source code restructurer to write code to take advantage of au-toscheduling.
Reference: [NWK85] <author> John Neter, William Wasserman, and Michael H. Keutner. </author> <title> Applied Linear Statistical Models. Erwin, </title> <address> 2 edition, </address> <year> 1985. </year>
Reference-contexts: Figures 3.12 through 3.21 show the predicted values of overhead, as determined by equation 5.1 vs. measured values from which the coefficients were determined. The coefficient of multiple determination, R 2 <ref> [NWK85] </ref> was calculated for each line to measure the fit of the line.
Reference: [PBK93] <author> C. D. Polychronopoulos, Nawaf Bitar, and Steve Kleiman. nanothreads: </author> <title> A user-level threads architecture. </title> <booktitle> In Proceedings of the ACM Symposium on Principles of Operating Systems, </booktitle> <year> 1993. </year>
Reference-contexts: Exploitation of nested parallelism can increase utilization and enhance load balance by making available more parallelism. 3.4 The NanoThreads Model NanoThreads <ref> [PBK93] </ref> is a threads architecture that combines low-overhead threads and autoscheduling. Each nanoThread corresponds to an HTG task.
Reference: [Pol90] <author> Constantine D. Polychronopoulos. </author> <title> Auto-scheduling: Control flow and data flow come together. </title> <type> Technical Report 1058, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: Of more importance in the context of this thesis are scheduling algorithms in which the run time of a particular task is unknown at the time it is scheduled. Dynamic scheduling lends itself to this situation, as conditions change unpredictably during program execution. 3.2 Autoscheduling Autoscheduling <ref> [Pol90] </ref> is a compiler-based scheduling environment which facilitates maximum exploitation of parallelism in a program. It addresses several important issues in parallel programming that inhibit efficient exploitation of parallelism.
Reference: [Sar92] <author> V. Sarkar. </author> <title> A concurrent execution semantics for parallel program graphs and program dependence graphs. </title> <editor> In Utpal Banerjee, David Gelernter, Alex Nicolau, and David Padua, editors, </editor> <booktitle> Fifth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 16-30. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1992. </year>
Reference-contexts: A parallelizing compiler, in addition to 11 standard optimization information, requires information that will assist in determining which parts of a program may be running concurrently, while preserving the semantics of the original serial code. There are a variety of Parallel Program Graphs <ref> [Sar92, SS93] </ref> used towards this end. The Data Dependence Graph [KKP + 81, Wol82] is a graph in which the edges represent data dependences between nodes, which may represent statements, instructions or even blocks of code.
Reference: [Sol93] <author> Solaris 2.2 User's manual, </author> <year> 1993. </year>
Reference-contexts: To deal with more general blocking, they suggest implementing Filaments on top of a heavier-weight threads package, using its threads as servers for the filaments. Chant [HCM94] is a threads model and implementation of `talking threads' for a distributed memory system. Talking threads communicate via point-to-point message passing. Solaris <ref> [Sol93] </ref> and Mach [CD90] are multithreaded versions of Unix. They supply kernel support for a user level threads package by allocating multiple kernel threads. In the simplest case, there is a one-to-one correspondence between kernel threads and user threads; one kernel thread is created for each user thread.
Reference: [SS93] <author> Vivek Sarkar and Barbara Simons. </author> <title> Parallel program graphs and their classification. </title> <editor> In Utpal Banerjee, David Gelernter, Alex Nicolau, and David Padua, editors, </editor> <booktitle> Sixth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 633-655. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: A parallelizing compiler, in addition to 11 standard optimization information, requires information that will assist in determining which parts of a program may be running concurrently, while preserving the semantics of the original serial code. There are a variety of Parallel Program Graphs <ref> [Sar92, SS93] </ref> used towards this end. The Data Dependence Graph [KKP + 81, Wol82] is a graph in which the edges represent data dependences between nodes, which may represent statements, instructions or even blocks of code.
Reference: [TRG + 87] <author> A. Tevanian, R. Rashid, D. Golub, D. Black, E. Cooper, and M. Young. </author> <title> Mach threads and the unix kernel: The battle for control. </title> <booktitle> In Proc. of the Summer 1987 USENIX Conference, </booktitle> <pages> pages 185-197, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: With user level threads, a user can obviate these problems. Threads have long been used as a paradigm for asynchronous execution of different instruction streams. Operating systems <ref> [CJR87, TRG + 87, GL91, BMR94] </ref> and simulation packages [Gru92, LWP93] have relied on threads as a model for concurrency. Threads are also useful as a model for parallel programming to take advantage of parallelization on multiprocessor machines.
Reference: [Tuc93] <author> Andrew Tucker. </author> <title> Efficient Schdeuling on Multiprogrammed Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> Dec </month> <year> 1993. </year> <month> 133 </month>
Reference-contexts: If a job requires N processors, and there are N 1 available, they cannot be used. Similarly, if N + 1 processors are available, only N can be used. The Process Control Approach <ref> [Tuc93] </ref> relies on periodic redistribution of processors between jobs. A user level scheduler schedules tasks within a process and gives up processors at safe points, at which no locks are held. <p> This situation is analogous to the preempted processor above, as the number of Scheduler Activations is now greater than the number of available processors. 4.1.2 The Process Control Approach In the process control approach <ref> [Tuc93] </ref>, the user level performs control over the processes, i.e. the number of threads running, while the kernel performs processor allocation. This requires kernel interface support for processor partitioning.

References-found: 35

