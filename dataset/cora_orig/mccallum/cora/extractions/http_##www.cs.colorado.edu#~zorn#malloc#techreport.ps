URL: http://www.cs.colorado.edu/~zorn/malloc/techreport.ps
Refering-URL: http://www.cs.colorado.edu/~zorn/malloc/overview.html
Root-URL: http://www.cs.colorado.edu
Title: Predicting References to Dynamically Allocated Objects  
Author: Matthew L. Seidl and Benjamin G. Zorn 
Date: January 1997  
Address: Campus Box 430  Boulder, CO 80309-0430 USA  Boulder  
Affiliation: Department of Computer Science  University of Colorado  ffi University of Colorado at  
Pubnum: CU-CS-826-97  
Abstract: Technical Report CU-CS-826-97 Department of Computer Science Campus Box 430 University of Colorado Boulder, Colorado 80309 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Thomas Ball and James R. Larus. </author> <title> Efficient path profiling. </title> <booktitle> In Proceedings of MICRO-29, </booktitle> <address> Paris, France, </address> <month> December </month> <year> 1996. </year>
Reference-contexts: This problem is similar in spirit to the problem of determining a trace of basic blocks (or path) within a procedure. There has been recent work in the area of path profiling by Ball and Larus <ref> [1] </ref> which may be used as a starting point for the stack contents implementation.
Reference: [2] <author> David Barrett and Benjamin Zorn. </author> <title> Using lifetime predictors to improve memory allocation performance. </title> <booktitle> In SIGPLAN'93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 187-196, </pages> <address> Albuquerque, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: We refer to this prediction as HR (highly referenced) prediction, and to the algorithms used for prediction has HR predictors. The technique we use is also analogous to a technique we previously published that predicts the lifetimes of heap-allocated objects <ref> [2] </ref>. Our approach uses profile-based optimization to first identify frequently referenced heap objects based on training inputs, and then to predict when such objects are allocated based on information present at the time of allocation. <p> Perhaps the closest work in spirit to our work is the previous work of Barrett and Zorn, in which we attempted to predict object lifetimes (and short-lived objects in particular) using information present at the time of allocation <ref> [2] </ref>. While the methods used in this paper are similar, our results are quite different. In particular, in the current work, we investigate a wider variety of predictions methods to solve the more important problem of improving reference locality. <p> As indicated in the previous figure, however, only a small fraction of heap references can directed to the HR segment in the Sis program. 5.2 Combining Stack Depth with Size Barrett and Zorn <ref> [2] </ref> concluded that combining both size and stack context information resulted in more effective prediction of short-lived objects in C programs. In this section, we consider the effect of using this combination for HR prediction.
Reference: [3] <author> Brad Calder, Dirk Grunwald, and Benjamin Zorn. </author> <title> Quantifying behavioral differences between C and C++ programs. </title> <journal> Journal of Programming Languages, </journal> <volume> 2(4) </volume> <pages> 313-351, </pages> <year> 1994. </year>
Reference-contexts: As a result, the use of dynamic storage allocation in application programs has increased dramatically. A recent study of ours comparing C and C++ programs showed that over a range of application domains, heap objects are allocated almost ten times more frequently in C++ than in C <ref> [3] </ref>. Because all objects in Java must be allocated on the heap, dynamic storage allocation in Java is likely to be even more frequent than in C++ [8]. At the same time, while basic processor cycle time has increased dramatically, the speed of bulk memory has not increased as fast.
Reference: [4] <author> Craig Chambers, Jeffrey Dean, and David Grove. </author> <title> Whole-program optimization of object-oriented languages. </title> <type> Technical Report UW-CSE-96-06-02, </type> <institution> University of Washington Department of Computer Science and Engineering, </institution> <address> Seattle, WA, </address> <year> 1996. </year>
Reference-contexts: This predictor will be harder to implement, but uses more context sensitive information about the object allocated and therefore tends to make better predictions. This kind of optimization is referred to as k-CCP by Chambers <ref> [4] </ref> (where k is the stack depth used). The depth the predictor looks to is of particular importance with this predictor. If the depth is set too deep, the predictor can overspecialize the prediction and not generate something that is useful across data sets.
Reference: [5] <author> David A. Cohn and Satinder Singh. </author> <title> Predicting lifetimes in dynamically allocated memory. </title> <booktitle> In Advances in Neural Information Processing Systems 9, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: In very recent work, Cohn and Singh showed that object lifetimes in allocation-intensive programs can be predicted using decision trees to extract relevant static features present at an object's allocation <ref> [5] </ref>. 3 Algorithms The section discusses the optimization process we use to do HR prediction, and describes the specific HR predictors we have considered and their implementation. 3.1 Overview the program to be optimized, which can be done either with a special compiler, or as we do, with a executable transformation
Reference: [6] <author> Robert Courts. </author> <title> Improving locality of reference in a garbage-collecting memory management system. </title> <journal> Communications of the ACM, </journal> <volume> 31(9) </volume> <pages> 1128-1138, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: This previous work did not consider the more speculative issue of predicting reference locality as we do in this paper. There have also been a number of papers investigating the effect of heap organization on reference locality in garbage collected languages <ref> [6, 13, 19] </ref>, including several recent papers that specifically consider the effect 2 of garbage collection on cache performance [7, 15, 20, 21]. This work differs from ours in its focus. <p> While much of the related garbage collection work has investigated how generational garbage collection interacts with processor cache architecture, none of the previous work we are aware of has attempted to classify objects using profiles and segregate them as we do. The work of Courts <ref> [6] </ref>, in which the working set for an entire Lisp system is obtained by performing a "training" of the system, is the closest in this group to our work.
Reference: [7] <author> Amer Diwan, David Tarditi, and Eliot Moss. </author> <title> Memory subsystem performance of programs using copying garbage collection. </title> <booktitle> In Conference Record of the 21st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL'94), </booktitle> <pages> pages 1-14, </pages> <address> Portland, Oregon, </address> <month> January 17-21, </month> <year> 1994. </year> <note> ACM Press. </note>
Reference-contexts: There have also been a number of papers investigating the effect of heap organization on reference locality in garbage collected languages [6, 13, 19], including several recent papers that specifically consider the effect 2 of garbage collection on cache performance <ref> [7, 15, 20, 21] </ref>. This work differs from ours in its focus.
Reference: [8] <author> James Gosling, Bill Joy, and Guy Steele. </author> <title> The Java Language Specification. The Java Series. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <year> 1997. </year>
Reference-contexts: Because all objects in Java must be allocated on the heap, dynamic storage allocation in Java is likely to be even more frequent than in C++ <ref> [8] </ref>. At the same time, while basic processor cycle time has increased dramatically, the speed of bulk memory has not increased as fast. As a result, modern architectures now include several levels of cache between the processor and main memory to hide this latency.
Reference: [9] <author> David Grove, Jeffrey Dean, Charles Garrett, and Craig Chambers. </author> <title> Profile-guided receiver class prediction. </title> <booktitle> In Proceedings of the 1995 Conference on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <pages> pages 108-123, </pages> <address> Austin, TX, </address> <month> October </month> <year> 1995. </year>
Reference-contexts: An alternate method of establishing the calling context is to selectively inline the chains of callers that result in predicted allocations of highly referenced objects. If the number of such call chains is small, then the impact on program code size may be negligible. Research by Chambers et al <ref> [9] </ref> suggests that such selective inlining may be effective for certain optimizations. In future work, we intend to investigate the effectiveness of such inlining in the context of storage allocation optimizations. 4 Evaluation Methods The infrastructure we utilized for our studies was a network of high performance DEC Alpha multiprocessors.
Reference: [10] <author> Dirk Grunwald, Benjamin Zorn, and Robert Henderson. </author> <title> Improving the cache locality of memory allocation. </title> <booktitle> In SIGPLAN'93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 177-186, </pages> <address> Albuquerque, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Surprisingly little work has been done with the specific goal of improving locality of reference (especially in the cache) in programs with explicit storage management (e.g., use malloc). Recent work of ours surveyed existing malloc implementations with the goal of understanding what techniques they provide to support cache locality <ref> [10] </ref>. Our conclusion was that the existing methods, including eliminating boundary value tags, and providing a fast allocator front end to rapidly reuse freed objects, did provide substantially better reference locality than the simple first-fit algorithm. We suspected that reference locality could be increased beyond what is currently being done. <p> The poor reference locality characteristics of first-fit storage allocation [11] has prompted the improved "better fit" methods [17] that are now often used. As mentioned, in our own previous work, we looked at existing malloc implementations with an eye to what impact they had on cache locality <ref> [10] </ref>. This previous work did not consider the more speculative issue of predicting reference locality as we do in this paper.
Reference: [11] <author> Donald E. Knuth. </author> <title> Fundamental Algorithms, </title> <booktitle> volume 1 of The Art of Computer Programming, chapter 2, </booktitle> <pages> pages 435-451. </pages> <publisher> Addison Wesley, </publisher> <address> Reading, MA, 2nd edition, </address> <year> 1973. </year>
Reference-contexts: The poor reference locality characteristics of first-fit storage allocation <ref> [11] </ref> has prompted the improved "better fit" methods [17] that are now often used. As mentioned, in our own previous work, we looked at existing malloc implementations with an eye to what impact they had on cache locality [10].
Reference: [12] <author> S. McFarling. </author> <title> Program optimization for instruction caches. </title> <booktitle> In 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 183-191, </pages> <address> Boston, MA, </address> <month> April </month> <year> 1989. </year>
Reference-contexts: Organizing program code to improve its locality of reference in the virtual memory (e.g., see [14]) and cache (e.g., see <ref> [12] </ref>) has been of interest for many years. These methods work because frequently executed code segments can be readily discovered using program profiling and/or static profile estimation [18]. In this paper, we investigate the analogous problem of predicting frequently referenced heap objects at the time they are allocated.
Reference: [13] <author> David A. Moon. </author> <title> Garbage collection in a large Lisp system. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on LISP and Functional Programming, </booktitle> <pages> pages 235-246, </pages> <address> Austin, Texas, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: This previous work did not consider the more speculative issue of predicting reference locality as we do in this paper. There have also been a number of papers investigating the effect of heap organization on reference locality in garbage collected languages <ref> [6, 13, 19] </ref>, including several recent papers that specifically consider the effect 2 of garbage collection on cache performance [7, 15, 20, 21]. This work differs from ours in its focus.
Reference: [14] <author> Judith B. Peachey, Richard B. Bunt, and Charles J. Colburn. </author> <title> Some empirical observations on program behavior with applications to program restructuring. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(2):188-193, </volume> <month> February </month> <year> 1985. </year>
Reference-contexts: Organizing program code to improve its locality of reference in the virtual memory (e.g., see <ref> [14] </ref>) and cache (e.g., see [12]) has been of interest for many years. These methods work because frequently executed code segments can be readily discovered using program profiling and/or static profile estimation [18].
Reference: [15] <author> Mark B. Reinhold. </author> <title> Cache performance of garbage-collected programs. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 206-217, </pages> <address> Orlando, Florida, </address> <month> June 20-24, </month> <year> 1994. </year> <journal> SIGPLAN Notices, </journal> <volume> 29(6), </volume> <month> June </month> <year> 1994. </year>
Reference-contexts: There have also been a number of papers investigating the effect of heap organization on reference locality in garbage collected languages [6, 13, 19], including several recent papers that specifically consider the effect 2 of garbage collection on cache performance <ref> [7, 15, 20, 21] </ref>. This work differs from ours in its focus.
Reference: [16] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the ACM SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 196-205, </pages> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: section discusses the optimization process we use to do HR prediction, and describes the specific HR predictors we have considered and their implementation. 3.1 Overview the program to be optimized, which can be done either with a special compiler, or as we do, with a executable transformation tool (e.g., ATOM <ref> [16] </ref>, described in the next section). The instrumented program is then run with a number of training inputs that are intended to be representative of the program in actual use. <p> On these computers we used a number of DEC tools, such as DEC's C++ compiler, cxx, and ATOM. ATOM is a tool that allows programmers to instrument existing binaries with additional code to gather run time data, without interfering with that program's execution <ref> [16] </ref>. ATOM has the facility to insert arbitrary procedures into a binary at many different places. Code can be inserted per object, procedure, or even instruction.
Reference: [17] <author> C. J. Stephenson. </author> <title> Fast fits: New methods for dynamic storage allocation. </title> <booktitle> In Proceedings of the Ninth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 30-32, </pages> <address> Bretton Woods, NH, </address> <month> October </month> <year> 1983. </year>
Reference-contexts: The poor reference locality characteristics of first-fit storage allocation [11] has prompted the improved "better fit" methods <ref> [17] </ref> that are now often used. As mentioned, in our own previous work, we looked at existing malloc implementations with an eye to what impact they had on cache locality [10].
Reference: [18] <author> Tim A. Wagner, Vance Maverick, Susan Graham, and Michael Harrison. </author> <title> Accurate static estimators for program optimization. </title> <booktitle> In Proceedings of the SIGPLAN'94 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 85-96, </pages> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Organizing program code to improve its locality of reference in the virtual memory (e.g., see [14]) and cache (e.g., see [12]) has been of interest for many years. These methods work because frequently executed code segments can be readily discovered using program profiling and/or static profile estimation <ref> [18] </ref>. In this paper, we investigate the analogous problem of predicting frequently referenced heap objects at the time they are allocated. We refer to this prediction as HR (highly referenced) prediction, and to the algorithms used for prediction has HR predictors.
Reference: [19] <author> Paul R. Wilson, Michael S. Lam, and Thomas G. Moher. </author> <title> Effective static-graph reorganization to improve locality in garbage-collected systems. </title> <booktitle> In Proceedings of the 1991 SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 177-191, </pages> <address> Toronto, Ontario, </address> <month> June </month> <year> 1991. </year> <note> ACM. Published as SIGPLAN Notices 26(6), </note> <month> June </month> <year> 1992. </year> <month> 17 </month>
Reference-contexts: This previous work did not consider the more speculative issue of predicting reference locality as we do in this paper. There have also been a number of papers investigating the effect of heap organization on reference locality in garbage collected languages <ref> [6, 13, 19] </ref>, including several recent papers that specifically consider the effect 2 of garbage collection on cache performance [7, 15, 20, 21]. This work differs from ours in its focus.
Reference: [20] <author> Paul R. Wilson, Michael S. Lam, and Thomas G. Moher. </author> <title> Caching considerations for generational garbage collection. </title> <booktitle> In SIGPLAN Symposium on LISP and Functional Programming, </booktitle> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: There have also been a number of papers investigating the effect of heap organization on reference locality in garbage collected languages [6, 13, 19], including several recent papers that specifically consider the effect 2 of garbage collection on cache performance <ref> [7, 15, 20, 21] </ref>. This work differs from ours in its focus.
Reference: [21] <author> Benjamin Zorn. </author> <title> The effect of garbage collection on cache performance. </title> <institution> Computer Science Technical Report CU-CS-528-91, University of Colorado, </institution> <address> Campus Box 430, Boulder, CO 80309, </address> <month> May </month> <year> 1991. </year> <month> 18 </month>
Reference-contexts: There have also been a number of papers investigating the effect of heap organization on reference locality in garbage collected languages [6, 13, 19], including several recent papers that specifically consider the effect 2 of garbage collection on cache performance <ref> [7, 15, 20, 21] </ref>. This work differs from ours in its focus.
References-found: 21

