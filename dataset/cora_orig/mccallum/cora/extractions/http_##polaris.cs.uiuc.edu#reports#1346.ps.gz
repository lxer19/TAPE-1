URL: http://polaris.cs.uiuc.edu/reports/1346.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: HARDWARE AND SOFTWARE FOR FUNCTIONAL AND FINE GRAIN PARALLELISM  
Author: BY CARL JOSEF BECKMANN 
Degree: THESIS Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Electrical Engineering in the Graduate College of the  
Date: 1989  
Address: 1984 M.S., University of Illinois,  1993 Urbana, Illinois  
Affiliation: B.S., Brown University,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Ramon D. Acosta, Jacob Kjelstrup, and H. C. Torng. </author> <title> An instruction issuing approach to enhancing performance in multiple functional unit processors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(9):815-828, </volume> <month> September </month> <year> 1986. </year>
Reference-contexts: Tomasulo's common data bus architecture featured similar capabilities and, in addition, implemented dynamic register renaming, allowing multiple instances of a given instruction to execute simultaneously [106]. A similar approach based on an instruction dispatch stack was proposed later by Acosta et al. <ref> [1] </ref>. Dataflow execution of a conventional dynamic instruction stream was also proposed in the HPS architecture [81]. In all of these approaches, instructions are issued to functional units in the same order encountered in a sequential execution, although instruction issuing can get ahead of instruction completion (thus exploiting pipeline parallelism). <p> By contrast, only MDG, FLO52Q and SPEC77 contain any significant amount of coarse grain functional parallelism, and in all three cases the average parallelism is less than a factor of two. Note that fine grain functional parallelism is closely related to what is commonly referred to as instruction-level parallelism <ref> [62, 94, 1, 55, 20, 98, 99, 109] </ref>. Instruction-level parallelism, however, may include some loop-level parallelism from innermost loops, if speculative execution, loop unrolling, or software pipelining are applied [62, 94, 55, 98, 99]. <p> Since the most frequently executed innermost loops in the Perfect Club R fl codes tend to be free of conditional branches, the fine grain functional parallelism figures of Table 5.4 are similar to basic block parallelism reported elsewhere <ref> [94, 1, 55, 20, 98, 99] </ref>. Note, however, that since we omit flow-of-control instructions from the HTG in order to measure arithmetic operation parallelism, our estimates of parallelism tend to be somewhat lower. Consider, for example, an innermost FORTRAN DO loop. <p> Dynamic Instruction Scheduling Another important issue in processor design is the instruction issuing mechanism. Conventional processors issue instructions in the order they are encountered in the program text (subject to control flow). Sophisticated Von Neumann architectures allow out-of-order execution of instructions <ref> [106, 81, 1, 14] </ref>, as do dataflow architectures [10, 32, 52, 76, 80, 28]. These architectures dynamically schedule instructions, subject to dependences evaluated at run time, in order to maximally utilize processors. <p> Larger loops will tend to use more registers, resulting in a smaller W max for a fixed register file size, and many dynamic scheduling architectures limit the number of look-ahead instructions in the dynamic instruction stream <ref> [1] </ref>, not the number of overlapped loop iterations. For the static scheduling experiments, modulo scheduling [90] was used to generate a static schedule.
Reference: [2] <author> Anant Agarwal, Beng-Hong Lim, David A. Kranz, and John Kubiatowicz. </author> <month> APRIL: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-112, </pages> <address> New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: It does not require single-assignment properties, and readily handles anti- and output-dependences. Chapters 2, 3 and 4 describe compiler algorithms needed to support our hardware, that are both practical and efficient. Multithreading has received a great deal of attention in the recent literature <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>. Almost all of these proposals involve changes to the hardware organization. <p> ADM 1.000 2.870 21.566 3.124 8.733 64.823 MDG 1.000 3.142 24.560 4.237 10.662 74.915 DYFESM 1.000 3.255 25.807 2.589 9.355 77.009 ARC2D 1.000 2.616 18.775 2.734 7.582 56.059 TRFD 1.000 3.156 24.715 2.669 9.137 73.815 FLO52Q 1.000 2.428 16.705 2.291 6.575 49.407 SPEC77 1.000 2.531 17.839 3.602 8.195 54.119 multithreading <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>, and various forms of dynamic instruction scheduling [106, 81, 14]. The focus here is not on the details of any particular method, but rather on the characteristics common to all of them. <p> Simply providing more outstanding loads may be an effective way of achieving this, without resorting to more complex approaches such as dataflow [10, 32, 52, 76, 80, 28] or multithreading <ref> [18, 104, 2, 113] </ref>. 123 CHAPTER 7 HARDWARE FOR FINE GRAIN ATG PARALLELISM The dynamic scheduling algorithms presented in Chapter 3 can be implemented in software with reasonable efficiency if task granularities are on the order of tens to hundreds of instructions. <p> As a means of avoiding processor idle time due to these delays, a number of multithreaded architectures have been proposed that attempt to hide such long-latency operations by maintaining multiple threads of execution on a single processor and rapidly context-switching between them <ref> [97, 104, 7, 52, 76, 80, 28, 79, 113, 47, 2] </ref>. This section proposes a method for achieving the same results on a conventional processor. Memory and synchronization operations are examples of operations incurring long periods of latency.
Reference: [3] <author> A. V. Aho, M. R. Garey, and J. D. Ullman. </author> <title> The transitive reduction of a directed graph. </title> <journal> SIAM Journal of Computing, </journal> <volume> 1(2) </volume> <pages> 131-137, </pages> <month> June </month> <year> 1972. </year>
Reference-contexts: Algorithms exist for computing the transitive reduction in O (-2:8 ) time (where is the number of nodes in G) <ref> [3] </ref>. 2. ATGs with control dependences. Girkar has shown that removing redundant data dependences in ATGs containing control dependences is an NP-complete problem [42]. <p> This can be done by computing the transitive closure of the graph in adjacency matrix form, then multiplying the original graph's adjacency matrix by this to obtain the indirect successors relation. Transitive closure and Boolean matrix multiplication can both be performed in O (-2:8 ) time <ref> [3] </ref>. Checking whether two tasks are conditional branch nodes and checking additional restrictions such as comparing against a granularity threshold, can be done in constant time, or O (-2 ) for all possible pairs. Step 4 thus requires O (-2:8 ) time complexity. Step 5.
Reference: [4] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: instructions, which must be adhered to for correct program execution. 2.1.1 Building the hierarchy The loop structure of the program is extracted by constructing its control flow graph (CFG), computing a dominator tree based on the CFG, and finding the back edges in the CFG based on the dominator tree <ref> [4] </ref>. By removing the back edges and collapsing each loop into a compound node, the body of each loop thus becomes an acyclic CFG, each of whose nodes is either a node in the original CFG, or a nested loop node [42]. Using the acyclic CFG and data flow analysis [4], <p> <ref> [4] </ref>. By removing the back edges and collapsing each loop into a compound node, the body of each loop thus becomes an acyclic CFG, each of whose nodes is either a node in the original CFG, or a nested loop node [42]. Using the acyclic CFG and data flow analysis [4], control and data dependences between nodes at a given level of the HTG are then computed. Consider the code example shown in Figure 2.1. <p> This can be done by considering each basic block in an ATG separately, removing redundant data dependences within a single basic block only. The definition of a basic block used here is similar to the standard definition <ref> [4] </ref>, where a basic block in a control flow graph consists of a sequence of statements with a single entry and a single exit point; once the first statement in a basic block is executed, the rest of the basic block is guaranteed to execute to completion. <p> For example, increasing the degree of loop unrolling or software pipelining increases the register pressure, which in turn hurts performance by forcing excessive spill code to be generated <ref> [4] </ref>. In hardware-based approaches, the degree of multithreading or register renaming, or the instruction lookahead buffer size may place limits on the effective window size. <p> In the example of Figure 8.8, the registers R0, R1 and R2, as well as x, thresh and M (in R9-R11) are invariant to the DOALL loop and thus need not be replicated. 2. Not all nonshared registers are live across SWITCH boundaries <ref> [4] </ref>. Only live registers need be replicated. Registers used as temporaries in between switch points can also be shared between threads. In this example, only nonshared registers R3 and R4 are live across the SWITCH of line 5b, and only R3, R4 and R7 are live across line 9b.
Reference: [5] <author> Alexander Aiken and Alexandru Nicolau. </author> <title> Perfect pipelining: A new loop parallelization technique. </title> <type> Technical Report 87-873, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York 14853-7501, </address> <month> October </month> <year> 1987. </year>
Reference-contexts: ATG parallelism also exists at coarser levels, where the nodes may be entire loops, subroutine calls, or basic blocks. Instruction-level parallelism is usually meant to refer to this instruction-level ATG parallelism. With techniques such as loop unrolling and software pipelining <ref> [90, 91, 5, 65] </ref>, however, inner loop parallelism is also exploited by multiple-instruction-issue machines. <p> This is possible if multiple loop iterations are allowed to execute concurrently on a single processor. This, in turn, can be accomplished by various methods such as loop unrolling, modulo scheduling <ref> [5, 21, 65, 90, 91, 31, 93] </ref>, 2 This discussion borrows the notation of Andrews and Polychronopoulos [9] wherein fi denotes the raw performance ratio of a "fast processor" to a "slow processor" in multiprocessor design trade-offs. 106 Table 6.2 Average Instruction Latency Measurements INT=1 FLOAT=1 INT=1 FLOAT=6 load latency load <p> These architectures dynamically schedule instructions, subject to dependences evaluated at run time, in order to maximally utilize processors. However, with appropriate compile-time instruction scheduling, more conventional architectures can also overlap the execution of unrelated instructions to improve utilization of pipelines <ref> [90, 91, 5, 65, 71] </ref>. This section attempts a quantitative comparison between static and dynamic scheduling in innermost loops, based on the HTG. In general it can be said that dynamic scheduling requires more sophisticated hardware than static scheduling. This extra complexity must be justified.
Reference: [6] <institution> Alliant Computer Systems Corporation. FX/SERIES Architecture Manual, </institution> <month> January </month> <year> 1986. </year> <title> Part Number 300-00001-B. </title>
Reference-contexts: Some architectures, such as the Alliant FX series, support a single outermost level of loop parallelism. If a parallel loop is encountered during the execution of an enclosing parallel loop, the inner loop is executed sequentially <ref> [6] </ref>. 5.2.2 Raw measurements Tables 5.1 and 5.2 present basic results using critical path analysis on six of the Perfect Club R fl codes. Table 5.1 shows results for statement-level HTGs, while Table 5.2 gives results for instruction-level HTGs. <p> Since the tree is analogous to a stack on a sequential machine, and is accessed like a stack from the point of view of each individual thread, it is sometimes referred to as a cactus stack <ref> [6, 114] </ref>. Memory allocation for the cactus stack is not as easy as for a sequential machine stack, since the dynamically changing tree cannot be laid out linearly in memory.
Reference: [7] <author> Robert Alverson, David Callahan, Daniel Cummings, Brian Koblenz, Allan Porterfield, and Burton Smith. </author> <title> The Tera computer system. </title> <booktitle> In Proceedings International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <year> 1990. </year>
Reference-contexts: The advantage of dynamic scheduling in the presence of cache misses is that lexically succeeding instructions that do not depend on the cache-missing load could be issued to keep the pipeline busy while the miss is being serviced <ref> [104, 7] </ref>. <p> As a means of avoiding processor idle time due to these delays, a number of multithreaded architectures have been proposed that attempt to hide such long-latency operations by maintaining multiple threads of execution on a single processor and rapidly context-switching between them <ref> [97, 104, 7, 52, 76, 80, 28, 79, 113, 47, 2] </ref>. This section proposes a method for achieving the same results on a conventional processor. Memory and synchronization operations are examples of operations incurring long periods of latency.
Reference: [8] <author> T. E. Anderson. </author> <title> The performance of spin-lock alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: The processor starts self-scheduling from the first queue entry it finds with available work. If no queue entries are found ready the idle processor spin waits <ref> [117, 8, 73] </ref> on the queue until a new entry is queued or a task in an existing entry becomes ready.
Reference: [9] <author> John B. Andrews and Constantine D. Polychronopoulos. </author> <title> An analytical approach to performance/cost modeling of parallel computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12 </volume> <pages> 343-356, </pages> <year> 1991. </year>
Reference-contexts: This is possible if multiple loop iterations are allowed to execute concurrently on a single processor. This, in turn, can be accomplished by various methods such as loop unrolling, modulo scheduling [5, 21, 65, 90, 91, 31, 93], 2 This discussion borrows the notation of Andrews and Polychronopoulos <ref> [9] </ref> wherein fi denotes the raw performance ratio of a "fast processor" to a "slow processor" in multiprocessor design trade-offs. 106 Table 6.2 Average Instruction Latency Measurements INT=1 FLOAT=1 INT=1 FLOAT=6 load latency load latency benchmark 1 10 100 3 30 300 ADM 1.000 2.870 21.566 3.124 8.733 64.823 MDG 1.000
Reference: [10] <author> Arvind and K. Gostelow. </author> <title> A computer capable of exchanging processors for time. </title> <booktitle> In IFIP Proceedings, </booktitle> <pages> pages 849-853, </pages> <year> 1977. </year>
Reference-contexts: The drawback of this is that compatibility with existing instruction sets is not possible. Our approach can be thought of as a restricted form of dataflow within a single processor. In classical dataflow, instructions are queued for execution as soon as all of their execution conditions have been met <ref> [10, 32] </ref>. This is signaled by the arrival of tokens carrying data as well 5 as synchronization information. The scheme developed here, by contrast, goes one step beyond the explicit token store concept [80] by making synchronization explicit and separating it from data transfer. <p> Dynamic Instruction Scheduling Another important issue in processor design is the instruction issuing mechanism. Conventional processors issue instructions in the order they are encountered in the program text (subject to control flow). Sophisticated Von Neumann architectures allow out-of-order execution of instructions [106, 81, 1, 14], as do dataflow architectures <ref> [10, 32, 52, 76, 80, 28] </ref>. These architectures dynamically schedule instructions, subject to dependences evaluated at run time, in order to maximally utilize processors. <p> Simply providing more outstanding loads may be an effective way of achieving this, without resorting to more complex approaches such as dataflow <ref> [10, 32, 52, 76, 80, 28] </ref> or multithreading [18, 104, 2, 113]. 123 CHAPTER 7 HARDWARE FOR FINE GRAIN ATG PARALLELISM The dynamic scheduling algorithms presented in Chapter 3 can be implemented in software with reasonable efficiency if task granularities are on the order of tens to hundreds of instructions.
Reference: [11] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Press, </publisher> <address> Boston, Mass., </address> <year> 1988. </year>
Reference-contexts: Well-known algorithms exist for determining dependence direction and dependence distance vectors, which describe the relative positions of the statement instances in iteration space with respect to any number of enclosing loops <ref> [11, 116, 72, 89] </ref>. Depending on the nature of the cross-iteration dependences, loop restructuring may be used to transform a loop nest such that one or more of the loops are parallel [12]. <p> While it was assumed above that task graphs contain only intraiteration dependences, the approach can be extended to include loop-carried dependences as follows. The HTG is constructed as described in Chapter 2, obtaining loops and ATGs with intraiteration dependences only. Then all loop-carried dependences and their associated dependence distances <ref> [61, 11, 116] </ref> are found. It is assumed below that the dependence distances are constants known at compile time. Next, exit code is generated for the bit vector or counter algorithm, using an array of ATG state variables instead of a single one, one array element for each iteration.
Reference: [12] <author> Utpal Banerjee, Rudolf Eigenmann, Alexandru Nicolau, and David Padua. </author> <title> Automatic program parallelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2), </volume> <month> February </month> <year> 1993. </year> <month> 164 </month>
Reference-contexts: Depending on the nature of the cross-iteration dependences, loop restructuring may be used to transform a loop nest such that one or more of the loops are parallel <ref> [12] </ref>. In the example in Figure 2.1, the inner loop containing statements 4 and 5 is sequential due to the cross-iteration dependence involving the reduction variable sum (n).
Reference: [13] <author> Carl J. Beckmann and Constantine D. Polychronopoulos. </author> <title> The effect of barrier synchro-nization and scheduling overhead on parallel loops. </title> <booktitle> In Proceedings 1989 International Conference on Parallel Processing, </booktitle> <pages> pages 200-204, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: It is assumed that scheduling is nonpreemptive, i.e., tasks are run to completion. 2.2.1 Scheduling parallel loops Loops are generally regarded as the largest source of parallelism within ordinary programs, and hence dynamic scheduling of parallel loops has received much attention in the literature to date <ref> [88, 13, 108, 51] </ref>. Dynamic scheduling of loops is not a topic of this thesis, since an array of existing techniques is available for our use. The loop scheduling problem consists of assigning iterations of the loop to processors for execution.
Reference: [14] <author> Carl J. Beckmann and Constantine D. Polychronopoulos. </author> <title> Microarchitecture support for dynamic scheduling of acyclic task graphs. </title> <booktitle> In Proceedings 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 140-148, </pages> <address> Portland, Oregon, December 1992. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: 3.255 25.807 2.589 9.355 77.009 ARC2D 1.000 2.616 18.775 2.734 7.582 56.059 TRFD 1.000 3.156 24.715 2.669 9.137 73.815 FLO52Q 1.000 2.428 16.705 2.291 6.575 49.407 SPEC77 1.000 2.531 17.839 3.602 8.195 54.119 multithreading [18, 47, 104, 113, 2, 79, 77, 17, 111], and various forms of dynamic instruction scheduling <ref> [106, 81, 14] </ref>. The focus here is not on the details of any particular method, but rather on the characteristics common to all of them. Let the window size W denote the number of loop iterations that execute concurrently on a single processor at any given time. <p> Dynamic Instruction Scheduling Another important issue in processor design is the instruction issuing mechanism. Conventional processors issue instructions in the order they are encountered in the program text (subject to control flow). Sophisticated Von Neumann architectures allow out-of-order execution of instructions <ref> [106, 81, 1, 14] </ref>, as do dataflow architectures [10, 32, 52, 76, 80, 28]. These architectures dynamically schedule instructions, subject to dependences evaluated at run time, in order to maximally utilize processors.
Reference: [15] <author> Carl Josef Beckmann. </author> <title> Reducing synchronization and scheduling overhead in parallel loops. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Research and Development, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: This ensures that the multiprocessor does not operate in a region of diminishing returns with respect to interprocessor concurrency, and encourages fair usage of processing resources in a multiprogrammed environment. In practice, multiprocessor scheduling and synchronization overhead also increase the optimal value of C <ref> [15] </ref>. 6.2.3 Experiments To determine the effects of W and C on , a number of important doall loops in the Perfect Club R fl benchmarks were simulated. The experiments simulated a dynamic instruction scheduling scheme (although similar results would be expected from static scheduling).
Reference: [16] <author> M. Berry, D. Chen, P. Koss, D. Kuck, et al. </author> <title> The Perfect Club R fl benchmarks: Effective performance evaluation of supercomputers. </title> <journal> International Journal of Supercomputing Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: is it likely to be found? What is the "grain size" of typical parallel tasks? A significant effort was spent as part of this thesis research in measuring the characteristics of functional parallelism, using as input a set of benchmark codes considered to be representative of numerical programs in general <ref> [16] </ref>. <p> Measurements of functional and loop-level parallelism inherent in HTGs are given in Section 5.2, and measurements relating to the ATG scheduling and optimization algorithms of Chapter 3 and Chapter 4 are given in Section 5.3. The measurements were made on programs taken from the Perfect Club R fl benchmarks <ref> [16, 29] </ref>. Unlike other benchmark suites based on kernels or individual loops, the Perfect Club R fl codes are a set of 13 complete applications. The eight codes used in our study range in size from 401 to 3,809 lines of FORTRAN.
Reference: [17] <author> Bob Boothe and Abhiram Ranade. </author> <title> Improved multithreading techniques for hiding communication latency in multiprocessors. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 214-223, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: It does not require single-assignment properties, and readily handles anti- and output-dependences. Chapters 2, 3 and 4 describe compiler algorithms needed to support our hardware, that are both practical and efficient. Multithreading has received a great deal of attention in the recent literature <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>. Almost all of these proposals involve changes to the hardware organization. <p> ADM 1.000 2.870 21.566 3.124 8.733 64.823 MDG 1.000 3.142 24.560 4.237 10.662 74.915 DYFESM 1.000 3.255 25.807 2.589 9.355 77.009 ARC2D 1.000 2.616 18.775 2.734 7.582 56.059 TRFD 1.000 3.156 24.715 2.669 9.137 73.815 FLO52Q 1.000 2.428 16.705 2.291 6.575 49.407 SPEC77 1.000 2.531 17.839 3.602 8.195 54.119 multithreading <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>, and various forms of dynamic instruction scheduling [106, 81, 14]. The focus here is not on the details of any particular method, but rather on the characteristics common to all of them.
Reference: [18] <author> Richard Buehrer and Kattamuri Ekanadham. </author> <title> Incorporating Data Flow ideas into von Neumann processors for parallel execution. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36(12) </volume> <pages> 1515-1522, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: It does not require single-assignment properties, and readily handles anti- and output-dependences. Chapters 2, 3 and 4 describe compiler algorithms needed to support our hardware, that are both practical and efficient. Multithreading has received a great deal of attention in the recent literature <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>. Almost all of these proposals involve changes to the hardware organization. <p> ADM 1.000 2.870 21.566 3.124 8.733 64.823 MDG 1.000 3.142 24.560 4.237 10.662 74.915 DYFESM 1.000 3.255 25.807 2.589 9.355 77.009 ARC2D 1.000 2.616 18.775 2.734 7.582 56.059 TRFD 1.000 3.156 24.715 2.669 9.137 73.815 FLO52Q 1.000 2.428 16.705 2.291 6.575 49.407 SPEC77 1.000 2.531 17.839 3.602 8.195 54.119 multithreading <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>, and various forms of dynamic instruction scheduling [106, 81, 14]. The focus here is not on the details of any particular method, but rather on the characteristics common to all of them. <p> Simply providing more outstanding loads may be an effective way of achieving this, without resorting to more complex approaches such as dataflow [10, 32, 52, 76, 80, 28] or multithreading <ref> [18, 104, 2, 113] </ref>. 123 CHAPTER 7 HARDWARE FOR FINE GRAIN ATG PARALLELISM The dynamic scheduling algorithms presented in Chapter 3 can be implemented in software with reasonable efficiency if task granularities are on the order of tens to hundreds of instructions.
Reference: [19] <author> Michael Butler and Yale Patt. </author> <title> An investigation of the performance of various dynamic scheduling techniques. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 1-9, </pages> <address> Portland, Oregon, December 1992. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: In these simulations, three functional unit types were 3 This ignores multiprocessor loop scheduling and synchronization overhead. 4 It was found that choosing the instructions issued using a critical-path-based priority did not significantly affect the results. Experiments in <ref> [19] </ref> also show that the order in which ready instructions are dispatched has a relatively minor effect. 5 Numerous other loops were simulated, with similar results.
Reference: [20] <author> Michael Butler, Tse-Yu Yeh, and Yale Patt. </author> <title> Single instruction stream parallelism is greater than two. </title> <booktitle> In Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pages 276-286, </pages> <year> 1991. </year>
Reference-contexts: By contrast, only MDG, FLO52Q and SPEC77 contain any significant amount of coarse grain functional parallelism, and in all three cases the average parallelism is less than a factor of two. Note that fine grain functional parallelism is closely related to what is commonly referred to as instruction-level parallelism <ref> [62, 94, 1, 55, 20, 98, 99, 109] </ref>. Instruction-level parallelism, however, may include some loop-level parallelism from innermost loops, if speculative execution, loop unrolling, or software pipelining are applied [62, 94, 55, 98, 99]. <p> Since the most frequently executed innermost loops in the Perfect Club R fl codes tend to be free of conditional branches, the fine grain functional parallelism figures of Table 5.4 are similar to basic block parallelism reported elsewhere <ref> [94, 1, 55, 20, 98, 99] </ref>. Note, however, that since we omit flow-of-control instructions from the HTG in order to measure arithmetic operation parallelism, our estimates of parallelism tend to be somewhat lower. Consider, for example, an innermost FORTRAN DO loop.
Reference: [21] <author> P. P. Chang, S. A. Mahlke, W. Y. Chen, Nancy J. Warter, and Wen-Mei W. Hwu. </author> <title> IMPACT: An architecture framework for multiple-instruction-issue processors. </title> <booktitle> In Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pages 266-275, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: One aspect of instruction issuing of particular interest is the number of instructions issued per clock cycle. Modern long instruction word (LIW) and superscalar architectures promise to deliver higher throughput by increasing the number of instructions issued per clock cycle beyond one <ref> [26, 21, 45, 31, 93, 115] </ref>, and many have delivered on the promise. While this can be quite effective in a uniprocessor, the advantages of multiple instruction issuing past a certain point are less clear in a multiprocessor environment. <p> In the discussion below, SI denotes a single-instruction-issue processor such as a conventional RISC, and MI denotes an architecture capable of issuing multiple instructions per clock cycle such as a VLIW or superscalar processor <ref> [26, 21, 45, 31, 93, 115] </ref>. 6.2.1 Latency, concurrency and utilization Let the utilization of a single processor be defined as the average number of instructions issued per clock cycle. A single-instruction-issue processor is fully utilized if it issues an instruction every cycle. <p> This is possible if multiple loop iterations are allowed to execute concurrently on a single processor. This, in turn, can be accomplished by various methods such as loop unrolling, modulo scheduling <ref> [5, 21, 65, 90, 91, 31, 93] </ref>, 2 This discussion borrows the notation of Andrews and Polychronopoulos [9] wherein fi denotes the raw performance ratio of a "fast processor" to a "slow processor" in multiprocessor design trade-offs. 106 Table 6.2 Average Instruction Latency Measurements INT=1 FLOAT=1 INT=1 FLOAT=6 load latency load <p> This could be ameliorated somewhat by moving the load of a (i,j) (line 9) ahead of line 6, although this involves moving a load ahead of a branch which is a nontrivial compiler optimization and is legal only under certain architectural assumptions <ref> [21] </ref>. Even so, if loads take more than a few clock cycles, a significant delay still remains. Note that this loop contains a conditional branch (the IF statement).
Reference: [22] <author> Ding-Kai Chen. </author> <title> MaxPar: An execution driven simulator for studying parallel systems. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Research and Development, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: Another form of parallelism in programs is nonloop or functional parallelism [42, 85, 86]. While data parallelism can provide large amounts of parallelism that often scales with problem size, recent studies have suggested that a significant amount of nonloop parallelism also exists in numerical codes <ref> [22, 82] </ref>. Theoretical foundations and methods for detecting functional parallelism automatically in a compiler were laid down in previous studies [42]. The focus of this thesis, by contrast, is the exploitation of functional parallelism at run time once it has been detected by the compiler. <p> Measurements of nonloop parallelism have been made by several researchers by using a compiler to insert source code which calculates a program's critical path at run time <ref> [22, 82] </ref>. In some cases, large amounts of parallelism have been found beyond that available from doall loops alone.
Reference: [23] <author> P.-Y. Chen, Duncan Lawrie, Pen-Chung Yew, and David Padua. </author> <title> Interconnection networks using shu*es. </title> <journal> Computer, </journal> <volume> 14(12) </volume> <pages> 55-64, </pages> <month> December </month> <year> 1981. </year>
Reference-contexts: In NUMA (nonuniform memory access) machines, the network distance, and hence the latency to different memory banks is not uniform across the machine. Even in UMA machines network conflicts and hot spots can cause significant variance in network transit times <ref> [63, 23] </ref>. 6.3.2 Static scheduling with variable memory latency One approach to dealing with variable memory access latencies is to generate a static schedule based on the maximum latency for each instruction [31, 93].
Reference: [24] <author> Jyh-Herng Chow and Luddy Harrison. Switch-stacks: </author> <title> A scheme for microtasking nested parallel loops. </title> <booktitle> Proceedings of Supercomputing '90, </booktitle> <pages> pages 190-199, </pages> <month> November 12-16 </month> <year> 1990. </year>
Reference-contexts: Thus, nonblocking barriers, in which the last processor to finish a loop iteration terminates the loop and releases the parent task's storage, are not possible with this P stacks scheme. It is shown in <ref> [24] </ref> how these scheduling constraints can be removed by allowing stack ownership to change hands during execution.
Reference: [25] <author> Coffman, Jr., E. G., </author> <title> editor. Computer and Job Shop Scheduling Theory. </title> <publisher> John Wiley & Sons, </publisher> <year> 1976. </year> <month> 165 </month>
Reference-contexts: However, static scheduling has received much attention in published research, and is fairly well-understood. Theoretical results tell us that optimal solutions are usually highly complex, but good heuristic algorithms often exist <ref> [25] </ref>. In many situations found in real programs, existing static scheduling techniques are not practical. To generate a static schedule for a task graph it is usually necessary to know the execution time of tasks, at least approximately.
Reference: [26] <author> Robert P. Colwell, W. Eric Hall, Chandra S. Joshi, David B. Papworth, Paul K. Rodman, and James E. Tornes. </author> <title> Architecture and implementation of a VLIW supercomputer. </title> <booktitle> In Proceedings Supercomputing '90, </booktitle> <pages> pages 910-919. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1990. </year>
Reference-contexts: One aspect of instruction issuing of particular interest is the number of instructions issued per clock cycle. Modern long instruction word (LIW) and superscalar architectures promise to deliver higher throughput by increasing the number of instructions issued per clock cycle beyond one <ref> [26, 21, 45, 31, 93, 115] </ref>, and many have delivered on the promise. While this can be quite effective in a uniprocessor, the advantages of multiple instruction issuing past a certain point are less clear in a multiprocessor environment. <p> In the discussion below, SI denotes a single-instruction-issue processor such as a conventional RISC, and MI denotes an architecture capable of issuing multiple instructions per clock cycle such as a VLIW or superscalar processor <ref> [26, 21, 45, 31, 93, 115] </ref>. 6.2.1 Latency, concurrency and utilization Let the utilization of a single processor be defined as the average number of instructions issued per clock cycle. A single-instruction-issue processor is fully utilized if it issues an instruction every cycle.
Reference: [27] <author> Thomas M. Conte. </author> <title> Tradeoffs in processor/memory interfaces for superscalar processors. </title> <booktitle> In Proceedings 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 202-205, </pages> <address> Portland, Oregon, December 1992. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Both scheduling methods achieve full utilization provided the window 7 Assuming the architecture contains a register scoreboard [43, 75] and the cache interface allows instruction issuing to proceed beyond a miss <ref> [27] </ref>. 8 In cases where W exceeded W max even for load latency cache , a schedule was generated assuming load latency cache .
Reference: [28] <author> David E. Culler, Anurag Sah, Klaus Erik Schauser, Thorsten von Eicken, and John Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Fourth International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-175, </pages> <year> 1991. </year>
Reference-contexts: This allows the compiler to choose the desired task granularity and to optimize away unnecessary synchronizations. It also simplifies the hardware considerably. Unlike various hybrid dataflow approaches <ref> [52, 76, 80, 79, 28] </ref>, our scheme does not employ separate synchronization or thread management instructions, and thus achieves zero scheduling overhead as long as there is sufficient parallelism to keep the pipeline utilized. This allows finer grained parallelism to be exploited than is possible in these hybrid dataflow approaches. <p> Dynamic Instruction Scheduling Another important issue in processor design is the instruction issuing mechanism. Conventional processors issue instructions in the order they are encountered in the program text (subject to control flow). Sophisticated Von Neumann architectures allow out-of-order execution of instructions [106, 81, 1, 14], as do dataflow architectures <ref> [10, 32, 52, 76, 80, 28] </ref>. These architectures dynamically schedule instructions, subject to dependences evaluated at run time, in order to maximally utilize processors. <p> Simply providing more outstanding loads may be an effective way of achieving this, without resorting to more complex approaches such as dataflow <ref> [10, 32, 52, 76, 80, 28] </ref> or multithreading [18, 104, 2, 113]. 123 CHAPTER 7 HARDWARE FOR FINE GRAIN ATG PARALLELISM The dynamic scheduling algorithms presented in Chapter 3 can be implemented in software with reasonable efficiency if task granularities are on the order of tens to hundreds of instructions. <p> In a sense, the proposed scheme goes one step beyond the explicit token store concept [80] by making synchronization explicit and separating it from data transfer. This allows the compiler to choose the desired task granularity and to optimize away unnecessary synchronizations. Unlike various hybrid dataflow approaches <ref> [52, 76, 80, 79, 28] </ref>, the proposed hardware scheme does not employ separate synchronization or thread management instructions, and thus achieves zero scheduling overhead as long as there is sufficient parallelism to keep the pipeline utilized. <p> As a means of avoiding processor idle time due to these delays, a number of multithreaded architectures have been proposed that attempt to hide such long-latency operations by maintaining multiple threads of execution on a single processor and rapidly context-switching between them <ref> [97, 104, 7, 52, 76, 80, 28, 79, 113, 47, 2] </ref>. This section proposes a method for achieving the same results on a conventional processor. Memory and synchronization operations are examples of operations incurring long periods of latency.
Reference: [29] <author> George Cybenko, Lyle Kipp, Lynn Pointer, and David Kuck. </author> <title> Supercomputer performance evaluation and the Perfect Benchmarks T M . In Proceedings of the International Conference on Supercomputing, </title> <address> Amsterdam, Netherlands, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: Measurements of functional and loop-level parallelism inherent in HTGs are given in Section 5.2, and measurements relating to the ATG scheduling and optimization algorithms of Chapter 3 and Chapter 4 are given in Section 5.3. The measurements were made on programs taken from the Perfect Club R fl benchmarks <ref> [16, 29] </ref>. Unlike other benchmark suites based on kernels or individual loops, the Perfect Club R fl codes are a set of 13 complete applications. The eight codes used in our study range in size from 401 to 3,809 lines of FORTRAN.
Reference: [30] <author> Ron G. Cytron. </author> <title> DOACROSS: Beyond vectorization for multiprocessors (extended abstract). </title> <booktitle> In Proceedings 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 836-844, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: Loops are sequential because of (loop-carried) dependence cycles, but executing the body of each iteration to completion before starting the next iteration may waste some interiteration, or doacross <ref> [30] </ref>, parallelism. While it was assumed above that task graphs contain only intraiteration dependences, the approach can be extended to include loop-carried dependences as follows. The HTG is constructed as described in Chapter 2, obtaining loops and ATGs with intraiteration dependences only. <p> In many sequential loops, however, there is some parallelism available from the partial overlap of loop iterations. This type of parallelism is known as doacross parallelism <ref> [30] </ref>. To exploit doacross parallelism, loop iterations are allowed to execute in parallel, subject to cross-iteration dependences that must be enforced. It is often the case that doacross loops contain more than one cross-iteration dependence. <p> The body of a loop may be any HTGL node type: another loop, an acyclic task graph, a subroutine call or an atom. Loops are further designated as either sequential (DO) or completely parallel (DOALL). Doacross loops <ref> [30] </ref> are not modeled in our experiments. Acyclic task graphs. Acyclic task graphs are described using the HTGL DAG (directed acyclic graph) construct. This consists of a list of tasks, and a description of the acyclic data and control dependences among them. <p> In the discussion below, the definition of the HTG is extended to include doacross loops in which partial overlap of iterations is possible even if the loop contains a dependence cycle which prevents it from being completely parallelized <ref> [30] </ref>. One aspect of instruction issuing of particular interest is the number of instructions issued per clock cycle. <p> For an ATG nested inside a parallel loop, the total amount of parallelism is the product of the loop and ATG parallelism: total = loop AT G : (6:4) 103 For loops with loop-carried dependence cycles, henceforth referred to as doacross loops <ref> [30] </ref>, the parallelism attributable to the loop loop is calculated by considering the parallel execution of the loop on an ideal machine with unlimited processing resources.
Reference: [31] <author> J. Dehnert, P. Hsu, and J. Bratt. </author> <title> Overlapped loop support in the Cydra 5. </title> <booktitle> In Proceedings Second International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 26-38, </pages> <year> 1989. </year>
Reference-contexts: One aspect of instruction issuing of particular interest is the number of instructions issued per clock cycle. Modern long instruction word (LIW) and superscalar architectures promise to deliver higher throughput by increasing the number of instructions issued per clock cycle beyond one <ref> [26, 21, 45, 31, 93, 115] </ref>, and many have delivered on the promise. While this can be quite effective in a uniprocessor, the advantages of multiple instruction issuing past a certain point are less clear in a multiprocessor environment. <p> In the discussion below, SI denotes a single-instruction-issue processor such as a conventional RISC, and MI denotes an architecture capable of issuing multiple instructions per clock cycle such as a VLIW or superscalar processor <ref> [26, 21, 45, 31, 93, 115] </ref>. 6.2.1 Latency, concurrency and utilization Let the utilization of a single processor be defined as the average number of instructions issued per clock cycle. A single-instruction-issue processor is fully utilized if it issues an instruction every cycle. <p> This is possible if multiple loop iterations are allowed to execute concurrently on a single processor. This, in turn, can be accomplished by various methods such as loop unrolling, modulo scheduling <ref> [5, 21, 65, 90, 91, 31, 93] </ref>, 2 This discussion borrows the notation of Andrews and Polychronopoulos [9] wherein fi denotes the raw performance ratio of a "fast processor" to a "slow processor" in multiprocessor design trade-offs. 106 Table 6.2 Average Instruction Latency Measurements INT=1 FLOAT=1 INT=1 FLOAT=6 load latency load <p> Even in UMA machines network conflicts and hot spots can cause significant variance in network transit times [63, 23]. 6.3.2 Static scheduling with variable memory latency One approach to dealing with variable memory access latencies is to generate a static schedule based on the maximum latency for each instruction <ref> [31, 93] </ref>. For example, if memory load instructions take cache on a cache hit and mem on a cache miss (ignoring network effects), a static schedule could be generated assuming that all loads incur latency mem . <p> A number of techniques exist for dealing with conditional branches in loops on statically scheduled machines. Some machines support predicated execution of instructions, in which instructions can be made to execute conditionally upon the value of a flag register <ref> [31, 93] </ref>. Conditional branches can then be substituted with instructions that set the appropriate flag register. Instructions from both branches of an if statement are issued, but only those from one of the branches are actually enabled for execution. This process is called if-conversion [112]. <p> This can be accomplished by adapting the rotating register windows scheme used in VLIW architectures such as the Cydra-5 <ref> [31, 93] </ref>. The Cydra-5 is a statically scheduled VLIW architecture in which software pipelining is used to generate an instruction schedule for a loop which generally overlaps several iterations at once [93]. <p> Since scheduling is fully dynamic, the proposed scheme has advantages over static software pipelining schemes <ref> [91, 65, 31] </ref> in that it readily handles both conditional branches and long and unpredictable latencies caused by cache misses. <p> To maximize the efficiency of such machines, the compiler must properly schedule instructions so that operands have traversed the pipeline by the time they are needed by later instructions. This can be done for vectorizable loops using software pipelining techniques <ref> [50, 64, 65, 70, 102, 91, 31] </ref>. Multithreading and software pipelining lie on two ends of a spectrum of methods for improving efficiency by overlapping latency with useful work.
Reference: [32] <author> Jack B. Dennis. </author> <title> Data Flow supercomputers. </title> <journal> Computer, </journal> <volume> 13(11) </volume> <pages> 48-56, </pages> <month> November </month> <year> 1980. </year>
Reference-contexts: The drawback of this is that compatibility with existing instruction sets is not possible. Our approach can be thought of as a restricted form of dataflow within a single processor. In classical dataflow, instructions are queued for execution as soon as all of their execution conditions have been met <ref> [10, 32] </ref>. This is signaled by the arrival of tokens carrying data as well 5 as synchronization information. The scheme developed here, by contrast, goes one step beyond the explicit token store concept [80] by making synchronization explicit and separating it from data transfer. <p> Dynamic Instruction Scheduling Another important issue in processor design is the instruction issuing mechanism. Conventional processors issue instructions in the order they are encountered in the program text (subject to control flow). Sophisticated Von Neumann architectures allow out-of-order execution of instructions [106, 81, 1, 14], as do dataflow architectures <ref> [10, 32, 52, 76, 80, 28] </ref>. These architectures dynamically schedule instructions, subject to dependences evaluated at run time, in order to maximally utilize processors. <p> Simply providing more outstanding loads may be an effective way of achieving this, without resorting to more complex approaches such as dataflow <ref> [10, 32, 52, 76, 80, 28] </ref> or multithreading [18, 104, 2, 113]. 123 CHAPTER 7 HARDWARE FOR FINE GRAIN ATG PARALLELISM The dynamic scheduling algorithms presented in Chapter 3 can be implemented in software with reasonable efficiency if task granularities are on the order of tens to hundreds of instructions.
Reference: [33] <institution> Digital Equipment Corporation. </institution> <note> DECChip 21064-AA RISC Microprocessor Preliminary Data Sheet, </note> <month> April </month> <year> 1992. </year>
Reference-contexts: Interestingly, AT G tends to increase with higher load latency in 1 The longer latencies are representative of contemporary RISC processors such as the DEC 21064 Alpha chip <ref> [33] </ref>, which has single cycle integer operations, 6-cycle latency for floating-point instructions, and 3-cycle latency for memory loads that hit in the on-chip cache. 104 Table 6.1 Average ATG Parallelism Measurements INT=1 FLOAT=1 INT=1 FLOAT=6 load latency load latency benchmark 1 10 100 3 30 300 ADM 2.58 3.26 3.65 2.49 <p> To achieve higher clock rates, modern microprocessors use increasing degrees of pipelining, resulting in higher (3 to 6 cycle floating-point latencies are not uncommon) <ref> [75, 54, 33] </ref>). Memory accesses also incur long latencies, especially in multiprocessors where cache miss rates tend to be higher due to sharing and main memory accesses must traverse multistage networks. <p> clock cycle, on average. 2 Note that in most superscalar and VLIW processors, there are some restrictions on the combination of instructions that can be issued in a single clock cycle because there are a limited number of functional units of each type (e.g., one integer and one floating-point unit <ref> [75, 33] </ref>). Therefore, fi is generally less than the peak theoretical issue rate, and depends on the instruction mix in the region of code being executed, since there is always one critical resource that is 100% utilized while other functional units receive a smaller fraction of instructions. <p> Table 7.3 (a) shows the effective average instruction latency assuming integer operations have single cycle latency, floating point operations take six cycles, and memory loads have a three-cycle latency. These figures are representative of modern RISC microprocessors such as the Motorola MC88100 and Digital's 21064 Alpha chip <ref> [75, 33] </ref>. In Tables 7.3 (b) and (c) the memory load latency is increased to 7 and 20 cycles, respectively, to account for an increasing number of cache misses and long main memory latency.
Reference: [34] <author> Michel Dubois, Jonas Skeppstedt, Livio Ricciulli, Krishnan Ramamurthy, and Per Sten-strom. </author> <title> The detection and elimination of useless misses in multiprocessors. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 88-97, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: It is difficult for the compiler to predict exactly when cache misses will occur. The problem is exacerbated in multiprocessors because other processors may invalidate a processors' cached locations, due to either true or false sharing <ref> [107, 34] </ref>. Another source of uncertainty is the variability in mem itself, due to the processor-memory interconnection network (or the processor-processor interconnection network in distributed-shared-memory machines). In NUMA (nonuniform memory access) machines, the network distance, and hence the latency to different memory banks is not uniform across the machine.
Reference: [35] <author> Rudolf Eigenmann and William Blume. </author> <title> An effectiveness study of parallelizing compiler techniques. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 17-25, </pages> <address> St. Charles, IL, </address> <month> August 12-16 </month> <year> 1991. </year>
Reference-contexts: In many cases, a significant increase in coarse grain functional parallelism can be realized with more aggressive compiler dependence analysis and code restructuring. As in the case of loops, privatization of data structures appears to be important in eliminating unnecessary data dependences <ref> [35] </ref>. A significant contribution of this thesis is in the area of dynamic scheduling algorithms for acyclic task graphs. In Chapter 3, two dynamic scheduling algorithms are presented which are efficient in terms of both storage and run-time overhead.
Reference: [36] <author> Rudolf Eigenmann, Jay Hoeflinger, Zhiyuan Li, and David Padua. </author> <title> Experience in the automatic parallelization of four perfect-benchmark programs. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 65-83, </pages> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Some of these transformations are primarily cosmetic and were not considered worth the effort of fully automating. The identification of certain parallel loops is also done manually, since this requires a level of analysis not currently available in Parafrase-2, although automatable in principle <ref> [36] </ref>. Manual editing of the HTGL descriptions can also be done to explore the effectiveness of, for example, removing conservative dependence arcs which could be eliminated with more aggressive compiler techniques. Using a textual intermediate representation also allows small example HTGs to be generated manually.
Reference: [37] <author> J. Ferrante, K. J. Ottenstein, and J. D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The formal definition is based on the postdominance relationship defined on the CFG <ref> [40, 37] </ref>. Let v ST OP be a unique node in the CFG to which control eventually flows no matter which CFG path is taken. <p> A CFG edge ab is said to satisfy a dependence v 1 ffi c v 2 if control is guaranteed to flow to v 2 when v 1 takes the branch a b. The following definitions are indirectly related to control dependence <ref> [40, 37] </ref>: N eg (v j ) denotes the set of control flow edges which, when taken, prevent v j from being executed. <p> By examining the code, it is clear that both branches 1-3 and 2-3 cannot be taken, since taking 1-3 bypasses 2. Theorem 1 in Section 3.1.4 shows that this is true generally, by the definition of control dependence given in Chapter 2 and [40] and <ref> [37] </ref>. 2 Recall from Chapter 2 that a control dependence labeled a b is satisfied by a conditional branch which takes the CFG edge from a to b. 23 This implies that there is no danger of multiple control dependence predecessors queuing a task, which simplifies the handling of control dependences.
Reference: [38] <author> Joseph A. Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30:478-490, </volume> <month> July </month> <year> 1981. </year> <month> 166 </month>
Reference-contexts: Since all instructions are issued in each iteration, generating a static schedule that overlaps multiple iterations is as easy as for a loop without conditional branches. Trace scheduling assumes that one direction of a conditional branch is taken much more frequently than the other <ref> [38] </ref>. A static instruction schedule is generated assuming that each branch is taken in its more frequent direction.
Reference: [39] <author> E. Freudenthal and O. Peze. </author> <title> Efficient synchronization algorithms using fetch & add on multiple bitfield integers. Ultracomputer Note 148, </title> <institution> New York University Courant Institute of Mathematical Sciences, </institution> <month> February </month> <year> 1988. </year>
Reference-contexts: Furthermore, since each counter takes on only nonnegative values, multiple counters can be placed in adjacent bit fields in an integer register or memory location, and integer subtraction using appropriate constants can be used to update multiple counters with a single operation <ref> [39] </ref>. Since no counter is decremented beyond zero, no carries between neighboring bitfields are generated using ordinary subtract instructions on the entire packed counter word.
Reference: [40] <author> Milind Girkar and Constantine Polychronopoulos. </author> <title> Automatic detection and generation of unstructured parallelism in ordinary programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2), </volume> <month> April </month> <year> 1992. </year>
Reference-contexts: It begins with the definition and semantics of the hierarchical task graph <ref> [40] </ref> and concludes with a discussion of dynamic scheduling techniques applicable to hierarchical task graphs. The following definitions and terminology are used below. A graph G = (V; E) consists of a set V of vertices or nodes connected via a set E of edges or arcs. <p> successors are the set of sinks of the node's outgoing edges, and its predecessors are the set of sources of its incoming edges. 2.1 Hierarchical Task Graphs The basis of the approach to program scheduling described below is the hierarchical task graph (HTG), which has the following definition and properties <ref> [85, 40, 86, 42] </ref>. The hierarchy is formed by extracting the loop structure of the program. This includes explicit loops, such as do and while statements, as well as implicit loops, e.g., those formed by using goto statements. <p> The formal definition is based on the postdominance relationship defined on the CFG <ref> [40, 37] </ref>. Let v ST OP be a unique node in the CFG to which control eventually flows no matter which CFG path is taken. <p> A CFG edge ab is said to satisfy a dependence v 1 ffi c v 2 if control is guaranteed to flow to v 2 when v 1 takes the branch a b. The following definitions are indirectly related to control dependence <ref> [40, 37] </ref>: N eg (v j ) denotes the set of control flow edges which, when taken, prevent v j from being executed. <p> However, they can be computed from the CFG in a straightforward manner during the control dependence calculation <ref> [40] </ref>. 2.1.2.2 Data dependences Data dependences represent the implied ordering of conflicting data accesses to program variables. The definition of data dependence is based on the sets IN (v) and OU T (v) which are, respectively, the memory locations read and written by a node v. <p> By examining the code, it is clear that both branches 1-3 and 2-3 cannot be taken, since taking 1-3 bypasses 2. Theorem 1 in Section 3.1.4 shows that this is true generally, by the definition of control dependence given in Chapter 2 and <ref> [40] </ref> and [37]. 2 Recall from Chapter 2 that a control dependence labeled a b is satisfied by a conditional branch which takes the CFG edge from a to b. 23 This implies that there is no danger of multiple control dependence predecessors queuing a task, which simplifies the handling of <p> The definition of BranN eg () implies that if task v i contains a conditional branch labeled l, for any task v j 2 BranN eg (l), v i ffi fl c v j <ref> [40] </ref>. Any task v i such that v i ffi fl c v j is said to be a control dependence ancestor of v j . <p> The resulting code can be analyzed to derive results on instruction-level program characteristics. hier This pass constructs the HTG as described in Chapter 2 and <ref> [40] </ref>. hcodegen This pass prints the HTG in HTGL format. 5.1.1.2 Instruction-level analysis Some of the results presented in this chapter involve instruction-level program characteristics. <p> The term "instruction-level parallelism" is somewhat ill-defined. In a sense, all parallelism is at the instruction level since ultimately it is instructions that execute in parallel. To avoid confusion, two kinds of parallelism are distinguished: loop parallelism, and ATG or functional parallelism <ref> [40] </ref>. As discussed in previous chapters, ATG parallelism generally exists at many levels of granularity in a program. At the finest level of granularity, the nodes are individual instructions within the body of inner loops.
Reference: [41] <author> Milind B. Girkar, Mohammad Haghighat, Chia Ling Lee, Bruce P. Leung, and Dale A. Schouten. </author> <title> Parafrase-2 programmer's manual. </title> <type> Technical report, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1993. </year>
Reference-contexts: A detailed description of other passes available in Parafrase-2 may also be found in <ref> [41] </ref>. bbinst This pass instruments all basic blocks and loops in the program to obtain a profile or trace of the program's dynamic behavior.
Reference: [42] <author> Milind Baburao Girkar. </author> <title> Functional parallelism: theoretical foundations and implementation. </title> <type> PhD thesis, </type> <institution> Computer Science Department, University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: Much work has been done to date on exploiting loop-based or data parallelism. Another form of parallelism in programs is nonloop or functional parallelism <ref> [42, 85, 86] </ref>. While data parallelism can provide large amounts of parallelism that often scales with problem size, recent studies have suggested that a significant amount of nonloop parallelism also exists in numerical codes [22, 82]. <p> Theoretical foundations and methods for detecting functional parallelism automatically in a compiler were laid down in previous studies <ref> [42] </ref>. The focus of this thesis, by contrast, is the exploitation of functional parallelism at run time once it has been detected by the compiler. This, too, involves the compiler, which must generate correct and efficient code for scheduling parallel tasks. <p> It is found that the compiler has a crucial role to play in preprocessing task graphs to work within these limitations and to minimize scheduling overhead. 1.2 Overview of Thesis Chapter 2 reviews material from <ref> [42] </ref>, which serves as a theoretical foundation for this work, and describes the dynamic program scheduling problem in more detail. Chapters 3 and 4 present dynamic scheduling algorithms for functional parallelism and related compiler optimizations, respectively. <p> This technique achieves some of the goals of the hardware described in Chapter 7, but on conventional microprocessors. Concluding remarks are given in Chapter 9. 1.3 Previous and Related Work The work of Girkar <ref> [42] </ref> serves as a theoretical foundation for this work. Girkar defines the hierarchical task graph rigorously and describes the steps in its construction. He also develops algorithms and complexity analysis for optimization of execution conditions. <p> successors are the set of sinks of the node's outgoing edges, and its predecessors are the set of sources of its incoming edges. 2.1 Hierarchical Task Graphs The basis of the approach to program scheduling described below is the hierarchical task graph (HTG), which has the following definition and properties <ref> [85, 40, 86, 42] </ref>. The hierarchy is formed by extracting the loop structure of the program. This includes explicit loops, such as do and while statements, as well as implicit loops, e.g., those formed by using goto statements. <p> By removing the back edges and collapsing each loop into a compound node, the body of each loop thus becomes an acyclic CFG, each of whose nodes is either a node in the original CFG, or a nested loop node <ref> [42] </ref>. Using the acyclic CFG and data flow analysis [4], control and data dependences between nodes at a given level of the HTG are then computed. Consider the code example shown in Figure 2.1. <p> These characteristics motivate interest in dynamic scheduling algorithms for acyclic task graphs. 18 The following chapter presents two dynamic scheduling algorithms for ATGs. They are based on a theory of dependences and execution conditions due to Girkar <ref> [42] </ref> as well as several additional theorems, which prove correctness. The dynamic nature of the algorithms provides good performance. Efficiency is achieved both by the nature of the base algorithms and by additional graph optimizations performed by the compiler. <p> Next it must be shown that a task's execution condition never evaluates to TRUE before all necessary dependences have been satisfied. This follows from the definition of execution conditions as discussed in Chapter 2 and <ref> [42] </ref>, and by the definition of data and control dependences. A more important and subtle result discussed in Section 3.1.4 is that there is always a unique task which satisfies any given data dependence and a unique task which satisfies the entire execution condition of a given task. <p> This result can be used to construct algorithms that evaluate execution conditions efficiently and do not enqueue ready tasks redundantly. 3.1.1 Form of execution conditions As shown in Chapter 2 and <ref> [42] </ref>, a task's execution condition is the conjunction of control condition and data condition terms: *(v i ) = * c (v i ) ^ * d (v i ); (3:1) ^ * d : (3:2) The control condition is the disjunction of all incoming control dependences, since any control dependence <p> is the conjunction of control condition and data condition terms: *(v i ) = * c (v i ) ^ * d (v i ); (3:1) ^ * d : (3:2) The control condition is the disjunction of all incoming control dependences, since any control dependence may satisfy this condition <ref> [42] </ref>. In Section 3.1.4 it is shown that at most one control dependence is ever satisfied, in any given execution. Each data condition term is due to a data dependence incident on that task. <p> Optimizations as described by Girkar <ref> [42] </ref> simplify the execution condition expressions but do not leave them in this canonical product-of-sums form. The canonical form serves as a convenient basis for the algorithms in Section 3.2. <p> Algorithms exist for computing the transitive reduction in O (-2:8 ) time (where is the number of nodes in G) [3]. 2. ATGs with control dependences. Girkar has shown that removing redundant data dependences in ATGs containing control dependences is an NP-complete problem <ref> [42] </ref>. The high complexity of the control dependence case is due to the fact that with conditional branches, some nodes in an ATG may not execute at all. This changes the way in which dependences emanating from these nodes are satisfied and enforced. <p> Two approaches to optimizing ATGs with conditional branches are discussed in this section. First, a simple heuristic algorithm is presented (other heuristic algorithms are also given in <ref> [42] </ref>). As an alternative, an "exact" algorithm can be used if it can be determined (in low-order complexity time) that removing redundant dependences for a particular ATG can be done within some predetermined time limits. <p> Extracting this description is done in several steps. Using the Parafrase-2 parallelizing compiler [87], the hierarchical task graph is extracted using the hier compiler pass <ref> [42] </ref>. A number of other compiler passes are also applied to aid in the construction of the HTG and to obtain output in the desired format. The HTG description is stored in an abstract, textual format called HTGL (hierarchical task graph language).
Reference: [43] <author> Richard Goss. </author> <title> Motorola's 88000: Integration, performance and applications. </title> <booktitle> In COMP-CON 89, </booktitle> <pages> pages 20-26, </pages> <year> 1989. </year>
Reference-contexts: The CDC6600 machine featured a register scoreboard which allowed instructions to be issued before previous instructions were completed, and allowed pipelined instructions to complete out of order [105]. Scoreboards can be found in modern pipelined RISC microprocessors, such as the MC88000 family <ref> [43, 75] </ref>. Tomasulo's common data bus architecture featured similar capabilities and, in addition, implemented dynamic register renaming, allowing multiple instances of a given instruction to execute simultaneously [106]. A similar approach based on an instruction dispatch stack was proposed later by Acosta et al. [1]. <p> When m = 0 (all loads hit in the cache), the performance of the static and dynamic schemes is practically identical. Both scheduling methods achieve full utilization provided the window 7 Assuming the architecture contains a register scoreboard <ref> [43, 75] </ref> and the cache interface allows instruction issuing to proceed beyond a miss [27]. 8 In cases where W exceeded W max even for load latency cache , a schedule was generated assuming load latency cache . <p> Further, no limit was placed on the number memory loads or cache misses that may be outstanding. While commercially available microprocessors limit the number of outstanding memory operations to two or three <ref> [43, 75, 57, 54] </ref>, the results presented here favor increasing this number for multiprocessor building blocks. It is argued that caching will tend to become less effective in large-scale multiprocessors, and that the ability to tolerate large mem will be an important feature of the processor architecture.
Reference: [44] <author> A. Gottlieb, B. Lubachevsky, and L. Rudolph. </author> <title> Basic techniques for the efficient coordination of very large numbers of cooperating sequential processors. </title> <journal> ACM Journal on Programming Languages and Systems, </journal> <volume> 5(2) </volume> <pages> 165-189, </pages> <month> April </month> <year> 1983. </year>
Reference-contexts: Consider, for example, a parallel loop spread across multiple processors. To achieve good load balancing and low run-time overhead, a dynamic loop scheduling algorithm such as GSS or TSS [88, 108] can be used. This requires each processor to issue a fetch-and- operation <ref> [44] </ref> to a shared memory location after every chunk of iterations it completes. Note that in both the GSS and TSS algorithms, this chunk size varies (decreases) as the loop progresses.
Reference: [45] <author> Randy D. Groves and Richard Oehler. </author> <title> RISC System/6000 processor architecture. </title> <booktitle> In IBM RISC System/6000 Technology, </booktitle> <pages> pages 16-23. </pages> <institution> IBM Austin Communications Department, </institution> <address> 11400 Burnett Road, Austin TX 78758, </address> <year> 1990. </year> <title> Order Number SA23-2619. </title>
Reference-contexts: One aspect of instruction issuing of particular interest is the number of instructions issued per clock cycle. Modern long instruction word (LIW) and superscalar architectures promise to deliver higher throughput by increasing the number of instructions issued per clock cycle beyond one <ref> [26, 21, 45, 31, 93, 115] </ref>, and many have delivered on the promise. While this can be quite effective in a uniprocessor, the advantages of multiple instruction issuing past a certain point are less clear in a multiprocessor environment. <p> In the discussion below, SI denotes a single-instruction-issue processor such as a conventional RISC, and MI denotes an architecture capable of issuing multiple instructions per clock cycle such as a VLIW or superscalar processor <ref> [26, 21, 45, 31, 93, 115] </ref>. 6.2.1 Latency, concurrency and utilization Let the utilization of a single processor be defined as the average number of instructions issued per clock cycle. A single-instruction-issue processor is fully utilized if it issues an instruction every cycle.
Reference: [46] <author> Mark D. Guzzi, David A. Padua, Jay P. Hoeflinger, and Duncan H. Lawrie. </author> <title> Cedar Fortran and other vector and parallel Fortran dialects. </title> <journal> Journal of Supercomputing, </journal> <volume> 4(1) </volume> <pages> 37-62, </pages> <year> 1990. </year>
Reference-contexts: The partial sharing of these variables thus implies a tree structure. In Cedar FORTRAN <ref> [46] </ref>, variables may be declared as local to a parallel loop. Like the iteration variables, these are unique to iterations, and are also stored on this tree.
Reference: [47] <author> R. Halstead and T. Fujita. MASA: </author> <title> A multithreaded processor architecture for parallel symbolic computing. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 443-451, </pages> <year> 1988. </year>
Reference-contexts: It does not require single-assignment properties, and readily handles anti- and output-dependences. Chapters 2, 3 and 4 describe compiler algorithms needed to support our hardware, that are both practical and efficient. Multithreading has received a great deal of attention in the recent literature <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>. Almost all of these proposals involve changes to the hardware organization. <p> ADM 1.000 2.870 21.566 3.124 8.733 64.823 MDG 1.000 3.142 24.560 4.237 10.662 74.915 DYFESM 1.000 3.255 25.807 2.589 9.355 77.009 ARC2D 1.000 2.616 18.775 2.734 7.582 56.059 TRFD 1.000 3.156 24.715 2.669 9.137 73.815 FLO52Q 1.000 2.428 16.705 2.291 6.575 49.407 SPEC77 1.000 2.531 17.839 3.602 8.195 54.119 multithreading <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>, and various forms of dynamic instruction scheduling [106, 81, 14]. The focus here is not on the details of any particular method, but rather on the characteristics common to all of them. <p> As a means of avoiding processor idle time due to these delays, a number of multithreaded architectures have been proposed that attempt to hide such long-latency operations by maintaining multiple threads of execution on a single processor and rapidly context-switching between them <ref> [97, 104, 7, 52, 76, 80, 28, 79, 113, 47, 2] </ref>. This section proposes a method for achieving the same results on a conventional processor. Memory and synchronization operations are examples of operations incurring long periods of latency.
Reference: [48] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: W = 100 * N 3 if (W &gt; 0) then 4 Y = X - 2 endif 6 A = 2 * (Y - 3) 7 if (W &lt; 10) then 8 B = B - X endif 9 C = Z + B A after write (WAW) dependence <ref> [48] </ref>. The data dependence graph (DDG) is the graph with nodes V obtained by adding an edge from v 1 to v 2 whenever v 2 is data dependent on v 1 (by either a flow-, anti- or output-dependence). 2.1.2.3 Execution conditions the branch direction. <p> A k-issue processor is fully utilized if it issues k instructions on each clock, hence for a k-issue processor can range from 0 to k. Utilization (multiplied by clock rate) is a measure of a processor's overall throughput <ref> [48] </ref>. The instruction-level concurrency exploited by a processor on a computation, proc , is defined as the number of instructions actually executed in parallel. We use the term concurrency to distinguish this from the potential parallelism inherent in the code itself. <p> Note that throughout this thesis, "instruction latency" means the latency of the execute phase (EX) of the instruction only, which excludes the latency of instruction fetching, decoding, and operand fetch and write-back provided the hardware includes register-bypass logic <ref> [48] </ref>. This latency is the minimum time between issuing dependent instructions without stalling the processor. In Figure 7.10 (a), instructions are statically scheduled for the conventional processor such that this latency is observed and the processor does not stall.
Reference: [49] <author> Roger W. Hockney. </author> <title> (r 1 ; n 1=2 ; s 1=2 ) measurements on the 2-cpu CRAY X-MP. </title> <journal> Parallel Computing, </journal> <volume> 2 </volume> <pages> 1-14, </pages> <year> 1985. </year>
Reference-contexts: The effect of C on is shown in Figure 6.2. In these experiments, the window size was fixed at W = 4. These curves also show asymptotic convergence to a maximum for large C. Unlike curves exhibit a smooth C C+C 1 behavior often used to characterize vector machines <ref> [49] </ref>. Here, C 1 represents the "vector length" for which half of the asymptotic throughput is achieved. The simplest model explaining this behavior consists of a fixed amount of "startup overhead" added to a per-iteration time 6 multiplied by the number of iterations executed C.
Reference: [50] <author> P. Y. Hsu. </author> <title> Highly concurrent scalar processing. </title> <type> PhD thesis, </type> <institution> Coordinated Science Laboratory, University of Illinois at Urbana-Champaign, </institution> <year> 1986. </year>
Reference-contexts: To maximize the efficiency of such machines, the compiler must properly schedule instructions so that operands have traversed the pipeline by the time they are needed by later instructions. This can be done for vectorizable loops using software pipelining techniques <ref> [50, 64, 65, 70, 102, 91, 31] </ref>. Multithreading and software pipelining lie on two ends of a spectrum of methods for improving efficiency by overlapping latency with useful work.
Reference: [51] <author> S. F. Hummel, E. Schonberg, and L. E. Flynn. </author> <title> Factoring: A method for scheduling parallel loops. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 90-101, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: It is assumed that scheduling is nonpreemptive, i.e., tasks are run to completion. 2.2.1 Scheduling parallel loops Loops are generally regarded as the largest source of parallelism within ordinary programs, and hence dynamic scheduling of parallel loops has received much attention in the literature to date <ref> [88, 13, 108, 51] </ref>. Dynamic scheduling of loops is not a topic of this thesis, since an array of existing techniques is available for our use. The loop scheduling problem consists of assigning iterations of the loop to processors for execution. <p> To remedy this, several algorithms decrease the chunk size as the loop nears completion. Guided self-scheduling (GSS) [88], trapezoidal self-scheduling (TSS) [108] and factoring <ref> [51] </ref> are all examples of this. These techniques differ in the initial chunk size, the way in which chunk size is decreased, robustness to certain anomalies, and total number of scheduling operations.
Reference: [52] <author> Robert A. </author> <title> Iannucci. Toward a Dataflow/von Neumann hybrid architecture. </title> <booktitle> In Proceedings 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 131-140, </pages> <year> 1988. </year>
Reference-contexts: This allows the compiler to choose the desired task granularity and to optimize away unnecessary synchronizations. It also simplifies the hardware considerably. Unlike various hybrid dataflow approaches <ref> [52, 76, 80, 79, 28] </ref>, our scheme does not employ separate synchronization or thread management instructions, and thus achieves zero scheduling overhead as long as there is sufficient parallelism to keep the pipeline utilized. This allows finer grained parallelism to be exploited than is possible in these hybrid dataflow approaches. <p> Dynamic Instruction Scheduling Another important issue in processor design is the instruction issuing mechanism. Conventional processors issue instructions in the order they are encountered in the program text (subject to control flow). Sophisticated Von Neumann architectures allow out-of-order execution of instructions [106, 81, 1, 14], as do dataflow architectures <ref> [10, 32, 52, 76, 80, 28] </ref>. These architectures dynamically schedule instructions, subject to dependences evaluated at run time, in order to maximally utilize processors. <p> Further benefits of dynamic scheduling which were not evaluated here include the ability to deal more effectively with synchronization and related operations, such as system-level dynamic scheduling. Synchronization latency has been cited by some researchers as a fundamental problem in multiprocessing justifying a dataflow approach <ref> [52] </ref>. Consider, for example, a parallel loop spread across multiple processors. To achieve good load balancing and low run-time overhead, a dynamic loop scheduling algorithm such as GSS or TSS [88, 108] can be used. <p> Simply providing more outstanding loads may be an effective way of achieving this, without resorting to more complex approaches such as dataflow <ref> [10, 32, 52, 76, 80, 28] </ref> or multithreading [18, 104, 2, 113]. 123 CHAPTER 7 HARDWARE FOR FINE GRAIN ATG PARALLELISM The dynamic scheduling algorithms presented in Chapter 3 can be implemented in software with reasonable efficiency if task granularities are on the order of tens to hundreds of instructions. <p> In a sense, the proposed scheme goes one step beyond the explicit token store concept [80] by making synchronization explicit and separating it from data transfer. This allows the compiler to choose the desired task granularity and to optimize away unnecessary synchronizations. Unlike various hybrid dataflow approaches <ref> [52, 76, 80, 79, 28] </ref>, the proposed hardware scheme does not employ separate synchronization or thread management instructions, and thus achieves zero scheduling overhead as long as there is sufficient parallelism to keep the pipeline utilized. <p> Memory and synchronization latency have often been cited as a fundamental issue in processor design justifying a dataflow approach <ref> [52] </ref>. <p> As a means of avoiding processor idle time due to these delays, a number of multithreaded architectures have been proposed that attempt to hide such long-latency operations by maintaining multiple threads of execution on a single processor and rapidly context-switching between them <ref> [97, 104, 7, 52, 76, 80, 28, 79, 113, 47, 2] </ref>. This section proposes a method for achieving the same results on a conventional processor. Memory and synchronization operations are examples of operations incurring long periods of latency.
Reference: [53] <author> Intel Corporation. </author> <title> i860 64-bit Microprocessor Programmer's Reference Manual, </title> <booktitle> 1989. </booktitle> <pages> 167 </pages>
Reference-contexts: for both threads: calli r29 ; switch to other thread mov r1,r29 ; delay slot: save this thread return address b) special case k = 2 8.2.4.2 Intel i860 Reduced instruction set (RISC) architectures, such as the i860 or mc88000, save the call return address in a register, usually r1 <ref> [53, 75] </ref>. Furthermore, the instruction following the branch, called the branch delay slot, is executed before the branch is taken.
Reference: [54] <author> Intel Corporation. </author> <title> i860 XP Microprocessor Data Book, </title> <month> November </month> <year> 1991. </year> <title> Order Number: </title> <type> 240874-002. </type>
Reference-contexts: To achieve higher clock rates, modern microprocessors use increasing degrees of pipelining, resulting in higher (3 to 6 cycle floating-point latencies are not uncommon) <ref> [75, 54, 33] </ref>). Memory accesses also incur long latencies, especially in multiprocessors where cache miss rates tend to be higher due to sharing and main memory accesses must traverse multistage networks. <p> Further, no limit was placed on the number memory loads or cache misses that may be outstanding. While commercially available microprocessors limit the number of outstanding memory operations to two or three <ref> [43, 75, 57, 54] </ref>, the results presented here favor increasing this number for multiprocessor building blocks. It is argued that caching will tend to become less effective in large-scale multiprocessors, and that the ability to tolerate large mem will be an important feature of the processor architecture.
Reference: [55] <author> Norman P. Jouppi and David W. Wall. </author> <title> Available instruction-level parallelism for superscalar and superpipelined machines. </title> <booktitle> In Proceedings of the Third International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 272-282, </pages> <year> 1989. </year>
Reference-contexts: The basic observation that pipelining and multiple instruction issuing are essentially equivalent methods of exploiting fine grain parallelism is well-known <ref> [55, 99, 98] </ref>. The relentless increase in microprocessor 4 clock speeds dictates that individual processors employ pipelined instruction execution. <p> By contrast, only MDG, FLO52Q and SPEC77 contain any significant amount of coarse grain functional parallelism, and in all three cases the average parallelism is less than a factor of two. Note that fine grain functional parallelism is closely related to what is commonly referred to as instruction-level parallelism <ref> [62, 94, 1, 55, 20, 98, 99, 109] </ref>. Instruction-level parallelism, however, may include some loop-level parallelism from innermost loops, if speculative execution, loop unrolling, or software pipelining are applied [62, 94, 55, 98, 99]. <p> Note that fine grain functional parallelism is closely related to what is commonly referred to as instruction-level parallelism [62, 94, 1, 55, 20, 98, 99, 109]. Instruction-level parallelism, however, may include some loop-level parallelism from innermost loops, if speculative execution, loop unrolling, or software pipelining are applied <ref> [62, 94, 55, 98, 99] </ref>. Fine grain ATG parallelism is similar to instruction-level parallelism within basic blocks, except that ATGs may contain more than one basic block inside an innermost loop (if the ATG contains control dependences). <p> Since the most frequently executed innermost loops in the Perfect Club R fl codes tend to be free of conditional branches, the fine grain functional parallelism figures of Table 5.4 are similar to basic block parallelism reported elsewhere <ref> [94, 1, 55, 20, 98, 99] </ref>. Note, however, that since we omit flow-of-control instructions from the HTG in order to measure arithmetic operation parallelism, our estimates of parallelism tend to be somewhat lower. Consider, for example, an innermost FORTRAN DO loop. <p> The argument is based on the observation that instruction-level parallelism can be exploited by either deep pipelining or multiple instruction issuing. This observation has also been made by Lilja, Jouppi and Wall, and others in the context of superscalar versus superpipelined processor design <ref> [67, 55, 99, 98] </ref>. It is argued that pipelining of functional units is inevitable because of increasing processor speeds, and further that split-phase transactions to shared memory in multiprocessors will tend to increase the effective average instruction latency (even with caches).
Reference: [56] <author> Leonard Kleinrock. </author> <title> Queuing Systems, Volume I: Theory. </title> <publisher> John Wiley and Sons, </publisher> <year> 1975. </year>
Reference-contexts: Since the average instruction latency is , the completion rate with a single outstanding instruction is 1= . The number of outstanding instructions is proc , hence 105 = proc = : (6:8) This is essentially a restatement of Little's Law <ref> [56] </ref>.
Reference: [57] <author> Leslie Kohn and Neal Margulis. </author> <title> The i860(TM) 64-bit supercomputing microprocessor. </title> <booktitle> In Proceedings Supercomputing '89, </booktitle> <pages> pages 450-456, </pages> <year> 1989. </year>
Reference-contexts: Further, no limit was placed on the number memory loads or cache misses that may be outstanding. While commercially available microprocessors limit the number of outstanding memory operations to two or three <ref> [43, 75, 57, 54] </ref>, the results presented here favor increasing this number for multiprocessor building blocks. It is argued that caching will tend to become less effective in large-scale multiprocessors, and that the ability to tolerate large mem will be an important feature of the processor architecture.
Reference: [58] <author> V. Krothapalli and P. Sadayappan. </author> <title> An approach to synchronization for parallel computing. </title> <booktitle> In Proceedings 1988 International Conference on Supercomputing, </booktitle> <pages> pages 573-579, </pages> <year> 1988. </year>
Reference-contexts: In several cases, it was observed that potential functional parallelism was inhibited by storage related dependences (anti- and output-dependences) that could be eliminated by use of additional local variables. In principle, a compiler could be made to detect these cases and modify the code to avoid these dependences <ref> [58] </ref>. Consider, for example, a code fragment taken from the program MDG, in the subroutine INTERF, shown in Figure 5.4.
Reference: [59] <author> C. Kruskal and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(10), </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: This method is called static scheduling or prescheduling and achieves good efficiency because the allocation of work has to be done only once at the beginning of the loop, with P atomic operations or critical sections (one per processor) <ref> [59] </ref>. However, if the body of the loop contains conditional branches or inner loops with a variable number of iterations, dividing the number of iterations statically may not result in optimal load balance among the processors.
Reference: [60] <author> Clyde P. Kruskal, Larry Rudolph, and Marc Snir. </author> <title> Efficient synchronization on multiprocessors with shared memory. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Distributed Computing, </booktitle> <pages> pages 218-228, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: This in turn can be achieved by guarding these sequences with critical sections, or by the use of appropriate hardware fetch-and- primitives <ref> [60, 69, 118, 103, 119] </ref>. 3.1.3 Handling control dependence While data dependences in task graphs merely define a precedence order among the tasks, control dependences determine whether or not certain tasks execute at all. Special attention must be paid to control dependences to guarantee correctness of the algorithms.
Reference: [61] <author> David J. Kuck. </author> <title> The Structure of Computers and Computations. </title> <publisher> John Wiley and Sons, </publisher> <year> 1978. </year>
Reference-contexts: This suggests that our scheme would still outperform a hybrid dataflow scheme, even if coupled with a conventional static instruction scheduling scheme for exploiting the instruction-level parallelism. Finally, the hardware of Chapter 7 is based on the theory of dependences <ref> [61] </ref> and the control flow execution model used by imperative languages such as C and FORTRAN. It does not require single-assignment properties, and readily handles anti- and output-dependences. Chapters 2, 3 and 4 describe compiler algorithms needed to support our hardware, that are both practical and efficient. <p> While it was assumed above that task graphs contain only intraiteration dependences, the approach can be extended to include loop-carried dependences as follows. The HTG is constructed as described in Chapter 2, obtaining loops and ATGs with intraiteration dependences only. Then all loop-carried dependences and their associated dependence distances <ref> [61, 11, 116] </ref> are found. It is assumed below that the dependence distances are constants known at compile time. Next, exit code is generated for the bit vector or counter algorithm, using an array of ATG state variables instead of a single one, one array element for each iteration. <p> Furthermore, the approach is based on the theory of dependences <ref> [61] </ref> and the control flow execution model used by imperative languages such as C and FORTRAN. It does not require single-assignment properties, and readily handles anti- and output-dependences. This hardware scheme can also be thought of as an alternative to dynamic instruction issue architectures [105, 106].
Reference: [62] <author> David J. Kuck, Y. Muraoka, and S. Chen. </author> <title> On the number of operations simultaneously executable in Fortran-like programs and their resulting speedup. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 21 </volume> <pages> 1293-1310, </pages> <year> 1972. </year>
Reference-contexts: By contrast, only MDG, FLO52Q and SPEC77 contain any significant amount of coarse grain functional parallelism, and in all three cases the average parallelism is less than a factor of two. Note that fine grain functional parallelism is closely related to what is commonly referred to as instruction-level parallelism <ref> [62, 94, 1, 55, 20, 98, 99, 109] </ref>. Instruction-level parallelism, however, may include some loop-level parallelism from innermost loops, if speculative execution, loop unrolling, or software pipelining are applied [62, 94, 55, 98, 99]. <p> Note that fine grain functional parallelism is closely related to what is commonly referred to as instruction-level parallelism [62, 94, 1, 55, 20, 98, 99, 109]. Instruction-level parallelism, however, may include some loop-level parallelism from innermost loops, if speculative execution, loop unrolling, or software pipelining are applied <ref> [62, 94, 55, 98, 99] </ref>. Fine grain ATG parallelism is similar to instruction-level parallelism within basic blocks, except that ATGs may contain more than one basic block inside an innermost loop (if the ATG contains control dependences).
Reference: [63] <author> Manoj Kumar and Gregory F. Pfister. </author> <title> The onset of hot spot contention. </title> <booktitle> In Proceedings 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 28-40, </pages> <year> 1986. </year>
Reference-contexts: In NUMA (nonuniform memory access) machines, the network distance, and hence the latency to different memory banks is not uniform across the machine. Even in UMA machines network conflicts and hot spots can cause significant variance in network transit times <ref> [63, 23] </ref>. 6.3.2 Static scheduling with variable memory latency One approach to dealing with variable memory access latencies is to generate a static schedule based on the maximum latency for each instruction [31, 93].
Reference: [64] <author> Monica S. Lam. </author> <title> A systolic array optimizing compiler. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1987. </year>
Reference-contexts: To maximize the efficiency of such machines, the compiler must properly schedule instructions so that operands have traversed the pipeline by the time they are needed by later instructions. This can be done for vectorizable loops using software pipelining techniques <ref> [50, 64, 65, 70, 102, 91, 31] </ref>. Multithreading and software pipelining lie on two ends of a spectrum of methods for improving efficiency by overlapping latency with useful work.
Reference: [65] <author> Monica S. Lam. </author> <title> Software pipelining: An effective scheduling technique for VLIW machines. </title> <booktitle> In Proceedings of the ACM SIGPLAN 1988 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 318-328, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: ATG parallelism also exists at coarser levels, where the nodes may be entire loops, subroutine calls, or basic blocks. Instruction-level parallelism is usually meant to refer to this instruction-level ATG parallelism. With techniques such as loop unrolling and software pipelining <ref> [90, 91, 5, 65] </ref>, however, inner loop parallelism is also exploited by multiple-instruction-issue machines. <p> This is possible if multiple loop iterations are allowed to execute concurrently on a single processor. This, in turn, can be accomplished by various methods such as loop unrolling, modulo scheduling <ref> [5, 21, 65, 90, 91, 31, 93] </ref>, 2 This discussion borrows the notation of Andrews and Polychronopoulos [9] wherein fi denotes the raw performance ratio of a "fast processor" to a "slow processor" in multiprocessor design trade-offs. 106 Table 6.2 Average Instruction Latency Measurements INT=1 FLOAT=1 INT=1 FLOAT=6 load latency load <p> These architectures dynamically schedule instructions, subject to dependences evaluated at run time, in order to maximally utilize processors. However, with appropriate compile-time instruction scheduling, more conventional architectures can also overlap the execution of unrelated instructions to improve utilization of pipelines <ref> [90, 91, 5, 65, 71] </ref>. This section attempts a quantitative comparison between static and dynamic scheduling in innermost loops, based on the HTG. In general it can be said that dynamic scheduling requires more sophisticated hardware than static scheduling. This extra complexity must be justified. <p> Since scheduling is fully dynamic, the proposed scheme has advantages over static software pipelining schemes <ref> [91, 65, 31] </ref> in that it readily handles both conditional branches and long and unpredictable latencies caused by cache misses. <p> To maximize the efficiency of such machines, the compiler must properly schedule instructions so that operands have traversed the pipeline by the time they are needed by later instructions. This can be done for vectorizable loops using software pipelining techniques <ref> [50, 64, 65, 70, 102, 91, 31] </ref>. Multithreading and software pipelining lie on two ends of a spectrum of methods for improving efficiency by overlapping latency with useful work.
Reference: [66] <author> Leslie Lamport. </author> <title> Time, clocks, and the ordering of events in a dsitributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: If one assumes for the sake of discussion that the execution time of the exit code is zero, the sequential execution of exit blocks does not degrade performance. Sequential execution can be relaxed in practice, if the sequential consistency of state variable accesses can be guaranteed. Sequential consistency <ref> [66] </ref> requires that the result of executing these operations is the same as some sequential execution. This can be done by executing each read-modify-write sequence affecting the global ATG state variables in an atomic fashion, i.e., as an indivisible sequence uninterrupted by any other read-modify-write sequences.
Reference: [67] <author> David Lilja and Pen-Chung Yew. </author> <title> The performance potential of fine-grain and coarse-grain parallel architecture. </title> <booktitle> In Proceedings of the Hawaii International Conference on System Sciences, </booktitle> <volume> volume 1: Architecture, </volume> <pages> pages 324-333, </pages> <address> Hawaii, </address> <month> January </month> <year> 1991. </year>
Reference-contexts: instruction issuing is appropriate, given that the system as a whole is a multiprocessor? Lilja and Yew have examined multiprocessing, pipelining and multiple instruction issuing, and conclude that the ideal architecture is a multiprocessor whose processing elements exploit a small degree of fine grain parallelism (roughly a factor of two) <ref> [68, 67] </ref>. They find similar performances between a superscalar multiprocessor and a pipelined multiprocessor, but argue against a lower-degree multiprocessor which exploits more fine grain parallelism (assuming the total architectural parallelism is fixed). <p> The argument is based on the observation that instruction-level parallelism can be exploited by either deep pipelining or multiple instruction issuing. This observation has also been made by Lilja, Jouppi and Wall, and others in the context of superscalar versus superpipelined processor design <ref> [67, 55, 99, 98] </ref>. It is argued that pipelining of functional units is inevitable because of increasing processor speeds, and further that split-phase transactions to shared memory in multiprocessors will tend to increase the effective average instruction latency (even with caches).
Reference: [68] <author> David John Lilja. </author> <title> Processor parallelism considerations and memory latency reduction in shared memory multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Research and Development, </institution> <month> July </month> <year> 1991. </year> <month> 168 </month>
Reference-contexts: instruction issuing is appropriate, given that the system as a whole is a multiprocessor? Lilja and Yew have examined multiprocessing, pipelining and multiple instruction issuing, and conclude that the ideal architecture is a multiprocessor whose processing elements exploit a small degree of fine grain parallelism (roughly a factor of two) <ref> [68, 67] </ref>. They find similar performances between a superscalar multiprocessor and a pipelined multiprocessor, but argue against a lower-degree multiprocessor which exploits more fine grain parallelism (assuming the total architectural parallelism is fixed).
Reference: [69] <author> G. J. Lipovski and Paul Vaughan. </author> <title> A fetch-and-op implementation for parallel computers. </title> <booktitle> In Proceedings International Symposium on Computer Architecture, </booktitle> <pages> pages 384-392, </pages> <year> 1988. </year>
Reference-contexts: This in turn can be achieved by guarding these sequences with critical sections, or by the use of appropriate hardware fetch-and- primitives <ref> [60, 69, 118, 103, 119] </ref>. 3.1.3 Handling control dependence While data dependences in task graphs merely define a precedence order among the tasks, control dependences determine whether or not certain tasks execute at all. Special attention must be paid to control dependences to guarantee correctness of the algorithms.
Reference: [70] <author> W. Mangione-Smith, S. Abraham, and E. Davidson. </author> <title> Vector register design for polycyclic vector scheduling. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 154-163, </pages> <year> 1991. </year>
Reference-contexts: To maximize the efficiency of such machines, the compiler must properly schedule instructions so that operands have traversed the pipeline by the time they are needed by later instructions. This can be done for vectorizable loops using software pipelining techniques <ref> [50, 64, 65, 70, 102, 91, 31] </ref>. Multithreading and software pipelining lie on two ends of a spectrum of methods for improving efficiency by overlapping latency with useful work.
Reference: [71] <author> William Mangione-Smith, Santosh G. Abraham, and Edward S. Davidson. </author> <title> Vector register design for polycyclic vector scheduling. </title> <booktitle> In Fourth International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 154-163, </pages> <year> 1991. </year>
Reference-contexts: These architectures dynamically schedule instructions, subject to dependences evaluated at run time, in order to maximally utilize processors. However, with appropriate compile-time instruction scheduling, more conventional architectures can also overlap the execution of unrelated instructions to improve utilization of pipelines <ref> [90, 91, 5, 65, 71] </ref>. This section attempts a quantitative comparison between static and dynamic scheduling in innermost loops, based on the HTG. In general it can be said that dynamic scheduling requires more sophisticated hardware than static scheduling. This extra complexity must be justified. <p> In this case, when a cache miss occurs, instruction issuing will stall as soon as the first instruction is encountered that depends on the load that missed. 7 To minimize the stall time, a larger value of load latency can be assumed when generating the static schedule <ref> [71] </ref>. Let W max denote the maximum latency for which a static schedule exists with a feasible W W max . Using W max minimizes the excess latency mem W max that gives rise to stalls on cache misses. <p> For the static scheduling experiments, modulo scheduling [90] was used to generate a static schedule. The load latency assumed for the schedule was increased from cache until a maximal W max was found for which W W max <ref> [71] </ref>. 8 size limitations I max = 20, 40, and 60. These experiments assumed an SI architecture with integer and floating-point operation latencies of 1 and 6 cycles, respectively. Memory load latencies were assumed to be cache = 2 cycles and mem = 50 cycles.
Reference: [72] <author> D. E. Maydan, J. L. Hennessy, and M. S. Lam. </author> <title> Efficient and exact data dependence analysis. </title> <booktitle> In Proceedings ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <year> 1991. </year>
Reference-contexts: Well-known algorithms exist for determining dependence direction and dependence distance vectors, which describe the relative positions of the statement instances in iteration space with respect to any number of enclosing loops <ref> [11, 116, 72, 89] </ref>. Depending on the nature of the cross-iteration dependences, loop restructuring may be used to transform a loop nest such that one or more of the loops are parallel [12].
Reference: [73] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: The processor starts self-scheduling from the first queue entry it finds with available work. If no queue entries are found ready the idle processor spin waits <ref> [117, 8, 73] </ref> on the queue until a new entry is queued or a task in an existing entry becomes ready.
Reference: [74] <institution> Motorola, Inc., </institution> <address> Englewood Cliffs, New Jersey 07632. </address> <note> MC68020 32-bit Microprocessor User's Manual, second edition, 1984. ISBN 0-13-566860-3. </note>
Reference-contexts: Thread switching thus proceeds in a round-robin fashion, with the order determined statically by the compiler. 8.2.4.1 Motorola mc68000 A generic implementation of SWITCH on a stack-based machine such as the mc68000 family <ref> [74] </ref> is given in Figure 8.10 (a) for k = 3. The special case of k = 2 is shown in Figure 8.10 (b). Here, a single register suffices to save the return address of the nonexecuting thread.
Reference: [75] <institution> Motorola, Inc., </institution> <address> Englewood Cliffs, New Jersey 07632. </address> <note> MC88100 RISC Microprocessor User's Manual, second edition, 1989. ISBN 0-13-567090-X. </note>
Reference-contexts: The CDC6600 machine featured a register scoreboard which allowed instructions to be issued before previous instructions were completed, and allowed pipelined instructions to complete out of order [105]. Scoreboards can be found in modern pipelined RISC microprocessors, such as the MC88000 family <ref> [43, 75] </ref>. Tomasulo's common data bus architecture featured similar capabilities and, in addition, implemented dynamic register renaming, allowing multiple instances of a given instruction to execute simultaneously [106]. A similar approach based on an instruction dispatch stack was proposed later by Acosta et al. [1]. <p> To achieve higher clock rates, modern microprocessors use increasing degrees of pipelining, resulting in higher (3 to 6 cycle floating-point latencies are not uncommon) <ref> [75, 54, 33] </ref>). Memory accesses also incur long latencies, especially in multiprocessors where cache miss rates tend to be higher due to sharing and main memory accesses must traverse multistage networks. <p> clock cycle, on average. 2 Note that in most superscalar and VLIW processors, there are some restrictions on the combination of instructions that can be issued in a single clock cycle because there are a limited number of functional units of each type (e.g., one integer and one floating-point unit <ref> [75, 33] </ref>). Therefore, fi is generally less than the peak theoretical issue rate, and depends on the instruction mix in the region of code being executed, since there is always one critical resource that is 100% utilized while other functional units receive a smaller fraction of instructions. <p> When m = 0 (all loads hit in the cache), the performance of the static and dynamic schemes is practically identical. Both scheduling methods achieve full utilization provided the window 7 Assuming the architecture contains a register scoreboard <ref> [43, 75] </ref> and the cache interface allows instruction issuing to proceed beyond a miss [27]. 8 In cases where W exceeded W max even for load latency cache , a schedule was generated assuming load latency cache . <p> Further, no limit was placed on the number memory loads or cache misses that may be outstanding. While commercially available microprocessors limit the number of outstanding memory operations to two or three <ref> [43, 75, 57, 54] </ref>, the results presented here favor increasing this number for multiprocessor building blocks. It is argued that caching will tend to become less effective in large-scale multiprocessors, and that the ability to tolerate large mem will be an important feature of the processor architecture. <p> Table 7.3 (a) shows the effective average instruction latency assuming integer operations have single cycle latency, floating point operations take six cycles, and memory loads have a three-cycle latency. These figures are representative of modern RISC microprocessors such as the Motorola MC88100 and Digital's 21064 Alpha chip <ref> [75, 33] </ref>. In Tables 7.3 (b) and (c) the memory load latency is increased to 7 and 20 cycles, respectively, to account for an increasing number of cache misses and long main memory latency. <p> for both threads: calli r29 ; switch to other thread mov r1,r29 ; delay slot: save this thread return address b) special case k = 2 8.2.4.2 Intel i860 Reduced instruction set (RISC) architectures, such as the i860 or mc88000, save the call return address in a register, usually r1 <ref> [53, 75] </ref>. Furthermore, the instruction following the branch, called the branch delay slot, is executed before the branch is taken. <p> Thus, SWITCH is simply a single instruction: jsr.n r1 ;pc &lt;- r1, r1 &lt;- old pc+2, ;execute delay slot at old pc+1 (The suffix .n on mc88000 instructions indicates use of the optional branch delay slot <ref> [75] </ref>.) This is not possible on the i860 because of a restriction that r1 not be the target register of a calli instruction ([53], p. 5-19).
Reference: [76] <author> Rishiyur S. Nikhil and Arvind. </author> <booktitle> Can Dataflow subsume von Neumann computing? In Proceedings of the 16th International Symposium on Computer Architecture, </booktitle> <pages> pages 262-272, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: This allows the compiler to choose the desired task granularity and to optimize away unnecessary synchronizations. It also simplifies the hardware considerably. Unlike various hybrid dataflow approaches <ref> [52, 76, 80, 79, 28] </ref>, our scheme does not employ separate synchronization or thread management instructions, and thus achieves zero scheduling overhead as long as there is sufficient parallelism to keep the pipeline utilized. This allows finer grained parallelism to be exploited than is possible in these hybrid dataflow approaches. <p> Dynamic Instruction Scheduling Another important issue in processor design is the instruction issuing mechanism. Conventional processors issue instructions in the order they are encountered in the program text (subject to control flow). Sophisticated Von Neumann architectures allow out-of-order execution of instructions [106, 81, 1, 14], as do dataflow architectures <ref> [10, 32, 52, 76, 80, 28] </ref>. These architectures dynamically schedule instructions, subject to dependences evaluated at run time, in order to maximally utilize processors. <p> Simply providing more outstanding loads may be an effective way of achieving this, without resorting to more complex approaches such as dataflow <ref> [10, 32, 52, 76, 80, 28] </ref> or multithreading [18, 104, 2, 113]. 123 CHAPTER 7 HARDWARE FOR FINE GRAIN ATG PARALLELISM The dynamic scheduling algorithms presented in Chapter 3 can be implemented in software with reasonable efficiency if task granularities are on the order of tens to hundreds of instructions. <p> In a sense, the proposed scheme goes one step beyond the explicit token store concept [80] by making synchronization explicit and separating it from data transfer. This allows the compiler to choose the desired task granularity and to optimize away unnecessary synchronizations. Unlike various hybrid dataflow approaches <ref> [52, 76, 80, 79, 28] </ref>, the proposed hardware scheme does not employ separate synchronization or thread management instructions, and thus achieves zero scheduling overhead as long as there is sufficient parallelism to keep the pipeline utilized. <p> As a means of avoiding processor idle time due to these delays, a number of multithreaded architectures have been proposed that attempt to hide such long-latency operations by maintaining multiple threads of execution on a single processor and rapidly context-switching between them <ref> [97, 104, 7, 52, 76, 80, 28, 79, 113, 47, 2] </ref>. This section proposes a method for achieving the same results on a conventional processor. Memory and synchronization operations are examples of operations incurring long periods of latency.
Reference: [77] <author> Rishiyur S. Nikhil, G. M. Papadopoulos, and Arvind. </author> <title> *T: A multithreaded massively parallel architecture. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: It does not require single-assignment properties, and readily handles anti- and output-dependences. Chapters 2, 3 and 4 describe compiler algorithms needed to support our hardware, that are both practical and efficient. Multithreading has received a great deal of attention in the recent literature <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>. Almost all of these proposals involve changes to the hardware organization. <p> ADM 1.000 2.870 21.566 3.124 8.733 64.823 MDG 1.000 3.142 24.560 4.237 10.662 74.915 DYFESM 1.000 3.255 25.807 2.589 9.355 77.009 ARC2D 1.000 2.616 18.775 2.734 7.582 56.059 TRFD 1.000 3.156 24.715 2.669 9.137 73.815 FLO52Q 1.000 2.428 16.705 2.291 6.575 49.407 SPEC77 1.000 2.531 17.839 3.602 8.195 54.119 multithreading <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>, and various forms of dynamic instruction scheduling [106, 81, 14]. The focus here is not on the details of any particular method, but rather on the characteristics common to all of them.
Reference: [78] <author> David Padua, David Kuck, and Duncan Lawrie. </author> <title> High speed multiprocessors and compilation techniques. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 29, </volume> <month> September </month> <year> 1980. </year>
Reference-contexts: the smallest distance is not one, i.e., the distances are k 1 ; k 2 ; :::k n , cycle shrinking can be applied to the loop to reduce these distances to 1; k 2 k 1 + 1; :::k n k 1 + 1 [84], or partial loop partitioning <ref> [78] </ref> can be used to divide all of the distances by gcd (k 1 ; k 2 ; :::k n ).
Reference: [79] <author> G. Papadopoulos and K. Traub. </author> <title> Multithreading: A revisionist view of dataflow architectures. </title> <booktitle> In Proceedings 1991 International Symposium on Computer Architecture, </booktitle> <pages> pages 342-351, </pages> <year> 1991. </year>
Reference-contexts: This allows the compiler to choose the desired task granularity and to optimize away unnecessary synchronizations. It also simplifies the hardware considerably. Unlike various hybrid dataflow approaches <ref> [52, 76, 80, 79, 28] </ref>, our scheme does not employ separate synchronization or thread management instructions, and thus achieves zero scheduling overhead as long as there is sufficient parallelism to keep the pipeline utilized. This allows finer grained parallelism to be exploited than is possible in these hybrid dataflow approaches. <p> It does not require single-assignment properties, and readily handles anti- and output-dependences. Chapters 2, 3 and 4 describe compiler algorithms needed to support our hardware, that are both practical and efficient. Multithreading has received a great deal of attention in the recent literature <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>. Almost all of these proposals involve changes to the hardware organization. <p> ADM 1.000 2.870 21.566 3.124 8.733 64.823 MDG 1.000 3.142 24.560 4.237 10.662 74.915 DYFESM 1.000 3.255 25.807 2.589 9.355 77.009 ARC2D 1.000 2.616 18.775 2.734 7.582 56.059 TRFD 1.000 3.156 24.715 2.669 9.137 73.815 FLO52Q 1.000 2.428 16.705 2.291 6.575 49.407 SPEC77 1.000 2.531 17.839 3.602 8.195 54.119 multithreading <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>, and various forms of dynamic instruction scheduling [106, 81, 14]. The focus here is not on the details of any particular method, but rather on the characteristics common to all of them. <p> In a sense, the proposed scheme goes one step beyond the explicit token store concept [80] by making synchronization explicit and separating it from data transfer. This allows the compiler to choose the desired task granularity and to optimize away unnecessary synchronizations. Unlike various hybrid dataflow approaches <ref> [52, 76, 80, 79, 28] </ref>, the proposed hardware scheme does not employ separate synchronization or thread management instructions, and thus achieves zero scheduling overhead as long as there is sufficient parallelism to keep the pipeline utilized. <p> As a means of avoiding processor idle time due to these delays, a number of multithreaded architectures have been proposed that attempt to hide such long-latency operations by maintaining multiple threads of execution on a single processor and rapidly context-switching between them <ref> [97, 104, 7, 52, 76, 80, 28, 79, 113, 47, 2] </ref>. This section proposes a method for achieving the same results on a conventional processor. Memory and synchronization operations are examples of operations incurring long periods of latency.
Reference: [80] <author> Gregory M. Papadopoulos and David E. Culler. Monsoon: </author> <title> An explicit token-store architecture. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 82-91, </pages> <address> New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: This is signaled by the arrival of tokens carrying data as well 5 as synchronization information. The scheme developed here, by contrast, goes one step beyond the explicit token store concept <ref> [80] </ref> by making synchronization explicit and separating it from data transfer. This allows the compiler to choose the desired task granularity and to optimize away unnecessary synchronizations. It also simplifies the hardware considerably. <p> This allows the compiler to choose the desired task granularity and to optimize away unnecessary synchronizations. It also simplifies the hardware considerably. Unlike various hybrid dataflow approaches <ref> [52, 76, 80, 79, 28] </ref>, our scheme does not employ separate synchronization or thread management instructions, and thus achieves zero scheduling overhead as long as there is sufficient parallelism to keep the pipeline utilized. This allows finer grained parallelism to be exploited than is possible in these hybrid dataflow approaches. <p> Dynamic Instruction Scheduling Another important issue in processor design is the instruction issuing mechanism. Conventional processors issue instructions in the order they are encountered in the program text (subject to control flow). Sophisticated Von Neumann architectures allow out-of-order execution of instructions [106, 81, 1, 14], as do dataflow architectures <ref> [10, 32, 52, 76, 80, 28] </ref>. These architectures dynamically schedule instructions, subject to dependences evaluated at run time, in order to maximally utilize processors. <p> Simply providing more outstanding loads may be an effective way of achieving this, without resorting to more complex approaches such as dataflow <ref> [10, 32, 52, 76, 80, 28] </ref> or multithreading [18, 104, 2, 113]. 123 CHAPTER 7 HARDWARE FOR FINE GRAIN ATG PARALLELISM The dynamic scheduling algorithms presented in Chapter 3 can be implemented in software with reasonable efficiency if task granularities are on the order of tens to hundreds of instructions. <p> As in dataflow, tasks are "queued for execution" when they have become ready for execution. In dataflow this is signaled by the arrival of tokens carrying data as well as synchronization information. In a sense, the proposed scheme goes one step beyond the explicit token store concept <ref> [80] </ref> by making synchronization explicit and separating it from data transfer. This allows the compiler to choose the desired task granularity and to optimize away unnecessary synchronizations. <p> In a sense, the proposed scheme goes one step beyond the explicit token store concept [80] by making synchronization explicit and separating it from data transfer. This allows the compiler to choose the desired task granularity and to optimize away unnecessary synchronizations. Unlike various hybrid dataflow approaches <ref> [52, 76, 80, 79, 28] </ref>, the proposed hardware scheme does not employ separate synchronization or thread management instructions, and thus achieves zero scheduling overhead as long as there is sufficient parallelism to keep the pipeline utilized. <p> As a means of avoiding processor idle time due to these delays, a number of multithreaded architectures have been proposed that attempt to hide such long-latency operations by maintaining multiple threads of execution on a single processor and rapidly context-switching between them <ref> [97, 104, 7, 52, 76, 80, 28, 79, 113, 47, 2] </ref>. This section proposes a method for achieving the same results on a conventional processor. Memory and synchronization operations are examples of operations incurring long periods of latency.
Reference: [81] <author> Yale N. Patt, Wen-Mei Hwu, and Michael Shebanow. HPS, </author> <title> a new microarchitecture: Rationale and introduction. </title> <booktitle> In Proceedings of the 18th Annual Workshop on Microprogramming, </booktitle> <pages> pages 103-108. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> December </month> <year> 1985. </year>
Reference-contexts: A similar approach based on an instruction dispatch stack was proposed later by Acosta et al. [1]. Dataflow execution of a conventional dynamic instruction stream was also proposed in the HPS architecture <ref> [81] </ref>. In all of these approaches, instructions are issued to functional units in the same order encountered in a sequential execution, although instruction issuing can get ahead of instruction completion (thus exploiting pipeline parallelism). <p> 3.255 25.807 2.589 9.355 77.009 ARC2D 1.000 2.616 18.775 2.734 7.582 56.059 TRFD 1.000 3.156 24.715 2.669 9.137 73.815 FLO52Q 1.000 2.428 16.705 2.291 6.575 49.407 SPEC77 1.000 2.531 17.839 3.602 8.195 54.119 multithreading [18, 47, 104, 113, 2, 79, 77, 17, 111], and various forms of dynamic instruction scheduling <ref> [106, 81, 14] </ref>. The focus here is not on the details of any particular method, but rather on the characteristics common to all of them. Let the window size W denote the number of loop iterations that execute concurrently on a single processor at any given time. <p> Dynamic Instruction Scheduling Another important issue in processor design is the instruction issuing mechanism. Conventional processors issue instructions in the order they are encountered in the program text (subject to control flow). Sophisticated Von Neumann architectures allow out-of-order execution of instructions <ref> [106, 81, 1, 14] </ref>, as do dataflow architectures [10, 32, 52, 76, 80, 28]. These architectures dynamically schedule instructions, subject to dependences evaluated at run time, in order to maximally utilize processors.
Reference: [82] <author> Paul Marx Petersen. </author> <title> Evaluation of programs and parallelizing compilers using dynamic analysis techniques. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Research and Development, </institution> <month> January </month> <year> 1993. </year> <month> 169 </month>
Reference-contexts: Another form of parallelism in programs is nonloop or functional parallelism [42, 85, 86]. While data parallelism can provide large amounts of parallelism that often scales with problem size, recent studies have suggested that a significant amount of nonloop parallelism also exists in numerical codes <ref> [22, 82] </ref>. Theoretical foundations and methods for detecting functional parallelism automatically in a compiler were laid down in previous studies [42]. The focus of this thesis, by contrast, is the exploitation of functional parallelism at run time once it has been detected by the compiler. <p> Measurements of nonloop parallelism have been made by several researchers by using a compiler to insert source code which calculates a program's critical path at run time <ref> [22, 82] </ref>. In some cases, large amounts of parallelism have been found beyond that available from doall loops alone.
Reference: [83] <author> Constantine D. Polychronopoulos. </author> <title> On program restructuring, scheduling, and communi-cation for parallel processor systems. </title> <type> PhD thesis, </type> <institution> Computer Science Department, University of Illinois at Urbana-Champaign, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: They generally guarantee load balance within one or two iteration times, while incurring significantly less overhead than self-scheduling, provided N is sufficiently larger than P . The GSS algorithm, like self-scheduling, is also robust to variances in processor starting times <ref> [83] </ref>. 2.2.2 Acyclic task graphs Dynamic scheduling of acyclic task graphs requires more attention to correctness. Tasks may be dispatched to processors only once it has been determined that any precedence constraints or dependences have been satisfied.
Reference: [84] <author> Constantine D. Polychronopoulos. </author> <title> Compiler optimizations for enhancing parallelism and their impact on architecture design. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(8), </volume> <month> August </month> <year> 1988. </year>
Reference-contexts: If the smallest distance is not one, i.e., the distances are k 1 ; k 2 ; :::k n , cycle shrinking can be applied to the loop to reduce these distances to 1; k 2 k 1 + 1; :::k n k 1 + 1 <ref> [84] </ref>, or partial loop partitioning [78] can be used to divide all of the distances by gcd (k 1 ; k 2 ; :::k n ). <p> Software multithreading utilizes a portion of the parallelism detected by the autoscheduling compiler to maintain high processor utilization despite long memory access latency. 8.1 Autoscheduling as a Source-to-Source Translation This section describes f2c, an experimental Parafrase-2 pass for generating autoscheduling drive code <ref> [84, 86] </ref> for parallel programs. Drive code is code inserted into the functional code of a program, which explicitly manages the program's parallelism. Since this user-level code does not involve the operating system, efficient execution can be achieved.
Reference: [85] <author> Constantine D. Polychronopoulos. </author> <title> Toward auto-scheduling compilers. </title> <journal> Journal of Supercomputing, </journal> <month> November </month> <year> 1988. </year>
Reference-contexts: Much work has been done to date on exploiting loop-based or data parallelism. Another form of parallelism in programs is nonloop or functional parallelism <ref> [42, 85, 86] </ref>. While data parallelism can provide large amounts of parallelism that often scales with problem size, recent studies have suggested that a significant amount of nonloop parallelism also exists in numerical codes [22, 82]. <p> Sarkar treats the task merging problem using the program 1 Autoscheduling consists of dynamic self-scheduling code generated by a compiler to drive a program's exe cution in a dataflow-like fashion <ref> [85, 86] </ref>. 3 dependence graph (PDG) [95, 96]. Whereas Sarkar's algorithms treat entire subroutines at once, considering the merging of sibling tasks and parent-child tasks simultaneously, the algorithm developed here processes each level of the HTG separately. <p> This is a very real consideration for practical compiler construction, since programmers are not likely to make use of an optimization that increases compile time beyond a certain point. In the area of dynamic scheduling algorithms, Polychronopoulos provided the baseline algorithms <ref> [85, 86] </ref> for which the bit vector and counter algorithms described in Chapter 3 represent improvements. In particular, the counter algorithm of Chapter 3 is similar to the task exit code described in [85, 86] with two significant improvements. First, separate predicate bits are not needed to represent control dependences. <p> In the area of dynamic scheduling algorithms, Polychronopoulos provided the baseline algorithms <ref> [85, 86] </ref> for which the bit vector and counter algorithms described in Chapter 3 represent improvements. In particular, the counter algorithm of Chapter 3 is similar to the task exit code described in [85, 86] with two significant improvements. First, separate predicate bits are not needed to represent control dependences. Second, multiple counters can be packed into a single variable, greatly improving the efficiency and storage requirements of the algorithm. <p> successors are the set of sinks of the node's outgoing edges, and its predecessors are the set of sources of its incoming edges. 2.1 Hierarchical Task Graphs The basis of the approach to program scheduling described below is the hierarchical task graph (HTG), which has the following definition and properties <ref> [85, 40, 86, 42] </ref>. The hierarchy is formed by extracting the loop structure of the program. This includes explicit loops, such as do and while statements, as well as implicit loops, e.g., those formed by using goto statements. <p> The serial queue is used for individual serial tasks and has priority over the parallel queue because, heuristically, serial tasks are more likely to lie on the program's critical path than are parallel tasks <ref> [85] </ref>. The parallel queue is used for parallel tasks such as doall loops. It is also accessed in FIFO order, except that acquiring a task from the queue does not remove the task from the queue.
Reference: [86] <author> Constantine D. Polychronopoulos. </author> <title> Auto-scheduling: Control flow and data flow come together. </title> <type> Technical Report 1058, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <year> 1990. </year>
Reference-contexts: Much work has been done to date on exploiting loop-based or data parallelism. Another form of parallelism in programs is nonloop or functional parallelism <ref> [42, 85, 86] </ref>. While data parallelism can provide large amounts of parallelism that often scales with problem size, recent studies have suggested that a significant amount of nonloop parallelism also exists in numerical codes [22, 82]. <p> Sarkar treats the task merging problem using the program 1 Autoscheduling consists of dynamic self-scheduling code generated by a compiler to drive a program's exe cution in a dataflow-like fashion <ref> [85, 86] </ref>. 3 dependence graph (PDG) [95, 96]. Whereas Sarkar's algorithms treat entire subroutines at once, considering the merging of sibling tasks and parent-child tasks simultaneously, the algorithm developed here processes each level of the HTG separately. <p> This is a very real consideration for practical compiler construction, since programmers are not likely to make use of an optimization that increases compile time beyond a certain point. In the area of dynamic scheduling algorithms, Polychronopoulos provided the baseline algorithms <ref> [85, 86] </ref> for which the bit vector and counter algorithms described in Chapter 3 represent improvements. In particular, the counter algorithm of Chapter 3 is similar to the task exit code described in [85, 86] with two significant improvements. First, separate predicate bits are not needed to represent control dependences. <p> In the area of dynamic scheduling algorithms, Polychronopoulos provided the baseline algorithms <ref> [85, 86] </ref> for which the bit vector and counter algorithms described in Chapter 3 represent improvements. In particular, the counter algorithm of Chapter 3 is similar to the task exit code described in [85, 86] with two significant improvements. First, separate predicate bits are not needed to represent control dependences. Second, multiple counters can be packed into a single variable, greatly improving the efficiency and storage requirements of the algorithm. <p> successors are the set of sinks of the node's outgoing edges, and its predecessors are the set of sources of its incoming edges. 2.1 Hierarchical Task Graphs The basis of the approach to program scheduling described below is the hierarchical task graph (HTG), which has the following definition and properties <ref> [85, 40, 86, 42] </ref>. The hierarchy is formed by extracting the loop structure of the program. This includes explicit loops, such as do and while statements, as well as implicit loops, e.g., those formed by using goto statements. <p> Software multithreading utilizes a portion of the parallelism detected by the autoscheduling compiler to maintain high processor utilization despite long memory access latency. 8.1 Autoscheduling as a Source-to-Source Translation This section describes f2c, an experimental Parafrase-2 pass for generating autoscheduling drive code <ref> [84, 86] </ref> for parallel programs. Drive code is code inserted into the functional code of a program, which explicitly manages the program's parallelism. Since this user-level code does not involve the operating system, efficient execution can be achieved.
Reference: [87] <author> Constantine D. Polychronopoulos, Milind Girkar, Mohammad Haghighat, Chia-Ling Lee, Bruce Leung, and Dale Schouten. </author> <title> Parafrase-2: An environment for parallelizing, partitioning, synchronizing, and scheduling programs on multiprocessors. </title> <booktitle> In Proceedings 1989 International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 39-48, </pages> <year> 1989. </year>
Reference-contexts: Some aspects of software implementations of autoscheduling 1 using conventional languages on conventional architectures are discussed in Chapter 8. A rudimentary implementation of autoscheduling is presented as a source-to-source translation in the Parafrase-2 compiler <ref> [87] </ref>, and practical issues related to memory allocation and thread management are discussed. A novel technique for memory latency tolerance on conventional architectures called software multithreading is also discussed. This technique achieves some of the goals of the hardware described in Chapter 7, but on conventional microprocessors. <p> Extracting this description is done in several steps. Using the Parafrase-2 parallelizing compiler <ref> [87] </ref>, the hierarchical task graph is extracted using the hier compiler pass [42]. A number of other compiler passes are also applied to aid in the construction of the HTG and to obtain output in the desired format. <p> These can be used not only for testing and debugging the analysis tools, but also as a means of interactively exploring the parallel structure of programs prior to actual coding. 5.1.1.1 Constructing HTGs in Parafrase-2 Parafrase-2 is a multipass source-to-source restructuring compiler for the FORTRAN and C programming languages <ref> [87] </ref>. The passes are invoked under user control, to customize its operation to a particular need. The primary pass used to construct the HTG in Parafrase-2 is called hier, but a number of other passes are required additionally to obtain best results. <p> 4452395 4315789 1.032 3789201 3619157 1.047 121 Table 6.6 Dynamic Scheduling Improvement Ratio in Loops with Branches I max arch load 20 40 60 80 SI 3 1.072 1.080 1.094 1.126 MI 3 1.075 1.081 1.086 1.112 HTGs derived from the Perfect Club R fl benchmarks with a parallelizing compiler <ref> [87] </ref>. It is argued that the ideal multiprocessor building block need not issue more than one instruction per clock, but should have the ability to issue instructions out of order.
Reference: [88] <author> Constantine D. Polychronopoulos and David Kuck. </author> <title> Guided self-scheduling: A practical scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 36(12) </volume> <pages> 1425-1439, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: It is assumed that scheduling is nonpreemptive, i.e., tasks are run to completion. 2.2.1 Scheduling parallel loops Loops are generally regarded as the largest source of parallelism within ordinary programs, and hence dynamic scheduling of parallel loops has received much attention in the literature to date <ref> [88, 13, 108, 51] </ref>. Dynamic scheduling of loops is not a topic of this thesis, since an array of existing techniques is available for our use. The loop scheduling problem consists of assigning iterations of the loop to processors for execution. <p> Chunk scheduling can guarantee only that processors finish executing within C iteration times of each other, as opposed to within a single iteration time in the case of self-scheduling. To remedy this, several algorithms decrease the chunk size as the loop nears completion. Guided self-scheduling (GSS) <ref> [88] </ref>, trapezoidal self-scheduling (TSS) [108] and factoring [51] are all examples of this. These techniques differ in the initial chunk size, the way in which chunk size is decreased, robustness to certain anomalies, and total number of scheduling operations. <p> Synchronization latency has been cited by some researchers as a fundamental problem in multiprocessing justifying a dataflow approach [52]. Consider, for example, a parallel loop spread across multiple processors. To achieve good load balancing and low run-time overhead, a dynamic loop scheduling algorithm such as GSS or TSS <ref> [88, 108] </ref> can be used. This requires each processor to issue a fetch-and- operation [44] to a shared memory location after every chunk of iterations it completes. Note that in both the GSS and TSS algorithms, this chunk size varies (decreases) as the loop progresses. <p> k do i=1, l sum = 0 sum = sum + a (i,k)*b (k,j) c (i,j) = c (i,j) + sum end do Guided self-scheduling This algorithm is similar to self-scheduling, except that the number of iterations dispatched to one thread at a time varies as the loop is executed <ref> [88] </ref>. If R is the number of iterations remaining to be executed, dR=P e iterations are dispatched to a requesting thread.
Reference: [89] <author> William Pugh. </author> <title> A practical algorithm for exact array dependence analysis. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Well-known algorithms exist for determining dependence direction and dependence distance vectors, which describe the relative positions of the statement instances in iteration space with respect to any number of enclosing loops <ref> [11, 116, 72, 89] </ref>. Depending on the nature of the cross-iteration dependences, loop restructuring may be used to transform a loop nest such that one or more of the loops are parallel [12].
Reference: [90] <author> B. R. Rau and C. D. Glaeser. </author> <title> Some scheduling techniques and an easily schedulable horizontal architecture for high performance scientific computing. </title> <booktitle> In 14th Annual Microprogramming Workshop, </booktitle> <pages> pages 183-198. </pages> <publisher> Computer Society Press, </publisher> <month> October </month> <year> 1981. </year>
Reference-contexts: Different levels of loop parallelism can also be distinguished. Innermost loop parallelism is due to loops containing no other parallel loops, although they may include subroutine calls or sequential loops. A subset of this is generalized vector computations (GVCs <ref> [90] </ref>), which includes loops containing only statements or fine grain ATGs, no loops or subroutine calls (except FORTRAN intrinsic functions such as sin, cos, real, and aimag). <p> ATG parallelism also exists at coarser levels, where the nodes may be entire loops, subroutine calls, or basic blocks. Instruction-level parallelism is usually meant to refer to this instruction-level ATG parallelism. With techniques such as loop unrolling and software pipelining <ref> [90, 91, 5, 65] </ref>, however, inner loop parallelism is also exploited by multiple-instruction-issue machines. <p> This is possible if multiple loop iterations are allowed to execute concurrently on a single processor. This, in turn, can be accomplished by various methods such as loop unrolling, modulo scheduling <ref> [5, 21, 65, 90, 91, 31, 93] </ref>, 2 This discussion borrows the notation of Andrews and Polychronopoulos [9] wherein fi denotes the raw performance ratio of a "fast processor" to a "slow processor" in multiprocessor design trade-offs. 106 Table 6.2 Average Instruction Latency Measurements INT=1 FLOAT=1 INT=1 FLOAT=6 load latency load <p> These architectures dynamically schedule instructions, subject to dependences evaluated at run time, in order to maximally utilize processors. However, with appropriate compile-time instruction scheduling, more conventional architectures can also overlap the execution of unrelated instructions to improve utilization of pipelines <ref> [90, 91, 5, 65, 71] </ref>. This section attempts a quantitative comparison between static and dynamic scheduling in innermost loops, based on the HTG. In general it can be said that dynamic scheduling requires more sophisticated hardware than static scheduling. This extra complexity must be justified. <p> For the static scheduling experiments, modulo scheduling <ref> [90] </ref> was used to generate a static schedule. The load latency assumed for the schedule was increased from cache until a maximal W max was found for which W W max [71]. 8 size limitations I max = 20, 40, and 60.
Reference: [91] <author> B. Ramakrishna Rau, Christopher D. Glaeser, and Raymond L. </author> <title> Picard. Efficient code generation for horizontal architectures: </title> <booktitle> Compiler techniques and architectural support. In Proceedings of the 9th International Symposium on Computer Architecture, </booktitle> <pages> pages 131-139, </pages> <year> 1982. </year>
Reference-contexts: ATG parallelism also exists at coarser levels, where the nodes may be entire loops, subroutine calls, or basic blocks. Instruction-level parallelism is usually meant to refer to this instruction-level ATG parallelism. With techniques such as loop unrolling and software pipelining <ref> [90, 91, 5, 65] </ref>, however, inner loop parallelism is also exploited by multiple-instruction-issue machines. <p> This is possible if multiple loop iterations are allowed to execute concurrently on a single processor. This, in turn, can be accomplished by various methods such as loop unrolling, modulo scheduling <ref> [5, 21, 65, 90, 91, 31, 93] </ref>, 2 This discussion borrows the notation of Andrews and Polychronopoulos [9] wherein fi denotes the raw performance ratio of a "fast processor" to a "slow processor" in multiprocessor design trade-offs. 106 Table 6.2 Average Instruction Latency Measurements INT=1 FLOAT=1 INT=1 FLOAT=6 load latency load <p> These architectures dynamically schedule instructions, subject to dependences evaluated at run time, in order to maximally utilize processors. However, with appropriate compile-time instruction scheduling, more conventional architectures can also overlap the execution of unrelated instructions to improve utilization of pipelines <ref> [90, 91, 5, 65, 71] </ref>. This section attempts a quantitative comparison between static and dynamic scheduling in innermost loops, based on the HTG. In general it can be said that dynamic scheduling requires more sophisticated hardware than static scheduling. This extra complexity must be justified. <p> Since scheduling is fully dynamic, the proposed scheme has advantages over static software pipelining schemes <ref> [91, 65, 31] </ref> in that it readily handles both conditional branches and long and unpredictable latencies caused by cache misses. <p> To maximize the efficiency of such machines, the compiler must properly schedule instructions so that operands have traversed the pipeline by the time they are needed by later instructions. This can be done for vectorizable loops using software pipelining techniques <ref> [50, 64, 65, 70, 102, 91, 31] </ref>. Multithreading and software pipelining lie on two ends of a spectrum of methods for improving efficiency by overlapping latency with useful work.
Reference: [92] <author> B. Ramakrishna Rau, Michael S. Schlansker, and P. P. Tirumalai. </author> <title> Code generation schema for modulo scheduled loops. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Microarchitecture (MICRO-25), </booktitle> <pages> pages 158-169, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: an individual MI processor may be underutilized, while the SI multiprocessor can spread its processors across high-level tasks or independent jobs if necessary, attaining a higher fraction of its peak throughput on 6 The per-iteration time is also referred to as the iteration interval, or II, in software pipelining literature <ref> [92] </ref>. 110 1 16 32 48 64 0.0 1.0 2.0 ADM loop 457 ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ffi ? ? ? . . . . . . . . . . . . . . . . . . .. . . . . .
Reference: [93] <author> B. Ramakrishna Rau, David W. L. Yen, Wei Yen, and Ross A. Towle. </author> <title> The Cydra 5 departmental supercomputer: Design philosophies, decisions and tradeoffs. </title> <journal> Computer, </journal> <volume> 22(1) </volume> <pages> 12-35, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: One aspect of instruction issuing of particular interest is the number of instructions issued per clock cycle. Modern long instruction word (LIW) and superscalar architectures promise to deliver higher throughput by increasing the number of instructions issued per clock cycle beyond one <ref> [26, 21, 45, 31, 93, 115] </ref>, and many have delivered on the promise. While this can be quite effective in a uniprocessor, the advantages of multiple instruction issuing past a certain point are less clear in a multiprocessor environment. <p> In the discussion below, SI denotes a single-instruction-issue processor such as a conventional RISC, and MI denotes an architecture capable of issuing multiple instructions per clock cycle such as a VLIW or superscalar processor <ref> [26, 21, 45, 31, 93, 115] </ref>. 6.2.1 Latency, concurrency and utilization Let the utilization of a single processor be defined as the average number of instructions issued per clock cycle. A single-instruction-issue processor is fully utilized if it issues an instruction every cycle. <p> This is possible if multiple loop iterations are allowed to execute concurrently on a single processor. This, in turn, can be accomplished by various methods such as loop unrolling, modulo scheduling <ref> [5, 21, 65, 90, 91, 31, 93] </ref>, 2 This discussion borrows the notation of Andrews and Polychronopoulos [9] wherein fi denotes the raw performance ratio of a "fast processor" to a "slow processor" in multiprocessor design trade-offs. 106 Table 6.2 Average Instruction Latency Measurements INT=1 FLOAT=1 INT=1 FLOAT=6 load latency load <p> Even in UMA machines network conflicts and hot spots can cause significant variance in network transit times [63, 23]. 6.3.2 Static scheduling with variable memory latency One approach to dealing with variable memory access latencies is to generate a static schedule based on the maximum latency for each instruction <ref> [31, 93] </ref>. For example, if memory load instructions take cache on a cache hit and mem on a cache miss (ignoring network effects), a static schedule could be generated assuming that all loads incur latency mem . <p> A number of techniques exist for dealing with conditional branches in loops on statically scheduled machines. Some machines support predicated execution of instructions, in which instructions can be made to execute conditionally upon the value of a flag register <ref> [31, 93] </ref>. Conditional branches can then be substituted with instructions that set the appropriate flag register. Instructions from both branches of an if statement are issued, but only those from one of the branches are actually enabled for execution. This process is called if-conversion [112]. <p> This can be accomplished by adapting the rotating register windows scheme used in VLIW architectures such as the Cydra-5 <ref> [31, 93] </ref>. The Cydra-5 is a statically scheduled VLIW architecture in which software pipelining is used to generate an instruction schedule for a loop which generally overlaps several iterations at once [93]. <p> The Cydra-5 is a statically scheduled VLIW architecture in which software pipelining is used to generate an instruction schedule for a loop which generally overlaps several iterations at once <ref> [93] </ref>. To avoid code expansion due to explicit, compile-time register renaming, the Cydra-5 allows register accesses in loops to refer to an array of registers indexed by the current iteration modulo the array size (the array size must be as large as the number of overlapped iterations).
Reference: [94] <author> Edward M. Riseman and Caxton C. Foster. </author> <title> The inhibition of parallelism due to conditional jumps. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 21(12) </volume> <pages> 1405-1411, </pages> <year> 1972. </year>
Reference-contexts: By contrast, only MDG, FLO52Q and SPEC77 contain any significant amount of coarse grain functional parallelism, and in all three cases the average parallelism is less than a factor of two. Note that fine grain functional parallelism is closely related to what is commonly referred to as instruction-level parallelism <ref> [62, 94, 1, 55, 20, 98, 99, 109] </ref>. Instruction-level parallelism, however, may include some loop-level parallelism from innermost loops, if speculative execution, loop unrolling, or software pipelining are applied [62, 94, 55, 98, 99]. <p> Note that fine grain functional parallelism is closely related to what is commonly referred to as instruction-level parallelism [62, 94, 1, 55, 20, 98, 99, 109]. Instruction-level parallelism, however, may include some loop-level parallelism from innermost loops, if speculative execution, loop unrolling, or software pipelining are applied <ref> [62, 94, 55, 98, 99] </ref>. Fine grain ATG parallelism is similar to instruction-level parallelism within basic blocks, except that ATGs may contain more than one basic block inside an innermost loop (if the ATG contains control dependences). <p> Since the most frequently executed innermost loops in the Perfect Club R fl codes tend to be free of conditional branches, the fine grain functional parallelism figures of Table 5.4 are similar to basic block parallelism reported elsewhere <ref> [94, 1, 55, 20, 98, 99] </ref>. Note, however, that since we omit flow-of-control instructions from the HTG in order to measure arithmetic operation parallelism, our estimates of parallelism tend to be somewhat lower. Consider, for example, an innermost FORTRAN DO loop.
Reference: [95] <author> Vivek Sarkar. </author> <title> Automatic partitioning of a program dependence graph into parallel tasks. </title> <journal> IBM Journal of Research and Development, </journal> 35(5/6):779-804, September/October 1991. <volume> 170 </volume>
Reference-contexts: Sarkar treats the task merging problem using the program 1 Autoscheduling consists of dynamic self-scheduling code generated by a compiler to drive a program's exe cution in a dataflow-like fashion [85, 86]. 3 dependence graph (PDG) <ref> [95, 96] </ref>. Whereas Sarkar's algorithms treat entire subroutines at once, considering the merging of sibling tasks and parent-child tasks simultaneously, the algorithm developed here processes each level of the HTG separately.
Reference: [96] <author> Vivek Sarkar and David Cann. </author> <title> POSC a partitioning and optimizing SISAL compiler. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 148-163, </pages> <year> 1990. </year>
Reference-contexts: Sarkar treats the task merging problem using the program 1 Autoscheduling consists of dynamic self-scheduling code generated by a compiler to drive a program's exe cution in a dataflow-like fashion [85, 86]. 3 dependence graph (PDG) <ref> [95, 96] </ref>. Whereas Sarkar's algorithms treat entire subroutines at once, considering the merging of sibling tasks and parent-child tasks simultaneously, the algorithm developed here processes each level of the HTG separately.
Reference: [97] <author> Burton Smith. </author> <title> A pipelined, shared resource MIMD computer. </title> <booktitle> In Proceedings 1978 International Conference on Parallel Processing, </booktitle> <pages> pages 6-8, </pages> <year> 1978. </year>
Reference-contexts: Instructions from independent tasks may thus be presented to the functional unit on successive clock cycles, as in other multithreaded architectures <ref> [97, 104] </ref>. 127 active matrix TEST BITS SET BITS D U S Y A R u SATISFIED logic e o e mux offsets start task ID ready instruction address Each instruction word contains two fields: a functional unit instruction, which is like an ordinary instruction in a conventional architecture (typically 32 <p> As a means of avoiding processor idle time due to these delays, a number of multithreaded architectures have been proposed that attempt to hide such long-latency operations by maintaining multiple threads of execution on a single processor and rapidly context-switching between them <ref> [97, 104, 7, 52, 76, 80, 28, 79, 113, 47, 2] </ref>. This section proposes a method for achieving the same results on a conventional processor. Memory and synchronization operations are examples of operations incurring long periods of latency.
Reference: [98] <author> Michael D. Smith, Mike Johnson, and Mark A. Horowitz. </author> <title> Limits on multiple instruction issue. </title> <booktitle> In Proceedings of the Third International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 290-302, </pages> <year> 1989. </year>
Reference-contexts: The basic observation that pipelining and multiple instruction issuing are essentially equivalent methods of exploiting fine grain parallelism is well-known <ref> [55, 99, 98] </ref>. The relentless increase in microprocessor 4 clock speeds dictates that individual processors employ pipelined instruction execution. <p> By contrast, only MDG, FLO52Q and SPEC77 contain any significant amount of coarse grain functional parallelism, and in all three cases the average parallelism is less than a factor of two. Note that fine grain functional parallelism is closely related to what is commonly referred to as instruction-level parallelism <ref> [62, 94, 1, 55, 20, 98, 99, 109] </ref>. Instruction-level parallelism, however, may include some loop-level parallelism from innermost loops, if speculative execution, loop unrolling, or software pipelining are applied [62, 94, 55, 98, 99]. <p> Note that fine grain functional parallelism is closely related to what is commonly referred to as instruction-level parallelism [62, 94, 1, 55, 20, 98, 99, 109]. Instruction-level parallelism, however, may include some loop-level parallelism from innermost loops, if speculative execution, loop unrolling, or software pipelining are applied <ref> [62, 94, 55, 98, 99] </ref>. Fine grain ATG parallelism is similar to instruction-level parallelism within basic blocks, except that ATGs may contain more than one basic block inside an innermost loop (if the ATG contains control dependences). <p> Since the most frequently executed innermost loops in the Perfect Club R fl codes tend to be free of conditional branches, the fine grain functional parallelism figures of Table 5.4 are similar to basic block parallelism reported elsewhere <ref> [94, 1, 55, 20, 98, 99] </ref>. Note, however, that since we omit flow-of-control instructions from the HTG in order to measure arithmetic operation parallelism, our estimates of parallelism tend to be somewhat lower. Consider, for example, an innermost FORTRAN DO loop. <p> The argument is based on the observation that instruction-level parallelism can be exploited by either deep pipelining or multiple instruction issuing. This observation has also been made by Lilja, Jouppi and Wall, and others in the context of superscalar versus superpipelined processor design <ref> [67, 55, 99, 98] </ref>. It is argued that pipelining of functional units is inevitable because of increasing processor speeds, and further that split-phase transactions to shared memory in multiprocessors will tend to increase the effective average instruction latency (even with caches).
Reference: [99] <author> Gurindar S. Sohi and Sriram Vajapeyam. </author> <title> Tradeoffs in instruction format design for horizontal architectures. </title> <booktitle> In Proceedings of the Third International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 15-25, </pages> <year> 1989. </year>
Reference-contexts: The basic observation that pipelining and multiple instruction issuing are essentially equivalent methods of exploiting fine grain parallelism is well-known <ref> [55, 99, 98] </ref>. The relentless increase in microprocessor 4 clock speeds dictates that individual processors employ pipelined instruction execution. <p> By contrast, only MDG, FLO52Q and SPEC77 contain any significant amount of coarse grain functional parallelism, and in all three cases the average parallelism is less than a factor of two. Note that fine grain functional parallelism is closely related to what is commonly referred to as instruction-level parallelism <ref> [62, 94, 1, 55, 20, 98, 99, 109] </ref>. Instruction-level parallelism, however, may include some loop-level parallelism from innermost loops, if speculative execution, loop unrolling, or software pipelining are applied [62, 94, 55, 98, 99]. <p> Note that fine grain functional parallelism is closely related to what is commonly referred to as instruction-level parallelism [62, 94, 1, 55, 20, 98, 99, 109]. Instruction-level parallelism, however, may include some loop-level parallelism from innermost loops, if speculative execution, loop unrolling, or software pipelining are applied <ref> [62, 94, 55, 98, 99] </ref>. Fine grain ATG parallelism is similar to instruction-level parallelism within basic blocks, except that ATGs may contain more than one basic block inside an innermost loop (if the ATG contains control dependences). <p> Since the most frequently executed innermost loops in the Perfect Club R fl codes tend to be free of conditional branches, the fine grain functional parallelism figures of Table 5.4 are similar to basic block parallelism reported elsewhere <ref> [94, 1, 55, 20, 98, 99] </ref>. Note, however, that since we omit flow-of-control instructions from the HTG in order to measure arithmetic operation parallelism, our estimates of parallelism tend to be somewhat lower. Consider, for example, an innermost FORTRAN DO loop. <p> The argument is based on the observation that instruction-level parallelism can be exploited by either deep pipelining or multiple instruction issuing. This observation has also been made by Lilja, Jouppi and Wall, and others in the context of superscalar versus superpipelined processor design <ref> [67, 55, 99, 98] </ref>. It is argued that pipelining of functional units is inevitable because of increasing processor speeds, and further that split-phase transactions to shared memory in multiprocessors will tend to increase the effective average instruction latency (even with caches).
Reference: [100] <author> Hong-Men Su. </author> <title> On multiprocessor synchronization and data transfer. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Research and Development, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: However, their fork-join task graphs do not include control dependences, as the acyclic task graphs described here do. Su and Yew have proposed a process-oriented scheme for synchronization of loop-carried dependences <ref> [101, 100] </ref>. Chapter 3 presents dynamic scheduling algorithms for acyclic task graphs and generalizes them to handle loop-carried dependences in Section 3.2.5. The resulting method is as powerful as Su's approach in handling loop-carried dependences, but has the added advantage of synchronizing functional parallelism within each loop body as well. <p> #1001 TEST BITS 4 #0110 The exit code for task 4 is shown below: result = atomic or (SATISFIED [i+3], #1000) if ((result & #1001) == #1001) then queue task 3 in iteration i+3 This scheme bears some resemblance to Su and Yew's process-oriented synchronization method for synchronizing doacross loops <ref> [101, 100] </ref>. In this scheme, a separate synchronization variable is associated with each process or loop iteration. The variable is a counter that is incremented whenever the source of a cross-iteration dependence is completed.
Reference: [101] <author> Hong-Men Su and Pen-Chung Yew. </author> <title> On data synchronizations for multiprocessor systems. </title> <booktitle> In Proceedings of 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 416-423, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: However, their fork-join task graphs do not include control dependences, as the acyclic task graphs described here do. Su and Yew have proposed a process-oriented scheme for synchronization of loop-carried dependences <ref> [101, 100] </ref>. Chapter 3 presents dynamic scheduling algorithms for acyclic task graphs and generalizes them to handle loop-carried dependences in Section 3.2.5. The resulting method is as powerful as Su's approach in handling loop-carried dependences, but has the added advantage of synchronizing functional parallelism within each loop body as well. <p> #1001 TEST BITS 4 #0110 The exit code for task 4 is shown below: result = atomic or (SATISFIED [i+3], #1000) if ((result & #1001) == #1001) then queue task 3 in iteration i+3 This scheme bears some resemblance to Su and Yew's process-oriented synchronization method for synchronizing doacross loops <ref> [101, 100] </ref>. In this scheme, a separate synchronization variable is associated with each process or loop iteration. The variable is a counter that is incremented whenever the source of a cross-iteration dependence is completed.
Reference: [102] <author> J.-H. Tang, E. Davidson, and J. Tong. </author> <title> Polycyclic vector scheduling vs. chaining on 1-port vector supercomputers. </title> <booktitle> In Proceedings Supercomputing 88, </booktitle> <pages> pages 122-129, </pages> <year> 1988. </year>
Reference-contexts: To maximize the efficiency of such machines, the compiler must properly schedule instructions so that operands have traversed the pipeline by the time they are needed by later instructions. This can be done for vectorizable loops using software pipelining techniques <ref> [50, 64, 65, 70, 102, 91, 31] </ref>. Multithreading and software pipelining lie on two ends of a spectrum of methods for improving efficiency by overlapping latency with useful work.
Reference: [103] <author> Peiyi Tang, Chuan-Qi Zhu, and Pen-Chung Yew. </author> <title> An implementation of cedar synchronization primitives. </title> <type> Technical report, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> April </month> <year> 1984. </year>
Reference-contexts: This in turn can be achieved by guarding these sequences with critical sections, or by the use of appropriate hardware fetch-and- primitives <ref> [60, 69, 118, 103, 119] </ref>. 3.1.3 Handling control dependence While data dependences in task graphs merely define a precedence order among the tasks, control dependences determine whether or not certain tasks execute at all. Special attention must be paid to control dependences to guarantee correctness of the algorithms.
Reference: [104] <author> Mark R. Thistle and Burton J. Smith. </author> <title> A processor architecture for Horizon. </title> <booktitle> In Proceedings Supercomputing '88, </booktitle> <pages> pages 35-41. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1988. </year>
Reference-contexts: It does not require single-assignment properties, and readily handles anti- and output-dependences. Chapters 2, 3 and 4 describe compiler algorithms needed to support our hardware, that are both practical and efficient. Multithreading has received a great deal of attention in the recent literature <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>. Almost all of these proposals involve changes to the hardware organization. <p> ADM 1.000 2.870 21.566 3.124 8.733 64.823 MDG 1.000 3.142 24.560 4.237 10.662 74.915 DYFESM 1.000 3.255 25.807 2.589 9.355 77.009 ARC2D 1.000 2.616 18.775 2.734 7.582 56.059 TRFD 1.000 3.156 24.715 2.669 9.137 73.815 FLO52Q 1.000 2.428 16.705 2.291 6.575 49.407 SPEC77 1.000 2.531 17.839 3.602 8.195 54.119 multithreading <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>, and various forms of dynamic instruction scheduling [106, 81, 14]. The focus here is not on the details of any particular method, but rather on the characteristics common to all of them. <p> The advantage of dynamic scheduling in the presence of cache misses is that lexically succeeding instructions that do not depend on the cache-missing load could be issued to keep the pipeline busy while the miss is being serviced <ref> [104, 7] </ref>. <p> Simply providing more outstanding loads may be an effective way of achieving this, without resorting to more complex approaches such as dataflow [10, 32, 52, 76, 80, 28] or multithreading <ref> [18, 104, 2, 113] </ref>. 123 CHAPTER 7 HARDWARE FOR FINE GRAIN ATG PARALLELISM The dynamic scheduling algorithms presented in Chapter 3 can be implemented in software with reasonable efficiency if task granularities are on the order of tens to hundreds of instructions. <p> Instructions from independent tasks may thus be presented to the functional unit on successive clock cycles, as in other multithreaded architectures <ref> [97, 104] </ref>. 127 active matrix TEST BITS SET BITS D U S Y A R u SATISFIED logic e o e mux offsets start task ID ready instruction address Each instruction word contains two fields: a functional unit instruction, which is like an ordinary instruction in a conventional architecture (typically 32 <p> As a means of avoiding processor idle time due to these delays, a number of multithreaded architectures have been proposed that attempt to hide such long-latency operations by maintaining multiple threads of execution on a single processor and rapidly context-switching between them <ref> [97, 104, 7, 52, 76, 80, 28, 79, 113, 47, 2] </ref>. This section proposes a method for achieving the same results on a conventional processor. Memory and synchronization operations are examples of operations incurring long periods of latency.
Reference: [105] <author> J. E. Thornton. </author> <title> Parallel operations in the Control Data 6600. </title> <booktitle> In AFIPS Proceedings FJCC, </booktitle> <volume> pt 2, volume 26, </volume> <pages> pages 33-40, </pages> <year> 1964. </year>
Reference-contexts: Architectures which exploit instruction-level parallelism have existed for some time. The CDC6600 machine featured a register scoreboard which allowed instructions to be issued before previous instructions were completed, and allowed pipelined instructions to complete out of order <ref> [105] </ref>. Scoreboards can be found in modern pipelined RISC microprocessors, such as the MC88000 family [43, 75]. Tomasulo's common data bus architecture featured similar capabilities and, in addition, implemented dynamic register renaming, allowing multiple instances of a given instruction to execute simultaneously [106]. <p> It does not require single-assignment properties, and readily handles anti- and output-dependences. This hardware scheme can also be thought of as an alternative to dynamic instruction issue architectures <ref> [105, 106] </ref>. Given a sufficiently large instruction scheduling window and hardware register renaming or loop unrolling by the compiler, these architectures can exploit the same parallelism as the proposed hardware.
Reference: [106] <author> R. M. Tomasulo. </author> <title> An efficient algorithm for exploiting multiple arithmetic units. </title> <journal> IBM Journal, </journal> <volume> 11 </volume> <pages> 25-33, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: Scoreboards can be found in modern pipelined RISC microprocessors, such as the MC88000 family [43, 75]. Tomasulo's common data bus architecture featured similar capabilities and, in addition, implemented dynamic register renaming, allowing multiple instances of a given instruction to execute simultaneously <ref> [106] </ref>. A similar approach based on an instruction dispatch stack was proposed later by Acosta et al. [1]. Dataflow execution of a conventional dynamic instruction stream was also proposed in the HPS architecture [81]. <p> 3.255 25.807 2.589 9.355 77.009 ARC2D 1.000 2.616 18.775 2.734 7.582 56.059 TRFD 1.000 3.156 24.715 2.669 9.137 73.815 FLO52Q 1.000 2.428 16.705 2.291 6.575 49.407 SPEC77 1.000 2.531 17.839 3.602 8.195 54.119 multithreading [18, 47, 104, 113, 2, 79, 77, 17, 111], and various forms of dynamic instruction scheduling <ref> [106, 81, 14] </ref>. The focus here is not on the details of any particular method, but rather on the characteristics common to all of them. Let the window size W denote the number of loop iterations that execute concurrently on a single processor at any given time. <p> Dynamic Instruction Scheduling Another important issue in processor design is the instruction issuing mechanism. Conventional processors issue instructions in the order they are encountered in the program text (subject to control flow). Sophisticated Von Neumann architectures allow out-of-order execution of instructions <ref> [106, 81, 1, 14] </ref>, as do dataflow architectures [10, 32, 52, 76, 80, 28]. These architectures dynamically schedule instructions, subject to dependences evaluated at run time, in order to maximally utilize processors. <p> It does not require single-assignment properties, and readily handles anti- and output-dependences. This hardware scheme can also be thought of as an alternative to dynamic instruction issue architectures <ref> [105, 106] </ref>. Given a sufficiently large instruction scheduling window and hardware register renaming or loop unrolling by the compiler, these architectures can exploit the same parallelism as the proposed hardware.
Reference: [107] <author> Josep Torellas, Monica S. Lam, and John L. Hennessy. </author> <title> False sharing and spatial locality in multiprocessor caches. </title> <journal> IEEE Transactions on Computers, </journal> <note> 1992. to appear. </note>
Reference-contexts: It is difficult for the compiler to predict exactly when cache misses will occur. The problem is exacerbated in multiprocessors because other processors may invalidate a processors' cached locations, due to either true or false sharing <ref> [107, 34] </ref>. Another source of uncertainty is the variability in mem itself, due to the processor-memory interconnection network (or the processor-processor interconnection network in distributed-shared-memory machines). In NUMA (nonuniform memory access) machines, the network distance, and hence the latency to different memory banks is not uniform across the machine.
Reference: [108] <author> Ten H. Tzen and Lionel M. Ni. </author> <title> Dynamic loop scheduling for shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 247-250. </pages> <publisher> CRC Press, Inc., </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: It is assumed that scheduling is nonpreemptive, i.e., tasks are run to completion. 2.2.1 Scheduling parallel loops Loops are generally regarded as the largest source of parallelism within ordinary programs, and hence dynamic scheduling of parallel loops has received much attention in the literature to date <ref> [88, 13, 108, 51] </ref>. Dynamic scheduling of loops is not a topic of this thesis, since an array of existing techniques is available for our use. The loop scheduling problem consists of assigning iterations of the loop to processors for execution. <p> To remedy this, several algorithms decrease the chunk size as the loop nears completion. Guided self-scheduling (GSS) [88], trapezoidal self-scheduling (TSS) <ref> [108] </ref> and factoring [51] are all examples of this. These techniques differ in the initial chunk size, the way in which chunk size is decreased, robustness to certain anomalies, and total number of scheduling operations. <p> Synchronization latency has been cited by some researchers as a fundamental problem in multiprocessing justifying a dataflow approach [52]. Consider, for example, a parallel loop spread across multiple processors. To achieve good load balancing and low run-time overhead, a dynamic loop scheduling algorithm such as GSS or TSS <ref> [88, 108] </ref> can be used. This requires each processor to issue a fetch-and- operation [44] to a shared memory location after every chunk of iterations it completes. Note that in both the GSS and TSS algorithms, this chunk size varies (decreases) as the loop progresses.
Reference: [109] <author> August K. Uht. </author> <title> Concurrency extraction via hardware methods executing the static instruction stream. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(7) </volume> <pages> 826-841, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Uht has proposed a scheme based on the static instruction stream instead of the dynamic instruction stream used in the previous schemes <ref> [109] </ref>. When a loop is encountered, the hardware calculates data and control dependences between instructions in the loop body, and uses these to guide instruction issuing into multiple functional units. Speculative execution and control dependence information are also used to avoid stalls due to conditional branches. <p> By contrast, only MDG, FLO52Q and SPEC77 contain any significant amount of coarse grain functional parallelism, and in all three cases the average parallelism is less than a factor of two. Note that fine grain functional parallelism is closely related to what is commonly referred to as instruction-level parallelism <ref> [62, 94, 1, 55, 20, 98, 99, 109] </ref>. Instruction-level parallelism, however, may include some loop-level parallelism from innermost loops, if speculative execution, loop unrolling, or software pipelining are applied [62, 94, 55, 98, 99].
Reference: [110] <author> Alexander Veidenbaum. </author> <title> Compiler optimizations and architecture design issues for multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> May </month> <year> 1985. </year> <month> 171 </month>
Reference-contexts: The measured speedups are thus indicative of parallelism that the compiler has already detected. Veidenbaum has made measurements of fine and coarse grain nonloop parallelism in a number of benchmark subroutines <ref> [110] </ref>. His results are similar to the results in this thesis in that he finds only a modest amount of parallelism from independent tasks and instructions (less than a factor of two, on average). He also finds somewhat more parallelism at the fine grain level.
Reference: [111] <author> Carl A. Waldspurger and William E. Weihl. </author> <title> Register relocation: Flexible contexts for multithreading. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 120-130, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: It does not require single-assignment properties, and readily handles anti- and output-dependences. Chapters 2, 3 and 4 describe compiler algorithms needed to support our hardware, that are both practical and efficient. Multithreading has received a great deal of attention in the recent literature <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>. Almost all of these proposals involve changes to the hardware organization. <p> Almost all of these proposals involve changes to the hardware organization. Waldspurger and Weihl describe an approach in which the compiler can partition a large physical register file to provide efficient use of registers and a large number of threads, given a limited physical register file space <ref> [111] </ref>. Additional instructions are added to the architecture to maintain the appearance of a fixed register file size to the software, while facilitating efficient partition swapping at thread switch points. The authors also mention a purely software-based implementation, similar to the software multithreading scheme of Chapter 8. <p> ADM 1.000 2.870 21.566 3.124 8.733 64.823 MDG 1.000 3.142 24.560 4.237 10.662 74.915 DYFESM 1.000 3.255 25.807 2.589 9.355 77.009 ARC2D 1.000 2.616 18.775 2.734 7.582 56.059 TRFD 1.000 3.156 24.715 2.669 9.137 73.815 FLO52Q 1.000 2.428 16.705 2.291 6.575 49.407 SPEC77 1.000 2.531 17.839 3.602 8.195 54.119 multithreading <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>, and various forms of dynamic instruction scheduling [106, 81, 14]. The focus here is not on the details of any particular method, but rather on the characteristics common to all of them.
Reference: [112] <author> Nancy J. Warter, Grant E. Haab, Krishna Subramanian, and John W. Bockhaus. </author> <title> Enhanced modulo scheduling for loops with conditional branches. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Microarchitecture (MICRO-25), </booktitle> <pages> pages 170-179, </pages> <month> De-cember </month> <year> 1992. </year>
Reference-contexts: Conditional branches can then be substituted with instructions that set the appropriate flag register. Instructions from both branches of an if statement are issued, but only those from one of the branches are actually enabled for execution. This process is called if-conversion <ref> [112] </ref>. Since all instructions are issued in each iteration, generating a static schedule that overlaps multiple iterations is as easy as for a loop without conditional branches. Trace scheduling assumes that one direction of a conditional branch is taken much more frequently than the other [38]. <p> A number of purely software techniques also exist for software pipelining both directions of conditional branches. These include hierarchical reduction, GURPR* and enhanced modulo scheduling (EMS) <ref> [112] </ref>. These techniques use if-conversion to transform the body of the loop into straight-line code for which a static schedule can be easily generated. This static schedule is then regenerated into executable code by replicating instructions and inserting appropriate conditional branches. <p> Warter et al. have shown that EMS has performance advantages over predicated execution as well as hierarchical reduction and GURPR*, and that code expansion is typically less than a factor of 2, with small instruction latencies <ref> [112] </ref>. However, these results assumed cache hit latencies for memory loads. Since the interest here is in 118 using software pipelining to achieve memory latency tolerance, window sizes tend to be larger and code explosion could be a serious problem. <p> (i) = x (i) - a (j,i)*x (j) 13 GOTO 4 ; ELSE 14 x [R3] := R0 ; x (i) = 0.0 15 GOTO 4 ;END DO (b) single-thread three-address code with conditional branches because it can lead to code explosion or reduced throughput due to nonoptimal iteration interval <ref> [112] </ref>. To overlap the latency of the loads at lines 5 and 9, the compiler creates multiple "software threads" (or simply threads), and generates code to explicitly switch between these threads. Multithreaded code for the example in Figure 8.8 is shown in Figure 8.9 for a single thread.
Reference: [113] <author> Wolf-Dietrich Weber and Anoop Gupta. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: Preliminary results. </title> <booktitle> In Proceedings of the 16th International Symposium on Computer Architecture, </booktitle> <pages> pages 273-280, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: It does not require single-assignment properties, and readily handles anti- and output-dependences. Chapters 2, 3 and 4 describe compiler algorithms needed to support our hardware, that are both practical and efficient. Multithreading has received a great deal of attention in the recent literature <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>. Almost all of these proposals involve changes to the hardware organization. <p> ADM 1.000 2.870 21.566 3.124 8.733 64.823 MDG 1.000 3.142 24.560 4.237 10.662 74.915 DYFESM 1.000 3.255 25.807 2.589 9.355 77.009 ARC2D 1.000 2.616 18.775 2.734 7.582 56.059 TRFD 1.000 3.156 24.715 2.669 9.137 73.815 FLO52Q 1.000 2.428 16.705 2.291 6.575 49.407 SPEC77 1.000 2.531 17.839 3.602 8.195 54.119 multithreading <ref> [18, 47, 104, 113, 2, 79, 77, 17, 111] </ref>, and various forms of dynamic instruction scheduling [106, 81, 14]. The focus here is not on the details of any particular method, but rather on the characteristics common to all of them. <p> Simply providing more outstanding loads may be an effective way of achieving this, without resorting to more complex approaches such as dataflow [10, 32, 52, 76, 80, 28] or multithreading <ref> [18, 104, 2, 113] </ref>. 123 CHAPTER 7 HARDWARE FOR FINE GRAIN ATG PARALLELISM The dynamic scheduling algorithms presented in Chapter 3 can be implemented in software with reasonable efficiency if task granularities are on the order of tens to hundreds of instructions. <p> As a means of avoiding processor idle time due to these delays, a number of multithreaded architectures have been proposed that attempt to hide such long-latency operations by maintaining multiple threads of execution on a single processor and rapidly context-switching between them <ref> [97, 104, 7, 52, 76, 80, 28, 79, 113, 47, 2] </ref>. This section proposes a method for achieving the same results on a conventional processor. Memory and synchronization operations are examples of operations incurring long periods of latency.
Reference: [114] <author> M. Weiss, Z. Fhang, C. R. Morgan, and P. </author> <title> Belmont. Dynamic scheduling and memory management for parallel programs. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 161-165, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: First, separate predicate bits are not needed to represent control dependences. Second, multiple counters can be packed into a single variable, greatly improving the efficiency and storage requirements of the algorithm. Dynamic scheduling of general program constructs is also treated by Weiss et al. <ref> [114] </ref>. They discuss scheduling issues as well as memory management, and they support both loop and "fork-join" task graph scheduling. However, their fork-join task graphs do not include control dependences, as the acyclic task graphs described here do. <p> Since the tree is analogous to a stack on a sequential machine, and is accessed like a stack from the point of view of each individual thread, it is sometimes referred to as a cactus stack <ref> [6, 114] </ref>. Memory allocation for the cactus stack is not as easy as for a sequential machine stack, since the dynamically changing tree cannot be laid out linearly in memory.
Reference: [115] <author> Andrew Wolfe and John P. Shen. </author> <title> A variable instruction stream extension to the VLIW architecture. </title> <booktitle> In Fourth International Conference on Architecture Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2-14, </pages> <year> 1991. </year>
Reference-contexts: One aspect of instruction issuing of particular interest is the number of instructions issued per clock cycle. Modern long instruction word (LIW) and superscalar architectures promise to deliver higher throughput by increasing the number of instructions issued per clock cycle beyond one <ref> [26, 21, 45, 31, 93, 115] </ref>, and many have delivered on the promise. While this can be quite effective in a uniprocessor, the advantages of multiple instruction issuing past a certain point are less clear in a multiprocessor environment. <p> In the discussion below, SI denotes a single-instruction-issue processor such as a conventional RISC, and MI denotes an architecture capable of issuing multiple instructions per clock cycle such as a VLIW or superscalar processor <ref> [26, 21, 45, 31, 93, 115] </ref>. 6.2.1 Latency, concurrency and utilization Let the utilization of a single processor be defined as the average number of instructions issued per clock cycle. A single-instruction-issue processor is fully utilized if it issues an instruction every cycle.
Reference: [116] <author> M. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> Pitman Publishing, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: Well-known algorithms exist for determining dependence direction and dependence distance vectors, which describe the relative positions of the statement instances in iteration space with respect to any number of enclosing loops <ref> [11, 116, 72, 89] </ref>. Depending on the nature of the cross-iteration dependences, loop restructuring may be used to transform a loop nest such that one or more of the loops are parallel [12]. <p> While it was assumed above that task graphs contain only intraiteration dependences, the approach can be extended to include loop-carried dependences as follows. The HTG is constructed as described in Chapter 2, obtaining loops and ATGs with intraiteration dependences only. Then all loop-carried dependences and their associated dependence distances <ref> [61, 11, 116] </ref> are found. It is assumed below that the dependence distances are constants known at compile time. Next, exit code is generated for the bit vector or counter algorithm, using an array of ATG state variables instead of a single one, one array element for each iteration.
Reference: [117] <author> John Zahorjan and Edward D. Lazowska. </author> <title> Spinning versus blocking in parallel systems with uncertainty. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Wash-ington, </institution> <month> February </month> <year> 1988. </year>
Reference-contexts: The processor starts self-scheduling from the first queue entry it finds with available work. If no queue entries are found ready the idle processor spin waits <ref> [117, 8, 73] </ref> on the queue until a new entry is queued or a task in an existing entry becomes ready.
Reference: [118] <author> Chuan-Qi Zhu and Pen-Chung Yew. </author> <title> A synchronization scheme and its applications for large multiprocessor systems. </title> <booktitle> Fourth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 486-493, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: This in turn can be achieved by guarding these sequences with critical sections, or by the use of appropriate hardware fetch-and- primitives <ref> [60, 69, 118, 103, 119] </ref>. 3.1.3 Handling control dependence While data dependences in task graphs merely define a precedence order among the tasks, control dependences determine whether or not certain tasks execute at all. Special attention must be paid to control dependences to guarantee correctness of the algorithms.

References-found: 118

