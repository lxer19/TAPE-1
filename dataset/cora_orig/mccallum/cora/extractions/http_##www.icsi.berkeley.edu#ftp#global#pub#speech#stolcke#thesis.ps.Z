URL: http://www.icsi.berkeley.edu/ftp/global/pub/speech/stolcke/thesis.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/ftp/global/pub/speech/stolcke/
Root-URL: http://www.icsi.berkeley.edu
Title: Chair Date  
Author: Date Date Date 
Date: 1994  
Affiliation: University of California at Berkeley  
Note: The dissertation of Andreas Stolcke is approved:  
Abstract-found: 0
Intro-found: 1
Reference: <author> AHO, ALFRED V., RAVI SETHI, & JEFFREY D. ULLMAN. </author> <year> 1986. </year> <title> Compilers: </title> <booktitle> Principles, Techniques and Tools. </booktitle> <address> Reading, Mass.: </address> <publisher> Addison-Wesley. </publisher> , & <address> JEFFREY D. ULLMAN. </address> <year> 1972. </year> <title> The Theory of Parsing, Translation, and Compiling. Volume1: Parsing. </title>
Reference-contexts: Section 5.5 discusses limitations and problems with the current approach and points out possible remedies. Section 5.6 summarizes the main points. 5.2 Probabilistic Attribute Grammars Attribute grammars are a familiar extension of non-probabilistic CFGs known from compiler theory and elsewhere <ref> (Aho et al. 1986) </ref>. Attributes are `registers' or `slots' attached to the nonterminal nodes in a context-free parse tree, and can hold `values' for various purposes, typically pertaining to the semantics of CHAPTER 5. PROBABILISTIC ATTRIBUTE GRAMMARS 105 the language. <p> However, the 6 Therefore the strict bottom-up feature format could be relaxed, e.g., by using the notion of L-attributed feature specifications <ref> (Aho et al. 1986) </ref>. CHAPTER 5. PROBABILISTIC ATTRIBUTE GRAMMARS 119 theory of Markov networks (Pearl 1988) tells us that such a marginal probabilities cannot be assigned locally in a consistent fashion.
Reference: <editor> Englewood Cliffs, </editor> <address> N.J.: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> ANGLUIN, D., & C. H. SMITH. </author> <year> 1983. </year> <title> Inductive inference: Theory and methods. </title> <journal> ACM Computing Surveys 15.237-269. </journal>
Reference-contexts: It has also been applied to the induction of non-probabilistic automata <ref> (Angluin & Smith 1983) </ref>. Still in the field of non-probabilistic automata induction, Tomita (1982) has used a simple hill-climbing procedure combined with a goodness measure based on positive/negative samples to search the space of possible models.
Reference: <author> BAHL, LALIT R., FREDERICK JELINEK, & ROBERT L. MERCER. </author> <year> 1983. </year> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 5.179-190. </journal>
Reference-contexts: The key feature in this context is that a string x that can be generated by both M 1 and M 2 has at least two possible derivations, whose identity is not observable. Mixing models is also known in the language modeling literature as interpolation <ref> (Bahl et al. 1983) </ref>. <p> It is amazing how much overlap, apparently without mutual knowledge, there is between the text compression field and probabilistic computational linguistics. For example, the problem of smoothing zero-probability estimates and the solutions using mixtures <ref> (Bahl et al. 1983) </ref> or back-off models (Katz 1987) all have almost perfect analogs in the various strategies for building code spaces for compression models. 9 Bell et al. (1990) attribute the state merging idea to Evans (1971). CHAPTER 3. HIDDEN MARKOV MODELS 48 complexity and data fit. <p> These conditional probabilities can then be used as word transition probabilities in a Viterbi-style decoder or to incrementally compute the cost function for a stack decoder <ref> (Bahl et al. 1983) </ref>. Another application where prefix probabilities play a central role is the extraction of n-gram probabilities from SCFGs, a problem that is the subject of Chapter 7. Here, too, efficient incremental computation saves time since the work for common prefix strings can be shared.
Reference: <author> BAKER, JAMES K. </author> <year> 1979. </year> <title> Trainable grammars for speech recognition. </title> <booktitle> In Speech Communication Papers for the 97th Meeting of the Acoustical Society of America, </booktitle> <editor> ed. by Jared J. Wolf & Dennis H. Klatt, </editor> <address> 547-550, </address> <publisher> MIT, </publisher> <address> Cambridge, Mass. </address>
Reference-contexts: As for HMMs, dynamic programming (in the guise of chart parsing) can be used to compute the expected counts ^c with relative efficiency. The dynamic programming scheme for SCFGs is known as the Inside-Outside algorithm <ref> (Baker 1979) </ref>. It was originally formulated for SCFGs in Chomsky Normal Form (CNF), but has since been generalized to cope with arbitrary SCFGs. Rule estimation is also part of the probabilistic Earley parsing framework described in Chapter 6 (Section 6.5.2). <p> Partial parses are assembled just as in non-probabilistic parsing (modulo possible pruning based on probabilities), while substring probabilities (also known as `inside' probabilities) can be computed in a straightforward way. Thus, the CYK chart parser underlies the `standard' solutions to problems (1) and (4) <ref> (Baker 1979) </ref>, as well as (2) (Jelinek 1985). While the Jelinek & Lafferty (1991) solution to problem (3) is not a direct extension of CYK parsing they nevertheless present their algorithm in terms of its similarities to the computation of inside probabilities. <p> This constitutes the main distinguishing feature of Earley parsing compared to the strict bottom-up computation used in the standard inside probability computation <ref> (Baker 1979) </ref>. There, inside probabilities for all positions and nonterminals are computed, regardless of possible prefixes. 7 The same technical complication was noticed by Wright (1990) in the computation of probabilistic LR parser tables. The relation to LR parsing will be discussed in Section 6.7.3. <p> The space complexity in terms of l is O (l 2 ) since l state sets of O (l) elements each have to be created. All in all, we get the same time O (l 3 ), space O (l 2 ) bounds as in the Inside/Outside <ref> (Baker 1979) </ref> and LRI (Jelinek & Lafferty 1991) algorithms, with the advantage of better results on known grammar classes. 6.4.9.2 Scaling with grammar size We will not try to give a precise characterization in the case of sparse grammars (Section 6.6.3 gives some hints on how to implement the algorithm efficiently <p> CHAPTER 6. EFFICIENT PARSING WITH STOCHASTIC CONTEXT-FREE GRAMMARS 165 Full CNF Sparse CFG Bottom-up Inside/outside Stochastic RTNs <ref> (Baker 1979) </ref> (Kupiec 1992a) Left-to-right LRI Probabilistic (Jelinek & Lafferty 1991) Earley Table 6.4: Tentative typology of SCFG algorithms according to prevailing directionality and sparseness of the CFG. zero.
Reference: <author> BALDI, PIERRE, YVES CHAUVIN, TIM HUNKAPILLER, & MARCELLA A. MCCLURE. </author> <year> 1993. </year> <title> Hidden Markov models in molecular biology: New algorithms and applications. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <editor> ed. by Stephen Jose Hanson, Jack D. Cowan, & C. Lee Giles, </editor> <address> 747-754. San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> BAUM, LEONARD E., TED PETRIE, GEORGE SOULES, & NORMAN WEISS. </author> <year> 1970. </year> <title> A maximization technique oc-curing in the statistical analysis of probabilistic functions in Markov chains. </title> <journal> The Annals of Mathematical Statistics 41.164-171. </journal>
Reference-contexts: Section 3.2 defines the HMM formalism and gives an overview of these standard estimation methods. In contrast to traditional HMM estimation based on the Baum-Welch technique <ref> (Baum et al. 1970) </ref>, our model merging method adjusts the model topology to the data. The merging operator for HMMs is very simple: it is realized by collapsing two model states (as well as their transitions and emissions). The resulting algorithm is described in Section 3.3. <p> probability mass without contributing to the total distribution P (xjM ).) These well-formed Ness conditions are always satisfied when obtaining HMMs through one of the standard estimation algorithms or through model merging, so they are of no immediate concern to us. 3.2.2 HMM estimation The Baum-Welch estimation method for HMMs <ref> (Baum et al. 1970) </ref> assumes a certain topology and adjusts the parameters so as to maximize the model likelihood on the given samples. <p> EM is a generalization of the well-known Baum-Welch algorithm for HMM estimation <ref> (Baum et al. 1970) </ref>; the original formulation for the case of SCFGs is due to Baker (1979). For SCFGs, the E-step involves computing the expected number of times each production is applied in generating the training corpus.
Reference: <author> BELL, TIMOTHY C., JOHN G. CLEARY, & IAN H. WITTEN. </author> <year> 1990. </year> <title> Text Compression. </title> <address> Englewood Cliffs, N.J.: </address> <publisher> Prentice Hall. </publisher>
Reference: <author> BOOTH, TAYLOR L., & RICHARD A. THOMPSON. </author> <year> 1973. </year> <title> Applying probability measures to abstract languages. </title> <journal> IEEE Transactions on Computers C-22.442-450. </journal>
Reference-contexts: Thus, the expected sentence string length is the S-entry in the solution vector. The problems and its solution are easily generalized to obtain the expected number of terminals of a particular type occuring in a string <ref> (Booth & Thompson 1973) </ref>. 7.8.2 Derivation entropy The derivation entropy is the average number of bits required to specify a derivation from a SCFG. Is is computed from a right-hand side vector that contains the average negative log probabilities for the productions of each LHS nonterminal.
Reference: <author> BOURLARD, HERV E, & NELSON MORGAN. </author> <year> 1993. </year> <title> Connectionist Speech Recognition. A Hybrid Approach. </title> <address> Boston, Mass.: </address> <publisher> Kluwer Academic Publishers. </publisher> <address> BIBLIOGRAPHY 185 BRILL, ERIC. </address> <year> 1993. </year> <title> Automatic grammar induction and parsing free text: A transformation-based approach. </title>
Reference: <institution> In Proceedings ARPA Workshop on Human Language Technology, Plainsboro, N.J. </institution>
Reference: <author> BRISCOE, TED, & JOHN CARROLL. </author> <year> 1993. </year> <title> Generalized probabilistic LR parsing of natural language (corpora) with unification-based grammars. </title> <note> Computational Linguistics 19.25-59. </note>
Reference: <author> BROWN, PETER F., VINCENT J. DELLA PIETRA, PETER V. DESOUZA, JENIFER C. LAI, & ROBERT L. MERCER. </author> <year> 1992. </year> <title> Class-based n-gram models of natural language. </title> <note> Computational Linguistics 18.467-479. </note>
Reference-contexts: A class-based n-gram grammar is easily represented as an HMM, with one state per class. Transition probabilities represent the conditional probabilities between classes, whereas emission probabilities correspond to the word distributions for each class (for n &gt; 2, higher-order HMMs are required). The incremental word clustering algorithm given in <ref> (Brown et al. 1992) </ref> then becomes an instance of HMM merging, albeit one that is entirely based on likelihoods. 10 3.6 Evaluation We have evaluated the HMM merging algorithm experimentally in a series of applications.
Reference: <author> BUNTINE, W. L. </author> <year> 1991. </year> <title> Theory refinement of Bayesian networks. </title> <booktitle> In Seventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Anaheim, CA. </address>
Reference: <author> BUNTINE, WRAY. </author> <year> 1992. </year> <title> Learning classification trees. </title> <booktitle> In Artificial Intelligence Frontiers in Statistics: AI and Statistics III, </booktitle> <editor> ed. by D. J. Hand. </editor> <publisher> Chapman & Hall. </publisher>
Reference-contexts: HMMs are a special kind of parameterized graph structure. Unsurprisingly, many aspects of the priors discussed in this section can be found in Bayesian approaches to the induction of graph-based models in other domains (e.g., Bayesian networks (Cooper & Herskovits 1992; Buntine 1991) and decision trees <ref> (Buntine 1992) </ref>). CHAPTER 3. HIDDEN MARKOV MODELS 35 3.3.3.1 Structural vs. parameter priors As discussed in Section 2.5.5, an HMM may be specified as a combination of structure and continuous parameters. For HMMs the structure or topology is given by as a set of states, transitions and emissions.
Reference: <author> CHEESEMAN, PETER, JAMES KELLY, MATTHEW SELF, JOHN STUTZ, WILL TAYLOR, & DON FREEMAN. </author> <year> 1988. </year> <title> AutoClass: A Bayesian classification system. </title> <booktitle> In Proceedings of the 5th International Conference on Machine Learning, </booktitle> <pages> 54-64, </pages> <institution> University of Michigan, Ann Arbor, Mich. </institution>
Reference-contexts: For example, HMMs with mixtures of Gaussians as emission densities are being used extensively (Gauvain & Lee 1991) for speech modeling. Our merging algorithm becomes applicable to such models provided that one has a prior for such densities, which should be straightforward <ref> (Cheeseman et al. 1988) </ref>. Efficient implementation of the merging operator may be a bigger problemone wants to avoid having to explicitly compute a merged density for each merge under consideration.
Reference: <author> CHEN, FRANCINE R. </author> <year> 1990. </year> <title> Identification of contextual factors for pronunciation networks. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> 753-756, </pages> <address> Albuquerque, NM. </address>
Reference: <author> CHURCH, KENNETH W., & WILLIAM A. GALE. </author> <year> 1991. </year> <title> A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language 5.19-54. </title> <type> CLEEREMANS, AXEL, </type> <year> 1991. </year> <title> Mechanisms of Implicit Learning. A Parallel Distributed Processing Model of Sequence Acquisition. </title> <institution> Pittsburgh, Pa.: Department of Psychology, Carnegie Mellon University dissertation. </institution>
Reference-contexts: As a result, very large corpora are needed for reliable estimation of n-gram models, often requiring additional sophisticated smoothing techniques to avoid the well-known problems of maximum-likelihood estimators <ref> (Church & Gale 1991) </ref>. The lack of linguistic motivation also makes n-gram practically incomprehensible to humans, and impossible to extend and maintain except by brute-force reestimation. While stochastic context-free grammars (SCFGs) have their own problems, these are to some degree complementary to those of n-grams.
Reference: <author> COOK, CRAIG M., AZRIEL ROSENFELD, & ALAN R. ARONSON. </author> <year> 1976. </year> <title> Grammatical inference by hill climbing. </title> <journal> Information Sciences 10.59-80. </journal>
Reference: <author> COOPER, GREGORY F., & EDWARD HERSKOVITS. </author> <year> 1992. </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <booktitle> Machine Learning 9.309-347. </booktitle>
Reference: <author> CORAZZA, ANNA, RENATO DE MORI, ROBERTO GRETTER, & GIORGIO SATTA. </author> <year> 1991. </year> <title> Computation of probabilities for an island-driven parser. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 13.936-950. </journal>
Reference-contexts: In the literature, SCFGs are used for the selection of parses for ambiguous inputs (Fujisaki et al. 1991); to guide the rule choice efficiently during parsing (Jones & Eisner 1992a); to compute island probabilities for non-linear parsing <ref> (Corazza et al. 1991) </ref>. In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models (Ney 1992), as well as in non-finite state acoustic and phonotactic modeling (Lari & Young 1991).
Reference: <author> COVER, THOMAS M., & JOY A. THOMAS. </author> <year> 1991. </year> <title> Elements of Information Theory. </title> <address> New York: </address> <publisher> John Wiley and Sons, Inc. </publisher>
Reference: <author> COX, R. T. </author> <year> 1946. </year> <title> Probability, frequency and reasonable expectation. </title> <journal> American Journal of Physics 14. </journal> <note> BIBLIOGRAPHY 186 DAGAN, </note> <author> IDO, FERNANDO PEREIRA, & LILLIAN LEE. </author> <year> 1994. </year> <title> Similarity-based estimation of word coocur-rence probabilities. </title> <booktitle> In Proceedings of the 31th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> New Mexico State University, Las Cruces, NM. </address>
Reference: <author> DE SAUSSURE, FERDINAND. </author> <year> 1916. </year> <institution> Cours de linguistique generale. Paris: Payot. </institution>
Reference: <author> DEMPSTER, A. P., N. M. LAIRD, & D. B. RUBIN. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B 34.1-38. </journal>
Reference-contexts: In many cases the estimation in the face of hidden variables can be solved using a general technique called expectation-maximization <ref> (Dempster et al. 1977) </ref>, or EM. <p> :Y ) as well as T 0 = Viterbi-parse (i : j Y ! -:) Adjoin T 0 to T as the right-most child at the root, and return T . 6.5.2 Rule probability estimation The rule probabilities in a SCFG can be iteratively estimated using the EM (Expectation-Maximization) algorithm <ref> (Dempster et al. 1977) </ref>. The estimation procedure finds a set of parameters that represent a local maximum of the grammar likelihood function P (DjG), given a sample corpus D.
Reference: <author> EARLEY, JAY. </author> <year> 1970. </year> <title> An efficient context-free parsing algorithm. </title> <journal> Communications of the ACM 6.451-455. </journal>
Reference: <author> EVANS, T. G. </author> <year> 1971. </year> <title> Grammatical inference techniques in pattern analysis. In Software engineering, </title> <editor> ed. by J. </editor> <booktitle> Tou, </booktitle> <pages> 183-202. </pages> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> FASS, LEONA F. </author> <year> 1983. </year> <title> Learning context-free languages from their structured sentences. </title> <journal> ACM SIGACT News 15.24-35. </journal>
Reference: <author> FELDMAN, J., G. LAKOFF, D. BAILEY, S. NARAYANAN, T. REGIER, & A. STOLCKE. </author> <year> 1994. </year> <title> L 0 the first four years. AI Review 8. Special issue on Integration of Natural Language and Vision Processing, </title> <note> to appear. </note>
Reference: <author> FELDMAN, JEROME A., GEORGE LAKOFF, ANDREAS STOLCKE, & SUSAN HOLLBACH WEBER. </author> <year> 1990. </year> <title> Miniature language acquisition: A touchstone for cognitive science. </title> <booktitle> In Proceedings of the 12th Annual Conference of the Cognitive Science Society, </booktitle> <pages> 686-693, </pages> <publisher> MIT, </publisher> <address> Cambridge, Mass. </address>
Reference: <author> FILLMORE, CHARLES J. </author> <year> 1988. </year> <title> The mechanisms of Construction Grammar. </title> <booktitle> In Proceedings of the Fourteenth Annual Meeting of the Berkeley Linguistics Society, </booktitle> <editor> ed. </editor> <booktitle> by Shelley Axmaker, Annie Jaisser, & Helen Singmeister, </booktitle> <pages> 35-55, </pages> <address> Berkeley, Ca. </address>
Reference-contexts: this thesis, in Chapter 6 and Chapter 7, respectively. 1 While bare CFGs aren't widely used in computational linguistics either, they form the basis or `backbone' of most of today's feature and unification-based grammar formalisms, such as LFG (Kaplan & Bresnan 1982), GPSG (Gazdar et al. 1985), and construction grammar <ref> (Fillmore 1988) </ref>. CHAPTER 4.
Reference: <author> FUJISAKI, T., F. JELINEK, J. COCKE, E. BLACK, & T. NISHINO. </author> <year> 1991. </year> <title> A probabilistic parsing method for sentence disambiguation. In Current Issues in Parsing Technology, ed. by Masaru Tomita, </title> <booktitle> chapter 10, </booktitle> <pages> 139-152. </pages> <address> Boston: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: A much more standard task is the use of a preexisting SCFG for various problems in computation linguistics applications requiring probabilistic processing. In the literature, SCFGs are used for the selection of parses for ambiguous inputs <ref> (Fujisaki et al. 1991) </ref>; to guide the rule choice efficiently during parsing (Jones & Eisner 1992a); to compute island probabilities for non-linear parsing (Corazza et al. 1991).
Reference: <author> GAROFOLO, J. S., </author> <year> 1988. </year> <title> Getting Started with the DARPA TIMIT CD-ROM: an Acoustic Phonetic Continuous Speech Database. </title> <institution> National Institute of Standards and Technology (NIST), Gaithersburgh, Maryland. </institution>
Reference-contexts: The TIMIT (Texas Instruments-MIT) database is a collection of hand-labeled speech samples compiled for the purpose of training speaker-independent phonetic recognition systems <ref> (Garofolo 1988) </ref>. It contains acoustic data segmented by words and aligned with discrete labels from an alphabet of 62 phones. For our purposes, we ignored the continuous, acoustic data and viewed the database simply as a collection of string samples over a discrete alphabet.
Reference: <author> GAUVAIN, JEAN-LUC, & CHIN-HIN LEE. </author> <year> 1991. </year> <title> Bayesian learning of Gaussian mixture densities for hidden Markov models. </title> <booktitle> In Proceedings DARPA Speech and Natural Language Processing Workshop, </booktitle> <pages> 271-277. </pages> <address> Pacific Grove, CA: </address> <institution> Defence Advanced Research Projects Agency, Information Science and Technology Office. </institution>
Reference-contexts: This could, and in fact should, change with the use of more informative priors. Likewise, we haven't pursued merging of HMMs with non-discrete outputs. For example, HMMs with mixtures of Gaussians as emission densities are being used extensively <ref> (Gauvain & Lee 1991) </ref> for speech modeling. Our merging algorithm becomes applicable to such models provided that one has a prior for such densities, which should be straightforward (Cheeseman et al. 1988).
Reference: <author> GAZDAR, GERALD, E. KLEIN, G. K. PULLUM, & I. A. SAG. </author> <year> 1985. </year> <title> Generalized Phrase Structure Grammar. </title> <address> Cambridge, Mass.: </address> <publisher> Harvard University Press. </publisher>
Reference-contexts: are described in the second part of this thesis, in Chapter 6 and Chapter 7, respectively. 1 While bare CFGs aren't widely used in computational linguistics either, they form the basis or `backbone' of most of today's feature and unification-based grammar formalisms, such as LFG (Kaplan & Bresnan 1982), GPSG <ref> (Gazdar et al. 1985) </ref>, and construction grammar (Fillmore 1988). CHAPTER 4.
Reference: <author> GEMAN, STUART, ELIE BIENENSTOCK, & REN E DOURSAT. </author> <year> 1992. </year> <title> Neural networks and the bias/variance dilemma. Neural Computation 4.1-58. BIBLIOGRAPHY 187 , & DONALD GEMAN. 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 6.721-741. </journal>
Reference-contexts: See Church & Gale (1991) for a survey and comparison of various methods for doing that. Another reason for introducing bias is to reduce the variance of an estimator <ref> (Geman et al. 1992) </ref>. 2.2.6 Likelihood and cross-entropy If the data X is given and fixed, we can view P (XjM ) as a function of M , the likelihood (function).
Reference: <author> GRAHAM, SUSAN L., MICHAEL A. HARRISON, & WALTER L. RUZZO. </author> <year> 1980. </year> <title> An improved context-free recognizer. </title> <journal> ACM Transactions on Programming Languages and Systems 2.415-462. </journal>
Reference-contexts: CHAPTER 6. EFFICIENT PARSING WITH STOCHASTIC CONTEXT-FREE GRAMMARS 158 6.6 Implementation Issues This section briefly discusses some of the experience gained from implementing the probabilistic Earley parser. Implementation is mainly straightforward and many of the standard techniques for context-free grammars can be used <ref> (Graham et al. 1980) </ref>. However, some aspects are unique due to the addition of probabilities. 6.6.1 Prediction Due to the collapsing of transitive predictions, this step can be implemented in a very efficient and straightforward manner. <p> In short, we can, without loss of generality, assume that the SCFGs in question is in CNF. The algorithm described here in fact generalizes to the more general Canonical Two-Form <ref> (Graham et al. 1980) </ref> format, and in the case of bigrams (n = 2) it can even be modified to work directly for arbitrary SCFGs.
Reference: <author> GULL, S. F. </author> <year> 1988. </year> <title> Bayesian inductive inference and maximum entropy. In Maximum Entropy and Bayesian Methods in Science and Engineering, Volume 1: Foundations, </title> <editor> ed. by G. J. Erickson & C. R. Smith, </editor> <address> 53-74. Dordrecht: </address> <publisher> Kluwer. </publisher>
Reference-contexts: However, as we will see below, the state-based priors by themselves produce a tendency towards reducing the number of states as a result of Bayesian `Occam factors' <ref> (Gull 1988) </ref>. In the case of narrow parameter priors we need to specify how the prior probability mass is distributed among all possible model topologies with a given number of states. <p> This result, of course, just confirms our intuition that a prophet whose predictions are specific (and true) is more credible than one whose predictions are more general. The ratio between the allowable range of a model's parameters a posterior and a priori is known as the Occam factor <ref> (Gull 1988) </ref>. In the discrete case these ranges are just the respective numbers of possible parameter settings: 1 3 versus 1 2 in the example.
Reference: <author> HAUSSLER, DAVID, ANDERS KROGH, I. SAIRA MIAN, & KIMMEN SJ OLANDER. </author> <year> 1992. </year> <title> Protein modeling using hidden Markov models: Analysis of globins. </title> <type> Technical Report UCSC-CRL-92-23, </type> <institution> Computer and Information Sciences, University of California, Santa Cruz, Ca. </institution> <note> Revised Sept. </note> <year> 1992. </year>
Reference: <author> HINTON, GEOFFREY E., & TERRENCE J. SEJNOWSKI. </author> <year> 1986. </year> <title> Learning and relearning in boltzmann machines. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition, ed. by David E. </title>
Reference: <editor> Rumelhart & James L. McClelland, </editor> <volume> volume 1: </volume> <booktitle> Foundations, </booktitle> <pages> 282-317. </pages> <address> Cambridge, Mass.: </address> <publisher> Bradford Books (MIT Press). </publisher>
Reference: <author> HJELMSLEV, LOUIS. </author> <year> 1953. </year> <title> Prolegomena to a theory of language. </title> <address> Baltimore: </address> <note> Waverly Press. Translated by Francis J. Whitfield from the Danish original Omkring sprogteoriens grundlaeggelse, Kopenhagen, </note> <year> 1943. </year>
Reference: <author> HOPCROFT, JOHN E., & JEFFREY D. ULLMAN. </author> <year> 1979. </year> <title> Introduction to Automata Theory, </title> <booktitle> Languages, and Computation. </booktitle> <address> Reading, Mass.: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: CHAPTER 2. FOUNDATIONS 15 I 1 2 0.5 0.5 State names appear within circles, outputs above their emitting states. 2.3.4 Hidden Markov Models Hidden Markov Models (HMMs) are the probabilistic counterpart of nondeterministic finite automata (NFAs) <ref> (Hopcroft & Ullman 1979) </ref>. As NFAs, HMMs have a finite number of states, including the initial and final ones. <p> CHAPTER 3. HIDDEN MARKOV MODELS 47 3.5.1 Non-probabilistic finite-state models At the most basic level we have the concept of state merging, which is implicit in the notion of state equivalence classes, and as such is pervasively used in much of automata theory <ref> (Hopcroft & Ullman 1979) </ref>. It has also been applied to the induction of non-probabilistic automata (Angluin & Smith 1983). <p> A CFG is in CNF if all productions are of the form X ! Y Z X ! a where X; Y; Z 2 N and a 2 . Any CFG structure can be converted into a weakly equivalent CNF grammar <ref> (Hopcroft & Ullman 1979) </ref>, and in the case of SCFGs the probabilities can be assigned such that the string probabilities remain unchanged. 3 Furthermore, parses in the original grammar can be reconstructed from corresponding CNF parses.
Reference: <author> HORNING, JAMES JAY. </author> <year> 1969. </year> <title> A study of grammatical inference. </title> <type> Technical Report CS 139, </type> <institution> Computer Science Department, Stanford University, Stanford, </institution> <address> Ca. </address>

Reference: <author> JONES, MARK A., & JASON M. EISNER. </author> <year> 1992a. </year> <title> A probabilistic parser and its applications. </title> <booktitle> In AAAI Workshop on Statistically-Based NLP Techniques, </booktitle> <pages> 20-27, </pages> <address> San Jose, CA. </address> <month> BIBLIOGRAPHY 188 , & . </month> <year> 1992b. </year> <title> A probabilistic parser applied to software testing documents. </title> <booktitle> In Proceedings of the 8th National Conference on Artificial Intelligence, </booktitle> <pages> 332-328, </pages> <address> San Jose, CA. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: A much more standard task is the use of a preexisting SCFG for various problems in computation linguistics applications requiring probabilistic processing. In the literature, SCFGs are used for the selection of parses for ambiguous inputs (Fujisaki et al. 1991); to guide the rule choice efficiently during parsing <ref> (Jones & Eisner 1992a) </ref>; to compute island probabilities for non-linear parsing (Corazza et al. 1991). In speech recognition, probabilistic context-free grammars play a central role in integrating low-level word models with higher-level language models (Ney 1992), as well as in non-finite state acoustic and phonotactic modeling (Lari & Young 1991).
Reference: <author> JURAFSKY, DANIEL, CHUCK WOOTERS, GARY TAJCHMAN, JONATHAN SEGAL, ANDREAS STOLCKE, ERIC FOSLER, & NELSON MORGAN. </author> <year> 1994a. </year> <title> The Berkeley Restaurant Project. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing, </booktitle> <address> Yokohama. </address> , <note> CHUCK WOOTERS, </note> <author> GARY TAJCHMAN, JONATHAN SEGAL, ANDREAS STOLCKE, & NELSON MORGAN. </author> <year> 1994b. </year> <title> Integrating experimental models of syntax, phonology and accent/dialect in a speech recognizer. </title> <booktitle> In AAAI Workshop on the Integration of Natural Language and Speech Processing, </booktitle> <editor> ed. by Paul McKevitt, </editor> <address> Seattle, WA. </address>
Reference-contexts: Historically, the algorithms presented here were developed to meet specific needs, either as part of the implementation of the learning algorithms described in the first part, or as a result of the tie-ins of the present work with an ongoing project in speech understanding. <ref> (Jurafsky et al. 1994a) </ref>. Chapter 6 describes a parser for SCFGs that solves several of the standard tasks for probabilistic grammars in a single elegant and efficient framework, namely a probabilistic generalization of Earley's algorithm for regular CFGs. <p> BeRP is medium vocabulary, speaker-independent spontaneous continuous speech understanding system that functions as a consultant for finding restaurants in the city of Berkeley, California <ref> (Jurafsky et al. 1994a) </ref>. In this application, the merging algorithm is run on strings of phone labels obtained by Viterbi-aligning previously existing word models to sample speech (using the TIMIT labels as the phone alphabet). <p> In a brief experiment, we applied our algorithm to a 1200 sentence corpus collected with the BeRP speech system <ref> (Jurafsky et al. 1994a) </ref>. The algorithm produced mostly plausible lexical categories and a large number of chunks corresponding to frequent phrases and collocations. However, the generalization achieved was nowhere near what would be required for a sufficient coverage of new data. <p> We essentially propose a solution for extending this approach to the probabilistic realm. The need for obtaining n-gram estimates from SCFGs originated in the BeRP speech understanding system already mentioned elsewhere in this thesis <ref> (Jurafsky et al. 1994a) </ref>. 2 This chapter describes an n-gram algorithm specifically for SCFGs. However, the methods described here are easily adapted to the simpler HMM case. CHAPTER 7. <p> A brief overview of these is given in the appendix to this chapter. 7.6 Experiments The algorithm described here has been implemented, and is being used to generate bigrams for a speech recognizer that is part of the BeRP spoken-language system <ref> (Jurafsky et al. 1994a) </ref>. The speech decoder and language model components of the BeRP system were used in an experiment to assess the benefit of using bigram probabilities obtained through SCFGs versus estimating them directly from the available training corpus.
Reference: <author> KAPLAN, RONALD M., & JOAN BRESNAN. </author> <year> 1982. </year> <title> Lexical functional grammar: A formal system for grammatical representation. In The Mental Representation of Grammatical Relations, </title> <editor> ed. </editor> <booktitle> by Joan Bresnan, </booktitle> <pages> 173-281. </pages> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: of these last two approaches are described in the second part of this thesis, in Chapter 6 and Chapter 7, respectively. 1 While bare CFGs aren't widely used in computational linguistics either, they form the basis or `backbone' of most of today's feature and unification-based grammar formalisms, such as LFG <ref> (Kaplan & Bresnan 1982) </ref>, GPSG (Gazdar et al. 1985), and construction grammar (Fillmore 1988). CHAPTER 4.
Reference: <author> KATZ, SLAVA M. </author> <year> 1987. </year> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing 35.400-401. </journal>
Reference-contexts: It is amazing how much overlap, apparently without mutual knowledge, there is between the text compression field and probabilistic computational linguistics. For example, the problem of smoothing zero-probability estimates and the solutions using mixtures (Bahl et al. 1983) or back-off models <ref> (Katz 1987) </ref> all have almost perfect analogs in the various strategies for building code spaces for compression models. 9 Bell et al. (1990) attribute the state merging idea to Evans (1971). CHAPTER 3. HIDDEN MARKOV MODELS 48 complexity and data fit. <p> Standard back-off models (where a second model is consulted if, and only if, the first one returns probability zero) do not yield consistent probabilities unless they are combined with `discounting' of probabilities to ensure that the total probability mass sums up to unity <ref> (Katz 1987) </ref>. The discounting scheme, as well as various smoothing approaches (e.g., adding a fixed number of virtual `Dirichlet' samples into parameter estimates) tend to be specific to the model used, and are therefore inherently problematic when comparing different model-building methods.
Reference: <author> KUPIEC, JULIAN. </author> <year> 1992a. </year> <title> Hidden Markov estimation for unrestricted stochastic context-free grammars. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> 177-180, </pages> <address> San Francisco. </address> . <year> 1992b. </year> <title> Robust part-of-speech tagging using a hidden Markov model. </title> <booktitle> Computer Speech and Language 6.225-242. </booktitle>
Reference-contexts: CHAPTER 6. EFFICIENT PARSING WITH STOCHASTIC CONTEXT-FREE GRAMMARS 165 Full CNF Sparse CFG Bottom-up Inside/outside Stochastic RTNs (Baker 1979) <ref> (Kupiec 1992a) </ref> Left-to-right LRI Probabilistic (Jelinek & Lafferty 1991) Earley Table 6.4: Tentative typology of SCFG algorithms according to prevailing directionality and sparseness of the CFG. zero.
Reference: <author> LANGLEY, PAT, </author> <year> 1994. </year> <title> Simplicity and representation change in grammar induction. </title> <note> Unpublished mss. </note>
Reference: <author> LARI, K., & S. J. YOUNG. </author> <year> 1990. </year> <title> The estimation of stochastic context-free grammars using the Inside-Outside algorithm. </title> <booktitle> Computer Speech and Language 4.35-56. </booktitle> , & . <year> 1991. </year> <title> Applications of stochastic context-free grammars using the Inside-Outside algorithm. </title> <booktitle> Computer Speech and Language 5.237-257. </booktitle>
Reference-contexts: Unfortunately, it seems that in the case of unconstrained SCFG estimation local maxima present a very real problem, and make success dependent on chance and initial conditions <ref> (Lari & Young 1990) </ref>. Pereira & Schabes (1992) showed that partially bracketed input samples can alleviate the problem in certain cases. The bracketing information constrains the parse of the inputs, and therefore the parameter estimates, steering it clear from some of the suboptimal solutions that could otherwise be found.
Reference: <author> LEE, H. C., & K. S. FU. </author> <year> 1972. </year> <title> Stochastic linguistics for picture recognition. </title> <type> Technical Report TR-EE 72-17, </type> <institution> School of Electical Engineering, Purdue University. </institution>
Reference: <author> MAGERMAN, DAVID M., & MITCHELL P. MARCUS. </author> <year> 1991. </year> <title> Pearl: A probabilistic chart parser. </title> <booktitle> In Proceedings of the 6th Conference of the European Chapter of the Association for Computational Linguistics, </booktitle> <address> Berlin, Germany. , & CARL WEIR. </address> <year> 1992. </year> <title> Efficiency, robustness and accuracy in Picky chart parsing. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 40-47, </pages> <institution> University of Delaware, Newark, Delaware. </institution> <note> BIBLIOGRAPHY 189 MITCHELL, </note> <author> TOM M. </author> <year> 1980. </year> <title> The need for biases in learning generalizations. </title> <type> Technical Report CBM-TR-117, </type> <institution> Computer Science Department, Rutgers University, </institution> <address> New Brunswick, N.J. </address> <year> 1982. </year> <title> Generalization as search. </title> <journal> Artificial Intelligence 18.203-226. </journal>
Reference: <author> MORGAN, JAMES L., RICHARD P. MEIER, & ELISSA L. NEWPORT. </author> <year> 1987. </year> <title> Structural packaging in the input to language learning: Contributions of prosodic and morphological marking of phrases to the acquisition of language. </title> <journal> Cognitive Psychology 19.498-550. </journal>
Reference-contexts: This is hardly surprising, given that a lot of the cues for human judgments of linguistic structure presumable come from other sources, such as the semantic referents of the syntactic elements, phonological cues, morphological markers, etc. <ref> (Morgan et al. 1987) </ref>. In a brief experiment, we applied our algorithm to a 1200 sentence corpus collected with the BeRP speech system (Jurafsky et al. 1994a). The algorithm produced mostly plausible lexical categories and a large number of chunks corresponding to frequent phrases and collocations.
Reference: <author> NAKAGAWA, SEI-ICHI. </author> <year> 1987. </year> <title> Spoken sentence recognition by time-synchronous parsing algorithm of context-free grammar. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> 829-832, </pages> <address> Dallas, Texas. </address>
Reference-contexts: In some work, context-free grammars are combined with scoring functions that are not strictly probabilistic <ref> (Nakagawa 1987) </ref>, or they are used with context-sensitive and/or semantic probabilities (Magerman & Marcus 1991; Magerman & Weir 1992; Jones & Eisner 1992a; Briscoe & Carroll 1993).
Reference: <author> NEAL, RADFORD M. </author> <year> 1993. </year> <title> Probabilistic inference using Markov chain Monte Carlo methods. </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Department of Computer Science, University of Toronto. </institution>
Reference-contexts: However, it carries a heavy computational price: Simply obtaining the probability of a given string/feature assignment pair for various alternative grammars generally requires stochastic simulations in order to compute the constant Z. The posterior probabilities of models can also be evaluated using Monte-Carlo techniques <ref> (Neal 1993) </ref>, but the approach seems too inefficient for the generate-and-test search strategies we have explored so far.
Reference: <author> NEY, HERMANN. </author> <year> 1984. </year> <title> The use of a one-stage dynamic programming algorithm for connected word recognition. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing 32.263-271. </journal> . <year> 1992. </year> <title> Stochastic grammars and pattern recognition. In Speech Recognition and Understanding. Recent Advances, Trends, and Applications, ed. by Pietro Laface & Renato De Mori, </title> <booktitle> volume F75 of NATO Advanced Sciences Institutes Series, </booktitle> <pages> 319-344. </pages> <address> Berlin: Springer Verlag. </address> <booktitle> Proceedings of the NATO Advanced Study Institute, </booktitle> <address> Cetraro, Italy, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: CHAPTER 2. FOUNDATIONS 9 The number of parameters in n-gram models grows exponentially with n, and only the cases n = 2 (bigram models) and n = 3 (trigram models) are of practical importance. Bigram and trigram models are popular for various applications, especially speech decoding <ref> (Ney 1984) </ref>, to approximate the true distributions of language elements (characters, words, etc.), which are known to violate the independence assumption embodied in (2.1). <p> An experiment illustrating this approach is reported below. On the other hand, even if more sophisticated language models give better results, n-grams will most likely still be important in applications such as speech recognition. The standard speech decoding technique of frame-synchronous dynamic programming <ref> (Ney 1984) </ref> is based on a first-order Markov assumption, which is satisfied by bigrams models (as well as by Hidden Markov Models), but not by more complex models incorporating non-local or higher-order constraints (including SCFGs).
Reference: <author> OMOHUNDRO, STEPHEN M. </author> <year> 1992. </year> <title> Best-first model merging for dynamic learning and recognition. </title> <type> Technical Report TR-92-004, </type> <institution> International Computer Science Institute, Berkeley, Ca. P ASELER, ANNEDORE. </institution> <year> 1988. </year> <title> Modification of Earley's algorithm for speech recognition. In Recent Advances in Speech Understanding and Dialog Systems, </title> <editor> ed. by H. Niemann, M. Lang, & G. Sagerer, </editor> <booktitle> volume F46 of NATO Advanced Sciences Institutes Series, </booktitle> <pages> 466-472. </pages> <address> Berlin: Springer Verlag. </address> <booktitle> Proceedings of the NATO Advanced Study Institute, </booktitle> <address> Bad Windsheim, Germany, </address> <month> July </month> <year> 1987. </year>
Reference: <author> PEARL, JUDEA. </author> <year> 1988. </year> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo, Ca.: </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: However, the 6 Therefore the strict bottom-up feature format could be relaxed, e.g., by using the notion of L-attributed feature specifications (Aho et al. 1986). CHAPTER 5. PROBABILISTIC ATTRIBUTE GRAMMARS 119 theory of Markov networks <ref> (Pearl 1988) </ref> tells us that such a marginal probabilities cannot be assigned locally in a consistent fashion.
Reference: <author> PEREIRA, FERNANDO, & YVES SCHABES. </author> <year> 1992. </year> <title> Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> 128-135, </pages> <institution> University of Delaware, Newark, Delaware. </institution>
Reference-contexts: As expected, the problem is observed increasingly in `deep' grammars with many intermediate of features, and hence hidden variables as part of a derivation. Thus, the problems with the parameter estimation approach to structural learning that were previously observed with HMMs (Section 3.6.1) and SCFGs <ref> (Pereira & Schabes 1992) </ref> seem to show up even more severely in the PAG formalism. This gives further motivation to our investigation of merging-based learning approaches. 5.3 PAG Merging The following sections describe the usual ingredients necessary to specify a merging-based learning algorithm for the PAG formalism.
Reference: <author> PORAT, SARA, & JEROME A. FELDMAN. </author> <year> 1991. </year> <title> Learning automata from ordered examples. </title> <booktitle> Machine Learning 7.109-138. </booktitle>
Reference-contexts: Thus the analyses of the previous samples can effectively guide the search for merges based on the longer, more recent ones. An interesting related result from non-probabilistic language induction is that strictly (lexicographically) ordered sample presentation makes the learning problem for certain classes of grammar provably easier <ref> (Porat & Feldman 1991) </ref>. 4.5.4 Summary and Discussion The above artificial examples show that SCFG learning based purely on distributional evidence and the generic Bayesian simplicity bias can correctly identify several of the common linguistic structures.
Reference: <author> PRESS, WILLIAM H., BRIAN P. FLANNERY, SAUL A. TEUKOLSKY, & WILLIAM T. VETTERLING. </author> <year> 1988. </year> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <address> Cambridge: </address> <publisher> Cambridge University Press. BIBLIOGRAPHY 190 QUINLAN, </publisher> <editor> J. ROSS, & RONALD L. RIVEST. </editor> <year> 1989. </year> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation 80.227-248. </journal>
Reference: <author> RABINER, L. R., & B. H. JUANG. </author> <year> 1986. </year> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine 3.4-16. </journal>
Reference-contexts: Some of their first uses were in the area of cryptanalysis and they are now the model of choice for speech recognition <ref> (Rabiner & Juang 1986) </ref>. Recent applications include part-of-speech tagging (Kupiec 1992b) and protein classification and alignment (Haussler et al. 1992; Baldi et al. 1993). Because HMMs can be seen as probabilistic generalizations of non-deterministic finite-state automata they are also of interest from the point of view of formal language induction. <p> For each sample sequence x we compute the posterior probability P (q t jx; M ) that the path generating x passes through state q at time t. This can be done by an efficient O (`jQj 2 ) dynamic programming algorithm know as the forward-backward algorithm (see, e.g., <ref> (Rabiner & Juang 1986) </ref>). <p> This can be accomplished by attaching two probabilistic quantities to each Earley state, as follows. The terminology is derived from analogous or similar quantities commonly used in the literature on Hidden Markov Models (HMMs) <ref> (Rabiner & Juang 1986) </ref> and in Baker (1979). <p> Both the definition of Viterbi parse, and its computation are straightforward generalizations of the corresponding notion for Hidden Markov Models <ref> (Rabiner & Juang 1986) </ref>, where one computes the Viterbi CHAPTER 6. EFFICIENT PARSING WITH STOCHASTIC CONTEXT-FREE GRAMMARS 148 path (state sequence) through an HMM. Precisely the same approach can be used in the Earley parser, using the fact that each derivation corresponds to a path. <p> than computing the zeroth state set filtered by the first input. 6.7 Discussion 6.7.1 Relation to finite-state models Throughout the exposition of the Earley algorithm and its probabilistic extension we have been alluding, in concepts and terminology, to the algorithms used with probabilistic finite-state models, in particular Hidden Markov Models <ref> (Rabiner & Juang 1986) </ref>. Many concepts carry over, if suitably generalized, most notably that of forward probabilities.
Reference: <author> REBER, A. S. </author> <year> 1969. </year> <title> Implicit learning of artifical grammars. </title> <journal> Journal of Verbal Learning and Verbal Behavior 6.855-863. </journal>
Reference: <author> REDNER, RICHARD A., & HOMER F. WALKER. </author> <year> 1984. </year> <title> Mixture densities, maximum likelihood and the EM algorithm. </title> <journal> SIAM Review 26.195-239. </journal>
Reference-contexts: For example, for the multinomial: E log P (x; dj) = E i X (Ec i ) log i : CHAPTER 2. FOUNDATIONS 14 EM for mixtures As a brief illustration of the EM method we consider the case of the simple mixture model (2.11) <ref> (Redner & Walker 1984) </ref>. In the E-step we compute the expected values for the counts c i , the number of times submodel i was used to generate a string.
Reference: <author> REGIER, TERRY, </author> <year> 1992. </year> <title> The acquisition of lexical semantics for spatial terms: A connectionist model of perceptual categorization. </title> <institution> Berkeley, Ca.: Computer Science Division, University of California dissertation. </institution>
Reference: <author> RESNIK, PHILIP. </author> <year> 1992. </year> <title> Probabilistic tree-adjoining grammar as a framework for statistical natural language processing. </title> <booktitle> In Proceedings of the 14th International Conference on Computational Linguistics, </booktitle> <pages> 418-424, </pages> <address> Nantes, France. </address>
Reference: <author> RICHARDS, I. A., & C. GIBSON. </author> <year> 1945. </year> <title> English Through Pictures. </title> <address> New York: </address> <publisher> Washington Square Press. </publisher>
Reference-contexts: needed to obtain the result, either best-first (BF), or beam search with width n (BS (n)). `Shape grammar' refers to a CFG used by Lee & Fu (1972) to model two-dimensional chromosome pictures (with samples from that paper). `Basic English' are the first 25 sentences from a language teaching textbook <ref> (Richards & Gibson 1945) </ref>. The priors used were the `standard' ones: a symmetrical Dirichlet distribution over each set of rule probabilities with ff 0 = 1:0, and the simple description length prior on the grammar structure. No extra global weighting on the prior was applied ( = 1:0).
Reference: <author> RILEY, MICHAEL D. </author> <year> 1991. </year> <title> A statistical model for generating pronunciation networks. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 2, </volume> <pages> 737-740, </pages> <address> Toronto. </address>
Reference: <author> RISSANEN, JORMA. </author> <year> 1983. </year> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> The Annals of Statistics 11.416-431. </journal>
Reference: <author> RON, DANA, YORAM SINGER, & NAFTALI TISHBY. </author> <year> 1994. </year> <title> The power of amnesia. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <editor> ed. by Jack Cowan, Gerald Tesauro, & Joshua Alspector. </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> SAKAKIBARA, YASUBUMI. </author> <year> 1990. </year> <title> Learning context-free grammars from structural data in polynomial time. </title> <note> Theoretical Computer Science 76.223-242. </note> , <author> MICHAEL BROWN, RICHARD HUGHEY, I. SAIRA MIAN, KIMMEN SJ OLANDER, & REBECCA C. UN-DERWOOD DAVID HAUSSLER. </author> <year> 1994. </year> <title> The application of stochastic context-free grammars to folding, aligning and modeling homologous RNA sequences. </title> <type> Technical Report UCSC-CRL-94-14, </type> <institution> Computer and Information Sciences, University of California, </institution> <address> Santa Cruz, Ca. </address>
Reference-contexts: For example, a partially bracketed sample for the example language of Section 4.3.2 is (a a (a b) b b) It is known that access to completely bracketed samples (equivalent to unlabeled derivation trees) makes learning non-probabilistic CFGs possible and tractable, by applying techniques borrowed from finite-state model induction <ref> (Sakakibara 1990) </ref>. Pereira & Schabes (1992) have shown that providing even partial bracketing information can help the induction of properly structured SCFGs using the standard estimation approach. This raises the question how bracketed samples can be incorporated into the merging algorithm described so far.
Reference: <author> SCHABES, YVES, </author> <year> 1991. </year> <title> An inside-outside algorithm for estimating the parameters of a hidden stochastic context-free grammar based on Earley's algorithm. Unpublished mss. </title> <booktitle> Presented at the Second Workshop on Mathematics of Language, </booktitle> <address> Tarritown, N.Y., </address> <month> May </month> <year> 1991. </year> <month> . </month> <year> 1992. </year> <title> Stochastic lexicalized tree-adjoining grammars. </title> <booktitle> In Proceedings of the 14th International Conference on Computational Linguistics, </booktitle> <pages> 426-433, </pages> <address> Nantes, France. </address> <note> BIBLIOGRAPHY 191 SCHWARTZ, </note> <author> RICHARD, & YEN-LU CHOW. </author> <year> 1990. </year> <title> The N -best algorithm: An efficient and exact procedure for finding the n most likely sentence hypotheses. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> 81-84, </pages> <address> Albuquerque, NM. </address>
Reference: <author> SHIEBER, STUART M. </author> <year> 1986. </year> <title> An Introduction to Unification-Based Approaches to Grammar. Number 4 in CSLI Lecture Note Series. Stanford, Ca.: Center for the Study of Language and Information. </title>
Reference-contexts: 1 Many linguistic formalisms based on context-free grammars employ a related concept, typically known by the name of `features.' Like attributes, these are attachments to the nonterminals in the grammar, but their values and their specifications are usually more restricted, in the form of directed acyclic graphs and unification equations <ref> (Shieber 1986) </ref>. The attribute grammar model used here is a much simplified variant of either of these two traditional CFG extensions, with a probabilistic component added to yield the usual notions of sample probability, likelihood, etc. <p> CHAPTER 5. PROBABILISTIC ATTRIBUTE GRAMMARS 120 cannot be adequately described, let alone learned. 8 The obvious solution to this representational problem are hierarchical feature structures (f-structures) as used by a number of linguistic theories of grammar <ref> (Shieber 1986) </ref>. However, this raises new problems concerning the `proper' probabilistic formulation. For example, the set of hierarchical feature specifications over a finite set of feature names becomes infinite, raising the question of an appropriate prior distribution over this space.

Reference: <author> THOMASON, MICHAEL G., & ERIK GRANUM. </author> <year> 1986. </year> <title> Dynamic programming inference of Markov networks from finite set of sample strings. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 8.491-501. </journal>
Reference: <author> TOMITA, MASARU. </author> <year> 1982. </year> <title> Dynamic construction of finite automata from examples using hill-climbing. </title> <booktitle> In Proceedings of the 4th Annual Conference of the Cognitive Science Society, </booktitle> <pages> 105-108, </pages> <address> Ann Arbor, Mich. </address> . <year> 1986. </year> <title> Efficient Parsing for Natural Language. </title> <address> Boston: </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference: <author> UNDERWOOD, REBECCA CHRISTINE. </author> <year> 1994. </year> <title> Stochastic context-free grammars for modeling three spliceosomal small nuclear ribonucleic acids. </title> <type> Technical Report CRL-94-23, </type> <institution> Computer and Information Sciences, University of California, </institution> <address> Santa Cruz, Ca. </address>
Reference: <author> VITERBI, A. </author> <year> 1967. </year> <title> Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. </title> <journal> IEEE Transactions on Information Theory 13.260-269. </journal> <note> BIBLIOGRAPHY 192 WALLACE, </note> <author> C. S., & P. R. FREEMAN. </author> <year> 1987. </year> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society, Series B 49.240-265. </journal>
Reference-contexts: choosing M i . 2.3.3 Viterbi derivations and approximate EM Often one is interested in the single most likely derivation for a string x within a model M , argmax d This is called the Viterbi derivation of x, and is some sense the `best' explanation of x within M <ref> (Viterbi 1967) </ref>. Finding the Viterbi derivation is the canonical way to disambiguate a string x using probabilistic grammar M .
Reference: <author> WOLFF, J. G. </author> <year> 1978. </year> <title> Grammar discovery as data compression. </title> <booktitle> In Proceedings of the AISB/GI Conference on Artificial Intelligence, </booktitle> <pages> 375-379, </pages> <address> Hamburg. </address>
Reference: <author> WOOTERS, CHARLES C., </author> <year> 1993. </year> <title> Lexical Modeling in a Speaker Independent Speech Understanding System. </title> <institution> Berkeley, CA: University of California dissertation. </institution>
Reference-contexts: However, the preliminary results do show that HMM merging is CHAPTER 3. HIDDEN MARKOV MODELS 73 <ref> (Wooters 1993) </ref>. CHAPTER 3. HIDDEN MARKOV MODELS 74 both practical and effective when embedded in a realistic speech system.
Reference: <author> WOOTERS, CHUCK, & ANDREAS STOLCKE. </author> <year> 1994. </year> <title> Multiple-pronunciation lexical modeling in a speaker-independent speech understanding system. </title> <booktitle> In Proceedings International Conference on Spoken Language Processing, </booktitle> <address> Yokohama. </address>
Reference: <author> WRIGHT, J. H. </author> <year> 1990. </year> <title> LR parsing of probabilistic grammars with input uncertainty for speech recognition. </title> <booktitle> Computer Speech and Language 4.297-323. </booktitle>
Reference-contexts: Generalized LR parsing is an extension that allows parallel tracking of multiple state transitions and stack actions by using a graph-structured stack (Tomita 1986). Probabilistic LR parsing <ref> (Wright 1990) </ref> is based on LR items augmented with certain conditional probabilities.
Reference: <author> ZUE, VICTOR, JAMES GLASS, DAVID GOODINE, HONG LEUNG, MICHAEL PHILLIPS, JOSEPH POLIFRONI, & STEPHANIE SENEFF. </author> <year> 1991. </year> <title> Integration of speech recognition and natural language processing in the MIT Voyager system. </title> <booktitle> In Proceedings IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> 713-716, </pages> <address> Toronto. </address>
References-found: 83

