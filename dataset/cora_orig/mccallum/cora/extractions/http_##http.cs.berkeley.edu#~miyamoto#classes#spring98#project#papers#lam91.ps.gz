URL: http://http.cs.berkeley.edu/~miyamoto/classes/spring98/project/papers/lam91.ps.gz
Refering-URL: http://http.cs.berkeley.edu/~miyamoto/classes/spring98/project/index.html
Root-URL: http://www.cs.berkeley.edu
Title: The Cache Performance and Optimizations of Blocked Algorithms  
Author: Monica S. Lam, Edward E. Rothberg and Michael E. Wolf 
Affiliation: Computer Systems Laboratory Stanford University,  
Date: April 9-11, 1991  
Address: Palo Alto, California,  CA 94305  
Note: Fourth Intern. Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV)  
Abstract: Blocking is a well-known optimization technique for improving the effectiveness of memory hierarchies. Instead of operating on entire rows or columns of an array, blocked algorithms operate on submatrices or blocks, so that data loaded into the faster levels of the memory hierarchy are reused. This paper presents cache performance data for blocked programs and evaluates several optimizations to improve this performance. The data is obtained by a theoretical model of data conflicts in the cache, which has been validated by large amounts of simulation. We show that the degree of cache interference is highly sensitive to the stride of data accesses and the size of the blocks, and can cause wide variations in machine performance for different matrix sizes. The conventional wisdom of trying to use the entire cache, or even a fixed fraction of the cache, is incorrect. If a fixed block size is used for a given cache size, the block size that minimizes the expected number of cache misses is very small. Tailoring the block size according to the matrix size and cache parameters can improve the average performance and reduce the variance in performance for different matrix sizes. Finally, whenever possible, it is beneficial to copy non-contiguous reused data into consecutive locations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Abu-Sufah, D. J. Kuck, and D. H. Lawrie. </author> <title> Automatic program transformations for virtual memory computers. </title> <booktitle> Proc. of the 1979 National Computer Conference, </booktitle> <pages> pages 969-974, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: The high ratio of memory fetches to numerical operations can significantly slow down the machine. (a) N = X k Z X Y k jj B matrix multiplication. It is well known that the memory hierarchy can be better utilized if scientific algorithms are blocked <ref> [1, 5, 6, 8, 10, 11, 12] </ref>. 1 Blocking is also known as tiling. Instead of operating on indi-vidual matrix entries, the calculation is performed on submatrices. Blocking can be applied to any and multiple levels of memory hierarchy, including virtual memory, caches, vector registers, and scalar registers.
Reference: [2] <author> E. Anderson and J. Dongarra. </author> <title> LAPACK working note 18, implementation guide for LAPACK. </title> <type> Technical Report CS-90-101, </type> <institution> University of Tennessee, </institution> <month> Apr </month> <year> 1990. </year>
Reference-contexts: Blocking has been shown to be useful for many algorithms in linear algebra. For example, the latest version of the basic linear algebra library (BLAS 3) [4] provides high-level matrix operations to support blocked algorithms. LAPACK <ref> [2] </ref>, a successor to LINPACK, is an example of a package built on top of the BLAS 3 library. Previous research on blocking focused on how to block an algorithm manually and automatically [5, 7, 11, 12]. The procedure consists of two steps [12].
Reference: [3] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the ACM SIGPLAN '90 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: The graph plots the performance levels obtained for three slightly different matrix sizes across a range of blocking factors. We use two different codes; one blocks for both the cache and registers <ref> [3] </ref>, while the other blocks only for the cache. While the performance curves for the 300 fi 300 matrix multiplication are well behaved, those for the other two drop sharply starting at different blocking factors depending on the matrix size.
Reference: [4] <author> J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <pages> pages 1-17, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: This reduction is especially important for multiprocessors since memory bandwidth is often the bottleneck of the system. Blocking has been shown to be useful for many algorithms in linear algebra. For example, the latest version of the basic linear algebra library (BLAS 3) <ref> [4] </ref> provides high-level matrix operations to support blocked algorithms. LAPACK [2], a successor to LINPACK, is an example of a package built on top of the BLAS 3 library. Previous research on blocking focused on how to block an algorithm manually and automatically [5, 7, 11, 12].
Reference: [5] <author> K. Gallivan, W. Jalby, U. Meier, and A. Sameh. </author> <title> The impact of hierarchical memory systems on linear algebra algorithm design. </title> <type> Technical Report UIUCSRD 625, </type> <institution> University of Illinios, </institution> <year> 1987. </year>
Reference-contexts: The high ratio of memory fetches to numerical operations can significantly slow down the machine. (a) N = X k Z X Y k jj B matrix multiplication. It is well known that the memory hierarchy can be better utilized if scientific algorithms are blocked <ref> [1, 5, 6, 8, 10, 11, 12] </ref>. 1 Blocking is also known as tiling. Instead of operating on indi-vidual matrix entries, the calculation is performed on submatrices. Blocking can be applied to any and multiple levels of memory hierarchy, including virtual memory, caches, vector registers, and scalar registers. <p> LAPACK [2], a successor to LINPACK, is an example of a package built on top of the BLAS 3 library. Previous research on blocking focused on how to block an algorithm manually and automatically <ref> [5, 7, 11, 12] </ref>. The procedure consists of two steps [12]. The first is to restructure the code to enable blocking those loops that carry reuse, and the second is to choose the blocking factor that maximizes locality.
Reference: [6] <author> D. Gannon and W. Jalby. </author> <title> The influence of memory hierarchy on algorithm organization: Programming FFTs on a vector multiprocessor. In The Characteristics of Parallel Algorithms. </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: The high ratio of memory fetches to numerical operations can significantly slow down the machine. (a) N = X k Z X Y k jj B matrix multiplication. It is well known that the memory hierarchy can be better utilized if scientific algorithms are blocked <ref> [1, 5, 6, 8, 10, 11, 12] </ref>. 1 Blocking is also known as tiling. Instead of operating on indi-vidual matrix entries, the calculation is performed on submatrices. Blocking can be applied to any and multiple levels of memory hierarchy, including virtual memory, caches, vector registers, and scalar registers. <p> We now investigate a totally different approach that eliminates self interference altogether, thus guaranteeing a high cache utilization for all problem sizes. The approach is to copy non-contiguous data to be reused into a contiguous area <ref> [6] </ref>. By doing so, each word within the block is mapped to its own cache location, thus making self interference within a block impossible. This technique, when applicable, can bound the cache misses to a factor of two of the ideal.
Reference: [7] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 587-616, </pages> <year> 1988. </year>
Reference-contexts: LAPACK [2], a successor to LINPACK, is an example of a package built on top of the BLAS 3 library. Previous research on blocking focused on how to block an algorithm manually and automatically <ref> [5, 7, 11, 12] </ref>. The procedure consists of two steps [12]. The first is to restructure the code to enable blocking those loops that carry reuse, and the second is to choose the blocking factor that maximizes locality.
Reference: [8] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: The high ratio of memory fetches to numerical operations can significantly slow down the machine. (a) N = X k Z X Y k jj B matrix multiplication. It is well known that the memory hierarchy can be better utilized if scientific algorithms are blocked <ref> [1, 5, 6, 8, 10, 11, 12] </ref>. 1 Blocking is also known as tiling. Instead of operating on indi-vidual matrix entries, the calculation is performed on submatrices. Blocking can be applied to any and multiple levels of memory hierarchy, including virtual memory, caches, vector registers, and scalar registers.
Reference: [9] <author> J.-W. Hong and H. T. Kung. </author> <title> I/O complexity: The red-blue pebble game. </title> <booktitle> In Proceedings of the Thirteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 326-333. </pages> <institution> ACM SIGACT, </institution> <month> May </month> <year> 1981. </year>
Reference-contexts: For example, the optimal blocking factor is roughly p C for matrix multiplication on a machine with a local memory of C words <ref> [9] </ref>. This is appropriate for registers or local memories, where the data placement is fully controlled.
Reference: [10] <author> A. C. McKeller and E. G. Coffman. </author> <title> The organization of matrices and matrix operations in a paged multiprogramming environment. </title> <journal> CACM, </journal> <volume> 12(3) </volume> <pages> 153-165, </pages> <year> 1969. </year>
Reference-contexts: The high ratio of memory fetches to numerical operations can significantly slow down the machine. (a) N = X k Z X Y k jj B matrix multiplication. It is well known that the memory hierarchy can be better utilized if scientific algorithms are blocked <ref> [1, 5, 6, 8, 10, 11, 12] </ref>. 1 Blocking is also known as tiling. Instead of operating on indi-vidual matrix entries, the calculation is performed on submatrices. Blocking can be applied to any and multiple levels of memory hierarchy, including virtual memory, caches, vector registers, and scalar registers.
Reference: [11] <author> A. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: The high ratio of memory fetches to numerical operations can significantly slow down the machine. (a) N = X k Z X Y k jj B matrix multiplication. It is well known that the memory hierarchy can be better utilized if scientific algorithms are blocked <ref> [1, 5, 6, 8, 10, 11, 12] </ref>. 1 Blocking is also known as tiling. Instead of operating on indi-vidual matrix entries, the calculation is performed on submatrices. Blocking can be applied to any and multiple levels of memory hierarchy, including virtual memory, caches, vector registers, and scalar registers. <p> LAPACK [2], a successor to LINPACK, is an example of a package built on top of the BLAS 3 library. Previous research on blocking focused on how to block an algorithm manually and automatically <ref> [5, 7, 11, 12] </ref>. The procedure consists of two steps [12]. The first is to restructure the code to enable blocking those loops that carry reuse, and the second is to choose the blocking factor that maximizes locality.
Reference: [12] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <note> Submitted for publication., 1990. 12 </note>
Reference-contexts: The high ratio of memory fetches to numerical operations can significantly slow down the machine. (a) N = X k Z X Y k jj B matrix multiplication. It is well known that the memory hierarchy can be better utilized if scientific algorithms are blocked <ref> [1, 5, 6, 8, 10, 11, 12] </ref>. 1 Blocking is also known as tiling. Instead of operating on indi-vidual matrix entries, the calculation is performed on submatrices. Blocking can be applied to any and multiple levels of memory hierarchy, including virtual memory, caches, vector registers, and scalar registers. <p> LAPACK [2], a successor to LINPACK, is an example of a package built on top of the BLAS 3 library. Previous research on blocking focused on how to block an algorithm manually and automatically <ref> [5, 7, 11, 12] </ref>. The procedure consists of two steps [12]. The first is to restructure the code to enable blocking those loops that carry reuse, and the second is to choose the blocking factor that maximizes locality. <p> LAPACK [2], a successor to LINPACK, is an example of a package built on top of the BLAS 3 library. Previous research on blocking focused on how to block an algorithm manually and automatically [5, 7, 11, 12]. The procedure consists of two steps <ref> [12] </ref>. The first is to restructure the code to enable blocking those loops that carry reuse, and the second is to choose the blocking factor that maximizes locality. It is the latter step that is sensitive to the characteristics of the level of memory hierarchy in question. <p> It is inadequate to simulate a sample of data points and infeasible to simulate all possibilities. Our methodology is to combine theory and experimentation together in understanding the behavior of the cache. Drawing insights from the experimental data and the theory of data locality from our compiler research <ref> [12] </ref>, we have developed a model of data conflicts that has been validated by simulating several representative data points.
References-found: 12

