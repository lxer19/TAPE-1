URL: http://www.cs.ucsd.edu/~rik/papers/alife94/alife94.ps
Refering-URL: http://www.cs.ucsd.edu/~rik/bibliography3_5.html
Root-URL: http://www.cs.ucsd.edu
Email: Email:-fil, rik-@cs.ucsd.edu  
Phone: Phone: (619)534-8187, Fax: (619)534-7029  
Title: EVOLVING SENSORS IN ENVIRONMENTS OF CONTROLLED COMPLEXITY  
Author: F. Menczer and R. K. Belew 
Address: La Jolla, CA 92093-0114 USA  
Affiliation: Cognitive Computer Science Research Group Computer Science Engineering Department University of California, San Diego  
Abstract: 1 . Sensors represent a crucial link between the evolutionary forces shaping a species' relationship with its environment, and the individual's cognitive abilities to behave and learn. We report on experiments using a new class of "latent energy environments" (LEE) models to define environments of carefully controlled complexity which allow us to state bounds for random and optimal behaviors that are independent of strategies for achieving the behaviors. Using LEE's analytic basis for defining environments, we then use neural networks (NNets) to model individuals and a steady - state genetic algorithm to model an evolutionary process shaping the NNets, in particular their sensors. Our experiments consider two types of "contact" and "ambient" sensors, and variants where the NNets are not allowed to learn, learn via error correction from internal prediction, and via reinforcement learning. We find that predictive learning, even when using a larger repertoire of the more sophisticated ambient sensors, provides no advantage over NNets unable to learn. However, reinforcement learning using a small number of crude contact sensors does provide a significant advantage. Our analysis of these results points to a tradeoff between the genetic "robustness" of sensors and their informativeness to a learning system. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackley, D. and M. </author> <title> Littman (1990). Generalization and Scaling in Reinforcement Learning. </title> <booktitle> Advances in Neural Information Processing Systes (NIPS2), </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ackley, D. and M. </author> <title> Littman (1992). Interactions between learning and evolution. </title> <booktitle> Artificial Life II Eds. </booktitle>
Reference-contexts: example, should we be more impressed by the abilities of "amoebas" that successfully evolve to forage for foods (as in (Nolfi, Elman et al. 1990; Parisi, Cecconi et al. 1990)), by "ants" capable of following "trails" (as in (Collins and Jefferson 1992)), or by "agents" that avoid predators (as in <ref> (Ackley and Littman 1992) </ref>)? It would be very desirable to be able to define artificial environments of controlled complexity, within which a wide range of ALife techniques might be directly compared. <p> Evolution itself can be viewed as a form of reinforcement search algorithm, but the reinforcement signal (death or reproduction) is heavily delayed and of relatively little use during life <ref> (Ackley and Littman 1992) </ref>. If however a useful reinforcement signal is made available by an organism's interaction with its environment, then it can be used to determine an error information directly from the association between inputs and actions.
Reference: <editor> C. Langton, C. Taylor, J. Farmer and S. Rasmussen. </editor> <address> Reading, MA, </address> <publisher> Addison Wesley. </publisher> <pages> 487-507. </pages>
Reference: <author> Barto, A. and P. </author> <month> Anandan </month> <year> (1985). </year> <title> Pattern Recognizing Stochastic Learning Automata. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 15: </journal> <pages> 360-375. </pages>
Reference-contexts: Notice that this setting avoids the problem of delayed reinforcement altogether. Therefore we use reinforcement to modify weights only immediately after actions catalyzing reactions. The implemented algorithm is a simplified version of associative reward-penalty <ref> (Barto and Anandan 1985) </ref>. When a change in energy |D E|&gt;e occurs, a two-valued reinforcement signal r = sgn (DE) (12) is generated (this does not violate our assumption that there be no external teacher, since the reinforcement can be generated internally, without intervention of cognitive skills).
Reference: <author> Cecconi, F., F. Menczer, and R. </author> <title> Belew (in preparation). Learning and the evolution of age at maturity. </title> <note> To be submitted to Adaptive Behavior </note> . 
Reference: <author> Cliff, D., P. Husband, and I. </author> <title> Harvey (1993). Analysis of evolved sensory-motor controller. </title> <booktitle> In Se c o n d European Conference on Artificial Life (ECAL93), </booktitle> <address> Brussels. </address>
Reference: <author> Collins, R. and D. </author> <title> Jefferson (1992). AntFarm: Toward Simulated Evolution. Artificial Life II Eds. </title> <editor> C. Langton, C. Taylor, J. Farmer and S. Rasmussen. </editor> <address> Reading, MA, </address> <publisher> Addison Wesley. </publisher> <pages> 579-601. </pages>
Reference-contexts: For example, should we be more impressed by the abilities of "amoebas" that successfully evolve to forage for foods (as in (Nolfi, Elman et al. 1990; Parisi, Cecconi et al. 1990)), by "ants" capable of following "trails" (as in <ref> (Collins and Jefferson 1992) </ref>), or by "agents" that avoid predators (as in (Ackley and Littman 1992))? It would be very desirable to be able to define artificial environments of controlled complexity, within which a wide range of ALife techniques might be directly compared.
Reference: <author> DeJong, K. and J. </author> <title> Sarma (1993). Generation Gaps Revisited. </title> <booktitle> Foundations of Genetic Algorithms 2, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: From the World Wide Web, use URL http://www-cse.ucsd.edu/users/fil. This cycle is repeated indefinitely, with organisms foraging independently in a shared world until some die, other reproduce, and their offspring repeat the process. An important consequences of this "steady-state" version of a genetic algorithm <ref> (DeJong and Sarma 1993) </ref> is that the population size does n o t remain constant throughout an experiment. Analysis of this dynamic variable, and its connection to individual behaviors, is considered in the next section.
Reference: <author> Gould, S. and R. </author> <month> Lewontin </month> <year> (1979). </year> <title> The spandrels of San Marco and the Panglossian paradigm: A critique of the adaptionist programme. </title> <journal> Proceedings of the Royal Society of London B205 : 581-598. </journal>
Reference-contexts: An important emerging feature of the model is its emphasis toward behaviors which are adapted as opposed to optimal in their environments. This adresses a common criticism of biologists toward adaptationist models of evolution as an optimization procedure <ref> (Gould and Lewontin 1979) </ref>.
Reference: <author> Holland, J. </author> <year> (1992). </year> <booktitle> Adaptation in natural and artificial systems , Second Edition. </booktitle> <address> Cambridge, MA, </address> <publisher> Bradford Books (MIT Press). </publisher>
Reference: <author> Jones, T. and M. </author> <title> Mitchell (1993). Introduction to the ECHO model. </title> <institution> Santa Fe Institute. </institution> <note> Working paper 93-12-074. </note>
Reference: <author> Menczer, F. </author> <year> (1994). </year> <title> Changing Latent Energy Environments: A Case for the Evolution of Plasticity. </title> <institution> University of California, </institution> <address> San Diego. </address> <note> Technical report CS94-336. </note>
Reference-contexts: It is important to note that these resources are shared by the population: these conditions lead to density-dependence of fitness (Stearns 1992). Finally let us define a small constant cost, e, incurred every time that an organism makes a move. It is then easy to show <ref> (Menczer and Belew 1994) </ref> that at equilibrium, the expected population size p is given by: p = e where h is the time-average of the fraction of catalyzed reactions yielding energy E . <p> This results in a "random choice" population level: p h=1/ 2 = 2e Equations (3) and (5) give us an upper bound and a baseline, respectively, to measure the average fitness of the population. We have shown elsewhere <ref> (Menczer and Belew 1994) </ref> how to use these equations to make accurate predictions about the outcomes of simulations with simple environments, and to compare behaviors in different environments. <p> First, the number of offspring generated up to age a grows linearly with a: n (a) ra (7) where r is a constant called reproductive rate . Second, in agreement with measured age distributions in LEE <ref> (Menczer and Belew 1994) </ref>, the number of organisms having age a at any given instant follows the Euler-Lotka equation (in the continuous limit): N (a) 0 a a max . (8) Using (7) we find R o = n (a)da a max r a max To find a max notice that, <p> The model has been successfully applied to other issues, such as assessing the adaptive advantages of random individual plasticity in nonstationary environments <ref> (Menczer 1994) </ref> and studying the evolution of age at maturity in the presence of cultural transmission by imitation (Cecconi et al. in preparation).
Reference: <author> Menczer, F. and R. Belew (1993). LEE: </author> <title> A Tool for Artificial Life Simulations. </title> <institution> University of California, </institution> <address> San Diego. </address> <note> Technical report CS93-301. </note>
Reference-contexts: activations; move; digest; /* catalyze reactions */ learn; /* change NNet weights */ if (energy &gt; a) - reproduce; /* copy genotype */ mutate; /* new genotype */ - else if (energy &lt; w) die; - replenish world; - 5 C source code and documentation for release 1.* of LEE <ref> (Menczer and Belew 1993) </ref> is available by anonymous ftp from cs.ucsd.edu (132.239.51.3) in the pub/LEE directory. From the World Wide Web, use URL http://www-cse.ucsd.edu/users/fil. This cycle is repeated indefinitely, with organisms foraging independently in a shared world until some die, other reproduce, and their offspring repeat the process.
Reference: <author> Menczer, F. and R. </author> <title> Belew (1994). Latent Energy Environments. Plastic Individuals in Evolving Populations Eds. </title> <editor> R. Belew and M. Mitchell. </editor> <address> Reading, MA, </address> <publisher> Addison Wesley. </publisher>
Reference-contexts: It is important to note that these resources are shared by the population: these conditions lead to density-dependence of fitness (Stearns 1992). Finally let us define a small constant cost, e, incurred every time that an organism makes a move. It is then easy to show <ref> (Menczer and Belew 1994) </ref> that at equilibrium, the expected population size p is given by: p = e where h is the time-average of the fraction of catalyzed reactions yielding energy E . <p> This results in a "random choice" population level: p h=1/ 2 = 2e Equations (3) and (5) give us an upper bound and a baseline, respectively, to measure the average fitness of the population. We have shown elsewhere <ref> (Menczer and Belew 1994) </ref> how to use these equations to make accurate predictions about the outcomes of simulations with simple environments, and to compare behaviors in different environments. <p> First, the number of offspring generated up to age a grows linearly with a: n (a) ra (7) where r is a constant called reproductive rate . Second, in agreement with measured age distributions in LEE <ref> (Menczer and Belew 1994) </ref>, the number of organisms having age a at any given instant follows the Euler-Lotka equation (in the continuous limit): N (a) 0 a a max . (8) Using (7) we find R o = n (a)da a max r a max To find a max notice that, <p> The model has been successfully applied to other issues, such as assessing the adaptive advantages of random individual plasticity in nonstationary environments <ref> (Menczer 1994) </ref> and studying the evolution of age at maturity in the presence of cultural transmission by imitation (Cecconi et al. in preparation).
Reference: <author> Menczer, F. and D. </author> <title> Parisi (1992). Recombination and unsupervised learning: effects of crossover in the genetic optimization of neural networks. </title> <booktitle> Network 3: </booktitle> <pages> 423-442. </pages>
Reference: <author> Miglino, O. and D. </author> <title> Parisi (1991). Evolutionary stable and unstable strategies in neural networks. </title> <booktitle> IJCNN, </booktitle> <address> Piscataway, NJ: </address> <publisher> IEEE. </publisher>
Reference-contexts: In fact, experiments by Miglino et al. in which the arbitrary specification of the sensory interface appeared to have important consequences for learning helped to motivate our work <ref> (Miglino and Parisi 1991) </ref>. Sensors transducers from external environmental signals to the internal cognitive system therefore represent a crucial link between two distinct adaptive forces.
Reference: <author> Mitchell, M. and S. </author> <title> Forrest (1993). </title> <booktitle> Genetic Algorithms and Artificial Life. </booktitle> <institution> Santa Fe Institute. </institution> <note> Working paper 93-11-072. </note>
Reference-contexts: If it collects more than a fixed threshold of energy in this way, it asexually reproduces. A critical observation is that this form of fitness is intrinsic to LEE environments, resulting directly from competition for resources within the population rather than being extrinsically applied as an evaluation on individuals <ref> (Mitchell and Forrest 1993) </ref>. After a LEE environment is specified, a simulation is begun by randomly placing an initial population of organisms within it.
Reference: <author> Nolfi, S., J. E. Elman, and D. </author> <title> Parisi (1990). Learning and Evolution in Neural Networks. </title> <institution> University of California, </institution> <address> San Diego. </address> <note> Technical report CRL-TR-9019. </note>
Reference-contexts: We will consider two variants of this basic NNet, each using a different source of feedback for learning. In the experiments of Section 4.3, we follow the work of Nolfi et al. <ref> (Nolfi, Elman et al. 1990) </ref>. The NNet's output is extended to produce not only a motor action, but also a prediction of the sensory input it will receive after that action.
Reference: <author> Parisi, D., F. Cecconi, and S. </author> <title> Nolfi (1990). Econets: Neural networks that learn in an environment. </title> <booktitle> Network 1: </booktitle> <pages> 149-168. </pages>
Reference: <author> Rssler, O. </author> <year> (1974). </year> <title> Adequate Locomotion Strategies for an Abstract Environment - A Relational Approach. Physics and Mathematics of the Nervous System Eds. </title>
Reference-contexts: Our work builds on several other attempts to define generic conditions on environmental complexity that are analytically tractable without constraining evolution's creative potential. The physicist Otto Rssler observed that varying food density could be used as a simple metric on the complexity of an environment facing an organism <ref> (Rssler 1974) </ref>. Even a random walk is an adequate foraging technique if food is abundant, but more coherent movement is required if the foodstuffs are more widely spaced; in even more scarce environments, the foraging organism may need to depend on landmarks, cognitive maps, etc.
Reference: <author> M. C. M, W. G. W and M. </author> <title> Cin. </title> <address> New York, NY, </address> <publisher> Sringer-Verlag. </publisher> <pages> 399-418. </pages>
Reference: <author> Rumelhart, D., G. H. Hinton, and R. </author> <title> Williams (1986). Learning internal representations by error propagation. Parallel Distributed Processing: Explorations in the Microstructure of Cognition Eds. </title> <editor> D. R. DE and J. McClelland. </editor> <address> Cambridge, MA, </address> <publisher> Bradford Books (MIT Press). </publisher>
Reference-contexts: A requirement for any learning mechanism in our model is the 11 assumption that there be no additional "teaching" input available to the organism, beyond the relatively neutral stimulus of the organisms environment. This would seem to preclude many successful "supervised" learning techniques, e.g., backpropagation <ref> (Rumelhart, Hinton et al. 1986) </ref>. However, if the organism is simply forced to predict the expected outcome of its actions, differences between its expectations and the actual outcome can generate the same sort of error information, without any additional teacher (Nolfi, Elman et al. 1990; Parisi, Cecconi et al. 1990).
Reference: <author> Stearns, S. </author> <year> (1992). </year> <title> The Evolution of Life Histories. </title> <address> New York, NY, </address> <publisher> Oxford University Press. </publisher>
Reference-contexts: Let us assume the environmental resources are introduced with uniform distribution in space and time, at rate r (measured in atoms per unit time and area). It is important to note that these resources are shared by the population: these conditions lead to density-dependence of fitness <ref> (Stearns 1992) </ref>. Finally let us define a small constant cost, e, incurred every time that an organism makes a move.
Reference: <author> Whitley, L., S. Dominic, and R. </author> <title> Das (1991). Genetic reinforcement learning with multilayer neural networks. </title> <booktitle> Fourth International Conference on Genetic Algorithms, </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Wilson, S. </author> <year> (1991). </year> <title> The Animat Path to AI. From Animals to Animats: </title> <booktitle> First Intl. Conf. on Simulation of Adaptive Behavior , Cambridge, </booktitle> <address> MA: </address> <publisher> Bradford Books (MIT Press). </publisher>
Reference-contexts: Wilson has pursued a similar analysis of environmental complexity as a function of spatial food density <ref> (Wilson 1991) </ref>. Food density is clearly a useful dimension for ALife simulation, but this single dimension of environmental variability must be extended to include other factors if we are to be able to test the full repertoire of ALife models.
References-found: 25

