URL: http://www.cs.berkeley.edu/~alanm/CP/mckinley.tr-94-35.94.ps
Refering-URL: http://www.cs.berkeley.edu/~alanm/CP/bib.html
Root-URL: 
Title: A Survey of Collective Communication in Wormhole-Routed Massively Parallel Computers  
Author: Philip K. McKinley, Yih-jia Tsai, and David F. Robinson 
Note: Submitted for publication, June 1994.  
Abstract: Technical Report MSU-CPS-94-35 June 1994 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. S. Sunderam, </author> <title> "PVM: A framework for parallel distributed computing," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> vol. 2(4), </volume> <pages> pp. 315-339, </pages> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: One method that has been used to address these problems is to construct communication libraries <ref> [1] </ref>, which hide the details of the underlying architecture and vendor-specific interfaces from the user but provide a common interface across multiple platforms, permitting user code to be more easily ported among machines.
Reference: [2] <author> D. B. Loveman, </author> <title> "High Performance Fortran," </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> vol. 1, </volume> <pages> pp. 25-42, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: In order to further simplify the programmer's task and improve code portability, an alternative approach to parallel programming is to use a data parallel language, such as High Performance Fortran (HPF) <ref> [2] </ref>, which provides the user with control over data alignment and realignment, but which hides the communication calls from the user. For a distributed-memory system, the compiler for such a language must translate high-level data parallel language constructs into appropriate low-level communication primitives [3, 4]. <p> Li and Chen [3] have studied this translation process. The communication operations generated in their approach include all those listed in Table 1, as well as others, such as permutation. More recently, a coalition of industrial and academic groups has standardized High Performance Fortran <ref> [2] </ref>, which is designed for distributed-memory platforms. When an HPF program is compiled, many of those communication operations generated by the compiler may be collective in nature. For example, HPF contains so- called intrinsics, which perform reduction and scan, rearranging and reshaping, and scatter and gather operations on data arrays.
Reference: [3] <author> J. Li and M. Chen, </author> <title> "Compiling communication-efficient programs for massively parallel machines," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 2, </volume> <pages> pp. 361-375, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: For a distributed-memory system, the compiler for such a language must translate high-level data parallel language constructs into appropriate low-level communication primitives <ref> [3, 4] </ref>. Whether communication operations are programmed by the user, contained in a library, or generated by a compiler, their latency directly affects the total computation time of the parallel application. <p> In spite of the presence of communication libraries and numerical libraries, many users prefer a shared-memory programming paradigm to a message-passing paradigm. A compiler translates a program written in a data parallel language into lower-level communication operations. Li and Chen <ref> [3] </ref> have studied this translation process. The communication operations generated in their approach include all those listed in Table 1, as well as others, such as permutation. More recently, a coalition of industrial and academic groups has standardized High Performance Fortran [2], which is designed for distributed-memory platforms. <p> Although a good deal of research is presently studying the use of collective communication in numerical libraries [10], and the relationship between data distribution patterns and the collective communication generated by compilers for data parallel languages <ref> [3] </ref>, the potential payoffs from both areas are high, and increased research is needed. Finally, another trend in high-performance computing is towards clusters of workstations connected by high-speed switches.
Reference: [4] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng, </author> <title> "Compiling Fortran D for MIMD distributed-memory machines," </title> <journal> Communications of the ACM, </journal> <pages> pp. 66-80, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: For a distributed-memory system, the compiler for such a language must translate high-level data parallel language constructs into appropriate low-level communication primitives <ref> [3, 4] </ref>. Whether communication operations are programmed by the user, contained in a library, or generated by a compiler, their latency directly affects the total computation time of the parallel application. <p> The growing interest in collective operations is evidenced by their inclusion in Message Passing Interface (MPI) [5], an emerging standard for communication routines used by message-passing programs, and by their increasing role in supporting various programming constructs in HPF <ref> [4] </ref>. A set of standard collective operations is reviewed in Section 2. Efficient implementation of collective communication operations depends on the underlying ar <p>- Communications Research Group 1 Michigan State University chitecture of the MPC.
Reference: [5] <author> Message Passing Interface Forum, </author> <title> "Document for standard message-passing interface," </title> <type> Tech. Rep. </type> <institution> CS-93214, University of Tennessee, </institution> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: In such applications, nodes use collective operations to distribute, gather, and exchange data, to perform global compute operations on distributed data, and to synchronize with one another at specific points in program flow. The growing interest in collective operations is evidenced by their inclusion in Message Passing Interface (MPI) <ref> [5] </ref>, an emerging standard for communication routines used by message-passing programs, and by their increasing role in supporting various programming constructs in HPF [4]. A set of standard collective operations is reviewed in Section 2. <p> The group, which may constitute all or a subset of the processes in the parallel application, is assumed to have been previously defined and is identified in one of the parameters to the collective operation <ref> [5] </ref>. Collective operations can operate synchronously or asynchronously, depending on whether a calling process can return before other processes in the group have called the routine. Definitions of Operations. Collective operations may be used for process control, data movement, or global operations; examples are listed in Table 1. <p> Collective communication operations are found in many algorithms designed for message-passing systems. In fact, it was their frequent use that led to their inclusion in several commercial communications libraries, and eventually to the standardization of their syntax and semantics in MPI <ref> [5] </ref>. Collective operations are used in numerous sorting, graph, and search algorithms [9]. Perhaps the largest class of message-passing applications that can take advantage of efficient collective operations is parallel numerical algorithms.
Reference: [6] <author> W. J. Dally and C. L. Seitz, </author> <title> "The torus routing chip," </title> <journal> Journal of Distributed Computing, </journal> <volume> vol. 1, no. 3, </volume> <pages> pp. 187-196, </pages> <year> 1986. </year> <institution> Communications Research Group 36 Michigan State University </institution>
Reference-contexts: While there has been little consensus on some aspects of communication architectures, such as network topology, there has been a good deal of agreement on the way in which messages are switched through the network. Specifically, many new generation MPCs employ wormhole routing <ref> [6] </ref>, where each message is divided into small pieces that are pipelined through the network by way of routers at each node. Compared to the store-and-forward switching method that was used in early multicomputers, wormhole routing often reduces the effect of path length on communication latency [7].
Reference: [7] <author> L. M. Ni and P. K. McKinley, </author> <title> "A survey of wormhole routing techniques in direct networks," </title> <journal> IEEE Computer, </journal> <volume> vol. 26, </volume> <pages> pp. 62-76, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Compared to the store-and-forward switching method that was used in early multicomputers, wormhole routing often reduces the effect of path length on communication latency <ref> [7] </ref>. In the absence of channel contention, which occurs when two messages simultaneously require the same channel, the measured latency of a wormhole-routed message has been shown to be nearly independent of the distance between the source and destination nodes [7]. <p> often reduces the effect of path length on communication latency <ref> [7] </ref>. In the absence of channel contention, which occurs when two messages simultaneously require the same channel, the measured latency of a wormhole-routed message has been shown to be nearly independent of the distance between the source and destination nodes [7]. However, in situations where multiple messages exist in the network concurrently, channel contention among those messages may be exacerbated by the use of wormhole routing, in which blocked messages hold some communication channels while waiting for other messages. <p> The invocation of a collective operation, whose implementation may involve many messages, poses precisely such a situation. This paper addresses the design of collective communication operations for wormhole-routed networks. Ni and McKinley <ref> [7] </ref> previously surveyed wormhole routing techniques for direct networks; that paper focused on switching architectures, performance comparisons with other switching strategies, deadlock prevention methods, and adaptive routing algorithms. <p> For relatively long messages, the value of t p d becomes small compared to that of `t f <ref> [7] </ref>. The start-up latency, t s , is the time required for the system to handle the message at both the source and destination nodes. Its value is mainly dependent on the design of system software and the interface between local processors and routers. <p> With the advent of wormhole routing, in which internode distance has less effect on communication delay, low-dimensional meshes and tori have attracted larger followings due to their simpler physical layouts and better scalability <ref> [7] </ref>. Communications Research Group 7 Michigan State University Notable exceptions to mesh-based direct networks include the fat tree, which is used in the TMC CM-5, and switch-based interconnection networks, as used in the IBM SP1.
Reference: [8] <author> P. K. McKinley, H. Xu, A.-H. Esfahanian, and L. M. Ni, </author> <title> "Unicast-based multicast communication in wormhole-routed networks," </title> <booktitle> in Proc. of the 1992 International Conference on Parallel Processing, </booktitle> <volume> vol. II, </volume> <pages> pp. 10-19, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: These approaches are designed for systems that support only point-to-point, or unicast, communication in hardware. In these environments, collective operations must be implemented by sending multiple unicast messages; such implementations are called unicast-based <ref> [8] </ref>. We first describe the implementation of tree-like arrangements of messages in various network topologies; these structures are used in several of the "elementary" collective operations, such as broadcast and global compute operations. <p> It is important to distinguish between the process view and the node view of a collective operation. For example, when one node in a network sends a message to a proper subset of the nodes, this is usually referred to as multicast <ref> [8] </ref>. However, the MPI standard does not explicitly discuss multicast. Rather, MPI would describe multicast as a broadcast to a group of processes that happen to reside on only a proper subset of the nodes in the network. <p> Many existing MPCs, however, support only unicast communication in hardware. In these environments, all communication operations must be implemented in software by sending one or more unicast messages; such implementations are called unicast-based <ref> [8] </ref>. Finally, operations may be partially supported in hardware. For example, a system may provide some elementary collective operations, such as multicast, in hardware, with several instances combined in software to implement a more complex operations. <p> Startup latency can be further decomposed into sending latency, t snd , and receiving latency, t rcv , the start-up latencies incurred at the sending node and the receiving node, respectively. For some systems, t s may be an order of magnitude greater than t n for small messages <ref> [8] </ref>. Thus, the latency of short messages is distance-insensitive due to startup latency, while the latency of long messages is distance-insensitive due to the pipelining effect of wormhole routing. <p> Such an ordering is also called dimension order, denoted by the symbol &lt; d . A sequence of node addresses that are ordered with respect to dimension order is referred to as a dimension-ordered chain <ref> [8] </ref>. Three properties of dimension-ordered chains, depicted in Figure 14, are used in these multicast algorithms. Consider any four nodes, u, v, x, and y, in a hypercube or mesh. <p> Consider any four nodes, u, v, x, and y, in a hypercube or mesh. If u &lt; d v &lt; d x &lt; d y, then dimension-ordered routes P (u; v) and P (x; y) in Figure 14 (a) have been shown to be arc-disjoint <ref> [8] </ref>. That is to say, the routes do not share any channels (although they may use two, oppositely directed channels which, in some systems, may be multiplexed on the same link). Two other pairs of routes, depicted in Figures 14 (b) and 14 (c), are also arc-disjoint under dimension-ordered routing.
Reference: [9] <author> V. Kumar, A. Grama, A. Gupta, and G. Karypis, </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <address> Redwood City, California: Benjamin/Cummings, </address> <year> 1994. </year>
Reference-contexts: In fact, it was their frequent use that led to their inclusion in several commercial communications libraries, and eventually to the standardization of their syntax and semantics in MPI [5]. Collective operations are used in numerous sorting, graph, and search algorithms <ref> [9] </ref>. Perhaps the largest class of message-passing applications that can take advantage of efficient collective operations is parallel numerical algorithms. Collective operations are used in a wide variety of matrix-related algorithms, including solving of linear systems, finding eigenvalues, and performing transform operations [9]. <p> used in numerous sorting, graph, and search algorithms <ref> [9] </ref>. Perhaps the largest class of message-passing applications that can take advantage of efficient collective operations is parallel numerical algorithms. Collective operations are used in a wide variety of matrix-related algorithms, including solving of linear systems, finding eigenvalues, and performing transform operations [9]. Many of these numerical algorithms have themselves been organized into libraries. For example, the ScaLAPACK project [10] is targeted to producing a distributed-memory version of LA- PACK, a popular sequential library for problems in numerical linear algebra. <p> In receiving-type operations, such as gather and reduction, messages are sent towards the root from the leaves. If the communication patterns of two operations are essentially identical except that the direction of the messages is reversed, then these operations are called duals of one another <ref> [9] </ref>. Which type of tree to use depends on the underlying architecture and which nodes are involved in the operation. Let us define a broadcast tree as one that involves all nodes in the network, and a multicast tree as one that involves only a partial subset of the nodes. <p> The same message-passing pattern can be used for other operations involving all nodes in the Communications Research Group 25 Michigan State University hypercube, although the size of the messages in each step differs <ref> [9] </ref>. In each step of all-to-all broadcast, every node sends its own message as well as all those it has received from other nodes; the message size in step i is m2 i1 . <p> Since the port utilization at some nodes is 100% over the entire operation, the network switching technique does not affect performance. Reduction and Scan. We mentioned previously that the standard exchange algorithm (Figure 25) can be used to implement N/N reduction in hypercubes. As described by Kumar <ref> [9] </ref>, the same approach can also be used to implement scan operations by adding an additional "result" buffer at each node [9]. The result buffer at each node is initialized with the data at that node. <p> Reduction and Scan. We mentioned previously that the standard exchange algorithm (Figure 25) can be used to implement N/N reduction in hypercubes. As described by Kumar <ref> [9] </ref>, the same approach can also be used to implement scan operations by adding an additional "result" buffer at each node [9]. The result buffer at each node is initialized with the data at that node.
Reference: [10] <author> J. Choi, J. J. Dongarra, R. Pozo, and D. W. Walker, </author> <title> "ScaLAPACK: A scalable linear algebra library for distributed memory concurrent computers," </title> <booktitle> in Proceedings of the Fourth Symposium on the Frontiers of Massively Paralllel Computation, </booktitle> <pages> pp. 120-127, </pages> <publisher> IEEE CS Press, </publisher> <year> 1992. </year>
Reference-contexts: Collective operations are used in a wide variety of matrix-related algorithms, including solving of linear systems, finding eigenvalues, and performing transform operations [9]. Many of these numerical algorithms have themselves been organized into libraries. For example, the ScaLAPACK project <ref> [10] </ref> is targeted to producing a distributed-memory version of LA- PACK, a popular sequential library for problems in numerical linear algebra. In order to improve portability, ScaLAPACK uses a library called BLACS (Basic Linear Algebra Communication Subpro- grams) [11], which provides basic matrix-related communications operations. <p> Third, the mapping between higher-level programming paradigms Communications Research Group 35 Michigan State University and collective operations is becoming a research area in its own right. Although a good deal of research is presently studying the use of collective communication in numerical libraries <ref> [10] </ref>, and the relationship between data distribution patterns and the collective communication generated by compilers for data parallel languages [3], the potential payoffs from both areas are high, and increased research is needed. Finally, another trend in high-performance computing is towards clusters of workstations connected by high-speed switches.
Reference: [11] <author> J. Dongarra, R. van de Geijn, and R. Whaley, </author> <title> "Two dimensional basic linear algebra communication subprograms," </title> <booktitle> in Proceedings of the sixth SIAM conference on Parallel Processing, </booktitle> <pages> pp. 347-352, </pages> <year> 1993. </year>
Reference-contexts: For example, the ScaLAPACK project [10] is targeted to producing a distributed-memory version of LA- PACK, a popular sequential library for problems in numerical linear algebra. In order to improve portability, ScaLAPACK uses a library called BLACS (Basic Linear Algebra Communication Subpro- grams) <ref> [11] </ref>, which provides basic matrix-related communications operations. In turn, BLACS can be implemented using architecture-specific implementations of collective operations. In spite of the presence of communication libraries and numerical libraries, many users prefer a shared-memory programming paradigm to a message-passing paradigm.
Reference: [12] <author> B. Nitzberg and V. Lo, </author> <title> "Distributed shared memory: A survey of issues and algorithms," </title> <journal> IEEE Computer, </journal> <volume> vol. 24, </volume> <pages> pp. 52-60, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: In order to improve the performance of local reads, most systems allow data to be replicated across multiple nodes, which implies that the system must take certain measures in order to maintain memory coherence. A wide variety of coherence protocols have been proposed <ref> [12] </ref>; many of them can be classified as either write <p>- Communications Research Group 5 Michigan State University update or write-invalidate. Both approaches rely on broadcast to send invalidations and updates, respectively.
Reference: [13] <author> H. Sullivan and T. R. Bashkow, </author> <title> "A large scale, homogeneous, fully distributed parallel machine," </title> <booktitle> in Proceedings of the 4th Annu. Symp. Comput. Architecture, </booktitle> <volume> vol. 5, </volume> <pages> pp. 105-124, </pages> <month> Mar. </month> <year> 1977. </year>
Reference-contexts: An approach that makes better use of the dense interconnection of the hypercube topology is the well-known spanning binomial tree (SBT) algorithm <ref> [13] </ref>, which is illustrated for a 3-cube in Communications Research Group 10 Michigan State University (a) Hamiltonian path (b) E-cube tree (c) Edge-disjoint spanning trees whose address differs from its own in the lowest (alternatively highest) bit position.
Reference: [14] <author> S. L. Johnsson and C.-T. Ho, </author> <title> "Optimum broadcasting and personalized communication in hypercubes," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-38, </volume> <pages> pp. 1249-1268, </pages> <month> Sept. </month> <year> 1989. </year>
Reference-contexts: This algorithm requires n message-passing steps to reach all nodes in an n-cube, with the last node receiving the message at time nt u . In order to reduce broadcast latency in hypercubes, Johnsson and Ho <ref> [14] </ref> proposed a tree structure that uses n edge-disjoint spanning trees (EDST) of the hypercube. In order to implement a broadcast operation using this method, the message is partitioned into n segments, each of which is transmitted along a different spanning tree, as illustrated in Figure 6 (c). <p> The source may then execute the chain algorithm using instead of the original addresses. An example is shown in Figure 15. Notice that this U-cube multicast algorithm degenerates to the well known spanning binomial tree <ref> [14] </ref> for the special case of broadcast. In meshes, symmetry cannot be used for cases in which the source address lies in the middle of a dimension-ordered chain. However, another relatively simple method, based on Figure 14 (c), may be used to address this problem. <p> To save communication startup time, the algorithm may combine messages into larger message blocks to be transmitted as a single unit between adjacent nodes. The standard exchange algorithm for hypercubes <ref> [14] </ref> is an example of a complete exchange algorithm designed for one-port, store-and-forward systems. As shown in Figure 25, the standard exchange algorithm consists of n steps; during each step, the cube is recursively divided into halves, and messages are exchanged across this new division, between pairs of corresponding nodes.
Reference: [15] <author> P. K. McKinley and C. Trefftz, </author> <title> "Efficient broadcast in all-port wormhole routed hypercubes," </title> <booktitle> in Proc. of the 1993 International Conference on Parallel Processing, </booktitle> <volume> vol. II, </volume> <pages> pp. 288-291, </pages> <year> 1993. </year>
Reference-contexts: McKinley and Trefftz <ref> [15] </ref> proposed a simple variation on the SBT called the Double Tree (DT) algorithm, which is designed for all-port, wormhole-routed hypercubes. The DT algorithm begins with the source node s sending the message to node s, whose address is the bitwise complement of s. <p> The DT algorithm requires three steps in a 6-cube, which is also optimal. Experiments on an nCUBE-2 hypercube, which possesses a multiple-port architecture, confirm the advantage of this approach over a spanning binomial tree, particularly for large messages <ref> [15] </ref>. Although the DT algorithm reduces broadcast latency compared to the SBT algorithm, it is not optimal for larger hypercubes. <p> Multiple ports allow a local processor to simultaneously transmit (receive) messages to (from) any of its external channels. Of course, high sending latency can cause message transmission to be staggered, even serialized if the message is small enough. However, our experiences with the nCUBE-2 <ref> [15] </ref> have shown that its all-port architecture allows considerable overlap among messages sent in succession from a given node, even though the startup latency of the nCUBE-2 is relatively high, between 80 and 100 microseconds for sending messages.
Reference: [16] <author> C. Ho and M. Kao, </author> <title> "Optimal broadcast on hypercubes with wormhole and e-cube routings," </title> <booktitle> in Proceedings of the 1993 International Conference on Parallel and Distributed Systems, </booktitle> <address> (Taipei, Taiwan), </address> <pages> pp. 694-697, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: Experiments on an nCUBE-2 hypercube, which possesses a multiple-port architecture, confirm the advantage of this approach over a spanning binomial tree, particularly for large messages [15]. Although the DT algorithm reduces broadcast latency compared to the SBT algorithm, it is not optimal for larger hypercubes. Ho and Kao <ref> [16] </ref> discovered a more general solution in which the network is recursively divided into subcubes of nearly equal size; the DT algorithm is used to finish the broadcast operation in small subcubes. The approach uses the concept of a dimension-simple path. <p> In this manner, an n-dimensional cube can be partitioned into n + 1 subcubes such that each subcube contains one node on the dimension-simple path. Ho and Kao <ref> [16] </ref> have shown that any cube can be partitioned equally or nearly equally, which implies that the number of steps required by their algorithm is optimal to within a multiplicative constant, specifically, the time required is fi ( n log 2 (n+1) ). the source node 0000000 sends a copy of
Reference: [17] <author> M. Barnett, D. G. Payne, and R. van de Geijn, </author> <title> "Optimal broadcasting in mesh-connected architectures," </title> <type> Tech. Rep. </type> <institution> TR-91-38, Department of Computer Science, The University of Texas at Austin, </institution> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: In this section, we describe algorithms for both broadcast and multicast in one-port systems; multiple-port architectures are addressed in Section 6. Dimensional Broadcast Tree Approach. The one-to-all broadcast problem in one-port wormhole-routed mesh networks was first studied by Barnett et al <ref> [17] </ref>. The algorithm presented in [17] operates in a recursive manner, similar to the spanning binomial tree, within each dimension of the mesh. Suppose, for example, that the source node is the leftmost node in a linear array topology, as illustrated in Figure 11. <p> In this section, we describe algorithms for both broadcast and multicast in one-port systems; multiple-port architectures are addressed in Section 6. Dimensional Broadcast Tree Approach. The one-to-all broadcast problem in one-port wormhole-routed mesh networks was first studied by Barnett et al <ref> [17] </ref>. The algorithm presented in [17] operates in a recursive manner, similar to the spanning binomial tree, within each dimension of the mesh. Suppose, for example, that the source node is the leftmost node in a linear array topology, as illustrated in Figure 11.
Reference: [18] <author> D. F. Robinson, P. K. McKinley, and B. H. C. Cheng, </author> <title> "Optimal multicast communication in torus net-works," </title> <booktitle> in Proc. of the 1994 International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1994. </year> <note> accepted to appear. </note>
Reference-contexts: The U-mesh algorithm is not contention-free when executed on a torus; however, a simple extension of the dimension-ordered chain can be used to avoid contention in a torus (and, incidentally, also Communications Research Group 18 Michigan State University in a mesh) <ref> [18] </ref>. The source x s and destinations are sorted into a dimension-ordered chain = fx 0 ; x 1 ; x 2 ; : : : ; x s , : : : x m1 g. Next, is "rotated" so that x s becomes the first element in the chain. <p> The U-torus algorithm also applies to bidirectional tori, in which messages can be sent in either direction in each dimension <ref> [18] </ref>. The reader may notice that two of the messages sent in step three appear to require the same channel from node (1; 4) to node (1; 5). However, not shown in Figure 17 are the virtual channels required to prevent deadlock in a unidirectional torus.
Reference: [19] <author> M. Barnett, D. G. Payne, R. A. van de Geijn, and J. Watts, </author> <title> "Broadcasting on meshes with worm-hole routing," </title> <type> Tech. Rep. </type> <institution> TR-93-24, Department of Computer Science, The University of Texas at Austin, </institution> <month> November 2 </month> <year> 1993. </year>
Reference-contexts: One algorithm that uses this approach in 2D mesh networks is similar to the EDST algorithm for hypercubes, and is appropriately named edge-disjoint spanning fences (EDSF) <ref> [19] </ref>. The EDSF algorithm embeds two disjoint spanning trees, or fences, in the mesh, as illustrated in Figure 19. The source node partitions the message and alternately pipelines the pieces through the two fences. For clarity, the source in Figure 19 is the node in the upper-left corner. <p> Despite these additional costs, however, this approach has been shown to provide better performance, for relatively long messages, than a non-pipelined tree <ref> [19] </ref>. Another broadcast method that involves message partitioning, called scatter-collect, is also proposed in [19]. Figure 20 illustrates the operation of this algorithm in an 8 fi 8 2D mesh. <p> Despite these additional costs, however, this approach has been shown to provide better performance, for relatively long messages, than a non-pipelined tree <ref> [19] </ref>. Another broadcast method that involves message partitioning, called scatter-collect, is also proposed in [19]. Figure 20 illustrates the operation of this algorithm in an 8 fi 8 2D mesh. <p> Next, the nodes in each column form a logical ring, and nodes circulate segments around the ring until all nodes hold all segments. This algorithm avoids channel contention. As with the EDSF algorithm, the pipelining of messages provides good performance for broadcast of long messages <ref> [19] </ref>. The scatter-collect algorithm can also be used to implement multicast in hypercubes, meshes, and tori, with the aid of the multicast tree algorithms described earlier. Specifically, the source node can use a multicast tree to scatter the data to all the nodes in the group.
Reference: [20] <author> Y.-J. Tsai and P. K. McKinley, </author> <title> "An extended dominating node approach to collective communication in wormhole-routed 2D meshes," </title> <booktitle> in Proceedings of the Scalable High Performance Computing Conference, </booktitle> <pages> pp. 199-206, </pages> <year> 1994. </year>
Reference-contexts: Moreover, newly announced wormhole-routed systems, such as the nCUBE-3 and Cray T3D, claim much lower startup latencies of a few microseconds. The EDN Approach. The Extended Dominating Node (EDN) model <ref> [20] </ref> has been developed recently to systematically construct unicast-based collective operations for multi-port wormhole-routed networks. The model is based on the concept of dominating sets from graph theory. <p> The EDN approach uses multiple levels of EDNs to define multiple message-passing steps of collective communication algorithms. The key issue in applying the EDN method to the development of collective operations lies in finding levels of EDNs that form regular and recursive patterns. mesh network under XY routing <ref> [20] </ref>. In the first two message-passing steps, called startup steps, the source node delivers the message to the highest-level EDNs, that is, the level-3 EDNs; there are four such nodes. The level-3 EDNs proceed to "dominate" the twelve level-2 EDNs by delivering the message to them in step 3. <p> For example, the EDN approach has also been applied to reduction and gather, which are duals of broadcast, to the problem of matrix transposition, an example of a permutation operation <ref> [20] </ref>, and Communications Research Group 24 Michigan State University to other topologies, including higher-dimension meshes and tori. Other research has addressed the problem of multicast in all-port hypercubes [21].
Reference: [21] <author> D. F. Robinson, D. Judd, P. K. McKinley, and B. H. C. Cheng, </author> <title> "Efficient collective data distribution in all-port wormhole-routed hypercubes," </title> <booktitle> in Proceedings of Supercomputing'93, </booktitle> <pages> pp. 792-803, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Other research has addressed the problem of multicast in all-port hypercubes <ref> [21] </ref>. However, numerous problems in the area of collective communication in multi-port systems remain to be explored. 7 All-to-All Communication The complex collective operations, such as complete exchange and all-to-all broadcast, can be implemented by having multiple nodes simply invoke other operations, such as broadcast, scatter, and gather.
Reference: [22] <author> S. R. Seidel, </author> <title> "Circuit switched vs. store-and-forward solutions to symmetric communication problems," </title> <booktitle> in Proceedings of the 4th Conference on Hypercube Computers and Concurrent Applications, </booktitle> <pages> pp. 253-255, </pages> <year> 1989. </year>
Reference-contexts: After the nth step, all nodes will hold the reduced value. Since the standard exchange algorithm uses only nearest-neighbor communication, its performance will be approximately the same on either a store-and-forward or wormhole-routed architecture. The direct algorithm for hypercubes <ref> [22] </ref>, is a complete exchange algorithm designed to take advantage of the distance-insensitive communication of one-port, wormhole-routed hypercubes.
Reference: [23] <author> R. Thakur and A. Choudhary, </author> <title> "All-to-all communication on meshes with wormhole routing," </title> <booktitle> in Proceedings of the 1994 International Parallel Processing Symposium, </booktitle> <pages> pp. 561-565, </pages> <year> 1994. </year>
Reference-contexts: Also, the time required to permute the data after each step of the standard exchange algorithm should not be ignored. Communications Research Group 26 Michigan State University In a variation of the direct algorithm for hypercubes, the direct algorithm for 2D meshes <ref> [23] </ref> performs the complete exchange in a wormhole-routed 2D mesh through a combination of pairwise message exchanges. Like its hypercube counterpart, the direct algorithm for 2D meshes is a simple algorithm: the nodes of the mesh are assigned the ordinal numbers 0 through N 1 in a row-major fashion. <p> Notice that in steps 2, 3, 6, and 7, messages simultaneously require common communication channels, leading to stepwise channel contention. Nonetheless, on systems with relatively high startup latency and, therefore, excessive communication capacity, this factor may not significantly degrade performance <ref> [23] </ref>. The algorithm can be generalized to accommodate arbitrary mesh sizes [23]. Another algorithm for complete exchange on a wormhole-routed 2D mesh, the binary exchange algorithm for 2D mesh [24, 25], is derived from the standard exchange algorithm for hypercubes. <p> Nonetheless, on systems with relatively high startup latency and, therefore, excessive communication capacity, this factor may not significantly degrade performance <ref> [23] </ref>. The algorithm can be generalized to accommodate arbitrary mesh sizes [23]. Another algorithm for complete exchange on a wormhole-routed 2D mesh, the binary exchange algorithm for 2D mesh [24, 25], is derived from the standard exchange algorithm for hypercubes.
Reference: [24] <author> S. H. Bokhari and H. Berryman, </author> <title> "Complete exchange on a circuit switched mesh," </title> <booktitle> in Proceedings of the 1992 Scalable High Performance Computing Conference, </booktitle> <pages> pp. 300-306, </pages> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: Nonetheless, on systems with relatively high startup latency and, therefore, excessive communication capacity, this factor may not significantly degrade performance [23]. The algorithm can be generalized to accommodate arbitrary mesh sizes [23]. Another algorithm for complete exchange on a wormhole-routed 2D mesh, the binary exchange algorithm for 2D mesh <ref> [24, 25] </ref>, is derived from the standard exchange algorithm for hypercubes. Whereas the standard exchange recursively halves the hypercube during each phase, the binary exchange algorithm recursively halves the 2D mesh during each phase, alternately in the X and Y dimensions. <p> As Figure 27 illustrates, all communication occurs between pairs of nodes in the same row or in the same column. Since this algorithm incurs channel contention in each phase, we have labeled each phase with the actual number of message-passing steps required. In contrast, the quadrant exchange algorithm <ref> [24] </ref> was designed specifically to account for wormhole routing and mesh topologies. During each stage of the algorithm, quadruples of nodes exchange messages among themselves in a series of three communication steps, as shown in Figure 28 (a).
Reference: [25] <author> S. Gupta, S. Hawkinson, and B. Baxter, </author> <title> "A binary interleaved algorithm for complete exchange on a mesh architecture," </title> <type> tech. rep., </type> <institution> Intel, Beaverton, Oregon. Communications Research Group 37 Michigan State University </institution>
Reference-contexts: Nonetheless, on systems with relatively high startup latency and, therefore, excessive communication capacity, this factor may not significantly degrade performance [23]. The algorithm can be generalized to accommodate arbitrary mesh sizes [23]. Another algorithm for complete exchange on a wormhole-routed 2D mesh, the binary exchange algorithm for 2D mesh <ref> [24, 25] </ref>, is derived from the standard exchange algorithm for hypercubes. Whereas the standard exchange recursively halves the hypercube during each phase, the binary exchange algorithm recursively halves the 2D mesh during each phase, alternately in the X and Y dimensions.
Reference: [26] <author> D. P. Bertsekas, C. Ozveren, G. D. Stamoulis, and J. N. Tsitsiklis, </author> <title> "Optimal communication algorithms for hypercubes," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 15, no. 11, </volume> <pages> pp. 263-175, </pages> <year> 1991. </year>
Reference-contexts: This operation requires w i steps in each dimension i of a n-dimensional mesh or torus. The message size increases by a factor of w i with each dimension. The total time is P n1 Communications Research Group 29 Michigan State University For all-port hypercubes, Bertsekas et al <ref> [26] </ref>, developed algorithms for all-to-all broadcast, scat-ter, and complete exchange that were shown to be optimal under both store-and-forward as well as wormhole-routed switching. These algorithms require relatively close synchronization among the nodes.
Reference: [27] <author> C. E. Leiserson, Z. S. Abuhamdeh, D. C. Douglas, C. R. Feynman, M. N. Ganmukhi, J. V. Hill, W. D. Hillis, B. C. Kuszmaul, M. A. St. Pierre, D. S. Wells, M. C. Wong, S.-W. Yang, and R. Zak, </author> <title> "The network architecture of the connection machine CM-5," </title> <booktitle> in Proc. 4th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 272-285, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: In the following , we briefly describe these systems and how they support collective com-munication. However, the readers are referred to the papers cited in the following text for many of the details that we omit due to space limitations. The CM-5 <ref> [27] </ref> is a descendent of the CM-2, a fine-grained SIMD system. The CM-5 is designed to support synchronized MIMD computing, whereby nodes are allowed to execute independently of one another, with special hardware provided to support relatively fine-grained synchronization and communication among nodes.
Reference: [28] <author> E. A. Brewer and B. C. Kuszmaul, </author> <title> "How to get good performance from the CM-5 data network," </title> <booktitle> in Proceedings of the 1994 International Parallel Processing Symposium, </booktitle> <pages> pp. 858-867, </pages> <year> 1994. </year>
Reference-contexts: The CM-5 wins easily for operations on small messages. However, for large messages, the software overhead of the Delta is sufficiently amortized over the network latencies of the messages that the operations execute faster on the Delta than on the CM-5. Finally, Brewer and Kuszmaul <ref> [28] </ref> suggest that the data and control networks of the CM-5 may be used in tandem to implement fast collective operations. In particular, barriers are used within collective data operations in order to reduce contention within and between different phases of the operation, greatly improving performance.
Reference: [29] <author> R. Ponnusamy, R. Thakur, A. Choudhary, K. Velamakanni, Z. Bozkus, and G. Fox, </author> <title> "Experimental per-formance evaluation of the CM-5," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 19, </volume> <pages> pp. 192-202, </pages> <year> 1993. </year>
Reference-contexts: Since the CM-5 data network supports adaptive routing, the issues associated with the design of unicast-based collective operations for this system are much different than those of deterministically- routed k-ary n-cube systems, on which this paper has focused. Ponnusamy et al <ref> [29] </ref> have experimented with various collective operations and parallel algorithms on the CM-5, concentrating primarily on the effect of contention in the network. Bozkus et al [30] have compared the performance of collective operations as implemented on the CM-5 with the Intel Touchstone Delta.
Reference: [30] <author> Z. Bozkus, S. Ranka, G. Fox, and A. Choudhary, </author> <title> "Performance comparison of the cm-5 and intel touchstone delta for dataparallel operations," </title> <booktitle> in Proceedings of the Fifth IEEE Symposium on Parallel and Distributed Processing, </booktitle> <address> (Dallas, Texas), </address> <pages> pp. 307-310, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Ponnusamy et al [29] have experimented with various collective operations and parallel algorithms on the CM-5, concentrating primarily on the effect of contention in the network. Bozkus et al <ref> [30] </ref> have compared the performance of collective operations as implemented on the CM-5 with the Intel Touchstone Delta. The CM-5 implementations take advantage of the control network, while the Delta implementations rely strictly on software. The CM-5 wins easily for operations on small messages.
Reference: [31] <author> X. Lin, P. K. McKinley, and L. M. Ni, </author> <title> "Deadlock-free multicast wormhole routing in 2D mesh multicomputers," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 5, </volume> <pages> pp. 793-804, </pages> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: Moreover, the dependencies among branches of the tree can lead to deadlock. The nCUBE-2 hypercube includes hardware support for message replication, which is used to implement tree-based broadcast and limited multicast where the destinations form a subcube, but this mechanism is not deadlock-free <ref> [31] </ref>. In order to avoid the disadvantages of the tree-like worms produced by message replication, and yet apply hardware support to the implementation of collective communication operations, the technique Communications Research Group 32 Michigan State University of intermediate reception (IR) has been proposed. <p> Lin, et al <ref> [31] </ref> have used an HP similar to that represented in Figure 30 (b) to develop a family of path-based multicast routing algorithms for 2D mesh networks. In the most basic of these algorithms, termed dual-path, the source node of a multicast generates at most two multi-destination worms.
Reference: [32] <author> D. K. Panda and S. Singal, </author> <title> "Broadcasting in k-ary n-cube wormhole routed networks using path-based routing," </title> <type> Tech. Rep. </type> <institution> TR36., Ohio State University, </institution> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Panda and Singal <ref> [32] </ref> have proposed just such an approach to broadcast in a mesh, which can achieve much better performance than a single-path approach in large meshes. Ho and Kao [33] have also proposed a path-based version of the broadcast algorithm described in Section 4. <p> The routing is identical to that of the algorithm in Section 4, except that each node sends to all its children along a single path in one step. It would appear that path-based collective communication has merits, although this area has received relatively little attention thus far <ref> [32] </ref>. 9 Conclusions As the supercomputing industry has migrated towards parallel architectures, many with distributed memory, wormhole-routed switching has played a major role in reducing the cost of unicast communication among nodes.
Reference: [33] <author> C.-T. Ho and M. Kao, </author> <title> "Optimal broadcast in all-port wormhole-routed hypercubes," </title> <booktitle> in Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1994. </year> <note> accepted to appear. </note>
Reference-contexts: Panda and Singal [32] have proposed just such an approach to broadcast in a mesh, which can achieve much better performance than a single-path approach in large meshes. Ho and Kao <ref> [33] </ref> have also proposed a path-based version of the broadcast algorithm described in Section 4. The routing is identical to that of the algorithm in Section 4, except that each node sends to all its children along a single path in one step.
Reference: [34] <author> C. C. Huang and P. K. McKinley, </author> <title> "Communication issues in parallel computing across ATM networks," </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <year> 1994. </year> <note> accepted to appear. </note> <institution> Communications Research Group 38 Michigan State University </institution>
Reference-contexts: As such systems become more widespread, the need arises to explore how to best support collective operations in this new class of architectures <ref> [34] </ref>. Acknowledgments The authors would like to thank several faculty and students at Michigan State University whose work has either directly or indirectly contributed to this paper: Lionel M. Ni, Abdol-Hossein Esfahanian, Betty H. C. Cheng, Xiaola Lin, Hong Xu, Christian Trefftz, Chengchang Huang, Dan Judd, and Jeremy Uniacke.
References-found: 34

