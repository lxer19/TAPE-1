URL: http://www.neci.nj.nec.com/homepages/wds/bpip1.ps
Refering-URL: http://www.ics.uci.edu/~eppstein/cgt/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: eric and wds @research.NJ.NEC.COM  
Title: Best Play for Imperfect Players and Game Tree Search; part I theory And with a
Author: Eric B. Baum and Warren D. Smith Charles Garrett. 
Date: REVISED September 4, 1995  
Address: 4 Independence Way Princeton NJ 08540  
Affiliation: NEC Research Institute  
Abstract: 1 . The point of game tree search is to insulate oneself from errors in the evaluation function. The standard approach is to grow a full width tree as deep as time allows, and then value the tree as if the leaf evaluations were exact. This has been effective in many games because of the computational efficiency of the Alpha-beta algorithm. But as Bayesians, we want to know the best way to use the inexact statistical information provided by the leaf evaluator to choose our next move. We add a model of uncertainty to the standard evaluation function. Within such a formal model, there is an optimal tree growth procedure and an optimal method of valuing the tree. We describe how to optimally value the tree within our model, and how to efficiently approximate the optimal tree to search. Our tree growth procedure provably approximates the contribution of each leaf to the utility in the limit where we grow a large tree, taking explicit account of the interactions between expanding different leaves. Our algorithms run (under reasonable assumptions) in linear time and hence except for a small constant factor, are as time efficient as Alpha-beta. In a given amount of time our algorithm can thus, at least in principle, grow a tree several times as deep as Alpha-beta along the lines judged most relevant, but at a cost of expanding sufficiently less along lines judged less relevant so that a small constant factor fewer nodes are searched in total. Our algorithm will apportion a greater fraction of its thinking time to more relevant moves, and will appropriately weigh the relevance of each leaf in choosing its move once it is finished searching. Empirical evidence of the efficacy of this approach is presented in part 2. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Anantharaman, M. Campbell, F. Hsu: </author> <title> Singular extensions; adding selectivity to brute force searching, </title> <booktitle> Artificial Intelligence 43 (1990) 99-109 </booktitle>
Reference: [2] <author> Thomas S. Anantharaman: </author> <title> A Statistical Study of Selective Min-Max Search in Computer Chess, </title> <type> (PhD thesis, </type> <institution> Carnegie Mellon University, Computer Science Dept.) </institution> <month> May </month> <year> 1990, </year> <month> CMU-CS-90-173 </month>
Reference-contexts: They grow a tree by expanding at each step the leaf with highest "utility". In Othello tournaments between MGSS* and Alpha-beta ("AB") using a high quality evaluation function, Russell and Wefald found the following results wins nodes sec. MGSS* 24.5 3666 40 AB <ref> [2] </ref> 7.5 2501 23 MGSS* 22 6132 68 AB [3] 10 9104 82 MGSS* 20 12237 170 AB [4] 12 42977 403 MGSS* 16.5 21155 435 AB [5] 15.5 133100 1356 MGSS* 17 45120 1590 AB [6] 15 581433 6863 Russell/Wefald results. (Note: the number in []'s is the depth of
Reference: [3] <author> Thomas S. Anantharaman: </author> <title> Extension heuristics, </title> <note> ICCA Journal 14,2 (June 1991) 47-65. </note>
Reference-contexts: This approach is fascinating, but somewhat ad hoc. Several proposals to extend particular search lines based on heuristic considerations have been incorporated in modern Alpha-beta programs. "Singular extensions", "Threat extensions", and "PV extensions" taken together were estimated to add 86 USCF rating points to the Deep Thought Chess Machine <ref> [3] </ref>. "Quies-ence search" [6] is an important feature of every strong Chess program, although much less important in most other games. Similar extensions as well as Alpha-beta style cutoffs occur automatically in our procedure, and also in the algorithms of Russell and Wefald [28]. <p> In Othello tournaments between MGSS* and Alpha-beta ("AB") using a high quality evaluation function, Russell and Wefald found the following results wins nodes sec. MGSS* 24.5 3666 40 AB [2] 7.5 2501 23 MGSS* 22 6132 68 AB <ref> [3] </ref> 10 9104 82 MGSS* 20 12237 170 AB [4] 12 42977 403 MGSS* 16.5 21155 435 AB [5] 15.5 133100 1356 MGSS* 17 45120 1590 AB [6] 15 581433 6863 Russell/Wefald results. (Note: the number in []'s is the depth of AB search.) 27 Of course, Rivest could test for
Reference: [4] <author> E. B. Baum: </author> <title> On optimal game tree propagation for imperfect players. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence. American Association for Artificial Intelligence 1992. </booktitle>
Reference-contexts: A number of authors have discussed probabilistic tree valuation procedures. Pearl [26] proposed a probabilistic propagation scheme we discuss in more detail in x3.3. Hansson and Mayer [12] advocated probabilistic propagation taking into account dependencies and evaluations at internal nodes in the search tree. Baum <ref> [4] </ref> remarked that the value of a node depends not only on the values of its children but also on an estimate of the `extra information' one would have if the node were the root of a search tree, and proposed a propagation scheme taking into account that the amount of <p> Reasoning along these lines, one discovers that in attempting to value the nodes in a search tree, one must take account of the extra information one will have when playing at the position <ref> [4] </ref>. <p> This extra information can only be modeled probabilistically. 8 See <ref> [4] </ref> for an attempt to take level-dependence into account in the context of scalar valued evaluation functions. 9 Otherwise, we would have had to deal with distributions of distributions of distributions. 10 N.B. the BPIP evaluator returns a probability distribution over the real numbers, not over game outcomes. <p> In Othello tournaments between MGSS* and Alpha-beta ("AB") using a high quality evaluation function, Russell and Wefald found the following results wins nodes sec. MGSS* 24.5 3666 40 AB [2] 7.5 2501 23 MGSS* 22 6132 68 AB [3] 10 9104 82 MGSS* 20 12237 170 AB <ref> [4] </ref> 12 42977 403 MGSS* 16.5 21155 435 AB [5] 15.5 133100 1356 MGSS* 17 45120 1590 AB [6] 15 581433 6863 Russell/Wefald results. (Note: the number in []'s is the depth of AB search.) 27 Of course, Rivest could test for this particular special case.
Reference: [5] <author> E. B. Baum: </author> <title> How a Bayesian approaches games like chess. In Games: Planning and Learning, </title> <booktitle> Papers from the 1993 Fall Symposium, Technical Report FS-93-02, </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park CA. </address> <note> (1993) pp 48-50. </note>
Reference-contexts: In practice, this computation is speeded up dramatically by the "Alpha-beta" algorithm, which allows one to prune off large sections of the search tree while still rigorously guaranteeing to evaluate 1 Portions of the abstract and introduction were previously excerpted in <ref> [5] </ref> 1 the full width tree of given depth. <p> MGSS* 24.5 3666 40 AB [2] 7.5 2501 23 MGSS* 22 6132 68 AB [3] 10 9104 82 MGSS* 20 12237 170 AB [4] 12 42977 403 MGSS* 16.5 21155 435 AB <ref> [5] </ref> 15.5 133100 1356 MGSS* 17 45120 1590 AB [6] 15 581433 6863 Russell/Wefald results. (Note: the number in []'s is the depth of AB search.) 27 Of course, Rivest could test for this particular special case. <p> So assuming our count of "board evaluations" is roughly the same as Russell's count of "nodes," a better move ordering for Russell could have gotten AB [6] quality information in AB <ref> [5] </ref> time, which would make MGSS* and AB [] about equal at equal time budgets, with MGSS* still having the advantage at equal node counts.
Reference: [6] <author> D.F. Beal: </author> <title> A generalized quiescence search algorithm, </title> <booktitle> Artificial Intelligence 43 (1990) 85-98 </booktitle>
Reference-contexts: Several proposals to extend particular search lines based on heuristic considerations have been incorporated in modern Alpha-beta programs. "Singular extensions", "Threat extensions", and "PV extensions" taken together were estimated to add 86 USCF rating points to the Deep Thought Chess Machine [3]. "Quies-ence search" <ref> [6] </ref> is an important feature of every strong Chess program, although much less important in most other games. Similar extensions as well as Alpha-beta style cutoffs occur automatically in our procedure, and also in the algorithms of Russell and Wefald [28]. <p> Second, the standard methods often employ certain highly selective, and highly effective, searches near the leafs of their trees. We are thinking in particular of "capture searches" <ref> [6] </ref> and of the "null move heuristic" [11]. Capture searches recursively expand only the capturing moves from a node and replace the other moves mentally by a mythical "just sit there" move. <p> MGSS* 24.5 3666 40 AB [2] 7.5 2501 23 MGSS* 22 6132 68 AB [3] 10 9104 82 MGSS* 20 12237 170 AB [4] 12 42977 403 MGSS* 16.5 21155 435 AB [5] 15.5 133100 1356 MGSS* 17 45120 1590 AB <ref> [6] </ref> 15 581433 6863 Russell/Wefald results. (Note: the number in []'s is the depth of AB search.) 27 Of course, Rivest could test for this particular special case. <p> Unfortunately, we've recently found out from Russell that MGSS*'s Alpha-beta opponent in these experiments had move ordering turned off. An AB <ref> [6] </ref> Othello program that we wrote, which does have decent move ordering heuristics, does 130000 board evaluations per game. So assuming our count of "board evaluations" is roughly the same as Russell's count of "nodes," a better move ordering for Russell could have gotten AB [6] quality information in AB [5] <p> An AB <ref> [6] </ref> Othello program that we wrote, which does have decent move ordering heuristics, does 130000 board evaluations per game. So assuming our count of "board evaluations" is roughly the same as Russell's count of "nodes," a better move ordering for Russell could have gotten AB [6] quality information in AB [5] time, which would make MGSS* and AB [] about equal at equal time budgets, with MGSS* still having the advantage at equal node counts.
Reference: [7] <author> Hans Berliner: </author> <title> The B* search algorithm: A best first proof procedure, </title> <booktitle> Artificial Intelligence 12 (1979) 23-40 </booktitle>
Reference-contexts: Berliner <ref> [7] </ref> had proposed the B* search which grew a search tree in an attempt to prove that some move is strictly better than all others. The leaf evaluator returned (lower bound, upper bound) intervals.
Reference: [8] <author> M. Blum, R. Floyd, V. Pratt, R. Rivest, R. Tarjan: </author> <title> Time Bounds for selection, </title> <journal> J. Computer System Sci. </journal> <month> 7 </month> <year> (1973) </year> <month> 448-461 </month>
Reference-contexts: Step 5 takes O (L) time and storage by the use of a linear-time selection algorithm [15] <ref> [8] </ref>.
Reference: [9] <author> M. Campbell: </author> <title> The graph-history interaction; on ignoring position history, </title> <booktitle> Proc. ACM National Conf. </booktitle> <year> (1985) </year> <month> 278-280. </month>
Reference: [10] <author> J.H. Condon and K. Thompson: </author> <title> Belle chess hardware, </title> <booktitle> 45-54 in Advances in computer chess 3, </booktitle> <editor> M.R.B. Clarke ed. </editor> <publisher> Pergamon 1982. </publisher>
Reference-contexts: We avoided such problems by using gulps. Palay [25] implemented his algorithm, which he called "PSB*", to solve tactical chess problems. He compared it to Belle, a 130 Knode/sec Alpha-beta machine <ref> [10] </ref>. In order to get a fair comparison, considering the different platforms, Palay "projected" the performance PSB* would have if implemented into 567fi faster (Belle-speed) hardware.
Reference: [11] <author> Chr. Donninger: </author> <title> Null move and deep search, </title> <journal> ICCA Journal 16,3 (Sept. </journal> <year> 1993) </year> <month> 137-143. </month>
Reference-contexts: Second, the standard methods often employ certain highly selective, and highly effective, searches near the leafs of their trees. We are thinking in particular of "capture searches" [6] and of the "null move heuristic" <ref> [11] </ref>. Capture searches recursively expand only the capturing moves from a node and replace the other moves mentally by a mythical "just sit there" move.
Reference: [12] <author> O. Hansson and A. Mayer: </author> <title> Heuristic search as evidential reasoning. </title> <booktitle> In Proceedings of the fifth Workshop on Uncertainty in Artificial Intelligence, </booktitle> <address> Windsor, Ontario. </address>
Reference-contexts: A number of authors have discussed probabilistic tree valuation procedures. Pearl [26] proposed a probabilistic propagation scheme we discuss in more detail in x3.3. Hansson and Mayer <ref> [12] </ref> advocated probabilistic propagation taking into account dependencies and evaluations at internal nodes in the search tree.
Reference: [13] <author> D. S. Nau: </author> <title> Pathology on game trees revisited, and an alternative to minimaxing, </title> <address> AI 21 (1983) 224-244. </address>
Reference: [14] <author> Chi, P-C, D. S. Nau: </author> <title> Comparison of the Minimax and Product Back-up Rules in a Variety of Games, </title> <booktitle> in Search in Artificial Intelligence, </booktitle> <editor> eds. L. Kanal and V. Kumar, </editor> <publisher> Springer Verlag, </publisher> <address> New York,(1989) pp451-471. </address>
Reference-contexts: Expanding according to immediate expansion utility (similar to the "metagreedy" idea of [28]) is not good enough because of its neglect of one's ability to expand other leaves 2 Note: we are not assuming that the probabilities of winning at different nodes are independent, as have <ref> [26, 14] </ref>. We are assuming that the errors in our estimates of these quantities are independent. Tests of the validity of our independence assumptions in several games are reported in Part 2. 2 before moving. <p> The same propagation formulas 3-5 are used as in BPIP, a fact which has led some unwary readers of previous drafts to conclude erroneously that BPIP is NPU. NPU is equivalent to the probability propagation proposal of Pearl [26], experimentally studied by Chi and Nau <ref> [14] </ref> and others. NPU yields at each interior node of the tree, an estimate of the probability that is a win with perfect play. You then move to the child of the root with the largest chance of being a win. Both these philosophies are wrong in principle.
Reference: [15] <author> R. Floyd and R. Rivest: </author> <title> Expected time bounds for selection, </title> <journal> Commun. </journal> <note> ACM 18,3 (March 1975) 165-173 </note>
Reference-contexts: Step 5 takes O (L) time and storage by the use of a linear-time selection algorithm <ref> [15] </ref> [8].
Reference: [16] <author> I.J. </author> <title> Good: A five year plan for automatic chess. </title> <booktitle> Machine Intelligence 2 (1968) 89-118 </booktitle>
Reference-contexts: Palay's main interest was in guiding tree growth, but according to very different criteria than the present paper. I. J. Good mentioned some vague notions of utility in tree searching in his "5 year plan" <ref> [16] </ref>. Russell and Wefald [28] gave a clearer proposal of what optimal search might mean. They also first proposed the use of a utility based stopping condition 4 , and they used evaluation functions which return distributions. However they did not propose or maintain a Bayesian model of one's uncertainty.
Reference: [17] <author> Richard M. Karp, Michael Luby, and Neal Madras: </author> <title> Monte-Carlo Approximation Algorithms for Enumeration Problems, </title> <note> Journal of Algorithms 10 (1989) 429-448 </note>
Reference-contexts: QED. Note that we can approximate the exact CDFs at all nodes, taking full account of the DAG dependencies, by Monte Carlo evaluation. (See also <ref> [17] </ref>.) This may be useful in evaluating our move choice, but we believe is likely to be too slow in convergence to be useful in deciding how to expand the tree. The reason for this is the following. <p> value of a game DAG. 24 One might imagine a more sophisticated Monte Carlo procedure which evaluated the utility of leaf L by sampling uniformly directly from the region in which the value of leaf L is relevant and then computed the integral by some procedure analogous to that of <ref> [17] </ref>, but we have been unable to construct a procedure along these lines which is efficient and rapidly mixing. 25 Actually the situation is worse than this for two reasons. First we must sample from such regions many times because of the inherent noise in the Monte Carlo procedure.
Reference: [18] <author> Feng-hsiung Hsu: </author> <title> Large Scale Parallelization of Alpha-Beta Search: An Algorithmic and Architectural Study with Computer Chess, </title> <type> (PhD thesis) Tech. </type> <institution> Rept. CMU-CS-90-108 (Feb 1990) School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh PA 15213 </address>
Reference: [19] <author> Han La Poutre and Warren D. Smith: </author> <title> Approximation of staircases by staircases, Work in progress, NECI, 4 Independence Way, </title> <address> Princeton NJ 08540. </address> <month> 25 </month>
Reference: [20] <author> Kai-Fu Lee and Sanjoy Mahajan: </author> <title> The development of a world class Othello program, </title> <note> Artificial Intelli--gence 43 (1990) 21-36 </note>
Reference-contexts: This effect apparently happened in Lee and Mahajan's program "Bill," since two matrices given on page 32 of <ref> [20] </ref> have elements identical to within 1%... 5 Step 3. Compress the staircase-CDF probability distributions in each bin, into staircase CDFs optimally approximating the original staircases, but having only a small number of stairsteps ([19] and see part II).
Reference: [21] <author> Kai-Fu Lee and Sanjoy Mahajan: </author> <title> A pattern classification approach to evaluation function learning, </title> <booktitle> Artificial Intelligence 36 (1988) 1-25 </booktitle>
Reference-contexts: This set may be thought of as a staircase CDF probability distribution, each jump of which corresponds to a single data point, and in which all jumps have equal magnitude. 5 Lee and Mahajan proposed an elegant-sounding quadratic fitting procedure as an alternative to a linear least-squares fit <ref> [21] </ref>.
Reference: [22] <author> D.A. McAllester: </author> <title> Conspiracy numbers for min max search, </title> <booktitle> Artificial Intelligence 35 (1988) 287-310 </booktitle>
Reference-contexts: We are assuming that the errors in our estimates of these quantities are independent. Tests of the validity of our independence assumptions in several games are reported in Part 2. 2 before moving. For example, frequently a "conspiracy" among many leaves is necessary to affect the best move choice <ref> [22] </ref>. We may similarly define the immediate expansion utility of any subset of the leaves. In fact, we can compute in linear time the decision theoretic utility U of expanding the entire game tree. <p> We briefly review here the relationship to the present paper of some modern proposals. The work of Palay [25], Russell and Wefald [28], McAllester <ref> [22] </ref>, and Rivest [27] is discussed in more detail in x12. Palay was the first author to propose the use of distribution valued evaluation functions and also proposed the same equations for propagation of probability distributions that we do. <p> McAllester <ref> [22] </ref> suggested a method of "conspiracies" in the negamax tree-searching philosophy.
Reference: [23] <author> Monroe Newborn: </author> <title> A hypothesis concerning the strength of chess programs, </title> <note> ICCA Journal 8,4 (1985) 209-215. </note>
Reference-contexts: What penalty function is the right one? Here is a derivation of one formula. We start with the table given by Newborn <ref> [23] </ref> of the probability p (d) that the the chess machine Belle, when searching d ply deep, will prefer a different move to the one it prefers at d 1 ply. (This data came from 447 searches performed by Belle while extending its opening book into the early middle game, and
Reference: [24] <author> A.J. Palay: </author> <title> The B* search algorithm new results, </title> <booktitle> Artificial Intelligence 19 (1982) 145-163. </booktitle>
Reference-contexts: The leaf evaluator returned (lower bound, upper bound) intervals. Berliner used heuristics to decide between attempting to "prove best" and attempting to "disprove rest." Palay's first proposal <ref> [24] </ref> was to use probabilistic information in place of such heuristics to guide the search for such a proof, and he experimentally showed that his use of such information led to improvements over a similar proof-seeking algorithm which did not use probabilistic information.
Reference: [25] <author> A.J. Palay: </author> <title> Searching with probabilities, </title> <publisher> Pitman 1985 </publisher>
Reference-contexts: Such an evaluation function may be readily learned. In essence one may empirically measure the likelihood of various opinion changes as a function of various features. Under our assumption that these distributions are independent, BPIP yields a certain formula <ref> [25] </ref> for propagating these distributions up the tree which associates to each node in the tree the probability node 's negamax value is x given that a value is assigned to each leaf from its distribution. <p> We briefly review here the relationship to the present paper of some modern proposals. The work of Palay <ref> [25] </ref>, Russell and Wefald [28], McAllester [22], and Rivest [27] is discussed in more detail in x12. Palay was the first author to propose the use of distribution valued evaluation functions and also proposed the same equations for propagation of probability distributions that we do. <p> In addition to the generalized density m (x), it will be convenient to associate two kinds of cumulative distribution functions (CDFs) with each node m <ref> [25] </ref>: c (m) (x) = i; x (m) p i = Prob (y xjy is distributed as above) (1) c (m) (x) = i; x (m) p i = Prob (y xjy is distributed as above) : (2) Observe these CDF's are "staircase" functions. <p> The formula for the CDF of a parent node in terms of its b children is <ref> [25] </ref> c (parent) (x) = b Y c (child i ) (x) (3) if we are doing negamaxing, c (parent) (x) = b Y c (child i ) (x) (4) if we are doing Maxing, and c (parent) (x) = b X i c (child i ) (x) (5) if we <p> MGSS* becomes completely helpless (terminates) if the move choice conspiracy number ever exceeds 1. Furthermore we have argued in x7.4 that the metagreedy approximation they make is a poor guide to the leaf relevance even when the conspiracy number is 1. 12.4 Palay Palay <ref> [25] </ref> proposed a probability based method propagating staircase CDFs with the same formula (3) we use. <p> Later in <ref> [25] </ref> (see p. 72), Palay relaxed his goal of proving that some move was better than all the alternatives, replacing it with the goal of stopping searching when the confidence that some move "dominated" all the alternatives, exceeded some fixed threshhold. <p> Rather than determine which leaf of the tree was best to expand (under his criteria), Palay marched down from the root at each node choosing the child most likely to be best until he reached a leaf. As he realized (p.13 & 85 <ref> [25] </ref>) this greedy procedure need not pick the globally best leaf, nor even a good approximation. Palay was forced into this expedient, however, because examining all the leaves to find the best would have caused his runtime to grow superlinearly. We avoided such problems by using gulps. Palay [25] implemented his <p> & 85 <ref> [25] </ref>) this greedy procedure need not pick the globally best leaf, nor even a good approximation. Palay was forced into this expedient, however, because examining all the leaves to find the best would have caused his runtime to grow superlinearly. We avoided such problems by using gulps. Palay [25] implemented his algorithm, which he called "PSB*", to solve tactical chess problems. He compared it to Belle, a 130 Knode/sec Alpha-beta machine [10]. In order to get a fair comparison, considering the different platforms, Palay "projected" the performance PSB* would have if implemented into 567fi faster (Belle-speed) hardware.
Reference: [26] <author> J. Pearl: </author> <title> Heuristics, </title> <publisher> Addison-Wesley 1984 </publisher>
Reference-contexts: Expanding according to immediate expansion utility (similar to the "metagreedy" idea of [28]) is not good enough because of its neglect of one's ability to expand other leaves 2 Note: we are not assuming that the probabilities of winning at different nodes are independent, as have <ref> [26, 14] </ref>. We are assuming that the errors in our estimates of these quantities are independent. Tests of the validity of our independence assumptions in several games are reported in Part 2. 2 before moving. <p> Our approach of stating a Bayesian model of search, and then giving a provably efficient algorithm approximating Best Play for Imperfect Players can thus be seen as formalizing this line of research. A number of authors have discussed probabilistic tree valuation procedures. Pearl <ref> [26] </ref> proposed a probabilistic propagation scheme we discuss in more detail in x3.3. Hansson and Mayer [12] advocated probabilistic propagation taking into account dependencies and evaluations at internal nodes in the search tree. <p> If two sibling positions are assigned by an ordinary evaluation function a value :5, meaning that the probability of winning in that position is :5, it could be unwise to assume that these are independent and assign a probability of winning of .75 to their parent,as is sometimes advocated <ref> [26] </ref>. Our distributions, however, are over the error in the evaluation function, or to put it another way, are distributions over changes in our opinion about the position that would arise from future expansion. <p> Such a distribution of errors in the evaluation function, would seem to be inherently less correlated than the evaluation function itself. 3.2 BPIP-DFISA When it was first remarked that any evaluation function returns probabilistic information <ref> [26] </ref>, the question arose whether minimax is the correct way to propagate this information up a tree. One might instead propose a rule for combination of probabilities [26]. <p> be inherently less correlated than the evaluation function itself. 3.2 BPIP-DFISA When it was first remarked that any evaluation function returns probabilistic information <ref> [26] </ref>, the question arose whether minimax is the correct way to propagate this information up a tree. One might instead propose a rule for combination of probabilities [26]. This reasoning correctly asserts that a position with 100 alternative moves, each of which has independently :1 probability of leading to a won game, is almost certainly a won position. <p> The same propagation formulas 3-5 are used as in BPIP, a fact which has led some unwary readers of previous drafts to conclude erroneously that BPIP is NPU. NPU is equivalent to the probability propagation proposal of Pearl <ref> [26] </ref>, experimentally studied by Chi and Nau [14] and others. NPU yields at each interior node of the tree, an estimate of the probability that is a win with perfect play. You then move to the child of the root with the largest chance of being a win. <p> In negamaxing of single numerical values, the graph of the influence function consists of three line segments, the outer two being constant and the inner one being of slope 1. The t-values of the corners are the "ff-fi window" <ref> [26] </ref>. In this section we describe how we may associate to each jump in the CDF at each node in our search tree an influence coefficient. Let inco F [i] be the influence coefficient at the i-th jump at some node for function F .
Reference: [27] <author> R.L. Rivest: </author> <title> Game tree searching by min max approximation, </title> <booktitle> Artificial Intelligence 34 (1988) 77-96 </booktitle>
Reference-contexts: We briefly review here the relationship to the present paper of some modern proposals. The work of Palay [25], Russell and Wefald [28], McAllester [22], and Rivest <ref> [27] </ref> is discussed in more detail in x12. Palay was the first author to propose the use of distribution valued evaluation functions and also proposed the same equations for propagation of probability distributions that we do. <p> So in a sense we take account of the integral over all (probabilistically weighted) conspiracies which a leaf participates in, and expand the leaf which is most likely to give useful information as we expand. 12.2 Rivest's suggestion R. Rivest <ref> [27] </ref> suggested a method where the "max" in negamaxing is replaced by an L p mean. The root value then depends differentiably on the leaf values. One may find the gradient of the root value with respect to the leaf values, by applying the chain rule.
Reference: [28] <author> S. Russell and E. Wefald: </author> <title> Do the Right Thing, </title> <note> MIT Press 1991 (see especially chapter 4) </note>
Reference-contexts: The "immediate expansion utility" of a leaf is the expected gain that would accrue if one expanded that one leaf and then chose one's move, rather than choosing one's move with no expansion. Expanding according to immediate expansion utility (similar to the "metagreedy" idea of <ref> [28] </ref>) is not good enough because of its neglect of one's ability to expand other leaves 2 Note: we are not assuming that the probabilities of winning at different nodes are independent, as have [26, 14]. We are assuming that the errors in our estimates of these quantities are independent. <p> We briefly review here the relationship to the present paper of some modern proposals. The work of Palay [25], Russell and Wefald <ref> [28] </ref>, McAllester [22], and Rivest [27] is discussed in more detail in x12. Palay was the first author to propose the use of distribution valued evaluation functions and also proposed the same equations for propagation of probability distributions that we do. <p> Palay's main interest was in guiding tree growth, but according to very different criteria than the present paper. I. J. Good mentioned some vague notions of utility in tree searching in his "5 year plan" [16]. Russell and Wefald <ref> [28] </ref> gave a clearer proposal of what optimal search might mean. They also first proposed the use of a utility based stopping condition 4 , and they used evaluation functions which return distributions. However they did not propose or maintain a Bayesian model of one's uncertainty. <p> Similar extensions as well as Alpha-beta style cutoffs occur automatically in our procedure, and also in the algorithms of Russell and Wefald <ref> [28] </ref>. Our approach of stating a Bayesian model of search, and then giving a provably efficient algorithm approximating Best Play for Imperfect Players can thus be seen as formalizing this line of research. A number of authors have discussed probabilistic tree valuation procedures. <p> expression above as 6 U S = U all leaves X " C S X (m 1 ; m 2 ; :::; m n ) = U all leaves U S ; (12) 16 The procedure of expanding the leaf with optimal SLEU is akin to the procedure advocated by <ref> [28] </ref> in that leaves are valued as if there was no possibility for future expansion. They call this approach "metagreedy." 12 thus 6 U S is just the difference between the utility of expanding all leaves and the utility of expanding all leaves other than S. <p> While this is less impressive than it at first seemed, it still gives us grounds for optimism about BPIP, since BPIP's runtime only grows linearly with node count. Russell and Wefald also experimented with MGSS2, a version permitting partial node expansion, and got good preliminary results (table p109 of <ref> [28] </ref>). The logic behind MGSS2 seems to us seriously flawed, however. We omit discussion here. Russell and Wefald's MGSS* results are evidence for the power of the ideas of "expansion utility" and the use of probability distributions at leaves to describe value changes after expansion.
Reference: [29] <author> J. Schaeffer: </author> <title> Conspiracy numbers, </title> <booktitle> Artificial Intelligence 43 (1990) 67-84 </booktitle>
Reference: [30] <author> J. Schaeffer: </author> <title> Experiments in search and knowledge, </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Waterloo, </institution> <year> 1986. </year>
Reference-contexts: One continues until either S is unwieldily large, or the conspiracy number, i.e. the cardinality of the smallest conspiracy, is greater than some desired value. McAllester's tree-shaping method was found experimentally to work well in tactical chess middlegames by J. Schaeffer <ref> [30] </ref>, but it worked badly in "positional" chess and in forced mates in which moves near the end of the forced mate line were "any"s. We now remark that large conspiracy number occurs even if one makes no special effort to shape the tree.
Reference: [31] <author> G. Schrufer: </author> <title> Presence and Absence of Pathology on Game Trees, </title> <editor> in D.F. Beal, ed., </editor> <booktitle> Advances in Computer Chess 4, </booktitle> <publisher> (Pergamon, Oxford, </publisher> <pages> 1986) pp 101-112. </pages>
Reference-contexts: A similar result holds in Schrufer's <ref> [31] </ref> more realistic probabilistic model of b-uniform depth-d game trees with Boolean leaf values. Indeed, Schrufer's theoretical results can be viewed as proving that, in his model, exponentially large conspiracy numbers occur if and only if negamax search is not pathological.
Reference: [32] <author> C.E. Shannon: </author> <title> Programming a computer for playing chess, </title> <journal> Philos. </journal> <note> Magazine 41,7 (1950) 256-275 </note>
Reference-contexts: 1 Introduction The standard method for computers to play games like chess, first suggested by Shannon <ref> [32] </ref> , is to grow a full width game tree as deeply as time permits, heuristically assign a numerical evaluation to each leaf, propagate these numbers up the tree by minimax, and choose as the "best move" the child of the root with the largest number. <p> BPIP's advantage in our experiments increases rapidly with the complexity of the game and the amount of time allotted to each contestant. 1.1 Relationship to Previous work There is an extensive history of proposals to selectively grow trees in the most interesting directions, dating back to Shannon's original paper <ref> [32] </ref>. We briefly review here the relationship to the present paper of some modern proposals. The work of Palay [25], Russell and Wefald [28], McAllester [22], and Rivest [27] is discussed in more detail in x12.
Reference: [33] <author> Warren D. Smith: </author> <title> Fixed point for negamaxing probability distributions on regular trees, </title> <type> NECI technical report </type>
Reference-contexts: In an actual game tree when we expand a leaf the probability distribution at that leaf generally gets sharper, but we are assuming that it actually sharpens to a spike. Some justification for this assumption of drastic shrinking, and hence of DFISA, is provided by the results of <ref> [33] </ref>. In any case we are only using this assumption to decide which order to expand leaves in. If we continue to expand to some depth below a node, we will in practice achieve substantial sharpening of the distribution at that node.
Reference: [34] <author> Warren D. Smith, Charles Garrett, Eric Baum, </author> <title> Rico Tudor: Best Play for Imperfect Players and Game Tree Search; part II experiments. </title>
Reference-contexts: The companion paper ("part II" <ref> [34] </ref>) reports experimental results on games including Kalah, "mod-9 Connect4," and Othello. <p> For experimental results on a variety of games see the companion paper <ref> [34] </ref>. We conclude with the remark that this still has little to do with how humans play games. The computer science approach (which we are attempting to perfect) has since Shannon basically regarded a game as defined by its game tree.
Reference: [35] <author> Alex & Barbara Szabo: </author> <title> The technology curve revisited, </title> <note> ICCA Journal 11,1 (1988) 14-20 </note>
Reference-contexts: .295 .260 .226 .177 .181 w1 p (d) 2633 2133 1895 1621 1580 1335 1178 923 872 718 769 389 118 29 2 0 It is not clear what formula should be used to describe p (d); David Levy once suggested c 1 d 2 but we prefer the Szabos's <ref> [35] </ref> suggestion exp (c 1 c 2 d), for some positive real constants c 1 , c 2 . Assume a depth-d search requires time B d and assume that any favorite move change represents some constant 20 utility increment (U ) typ .
Reference: [36] <author> Robert E. Tarjan,: </author> <title> Data structures and network algorithms, </title> <note> SIAM 1983 </note>

References-found: 36

