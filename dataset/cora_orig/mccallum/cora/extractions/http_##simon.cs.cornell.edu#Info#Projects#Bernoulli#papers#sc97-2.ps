URL: http://simon.cs.cornell.edu/Info/Projects/Bernoulli/papers/sc97-2.ps
Refering-URL: 
Root-URL: 
Email: vsm@cs.cornell.edu  anne@tc.cornell.edu  
Title: MultiMATLAB: Integrating MATLAB with High-Performance Parallel Computing  
Author: Vijay Menon Anne E. Trefethen 
Address: Ithaca, NY 14853  Ithaca, NY 14853  
Affiliation: Department of Computer Science Cornell University  Cornell Theory Center Cornell University  
Abstract: Matlab is the most popular scientific computing environment available on unipro-cessors today. Unfortunately, no such environment is currently available for multiprocessors. MultiMatlab [1] is a general extension of the Matlab environment to any distributed memory multiprocessors. This paper presents a new MultiMatlab system designed to provide high-performance on multiprocessors while maintaining the functionality and usability of the Matlab environment. This system will enable users to access high-performance parallel routines from within the Matlab environment, to extend the environment with new parallel routines, and to use these routines to develop parallel applications with the Matlab language. We discuss a general MultiMatlab architecture, present two implementations based upon the MPI communication standard [2], and demonstrate the use of this system. Preliminary results indicate that the MultiMatlab system can offer the full performance of the underlying multiprocessor to the Matlab environment.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. E. Trefethen, V. S. Menon, C. C. Chang, G. J. Czajkowski, C. Myers, and L. N. Trefethen. </author> <title> MultiMatlab: Matlab on multiple processors. </title> <type> Technical Report 96-239, </type> <institution> Cornell Theory Center, </institution> <year> 1996. </year> <note> http://www.cs.cornell.edu/Info/People/lnt/multimatlab.html. </note>
Reference-contexts: Unfortunately, there is no computing environment that provides the usability and functionality of Matlab for multiprocessors. We argue that such an environment is realizable on a generic distributed memory multiprocessor using readily available software. In <ref> [1] </ref>, Trefethen, et al. present MultiMatlab, a general extension of the Matlab environment to distributed memory multiprocessors, as a first step in this direction. In particular, we demonstrate that MultiMatlab achieves the following goals. * Familiarity: The original Matlab environment is preserved. <p> From the beginning, the primary focus was programmability within Matlab with only coarse-grain performance. These efforts culminated in the first MultiMatlab architecture <ref> [1] </ref>. The first MultiMatlab architecture is different from the newer one described in this paper. It is more tightly coupled to MPICH [5] and its underlying P4 [7] layer resulting in some performance limitations.
Reference: [2] <author> Message Passing Interface Forum. </author> <title> A Message-Passing Interface Standard, </title> <month> May </month> <year> 1994. </year> <note> http://www.mcs.anl.gov/mpi. </note>
Reference-contexts: The key component in this architecture is the MultiMatlab interface module. The interface module, shown in Figure 2, is responsible for initializing an underlying communication layer, such as MPI <ref> [2] </ref>, PVM [3], BLACS [4], or any other package available on the platform, and exposing it to the rest of the system. MEX routines, also shown in the Figure 2, are executables originally written in C or Fortran that may be run directly from Matlab.
Reference: [3] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM: Parallel Virtual Machine. A Users' Guide and Tutorial for Networked Parallel Computing. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: The key component in this architecture is the MultiMatlab interface module. The interface module, shown in Figure 2, is responsible for initializing an underlying communication layer, such as MPI [2], PVM <ref> [3] </ref>, BLACS [4], or any other package available on the platform, and exposing it to the rest of the system. MEX routines, also shown in the Figure 2, are executables originally written in C or Fortran that may be run directly from Matlab.
Reference: [4] <author> J. Dongarra and R. C. Whaley. </author> <title> A user's guide to the BLACS. </title> <type> Technical Report CS-95-281, </type> <institution> Department of Computer Science, University of Tennessee, Knoxville, TN, </institution> <year> 1995. </year> <note> Also LAPACK Working Note No.94. </note>
Reference-contexts: The key component in this architecture is the MultiMatlab interface module. The interface module, shown in Figure 2, is responsible for initializing an underlying communication layer, such as MPI [2], PVM [3], BLACS <ref> [4] </ref>, or any other package available on the platform, and exposing it to the rest of the system. MEX routines, also shown in the Figure 2, are executables originally written in C or Fortran that may be run directly from Matlab.
Reference: [5] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum. </author> <title> A high-performance, portable implementation of the MPI message passing interface standard. </title> <booktitle> Parallel Computing, </booktitle> <month> July </month> <year> 1996. </year>
Reference-contexts: One implementation of MultiMatlab, designed to run on a generic network of Unix workstations, is based upon MPICH <ref> [5] </ref>, a popular public domain version of MPI developed at Argonne National Laboratory and Mississippi State University. The second was designed for the IBM SP2, a modern high performance distributed memory multiprocessor. <p> From the beginning, the primary focus was programmability within Matlab with only coarse-grain performance. These efforts culminated in the first MultiMatlab architecture [1]. The first MultiMatlab architecture is different from the newer one described in this paper. It is more tightly coupled to MPICH <ref> [5] </ref> and its underlying P4 [7] layer resulting in some performance limitations. It cannot take advantage of vendor specific implementations of MPI, such as MPI-F [6], which can offer much better performance. Furthermore, it does not permit extensions via parallel MEX routines as described in Section 4.
Reference: [6] <author> H. Franke. </author> <title> MPI programming environment for IBM SP1/SP2. </title> <booktitle> In ICDCS '95, </booktitle> <address> Vancou-ver, </address> <year> 1995. </year> <month> 16 </month>
Reference-contexts: The second was designed for the IBM SP2, a modern high performance distributed memory multiprocessor. This implementation is based upon MPI-F <ref> [6] </ref>, IBM's proprietary version of MPI specifically optimized for the SP2. 3.1 MPICH: Network of Workstations The MPICH implementation of MultiMatlab is based upon the P4 [7] communication subsystem, allowing it to run over a network of workstations connected by TCP/IP. <p> The first MultiMatlab architecture is different from the newer one described in this paper. It is more tightly coupled to MPICH [5] and its underlying P4 [7] layer resulting in some performance limitations. It cannot take advantage of vendor specific implementations of MPI, such as MPI-F <ref> [6] </ref>, which can offer much better performance. Furthermore, it does not permit extensions via parallel MEX routines as described in Section 4. It does, however, already provide similar functionality to that shown in Table 1.
Reference: [7] <author> R. Butler and E. Lusk. </author> <title> Monitors, messages, and clusters: The P4 parallel programming system. </title> <booktitle> Parallel Computing, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: The second was designed for the IBM SP2, a modern high performance distributed memory multiprocessor. This implementation is based upon MPI-F [6], IBM's proprietary version of MPI specifically optimized for the SP2. 3.1 MPICH: Network of Workstations The MPICH implementation of MultiMatlab is based upon the P4 <ref> [7] </ref> communication subsystem, allowing it to run over a network of workstations connected by TCP/IP. In particular, this implementation should run on any platform providing both Matlab and MPICH. To date, we have successfully run it on IBM RS6000 and Sun Sparc workstations and the IBM SP2. <p> From the beginning, the primary focus was programmability within Matlab with only coarse-grain performance. These efforts culminated in the first MultiMatlab architecture [1]. The first MultiMatlab architecture is different from the newer one described in this paper. It is more tightly coupled to MPICH [5] and its underlying P4 <ref> [7] </ref> layer resulting in some performance limitations. It cannot take advantage of vendor specific implementations of MPI, such as MPI-F [6], which can offer much better performance. Furthermore, it does not permit extensions via parallel MEX routines as described in Section 4.
Reference: [8] <author> IBM Corporation. </author> <title> IBM Parallel Engineering and Scientific Subroutine Library. Release 2. Guide and Reference, </title> <year> 1996. </year>
Reference-contexts: However, modifying and recompiling libraries is not a viable option when source code is not available. As an alternative, parallel libraries could be pushed down to the underlying communication layer. For example, on the IBM SP2, we can push Parallel ESSL <ref> [8] </ref>, a parallel numerical library optimized for that platform, into the communication layer along with MPI-F. As a result, parallel MEX routines on the SP2 may directly call Parallel ESSL functions in addition to MPI functions. In this case, the library is not reimplemented as a MEX routine.
Reference: [9] <author> L. S. Blackford, J. Choi, A. Cleary, J. Demmel, I. Dhillon, J. Dongarra, S. Hammarling, G. Henry, A. Petitet, K. Stanley, D. W. Walker, and R. C. Whaley. </author> <title> ScaLAPACK: A portable linear algebra library for distributed memory computers Design issues and performance. In Supercomputing '96. </title> <journal> ACM SIGARCH and IEEE Computer Society, </journal> <note> 1996. http://www.supercomp.org/sc96/proceedings/sc96proc/dongarra/index.htm. </note>
Reference-contexts: Parallel MEX routines can then perform high-level Matlab operations on these distributed matrix structures. In fact, computations could actually be performed by a library such ScaLAPACK <ref> [9] </ref> or PLAPACK [10]. Distributed matrix structures permit the MultiMatlab system to provide users an interactive interface to such parallel numerical libraries. In addition, implementing these structures with Matlab 5.0's object-oriented features permits overloading of standard Matlab operations for distributed matrices.
Reference: [10] <author> R. van de Geijn. </author> <title> Using PLAPACK: Parallel Linear Algebra Package. </title> <publisher> The MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: Parallel MEX routines can then perform high-level Matlab operations on these distributed matrix structures. In fact, computations could actually be performed by a library such ScaLAPACK [9] or PLAPACK <ref> [10] </ref>. Distributed matrix structures permit the MultiMatlab system to provide users an interactive interface to such parallel numerical libraries. In addition, implementing these structures with Matlab 5.0's object-oriented features permits overloading of standard Matlab operations for distributed matrices.
Reference: [11] <author> B.T. Smith, J.M. Boyle, Y. Ikebe, V.C. Klema, and C.B. Moler. </author> <title> Matrix Eigensystem Routines: EISPACK Guide. </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <note> second edition, </note> <year> 1970. </year>
Reference-contexts: In addition, implementing these structures with Matlab 5.0's object-oriented features permits overloading of standard Matlab operations for distributed matrices. The result is an intuitive and familiar interface to distributed matrices and corresponding parallel routines. As Matlab was originally developed as an interface to EISPACK <ref> [11] </ref> and LINPACK [12], linear algebra packages for uniproces 10 sors, this paradigm provides a natural extension of functionality to multiprocessors. 6 An Example: Conjugate Gradients r = b; k = 0; while ((k &lt; kmax)) k = k+1; p = r; else beta = rho/oldrho; p = r + beta*p;
Reference: [12] <author> J.J. Dongarra, J.R. Bunch, C.B. Moler, and G.W. Stewart. </author> <title> LINPACK Users Guide. </title> <address> Philadelphia, PA, </address> <year> 1978. </year>
Reference-contexts: In addition, implementing these structures with Matlab 5.0's object-oriented features permits overloading of standard Matlab operations for distributed matrices. The result is an intuitive and familiar interface to distributed matrices and corresponding parallel routines. As Matlab was originally developed as an interface to EISPACK [11] and LINPACK <ref> [12] </ref>, linear algebra packages for uniproces 10 sors, this paradigm provides a natural extension of functionality to multiprocessors. 6 An Example: Conjugate Gradients r = b; k = 0; while ((k &lt; kmax)) k = k+1; p = r; else beta = rho/oldrho; p = r + beta*p; end alpha =
Reference: [13] <author> G. Golub and C. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1996. </year>
Reference-contexts: The computational core of this method is a single matrix-vector multiplication at each iteration. The sequential Matlab code, adapted from <ref> [13] </ref>, is shown in Figure 4a. First, it is worthwhile to examine the performance of the sequential algorithm in Matlab and C. Figure 5 compares the performance of both versions on different matrix sizes.
Reference: [14] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen, </author> <title> editors. LAPACK Users' Guide. Second Edition. </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1995. </year>
Reference-contexts: Moreover, this gap steadily increases along with the matrix size. This degradation in conjugate gradients is due to the poor cache performance of Matlab's matrix-vector multiplication for large matrices. It should be noted that, in the future, Matlab's multiplication routine will be redesigned to use LAPACK <ref> [14] </ref>, a high-performance numerical library for uniprocessors. In the meantime, however, Matlab imposes a significant overhead compared to C code. the SPMD message passing described in Section 5.2. In this case, the matrix A and the different vectors are each distributed by row over all processes as in Figure 6.
Reference: [15] <author> J. Hollingsworth, K. Liu, and P. Pauca. </author> <title> Parallel Toolbox for Matlab. </title> <institution> Wake Forest University, </institution> <year> 1996. </year> <note> http://www.mthcsc.wfu.edu/pt/pt.html. </note>
Reference-contexts: It does, however, already provide similar functionality to that shown in Table 1. It has been in use at the Cornell Theory Center since 1996 and is now available to the general public at http://www.tc.cornell.edu/~anne/projects/MM.html. We are aware of two other projects <ref> [15, 16, 17] </ref> that also attempt to extend the Matlab environment to multiprocessors. Both provide systems built upon PVM, and both expose PVM functionality at the Matlab level. These systems are also designed for coarse-grain distributed computing.
Reference: [16] <author> S. Pawletta, T. Pawletta, and W. Drewelow. </author> <title> Distributed and parallel simulation in an interactive environment. </title> <type> Technical report, </type> <institution> University of Rostock, Germany, </institution> <year> 1995. </year> <type> Preprint. </type>
Reference-contexts: It does, however, already provide similar functionality to that shown in Table 1. It has been in use at the Cornell Theory Center since 1996 and is now available to the general public at http://www.tc.cornell.edu/~anne/projects/MM.html. We are aware of two other projects <ref> [15, 16, 17] </ref> that also attempt to extend the Matlab environment to multiprocessors. Both provide systems built upon PVM, and both expose PVM functionality at the Matlab level. These systems are also designed for coarse-grain distributed computing.
Reference: [17] <author> S. Pawletta, T. Pawletta, and W. Drewelow. </author> <title> Comparison of parallel simulation techniques Matlab/psi. Simulation News Europe, </title> <booktitle> 13 </booktitle> <pages> 38-39, </pages> <year> 1995. </year>
Reference-contexts: It does, however, already provide similar functionality to that shown in Table 1. It has been in use at the Cornell Theory Center since 1996 and is now available to the general public at http://www.tc.cornell.edu/~anne/projects/MM.html. We are aware of two other projects <ref> [15, 16, 17] </ref> that also attempt to extend the Matlab environment to multiprocessors. Both provide systems built upon PVM, and both expose PVM functionality at the Matlab level. These systems are also designed for coarse-grain distributed computing.
Reference: [18] <author> L. De Rose and D. Padua. </author> <title> A Matlab to Fortran 90 translator and its effectiveness. </title> <booktitle> In 10th ACM International Conference on Supercomputing, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: In particular, a number of projects have studied and implemented the compilation of Matlab programs into C, Fortran, and C++ <ref> [18, 19, 20, 21, 22] </ref>. In addition, a couple of projects relate to compilation of Matlab programs into high performance parallel code. The Falcon project [18] at Illinois is currently studying compilation of Matlab into High Performance Fortran [23]. <p> In particular, a number of projects have studied and implemented the compilation of Matlab programs into C, Fortran, and C++ [18, 19, 20, 21, 22]. In addition, a couple of projects relate to compilation of Matlab programs into high performance parallel code. The Falcon project <ref> [18] </ref> at Illinois is currently studying compilation of Matlab into High Performance Fortran [23]. RTExpress [24], from ISI, provides a system that facilitates the compilation of Matlab into C with MPI, ScaLAPACK, and other library calls. <p> A further step in this direction would be the automatic parallelization of programs written in the Matlab language. The same technology available in High Performance Fortran compilers should be applicable to Matlab programs with similar annotations. In fact, related work <ref> [18] </ref> suggests that a Matlab compiler may be able use the higher level of information in Matlab programs to generate even better parallel code. This technology would significantly simplify the process of generating efficient parallel MEX routines in Matlab.
Reference: [19] <author> L. De Rose, K. Gallivan, E. Gallopoulos, B. Marsolf, and D. Padua. </author> <title> FALCON: A Matlab interactive restructuring compiler. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 269-288. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1995. </year>
Reference-contexts: In particular, a number of projects have studied and implemented the compilation of Matlab programs into C, Fortran, and C++ <ref> [18, 19, 20, 21, 22] </ref>. In addition, a couple of projects relate to compilation of Matlab programs into high performance parallel code. The Falcon project [18] at Illinois is currently studying compilation of Matlab into High Performance Fortran [23].
Reference: [20] <author> Y. Keren. MATCOM: </author> <title> A Matlab to C++ translator and support libraries. </title> <type> Technical report, </type> <institution> Israel Institute of Technology, </institution> <year> 1995. </year> <month> 17 </month>
Reference-contexts: In particular, a number of projects have studied and implemented the compilation of Matlab programs into C, Fortran, and C++ <ref> [18, 19, 20, 21, 22] </ref>. In addition, a couple of projects relate to compilation of Matlab programs into high performance parallel code. The Falcon project [18] at Illinois is currently studying compilation of Matlab into High Performance Fortran [23].
Reference: [21] <author> The MathWorks, Inc. </author> <title> Matlab Compiler, </title> <year> 1995. </year>
Reference-contexts: In particular, a number of projects have studied and implemented the compilation of Matlab programs into C, Fortran, and C++ <ref> [18, 19, 20, 21, 22] </ref>. In addition, a couple of projects relate to compilation of Matlab programs into high performance parallel code. The Falcon project [18] at Illinois is currently studying compilation of Matlab into High Performance Fortran [23].
Reference: [22] <author> P. Jacobson, B. K-agstrom, and M. Ramnnar. </author> <title> Algorithm development for distributed memory multicomputers using CONLAB. </title> <journal> Scientific Programming, </journal> <volume> 1 </volume> <pages> 185-203, </pages> <year> 1992. </year>
Reference-contexts: In particular, a number of projects have studied and implemented the compilation of Matlab programs into C, Fortran, and C++ <ref> [18, 19, 20, 21, 22] </ref>. In addition, a couple of projects relate to compilation of Matlab programs into high performance parallel code. The Falcon project [18] at Illinois is currently studying compilation of Matlab into High Performance Fortran [23].
Reference: [23] <author> High Performance Fortran Forum. </author> <title> High performance fortran language specification version 1.0, </title> <year> 1993. </year>
Reference-contexts: In addition, a couple of projects relate to compilation of Matlab programs into high performance parallel code. The Falcon project [18] at Illinois is currently studying compilation of Matlab into High Performance Fortran <ref> [23] </ref>. RTExpress [24], from ISI, provides a system that facilitates the compilation of Matlab into C with MPI, ScaLAPACK, and other library calls.
Reference: [24] <institution> Integrated Sensors Inc. RTExpress. </institution> <note> http://www.sensors.com/Conf97. 18 </note>
Reference-contexts: In addition, a couple of projects relate to compilation of Matlab programs into high performance parallel code. The Falcon project [18] at Illinois is currently studying compilation of Matlab into High Performance Fortran [23]. RTExpress <ref> [24] </ref>, from ISI, provides a system that facilitates the compilation of Matlab into C with MPI, ScaLAPACK, and other library calls.
References-found: 24

