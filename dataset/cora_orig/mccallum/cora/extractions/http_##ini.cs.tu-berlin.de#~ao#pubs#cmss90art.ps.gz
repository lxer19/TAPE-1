URL: http://ini.cs.tu-berlin.de/~ao/pubs/cmss90art.ps.gz
Refering-URL: http://ini.cs.tu-berlin.de/~ao/pubs-e.html
Root-URL: http://ini.cs.tu-berlin.de/~ao/pubs-e.html
Title: A Modularization Scheme for Feedforward Networks  
Author: Arnfried Ossen 
Address: Berlin W-1000/19, FR Germany  
Affiliation: Institut fur Angewandte Informatik Technical University of Berlin  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [Chauvin, 1989] <author> Yves Chauvin. </author> <title> A back-propagation algorithm with optimal use of hidden units. </title> <editor> In David S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems I, </booktitle> <pages> pages 519-526. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1989. </year>
Reference: [Hancock, 1989] <author> Peter J. B. Hancock. </author> <title> Data representation in neural nets: An empirical study. </title> <editor> In David Touret-zky, Geoffrey Hinton, and Terrence Sejnowski, editor, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 11-20, </pages> <address> San Mateo, CA, 1989. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: I have chosen a coarse coding [Hinton et al., 1986] scheme of scalar values. In comparison to value-unit codings, it shows improved convergence <ref> [Hancock, 1989] </ref>. It also provides the resolution needed for the development of internal representations. The intended scalar value can simply be represented by sampling a unimodal function centered at this value [Saund, 1989].
Reference: [Hinton et al., 1986] <author> Geoffrey E. Hinton, James L. Mc--Clelland, and David E. Rumelhart. </author> <title> Distributed representations. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition, chapter 3. </title> <publisher> MIT Press/Bradford Books, </publisher> <year> 1986. </year>
Reference-contexts: A promising way of defining these patterns is the use of coding schemes that restrict patterns of activation to specific subsets with well-defined values related to them. I have chosen a coarse coding <ref> [Hinton et al., 1986] </ref> scheme of scalar values. In comparison to value-unit codings, it shows improved convergence [Hancock, 1989]. It also provides the resolution needed for the development of internal representations. The intended scalar value can simply be represented by sampling a unimodal function centered at this value [Saund, 1989].
Reference: [Kohonen, 1989] <author> Teuvo Kohonen. </author> <title> Self-Organization and Associative Memory. </title> <publisher> Springer Verlag, </publisher> <year> 1989. </year>
Reference: [le Cun et al., 1990] <author> Yann le Cun, John S. Denker, and Sara A. Solla. </author> <title> Optimal brain damage. </title> <editor> In David S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems II, </booktitle> <pages> pages 598-605. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: Either an auxiliary error term is added to the cost function in order to penalize network configurations with many active links and/or units [Chau-vin, 1989], or the relevance of links/units with respect to the error is determined, and the least relevant links/units are deleted <ref> [Mozer and Smolensky, 1989; le Cun et al., 1990] </ref>. The problem with minimal network construction using auxiliary error terms is the difficulty of relative weighting of terms, which may cause convergence problems. Also, it might still be very difficult to understand the emerging patterns of activity in the hidden layers.
Reference: [le Cun, 1989] <author> Yann le Cun. </author> <title> Generalization and network design strategies. </title> <editor> In Rolf Pfeiffer, Zoltan Schreter, Francoise Fogelman-Solie, and Luc Steels, editor, </editor> <booktitle> Proceedings Connectionism in Perspective. Swiss Group for Artificial Intelligence and Cognititive Science (SGAICO), </booktitle> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1989. </year>
Reference-contexts: A more general approach is to minimize the number of free parameters in the network, e.g. by incorporating equality constraints between the weights of the network based on a priori knowledge of the problem <ref> [le Cun, 1989] </ref>. A significant reduction in learning complexity and better generalization can be obtained.
Reference: [Mozer and Smolensky, 1989] <author> Michael C. Mozer and Paul Smolensky. </author> <title> Skeletonization: A technique for trimming the fat from a network via relevance assessment. </title> <type> Technical Report CU-CS-421-89, </type> <institution> University of Colorado at Boulder, Boulder, </institution> <month> January </month> <year> 1989. </year>
Reference-contexts: Either an auxiliary error term is added to the cost function in order to penalize network configurations with many active links and/or units [Chau-vin, 1989], or the relevance of links/units with respect to the error is determined, and the least relevant links/units are deleted <ref> [Mozer and Smolensky, 1989; le Cun et al., 1990] </ref>. The problem with minimal network construction using auxiliary error terms is the difficulty of relative weighting of terms, which may cause convergence problems. Also, it might still be very difficult to understand the emerging patterns of activity in the hidden layers.
Reference: [Salomon, 1989] <author> Ralf Salomon. </author> <title> Adaptiv geregelte Lern-rate bei Back-propagation. </title> <type> Technical Report 89-24, </type> <institution> Technische Universitat Berlin, 1989. Forschungs-berichte des Fachbereichs Informatik. </institution>
Reference-contexts: To speed up learning, the gradient was normalized before updating the weight. This follows from the empirical observation that the product of optimal learning rate and absolute value of the gradient remains almost constant over all learning cycles, see <ref> [Salomon, 1989] </ref>. Scalar codings were created using the derivative of the sigmoidal function 1=(1 + e x=t ) at a temperature of t = 1=8. Acknowledgement I would like to thank Albrecht Biedl for his helpful comments on early drafts of the paper.
Reference: [Saund, 1989] <author> Eric Saund. </author> <title> Dimensionality-reduction using connectionist networks. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 11(3) </volume> <pages> 304-314, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: In comparison to value-unit codings, it shows improved convergence [Hancock, 1989]. It also provides the resolution needed for the development of internal representations. The intended scalar value can simply be represented by sampling a unimodal function centered at this value <ref> [Saund, 1989] </ref>. Decoding of scalars is possible via auxiliary networks that map scalar representations to an activation value or by a least squares error procedure. 2.1 A CONSTRAINED NETWORK Constraints are enforced by replacing the hidden layer of a standard feedforward network with a pretrained auto-associative network. <p> complexity of the training cycles for the 8-4-4-4-8 (120 free weights) and 8-[4-4-4]-8 (80 free weights) systems. 8-4-8 8-[4-4-4]-8 total sum of squares training epochs 5 1e-01 2 5 1e+00 2 5 1e+01 module. 3 APPLICATIONS 3.1 NONLINEAR DIMENSIONALITY REDUCTION This system can be applied to nonlinear dimensionality reduction problems <ref> [Saund, 1989] </ref>, which turn out to be the special case of equal representations at input and hidden layers. The constraining module has to be pretrained to auto-associate the chosen scalar coding, while the original modules auto-associate the higher dimensional data.
References-found: 9

