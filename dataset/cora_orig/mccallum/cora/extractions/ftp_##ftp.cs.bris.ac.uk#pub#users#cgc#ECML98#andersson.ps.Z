URL: ftp://ftp.cs.bris.ac.uk/pub/users/cgc/ECML98/andersson.ps.Z
Refering-URL: http://www.cs.bris.ac.uk/~cgc/ECML98-WS/Summary.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: arne@dna.lth.se, pdv@ide.hk-r.se, johan@dna.lth.se  
Phone: 2  
Title: Model selection using measure functions  
Author: Arne Andersson Paul Davidsson and Johan Linden 
Address: Box 118, S-221 00 Lund, Sweden  S-372 25 Ronneby, Sweden  
Affiliation: 1 Department of Computer Science Lund University  Department of Computer Science University of Karlskrona/Ronneby  
Abstract: The concept of measure functions for generalization performance is suggested. This concept provides an alternative way of selecting and evaluating learned models (classifiers). In addition, it makes it possible to state a learning problem as a computational problem. The the known prior (meta-)knowledge about the problem domain is captured in a measure function that, to each possible combination of a training set and a classifier, assigns a value describing how good the classifier is. The computational problem is then to find a classifier maximizing the measure function. We argue that measure functions are of great value for practical applications. Besides of being a tool for model selection, they: (i) force us to make explicit the relevant prior knowledge about the learning problem at hand, (ii) provide a deeper understanding of existing algorithms, and (iii) help us in the construction of problem-specific algorithms. We illustrate the last point by suggesting a novel algorithm based on incremental search for a classifier that optimizes a given measure function.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Akaike. </author> <title> A new look at statistical model identification. </title> <journal> IEEE Transactions on Automatic control, </journal> <volume> 19 </volume> <pages> 716-723, </pages> <year> 1974. </year>
Reference-contexts: One example is model-order selection in linear prediction, where the order or dimension of the predictor, i.e., the number of previous values used to estimate the next value, is often chosen according to a measure-like criterion, such as Akaike's information-theoretic criteria <ref> [1] </ref>, and Parzen's criterion autoregressive transfer [10]. In the next section we introduce the concept of measure functions for generalization performance. This is followed by a discussion of the relation between commonly used heuristics and measure functions.
Reference: [2] <author> E. Anderson. </author> <title> The Irises of the Gaspe Peninsula. </title> <journal> Bulletin of the American Iris Society, </journal> <volume> 59 </volume> <pages> 2-5, </pages> <year> 1935. </year>
Reference-contexts: Right: Nearest neighbor. The upper version is 1-nearest neighbor. The lower version use 10 neighbors. 4.3 Model evaluation and selection We have used the well-known Iris data base <ref> [2] </ref>. For the sake of presentation, we only used two of the four dimensions (petal length and width).
Reference: [3] <author> A. Andersson, P. Davidsson, and J. Linden. </author> <title> Measuring generalization quality. </title> <type> Technical Report LU-CS-TR: 98-202, </type> <institution> Department of Computer Science, Lund University, Lund, Sweden, </institution> <year> 1998. </year>
Reference-contexts: One feature of our notion of measure functions is that it helps in simplifying and clarifying the discussion on when generalization is meaningful (cf. [16, 17, 13]). In short, it can be proved that once a non-trivial measure function is defined, some algorithms are better than others, see <ref> [3] </ref>.
Reference: [4] <author> B.V. Dasarathy. </author> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. </title> <publisher> IEEE Computer Society Press, </publisher> <year> 1990. </year>
Reference-contexts: In this way, the decision borders represented by neurons have a tendency to place themselves in the middle between clusters of different categories. Nearest neighbor algorithms This class of algorithms <ref> [4] </ref> is clearly based on similarity. A method to avoid over-fitting is to look at the k nearest neighbors rather than only looking at one neighbor.
Reference: [5] <author> D.F. Gordon and M. </author> <title> des Jardins. Evaluation and selection of biases in machine learning. </title> <journal> Machine Learning, </journal> 20(1/2):5-22, 1995. 
Reference-contexts: For example, characterizing algorithms in terms of which measure function they maximize seems to be a plausible way to identify their strengths and weaknesses as well as the regions of expertize for different biases <ref> [5] </ref>.
Reference: [6] <author> R. Kohavi. </author> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <booktitle> In IJCAI-95, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: 1 Introduction In this work, we suggest a new approach to model selection and evaluation. Today, most methods for evaluating the quality of a learned model (classifier) is based on some kind of 54 cross-validation <ref> [6] </ref>. However, we argue that it is possible to make evaluations that take into account other important aspects of the model than just classification accuracy on a few instances.
Reference: [7] <author> D.G. Lowe. </author> <title> Similarity metric learning for a variable-kernel classifier. </title> <journal> Neural Computation, </journal> <volume> 7(1) </volume> <pages> 72-85, </pages> <year> 1995. </year>
Reference-contexts: Or, similarly, we can use subset-fit measures for tuning the parameters of a single algorithm (cf. VSM <ref> [7] </ref>). An intuitive property of good generalization is that "similar" instances should be classified similarly. A problem with similarity is that there is no objective way of measuring it. A distance measure is needed and how distances should be measured varies a lot between applications.
Reference: [8] <editor> D. Michie, D.J. Spiegelhalter, and C.C. Taylor. </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: For instance, rather than being measured by the size of decision borders, simplicity can be measured by the minimal description length [14] of the generalization. Similarity, on the other hand, can be measured in terms of Hamming distances. 59 and the lower version is a pruned tree, taken from <ref> [8, page 10] </ref>. Middle: Backpropagation. The upper version use 30 hidden nodes and 26 500 training epochs; the lower used 2 hidden nodes and 20 000 epochs. Right: Nearest neighbor. The upper version is 1-nearest neighbor.
Reference: [9] <author> G. Nakhaeizadeh and A. Schnabl. </author> <title> Development of multi-criteria metrics for evaluation of of data mining algorithms. </title> <booktitle> In KDD'97, </booktitle> <year> 1997. </year>
Reference-contexts: Our approach is based on measuring explicit properties of the learned model rather than properties of the algorithm that produced the model. Therefore, in contrast to for example Nakhaeizadeh and Schnabl <ref> [9] </ref>, we pay no attention to properties such as the employed algorithm's time and space complexity. For each possible combination of a training set and a classifier, a measure function assigns a value describing how good the classifier is.
Reference: [10] <author> E. Parzen. </author> <title> Multiple time series modeling: determining the order of approximating autoregressive schemes. In Multivariate Analysis, </title> <booktitle> IV, </booktitle> <pages> pages 283-295. </pages> <publisher> North-Holland, </publisher> <year> 1977. </year>
Reference-contexts: One example is model-order selection in linear prediction, where the order or dimension of the predictor, i.e., the number of previous values used to estimate the next value, is often chosen according to a measure-like criterion, such as Akaike's information-theoretic criteria [1], and Parzen's criterion autoregressive transfer <ref> [10] </ref>. In the next section we introduce the concept of measure functions for generalization performance. This is followed by a discussion of the relation between commonly used heuristics and measure functions.
Reference: [11] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Different decision tree algorithms balance the trade-off between subset-fit on the training set and simplicity differently: ID3 <ref> [11] </ref> gives priority to subset-fit (i.e., it tries to create the simplest possible tree consistent with the training examples) whereas pruning algorithms such as C4.5 [12] tries to balance the trade-off (i.e., it tries to create an even simpler tree that do not have to be consistent with the training examples). <p> Take for instance the ID3 algorithm <ref> [11] </ref>, the intention is to compute the simplest decision tree consistent with the training examples.
Reference: [12] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year> <month> 64 </month>
Reference-contexts: Different decision tree algorithms balance the trade-off between subset-fit on the training set and simplicity differently: ID3 [11] gives priority to subset-fit (i.e., it tries to create the simplest possible tree consistent with the training examples) whereas pruning algorithms such as C4.5 <ref> [12] </ref> tries to balance the trade-off (i.e., it tries to create an even simpler tree that do not have to be consistent with the training examples). 2 We here concentrate on the main heuristics on which the algorithms are founded, rather than on specific details of the algorithms.
Reference: [13] <author> R.B. Rao, D. Gordon, and W. Spears. </author> <title> For every generalization action, is there really an equal and opposite reaction? Analysis of the conservation law for generalization performance. </title> <booktitle> In Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 471-479. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: One feature of our notion of measure functions is that it helps in simplifying and clarifying the discussion on when generalization is meaningful (cf. <ref> [16, 17, 13] </ref>). In short, it can be proved that once a non-trivial measure function is defined, some algorithms are better than others, see [3].
Reference: [14] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: However, the definition of measure functions suits nominal or logical features just as well. The measures themselves, on the other hand, must sometimes be defined differently. For instance, rather than being measured by the size of decision borders, simplicity can be measured by the minimal description length <ref> [14] </ref> of the generalization. Similarity, on the other hand, can be measured in terms of Hamming distances. 59 and the lower version is a pruned tree, taken from [8, page 10]. Middle: Backpropagation.
Reference: [15] <author> D.E. Rumelhart, G.E. Hinton, and R.J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Vol.1: Foundations. </booktitle> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The most used approach is to choose cut-points that lie centered between training instances of different categories. The backpropagation algorithm Subset-fit: Roughly speaking, the backpropagation algorithm <ref> [15] </ref> tries to optimize the subset-fit measure function defined by the training set. The generalization is created incrementally in small steps by an attempt to minimize an error function. Simplicity: A problem with the plain backpropagation algorithm is that the error function does not provide any penalty for over-fitting.
Reference: [16] <author> C. Schaffer. </author> <title> A conservation law for generalization performance. </title> <booktitle> In Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 259-265. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: They provide a complementary tool for selecting models as well as assistance in designing new learning algorithms. It has been known for a long time, see for example the "no free lunch" theorems (cf. <ref> [16, 17] </ref>), that the task of computing a good classifier from a data set is not easily defined as a simple computational problem. One purpose of the concept of measure functions is to remedy this situation. <p> One feature of our notion of measure functions is that it helps in simplifying and clarifying the discussion on when generalization is meaningful (cf. <ref> [16, 17, 13] </ref>). In short, it can be proved that once a non-trivial measure function is defined, some algorithms are better than others, see [3].
Reference: [17] <author> D.H. Wolpert. </author> <title> Off-training set error and a priori distinctions between learning algorithms. </title> <type> Technical Report SFI TR: </type> <institution> 95-01-003, Santa Fe Institute, </institution> <address> Santa Fe NM, USA, </address> <year> 1995. </year> <month> 65 </month>
Reference-contexts: They provide a complementary tool for selecting models as well as assistance in designing new learning algorithms. It has been known for a long time, see for example the "no free lunch" theorems (cf. <ref> [16, 17] </ref>), that the task of computing a good classifier from a data set is not easily defined as a simple computational problem. One purpose of the concept of measure functions is to remedy this situation. <p> One feature of our notion of measure functions is that it helps in simplifying and clarifying the discussion on when generalization is meaningful (cf. <ref> [16, 17, 13] </ref>). In short, it can be proved that once a non-trivial measure function is defined, some algorithms are better than others, see [3].
References-found: 17

