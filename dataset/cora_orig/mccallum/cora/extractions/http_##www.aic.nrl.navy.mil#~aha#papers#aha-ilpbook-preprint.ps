URL: http://www.aic.nrl.navy.mil/~aha/papers/aha-ilpbook-preprint.ps
Refering-URL: http://www.aic.nrl.navy.mil/~aha/pub-details.html
Root-URL: 
Email: aha@turing.ac.uk  
Title: Relating Relational Learning Algorithms  
Author: David W. Aha 
Keyword: supervised learning, representation, relational learning  
Address: 36 North Hanover Street Glasgow, Scotland G1 2AD  
Affiliation: Turing Institute  
Abstract: Relational learning algorithms are of special interest to members of the machine learning community; they offer practical methods for extending the representations used in algorithms that solve supervised learning tasks. Five approaches are currently being explored to address issues involved with using relational representations. This paper surveys algorithms embodying these approaches, summarizes their empirical evaluations, highlights their commonalities, and suggests potential directions for future research. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W., Kibler, D., & Albert, M. K. </author> <year> (1991). </year> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66. </pages>
Reference: <author> Bain, M. </author> <year> (1991). </year> <title> Experiments in non-monotonic first-order induction. </title> <booktitle> In Proceedings of the First International Workshop on Inductive Logic Programming (pp. </booktitle> <pages> 195-206). </pages> <address> Vienna de Castelo, Portugal: </address> <note> Unpublished. 19 Branting, </note> <author> L. K. </author> <year> (1989). </year> <title> Integrating generalizations with exemplar-based reasoning. </title> <booktitle> In Proceedings of the Eleventh Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 139-146). </pages> <address> Ann Arbor, MI: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: endgame problem, which is somewhat surprising since their descriptor construction method has 1 Similar additions have been incorporated into more recent versions of FOIL. 7 the capacity to learn the conjunctive definition of between (X,Y) :- less than (X,Z), less than (Z,Y) and, subsequently, attain perfect accuracies on this application <ref> (Bain, 1991) </ref>. <p> Related investigations extend GOLEM to the task of learning small disjuncts using a non-monotonic inferencing perspective <ref> (Bain, 1991) </ref> and explore a method in which rlggs are used to construct new predicate descriptors as guided by an oracle (Morales, 1991). To date, GOLEM is the only relational learning algorithm that has been shown to be effective on tasks involving tens of thousands of background facts.
Reference: <author> Bratko, I., Muggleton, S., & Varsek, A. </author> <year> (1991). </year> <title> Learning qualitative models of dynamic systems. </title> <booktitle> In Proceedings of the First International Workshop on Inductive Logic Programming (pp. </booktitle> <pages> 207-224). </pages> <address> Vienna de Castelo, Portugal: </address> <note> Unpublished. </note>
Reference-contexts: Muggleton and Feng reported that GOLEM has efficiently induced descriptions for seven simple concepts (e.g., quicksort, reverse, n-choose-m). GOLEM has subsequently been applied to several more practical problems, including learning qualitative models <ref> (Bratko, Muggleton, & Varsek, 1991) </ref>, finite element mesh design (Dolsak & Muggleton, 1991), and satellite temporal fault diagnosis (Feng, 1991).
Reference: <author> Buntine, W. L. </author> <year> (1988). </year> <title> Generalized subsumption and its applications to induction and redundency. </title> <journal> Artificial Intelligence, </journal> <volume> 36, </volume> <pages> 149-176. </pages>
Reference: <author> Cestnik, B., & Bratko, I. </author> <year> (1991). </year> <title> On estimating proabilities in tree pruning. </title> <booktitle> In Proceedings of the Fifth European Working Session on Learning (pp. </booktitle> <pages> 138-150). </pages> <address> Porto, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Cestnik, B., Kononenko, I., & Bratko, I. </author> <year> (1987). </year> <title> ASSISTANT-86: A knowledge-elicitation tool for sophisticated users. </title> <editor> In I. Bratko & N. Lavrac (Eds.), </editor> <booktitle> Progress in machine learning. </booktitle> <address> Bled, Yugoslavia: </address> <publisher> Sigma Press. </publisher>
Reference-contexts: This also allows LINUS to use any monadic learning algorithm to learn concept descriptions. For example, Navrac and Dzeroski (1991) discuss how ASSISTANT <ref> (Cestnik, Kononenko, & Bratko, 1987) </ref> can be used to support noise-tolerant capabilities in LINUS.
Reference: <author> Clark, P. E., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-284. </pages>
Reference: <author> Clark, P. E., & Boswell, R. </author> <year> (1991). </year> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Proceedings of the Fifth European Working Session on Learning (pp. </booktitle> <pages> 151-163). </pages> <address> Porto, Portugal: </address> <publisher> Springer-Verlag. </publisher> <editor> Colmerauer and Kowalski, </editor> <title> 1970 reference??? De Raedt, </title> <editor> L., & Bruynooghe, M. </editor> <year> (1989a). </year> <title> Constructive induction by analogy. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning (pp. </booktitle> <pages> 476-477). </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Suppose that the target concept is "linked to each other" (i.e., positive examples are pairs of nodes that are joined by two directed arcs). Assuming that every node has at most three outward arcs, a monadic learning algorithm such as CN2 <ref> (Clark & Boswell, 1991) </ref> requires that the 81 instances have the form of the two instances displayed in the figure (i.e., each instance encodes two nodes, their outward arcs, and their classification and "x" means "no such link").
Reference: <author> De Raedt, L., & Bruynooghe, M. </author> <year> (1989b). </year> <title> Constructive induction by analogy: A method to learn how to learn? In Proceedings of the Fourth European Working Session on Learning (pp. </title> <address> 189-200). Montpel-lier, France: </address> <publisher> Pitman. </publisher>
Reference: <author> Dietterich, T. G., & Michalski, R. S. </author> <year> (1981). </year> <title> Inductive learning of structural descriptions. </title> <journal> Artificial Intelligence, </journal> <volume> 16, </volume> <pages> 257-294. </pages>
Reference: <author> Dolsak, B., & Muggleton, S. </author> <year> (1991). </year> <title> The application of inductive logic programming to finite element mesh design. </title> <booktitle> In Proceedings of the First International Workshop on Inductive Logic Programming (pp. </booktitle> <pages> 225-242). </pages> <address> Vienna de Castelo, Portugal: </address> <note> Unpublished. </note>
Reference-contexts: Muggleton and Feng reported that GOLEM has efficiently induced descriptions for seven simple concepts (e.g., quicksort, reverse, n-choose-m). GOLEM has subsequently been applied to several more practical problems, including learning qualitative models (Bratko, Muggleton, & Varsek, 1991), finite element mesh design <ref> (Dolsak & Muggleton, 1991) </ref>, and satellite temporal fault diagnosis (Feng, 1991). Related investigations extend GOLEM to the task of learning small disjuncts using a non-monotonic inferencing perspective (Bain, 1991) and explore a method in which rlggs are used to construct new predicate descriptors as guided by an oracle (Morales, 1991).
Reference: <author> Elliot, T., & Scott, P. D. </author> <year> (1991). </year> <title> Instance-based and generalization-based learning procedures applied to solving integration problems. </title> <booktitle> In Proceedings of the Eighth Conference of the Society for the Study of Artificial Intelligence (pp. </booktitle> <pages> 256-265). </pages> <address> Leeds, England: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Falkenhainer, B. C., & Michalski, R. S. </author> <year> (1990). </year> <title> Integrating quantitative and qualitative discovery in the ABACUS system. </title> <editor> In Y. Kodratoff & R. Michalski (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach (Vol. III). </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Feng, C. </author> <year> (1991). </year> <title> Inducing temporal fault diagnostic rules from a qualitative model. </title> <booktitle> In Proceedings of the First International Workshop on Inductive Logic Programming (pp. </booktitle> <pages> 243-258). </pages> <address> Vienna de Castelo, Portugal: </address> <note> Unpublished. </note>
Reference-contexts: GOLEM has subsequently been applied to several more practical problems, including learning qualitative models (Bratko, Muggleton, & Varsek, 1991), finite element mesh design (Dolsak & Muggleton, 1991), and satellite temporal fault diagnosis <ref> (Feng, 1991) </ref>. Related investigations extend GOLEM to the task of learning small disjuncts using a non-monotonic inferencing perspective (Bain, 1991) and explore a method in which rlggs are used to construct new predicate descriptors as guided by an oracle (Morales, 1991).
Reference: <author> Flann, N. S., & Dietterich, T. G. </author> <year> (1986). </year> <title> Selecting appropriate representations for learning from examples. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 460-466). </pages> <address> Philadelphia, PA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Frisch, A. M., & Page, C. D. </author> <year> (1990). </year> <title> Generalization with taxonomic information. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 755-761). </pages> <address> Boston, MA: </address> <publisher> AAAI Press. </publisher> <address> 20 Fu, </address> <note> L-M., </note> & <author> Buchanan, B. G. </author> <year> (1985). </year> <title> Learning intermediate concepts in constructing a hierarchical knowl-edge base. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 659-666). </pages> <address> Los Angeles, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hirsh, H. </author> <year> (1990). </year> <title> Incremental version-space merging. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 330-338). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Holland, J. H. </author> <year> (1986). </year> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Monadic supervised learning algorithms include the candidate elimination algorithm (Mitchell, 1982) and many algorithms that construct decision trees (Quin-lan, 1986), rules (Michalski, Mozetic, Hong, & Lavrac, 1986; Clark & Niblett, 1989), employ genetic classifiers <ref> (Holland, 1986) </ref>, learn connectionist weights (Rumelhart, McClelland, & The PDP Research Group, 1986) or lazily induce predictions from stored instances (Aha, Kibler, & Albert, 1991; Salzberg, 1991).
Reference: <author> Kietz, J-U., & Wrobel, S. </author> <year> (1991). </year> <title> Controlling the complexity of learning in logic through syntactic and task-oriented models. </title> <booktitle> In Proceedings of the First International Workshop on Inductive Logic Programming (pp. </booktitle> <pages> 107-126). </pages> <address> Vienna de Castelo, Portugal: </address> <note> Unpublished. </note>
Reference-contexts: highly useful for the small subset of instance space in which the white king interposes the white rook and black king. 2.3 Higher-Order Rules Relational descriptor construction methods are strongly reminiscent of the higher-order rule schemas used in MODELER (Wrobel, 1988), CIA (De Raedt & Bruynooghe, 1989a; 1989b), and RDT <ref> (Kietz & Wrobel, 1991) </ref>. However, whereas the former are restricted to extending existing Horn clauses with an additional literal, the latter can be used to guide the learning of entire clauses. CIA (Constructive Induction by Analogy) is a part of the CLINT learning apprentice.
Reference: <author> Kijsirikul, B., Numao, M., & Shimura, M. </author> <year> (1991). </year> <title> Efficient learning of logic programs with non-determinate, non-discriminating literals. </title> <booktitle> In Proceedings of the First International Workshop on Inductive Logic Programming (pp. </booktitle> <pages> 33-40). </pages> <address> Vienna de Castelo, Portugal: </address> <note> Unpublished. </note>
Reference-contexts: Similar extensions could help FOIL.1 either by improving its information-gain evaluation function directly or by providing it with information that allows it to extend clauses with several literals simultaneously. These two approaches have been implemented 6 in CHAM and FOCL respectively. CHAM <ref> (Kijsirikul, Numao, & Shimura, 1991) </ref> extends FOIL.1's information-gain heuristic directly. Some useful literals, such as those that transform a clause's input variables to a form more closely representing its output variables, are not always good discriminators of positive and negative instances. Subsequently, FOIL.1 will not choose them.
Reference: <author> Langley, P. </author> <year> (1985). </year> <title> Learning to search: From weak methods to domain-specific heuristics. </title> <journal> Cognitive Science, </journal> <volume> 9, </volume> <pages> 217-260. </pages>
Reference: <author> Langley, P., Simon, H. A., Bradshaw, G. L., & Zytkow, J. M. </author> <year> (1987). </year> <title> Scientific discovery: Computational explorations of the creative processes. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Lavrac, N., Dzeroski, S., & Grobelnik, M. </author> <year> (1991). </year> <title> Learning nonrecursive definitions of relations with LINUS. </title> <booktitle> In Proceedings of the Fifth European Working Session on Learning (pp. </booktitle> <pages> 265-281). </pages> <address> Porto, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: However, Wyl's last step is inefficient; it generates the set of all possible instances corresponding to the induced target concept and then compresses this set by replacing disjuncts with constructed descriptors. Thus, Wyl sacrifices efficiency to express concept descriptions in full first-order logic. tasks with monadic learning algorithms <ref> (Lavrac, Dzeroski, & Grobelnik, 1991) </ref>. It inputs instances in the Deductive Hierarchical Database (DHDB) representation, which can express typed nonre-cursive Horn clauses with negation (Mozetic, 1987).
Reference: <author> Lavrac, N., Dzeroski, S., & Varsek, A. </author> <year> (1991). </year> <title> Inductive learning of relational descriptions from noisy examples. </title> <booktitle> In Proceedings of the First International Workshop on Inductive Logic Programming (pp. </booktitle> <pages> 207-224). </pages> <address> Vienna de Castelo, Portugal: </address> <note> Unpublished. </note>
Reference-contexts: However, Wyl's last step is inefficient; it generates the set of all possible instances corresponding to the induced target concept and then compresses this set by replacing disjuncts with constructed descriptors. Thus, Wyl sacrifices efficiency to express concept descriptions in full first-order logic. tasks with monadic learning algorithms <ref> (Lavrac, Dzeroski, & Grobelnik, 1991) </ref>. It inputs instances in the Deductive Hierarchical Database (DHDB) representation, which can express typed nonre-cursive Horn clauses with negation (Mozetic, 1987).
Reference: <author> Lloyd, J. W. </author> <year> (1987). </year> <booktitle> Foundations of logic programming (Second edition). </booktitle> <address> Berlin, Germany: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Matheus, C. J. </author> <year> (1990). </year> <title> Adding domain knowledge to SBL through feature construction. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 803-808). </pages> <address> Boston, MA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Several other descriptor construction approaches have proved useful with monadic learning algorithms. Multiple representation algorithms could immediately use such algorithms such as STAGGER (Schlimmer, 1987) and CITRE <ref> (Matheus, 1990) </ref> to create new symbolic-valued descriptors while methods used in scientific discovery algorithms could be used to learn numeric-valued descriptors (Langley, Simon, Bradshaw, & Zytkow, 1987; Zytkow, Zhu, & Hussam, 1990; Falkenhainer & Michalski, 1990; Zytkow & Zhu, 1991).
Reference: <author> Michalski, R. S., Mozetic, I., Hong, J., & Lavrac, N. </author> <year> (1986). </year> <title> The multi-purpose incremental learning system AQ15 and its testing application to three medical domains. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 1041-1045). </pages> <address> Philadelphia, PA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The first relational learning algorithm to use this approach was FOIL.1 (Quinlan, 1990). Unlike ID3, which constructs decision trees using a divide-and-conquer algorithm, FOIL.1 uses a form of the covering search method that was used previously in AQ15 <ref> (Michalski, Mozetic, Hong, & Lavrac, 1986) </ref>, although a different evaluation function is used to guide search. For example, MIS used an exhaustive covering search strategy to locate relational concept descriptions. FOIL.1 instead employs a hill-climbing search.
Reference: <author> Minton, S., Carbonell, J. G., Etzioni, O., Knoblock, C. A., & Kuokka, D. R. </author> <year> (1987). </year> <title> Acquiring effective search control rules: Explanation-based learning in the PRODIGY system. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning (pp. </booktitle> <pages> 122-133). </pages> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Other useful behaviors can be taken from monadic learning algorithms. These include adopting incremental learning techniques to decrease the training costs of information-gain algorithms (e.g., Utgoff, 1989), using genetic algorithms to suggest alternative higher-order schemas, performing utility analysis on higher-order schemas <ref> (e.g., Minton, Carbonell, Etzioni, Knoblock, & Kuokka, 1987) </ref>, and using case-based approaches to manage the organization of stored Horn clauses and provide partial matching capabilities. 18 3.5 Theory Revision The majority of current-generation relational learning algorithms assume that their background knowledge is static; it will not change during learning.
Reference: <author> Mitchell, T. M. </author> <year> (1982). </year> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18, </volume> <pages> 203-226. </pages>
Reference-contexts: The most frequently used representation for instances in supervised learning algorithms is the attribute-value representation, which represents each descriptor with a value from a monadic relation (i.e., a set of singletons). Monadic supervised learning algorithms include the candidate elimination algorithm <ref> (Mitchell, 1982) </ref> and many algorithms that construct decision trees (Quin-lan, 1986), rules (Michalski, Mozetic, Hong, & Lavrac, 1986; Clark & Niblett, 1989), employ genetic classifiers (Holland, 1986), learn connectionist weights (Rumelhart, McClelland, & The PDP Research Group, 1986) or lazily induce predictions from stored instances (Aha, Kibler, & Albert, 1991; Salzberg,
Reference: <author> Mitchell, T., Keller, R., & Kedar-Cabelli, S. </author> <year> (1986). </year> <title> Explanation-based learning: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 47-80. </pages>
Reference-contexts: RINCON would yield turing_employee (pete) :- english (pete), created_good_product (pete,optimist). turing_employee (ketil) :- norweigan (ketil), created_good_product (ketil,robot_head), 2 Thus, RINCON resembles an inverse-EBL algorithm since it begins with specific instances and gradually builds an EBL domain theory <ref> (Mitchell, Kellar, & Kedar-Cabelli, 1986) </ref>. 10 created_good_product (X,Y) :- created (X,Y), good_product (Y). where "created good product" is a user-provided name. Absorption steps are then executed with the newly created predicate and all stored clauses whose bodies contain a conjunct of literals that can instantiate its body.
Reference: <author> Morales, E. </author> <year> (1991). </year> <title> Learning chess patterns. </title> <booktitle> In Proceedings of the First International Workshop on Inductive Logic Programming (pp. </booktitle> <pages> 291-307). </pages> <address> Vienna de Castelo, Portugal: </address> <note> Unpublished. </note>
Reference-contexts: Related investigations extend GOLEM to the task of learning small disjuncts using a non-monotonic inferencing perspective (Bain, 1991) and explore a method in which rlggs are used to construct new predicate descriptors as guided by an oracle <ref> (Morales, 1991) </ref>. To date, GOLEM is the only relational learning algorithm that has been shown to be effective on tasks involving tens of thousands of background facts. However, unlike the algorithms based on explicitly inverting resolution steps, GOLEM cannot automatically induce new predicate descriptors.
Reference: <author> Mozetic, I. </author> <year> (1987). </year> <title> Learning of qualitative models. </title> <editor> In I. Bratko & N. Lavrac (Eds.), </editor> <booktitle> Progress in machine learning. </booktitle> <address> Bled, Yugoslavia: </address> <publisher> Sigma Press. </publisher>
Reference-contexts: Thus, Wyl sacrifices efficiency to express concept descriptions in full first-order logic. tasks with monadic learning algorithms (Lavrac, Dzeroski, & Grobelnik, 1991). It inputs instances in the Deductive Hierarchical Database (DHDB) representation, which can express typed nonre-cursive Horn clauses with negation <ref> (Mozetic, 1987) </ref>. Whereas Wyl reduces the relational learning problem into one where explanation-based learning techniques can be applied, LINUS instead reduces the problem to an attribute-value learning problem, thus avoiding the difficult search in the space of Horn clause target concept descriptions.
Reference: <author> Muggleton, S. </author> <year> (1987). </year> <title> Duce, an oracle based approach to constructive induction. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 287-292). </pages> <address> Milan, Italy: </address> <publisher> Morgan Kaufmann. </publisher> <address> 21 Muggleton, S. </address> <year> (1991). </year> <title> Inductive logic programming. </title> <journal> New Generation Technology, </journal> <volume> 8, </volume> <pages> 295-318. </pages>
Reference: <author> Muggleton, S., Bain, M., Hayes-Michie, J., & Michie, D. </author> <year> (1989). </year> <title> An experimental comparison of human and machine learning formalisms. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning (pp. </booktitle> <pages> 113-118). </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Size is defined in a straightforward syntactic manner. CIGOL extends both DUCE, a non-incremental algorithm that is restricted to generating propositional Horn clauses, and MAR-VIN, which cannot construct new relational descriptors. Although CIGOL has been evaluated empirically and can learn quickly on the KRK chess endgame domain <ref> (Muggleton, Bain, Michie, & Michie, 1989) </ref>, it is a relatively inefficient algorithm because there may be several resolvents for an inverse resolution step and many such steps may be needed to construct a clause for the target concept description.
Reference: <author> Muggleton, S., & Buntine, W. </author> <year> (1988). </year> <title> Machine invention of first order predicates by inverting resolution. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning (pp. </booktitle> <pages> 339-352). </pages> <address> Ann Arbor, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Muggleton, S., & Feng, C. </author> <year> (1990). </year> <title> Efficient induction of logic programs. </title> <booktitle> Proceedings of the First International Workshop on Algorithmic Learning Theory (pp. </booktitle> <pages> 368-381). </pages> <address> Tokyo, Japan: </address> <booktitle> Japanese Society for Artificial Intelligence. </booktitle>
Reference-contexts: The concept description generated by CN2 is not especially intuitive due to its restricted concept description language. In contrast, the relational learning algorithm GOLEM <ref> (Muggleton & Feng, 1990) </ref> requires only one predictor descriptor and fourteen background facts (i.e., corresponding to the directed arcs) to induce the more comprehensible concept description shown. GOLEM's representation simplifies instance descriptions by storing many of their descriptors as background facts, which are searched implicitly during learning.
Reference: <author> Pagallo, G. </author> <year> (1989). </year> <title> Learning DNF by decision trees. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 639-644). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Their STRUCT algorithm learns decision trees where the root is the head of the target relation, each interior node is a literal, and paths through the tree encode Horn clauses. They adapted FRINGE <ref> (Pagallo, 1989) </ref>, a monadic algorithm that constructs descriptors for decision trees, to work within STRUCT. They found that STRUCT learns more quickly than FOIL.1 in the kinship relations domain.
Reference: <author> Pazzani, M., & Kibler, D. </author> <year> (1990). </year> <title> The utility of knowledge in inductive learning (Technical Report 90-18). </title> <institution> University of California, Irvine, Department of Information and Computer Science. </institution>
Reference-contexts: However, not all target concept clauses have this property. Perhaps knowledge-intensive definitions for CHAM's closeness measure will extend its applicability. FOCL <ref> (Pazzani & Kibler, 1990) </ref> is a complementary extension of FOIL.1 that extends it in many ways. <p> However, this limits their applicability, especially given current interests in supporting incremental learning capabilities. While AUDREY (Wogulis, 1991) can explicitly revise its domain theories and FOCL <ref> (Pazzani & Kibler, 1990) </ref> has been shown to recover from incomplete and incorrect domain theories, few relational learning algorithms have been designed to accept incremental revisions of background knowledge (e.g., Wrobel, 1988). An algorithm that solves this open problem could gradually build up its expertise in domains with interelated concepts.
Reference: <author> Plotkin, G. D. </author> <year> (1970). </year> <title> A note on inductive generalisation. </title> <editor> In B. Meltzer & D. Michie (Eds.), </editor> <booktitle> Machine Intelligence (Vol. V). </booktitle> <address> New York, NY: </address> <publisher> Elsevier North-Holland. </publisher>
Reference: <author> Plotkin, G. D. </author> <year> (1971). </year> <title> A further note on inductive generalisation. </title> <editor> In B. Meltzer & D. Michie (Eds.), </editor> <booktitle> Machine Intelligence (Vol. VI). </booktitle> <address> New York, NY: </address> <publisher> Elsevier North-Holland. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: However, some still have close ties with monadic learning algorithms. For example, one group of relational learning algorithms uses information gain to guide their search through concept description space, which was previously used to guide ID3's search <ref> (Quinlan, 1986) </ref>. The first relational learning algorithm to use this approach was FOIL.1 (Quinlan, 1990). <p> This occurs because it measures the information gain of only single rather than sets of literals (i.e., it suffers from a horizon effect). Several extensions of ID3 <ref> (Quinlan, 1986) </ref> have attempted to correct this problem by providing look-ahead capabilities, multi-attribute splitting tests, or supporting constructive induction (e.g., Seshu, 1989; Utgoff & Brodley, 1990; Matheus, 1990).
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239-266. </pages>
Reference-contexts: Their experiments have shown that it does not suffer from the limitations of FOIL.1's encoding length restriction (Section 2.2 and learns more quickly than FOIL.1 on the illegal KRK chess endgame position task <ref> (Quinlan, 1990) </ref>, on which it is also more noise-tolerant. Thus, LINUS relies on efficient monadic learning algorithms while still delivering good performance on some relational learning tasks. However, it cannot learn relations with existentially quantified variables (i.e., all variables are required to appear in the head of the Horn clause). <p> For example, one group of relational learning algorithms uses information gain to guide their search through concept description space, which was previously used to guide ID3's search (Quinlan, 1986). The first relational learning algorithm to use this approach was FOIL.1 <ref> (Quinlan, 1990) </ref>. Unlike ID3, which constructs decision trees using a divide-and-conquer algorithm, FOIL.1 uses a form of the covering search method that was used previously in AQ15 (Michalski, Mozetic, Hong, & Lavrac, 1986), although a different evaluation function is used to guide search. <p> None of them learn relations represented in the full first-order predicate calculus. Thus, they cannot learn certain clauses <ref> (e.g., Quinlan, 1990, page 260) </ref>. However, analyses of earlier attempts with less restricted algorithms may suggest methods for alleviating this restriction (Vere, 1978; Flann & Dietterich, 1986). The tradeoffs involved with expanding the target concept language is open for exploration.
Reference: <author> Robinson, J. A. </author> <year> (1965). </year> <title> A machine-oriented logic based on the resolution principle. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 12, </volume> <pages> 23-41. </pages>
Reference: <author> Rouveirol, C., & Puget, J. F. </author> <year> (1990). </year> <title> Beyond inversion of resolution. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 122-130). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rumelhart D. E., McClelland, J. L., </author> & <title> The PDP Research Group (Eds.), </title> <booktitle> (1986). Parallel distributed processing: Explorations in the microstructure of cognition (Vol. 1). </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Monadic supervised learning algorithms include the candidate elimination algorithm (Mitchell, 1982) and many algorithms that construct decision trees (Quin-lan, 1986), rules (Michalski, Mozetic, Hong, & Lavrac, 1986; Clark & Niblett, 1989), employ genetic classifiers (Holland, 1986), learn connectionist weights <ref> (Rumelhart, McClelland, & The PDP Research Group, 1986) </ref> or lazily induce predictions from stored instances (Aha, Kibler, & Albert, 1991; Salzberg, 1991). Although these algorithms differ in how they process instances and represent concept descriptions, they are generally applicable to a large set of supervised learning tasks.
Reference: <author> Sammut, C., & Banerji, R. B. </author> <year> (1986). </year> <title> Learning concepts by asking questions. </title> <editor> In R. S. Michalski, J. G. Car-bonell, & T. M. Mitchell (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach (Vol. II). </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Resolution deductively derives refutation trees as the basis for computations (Robinson, 1965; Colmerauer & Kowalski, 1970). Inverting this process could form the foundation of proof-theoretic methods. MARVIN <ref> (Sammut & Banerji, 1986) </ref> was the first relational algorithm to incorporate this approach. It processes instances incrementally, generalizing them with reference to a set of back 9 ground Horn clauses using a single generalization operator.
Reference: <author> Salzberg, S. L. </author> <year> (1991). </year> <title> A nearest hyperrectangle learning method. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 251-276. </pages>
Reference: <author> Schlimmer, J. C. </author> <year> (1987). </year> <title> Incremental adjustment of representations for learning. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning (pp. </booktitle> <pages> 79-90). </pages> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Several other descriptor construction approaches have proved useful with monadic learning algorithms. Multiple representation algorithms could immediately use such algorithms such as STAGGER <ref> (Schlimmer, 1987) </ref> and CITRE (Matheus, 1990) to create new symbolic-valued descriptors while methods used in scientific discovery algorithms could be used to learn numeric-valued descriptors (Langley, Simon, Bradshaw, & Zytkow, 1987; Zytkow, Zhu, & Hussam, 1990; Falkenhainer & Michalski, 1990; Zytkow & Zhu, 1991).
Reference: <author> Seshu, R. </author> <year> (1989). </year> <title> Solving the parity problem. </title> <booktitle> In Proceedings of the Fourth European Working Session on Learning (pp. </booktitle> <pages> 263-271). </pages> <address> Montpellier, France: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Shapiro, E. Y. </author> <year> (1983). </year> <title> Algorithmic program debugging. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Shen, W-M. </author> <year> (1990). </year> <title> Functional transformations in AI discovery systems. </title> <journal> Artificial Intelligence, </journal> <volume> 41, </volume> <pages> 257-272. </pages>
Reference: <author> Silverstein, G., & Pazzani, M. J. </author> <year> (1991). </year> <title> Relational cliches: Constraining constructive induction during relational learning. </title> <booktitle> To appear in Proceedings of the Eighth International Workshop on Machine Learning. </booktitle> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Specht, D. F. </author> <year> (1990). </year> <title> Probabilistic neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 3, </volume> <pages> 109-118. </pages> <note> 22 Spiessens, </note> <author> P. </author> <year> (1990). </year> <title> PCS: A classifier system that builds a predictive internal world model. </title> <booktitle> In Proceedings of the Ninth European Conference on Artificial Intelligence (pp. </booktitle> <pages> 622-627). </pages> <address> London: </address> <publisher> Pitman. </publisher>
Reference: <author> Utgoff, P. E. </author> <year> (1989). </year> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 161-186. </pages>
Reference-contexts: Higher-order predicates and intelligent abstraction hierarchies for cliches might be used to help control the search for these new descriptors. Other useful behaviors can be taken from monadic learning algorithms. These include adopting incremental learning techniques to decrease the training costs of information-gain algorithms <ref> (e.g., Utgoff, 1989) </ref>, using genetic algorithms to suggest alternative higher-order schemas, performing utility analysis on higher-order schemas (e.g., Minton, Carbonell, Etzioni, Knoblock, & Kuokka, 1987), and using case-based approaches to manage the organization of stored Horn clauses and provide partial matching capabilities. 18 3.5 Theory Revision The majority of current-generation relational
Reference: <author> Utgoff, P. E., & Brodley, C. E. </author> <year> (1990). </year> <title> An incremental method for finding multivariate splits for decision trees. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 58-65). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Vere, S. A. </author> <year> (1980). </year> <title> Multilevel counterfactuals for generalizations of relational concepts and productions. </title> <journal> Artificial Intelligence, </journal> <volume> 14, </volume> <pages> 139-164. </pages>
Reference-contexts: At that time, RINCON re-expresses the instance in terms of the most specific concepts that it matches and adds the instance as a disjunct to its current domain theory. 2 It then uses THOTH's <ref> (Vere, 1980) </ref> maximally specific generalization technique to generalize the new instance against the concepts it does not match. The generalization leading to the largest number of re-expressed concepts is selected.
Reference: <author> Watanabe, L., & Rendell, L. </author> <year> (1991). </year> <title> Learning structural decision trees from examples. </title> <booktitle> To appear in Proceedings of the Twelfth International Joint Conference on Artificial Intelligence. </booktitle> <address> Sydney, Australia: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Wirth, R. </author> <year> (1989). </year> <title> Completing logic programs by inverse resolution. </title> <booktitle> In Proceedings of the Fourth European Working Session on Learning (pp. </booktitle> <pages> 239-250). </pages> <address> Montpellier, France: </address> <publisher> Pitman. </publisher>
Reference: <author> Wirth, R., & O'Rorke, P. </author> <year> (1991). </year> <title> Inductive completion of SLD proofs. </title> <booktitle> In Proceedings of the First International Workshop on Inductive Logic Programming (pp. </booktitle> <pages> 167-176). </pages> <address> Vienna de Castelo, Portugal: </address> <note> Unpublished. </note>
Reference: <author> Wogulis, J. </author> <year> (1989). </year> <title> A framework for improving efficiency and accuracy. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning (pp. </booktitle> <pages> 78-80). </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Wogulis, J., & Langley, P. </author> <year> (1989). </year> <title> Improving efficiency by learning intermediate concepts. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 657-662). </pages> <address> Detroit, MI: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Wrobel, S. </author> <year> (1988). </year> <title> Automatic representation adjustment in an observational discovery system. </title> <booktitle> In Proceedings of the Third European Working Session on Learning (pp. </booktitle> <pages> 253-262). </pages> <address> Glasgow, Scotland: </address> <publisher> Pitman. </publisher>
Reference-contexts: of constructed features prevents it from learning that this definition is highly useful for the small subset of instance space in which the white king interposes the white rook and black king. 2.3 Higher-Order Rules Relational descriptor construction methods are strongly reminiscent of the higher-order rule schemas used in MODELER <ref> (Wrobel, 1988) </ref>, CIA (De Raedt & Bruynooghe, 1989a; 1989b), and RDT (Kietz & Wrobel, 1991). However, whereas the former are restricted to extending existing Horn clauses with an additional literal, the latter can be used to guide the learning of entire clauses. <p> While AUDREY (Wogulis, 1991) can explicitly revise its domain theories and FOCL (Pazzani & Kibler, 1990) has been shown to recover from incomplete and incorrect domain theories, few relational learning algorithms have been designed to accept incremental revisions of background knowledge <ref> (e.g., Wrobel, 1988) </ref>. An algorithm that solves this open problem could gradually build up its expertise in domains with interelated concepts.
Reference: <author> Zytkow, J. M., & Zhu, J. </author> <year> (1991). </year> <title> Application of empirical discovery in knowledge acquisition. </title> <booktitle> In Proceedings of the Fifth European Working Session on Learning (pp. </booktitle> <pages> 101-117). </pages> <address> Porto, Portugal: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Zytkow, J. M., Zhu, J., & Hussam, A. </author> <year> (1990). </year> <title> Automated discovery in a chemistry laboratory. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 889-894). </pages> <address> Boston, MA: </address> <publisher> AAAI Press. </publisher> <pages> 23 </pages>
References-found: 64

