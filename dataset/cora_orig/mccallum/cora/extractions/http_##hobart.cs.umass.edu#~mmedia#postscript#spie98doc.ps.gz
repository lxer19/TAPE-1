URL: http://hobart.cs.umass.edu/~mmedia/postscript/spie98doc.ps.gz
Refering-URL: http://hobart.cs.umass.edu/~mmedia/mm.html
Root-URL: 
Email: Email:fvwu,manmathag@cs.umass.edu  
Title: DOCUMENT IMAGE CLEAN-UP AND some 50 images from a wide variety of sources: digitized video
Author: BINARIZATION Victor Wu, R. Manmatha 
Note: The algorithm has been applied to  
Date: December 12, 1997  
Address: Amherst, MA 01003-4610  
Affiliation: Multimedia Indexing And Retrieval Group Computer Science Department University of Massachusetts,  
Abstract: Image binarization is a difficult task for documents with text over textured or shaded backgrounds, poor contrast, and/or considerable noise. Current optical character recognition (OCR) and document analysis technology do not handle such documents well. We have developed a simple yet effective algorithm for document image clean-up and binarization. The algorithm consists of two basic steps. In the first step, the input image is smoothed using a low-pass (Gaussian) filter. The smoothing operation enhances the text relative to any background texture. This is because background texture normally has higher frequency than text does. The smoothing operation also removes speckle noise. In the second step, the intensity histogram of the smoothed image is computed and a threshold automatically selected as follows. For black text, the first peak of the histogram corresponds to text. Thresholding the image at the value of the valley between the first and second peaks of the histogram binarizes the image well. In order to reliably identify the valley, the histogram is smoothed by a low-pass filter before the threshold is computed. fl This material is based on work supported in part by the National Science Foundation, Library of Congress and Department of Commerce under cooperative agreement number EEC-9209623, in part by the United States Patent and Trademark Office and Defense Advanced Research Projects Agency/ITO under ARPA order number D468, issued by ESC/AXS contract number F19628-95-C-0235, in part by the National Science Foundation under grant number IRI-9619117 and in part by NSF Multimedia CDA-9502639. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. S. Abutaleb. </author> <title> Automatic Thresholding of Gray-Level Picture Using Two-Dimensional Entropy. CVGIP: Graphical Models and Image Processing, </title> <booktitle> 47(1) </booktitle> <pages> 22-32, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Otsu [11] models the intensity histogram as a probability distribution and the threshold is chosen to maximize the separability of the resultant foreground and background classes. Similarly, entropy measures have been used <ref> [1, 6, 12] </ref> to select the threshold which maximizes the sum of foreground and background entropies. In addition, Tsai [15] uses the threshold which best preserve the moment statistics of the binarized image as compared with the original grey-scale image.
Reference: [2] <editor> Mindy Bokser. Omnidocument Technoligies. </editor> <booktitle> Proceedings of The IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1066-1078, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Current optical character recognition (OCR) technology <ref> [2, 9] </ref> is largely restricted to recognize text printed against clean backgrounds. Most OCR engines require that the input image be binarized before the characters can be processed.
Reference: [3] <author> R. G. Casey and K. Y. Wong. </author> <title> Document Analysis System and Techniques. in Image Analysis Applications, </title> <editor> R. Kasturi and M. M. Trivedi eds. </editor> <publisher> Marcel Dekker, </publisher> <address> New York, N.Y., </address> <pages> pages 1-36, </pages> <year> 1990. </year>
Reference-contexts: Then, text features are measured from each binarized image. These features are used to pick the best threshold among the candidates. In contrast, adaptive algorithms compute a threshold for each pixel based on information extracted from its neighborhood (local window) <ref> [3, 5] </ref>. For images in which the intensity ranges of the foreground objects and backgrounds entangle, different thresholds must be used for different regions. Domain dependent infomation can also be coded in the algorithm to get the work done [5].
Reference: [4] <author> C. A. Glasbey. </author> <title> An Analysis of Histogram-Based Thresholding Algorithms. CVGIP: Graphical Models and Image Processing, </title> <booktitle> 55(6) </booktitle> <pages> 532-537, </pages> <month> Nov. </month> <year> 1993. </year>
Reference: [5] <author> Mohamed Kamel and Aiguo Zhao. </author> <title> Extraction of Binary Character/Graphics Images from Grayscale Document Images. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 55(3) </volume> <pages> 203-217, </pages> <month> May. </month> <year> 1993. </year>
Reference-contexts: Then, text features are measured from each binarized image. These features are used to pick the best threshold among the candidates. In contrast, adaptive algorithms compute a threshold for each pixel based on information extracted from its neighborhood (local window) <ref> [3, 5] </ref>. For images in which the intensity ranges of the foreground objects and backgrounds entangle, different thresholds must be used for different regions. Domain dependent infomation can also be coded in the algorithm to get the work done [5]. <p> For images in which the intensity ranges of the foreground objects and backgrounds entangle, different thresholds must be used for different regions. Domain dependent infomation can also be coded in the algorithm to get the work done <ref> [5] </ref>. <p> out that the clean-up output looks fine to a person in the places where the rest of the OCR errors occurred. 3.3 Comparison With Other Thresholding Methods In this section, we compare the performance of our algorithm with those of Tsai [15], Otsu [11] and Kamel and Zhao's adaptive method <ref> [5] </ref>. 12 image chips cropped from our test image set are used for this experiment. The total number of characters/words are counted as shown in Table 2. Each of these 6 (a) (b) (c) using Caere's WordScan Plus 4.0 on b. image chip is then binarized using the four algorithms.
Reference: [6] <author> J. N. Kapur, P. K. Sahoo, and A. K. C. Wong. </author> <title> A New Method for Gray-Level Picture Thresholding Using the Entropy of the Histogram. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 29(3) </volume> <pages> 273-285, </pages> <month> Mar. </month> <year> 1985. </year> <month> 10 </month>
Reference-contexts: Otsu [11] models the intensity histogram as a probability distribution and the threshold is chosen to maximize the separability of the resultant foreground and background classes. Similarly, entropy measures have been used <ref> [1, 6, 12] </ref> to select the threshold which maximizes the sum of foreground and background entropies. In addition, Tsai [15] uses the threshold which best preserve the moment statistics of the binarized image as compared with the original grey-scale image.
Reference: [7] <author> J. Kittler and J. Illingworth. </author> <title> Minimum Error Thresholding. </title> <journal> Pattern Recognition, </journal> <volume> 19(1) </volume> <pages> 41-47, </pages> <year> 1986. </year>
Reference-contexts: Some research has been carried out to overcome some of these problems. For example, weighted histograms [16] are used to balance the size difference between the foreground and background, and/or convert the valley-finding into maximum peak detection. Minimum-error thresholding <ref> [7, 18] </ref> models the foreground and background intensity distributions as Gaussian distributions and the threshold is selected to minimize the misclassification error. Otsu [11] models the intensity histogram as a probability distribution and the threshold is chosen to maximize the separability of the resultant foreground and background classes.
Reference: [8] <author> Ying Liu and Sargur N. Srihari. </author> <title> Document Image Binarization Based on Texture Features. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 19(5) </volume> <pages> 540-544, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: Similarly, entropy measures have been used [1, 6, 12] to select the threshold which maximizes the sum of foreground and background entropies. In addition, Tsai [15] uses the threshold which best preserve the moment statistics of the binarized image as compared with the original grey-scale image. Liu and Srihari <ref> [8] </ref> uses Otsu's algorithm [11] to obtain candidate 2 (a) (c) The OCR result of b using Caere's Wordscan Plus 4.0 . thresholds, each of which is used to produce a intermediate binarized image. Then, text features are measured from each binarized image.
Reference: [9] <author> S. Mori, C. Y. Suen, and K. Yamamoto. </author> <title> Historical Review of OCR Research and Development. </title> <booktitle> Proceedings of The IEEE, </booktitle> <volume> 80(7) </volume> <pages> 1029-1058, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Current optical character recognition (OCR) technology <ref> [2, 9] </ref> is largely restricted to recognize text printed against clean backgrounds. Most OCR engines require that the input image be binarized before the characters can be processed.
Reference: [10] <author> Lawrence O'Gorman. </author> <title> Binarization and Multithresholding of Document Images Using Connectivity. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 56(6) </volume> <pages> 494-506, </pages> <month> Nov. </month> <year> 1994. </year>
Reference: [11] <author> Nobuyuki Otsu. </author> <title> A Threshold Selection Method from Gray-Level Histogram. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics, </journal> <volume> SMC-9(1):62-66, </volume> <month> Jan. </month> <year> 1979. </year>
Reference-contexts: Minimum-error thresholding [7, 18] models the foreground and background intensity distributions as Gaussian distributions and the threshold is selected to minimize the misclassification error. Otsu <ref> [11] </ref> models the intensity histogram as a probability distribution and the threshold is chosen to maximize the separability of the resultant foreground and background classes. Similarly, entropy measures have been used [1, 6, 12] to select the threshold which maximizes the sum of foreground and background entropies. <p> In addition, Tsai [15] uses the threshold which best preserve the moment statistics of the binarized image as compared with the original grey-scale image. Liu and Srihari [8] uses Otsu's algorithm <ref> [11] </ref> to obtain candidate 2 (a) (c) The OCR result of b using Caere's Wordscan Plus 4.0 . thresholds, each of which is used to produce a intermediate binarized image. Then, text features are measured from each binarized image. <p> It should be pointed out that the clean-up output looks fine to a person in the places where the rest of the OCR errors occurred. 3.3 Comparison With Other Thresholding Methods In this section, we compare the performance of our algorithm with those of Tsai [15], Otsu <ref> [11] </ref> and Kamel and Zhao's adaptive method [5]. 12 image chips cropped from our test image set are used for this experiment. The total number of characters/words are counted as shown in Table 2.
Reference: [12] <author> T. Pun. </author> <title> Entropic Thresholding: A New Approach. CVGIP: Graphical Models and Image Processing, </title> <booktitle> 16(3) </booktitle> <pages> 210-239, </pages> <month> July </month> <year> 1981. </year>
Reference-contexts: Otsu [11] models the intensity histogram as a probability distribution and the threshold is chosen to maximize the separability of the resultant foreground and background classes. Similarly, entropy measures have been used <ref> [1, 6, 12] </ref> to select the threshold which maximizes the sum of foreground and background entropies. In addition, Tsai [15] uses the threshold which best preserve the moment statistics of the binarized image as compared with the original grey-scale image.
Reference: [13] <author> A. Rosenfeld and P. De La Torre. </author> <title> Histogram Concavity Analysis as an Aid in Threshold Selection. </title> <journal> IEEE Trans. Systems, Man, and Cybernetics, </journal> <volume> SMC-13:231-235, </volume> <year> 1983. </year> <title> [14] ivind Due Trier and Torfinn Taxt. Evaluation of Binarization Methods for Document Images. </title> <journal> IEEE Transactions on Pattern Analysis And Machine Intelligence, </journal> <volume> 17(3) </volume> <pages> 312-315, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Methods have also been proposed to facilitate more robust valley picking <ref> [13] </ref>. There are problems with the above global thresholding paradigm. First, due to noise and poor contrast, many images do not have well-differentiated foreground and background intensities. Second, the bimodal histogram assumption is often not valid for images of complicated documents such as advertisements and photographs.
Reference: [15] <author> W. H. Tsai. </author> <title> Moment-Preserving Thresholding: A New Approach. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 29(3) </volume> <pages> 377-393, </pages> <month> Mar. </month> <year> 1985. </year>
Reference-contexts: Similarly, entropy measures have been used [1, 6, 12] to select the threshold which maximizes the sum of foreground and background entropies. In addition, Tsai <ref> [15] </ref> uses the threshold which best preserve the moment statistics of the binarized image as compared with the original grey-scale image. <p> It should be pointed out that the clean-up output looks fine to a person in the places where the rest of the OCR errors occurred. 3.3 Comparison With Other Thresholding Methods In this section, we compare the performance of our algorithm with those of Tsai <ref> [15] </ref>, Otsu [11] and Kamel and Zhao's adaptive method [5]. 12 image chips cropped from our test image set are used for this experiment. The total number of characters/words are counted as shown in Table 2.
Reference: [16] <author> J. S. Weszka and A. Rosenfeld. </author> <title> Histogram Modification for Threshold Selection. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics, </journal> <volume> SMC-9(1):38-52, </volume> <month> Jan. </month> <year> 1979. </year>
Reference-contexts: Third, the foreground peak is often overshadowed by other peaks which makes the valley detection difficult or impossible. Some research has been carried out to overcome some of these problems. For example, weighted histograms <ref> [16] </ref> are used to balance the size difference between the foreground and background, and/or convert the valley-finding into maximum peak detection. Minimum-error thresholding [7, 18] models the foreground and background intensity distributions as Gaussian distributions and the threshold is selected to minimize the misclassification error.
Reference: [17] <author> Victor Wu, R. Manmatha, and Edward. M. Riseman. </author> <title> Finding Text In Images. </title> <booktitle> Proc. of the 2nd intl. conf. on Digital Libraries. </booktitle> <address> Philadaphia, PA, </address> <pages> pages 1-10, </pages> <month> July </month> <year> 1997. </year>
Reference-contexts: To extract text against darker background, a threshold at the last valley is picked instead. 4 The thresholded image is shown at bottom left in Figure 2. This has been success-fully recognized by an OCR system. Wu, Manmatha and Riseman <ref> [17] </ref> have developed a system called TextFinder which detects and generate text chips automatically. Thus, even though one threshold is used for the entire text chip, different threholds may be used for the input image. <p> In fact, no assumptions are made about the resolution of the input images, since such information is normally not available for the images from outside sources, such as those downloaded from the Internet. 3.1 Text Clean-up Characters and words (as perceived by one of the authors) detected by the TextFinder <ref> [17] </ref> were counted in each image (the ground truth). The total numbers over the whole test set are shown in the "Total Detected" column in Table 1. Then, characters and words which are clearly readable by a person after the binarization operation were counted for each image.
Reference: [18] <author> Q. Z. Ye and P. E. Danielson. </author> <title> On Minimum Error Thresholding and Its Implementation. </title> <journal> Pattern Recognition Letters, </journal> <volume> 7(4) </volume> <pages> 201-206, </pages> <month> Apr. </month> <year> 1988. </year> <month> 11 </month>
Reference-contexts: Some research has been carried out to overcome some of these problems. For example, weighted histograms [16] are used to balance the size difference between the foreground and background, and/or convert the valley-finding into maximum peak detection. Minimum-error thresholding <ref> [7, 18] </ref> models the foreground and background intensity distributions as Gaussian distributions and the threshold is selected to minimize the misclassification error. Otsu [11] models the intensity histogram as a probability distribution and the threshold is chosen to maximize the separability of the resultant foreground and background classes.
References-found: 17

