URL: http://www.eecs.uic.edu/~sloan/my-papers/ilp96.ps
Refering-URL: http://www.eecs.uic.edu/~sloan/papers.html
Root-URL: 
Phone: 2  3  
Title: Learning Logic Programs with Random Classification Noise  
Author: Tamas Horvath ? Robert H. Sloan ?? and Gyorgy Turan ??? 
Address: Szeged 6720, Hungary  851 S. Morgan St. Rm 1120, Chicago, IL 60607-7053, USA  Chicago,  
Affiliation: 1 Dept. of Applied Informatics, Jozsef A. University,  Dept. of EE Computer Science, University of Illinois at Chicago  Dept. of Mathematics, Stat., Computer Science, University of Illinois at  
Abstract: Research Group on Artificial Intelligence, Hungarian Academy of Sciences In Proc. 6th International Workshop on Inductive Logic Programming (ILP-96), pages 97-118 of Stockholm University Dept. of Comuputer and System Sciences Tech. report version of proceedings, August, 1996. Springer-Verlang Lecture Notes in Computer Science volume version of proceedings is forthcoming. Abstract. We consider the learnability of classes of logic programs in the presence of noise, assuming that the label of each example is reversed with a fixed probability. We review the polynomial PAC learnability of nonrecursive, determinate, constant-depth Horn clauses in the presence of such noise. This result is extended to an analogous class of recursive logic programs that consist of a recursive clause, a base case clause, and ground background knowledge. Also, we show that arbitrary nonrecur-sive Horn clauses with forest background knowledge remain polynomially PAC learnable in the presence of noise. We point out that the sample size can be decreased by using dependencies among the literals.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Angluin and P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: Computational learning theory has also long recognized that noise is of central importance for learnability. Several formal models have been developed based on different assumptions about the noise occurring in the data. In this paper, we consider the random classification noise model <ref> [1] </ref>: each labeled example has its label reversed with the same fixed probability. Kearns's statistical query method [12, 14] gives a general approach for the construction of provably ? Partially supported by OTKA grant T-14228 and Phare TDQM grant 9305-02/1022 (ILP2/HUN). Email: thorvath@sol.cc.u-szeged.hu. ?? Partially supported by NSF grant CCR-9314258. <p> Note: The value ff should be a lower bound for the smallest tolerance used by a statistical query algorithm that learns with an accuracy of *=2 (not *), for reasons we sketch in Appendix A. 4 Learning Conjunctions from Noisy Data Angluin and Laird <ref> [1] </ref> gave the first algorithm for PAC learning conjunctions from data with random classification noise. Their algorithm can be modified using the statistical query method to get an improved bound on the sample size. <p> Notice that if v is actually in the target concept, then p 01 (v) = 0. Any conjunction that includes all variables with p 01 = 0 and no harmful variables will differ from the target conjunction by at most * <ref> [1] </ref>. Thus a statistical query learning algorithm for conjunctions is to ask for p 01 (v) for every variable v with tolerance K=2, and include all those variables where the query is answered with a value of K=2 or less. <p> If computation time is not taken into account, then it is sufficient to take a sample of size linear in n and output a hypothesis that makes the least number of errors in the training set <ref> [1] </ref>. Another topic for further work is noise models between the random and the malicious classification models.
Reference: 2. <author> J. A. Aslam and S. E. Decatur. </author> <title> Improved noise-tolerant learning and generalized statistical queries. </title> <type> Technical Report TR-17-94, </type> <institution> Center for Research in Computing Technology, Division of Applied Sciences, Harvard University, </institution> <year> 1994. </year>
Reference-contexts: Our presentation generally follows Kearns and Vazirani's textbook [14], although we provide explicit, and slightly different, bounds on sample size instead of big-O bounds. Another proof of Theorem 1, below, is given by Aslam and Decatur <ref> [2] </ref>. Let us assume that we are trying to learn target concept C on instance space X, with distribution D. A statistical query learning algorithm has access to the Stat oracle instead of the usual Examples oracle. <p> However, it is an interesting feature of statistical query learning algorithms that this improvement in query complexity does not lead to an improvement in sample size for the following reason (also observed by Aslam and Decatur <ref> [2] </ref>). The queries of Algorithm Forest are non-adaptive in the sense that they can all be asked at the same time, without depending on the results of the other queries.
Reference: 3. <author> W. W. Cohen. </author> <title> Pac-learning recursive logic programs: efficient algorithms. </title> <journal> J. AI Research, </journal> <volume> 2 </volume> <pages> 501-539, </pages> <year> 1995. </year>
Reference-contexts: This class was shown to be PAC-learnable by Dzeroski, Muggleton, and Russell [7]. We review the algorithm to learn this class with random classification noise [6, 8], using the framework and the description of the noise-free learning algorithm given by Cohen <ref> [3] </ref>. The noisy learning algorithm proceeds by building a large set of literals that contains all possible literals that might occur in the body of the target clause, and then applying the noisy conjunction learning algorithm to find the body of the hypothesis clause. <p> As the sample size depends quadratically on the inverse of the smallest tolerance, this results in a significant decrease in the sample size. In Sect. 7 we consider the learnability of recursive logic programs in the presence of random classification noise. Cohen's results <ref> [3, 4] </ref> establish a sharp boundary for those classes of recursive logic programs that are PAC-learnable. In particular, he shows that a restricted class of logic programs consisting of a determinate recursive clause, a nonrecursive clause, and ground background knowledge is PAC-learnable. <p> The reason for this is that target concepts for application domains with noisy data appear to be nonrecursive (see e.g., Cohen <ref> [3] </ref>). The learning algorithm presented in Sect. 7 selects the literals to be included in the body of the recursive target clause using statistical queries, similarly to the nonrecursive case. <p> It is a hard problem to decide if a clause is determinate with respect to a background knowledge. The problem in general can be shown to be co-NP-complete. Cohen introduced natural additional conditions that lead to a nice syntactic class of determinate clauses <ref> [3] </ref>. The mode of an atom is a partition of its variables into input and output variables. For example, if Father (x; y) is a binary predicate corresponding to "the father of x is y", then [Father; +; ] is a mode. <p> We assume that the predicate Equal with mode [Equal; +; +] is part of B and M . Our results remain valid without the assumptions about the unique mode and the Equal predicate <ref> [3] </ref>. Now, a mode for a given predicate R is determinate with respect to a background knowledge B if the R-facts in B form a (partial) function mapping the input variables to the output variables. <p> We assume throughout that the arity of every background predicate is bounded by a constant. 2.2 Recursive Logic Programs Now we describe a modification of the framework above that is suitable for determinate recursive clauses, due to Cohen <ref> [3] </ref>. The basic objects are assumed to come from an infinite set, such as f0; 1g fl or IN fl , replacing the constants a 1 ; : : : ; a n . <p> We use Co-hen's approach <ref> [3] </ref> to the noise-free case as a starting point. We assume that we are given a background knowledge B, a depth bound d, and a mode constraint M that is determinate with respect to B. We are trying to learn a target concept from C B;d;M . <p> We are trying to learn a target concept from C B;d;M . One can construct a clause BOTTOM fl d (P; m; M ) in polynomial time such that the target clause must be semantically equivalent to one of its subclauses <ref> [3] </ref>. We could almost view the body of this clause as a large conjunction, and apply the algorithm of Sect. 4, with the atoms in the clause's body playing the role of the variables. <p> We call base cases meeting these assumptions simple base cases. Simple base cases are actually a special case of Cohen's "base case oracle" <ref> [3] </ref> that we believe can handle many practical logic programs. (Our assumptions can be slightly weakened; for instance, we could allow a constant number of such clauses.) In this section, for BOTTOM fl d (P; m; M ) we will just write BOTTOM, and we denote the literals of BOTTOM by <p> We also must show that the statistical query can be evaluated in polynomial time. A simple counting argument shows that there is a polynomial bound on the depth of recursion of such that if goes beyond this bound, then it will never succeed <ref> [3] </ref>. Thus the limit 1 in (8) can be replaced by a polynomial bound. In Step 4, we will need the correct Step 3 hypothesis to have error *=2. The reason for this is explained in Appendix A.2.
Reference: 4. <author> W. W. Cohen. </author> <title> Pac-learning recursive logic programs: negative results. </title> <journal> J. AI Research, </journal> <volume> 2 </volume> <pages> 541-573, </pages> <year> 1995. </year>
Reference-contexts: As the sample size depends quadratically on the inverse of the smallest tolerance, this results in a significant decrease in the sample size. In Sect. 7 we consider the learnability of recursive logic programs in the presence of random classification noise. Cohen's results <ref> [3, 4] </ref> establish a sharp boundary for those classes of recursive logic programs that are PAC-learnable. In particular, he shows that a restricted class of logic programs consisting of a determinate recursive clause, a nonrecursive clause, and ground background knowledge is PAC-learnable.
Reference: 5. <author> S. E. Decatur. </author> <title> Statistical queries and faulty PAC oracles. </title> <booktitle> In Proc. 6th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 262-268. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: For conjunctions, that meta-algorithm can tolerate a noise rate up to O (*=n), where n is the number of variables. For recent work on the fi (1=n) gap between that rate and the lower bound, see <ref> [5, 18] </ref>. Dzeroski discusses the application of the model to ILP [6]. In the case of malicious classification noise, the omnipotent, omniscient adversary can alter the label of some fraction of the training data, but not the instances.
Reference: 6. <author> S. Dzeroski. </author> <title> Learning first-order clausal theories in the presence of noise. </title> <booktitle> In Proc. 5th Scandinavian Conf. on Artificial Intelligence, </booktitle> <address> Amsterdam, 1995. </address> <publisher> IOS Press. </publisher>
Reference-contexts: His results apply to nonrecursive Horn clauses of constant locality. Dzeroski <ref> [6] </ref>, besides proving related results, gave positive results in the framework of nonmonotonic ILP. <p> His results apply to nonrecursive Horn clauses of constant locality. Dzeroski [6], besides proving related results, gave positive results in the framework of nonmonotonic ILP. Both discussed the learnability of restricted multiple clause nonrecursive logic programs with noise <ref> [6, 8] </ref>, although mostly not in the PAC setting studied in this paper, where we assume that the hypothesis class and the concept class are the same, and permit arbitrary probability distributions on the examples. <p> Section 5 discusses nonrecursive determinate constant-depth Horn clauses (defined by Mug-gleton and Feng [19]). This class was shown to be PAC-learnable by Dzeroski, Muggleton, and Russell [7]. We review the algorithm to learn this class with random classification noise <ref> [6, 8] </ref>, using the framework and the description of the noise-free learning algorithm given by Cohen [3]. <p> This modification does not change the asymptotic sample complexity of the algorithm, but the constants do increase so the sample complexity becomes 1 ffi + ln 8n 5 Nonrecursive Determinate Constant-Depth Clauses We present a version of the algorithm to learn nonrecursive, determinate constant-depth Horn clauses with random classification noise <ref> [6, 8] </ref>. We use Co-hen's approach [3] to the noise-free case as a starting point. We assume that we are given a background knowledge B, a depth bound d, and a mode constraint M that is determinate with respect to B. <p> For conjunctions, that meta-algorithm can tolerate a noise rate up to O (*=n), where n is the number of variables. For recent work on the fi (1=n) gap between that rate and the lower bound, see [5, 18]. Dzeroski discusses the application of the model to ILP <ref> [6] </ref>. In the case of malicious classification noise, the omnipotent, omniscient adversary can alter the label of some fraction of the training data, but not the instances. The difference between this model and random classification noise is that the adversary may choose not to alter the labels of some examples. <p> Another issue is to study types of noise that are specific to learning in predicate logic, such as noise in the background knowledge. Gennaro [8] presents an initial proposal in this direction. Acknowledgments We thank an anonymous referee for calling our attention to references <ref> [6] </ref> and [8]. We thank Jorge Lobo for useful discussions.
Reference: 7. <author> S. Dzeroski, S. Muggleton, and S. Russell. </author> <title> PAC-learnability of determinate logic programs. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 128-135. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: In Sects. 3 and 4 we present the statistical query model and the standard algorithm for learning Boolean conjunctions with statistical queries. Section 5 discusses nonrecursive determinate constant-depth Horn clauses (defined by Mug-gleton and Feng [19]). This class was shown to be PAC-learnable by Dzeroski, Muggleton, and Russell <ref> [7] </ref>. We review the algorithm to learn this class with random classification noise [6, 8], using the framework and the description of the noise-free learning algorithm given by Cohen [3].
Reference: 8. <author> R. Gennaro. </author> <title> PAC-learning PROLOG clauses with or without errors. </title> <type> Tech. Memo 500, </type> <institution> MIT Laboratory for Computer Science, </institution> <year> 1994. </year>
Reference-contexts: The usual approaches to these problems, such as least general generalizations, assume that the data are noiseless and thus cannot be used in the present context.) Gennaro <ref> [8] </ref> obtained the first theoretical results on learnability in predicate logic in the presence of noise, using the basic argument outlined above. His results apply to nonrecursive Horn clauses of constant locality. Dzeroski [6], besides proving related results, gave positive results in the framework of nonmonotonic ILP. <p> His results apply to nonrecursive Horn clauses of constant locality. Dzeroski [6], besides proving related results, gave positive results in the framework of nonmonotonic ILP. Both discussed the learnability of restricted multiple clause nonrecursive logic programs with noise <ref> [6, 8] </ref>, although mostly not in the PAC setting studied in this paper, where we assume that the hypothesis class and the concept class are the same, and permit arbitrary probability distributions on the examples. <p> Section 5 discusses nonrecursive determinate constant-depth Horn clauses (defined by Mug-gleton and Feng [19]). This class was shown to be PAC-learnable by Dzeroski, Muggleton, and Russell [7]. We review the algorithm to learn this class with random classification noise <ref> [6, 8] </ref>, using the framework and the description of the noise-free learning algorithm given by Cohen [3]. <p> This modification does not change the asymptotic sample complexity of the algorithm, but the constants do increase so the sample complexity becomes 1 ffi + ln 8n 5 Nonrecursive Determinate Constant-Depth Clauses We present a version of the algorithm to learn nonrecursive, determinate constant-depth Horn clauses with random classification noise <ref> [6, 8] </ref>. We use Co-hen's approach [3] to the noise-free case as a starting point. We assume that we are given a background knowledge B, a depth bound d, and a mode constraint M that is determinate with respect to B. <p> It would be interesting to study models where the error probabilities of different instances may differ, as appears to be the case in practice. Another issue is to study types of noise that are specific to learning in predicate logic, such as noise in the background knowledge. Gennaro <ref> [8] </ref> presents an initial proposal in this direction. Acknowledgments We thank an anonymous referee for calling our attention to references [6] and [8]. We thank Jorge Lobo for useful discussions. <p> Another issue is to study types of noise that are specific to learning in predicate logic, such as noise in the background knowledge. Gennaro <ref> [8] </ref> presents an initial proposal in this direction. Acknowledgments We thank an anonymous referee for calling our attention to references [6] and [8]. We thank Jorge Lobo for useful discussions.
Reference: 9. <author> S. A. Goldman and R. H. Sloan. </author> <title> Can PAC learning algorithms tolerate random attribute noise? Algorithmica, </title> <booktitle> 14 </booktitle> <pages> 70-84, </pages> <year> 1995. </year>
Reference-contexts: If this noise is uniform, then each variable is altered with the same probability; with product random attribute noise, there is a distinct probability p i of each variable i being flipped. For some simple classes including conjunctions, large amounts of uniform random attribute noise can be tolerated <ref> [9, 20] </ref>. For the more realistic case of product random attribute noise, the noise rate (i.e., maximum probability that any bit is flipped) must be less than * for PAC learning to be possible [9]. <p> For the more realistic case of product random attribute noise, the noise rate (i.e., maximum probability that any bit is flipped) must be less than * for PAC learning to be possible <ref> [9] </ref>. Thus, somewhat surprisingly, the PAC algorithms situation for product random attribute noise is essentially the same as for malicious noise. 8.2 Further Research The bounds of Sect. 6 can perhaps be improved by considering Step 3 of Algorithm Forest more carefully.
Reference: 10. <author> W. Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58(301) </volume> <pages> 13-30, </pages> <month> Mar. </month> <year> 1963. </year>
Reference: 11. <author> T. Horvath and G. Turan. </author> <title> Learning logic programs with structured background knowledge. </title> <editor> In L. De Raedt, editor, </editor> <booktitle> 5th Int. Workshop on Inductive Logic Programming, </booktitle> <pages> pages 53-76, </pages> <year> 1995. </year> <note> Also in Advances in Inductive Logic Programming (ed. </note> <editor> L. De Raedt). </editor> <publisher> IOS Press, </publisher> <year> 1996, </year> <pages> pages 172-191. </pages> <booktitle> (IOS Frontiers in AI and Appl.). </booktitle>
Reference-contexts: In Sect. 6 we consider nonrecursive Horn clauses with forest background knowledge. This is a class of unrestricted clauses with a single binary background predicate that was proven to be PAC-learnable by Horvath and Turan <ref> [11] </ref>. Although these clauses are not necessarily determinate, one can still view the body of such a clause as a conjunction of literals (corresponding to new predicates) depending only on the variables occurring in the head. <p> The facts R (a i ; a j ) belonging to the background knowledge B may be viewed as a directed graph with edges (a i ; a j ). Horvath and Turan <ref> [11] </ref> showed that if this graph is a forest with edges directed towards the roots, then a single non-recursive Horn clause is polynomially P AC-learnable. Their algorithm is based on computing least general generalizations, and thus it is not noise tolerant. <p> Every concept defined by a nonrecursive Horn clause has a definition of the form P (x) L 1 (x); : : : ; L s (x), where each literal in the body is of the form E u;v;d 1 ;d 2 (x), F u;l;k (x), or I u;j (x) <ref> [11] </ref>.
Reference: 12. <author> M. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proc. 25th Annu. ACM Sympos. Theory Comput., </booktitle> <pages> pages 392-401. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: Several formal models have been developed based on different assumptions about the noise occurring in the data. In this paper, we consider the random classification noise model [1]: each labeled example has its label reversed with the same fixed probability. Kearns's statistical query method <ref> [12, 14] </ref> gives a general approach for the construction of provably ? Partially supported by OTKA grant T-14228 and Phare TDQM grant 9305-02/1022 (ILP2/HUN). Email: thorvath@sol.cc.u-szeged.hu. ?? Partially supported by NSF grant CCR-9314258. <p> Kearns's statistical query method <ref> [12, 14] </ref> allows one to learn most, although not all, PAC-learnable concept classes with noisy data of this type. Here we will present an overview of the method. In Appendix A we will prove some of the specific bounds on sample size. <p> We assume that we have a statistical query algorithm that draws all its queries from a finite set Q, and that the smallest tolerance that it ever uses in any query is ff. We also assume we are given an upper bound b on the actual noise rate. (Kearns <ref> [12] </ref> shows how to remove this assumption.) Moreover, our final goal is to PAC learn with the usual parameters * and ffi. <p> Kearns <ref> [12] </ref> showed that the sample size can be improved if the conjunctions are known to contain few literals. The bound of [12] applied to our case is of the order (m 2 + mn) 3 . We now give another statistical query learning algorithm for this particular case. <p> Kearns <ref> [12] </ref> showed that the sample size can be improved if the conjunctions are known to contain few literals. The bound of [12] applied to our case is of the order (m 2 + mn) 3 . We now give another statistical query learning algorithm for this particular case. The sample size provided by this algorithm is of the order m 4 n 2 .
Reference: 13. <author> M. Kearns and M. Li. </author> <title> Learning in the presence of malicious errors. </title> <journal> SIAM J. Comput., </journal> <volume> 22 </volume> <pages> 807-837, </pages> <year> 1993. </year>
Reference-contexts: With malicious noise, an omnipotent, omniscient adversary (i.e., the adversary has unlimited computing time, and knowledge of the learner's algorithm) gets to alter a small fraction of the training examples in any way he chooses. Kearns and Li <ref> [13] </ref> show that the fraction of the examples the adversary alters (i.e., the noise rate) must be less than *=(1 + *) (where * as usual is the desired accuracy) for PAC learning to be possible.
Reference: 14. <author> M. J. Kearns and U. V. Vazirani. </author> <title> An Introduction to Computational Learning Theory. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1994. </year>
Reference-contexts: Several formal models have been developed based on different assumptions about the noise occurring in the data. In this paper, we consider the random classification noise model [1]: each labeled example has its label reversed with the same fixed probability. Kearns's statistical query method <ref> [12, 14] </ref> gives a general approach for the construction of provably ? Partially supported by OTKA grant T-14228 and Phare TDQM grant 9305-02/1022 (ILP2/HUN). Email: thorvath@sol.cc.u-szeged.hu. ?? Partially supported by NSF grant CCR-9314258. <p> Determinateness is required for each example with respect to B and its description. 2.3 PAC Learning and Random Classification Noise Now we give a brief definition of PAC-learnability. Our description is informal; more details are given, for example, in Kearns and Vazirani <ref> [14] </ref>. It is assumed that there is a probability distribution on the set of all ground P -atoms. A learning algorithm first draws a sequence of random examples and then it outputs a hypothesis from the target class. <p> Kearns's statistical query method <ref> [12, 14] </ref> allows one to learn most, although not all, PAC-learnable concept classes with noisy data of this type. Here we will present an overview of the method. In Appendix A we will prove some of the specific bounds on sample size. <p> Here we will present an overview of the method. In Appendix A we will prove some of the specific bounds on sample size. Our presentation generally follows Kearns and Vazirani's textbook <ref> [14] </ref>, although we provide explicit, and slightly different, bounds on sample size instead of big-O bounds. Another proof of Theorem 1, below, is given by Aslam and Decatur [2]. Let us assume that we are trying to learn target concept C on instance space X, with distribution D. <p> We also assume we are given an upper bound b on the actual noise rate. (Kearns [12] shows how to remove this assumption.) Moreover, our final goal is to PAC learn with the usual parameters * and ffi. We use ff (1 2 b ) 2 =16 <ref> [14] </ref>. 4 The other two values are m 1 = ff 2 (1 2 b ) 2 ln 12jQj m 2 = * 2 (1 2 b ) 2 ln 16 (1 2 b ) 2 ffffi The total number of samples required is m 1 + m 2 . <p> Let us assume that our target concept C is a conjunction of some subset of a set of n given variables. (That is, negation is not allowed. It is easy to generalize 4 Kearns and Vazirani <ref> [14] </ref> show that = c 0 ff 0 (1 2) 2 =2, where ff 0 is a parameter smaller than ff which we show in Appendix A can be safely set to ff=8. <p> Another topic for further work is noise models between the random and the malicious classification models. For example, Kearns <ref> [14] </ref> considers the case where the noise rate may be different for each round of drawing a random example, but it is still assumed that in each round the error probability is the same for each instance. He showed that statistical query algorithms generalize to this model.
Reference: 15. <author> N. Lavrac and S. Dzeroski. </author> <title> Inductive learning of relations from noisy examples. </title> <editor> In S. H. Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 495-514, </pages> <address> London, 1992. </address> <publisher> Academic Press. </publisher>
Reference-contexts: 1 Introduction Learnability in the presence of noise is a major issue in machine learning. Its importance in inductive logic programming (ILP) is illustrated by the great emphasis given to noise handling mechanisms in Lavrac and Dzeroski's book [16], and by numerous papers (e.g., <ref> [15, 17, 22] </ref>). Noise-tolerant heuristics often depend on probabilities estimated from the training set. Such heuristics usually work because a moderate amount of noise typically alters these estimates only slightly. Computational learning theory has also long recognized that noise is of central importance for learnability.
Reference: 16. <author> N. Lavrac and S. Dzeroski. </author> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Learnability in the presence of noise is a major issue in machine learning. Its importance in inductive logic programming (ILP) is illustrated by the great emphasis given to noise handling mechanisms in Lavrac and Dzeroski's book <ref> [16] </ref>, and by numerous papers (e.g., [15, 17, 22]). Noise-tolerant heuristics often depend on probabilities estimated from the training set. Such heuristics usually work because a moderate amount of noise typically alters these estimates only slightly. <p> We can sometimes get such a reduction in the size of the learned conjunction by omitting those variables that are almost always 1. Let us call a variable insignificant (or irrelevant in the terminology of Lavrac and Dzeroski <ref> [16] </ref>) if the probability that it is false in any example is less than *=2n. If our learning algorithm omits all insignificant variables from its output, it increases its error rate by at most *=2.
Reference: 17. <author> N. Lavrac, S. Dzeroski, and I. Bratko. </author> <title> Handling imperfect data in inductive logic programming. </title> <editor> In L. De Raedt, editor, </editor> <booktitle> Advances in Inductive Logic Programming, </booktitle> <pages> pages 48-64. </pages> <publisher> IOS Press, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction Learnability in the presence of noise is a major issue in machine learning. Its importance in inductive logic programming (ILP) is illustrated by the great emphasis given to noise handling mechanisms in Lavrac and Dzeroski's book [16], and by numerous papers (e.g., <ref> [15, 17, 22] </ref>). Noise-tolerant heuristics often depend on probabilities estimated from the training set. Such heuristics usually work because a moderate amount of noise typically alters these estimates only slightly. Computational learning theory has also long recognized that noise is of central importance for learnability.
Reference: 18. <author> Y. Mansour and M. Parnas. </author> <title> On learning conjunctions with malicious noise. </title> <booktitle> In Israel System and Theory Computer Symposium (ISTCS 96), </booktitle> <year> 1996. </year> <note> (To appear). </note>
Reference-contexts: For conjunctions, that meta-algorithm can tolerate a noise rate up to O (*=n), where n is the number of variables. For recent work on the fi (1=n) gap between that rate and the lower bound, see <ref> [5, 18] </ref>. Dzeroski discusses the application of the model to ILP [6]. In the case of malicious classification noise, the omnipotent, omniscient adversary can alter the label of some fraction of the training data, but not the instances.
Reference: 19. <author> S. Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 281-298. </pages> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: We consider single unrestricted nonrecursive clauses with restricted background knowledge, and restricted recursive clauses. In Sects. 3 and 4 we present the statistical query model and the standard algorithm for learning Boolean conjunctions with statistical queries. Section 5 discusses nonrecursive determinate constant-depth Horn clauses (defined by Mug-gleton and Feng <ref> [19] </ref>). This class was shown to be PAC-learnable by Dzeroski, Muggleton, and Russell [7]. We review the algorithm to learn this class with random classification noise [6, 8], using the framework and the description of the noise-free learning algorithm given by Cohen [3].
Reference: 20. <author> G. Shackelford and D. Volper. </author> <title> Learning k-DNF with noise in the attributes. </title> <booktitle> In Proc. 1st Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 97-103, </pages> <address> San Mateo, CA, 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We now present a brief overview of other models of noise in the learning theory literature. Some models that make the most sense from the point of view of ILP include malicious noise [23], malicious classification noise [21], and random attribute noise <ref> [20, 21] </ref>. With malicious noise, an omnipotent, omniscient adversary (i.e., the adversary has unlimited computing time, and knowledge of the learner's algorithm) gets to alter a small fraction of the training examples in any way he chooses. <p> If this noise is uniform, then each variable is altered with the same probability; with product random attribute noise, there is a distinct probability p i of each variable i being flipped. For some simple classes including conjunctions, large amounts of uniform random attribute noise can be tolerated <ref> [9, 20] </ref>. For the more realistic case of product random attribute noise, the noise rate (i.e., maximum probability that any bit is flipped) must be less than * for PAC learning to be possible [9].
Reference: 21. <author> R. H. Sloan. </author> <title> Four types of noise in data for PAC learning. </title> <journal> Inf. Process. Lett., </journal> <volume> 54 </volume> <pages> 157-162, </pages> <year> 1995. </year>
Reference-contexts: We now present a brief overview of other models of noise in the learning theory literature. Some models that make the most sense from the point of view of ILP include malicious noise [23], malicious classification noise <ref> [21] </ref>, and random attribute noise [20, 21]. With malicious noise, an omnipotent, omniscient adversary (i.e., the adversary has unlimited computing time, and knowledge of the learner's algorithm) gets to alter a small fraction of the training examples in any way he chooses. <p> We now present a brief overview of other models of noise in the learning theory literature. Some models that make the most sense from the point of view of ILP include malicious noise [23], malicious classification noise [21], and random attribute noise <ref> [20, 21] </ref>. With malicious noise, an omnipotent, omniscient adversary (i.e., the adversary has unlimited computing time, and knowledge of the learner's algorithm) gets to alter a small fraction of the training examples in any way he chooses. <p> statistical query method relies heavily on the independence of instances and label noise, and, as far as we know, does not work at all for random misclassifica-tion noise. (If computational efficiency is not an issue, then it suffices to choose the hypothesis that has minimal disagreement with the training data <ref> [21] </ref>, but this is almost always NP-hard.) With random attribute noise, the label is always accurate, but the value of each Boolean variable in each training instance is flipped with some probability.
Reference: 22. <author> A. Srinivasan, S. H. Muggleton, and M. Bain. </author> <title> Distinguishing exceptions from noise in non-monotonic learning. </title> <booktitle> In Proc. Second International Workshop on Inductive Logic Programming, </booktitle> <address> Tokyo, Japan, 1992. </address> <publisher> ICOT TM-1182. </publisher>
Reference-contexts: 1 Introduction Learnability in the presence of noise is a major issue in machine learning. Its importance in inductive logic programming (ILP) is illustrated by the great emphasis given to noise handling mechanisms in Lavrac and Dzeroski's book [16], and by numerous papers (e.g., <ref> [15, 17, 22] </ref>). Noise-tolerant heuristics often depend on probabilities estimated from the training set. Such heuristics usually work because a moderate amount of noise typically alters these estimates only slightly. Computational learning theory has also long recognized that noise is of central importance for learnability.
Reference: 23. <author> L. G. Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proceedings of the 9th International Joint Conference on Artificial Intelligence, </booktitle> <volume> vol. 1, </volume> <pages> pages 560-566, </pages> <address> Los Angeles, California, </address> <year> 1985. </year> <booktitle> International Joint Committee for Artificial Intelligence. </booktitle>
Reference-contexts: We now present a brief overview of other models of noise in the learning theory literature. Some models that make the most sense from the point of view of ILP include malicious noise <ref> [23] </ref>, malicious classification noise [21], and random attribute noise [20, 21]. With malicious noise, an omnipotent, omniscient adversary (i.e., the adversary has unlimited computing time, and knowledge of the learner's algorithm) gets to alter a small fraction of the training examples in any way he chooses.
References-found: 23

