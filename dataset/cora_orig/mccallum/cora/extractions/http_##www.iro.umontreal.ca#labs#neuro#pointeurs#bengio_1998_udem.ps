URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/bengio_1998_udem.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/other.html
Root-URL: http://www.iro.umontreal.ca
Email: samy@labs.microcell.ca  bengioy@iro.umontreal.ca  robertj@cirano.umontreal.ca  belangerg@cirano.umontreal.ca  
Title: Stochastic Learning of Strategic Equilibria for Auctions  
Author: Samy Bengio Yoshua Bengio Jacques Robert Gilles Belanger 
Date: April 15, 1998  
Address: 1250, Rene-Levesque West, suite 400 Montreal, Quebec, Canada, H3B 4W8  Montreal, Quebec, Canada, H3C 3J7  Montreal, Quebec, Canada, H3C 3J7  Montreal, Quebec, Canada, H3C 3J7  
Affiliation: Microcell Labs  CIRANO and Dept. IRO Universite de Montreal  CIRANO and Dept. Sciences Economiques, Universite de Montreal  Dept. Sciences Economiques Universite de Montreal  
Abstract: Technical Report #1119, Departement d'Informatique et Recherche Operationnelle, Universite de Montreal Abstract This paper presents a new application of stochastic adaptive learning algorithms to the computation of strategic equilibria in auctions. The proposed approach addresses the problems of tracking a moving target and balancing exploration (of action space) versus exploitation (of better modeled regions of action space). Neural networks are used to represent a stochastic decision model for each bidder. Experiments confirm the correctness and usefulness of the approach. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G. </author> <year> (1992). </year> <title> Connectionist learning for control: An overview. </title> <editor> In Miller, W., Sutton, R., and Werbos, P., editors, </editor> <title> Neural Networks for Control. </title> <publisher> MIT Press. </publisher>
Reference: <author> Dorsey, R., Johnson, J., and Van Boening, M. </author> <year> (1994). </year> <title> The use of artificial neural networks for estimation of decision surfaces in first price sealed bid auctions. </title> <booktitle> In New Directions in Computational Economics, </booktitle> <pages> pages 19-39. </pages> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Whereas previous work on the application of neural networks to auctions focused on emulating the behavior of human players or improving a decision model when the other players are fixed <ref> (Dorsey, Johnson and Van Boening, 1994) </ref>, the objective of this paper is to provide new numerical techniques to characterize strategic equilibria in auctions, i.e., take into account the feedback of the actions of one player through the strategies of the others.
Reference: <author> Gullapalli, V. </author> <year> (1990). </year> <title> A stochastic reinforcement learning algorithm for learning real-valued functions. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 671-692. </pages>
Reference-contexts: SLAs have usually a finite set of actions whereas we consider a continuous range of actions. SLAs are usually trained using sample rewards where we propose to optimize the expected utility. Gullapalli <ref> (Gullapalli, 1990) </ref> and Williams (Williams, 1992) also used a probability distribution for the actions. In (Gullapalli, 1990), the param eters (mean, standard deviation) of this distribution (a normal) were not trained using the expected utility. <p> SLAs have usually a finite set of actions whereas we consider a continuous range of actions. SLAs are usually trained using sample rewards where we propose to optimize the expected utility. Gullapalli <ref> (Gullapalli, 1990) </ref> and Williams (Williams, 1992) also used a probability distribution for the actions. In (Gullapalli, 1990), the param eters (mean, standard deviation) of this distribution (a normal) were not trained using the expected utility.
Reference: <author> Holland, J. </author> <year> (1975). </year> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press. </publisher>
Reference: <author> McAfee, R. and McMillan, J. </author> <year> (1987). </year> <title> Auctions and bidding. </title> <journal> Journal of Economic Literature, XXV:699-738. </journal>
Reference-contexts: In this paper, we focus on the application to auctions. An auction is a market mechanism with a set of rules that determine who gets the goods and at what price, based on the bids of the participants. Auctions appear in many different forms (see <ref> (McAfee and McMillan, 1987) </ref>). Auction theory is one of the applications of game theory which has generated considerable interest (McMillan, 1994). Unfortunately theoretical analysis of auctions has some limits.
Reference: <author> McMillan, J. </author> <year> (1994). </year> <title> Selling spectrum rights. </title> <journal> Journal of Economic Perspectives, </journal> <volume> 8 </volume> <pages> 145-162. </pages>
Reference-contexts: Auctions appear in many different forms (see (McAfee and McMillan, 1987)). Auction theory is one of the applications of game theory which has generated considerable interest <ref> (McMillan, 1994) </ref>. Unfortunately theoretical analysis of auctions has some limits. One of the main difficulty in pursuing theoretical research on auctions is that all but the simplest auctions are impossible to solve analytically.
Reference: <author> Narendra, K. and Thathachar, M. </author> <year> (1989). </year> <title> Learning Automata: an introduction. </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: The approach proposed here is quite general and can be applied to many game-theoretical problems. A lot of research has been done in the field of stochastic learning automata applied to game problems. A good review can be found in <ref> (Narendra and Thathachar, 1989) </ref>. We will explain in section 3 the main differences between our approach and others. In this paper, we focus on the application to auctions.
Reference: <author> Schaerf, A., Yoav, S., and Tennenholtz, M. </author> <year> (1995). </year> <title> Adaptive load balancing: a study in multi-agent learning. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 475-500. </pages>
Reference: <author> Williams, R. </author> <year> (1992). </year> <title> Simple statistical gradient-following for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 229-256. 10 </pages>
Reference-contexts: SLAs have usually a finite set of actions whereas we consider a continuous range of actions. SLAs are usually trained using sample rewards where we propose to optimize the expected utility. Gullapalli (Gullapalli, 1990) and Williams <ref> (Williams, 1992) </ref> also used a probability distribution for the actions. In (Gullapalli, 1990), the param eters (mean, standard deviation) of this distribution (a normal) were not trained using the expected utility.
References-found: 9

