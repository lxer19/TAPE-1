URL: http://www-dbv.cs.uni-bonn.de/postscript/hofmann.nips97.ps.gz
Refering-URL: http://www-dbv.cs.uni-bonn.de/abstracts/hofmann.nips97.html
Root-URL: http://cs.uni-bonn.de
Email: hofmann@psyche.mit.edu  jb@cs.uni-bonn.de  
Title: Active Data Clustering  
Author: Thomas Hofmann Joachim M. Buhmann 
Address: Cambridge 02139, MA, USA,  Romerstrae 164, D-53117 Bonn, Germany,  
Affiliation: Center for Biological and Computational Learning, MIT  Institut fur Informatik III, Universitat Bonn  
Abstract: Active data clustering is a novel technique for clustering of proximity data which utilizes principles from sequential experiment design in order to interleave data generation and data analysis. The proposed active data sampling strategy is based on the expected value of information, a concept rooting in statistical decision theory. This is considered as an important step towards the analysis of large-scale data sets, because it offers a way to overcome the inherent data sparseness of proximity data. We present applications to unsupervised texture segmentation in computer vision and information retrieval in document databases. 
Abstract-found: 1
Intro-found: 1
Reference: [Geman et al., 1990] <author> Geman, D., Geman, S., Graffigne, C., Dong, P. </author> <year> (1990). </year> <title> Boundary Detection by Constrained optimization. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(7), </volume> <pages> 609-628. </pages>
Reference-contexts: In the unsupervised case, this has to be achieved on the basis of texture similarities without prior knowledge about the occuring textures. Our approach follows the ideas of <ref> [Geman et al., 1990] </ref> to apply a statistical test to empirical distributions of image features at different sites. Suppose we decided to work with the gray-scale representation directly. At every image location p = (x; y) we consider a local sample of gray-values, e.g., in a squared neighborhood around p.
Reference: [Geman, Geman, 1984] <author> Geman, S., Geman, D. </author> <year> (1984). </year> <title> Stochastic relaxation, Gibbs Distributions, and the Bayesian Restoration of Images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6(6), </volume> <pages> 721-741. </pages>
Reference-contexts: To avoid unfavorable local minima one may also introduce a computational temperature T and utilize fg i g for simulated annealing based on the Gibbs sampler <ref> [Geman, Geman, 1984] </ref>, P fM iff = 1g = exp 1 fl P K fi T g i : Alternatively, Eq. (4) may also serve as the starting point to derive mean-field equations in a deterministic annealing framework, cf. [Hofmann, Buhmann, 1997].
Reference: [Hofmann, Buhmann, 1997] <author> Hofmann, Th., Buhmann, J. M. </author> <year> (1997). </year> <title> Pairwise Data Clustering by Deterministic Annealing. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 19(1), </volume> <pages> 1-14. </pages>
Reference-contexts: fg i g for simulated annealing based on the Gibbs sampler [Geman, Geman, 1984], P fM iff = 1g = exp 1 fl P K fi T g i : Alternatively, Eq. (4) may also serve as the starting point to derive mean-field equations in a deterministic annealing framework, cf. <ref> [Hofmann, Buhmann, 1997] </ref>. These local optimization algorithms are well-suited for an incremental update after new data has been sampled, as they do not require a complete re-calculation.
Reference: [Hofmann et al., 1997] <author> Hofmann, Th., Puzicha, J., Buhmann, J.M. </author> <year> (1997). </year> <title> Deterministic Annealing for Unsupervised Texture Segmentation. In: Workshop on Energy Minimization Methods in Computer Vision and Pattern Recognition (EMMCVPR), Venice (to appear). </title> <booktitle> Lecture Notes on Computer Science. </booktitle> <publisher> Springer Verlag. </publisher>
Reference-contexts: ) f ij (t k )) 2 =f ij (t k ); with f ij (t k ) = (f i (t k ) + f j (t k ))=2 : (1) In fact, our experiments are based on a multi-scale Gabor filter representation instead of the raw data, cf. <ref> [Hofmann et al., 1997] </ref> for more details. The main advantage of the similarity-based approach is that it does not reduce the distributional information, e.g., to some simple first and second order statistics, before comparing textures. <p> In the case of similarity-based clustering this is not at all a trivial problem and we have systematically developed an axiomatic approach based on invariance and robustness principles <ref> [Hofmann et al., 1997] </ref>. Here, we can only give some informal justifications for our choice. Let us introduce indicator functions to represent data partitionings, M i being the indicator function for entity o i belonging to cluster C .
Reference: [Jain, Dubes, 1988] <author> Jain, A. K., Dubes, R. C. </author> <year> (1988). </year> <title> Algorithms for Clustering Data. </title> <address> Englewood Cliffs, NJ 07632: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: 1 Introduction Data clustering is one of the core methods for numerous tasks in pattern recognition, exploratory data analysis, computer vision, machine learning, data mining, and in many other related fields. Concerning the data representation it is important to distinguish between vectorial data and proximity data, cf. <ref> [Jain, Dubes, 1988] </ref>. In vectorial data each measurement corresponds to a certain `feature' evaluated at an external scale. The elementary measurements of proximity data are, in contrast, (dis-)similarity values obtained by comparing pairs of entities from a given data set.
Reference: [Raiffa, Schlaifer, 1961] <author> Raiffa, H., Schlaifer, R. </author> <year> (1961). </year> <title> Applied Statistical Decision Theory. </title> <address> Cambridge MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: To model the problem of selecting an optimal experiment we follow the Bayesian approach developed by Raiffa & Schlaifer <ref> [Raiffa, Schlaifer, 1961] </ref> and compute the so-called Expected Value of Sampling Information (EVSI). As a fundamental step this involves the calculation of distributions for the quantities d N . <p> The outcome of additional experiments can only be anticipated by making use the information which is already available. This is known as preposterior analysis. The linearity of the utility measure implies that it suffices to calculate averages with respect to the preposterous distribution <ref> [Raiffa, Schlaifer, 1961, Chapter 5.3] </ref>.
Reference: [Van Rijsbergen, 1979] <author> Van Rijsbergen, C. J. </author> <year> (1979). </year> <title> Information Retrieval. </title> <address> But-terworths, London Boston. </address>
Reference-contexts: As a second application we consider structuring a database of documents for improved information retrieval. Typical measures of association are based on the number of shared index terms <ref> [Van Rijsbergen, 1979] </ref>. For example, a document is represented by a (sparse) binary vector B, where each entry corresponds to the occurrence of a certain index term.
References-found: 7

