URL: http://www.cs.indiana.edu/l/www/pub/port/pollack.ps
Refering-URL: http://www.cs.indiana.edu/l/www/pub/port/
Root-URL: http://www.cs.indiana.edu
Email: ollack@cis.ohio-state.edu  
Phone: 1 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 1 0 1 0 1 1  
Title: %I didnt finish the addendum yet but %but added some stuff up front and %played
Author: L Jordan B. Pollack p enter box; c e c l e l 
Note: erate  
Web: A  
Address: 2036 Neil Avenue olumbus, OH 43210  REJECT i  
Affiliation: aboratory for AI Research tComputer Information Science Departmen The Ohio State University C  ACCEPT  
Abstract: higher order recurrent neural network architecture learns to recognize and gen - s a "bifurcation" in the limit behavior of the network. This phase transition correspond o the onset of the network's capacity for generalizing to arbitrary-length strings. t Second, a study of the automata resulting from the acquisition of previously published raining sets indicates that while the architecture is not guaranteed to find a minimal , 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, J. A., Silverstein, J. W., Ritz, S. A. & Jones, R. S. </author> <year> (1977). </year> <title> Distinctive f Features, Categorical Perception, and Probability Learning: Some Applications o Neural Model. </title> <journal> Psychological Review, </journal> <volume> 84, </volume> <pages> 413-451. </pages> <address> .Angluin, D. </address> <year> (1978). </year> <title> On the complexity of minimum inference of regular sets Information and Control, </title> <booktitle> 39, </booktitle> <pages> 337-350. </pages>
Reference-contexts: Each o figure displays 2048 such states, corresponding to all boolean strings up t ength 10. Because the states of the benchmark networks are "in a box" <ref> (Anderson et al., s 1977) </ref> of low dimension, we can view these machines graphically to gain some under-tanding of how the state space is being arranged. Each 3-d state vector is plotted as a e point in the unit cube.
Reference: <author> Angluin, D. & Smith, C. H. </author> <year> (1983). </year> <title> Inductive Inference: Theory and Methods. </title> <journal> B Computing Surveys, </journal> <volume> 15, </volume> <pages> 237-269. </pages> <editor> arnsley, M. F. </editor> <year> (1988). </year> <title> Fractals Everywhere. </title> <address> San Diego: </address> <publisher> Academic Press. </publisher> . 
Reference-contexts: An excellent survey of thi pproach to the problem has been written by <ref> (Angluin & Smith, 1983) </ref>.
Reference: <author> Berwick, R. </author> <year> (1985). </year> <title> The Acquisition of Syntactic Knowledge. </title> <publisher> Cambridge: MIT Press haitin, </publisher> <editor> G. J. </editor> <year> (1966). </year> <title> On the length of programs for computing finite binary C sequences. </title> <journal> Journal of the AM, </journal> <volume> 13, </volume> <pages> 547-569. </pages> <address> homsky, N. </address> <year> (1956). </year> <title> Three Models for the Description of Language. </title> <journal> IRE C Transactions on Information Theory, </journal> <volume> IT-2, </volume> <pages> 113-124. </pages> <address> homsky, N. </address> <year> (1965). </year> <title> Aspects of the Theory of Syntax. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher> <editor> .Crutchfield, J. P., Farmer, J. D., Packard, N, H. & Shaw, R. S. </editor> <booktitle> (1986). Chaos Scientific American, </booktitle> <volume> 255, </volume> <pages> 46-57. </pages>
Reference: <author> Crutchfield, J. P & Young, K. </author> <year> (1989). </year> <title> Computation at the Onset of Chaos. </title> <editor> In W. ,Zurek, (Ed.), </editor> <title> Complexity, Entropy and the Physics of INformation. Reading Dynamical Recognizers 23 D MA: </title> <publisher> Addison-Wesley. </publisher> <editor> errida, B. & Meir, R. </editor> <year> (1988). </year> <title> Chaotic behavior of a layered neural network. </title> <journal> Phys. D Rev. A, </journal> <volume> 38. </volume> <editor> evaney, R. L. </editor> <year> (1987). </year> <title> An introduction to chaotic dynamical systems. </title> <address> Reading, MA: </address> <publisher> E Addison-Wesley. </publisher> <editor> lman, J. L. </editor> <year> (1990). </year> <title> Finding Structure in Time. </title> <journal> Cognitive Science, </journal> <volume> 14, </volume> <pages> 179-212. </pages> <editor> .Feldman, J. A. </editor> <year> (1972). </year> <title> Some Decidability Results in grammatical Inference Information & Control, </title> <booktitle> 20, </booktitle> <pages> 244-462. </pages>
Reference: <author> Fodor, J. & Pylyshyn, A. </author> <year> (1988). </year> <title> Connectionism and Cognitive Architecture: A G Critical Analysis. </title> <journal> Cognition, </journal> <volume> 28, </volume> <pages> 3-71. </pages> <note> iles, </note> <author> C. L., Sun, G. Z., Chen, H. H., Lee, Y. C. & Chen, D. </author> <year> (1990). </year> <title> Higher Order A Recurrent Networks and Grammatical Inference. </title> <editor> In D. S. Touretzky, (Ed.), </editor> <booktitle> dvances in Neural Information Processing Systems. </booktitle> <address> Los Gatos, CA: </address> <publisher> Morgan G Kaufman. </publisher> <editor> leick, J. </editor> <year> (1987). </year> <title> Chaos: Making a new science. </title> <address> New York: </address> <note> Viking. </note> <author> ,Gold, E. M. </author> <year> (1967). </year> <title> Language Identification in the Limit. </title> <booktitle> Information & Control 10, </booktitle> <pages> 447-474. </pages>

Reference: <author> Hornik, K., Stinchcombe, M. & White, H. </author> <year> (1990). </year> <title> Multi-layer Feedforward Networks are Universal Approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 3, </volume> <pages> 551-560. </pages> <note> 24 J. </note> <author> B. Pollack eHuberman, B. A. & Hogg, T. </author> <year> (1987). </year> <title> Phase Transitions in Artificial Intelligenc Systems. </title> <journal> Artificial Intelligence, </journal> <volume> 33, </volume> <pages> 155-172. </pages> <editor> sJoshi, A. K. </editor> <year> (1985). </year> <title> Tree Adjoining Grammars: How much context-sensitivity i required to provide reasonable structural descriptions?. </title> <editor> In D. R. Dowty, L. C Karttunen & A. M. Zwicky, (Eds.), </editor> <booktitle> Natural Language Parsing. </booktitle> <address> Cambridge: </address> <publisher> ambridge University Press. </publisher>

Reference: <author> Minsky, M. </author> <year> (1972). </year> <title> Computation: Finite and Infinite Machines. </title> <address> Cambridge, MA: </address> <publisher> M MIT Press. </publisher> <editor> insky, M. & Papert, S. </editor> <year> (1988). </year> <title> Perceptrons. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. .Moore, </publisher> <address> C. </address> <year> (1990). </year> <journal> Unpredictability and undecidability in dynamical systems Physical Review Letters, </journal> <volume> 62, </volume> <pages> 2354-2357. </pages> <editor> nMozer, M. </editor> <year> (1988). </year> <title> A focused Back-propagation Algorithm for Temporal Patter Recognition. </title> <institution> CRG-Technical Report-88-3: University of Toronto. </institution> <note> lPearlmutter, </note> <author> B. A. </author> <year> (1989). </year> <title> Learning state space trajectories in recurrent neura networks. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 263-269. </pages> <editor> lPineda, F. J. </editor> <year> (1987). </year> <title> Generalization of Back-Propagation to Recurrent Neura Networks. </title> <journal> Physical Review Letters, </journal> <volume> 59, </volume> <pages> 2229-2232. </pages> <editor> :Pinker, S. </editor> <year> (1984). </year> <title> Language Learnability and Language Development. </title> <publisher> Cambridge Harvard University Press. </publisher>
Reference: <author> Pinker, S. & Prince, A. </author> <year> (1988). </year> <title> On Language and Connectionism: Analysis of a - parallel distributed processing model of language inquisition.. </title> <journal> Cognition, </journal> <volume> 28, 73 93. </volume>

Reference: <author> Pollack, J. B. </author> <year> (1990). </year> <title> Recursive Distributed Representation. </title> <booktitle> Artificial Intelligence, P ollard, </booktitle> <address> C. </address> <year> (1984). </year> <title> Generalized Context-Free Grammars, Head Grammars and Natural U Language.. </title> <type> Doctoral Dissertation, </type> <institution> Palo Alto: Dept of Linguistics, Stanford niversity. </institution>
Reference-contexts: To begin to address the question of learnability, I now present and elaborate upon a my earlier work on Cascaded Networks (Pollack, 1987a), which were used in ecurrent fashion to learn parity and depth-limited parenthesis balancing, and to map 3 between word sequences and propositional representations <ref> (Pollack, 1990) </ref>. . The Model A Cascaded Network is a well-behaved higher-order (Sigma-Pi) connectionist t architecture to which the back-propagation technique of weight adjustment (Rumelhar t al., 1986) can be applied. <p> The method applies with smal ariations whether or not there are hidden units in the function or context network, and a whether or not the system is trained with a single "accept" bit for desired output, or arger pattern (representing a tree structure, for example <ref> (Pollack, 1990) </ref>). The impor d tant point is that the gradients connected to a subset of the outputs are calculated irectly, but the gradients connected to don't-care recurrent states are calculated one a step back in time. <p> Dynamical Recognizers 15 - In the spirit of the machine learning community, I recently ran a series of experi ents to make these results more empirical. Table 5 compares Tomita's stage one i "number of mutations" to my "average number of epochs". Because back-propagation s sensitive to initial conditions <ref> (Kolen & Pollack, 1990) </ref>, running each problem once d does not give a good indication of its difficulty, and running it many times from ifferent random starting weights can result in widely disparate timings.


References-found: 9

