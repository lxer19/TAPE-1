URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1993/GIT-CC-93-07.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.93.html
Root-URL: 
Title: Scalable Implementation of Synchronization Primitives on Broadcast Rings  
Author: Martin H. Davis, Jr. Umakishore Ramachandran 
Keyword: scalable synchronization primitives; broadcast ring interconnect; shared memory multiprocessor; distributed queues; local decision algorithms.  
Note: This work has been funded in part by NSF PYI Award MIP-9058430.  
Address: Atlanta, GA 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Pubnum: GIT-CC-93/07  
Email: fdavism,ramag@cc.gatech.edu  
Date: January 1993  
Abstract: Synchronization is an important aspect of parallel program design. By definition synchronization is an aspect of a program where multiple processors participate. Thus it is important to design and implement hardware primitives that scale well with the size of the parallel machine, both in terms of space and time requirements. The focus of this research is to propose implementation for some well-known synchronization primitives in a broadcast ring network. The key aspects of the implementation are to make local decisions to determine the outcome of the synchronization operations; and to keep the space overhead per node constant independent of the number of processors participating in such operations. It is also shown that the implementation incurs exactly the minimum amount of communication to perform the synchronization operations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. H. Davis, Jr. </author> <title> Optical Waveguides in General Purpose Parallel Computers. </title> <type> PhD thesis, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <address> Atlanta, GA 30332-0280, </address> <month> Dec. </month> <year> 1992. </year> <note> Available as Technical Report GIT-CC-93/06. </note>
Reference-contexts: The algorithms can be summarized as follows (the details can be found in <ref> [1] </ref>). The first algorithm is for when a node sees a lock request from any node except itself: the node updates the most-recent and next-recent groups. <p> Together, the third and fourth observations demonstrate that the implementation of these synchronization primitives are scalable in both space and time requirements. 3.2 Implementation-dependent comments In <ref> [1] </ref> we assumed an optical fiber implementation of the broadcast ring; this assumption allows us to make two implementation-dependent observations. First, the previous discussion has implicitly assumed that all of a given synchronization command's traffic is carried on only one broadcast ring. <p> As a second observation (that assumes using optical fibers), we show some minimum 8 and maximum execution times of these synchronization primitives for various system config-urations (the details of the derivation of these times are given in <ref> [1] </ref>). Tables 1, 2, and 3 list these execution times for idle and busy locks, last arrival and simultaneous arrival barriers, and single and multiple F&OP commands respectively.
Reference: [2] <author> A. Gottlieb and C. Kruskal. </author> <title> Coordinating parallel processors: A partial unification. </title> <booktitle> Computer Architecture News, </booktitle> <pages> pages 16-24, </pages> <month> Oct. </month> <year> 1981. </year>
Reference-contexts: If the data structure is to represent a different logical barrier, then an explicit barrier initialization command is required to reset the participation and counter fields. 2.3 Fetch-&-OP The F&OP (Fetch-&-OP) primitive is a generalization of the Fetch-&-ADD primitive introduced by Gottlieb <ref> [2] </ref>. Give that the OP represents any associative, commutative operator, 5 the semantics is that when a node issues F&OP (V,e), the command returns the old value of V to the node and atomically replaces V with V OP e.
Reference: [3] <institution> Kendall Square Research Corp., Waltham, Massachusetts. Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: either electronic or optical hardware? How many nodes can be supported in either technology? What are the design issues involved in storing the replicated data structures distinct from the architecture's shared memory? Should local hardware tables be used for this storage? Are there existing parallel systems (e.g., the KSR machine <ref> [3] </ref>) having a ring-style interconnection network which could utilize these algorithms? Are there other interconnection network structures which can be utilized in a similar manner? Finally, how do "real" parallel programs perform when using these synchronization primitives' implementations versus using other (previously suggested) software and hardware implementations?

References-found: 3

