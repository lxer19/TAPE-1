URL: http://www.cs.gmu.edu:80/research/gag/papers/TAI92.ps
Refering-URL: http://www.cs.gmu.edu:80/research/gag/pubs.html
Root-URL: 
Title: Genetic Algorithms as a Tool for Feature Selection in Machine Learning both the size of
Author: Haleh Vafaie and Kenneth De Jong 
Note: 1.0 Introduction in which  The first  provided by the original data and are more reliable because  The conclusion of these observations is that there is a  
Affiliation: Center for Artificial Intelligence, George Mason University  
Abstract: This paper describes an approach being explored to improve the usefulness of machine learning techniques for generating classification rules for complex, real world data. The approach involves the use of genetic algorithms as a "front end" to traditional rule induction systems in order to identify and select the best subset of features to be used by the rule induction system. This approach has been implemented and tested on difficult texture classification problems. The results are encouraging and indicate significant advantages to the presented approach in this domain. In recent years there has been a significant increase in research on automatic image recognition in more realistic contexts involving noise, changing lighting conditions, and shifting viewpoints. The corresponding increase in difficulty in designing effective classification procedures for the important components of these more complex recognition problems has led to an interest in machine techniques as a possible strategy for automatically producing classification rules. This paper describes part of a larger effort to apply machine learning techniques to such problems in an attempt to generate and improve the classification rules required for various recognition tasks. The immediate problem attacked is that of texture recognition in the context of noise and changing lighting conditions. In this context standard rule induction systems like AQ15 produce sets of classification rules which are suboptimal in two respects. First, there is a need to minimize the number of features actually used for classification, since each feature used adds to the design and manufacturing costs as well as the running time of a recognition system. At the same time there is a need to achieve high recognition rates in the presence of noise and changing environmental conditions. This paper describes an approach being explored to improve the usefulness of machine learning techniques for such problems. The approach described here involves the use of genetic algorithms as a "front end" to traditional rule induction systems in order to identify and select the best subset of features to be used by the rule induction system. The results presented suggest that genetic algorithms are a useful tool for solving difficult feature selection problems Since each feature used as part of a classification procedure can increase the cost and running time of a recognition system, there is strong motivation within the image processing community to design and implement systems with small feature sets. At the same time there is a potentially opposing need to include a sufficient set of features to achieve high recognition rates under difficult conditions. This has led to the development of a variety of techniques within the image processing community for finding an "optimal" subset of features from a larger set of possible features. These feature selection strategies fall into two main categories. The second approach directly selects a subset d of the available m features in such a way as to not significantly degrading the performance of the classifier system [5]. The main issue for this approach is how to account for dependencies between features when ordering them initially and selecting an effective subset in a later step. The machine learning community has only attacked the problem of "optimal" feature selection indirectly in that the traditional biases for simple classification rules (trees) leads to efficient induction procedures for producing individual rules (trees) containing only a few features to be evaluated. However, each rule (tree) can and frequently does use a different set of features, resulting in much larger cumulative features sets than those typically acceptable for image classification problems. This problem is magnified by the tendency of traditional machine learning algorithms to overfit the training data, particularly in the context of noisy data, resulting in the need for a variety of ad hoc truncating (pruning) procedures for simplifying the induced rules (trees). 
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> Brodatz, P. </author> <title> A Photographic Album for Arts and Design, </title> <publisher> Dover Publishing Co., </publisher> <address> Toronto, Canada, </address> <year> 1966. </year>
Reference-contexts: An attempt was made to identify reasonable values for these parameters for the texture classification problems used. (for more details, see [7]). In these experiments four texture images were randomly selected from Brodatz <ref> [1] </ref> album of textures. These images are water, beach pebbles, hand made paper, and cotton canvas as depicted in [1] and [7]. Two hundred feature vectors, each containing 18 features were then randomly extracted from an arbitrary selected area of 30 by 30 pixels from each of the chosen textures. <p> In these experiments four texture images were randomly selected from Brodatz <ref> [1] </ref> album of textures. These images are water, beach pebbles, hand made paper, and cotton canvas as depicted in [1] and [7]. Two hundred feature vectors, each containing 18 features were then randomly extracted from an arbitrary selected area of 30 by 30 pixels from each of the chosen textures.
Reference: [2] <author> De Jong, K. </author> <title> Learning with Genetic Algorithms : An overview, </title> <journal> Machine Learning Vol. </journal> <volume> 3, </volume> <publisher> Kluwer Academic publishers, </publisher> <year> 1988. </year>
Reference-contexts: In this section we describe this approach in more detail. 5.1 Genetic Algorithms Genetic algorithms (GAs), a form of inductive learning strategy, are adaptive search techniques which have demonstrated substantial improvement over a variety of random and local search methods <ref> [2] </ref>. This is accomplished by their ability to exploit accumulating information about an initially unknown search space in order to bias subsequent search into promising subspaces. <p> Since GAs are basically a domain independent search technique, they are ideal for applications where domain knowledge and theory is difficult or impossible to provide <ref> [2] </ref>. The main issues in applying GAs to any problem are selecting an appropriate representation and an adequate evaluation function. For detailed description of both of these issues for the problem of feature selection see [7].
Reference: [3] <author> Devijver, P., and Kittler, J. </author> <title> PATTERN RECOGNITION: A STATISTICAL APPROACH, </title> <publisher> Prentice Hall, </publisher> <year> 1982. </year>
Reference-contexts: A standard approach to feature selection involves the use of sequential backward selection (SBS), a top down search procedure that starts with the complete set of features and discards one feature at a time until the desired number of features have been deleted. For detailed description see <ref> [3] </ref>. As noted earlier, the performance of a feature subset is measured by applying the evaluation process presented in to the new reduced training data to generate the decision rules for each of the given classes in the training data. <p> After all the testing example have been classified using AQ generated rules, a statistical separability measure is computed as the estimate of fitness of the given feature set. The basic idea is to find feature subsets which increase (maximize) the distance between the classes to be recognized <ref> [3] </ref>. More formally, one would like to maximize c c ni nj i=1 j=1 k=1 l= 1 where, ( e ik , e jl ) represents the distance between two elements. <p> Typically, this distance measure is Euclidean distance since it allows for both analytical and computational simplifications of the interclass distance criterion <ref> [3] </ref>. Then, ( e ik , e jl )= (e ik - e jl ) t ( e ik - e jl ). For detailed explanation refer to [3]. 4.1 Initial Experimental Results The AQ15 system used for rule induction has a number of parameters which affects its own performance on <p> Typically, this distance measure is Euclidean distance since it allows for both analytical and computational simplifications of the interclass distance criterion <ref> [3] </ref>. Then, ( e ik , e jl )= (e ik - e jl ) t ( e ik - e jl ). For detailed explanation refer to [3]. 4.1 Initial Experimental Results The AQ15 system used for rule induction has a number of parameters which affects its own performance on a given problem class. An attempt was made to identify reasonable values for these parameters for the texture classification problems used. (for more details, see [7]).
Reference: [4] <author> Grefenstette, John J. </author> <type> Technical Report CS-83-11, </type> <institution> Computer Science Dept., Vanderbilt Univ., </institution> <year> 1984. </year>
Reference-contexts: In addition, GENESIS <ref> [4] </ref>, a general purpose genetic algorithm program, was used as the search procedure (replacing SBS). We used the standard parameter settings for GENESIS.
Reference: [5] <author> Ichino, M., and Sklansky, J.. </author> <title> Optimum Feature selection by zero-one Integer Programming, </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> Vol. 14, No. 5, </volume> <year> 1984a. </year>
Reference-contexts: The second approach directly selects a subset d of the available m features in such a way as to not significantly degrading the performance of the classifier system <ref> [5] </ref>. The main issue for this approach is how to account for dependencies between features when ordering them initially and selecting an effective subset in a later step.
Reference: [6] <author> Michalski, R.S., Mozetic, I., Hong, J.R., and Lavrac, N.. </author> <title> The Multipurpose Incremental Learning System AQ15 and its Testing Application to Three Medical Domains, </title> <publisher> AAAI, </publisher> <year> 1986. </year>
Reference-contexts: In our case we use AQ15, a rule induction technique used to produce a complete and consistent description of classes of examples <ref> [6] </ref>. A class description is formed by a set of decision rules describing all the training examples given for that particular class. A decision rule is simply a set of conjuncts of allowable tests of feature values. For a more detailed description see [7].
Reference: [7] <author> Vafaie, H., and De Jong, K.A., </author> <title> Improving the performance of a Rule Induction System Using Genetic Algorithms, </title> <booktitle> Proceedings of the First International Workshop on MULTISTRATEGY LEARNING, </booktitle> <address> Harpers Ferry, </address> <publisher> W. </publisher> <address> Virginia, USA, </address> <year> 1991. </year>
Reference-contexts: A class description is formed by a set of decision rules describing all the training examples given for that particular class. A decision rule is simply a set of conjuncts of allowable tests of feature values. For a more detailed description see <ref> [7] </ref>. The last step is to evaluate the classification performance of the induced rules on the unseen test data. <p> The fitness function then evaluates the AQ generated rules on the testing examples as follows. For every testing example a match score (for more detailed description see <ref> [7] </ref>) is evaluated for each of the classification rules generated by the AQ algorithm, in order to find the rule (s) with the highest or best match. <p> An attempt was made to identify reasonable values for these parameters for the texture classification problems used. (for more details, see <ref> [7] </ref>). In these experiments four texture images were randomly selected from Brodatz [1] album of textures. These images are water, beach pebbles, hand made paper, and cotton canvas as depicted in [1] and [7]. <p> to identify reasonable values for these parameters for the texture classification problems used. (for more details, see <ref> [7] </ref>). In these experiments four texture images were randomly selected from Brodatz [1] album of textures. These images are water, beach pebbles, hand made paper, and cotton canvas as depicted in [1] and [7]. Two hundred feature vectors, each containing 18 features were then randomly extracted from an arbitrary selected area of 30 by 30 pixels from each of the chosen textures. <p> The main issues in applying GAs to any problem are selecting an appropriate representation and an adequate evaluation function. For detailed description of both of these issues for the problem of feature selection see <ref> [7] </ref>. In the feature selection problem the main interest is in representing the space of all possible subsets of the given feature set. <p> r e 120010008 006 004 002 000 40 trials R c o n n t e distance measure over time by the best individual set fitness over time correct recognitions and subtracting the weighted sum of the match score of all of the incorrect recognitions (for a detailed explanation see <ref> [7] </ref>), i.e. n m i=1 j=n+ 1 The range of the value of F is dependent on the number of testing events and their weights.
References-found: 7

