URL: http://www.cs.umd.edu/~tseng/papers/crpc91162.ps
Refering-URL: http://www.cs.umd.edu/~tseng/papers.html
Root-URL: 
Title: Compiler Optimizations for Fortran D on MIMD Distributed-Memory Machines  
Author: Seema Hiranandani Ken Kennedy Chau-Wen Tseng 
Note: Center for Research on Parallel Computation  In Proceedings of Supercomputing '91, Albuquerque, NM, November 1991.  
Date: August 1991  
Address: 91162  P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: CRPC-TR  Rice University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. Albert, K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Compiling Fortran 8x array features for the Connection Machine computer system. </title> <booktitle> In Symposium on Parallel Programming: Experience with Applications, Languages and Systems, </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: It is related to other distributed-memory compilation systems such as Al [30], CM Fortran <ref> [1] </ref>, Dino [28], MIMDizer [15], Pandore [3], Paragon [10], and Spot [29], but mostly builds on the following research projects. SUPERB is a semi-automatic parallelization tool that supports arbitrary user-specified contiguous BLOCK distributions [13, 34]. It originated overlaps as a means to both specify and store nonlocal data accesses.
Reference: [2] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Dependences may be either loop-independent or loop-carried. Loop-independent dependences occur on the same loop iteration; loop-carried dependences occur on different iterations of a particular loop. The level of a loop-carried dependence is the depth of the loop carrying the dependence <ref> [2] </ref>. Loop-independent dependences have infinite depth. The number of loop iterations separating the source and sink of the loop-carried dependence may be characterized by a dependence distance or direction [31]. Dependence analysis is vital to shared-memory vec-torizing and parallelizing compilers. <p> Finally, all nonlocal array references in the program are modified to reflect the actual data location selected. 3.4 Advanced Optimizations 3.4.1 Program Transformations Shared-memory parallelizing compilers apply program transformations to expose or enhance parallelism in scientific codes, using dependence information to determine their legality and profitability <ref> [2, 20, 22, 31] </ref>. Program transformations are also useful for distributed-memory compilers. The legality of each transformation is determined in exactly the same manner, since the same execution order must be preserved in order to retain the meaning of the original program. However, their profitability criteria are now totally different. <p> It may be applied only if the source and sink of each dependence are not reversed in the resulting program. This may be determined by examining the distance or direction vector associated with each dependence <ref> [2, 31] </ref>. Strip Mining Strip mining increases the step size of an existing loop and adds an additional inner loop. Strip mining is always legal. The Fortran D compiler may apply strip mining in order to reduce storage requirements for computations.
Reference: [3] <author> F. Andre, J. Pazat, and H. Thomas. </author> <title> Pandore: A system to manage data distribution. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: It is related to other distributed-memory compilation systems such as Al [30], CM Fortran [1], Dino [28], MIMDizer [15], Pandore <ref> [3] </ref>, Paragon [10], and Spot [29], but mostly builds on the following research projects. SUPERB is a semi-automatic parallelization tool that supports arbitrary user-specified contiguous BLOCK distributions [13, 34]. It originated overlaps as a means to both specify and store nonlocal data accesses.
Reference: [4] <author> V. Balasundaram. </author> <title> Translating control parallelism to data parallelism. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <address> Houston, TX, </address> <month> March </month> <year> 1991. </year>
Reference-contexts: Computing the result on the processor owning the rhs and then sending the result to the owner of the lhs could reduce the amount of data communicated. This optimization is a simple case of the owner stores rule proposed by Balasundaram <ref> [4] </ref>. In particular, for computations performing irregular data accesses it may be desirable for the Fortran D compiler to partition loops amongst processors so that each loop iteration is executed on a single processor, such as in Kali [21] and Parti [24].
Reference: [5] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kre-mer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Message vectorization uses the level of loop-carried cross-processor dependences to calculate whether messages may be legally combined into larger messages, enabling efficient program execution. Algorithm We use the following algorithm from Bal-asundaram et al. and Gerndt to calculate the appropriate loop level for inserting messages for nonlocal references <ref> [5, 13] </ref>. We define the commlevel for loop-carried dependences to be the level of the dependence. For loop-independent dependences we define it to be the level of the deepest loop common to both the source and sink of the dependence. <p> We believe that Fortran D, a version of Fortran enhanced with data decompositions, provides such a portable data-parallel programming model. Its success will depend on the effectiveness of the compiler, as well as environmental support for automatic data decomposition and static performance estimation <ref> [5, 6, 16] </ref>. The current prototype of the Fortran D compiler performs message vectorization for block-distributed arrays.
Reference: [6] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. </author> <title> Kre Page 14 mer. A static performance estimator to guide data par-titioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: We believe that Fortran D, a version of Fortran enhanced with data decompositions, provides such a portable data-parallel programming model. Its success will depend on the effectiveness of the compiler, as well as environmental support for automatic data decomposition and static performance estimation <ref> [5, 6, 16] </ref>. The current prototype of the Fortran D compiler performs message vectorization for block-distributed arrays.
Reference: [7] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: The compiler partitions the program using the owner computes rule, by which each processor computes values of data it owns [8, 27, 34]. The prototype Fortran D compiler is being developed in the context of the ParaScope programming environment <ref> [7] </ref>. The compiler is subdivided into three major phases|program analysis, program optimization, and code generation.
Reference: [8] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: It translates Fortran D programs into single-program, multiple-data (SPMD) form with explicit message-passing that execute directly on the nodes of the distributed-memory machine. The compiler partitions the program using the owner computes rule, by which each processor computes values of data it owns <ref> [8, 27, 34] </ref>. The prototype Fortran D compiler is being developed in the context of the ParaScope programming environment [7]. The compiler is subdivided into three major phases|program analysis, program optimization, and code generation. <p> We mark such references that do not cause loop-carried true dependences as candidates for applying the inspector/executor strategy to calculate nonlocal accesses at runtime [24]. For references that may cause loop-carried true dependences, the Fortran D compiler will resolve the communication at runtime using previously proposed runtime techniques <ref> [8, 27, 34] </ref>. 3.2.4 Additional Optimizations Other optimizations are being considered for the Fortran D compiler. Communication may be further optimized by considering interactions between all the loop nests in the program. <p> With multiple statements in the loop, the local iteration set of a statement may be a subset of the reduced loop bounds. For these statements we need to also add explicit guards based on membership tests for the local iteration set of the statement <ref> [8, 27, 34] </ref>. 3.3.2 Message Generation The Fortran D compiler uses information calculated in the communication analysis and optimization phases to guide message generation. <p> The inspector strategy is not applicable for unanalyzable references causing loop-carried true dependences. In this case the Fortran D compiler inserts guards to resolve the needed communication and program execution at runtime <ref> [8, 27, 34] </ref>. 3.3.3 Storage Management One of the major responsibilities of the Fortran D compiler is to select and manage storage for all nonlocal array references. There are three different storage schemes. Overlaps Overlaps are expansions of local array sections to accommodate neighboring nonlocal elements [13]. <p> Data dependence information is then used to apply loop distribution and vectorize these statements, resulting in vectorized mes sages. Superb also performs interprocedural analysis and code generation. ParaScope is a parallel programming environment that supports a prototype distributed-memory compiler <ref> [8] </ref>. User-defined distribution functions are used to specify the data decomposition for Fortran programs. The compiler inserts load and store statements to handle data movement and then applies numerous program transformations to optimize guards and mes sages.
Reference: [9] <author> D. Callahan, K. Kennedy, and U. Kremer. </author> <title> A dynamic study of vectorization in PFC. </title> <type> Technical Report TR89-97, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: However, parallel computers are not likely to be widely successful until they are easy to program. A major component in the success of vector supercomputers is the ability of scientists to write Fortran programs in a "vectorizable" style and expect vectorizing compilers to automatically produce efficient code <ref> [9, 32] </ref>. The resulting programs are easily maintained, debugged, and portable across different vector machines. Compare this with the current situation for programming parallel machines.
Reference: [10] <author> A. Cheung and A. Reeves. </author> <title> The Paragon multicom-puter environment: A first implementation. </title> <type> Technical Report EE-CEG-89-9, </type> <institution> Cornell University Computer Engineering Group, </institution> <address> Ithaca, NY, </address> <month> July </month> <year> 1989. </year>
Reference-contexts: It is related to other distributed-memory compilation systems such as Al [30], CM Fortran [1], Dino [28], MIMDizer [15], Pandore [3], Paragon <ref> [10] </ref>, and Spot [29], but mostly builds on the following research projects. SUPERB is a semi-automatic parallelization tool that supports arbitrary user-specified contiguous BLOCK distributions [13, 34]. It originated overlaps as a means to both specify and store nonlocal data accesses.
Reference: [11] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: It also supports irregular data distributions and dynamic data decomposition, i.e., changing the alignment or distribution of a decomposition at any point in the program. The complete language is described in detail elsewhere <ref> [11] </ref>. 3 Fortran D Compiler The two major steps in writing a data-parallel program are selecting a data decomposition and using it to derive node programs with explicit data movement. We leave the task of selecting a data decomposition to the user or automatic tools.
Reference: [12] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: It may also be used with loop interchange to help exploit pipeline parallelism, as discussed in the next section. 3.4.2 Pipelined Computations In loosely synchronous computations all processors execute SPMD programs in a loose lockstep, alternating between phases of local computation and synchronous global communication <ref> [12] </ref>. These problems are well structured; they achieve good load balance because all processors are utilized during the computation phase. For instance, Jacobi and red-black SOR are loosely synchronous.
Reference: [13] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency|Practice & Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Message vectorization uses the level of loop-carried cross-processor dependences to calculate whether messages may be legally combined into larger messages, enabling efficient program execution. Algorithm We use the following algorithm from Bal-asundaram et al. and Gerndt to calculate the appropriate loop level for inserting messages for nonlocal references <ref> [5, 13] </ref>. We define the commlevel for loop-carried dependences to be the level of the dependence. For loop-independent dependences we define it to be the level of the deepest loop common to both the source and sink of the dependence. <p> There are three different storage schemes. Overlaps Overlaps are expansions of local array sections to accommodate neighboring nonlocal elements <ref> [13] </ref>. For programs with high locality of reference, they are useful for generating clean code. However, overlaps are permanent and specific to each array, and thus may require more storage. Buffers Buffers avoid the contiguous and permanent nature of overlaps. <p> It is related to other distributed-memory compilation systems such as Al [30], CM Fortran [1], Dino [28], MIMDizer [15], Pandore [3], Paragon [10], and Spot [29], but mostly builds on the following research projects. SUPERB is a semi-automatic parallelization tool that supports arbitrary user-specified contiguous BLOCK distributions <ref> [13, 34] </ref>. It originated overlaps as a means to both specify and store nonlocal data accesses. Exsr statements are inserted in the program to communicate overlap regions. Data dependence information is then used to apply loop distribution and vectorize these statements, resulting in vectorized mes sages.
Reference: [14] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: In many cases, the Fortran D compiler can construct iteration or index sets using regular section descriptors (RSDs), a compact representation of rectangular sections (with some constant step) and their higher dimension analogs <ref> [14] </ref>. The union and intersection of RSDs can be calculated inexpensively, making them highly useful for the Fortran D compiler.
Reference: [15] <author> R. Hill. MIMDizer: </author> <title> A new tool for parallelization. </title> <journal> Supercomputing Review, </journal> <volume> 3(4) </volume> <pages> 26-28, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: It is related to other distributed-memory compilation systems such as Al [30], CM Fortran [1], Dino [28], MIMDizer <ref> [15] </ref>, Pandore [3], Paragon [10], and Spot [29], but mostly builds on the following research projects. SUPERB is a semi-automatic parallelization tool that supports arbitrary user-specified contiguous BLOCK distributions [13, 34]. It originated overlaps as a means to both specify and store nonlocal data accesses.
Reference: [16] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: We believe that Fortran D, a version of Fortran enhanced with data decompositions, provides such a portable data-parallel programming model. Its success will depend on the effectiveness of the compiler, as well as environmental support for automatic data decomposition and static performance estimation <ref> [5, 6, 16] </ref>. The current prototype of the Fortran D compiler performs message vectorization for block-distributed arrays.
Reference: [17] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Compilers and Runtime Software for Scalable Multiprocessors. </title> <publisher> Elsevier, </publisher> <address> Amsterdam, </address> <note> The Nether-lands, to appear 1991. </note>
Reference-contexts: Note how the local indices calculated for the local index set of each array has been used to derive the local indices for the local iteration set. The calculation of local index and iteration sets is described in greater detail elsewhere <ref> [17] </ref>. Handling boundary conditions Because alignment and distribution specifications in Fortran D are fairly simple, local index sets and their derived iteration sets may usually be calculated at compile time. In fact, for most regular computations local index and iteration sets are identical for every processor except for boundary conditions.
Reference: [18] <author> S. Hiranandani, J. Saltz, P. Mehrotra, and H. Berry-man. </author> <title> Performance of hashed cache data migration schemes on multicomputers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(4), </volume> <month> August </month> <year> 1991. </year>
Reference-contexts: Hash tables Hash tables may be used when the set of nonlocal elements accessed is sparse, as for many irregular computations. They also provide a quick lookup mechanism for arbitrary sets of nonlocal values <ref> [18] </ref>. The extent of all RSDs representing nonlocal array accesses produced during the message generation phase are examined to select the appropriate storage type for each array reference. If overlaps have been selected, array declarations are modified to take into account storage for nonlocal data. <p> PARTI is a set of runtime library routines that support irregular computations on MIMD distributed-memory machines. Parti is the first to propose and implement user-defined irregular distributions [24] and a hashed cache for nonlocal values <ref> [18] </ref>.
Reference: [19] <author> K. Ikudome, G. Fox, A. Kolawa, and J. Flower. </author> <title> An automatic and symbolic parallelization system for distributed memory parallel computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Program analysis and optimization is simplified because it targets a purely functional language. Crystal pioneered the strategy of identifying collective communication opportunities used in the Fortran D compiler. ASPAR is a compiler that automatically generates data decompositions and communication for Fortran programs with BLOCK distributions <ref> [19] </ref>. Aspar performs simple dependence analysis using A-lists to detect parallel loops performing regular computations on distributed arrays. A micro-stencil is derived and used to generate a macro-stencil to identify communication requirements. Kali is the first compiler that supports both regular and irregular computations on MIMD distributed-memory machines [21].
Reference: [20] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in the ParaScope Editor. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: Program optimization (a) Program transformations (b) Message vectorization (c) Collective communication (d) Runtime processing 3. Code generation (a) Program partitioning (b) Message generation (c) Storage management for distributed-memory machines. The Fortran D compiler incorporates the following ParaScope dependence analysis capabilities <ref> [20] </ref>. Scalar dataflow analysis Control flow, control dependence, and live range information are computed during the scalar dataflow analysis phase. <p> Finally, all nonlocal array references in the program are modified to reflect the actual data location selected. 3.4 Advanced Optimizations 3.4.1 Program Transformations Shared-memory parallelizing compilers apply program transformations to expose or enhance parallelism in scientific codes, using dependence information to determine their legality and profitability <ref> [2, 20, 22, 31] </ref>. Program transformations are also useful for distributed-memory compilers. The legality of each transformation is determined in exactly the same manner, since the same execution order must be preserved in order to retain the meaning of the original program. However, their profitability criteria are now totally different. <p> Loop Distribution Loop distribution separates independent statements inside a single loop into multiple loops with identical headers. Loop distribution may be applied only if the statements are not involved in a recurrence and the direction of existing loop-carried dependences are not reversed in the resulting loops <ref> [20, 22] </ref>. <p> For a block distribution, interchange cross-processor loops as deeply as possible. Strip mining the loop may also reduce communication overhead. Legality is determined in the same manner as for unroll-and-jam <ref> [20] </ref>. The strip size is machine dependent and will be determined empirically. These values will be fed into the compiler to enable calculation of the strip size. mining may be used in conjunction to exploit pipeline parallelism.
Reference: [21] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4), </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: In particular, for computations performing irregular data accesses it may be desirable for the Fortran D compiler to partition loops amongst processors so that each loop iteration is executed on a single processor, such as in Kali <ref> [21] </ref> and Parti [24]. This technique may improve communication and provide greater control over load balance, especially for irregular computations. <p> Despite the expense of additional communication, experimental evidence from several systems show that it can improve performance by grouping communication to access nonlocal data outside of the loop nest, especially if the information generated may be reused on later iterations <ref> [21, 24] </ref>. The inspector strategy is not applicable for unanalyzable references causing loop-carried true dependences. <p> Aspar performs simple dependence analysis using A-lists to detect parallel loops performing regular computations on distributed arrays. A micro-stencil is derived and used to generate a macro-stencil to identify communication requirements. Kali is the first compiler that supports both regular and irregular computations on MIMD distributed-memory machines <ref> [21] </ref>. Since dependence analysis is not provided, programmers must declare all parallel loops. Instead of deriving a parallel program from the data decomposition, Kali requires that the programmer explicitly partition loop iterations onto processors using an on clause.
Reference: [22] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: We present a code generation strategy based on the concept of data depen Page 1 ALIGN A (I,J) with D (J-2,I+3) D (N,N) DECOMPOSITION REAL A (N,N) p 4 p 2 p 4 p 2 p 4 p 2 DISTRIBUTE D (:,BLOCK) D (CYCLIC,:) DISTRIBUTE dence <ref> [22] </ref> that unifies and extends previous techniques. The rest of this paper presents the data decomposition specifications in Fortran D, basic compiler analysis and code generation algorithms, and compiler optimizations to reduce communication costs and load imbalance. <p> A data dependence between two references R 1 and R 2 indicates that they read or write a common memory location in a way that requires their execution order to be maintained <ref> [22] </ref>. We call R 1 the source and R 2 the sink of the dependence if R 1 must be executed before R 2 . If R 1 is a write and R 2 is a read, we call the result a true (or flow) dependence. <p> Finally, all nonlocal array references in the program are modified to reflect the actual data location selected. 3.4 Advanced Optimizations 3.4.1 Program Transformations Shared-memory parallelizing compilers apply program transformations to expose or enhance parallelism in scientific codes, using dependence information to determine their legality and profitability <ref> [2, 20, 22, 31] </ref>. Program transformations are also useful for distributed-memory compilers. The legality of each transformation is determined in exactly the same manner, since the same execution order must be preserved in order to retain the meaning of the original program. However, their profitability criteria are now totally different. <p> Loop Distribution Loop distribution separates independent statements inside a single loop into multiple loops with identical headers. Loop distribution may be applied only if the statements are not involved in a recurrence and the direction of existing loop-carried dependences are not reversed in the resulting loops <ref> [20, 22] </ref>.
Reference: [23] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: To take advantage of these cases, we apply techniques pioneered by Li and Chen to utilize collective communication primitives <ref> [23] </ref>. As previously described, opportunities for utilizing collective communication are recognized by the Fortran D compiler during communication selection by ex-aming both the array reference and data decomposition information. Loop-invariant subscripts in distributed dimensions correspond to broadcasts. <p> Global accumulate (reduction) operations are recognized and supported. Unlike other systems, program partitioning produces distinct programs for each node processor. Crystal is a high-level functional language compiled to distributed-memory machines using both automatic data decomposition and communication generation <ref> [23] </ref>. Program analysis and optimization is simplified because it targets a purely functional language. Crystal pioneered the strategy of identifying collective communication opportunities used in the Fortran D compiler. ASPAR is a compiler that automatically generates data decompositions and communication for Fortran programs with BLOCK distributions [19].
Reference: [24] <author> R. Mirchandaney, J. Saltz, R. Smith, D. Nicol, and K. Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: We mark such references that do not cause loop-carried true dependences as candidates for applying the inspector/executor strategy to calculate nonlocal accesses at runtime <ref> [24] </ref>. For references that may cause loop-carried true dependences, the Fortran D compiler will resolve the communication at runtime using previously proposed runtime techniques [8, 27, 34]. 3.2.4 Additional Optimizations Other optimizations are being considered for the Fortran D compiler. <p> In particular, for computations performing irregular data accesses it may be desirable for the Fortran D compiler to partition loops amongst processors so that each loop iteration is executed on a single processor, such as in Kali [21] and Parti <ref> [24] </ref>. This technique may improve communication and provide greater control over load balance, especially for irregular computations. <p> Additional communication is also appended following loops containing reductions to accumulate and broadcast the results of each reduction. Runtime Processing Runtime processing is applied to computations whose nonlocal data requirements are not known at compile time. An inspector <ref> [24] </ref> is constructed to preprocess the loop body at run-time to determine what nonlocal data will be accessed. This in effect calculates the receive index set for each processor. A global transpose operation between processors is then used to calculate the send index sets. <p> Despite the expense of additional communication, experimental evidence from several systems show that it can improve performance by grouping communication to access nonlocal data outside of the loop nest, especially if the information generated may be reused on later iterations <ref> [21, 24] </ref>. The inspector strategy is not applicable for unanalyzable references causing loop-carried true dependences. <p> PARTI is a set of runtime library routines that support irregular computations on MIMD distributed-memory machines. Parti is the first to propose and implement user-defined irregular distributions <ref> [24] </ref> and a hashed cache for nonlocal values [18].
Reference: [25] <author> C. Pancake and D. Bergmark. </author> <title> Do parallel languages respond to the needs of scientific programmers? IEEE Computer, </title> <booktitle> 23(12) </booktitle> <pages> 13-23, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: To be successful, the compiler needs additional information not present in vanilla Fortran. Current parallel programming languages provide little support for data decomposition <ref> [25] </ref>. We have therefore developed an enhanced version of Fortran that introduces data decomposition specifications. We call the extended language Fortran D, where "D" suggests data, decomposition, or distribution.
Reference: [26] <author> K. Pingali and A. Rogers. </author> <title> Compiling for locality. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: The nonlocal RSD for P roc (1) and P roc (2:3) are both <ref> [2:99, 26] </ref> and are therefore combined. The RSD for P roc (4) consists of only local data and is discarded. The sending processor is determined by computing the owners of the section [2:99,26] @ P roc (1:3) resulting in P roc (2:4) sending data to their left processors. <p> User-defined distribution functions are used to specify the data decomposition for Fortran programs. The compiler inserts load and store statements to handle data movement and then applies numerous program transformations to optimize guards and mes sages. Id Nouveau is a functional language enhanced with BLOCK and CYCLIC distributions <ref> [26, 27] </ref>. Dependence analysis is avoided through the use of write-once arrays called I-structures. Initially, send and receive statements are inserted to communicate each nonlocal array access. Message vectorization is then applied to previously written array elements to reduce message Page 13 overhead.
Reference: [27] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: It translates Fortran D programs into single-program, multiple-data (SPMD) form with explicit message-passing that execute directly on the nodes of the distributed-memory machine. The compiler partitions the program using the owner computes rule, by which each processor computes values of data it owns <ref> [8, 27, 34] </ref>. The prototype Fortran D compiler is being developed in the context of the ParaScope programming environment [7]. The compiler is subdivided into three major phases|program analysis, program optimization, and code generation. <p> We mark such references that do not cause loop-carried true dependences as candidates for applying the inspector/executor strategy to calculate nonlocal accesses at runtime [24]. For references that may cause loop-carried true dependences, the Fortran D compiler will resolve the communication at runtime using previously proposed runtime techniques <ref> [8, 27, 34] </ref>. 3.2.4 Additional Optimizations Other optimizations are being considered for the Fortran D compiler. Communication may be further optimized by considering interactions between all the loop nests in the program. <p> With multiple statements in the loop, the local iteration set of a statement may be a subset of the reduced loop bounds. For these statements we need to also add explicit guards based on membership tests for the local iteration set of the statement <ref> [8, 27, 34] </ref>. 3.3.2 Message Generation The Fortran D compiler uses information calculated in the communication analysis and optimization phases to guide message generation. <p> The inspector strategy is not applicable for unanalyzable references causing loop-carried true dependences. In this case the Fortran D compiler inserts guards to resolve the needed communication and program execution at runtime <ref> [8, 27, 34] </ref>. 3.3.3 Storage Management One of the major responsibilities of the Fortran D compiler is to select and manage storage for all nonlocal array references. There are three different storage schemes. Overlaps Overlaps are expansions of local array sections to accommodate neighboring nonlocal elements [13]. <p> Improve Partial Parallelism Parallelism may be exploited in pipelined computations through message pipelining|sending a message when its value is computed, rather than when its value is needed <ref> [27] </ref>. Rogers and Pingali applied this optimization to a Gauss-Seidel (a special case of SOR) computation that is distributed cyclically. For pipelined computations, transformations such as loop interchange and strip mining may also be needed to balance computation and communication. <p> User-defined distribution functions are used to specify the data decomposition for Fortran programs. The compiler inserts load and store statements to handle data movement and then applies numerous program transformations to optimize guards and mes sages. Id Nouveau is a functional language enhanced with BLOCK and CYCLIC distributions <ref> [26, 27] </ref>. Dependence analysis is avoided through the use of write-once arrays called I-structures. Initially, send and receive statements are inserted to communicate each nonlocal array access. Message vectorization is then applied to previously written array elements to reduce message Page 13 overhead.
Reference: [28] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> Expressing complex parallel algorithms in DINO. </title> <booktitle> In Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: It is related to other distributed-memory compilation systems such as Al [30], CM Fortran [1], Dino <ref> [28] </ref>, MIMDizer [15], Pandore [3], Paragon [10], and Spot [29], but mostly builds on the following research projects. SUPERB is a semi-automatic parallelization tool that supports arbitrary user-specified contiguous BLOCK distributions [13, 34]. It originated overlaps as a means to both specify and store nonlocal data accesses.
Reference: [29] <author> D. Socha. </author> <title> Compiling single-point iterative programs for distributed memory computers. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: It is related to other distributed-memory compilation systems such as Al [30], CM Fortran [1], Dino [28], MIMDizer [15], Pandore [3], Paragon [10], and Spot <ref> [29] </ref>, but mostly builds on the following research projects. SUPERB is a semi-automatic parallelization tool that supports arbitrary user-specified contiguous BLOCK distributions [13, 34]. It originated overlaps as a means to both specify and store nonlocal data accesses. Exsr statements are inserted in the program to communicate overlap regions.
Reference: [30] <author> P.-S. Tseng. </author> <title> A parallelizing compiler for distributed memory parallel computers. </title> <booktitle> In Proceedings of the SIG-PLAN '90 Conference on Program Language Design and Implementation, </booktitle> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: It is related to other distributed-memory compilation systems such as Al <ref> [30] </ref>, CM Fortran [1], Dino [28], MIMDizer [15], Pandore [3], Paragon [10], and Spot [29], but mostly builds on the following research projects. SUPERB is a semi-automatic parallelization tool that supports arbitrary user-specified contiguous BLOCK distributions [13, 34].
Reference: [31] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: The level of a loop-carried dependence is the depth of the loop carrying the dependence [2]. Loop-independent dependences have infinite depth. The number of loop iterations separating the source and sink of the loop-carried dependence may be characterized by a dependence distance or direction <ref> [31] </ref>. Dependence analysis is vital to shared-memory vec-torizing and parallelizing compilers. We show that it is also highly useful for guiding compiler optimizations 1. Program analysis (a) Dependence analysis (b) Data decomposition analysis (c) Partitioning analysis (d) Communication analysis 2. <p> Finally, all nonlocal array references in the program are modified to reflect the actual data location selected. 3.4 Advanced Optimizations 3.4.1 Program Transformations Shared-memory parallelizing compilers apply program transformations to expose or enhance parallelism in scientific codes, using dependence information to determine their legality and profitability <ref> [2, 20, 22, 31] </ref>. Program transformations are also useful for distributed-memory compilers. The legality of each transformation is determined in exactly the same manner, since the same execution order must be preserved in order to retain the meaning of the original program. However, their profitability criteria are now totally different. <p> It may be applied only if the source and sink of each dependence are not reversed in the resulting program. This may be determined by examining the distance or direction vector associated with each dependence <ref> [2, 31] </ref>. Strip Mining Strip mining increases the step size of an existing loop and adds an additional inner loop. Strip mining is always legal. The Fortran D compiler may apply strip mining in order to reduce storage requirements for computations.
Reference: [32] <author> M. J. Wolfe. </author> <title> Semi-automatic domain decomposition. </title> <booktitle> In Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: However, parallel computers are not likely to be widely successful until they are easy to program. A major component in the success of vector supercomputers is the ability of scientists to write Fortran programs in a "vectorizable" style and expect vectorizing compilers to automatically produce efficient code <ref> [9, 32] </ref>. The resulting programs are easily maintained, debugged, and portable across different vector machines. Compare this with the current situation for programming parallel machines.
Reference: [33] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Parti is the first to propose and implement user-defined irregular distributions [24] and a hashed cache for nonlocal values [18]. Parti has also motivated the development of Arf, a compiler that automatically generates inspector and executor loops for runtime preprocessing of programs with BLOCK, CYCLIC, and user-defined irregular distributions <ref> [33] </ref>. 4.1 Comparison with Fortran D The Fortran D compiler integrates more compiler optimizations than the first generation research systems described, and in addition possesses two main advantages.
Reference: [34] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year> <pages> Page 15 </pages>
Reference-contexts: It translates Fortran D programs into single-program, multiple-data (SPMD) form with explicit message-passing that execute directly on the nodes of the distributed-memory machine. The compiler partitions the program using the owner computes rule, by which each processor computes values of data it owns <ref> [8, 27, 34] </ref>. The prototype Fortran D compiler is being developed in the context of the ParaScope programming environment [7]. The compiler is subdivided into three major phases|program analysis, program optimization, and code generation. <p> We mark such references that do not cause loop-carried true dependences as candidates for applying the inspector/executor strategy to calculate nonlocal accesses at runtime [24]. For references that may cause loop-carried true dependences, the Fortran D compiler will resolve the communication at runtime using previously proposed runtime techniques <ref> [8, 27, 34] </ref>. 3.2.4 Additional Optimizations Other optimizations are being considered for the Fortran D compiler. Communication may be further optimized by considering interactions between all the loop nests in the program. <p> With multiple statements in the loop, the local iteration set of a statement may be a subset of the reduced loop bounds. For these statements we need to also add explicit guards based on membership tests for the local iteration set of the statement <ref> [8, 27, 34] </ref>. 3.3.2 Message Generation The Fortran D compiler uses information calculated in the communication analysis and optimization phases to guide message generation. <p> The inspector strategy is not applicable for unanalyzable references causing loop-carried true dependences. In this case the Fortran D compiler inserts guards to resolve the needed communication and program execution at runtime <ref> [8, 27, 34] </ref>. 3.3.3 Storage Management One of the major responsibilities of the Fortran D compiler is to select and manage storage for all nonlocal array references. There are three different storage schemes. Overlaps Overlaps are expansions of local array sections to accommodate neighboring nonlocal elements [13]. <p> It is related to other distributed-memory compilation systems such as Al [30], CM Fortran [1], Dino [28], MIMDizer [15], Pandore [3], Paragon [10], and Spot [29], but mostly builds on the following research projects. SUPERB is a semi-automatic parallelization tool that supports arbitrary user-specified contiguous BLOCK distributions <ref> [13, 34] </ref>. It originated overlaps as a means to both specify and store nonlocal data accesses. Exsr statements are inserted in the program to communicate overlap regions. Data dependence information is then used to apply loop distribution and vectorize these statements, resulting in vectorized mes sages.
References-found: 34

