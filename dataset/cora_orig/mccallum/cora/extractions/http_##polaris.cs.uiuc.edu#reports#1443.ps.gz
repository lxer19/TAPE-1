URL: http://polaris.cs.uiuc.edu/reports/1443.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: c  
Author: flCopyright by Stephen Wilson Turner, 
Date: 1995  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. D. Granston, S. W. Turner, and A. V. Veidenbaum, </author> <title> "Design of a scalable, shared-memory system with support for burst traffic," </title> <booktitle> in Proceedings of the 1st Annual Workshop on Scalable Shared-Memory Architectures (M. </booktitle> <editor> Dubois, ed.), </editor> <publisher> Kluwer & Assoc., </publisher> <month> Feb </month> <year> 1991. </year>
Reference-contexts: These simulations are used to determine system bottlenecks, suggest improvements in the design, verify our simulation methodology, and confirm the results of our previous work regarding the performance of MINs <ref> [1, 2, 3] </ref>. This examination of a real machine forms the basis of our later evaluation of a more modern shared-memory multiprocessor design and provides evidence that valid estimations of system performance are possible through use of detailed traffic and hardware models. <p> To achieve this goal with an inexpensive extension of the current organization is still a challenge. 16 2.5 Traffic throttling In this section we examine an inexpensive yet effective technique to increase the performance of the Cedar shared-memory system. In previous studies <ref> [1, 3] </ref> we have shown that Cedar's shared-memory system could be improved by increasing the size of the buffers in the switches which make up its interconnection networks. However, without a concomitant increase in the speed of the memory modules, the performance improvement from such improved switches is minimal. <p> ($i == $k / 2) - return ($k-1) * 2; - elsif ($i &lt; $k) - # && ($i &gt; $k/2) return 4 * ($k - $i); - else - # ($i == $k) return 1; - - sub choose - local ($a) = $_ [0]; local ($b) = $_ <ref> [1] </ref>; local ($r); $r = &factorial ($a); $r = $r / &factorial ($a-$b); $r = $r / &factorial ($b); return $r; - sub factorial - local ($a) = $_ [0]; local ($i, $r); $r = 1; $r = $r * $i; return ($r); -
Reference: [2] <author> D. Kuck, et al., </author> <title> "The cedar system and an initial performance study," </title> <booktitle> in Proceedings 20th International Symposium on Computer Architecture, </booktitle> <pages> pp. 213-223, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: These simulations are used to determine system bottlenecks, suggest improvements in the design, verify our simulation methodology, and confirm the results of our previous work regarding the performance of MINs <ref> [1, 2, 3] </ref>. This examination of a real machine forms the basis of our later evaluation of a more modern shared-memory multiprocessor design and provides evidence that valid estimations of system performance are possible through use of detailed traffic and hardware models.
Reference: [3] <author> S. W. Turner, </author> <title> "Shared memory and interconnection network performance for vector multiprocessors," </title> <type> Master's thesis, </type> <institution> University of Illinois, Urbana-Champaign, </institution> <month> May </month> <year> 1989. </year> <note> CSRD-TR No. 876. </note>
Reference-contexts: These simulations are used to determine system bottlenecks, suggest improvements in the design, verify our simulation methodology, and confirm the results of our previous work regarding the performance of MINs <ref> [1, 2, 3] </ref>. This examination of a real machine forms the basis of our later evaluation of a more modern shared-memory multiprocessor design and provides evidence that valid estimations of system performance are possible through use of detailed traffic and hardware models. <p> Also, as discussed previously, the mapping of memory addresses to memory banks alters. These changes introduce second-order effects on system performance, which subtly alter the shape of 14 the speedup curves. Clearly, though, all of the four benchmarks achieve sub-optimal speedups. Where does the fault lie? In previous work <ref> [3] </ref>, we showed that there is no single element of the shared-memory system which can be pointed to as the bottleneck. The performance of both networks (forward and reverse) and the memory modules is insufficient and must be increased to realize significant overall performance gain. <p> There are two possible solutions. Either the code must be re-written to better spread the memory addresses across the memory modules, or the shared-memory system must include an address-randomizing function within itself. The latter approach was explored in earlier work <ref> [3] </ref>. It helped smooth out the differences in performance between different benchmarks, although the overall performance gain was not very large. In any case, hardware address randomization eliminates the ability of the compiler and/or programmer to optimize an application's address distribution for a particular architecture. <p> To achieve this goal with an inexpensive extension of the current organization is still a challenge. 16 2.5 Traffic throttling In this section we examine an inexpensive yet effective technique to increase the performance of the Cedar shared-memory system. In previous studies <ref> [1, 3] </ref> we have shown that Cedar's shared-memory system could be improved by increasing the size of the buffers in the switches which make up its interconnection networks. However, without a concomitant increase in the speed of the memory modules, the performance improvement from such improved switches is minimal. <p> We showed that the system "as-is" is basically scalable in performance, but that it fails to achieve the goal of unity-slope speedups on memory-bound benchmarks. In previous work <ref> [3] </ref> we demonstrated that improving the performance of any single component of the system was insufficient to realize this goal. Several key components needed to be modified simultaneously in order to significantly improve system performance.
Reference: [4] <author> S. L. Scott and G. S. Sohi, </author> <title> "The use of feedback in multiprocessors and its application to tree saturation control," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 1, </volume> <pages> pp. 385-399, </pages> <month> Oct </month> <year> 1990. </year>
Reference-contexts: Traffic throttling is not a new idea. In the WAN-world it has long been commonplace for network routers to examine round-trip latencies and throttle back traffic on congested routes accordingly. Scott and Sohi <ref> [4] </ref> as well as Farrens, Wetmore and Woodruff [5] propose using feedback from "hot" memory modules to restrict the injection of traffic destined for those modules.
Reference: [5] <author> M. Farrens, B. Wetmore, and A. Woodruff, </author> <title> "Alleviation of tree saturation in multistage interconnection networks," </title> <booktitle> in Proceedings of Supercomputing '91, </booktitle> <pages> pp. 400-409, </pages> <year> 1991. </year>
Reference-contexts: Traffic throttling is not a new idea. In the WAN-world it has long been commonplace for network routers to examine round-trip latencies and throttle back traffic on congested routes accordingly. Scott and Sohi [4] as well as Farrens, Wetmore and Woodruff <ref> [5] </ref> propose using feedback from "hot" memory modules to restrict the injection of traffic destined for those modules.
Reference: [6] <author> W. J. Dally and H. Aoki, </author> <title> "Deadlock-free adaptive routing in multicomputer networks using virtual channels," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 4, </volume> <pages> pp. 466-475, </pages> <month> Apr </month> <year> 1993. </year>
Reference-contexts: Scott and Sohi [4] as well as Farrens, Wetmore and Woodruff [5] propose using feedback from "hot" memory modules to restrict the injection of traffic destined for those modules. We are not proposing to add any such complex mechanisms, though. 17 Dally and Aoki <ref> [6] </ref> have suggested that since their network design is "unstable" at high loads, nodes should be restricted to injecting new packets into a subset of the available virtual channels. <p> A sub-issue within adaptive routers is minimality, which is discussed next. While non-blocking switches almost certainly improve channel utilization, adaptive switches do not always outperform non-adaptive ones. Recent work has shown that adaptive routers can actually degrade performance by "oversubscribing" network resources <ref> [6, 37] </ref>. Some type of input throttling, as discussed in Section 2.5, may be necessary in order to avoid oversaturating the network. Adaptive schemes may also negatively impact network performance in just the same way non-blocking switches may do so.
Reference: [7] <author> T. Callahan and S. C. Goldstein, "NIFDY: </author> <title> A low overhead, high throughput network interface," </title> <booktitle> in Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <pages> pp. 230-241, </pages> <year> 1995. </year>
Reference-contexts: Our traffic limitation proposal imposes no reduction of throughput for any load level and dramatically improves it at high loads. More recently, Callahan and Goldstein <ref> [7] </ref> have proposed a network interface which provides both packet reordering and flow-control. The amount of traffic injected into the network is limited by allowing each processor to send only a limited number of requests to a single memory module before blocking until one returns.
Reference: [8] <author> F. Darema-Rogers, G. F. Pfister, and K. </author> <title> So, "Memory access patterns of parallel scientific programs," </title> <institution> Computer Science Tech Report 54146, IMB T. J. Watson Research Center, </institution> <address> Yorktown Heights, NY 10598, </address> <month> July </month> <year> 1986. </year>
Reference-contexts: Not all studies of interconnection and memory systems fail to account for these critical aspects of modern systems. For example Darema-Rogers, Pfister, and So demonstrated as far back as 1986 that "low average numbers do not preclude bursts in the reference rates, and that bursts can affect the performance." <ref> [8] </ref> The bursts they refer to are larger in scale than the cache-line or vector-register sized bursts we discuss in this work, but the disproportionate performance impact of those bursts is the same.
Reference: [9] <author> Y.-H. Lee, S. E. Cheung, and J.-K. Peir, </author> <title> "Consecutive request traffic model in multistage interconnection networks," </title> <booktitle> in Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> vol. I, </volume> <pages> pp. 534-541, </pages> <year> 1991. </year> <month> 159 </month>
Reference-contexts: Lee, Cheung and Peir point out that coherent bursts of requests have quite different throughput and latency characteristics than uniformly random traffic patterns <ref> [9] </ref>. Bucher and Calahan provide useful queuing models for delays due to bank conflicts in multi-bank memory systems that include the impact of the closed-loop nature of real systems, though they avoid any consideration of conflicts in the network leading to and from the memory banks [10].
Reference: [10] <author> I. Y. Bucher and D. A. Calahan, </author> <title> "Models of access delays in multiprocessor memo-ries," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <year> 1992. </year>
Reference-contexts: Bucher and Calahan provide useful queuing models for delays due to bank conflicts in multi-bank memory systems that include the impact of the closed-loop nature of real systems, though they avoid any consideration of conflicts in the network leading to and from the memory banks <ref> [10] </ref>. But a dissapointingly large, and surprisingly influential, set of papers do not address these two issues.
Reference: [11] <author> G. Lee, </author> <title> "A performance bound of multistage combining networks," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-38, </volume> <pages> pp. 1387-1395, </pages> <month> Oct </month> <year> 1989. </year>
Reference-contexts: Using simu-lations of this networked system, we then compare two different performance evaluation methodologies: open-loop and closed-loop. The effect of adding adaptivity to a virtual-channel router is evaluated using both the standard "Open Network" <ref> [11] </ref> model and a methodology derived from our study of Cedar.
Reference: [12] <author> M. J. Karol, M. G. Hluchyj, and S. P. Morgan, </author> <title> "Input versus output queueing on a space-division packet switch," </title> <journal> IEEE Transactions on Communications, </journal> <volume> vol. COM-35, </volume> <pages> pp. 1347-1356, </pages> <month> Dec </month> <year> 1987. </year>
Reference-contexts: Because of the small size of these switches and the importance of the nodal switch in reducing a potential bottleneck it is output buffered, or non-blocking. <ref> [12] </ref> The shared-memory module has an access time of 2 cycles. <p> If the packet (s) behind the blocked HOL packets can be routed around the contending packets, additional output channels can be utilized. Karol et al. <ref> [12] </ref> show that blocking switches are limited to approximately 60% of their peak throughput (assuming random traffic and large switches). Non-blocking switches can realize 100% of their peak throughput, if the traffic is distributed so as to utilize all output links. <p> Non-blocking switches can realize 100% of their peak throughput, if the traffic is distributed so as to utilize all output links. Non-blocking switches do this in one of a few ways. First, they may transfer all HOL packets (regardless of conflicts) into their output queues in a single cycle <ref> [12, 29] </ref>. This can be achieved by using a switch implementation that runs N times as fast as the links between switches. The former is impractical, considering the current technique of 58 using pipelined channels [30] to maintain multiple bits "on the fly" in a wire. <p> The amount of traffic in the each section is in turn dependent on the latency which traffic experiences in that section. Hence this model results in a set of coupled equations linking latency and occupancy. To eliminate the impact of blocking due to "head-of-line waiting" effects <ref> [12] </ref>, for this model we assume switches are buffered on their outputs, and each output buffer can accept a flit from all inputs in one cycle.
Reference: [13] <author> P. Kermani and L. Kleinrock, </author> <title> "Virtual cut-through: A new computer communication switching technique," </title> <booktitle> Computer Networks 3, </booktitle> <pages> pp. 267-286, </pages> <year> 1979. </year>
Reference-contexts: If a router simultaneously receives multiple packets which are to be sent to a single output port, they are transferred to the associated output queue sequentially; the order is arbitrated by random selection. The switches use virtual cut-through flow control to regulate buffer occupancy <ref> [13] </ref>. Network switches operate on network traffic in units of flits. A flit is the basic flow-control unit and is equal in size to the width of the inter-switch connections or channels. Channels are not pipelined.
Reference: [14] <author> W. J. Dally, </author> <title> "Virtual-channel flow control," </title> <booktitle> in Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pp. 60-68, </pages> <year> 1990. </year>
Reference-contexts: Routers using this algorithm assume that each input has unbounded buffer capacity so that deadlock prevention is not required. The a more sophisticated scheme is oblivious virtual-channel routing (VC), which uses the virtual-channel routing scheme proposed by Dally <ref> [14] </ref>. Although packets are routed 28 via the same paths as in the DIMO router (i.e., dimension order routing is used), each physical channel is shared by two virtual channels, which allows this router to ensure deadlock freedom even in the absence of infinite buffering. <p> Deadlock avoidance is usually preferable to deadlock detection and recovery, because recovery from deadlock introduces additional complexity into the processing nodes, and detection of a deadlock condition requires additional hardware in the interconnection system. A common method of deadlock avoidance involves the use of virtual channels <ref> [14, 15, 41] </ref>: multiple logical channels mapped onto each physical channel in the topology. Each virtual channel has it's own dedicated buffer space, and deadlock freedom is assured by guaranteeing that there are no cycles among the virtual channels.
Reference: [15] <author> P. E. Berman, L. Gravano, G. D. Pifarre, and J. L. C. Sanz, </author> <title> "Adaptive deadlock-and livelock-free routing with all minimal paths in torus networks," </title> <booktitle> in Proceedings of the Symposium on Parallel Architectures and Algorithms, </booktitle> <year> 1992. </year>
Reference-contexts: The STAR router is based on the *-Channels routing scheme <ref> [15] </ref>. It uses a single additional virtual-channel per physical channel (for a total of 3) to provide adaptive paths through the network. Although it allows adaptivity and use with finite buffers, its buffer utilization is hampered by non-uniform virtual channel occupancy, which is an unavoidable consequence of its routing scheme. <p> Deadlock avoidance is usually preferable to deadlock detection and recovery, because recovery from deadlock introduces additional complexity into the processing nodes, and detection of a deadlock condition requires additional hardware in the interconnection system. A common method of deadlock avoidance involves the use of virtual channels <ref> [14, 15, 41] </ref>: multiple logical channels mapped onto each physical channel in the topology. Each virtual channel has it's own dedicated buffer space, and deadlock freedom is assured by guaranteeing that there are no cycles among the virtual channels.
Reference: [16] <institution> Digital Equipment Corporation, Alpha Architecture Handbook, </institution> <month> Apr </month> <year> 1992. </year>
Reference-contexts: We derive our traffic model from two such significant aspects of Cedar's memory access behavior. First, accesses occur in bursts. In Cedar vector accesses and prefetch operations dominate shared-memory traffic. This may also be true in current and near-future systems in part because they include prefetch instructions <ref> [16, 17, 18, 19, 20] </ref>. Some of these prefetch instructions are capable of specifying large blocks of data to be fetched.
Reference: [17] <author> D. L. Weaver and T. </author> <title> Germond, The SPARC Architecture Manual. </title> <booktitle> Sparc International, </booktitle> <address> 535 Middlefield Road, Suite 210, Menlo Park, CA 94025, </address> <publisher> sparc v9 ed. </publisher>
Reference-contexts: We derive our traffic model from two such significant aspects of Cedar's memory access behavior. First, accesses occur in bursts. In Cedar vector accesses and prefetch operations dominate shared-memory traffic. This may also be true in current and near-future systems in part because they include prefetch instructions <ref> [16, 17, 18, 19, 20] </ref>. Some of these prefetch instructions are capable of specifying large blocks of data to be fetched.
Reference: [18] <author> Hewlett Packard, </author> <title> PA-RISC 1.1 Architecture and Instruction Set Reference Manual, </title> <note> 3rd edition ed., </note> <month> Feb </month> <year> 1994. </year>
Reference-contexts: We derive our traffic model from two such significant aspects of Cedar's memory access behavior. First, accesses occur in bursts. In Cedar vector accesses and prefetch operations dominate shared-memory traffic. This may also be true in current and near-future systems in part because they include prefetch instructions <ref> [16, 17, 18, 19, 20] </ref>. Some of these prefetch instructions are capable of specifying large blocks of data to be fetched.
Reference: [19] <institution> IBM Microelectronics and Motorola, </institution> <month> (800) 769-3772, </month> <title> PowerPC 604 RISC Microprocessor User's Manual, </title> <month> Nov </month> <year> 1994. </year>
Reference-contexts: We derive our traffic model from two such significant aspects of Cedar's memory access behavior. First, accesses occur in bursts. In Cedar vector accesses and prefetch operations dominate shared-memory traffic. This may also be true in current and near-future systems in part because they include prefetch instructions <ref> [16, 17, 18, 19, 20] </ref>. Some of these prefetch instructions are capable of specifying large blocks of data to be fetched.
Reference: [20] <institution> MIPS Technologies, Inc., </institution> <address> 2011 North Shoreline, Mountain View CA 94039-7311, </address> <note> MIPS R1000 Microprocessor User's Manual, alpha rev 2.0 ed., </note> <year> 1995. </year>
Reference-contexts: We derive our traffic model from two such significant aspects of Cedar's memory access behavior. First, accesses occur in bursts. In Cedar vector accesses and prefetch operations dominate shared-memory traffic. This may also be true in current and near-future systems in part because they include prefetch instructions <ref> [16, 17, 18, 19, 20] </ref>. Some of these prefetch instructions are capable of specifying large blocks of data to be fetched.
Reference: [21] <author> R. M. Gray and L. D. Davisson, </author> <title> Random Processes. </title> <publisher> Prentice-Hall, </publisher> <year> 1986. </year>
Reference-contexts: The amount of local processing that occurs between bursts of accesses varies across a wide rage, but the average value is small compared to the maximum. A negative-exponentially distributed random variable, which is the result of a Poisson process <ref> [21] </ref>, also shares this characteristic. This distribution is characteristic of repeated trials with constant probability 31 and is often used to model real-world behavior of unpredictable systems.
Reference: [22] <editor> F. M. auf der Heide, </editor> <title> "Hashing strategies for simulating shared memory on distributed memory machines," </title> <type> Tech. Rep. </type> <institution> TR-RF-93-006, Heinz Nixdorf Institute and Computer Science Department, University of Paderborn, </institution> <address> 4970 Paderborn, Germany, </address> <year> 1993. </year>
Reference-contexts: This approach has the disadvantage of neglecting the impact of real address distributions, which will normally be non-uniform, but it has the advantage of making our results relatively independent of data placement and memory interleaving schemes. In addition, theoretic studies <ref> [22] </ref> have been presented which advocate the use of randomizing memory addresses via a hashing function so that the memory behavior of distributed memory machines (like those simulated here) more closely resembles that of the ideal PRAM model.
Reference: [23] <author> C. J. Glass and L. M. Ni, </author> <title> "The turn model for adaptive routing," </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> vol. 41, </volume> <pages> pp. 874-902, </pages> <month> Sep </month> <year> 1994. </year>
Reference-contexts: All experiments were run long enough so that steady-state behavior was observed. 3.4.1 Open-loop evaluation In published evaluations of interconnection network performance, the most common method of determining the impact of some change to the system under study is to compare latency vs. throughput curves <ref> [23, 24, 25, 26, 27] </ref>. These curves plot throughput on the x-axis and latency on y-axis, often expressing throughput as a value normalized by some ideal maximum.
Reference: [24] <author> R. Cypher and L. Gravano, </author> <title> "Adaptive, deadlock-free packet routing in torus networks with minimal storage," </title> <booktitle> in Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> vol. III, </volume> <pages> pp. 204-211, </pages> <year> 1992. </year> <month> 160 </month>
Reference-contexts: All experiments were run long enough so that steady-state behavior was observed. 3.4.1 Open-loop evaluation In published evaluations of interconnection network performance, the most common method of determining the impact of some change to the system under study is to compare latency vs. throughput curves <ref> [23, 24, 25, 26, 27] </ref>. These curves plot throughput on the x-axis and latency on y-axis, often expressing throughput as a value normalized by some ideal maximum. <p> Another way to reduce HOL blocking is to use a single shared buffer for all messages in the switch, and equip that buffer with N input and N output ports. This structure essentially an N-ported register file and is sometimes referred to as a dynamically-allocated multiqueue (DAMQ) <ref> [24, 34, 35] </ref>. Given an arbiter capable of optimally allocating packets to output ports so that the maximum possible number of output ports is in use, this structure can provide improved performance by eliminating HOL blocking effects [36].
Reference: [25] <author> M. T. Raghunath, </author> <title> Interconnection Network Design Based on Packaging Considera--tions. </title> <type> PhD thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1993. </year>
Reference-contexts: All experiments were run long enough so that steady-state behavior was observed. 3.4.1 Open-loop evaluation In published evaluations of interconnection network performance, the most common method of determining the impact of some change to the system under study is to compare latency vs. throughput curves <ref> [23, 24, 25, 26, 27] </ref>. These curves plot throughput on the x-axis and latency on y-axis, often expressing throughput as a value normalized by some ideal maximum.
Reference: [26] <author> G. D. Pifarre, L. Gravano, G. Denicolay, and J. L. C. Sanz, </author> <title> "Adaptive deadlock-and livelock-free routing in hypercube network," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 5, </volume> <pages> pp. 1121-1138, </pages> <month> Nov </month> <year> 1994. </year>
Reference-contexts: All experiments were run long enough so that steady-state behavior was observed. 3.4.1 Open-loop evaluation In published evaluations of interconnection network performance, the most common method of determining the impact of some change to the system under study is to compare latency vs. throughput curves <ref> [23, 24, 25, 26, 27] </ref>. These curves plot throughput on the x-axis and latency on y-axis, often expressing throughput as a value normalized by some ideal maximum.
Reference: [27] <author> A. R. Lebeck and G. S. Sohi, </author> <title> "Request combining in multiprocessors with arbitrary interconnection networks," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 5, </volume> <pages> pp. 1140-1155, </pages> <month> Nov </month> <year> 1994. </year>
Reference-contexts: All experiments were run long enough so that steady-state behavior was observed. 3.4.1 Open-loop evaluation In published evaluations of interconnection network performance, the most common method of determining the impact of some change to the system under study is to compare latency vs. throughput curves <ref> [23, 24, 25, 26, 27] </ref>. These curves plot throughput on the x-axis and latency on y-axis, often expressing throughput as a value normalized by some ideal maximum.
Reference: [28] <author> L. Kleinrock, </author> <title> Queuing Theory, vol. I. </title> <publisher> Wiley, </publisher> <year> 1975. </year>
Reference-contexts: These curves plot throughput on the x-axis and latency on y-axis, often expressing throughput as a value normalized by some ideal maximum. They are simple to create and analyze because the traffic model assumes completely independent requests with Markovian arrivals <ref> [28] </ref>. 42 the three basic router models: DIMO, VC and STAR. In Figure 3.3 (a) each response contains the single 64-bit data word corresponding to the short-response traffic model (i.e., access size (N d ) is 4 flits).
Reference: [29] <author> J. S.-C. Chen and T. E. Stern, </author> <title> "Throughput analysis, optimal buffer allocation, and traffic imbalance study of a generic nonblocking packet switch," </title> <journal> IEEE Journal on Selected Areas in Communication, </journal> <volume> vol. 9, </volume> <pages> pp. 439-449, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: When feasible, non-blocking switch designs are preferable because blocking switches encounter queuing delays and decreased network throughput from Head-Of-Line (HOL) blocking <ref> [29] </ref>. HOL blocking occurs whenever two packets at the heads of their respective input queues contend for the same output port, and one or more input queues contain successive packets destined for a free output queue. <p> Non-blocking switches can realize 100% of their peak throughput, if the traffic is distributed so as to utilize all output links. Non-blocking switches do this in one of a few ways. First, they may transfer all HOL packets (regardless of conflicts) into their output queues in a single cycle <ref> [12, 29] </ref>. This can be achieved by using a switch implementation that runs N times as fast as the links between switches. The former is impractical, considering the current technique of 58 using pipelined channels [30] to maintain multiple bits "on the fly" in a wire.
Reference: [30] <author> S. L. Scott, </author> <title> Toward the Design of Large-Scale Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Wisconsin - Madison, </institution> <year> 1992. </year>
Reference-contexts: This can be achieved by using a switch implementation that runs N times as fast as the links between switches. The former is impractical, considering the current technique of 58 using pipelined channels <ref> [30] </ref> to maintain multiple bits "on the fly" in a wire. Pipelined channel techniques mean that speed-of-light and distance limitations no longer need to restrict the clock rate of inter-switch links, only transmission-line effects need do so.
Reference: [31] <author> H. B. Bakoglu, </author> <title> Circuits, Interconnections and Packaging for VLSI. </title> <publisher> Addison Wesley, </publisher> <year> 1990. </year>
Reference-contexts: Pipelined channel techniques mean that speed-of-light and distance limitations no longer need to restrict the clock rate of inter-switch links, only transmission-line effects need do so. Since modern cables and circuit board traces are capable of reliably propagating 100MHz signals <ref> [31] </ref>, switches that run N times faster are not currently feasible. In the future, optical interconnections and/or switches may change this, but we are considering the near-term design space and so cover only electrical systems.
Reference: [32] <author> E. D. B. III, </author> <title> "A butterfly processor-memory interconnection for a vector processing environment," </title> <journal> Parallel Computing, </journal> <volume> vol. 4, </volume> <pages> pp. 103-110, </pages> <year> 1987. </year>
Reference-contexts: In the future, optical interconnections and/or switches may change this, but we are considering the near-term design space and so cover only electrical systems. Another way to design a non-blocking switch is to employ a separate buffer for each input-output pair <ref> [32] </ref>. This requires a buffer structure within the switch that is O (N 2 ) in size, and the utilization of the aforementioned buffers will naturally be only O (N ) at best.
Reference: [33] <author> M. A. Franklin, D. F. Wann, and W. J. Thomas, </author> <title> "Pin limitations and partitioning of vlsi interconnection networks," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-31, </volume> <pages> pp. 1109-1116, </pages> <month> Nov </month> <year> 1982. </year>
Reference-contexts: Because on-chip resources are relatively inexpensive (in comparison to packaging and inter-chip wiring costs) this could be a feasible solution. However, even through on-chip resources are plentiful in terms of chip area, the use of those resources results in longer switch cycle times <ref> [33] </ref>. Another way to reduce HOL blocking is to use a single shared buffer for all messages in the switch, and equip that buffer with N input and N output ports.
Reference: [34] <author> S. Konstantinidou, </author> <title> Deterministic and Chaotic Adaptive Routing in Multicomputers. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Another way to reduce HOL blocking is to use a single shared buffer for all messages in the switch, and equip that buffer with N input and N output ports. This structure essentially an N-ported register file and is sometimes referred to as a dynamically-allocated multiqueue (DAMQ) <ref> [24, 34, 35] </ref>. Given an arbiter capable of optimally allocating packets to output ports so that the maximum possible number of output ports is in use, this structure can provide improved performance by eliminating HOL blocking effects [36]. <p> This lack of flow control also provides the basic assumption upon which the proof that the router will not internally deadlock rests. In these proofs, this is the major difference between the Shunt router and the Chaos router proposed by Konstantinidou <ref> [34] </ref>. In other respects the proofs are quite similar to that of Konstantinidou, and draw heavily on them. First, as stated above, the input buffers never refuse a message. <p> So, no message ever experiences an unbounded wait, and so no deadlock can occur within the router. By the Postulate, if no deadlock occurs within the router, no deadlock will occur in a system constructed of them. 74 4.3.3 Freedom from livelock Konstantinidou has shown <ref> [34] </ref> that nonminimal routers which randomly choose a message to deroute can guarantee message delivery with a likelihood that approaches unity as time approaches infinity, in the limit.
Reference: [35] <author> Y. Tamir and G. L. Frazier, </author> <title> "High-performance multi-queue buffers for vlsi communication switches," </title> <booktitle> in Proceedings of the 15th International Symposium on Computer Architecture, </booktitle> <pages> pp. 343-354, </pages> <year> 1988. </year>
Reference-contexts: Another way to reduce HOL blocking is to use a single shared buffer for all messages in the switch, and equip that buffer with N input and N output ports. This structure essentially an N-ported register file and is sometimes referred to as a dynamically-allocated multiqueue (DAMQ) <ref> [24, 34, 35] </ref>. Given an arbiter capable of optimally allocating packets to output ports so that the maximum possible number of output ports is in use, this structure can provide improved performance by eliminating HOL blocking effects [36].
Reference: [36] <author> Y. Tamir and H.-C. Chi, </author> <title> "Symmetric crossbar arbiters for vlsi communication switches," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 4, </volume> <pages> pp. 13-27, </pages> <month> Jan </month> <year> 1993. </year>
Reference-contexts: Given an arbiter capable of optimally allocating packets to output ports so that the maximum possible number of output ports is in use, this structure can provide improved performance by eliminating HOL blocking effects <ref> [36] </ref>. However, such a structure is necessarily quite complex and so challenging to fabricate in such a way as to support the short cycle times which are required by modern and near-future networks.
Reference: [37] <author> F. Hady and D. Smitley, </author> <title> "Adaptive vs. non-adaptive routing: An application driven case study," </title> <type> Tech. Rep. </type> <institution> SRC-TR-93-099, Supercomputing Research Center, 17100 Science Drive, Bowie, MD 20715, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: A sub-issue within adaptive routers is minimality, which is discussed next. While non-blocking switches almost certainly improve channel utilization, adaptive switches do not always outperform non-adaptive ones. Recent work has shown that adaptive routers can actually degrade performance by "oversubscribing" network resources <ref> [6, 37] </ref>. Some type of input throttling, as discussed in Section 2.5, may be necessary in order to avoid oversaturating the network. Adaptive schemes may also negatively impact network performance in just the same way non-blocking switches may do so.
Reference: [38] <author> A. A. Chien, </author> <title> "A cost and speed model for k-ary n-cube wormhole routers," in Hot Interconnects '93, </title> <booktitle> 1993. </booktitle> <pages> 161 </pages>
Reference-contexts: The impact of adaptivity on router latency for a particular fabrication technology and various adaptive routing strategies has been discussed by Chien <ref> [38] </ref>. However, he fails to examine the adaptive model most similar to ours, the Chaos router. Both the Chaos router and our router minimize the impact of adaptivity by pushing the adaptive decision off of the shortest path through the switch.
Reference: [39] <author> L. M. Ni and P. K. McKinley, </author> <title> "A survey of routing techniques in wormhole net-works," </title> <type> Tech. Rep. </type> <institution> MSU-CPS-ACS-46, Michigan State University, </institution> <address> East Lansing, MI 48824-1027, </address> <month> Oct </month> <year> 1991. </year>
Reference-contexts: We will pay particular attention to the network states that adaptivity helps and how much system-wide improvement it can possibly exploit. 4.1.3 Derouting An adaptive router is said to be minimal if it exploits only the shortest paths through a network, and non-minimal otherwise <ref> [39] </ref>. The action of sending a message along a link that does not yield progress towards its destination is called "derouting". The adaptive routing function that we present here is nonminimal, because it uses derouting. In minimal adaptive routers, messages are always routed closer to their destination.
Reference: [40] <author> S. Konstantinidou and L. Snyder, </author> <title> "Chaos router: Architecture and performance," </title> <booktitle> in Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pp. 212-221, </pages> <year> 1991. </year>
Reference-contexts: Because each time a packet is involved 61 in a conflict it has an equal chance of winning the conflict resolution, the probability of reaching its destination approaches unity with increasing time in the network. This approach was pioneered in the Chaos router <ref> [40] </ref>. A random choice function can be simply implemented in hardware, and no extra packet overhead is required. 4.1.4 Deadlock When considering direct networks, the issue of message deadlock must be addressed.
Reference: [41] <author> A. A. Chien and J. H. Kim, </author> <title> "Planar-adaptive routing: Low-cost adaptive networks for multiprocessors," </title> <booktitle> in Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pp. 268-277, </pages> <year> 1992. </year>
Reference-contexts: Deadlock avoidance is usually preferable to deadlock detection and recovery, because recovery from deadlock introduces additional complexity into the processing nodes, and detection of a deadlock condition requires additional hardware in the interconnection system. A common method of deadlock avoidance involves the use of virtual channels <ref> [14, 15, 41] </ref>: multiple logical channels mapped onto each physical channel in the topology. Each virtual channel has it's own dedicated buffer space, and deadlock freedom is assured by guaranteeing that there are no cycles among the virtual channels.
Reference: [42] <author> K. Bolding, </author> <title> "Non-uniformities introduced by virtual channel deadlock prevention," </title> <type> Tech. Rep. </type> <institution> UW-CSE-92-07-07, Department of Computer Science and Engineering, University of Washington, </institution> <address> Seatle, Washington 98195, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Unfortunately, existing virtual channel schemes introduce asymmetry into the symmetric torus network, and reduce buffer utilization and effective channel capacity in both tori and meshes <ref> [42] </ref>. Deflection (or "hot-potato") routing misroutes packets in the event of contention for network channels. It was used in the HEP and CM-2 machines, and a variety of alternative implementations, both adaptive and oblivious, have been proposed [43, 44, 45].
Reference: [43] <author> S. Konstantinidou, </author> <title> "Priorities in nonminimal, adaptive routing," </title> <booktitle> in Proceedings of the International Conference on Parallel Processing, </booktitle> <volume> vol. I, </volume> <pages> pp. 67-72, </pages> <year> 1992. </year>
Reference-contexts: Deflection (or "hot-potato") routing misroutes packets in the event of contention for network channels. It was used in the HEP and CM-2 machines, and a variety of alternative implementations, both adaptive and oblivious, have been proposed <ref> [43, 44, 45] </ref>. Deflection routing does not introduce the asymmetries of virtual channels, does not require dividing the switch buffer pool into a large number of segments and, with the use of random misrouting decisions, the routing hardware can be made to operate very 62 quickly.
Reference: [44] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith, </author> <title> "The tera computer system," </title> <booktitle> in International Conference on Supercomputing 1990, </booktitle> <pages> pp. 1-6, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: Deflection (or "hot-potato") routing misroutes packets in the event of contention for network channels. It was used in the HEP and CM-2 machines, and a variety of alternative implementations, both adaptive and oblivious, have been proposed <ref> [43, 44, 45] </ref>. Deflection routing does not introduce the asymmetries of virtual channels, does not require dividing the switch buffer pool into a large number of segments and, with the use of random misrouting decisions, the routing hardware can be made to operate very 62 quickly.
Reference: [45] <author> A. Nowatzyk, </author> <title> A Communication Architecture for Multiprocessor Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213, </address> <month> Dec </month> <year> 1989. </year>
Reference-contexts: Deflection (or "hot-potato") routing misroutes packets in the event of contention for network channels. It was used in the HEP and CM-2 machines, and a variety of alternative implementations, both adaptive and oblivious, have been proposed <ref> [43, 44, 45] </ref>. Deflection routing does not introduce the asymmetries of virtual channels, does not require dividing the switch buffer pool into a large number of segments and, with the use of random misrouting decisions, the routing hardware can be made to operate very 62 quickly.
Reference: [46] <author> D. B. Wagner, </author> <title> "Approximate mean value analysis of delection-routed shu*e-loop networks," </title> <type> Tech. Rep. </type> <institution> CU-CS-608-92, Department of Computer Science, University of Colorado, Boulder, </institution> <month> Aug </month> <year> 1992. </year>
Reference-contexts: Because of its ability to support high switching speeds and its ability to avoid using inter-node flow-control, deflection routers have been suggested as suitable for use in opto-electronic networks <ref> [46] </ref>. The Shunt router design which we propose and study here uses deflection routing. 4.2 Operation of the Shunt router This section describes in detail the way the Shunt router operates. A block diagram of its structure is shown in Figure 4.1.
Reference: [47] <author> R. Jain, </author> <title> The Art of Computer Systems Performance Analysis. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: If all processors are constantly issuing bursts into the network then they are all competing for a share of that bandwidth. This limit is important because it is a good approximation of system behavior under very heavy load. The remaining model is based on a mean-value analysis <ref> [47] </ref> of the system. This model is called MVA. It is important because it provides an estimate of system performance between the loading limits provided by the previous two: when the system is neither unloaded nor overloaded the MVA model serves best. <p> We base our analysis on the delay model of Reiser and Lavenberg [49] for the response time of devices in closed queuing networks. We also make use of Shweitzer's approximation ([50] cited in <ref> [47] </ref>), to make our MVA analysis more tractable. In order to determine the time required for a packet to traverse a component, we determine the time required for the first flit in the packet to do so.
Reference: [48] <author> W. J. Dally, </author> <title> A VLSI Architecture for Concurrent Data Structures. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, Pasadena, California, </institution> <month> March </month> <year> 1986. </year>
Reference-contexts: In our system we have assumed that this requires a single cycle, so the total additional latency is 4 cycles. 120 Assuming a bidirectional torus with dimension n and radix k (i.e., a k-ary n-cube <ref> [48] </ref>), if k is even then the average distance in network hops (d) is given by: d = 4 If k is odd then the exact expression is: d = n 1 Because we consider only systems with k even and greater than 8, we use the simpler form throughout.
Reference: [49] <author> M. Reiser and S. S. Lavenberg, </author> <title> "Mean-value analysis of closed multichain queuing networks," </title> <journal> Journal of the ACM, </journal> <volume> vol. 27, no. 2, </volume> <pages> pp. 313-322, </pages> <year> 1980. </year>
Reference-contexts: We assume that the system has reached a condition of flow-equilibrium (i.e., "steady-state") and that messages are evenly distributed across the network switches and the memory modules. We base our analysis on the delay model of Reiser and Lavenberg <ref> [49] </ref> for the response time of devices in closed queuing networks. We also make use of Shweitzer's approximation ([50] cited in [47]), to make our MVA analysis more tractable.
Reference: [50] <author> P. Schweitzer, </author> <title> "Approximate analysis of multiclass closed networks of queues," </title> <booktitle> in International Conference on Stochastic Control and Optimization, </booktitle> <address> Amsterdam, </address> <year> 1979. </year> <month> 162 </month>
References-found: 50

