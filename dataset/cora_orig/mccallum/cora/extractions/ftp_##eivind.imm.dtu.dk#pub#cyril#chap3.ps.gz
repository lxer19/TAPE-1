URL: ftp://eivind.imm.dtu.dk/pub/cyril/chap3.ps.gz
Refering-URL: http://eivind.imm.dtu.dk/staff/goutte/PUBLIS/thesis.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: A model f w depending on a set of parameters w is used to estimate
Author: C (w) S (w) ~R (w) (.) 
Note: 3 Hyper-parameters 3.1 Introduction The general setting for this chapter is similar to chapters 1 and 2.  
Abstract: We have introduced earlier the use of regularisation in the learning procedure. It should now be understood that regularisation is most often a necessity to increase the quality of the results. Even when the unregularised solution is acceptable, it is likely that some regularisation will produce an improvement in performance. There does not exist any method giving directly the best value for the regularisation parameter ~, even in the linear case. The topic of this chapter is thus to propose some methods to estimate the best value. The best ~ being the one that leads to the smallest generalisation error, the methods presented and compared here propose estimators of the generalisation error. This estimation can then be used to approximate the best regularisation level. In sections 3.2 to 3.4 we present validation-based techniques. They estimate the generalisation error on the basis of some extra data. In sections 3.6 to 3.9, we deal with algebraic estimates of this error, that do not use any extra data, but rely on a number of assumptions. The contribution of this chapter is to present all these techniques and analyse them on the same ground. We also present some short derivations clarifying the links between different estimators of generalisation error, as well as a comparison between them. During the course of this chapter, the error will be the quadratic difference. For the validation-based methods, it is possible to consider any kind of error without modification of the method. On the other hand, the algebraic estimates are specific to the quadratic cost. Adapting them to another cost function would require to derive new expressions for the estimators. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Akaike, H. </author> <year> (1969). </year> <title> Fitting autoregressive models for prediction. </title> <journal> Annals of the Institute of Statistical Mathematics, </journal> <volume> 21 </volume> <pages> 243-247. </pages>
Reference: <author> Akaike, H. </author> <year> (1974). </year> <title> A new look at the statistical model identification. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 19(6) </volume> <pages> 716-723. </pages>
Reference-contexts: The counting method for estimating the effective number of parameters for Laplace/L 1 regularisation is illustrated by Goutte (1996). 3.8 Mallows' C p was introduced by Mallows (1973), soon followed by Akaike's AIC in <ref> (Akaike, 1974) </ref>.
Reference: <author> Efron, B. E. </author> <year> (1982). </year> <title> The Jacknife, the Bootstrap and Other Resampling plans, </title> <booktitle> volume 38 of CBMS-NSF Regional Conference Series in Applied Mathematics. </booktitle> <publisher> SIAM. </publisher> <address> c flC. Goutte 1996 References 55 Goutte, C. </address> <year> (1996). </year> <title> On the use of a pruning prior for neural networks. </title> <booktitle> In NNSP96 (1996), </booktitle> <pages> pages 52-61. </pages>
Reference-contexts: However there does not seem to be any consensus yet on the optimal split ratio. 3.4 The leave-one-out cross-validation scheme has been named e.g. hold-out method by Weigend et al. (1990). It also shares some similarity with the jackknife sta tistical procedure, but differs in the purpose <ref> (see Efron, 1982) </ref>. 3.5 A more precise study of the computational cost is given in section 3.12. The "no free lunch" theorem is due to (Wolpert and Macready, 1995). This result c flC. Goutte 1996 54 Hyper-parameters has sparked heated debates in the computational learning community.
Reference: <author> Goutte, C. </author> <year> (1997). </year> <title> Note on free lunches and cross-validation. Neural Computation, </title> <type> 9(6). </type> <note> to appear. </note>
Reference: <author> Hansen, L. K. and Larsen, J. </author> <year> (1996). </year> <title> Linear unlearning for cross-validation. </title> <booktitle> Advances in Computational Mathematics, </booktitle> <address> 5(2,3):269-280. </address>
Reference: <author> Hornik, K., Stinchcombe, M., and White, H. </author> <year> (1989). </year> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-368. </pages>
Reference-contexts: It illustrates the effect of the number of effective parameters, and shows that GPE and FPER provide close estimates (in that case). 3.11 The universal approximation of neural networks is demonstrated in the clas sical <ref> (Hornik et al., 1989) </ref>. 3.12 The computational cost of several optimisation techniques applied to neural networks has been studied by Mtller (1993). The resource asymptotics men-tionned in this section relate to efficient optimisation methods such as the scaled conjugate gradient.
Reference: <author> Kearns, M. </author> <year> (1996). </year> <title> A bound on the error of cross validation using the approximation and estimation rates, with consequences for the training-test split. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, number 8 in NIPS. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Larsen, J. </author> <year> (1992). </year> <title> A generalization error estimate for nonlinear systems. </title> <editor> In Kung, S. Y., Fallside, F., and Strensen, J. A., editors, </editor> <booktitle> Neural Networks for Signal Processing Proceedings of the 1992 IEEE Workshop, number II in NNSP, </booktitle> <pages> pages 29-38, </pages> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Larsen, J. and Hansen, L. K. </author> <year> (1994). </year> <title> Generalized performance of regularized neural networks models. </title> <editor> In Vlontzos, J., Hwang, J. N., and Wilson, E., editors, </editor> <booktitle> Neural Networks for Signal Processing IV Proceedings of the 1994 IEEE Workshop, number IV in NNSP, </booktitle> <pages> pages 42-51, </pages> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference-contexts: A brief empirical comparison between FPE (with raw parameter number), GPE and FPER is included at the end of <ref> (Larsen and Hansen, 1994) </ref>.
Reference: <author> Larsen, J. and Hansen, L. K. </author> <year> (1995). </year> <title> Empirical generalization assessment of neural network models. </title> <editor> In Girosi, F., editor, </editor> <booktitle> Neural Networks for Signal Processing V Proceedings of the 1995 IEEE Workshop, number V in NNSP, </booktitle> <pages> pages 42-51, </pages> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Larsen, J., Hansen, L. K., Svarer, C., and Ohlsson, M. </author> <year> (1996). </year> <title> Design and regularization of neural networks: the optimal use of a validation set. </title> <booktitle> In NNSP96 (1996), </booktitle> <pages> pages 62-71. </pages>
Reference: <author> Ljung, L., Sjoberg, J., and McKelvey, T. </author> <year> (1992). </year> <title> On the use of regularization in system identification. </title> <type> Technical Report 1379, </type> <institution> Department of Electrical Engineering, Linkoping University, S-581 83 Linkoping, Sweden. </institution>
Reference: <author> Mallows, C. </author> <year> (1973). </year> <title> Some comments on c p . Technometrics, </title> <booktitle> 15 </booktitle> <pages> 661-675. </pages>
Reference: <author> Mtller, M. </author> <year> (1993). </year> <title> A scaled conjugate gradient algorithm for fast supervised learning. </title> <booktitle> Neural Networks, </booktitle> <volume> 6(4) </volume> <pages> 525-533. </pages>
Reference: <author> Moody, J. </author> <year> (1991). </year> <title> Note on generalization, regularization and architecture selection in nonlinear learning systems. </title> <editor> In Juang, B. H., Kung, S. Y., and Kamm, C. A., editors, </editor> <booktitle> Proceedings of the first IEEE Workshop on Neural Networks for Signal Processing, number I in NNSP, </booktitle> <pages> pages 1-10, </pages> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Murata, N., Yoshizawa, S., and Amari, S. </author> <year> (1994). </year> <title> Network Information Criterion| determining the number of hidden units for an artificial neural network model. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(6) </volume> <pages> 865-872. </pages> <address> c flC. Goutte 1996 56 Hyper-parameters NNSP96 (1996). </address> <booktitle> Neural Networks for Signal Processing VI Proceedings of the 1996 IEEE Workshop, number VI in NNSP, </booktitle> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Schwartz, G. </author> <year> (1978). </year> <title> Estimating the dimension of a model. </title> <journal> The Annals of Statistics, </journal> <volume> 6(2) </volume> <pages> 461-464. </pages>
Reference: <author> Strensen, P. H., Ntrg-ard, M., Hansen, L. K., and Larsen, J. </author> <year> (1996). </year> <title> Cross-validation with luloo. </title> <booktitle> In Proceedings of 1996 International Conference on Neural Information Processing, </booktitle> <address> ICONIP'96. </address>
Reference: <author> Weigend, A. S., Huberman, B. A., and Rumelhart, D. E. </author> <year> (1990). </year> <title> Predicting the future: a connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(3) </volume> <pages> 193-210. </pages>
Reference: <author> Wolpert, D. H. and Macready, W. G. </author> <year> (1995). </year> <title> The mathematics of search. </title> <type> Technical Report SFI-TR-95-02-010, </type> <institution> Santa Fe Institute. </institution>
Reference-contexts: Practical application of cross-validation is then subject to an unpleasant trade-off between getting noisy estimates and computational cost. Unfortunately, this is not the only limitation of this method. Recent developments in computational learning theory have led to the so-called "no free lunch" theorem <ref> (Wolpert and Macready, 1995) </ref>, showing that no learning strategy is, on average, better than random guessing. This result is not so much a drawback as it is a plea for insisting on the assumptions that make a given algorithm efficient. <p> It also shares some similarity with the jackknife sta tistical procedure, but differs in the purpose (see Efron, 1982). 3.5 A more precise study of the computational cost is given in section 3.12. The "no free lunch" theorem is due to <ref> (Wolpert and Macready, 1995) </ref>. This result c flC. Goutte 1996 54 Hyper-parameters has sparked heated debates in the computational learning community.
Reference: <author> Zhu, H. and Rohwer, R. </author> <year> (1996). </year> <title> No free lunch for cross validation. </title> <journal> Neural Computation, </journal> <volume> 8(7) </volume> <pages> 1421-1426. </pages> <address> c flC. </address> <month> Goutte </month> <year> 1996 </year>
References-found: 21

