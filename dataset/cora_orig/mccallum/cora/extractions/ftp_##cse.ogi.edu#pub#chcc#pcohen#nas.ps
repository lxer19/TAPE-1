URL: ftp://cse.ogi.edu/pub/chcc/pcohen/nas.ps
Refering-URL: http://www.cse.ogi.edu/~pcohen/online.html
Root-URL: http://www.cse.ogi.edu
Title: The Role of Voice Input for Human-Machine Communication  
Author: Philip R. Cohen and Sharon L. Oviatt 
Affiliation: Department of Computer Science and Engineering Oregon Graduate Institute of Science and Technology  
Abstract: 1 Abstract 
Abstract-found: 1
Intro-found: 1
Reference: <institution> Advanced Research Projects Agency. ARPA Spoken Language Systems Technology Workshop, Cambridge, Massachusetts, 1993. Massachusetts Institute of Technology. </institution>
Reference-contexts: The ARPA-supported air-travel information systems <ref> (Advanced Research Projects Agency, 1993) </ref>, developed at Bolt, Beranek and Newman (Kubala et al., 1992), Carnegie-Mellon University (Huang et al., 1993), Massachusetts Institute of Technology (Zue et al., 1992), SRI International (Appelt and Jackson, 1992), and other institutions, allow novice users to obtain information in real-time from the Official Airline Guide <p> The system should have sufficient vocabulary, as well as linguistic, semantic, and dialogue capabilities, to support interactive problem solving by infrequent users. For example, at its present state of development, many users can successfully solve travel-planning problems with one of the ATIS systems <ref> (Advanced Research Projects Agency, 1993) </ref> within a few minutes of introduction to the system and its coverage. <p> At the same time, for service-oriented tasks, research has shown that users sometimes prefer structured spoken interaction over unconstrained ones by as much as a factor of 2-to-1 (Oviatt et al., in press). 10 5.3 Interaction and Dialogue Present spoken language systems have supported question-answer dialogues <ref> (Advanced Research Projects Agency, 1993) </ref>, or dialogues in which the user is prompted for information (Andry, 1992; Peckham, 1991). To support a broader range of dialogue behavior, more general models of dialogue are being investigated, both mathematically and computationally. These include both dialogue grammars and plan-based models of dialogue.
Reference: <author> F. Andry. </author> <title> Static and dynamic predictions: A method to improve speech understanding in cooperative dialogues. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <address> Banff, Alberta, Canada, </address> <month> October </month> <year> 1992. </year> <institution> University of Alberta. </institution>
Reference: <author> D. E. Appelt and E. Jackson. </author> <title> SRI International February 1992 ATIS benchmark test results. </title> <booktitle> In Fifth DARPA Workshop on Speech and Natural Language, </booktitle> <address> San Mateo, Calif., </address> <year> 1992. </year> <title> Defense Advanced Research Projects Agency, </title> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: The ARPA-supported air-travel information systems (Advanced Research Projects Agency, 1993), developed at Bolt, Beranek and Newman (Kubala et al., 1992), Carnegie-Mellon University (Huang et al., 1993), Massachusetts Institute of Technology (Zue et al., 1992), SRI International <ref> (Appelt and Jackson, 1992) </ref>, and other institutions, allow novice users to obtain information in real-time from the Official Airline Guide database, through speaker-independent, continuously spoken English questions.
Reference: <author> J. M. Baker. </author> <title> Large vocabulary speaker-adaptive continuous speech recognition research overview at Dragon systems. </title> <booktitle> In Proceedings of Eurospeech'91: 2nd European Conference on Speech Communication and Technology, </booktitle> <pages> pages 29-32, </pages> <address> Genova, Italy, </address> <year> 1991. </year>
Reference: <author> S. </author> <title> Basson. </title> <booktitle> Prompting the user in ASR applications. In Proceedings of COST232 Workshop | European Cooperation in Science and Technology, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: A number of studies have investigated methods for shaping user's language into the system's coverage. For telecommunications applications, the phrasing of system prompts for information spoken over the telephone dramatically influences the rate of caller compliance for expected words and phrases <ref> (Basson, 1992) </ref>. For systems with screen-based feedback, human spoken language can be effectively channeled through the use of a form that the user fills out with speech (Oviatt et al., in press).
Reference: <author> J. Bear, J. Dowding, and E. Shriberg. </author> <title> Detection and correction of repairs in human-computer dialog. </title> <editor> In D. Walker, editor, </editor> <booktitle> Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <address> Newark, Delaware, </address> <month> June </month> <year> 1992. </year>
Reference: <author> J. Bernstein. </author> <title> Applications of speech recognition technology in rehabilitation. </title> <editor> In J. E. Harkins and B. M. Vir-van, editors, </editor> <title> Speech to text: Today and tomorrow, </title> <address> Washington, D. C., </address> <year> 1988. </year> <journal> Gallaudet University Research Institute. GRI Monograph Series B., </journal> <volume> No. </volume> <pages> 2. </pages>
Reference-contexts: applications of speech recognition can be found in Karis and Dobroth (1991). 3 devices will offer the user the capability to speak to them as well. 3.1.2 Disability A major potential use of voice technology will be to assist deaf users in communicating with the hearing world using a telephone <ref> (Bernstein, 1988) </ref>. Speech recognition could also be used by motorically impaired users to control suitably augmented household appliances, wheel chairs, and robotic prostheses.
Reference: <author> J. Bernstein, M. Cohen, H. Murveit, D. Rtischev, and M. Weintraub. </author> <title> Automatic evaluation and training in English pronunciation. </title> <booktitle> In Proceedings of the 1990 International Conference on Spoken Language Processing, </booktitle> <pages> pages 1185-1188, </pages> <address> Kobe, Japan, </address> <year> 1990. </year> <journal> The Acoustical Society of Japan. </journal>
Reference: <author> A. Chapanis, R. B. Ochsman, R. N. Parrish, and G. D. Weeks. </author> <title> Studies in interactive communication: I. The effects of four communication modes on the behavior of teams during cooperative problem solving. </title> <booktitle> Human Factors, </booktitle> <volume> 14 </volume> <pages> 487-509, </pages> <year> 1972. </year>
Reference: <author> A. Chapanis, R. N. Parrish, R. B. Ochsman, and G. D. Weeks. </author> <title> Studies in interactive communication: II. The effects of four communication modes on the linguistic performance of teams during cooperative problem solving. </title> <booktitle> Human Factors, </booktitle> <volume> 19(2) </volume> <pages> 101-125, </pages> <month> April </month> <year> 1977. </year>
Reference: <author> H. H. Clark and D. Wilkes-Gibbs. </author> <title> Referring as a collaborative process. </title> <journal> Cognition, </journal> <volume> 22 </volume> <pages> 1-39, </pages> <year> 1986. </year>
Reference: <author> P. R. Cohen. </author> <title> The pragmatics of referring and the modality of communication. </title> <journal> Computational Linguistics, </journal> <volume> 10(2) </volume> <pages> 97-146, </pages> <note> April-June 1984. 13 P. </note> <author> R. Cohen. </author> <title> The role of natural language in a multimodal interface. </title> <booktitle> In Proceedings of UIST'92, </booktitle> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1992, </year> <pages> 143-149. </pages>
Reference: <author> P. R. Cohen. </author> <title> Models of dialogue. </title> <editor> In M. Nagao, editor, </editor> <title> Cognitive Processing for Vision and Voice: </title> <booktitle> Proceedings of the Fourth NEC Research Symposium. </booktitle> <publisher> SIAM, </publisher> <year> 1994. </year>
Reference: <author> P. R. Cohen and H. J. Levesque. </author> <booktitle> Confirmations and joint action. In Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 951-957, </pages> <address> Sydney, Australia, August 1991. </address> <publisher> Morgan Kauf-mann Publishers, Inc. </publisher>
Reference: <author> P. R. Cohen and C. R. Perrault. </author> <title> Elements of a plan-based theory of speech acts. </title> <journal> Cognitive Science, </journal> <volume> 3(3) </volume> <pages> 177-212, </pages> <year> 1979. </year>
Reference: <author> R. Cole, L. Hirschman, L. Atlas, M. Beckman, A. Bierman, M. Bush, J. Cohen, O. Garcia, B. Hanson, H. Hermansky, S. Levinson, K. McKeown, N. Morgan, D. Novick, M. Ostendorf, S. Oviatt, P. Price, H. Sil-verman, J. Spitz, A. Waibel, C. Weinstein, S. Zahorain, and V. Zue. </author> <title> NSF workshop on spoken language understanding. </title> <type> Technical Report CS/E 92-014, </type> <institution> Oregon Graduate Institute, </institution> <month> September </month> <year> 1992. </year> <title> Committee on Computerized Speech Recognition Technologies. Automatic Speech Recognition in Severe Environments. </title> <institution> Commission on Engineering and Technical Systems, National Research Council, National Academy of Sciences Press, </institution> <address> Washington, D. C., </address> <year> 1984. </year>
Reference-contexts: Even recognizing that an out-of-vocabulary word has occurred is itself a difficult issue <ref> (Cole et al., 1992) </ref>. If users can discern the system's vocabulary, one can be optimistic that they can adapt to that vocabulary.
Reference: <author> H. D. Crane. </author> <title> Writing and talking to computers. Business Intelligence Program Report D91-1557, </title> <booktitle> SRI International, </booktitle> <address> Menlo Park, California, </address> <month> July </month> <year> 1991. </year>
Reference: <author> N. Dahlback and A. Jonsson. </author> <title> An empirically based computationally tractable dialogue model. </title> <booktitle> In Proceedings of the 14th Annual Conference of the Cognitive Science Society (COGSCI-92), </booktitle> <address> Bloomington, Indiana, </address> <month> July </month> <year> 1992. </year>
Reference: <author> D. Englebart. </author> <title> Design considerations for knowledge workshop terminals. </title> <booktitle> In National Computer Conference, </booktitle> <pages> pages 221-227, </pages> <year> 1973. </year>
Reference: <author> W. K. English, D. C. Englebart, and M. A. Berman. </author> <title> Display-selection techniques for text manipulation. </title> <booktitle> IEEE Transactions on Human Factors in Electonics, </booktitle> <address> HFE-8(1):5-15, </address> <month> March </month> <year> 1967. </year>
Reference-contexts: This paradigm, which later was popularized by the Apple Macintosh and by Microsoft Windows, offers the user menus, icons, and pointing devices, such as the "mouse" ) <ref> (English et al., 1967) </ref>, as well as multiple windows in which to display output. With GUIs, users perform actions by selecting objects and then choosing the desired action from a menu, rather than by typing commands.
Reference: <author> O. N. Garcia, A. J. Goldschen, and E. D. Petajan. </author> <title> Feature extraction for optical speech recognition or automatic lipreading. </title> <type> Technical report, </type> <institution> Institute for Information Science and Technology, Department of Electrical Engineering and Computer Science, The George Washington University, </institution> <address> Washington, D.C., </address> <month> November </month> <year> 1992. </year>
Reference: <author> H. Giles, A. Mulac, J. J. Bradac, and P. Johnson. </author> <title> Speech accommodation theory: The first decade and beyond. </title> <editor> In M. L. McLaughlin, editor, </editor> <booktitle> Communication Yearbook 10, </booktitle> <pages> pages 13-48. </pages> <publisher> Sage Publishers, </publisher> <address> Beverly Hills, California, </address> <year> 1987. </year>
Reference: <author> B. Grosz and C. Sidner. </author> <title> Attention, Intentions, and the Structure of Discourse, </title> <booktitle> Computational Linguistics 12(36), </booktitle> <pages> pp. 175-204, </pages> <year> 1986. </year>
Reference: <author> B. Grosz and C. Sidner. </author> <title> Plans for discourse. </title> <editor> In P. R. Cohen, J. Morgan, and M. E. Pollack, editors, </editor> <booktitle> Intentions in Communication, </booktitle> <pages> pages 417-444. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference: <author> A. G. Hauptmann and P. McAvinney. </author> <title> Gestures with speech for direct manipulation. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 38 </volume> <pages> 231-249, </pages> <year> 1993. </year>
Reference: <author> A. G. Hauptmann and A. I. Rudnicky. </author> <title> A comparison of speech and typed input. </title> <booktitle> In Proceedings of the Speech and Natural Language Workshop, </booktitle> <pages> pages 219-224, </pages> <address> San Mateo, California, June 1990. </address> <publisher> Morgan Kaufmann, Publishers, Inc. </publisher>
Reference-contexts: Moreover, results showing the elimination of benefits once error correction is considered have been found in tasks as simple as entry of connected digits <ref> (Hauptmann and Rudnicky, 1990) </ref>.
Reference: <author> D. Hindle. </author> <title> Deterministic parsing of syntactic non-fluencies. </title> <booktitle> In Proceedings of the 21st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 123-128, </pages> <address> Cambridge, Mass., </address> <month> June </month> <year> 1983. </year>
Reference: <author> X. Huang, F. Alleva, M.-Y. Hwang, and R. Rosenfeld. </author> <title> An overview of the SPHINX-II speech recognition system. </title> <booktitle> In Proc. of the ARPA Workshop on Human Language Technology, </booktitle> <address> San Mateo, Calif., </address> <year> 1993. </year> <title> Advanced Research Projects Agency, </title> <publisher> Morgan Kaufmann, Publishers, Inc. </publisher>
Reference-contexts: The ARPA-supported air-travel information systems (Advanced Research Projects Agency, 1993), developed at Bolt, Beranek and Newman (Kubala et al., 1992), Carnegie-Mellon University <ref> (Huang et al., 1993) </ref>, Massachusetts Institute of Technology (Zue et al., 1992), SRI International (Appelt and Jackson, 1992), and other institutions, allow novice users to obtain information in real-time from the Official Airline Guide database, through speaker-independent, continuously spoken English questions.
Reference: <author> E. L. Hutchins, J. D. Hollan, and D. A. Norman. </author> <title> Direct manipulation interfaces. </title> <editor> In D. A. Norman and S. W. Draper, editors, </editor> <booktitle> User Centered System Design, </booktitle> <pages> pages 87-124. </pages> <publisher> Lawrence Erlbaum Publisher, </publisher> <address> Hillsdale, New Jersey, </address> <year> 1986. </year>
Reference: <author> D. Karis and K. M. Dobroth. </author> <title> Automating servies with speech recognition over the public switched telephone network: Human factors considerations. </title> <journal> IEEE Journal of Selected Areas in Communications, </journal> <volume> 9(4) </volume> <pages> 574-585, </pages> <month> 14 </month> <year> 1991. </year>
Reference: <author> A. Kay and A. Goldberg. </author> <title> Personal dynamic media. </title> <journal> IEEE Computer, </journal> <volume> 10(1) </volume> <pages> 31-42, </pages> <year> 1977. </year>
Reference: <author> M. J. Kelly and A. Chapanis. </author> <title> Limited vocabulary natural language dialogue. </title> <journal> International Journal of Man-machine Studies, </journal> <volume> 9 </volume> <pages> 479-501, </pages> <year> 1977. </year>
Reference: <author> R. M. Krauss and P. D. Bricker. </author> <title> Effects of transmission delay and access delay on the efficiency of verbal communication. </title> <journal> The Journal of the Acoustical Society of America, </journal> <volume> 41(2) </volume> <pages> 286-292, </pages> <year> 1967. </year>
Reference: <author> F. Kubala, C. Barry, M. Bates, R. Bobrow, P. Fung, R. Ingria, J. Makhoul, L. Nguyen, R. Schwartz, and D. Stallard. </author> <title> BBN BYBLOS and HARC February 1992 ATIS benchmark results. </title> <booktitle> In Fifth DARPA Workshop on Speech and Natural Language, </booktitle> <address> San Mateo, Calif., </address> <year> 1992. </year> <title> Defense Advanced Research Projects Agency, </title> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: The ARPA-supported air-travel information systems (Advanced Research Projects Agency, 1993), developed at Bolt, Beranek and Newman <ref> (Kubala et al., 1992) </ref>, Carnegie-Mellon University (Huang et al., 1993), Massachusetts Institute of Technology (Zue et al., 1992), SRI International (Appelt and Jackson, 1992), and other institutions, allow novice users to obtain information in real-time from the Official Airline Guide database, through speaker-independent, continuously spoken English questions.
Reference: <author> A. Kurematsu. </author> <title> Future perspective of automatic telephone interpretation. </title> <journal> Transactions of IEICE, </journal> <volume> E75(1):14-19, </volume> <month> January </month> <year> 1992. </year>
Reference: <author> W. </author> <title> A Lea. Practical lessons from configuring voice I/O systems. </title> <booktitle> In Proceedings of Speech Tech/Voice Systems Worldwide, </booktitle> <address> New York, </address> <year> 1992. </year> <title> Media Dimensions, </title> <publisher> Inc. </publisher>
Reference-contexts: For instance, wire installers, who spoke a wire's serial number and then were guided verbally by the computer to install that wire achieved a 20-30% speedup in productivity, with improved accuracy and lower training time, over their prior manual method of wire identification and installation <ref> (Marshall, 1992) </ref>. Although individual field studies are rarely conclusive, many field studies of highly accurate speech recognition systems with hands/eyes-busy tasks have found that spoken input leads to higher task productivity and accuracy. <p> Moreover, results showing the elimination of benefits once error correction is considered have been found in tasks as simple as entry of connected digits (Hauptmann and Rudnicky, 1990). However, it is claimed that most failures of speech technology have been the result of poor human factors engineering and management <ref> (Lea, 1992) </ref>, rather than low recognition accuracy per se. 3.1.1 Limited Keyboard/Screen Option The most prevalent current uses of speech recognition are telephone-based applications that replace or augment operator services (e.g., collect calls), handling hundreds of millions of callers each year and resulting in multi-million dollar savings (Lennig, 1989; Nakatsu, in <p> The ARPA-supported air-travel information systems (Advanced Research Projects Agency, 1993), developed at Bolt, Beranek and Newman <ref> (Kubala et al., 1992) </ref>, Carnegie-Mellon University (Huang et al., 1993), Massachusetts Institute of Technology (Zue et al., 1992), SRI International (Appelt and Jackson, 1992), and other institutions, allow novice users to obtain information in real-time from the Official Airline Guide database, through speaker-independent, continuously spoken English questions. <p> The ARPA-supported air-travel information systems (Advanced Research Projects Agency, 1993), developed at Bolt, Beranek and Newman (Kubala et al., 1992), Carnegie-Mellon University (Huang et al., 1993), Massachusetts Institute of Technology <ref> (Zue et al., 1992) </ref>, SRI International (Appelt and Jackson, 1992), and other institutions, allow novice users to obtain information in real-time from the Official Airline Guide database, through speaker-independent, continuously spoken English questions. <p> The ARPA-supported air-travel information systems (Advanced Research Projects Agency, 1993), developed at Bolt, Beranek and Newman (Kubala et al., 1992), Carnegie-Mellon University (Huang et al., 1993), Massachusetts Institute of Technology (Zue et al., 1992), SRI International <ref> (Appelt and Jackson, 1992) </ref>, and other institutions, allow novice users to obtain information in real-time from the Official Airline Guide database, through speaker-independent, continuously spoken English questions. <p> Other major efforts to develop spoken dialogue systems also are ongoing in Europe (Mariani, 1992; Peckham, 1991), and Japan <ref> (Yato et al., 1992) </ref>. 4.2 Comparison of Language-based Communication Modalities In a series of studies of interactive human-human communication, Chapanis and colleagues ) compared the efficiency of human-human communication when subjects used any of 10 communication modalities, including face-to-face, voice-only, linked teletypes, interactive handwriting (Chapanis et al., 1972; Chapanis et al., <p> Thus, 4 The ATIS effort has required the collection and annotation of over 10,000 user utterances, some of which is used for system development, and the rest for testing during comparative evaluations conducted by the National Institute of Standards and Technology. 8 such systems can be error-prone and, some claim <ref> (Shneiderman, 1992) </ref>, lead to frustration and disillusionment. Many natural language sentences are ambiguous, and parsers often find more ambiguities than people do. Hence, a natural language system often engages in some form of clarification or confirmation subdialogue to determine if its interpretation is the intended one. <p> Even recognizing that an out-of-vocabulary word has occurred is itself a difficult issue <ref> (Cole et al., 1992) </ref>. If users can discern the system's vocabulary, one can be optimistic that they can adapt to that vocabulary. <p> A number of studies have investigated methods for shaping user's language into the system's coverage. For telecommunications applications, the phrasing of system prompts for information spoken over the telephone dramatically influences the rate of caller compliance for expected words and phrases <ref> (Basson, 1992) </ref>. For systems with screen-based feedback, human spoken language can be effectively channeled through the use of a form that the user fills out with speech (Oviatt et al., in press). <p> Among the many advantages of multimodal systems are the following: Enhanced Error Avoidance and Correction. Multimodal interfaces offer the opportunity for users to avoid errors that would otherwise occur in a unimodal interface <ref> (Oviatt, 1992) </ref>. For example, users can select to write rather than speak a difficult-to-pronounce foreign surname. Furthermore, 11 when repeat "spiral" errors are encountered during spoken input, an alternate mode enable short--cutting them (Oviatt, 1992; Rhyne and Wolf, 1994). <p> Likewise, individual and task differences can strongly influence people's willingness to use one input mode over another. Multimodal systems thus have the potential to accomodate a wider array of different users, tasks, and situations than unimodal ones <ref> (Oviatt, 1992) </ref>. User Preference. Users may strongly prefer multimodal interaction.
Reference: <author> M. Lennig. </author> <title> Using speech recognition in the telephone network to automate collect and third-number-billed calls. </title> <booktitle> In Proceedings of Speech Tech'89, </booktitle> <pages> pages 124-125, </pages> <address> Arlington, Virginia, </address> <year> 1989. </year> <title> Media Dimensions. </title>
Reference: <author> S. Levinson. </author> <title> Some pre-observations on the modelling of dialogue. </title> <booktitle> Discourse Processes, </booktitle> <volume> 4(1), </volume> <year> 1981. </year>
Reference: <author> D. J. Litman and J. F. Allen. </author> <title> A plan recognition model for subdialogues in conversation. </title> <journal> Cognitive Science, </journal> <volume> 11 </volume> <pages> 163-200, </pages> <year> 1987. </year>
Reference: <author> J. Mariani. </author> <title> Spoken language processing in the framework of human-machine communication at LIMSI. </title> <booktitle> In Proceedings of Speech and Natural Language Workshop, </booktitle> <pages> pages 55-60, </pages> <address> San Mateo, California, </address> <year> 1992. </year> <title> Defense Advanced Research Projects Agency, </title> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> R. E. Markinson. </author> <title> Associate clinical professor of surgery, </title> <institution> University of California at San Francisco. </institution> <type> Personal communication, </type> <month> June </month> <year> 1993. </year>
Reference-contexts: However, speech recognizers may themselves lead to different repetitive stress injuries <ref> (Markinson, personal communication, 1993) </ref>. 3.1.3 Subject Matter is Pronunciation Speech recognition will become a component of future computer-based aids for foreign language learning and for the teaching of reading (Bernstein et al., 1990; Mostow, 1993).
Reference: <author> J. P. Marshall. </author> <title> A manufacturing application of voice recognition for assembly of aircraft wire harnesses. </title> <booktitle> In Proceedings of Speech Tech/Voice Systems Worldwide, </booktitle> <address> New York, </address> <year> 1992. </year> <title> Media Dimensions, </title> <publisher> Inc. </publisher>
Reference-contexts: For instance, wire installers, who spoke a wire's serial number and then were guided verbally by the computer to install that wire achieved a 20-30% speedup in productivity, with improved accuracy and lower training time, over their prior manual method of wire identification and installation <ref> (Marshall, 1992) </ref>. Although individual field studies are rarely conclusive, many field studies of highly accurate speech recognition systems with hands/eyes-busy tasks have found that spoken input leads to higher task productivity and accuracy.
Reference: <author> G. L. Martin. </author> <title> The utility of speech input in user-computer interfaces. </title> <journal> International Journal of Man-machine Studies, </journal> <volume> 30(4) </volume> <pages> 355-375, </pages> <year> 1989. </year>
Reference-contexts: Early laboratory studies of a direct-manipulation VLSI design system augmented with speaker-dependent speech recognition indicate that users are as fast at speaking single-word commands as they are at invoking the same comands with mouse-button clicks, or by typing a single letter command abbreviation <ref> (Martin, 1989) </ref>. Furthermore, circuit designers were able to complete 24% more tasks when spoken commands were available than when they used only a keyboard and mouse interface.
Reference: <author> T. B. Martin. </author> <title> Practical applications of voice input to machines. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 64(4) </volume> <pages> 487-501, </pages> <month> April </month> <year> 1976. </year>
Reference-contexts: Other hands/eyes busy applications that have benefited from voice interaction include data entry and machine control in factories and field applications <ref> (Martin, 1976) </ref>, access to information for military command-and-control, cockpit management (Simpson et al., 1985; Weinstein, in this volume), astronauts' information management during extra-vehicular access in space, dictation of medical diagnoses, maintenance and repair of equipment, control of automobile equipment (e.g., radios, telephones, climate control), and navigational aids. 2 To attain a
Reference: <author> P. R. Michaelis, A. Chapanis, G. D. Weeks, and M. J. Kelly. </author> <title> Word usage in interactive dialog with restricted and unrestricted vocabularies. </title> <journal> IEEE Transactions on Professional Communication, </journal> <volume> PC-20(4), </volume> <month> December </month> <year> 1977. </year>
Reference: <author> J. Mostow, A. G. Hauptmann, L. L. Chase, and S. Roth. </author> <title> Towards a reading coach that listens: Automated detection of oral reading errors. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence (AAAI93), </booktitle> <address> Menlo Park, California, </address> <year> 1993. </year> <booktitle> American Association for Artificial Intelligence, </booktitle> <publisher> AAAI Press/The MIT Press. </publisher>
Reference: <author> I. R. Murray, J. L. Arnott, A. F. Newell, G. Cruickshank, K. E. P. Carter, and R. Dye. </author> <title> Experiments with a full-speed speech-driven word processor. </title> <type> Technical Report CS 91/09, </type> <institution> Mathematics and Computer Science Department, University of Dundee, </institution> <address> Dundee, Scotland, </address> <month> April </month> <year> 1991. </year>
Reference: <author> C. Nakatani and J. Hirschberg. </author> <title> A speech-first model for repair detection and correction. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 46-53, </pages> <address> Columbus, Ohio, </address> <month> June </month> <year> 1993. </year>
Reference: <author> A. F. Newell, J. L. Arnott, K. Carter, and G. Cruickshank. </author> <title> Listening typewriter simulation studies. </title> <journal> International Journal of Man-machine Studies, </journal> <volume> 33(1) </volume> <pages> 1-19, </pages> <year> 1990. </year>
Reference: <author> R. B. Ochsman and A. Chapanis. </author> <title> The effects of 10 communication modes on the behaviour of teams during co-operative problem-solving. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 6(5) </volume> <pages> 579-620, </pages> <month> Sept. </month> <year> 1974. </year>
Reference: <author> S. L. Oviatt. Pen/voice: </author> <title> Complementary multimodal communication. </title> <booktitle> In Proceedings of Speech Tech'92, </booktitle> <pages> pages 238-241, </pages> <address> New York, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: Among the many advantages of multimodal systems are the following: Enhanced Error Avoidance and Correction. Multimodal interfaces offer the opportunity for users to avoid errors that would otherwise occur in a unimodal interface <ref> (Oviatt, 1992) </ref>. For example, users can select to write rather than speak a difficult-to-pronounce foreign surname. Furthermore, 11 when repeat "spiral" errors are encountered during spoken input, an alternate mode enable short--cutting them (Oviatt, 1992; Rhyne and Wolf, 1994). <p> Likewise, individual and task differences can strongly influence people's willingness to use one input mode over another. Multimodal systems thus have the potential to accomodate a wider array of different users, tasks, and situations than unimodal ones <ref> (Oviatt, 1992) </ref>. User Preference. Users may strongly prefer multimodal interaction.
Reference: <author> S. L. Oviatt. </author> <title> Predicting spoken disfluencies during human-computer interaction. Computer Speech and Language, </title> <note> in press. </note> <author> 15 S. L. Oviatt. </author> <title> Toward multimodal support for interpreted telephone dialogues. </title> <editor> In M. M. Taylor, F. Neel, and D. G. Bouwhuis, editors, </editor> <title> Structure of Multimodal Dialogue. </title> <publisher> Elsevier Science Publishers B. V., </publisher> <address> Amsterdam, Netherlands, </address> <publisher> in press. </publisher>
Reference: <author> S. L. Oviatt and P. R. Cohen. </author> <title> The contributing influence of speech and interaction on human discourse patterns. </title> <editor> In J. W. Sullivan and S. W. Tyler, editors, </editor> <booktitle> Intelligent User Interfaces, chapter 3, </booktitle> <pages> pages 69-83. </pages> <publisher> ACM Press Frontier Series, Addison-Wesley Publishing Co., </publisher> <address> New York, New York, </address> <year> 1991. </year>
Reference: <author> S. L. Oviatt, P. R. Cohen, and M. Q. Wang. </author> <title> Toward interface design for human language technology: Modality and structure as determinants of linguistic complexity. Speech Communication, </title> <type> 15(3-4), </type> <note> in press. </note>
Reference: <author> S. L. Oviatt and E. Olsen. </author> <title> Integration themes in multimodal human-computer interaction, </title> <booktitle> Proceedings of the International Conference on Spoken Language Processing, </booktitle> <editor> Shirai, K., Furui, S. and Kakehi, K. (eds.), </editor> <booktitle> Acoustical Society of Japan, 1994, </booktitle> <volume> vol. 2, </volume> <pages> 551-554. </pages>
Reference-contexts: User Preference. Users may strongly prefer multimodal interaction. In a recent comparison of spoken, written, and combined pen/voice input, it was found that 56-89% of users preferred interacting multimodally, which was perceived to be easier and more flexible <ref> (Oviatt and Olsen, 1994) </ref>. 6 Scientific Research on Spoken and Multimodal Interaction with Computers The present research and development climate for speech-based technology is more active than it was at the time of the 1984 National Research Council report on speech recognition in severe environments (National Research Council, 1984).
Reference: <author> J. Peckham. </author> <title> Speech understanding and dialogue over the telephone: An overview of the ESPRIT SUNDIAL project. </title> <booktitle> In Proc. of the Speech and Natural Language Workshop, </booktitle> <pages> pages 14-28, </pages> <address> San Mateo, California, </address> <month> February </month> <year> 1991. </year> <title> DARPA/ISTO, </title> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> C. R. Perrault and J. F. Allen. </author> <title> A plan-based analysis of indirect speech acts. </title> <journal> American Journal of Computational Linguistics, </journal> <volume> 6(3) </volume> <pages> 167-182, </pages> <year> 1980. </year>
Reference: <author> E. Petajan, B. Bradford, D. Bodoff, and N. M. Brooke. </author> <title> An improved automatic lipreading system to enhance speech recognition. </title> <booktitle> In Proc. of Human Factors in Computing Systems (CHI'88), </booktitle> <pages> pages 19-25, </pages> <address> New York, </address> <year> 1988. </year> <title> Association for Computing Machinery, </title> <publisher> ACM Press. </publisher>
Reference: <author> P. Price, M. Ostendorf, S. Shattuck-Hufnagel, and C. Fong. </author> <title> The use of prosody in syntactic disambiguation. </title> <booktitle> In Proceedings of the Speech and Natural Language Workshop, </booktitle> <pages> pages 372-377, </pages> <address> San Mateo, California, October 1991. </address> <publisher> Morgan Kaufmann, Publishers, Inc. </publisher>
Reference: <author> H. Rheingold. </author> <title> Virtual Reality. </title> <publisher> Summit Books, </publisher> <year> 1991. </year>
Reference-contexts: Apart from the mouse, numerous pointing devices exist, such as trackballs and joysticks, and some devices offer multiple capabilities, such as the use of pens for pointing, gesturing, and handwriting. Finally, users now can directly manipulate 3-D virtual worlds using computer-instrumented gloves and body-suits <ref> (Rheingold, 1991) </ref>, permitting body motion to affect the virtual environment. Strengths.
Reference: <author> J. A. Rhyne and C. Wolf. </author> <title> Recognition-based user interfaces. </title> <booktitle> In Advances in Human-Computer Interaction, </booktitle> <volume> volume 4, </volume> <pages> pages 191-250. </pages> <publisher> Ablex Publishing Co., Norwood, </publisher> <editor> N. J., </editor> <year> 1993. </year>
Reference: <author> D. B. Roe, F. Pereira, R. W. Sproat, and M. D. Riley. </author> <title> Toward a spoken language translator for restricted-domain context-free languages. </title> <booktitle> In Proceedings of Eurospeech'91: 2nd European Conference on Speech Communication and Technology, </booktitle> <pages> pages 1063-1066, </pages> <address> Genova, Italy, </address> <year> 1991. </year> <journal> European Speech Communication Association. </journal>
Reference: <author> A. I. Rudnicky. </author> <title> Mode preference in a simple data-retrieval task. </title> <booktitle> In ARPA Human Language Technology Workshop, </booktitle> <address> Princeton, New Jersey, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: Major questions remain as to the applications where voice will be favored when other communication modalities are options. Whereas some studies report a decided preference for speech in comparison with other modalities <ref> (Rudnicky, 1993) </ref>, other studies report the opposite conclusion (Murray et al., 1991; Newell et al., 1990). <p> In a recent study of human-computer interaction to retrieve information from a small database of 240 entries, it was found that speech was substantially preferred over direct-manipulation use of scrolling, even though the overall time to complete the task with voice was longer <ref> (Rudnicky, 1993) </ref>. This study suggests that, for simple risk-free tasks, user preference may be based on time-to-input rather than overall task completion times or task accuracy. 4.3.2 Natural Language Interaction Strengths. Natural language is the paradigmatic case of an expressive mode of communication.
Reference: <author> J. R. Searle. </author> <title> Speech acts: An essay in the philosophy of language. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1969. </year>
Reference-contexts: Some functional equivalent of planning is thus likely to be required. Plan-based models (Cohen and Perrault, 1979; Perrault and Allen, 1980) are founded on the observation that utterances are not simply strings of words, but rather are the observable performance of communicative actions, or speech acts <ref> (Searle, 1969) </ref>, such as requesting, informing, warning, suggesting, and confirming. These models propose that the listener's job is to uncover and respond appropriately to the speaker's underlying plan, rather than just to the utterance.
Reference: <author> B. Shneiderman. </author> <title> Direct manipulation: A step beyond programming languages. </title> <journal> IEEE Computer, </journal> <volume> 16(8) </volume> <pages> 57-69, </pages> <year> 1983. </year>
Reference: <author> B. Shneiderman. </author> <title> Designing the User Interface: Strategies for Effective Human-Computer Interaction, </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: Thus, 4 The ATIS effort has required the collection and annotation of over 10,000 user utterances, some of which is used for system development, and the rest for testing during comparative evaluations conducted by the National Institute of Standards and Technology. 8 such systems can be error-prone and, some claim <ref> (Shneiderman, 1992) </ref>, lead to frustration and disillusionment. Many natural language sentences are ambiguous, and parsers often find more ambiguities than people do. Hence, a natural language system often engages in some form of clarification or confirmation subdialogue to determine if its interpretation is the intended one.
Reference: <author> E. Shriberg, E. Wade, and P. Price. </author> <title> Human-machine problem-solving using spoken language systems (SLS): Factors affecting performance and user satisfaction. </title> <booktitle> In Proceedings of Speech and Natural Language Workshop, </booktitle> <pages> pages 49-54, </pages> <address> San Mateo, California, </address> <year> 1992. </year> <title> Defense Advanced Research Projects Agency, </title> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference: <author> C. A. Simpson, M. E. McCauley, E. F. Roland, J. C. Ruth, and B. H. Williges. </author> <title> System design for speech recognition and generation. </title> <booktitle> Human Factors, </booktitle> <volume> 27(2) </volume> <pages> 115-141, </pages> <year> 1985. </year>
Reference: <author> D. Small and L. Weldon. </author> <title> An experimental comparison of natural and structured query languages. </title> <booktitle> Human Factors, </booktitle> <volume> 25 </volume> <pages> 253-263, </pages> <year> 1983. </year>
Reference: <author> R. L. Jr. Street, R. M. Brady, and W. B. Putman. </author> <title> The influence of speech rate stereotypes and rate similarity on listeners' evaluations of speakers. </title> <journal> Journal of Language and Social Psychology, </journal> <volume> 2(1) </volume> <pages> 37-56, </pages> <year> 1983. </year>

Reference: <author> T. Winograd and F. Flores. </author> <title> Understanding Computers and Cognition: A New Foundation for Design. </title> <publisher> Ablex Publishing Co., </publisher> <address> Norwood, New Jersey, </address> <year> 1986. </year>
Reference: <author> F. Yato, T. Takezawa, S. Sagayama, J. Takami, H. Singer, N. Uratani, T. Morimoto, and A. Kurematsu. </author> <title> International joint experiment toward interpreting telephony (in Japanese). </title> <type> Technical report, </type> <institution> The Institute of Electronics, Information, and Communication Engineers, </institution> <year> 1992. </year>
Reference-contexts: Other major efforts to develop spoken dialogue systems also are ongoing in Europe (Mariani, 1992; Peckham, 1991), and Japan <ref> (Yato et al., 1992) </ref>. 4.2 Comparison of Language-based Communication Modalities In a series of studies of interactive human-human communication, Chapanis and colleagues ) compared the efficiency of human-human communication when subjects used any of 10 communication modalities, including face-to-face, voice-only, linked teletypes, interactive handwriting (Chapanis et al., 1972; Chapanis et al.,
Reference: <author> S. R. Young, A. G. Hauptmann, W. H. Ward, E. T. Smith, and P. Werner. </author> <title> High level knowledge sources in usable speech recognition systems. </title> <journal> Communications of the ACM, </journal> <volume> 32(2), </volume> <month> February </month> <year> 1989. </year>
Reference: <author> E. Zoltan-Ford. </author> <title> How to get people to say and type what computers can understand. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 34 </volume> <pages> 527-547, </pages> <year> 1991. </year>
Reference: <author> V. Zue, J. Glass, D. Goddeau, D. Goodine, L. Hirschman, M. Phillips, J. Polifroni, and S. Seneff. </author> <title> The MIT ATIS system: February 1992 progress report. </title> <booktitle> In Fifth DARPA Workshop on Speech and Natural Language, </booktitle> <address> San Mateo, Calif., </address> <year> 1992. </year> <title> Defense Advanced Research Projects Agency, </title> <publisher> Morgan Kaufmann Publishers, Inc. </publisher> <pages> 17 </pages>
Reference-contexts: The ARPA-supported air-travel information systems (Advanced Research Projects Agency, 1993), developed at Bolt, Beranek and Newman (Kubala et al., 1992), Carnegie-Mellon University (Huang et al., 1993), Massachusetts Institute of Technology <ref> (Zue et al., 1992) </ref>, SRI International (Appelt and Jackson, 1992), and other institutions, allow novice users to obtain information in real-time from the Official Airline Guide database, through speaker-independent, continuously spoken English questions.
References-found: 75

