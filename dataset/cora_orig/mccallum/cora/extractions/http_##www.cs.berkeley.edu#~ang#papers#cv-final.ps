URL: http://www.cs.berkeley.edu/~ang/papers/cv-final.ps
Refering-URL: http://www.cs.berkeley.edu/~ang/
Root-URL: http://www.cs.berkeley.edu/~ang/
Email: Andrew.Ng@cs.cmu.edu  
Title: Preventing "Overfitting" of Cross-Validation Data  
Author: Andrew Y. Ng 
Address: Pittsburgh PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Suppose that, for a learning task, we have to select one hypothesis out of a set of hypotheses (that may, for example, have been generated by multiple applications of a randomized learning algorithm). A common approach is to evaluate each hypothesis in the set on some previously unseen cross-validation data, and then to select the hypothesis that had the lowest cross-validation error. But when the cross-validation data is partially corrupted such as by noise, and if the set of hypotheses we are selecting from is large, then "folklore" also warns about "overfitting" the cross-validation data [Klockars and Sax, 1986, Tukey, 1949, Tukey, 1953]. In this paper, we explain how this "overfitting" really occurs, and show the surprising result that it can be overcome by selecting a hypothesis with a higher cross-validation error, over others with lower cross-validation errors. We give reasons for not selecting the hypothesis with the lowest cross-validation error, and propose a new algorithm, LOOCVCV, that uses a computa-tionally efficient form of leave-one-out cross-validation to select such a hypothesis. Finally, we present experimental results for one domain, that show LOOCVCV consistently beating picking the hypothesis with the lowest cross-validation error, even when using reasonably large cross-validation sets. 
Abstract-found: 1
Intro-found: 1
Reference: [Baum and Haussler, 1989] <author> Baum, E. and Haussler, D. </author> <year> (1989). </year> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1(1) </booktitle> <pages> 151-160. </pages>
Reference: [Efron, 1979] <author> Efron, B. </author> <year> (1979). </year> <title> Bootstrap methods: Another look at the jackknife. </title> <journal> Anns. Statist., </journal> <volume> 7 </volume> <pages> 1-26. </pages>
Reference-contexts: So instead, we will generate only a large but finite set H of hypotheses, and choose uniformly and with replacement from this set whenever we need to generate a hypothesis (this, from the Statistics literature, is very similar to non-parametric bootstrap <ref> [Efron, 1979] </ref>). However, even doing this, we found that we would still require averaging over an intractable or otherwise very large number of repetitions to get a smooth curve for the LOOCV-error as a function of ^n.
Reference: [Judd, 1990] <author> Judd, J. S. </author> <year> (1990). </year> <title> Neural Network Design and the Complexity of Learning. </title> <publisher> MIT Press. </publisher>
Reference: [Kearns, 1996] <author> Kearns, M. J. </author> <year> (1996). </year> <title> A bound on the error of Cross Validation using the approximation and estimation rates, with consequences for the training-test split. </title> <booktitle> In Advances in Neural Information Processing Systems 8, </booktitle> <pages> pages 183-189. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, within the set of hypotheses we are selecting from, there is no notion of a structure or a sequence of nested hypothesis classes of increasing complexity such as assumed in some models <ref> [Kearns, 1996, Vapnik and Chervonenkis, 1971] </ref>, or of some hypotheses having been trained using a more complex hypothesis class.
Reference: [Kearns et al., 1995] <author> Kearns, M. J., Mansour, Y., Ng, A. Y., and Ron, D. </author> <year> (1995). </year> <title> An experimental and theoretical comparison of model selection methods. </title> <booktitle> In Proceedings of the Eighth ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 21-30. </pages> <publisher> ACM Press. </publisher>
Reference: [Klockars and Sax, 1986] <author> Klockars, A. J. and Sax, G. </author> <year> (1986). </year> <title> Multiple Comparisons. </title> <publisher> Sage Publications. </publisher>
Reference-contexts: the CV data is partially corrupted by noise, then "folklore" also warns that if we used too small a set of CV data to test too large a number of hypotheses, we may end up picking a poor hypothesis that had fit the corrupted CV data well "just by chance" <ref> [Klockars and Sax, 1986, Tukey, 1949, Tukey, 1953] </ref>. In this paper, we examine this problem of "overfit-ting" of CV data.
Reference: [Leadbetter et al., 1980] <author> Leadbetter, M. R., Lindgren, G., and Rootzen, H. </author> <year> (1980). </year> <title> Extremes and Related Properties of Random Sequences and Processes. </title> <publisher> Springer Verlag. </publisher>
Reference-contexts: See <ref> [Leadbetter et al., 1980] </ref>, for example.) We therefore propose using the following value of k: k = 100 (1 n opt + 1 Using notation from the proof of Theorem 1, we can also think of picking the best-of-n hypothesis as a way of choosing ^" 0 (and then picking a
Reference: [Miller, 1990] <author> Miller, A. J. </author> <year> (1990). </year> <title> Subset Selection in Regression. </title> <publisher> Chapman and Hall. </publisher>
Reference: [Mosteller and Tukey, 1968] <author> Mosteller, F. and Tukey, J. W. </author> <year> (1968). </year> <title> Data analysis, including statistics. </title> <editor> In Lindzey, G. and Aronson, E., editors, </editor> <booktitle> Handbook of Social Psychology Vol. </booktitle> <volume> 2, </volume> <pages> pages 1-26. </pages> <publisher> Addison-Wesley. </publisher>
Reference-contexts: a set of training data, or equivalently, applying a deterministic algorithm multiple times but using randomly re-chosen parameters each time), a common approach is to test all of them on some set of previously unseen cross-validation (CV) data, and then to pick the hypothesis that had the smallest CV error <ref> [Mosteller and Tukey, 1968, Stone, 1974, Stone, 1977] </ref>.
Reference: [Quinlan, 1986] <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106. </pages>
Reference-contexts: k-th percentile kopt-th percentile percentile hypothesis, with k = 100 (1 (1=(n opt + 1))), bottom curve is k opt -th percentile pothesis was generated by drawing 20 samples from D with their correct labels, and running an ID3-like decision tree algorithm using a greedy information-gain heuristic to choose splits <ref> [Quinlan, 1986] </ref>. (Of course, no practitioner would ever see such a distribution of hypotheses; but, we are primarily interested in the problem of selecting a hypothesis out of a set, and this was a convenient way of creating such a set of hypotheses.) All experimental results reported in this section are
Reference: [Rissanen, 1978] <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471. </pages>
Reference: [Stone, 1974] <author> Stone, M. </author> <year> (1974). </year> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> J. Royal Statistical Society B, </journal> <volume> 36 </volume> <pages> 111-147. </pages>
Reference-contexts: a set of training data, or equivalently, applying a deterministic algorithm multiple times but using randomly re-chosen parameters each time), a common approach is to test all of them on some set of previously unseen cross-validation (CV) data, and then to pick the hypothesis that had the smallest CV error <ref> [Mosteller and Tukey, 1968, Stone, 1974, Stone, 1977] </ref>.
Reference: [Stone, 1977] <author> Stone, M. </author> <year> (1977). </year> <title> Asymtotics for and against cross-validation. </title> <journal> Biometrika, </journal> <volume> 64(1) </volume> <pages> 29-35. </pages>
Reference-contexts: a set of training data, or equivalently, applying a deterministic algorithm multiple times but using randomly re-chosen parameters each time), a common approach is to test all of them on some set of previously unseen cross-validation (CV) data, and then to pick the hypothesis that had the smallest CV error <ref> [Mosteller and Tukey, 1968, Stone, 1974, Stone, 1977] </ref>.
Reference: [Tukey, 1949] <author> Tukey, J. W. </author> <year> (1949). </year> <title> Comparing individual means in the analysis of variance. </title> <journal> Biometrics, </journal> <volume> 9 </volume> <pages> 99-114. </pages>
Reference-contexts: the CV data is partially corrupted by noise, then "folklore" also warns that if we used too small a set of CV data to test too large a number of hypotheses, we may end up picking a poor hypothesis that had fit the corrupted CV data well "just by chance" <ref> [Klockars and Sax, 1986, Tukey, 1949, Tukey, 1953] </ref>. In this paper, we examine this problem of "overfit-ting" of CV data.
Reference: [Tukey, 1953] <author> Tukey, J. W. </author> <year> (1953). </year> <title> The problem of multiple comparisons. </title> <type> Unpublished manuscript, </type> <institution> Princeton University. </institution>
Reference-contexts: the CV data is partially corrupted by noise, then "folklore" also warns that if we used too small a set of CV data to test too large a number of hypotheses, we may end up picking a poor hypothesis that had fit the corrupted CV data well "just by chance" <ref> [Klockars and Sax, 1986, Tukey, 1949, Tukey, 1953] </ref>. In this paper, we examine this problem of "overfit-ting" of CV data.
Reference: [Vapnik and Chervonenkis, 1971] <author> Vapnik, V. N. and Chervonenkis, A. Y. </author> <year> (1971). </year> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280. </pages>
Reference-contexts: For example, within the set of hypotheses we are selecting from, there is no notion of a structure or a sequence of nested hypothesis classes of increasing complexity such as assumed in some models <ref> [Kearns, 1996, Vapnik and Chervonenkis, 1971] </ref>, or of some hypotheses having been trained using a more complex hypothesis class.
References-found: 16

