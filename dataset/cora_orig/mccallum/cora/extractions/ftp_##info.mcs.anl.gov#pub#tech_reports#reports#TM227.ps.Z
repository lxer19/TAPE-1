URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/TM227.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts97.htm
Root-URL: http://www.mcs.anl.gov
Title: Computational Experience with a Dense Column Feature for Interior-Point Methods  
Author: by Marc Wenzel, Joseph Czyzyk, and Stephen Wright 
Note: This work was supported by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng-38. On Leave from the  
Address: 9700 South Cass Avenue Argonne, IL 60439  Am Hubland, 97074 Wurzburg, Germany  
Affiliation: ARGONNE NATIONAL LABORATORY  Mathematics and Computer Science Division  Institut fur angewandte Mathematik,  
Pubnum: Technical Memorandum No. 227  
Email: [mwenzel@mathematik.uni-wuerzburg.de]. fczyzyk,wrightg@mcs.anl.gov.  
Web: ANL/MCS-TM-227  
Date: August 1997  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Ilan Adler, Narenda Karmakar, Mauricio G. C. Resende, and Geraldo Veiga. </author> <title> An implementation of Karmakar's algorithm for linear programming. </title> <booktitle> Mathematical Programming, </booktitle> <pages> pages 297-335, </pages> <year> 1989. </year> <note> Errata in Mathematical Programming 50:415, </note> <year> 1991. </year>
Reference-contexts: They point out that an excellent preconditioner is needed to keep the number of pcg iterations at a reasonable level. This goal is important because each conjugate gradient iteration is about as expensive as a simplex step. Adler et al. <ref> [1] </ref> rely on a similar method, but report difficulties in generating a direction precise enough for computing an accurate primal solution at termination. They use an exact factorization of the full matrix in the last iteration to accomplish this.
Reference: [2] <author> Chan Choi, Clyde L. Monma, and David L. Shanno. </author> <title> Further development of a primal-dual interior point method. </title> <journal> ORSA Journal on Computing, </journal> <volume> 2(4) </volume> <pages> 304-311, </pages> <month> Fall </month> <year> 1990. </year>
Reference-contexts: Adler et al. [1] rely on a similar method, but report difficulties in generating a direction precise enough for computing an accurate primal solution at termination. They use an exact factorization of the full matrix in the last iteration to accomplish this. Choi, Monma, and Shanno <ref> [2] </ref> prefer the Sherman-Morrison-Woodbury update, yielding a direct method in place of the iterative conjugate gradient approach. They resort to iterative refinement when solving the equations during the last stages of the IPM to defeat the numerical instability incorporated in the SMW approach.
Reference: [3] <author> Joseph Czyzyk, Sanjay Mehrotra, and Stephen J. Wright. </author> <title> PCx User Guide. </title> <institution> Optimization Technology Center, Argonne National Laboratory and Northwestern University, Mathematics and Computer Science Division, Argonne National Laboratory 9700 South Cass Avenue, Argonne, </institution> <address> IL 60439, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: However, we need the ability to perform forward and backward solves with the ~ L factor independently, rather than having to perform both operations jointly. 4 Computational Experience Our implementation of the dense column handling strategy was based on the beta-2.0 release (October 1996) of PCx <ref> [3] </ref>, a primal-dual interior-point code that implements Mehrotra's [9] predictor-corrector 4 algorithm for linear programming. (The modifications were subsequently incorporated into release 1.0 of PCx, dated March 1997.) The sparse Cholesky routine is from the code of Ng and Peyton [10], release 0.4 (May 1995), which implements a multiple minimum degree
Reference: [4] <author> Robert Fourer and Sanjay Mehrotra. </author> <title> Solving symmetric indefinite systems in an interior-point method for linear programming. </title> <journal> Mathematical Programming, </journal> <volume> 62 </volume> <pages> 15-39, </pages> <year> 1993. </year>
Reference-contexts: By contrast, pivot ordering in the augmented system needs to take account of issues of numerical stability, so it typically needs to be recomputed a number of times during execution of the algorithm. Fourer and Mehrotra <ref> [4] </ref> use a sparse Bunch-Parlett solver but do not explicitly account for the special block structure of the augmented system (the presence of a zero block in the upper left and a diagonal matrix in the lower right), as would be necessary to implement a method with comparable efficiency to the
Reference: [5] <author> Philip E. Gill, Walter Murray, Michael A. Saunders, J. A. Tomlin, and Margaret H. Wright. </author> <title> On projected Newton methods for linear programming and equivalence to Karmarkar's projective method. </title> <journal> Mathematical Programming, </journal> <volume> 36 </volume> <pages> 183-209, </pages> <year> 1986. </year>
Reference-contexts: For the two alternative approaches|SMW and pcg|computational results have already been reported by other researchers. In Gill et al. <ref> [5] </ref>, the authors use a pure pcg with a sparse matrix as preconditioner. They point out that an excellent preconditioner is needed to keep the number of pcg iterations at a reasonable level. This goal is important because each conjugate gradient iteration is about as expensive as a simplex step.
Reference: [6] <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: Whenever this occurs, the infeasibility of the iterates increases dramatically, causing the interior-point method to break down. Hence, we use a pcg algorithm (see Golub and Van Loan <ref> [6, algorithm 10.3.1, p. 529] </ref>) to refine the solutions. The sparse part A s D 2 s A T s is used as a preconditioner; no additional work is needed to compute it since its Cholesky factorization is known already.
Reference: [7] <author> Irvin J. Lustig, Roy E. Marsten, and David F. Shanno. </author> <title> Computational experience with a primal-dual interior point method for linear programming. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 152 </volume> <pages> 191-222, </pages> <year> 1991. </year>
Reference-contexts: They resort to iterative refinement when solving the equations during the last stages of the IPM to defeat the numerical instability incorporated in the SMW approach. Lustig, Marsten, and Shanno <ref> [7] </ref> use Schur complements, but report problems of ill conditioning. They try to combat this by factoring A s D 2 s A T s + o I, where o = * max D 2 s and * is a small multiple of the machine precision.
Reference: [8] <author> Irvin J. Lustig, Roy E. Marsten, and David F. Shanno. </author> <title> On implementing Mehrotra's predictor-corrector interior point method for linear programming. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2(3) </volume> <pages> 435-449, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: In the final version of their code OB1, they used a default setting of OFF for the "no dense columns removed" option. Subsequently <ref> [8] </ref>, the authors introduced a switch option that uses Schur complements as the default technique, and switches to the more expensive pcg strategy whenever the spread of the diagonal elements of the factorization is larger than 10 14 . 3 Linear Algebra The SMW formula for updating the inverse of a <p> Even these digits may be lost in forming D 2 d + W T W , and so the solutions calculated by SMW are often inaccurate. Lustig, Marsten, and Shanno <ref> [8] </ref> state a example in three dimensions where A s D 2 s A T s approaches rank deficiency as the interior-point method approaches the optimum.
Reference: [9] <author> Sanjay Mehrotra. </author> <title> On the implementation of a primal-dual interior point method. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2(4) </volume> <pages> 575-601, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: ability to perform forward and backward solves with the ~ L factor independently, rather than having to perform both operations jointly. 4 Computational Experience Our implementation of the dense column handling strategy was based on the beta-2.0 release (October 1996) of PCx [3], a primal-dual interior-point code that implements Mehrotra's <ref> [9] </ref> predictor-corrector 4 algorithm for linear programming. (The modifications were subsequently incorporated into release 1.0 of PCx, dated March 1997.) The sparse Cholesky routine is from the code of Ng and Peyton [10], release 0.4 (May 1995), which implements a multiple minimum degree ordering strategy.
Reference: [10] <author> E. Ng and B. W. Peyton. </author> <title> Block sparse Cholesky algorithms on advanced uniprocessor computers. </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 14 </volume> <pages> 1034-1056, </pages> <year> 1993. </year>
Reference-contexts: was based on the beta-2.0 release (October 1996) of PCx [3], a primal-dual interior-point code that implements Mehrotra's [9] predictor-corrector 4 algorithm for linear programming. (The modifications were subsequently incorporated into release 1.0 of PCx, dated March 1997.) The sparse Cholesky routine is from the code of Ng and Peyton <ref> [10] </ref>, release 0.4 (May 1995), which implements a multiple minimum degree ordering strategy.
Reference: [11] <author> Robert J. Vanderbei. </author> <title> Splitting dense columns in sparse linear systems. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 152 </volume> <pages> 107-117, </pages> <year> 1991. </year>
Reference-contexts: Still, the average ratio of CPU time for augmented systems to CPU time for the normal equation approach is around 1:6 over all problems without dense columns in A. 2 The splitting technique is described by Vanderbei <ref> [11] </ref>. Each dense column a i is split into a number of columns a 1 i ; : : : ; a k i i with lower density, such that P k i j i a s T = 0 for r 6= s. <p> Hence, the outer product P k i j j i will be approximately 1=k i as dense as a T i a i . Each splitting results in the introduction of a new variable and a new constraint via a linking matrix (see Vanderbei <ref> [11] </ref>) to the problem, resulting in a transformed problem with coefficient matrix ^ A 2 IR ^mfi^n , where ^m := m + P P (k i 1).
Reference: [12] <author> Stephen Wright. </author> <title> Modified Cholesky factorizations in interior-point algorithms for linear programming. </title> <type> Preprint ANL/MCS-P600-0596, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <address> 9700 South Cass Avenue, Argonne, IL 60439, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: This modification is well established for interior-point codes in various contexts; see Wright <ref> [12] </ref> for a theoretical investigation. We report results on a Sun SPARCstation 20 running SunOS 4.1.4. In the current netlib set only eight problems contain dense columns. We consider five of these problems to be large and the others to be small.
Reference: [13] <author> Stephen J. Wright. </author> <title> Primal-Dual Interior-Point Methods. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, PA, </address> <month> December </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Primal-dual interior-point methods solve the linear programming problem (LP ) min c T x subject to Ax = b; x 0 by applying Newton-like methods to the optimality conditions for this constrained problem, also known as the Karush-Kuhn-Tucker (KKT) conditions (see Wright <ref> [13] </ref>).
Reference: [14] <author> Yin Zhang. </author> <title> User's Guide to LIPSOL. </title> <institution> Department of Mathematics and Statistics, University of Maryland, Baltimore County, Baltimore, Maryland 21228-5398, U.S.A., </institution> <note> version 0.3 edition, </note> <month> July </month> <year> 1995. </year> <title> Documentation for the package LIPSOL. </title> <type> 13 </type>
Reference-contexts: In the latter case, restore the original solution obtained from the SMW formula if the relative residual has not been improved by the pcg procedure. A similar technique is used by the LIPSOL code of Zhang <ref> [14, 15] </ref>. The pcg procedure need not be used only in conjunction with SMW. It can be used in place of iterative refinement to improve the accuracy of the solutions even when dense columns are absent.
Reference: [15] <author> Yin Zhang. </author> <title> Solving large-scale linear programs by interior-point methods under the MATLAB enviroment. </title> <type> Technical Report TR96-01, </type> <institution> Department of Mathematics and Statistics, University of Maryland, Baltimore County, </institution> <address> Baltimore, Maryland 21228-5398, U.S.A., </address> <month> February </month> <year> 1996. </year> <title> Documentation for the package LIPSOL. </title> <type> 14 </type>
Reference-contexts: In the latter case, restore the original solution obtained from the SMW formula if the relative residual has not been improved by the pcg procedure. A similar technique is used by the LIPSOL code of Zhang <ref> [14, 15] </ref>. The pcg procedure need not be used only in conjunction with SMW. It can be used in place of iterative refinement to improve the accuracy of the solutions even when dense columns are absent.
References-found: 15

