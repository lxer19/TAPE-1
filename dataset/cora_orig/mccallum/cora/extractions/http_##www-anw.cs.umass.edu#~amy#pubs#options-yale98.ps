URL: http://www-anw.cs.umass.edu/~amy/pubs/options-yale98.ps
Refering-URL: http://www-anw.cs.umass.edu/~amy/pubs.html
Root-URL: 
Email: amy@cs.umass.edu  dprecup@cs.umass.edu  ravi@cs.umass.edu  baveja@cs.colorado.edu  rich@cs.umass.edu  
Title: Hierarchical Optimal Control of MDPs  
Author: Amy McGovern Doina Precup Balaraman Ravindran Satinder Singh Richard S. Sutton 
Address: Amherst, MA 01003  Amherst, MA 01003  Amherst, MA 01003  Boulder, CO 80309  Amherst, MA 01003  
Affiliation: Univ. of Massachusetts  Univ. of Massachusetts  Univ. of Massachusetts  Univ. of Colorado  Univ. of Massachusetts  
Abstract: Fundamental to reinforcement learning, as well as to the theory of systems and control, is the problem of representing knowledge about the environment and about possible courses of action hierarchically, at a multiplicity of interrelated temporal scales. For example, a human traveler must decide which cities to go to, whether to fly, drive, or walk, and the individual muscle contractions involved in each step. In this paper we survey a new approach to reinforcement learning in which each of these decisions is treated uniformly. Each low-level action and high-level course of action is represented as an option, a (sub)controller and a termination condition. The theory of options is based on the theories of Markov and semi-Markov decision processes, but extends these in significant ways. Options can be used in place of actions in all the planning and learning methods conventionally used in reinforcement learning. Options and models of options can be learned for a wide variety of different subtasks, and then rapidly combined to solve new tasks. Options enable planning and learning simultaneously at a wide variety of times scales, and toward a wide variety of subtasks, substantially increasing the efficiency and abilities of reinforcement learning systems. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bradtke, S. J., and Duff, M. O. </author> <year> 1995. </year> <title> Reinforcement learning methods for continuous-time Markov decision problems. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <volume> 393 400. </volume> <publisher> MIT Press. </publisher>
Reference-contexts: This is the main idea behind intra-option methods . negative rewards were introduced at all the states, and the goal was located at G. We experimented with two SMDP methods: one-step SMDP Q-learning <ref> (Bradtke & Duff, 1995) </ref> and a hierarchical form of Q-learning called Macro Q-learning (McGovern, Sutton & Fagg, 1997). Although the SMDP methods can be used here, they were much slower than the intra-option method.
Reference: <author> Dayan, P., and Hinton, G. E. </author> <year> 1993. </year> <title> Feudal reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <volume> 271278. </volume> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dietterich, T. G. </author> <year> 1998. </year> <title> The MAXQ method for hierarchical reinforcement learning. </title> <booktitle> In Proc. of the 15th Intl. Conf. on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Huber, M., and Grupen, R. A. </author> <year> 1997. </year> <title> A feedback control structure for on-line learning tasks. </title> <booktitle> Robotics and Autonomous Systems 22(3-4):303315. </booktitle>
Reference: <author> Kaelbling, L. P. </author> <year> 1993. </year> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> In Proc. of the 10th Intl. Conf. on Machine Learning, </booktitle> <volume> 167173. </volume> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lin, L.-J. </author> <year> 1993. </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> Ph.D. Dissertation, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Mahadevan, S.; Marchallek, N.; Das, T. K.; and Gosavi, A. </author> <year> 1997. </year> <title> Self-improving factory simulation using continuous-time average-reward reinforcement learning. </title> <booktitle> In Proc. of the 14th Intl. Conf. on Machine Learning, </booktitle> <address> 202210. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> McGovern, A.; Sutton, R. S.; and Fagg, A. H. </author> <year> 1997. </year> <title> Roles of macro-actions in accelerating reinforcement learning. </title> <booktitle> In Grace Hopper Celebration of Women in Computing, </booktitle> <pages> 1317. </pages>
Reference-contexts: This is the main idea behind intra-option methods . negative rewards were introduced at all the states, and the goal was located at G. We experimented with two SMDP methods: one-step SMDP Q-learning (Bradtke & Duff, 1995) and a hierarchical form of Q-learning called Macro Q-learning <ref> (McGovern, Sutton & Fagg, 1997) </ref>. Although the SMDP methods can be used here, they were much slower than the intra-option method. Analyzing the Effects of Options Adding options can either accelerate or retard learning depending on their appropriateness to the particular task.
Reference: <author> Parr, R., and Russell, S. </author> <year> 1998. </year> <title> Reinforcement learning with hierarchies of machines. </title> <booktitle> In Advances in Neural Information Processing Systems 10. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Precup, D.; Sutton, R. S.; and Singh, S. </author> <year> 1998a. </year> <title> Multi-time models for temporally abstract planning. </title> <booktitle> In Advances in Neural Information Processing Systems 10. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: We keep the original state representation and instead alter the temporal aspects of the actions. In this paper we survey our recent and ongoing work in temporal abstraction and hierarchical control of Markov decision processes <ref> (Precup, Sutton & Singh 1998a,b, in prep.) </ref>. This work is part of a larger trend toward focusing on these issues by many researchers in reinforcement learning (e.g.
Reference: <author> Precup, D.; Sutton, R. S.; and Singh, S. </author> <year> 1998b. </year> <title> Theoretical results on reinforcement learning with temporally abstract options. </title> <booktitle> In Machine Learning: ECML98. 10th European Conference on Machine Learning. Proceedings, </booktitle> <volume> 382393. </volume> <publisher> Springer. </publisher>
Reference: <author> Singh, S. P. </author> <year> 1992a. </year> <title> Reinforcement learning with a hierarchy of abstract models. </title> <booktitle> In Proc. of the 10th National Conf. on Artificial Intelligence, </booktitle> <address> 202207. </address> <publisher> MIT/AAAI Press. </publisher>
Reference: <author> Singh, S. P. </author> <year> 1992b. </year> <title> Scaling reinforcement learning by learning variable temporal resolution models. </title> <booktitle> In Proc. of the 9th Intl. Conf. on Machine Learning, </booktitle> <volume> 406415. </volume> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. S.; Precup, D.; and Singh, S. </author> <year> 1998. </year> <title> Intra-option learning about temporally abstract actions. </title> <booktitle> In Proc. of the 15th Intl. Conf. on Machine Learning. </booktitle> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Thus, p o ss 0 is a combination of the likelihood that s 0 is the state in which o terminates together with a measure of how delayed that outcome is relative to fl. We call this kind of model a multi-time model <ref> (Precup and Sutton, 1998) </ref> because it describes the outcome of an option not at a single time, but at potentially many different times, appropriately combined. Using multi-time models we can write Bellman equations for general policies and options.
Reference: <author> Sutton, R. S.; Precup, D.; and Singh, S. </author> <title> in preparation. Between MDPs and Semi-MDPs: learning, planning, and representing knowledge at multiple temporal scales. </title>
Reference: <author> Puterman, M. L. </author> <year> (1994). </year> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> Wiley. </publisher>
Reference-contexts: MDP + Options = SMDP Options are closely related to the actions in a special kind of decision problem known as a semi-Markov decision process, or SMDP <ref> (e.g., see Puterman, 1994) </ref>. In fact, a fixed set of options defines a new discrete-time SMDP embedded within the original MDP, as suggested by Figure 1.
Reference: <author> Thrun, S., and Schwartz, A. </author> <year> 1995. </year> <title> Finding structure in reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <volume> 385392. </volume> <publisher> MIT Press. </publisher>
References-found: 17

