URL: http://www.cs.utexas.edu/users/lorenzo/papers/Dissertation/thesis.ps
Refering-URL: http://www.cs.utexas.edu/users/lorenzo/publications.html
Root-URL: http://www.cs.utexas.edu
Title: UNDERSTANDING THE MESSAGE LOGGING PARADIGM FOR MASKING PROCESS CRASHES  
Author: Lorenzo Alvisi 
Degree: A Dissertation Presented to the Faculty of the Graduate School  in Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy by  
Date: January 1996  
Affiliation: of Cornell University  
Abstract-found: 0
Intro-found: 1
Reference: [AHM93] <author> Lorenzo Alvisi, Bruce Hoppe, and Keith Marzullo. </author> <title> Nonblocking and orphan-free message logging protocols. </title> <booktitle> In Proceedings of the 23rd Fault-Tolerant Computing Symposium, </booktitle> <pages> pages 145-154, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: The details of how this optimization can be implemented in an FBL protocol for f = 1 are presented in <ref> [AHM93] </ref>. A similar compression of the representation of DS p 1 p i can be achieved by applying directly run length encoding to the sequence of parents m l :source; : : : m h :source in Figure 6.6.
Reference: [AM94a] <author> L. Alvisi and K. Marzullo. </author> <title> Deriving optimal checkpointing protocols for distributed shared memory architectures. </title> <editor> In K. Birman, F. Mattern, and A. Schiper, editors, </editor> <booktitle> Selected Papers, International Workshop in Theory and Practice in Distributed Systems, </booktitle> <pages> pages 111-120. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: We hope to show that these conditions can be easily specialized for each individual memory coherency model to obtain fault-tolerant protocols that are optimal according to definitions similar to those given in Chapter 5. Some preliminary results in this direction are presented in <ref> [AM94a] </ref>. Appendix A Three FBL Logging Components This Appendix contains a detailed description of an implementation of the logging components of Det , + jLogj and + Log assuming a system N of n processes, of which at most f can fail concurrently.
Reference: [AM94b] <author> Lorenzo Alvisi and Keith Marzullo. </author> <title> Optimal message logging protocols. </title> <type> Technical Report TR94-1457, </type> <institution> Cornell University Department of Computer Science, </institution> <month> September </month> <year> 1994. </year>
Reference: [AM95] <author> L. Alvisi and K. Marzullo. </author> <title> Message logging: Pessimistic, optimistic, and causal. </title> <booktitle> In Proceedings of the 15th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 229-236. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1995. </year>
Reference: [BA + 92] <author> O. Babaoglu, L. Alvisi, et al. </author> <title> Paralex: An environment for parallel programming in distributed systems. </title> <booktitle> In Proceedings of the 6th ACM International Conference on Supercomputing, </booktitle> <pages> pages 178-187, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: There are applications, however, for which Det performs as well as Log even for large values of f . For example, Figure 6.9 shows an application for which Det does as well as + Log when f = n <ref> [BA + 92] </ref>. The application is a parallel solution to the Synthetic Aperture Radar problem (SAR) in which radar echoes, collected by aircraft or spacecraft, are used to construct terrain contours.
Reference: [BBG83] <author> Anita Borg, J. Baumbach, and S. Glazer. </author> <title> A message system supporting fault tolerance. </title> <booktitle> In Proceedings of the Symposium on Operating Systems Principles, </booktitle> <pages> pages 90-99. </pages> <publisher> ACM SIGOPS, </publisher> <month> October </month> <year> 1983. </year>
Reference-contexts: For completeness, we define pessimistic protocols that log the determinant of m in local stable storage to be 1-blocking, since there is one event (the acknowledgment of #m being written to stable storage) that must occur between deliver m:dest (m) and a subsequent send <ref> [BBG83] </ref>. Clearly, a lower bound for k is 0; furthermore, the existence of optimistic protocols establishes that this bound is tight. 42 3.
Reference: [BJ87] <author> Kenneth Birman and Tommy Joseph. </author> <title> Reliable communication in the presence of failures. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(2) </volume> <pages> 47-76, </pages> <month> February </month> <year> 1987. </year> <pages> 133 134 </pages>
Reference-contexts: Causal delivery order <ref> [BJ87] </ref> strengthens FIFO delivery order by removing the requirement that ordering occurs only when the source of m and m 0 are the same. <p> The second approach is for each process p to piggyback, on each message m that p sends, all messages m 0 sent in the causal history of the event send p (m) such that p does not know if m 0 has already been delivered <ref> [BJ87] </ref>. The piggybacked messages are placed in a total order that extends the partial order imposed by the happens-before relation. Before delivering m, process m.dest first checks if any message m 0 in m's piggyback has m:dest for destination.
Reference: [BLL90] <author> B. Barghava, S-R. Lian, and P-J. Leu. </author> <title> Experimental evaluation of concurrent checkpointing and rollback-recovery algorithms. </title> <booktitle> In Proocedings of the International Conference on Data Engineering, </booktitle> <pages> pages 182-189, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: Coordination, however, comes at a cost in an asynchronous system: since processes cannot rely on a common clock to coordinate, they need to exchange messages, at a considerable overhead for environments in which communication is expensive <ref> [BLL90] </ref>. The original motivation for message logging was precisely to avoid the overhead of checkpoint coordination while still preventing the domino effect. Message logging achieves this result by requiring each process to save on stable storage some additional data collected between taking consecutive independent checkpoints.
Reference: [BMST92] <author> Navin Budhiraja, Keith Marzullo, Fred B. Schneider, and Sam Toueg. </author> <title> Primary-backup protocols: Lower bounds and optimal implementations. </title> <booktitle> In Proceedings of the Third IFIP Conference on Dependable Computing for Critical Applications, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: Message logging is not the only technique for building systems that tolerate this kind of process failures. For example, active replication [Sch90] or primary-backup <ref> [BMST92] </ref> are other commonly-used techniques. What makes message logging attractive is its simplicity and low cost. A further advantage of message logging is that it can be readily applied to any inter-process communication structure, while both active replication and primary-backup are typically applied in a client-server setting. <p> The current design of FBL protocols is no exception. Hence, we are developing an approach that employs techniques used in primary-backup protocols <ref> [BMST92] </ref> in order to achieve FBL protocols that do not block when communicating with the environment. 7.2.3 Combining Checkpointing with Recovery We are considering a novel approach that integrates coordinated checkpointing with recovery.
Reference: [BSS91] <author> K. Birman, A. Schiper, and P. Stephenson. </author> <title> Lightweight causal and atomic group multicast. </title> <journal> ACM Transactions on Computer System, </journal> <volume> 9(3) </volume> <pages> 272-314, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Therefore, r must deliver m 3 only after m 1 has been received and delivered. There are two fundamental approaches to implementing causal message delivery. The first is to add to each message m additional information <ref> [RST91,SS92, BSS91] </ref> that m's destination process uses to determine when m can be delivered. 46 Using this approach, process r in Figure 5.1 would realize, when it receives m 3 , that it must wait to receive m 1 , and will delay delivery of m 3 accordingly.
Reference: [CL85] <author> K. M. Chandy and L. Lamport. </author> <title> Distributed snapshots: determining global states of distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(1) </volume> <pages> 63-75, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: The major limitations of message logging are two: 1. The time necessary to recover a process using message logging tends to be higher than the time necessary to recover a process using active-replication or primary-backup. 4 2. Reaching a consistent global state <ref> [CL85] </ref> upon recovery of a faulty process is complicated, unless blocking is introduced during failure free execution. The first limitation can be addressed by increasing the frequency with which checkpoints are taken (although at the cost of slowing down the application). <p> A collection of states, one from each process, defines a global state. A global state is consistent if all pairs of states are mutually consistent <ref> [CL85] </ref>; 1 otherwise it is inconsistent. Given an initial state and a local history h p of a process p, one can construct a local state history p of p. <p> can then define over states: 1. for all events e, s e s 0 e ; 2. for all pairs of events e 1 and e 2 , e 1 e 2 implies that s e 1 s 0 e 2 . 1 This definition is different from that of <ref> [CL85] </ref> in that it is defined in terms of deliver events rather than receive events. Our usage corresponds to the literature on message-logging protocols. 9 The transitive closure of over states yields the happens-before relation ! over states. <p> A checkpoint of the global state of the application can therefore only be obtained by taking local checkpoints of the application processes. Unfortunately, an arbitrary set of local checkpoints need not constitute a consistent global state <ref> [CL85] </ref>. For example, consider Figure 3.1. Process p 1 and process p 2 exchange messages and take periodic and independent checkpoints of their local states, denoted as C 1i and C 2i for 1 i 3. <p> This uncontrolled cascade of rollbacks is called the domino effect [Ran75]. The domino effect can be prevented if processes coordinate their checkpoints, rather than checkpointing their states independently. Through coordination, it is possible to guarantee that a set of local checkpoints will indeed constitute a consistent global state <ref> [CL85] </ref>. Coordination, however, comes at a cost in an asynchronous system: since processes cannot rely on a common clock to coordinate, they need to exchange messages, at a considerable overhead for environments in which communication is expensive [BLL90]. <p> This solution requires a recovering process executing Phase 1 to restart Phase 1 whenever it detects the failure of another process. In Section 7.2 we sketch a new scheme, based on Chandy and Lamport's snapshot protocol <ref> [CL85] </ref>, that also does not delay the delivery of application messages. <p> In this approach, a recovering process p, after being restored to its latest checkpointed state, begins a distributed snapshot <ref> [CL85] </ref>. We believe that this approach can be used to guarantee recovery without forcing correct processes 119 to delay the delivery of application messages.
Reference: [DGB + 96] <author> R. Davoli, L. A. Giachini, O. Babaoglu, S. Amoroso, and L. Alvisi. </author> <title> Parallel computing in networks of workstations with paralex. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1996. To appear. </note>
Reference-contexts: Programming these systems, however, poses challenges that are not presented by a traditional supercomputer. One of the major challenges is providing fault-tolerance. 1 2 Today's important computing applications (e.g. genome analysis) require days or weeks to execute on networks with dozens of workstations <ref> [DGB + 96] </ref>. Hours of computing can be wasted if there are hardware failures, or even if one of the processors is turned off, rebooted, or disconnected from the network by an unknowing user.
Reference: [EJZ92] <author> E.N. Elnozahy, D.B. Johnson, and W. Zwaenepoel. </author> <title> The performance of consistent checkpointing. </title> <booktitle> In Proceedings of the Eleventh Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 39-47, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Initialize p to the latest checkpointed state, and restart p. 3. Replay to p the messages p originally delivered after the latest checkpoint, according to the order specified by the corresponding message determinants. We assume the existence of suitable mechanisms to implement Steps 1 and 2 <ref> [EJZ92] </ref>, and concentrate on the issues that arise in the implementation of Step 3. The logging and recovery components are implemented by a layer of software that resides between the application processes and the rest of the system.
Reference: [Eln93] <author> E. N. Elnozahy. Manetho: </author> <title> Fault Tolerance in Distributed Systems Using Rollback-Recovery and Process Replication. </title> <type> Ph.D. dissertation, </type> <institution> Rice University, </institution> <month> October </month> <year> 1993. </year> <note> Available as report COMP TR93-212. </note>
Reference-contexts: One of the results of this dissertation is to show that it is possible to implement stable storage efficiently using volatile memory and tolerate an arbitrary number of failures. A similar technique is independently developed in <ref> [Eln93] </ref>. If a protocol is designed to tolerate an arbitrary number of failures, then stable storage is traditionally implemented using stable memory, such as replicated disks or battery-backed RAM. <p> requires processes to have instantaneous access to Log (m), which is defined over the entire distributed system. 57 5.4 Existing Optimal Message-Logging Protocols The piggybacking scheme used by oc is the core of the logging components of the only two known optimal and causal message-logging protocols: family-based logging and Manetho <ref> [Eln93] </ref>. Family-based logging is described in detail in Chapter 6. We briefly describe the aspects of Manetho that concern the logging of determinants in the following section. 5.4.1 Manetho Manetho is an optimal causal message-logging protocol designed to tolerate up to n concurrent failures. <p> Flush DS p to stable storage. For example, in Manetho a process asynchronously flushes DS p to stable storage whenever the size of DS p exceeds 64 bytes <ref> [Eln93] </ref>. Note that if this solution is used then the threshold must be chosen conservatively, in order to account for the possible increase of the size of DS p while the asynchronous flushing completes. 2. Send extra messages carrying the non-stable determinants. <p> Using this scheme in the execution in Figure 6.14, process p 3 would not deliver m 3 , since it would learn that p 1 can not be recovered to deliver m 1 . This scheme is refined in <ref> [Eln93] </ref> using the fact that a process q, when it forwards a "reply" message and receives the corresponding acknowledgment, knows that the recovering process has received all the determinants in q's "reply" message. <p> Since the state of p 1 saved in C 11 reflects the delivery of m 3 , p 1 must be rolled back past its latest checkpoint in order for the system to reach a consistent global state. 6.8.2 Coordinated Checkpointing The coordinator-based scheme proposed in Manetho <ref> [Eln93] </ref> to coordinate checkpoints can be used, unchanged, by FBL protocols. Experiments conducted in Manetho show that the difference in performance between coordinated and independent checkpoints is negligible. <p> By doing so, p makes stable all the determinants needed to reproduce, during recovery, the causal path that led p to the state in which m was sent. The same scheme is also used by Manetho <ref> [Eln93] </ref>. 115 6.9.1 Discussion FBL's output commit protocol has several desirable properties: * It does not require p to synchronize with the other processes in the system before sending m. * It writes only a small amount of data to stable storage.
Reference: [Eln95] <author> E.N. Elnozahy. </author> <title> On the relevance of communication costs of rollback-recovery protocols. </title> <booktitle> In Proceedings of the Fourteenth Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 74-79, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: However, q must still wait for the conclusion of Phase 1 of p's recovery before delivering any other application message. Elnozahy <ref> [Eln95] </ref> presents a scheme that does not require correct processes to delay the delivery of any application message while waiting for a process to complete Phase 1 of its recovery procedure.
Reference: [EZ92] <author> E. N. Elnozahy and W. Zwaenepoel. Manetho: </author> <title> Transparent rollback-recovery with low overhead, limited rollback and fast output commit. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(5) </volume> <pages> 526-531, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Therefore, it is not necessary to save the message log on stable storage. Most protocols, however, do so in order to expedite recovery [SY85,SW89,SBY88,PP83, SBY88,JZ90,VJ94]. The disadvantage is added overhead. An alternative scheme is to save the message log in the volatile memory of the senders <ref> [JZ87,JV87,SBY88, EZ92] </ref>. In this scheme, if the process m:dest that delivered m fails, then m:dest requests m:source to replay m's contents during recovery. <p> For most applications, the 102 103 most sensible action is for q to ignore stale messages. Hence, we assume that the recovery component implements a mechanism to recognize stale messages: this can be accomplished, for instance, by assigning to each instance of a process a unique incarnation number <ref> [EZ92] </ref>, and by requiring each message to be tagged with the current incarnation number of the sender. In the following, we describe the recovery procedure assuming that no checkpoints are taken by p old during the execution preceding the failure. <p> Garbage collection can be achieved using techniques similar to those described in <ref> [EZ92] </ref>.
Reference: [EZ94] <author> E.N. Elnozahy and W. Zwaenepoel. </author> <title> On the use and implementation of message logging. </title> <booktitle> In Digest of Papers: 24 Annual International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 298-307. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1994. </year> <month> 135 </month>
Reference-contexts: Before an application can send output to the environment, a special protocol, commonly called output commit protocol, is run to ensure that, in case of a failure, the application will never roll back to a state prior to the one in which the output was produced. Following <ref> [EZ94] </ref>, we call the time necessary to execute such a protocol the output latency. In the absence of message logging, the output commit protocol requires 16 a coordinated checkpoint. <p> This weaker requirement results in potentially much lower output latency. Recently, Elnozahy and Zwaenepoel have experimentally compared the performance of message logging and coordinated checkpointing for compute-intensive applications running on networks of workstations <ref> [EZ94] </ref>. Their data confirm that message logging substantially outperforms coordinated checkpointing in output latency. However, their work also suggests that the cost of coordinating checkpoints is comparable with the overhead incurred in maintaining the additional information required by message logging. <p> A recovery component, which uses the information saved by the logging component to recover the application to a state that satisfies the no-orphan consistency condition. Checkpoints can be independent [SY85,KT87,SW89,JZ87,SBY88,JV87,JZ90, Joh93,WF92,EZ92,VJ94] or coordinated <ref> [EZ94] </ref>. The storage abstraction implemented by the logging component is called a log. To emphasize the fundamentally different roles played by message determinants and message contents, we explicitly distinguish between storing messages and storing determinants. We will therefore talk of a determinant log, distinct from a message log. <p> Hence, logging a single copy of m in the volatile memory of m:source is prudent. A scheme that stores the message log on volatile memory has been shown experimentally to outperform significantly a scheme that saves the message log on stable storage. <ref> [EZ94] </ref> 3.5.2 Choosing a logging policy The policy used to log determinants affects how the application is restarted upon recovery of the failed processes. Recall, no process may be an orphan in the global state from which the application is resumed after a failure. <p> In addition, other issues that are difficult to quantify, such as the cost of output commit, must be taken into consideration in assessing the performance of a message-logging protocol <ref> [EZ94] </ref>. Nevertheless, the protocols we call optimal are unique in optimally addressing the theoretical desiderata of the message-logging approach. 5.2 Using Causal Delivery Order to Enforce the Always-No-Orphans Consistency Condition We now show how to derive an optimal causal message-logging protocol, i.e. an optimal message-logging protocol that implements Property (4.10).
Reference: [Gra77] <author> James N. Gray. </author> <title> Notes on data base operating systems. </title> <editor> In R. Bayer, R. M. Graham, and G. Seegmueller, editors, </editor> <booktitle> Operating Systems: An Advanced Course, </booktitle> <pages> pages 393-481. </pages> <publisher> Springer-Verlag, </publisher> <year> 1977. </year> <note> Lecture Notes on Computer Science 60. </note>
Reference-contexts: In this model, processes fail by halting, and the fact that a process has failed is eventually detected by all non-faulty processes. * Each process knows the identities of the fixed set of processes comprsing the system. * Stable storage, a system utility used to atomically and reliably log data <ref> [Gra77] </ref>, is available across the system and persists across failures. * There are sufficient resources to restart a faulty process eventually. Chapter 3 The Message-Logging Approach This chapter provides an informal introduction to the message-logging technique for providing fault-tolerance. The chapter is structured in five sections.
Reference: [Joh93] <author> D.B. Johnson. </author> <title> Efficient transparent optimistic rollback recovery for distributed application programs. </title> <booktitle> In Proceedings of the 12th Symposium on Reliable Distributed Systems, </booktitle> <month> October </month> <year> 1993. </year>
Reference: [JV87] <author> T.Y. Juang and S. Venkatesan. </author> <title> Crash recovery with little overhead. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 454-461. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1987. </year>
Reference-contexts: Considering that pessimistic protocols typically use replicated disks to implement stable storage, such blocking may unacceptably slow down system performance. Optimistic logging protocols <ref> [SY85, KT87, JZ87, SBY88, SW89, JV87, JZ90, WF92,VJ94] </ref>, in contrast, do not synchronize logging and communication. They allow processes to send messages even when the determinants of previously delivered messages are not yet logged. Optimistic protocols only require that determinants be logged eventually. <p> One can imagine protocols where no single process knows the value of #m but a set of processes collectively do. An example of such a protocol is given in <ref> [JV87] </ref>, where the value of #m for some message m may be inferred by "holes" in the sequence of logged receive sequence numbers.
Reference: [JZ87] <author> D.B. Johnson and W. Zwaenepoel. </author> <title> Sender-based message logging. </title> <booktitle> In Digest of Papers: 17 Annual International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 14-19. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1987. </year>
Reference-contexts: Considering that pessimistic protocols typically use replicated disks to implement stable storage, such blocking may unacceptably slow down system performance. Optimistic logging protocols <ref> [SY85, KT87, JZ87, SBY88, SW89, JV87, JZ90, WF92,VJ94] </ref>, in contrast, do not synchronize logging and communication. They allow processes to send messages even when the determinants of previously delivered messages are not yet logged. Optimistic protocols only require that determinants be logged eventually. <p> Suppose now that p 1 sends a message m 0 to process p 2 . Since deliver p 1 (m) ! deliver p 2 (m 0 ), when process p 2 delivers m 0 it becomes a member of Depend (m), and jDepend (m)j 1 In pessimistic sender-based logging <ref> [JZ87] </ref> process m.dest increases jLog (m)j by sending the value of m.rsn to process m.source, piggybacked on the acknowledgment of message m. <p> We say that a message-logging protocol is k-blocking if, in all failure-free runs and for all messages m, process m.dest delivers no less than k messages between deliver m:dest (m) and e. For example, pessimistic sender-based logging <ref> [JZ87] </ref> is 1-blocking because process m.dest must receive a message acknowledging the logging of m.rsn before sending a message subsequent to the delivery of m. Optimistic protocols are, by design, 0-blocking. <p> Suppose now that is run using instead of ` . We say that sends additional messages in if sends more messages than ` . For example, in order to tolerate single crash failures, pessimistic sender-based logging <ref> [JZ87] </ref> potentially requires that one extra acknowledgment be sent for each application message sent. Clearly, a lower bound is to send no additional messages, and the existence of optimistic protocols establishes that this bound is tight. 4.
Reference: [JZ90] <author> D.B. Johnson and W. Zwaenepoel. </author> <title> Recovery in distributed systems using optimistic message logging and checkpointing. </title> <journal> Journal of Algorithms, </journal> <volume> 11 </volume> <pages> 462-491, </pages> <year> 1990. </year>
Reference-contexts: Considering that pessimistic protocols typically use replicated disks to implement stable storage, such blocking may unacceptably slow down system performance. Optimistic logging protocols <ref> [SY85, KT87, JZ87, SBY88, SW89, JV87, JZ90, WF92,VJ94] </ref>, in contrast, do not synchronize logging and communication. They allow processes to send messages even when the determinants of previously delivered messages are not yet logged. Optimistic protocols only require that determinants be logged eventually. <p> Several schemes have been proposed in order to guarantee that Phase 1 terminates and that Property 6.10 holds for each recovering process at the end of Phase 1 of its recovery procedure. In <ref> [JZ90] </ref>, no correct process delivers any application message while a process is executing Phase 1 of its recovery procedure. At the end of Phase 1, the recovering process sends to all processes the value of the highest receive sequence number among all the determinants in M.
Reference: [KT87] <author> R. Koo and S. Toueg. </author> <title> Checkpointing and rollback-recovery for distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1):23-31, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: Considering that pessimistic protocols typically use replicated disks to implement stable storage, such blocking may unacceptably slow down system performance. Optimistic logging protocols <ref> [SY85, KT87, JZ87, SBY88, SW89, JV87, JZ90, WF92,VJ94] </ref>, in contrast, do not synchronize logging and communication. They allow processes to send messages even when the determinants of previously delivered messages are not yet logged. Optimistic protocols only require that determinants be logged eventually.
Reference: [Lam78] <author> L. Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: The transitive closure of relation is denoted by ! <ref> [Lam78] </ref>. If e i ! e j then we say that "e i happens before e j ", "e i causally precedes e j ", "e j causally follows e i " or "e j depends on e i ". <p> A vector clock is a generalization of Lamport's logical clocks <ref> [Lam78] </ref>. Given a set N of n processes, a vector clock is a vector of size n of natural numbers. Each process p in the set maintains its local vector clock and assigns a (vector) time-stamp to each event it executes.
Reference: [LSP82] <author> L. Lamport, R. Shostack, and M. Pease. </author> <title> The byzantine generals problem. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(3) </volume> <pages> 382-401, </pages> <year> 1982. </year>
Reference-contexts: For instance, spatial redundancy, which guarantees that no computation will ever need to be repeated because of a failure, is well suited for applications that have strict timing constraints. Spatial redundancy is also the only way to tolerate failures that exhibit arbitrarily malicious behavior (sometimes called Byzantine failures) <ref> [LSP82] </ref>. Spatial redundancy, however, requires a considerable investment of computing resources. To tolerate a single malicious failure, it is necessary to execute copies of the same application on three independent processors [vN56].
Reference: [Mat89] <author> Friedmann Mattern. </author> <title> Virtual time and global states of distributed systems. </title> <editor> In M. Cosnard et. al., editor, </editor> <booktitle> Parallel and Distributed Algorithms, </booktitle> <pages> pages 215-226. </pages> <publisher> Elsevir Science Publishers B. V., </publisher> <year> 1989. </year>
Reference-contexts: Observe that the following definition of Log (m) satisfies Prop erty 4.10: Log (m) = &gt; &gt; &lt; Depend (m) if jDepend (m)j f any set S such that jSj &gt; f otherwise A process can use jDepend (m)j to evaluate jLog (m)j, and take advantage of vector clocks <ref> [Mat89] </ref> to detect causal dependencies. A vector clock is a generalization of Lamport's logical clocks [Lam78]. Given a set N of n processes, a vector clock is a vector of size n of natural numbers.
Reference: [Pnu77] <author> Amir Pnueli. </author> <title> The temporal logic of programs. </title> <booktitle> In Proceedings of the Eighteenth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 46-57, </pages> <month> November </month> <year> 1977. </year>
Reference-contexts: of )ii 8p; m : ((Log (m) F) ) ((p 2 Depend (m)) ) (p 2 F ))) = hhP Q = 8p : (p 2 P ) p 2 Q)ii 8m : ((Log (m) F ) ) (Depend (m) F )) 31 where 2 is the temporal "always" operator <ref> [Pnu77] </ref>. Property (4.4) is a safety property: it must always hold during an execution in order to ensure that no orphans will be created. <p> Optimistic protocols implement the following property: 8m : 2 (:stable (m) ) ((Log (m) F ) ) 3 (Depend (m) F ))) (4.8) where 3 is the temporal "eventually" operator <ref> [Pnu77] </ref>. Property (4.8) is weaker than Property (4.2), and therefore weaker than Property (4.6). Property (4.8) permits the temporary creation of orphan processes, but guarantees that, by the time recovery is complete, no surviving process will be an orphan and Property (4.2) will hold.
Reference: [PP83] <author> M.L. Powell and D.L. Presotto. </author> <title> Publishing: A reliable broadcast communication mechanism. </title> <booktitle> In Proceedings of the Ninth Symposium on Operating System Principles, </booktitle> <pages> pages 100-109. </pages> <publisher> ACM SIGOPS, </publisher> <month> October </month> <year> 1983. </year> <month> 136 </month>
Reference: [Ran75] <author> B. Randell. </author> <title> System structure for software fault-tolerance. </title> <journal> IEEE Transaction on Software Engineering, </journal> <volume> SE-1(2):220-232, </volume> <month> June </month> <year> 1975. </year>
Reference-contexts: The pattern of communication of Figure 3.2 will eventually force p 1 and p 2 back to their initial states. This uncontrolled cascade of rollbacks is called the domino effect <ref> [Ran75] </ref>. The domino effect can be prevented if processes coordinate their checkpoints, rather than checkpointing their states independently. Through coordination, it is possible to guarantee that a set of local checkpoints will indeed constitute a consistent global state [CL85].
Reference: [RST91] <author> M. Raynal, A. Schiper, and Sam Toueg. </author> <title> The causal ordering abstraction and a simple way to implement it. </title> <journal> Information Processing Letters, </journal> <volume> 39(6) </volume> <pages> 343-350, </pages> <year> 1991. </year>
Reference: [SBY88] <author> R. E. Strom, D. F. Bacon, and S. A. Yemini. </author> <title> Volatile logging in n-fault-tolerant distributed systems. </title> <booktitle> In Proceedings of the Eighteenth Annual International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 44-49, </pages> <year> 1988. </year>
Reference-contexts: Our usage corresponds to the literature on message-logging protocols. 9 The transitive closure of over states yields the happens-before relation ! over states. Processes are piecewise deterministic <ref> [SBY88] </ref>: execution of a process consists of a sequence of deterministic intervals of execution, joined by non-deterministic events. For each process, the first interval of execution begins with the process' initial state; subsequent intervals begin with each nondeterministic event. <p> Considering that pessimistic protocols typically use replicated disks to implement stable storage, such blocking may unacceptably slow down system performance. Optimistic logging protocols <ref> [SY85, KT87, JZ87, SBY88, SW89, JV87, JZ90, WF92,VJ94] </ref>, in contrast, do not synchronize logging and communication. They allow processes to send messages even when the determinants of previously delivered messages are not yet logged. Optimistic protocols only require that determinants be logged eventually. <p> Should p fail, q's message log contains sufficient information to recover p. This optimization can be generalized to the case where p has multiple parents. The result is similar to the null-logging technique described in <ref> [SBY88] </ref>. Process p logs receive sequence numbers in order to record the nondeterministic choices made during a run. Since channels are FIFO, however, p need only log the order in which p interleaves messages from different parents.
Reference: [Sch84] <author> Fred B. Schneider. </author> <title> Byzantine generals in action: Implementing fail-stop processors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(2) </volume> <pages> 145-154, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: However, the results of this dissertation can be easily extended in principle to arbitrary types of non-deterministic events. 10 transiently losing messages. * Processes fail independently according to the fail-stop model <ref> [Sch84] </ref>.
Reference: [Sch86] <author> Fred B. Schneider. </author> <title> Abstractions for fault-tolerance in distributed systems. </title> <booktitle> In Proceedings of the Tenth World Computer Congress, </booktitle> <pages> pages 727-733, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: This procedure is called recovery. In spatial redundancy, several independent copies of the application are executed concurrently on different processors, and the final result is determined through some form of voting <ref> [Sch86] </ref>. The most appropriate form of redundancy for a specific application depends on the nature of the application and the type of failures that must be tolerated.
Reference: [Sch90] <author> Fred B. Schneider. </author> <title> Implementing fault-tolerant services using the state machine approach: A tutorial. </title> <journal> Computing Surveys, </journal> <volume> 22(3) </volume> <pages> 299-319, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Message logging is not the only technique for building systems that tolerate this kind of process failures. For example, active replication <ref> [Sch90] </ref> or primary-backup [BMST92] are other commonly-used techniques. What makes message logging attractive is its simplicity and low cost. A further advantage of message logging is that it can be readily applied to any inter-process communication structure, while both active replication and primary-backup are typically applied in a client-server setting.
Reference: [SS92] <author> A. Sandoz and A. Schiper. </author> <title> A characterization of consistent distributed snapshots using causal order. </title> <type> Technical Report TR92-14, </type> <institution> Departement d'Informatique, Ecole Politechnique Federale de Lausanne, </institution> <year> 1992. </year>
Reference: [SW89] <author> A.P. Sistla and J.L. Welch. </author> <title> Efficient distributed recovery using message logging. </title> <booktitle> In Proceedings of the Eighth Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 223-238. </pages> <publisher> ACM SIGACT/SIGOPS, </publisher> <month> August </month> <year> 1989. </year>
Reference-contexts: Considering that pessimistic protocols typically use replicated disks to implement stable storage, such blocking may unacceptably slow down system performance. Optimistic logging protocols <ref> [SY85, KT87, JZ87, SBY88, SW89, JV87, JZ90, WF92,VJ94] </ref>, in contrast, do not synchronize logging and communication. They allow processes to send messages even when the determinants of previously delivered messages are not yet logged. Optimistic protocols only require that determinants be logged eventually.
Reference: [SY85] <author> R. B. Strom and S. Yemeni. </author> <title> Optimistic recovery in distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 204-226, </pages> <month> April </month> <year> 1985. </year>
Reference-contexts: For any message m delivered by process p, the receive sequence number of m, denoted m.rsn, encodes the order in which m was delivered: m.rsn = ` iff m is the ` th message delivered by p <ref> [SY85] </ref>. The state interval that p initiates with the delivery of m is denoted p [`] where `, the index of p [`], is equal to m.rsn. <p> Considering that pessimistic protocols typically use replicated disks to implement stable storage, such blocking may unacceptably slow down system performance. Optimistic logging protocols <ref> [SY85, KT87, JZ87, SBY88, SW89, JV87, JZ90, WF92,VJ94] </ref>, in contrast, do not synchronize logging and communication. They allow processes to send messages even when the determinants of previously delivered messages are not yet logged. Optimistic protocols only require that determinants be logged eventually. <p> In particular, given event e p of process p and event e q of process q 6= p, the following condition holds: e p ! e q V C (e p )[p] &lt; V C (e q )[p]: (6.6) Strom and Yemini <ref> [SY85] </ref> were the first to use vector clocks in conjunction with message logging when they introduced the notion of dependency vector. A dependency vector is a vector clock that is specialized to determine causal dependencies between delivery events occurring at different processes.
Reference: [VJ94] <author> S. Venkatesan and T.Y. Juang. </author> <title> Efficient algorithms for optimistic crash recovery. </title> <journal> Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 105-114, </pages> <month> June </month> <year> 1994. </year>
Reference: [vN56] <author> John von Neumann. </author> <title> Probabilistic logics and the synthesis of reliable organisms from unreliable components. </title> <editor> In C. E. Shannon and J. Mc-Marthy, editors, </editor> <booktitle> Automata Studies, </booktitle> <pages> pages 43-98. </pages> <publisher> Princeton University Press, </publisher> <year> 1956. </year> <month> 137 </month>
Reference-contexts: Spatial redundancy, however, requires a considerable investment of computing resources. To tolerate a single malicious failure, it is necessary to execute copies of the same application on three independent processors <ref> [vN56] </ref>. Applications that do not have strict timing constraints or for which failures do not have potentially catastrophic consequences may not warrant this kind of investment. Consider, for instance, parallel scientific applications.
Reference: [WF92] <author> Y.M. Wang and W.K. Fuchs. </author> <title> Optimisitc message logging for independent checkpointing in message-passing systems. </title> <booktitle> In Proceedings of the 11th Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 147-154, </pages> <month> October </month> <year> 1992. </year>
References-found: 40

