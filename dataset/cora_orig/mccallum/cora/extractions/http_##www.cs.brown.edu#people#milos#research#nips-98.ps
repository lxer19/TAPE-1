URL: http://www.cs.brown.edu/people/milos/research/nips-98.ps
Refering-URL: http://www.cs.brown.edu/people/milos/papers.html
Root-URL: http://www.cs.brown.edu/
Email: milos@cs.brown.edu  
Title: Planning with macro-actions: Effect of initial value function estimate on convergence rate of value iteration.  
Author: Milos Hauskrecht 
Keyword: Category: Reinforcement Learning and Control, Planning, Markov decision processes. Preference: Plenary  
Date: 1910  
Address: Box  Providence, RI 02912  
Affiliation: Department of Computer Science,  Brown University,  
Abstract: We investigate the use of temporally abstract actions, or macro-actions, in speeding-up the solution of Markov decision processes. We focus on an augmented MDP model that combines both primitive actions and macro-actions and was shown empirically to speed-up the convergence of value iteration routines under the proper choice of macro-actions. We show that the convergence rate of value iteration methods for this model is sensitive not only to the choice of macro-actions but also to the value function estimate used to initialize the procedure. The theoretical result is demonstrated also experi mentally on a simple robot maze navigation problem.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, Princeton, </publisher> <year> 1957. </year>
Reference-contexts: The new framework allows us to represent uniformly actions on different time scales, as well as, to apply standard problem-solving procedures developed for MDPs, including value and policy iteration <ref> [1, 10] </ref>. There are only a few reasonably direct ways in which macro-actions can be applied to solve MDPs. Two of them are most common. First, macro-actions can be combined with primitive actions to form an augmented MDP model. <p> Model of Temporally Abstract Actions for MDPs 2.1 Markov Decision Processes A Markov decision process is a 4-tuple hS; A; T; Ri where: S is a finite set of states; A is a finite set of actions; T is a transition model T : S fi A fi S ! <ref> [0; 1] </ref>, such that T (s; a; ) is a probability distribution over S for any s 2 S and a 2 A; and R : S fi A ! IR is a bounded reward function. <p> In such a setting, we restrict our attention to stationary policies of the form : S ! A, with (s) denoting the action to be executed in state s. The value of the optimal policy satisfies <ref> [1] </ref>: V fl (s) = max " X T (s; a; s 0 ) V fl (s 0 ) : The equation could be rewritten also as V fl = HV fl , where H : B ! B denotes a value function mapping, and B : S ! R is <p> A number of techniques for constructing optimal policies exist. An especially simple algorithm is value iteration <ref> [1] </ref>. <p> They propose the following method of modeling macros. Definition 3 A discounted transition model T i (; i ; ) for macro i (defined on region S i ) is a mapping T i : S i fi XPer (S i ) ! <ref> [0; 1] </ref> such that T i (s; i ; s 0 ) = E t (fi t1 Pr (s t = s 0 j s 0 = s; i )); where the expectation is taken with respect to time t of termination of i .
Reference: [2] <author> D. P. Bertsekas and J.. N. Tsitsiklis. </author> <title> Neuro-dynamic Programming. </title> <publisher> Athena, </publisher> <year> 1996. </year>
Reference-contexts: This model is called an abstract MDP model (after [9]) and was studied in [7, 9, 8, 14]. The abstract model is suitable for hierarchical methods (see e.g. [9]) and provides an alternative to various approximation methods for solving large MDPs <ref> [5, 2, 4, 3] </ref>. In our work we focus our attention solely on the augmented model that works with both primitive actions and macro-actions.
Reference: [3] <author> T. Dean and R. Givan. </author> <title> Model minimization in Markov decision processes. </title> <address> AAAI-97, pp.106-111, Providence, </address> <year> 1997. </year>
Reference-contexts: This model is called an abstract MDP model (after [9]) and was studied in [7, 9, 8, 14]. The abstract model is suitable for hierarchical methods (see e.g. [9]) and provides an alternative to various approximation methods for solving large MDPs <ref> [5, 2, 4, 3] </ref>. In our work we focus our attention solely on the augmented model that works with both primitive actions and macro-actions.
Reference: [4] <author> T. Dean and S.-H. Lin. </author> <title> Decomposition techniques for planning in stochastic domains. </title> <booktitle> IJCAI-95, </booktitle> <address> pp.1121-1127, Montreal, </address> <year> 1995. </year>
Reference-contexts: This model is called an abstract MDP model (after [9]) and was studied in [7, 9, 8, 14]. The abstract model is suitable for hierarchical methods (see e.g. [9]) and provides an alternative to various approximation methods for solving large MDPs <ref> [5, 2, 4, 3] </ref>. In our work we focus our attention solely on the augmented model that works with both primitive actions and macro-actions. <p> In our work we assume a simpler macro-action model based on Hauskrecht et al. [9], in which every macro-action is represented as a local policy restricted to some region of a state space. Formally, our model relies on a region-based decomposition of an MDP as defined in <ref> [4, 9] </ref>. Definition 1 A region-based decomposition of an MDP M = hS; A; T; Ri is a partitioning = fS 1 ; ; S n g of the state space S. We call the elements S i of the regions of M .
Reference: [5] <author> R. Dearden and C. Boutilier. </author> <title> Abstraction and approximate decision theoretic planning. </title> <journal> Artif. Intell., </journal> <volume> 89 </volume> <pages> 219-283, </pages> <year> 1997. </year>
Reference-contexts: This model is called an abstract MDP model (after [9]) and was studied in [7, 9, 8, 14]. The abstract model is suitable for hierarchical methods (see e.g. [9]) and provides an alternative to various approximation methods for solving large MDPs <ref> [5, 2, 4, 3] </ref>. In our work we focus our attention solely on the augmented model that works with both primitive actions and macro-actions.
Reference: [6] <author> R. Fikes, P. Hart, and N. Nilsson. </author> <title> Learning and executing generalized robot plans. </title> <journal> Artif. Intell., </journal> <volume> 3 </volume> <pages> 251-288, </pages> <year> 1972. </year>
Reference-contexts: 1 Introduction Complex sequences of actions or macro-actions have been used in planning in deterministic domains for some time <ref> [6, 12, 11] </ref> and they have been proven usefull in many applications. The ability to extend the idea to Markov Decision Processes (MDPs) is crucial for domains that cannot be modeled deterministically and that require a stochastic model of the dynamics.
Reference: [7] <author> J. P. Forestier, P. Varaiya. </author> <title> Multilayer control of large Markov chains. </title> <journal> IEEE Trans. on Aut. Control, </journal> <volume> 23 </volume> <pages> 298-304, </pages> <year> 1978. </year>
Reference-contexts: The second model works with macro-actions only and allow one to reduce the number of states of the underlying MDP to states connected by macro-actions. This model is called an abstract MDP model (after [9]) and was studied in <ref> [7, 9, 8, 14] </ref>. The abstract model is suitable for hierarchical methods (see e.g. [9]) and provides an alternative to various approximation methods for solving large MDPs [5, 2, 4, 3]. <p> An alternative model with macro-actions is abstract MDP <ref> [7, 9, 8, 14] </ref> that works with macro-actions and peripheral states only. This allows one to reduce significantly the size of the state space compared to M .
Reference: [8] <author> M. Hauskrecht. </author> <title> Planning with temporally abstract actions. </title> <type> Technical report, </type> <institution> CS-98-01, Brown University, </institution> <year> 1998. </year>
Reference-contexts: There are only a few reasonably direct ways in which macro-actions can be applied to solve MDPs. Two of them are most common. First, macro-actions can be combined with primitive actions to form an augmented MDP model. This model has been studied extensively in <ref> [17, 15, 16, 8, 9] </ref>. The second model works with macro-actions only and allow one to reduce the number of states of the underlying MDP to states connected by macro-actions. This model is called an abstract MDP model (after [9]) and was studied in [7, 9, 8, 14]. <p> The second model works with macro-actions only and allow one to reduce the number of states of the underlying MDP to states connected by macro-actions. This model is called an abstract MDP model (after [9]) and was studied in <ref> [7, 9, 8, 14] </ref>. The abstract model is suitable for hierarchical methods (see e.g. [9]) and provides an alternative to various approximation methods for solving large MDPs [5, 2, 4, 3]. <p> There are only a few reasonably direct ways in which macro-actions can be applied to solve M . In our work we focus on augmented MDP model <ref> [17, 15, 16, 8, 9] </ref>, denoted M 0 , constructed by extending the action space A using a set of macro-actions, assuming that macro models are used to determine transitions and rewards associated with these new actions. <p> An alternative model with macro-actions is abstract MDP <ref> [7, 9, 8, 14] </ref> that works with macro-actions and peripheral states only. This allows one to reduce significantly the size of the state space compared to M .
Reference: [9] <author> M. Hauskrecht, N. Meuleau, C. Boutilier, L. Kaelbling, and T. Dean. </author> <title> Hierarchical Solution of Markov Decision Processes using Macro-actions. </title> <note> To appear in UAI-98, </note> <year> 1998. </year>
Reference-contexts: There are only a few reasonably direct ways in which macro-actions can be applied to solve MDPs. Two of them are most common. First, macro-actions can be combined with primitive actions to form an augmented MDP model. This model has been studied extensively in <ref> [17, 15, 16, 8, 9] </ref>. The second model works with macro-actions only and allow one to reduce the number of states of the underlying MDP to states connected by macro-actions. This model is called an abstract MDP model (after [9]) and was studied in [7, 9, 8, 14]. <p> This model has been studied extensively in [17, 15, 16, 8, 9]. The second model works with macro-actions only and allow one to reduce the number of states of the underlying MDP to states connected by macro-actions. This model is called an abstract MDP model (after <ref> [9] </ref>) and was studied in [7, 9, 8, 14]. The abstract model is suitable for hierarchical methods (see e.g. [9]) and provides an alternative to various approximation methods for solving large MDPs [5, 2, 4, 3]. <p> The second model works with macro-actions only and allow one to reduce the number of states of the underlying MDP to states connected by macro-actions. This model is called an abstract MDP model (after [9]) and was studied in <ref> [7, 9, 8, 14] </ref>. The abstract model is suitable for hierarchical methods (see e.g. [9]) and provides an alternative to various approximation methods for solving large MDPs [5, 2, 4, 3]. <p> This model is called an abstract MDP model (after <ref> [9] </ref>) and was studied in [7, 9, 8, 14]. The abstract model is suitable for hierarchical methods (see e.g. [9]) and provides an alternative to various approximation methods for solving large MDPs [5, 2, 4, 3]. In our work we focus our attention solely on the augmented model that works with both primitive actions and macro-actions. <p> In our work we assume a simpler macro-action model based on Hauskrecht et al. <ref> [9] </ref>, in which every macro-action is represented as a local policy restricted to some region of a state space. Formally, our model relies on a region-based decomposition of an MDP as defined in [4, 9]. <p> In our work we assume a simpler macro-action model based on Hauskrecht et al. [9], in which every macro-action is represented as a local policy restricted to some region of a state space. Formally, our model relies on a region-based decomposition of an MDP as defined in <ref> [4, 9] </ref>. Definition 1 A region-based decomposition of an MDP M = hS; A; T; Ri is a partitioning = fS 1 ; ; S n g of the state space S. We call the elements S i of the regions of M . <p> There are only a few reasonably direct ways in which macro-actions can be applied to solve M . In our work we focus on augmented MDP model <ref> [17, 15, 16, 8, 9] </ref>, denoted M 0 , constructed by extending the action space A using a set of macro-actions, assuming that macro models are used to determine transitions and rewards associated with these new actions. <p> An alternative model with macro-actions is abstract MDP <ref> [7, 9, 8, 14] </ref> that works with macro-actions and peripheral states only. This allows one to reduce significantly the size of the state space compared to M . <p> We compared the results of value iteration for both the original MDP and the augmented MDP, the latter using a set of additional macros for every room in the problem. The macros were generated automatically using a simple heuristic strategy described in <ref> [9] </ref> that results in jXP er (S i )j + 1 macros for every region S i : one macro per exit state pressing the agent towards that exit, plus a 'stay-in' macro pressing the agent not to move out of the region. improves with the time (in seconds) taken by
Reference: [10] <author> R. A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <year> 1960. </year>
Reference-contexts: The new framework allows us to represent uniformly actions on different time scales, as well as, to apply standard problem-solving procedures developed for MDPs, including value and policy iteration <ref> [1, 10] </ref>. There are only a few reasonably direct ways in which macro-actions can be applied to solve MDPs. Two of them are most common. First, macro-actions can be combined with primitive actions to form an augmented MDP model.
Reference: [11] <author> R. Korf. Macro-operators: </author> <title> A weak method for learning. </title> <journal> Artif. Intell., </journal> <volume> 26 </volume> <pages> 35-77, </pages> <year> 1985. </year>
Reference-contexts: 1 Introduction Complex sequences of actions or macro-actions have been used in planning in deterministic domains for some time <ref> [6, 12, 11] </ref> and they have been proven usefull in many applications. The ability to extend the idea to Markov Decision Processes (MDPs) is crucial for domains that cannot be modeled deterministically and that require a stochastic model of the dynamics.
Reference: [12] <author> S. Minton. </author> <title> Selectively generalizing plans for problem solving. </title> <booktitle> IJCAI-85, </booktitle> <address> pp.596-599, Boston, </address> <year> 1985. </year>
Reference-contexts: 1 Introduction Complex sequences of actions or macro-actions have been used in planning in deterministic domains for some time <ref> [6, 12, 11] </ref> and they have been proven usefull in many applications. The ability to extend the idea to Markov Decision Processes (MDPs) is crucial for domains that cannot be modeled deterministically and that require a stochastic model of the dynamics.
Reference: [13] <author> R. Parr and S. Russell. </author> <title> Reinforcement learning with hierarchies of machines. </title> <editor> In M. Mozer, M. Jordan, T. Petsche, eds., NIPS-11. </editor> <publisher> MIT Press, </publisher> <year> 1998. </year>
Reference-contexts: Macro-actions can be modeled by different means. For example, macro-actions can be represented as programs with arbitrary termination conditions as suggested by Precup, Sutton and Singh [17, 15, 16] or using finite state machine representation and its hierarchical extensions as proposed by Parr <ref> [13] </ref>. In our work we assume a simpler macro-action model based on Hauskrecht et al. [9], in which every macro-action is represented as a local policy restricted to some region of a state space. Formally, our model relies on a region-based decomposition of an MDP as defined in [4, 9]. <p> A macro-action is simply a local policy defined for a particular region S i . Intuitively, this policy can be executed whenever an agent enters or is in the region and terminates when the agent leaves the region (if ever). Definitions in [15] or <ref> [13] </ref> are more general and can use arbitrary starting and termination conditions, and allow also non-Markovian policies. The main problem with integrating macro-actions into the MDP is that macro-actions when executed can extend over different periods of time.
Reference: [14] <author> R. Parr. </author> <title> Hierarchical control and learning with hierarchies of machines. </title> <note> Chapters 1-3, under preparation, </note> <year> 1998. </year>
Reference-contexts: The second model works with macro-actions only and allow one to reduce the number of states of the underlying MDP to states connected by macro-actions. This model is called an abstract MDP model (after [9]) and was studied in <ref> [7, 9, 8, 14] </ref>. The abstract model is suitable for hierarchical methods (see e.g. [9]) and provides an alternative to various approximation methods for solving large MDPs [5, 2, 4, 3]. <p> An alternative model with macro-actions is abstract MDP <ref> [7, 9, 8, 14] </ref> that works with macro-actions and peripheral states only. This allows one to reduce significantly the size of the state space compared to M .
Reference: [15] <author> D. Precup and R. S. Sutton. </author> <title> Multi-time models for temporally abstract planning. </title> <editor> In M. Mozer, M. Jordan, and T. Petsche, eds., NIPS-11. </editor> <publisher> MIT Press, </publisher> <year> 1998. </year>
Reference-contexts: Integrating programs or macro-actions into the decision process is a difficult task given that the execution of different macro-actions may extend over different periods of time. To deal with this problem, Precup, Sutton and Singh <ref> [17, 15, 16] </ref> have developed multi-time models and applied them to planning with MDPs. The new framework allows us to represent uniformly actions on different time scales, as well as, to apply standard problem-solving procedures developed for MDPs, including value and policy iteration [1, 10]. <p> There are only a few reasonably direct ways in which macro-actions can be applied to solve MDPs. Two of them are most common. First, macro-actions can be combined with primitive actions to form an augmented MDP model. This model has been studied extensively in <ref> [17, 15, 16, 8, 9] </ref>. The second model works with macro-actions only and allow one to reduce the number of states of the underlying MDP to states connected by macro-actions. This model is called an abstract MDP model (after [9]) and was studied in [7, 9, 8, 14]. <p> In our work we focus our attention solely on the augmented model that works with both primitive actions and macro-actions. Precup, Sutton and Singh <ref> [15, 16] </ref> demonstrated empirically the advantage of adding macro-actions to the original model by speeding-up the convergence rate of value iteration when a good set of macros is used. 1 However this improvement is not always guaranteed and this also in the case when only the optimal macro-action is added to <p> Such a macro consists of multiple steps involving multiple primitive move actions. Macro-actions can be modeled by different means. For example, macro-actions can be represented as programs with arbitrary termination conditions as suggested by Precup, Sutton and Singh <ref> [17, 15, 16] </ref> or using finite state machine representation and its hierarchical extensions as proposed by Parr [13]. <p> A macro-action is simply a local policy defined for a particular region S i . Intuitively, this policy can be executed whenever an agent enters or is in the region and terminates when the agent leaves the region (if ever). Definitions in <ref> [15] </ref> or [13] are more general and can use arbitrary starting and termination conditions, and allow also non-Markovian policies. The main problem with integrating macro-actions into the MDP is that macro-actions when executed can extend over different periods of time. <p> The main problem with integrating macro-actions into the MDP is that macro-actions when executed can extend over different periods of time. The key insight of Precup, Sutton and Singh <ref> [17, 15, 16] </ref> is that one can treat a macro-action as a primitive action in the original MDP if one has an appropriate reward and transition model for the macro. They propose the following method of modeling macros. <p> The discounted transition model specifies the probability of leaving S i via a specific exit state, similarly to a standard stochastic transition matrix, with one exception: the probability is discounted according to the expected time at which that exit occurs. As demonstrated in <ref> [15, 16] </ref>, this clever addition allows the transition model to be used as a normal transition matrix in any standard MDP solution technique, such as policy or value iteration. 2 The reward model is similar, simply measuring the expected accrued reward during execution of i starting from a particular state in <p> There are only a few reasonably direct ways in which macro-actions can be applied to solve M . In our work we focus on augmented MDP model <ref> [17, 15, 16, 8, 9] </ref>, denoted M 0 , constructed by extending the action space A using a set of macro-actions, assuming that macro models are used to determine transitions and rewards associated with these new actions. <p> Because all base level actions (those in A) are present, the policy found is guaranteed to be optimal. Furthermore, the presence of macros may enhance the convergence of value iteration, as demonstrated in <ref> [15, 16] </ref>. 3 This is because the single "application" of a macro can propagate values through a large number of states and over a large period of time in a single step. <p> This behavior was demonstrated empirically in <ref> [15, 16] </ref>.
Reference: [16] <author> D. Precup, R. S. Sutton, and S. Singh. </author> <title> Theoretical results on reinforcement learning with temporally abstract behaviors. </title> <booktitle> 10th Eur. Conf. Mach. Learn., </booktitle> <address> Chemnitz, </address> <year> 1998. </year>
Reference-contexts: Integrating programs or macro-actions into the decision process is a difficult task given that the execution of different macro-actions may extend over different periods of time. To deal with this problem, Precup, Sutton and Singh <ref> [17, 15, 16] </ref> have developed multi-time models and applied them to planning with MDPs. The new framework allows us to represent uniformly actions on different time scales, as well as, to apply standard problem-solving procedures developed for MDPs, including value and policy iteration [1, 10]. <p> There are only a few reasonably direct ways in which macro-actions can be applied to solve MDPs. Two of them are most common. First, macro-actions can be combined with primitive actions to form an augmented MDP model. This model has been studied extensively in <ref> [17, 15, 16, 8, 9] </ref>. The second model works with macro-actions only and allow one to reduce the number of states of the underlying MDP to states connected by macro-actions. This model is called an abstract MDP model (after [9]) and was studied in [7, 9, 8, 14]. <p> In our work we focus our attention solely on the augmented model that works with both primitive actions and macro-actions. Precup, Sutton and Singh <ref> [15, 16] </ref> demonstrated empirically the advantage of adding macro-actions to the original model by speeding-up the convergence rate of value iteration when a good set of macros is used. 1 However this improvement is not always guaranteed and this also in the case when only the optimal macro-action is added to <p> Such a macro consists of multiple steps involving multiple primitive move actions. Macro-actions can be modeled by different means. For example, macro-actions can be represented as programs with arbitrary termination conditions as suggested by Precup, Sutton and Singh <ref> [17, 15, 16] </ref> or using finite state machine representation and its hierarchical extensions as proposed by Parr [13]. <p> The main problem with integrating macro-actions into the MDP is that macro-actions when executed can extend over different periods of time. The key insight of Precup, Sutton and Singh <ref> [17, 15, 16] </ref> is that one can treat a macro-action as a primitive action in the original MDP if one has an appropriate reward and transition model for the macro. They propose the following method of modeling macros. <p> The discounted transition model specifies the probability of leaving S i via a specific exit state, similarly to a standard stochastic transition matrix, with one exception: the probability is discounted according to the expected time at which that exit occurs. As demonstrated in <ref> [15, 16] </ref>, this clever addition allows the transition model to be used as a normal transition matrix in any standard MDP solution technique, such as policy or value iteration. 2 The reward model is similar, simply measuring the expected accrued reward during execution of i starting from a particular state in <p> There are only a few reasonably direct ways in which macro-actions can be applied to solve M . In our work we focus on augmented MDP model <ref> [17, 15, 16, 8, 9] </ref>, denoted M 0 , constructed by extending the action space A using a set of macro-actions, assuming that macro models are used to determine transitions and rewards associated with these new actions. <p> Because all base level actions (those in A) are present, the policy found is guaranteed to be optimal. Furthermore, the presence of macros may enhance the convergence of value iteration, as demonstrated in <ref> [15, 16] </ref>. 3 This is because the single "application" of a macro can propagate values through a large number of states and over a large period of time in a single step. <p> This behavior was demonstrated empirically in <ref> [15, 16] </ref>.
Reference: [17] <author> R. S. Sutton. </author> <title> TD models: Modeling the world at a mixture of time scales. </title> <booktitle> In ICML-95, </booktitle> <address> pp.531-539, Lake Tahoe, </address> <year> 1995. </year>
Reference-contexts: Integrating programs or macro-actions into the decision process is a difficult task given that the execution of different macro-actions may extend over different periods of time. To deal with this problem, Precup, Sutton and Singh <ref> [17, 15, 16] </ref> have developed multi-time models and applied them to planning with MDPs. The new framework allows us to represent uniformly actions on different time scales, as well as, to apply standard problem-solving procedures developed for MDPs, including value and policy iteration [1, 10]. <p> There are only a few reasonably direct ways in which macro-actions can be applied to solve MDPs. Two of them are most common. First, macro-actions can be combined with primitive actions to form an augmented MDP model. This model has been studied extensively in <ref> [17, 15, 16, 8, 9] </ref>. The second model works with macro-actions only and allow one to reduce the number of states of the underlying MDP to states connected by macro-actions. This model is called an abstract MDP model (after [9]) and was studied in [7, 9, 8, 14]. <p> Such a macro consists of multiple steps involving multiple primitive move actions. Macro-actions can be modeled by different means. For example, macro-actions can be represented as programs with arbitrary termination conditions as suggested by Precup, Sutton and Singh <ref> [17, 15, 16] </ref> or using finite state machine representation and its hierarchical extensions as proposed by Parr [13]. <p> The main problem with integrating macro-actions into the MDP is that macro-actions when executed can extend over different periods of time. The key insight of Precup, Sutton and Singh <ref> [17, 15, 16] </ref> is that one can treat a macro-action as a primitive action in the original MDP if one has an appropriate reward and transition model for the macro. They propose the following method of modeling macros. <p> There are only a few reasonably direct ways in which macro-actions can be applied to solve M . In our work we focus on augmented MDP model <ref> [17, 15, 16, 8, 9] </ref>, denoted M 0 , constructed by extending the action space A using a set of macro-actions, assuming that macro models are used to determine transitions and rewards associated with these new actions.
References-found: 17

