URL: http://www.cs.huji.ac.il/~rousso/TRs/ego-mtn.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~rousso/
Root-URL: http://www.cs.huji.ac.il
Email: Email: fmichalb, pelegg@cs.huji.ac.il  
Title: Recovery of Ego-Motion Using Image Stabilization method avoids the inherent problems in the computation of
Author: Michal Irani Benny Rousso Shmuel Peleg 
Note: The presented  correspondence. This research has been sponsored by the U.S. Office of Naval Research under Grant N00014-93-1-1202, R&T Project Code 4424341|01.  
Date: August 1993  
Address: 91904 Jerusalem, ISRAEL  
Affiliation: Institute of Computer Science The Hebrew University of Jerusalem  
Abstract: A robust method is introduced for computing the camera motion (the ego-motion) in a static scene. The method is based on detecting a single image region and computing its 2D motion in the image plane directly from image intensities. The detected 2D motion of this image region is used to register the images (so that the detected image region appears stationary). The resulting displacement field for the entire scene in such registered frames is affected only by the 3D translation of the camera. Canceling the effects of the 3D rotation of the camera by using such 2D image registration simplifies the computation of the translation. The 3D translation of the camera is computed by finding the focus-of-expansion in the translation-only set of registered frames. This step is followed by computing the 3D rotation to complete the computation of the ego-motion. 
Abstract-found: 1
Intro-found: 1
Reference: [AD92] <author> Y. Aloimonos and Z. Duric. </author> <title> Active egomotion estimation: A qualitative approach. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 497-510, </pages> <address> Santa Margarita Ligure, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: introduction of a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from successive frames [OFT87, Hor90, COK93]. Both approaches depend on the accuracy of the pre-processing stage, which can not always be assured [WHA89].
Reference: [Adi85] <author> G. Adiv. </author> <title> Determining three-dimensional motion and structure from optical flow generated by several moving objects. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(4) </volume> <pages> 384-401, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: To overcome this difficulty, additional constraints are usually added to the motion model or to the environment model. Such constraints include the introduction of a regularization term [HS81], assuming a limited model of the world <ref> [Adi85] </ref>, restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93], or from the <p> introduction of a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from successive frames [OFT87, Hor90, COK93]. Both approaches depend on the accuracy of the pre-processing stage, which can not always be assured [WHA89]. <p> An imaged planar surface in the 3D scene produces the required 2D image region (but the scene needs not be piecewise linear, as in <ref> [Adi85] </ref>). Most indoor scenes have a planar surface (e.g., walls, floor, pictures, windows, etc.), and in outdoor scenes the ground or any distant object can serve as a planar surface. <p> When the field of view is not very large and the rotation is relatively small <ref> [Adi85] </ref>, a 3D motion of the camera between two image frames creates a 2D displacement (u; v) of an image point (x; y) in the image plane, which can be expressed by [LH84, BAHH92]: " v = f c ( T X Z + y Z x 2 Y f c <p> rearranging the terms, we get: 1 = A B X A Z which can be rewritten using Equation (1) as: 1 = ff + fi x + fl y (4) where: ff = 1 A ; fi = B f c A : In a similar manipulation to that in <ref> [Adi85] </ref>, substituting (4) in (3) yields: " v = a + b x + c y + g x 2 + h xy # where: a = f c ffT X f c Y e = Z f c fiT Y c = Z f c flT X g = Y <p> Deriving Err (t) (u; v) with respect to the motion parameters and setting to zero yields six linear equations in the six unknowns: a, b, c, d, e, f [BBH + 91, BAHH92]. 3. A Moving planar surface (a pseudo 2D projective transformation): 8 parameters <ref> [Adi85, BAHH92] </ref> (see Section 2.1, Equation (5)), u (x; y; t) = a + bx + cy + gx 2 +hxy, v (x; y; t) = d+ex+f y +gxy +hy 2 .
Reference: [Adi89] <author> G. Adiv. </author> <title> Inherent ambiguities in recovering 3D motion and structure from a noisy flow field. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 11 </volume> <pages> 477-489, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: Hanna [Han91] suggests an iterative method for the most general case, but relies on a reasonably accurate initial guess. Taalebinezhaad [Taa92] has a complete mathematical formulation, but relies on computations at a single image point, which can be highly unstable. 3D rotations and translations induce similar 2D image motions <ref> [Adi89, DN93, DMRS88] </ref> which cause ambiguities in their interpretation, especially in the presence of noise. <p> Moreover, the problem of recovering the 3D camera motion directly from the image motion field is an ill-conditioned problem, since small errors in the 2D flow field usually result in large perturbations in the 3D motion <ref> [WHA89, Adi89] </ref>. To overcome the difficulties and ambiguities in the computation of the ego-motion, we introduce the following scheme: The first frame is warped towards the second frame using the computed 2D image motion at the detected image region.
Reference: [BA87] <author> J.R. Bergen and E.H. Adelson. </author> <title> Hierarchical, computationally efficient motion estimation algorithm. </title> <journal> J. Opt. Soc. Am. A., </journal> <volume> 4:35, </volume> <year> 1987. </year>
Reference-contexts: Our experience shows that the computed parameters g and h are not as reliable as the other six parameters (a; b; c; d; e; f ), as g and h are second order terms. 4.3 The Motion Computation Framework A multiresolution gradient-based iterative framework <ref> [BA87, BBH + 91, BBHP90] </ref> is used to compute the 2D motion parameters. The basic components of this framework are: * Construction of a Gaussian pyramid [Ros84], where the images are represented in multiple resolutions. 19 * Starting at the lowest resolution level: 1.
Reference: [BAHH92] <author> J.R. Bergen, P. Anandan, K.J. Hanna, and R. Hingorani. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 237-252, </pages> <address> Santa Margarita Ligure, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: When the field of view is not very large and the rotation is relatively small [Adi85], a 3D motion of the camera between two image frames creates a 2D displacement (u; v) of an image point (x; y) in the image plane, which can be expressed by <ref> [LH84, BAHH92] </ref>: " v = f c ( T X Z + y Z x 2 Y f c Z X ) x Z + y T Z f c + y 2 X # It is important to note the following points from Equation (3): 5 The coordinate system (X; <p> flow at (x; y): w (x; y) = cond](x; y) jf 2 (x + u Reg ; y + v Reg ) f Reg where cond](x; y) is the a-priori reliability of the optical flow at (x; y), determined by the condition number of the two well-known optical flow equations <ref> [LK81, BAHH92] </ref>, and jf 2 (x + u Reg ; y + v Reg ) f 1 (x; y)j is the posteriori reliability of the computed optical flow at (x; y), determined by the intensity difference between the source location of (x; y) in image f Reg 1 and its computed <p> In order to minimize Err (t) (u; v), its derivatives with respect to a and d are set to zero. This yields two linear equations in the two unknowns, a and d. Those are the two well-known optical flow equations <ref> [LK81, BAHH92] </ref>, where every small window is assumed to have a single translation in the image plane. In this translation model, the entire object is assumed to have a single translation in the image plane. 2. <p> Deriving Err (t) (u; v) with respect to the motion parameters and setting to zero yields six linear equations in the six unknowns: a, b, c, d, e, f <ref> [BBH + 91, BAHH92] </ref>. 3. A Moving planar surface (a pseudo 2D projective transformation): 8 parameters [Adi85, BAHH92] (see Section 2.1, Equation (5)), u (x; y; t) = a + bx + cy + gx 2 +hxy, v (x; y; t) = d+ex+f y +gxy +hy 2 . <p> Deriving Err (t) (u; v) with respect to the motion parameters and setting to zero yields six linear equations in the six unknowns: a, b, c, d, e, f [BBH + 91, BAHH92]. 3. A Moving planar surface (a pseudo 2D projective transformation): 8 parameters <ref> [Adi85, BAHH92] </ref> (see Section 2.1, Equation (5)), u (x; y; t) = a + bx + cy + gx 2 +hxy, v (x; y; t) = d+ex+f y +gxy +hy 2 .
Reference: [BBH + 91] <author> J.R. Bergen, P.J. Burt, K. Hanna, R. Hingorani, P. Jeanne, and S. Peleg. </author> <title> Dynamic multiple-motion computation. In Y.A. </title> <editor> Feldman and A. Bruck-stein, editors, </editor> <booktitle> Artificial Intelligence and Computer Vision: Proceedings of the Israeli Conference, </booktitle> <pages> pages 147-156. </pages> <publisher> Elsevier, </publisher> <year> 1991. </year>
Reference-contexts: It is a coarse-to-fine technique in the sense that at first only a robust 2D image motion is extracted, and later this 2D motion is used to simplify the computation of the 3D ego-motion. We use previously developed methods <ref> [BBH + 91, IRP92, IRP93] </ref> to detect and track a single image region and to compute its 2D parametric image motion. <p> Image motion is highly chaotic. b) Average of the stabilized sequence. Image motion represents camera translations only. Camera rotations were cancelled (as if the camera has been mounted on a gyroscopic stabilizer). 4 Computing 2D Motion of a Planar Surface We use previously developed methods <ref> [BBH + 91, IRP92, IRP93] </ref> in order to detect an image region corresponding to a planar surface in the scene with its pseudo 2D projective transformation. These methods treated dynamic scenes, in which there were assumed to be multiple moving planar objects. <p> Deriving Err (t) (u; v) with respect to the motion parameters and setting to zero yields six linear equations in the six unknowns: a, b, c, d, e, f <ref> [BBH + 91, BAHH92] </ref>. 3. A Moving planar surface (a pseudo 2D projective transformation): 8 parameters [Adi85, BAHH92] (see Section 2.1, Equation (5)), u (x; y; t) = a + bx + cy + gx 2 +hxy, v (x; y; t) = d+ex+f y +gxy +hy 2 . <p> Our experience shows that the computed parameters g and h are not as reliable as the other six parameters (a; b; c; d; e; f ), as g and h are second order terms. 4.3 The Motion Computation Framework A multiresolution gradient-based iterative framework <ref> [BA87, BBH + 91, BBHP90] </ref> is used to compute the 2D motion parameters. The basic components of this framework are: * Construction of a Gaussian pyramid [Ros84], where the images are represented in multiple resolutions. 19 * Starting at the lowest resolution level: 1.
Reference: [BBHP90] <author> J.R. Bergen, P.J. Burt, R. Hingorani, and S. Peleg. </author> <title> Computing two motions from three frames. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 27-32, </pages> <address> Osaka, Japan, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: Our experience shows that the computed parameters g and h are not as reliable as the other six parameters (a; b; c; d; e; f ), as g and h are second order terms. 4.3 The Motion Computation Framework A multiresolution gradient-based iterative framework <ref> [BA87, BBH + 91, BBHP90] </ref> is used to compute the 2D motion parameters. The basic components of this framework are: * Construction of a Gaussian pyramid [Ros84], where the images are represented in multiple resolutions. 19 * Starting at the lowest resolution level: 1.
Reference: [BHK91] <author> P.J. Burt, R. Hingorani, and R.J. Kolczynski. </author> <title> Mechanisms for isolating component patterns in the sequential analysis of multiple motion. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 187-193, </pages> <address> Princeton, New Jersey, </address> <month> Octo-ber </month> <year> 1991. </year>
Reference-contexts: For more details refer to [IRP92, IRP93]. 4.1 Detecting Multiple Moving Planar Objects 2D motion estimation is difficult when the region of support of an object in the image is not known, which is the common case. It was shown in <ref> [BHK91] </ref> that the motion parameters of a single translating object in the image plane can be recovered accurately by applying the 2D motion computation framework described in Section 4.3 to the entire image, using a 2D translation motion model (see Section 4.2). <p> This object is called the dominant object, and its 2D motion the dominant 2D motion. Thorough analysis of hierarchical 2D translation estimation is found in <ref> [BHK91] </ref>. In [IRP92, IRP93] this method was extended to compute higher order 2D motions (2D affine, 2D projective) of a single planar object among differently moving objects. A segmentation step was added to the process, which segments out the region corresponding to the computed dominant 2D motion.
Reference: [COK93] <author> R. Chipolla, Y. Okamoto, and Y. Kuno. </author> <title> Robust structure from motion using motion paralax. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 374-382, </pages> <address> Berlin, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93], or from the correspondence of distinguished features (points, lines, contours) previously extracted from successive frames <ref> [OFT87, Hor90, COK93] </ref>. Both approaches depend on the accuracy of the pre-processing stage, which can not always be assured [WHA89]. Increasing the temporal region to more than two frames improves the accuracy of the computed motion. <p> Based on this observation [LHP80], methods using motion parallax were constructed to obtain the 3D motion of the camera <ref> [LR83, LHP80, Lee88, LC93, COK93] </ref>. <p> This work relies on prior point extraction and accurate correspondence because the data is very sparse. <ref> [LC93, COK93] </ref> locally compute a 2-D affine motion from three non-colinear points, and use a fourth point which is non-coplanar with the other three points to estimate its local motion parallax. <p> The 2D image region registration technique used in this work allows easy decou-pling of the translational and rotational motions, as only motion parallax information remains after the registration. As opposed to other methods using motion parallax <ref> [LR83, LHP80, Lee88, LC93, COK93] </ref>, our method does not rely on 2D motion information computed near depth discontinuities, where it is inaccurate, but on motion computed over an entire image region.
Reference: [DF90] <author> R. Deriche and O. Faugeras. </author> <title> Tracking line segments. </title> <booktitle> In Proc. 1st European Conference on Computer Vision, </booktitle> <pages> pages 259-268, </pages> <address> Antibes, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: Such constraints include the introduction of a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence <ref> [DF90] </ref>. 3D motion is often estimated from the optical or normal flow field derived between two frames [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93], or from the correspondence of distinguished features (points, lines, contours) previously extracted from successive frames [OFT87, Hor90, COK93]. <p> These methods assume motion constancy in the temporal regions, i.e. motion should remain uniform in the analyzed sequence. In feature based methods, features are tracked over a larger time interval using recursive temporal filtering <ref> [DF90] </ref>. The issue of extracting good features and overcoming occlusions still remains. Methods for computing the ego-motion directly from image intensities were also suggested [Han91, HW88, Taa92]. Horn and Weldon [HW88] deal only with restricted cases (pure translation, pure rotation, or a general motion with known depth).
Reference: [DMRS88] <author> R. Dutta, R. Manmatha, E.M. Riseman, and M.A. Snyder. </author> <title> Issues in extracting motion parameters and depth from approximate translational motion. </title> <booktitle> In DARPA IU Workshop, </booktitle> <pages> pages 945-960, </pages> <year> 1988. </year>
Reference-contexts: Hanna [Han91] suggests an iterative method for the most general case, but relies on a reasonably accurate initial guess. Taalebinezhaad [Taa92] has a complete mathematical formulation, but relies on computations at a single image point, which can be highly unstable. 3D rotations and translations induce similar 2D image motions <ref> [Adi89, DN93, DMRS88] </ref> which cause ambiguities in their interpretation, especially in the presence of noise.
Reference: [DN93] <author> K. Daniilidis and H.-H. Nagel. </author> <title> The coupling of rotation and translation in motion estimation of planar surfaces. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 188-193, </pages> <address> New York, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Hanna [Han91] suggests an iterative method for the most general case, but relies on a reasonably accurate initial guess. Taalebinezhaad [Taa92] has a complete mathematical formulation, but relies on computations at a single image point, which can be highly unstable. 3D rotations and translations induce similar 2D image motions <ref> [Adi89, DN93, DMRS88] </ref> which cause ambiguities in their interpretation, especially in the presence of noise.
Reference: [DRD93] <author> Z. Duric, A. Rosenfeld, and L. Davis. </author> <title> Egomotion analysis based on the Frenet-Serret motion model. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 703-712, </pages> <address> Berlin, </address> <month> May </month> <year> 1993. </year> <month> 21 </month>
Reference-contexts: introduction of a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from successive frames [OFT87, Hor90, COK93]. Both approaches depend on the accuracy of the pre-processing stage, which can not always be assured [WHA89].
Reference: [FJ90] <author> D.J. Fleet and A.D. Jepson. </author> <title> Computation of component image velocity from local phase information. </title> <journal> International Journal of Computer Vision, </journal> <volume> 5(1) </volume> <pages> 77-104, </pages> <year> 1990. </year>
Reference-contexts: Increasing the temporal region to more than two frames improves the accuracy of the computed motion. Methods for estimating local image velocities with large temporal regions have been introduced using a combined spatio-temporal analysis <ref> [FJ90, Hee88, SM90] </ref>. These methods assume motion constancy in the temporal regions, i.e. motion should remain uniform in the analyzed sequence. In feature based methods, features are tracked over a larger time interval using recursive temporal filtering [DF90]. The issue of extracting good features and overcoming occlusions still remains.
Reference: [GU91] <author> R. Guissin and S. Ullman. </author> <title> Direct computation of the focus of expansion from velocity field measurements. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 146-155, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: introduction of a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from successive frames [OFT87, Hor90, COK93]. Both approaches depend on the accuracy of the pre-processing stage, which can not always be assured [WHA89].
Reference: [HA91] <author> L. Huang and Y. Aloimonos. </author> <title> Relative depth from motion using normal flow: An active and purposive solution. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 196-204, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: introduction of a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from successive frames [OFT87, Hor90, COK93]. Both approaches depend on the accuracy of the pre-processing stage, which can not always be assured [WHA89].
Reference: [Han91] <author> K. Hanna. </author> <title> Direct multi-resolution estimation of ego-motion and structure from motion. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 156-162, </pages> <address> Prince-ton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: In feature based methods, features are tracked over a larger time interval using recursive temporal filtering [DF90]. The issue of extracting good features and overcoming occlusions still remains. Methods for computing the ego-motion directly from image intensities were also suggested <ref> [Han91, HW88, Taa92] </ref>. Horn and Weldon [HW88] deal only with restricted cases (pure translation, pure rotation, or a general motion with known depth). Hanna [Han91] suggests an iterative method for the most general case, but relies on a reasonably accurate initial guess. <p> The issue of extracting good features and overcoming occlusions still remains. Methods for computing the ego-motion directly from image intensities were also suggested [Han91, HW88, Taa92]. Horn and Weldon [HW88] deal only with restricted cases (pure translation, pure rotation, or a general motion with known depth). Hanna <ref> [Han91] </ref> suggests an iterative method for the most general case, but relies on a reasonably accurate initial guess. <p> Once the 3D motion parameters of the camera were computed, the 3D scene structure can be reconstructed using a scheme similar to that suggested in <ref> [Han91] </ref>. Correspondence between small image patches (currently 5 fi 5 pixels) is computed only on the radial line emerging from the FOE. The depth is computed from the magnitude of these displacements.
Reference: [Hee88] <author> D.J. Heeger. </author> <title> Optical flow using spatiotemporal filters. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1 </volume> <pages> 279-302, </pages> <year> 1988. </year>
Reference-contexts: Increasing the temporal region to more than two frames improves the accuracy of the computed motion. Methods for estimating local image velocities with large temporal regions have been introduced using a combined spatio-temporal analysis <ref> [FJ90, Hee88, SM90] </ref>. These methods assume motion constancy in the temporal regions, i.e. motion should remain uniform in the analyzed sequence. In feature based methods, features are tracked over a larger time interval using recursive temporal filtering [DF90]. The issue of extracting good features and overcoming occlusions still remains.
Reference: [HJ90] <author> D.J. Heeger and A. Jepson. </author> <title> Simple method for computing 3d motion and depth. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 96-100, </pages> <year> 1990. </year>
Reference-contexts: introduction of a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from successive frames [OFT87, Hor90, COK93]. Both approaches depend on the accuracy of the pre-processing stage, which can not always be assured [WHA89].
Reference: [Hor90] <author> B.K.P. Horn. </author> <title> Relative orientation. </title> <journal> International Journal of Computer Vision, </journal> <volume> 4(1) </volume> <pages> 58-78, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93], or from the correspondence of distinguished features (points, lines, contours) previously extracted from successive frames <ref> [OFT87, Hor90, COK93] </ref>. Both approaches depend on the accuracy of the pre-processing stage, which can not always be assured [WHA89]. Increasing the temporal region to more than two frames improves the accuracy of the computed motion.
Reference: [HS81] <author> B.K.P. Horn and B.G. Schunck. </author> <title> Determining optical flow. </title> <journal> Artificial Intelligence, </journal> <volume> 17 </volume> <pages> 185-203, </pages> <year> 1981. </year>
Reference-contexts: To overcome this difficulty, additional constraints are usually added to the motion model or to the environment model. Such constraints include the introduction of a regularization term <ref> [HS81] </ref>, assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames [Adi85, LR83, HJ90, GU91, JH91, Sun91, <p> Equations (16) and (17) yield the well-known constraint <ref> [HS81] </ref>: uI x + vI y + I t = 0: (18) We look for a 2D motion (u; v) which minimize the error function at Frame t in the region of analysis R: Err (t) (u; v) = (x;y)2R We perform the error minimization over the parameters of one of
Reference: [HW88] <author> B.K.P. Horn and E.J. Weldon. </author> <title> Direct methods for recovering motion. </title> <journal> International Journal of Computer Vision, </journal> <volume> 2(1) </volume> <pages> 51-76, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: To overcome this difficulty, additional constraints are usually added to the motion model or to the environment model. Such constraints include the introduction of a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions <ref> [HW88] </ref>, or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93], or from the correspondence of distinguished features (points, lines, contours) <p> In feature based methods, features are tracked over a larger time interval using recursive temporal filtering [DF90]. The issue of extracting good features and overcoming occlusions still remains. Methods for computing the ego-motion directly from image intensities were also suggested <ref> [Han91, HW88, Taa92] </ref>. Horn and Weldon [HW88] deal only with restricted cases (pure translation, pure rotation, or a general motion with known depth). Hanna [Han91] suggests an iterative method for the most general case, but relies on a reasonably accurate initial guess. <p> In feature based methods, features are tracked over a larger time interval using recursive temporal filtering [DF90]. The issue of extracting good features and overcoming occlusions still remains. Methods for computing the ego-motion directly from image intensities were also suggested [Han91, HW88, Taa92]. Horn and Weldon <ref> [HW88] </ref> deal only with restricted cases (pure translation, pure rotation, or a general motion with known depth). Hanna [Han91] suggests an iterative method for the most general case, but relies on a reasonably accurate initial guess.
Reference: [IRP92] <author> M. Irani, B. Rousso, and S. Peleg. </author> <title> Detecting and tracking multiple moving objects using temporal integration. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 282-287, </pages> <address> Santa Margarita Ligure, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: It is a coarse-to-fine technique in the sense that at first only a robust 2D image motion is extracted, and later this 2D motion is used to simplify the computation of the 3D ego-motion. We use previously developed methods <ref> [BBH + 91, IRP92, IRP93] </ref> to detect and track a single image region and to compute its 2D parametric image motion. <p> Image motion is highly chaotic. b) Average of the stabilized sequence. Image motion represents camera translations only. Camera rotations were cancelled (as if the camera has been mounted on a gyroscopic stabilizer). 4 Computing 2D Motion of a Planar Surface We use previously developed methods <ref> [BBH + 91, IRP92, IRP93] </ref> in order to detect an image region corresponding to a planar surface in the scene with its pseudo 2D projective transformation. These methods treated dynamic scenes, in which there were assumed to be multiple moving planar objects. <p> When the scene is not piecewise planar, but contains planar surfaces, the 2D detection algorithm still detects the image motion of its planar regions. This section describes very briefly the technique for detecting differently moving planar objects and their 2D motion parameters. For more details refer to <ref> [IRP92, IRP93] </ref>. 4.1 Detecting Multiple Moving Planar Objects 2D motion estimation is difficult when the region of support of an object in the image is not known, which is the common case. <p> This object is called the dominant object, and its 2D motion the dominant 2D motion. Thorough analysis of hierarchical 2D translation estimation is found in [BHK91]. In <ref> [IRP92, IRP93] </ref> this method was extended to compute higher order 2D motions (2D affine, 2D projective) of a single planar object among differently moving objects. A segmentation step was added to the process, which segments out the region corresponding to the computed dominant 2D motion. <p> More details are found in <ref> [IRP92] </ref>. Temporal integration over longer sequences is used in order to improve the segmentation and the accuracy of the 2D motion parameters [IRP92, IRP93]. <p> More details are found in [IRP92]. Temporal integration over longer sequences is used in order to improve the segmentation and the accuracy of the 2D motion parameters <ref> [IRP92, IRP93] </ref>. A temporally integrated image is obtained by averaging several frames of the sequence after registration with respect to the 2D image motion of the tracked planar surface.
Reference: [IRP93] <author> M. Irani, B. Rousso, and S. Peleg. </author> <note> Computing occluding and transparent motions. To appear in International Journal of Computer Vision, </note> <month> February </month> <year> 1993. </year>
Reference-contexts: It is a coarse-to-fine technique in the sense that at first only a robust 2D image motion is extracted, and later this 2D motion is used to simplify the computation of the 3D ego-motion. We use previously developed methods <ref> [BBH + 91, IRP92, IRP93] </ref> to detect and track a single image region and to compute its 2D parametric image motion. <p> Image motion is highly chaotic. b) Average of the stabilized sequence. Image motion represents camera translations only. Camera rotations were cancelled (as if the camera has been mounted on a gyroscopic stabilizer). 4 Computing 2D Motion of a Planar Surface We use previously developed methods <ref> [BBH + 91, IRP92, IRP93] </ref> in order to detect an image region corresponding to a planar surface in the scene with its pseudo 2D projective transformation. These methods treated dynamic scenes, in which there were assumed to be multiple moving planar objects. <p> When the scene is not piecewise planar, but contains planar surfaces, the 2D detection algorithm still detects the image motion of its planar regions. This section describes very briefly the technique for detecting differently moving planar objects and their 2D motion parameters. For more details refer to <ref> [IRP92, IRP93] </ref>. 4.1 Detecting Multiple Moving Planar Objects 2D motion estimation is difficult when the region of support of an object in the image is not known, which is the common case. <p> This object is called the dominant object, and its 2D motion the dominant 2D motion. Thorough analysis of hierarchical 2D translation estimation is found in [BHK91]. In <ref> [IRP92, IRP93] </ref> this method was extended to compute higher order 2D motions (2D affine, 2D projective) of a single planar object among differently moving objects. A segmentation step was added to the process, which segments out the region corresponding to the computed dominant 2D motion. <p> More details are found in [IRP92]. Temporal integration over longer sequences is used in order to improve the segmentation and the accuracy of the 2D motion parameters <ref> [IRP92, IRP93] </ref>. A temporally integrated image is obtained by averaging several frames of the sequence after registration with respect to the 2D image motion of the tracked planar surface.
Reference: [JH91] <author> A.D. </author> <title> Jepson and D.J. Heeger. A fast subspace algorithm for recovering rigid motion. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 124-131, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: introduction of a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from successive frames [OFT87, Hor90, COK93]. Both approaches depend on the accuracy of the pre-processing stage, which can not always be assured [WHA89].
Reference: [LC93] <author> J. Lawn and R. Chipolla. </author> <title> Epipole estimation using affine motion parallax. </title> <type> Technical Report CUED/F-INFENG/TR-138, </type> <address> Cambridge, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Based on this observation [LHP80], methods using motion parallax were constructed to obtain the 3D motion of the camera <ref> [LR83, LHP80, Lee88, LC93, COK93] </ref>. <p> This work relies on prior point extraction and accurate correspondence because the data is very sparse. <ref> [LC93, COK93] </ref> locally compute a 2-D affine motion from three non-colinear points, and use a fourth point which is non-coplanar with the other three points to estimate its local motion parallax. <p> The 2D image region registration technique used in this work allows easy decou-pling of the translational and rotational motions, as only motion parallax information remains after the registration. As opposed to other methods using motion parallax <ref> [LR83, LHP80, Lee88, LC93, COK93] </ref>, our method does not rely on 2D motion information computed near depth discontinuities, where it is inaccurate, but on motion computed over an entire image region.
Reference: [Lee88] <author> C.H. Lee. </author> <title> Structure and motion from two perspective views via planar patch. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 158-164, </pages> <year> 1988. </year>
Reference-contexts: Based on this observation [LHP80], methods using motion parallax were constructed to obtain the 3D motion of the camera <ref> [LR83, LHP80, Lee88, LC93, COK93] </ref>. <p> The optical flow, however, is most inaccurate near depth discontinuities, which is where motion parallax occurs. These methods therefore suffer from these inaccuracies. <ref> [Lee88] </ref> shows that given four coplanar points in an object, and two additional points outside that plane, structure and motion of that object can be recovered from two perspective views using the motion parallax at the two additional points. <p> The 2D image region registration technique used in this work allows easy decou-pling of the translational and rotational motions, as only motion parallax information remains after the registration. As opposed to other methods using motion parallax <ref> [LR83, LHP80, Lee88, LC93, COK93] </ref>, our method does not rely on 2D motion information computed near depth discontinuities, where it is inaccurate, but on motion computed over an entire image region.
Reference: [LH84] <author> H.C. Longuet-Higgins. </author> <title> Visual ambiguity of a moving plane. </title> <journal> Proceedings of The Royal Society of London B, </journal> <volume> 223 </volume> <pages> 165-175, </pages> <year> 1984. </year> <month> 22 </month>
Reference-contexts: It is important to emphasize that the 3D camera motion cannot be recovered solely from the 2D parametric image motion of a single image region, as there are a couple of such 3D interpretations <ref> [LH84] </ref>. It was shown that 3D motion of a planar surface can be computed from its 2D affine motion in the image and from the motion derivatives [MB92], but the use of motion derivatives might introduce sensitivity to noise. <p> When the field of view is not very large and the rotation is relatively small [Adi85], a 3D motion of the camera between two image frames creates a 2D displacement (u; v) of an image point (x; y) in the image plane, which can be expressed by <ref> [LH84, BAHH92] </ref>: " v = f c ( T X Z + y Z x 2 Y f c Z X ) x Z + y T Z f c + y 2 X # It is important to note the following points from Equation (3): 5 The coordinate system (X; <p> This case, however, cannot be resolved by humans either, due to a visual ambiguity <ref> [LH84] </ref>. This case can be identified by noticing that the image registration according to the 2D motion parameters of the detected planar surface brings the entire image into registration. 2. T = 0, i.e., the motion of the camera is purely rotational.
Reference: [LHP80] <author> H.C. Longuet-Higgins and K. Prazdny. </author> <title> The interpretation of a moving retinal image. </title> <journal> Proceedings of The Royal Society of London B, </journal> <volume> 208 </volume> <pages> 385-397, </pages> <year> 1980. </year>
Reference-contexts: At depth discontinuities, however, it is much easier to distinguish between the effects of 3D rotations and translations, as the 2D motion of neighboring pixels at different depths will have very similar rotational components, but very different translational components. Based on this observation <ref> [LHP80] </ref>, methods using motion parallax were constructed to obtain the 3D motion of the camera [LR83, LHP80, Lee88, LC93, COK93]. <p> Based on this observation [LHP80], methods using motion parallax were constructed to obtain the 3D motion of the camera <ref> [LR83, LHP80, Lee88, LC93, COK93] </ref>. <p> Based on this observation [LHP80], methods using motion parallax were constructed to obtain the 3D motion of the camera [LR83, LHP80, Lee88, LC93, COK93]. In <ref> [LR83, LHP80] </ref> difference vectors are computed from the optical flow to cancel the effects of rotations, and the intersection of these difference vectors is computed to yield the focus of expansion (the FOE, which is the intersection point of the direction of the camera translation with the image 2 plane). <p> The 2D image region registration technique used in this work allows easy decou-pling of the translational and rotational motions, as only motion parallax information remains after the registration. As opposed to other methods using motion parallax <ref> [LR83, LHP80, Lee88, LC93, COK93] </ref>, our method does not rely on 2D motion information computed near depth discontinuities, where it is inaccurate, but on motion computed over an entire image region.
Reference: [LK81] <author> B.D. Lucas and T. Kanade. </author> <title> An iterative image registration technique with an application to stereo vision. </title> <booktitle> In Image Understanding Workshop, </booktitle> <pages> pages 121-130, </pages> <year> 1981. </year>
Reference-contexts: flow at (x; y): w (x; y) = cond](x; y) jf 2 (x + u Reg ; y + v Reg ) f Reg where cond](x; y) is the a-priori reliability of the optical flow at (x; y), determined by the condition number of the two well-known optical flow equations <ref> [LK81, BAHH92] </ref>, and jf 2 (x + u Reg ; y + v Reg ) f 1 (x; y)j is the posteriori reliability of the computed optical flow at (x; y), determined by the intensity difference between the source location of (x; y) in image f Reg 1 and its computed <p> In order to minimize Err (t) (u; v), its derivatives with respect to a and d are set to zero. This yields two linear equations in the two unknowns, a and d. Those are the two well-known optical flow equations <ref> [LK81, BAHH92] </ref>, where every small window is assumed to have a single translation in the image plane. In this translation model, the entire object is assumed to have a single translation in the image plane. 2.
Reference: [LR83] <author> D.T. Lawton and J.H. Rieger. </author> <title> The use of difference fields in processing sensor motion. </title> <booktitle> In DARPA IU Workshop, </booktitle> <pages> pages 78-83, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: introduction of a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from successive frames [OFT87, Hor90, COK93]. Both approaches depend on the accuracy of the pre-processing stage, which can not always be assured [WHA89]. <p> Based on this observation [LHP80], methods using motion parallax were constructed to obtain the 3D motion of the camera <ref> [LR83, LHP80, Lee88, LC93, COK93] </ref>. <p> Based on this observation [LHP80], methods using motion parallax were constructed to obtain the 3D motion of the camera [LR83, LHP80, Lee88, LC93, COK93]. In <ref> [LR83, LHP80] </ref> difference vectors are computed from the optical flow to cancel the effects of rotations, and the intersection of these difference vectors is computed to yield the focus of expansion (the FOE, which is the intersection point of the direction of the camera translation with the image 2 plane). <p> The 2D image region registration technique used in this work allows easy decou-pling of the translational and rotational motions, as only motion parallax information remains after the registration. As opposed to other methods using motion parallax <ref> [LR83, LHP80, Lee88, LC93, COK93] </ref>, our method does not rely on 2D motion information computed near depth discontinuities, where it is inaccurate, but on motion computed over an entire image region. <p> To locate the FOE, the optical flow between the registered frames is computed, and the FOE is located using a search method similar to that described in <ref> [LR83] </ref>. Candidates for the FOE are sampled over a half sphere and projected onto the image plane. For each such candidate, an error measure is computed. <p> The search process is repeated by refining the sampling (on the sphere) around good FOE candidates. After a few refinement iterations, the FOE is taken to be the candidate with the smallest error. (In <ref> [LR83] </ref> the error is taken to be the sum of the magnitude of the error angles, which we found to give less accurate results). Since the problem of locating the FOE in a purely translational flow field is a highly overdetermined problem, the computed flow field need not be accurate.
Reference: [MB92] <author> F. Meyer and P. Bouthemy. </author> <title> Estimation of time-to-collision maps from first order motion models and normal flows. </title> <booktitle> In International Conference on Pattern Recognition, </booktitle> <pages> pages 78-82, </pages> <address> The Hague, </address> <year> 1992. </year>
Reference-contexts: It was shown that 3D motion of a planar surface can be computed from its 2D affine motion in the image and from the motion derivatives <ref> [MB92] </ref>, but the use of motion derivatives might introduce sensitivity to noise. Moreover, the problem of recovering the 3D camera motion directly from the image motion field is an ill-conditioned problem, since small errors in the 2D flow field usually result in large perturbations in the 3D motion [WHA89, Adi89].
Reference: [NL91] <author> S. Negahdaripour and S. Lee. </author> <title> Motion recovery from image sequences using first-order optical flow information. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 132-139, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: introduction of a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from successive frames [OFT87, Hor90, COK93]. Both approaches depend on the accuracy of the pre-processing stage, which can not always be assured [WHA89].
Reference: [OFT87] <author> F. Lustman O.D. Faugeras and G. Toscani. </author> <title> Motion and structure from motion from point and line matching. </title> <booktitle> In Proc. 1st International Conference on Computer Vision, </booktitle> <pages> pages 25-34, </pages> <address> London, </address> <year> 1987. </year>
Reference-contexts: temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93], or from the correspondence of distinguished features (points, lines, contours) previously extracted from successive frames <ref> [OFT87, Hor90, COK93] </ref>. Both approaches depend on the accuracy of the pre-processing stage, which can not always be assured [WHA89]. Increasing the temporal region to more than two frames improves the accuracy of the computed motion.
Reference: [Ros84] <editor> A. Rosenfeld, editor. </editor> <title> Multiresolution Image Processing and Analysis. </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: The basic components of this framework are: * Construction of a Gaussian pyramid <ref> [Ros84] </ref>, where the images are represented in multiple resolutions. 19 * Starting at the lowest resolution level: 1. Motion parameters are estimated by solving the set of linear equations to minimize Err (t) (u; v) (Equation (19)) according to the appropriate 2D motion model (Section 4.2).
Reference: [SM90] <author> M. Shizawa and K. Mase. </author> <title> Simultaneous multiple optical flow estimation. </title> <booktitle> In International Conference on Pattern Recognition, </booktitle> <pages> pages 274-278, </pages> <address> Atlantic City, New Jersey, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Increasing the temporal region to more than two frames improves the accuracy of the computed motion. Methods for estimating local image velocities with large temporal regions have been introduced using a combined spatio-temporal analysis <ref> [FJ90, Hee88, SM90] </ref>. These methods assume motion constancy in the temporal regions, i.e. motion should remain uniform in the analyzed sequence. In feature based methods, features are tracked over a larger time interval using recursive temporal filtering [DF90]. The issue of extracting good features and overcoming occlusions still remains.
Reference: [Sun91] <author> V. Sundareswaran. </author> <title> Egomotion from global flow field data. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 140-145, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: introduction of a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from successive frames [OFT87, Hor90, COK93]. Both approaches depend on the accuracy of the pre-processing stage, which can not always be assured [WHA89].
Reference: [Taa92] <author> M.A. Taalebinezhaad. </author> <title> Direct recovery of motion and shape in the general case by fixation. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 14 </volume> <pages> 847-853, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: In feature based methods, features are tracked over a larger time interval using recursive temporal filtering [DF90]. The issue of extracting good features and overcoming occlusions still remains. Methods for computing the ego-motion directly from image intensities were also suggested <ref> [Han91, HW88, Taa92] </ref>. Horn and Weldon [HW88] deal only with restricted cases (pure translation, pure rotation, or a general motion with known depth). Hanna [Han91] suggests an iterative method for the most general case, but relies on a reasonably accurate initial guess. <p> Horn and Weldon [HW88] deal only with restricted cases (pure translation, pure rotation, or a general motion with known depth). Hanna [Han91] suggests an iterative method for the most general case, but relies on a reasonably accurate initial guess. Taalebinezhaad <ref> [Taa92] </ref> has a complete mathematical formulation, but relies on computations at a single image point, which can be highly unstable. 3D rotations and translations induce similar 2D image motions [Adi89, DN93, DMRS88] which cause ambiguities in their interpretation, especially in the presence of noise.
Reference: [TS91] <author> M. Tistarelli and G. </author> <title> Sandini. Direct estimation of time-to-impact from optical flow. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 226-233, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: introduction of a regularization term [HS81], assuming a limited model of the world [Adi85], restricting the range of possible motions [HW88], or assuming some type of temporal motion constancy over a longer sequence [DF90]. 3D motion is often estimated from the optical or normal flow field derived between two frames <ref> [Adi85, LR83, HJ90, GU91, JH91, Sun91, NL91, HA91, TS91, AD92, DRD93] </ref>, or from the correspondence of distinguished features (points, lines, contours) previously extracted from successive frames [OFT87, Hor90, COK93]. Both approaches depend on the accuracy of the pre-processing stage, which can not always be assured [WHA89].
Reference: [WHA89] <author> J. Weng, T.S. Huang, and N. Ahuja. </author> <title> Motion and structure from two perspective views: Algorithms, error analysis, and error estimation. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 11(5), </volume> <year> 1989. </year> <month> 23 </month>
Reference-contexts: Both approaches depend on the accuracy of the pre-processing stage, which can not always be assured <ref> [WHA89] </ref>. Increasing the temporal region to more than two frames improves the accuracy of the computed motion. Methods for estimating local image velocities with large temporal regions have been introduced using a combined spatio-temporal analysis [FJ90, Hee88, SM90]. <p> Moreover, the problem of recovering the 3D camera motion directly from the image motion field is an ill-conditioned problem, since small errors in the 2D flow field usually result in large perturbations in the 3D motion <ref> [WHA89, Adi89] </ref>. To overcome the difficulties and ambiguities in the computation of the ego-motion, we introduce the following scheme: The first frame is warped towards the second frame using the computed 2D image motion at the detected image region.
References-found: 40

