URL: http://www.cs.cmu.edu/~lafferty/ps/emnlp.ps
Refering-URL: http://cs.cornell.edu/Info/People/pierce/CS775.html
Root-URL: 
Title: Text Segmentation Using Exponential Models  
Author: Doug Beeferman Adam Berger John Lafferty 
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: This paper introduces a new statistical approach to partitioning text automatically into coherent segments. Our approach enlists both short-range and long-range language models to help it sniff out likely sites of topic changes in text. To aid its search, the system consults a set of simple lexical hints it has learned to associate with the presence of boundaries through inspection of a large corpus of annotated data. We also propose a new probabilistically motivated error metric for use by the natural language processing and information retrieval communities, intended to supersede precision and recall for appraising segmentation algorithms. Qualitative assessment of our algorithm as well as evaluation using this new metric demonstrate the effectiveness of our approach in two very different domains, Wall Street Journal articles and the TDT Corpus, a collection of newswire articles and broadcast news transcripts.
Abstract-found: 1
Intro-found: 1
Reference: <author> Allan, J. </author> <title> To appear. Topic Detection and Tracking Corpus, Linguistic Data Consortium, </title> <institution> University of Pennsylvania. </institution>
Reference: <author> Beeferman, D., A. Berger, and J. Lafferty. </author> <year> 1997. </year> <title> A model of lexical attraction and repulsion. </title> <booktitle> In Proceedings of the 35th Annual Meeting of the ACL, </booktitle> <address> Madrid, Spain. </address>
Reference-contexts: In general, the cache consists of content words s which promote the probability of their mate t, and correspondingly demote the probability of other words. As described in <ref> (Beeferman, Berger, and Lafferty, 1997) </ref>, for each (s; t) trigger pair there corresponds a real-valued parameter ; the probability of t is boosted by a factor of e for W words following the occurrence of s i .
Reference: <author> Berger, A., S. Della Pietra, and V. Della Pietra. </author> <year> 1996. </year> <title> A maximum entropy approach to natural language processing. </title> <journal> Computational Linguistics, </journal> <volume> 22(1) </volume> <pages> 39-71. </pages>
Reference: <author> Christel, M., T. Kanade, M. Mauldin, R. Reddy, M. Sirbu, S. Stevens, and H. Wactlar. </author> <year> 1995. </year> <title> In-formedia digital video library. </title> <journal> Communications of the ACM, </journal> <volume> 38(4) </volume> <pages> 57-58. </pages>
Reference-contexts: This can manifest itself in quite unfortunate ways. For example, a video-on-demand application (such as the one described in <ref> (Christel et al., 1995) </ref>) responding to a query about a recent news event may provide the user with a news clip related to the event, followed or preceded by part of an unrelated story or even a commercial. Document summarization is another fertile area for an automatic segmenter. <p> The upper verticle lines are boundaries placed by the algorithm. The fluctuating curve is the probability of a segment boundary according to the exponential model after 70 features were in duced. <ref> (Christel et al., 1995) </ref>. We intend to mix simple audio and video features such as statistics from pauses, black frames, and color histograms with our lexical features in order to segment news broadcasts into component stories.
Reference: <author> Della Pietra, S., V. Della Pietra, and J. Lafferty. </author> <year> 1997. </year> <title> Inducing features of random fields. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 19(4) </volume> <pages> 380-393, </pages> <month> April. </month>
Reference-contexts: The training algorithm we use for estimating the values is the Improved Iterative Scaling algorithm of <ref> (Della Pietra, Della Pietra, and Lafferty, 1997) </ref>, which is a scheme for solving the maximum likelihood problem that is "dual" to a corresponding maximum entropy problem. Assuming robust estimates for the parameters, the resulting model is essentially guaranteed to be superior to the trigram model. <p> Under certain mild regularity conditions, the maximum likelihood solution q ? = arg min D (p k q) exists and is unique. To find this solution, we use the iterative scaling algorithm presented in <ref> (Della Pietra, Della Pietra, and Lafferty, 1997) </ref>.
Reference: <author> Hearst, M.A. </author> <year> 1994. </year> <title> Multi-paragraph segmentation of expository text. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the ACL, </booktitle> <address> Las Cruces, NM. </address>
Reference-contexts: Finally, in Section 7 we demonstrate our model's effectiveness on two distinct domains. 2 Some Previous Work In this section we very briefly discuss some previous approaches to the text segmentation problem. 2.1 Text tiling The TextTiling algorithm, introduced by Hearst <ref> (Hearst, 1994) </ref>, segments expository texts into multiple paragraphs of coherent discourse units. A cosine measure is used to gauge the similarity between constant-size blocks of morphologically analyzed tokens.
Reference: <author> Jelinek, F., B. Merialdo, S. Roukos, and M. Strauss. </author> <year> 1991. </year> <title> A dynamic language model for speech recognition. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <pages> pp. 293-295, </pages> <month> Febru-ary. </month>
Reference: <author> Katz, S. </author> <year> 1987. </year> <title> Estimation of probabilities from sparse data for the langauge model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-35(3):400-401, </volume> <month> March. </month>
Reference-contexts: In this section and the next, we describe these language models and explain their utility in identifying segments. The trigram models p tri (w j w 2 ; w 1 ) we employ use the Katz backoff scheme <ref> (Katz, 1987) </ref> for smoothing. We trained trigram models on two different corpora. The Wall Street Journal corpus (WSJ) is a 38-million word corpus of articles from the newspaper. The model was constructed using a set W of the approximately 20; 000 most frequently occurring words in the corpus.
Reference: <author> Kozima, H. </author> <year> 1993. </year> <title> Text segmentation based on similarity between words. </title> <booktitle> in Proceedings of the 31st Annual Meeting of the ACL, </booktitle> <address> Columbus, OH, </address> <pages> pp. 286-288. </pages>
Reference-contexts: Furthermore, Hearst's approach segments at the paragraph level, which may be too coarse for applications like information retrieval on transcribed or automatically recognized spoken documents, in which paragraph boundaries are not known. 2.2 Lexical cohesion <ref> (Kozima, 1993) </ref> employs a "lexical cohesion profile" to keep track of the semantic cohesiveness of words in a text within a fixed-length window. In contrast to Hearst's focus on strict repetition, Kozima uses a semantic network to provide knowledge about related word pairs.
Reference: <author> Kozima, H. and T. Furugori. </author> <year> 1994. </year> <title> Segmenting narrative text into coherent scenes. </title> <journal> Literary and Linguistic Computing, </journal> <volume> 9 </volume> <pages> 13-19. </pages>
Reference: <author> Kuhn, R. and R. de Mori. </author> <year> 1990. </year> <title> A cache-based natural language model for speech recognition. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 12 </volume> <pages> 570-583. </pages>
Reference: <author> Lau, R., R. Rosenfeld, and S. Roukos. </author> <year> 1993. </year> <title> Adaptive language modeling using the maximum entropy principle. </title> <booktitle> In Proceedings of the ARPA Human Language Technology Workshop, </booktitle> <pages> pages 108-113. </pages> <publisher> Morgan Kaufman Publishers. </publisher>
Reference-contexts: Another approach, using maximum entropy methods, introduces a parameter for trigger pairs of mutually informative words, so that the occurrence of certain words in recent context boosts the probability of the words that they trigger <ref> (Lau, Rosenfeld, and Roukos, 1993) </ref>. The method we use here, described in (Beefer-man, Berger, and Lafferty, 1997), employs a static trigram model as a "prior," or default distribution, and adds certain features to a family of conditional exponential models to capture some of the nonstationary features of text.
Reference: <author> Litman, D. J. and R. J. Passonneau. </author> <year> 1995. </year> <title> Combining multiple knowledge sources for discourse segmentation. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the ACL, </booktitle> <address> Cambridge, MA. </address>
Reference-contexts: A graphically motivated segmentation technique called dotplotting is offered in (Reynar, 1994). This technique uses a simplified notion of lexical cohesion, depending exclusively on word repetition to find tight regions of topic similarity. 2.3 Decision trees <ref> (Litman and Passonneau, 1995) </ref> presents an algorithm that uses decision trees to combine multiple linguistic features extracted from corpora of spoken text, including prosodic and lexical cues. The decision tree algorithm, like ours, chooses from a space of candidate features, some of which are similar to our vocabulary questions. <p> the current image and an image near the last segment boundary? * Are there blank video frames nearby? * Is there a sharp change in the audio stream in the next utterance? The idea of using features is a natural one, and indeed other recent work on segmentation, such as <ref> (Litman and Passonneau, 1995) </ref>, adopts this approach.
Reference: <author> Neal, R. </author> <year> 1992. </year> <title> Connectionist learning of belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 56 </volume> <pages> 71-113. </pages>
Reference-contexts: f 1 (!) + 2 f 2 (!) + n f n (!) : The normalization constants Z (!) = 1 + e f (!) insure that this is indeed a family of conditional probability distributions. (This family of models is closely related to the class of sigmoidal belief networks <ref> (Neal, 1992) </ref>.) Our judgment of the merit of a model q 2 Q (f; q 0 ) relative to a reference distribution p 62 Q (f; q 0 ) during training is made in terms of the Kullback-Leibler divergence D (p k q) = !2 X b2fyes;nog p (b j !)
Reference: <author> Reynar, J. C. </author> <year> 1994. </year> <booktitle> In Proceedings of the 32nd Annual Meeting of the ACL, student session, </booktitle> <address> Las Cruces, NM. </address>
Reference-contexts: Kozima generalizes lexical cohesiveness to apply to a window of text, and plots the cohesiveness of successive text windows in a document, identifying the valleys in the measure as segment boundaries. A graphically motivated segmentation technique called dotplotting is offered in <ref> (Reynar, 1994) </ref>. <p> It is natural to expect that in a segmenter, close should count for something. A useful metric should also be robust with respect to the scale (words, sentences, paragraphs, for in stance) at which boundaries are determined. However, precision and recall are scale-dependent quantities. <ref> (Reynar, 1994) </ref> uses an error window that redefines "correct" to mean hypothesized within some constant window of units away from a reference boundary, but this approach still suffers from overdiscretizing error, drawing all-or-nothing lines insensitive to gradations of correctness.
Reference: <author> Youmans, G. </author> <year> 1991. </year> <title> A new tool for discourse analysis: The vocabulary-management profile. </title> <booktitle> Language, </booktitle> <volume> 67 </volume> <pages> 763-789. </pages>
References-found: 16

