URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-94-46/s2k-94-46.ps.Z
Refering-URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-94-46/
Root-URL: http://www.cs.berkeley.edu
Title: RP* A Family of Order Preserving Scalable Distributed Data Structures file with the same key
Author: W. Litwin MA Neimat D. Schneider A BSTRACT 
Keyword: Scalable Distributed Data Structures, Network files, Ordered files, Multicomputers, Multicast, Broadcast  
Note: builds the  to LH*.  
Abstract: Hash-based scalable distributed data structures (SDDSs), like LH* or DDH, for networks of interconnected computers (multicomputers) were shown to open new perspectives for file management. We propose a family of ordered SDDSs, called RP*, providing for ordered and dynamic files on multicomputers, and thus for more efficient processing of range queries and of ordered traversals of files. The basic algorithm termed RP* N 
Abstract-found: 1
Intro-found: 1
Reference: [BZS93] <author> Bershad, B., Zekauskas, M., Sawdon, W. </author> <title> The Midway Distributed Shared Memory System. </title> <journal> IEEE-COMPSAC 1993, </journal> <volume> 528 537. </volume>
Reference-contexts: An application should be able to use storage and processors of other computers whenever it needs, the number of computers involved being transparent. Terms are being coined for computers organized this way: multicomputer, network computer, and distributed memory are especially popular terms [ILP93], <ref> [BZS93] </ref>.
Reference: [C93] <author> Culler, D. & al. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> ACM-SIGPLAN Symp. on Principles and Practice of Parallel Programming, </booktitle> <year> 1993. </year>
Reference: [C94] <author> Culler, D. </author> <title> NOW: Towards Everyday Supercomputing on a Network of Workstations. </title> <type> EECS Tech. Rep. </type> <institution> UC Berkeley, </institution> <note> to app. </note>
Reference-contexts: HP Labs today has 1300 interconnected workstations with a total of 32 GB of RAM for applications, and terabytes (TB) of disks. The UC Berkeley Soda Hall multicomputer should include 500 workstations, delivering 25 Gflops, 64 GB of RAM, and about 1 TB of disk storage <ref> [C94] </ref>. Many organizations have similar configurations. At present, cooperation between interconnected computers is limited. Typically, an application uses the resources of a single computer, or perhaps, this computer is a client to some server. Many researchers believe that cooperation between interconnected computers should be much more extensive. <p> Terms are being coined for computers organized this way: multicomputer, network computer, and distributed memory are especially popular terms [ILP93], [BZS93]. Multicomputers should be an attractive alternative architecture to dedicated MPPs, and supercomputers, offering much better price-performance ratio <ref> [C94] </ref>. 1 University Paris 9, visiting HP Laboratories, Palo Alto CA & UC Berkeley 2 HP Laboratories, Palo Alto CA. - 2 - A fundamental component of a multicomputer will be data structures, especially for files.
Reference: [ChS92] <author> Chamberlin, D., Schmuck, F. </author> <title> Dynamic Data Distribution ( D 3 ) in a Shared-Nothing Multiprocessor Data Store. </title> <address> VLDB-92, </address> <year> 1992. </year>
Reference: [D93] <author> Devine, R. </author> <title> Design and Implementation of DDH: Distributed Dynamic Hashing. </title> <booktitle> Int. Conf. on Foundations of Data Organizations, FODO-93. Lecture Notes in Comp. </booktitle> <address> Sc., </address> <publisher> Springer-Verlag (publ.), </publisher> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: An SDDS stores data on some sites called servers, and is used from some sites called clients, basically distinct, but not necessarily. Clients typically retain some file access computation parameters, e.g., parameters of the actual dynamic function in LH* or DDH, but not the actual records [LNS93], <ref> [D93] </ref>. These parameters create the client's image of the actual file. Every SDDS must respect three design requirements. First, no central directory is used for data addressing, to avoid a hot-spot. Next, client's images can be outdated, being updated only through messages, called Image Adjustment Messages (IAMs). <p> Finally, a client with an outdated image can send a key to an incorrect address, in which case the structure should deliver it to the right server, and trigger an IAM. Only hash-based SDDSs have been proposed: LH* [LNS93], [LNS93a], DDH <ref> [D93] </ref>, and variants of LH* in [VBWY94], and [LNS94b]. All generalize popular extensible hashing algorithms. A file can start on one computer and scale up to practically any number of computers, and any size. It can be manipulated by any number of distributed clients, and supports parallel queries.
Reference: [G88] <author> Gray, J. </author> <title> The Cost of Messages. </title> <booktitle> 7th ACM Symp. on Principles of Distributed Systems. </booktitle> <year> 1988. </year>
Reference-contexts: Discussion of underlying formulas is in [LNS94]. The network model is from <ref> [G88] </ref>. Site speed is accordingly assumed 100 MIPS, and OS time to process a message is 25 m s (2500 instructions). <p> There are also the times to traverse the network, by an insert (i i, t ), and a search (t s, t ), computed according to <ref> [G88] </ref>. These are absolute bounds on any algorithm performing these inserts or searches, regardless of its own efficiency. The elapsed times for search and insert are about 1 ms for an m-net, and under 100 m s for g-net. <p> The values of s i s show that RP* N can sustain about 1000 o/s on m-net, up to 22,000 on g-net. These figures assume the use of 100 % of network bandwidth, hence practical figures should be somehow lower <ref> [G88] </ref>. <p> If buckets are chosen randomly, the overall OS throughput s n increases M times to s n n t = 25 m s in our case <ref> [G88] </ref>. For a larger M, the throughputs s i s approach to the bounds s i, t s , possibly leading to substantially better performance for g-nets, as in Table 2. However, addressing errors deteriorate the access costs.
Reference: [G88a] <author> Garcia-Molina, H. Kogan, B. </author> <title> Node Autononmy in Distributed Systems. </title> <address> IEEE-PDIS. </address> <year> 1988, </year> <pages> 85-93. </pages>
Reference-contexts: Next, client's images can be outdated, being updated only through messages, called Image Adjustment Messages (IAMs). These are sent only when a client makes an addressing error. Client's autonomy is preserved in this way which is a major requirement for multicomputers <ref> [G88a] </ref>. Finally, a client with an outdated image can send a key to an incorrect address, in which case the structure should deliver it to the right server, and trigger an IAM.
Reference: [K93] <author> Karp, R. M. </author> <title> A Generalization of Binary Search. Algorithms and Data Structures. </title> <booktitle> Lecture Notes in CS. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1993. </year> <note> Dehne & al (ed.). </note>
Reference-contexts: Numerous interesting design decisions omitted above will then also be addressed, e.g., the internal organization of buckets, of bulks, and of image, the overall organization of file management etc. Bulk operations lead to interesting generalizations of in-bucket, in-bulk, and in-image binary search, to find more efficiently several keys <ref> [K93] </ref>. The overall goal of implementation of a file system able to scale up to a thousand sites is clearly not simple, but is among most important goals in the computer science at present.
Reference: [ILP93] <author> Iftode, L., Li, K., Petersen, K. </author> <title> Memory Servers for Multicomputers. </title> <booktitle> IEEE-COMPSAC 1993, </booktitle> <pages> 538-547. </pages>
Reference-contexts: An application should be able to use storage and processors of other computers whenever it needs, the number of computers involved being transparent. Terms are being coined for computers organized this way: multicomputer, network computer, and distributed memory are especially popular terms <ref> [ILP93] </ref>, [BZS93].
Reference: [JK93] <author> Johnson, T. and P. Krishna. </author> <title> Lazy Updates for Distributed Search Structure. </title> <booktitle> ACM-SIGMOD Int. Conf. On Management of Data, </booktitle> <year> 1993. </year>
Reference-contexts: Traditional files are not as large and fast, hence RP* schemes are highly promising. With respect to the related work, there was no proposal of distributed ordered file structure without index. There were several proposals for parallel distributed B-trees, e.g., [MS90], [MS91], <ref> [JK93] </ref>, but none defined an SDDS. The indexes existed only on the servers and a much more limited scalability would result from, if these proposals were applied to multicomputers. Section 2 presents RP* N , and Section 3 discusses its performance. Sections 4 and 5 describe RP* C RP* .
Reference: [LNS93] <author> Litwin, W. Neimat, MA., Schneider, D. </author> <title> LH* : Linear Hashing for Distributed Files. </title> <booktitle> ACM-SIGMOD Int. Conf. On Management of Data, </booktitle> <year> 1993. </year>
Reference-contexts: Also, parallel data structures as defined for supercomputers will typically not be adequate <ref> [LNS93] </ref>. One class of data structures that is defined specifically for multicomputers are Scalable Distributed Data Structures (SDDSs) [LNS93]. An SDDS stores data on some sites called servers, and is used from some sites called clients, basically distinct, but not necessarily. <p> Also, parallel data structures as defined for supercomputers will typically not be adequate <ref> [LNS93] </ref>. One class of data structures that is defined specifically for multicomputers are Scalable Distributed Data Structures (SDDSs) [LNS93]. An SDDS stores data on some sites called servers, and is used from some sites called clients, basically distinct, but not necessarily. Clients typically retain some file access computation parameters, e.g., parameters of the actual dynamic function in LH* or DDH, but not the actual records [LNS93], [D93]. <p> Data Structures (SDDSs) <ref> [LNS93] </ref>. An SDDS stores data on some sites called servers, and is used from some sites called clients, basically distinct, but not necessarily. Clients typically retain some file access computation parameters, e.g., parameters of the actual dynamic function in LH* or DDH, but not the actual records [LNS93], [D93]. These parameters create the client's image of the actual file. Every SDDS must respect three design requirements. First, no central directory is used for data addressing, to avoid a hot-spot. Next, client's images can be outdated, being updated only through messages, called Image Adjustment Messages (IAMs). <p> Finally, a client with an outdated image can send a key to an incorrect address, in which case the structure should deliver it to the right server, and trigger an IAM. Only hash-based SDDSs have been proposed: LH* <ref> [LNS93] </ref>, [LNS93a], DDH [D93], and variants of LH* in [VBWY94], and [LNS94b]. All generalize popular extensible hashing algorithms. A file can start on one computer and scale up to practically any number of computers, and any size. <p> An ordered traversal visits each bucket (page) of the file only once. Hash-based SDDSs are much more efficient than traditional hash files for range queries, and general multikey queries, through parallel processing <ref> [LNS93] </ref>. One can nevertheless still expect gains from an order preserving SDDS. If a range query retrieves for instance a hundred records, they could easily be in the same bucket in a large order preserving SDDS, being then delivered with a single message. <p> When it overflows, it splits and a new bucket is created, termed bucket 1. In general, the new bucket is numbered bucket M if the file already has M buckets. It is assumed that some easy translation exists between bucket numbers and physical site addresses, e.g., as for LH* <ref> [LNS93] </ref>. Fig 3 illustrates the creation of an RP* file with b = 4. The file undergoes three splits, under the insertions of the most commonly used English words [LRLH91]. The RP* N split algorithm is as follows: RP* split algorithm: 1. <p> When those factors are taken into account, one can compute elapsed times and throughputs. For SDDSs, specific performance measures were introduced <ref> [LNS93] </ref>. Client image size is one concern. Also, access performance may vary between clients, because of different images and numbers of IAMs received. <p> The fanout m for RP* S is m = 100. The curves result from simulations like those performed for LH* <ref> [LNS93] </ref>. LH* messaging costs are plotted for comparison. Insert costs are total messaging costs to build the file, including the split messages and IAMs, divided by the number of inserts. In these experiments, each file was built through insertions of 100,000 random keys. <p> Split cost is ignored since splits are assumed to take place concurrently with inserts, and we are interested only in the client access cost. With respect to a single client case, performance of both clients may be affected <ref> [LNS93] </ref>. Tables 4 summarizes the results obtained for the RP* family, and includes for comparison those of LH*. The insert costs are computed for a small and a large b, and various N values. Client C 1 always inserts 100,000 keys. The results confirm the expectations.
Reference: [LNS93a] <author> Litwin, W., Neimat, MA., Schneider, D. </author> <title> LH*: A Scalable Distributed Data Structure. </title> <month> (Nov. </month> <year> 1993). </year> <note> Submitted for journal publ. </note>
Reference-contexts: Finally, a client with an outdated image can send a key to an incorrect address, in which case the structure should deliver it to the right server, and trigger an IAM. Only hash-based SDDSs have been proposed: LH* [LNS93], <ref> [LNS93a] </ref>, DDH [D93], and variants of LH* in [VBWY94], and [LNS94b]. All generalize popular extensible hashing algorithms. A file can start on one computer and scale up to practically any number of computers, and any size. It can be manipulated by any number of distributed clients, and supports parallel queries.
Reference: [LNS93b] <author> Litwin, W., Neimat, MA., Schneider, D. </author> <title> RP* : a Scalable Distributed Data Structure using Multicast (extended abstract) HPL-DTD-93-009, </title> <month> (Sept. </month> <year> 1993). </year>
Reference-contexts: As files rarely shrink in practice, we assume no merges in what follows. See <ref> [LNS93b] </ref> and [LNS94] for the discussion of the subject. 3.
Reference: [LNS94] <author> Litwin, W., Neimat, MA., Schneider, D. </author> <title> RP* : A Family of Order Preserving Scalable Distributed Data Structures. </title> <address> HPL DTD-94-012, </address> <month> (Feb. </month> <year> 1994). </year>
Reference-contexts: As files rarely shrink in practice, we assume no merges in what follows. See [LNS93b] and <ref> [LNS94] </ref> for the discussion of the subject. 3. <p> Discussion of underlying formulas is in <ref> [LNS94] </ref>. The network model is from [G88]. Site speed is accordingly assumed 100 MIPS, and OS time to process a message is 25 m s (2500 instructions). <p> Table 2 shows throughputs of inserts (s i ), and of searches (s s N file with M &gt;&gt; 1. The formulas are in <ref> [LNS94] </ref>. The table also contains the corresponding network throughputs (s i, t s, t ) resulting from the times t i, t s, t , and from the OS time above. Finally, we show the percentage of CPU bandwidth that RP* N should use on a server. <p> T T T T it, that, in in the file in Fig. 3. For every static file, an image T can acquire all the ranges in the file, provided the searches probe every bucket, e.g., through sufficiently many random searches in practice <ref> [LNS94] </ref>. The client then no longer multicasts key searches and updates. Splits progressively outdate existing images, triggering IAMs and multicasts.
Reference: [LRLH91] <author> Litwin, W., Roussopoulos, N., Levy, G., Hong, W. </author> <title> Trie Hashing With Controlled Load. </title> <journal> IEEE-TSE, </journal> <volume> 17, 7 1991, </volume> <pages> 678-691. </pages>
Reference-contexts: It is assumed that some easy translation exists between bucket numbers and physical site addresses, e.g., as for LH* [LNS93]. Fig 3 illustrates the creation of an RP* file with b = 4. The file undergoes three splits, under the insertions of the most commonly used English words <ref> [LRLH91] </ref>. The RP* N split algorithm is as follows: RP* split algorithm: 1. Determine (as for a B-tree) the middle key c m in the overflowing bucket B. 2. Attempt the creation of bucket M. Wait for ack, or denial if bucket M exists already. 3.
Reference: [MS90] <author> Matsliach, G., Shmueli, O. </author> <title> Distributing a B+-tree in a loosely coupled environment. </title> <journal> Inf. Proc. Letters, </journal> <volume> 34, </volume> <year> 1990, </year> <pages> 313-321. </pages>
Reference-contexts: Traditional files are not as large and fast, hence RP* schemes are highly promising. With respect to the related work, there was no proposal of distributed ordered file structure without index. There were several proposals for parallel distributed B-trees, e.g., <ref> [MS90] </ref>, [MS91], [JK93], but none defined an SDDS. The indexes existed only on the servers and a much more limited scalability would result from, if these proposals were applied to multicomputers. Section 2 presents RP* N , and Section 3 discusses its performance.
Reference: [MS91] <author> Matsliach, G., Shmueli, O. </author> <title> An Efficient Method for Distributing Search Structures. </title> <address> IEEE-PDIS, </address> <year> 1991. </year>
Reference-contexts: Traditional files are not as large and fast, hence RP* schemes are highly promising. With respect to the related work, there was no proposal of distributed ordered file structure without index. There were several proposals for parallel distributed B-trees, e.g., [MS90], <ref> [MS91] </ref>, [JK93], but none defined an SDDS. The indexes existed only on the servers and a much more limited scalability would result from, if these proposals were applied to multicomputers. Section 2 presents RP* N , and Section 3 discusses its performance.
Reference: [PLH89] <author> Perrizo, W., Lin, J., Hoffman, W. </author> <title> Algorithms for Distributed Query Processing in Broadcast Local Area Networks. </title> <journal> IEEE TKDE, </journal> <volume> 1, 2, </volume> <year> 1989, </year> <pages> 215-225. </pages>
Reference: [S89] <author> Samet, H. </author> <title> The Design and Analysis of Spatial Data Structures. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: One should study also ordered SDDSs applying ideas in numerous traditional variants of B-trees, and in other ordered structures <ref> [S89] </ref>. Finally, one should analyze the RP* family as support for database operations and transaction processing. 8. ACKNOWLEDGMENTS We thank Jim Davis (HP Labs) and Domenico Ferrari (UC Berkeley) for valuable information on broadcast and multicast protocols.
Reference: [VBWY94] <author> Vingralek, R., Breitbart, Y., Weikum, G. and R. Yavatkar. </author> <title> Distributed File Organization with Scalable Cost/Performance. </title> <booktitle> ACM-SIGMOD Int. Conf. On Management of Data, </booktitle> <year> 1994. </year>
Reference-contexts: Finally, a client with an outdated image can send a key to an incorrect address, in which case the structure should deliver it to the right server, and trigger an IAM. Only hash-based SDDSs have been proposed: LH* [LNS93], [LNS93a], DDH [D93], and variants of LH* in <ref> [VBWY94] </ref>, and [LNS94b]. All generalize popular extensible hashing algorithms. A file can start on one computer and scale up to practically any number of computers, and any size. It can be manipulated by any number of distributed clients, and supports parallel queries.
References-found: 20

