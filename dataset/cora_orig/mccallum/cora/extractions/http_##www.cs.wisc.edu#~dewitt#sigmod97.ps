URL: http://www.cs.wisc.edu/~dewitt/sigmod97.ps
Refering-URL: http://www.cs.wisc.edu/~dewitt/
Root-URL: 
Title: Building a Scalable Geo-Spatial DBMS: Technology, Implementation, and Evaluation  
Author: Jignesh Patel, JieBing Yu, Navin Kabra, Kristin Tufte, Biswadeep Nag, Josef Burger, Nancy Hall, Karthikeyan Ramasamy, Roger Lueder, Curt Ellmann, Jim Kupsch, Shelly Guo, Johan Larson, David DeWitt, and Jeffrey Naughton 
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: This paper presents a number of new techniques for parallelizing geo-spatial database systems and discusses their implementation in the Paradise object-relational database system. The effectiveness of these techniques is demonstrated using a variety of complex geo-spatial queries over a 120 GB global geo-spatial data set. 
Abstract-found: 1
Intro-found: 1
Reference: [Beck90] <author> Beckmann, N. et. al., </author> <title> The R*-tree: An Efficient and Robust Access Method for Points and Rectangles, </title> <booktitle> Proceed ings of the 1990 ACM-SIGMOD Conference , June 1990. </booktitle>
Reference-contexts: The QC and DS are implemented as multithreaded processes on top of the SHORE Storage Manager [Care94]. The SHORE Storage Manager provides storage volumes, files of untyped objects, B+-trees, and R*-trees <ref> [Beck90, Gutm84] </ref>. Objects can be arbitrarily large, up to the size of a storage volume. Allocation of space inside a storage volume is performed in terms of fixed-size extents. ARIES [Moha92] is used as SHORE's recovery mechanism.
Reference: [Bonc96] <author> Peter A. Boncz, Wilko Quak, Martin L. Kersten: </author> <title> Monet And Its Geographic Extensions: a Novel Approach to High Performance GIS Processing. EDBT 1996: 147-166 [Care94] "Shoring Up Persistent Applications," </title> <editor> M. Carey, D. DeWitt, J. Naughton, M. Solomon, et. al., </editor> <booktitle> Proceedings of the 1994 SIGMOD Conference, </booktitle> <address> Minneapolis, MN, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: There is a substantial, and growing, body of work related to performance on geo-spatial workloads; throughout this section we discuss relevant related work where applicable. In general terms, perhaps the system with a stated goal most similar to ours is the Monet system <ref> [Bonc96] </ref>, which also explicitly names parallelism as a project goal. 2.1 Paradise Data Model and Query Language Paradise provides what can be loosely interpreted as an object relational data model.
Reference: [DCW92] <institution> VPFView 1.0 Users Manual for the Digital Chart of the World, Defense Mapping Agency, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: In order to validate these techniques, we sought a large geo-spatial database with which to experiment. Our answer was to acquire 10 years of worldwide AVHRR satellite images along with DMA <ref> [DCW92] </ref> polygonal map-data from the entire globe. The size of this data set is about 30 GB - about 30 times larger than the regional Sequoia 2000 benchmark. We also produced 60 GB and 120 GB versions of this data set to test system scale-ability. <p> Each raster is a composite image covering the entire world. For polygon data, we used a DCW <ref> [DCW92] </ref> global data set containing a variety of information about such things as roads, cities, land use, and drainage properties. Both data sets were geo-registered to the same coordinate system.
Reference: [DeW90] <author> D. DeWitt, et. al., </author> <title> The Gamma Database Machine Project, </title> <journal> IEEE Transactions on Knowledge and Data Engi neering, </journal> <month> March, </month> <year> 1990. </year>
Reference: [DeWi92] <author> D. DeWitt and J. Gray, </author> <title> Parallel Database Systems: The Future of Database Processing or a Passing Fad?, </title> <journal> Communications of the ACM, </journal> <month> June, </month> <year> 1992. </year>
Reference-contexts: and the main SHORE server process share the SHORE buffer pool which is kept in shared memory. [Yu96] describes an extension of SHORE and Paradise to handle tape-based storage volumes. 2.3 Basic Parallelism Mechanisms Paradise uses many of the basic parallelism mechanisms first developed as part of the Gamma project <ref> [DeWi90, DeWi92] </ref>. Tables are fully partitioned across all disks in the system using round-robin, hash, or spatial declustering (to be discussed below). When a scan or selection query is executed, a separate thread is started for each fragment of each table. <p> When a scan or selection query is executed, a separate thread is started for each fragment of each table. Paradise uses a push model of parallelism to implement partitioned execution <ref> [DeWi92] </ref> in which tuples are pushed from leaves of the operator tree upward. Every Paradise operator (e.g. join, sort, select, ...) takes its input from an input stream and places its result tuples on an output stream. <p> Network streams can be further specialized into split streams which are used to demultiplex an output stream into multiple output streams based on a function being applied to each tuple. Split streams are one of the key mechanisms used to parallelize queries <ref> [DeWi92] </ref>. Since all types of streams are derived from a base stream class, their interfaces are identical and the implementation of each operator can be totally isolated from the type of stream it reads or writes.
Reference: [DeWi92] <author> DeWitt, D., Naughton, J., Schneider, D, and S. Se-shadri, </author> <title> Practical Skew Handling in Parallel Joins, </title> <booktitle> Proceedings of the 1992 Very Large Data Base Conference, </booktitle> <address> Vancou ver, CA, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: and the main SHORE server process share the SHORE buffer pool which is kept in shared memory. [Yu96] describes an extension of SHORE and Paradise to handle tape-based storage volumes. 2.3 Basic Parallelism Mechanisms Paradise uses many of the basic parallelism mechanisms first developed as part of the Gamma project <ref> [DeWi90, DeWi92] </ref>. Tables are fully partitioned across all disks in the system using round-robin, hash, or spatial declustering (to be discussed below). When a scan or selection query is executed, a separate thread is started for each fragment of each table. <p> When a scan or selection query is executed, a separate thread is started for each fragment of each table. Paradise uses a push model of parallelism to implement partitioned execution <ref> [DeWi92] </ref> in which tuples are pushed from leaves of the operator tree upward. Every Paradise operator (e.g. join, sort, select, ...) takes its input from an input stream and places its result tuples on an output stream. <p> Network streams can be further specialized into split streams which are used to demultiplex an output stream into multiple output streams based on a function being applied to each tuple. Split streams are one of the key mechanisms used to parallelize queries <ref> [DeWi92] </ref>. Since all types of streams are derived from a base stream class, their interfaces are identical and the implementation of each operator can be totally isolated from the type of stream it reads or writes.
Reference: [DeWi94] <author> DeWitt, D. J., N. Kabra, J. Luo, J. M. Patel, and J. Yu, </author> <title> "Client-Server Paradise". </title> <booktitle> In Proceedings of the 20th VLDB Conference, </booktitle> <month> September, </month> <year> 1994. </year>
Reference-contexts: The spatial index on landCover is better packed since the new satellite features are smaller and are easier to pack when bulk loading the index <ref> [DeWi94] </ref>. This, however, benefits mainly Query 6 as it scans a substantial portion of the index. Query 9: Query 9 selects the oil field polygons and sends them to all the nodes. These polygons are joined with one raster.
Reference: [EOS96] <author> See: </author> <note> http://eos.nasa.gov/ </note>
Reference: [Grae90] <author> Graefe, G., </author> <title> "Encapsulation of Parallelism in the Volcano Query Processing System," </title> <booktitle> Proceedings of the 1990 ACM-SIGMOD International Conference on Management of Data, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Every Paradise operator (e.g. join, sort, select, ...) takes its input from an input stream and places its result tuples on an output stream. Streams are similar in concept to Volcanos exchange operator <ref> [Grae90] </ref> except that they are not active entities. Streams themselves are C++ objects and can be specialized in the form of "file streams" and "network streams". File streams are used to read/write tuples from/to disk.
Reference: [Gutm84] <author> A. Gutman, R-trees: </author> <title> A Dynamic Index Structure for Spatial Searching, </title> <booktitle> Proceedings of the 1984 ACM-SIGMOD Conference , Boston, </booktitle> <address> Mass. </address> <month> June </month> <year> 1984. </year>
Reference-contexts: The QC and DS are implemented as multithreaded processes on top of the SHORE Storage Manager [Care94]. The SHORE Storage Manager provides storage volumes, files of untyped objects, B+-trees, and R*-trees <ref> [Beck90, Gutm84] </ref>. Objects can be arbitrarily large, up to the size of a storage volume. Allocation of space inside a storage volume is performed in terms of fixed-size extents. ARIES [Moha92] is used as SHORE's recovery mechanism.
Reference: [Hua91] <author> Hua, K.A. and C. Lee, </author> <title> "Handling Data Skew in Multiprocessor Database Computers Using Partition Tuning," </title> <booktitle> Pro ceedings of the 17 th VLDB Conference", Barcelon a, </booktitle> <address> Spain, </address> <month> September, </month> <year> 1991. </year>
Reference-contexts: Likewise, partitions containing large areas of Lake Michigan or Lake Superior will have relatively few tuples mapped to them. As proposed in <ref> [Hua91, DeWi92b] </ref>, one way to reduce the effects of partition skew [Walt91] is to significantly increase the number of partitions being used. Unfortunately, increasing the number of partitions, increases the number of tuples spanning multiple partitions and, hence, the percentage of tuples that must be replicated.
Reference: [Info96] <institution> See http://www.informix.com/ </institution>
Reference: [Illu96] <institution> See http://www.illustra.com/ </institution>
Reference: [Moha92] <author> Mohan, C., et. Al., </author> <title> ARIES: A transaction recovery methods supporting fine-granularity locking and partial roll backs using write-ahead logging, </title> <journal> ACM TODS, </journal> <month> March </month> <year> 1992. </year>
Reference-contexts: The SHORE Storage Manager provides storage volumes, files of untyped objects, B+-trees, and R*-trees [Beck90, Gutm84]. Objects can be arbitrarily large, up to the size of a storage volume. Allocation of space inside a storage volume is performed in terms of fixed-size extents. ARIES <ref> [Moha92] </ref> is used as SHORE's recovery mechanism. Locking can be done at multiple granularities (e.g. object, page, or file) with optional lock escalation. Because I/O in UNIX is blocking, SHORE uses a separate I/O process for each mounted storage volume.
Reference: [Kabr96] <author> Kabra, N. and D. DeWitt, </author> <title> Opt++ - An Object Oriented Implementation for Extensible Database Query Optimi zation, </title> <note> submitted for publication. See also: http://www.cs.wisc.edu/~navin/research/opt++.ps </note>
Reference-contexts: Indexed selections are provided for both nonspatial and spatial selections. For join operations, the optimizer can choose from nested loops, indexed nested loops, and dynamic memory Grace hash join [Kits89]. Paradise's query optimizer (which is written using OPT++ <ref> [Kabr96] </ref>) will consider replicating small outer tables when an index exists on the join column of the inner table. Most parallel database systems [Shat95] use a two phase approach to the parallel execution of aggregate operations. For example, consider a query involving an average operator with a group by clause.
Reference: [Kits90] <author> Kitsuregawa, M., Nakayama, M. and M. Takagi, </author> <title> The Effect of Bucket Size Tuning in the Dynamic Hybrid Grace Hash Join Method, </title> <booktitle> Proceedings of the 1989 VLDB Confer ence, </booktitle> <address> Amsterdam, </address> <month> August </month> <year> 1989. </year>
Reference: [Koud96] <author> Nick Koudas, Christos Faloutsos, Ibrahim Kamell, </author> <title> Declustering Spatial Databases on a Multi-Computer Archi tecture, </title> <booktitle> EDBT 1996: </booktitle> <pages> 592-614 </pages>
Reference-contexts: Our preliminary tests showed that one needs thousands of partitions to smooth out the skew to any significant extent. An alternative approach to dealing with skew is to decluster based upon the distribution of the data set. Koudas et al. <ref> [Koud96] </ref> describe an interesting approach based upon declustering the leaves of an R-tree.
Reference: [Kim96] <author> Min-Soo Kim and Ki-Joune Li. </author> <title> A Spatial Query Proc essing Method for Nearest Object Search Using R+-trees. </title> <note> Paper submitted for publication. </note>
Reference-contexts: Currently, there is only one global aggregate operator in the entire system. This operator represents a sequential portion of the query execution, and hurts the speedup and scaleup somewhat. Another approach to this problem would have been to use modifications of R-tree search algorithms, as suggested in <ref> [Kim96, Rous95] </ref>. We did not explore that a pproach here. Query 13: Find all drainage features (lakes, rivers, etc.) which cross a road.
Reference: [Pate96] <author> Patel, J. M., and D. J. DeWitt, </author> <title> "Partition Based Spatial Merge Join". </title> <booktitle> In Proceedings of the 1996 ACM-SIGMOD Conference, </booktitle> <month> June, </month> <year> 1996. </year>
Reference-contexts: A similar approach is used in Illustra/Informix. Paradise has two algorithms for executing spatial join operations. When an R-tree exists on the join attribute of one of the two relations being joined, an indexed nested loops algorithm is generally used. Otherwise, the PBSM algorithm <ref> [Pate96] </ref> is used. 2.5 Dealing with Large Satellite Images A key focus of the Paradise DBMS project has been the development of techniques for dealing with large satellite images and arrays of scientific data. <p> In the second phase, each of the participating processors joins the two partitions of tuples it receives during the first phase. Any single processor spatial join algorithm (e.g. PBSM <ref> [Pate96] </ref>) can be used during this second phase. If either of the input tables are already declustered on their joining attributes, then the first phase of the algorithm can be eliminated for that table. For joins on spatial attributes the process is slightly more complicated.
Reference: [Pate97] <author> Patel, J. M., and D. J. DeWitt, </author> <title> "A Study of Alternative Parallel Spatial Join Algorithms". </title> <booktitle> Submitted to the 1997 VLDB Conference, </booktitle> <month> August </month> <year> 1997. </year>
Reference-contexts: We have also designed and implemented other parallel spatial join algorithms involving the replication of only bounding box information and not full tuples. A description and analysis of the performance of these algorithms can be found in <ref> [Pate97] </ref>. 2.7.3 Spatial Aggregation Processing A geo-spatial database system opens up an entire new spectrum of operations that cannot be handled efficiently by normal relational query processing techniques. Consider, for example a query to find the river closest to a particular point.
Reference: [Prep88] <editor> Preparata, F.P. and M. Shamos, editors, </editor> <booktitle> Computation Geometry, </booktitle> <publisher> Springer, </publisher> <year> 1988. </year>
Reference: [Rous95] <author> Nick Rossopoulos, Steve Kelly, and F. Vincent,. </author> <title> Nearest Neighbor Queries. </title> <booktitle> Proc. of ACM-SIGMOD, </booktitle> <pages> pages 71-79, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Currently, there is only one global aggregate operator in the entire system. This operator represents a sequential portion of the query execution, and hurts the speedup and scaleup somewhat. Another approach to this problem would have been to use modifications of R-tree search algorithms, as suggested in <ref> [Kim96, Rous95] </ref>. We did not explore that a pproach here. Query 13: Find all drainage features (lakes, rivers, etc.) which cross a road.
Reference: [Shat95] <author> Shatdal, A., and J. F. Naughton,, </author> <title> Aggregate Processing in Parallel RDBMS, </title> <booktitle> Proceedings of the ACM SIGMOD Conference, </booktitle> <address> San Jose, California, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Paradise's query optimizer (which is written using OPT++ [Kabr96]) will consider replicating small outer tables when an index exists on the join column of the inner table. Most parallel database systems <ref> [Shat95] </ref> use a two phase approach to the parallel execution of aggregate operations. For example, consider a query involving an average operator with a group by clause. During the first phase each participating thread processes its fragment of the input table producing a running sum and count for each group.
Reference: [Shat96] <author> Shatdal, A., </author> <title> The Interaction between Software and Hardware Architecture in Parallel Relational Database Query Processing, </title> <type> PhD Thesis, </type> <institution> Computer Science Department, UW Madison,1996. </institution>
Reference: [Ston93] <author> M. Stonebraker, J. Frew, K. Gardels, and J. Meredith, </author> <title> The SEQUOIA 2000 Storage Benchmark, </title> <booktitle> Proceedings of the 1993 SIGMOD Conference, </booktitle> <address> Washington, D.C. </address> <month> May, </month> <year> 1993. </year>
Reference-contexts: For example, one might ask for all images of northern Wisconsin in which more than 50% of the lakes were covered with ice. The Sequoia 2000 benchmark <ref> [Ston93] </ref> captures many of the types of queries expected in the EOSDIS project. <p> An evaluation of this type of query is contained in Section 3. Madison 3. Benchmark Description and Performance Evaluation 3.1 Benchmark Description Generating a meaningful test suite for a parallel geo-spatial database system turned out to be a nontrivial project. Although the Sequoia 2000 benchmark <ref> [Ston93] </ref> was about what we wanted, there were several problems in using it as is. First, although the paper alludes to a national size benchmark, the only available data set is a state level benchmark, which, at approximately 1GB, was far too small to use for testing scaleup or speedup.
Reference: [SQL3] <author> ISO/IEC SQL Revision. </author> <title> ISO-ANSI Working Draft Database Language SQL (SQL3), </title> <editor> Jim Melton - Editor, document ISO/IECJCT1/SC21 N6931, </editor> <booktitle> American National Standards In stitute, </booktitle> <address> N.Y., NY 10036, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Copies of any large attributes are made only when a tuple is inserted into a permanent relation. While this copy could be avoided by using the SQL3 <ref> [SQL3] </ref> notion of a reference attribute, the copy ensures that the raster data will be located at the same node as the rest of the tuple. This has the advantage of preventing unnecessary network traffic during the execution of subsequent queries.
Reference: [Suni94] <author> S. Sarawagi. </author> <title> Efficient Processing for Multidimensional Arrays, </title> <booktitle> Proceedings of the 1994 IEEE Data En gineering Conference, </booktitle> <month> February, </month> <year> 1994. </year>
Reference-contexts: Third, queries not requiring access to the array do not pay the cost of reading the array from disk into the buffer pool. For very large arrays the array ADT code chunks <ref> [Suni94] </ref> the array into subarrays called tiles such that the size of each tile is approximately 128 Kbytes. Each tile is stored as a separate SHORE object as is a mapping table that keeps track of the objects used to store the subarrays. <p> Each subarray has the same dimensionality as the original array and the size of each dimension is proportional to the size of each dimension in the original array as proposed in <ref> [Suni94] </ref>. Figure 2.3 illustrates this process for a 2-D array. Decomposition of an array allows Paradise to fetch only those portions that are required to execute an operation.
Reference: [Tera85] <author> Teradata, </author> <title> DBC/1012 Database Computer System Manual Release 2.0, Document No. </title> <institution> C10-0001-02, Teradata Corp., </institution> <month> NOV </month> <year> 1985. </year>
Reference: [Yu96] <author> Yu, J. and D. DeWitt. </author> <title> Query Pre-Execution and Batching: A Two-Pronged Approach to the Efficient Processing of TapeResident Data Sets, </title> <note> Submitted for publication, </note> <month> September, </month> <year> 1996. </year>
Reference-contexts: Because I/O in UNIX is blocking, SHORE uses a separate I/O process for each mounted storage volume. The I/O processes and the main SHORE server process share the SHORE buffer pool which is kept in shared memory. <ref> [Yu96] </ref> describes an extension of SHORE and Paradise to handle tape-based storage volumes. 2.3 Basic Parallelism Mechanisms Paradise uses many of the basic parallelism mechanisms first developed as part of the Gamma project [DeWi90, DeWi92].
Reference: [Walt91] <author> Walton, C.B., Dale, </author> <title> A.G., and R.M. Jenevein, "A Taxonomy and Performance Model of Data Skew Effects in Parallel Joins," </title> <booktitle> Proceedings of the SeventeenthVLDB Confer ence", Barcelon a, </booktitle> <address> Spain, </address> <month> September, </month> <year> 1991. </year> <title> [Welc84], Welch, T.A., A Technique for High Performance Data Compression,, </title> <journal> IEEE Computer, </journal> <volume> Vol 17, No. 6, </volume> <year> 1984. </year>
Reference-contexts: Likewise, partitions containing large areas of Lake Michigan or Lake Superior will have relatively few tuples mapped to them. As proposed in [Hua91, DeWi92b], one way to reduce the effects of partition skew <ref> [Walt91] </ref> is to significantly increase the number of partitions being used. Unfortunately, increasing the number of partitions, increases the number of tuples spanning multiple partitions and, hence, the percentage of tuples that must be replicated.
References-found: 30

