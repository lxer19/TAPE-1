URL: ftp://eivind.imm.dtu.dk/pub/cyril/chap2.ps.gz
Refering-URL: http://eivind.imm.dtu.dk/staff/goutte/PUBLIS/thesis.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: a simple parabolic interpolation involving 3 points. These 3 points determine a unique second-order polynomial
Author:  
Keyword: 2.2 Stability of non-regularised solutions  
Note: Let us consider  
Date: 2.1 Introduction  
Pubnum: 2 Regularisation  
Abstract: The purpose of this chapter is to carry on with the analysis started in chapter 1. In the previous chapter, we have linked the problem of learning to (parametric) regression estimation. The underlying inductive principle consists in minimising a cost functional, such as the quadratic cost, depending on the parameters and the data, so as to identify the parameters of the model. In this chapter, we show that minimising the simple cost calculated on the data does not yield good result in real, noisy situations. It is shown that the learning problem presented as a simple empirical risk minimisation, is an ill-posed problem. The concepts of well-posed and ill-posed problems and the technique of regularisation are introduced in a general context. Afterwards, we link regularisation and noise injection on the input. We derive an original equivalence between stopped training and regularisation in section 2.14. We introduce afterwards the dichotomy between formal and structural regularisation techniques, in the context of non-linear parametric neural networks models. Another original contribution of this chapter is the analysis in section 2.16 of a regularisation technique that combines both aspects, an analysis that will be carried out further in chapter 6. At the end of chapter 1, we have displayed an example suggesting that the minimi-sation of the empirical cost could lead to inappropriate models. We can carry this idea further by focusing on the case of polynomial regression. For a number N of points, there exists a polynomial of degree N 1 that approximates these points arbitrarily closely (Stone-Weierstrass theorem). 
Abstract-found: 1
Intro-found: 1
Reference: <author> Badeva, V. and Morozov, V. </author> <year> (1991). </year> <note> Problemes incorrectement poses. Serie automa-tique. Masson, Paris. </note>
Reference-contexts: However, it stayed a mathematical curiosity until the sixties, with the work of Russian mathematicians. Well-posed and ill-posed problems in Hadamard's sense are defined e.g. in chapter 1 of <ref> (Badeva and Morozov, 1991) </ref>. 2:4 Tikhonov well-posedness is also defined by Badeva and Morozov (1991). The links between Hadamard well-posedness and Tikhonov well-posedness are stud ied in Dontchev and Zolezzi (1992). 2:5 Badeva and Morozov (1991), already cited above, gives a long introduction to Tikhonov regularisation method.
Reference: <author> Bishop, C. M. </author> <year> (1995). </year> <title> Training with noise is equivalent to thikonov regularization. </title> <booktitle> NC, </booktitle> <volume> 7(1) </volume> <pages> 110-116. </pages>
Reference: <author> Charton, F. </author> <year> (1994). </year> <title> Discussion on the denker example. </title> <type> Personal communication. </type>
Reference-contexts: A brief description is also given by Vapnik (1995). 2:9 Hansen (1996) explores briefly the use of different regularisation functionals, in the context of linear inverse problems. E.g. the use of the 2-norm or the 1-norm for the regularisation functional, and Backus-Gilbert regularisation. 2:7 This is a classical example <ref> (Charton, 1994) </ref>. 2:8 The ill-posedness of density estimation is mentioned by Vapnik (1995) in sec tion 1.8.2. 2:9 The choice of the appropriate regularisation parameter is a topic explored in chapter 3. 2:10 Calculations in this section follow those of Bishop (1995), although we apply these derivations directly to the empirical
Reference: <author> Denker, J., Schwartz, D., Wittner, B., Solla, S., Howard, R., Jackel, L., and Hopfield, J. </author> <year> (1987). </year> <title> Large automatic learning, rule extraction, and generalization. </title> <journal> Complex Systems, </journal> <volume> 1(5) </volume> <pages> 877-922. </pages>
Reference: <author> Dontchev, A. L. and Zolezzi, T. </author> <year> (1992). </year> <title> Well-posed optimization problems. </title> <booktitle> Number 1543 in Lecture notes in mathematics. </booktitle> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference: <author> Goutte, C. </author> <year> (1996). </year> <title> On the use of a pruning prior for neural networks. </title> <booktitle> In Neural Networks for Signal Processing VI Proceedings of the 1996 IEEE Workshop, number VI in NNSP, </booktitle> <pages> pages 52-61, </pages> <address> Piscataway, New Jersey. </address> <publisher> IEEE. </publisher>
Reference: <author> Goutte, C. and Hansen, L. K. </author> <year> (1997). </year> <title> Regularization with a pruning prior. Neural Networks. </title> <note> to appear. </note>
Reference: <author> Grandvalet, Y. </author> <year> (1995). </year> <title> Injection de bruit dans les perceptrons multicouches. </title> <type> PhD thesis, </type> <institution> Universite Technologique de Compiegne, France. </institution>
Reference: <author> Hadamard, J. </author> <year> (1902). </year> <editor> Sur les problemes aux derivees partielles et leur signification physique. Bul. </editor> <publisher> Univ. Princeton, </publisher> <pages> 13(49). </pages>
Reference: <author> Hansen, P. C. </author> <year> (1996). </year> <title> Rank-Deficient and Discrete Ill-Posed Problems. </title> <type> Doctoral Dissertation. </type> <institution> Polyteknisk Forlag, Lyngby (Denmark). c flC. </institution> <note> Goutte 1996 40 Regularisation Hassibi, </note> <author> B. and Stork, D. G. </author> <year> (1993). </year> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title> <editor> In Hanson, S., Cowan, J., and Giles, C., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 5 of NIPS, </booktitle> <pages> pages 164-171. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It has been proposed to the neural network community by Williams (1995). Besides chapter 6, further studies can be found in (Goutte and Hansen, 1997; Goutte, 1996). The study of the influence of different regularisers than the usual 2-norm is also of interest to linear systems, cf. <ref> (Hansen, 1996) </ref> already cited.
Reference: <author> Krogh, A. and Hertz, J. A. </author> <year> (1992). </year> <title> A simple weight decay can improve generalization. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, volume 4 of NIPS. </booktitle>
Reference: <author> Le Cun, Y., Denker, J. S., and Solla, S. A. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems, number 2 in NIPS, </booktitle> <pages> pages 598-605. </pages> <publisher> Morgan-Kaufmann. </publisher>
Reference: <author> Ljung, L., Sjoberg, J., and McKelvey, T. </author> <year> (1992). </year> <title> On the use of regularization in system identification. </title> <type> Technical Report 1379, </type> <institution> Department of Electrical Engineering, Linkoping University, S-581 83 Linkoping, Sweden. </institution>
Reference: <author> Pedersen, M. W., Hansen, L. K., and Larsen, J. </author> <year> (1996). </year> <title> Pruning with generalization based weight saliencies: </title> <editor> flOBD, flOBS. In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, number 8 in NIPS. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Plaut, D. C., Nowlan, S. J., and Hinton, G. E. </author> <year> (1986). </year> <title> Experiments on learning by backpropagation. </title> <type> Technical Report CMU-CS-86-126, </type> <institution> Carnegie Mellon University, Pittsburgh, USA. </institution>
Reference-contexts: regularisation with (2.20) fails to give good results. 2:11 The distinction between formal and structural regularisation in neural net works was introduced by Denker et al. (1987). 2:12 The use of weight-decay for improvement of the generalisation error has been a standard technique for a long time in neural computation <ref> (Plaut et al., 1986) </ref>. It is analysed, e.g. by Krogh and Hertz (1992).
Reference: <author> Sjoberg, J. and Ljung, L. </author> <year> (1995). </year> <title> Overtraining, regularization and searching for minimum with application to neural nets. </title> <journal> International Journal of Control. </journal>
Reference: <author> Vapnik, V. N. </author> <year> (1995). </year> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer. </publisher>
Reference: <author> Williams, P. M. </author> <year> (1995). </year> <title> Bayesian regularization and pruning using a Laplace prior. </title> <journal> Neural Computation, </journal> <volume> 7(1) </volume> <pages> 117-143. </pages> <address> c flC. </address> <month> Goutte </month> <year> 1996 </year>
References-found: 18

