URL: ftp://theory.lcs.mit.edu/pub/people/oded/bgs5.ps
Refering-URL: http://theory.lcs.mit.edu/~oded/bgs.html
Root-URL: 
Email: Email: oded@wisdom.weizmann.ac.il.  E-mail: madhu@theory.lcs.mit.edu.  
Title: Free Bits, PCPs and Non-Approximability|  3 and approximating the Chromatic Number within N  
Author: Mihir Bellare Oded Goldreich Madhu Sudan 
Address: Rehovot, Israel.  545 Technology Sq., Cambridge, MA 02139, USA.  
Affiliation: Department of Computer Science and Applied Mathematics, Weizmann Institute of Sciences,  Laboratory for Computer Science, MIT,  
Date: 1995.  
Note: In honor of Shimon Even's 60 th birthday. To appear in SIAM Journal on Computing. Extended abstract appeared in Proceedings of the 0th Symposium on Foundations of Computer Science, IEEE,  Supported in part by grant No. 92-00226 from the US-Israel Binational Science Foundation (BSF), Jerusalem, Israel.  Some of this work was done when the author was at IBM.  
Abstract: Towards Tight Results fl Abstract This paper continues the investigation of the connection between probabilistically checkable proofs (PCPs) the approximability of NP-optimization problems. The emphasis is on proving tight non-approximability results via consideration of measures like the "free bit complexity" and the "amortized free bit complexity" of proof systems. The first part of the paper presents a collection of new proof systems based on a new error-correcting code called the long code. We provide a proof system which has amortized free bit complexity of 2 + *, implying that approximating Max Clique within N 1 5 * , are hard assuming NP 6= coRP, for any * &gt; 0. We also derive the first explicit and reasonable constant hardness factors for Min Vertex Cover, Max2SAT, and Max Cut, and improve the hardness factor for Max3SAT. We note that our non-approximability factors for MaxSNP problems are appreciably close to the values known to be achievable by polynomial time algorithms. Finally we note a general approach to the derivation of strong non-approximability results under which the problem reduces to the construction of certain "gadgets." The increasing strength of non-approximability results obtained via the PCP connection motivates us to ask how far this can go, and whether PCPs are inherent in any way. The second part of the paper addresses this. The main result is a "reversal" of the FGLSS connection: where the latter had shown how to translate proof systems for NP into NP-hardness of approximation results for Max Clique, we show how any NP-hardness of approximation result for Max Clique yields a proof system for NP. Roughly our result says that for any constant f if Max Clique is NP-hard to approximate within N 1=(1+f) then NP FPCP[log; f ], the latter being the class of languages possessing proofs of logarithmic randomness and amortized free bit complexity f . This suggests that PCPs are inherent to obtaining non-approximability results. Furthermore the tight relation suggests that reducing the amortized free bit complexity is necessary for improving the non-approximability results for Max Clique. The third part of our paper initiates a systematic investigation of the properties of PCP and FPCP as a function of the various parameters: randomness, query complexity, free bit complexity, amortized free bit complexity, proof size, etc. We are particularly interested in "triviality" results, which indicate which classes are not powerful enough to capture NP. We also distill the role of randomized reductions in this area, and provide a variety of useful transformations between proof y Department of Computer Science and Engineering, University of California at San Diego, La Jolla, CA 92093, USA. E-mail: mihir@cs.ucsd.edu. Supported in part by NSF CAREER Award CCR-9624439 and a 1996 Packard Foundation Fellowship in Science and Engineering. Some of this work was done when the author was at IBM. checking complexity classes.
Abstract-found: 1
Intro-found: 1
Reference: [Ad] <author> L. Adleman. </author> <title> Two theorems on random polynomial time. </title> <booktitle> Proceedings of the 19th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1978, </year> <pages> pp. 75-83. </pages>
Reference-contexts: ) (1 c) c * 1 , s 0 = (1 + * 2 ) s and r 0 = fl + maxf log 2 (* 2 1 (1 c)) ; log 2 (l) log 2 (* 2 Proof: The proof is reminiscent of Adleman's proof that RP P= poly <ref> [Ad] </ref>. Suppose we are given a pcp system for which we want to reduce the randomness complexity. The idea is that it suffices to choose the random pad for the verifier out of a relatively small set of possibilities (instead than from all 2 r possibilities).
Reference: [AKS] <author> M. Ajtai, J. Komlos and E. Szemeredi. </author> <title> Deterministic Simulation in Logspace. </title> <booktitle> Proceedings of the 19th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1987, </year> <pages> pp. 132-140. </pages>
Reference-contexts: This transformation is analogous to the well-known transformation of Berman and Schnitger [BeSc]. Alternatively, using a known deterministic amplification method based on <ref> [AKS, LPS] </ref> one can transform FPCP 1; 1 2 [log; f ] into FPCP 1;2 k [log +2k; k f ]. (To the best of our knowledge this transformation has never appeared with a full proof.) Both alternatives are important ingredients in transforming pcp results into clique in-approximability results via the <p> On the other hand, it is easy to see that that random bits can be recycled for error-reduction via the standard techniques of <ref> [AKS, CW, ImZu] </ref>. The consequence was the first NP-hardness result for Max Clique approximation. <p> Another alternative description, carried out in the proof system, is presented in Section 11. The de-randomized error reduction method consists of applying general, de-randomized, error-reduction techniques to the proof system setting. 3 The best method knows as the "Expander Walk" technique is due to Ajtai, Komlos and Szemeredi <ref> [AKS] </ref> (see also [CW, ImZu]). <p> function k : Z + ! Z + FPCP 1;s [r; f; l] FPCP 1;s k [O (r) + (2 + *) k log (1=s); (1 + *) kf; l]: Actually, the constant in the O-notation is minf1; 2+(4=*) We use random walks on expander graphs for error reduction (cf., <ref> [AKS, CW] </ref>). The value of the constant multiplier of k log (1=s) in the randomness complexity of the resulting pcp system, depends on the expander graph used. Specifically, using a degree d expander graph with second eigenvalue 109 yields a factor of log 2 d 1+log 2 . <p> It is well-known that a random walk of length t in an expander avoids a set of density with probability at most ( + d ) t (cf., <ref> [AKS, Kah] </ref>). Thus, as a preparation step, we reduce the error probability of the pcp system to p = d 2 d This is done using the trivial reduction of Proposition 11.1.
Reference: [AFWZ] <author> N. Alon, U. Feige, A. Wigderson, D. Zuckerman. </author> <title> Derandomized Graph Products. </title> <journal> Computational Complexity, </journal> <volume> Vol. 5, No. 1, </volume> <year> 1995, </year> <pages> pp. 60-75. </pages>
Reference-contexts: [ArSa].) It turns out that the (constant) parameters of the expander, specifically the ratio def log 2 , where d is the degree of the expander and is the second eigenvalue (of its adjacency matrix), play an important 3 An alternative approach, applicable to the Gap-Clique problem and presented in <ref> [AFWZ] </ref>, is to "de-randomize" the graph product construction of [BeSc]. 30 Bellare, Goldreich, Sudan role here.
Reference: [ASE] <author> N. Alon, J. Spencer and P. Erdos. </author> <title> The Probabilistic Method. </title> <publisher> John Wiley and Sons, </publisher> <year> 1992. </year>
Reference: [AmKa] <author> E. Amaldi and V. Kann. </author> <title> The complexity and approximability of finding maximum feasible subsystems of linear relations. </title> <journal> Theoretical Computer Science, </journal> <volume> Vol. 147, </volume> <year> 1995, </year> <pages> pp 181-210. </pages>
Reference-contexts: Also the case of maximum satisfiable linear constraints over larger fields (of size q) has been considered by Amaldi and Kann <ref> [AmKa] </ref>, who show that this problem is hard to approximate to within a factor of q * for some universal * &gt; 0. See Section 4.2.2 for our results. Max Cut. In 1976, Sahni and Gonzales [SaGo] gave a simple 2-approximation algorithm for this problem.
Reference: [Ar] <author> S. Arora. </author> <title> Reductions, Codes, </title> <booktitle> PCPs and Inapproximability. Proceedings of the 36th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1995, </year> <pages> pp. 404-413. </pages>
Reference-contexts: Detailed histories for specific topics are given in Sections 2.2.3 and 2.4.3. 1.4 Related work Following the presentation of our results, Arora has also investigated the limitations of proof checking techniques in proving non-approximability results <ref> [Ar] </ref>. Like in our free-bit lower bound result, he tries to assess the limitations of current techniques by making some assumptions about these techniques and then showing a lower bound.
Reference: [ABSS] <author> S. Arora, L. Babai, J. Stern and Z. Sweedyk. </author> <title> The hardness of approximate optima in lattices, codes and systems of linear equations. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> Vol. 54, No. 2, </volume> <year> 1997, </year> <pages> pp. 317-331. </pages>
Reference-contexts: Linear equations. The MaxLinEq problem is known to be Max-SNP complete (see [BrNa] or [Pet]). We remark that the problem of maximizing the number of satisfiable equations should not be confused with the "complementary" problem of minimizing the number of violated constraints, investigated by Arora et. al. <ref> [ABSS] </ref>. Also the case of maximum satisfiable linear constraints over larger fields (of size q) has been considered by Amaldi and Kann [AmKa], who show that this problem is hard to approximate to within a factor of q * for some universal * &gt; 0.
Reference: [ALMSS] <author> S. Arora, C. Lund, R. Motwani, M. Sudan and M. Szegedy. </author> <title> Proof verification and intractability of approximation problems. </title> <booktitle> Proceedings of the 33rd Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1992, </year> <pages> pp. 14-23. </pages>
Reference-contexts: First, one exhibits an appropriate proof system for NP. Then one applies the FGLSS reduction. The factor indicated hard depends on the Free Bits in PCP 7 proof system parameters. A key element in getting better results has been the distilling of appropriate pcp-parameters. The sequence of works <ref> [FGLSS, ArSa, ALMSS, BGLR, FeKi1, BeSu] </ref> lead us through a sequence of parameters: query complexity, free-bit complexity and, finally, for the best known results, amortized free-bit complexity. <p> O (2 m ) 2 3m 2m 3m free-bits [BeSu] 8 Bellare, Goldreich, Sudan Problem Approx Non-Approx Factor Due to Our Factor Previous Factor Assumption Max3SAT 1:258 [Yan, GoWi2, TSSW] 1:038 1 + 1 72 [BeSu] P 6= NP MaxE3SAT 1 + 1 7 folklore 1 + 1 26 unspecified <ref> [ALMSS] </ref> P 6= NP Max2SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP MaxSAT 2 folklore 1 + 1 7 P 6= NP MaxCUT 1:139 [GoWi2] 1:014 unspecified [ALMSS] P 6= NP MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= <p> 1 72 [BeSu] P 6= NP MaxE3SAT 1 + 1 7 folklore 1 + 1 26 unspecified <ref> [ALMSS] </ref> P 6= NP Max2SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP MaxSAT 2 folklore 1 + 1 7 P 6= NP MaxCUT 1:139 [GoWi2] 1:014 unspecified [ALMSS] P 6= NP MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o <p> 26 unspecified <ref> [ALMSS] </ref> P 6= NP Max2SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP MaxSAT 2 folklore 1 + 1 7 P 6= NP MaxCUT 1:139 [GoWi2] 1:014 unspecified [ALMSS] P 6= NP MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 coR ~ P Number N 1 1 <p> We have the advantage of being able to use, at the outer level, the recent verifier of Raz [Raz], which was not available to previous authors. The inner level verifier relies on the use of a "good" encoding scheme. Beginning with <ref> [ALMSS] </ref>, constructions of this verifier have used the Hadamard Code; in this paper we use instead the long code. 1.2.2 Proofs and approximation: Potential and limits As the above indicates, non-approximability results are getting steadily stronger, especially for Max Clique. <p> After the work of [FGLSS] the field took off in two major directions. One was to extend the interactive proof approach to prove the non-approximability of other optimization problems. Direct reductions from proofs were used to show the hardness of quadratic programming [BeRo, FeLo], Max3SAT <ref> [ALMSS] </ref>, set cover [LuYa], and other problems [Be]. The earlier work of Papadimitriou and Yannakakis introducing the class MaxSNP [PaYa] now came into play; by reduction from Max3SAT it implied hardness of approximation for any MaxSNP-hard problem. <p> This work introduced the idea of recursive proof checking, which turned out to play a fundamental role in all subsequent developments. Interestingly, the idea of encoding inputs in an error-correcting form (as suggested in [BFLS]) is essential to make "recursion" work. Arora, Lund, Motwani, Sudan and Szegedy <ref> [ALMSS] </ref> reduced the query complexity of pcp systems for NP to a constant, while preserving the logarithmic randomness complexity; namely, they showed that NP = PCP 1;1=2 [log; O (1)]. This immediately implied the NP-hardness of approximating Max Clique within N * , for some * &gt; 0. <p> This immediately implied the NP-hardness of approximating Max Clique within N * , for some * &gt; 0. Furthermore, it also implied that Max-3-Sat is NP-hard to approximate to within some constant factor <ref> [ALMSS] </ref> and so is any MaxSNP-hard problem [PaYa]. The second stage of this enterprise started with the work of Bellare, Goldwasser, Lund and Russell [BGLR]. <p> Query complexity minimization. One seeks results of the form NP = PCP 1;1=2 [ coins = log ; query = q ; query av = q av ] : (4) 20 Bellare, Goldreich, Sudan Due to q q av <ref> [ALMSS] </ref> some constant some constant [BGLR] 36 29 [FeKi1] 32 24 This paper 11 10.9 logarithmic randomness; that is, results of the form of Eq. (4). <p> Specifically, we consider the question of determining the values of q; q av for which Eq. (4) holds. The fundamental result of <ref> [ALMSS] </ref> states that q; q av may be constants (independent of the input length). Reductions in the values of these constants were obtained since then and are depicted in Role of constant-prover proofs in PCP perspective. Constant-prover proofs have been instrumental in the derivation of non-approximability results in several ways. <p> However, it is a different aspect of constant prover proofs that is of more direct concern to us. This aspect is the use of constant-prover proof systems as the penultimate step of the recursion, and begins with <ref> [ALMSS] </ref>. It is instrumental in getting PCP systems with only a constant number of queries. Their construction requires that these proof systems have low complexity: error which is any constant, and randomness and answer sizes that are preferably logarithmic. <p> The available constant-prover proof systems appear in Figure 5 and are discussed below. Throughout this discussion we consider proof systems obtaining an arbitrary small constant error probability. The two-prover proofs of Lapidot-Shamir and Feige-Lovasz [LaSh, FeLo] had poly-logarithmic randomness and answer sizes, so <ref> [ALMSS] </ref> used a modification of these, in the process increasing the number of provers to a constant much larger than two. The later constructions of few-prover proofs of [BGLR, Tar, FeKi1] lead to better non-approximability results. <p> A breakthrough result in this area is Raz's Parallel Repetition Theorem which implies the ex Free Bits in PCP 21 Due to Provers Coins Answer size Canonical? Can be made canonical? [LaSh, FeLo] 2 polylog polylog No Yes [BeSu] <ref> [ALMSS] </ref> poly (* 1 ) log polylog No ? [BGLR] 4 log polyloglog No ? [Tar] 3 log O (1) No ? [FeKi1] 2 log O (1) No At cost of one more prover [BeSu] [Raz] 2 log O (1) Yes (NA) istence of a two-provers proof system with logarithmic randomness <p> We will use these notations in what follows. By the above, MaxXSAT (') is the maximum number of simultaneously satisfiable clauses in '. We abuse notation slightly by dropping the "X", writing just MaxSAT ('). Similarly for the normalized version. 26 Bellare, Goldreich, Sudan Due to Assuming Factor Technique <ref> [ALMSS] </ref> P 6= NP some constant NP PCP 1;1=2 [ log; O (1) ]; Reduction of this to Max3SAT. [BGLR] e P 6= N e P 94=93 Framework; better analyses; uses proof systems of [LaSh, FeLo]. [BGLR] P 6= NP 113=112 New four-prover proof systems. [FeKi1] P 6= NP 94=93 New <p> Specifically, Goemans and Williamson [GoWi2] exhibited a polynomial time algorithm achieving an approximation factor of 1 0:878 1:139, and subsequently Feige and Goemans [FeGo] exhibited an algorithm achieving 1 0:931 1:074. Non-approximability results for Max-SNP problems begin with <ref> [ALMSS] </ref> who proved that there exists a constant * &gt; 0 such that Gap-3SAT 1;1* is NP-hard. <p> The basic paradigm of their reduction has been maintained in later improvements. factor) begin with [BGLR]. They used Hadamard code based inner verifiers following <ref> [ALMSS] </ref>. They also introduced a framework for better analysis, and improved some previous analyses; we exploit in particular their better analyses of linearity testing (cf. Section 3.5) and of Freivalds's matrix multiplication test (cf. Lemma 3.16). <p> Recently, in a breakthrough result, Goemans and Williamson [GoWi2] gave a new algorithm which achieves a ratio of 1 0:878 = 1:139 for this problem. On the other hand, [PaYa] give an approximation preserving reduction from Max3SAT to MaxCUT. Combined with <ref> [ALMSS] </ref> this shows that there exists a constant ff &gt; 1 such that approximating MaxCUT within a factor of ff is NP-hard. <p> The version of MinVC in which one restricts attention to graphs of degree bounded by a constant B, is Max-SNP complete for suitably large B [PaYa]. In particular they provide a reduction from Max3SAT. Combined with <ref> [ALMSS] </ref> this implies the existence of a constant ffi &gt; 0 such that approximating MinVC within a factor of 1 + ffi is hard unless P = NP. No explicit value of ffi has been stated until now. <p> On the other hand, it is easy to see that that random bits can be recycled for error-reduction via the standard techniques of [AKS, CW, ImZu]. The consequence was the first NP-hardness result for Max Clique approximation. The corresponding factor was 2 p Arora et. al. <ref> [ALMSS] </ref> showed that NP = PCP 1;1=2 [ coins = log ; query = O (1) ], which implied that there exists an * &gt; 0 for which approximating Max Clique within N * was NP-complete. <p> In all reductions ffi (*) = minfh (*); h (0:5)g, for some function h. The bigger h, the better the reduction. The first reduction, namely that of Lund and Yannakakis [LuYa], obtained h (*) = *=(5 4*). Via the Max Clique hardness results of <ref> [ArSa, ALMSS] </ref> this implies the chromatic number is hard to 2 Actually all the reductions presented here, make assumptions regarding the structure of the graph and hence do not directly yield the hardness results stated here. <p> See Section 8.4 for details. Free Bits in PCP 29 Due to Factor Assumption [FGLSS] 2 log 1* N for any * &gt; 0 NP 6 e P [ArSa] 2 p <ref> [ALMSS] </ref> N * for some * &gt; 0 P 6= NP [BGLR] N 1=30 NP 6= coRP [BGLR] N 1=25 NP 6 coR e P [FeKi1] N 1=15 NP 6= coRP [BeSu] N 1=6 P 6= NP [BeSu] N 1=4 NP 6 coR e P This paper N 1=4 P 6= <p> This makes an ideal starting point. To simplify the definitions below we insisted on constant answer size and two provers from the start. The inner verifiers used in all previous works are based on the use of the Hadamard code constructions of <ref> [ALMSS] </ref>. (The improvements mentioned above are obtained by checking this same code in more efficient ways). We instead use a new code, namely the long code, as the basis of our inner verifiers. <p> Employing the FRS-method [FRS] to any PCP (log,O (1))-system for NP (e.g., <ref> [ALMSS] </ref>) one gets a canonical verifier which is ffi-good for some ffi &lt; 1. (Roughly, the method is to take the given pcp system, send all queries to one oracle, and, as a check, a random one of them to the other oracle.) Using the Parallel Repetition Theorem of Raz, we <p> The latter explains why we are interested in outer verifiers which achieve a constant, but arbitrarily small, error *. For completeness we provide a proof, following the ideas of <ref> [ArSa, ALMSS, BGLR] </ref>. 39 Theorem 3.12 (the composition theorem): Let V outer be a (l; l 1 )-canonical outer verifier. Suppose it is *-good for L. Let V inner be an (l; l 1 )-canonical inner verifier that is (; ffi 1 ; ffi 2 )-good. <p> Their analysis was used in the proof system and Max3SAT non-approximability result of <ref> [ALMSS] </ref>. Interest in the tightness of the analysis began with [BGLR], with the motivation of improving the Max3SAT non-approximability results. They showed that r A 3x A 6x 2 A , for every A. <p> As for c 0 , it equals 3ff 1 +2ff 2 3m 1 +2m 2 &gt; 0:6. 5 Free bits and vertex cover It is known that approximating the minimum vertex cover of a graph to within a 1 + * factor is hard, for some * &gt; 0 <ref> [PaYa, ALMSS] </ref>. However, we do not know of any previous attempt to provide a lower bound for *. <p> The condition regarding the distance of the code is essential since the task is easy with respect to the identity map (which is a code of distance 1). We remark that testing "closeness" to codewords with respect to codes of large distance is essential in all known pcp constructions <ref> [BFLS, FGLSS, ArSa, ALMSS, BGLR, FeKi1, BeSu] </ref>. The absolute distance between two words w; u 2 f0; 1g n , denoted (w; u), is the number of bits on which w and u disagree. <p> Some of the qualitative assertions are well-known; for example, when considering perfect completeness, 3 queries are the minimum needed (and sufficient <ref> [ALMSS] </ref>) to get above P.
Reference: [ArSa] <author> S. Arora and S. Safra. </author> <title> Probabilistic checking of proofs: a new characterization of NP. </title> <booktitle> Proceedings of the 33rd Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1992, </year> <pages> pp. 2-13. </pages>
Reference-contexts: First, one exhibits an appropriate proof system for NP. Then one applies the FGLSS reduction. The factor indicated hard depends on the Free Bits in PCP 7 proof system parameters. A key element in getting better results has been the distilling of appropriate pcp-parameters. The sequence of works <ref> [FGLSS, ArSa, ALMSS, BGLR, FeKi1, BeSu] </ref> lead us through a sequence of parameters: query complexity, free-bit complexity and, finally, for the best known results, amortized free-bit complexity. <p> We show that this code is also easily "testable" and "correctable", and derive the new proof systems based on this. As in all recent constructions of efficient pcp's our construction also relies on the use of recursive construction of verifiers, introduced by Arora and Safra <ref> [ArSa] </ref>. We have the advantage of being able to use, at the outer level, the recent verifier of Raz [Raz], which was not available to previous authors. The inner level verifier relies on the use of a "good" encoding scheme. <p> The other direction was to increase factors, and reduce assumptions, for existing hardness of approximation results. This involves improving the efficiency of the underlying proof systems and/or the efficiency of the reductions. The first stage of this enterprise started with the work of Arora and Safra <ref> [ArSa] </ref>. They showed that NP PCP 1;1=2 [log; o (log)]. This provided the first strong NP-hardness result for Max Clique (specifically, a hardness factor of 2 p log N ). <p> The FGLSS-reduction (cf., [FGLSS]), stated in terms of the query complexity (number of binary queries), randomness complexity and error probability of the proof system, has focused attention on the above model and these parameters. The class PCP 1;1=2 [r; q] was made explicit by <ref> [ArSa] </ref>. The parameterization was expanded by [BGLR] to explicitly consider the answer size (the oracle was allowed to return more than one bit at a time) and query size- their notation included five parameters: randomness, number of queries, size of each query, size of each answer, and error probability. <p> They also initiated work on improving the factors and assumptions via better proof systems. The best result in their paper is indicated in Figure 7. Arora and Safra <ref> [ArSa] </ref> reduced the randomness complexity of a PCP verifier for NP to logarithmic | they showed NP = PCP 1;1=2 [ coins = log ; query = p log N ]. <p> In all reductions ffi (*) = minfh (*); h (0:5)g, for some function h. The bigger h, the better the reduction. The first reduction, namely that of Lund and Yannakakis [LuYa], obtained h (*) = *=(5 4*). Via the Max Clique hardness results of <ref> [ArSa, ALMSS] </ref> this implies the chromatic number is hard to 2 Actually all the reductions presented here, make assumptions regarding the structure of the graph and hence do not directly yield the hardness results stated here. <p> See Section 8.4 for details. Free Bits in PCP 29 Due to Factor Assumption [FGLSS] 2 log 1* N for any * &gt; 0 NP 6 e P <ref> [ArSa] </ref> 2 p [ALMSS] N * for some * &gt; 0 P 6= NP [BGLR] N 1=30 NP 6= coRP [BGLR] N 1=25 NP 6 coR e P [FeKi1] N 1=15 NP 6= coRP [BeSu] N 1=6 P 6= NP [BeSu] N 1=4 NP 6 coR e P This paper N <p> It is easy to see that this applies in the pcp context. (The usage of these methods in the pcp context begins with <ref> [ArSa] </ref>.) It turns out that the (constant) parameters of the expander, specifically the ratio def log 2 , where d is the degree of the expander and is the second eigenvalue (of its adjacency matrix), play an important 3 An alternative approach, applicable to the Gap-Clique problem and presented in [AFWZ], <p> Thus, verification in it amounts to checking that the first answer satisfies some predicate and that the second answer equals the value obtained from the first answer. Following the "proof composition" paradigm of Arora and Safra <ref> [ArSa] </ref>, the proof string provided to the PCP verifier will consist of "encodings" of the answers of the two provers under a suitable code. The PCP verifier will then check these encodings. <p> It is useful to understand these things before proceeding to the tests. Overview. The constructions of efficient proofs that follow will exploit the notion of recursive verifier construction due to Arora and Safra <ref> [ArSa] </ref>. We will use just one level of recursion. We first define a notion of a canonical outer verifier whose intent is to capture two-prover one-round proof systems [BGKW] having certain special properties; these verifiers will be our starting point. We then define 36 a canonical inner verifier. <p> The latter explains why we are interested in outer verifiers which achieve a constant, but arbitrarily small, error *. For completeness we provide a proof, following the ideas of <ref> [ArSa, ALMSS, BGLR] </ref>. 39 Theorem 3.12 (the composition theorem): Let V outer be a (l; l 1 )-canonical outer verifier. Suppose it is *-good for L. Let V inner be an (l; l 1 )-canonical inner verifier that is (; ffi 1 ; ffi 2 )-good. <p> The condition regarding the distance of the code is essential since the task is easy with respect to the identity map (which is a code of distance 1). We remark that testing "closeness" to codewords with respect to codes of large distance is essential in all known pcp constructions <ref> [BFLS, FGLSS, ArSa, ALMSS, BGLR, FeKi1, BeSu] </ref>. The absolute distance between two words w; u 2 f0; 1g n , denoted (w; u), is the number of bits on which w and u disagree.
Reference: [Bab] <author> L. Babai. </author> <title> Trading Group Theory for Randomness. </title> <booktitle> Proceedings of the 17th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1985, </year> <pages> pp. 421-429. </pages>
Reference-contexts: The indication of higher factors, and results for other problems, had to wait for the interactive proof approach. Interactive proofs were introduced by Goldwasser, Micali and Rackoff [GMR] and Babai <ref> [Bab] </ref>. Ben-Or, Goldwasser, Kilian and Wigderson [BGKW] extended these ideas to define a notion of multi-prover interactive proofs. <p> 3 queries): (1) (PCP with 1 query is relatively very weak): For all admissible functions s; c : Z + ! [0; 1], so that c (n) s (n) is non-negligible 10 PCP c;s [poly; 1] AM where AM is the class of languages having one-round Arthur-Merlin proof systems (cf., <ref> [Bab] </ref>). (2) (One-sided error pcp with 2 queries is relatively weak): For all admissible functions s : Z + ! [0; 1] strictly less than 1, PCP 1;s [poly; 2] PSPACE. (3) (Two-sided error pcp with 2 queries is not weak): On the other hand, there exists 0 &lt; s &lt; <p> We stress that all the transformations maintain the number of rounds upto a constant and that the constant-round Arthur-Merlin hierarchy collapses to one-round <ref> [Bab] </ref>. Proof of Proposition 10.8, Parts (3) and (4): For these parts we observe that the proof systems used in the corresponding parts of the proof of Proposition 10.3, do "scale-up".
Reference: [BFL] <author> L. Babai, L. Fortnow and C. Lund. </author> <title> Non-deterministic Exponential time has two-prover interactive protocols. </title> <journal> Computational Complexity, </journal> <volume> Vol. 1, </volume> <year> 1991, </year> <pages> pp. 3-40. </pages> <note> (See also addendum in Vol. 2, </note> <year> 1992, </year> <pages> pp. 374.) </pages>
Reference-contexts: Trying to understand the power of pcp systems with low free-bit complexity, we have waived the bound on the randomness complexity. Recall that in this case pcp systems are able to recognize 12 Bellare, Goldreich, Sudan non-deterministic exponential time (i.e., NEXPT = PCP 1;1=2 [poly; poly]) <ref> [BFL] </ref>. <p> These techniques were used by Shamir [Sha] to show that IP = PSPACE. A central result that enabled the connection to hardness of approximation is that of Babai, Fortnow and Lund <ref> [BFL] </ref>. They showed that the class MIP equals the class NEXP (i.e., languages recognizable in non-deterministic exponential time). The latter result has been "scaled-down" to the NP-level by two independent groups of researchers. <p> In particular, these 28 Bellare, Goldreich, Sudan authors were able to "scale-down" the proof system of <ref> [BFL] </ref> to indicate strong non-approximability factors of 2 log 1* N for all * &gt; 0, assuming NP is not in quasi-polynomial deterministic time. They also initiated work on improving the factors and assumptions via better proof systems. The best result in their paper is indicated in Figure 7. <p> Proof systems with unrestricted randomness (as considered in the next proposition) may also provide some indication to the effect of very low query complexity. The results we obtain are somewhat analogous to those of Proposition 10.3. Recall that PCP 1; 1 2 [poly; poly] equals NEXPT (Non-deterministic exponential time) <ref> [BFL] </ref>.
Reference: [BFLS] <author> L. Babai, L. Fortnow, L. Levin, and M. Szegedy. </author> <title> Checking computations in poly-logarithmic time. </title> <booktitle> Proceedings of the 23rd Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1991, </year> <pages> pp. 21-31. </pages>
Reference-contexts: They showed that the class MIP equals the class NEXP (i.e., languages recognizable in non-deterministic exponential time). The latter result has been "scaled-down" to the NP-level by two independent groups of researchers. Babai, Fortnow, Lund and Szegedy <ref> [BFLS] </ref> showed that if the input is encoded using a special error-correcting code (for which encoding and decoding can be performed in polynomial-time) then NP has transparent proof systems (i.e., it is possible to verify the correctness of the proof in poly-logarithmic time). <p> This work introduced the idea of recursive proof checking, which turned out to play a fundamental role in all subsequent developments. Interestingly, the idea of encoding inputs in an error-correcting form (as suggested in <ref> [BFLS] </ref>) is essential to make "recursion" work. Arora, Lund, Motwani, Sudan and Szegedy [ALMSS] reduced the query complexity of pcp systems for NP to a constant, while preserving the logarithmic randomness complexity; namely, they showed that NP = PCP 1;1=2 [log; O (1)]. <p> The model underlying what are now known as "probabilistically checkable proofs" is the "oracle model" of Fortnow, Rompel and Sipser [FRS], introduced as an equivalent version (with respect to language recognition power) of the multi-prover model of Ben-Or, Goldwasser, Kilian and Wigderson [BGKW]. Interestingly, as shown by <ref> [BFLS, FGLSS] </ref>, this framework can be applied in a meaningful manner also to languages in NP. <p> These works provide the verifier V with a "written" proof, modeled as an oracle to which V provides the "address" of a bit position in the proof string and is returned the corresponding bit of the proof. Babai et. al. <ref> [BFLS] </ref> suggested a model in which the inputs are encoded in a special (polynomial-time computable and decodable) error-correcting code and the verifier works in poly-logarithmic time. <p> Free bits are implicit in [FeKi1] and formalized in [BeSu]. Amortized free bits are introduced in [BeSu] but formalized a little better here. Proof sizes were considered in <ref> [BFLS, PoSp] </ref>. We consider them here for a different reason they play an important role in that the randomized FGLSS reduction [BeSc, Zuc] depends actually on this parameter (rather than on the randomness complexity). <p> The condition regarding the distance of the code is essential since the task is easy with respect to the identity map (which is a code of distance 1). We remark that testing "closeness" to codewords with respect to codes of large distance is essential in all known pcp constructions <ref> [BFLS, FGLSS, ArSa, ALMSS, BGLR, FeKi1, BeSu] </ref>. The absolute distance between two words w; u 2 f0; 1g n , denoted (w; u), is the number of bits on which w and u disagree.
Reference: [BaEv1] <author> R. Bar-Yehuda and S. </author> <title> Even. A linear time approximation algorithm for the weighted vertex cover problem. </title> <journal> Journal of Algorithms, </journal> <volume> Vol. 2, </volume> <year> 1981, </year> <pages> pp. 198-201. </pages>
Reference-contexts: Vertex cover. There is a simple polynomial time algorithm to approximate MinVC in unweighted graphs within a factor of 2. The algorithm, due to F. Gavril (cf. [GJ2]), consists of taking all vertices which appear in a maximal matching of the graph. For weighted graphs, Bar-Yehuda and Even <ref> [BaEv1] </ref> and Hochbaum [Hoc], gave algorithms achieving the same approximation factor. The best known algorithm today achieves a factor only slightly better, namely 2 (log log jV j)=(2 log jV j) [BaEv2, MoSp].
Reference: [BaEv2] <author> R. Bar-Yehuda and S. </author> <title> Even. A local ratio theorem for approximating the weighted vertex cover problem. In Analysis and Design of Algorithms for Combinatorial Problems, </title> <journal> Vol. 25 of Annals of Discrete Math, </journal> <volume> Elsevier, </volume> <year> 1985. </year>
Reference-contexts: + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max2SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP MaxSAT 2 folklore 1 + 1 7 P 6= NP MaxCUT 1:139 [GoWi2] 1:014 unspecified [ALMSS] P 6= NP MinVC 2 o (1) <ref> [BaEv2, MoSp] </ref> 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 <p> We are obtaining the first explicit and reasonable non-approximability factor for Max2SAT, MaxCUT and minimum Vertex Cover. Recall that the latter is approximable within 2-o (1) <ref> [BaEv2, MoSp] </ref>. Our results for MaxCUT and Max2SAT show that it is infeasible to find a solution with value which is only a factor of 1.01 from optimal. <p> 1 7 folklore 1 + 1 26 1 + 1 Max2SAT 1:075 [GoWi2, FeGo] 1:013 1:047 [H3] P 6= NP No MaxSAT 2 folklore 1 + 1 7 * 2 * [H3] P 6= NP Yes MaxCUT 1:139 [GoWi2] 1:014 1:062 [H3] P 6= NP No MinVC 2 o (1) <ref> [BaEv2, MoSp] </ref> 1 + 1 15 1 + 1 Max-Clique N 1o (1) [BoHa] N 1 3 * N 1* [H2] coRP 6= NP Yes N 4 * N 2 * [H2] P 6= NP No Chromatic No. <p> For weighted graphs, Bar-Yehuda and Even [BaEv1] and Hochbaum [Hoc], gave algorithms achieving the same approximation factor. The best known algorithm today achieves a factor only slightly better, namely 2 (log log jV j)=(2 log jV j) <ref> [BaEv2, MoSp] </ref>.
Reference: [BaMo] <author> R. Bar-Yehuda and S. Moran. </author> <title> On approximation problems related to the independent set and vertex cover problems. </title> <journal> Discrete Applied Mathematics, </journal> <volume> Vol. 9, </volume> <year> 1984, </year> <pages> pp. 1-10. 115 </pages>
Reference-contexts: the hardness of approximating MinVC was given by Bar-Yehuda and Moran who showed that, for every k 2 and * &gt; 0, a 1 + 1 k * approximator for (finding) a minimum vertex cover would yield an algorithm for coloring (k + 1)-colorable graphs using only logarithmically many colors <ref> [BaMo] </ref>. The version of MinVC in which one restricts attention to graphs of degree bounded by a constant B, is Max-SNP complete for suitably large B [PaYa]. In particular they provide a reduction from Max3SAT.
Reference: [Be] <author> M. Bellare. </author> <title> Interactive proofs and approximation: reductions from two provers in one round. </title> <booktitle> Proceedings of the Second Israel Symposium on Theory and Computing Systems, IEEE, </booktitle> <year> 1993, </year> <pages> pp. 266-274. </pages>
Reference-contexts: One was to extend the interactive proof approach to prove the non-approximability of other optimization problems. Direct reductions from proofs were used to show the hardness of quadratic programming [BeRo, FeLo], Max3SAT [ALMSS], set cover [LuYa], and other problems <ref> [Be] </ref>. The earlier work of Papadimitriou and Yannakakis introducing the class MaxSNP [PaYa] now came into play; by reduction from Max3SAT it implied hardness of approximation for any MaxSNP-hard problem. Also, reductions from Max Clique lead to hardness results for the chromatic number [LuYa] and other problems [Zuc].
Reference: [BCHKS] <author> M. Bellare, D. Coppersmith, J. H -astad, M. Kiwi and M. Sudan. </author> <title> Linearity testing in characteristic two. </title> <journal> IEEE Transactions on Information Theory Vol. </journal> <volume> 42, No. 6, </volume> <month> November </month> <year> 1996, </year> <pages> pp. 1781-1795. </pages>
Reference-contexts: The improvement in the complexities of the proof systems is the main source of our improved non-approximability results. In addition we also use (for the Max-SAT and Max-CUT problems) a recent improvement in the analysis of linearity testing <ref> [BCHKS] </ref>, and introduce (problem specific) gadgets which represent the various tests of the resulting PCP system. 3.2 Preliminaries to the Long Code Here = f0; 1g will be identified with the finite field of two elements, the field operations being addition and multiplication modulo two. <p> The idea is to exploit the characterization of Proposition 3.2. Thus we will perform (on A) a linearity test, and then a "Respect of Monomial Basis" test. Linearity testing is well understood, and we will use the test of [BLR], with the analyses of <ref> [BLR, BGLR, BCHKS] </ref>. The main novelty is the Respect of Monomial Basis Test. Circuit and projection. Having established that A is close to some evaluation operator E a , we now want to test two things. The first is that h (a) = 0 for some predetermined function h. <p> We want to lower bound the probability 1 LinPass (A) that the test rejects when its inputs f 1 ; f 2 are chosen at random, as a function of x = Dist (A; Lin). The following lemma, due to Bellare et. al. <ref> [BCHKS] </ref>, gives the best known lower bound today. Lemma 3.15 [BCHKS] Let A: F l ! and let x = Dist (A; Lin). <p> The following lemma, due to Bellare et. al. <ref> [BCHKS] </ref>, gives the best known lower bound today. Lemma 3.15 [BCHKS] Let A: F l ! and let x = Dist (A; Lin). <p> It was shown in <ref> [BCHKS] </ref> (see below) that this combined lower bound is close to the best one possible. Perspective. <p> The work of <ref> [BCHKS] </ref> focused on this case and improved the bound on r A for the case x A 1 4 where A: GF (2) n ! GF (2). Specifically, they showed that r A 45=128 for 43 4 which establishes the second segment of lin . <p> They also showed that r A x A , for every A: GF (2) n ! GF (2). Combining the three lower bounds, they have derived the three-segment lower bound stated in Lemma 3.15. The optimality of the above analysis has been demonstrated as well in <ref> [BCHKS] </ref>. Essentially 4 , for every x 5=16 there are functions A: GF (2) n ! GF (2) witnessing r A = lin (x A ) with x A = x. For the interval ( 5 16 ; 1 2 ], no tight results are known. Instead, [BCHKS] reports of computer <p> as well in <ref> [BCHKS] </ref>. Essentially 4 , for every x 5=16 there are functions A: GF (2) n ! GF (2) witnessing r A = lin (x A ) with x A = x. For the interval ( 5 16 ; 1 2 ], no tight results are known. Instead, [BCHKS] reports of computer constructed examples of functions A: GF (2) n ! GF (2) with x A in every interval [ k 100 ; k+1 and r A &lt; lin (x A ) + 1 20 . <p> Thus, a quantitative comparison to previous works is not readily available. Certainly, we improve over these works thanks to the use of the new LongCode-based inner-verifier, the atomic tests and their analysis in Section 3.5, the new idea of folding, and the improved analysis of linearity testing due to <ref> [BCHKS] </ref>. 4.1.3 Another application: minimizing soundness error in 3-query pcp As a direct corollary to Proposition 4.3, we obtain Theorem 4.5 For any s &gt; 0:85, NP PCP 1;s [ coins = log ; query = 3 ; free = 2 ]. 4.2 Satisfiability problems In this section we mainly deal
Reference: [BGG] <author> M. Bellare, O. Goldreich and S. Goldwasser. </author> <title> Randomness in interactive proofs. </title> <journal> Computational Complexity, </journal> <volume> Vol. 3, No. 4, </volume> <year> 1993, </year> <pages> pp. 319-354. </pages>
Reference-contexts: V 0 will then query the oracle as to which random-pad to use, in the simulation of V , and complete its computation by invoking V with the specified random-pad. To generate the "pseudorandom" sequence we use the sampling procedure of <ref> [BGG] </ref>. Specifically, for m 1=c this merely amounts to generating a pairwise independent sequence of uniformly distributed strings in f0; 1g r , which can be done using randomness maxf2r; 2 log 2 mg. Otherwise (i.e., for m &gt; 1=c) the construction of [BGG] amounts to generating fi (cm) such related <p> sequence we use the sampling procedure of <ref> [BGG] </ref>. Specifically, for m 1=c this merely amounts to generating a pairwise independent sequence of uniformly distributed strings in f0; 1g r , which can be done using randomness maxf2r; 2 log 2 mg. Otherwise (i.e., for m &gt; 1=c) the construction of [BGG] amounts to generating fi (cm) such related sequences, where the sequences are related via a random walk on a constant degree expander. Part (2) follows. The following corollary exemplifies the usage of the above proposition.
Reference: [BGS1] <author> M. Bellare, O. Goldreich and M. Sudan. </author> <title> Free Bits, PCPs and Non-Approximability | Towards Tight Results. Extended abstract of this paper, </title> <booktitle> Proceedings of the 36th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1995, </year> <pages> pp. 422-431. </pages>
Reference-contexts: c;s [log; f ] is contained in the class FPCP 1; log c s [log; f + log (1=c) + log log] (rather than being randomly reducible to it). 1.7 Previous versions of this paper An extended abstract of this work appeared in the proceedings of the FOCS 95 conference <ref> [BGS1] </ref>. It was backed up by the first versions of this large manuscript [BGS2], posted on ECCC. The paper went through several revisions due to improvements and corrections in the results. These were regularly posted on ECCC (as revisions to [BGS2]).
Reference: [BGS2] <author> M. Bellare, O. Goldreich and M. Sudan. </author> <title> Free Free Bits, PCPs and Non-Approximability | Towards Tight Results. Preliminary versions of this paper. </title> <booktitle> TR95-024 of ECCC, the Electronic Colloquium on Computational Complexity. </booktitle> <month> May </month> <year> 1995 </year> <month> (revised Sept. </month> <year> 1995, </year> <month> Jan. </month> <year> 1996, </year> <month> Dec. </month> <note> 96). See http://www.eccc.uni-trier.de/eccc/. </note>
Reference-contexts: The latter results were obtained subsequently to the current work, and while building on it. Amortized free-bits and Max-Clique. The most intriguing problem left open by our work has been resolved by H-astad [H1, H2]. He proved our conjecture (cf., <ref> [BGS2] </ref>) by which, for every * &gt; 0, it is the case that NP FPCP [log; *]. The Long-Code, introduced in this work, plays a pivotal role in H-astad's work. He also uses the idea of folding. <p> They were able to similarly modify H-astad's proof systems [H1, H2] and thereby improve the hard factor to N 1* , for any * &gt; 0. Gadgets. Another research direction, suggested in early versions of this work <ref> [BGS2] </ref>, was taken on by Trevisan et. al. [TSSW] who initiated a systematic study of the construction of gadgets. In particular, they showed that the gadgets we have used in our reductions to the MaxSAT problems were optimal, and constructed better (and optimal) gadgets for reduction to MaxCUT. Weights. <p> Essentially, they show that the unweighted cases of all problems considered in our paper are as hard as the weighted cases. 1.6 Directions for further research Although the most intriguing open problems suggested in previous versions of this work <ref> [BGS2] </ref> have been resolved, some very interesting problems remain. We mention a few. 2-free-bits proofs and Min-VC. As we show, NP FPCP c;s [log; f ] implies that approximating Min Vertex-Cover up to a 2 f s 2 f c factor is NP-hard. <p> It was backed up by the first versions of this large manuscript <ref> [BGS2] </ref>, posted on ECCC. The paper went through several revisions due to improvements and corrections in the results. These were regularly posted on ECCC (as revisions to [BGS2]). <p> It was backed up by the first versions of this large manuscript <ref> [BGS2] </ref>, posted on ECCC. The paper went through several revisions due to improvements and corrections in the results. These were regularly posted on ECCC (as revisions to [BGS2]). This is the fifth version of the work. 1.8 Organization This introduction is followed by a section that contains definitions as well as detailed histories. <p> The resulting test is A (f 1 ) A (f 2 ) = A (f 1 f 2 + f 3 ) A (f 3 ) (7) This test was analyzed in a previous version of this work <ref> [BGS2] </ref>; specifically, this test was shown to reject a folded oracle A, with ~ A (the linear function closest to A) which does not respect the monomial basis, with probability at least (1 2x) ( 3 8 x + x 2 8 7 2 x 2 x 3 , where x <p> Thus, adaptivity is used to improve the performance of our verifier and to strengthen the non-approximability results which follow (cf., previous versions of this paper <ref> [BGS2] </ref>). The inner verifier, V SNPinner , takes the usual length parameters l; l 1 as well as additional (probability) parameters p 1 ; p 2 and p 3 such that p 1 + p 2 + p 3 = 1. <p> Surprisingly enough it turns out (cf., <ref> [BGS2] </ref>) that the optimization is not a function of the performance of the gadgets and indeed the choice of parameters p 1 ; p 2 and p 3 as in Equation (12) is optimal for the following reductions. 53 4.2.1 The Hardness of MaxE3SAT and Max2SAT Gadgets. <p> Remark 4.10 In previous versions of this work <ref> [BGS2] </ref>, it was observed that a ratio of 4 between the number of clauses and the second parameter (i.e., fi) is minimal for both E3-SAT gadgets. Several questions regarding the ff=fi ratios achievable by 3-SAT and 2-SAT gadgets were posed. <p> This yields a lower bound of * &gt; 1 43 &gt; 0:023 (see details in previous versions of this work <ref> [BGS2] </ref>). However, a stronger result is obtained via free-bit complexity: 8 We apply the FGLSS-reduction to a proof system (for NP) of low free-bit complexity; specifically to a proof system which uses 2 free-bits and has soundness error below 0.8. <p> Furthermore, 8s 0:299, naPCP 1;s [log; 3] = P, where naPCP is a restriction of PCP in which the verifier is required to be non-adaptive. We remark that PCP 1;0:8999 [log; 3] = NP with a non-adaptive verifier was presented in an earlier version of this paper <ref> [BGS2] </ref>. Using Proposition 10.2, we have MIP 1;0:95 [log; 3] = NP. <p> The (tedious) proof of the non-adaptive case can be found in earlier versions of this paper <ref> [BGS2] </ref>. The paper of Trevisan et. al. [TSSW] contains a stronger result which holds for all verifiers; that is, PCP 1;0:367 [log; 3] = P. The latter result (i.e., PCP 1;0:367 [log; 3] = P) is weaker than what can be proven for MIP proof systems (see next corollary).
Reference: [BGLR] <author> M. Bellare, S. Goldwasser, C. Lund and A. Russell. </author> <title> Efficient probabilistically checkable proofs and applications to approximation. </title> <booktitle> Proceedings of the 25th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1993, </year> <pages> pp. 294-304. </pages> <booktitle> (See also Errata sheet in Proceedings of the 26th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1994, </year> <pages> pp. 820-820). </pages>
Reference-contexts: First, one exhibits an appropriate proof system for NP. Then one applies the FGLSS reduction. The factor indicated hard depends on the Free Bits in PCP 7 proof system parameters. A key element in getting better results has been the distilling of appropriate pcp-parameters. The sequence of works <ref> [FGLSS, ArSa, ALMSS, BGLR, FeKi1, BeSu] </ref> lead us through a sequence of parameters: query complexity, free-bit complexity and, finally, for the best known results, amortized free-bit complexity. <p> Free Bits in PCP 9 General framework. We emphasize a general framework for the derivation of strong non-approximability results for MaxSNP problems which results from our tests and proof systems. We use direct reductions from verifiers to the problems of interest. (This follows and extends <ref> [BGLR] </ref>, prior to which results had used "generic" reductions, which did not take advantage of the nature of the tests performed by the verifier.) In particular, in our case it turns out that the verifier only performs two kinds of tests | (1) verify that a + b + c = <p> Furthermore, it also implied that Max-3-Sat is NP-hard to approximate to within some constant factor [ALMSS] and so is any MaxSNP-hard problem [PaYa]. The second stage of this enterprise started with the work of Bellare, Goldwasser, Lund and Russell <ref> [BGLR] </ref>. The goal was to improve (increase) the constant * in the exponent of the hardness of approximation factor for Max Clique, and also to improve the constant values of the hardness factors in the MaxSNP hardness results. <p> Feige and Kilian [FeKi1], however, observed that one should work with free-bits, and noted that the free-bit complexity of the system of <ref> [BGLR] </ref> was 14, yielding a N 1=15 hardness factor. Bellare and Sudan then suggested the notion of amortized free-bits. 14 Bellare, Goldreich, Sudan Problem EASY to Approx. Factor HARD to Approx. <p> By now, tight results are known for central problems such as Min-Set-Cover (cf., <ref> [LuYa, BGLR, Fe2] </ref>), Max-Clique (cf., [H1, H2]), Min-Coloring ([FeKi2]), and Max-3SAT (cf., [H3]). The latter results were obtained subsequently to the current work, and while building on it. Amortized free-bits and Max-Clique. The most intriguing problem left open by our work has been resolved by H-astad [H1, H2]. <p> The class PCP 1;1=2 [r; q] was made explicit by [ArSa]. The parameterization was expanded by <ref> [BGLR] </ref> to explicitly consider the answer size (the oracle was allowed to return more than one bit at a time) and query size- their notation included five parameters: randomness, number of queries, size of each query, size of each answer, and error probability. <p> Query complexity minimization. One seeks results of the form NP = PCP 1;1=2 [ coins = log ; query = q ; query av = q av ] : (4) 20 Bellare, Goldreich, Sudan Due to q q av [ALMSS] some constant some constant <ref> [BGLR] </ref> 36 29 [FeKi1] 32 24 This paper 11 10.9 logarithmic randomness; that is, results of the form of Eq. (4). This was originally done for deriving NP-hardness results for the approximation of MaxClique, but subsequent work has indicated that other parameters actually govern this application. <p> The two-prover proofs of Lapidot-Shamir and Feige-Lovasz [LaSh, FeLo] had poly-logarithmic randomness and answer sizes, so [ALMSS] used a modification of these, in the process increasing the number of provers to a constant much larger than two. The later constructions of few-prover proofs of <ref> [BGLR, Tar, FeKi1] </ref> lead to better non-approximability results. Bellare and Sudan [BeSu] identified some extra features of constant prover proofs whose presence they showed could be exploited to further increase the non-approximability factors. These features are captured in their definition of canonical verifiers (cf. Section 3.4). <p> A breakthrough result in this area is Raz's Parallel Repetition Theorem which implies the ex Free Bits in PCP 21 Due to Provers Coins Answer size Canonical? Can be made canonical? [LaSh, FeLo] 2 polylog polylog No Yes [BeSu] [ALMSS] poly (* 1 ) log polylog No ? <ref> [BGLR] </ref> 4 log polyloglog No ? [Tar] 3 log O (1) No ? [FeKi1] 2 log O (1) No At cost of one more prover [BeSu] [Raz] 2 log O (1) Yes (NA) istence of a two-provers proof system with logarithmic randomness and constant answer size [Raz]. <p> We abuse notation slightly by dropping the "X", writing just MaxSAT ('). Similarly for the normalized version. 26 Bellare, Goldreich, Sudan Due to Assuming Factor Technique [ALMSS] P 6= NP some constant NP PCP 1;1=2 [ log; O (1) ]; Reduction of this to Max3SAT. <ref> [BGLR] </ref> e P 6= N e P 94=93 Framework; better analyses; uses proof systems of [LaSh, FeLo]. [BGLR] P 6= NP 113=112 New four-prover proof systems. [FeKi1] P 6= NP 94=93 New two-prover proof systems. [BeSu] e P 6= N e P 66=65 Canonicity and some optimizations. [BeSu] P 6= NP <p> Similarly for the normalized version. 26 Bellare, Goldreich, Sudan Due to Assuming Factor Technique [ALMSS] P 6= NP some constant NP PCP 1;1=2 [ log; O (1) ]; Reduction of this to Max3SAT. <ref> [BGLR] </ref> e P 6= N e P 94=93 Framework; better analyses; uses proof systems of [LaSh, FeLo]. [BGLR] P 6= NP 113=112 New four-prover proof systems. [FeKi1] P 6= NP 94=93 New two-prover proof systems. [BeSu] e P 6= N e P 66=65 Canonicity and some optimizations. [BeSu] P 6= NP 73=72 Canonicity and some optimizations. <p> The basic paradigm of their reduction has been maintained in later improvements. factor) begin with <ref> [BGLR] </ref>. They used Hadamard code based inner verifiers following [ALMSS]. They also introduced a framework for better analysis, and improved some previous analyses; we exploit in particular their better analyses of linearity testing (cf. Section 3.5) and of Freivalds's matrix multiplication test (cf. Lemma 3.16). <p> Meanwhile the assumption involved the probabilistic, rather than deterministic time complexity of NP- it would be NP 6 coR e P if r = polylog (n) and NP 6= coRP if r = log (n). New proof systems of <ref> [BGLR] </ref> were able to obtain significantly smaller query complexity: they showed NP PCP 1;1=2 [ coins = polylog ; query = 24 ] and NP PCP 1;1=2 [ coins = log ; query = 29 ]. This leads to their hardness results shown in Figure 7. <p> Namely, the factor in the above mentioned reduction is N 1=(1+f) where f is the free-bit complexity. They observed that the proof system of <ref> [BGLR] </ref> has free-bit complexity 14, yielding a N 1=15 hardness of approximation factor. The notion of amortized free-bits was introduced in [BeSu]. <p> See Section 8.4 for details. Free Bits in PCP 29 Due to Factor Assumption [FGLSS] 2 log 1* N for any * &gt; 0 NP 6 e P [ArSa] 2 p [ALMSS] N * for some * &gt; 0 P 6= NP <ref> [BGLR] </ref> N 1=30 NP 6= coRP [BGLR] N 1=25 NP 6 coR e P [FeKi1] N 1=15 NP 6= coRP [BeSu] N 1=6 P 6= NP [BeSu] N 1=4 NP 6 coR e P This paper N 1=4 P 6= NP This paper N 1=3 NP 6= coRP approximate within N <p> See Section 8.4 for details. Free Bits in PCP 29 Due to Factor Assumption [FGLSS] 2 log 1* N for any * &gt; 0 NP 6 e P [ArSa] 2 p [ALMSS] N * for some * &gt; 0 P 6= NP <ref> [BGLR] </ref> N 1=30 NP 6= coRP [BGLR] N 1=25 NP 6 coR e P [FeKi1] N 1=15 NP 6= coRP [BeSu] N 1=6 P 6= NP [BeSu] N 1=4 NP 6 coR e P This paper N 1=4 P 6= NP This paper N 1=3 NP 6= coRP approximate within N ffi for some ffi &gt; 0. <p> We show how the composed verifier hV outer ; V inner i inherits the goodness of the V outer and V inner . To do so we need the following Lemma. It is the counterpart of a claim in <ref> [BGLR, Lemma 3.5] </ref> and will be used in the same way. The lemma is derived from a coding theory bound which is slight extension of bounds in [McSl, Ch. 7] (see Appendix). Lemma 3.11 Suppose 0 ffi 1=2 and A: F l ! . <p> The latter explains why we are interested in outer verifiers which achieve a constant, but arbitrarily small, error *. For completeness we provide a proof, following the ideas of <ref> [ArSa, ALMSS, BGLR] </ref>. 39 Theorem 3.12 (the composition theorem): Let V outer be a (l; l 1 )-canonical outer verifier. Suppose it is *-good for L. Let V inner be an (l; l 1 )-canonical inner verifier that is (; ffi 1 ; ffi 2 )-good. <p> The idea is to exploit the characterization of Proposition 3.2. Thus we will perform (on A) a linearity test, and then a "Respect of Monomial Basis" test. Linearity testing is well understood, and we will use the test of [BLR], with the analyses of <ref> [BLR, BGLR, BCHKS] </ref>. The main novelty is the Respect of Monomial Basis Test. Circuit and projection. Having established that A is close to some evaluation operator E a , we now want to test two things. The first is that h (a) = 0 for some predetermined function h. <p> Their analysis was used in the proof system and Max3SAT non-approximability result of [ALMSS]. Interest in the tightness of the analysis began with <ref> [BGLR] </ref>, with the motivation of improving the Max3SAT non-approximability results. They showed that r A 3x A 6x 2 A , for every A. This establishes the first segment of the lower bound quoted above (i.e., of the function lin ). <p> Putting these together implies a two segment lower bound with phase transition at the largest root of the equation 3x 6x 2 = 2 9 (i.e., at 1 p 36 ). This lower bound was used in the Max3SAT analyses of <ref> [BGLR] </ref> and [BeSu]. <p> The resulting test is depicted in Figure 8. A technical lemma. First we recall the following lemma of <ref> [BGLR] </ref> which provides an improved analysis of Freivalds's matrix multiplication test in the special case when the matrices are symmetric with common diagonal. Lemma 3.16 (symmetric matrix multiplication test [BGLR]): Let M 1 ; M 2 be N -by-N symmetric matrices over which agree on their diagonals. <p> The resulting test is depicted in Figure 8. A technical lemma. First we recall the following lemma of <ref> [BGLR] </ref> which provides an improved analysis of Freivalds's matrix multiplication test in the special case when the matrices are symmetric with common diagonal. Lemma 3.16 (symmetric matrix multiplication test [BGLR]): Let M 1 ; M 2 be N -by-N symmetric matrices over which agree on their diagonals. Suppose that M 1 6= M 2 . <p> By repeating the proof system of Theorem 4.5 five times, we obtain that Eq. (4) holds for q = 15. (Note that 5 = minfi 2 N : 0:85 i &lt; 0:5g.) A straightforward implementation of the recycling technique of <ref> [BGLR] </ref> yields that Eq. (4) holds for q = 12 and q av = 11:74. <p> The condition regarding the distance of the code is essential since the task is easy with respect to the identity map (which is a code of distance 1). We remark that testing "closeness" to codewords with respect to codes of large distance is essential in all known pcp constructions <ref> [BFLS, FGLSS, ArSa, ALMSS, BGLR, FeKi1, BeSu] </ref>. The absolute distance between two words w; u 2 f0; 1g n , denoted (w; u), is the number of bits on which w and u disagree. <p> The latter result (i.e., PCP 1;0:367 [log; 3] = P) is weaker than what can be proven for MIP proof systems (see next corollary). This contrast may provide a testing ground to separate PCP from MIP, a question raised by <ref> [BGLR] </ref>. Corollary 10.5 For s &lt; 1=2, MIP 1;s [coins = log; provers = 3] = P. Proof: Combining (the two parts of) Lemma 10.1 and (Part 2 of) Proposition 10.3, we have MIP 1;s [log; 3] MIP 1;2s [log; 2] PCP 1;2s [log; 2] P.
Reference: [BeRo] <author> M. Bellare and P. Rogaway. </author> <title> The complexity of approximating a quadratic program. </title> <journal> Journal of Mathematical Programming B, </journal> <volume> Vol. 69, No. 3, </volume> <month> September </month> <year> 1995, </year> <pages> pp. 429-441. </pages> <note> Also in Complexity of Numerical Optimization, </note> <editor> Ed. P. M. Pardalos, </editor> <publisher> World Scientific, </publisher> <year> 1993. </year>
Reference-contexts: After the work of [FGLSS] the field took off in two major directions. One was to extend the interactive proof approach to prove the non-approximability of other optimization problems. Direct reductions from proofs were used to show the hardness of quadratic programming <ref> [BeRo, FeLo] </ref>, Max3SAT [ALMSS], set cover [LuYa], and other problems [Be]. The earlier work of Papadimitriou and Yannakakis introducing the class MaxSNP [PaYa] now came into play; by reduction from Max3SAT it implied hardness of approximation for any MaxSNP-hard problem. <p> Constant-prover proofs have been instrumental in the derivation of non-approximability results in several ways. One of these is that they are a good starting point for reductions| examples of such are reductions of two-prover proofs to quadratic programming <ref> [BeRo, FeLo] </ref> and set cover [LuYa]. However, it is a different aspect of constant prover proofs that is of more direct concern to us. This aspect is the use of constant-prover proof systems as the penultimate step of the recursion, and begins with [ALMSS].
Reference: [BeSu] <author> M. Bellare and M. Sudan. </author> <title> Improved non-approximability results. </title> <booktitle> Proceedings of the 26th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1994, </year> <pages> pp. 184-193. </pages>
Reference-contexts: First, one exhibits an appropriate proof system for NP. Then one applies the FGLSS reduction. The factor indicated hard depends on the Free Bits in PCP 7 proof system parameters. A key element in getting better results has been the distilling of appropriate pcp-parameters. The sequence of works <ref> [FGLSS, ArSa, ALMSS, BGLR, FeKi1, BeSu] </ref> lead us through a sequence of parameters: query complexity, free-bit complexity and, finally, for the best known results, amortized free-bit complexity. <p> Some of these results are motivated by applications, others purely as interesting items in proof theory. The search for proof systems of low amortized free-bit complexity is motivated of course by the FGLSS reduction. Bellare and Sudan <ref> [BeSu] </ref> have shown that NP FPCP [ log; 3 + * ] for every * &gt; 0. The first result above improves upon this, presenting a new proof system with amortized free-bit complexity 2 + *. <p> Indeed, if there is an NP-hard Max 3SAT gap problem with certain focus error queries free-bits previous related result 3 queries 0:85 3 2 error 72 73 via MaxSAT <ref> [BeSu] </ref> 2 free-bits 0.794 O (1) 2 error 1/2 1 2 11 7 32 queries (24 on average) [FeKi1] amortized free-bits O (2 m ) 2 3m 2m 3m free-bits [BeSu] 8 Bellare, Goldreich, Sudan Problem Approx Non-Approx Factor Due to Our Factor Previous Factor Assumption Max3SAT 1:258 [Yan, GoWi2, TSSW] <p> problem with certain focus error queries free-bits previous related result 3 queries 0:85 3 2 error 72 73 via MaxSAT <ref> [BeSu] </ref> 2 free-bits 0.794 O (1) 2 error 1/2 1 2 11 7 32 queries (24 on average) [FeKi1] amortized free-bits O (2 m ) 2 3m 2m 3m free-bits [BeSu] 8 Bellare, Goldreich, Sudan Problem Approx Non-Approx Factor Due to Our Factor Previous Factor Assumption Max3SAT 1:258 [Yan, GoWi2, TSSW] 1:038 1 + 1 72 [BeSu] P 6= NP MaxE3SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max2SAT 1:075 [GoWi2, FeGo] 1:013 1 <p> 2 error 1/2 1 2 11 7 32 queries (24 on average) [FeKi1] amortized free-bits O (2 m ) 2 3m 2m 3m free-bits <ref> [BeSu] </ref> 8 Bellare, Goldreich, Sudan Problem Approx Non-Approx Factor Due to Our Factor Previous Factor Assumption Max3SAT 1:258 [Yan, GoWi2, TSSW] 1:038 1 + 1 72 [BeSu] P 6= NP MaxE3SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max2SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP MaxSAT 2 folklore 1 + 1 7 P 6= NP MaxCUT 1:139 [GoWi2] 1:014 unspecified [ALMSS] P 6= <p> Problem Approx Non-Approx Factor Due to Our Factor Previous Factor Assumption Max3SAT 1:258 [Yan, GoWi2, TSSW] 1:038 1 + 1 72 <ref> [BeSu] </ref> P 6= NP MaxE3SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max2SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP MaxSAT 2 folklore 1 + 1 7 P 6= NP MaxCUT 1:139 [GoWi2] 1:014 unspecified [ALMSS] P 6= NP MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ <p> + 1 504 (implied <ref> [BeSu] </ref>) P 6= NP MaxSAT 2 folklore 1 + 1 7 P 6= NP MaxCUT 1:139 [GoWi2] 1:014 unspecified [ALMSS] P 6= NP MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 coR ~ P Number N 1 1 7 [Fu] coRP 6= NP N 7 N 14 [BeSu] P 6= <p> NP MaxCUT 1:139 [GoWi2] 1:014 unspecified [ALMSS] P 6= NP MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 <ref> [BeSu] </ref> NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 coR ~ P Number N 1 1 7 [Fu] coRP 6= NP N 7 N 14 [BeSu] P 6= NP we show are hard to achieve (Non-Approx). <p> o (1) [BaEv2, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 <ref> [BeSu] </ref> NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 coR ~ P Number N 1 1 7 [Fu] coRP 6= NP N 7 N 14 [BeSu] P 6= NP we show are hard to achieve (Non-Approx). <p> 1 4 <ref> [BeSu] </ref> NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 coR ~ P Number N 1 1 7 [Fu] coRP 6= NP N 7 N 14 [BeSu] P 6= NP we show are hard to achieve (Non-Approx). MaxE3SAT (resp., MaxSAT) denote the maximization problem for CNF formulae having exactly 3 different literals in each clause (resp., a conjunction of parity clauses). gap then one can easily get a three query proof system with the same gap. <p> The conclusion for Max Clique follows, of course, from the FGLSS-reduction and the first proof system listed above. The conclusion for the Chromatic Number follows from a recent reduction of Furer [Fu], which in turn builds on reductions in <ref> [LuYa, KLS, BeSu] </ref>. (Furer's work and ours are contemporaneous and thus we view the N 1=5 hardness result as jointly due to both papers.) The improvements for the MaxSNP problems are perhaps more significant than the Max Clique one: We see hardness results for MaxSNP problems that are comparable to the <p> This served to focus attention on the roles of various parameters, both in reductions and in constructions. Also they introduced the consideration of average query complexity, the first in a sequence of parameter changes towards doing better for clique. Free bits are implicit in [FeKi1] and formalized in <ref> [BeSu] </ref>. Amortized free bits are introduced in [BeSu] but formalized a little better here. Proof sizes were considered in [BFLS, PoSp]. <p> Also they introduced the consideration of average query complexity, the first in a sequence of parameter changes towards doing better for clique. Free bits are implicit in [FeKi1] and formalized in <ref> [BeSu] </ref>. Amortized free bits are introduced in [BeSu] but formalized a little better here. Proof sizes were considered in [BFLS, PoSp]. We consider them here for a different reason they play an important role in that the randomized FGLSS reduction [BeSc, Zuc] depends actually on this parameter (rather than on the randomness complexity). <p> The later constructions of few-prover proofs of [BGLR, Tar, FeKi1] lead to better non-approximability results. Bellare and Sudan <ref> [BeSu] </ref> identified some extra features of constant prover proofs whose presence they showed could be exploited to further increase the non-approximability factors. These features are captured in their definition of canonical verifiers (cf. Section 3.4). <p> These features are captured in their definition of canonical verifiers (cf. Section 3.4). But the proof systems of [FeKi1] that had worked above no longer sufficed| they are not canonical. So instead <ref> [BeSu] </ref> used (a slight modification of) the proofs of [LaSh, FeLo], thereby incurring poly-logarithmic randomness and answer sizes, so that the assumptions in their non-approximability results pertain to quasi-polynomial time classes. (Alternatively they modify the [FeKi1] system to a canonical three-prover one, but then incur a decrease in the non-approximability factors <p> A breakthrough result in this area is Raz's Parallel Repetition Theorem which implies the ex Free Bits in PCP 21 Due to Provers Coins Answer size Canonical? Can be made canonical? [LaSh, FeLo] 2 polylog polylog No Yes <ref> [BeSu] </ref> [ALMSS] poly (* 1 ) log polylog No ? [BGLR] 4 log polyloglog No ? [Tar] 3 log O (1) No ? [FeKi1] 2 log O (1) No At cost of one more prover [BeSu] [Raz] 2 log O (1) Yes (NA) istence of a two-provers proof system with logarithmic <p> Coins Answer size Canonical? Can be made canonical? [LaSh, FeLo] 2 polylog polylog No Yes <ref> [BeSu] </ref> [ALMSS] poly (* 1 ) log polylog No ? [BGLR] 4 log polyloglog No ? [Tar] 3 log O (1) No ? [FeKi1] 2 log O (1) No At cost of one more prover [BeSu] [Raz] 2 log O (1) Yes (NA) istence of a two-provers proof system with logarithmic randomness and constant answer size [Raz]. Furthermore, this proof system is canonical. 2.3 Reductions between problems and classes We will consider reductions between promise problems. <p> some constant NP PCP 1;1=2 [ log; O (1) ]; Reduction of this to Max3SAT. [BGLR] e P 6= N e P 94=93 Framework; better analyses; uses proof systems of [LaSh, FeLo]. [BGLR] P 6= NP 113=112 New four-prover proof systems. [FeKi1] P 6= NP 94=93 New two-prover proof systems. <ref> [BeSu] </ref> e P 6= N e P 66=65 Canonicity and some optimizations. [BeSu] P 6= NP 73=72 Canonicity and some optimizations. This paper P 6= NP 27=26 Long code and new proof systems. 2.4.3 History of approximability results for these problems Satisfiability problems. <p> this to Max3SAT. [BGLR] e P 6= N e P 94=93 Framework; better analyses; uses proof systems of [LaSh, FeLo]. [BGLR] P 6= NP 113=112 New four-prover proof systems. [FeKi1] P 6= NP 94=93 New two-prover proof systems. <ref> [BeSu] </ref> e P 6= N e P 66=65 Canonicity and some optimizations. [BeSu] P 6= NP 73=72 Canonicity and some optimizations. This paper P 6= NP 27=26 Long code and new proof systems. 2.4.3 History of approximability results for these problems Satisfiability problems. Max3SAT is the canonical Max-SNP complete problem [PaYa]. <p> Section 3.5) and of Freivalds's matrix multiplication test (cf. Lemma 3.16). The improvement of Feige and Kilian [FeKi1] was obtained via new proof systems; that of <ref> [BeSu] </ref> by use of the canonicity property of constant prover proofs and some optimizations. (See Section 2.2.3 for a discussion of the role of constant-prover proofs in this context). <p> Namely, the factor in the above mentioned reduction is N 1=(1+f) where f is the free-bit complexity. They observed that the proof system of [BGLR] has free-bit complexity 14, yielding a N 1=15 hardness of approximation factor. The notion of amortized free-bits was introduced in <ref> [BeSu] </ref>. They observed that the performance of the reduction depended in fact on this quantity, and that the factor was N 1=(1+ f) where f is the amortized free bit complexity. They then showed that NP FPCP [polylog; 3]. <p> Factor Assumption [FGLSS] 2 log 1* N for any * &gt; 0 NP 6 e P [ArSa] 2 p [ALMSS] N * for some * &gt; 0 P 6= NP [BGLR] N 1=30 NP 6= coRP [BGLR] N 1=25 NP 6 coR e P [FeKi1] N 1=15 NP 6= coRP <ref> [BeSu] </ref> N 1=6 P 6= NP [BeSu] N 1=4 NP 6 coR e P This paper N 1=4 P 6= NP This paper N 1=3 NP 6= coRP approximate within N ffi for some ffi &gt; 0. But, again, ffi is very small. <p> N for any * &gt; 0 NP 6 e P [ArSa] 2 p [ALMSS] N * for some * &gt; 0 P 6= NP [BGLR] N 1=30 NP 6= coRP [BGLR] N 1=25 NP 6 coR e P [FeKi1] N 1=15 NP 6= coRP <ref> [BeSu] </ref> N 1=6 P 6= NP [BeSu] N 1=4 NP 6 coR e P This paper N 1=4 P 6= NP This paper N 1=3 NP 6= coRP approximate within N ffi for some ffi &gt; 0. But, again, ffi is very small. <p> A subsequent reduction of Khanna, Linial and Safra [KLS] is simpler but in fact slightly less efficient, having h (*) = *=(5 + *). A more efficient reduction is given by <ref> [BeSu] </ref> they present a reduction obtaining h (*) = *=(3 2*). Our N 1=3 hardness for Clique would yield, via this, a N 1=7 hardness for the chromatic number. But more recently an even more efficient reduction has become available, namely that of Furer [Fu]. <p> Comparison with previous work. For a history and a better understanding of the role of constant-prover proof systems in this context, see Section 2.4.3. In comparison, our definition of outer verifiers (below) asks for almost the same canonicity properties as in <ref> [BeSu] </ref>. (The only difference is that they have required to be a projection function, whereas we can deal with an arbitrary function. <p> Putting these together implies a two segment lower bound with phase transition at the largest root of the equation 3x 6x 2 = 2 9 (i.e., at 1 p 36 ). This lower bound was used in the Max3SAT analyses of [BGLR] and <ref> [BeSu] </ref>. <p> Here, unlike previous works (for instance <ref> [BeSu] </ref>), may be an arbitrary mapping from l to l 1 rather than being a projection (i.e., satisfying (x) = x (i 1 ) : : : x (i l 1 ) for some sequence 1 i 1 &lt; &lt; i l 1 l and all x 2 l ). <p> Intuitively, we will be running each of the atomic tests many times, but, to keep the free-bit count low, these will not be independent repetitions. Rather, following <ref> [BeSu] </ref>, we will run about 2 O (m) copies of each test in a way which is pairwise, or "almost" pairwise independent, to lower the error probability to O (2 m ). This will be done using 2m free-bits. <p> The analysis of the other iterated tests, where the atomic tests are invoked on two/three linear combinations of the same sequence of random function, require slightly more care. The corresponding lemmas could have been proven using the notion of "weak pairwise independence" introduced in <ref> [BeSu] </ref>. However, we present here an alternative approach. 72 7.1.2 Iterated projection test The iterated projection test described in Figure 15 takes as input vectors ~ f ; ~g 2 F m l and also a linear function L 2 L m . <p> Let a 2 l be such that Dist (E a ; A) 1=4, and let a 1 = (a) 2 l 1 . If ProjPass m then Dist (E a 1 ; A 1 ) 0:1. Proof: The proof is similar to that of <ref> [BeSu, Lemma 3.5] </ref>. Let * 1 = Dist (A 1 ; E a 1 ) and assume it is at least 0:1. We show that there is a constant c 3 such that ProjPass m h (A) &lt; c 3 2 m . m j = 2 m 1. <p> Finally, by Proposition 3.6, we have h (a) = 0. 7.2 NP in amortized free-bit complexity 2 Sources of our improvements We adopt the basic framework of the construction of proof systems with low free-bit complexity as presented in <ref> [BeSu] </ref>. Our improvement comes from the use of the new long code instead of the Hadamard code as a basis for the construction of inner verifiers. This allows us to save one bit in the amortized free-bit complexity. <p> Instead, as seen above, the long code allows us to directly handle any function. The fact that we take linear combinations of these functions should not confuse the reader; these are linear combinations of random functions rather than being linear combinations of random linear functions (as in <ref> [BeSu] </ref>). Our construction of a proof systems with amortized free-bit complexity of two bits is obtained by composing the (l; l 1 )-canonical outer verifier of Lemma 3.8 with a (l; l 1 )-canonical inner verifier, denoted V free-in , which is depicted in Figure 16. <p> Each of the two parts of Proposition 8.7 shows that the well-known method of obtaining clique-approximation results from efficient pcp systems (cf., <ref> [FGLSS, BeSc, Zuc, FeKi1, BeSu] </ref>) is "complete" in the sense that if clique-approximation can be shown NP-hard then this can be done via this method. The precise statement is given in Theorems 8.10 and 8.11 (below). As a preparatory step, we first provide an easier-to-use form of the above proposition. <p> However now we know that FPCP and Max Clique are equivalent, so we can go back and rephrase the old statements. Thus results of <ref> [LuYa, KLS, BeSu, Fu] </ref> can be summarized as: For every ff; *; fl &gt; 0, Gap-MaxClique N ff1 ;N *1 K R Gap-ChromNum N (*+fl) ;N h (ff) , where (1) h (ff) = minf 1 6 ; ff 54ff g [LuYa]. (2) h (ff) = minf 1 11 ; ff <p> fl &gt; 0, Gap-MaxClique N ff1 ;N *1 K R Gap-ChromNum N (*+fl) ;N h (ff) , where (1) h (ff) = minf 1 6 ; ff 54ff g [LuYa]. (2) h (ff) = minf 1 11 ; ff (3) h (ff) = minf 1 4 ; ff 32ff g <ref> [BeSu] </ref>. (4) h (ff) = minf 1 3 ; ff We note that it is an open problem whether one can get a reduction in which h (ff) ! 1 as ff ! 1. We also note that Furer's reduction is randomized while the rest are deterministic. <p> The condition regarding the distance of the code is essential since the task is easy with respect to the identity map (which is a code of distance 1). We remark that testing "closeness" to codewords with respect to codes of large distance is essential in all known pcp constructions <ref> [BFLS, FGLSS, ArSa, ALMSS, BGLR, FeKi1, BeSu] </ref>. The absolute distance between two words w; u 2 f0; 1g n , denoted (w; u), is the number of bits on which w and u disagree.
Reference: [BGKW] <author> M. Ben-Or, S. Goldwasser, J. Kilian and A. Wigderson. </author> <title> Multi-Prover interactive proofs: How to remove intractability assumptions. </title> <booktitle> Proceedings of the 20th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1988, </year> <pages> pp. 113-131. </pages>
Reference-contexts: The indication of higher factors, and results for other problems, had to wait for the interactive proof approach. Interactive proofs were introduced by Goldwasser, Micali and Rackoff [GMR] and Babai [Bab]. Ben-Or, Goldwasser, Kilian and Wigderson <ref> [BGKW] </ref> extended these ideas to define a notion of multi-prover interactive proofs. Fortnow, Rompel and Sipser [FRS] showed that the class, MIP, of languages possessing multi-prover interactive proofs equals the class of languages which have (using todays terms) probabilistically checkable proofs (of unrestricted, and thus polynomial, randomness and query complexity). <p> The model underlying what are now known as "probabilistically checkable proofs" is the "oracle model" of Fortnow, Rompel and Sipser [FRS], introduced as an equivalent version (with respect to language recognition power) of the multi-prover model of Ben-Or, Goldwasser, Kilian and Wigderson <ref> [BGKW] </ref>. Interestingly, as shown by [BFLS, FGLSS], this framework can be applied in a meaningful manner also to languages in NP. <p> Overview. The constructions of efficient proofs that follow will exploit the notion of recursive verifier construction due to Arora and Safra [ArSa]. We will use just one level of recursion. We first define a notion of a canonical outer verifier whose intent is to capture two-prover one-round proof systems <ref> [BGKW] </ref> having certain special properties; these verifiers will be our starting point. We then define 36 a canonical inner verifier. Recursion is captured by an appropriate definition of a composed verifier whose attributes we relate to those of the original verifiers in Theorem 3.12.
Reference: [BeSc] <author> P. Berman and G. Schnitger. </author> <title> On the complexity of approximating the independent set problem. </title> <journal> Information and Computation, </journal> <volume> Vol. 96, </volume> <year> 1992, </year> <pages> pp. 77-94. </pages>
Reference-contexts: For the best results one typically uses a randomized form of this reduction due to <ref> [BeSc, Zuc] </ref> and it is this that we will assume henceforth. A NP-hard gap problem is obtained roughly as follows. First, one exhibits an appropriate proof system for NP. Then one applies the FGLSS reduction. The factor indicated hard depends on the Free Bits in PCP 7 proof system parameters. <p> Specifically, using a randomized reduction we can transform FPCP 1; 1 2 [log; f ] into FPCP 1;2 k [log +k; k f ] (ignoring multiplicative factors of 1 + * for arbitrarily small * &gt; 0). This transformation is analogous to the well-known transformation of Berman and Schnitger <ref> [BeSc] </ref>. <p> They presented new proof systems minimizing query complexity and exploited a slightly improved version of the FGLSS-reduction due to <ref> [BeSc, Zuc] </ref> to get a N 1=30 hardness of approximation factor for Max Clique. Feige and Kilian [FeKi1], however, observed that one should work with free-bits, and noted that the free-bit complexity of the system of [BGLR] was 14, yielding a N 1=15 hardness factor. <p> Free bits are implicit in [FeKi1] and formalized in [BeSu]. Amortized free bits are introduced in [BeSu] but formalized a little better here. Proof sizes were considered in [BFLS, PoSp]. We consider them here for a different reason they play an important role in that the randomized FGLSS reduction <ref> [BeSc, Zuc] </ref> depends actually on this parameter (rather than on the randomness complexity). The discussion of previous proof systems is coupled with the discussion of Max Clique in Section 2.4.3. We conclude the current section, by discussing two somewhat related topics: query minimization and constant-prover proof systems. Query complexity minimization. <p> If the reduction is deterministic we omit the subscript of "R," or, sometimes, for emphasis, replace it by a subscript of "D." An example is the randomized FGLSS transformation <ref> [FGLSS, BeSc, Zuc] </ref>. <p> The number of queries was unspecified, but indicated to be 10 4 , so * 10 4 . Later work has focused on reducing the constant value of * in the exponent. In later work a slightly tighter form of the FGLSS reduction due to <ref> [BeSc, Zuc] </ref> has been used. <p> that the initial randomness cost can be ignored (as long as it were logarithmic). (Otherwise, one would have needed to construct proof systems which minimize also this parameter; i.e., the constant factor in the logarithmic randomness complexity.) The randomized error reduction method originates in the work of Berman and Schnitger <ref> [BeSc] </ref> were it is applied to the Clique Gap promise problem. An alternative description is given by Zuckerman [Zuc]. Another alternative description, carried out in the proof system, is presented in Section 11. <p> the expander, specifically the ratio def log 2 , where d is the degree of the expander and is the second eigenvalue (of its adjacency matrix), play an important 3 An alternative approach, applicable to the Gap-Clique problem and presented in [AFWZ], is to "de-randomize" the graph product construction of <ref> [BeSc] </ref>. 30 Bellare, Goldreich, Sudan role here. <p> Each of the two parts of Proposition 8.7 shows that the well-known method of obtaining clique-approximation results from efficient pcp systems (cf., <ref> [FGLSS, BeSc, Zuc, FeKi1, BeSu] </ref>) is "complete" in the sense that if clique-approximation can be shown NP-hard then this can be done via this method. The precise statement is given in Theorems 8.10 and 8.11 (below). As a preparatory step, we first provide an easier-to-use form of the above proposition. <p> Combining Propositions 11.1 and 11.2, we obtain a randomized reduction of pcp systems which yields the effect of Proposition 11.1 at much lower (and in fact minimal) cost in the randomness complexity of the resulting pcp system. This reduction is analogous to the well-known transformation of Berman and Schnitger <ref> [BeSc] </ref>.
Reference: [Bl] <author> A. Blum. </author> <title> Algorithms for approximate graph coloring. </title> <type> Ph. D Thesis, </type> <institution> Dept. of Computer Science, MIT, </institution> <year> 1991. </year>
Reference-contexts: Max Clique problem, or any other problem about graphs, N denotes the number of vertices in the graph.) Can one find even an N 1* factor approximation algorithm for Max Clique for some * &lt; 1? An additional motivation for searching for such "weak" approximation algorithms was suggested by Blum <ref> [Bl] </ref>. He showed that a polynomial-time N 1* -factor approximation algorithm for Max Clique implies a polynomial time algorithm to color a three colorable graph with O (log N ) colors [Bl], which is much better than currently known [KMS]. But perhaps N 1o (1) is the best possible. <p> for some * &lt; 1? An additional motivation for searching for such "weak" approximation algorithms was suggested by Blum <ref> [Bl] </ref>. He showed that a polynomial-time N 1* -factor approximation algorithm for Max Clique implies a polynomial time algorithm to color a three colorable graph with O (log N ) colors [Bl], which is much better than currently known [KMS]. But perhaps N 1o (1) is the best possible. Resolving the approximation complexity of this basic problem seems, in any case, to be worth some effort. Gaps in clique size. <p> we get Gap-MaxClique N * 1 ;N k* 1 (N ) K R Gap-MaxClique 1 2 M * 0 2 ;2M k* 0 2 (M ), for * 0 2 * 2 , but this can be corrected by invoking item (1).) The following theorem was first shown by Blum <ref> [Bl] </ref>, using the technique of randomized graph products. <p> Inequality (17) follows from the fact that t = c log 2 N . The following result was derived as a corollary by Blum <ref> [Bl] </ref> and shows the application of the above theorem to coloring graphs with low-chromatic number with relatively small number of colors.
Reference: [BLR] <author> M. Blum, M. Luby and R. Rubinfeld. </author> <title> Self-testing/correcting with applications to numerical problems. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> Vol. 47, </volume> <year> 1993, </year> <pages> pp. 549-595. </pages>
Reference-contexts: The claim follows. As a corollary to the above (combined with the self-correcting paradigm <ref> [BLR] </ref>), we get Proposition 3.6 (folding and the evaluation operator): Let A: F l ! , h 2 F l , b 2 and a 2 l . <p> The idea is to exploit the characterization of Proposition 3.2. Thus we will perform (on A) a linearity test, and then a "Respect of Monomial Basis" test. Linearity testing is well understood, and we will use the test of <ref> [BLR] </ref>, with the analyses of [BLR, BGLR, BCHKS]. The main novelty is the Respect of Monomial Basis Test. Circuit and projection. Having established that A is close to some evaluation operator E a , we now want to test two things. <p> The idea is to exploit the characterization of Proposition 3.2. Thus we will perform (on A) a linearity test, and then a "Respect of Monomial Basis" test. Linearity testing is well understood, and we will use the test of [BLR], with the analyses of <ref> [BLR, BGLR, BCHKS] </ref>. The main novelty is the Respect of Monomial Basis Test. Circuit and projection. Having established that A is close to some evaluation operator E a , we now want to test two things. The first is that h (a) = 0 for some predetermined function h. <p> Thus, it is left to test that the two oracles are consistent in the sense that A 1 is not too far from an evaluation operator which corresponds to (a) for some predetermined function . Self-correction. The following self-correction lemma is due to <ref> [BLR] </ref> and will be used throughout. Lemma 3.13 (Self Correction Lemma [BLR]): Let A; ~ A: F l ! with ~ A linear, and let x = Dist (A; ~ A). <p> Self-correction. The following self-correction lemma is due to <ref> [BLR] </ref> and will be used throughout. Lemma 3.13 (Self Correction Lemma [BLR]): Let A; ~ A: F l ! with ~ A linear, and let x = Dist (A; ~ A). <p> Convention. All our tests output a bit, with 0 standing for accept and 1 for reject. 3.5.1 Atomic linearity test The atomic linearity test shown in Figure 8 is the one of Blum, Luby and Rubinfeld <ref> [BLR] </ref>. We want to lower bound the probability 1 LinPass (A) that the test rejects when its inputs f 1 ; f 2 are chosen at random, as a function of x = Dist (A; Lin). <p> It was shown in [BCHKS] (see below) that this combined lower bound is close to the best one possible. Perspective. The general problem of linearity testing as introduced and studied by Blum et. al. <ref> [BLR] </ref> is stated as follows: Given a function A: G ! H, where G; H are groups, obtain a lower bound on r A as a function of x A , where r A = Pr [A (a) + A (b) 6= A (a + b)] x A = Dist (A; <p> They showed that r A 3x A 6x 2 A , for every A. This establishes the first segment of the lower bound quoted above (i.e., of the function lin ). Also, it is possible to use <ref> [BLR] </ref> to show that r A 2=9 when x A 1=4. Putting these together implies a two segment lower bound with phase transition at the largest root of the equation 3x 6x 2 = 2 9 (i.e., at 1 p 36 ). <p> The test consists of checking whether A (h + f ) = A (f ) and it outputs 0 if equality holds and 1 otherwise. Assuming that A is close to some evaluation operator E a , the atomic circuit test uses self-correction <ref> [BLR] </ref> to test that a given function h has value 0 at a. As explained above, this test is not needed since all our proof systems will use a (h; 0)-folding (of A) and thus will impose h (a) = 0.
Reference: [BrNa] <author> J. Bruck and M. Naor. </author> <title> The hardness of decoding with preprocessing. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 36, No. 2, </volume> <year> 1990, </year> <pages> pp. 381-385. </pages>
Reference-contexts: In fact, even using our new Max3SAT result we would only get only a hardness factor of 185=184. See Section 4.2 for our results. Linear equations. The MaxLinEq problem is known to be Max-SNP complete (see <ref> [BrNa] </ref> or [Pet]). We remark that the problem of maximizing the number of satisfiable equations should not be confused with the "complementary" problem of minimizing the number of violated constraints, investigated by Arora et. al. [ABSS].
Reference: [BoHa] <author> R. Boppana and M. Hald orsson. </author> <title> Approximating maximum independent sets by excluding subgraphs. </title> <journal> BIT, </journal> <volume> Vol. 32, No. 2, </volume> <year> 1992. </year> <month> 116 </month>
Reference-contexts: Max Clique approximation. Although we look at many optimization problems there is a particular focus on Max Clique. Recall the best known polynomial time approximation algorithm for Max Clique achieves a factor of only N 1o (1) <ref> [BoHa] </ref>, scarcely better than the trivial factor of N . (Throughout the paper, when discussing the Max Clique problem, or any other problem about graphs, N denotes the number of vertices in the graph.) Can one find even an N 1* factor approximation algorithm for Max Clique for some * &lt; <p> [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP MaxSAT 2 folklore 1 + 1 7 P 6= NP MaxCUT 1:139 [GoWi2] 1:014 unspecified [ALMSS] P 6= NP MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) <ref> [BoHa] </ref> N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 coR ~ P Number N 1 1 7 [Fu] coRP 6= NP N 7 N <p> 6= NP MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) <ref> [BoHa] </ref> N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 coR ~ P Number N 1 1 7 [Fu] coRP 6= NP N 7 N 14 [BeSu] P 6= NP we show are hard to achieve (Non-Approx). <p> FeGo] 1:013 1:047 [H3] P 6= NP No MaxSAT 2 folklore 1 + 1 7 * 2 * [H3] P 6= NP Yes MaxCUT 1:139 [GoWi2] 1:014 1:062 [H3] P 6= NP No MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 1 + 1 Max-Clique N 1o (1) <ref> [BoHa] </ref> N 1 3 * N 1* [H2] coRP 6= NP Yes N 4 * N 2 * [H2] P 6= NP No Chromatic No. N 1o (1) [BoHa] N 1 5 * N 1* [FeKi2] coRP 6= NP Yes * &gt; 0 is an arbitrarily small constant. <p> [H3] P 6= NP No MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 1 + 1 Max-Clique N 1o (1) <ref> [BoHa] </ref> N 1 3 * N 1* [H2] coRP 6= NP Yes N 4 * N 2 * [H2] P 6= NP No Chromatic No. N 1o (1) [BoHa] N 1 5 * N 1* [FeKi2] coRP 6= NP Yes * &gt; 0 is an arbitrarily small constant. They constructed proof systems achieving amortized free-bit complexity three, and in thus obtained a N 1=4 hardness for Max Clique assuming NP 6 coR e P. <p> See Section 5.2 for our results. Max Clique. The best known polynomial time approximation algorithm for Max Clique achieves a factor of only N 1o (1) <ref> [BoHa] </ref>, scarcely better than the trivial factor of N .
Reference: [CrKa] <author> P. Crescenzi and V. Kann. </author> <title> A compendium of NP optimization problems. </title> <type> Technical Report, </type> <institution> Dipartimento di Scienze dell'Informazione, Universita di Roma "La Sapienza", SI/RR-95/02, </institution> <year> 1995. </year> <note> The list is updated continuously. The latest version is available via http://www.nada.kth.se/~viggo/problemlist/compendium.html. </note>
Reference: [CST] <author> P. Crescenzi, R. Silvestri and L. Trevisan. </author> <title> To weight or not to weight: </title> <booktitle> where is the question? Proceedings of the Fourth Israel Symposium on Theory and Computing Systems, IEEE, </booktitle> <year> 1996. </year>
Reference-contexts: Certainly, one may want non-approximability results for the unweighted case (where one does not allow multiple occurrences of the same clause). This issue is treated in a subsequent paper by Crescenzi et. al. <ref> [CST] </ref>. Essentially, they show that the unweighted cases of all problems considered in our paper are as hard as the weighted cases. 1.6 Directions for further research Although the most intriguing open problems suggested in previous versions of this work [BGS2] have been resolved, some very interesting problems remain. <p> This causes a loss of a factor of 3 in the hardness factor; that is, we would get a hardness factor of 214=213 for the MaxCUT problem restricted to simple graphs. A better reduction which preserves the non-approximation ratio has been recently suggested by Crescenzi et. al. <ref> [CST] </ref>. Gadgets. Unlike with MaxSAT problem, here we cannot negate variables at zero cost. Still, we first define simplified gadgets for Parity and RMB checking and make the necessary adaptations inside Lemma 4.15. Gadgets will be used to express the verifier's computation in terms of cuts in graphs.
Reference: [CW] <author> A. Cohen and A. Wigderson. Dispersers, </author> <title> deterministic amplification, and weak random sources. </title> <booktitle> Proceedings of the 30th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1989, </year> <pages> pp. 14-19. </pages>
Reference-contexts: On the other hand, it is easy to see that that random bits can be recycled for error-reduction via the standard techniques of <ref> [AKS, CW, ImZu] </ref>. The consequence was the first NP-hardness result for Max Clique approximation. <p> The de-randomized error reduction method consists of applying general, de-randomized, error-reduction techniques to the proof system setting. 3 The best method knows as the "Expander Walk" technique is due to Ajtai, Komlos and Szemeredi [AKS] (see also <ref> [CW, ImZu] </ref>). <p> function k : Z + ! Z + FPCP 1;s [r; f; l] FPCP 1;s k [O (r) + (2 + *) k log (1=s); (1 + *) kf; l]: Actually, the constant in the O-notation is minf1; 2+(4=*) We use random walks on expander graphs for error reduction (cf., <ref> [AKS, CW] </ref>). The value of the constant multiplier of k log (1=s) in the randomness complexity of the resulting pcp system, depends on the expander graph used. Specifically, using a degree d expander graph with second eigenvalue 109 yields a factor of log 2 d 1+log 2 .
Reference: [Con] <author> A. Condon. </author> <title> The complexity of the max word problem and the power of one-way interactive proof systems. </title> <journal> Computational Complexity, </journal> <volume> Vol. 3, </volume> <year> 1993, </year> <pages> pp. 292-305. </pages>
Reference-contexts: A hardness of approximation result based on interactive proofs was first proved by Condon <ref> [Con] </ref>. The breakthrough PCP connection to approximation was made by Feige, Goldwasser, Lovasz, Safra and Szegedy [FGLSS].
Reference: [Coo] <author> S. Cook. </author> <title> The complexity of theorem-proving procedures. </title> <booktitle> Proceedings of the 3rd Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1971, </year> <pages> pp. 151-158. </pages>
Reference-contexts: We've followed the common tradition regarding the names of polynomial-time reductions: many-to-one reductions are called Karp-reductions whereas (polynomial-time) Turing reductions are called Cook-reductions. This terminology is somewhat unfair towards Levin whose work on NP-completeness [Lev] was independent of those of Cook <ref> [Coo] </ref> and Karp [Kar]. Actually, the reductions considered by Levin are more restricted as they also efficiently transform the corresponding NP-witnesses (this is an artifact of Levin's desire to treat search problems rather than decision problems).
Reference: [EIS] <author> S. Even, A. Itai and A. Shamir. </author> <title> On the complexity of timetable and multicommodity flow problems. </title> <journal> SIAM J. on Computing, </journal> <volume> Vol. 5, </volume> <year> 1976, </year> <pages> pp. 691-703. </pages>
Reference-contexts: Furthermore, x can be constructed in polynomial-time given x (and V ). Using a (polynomial-time) decision procedure for satisfiability of Horn Formulae, we are done. (Alternatively, we can use the linear-time decision procedure for 2-SAT due to Even et. al. <ref> [EIS] </ref>.) Proof of Proposition 10.3, Part (4): To see that PCP 1;s [log; poly] NP, for every s &lt; 1, consider a non-deterministic machine which tries to guess an oracle which makes the verifier (of the above system) always accept. <p> One can easily verify that the formula is not satisfied iff there exists a variable for which every truth assignment yields a contradiction (i.e., "forcing paths" to contradicting values - cf., <ref> [EIS] </ref>). Thus, a non-deterministic 102 Proof of Proposition 10.8, Part (5): The result for non-adaptive verifiers follows from Part (2) by using the same strategy as in the analogous proof in Proposition 10.3.
Reference: [ESY] <author> S. Even, A. Selman and Y. Yacobi. </author> <title> The complexity of promise problems with applications to public-key cryptography. </title> <journal> Information and Control, </journal> <volume> Vol. 2, </volume> <year> 1984, </year> <pages> 159-173. </pages>
Reference-contexts: While the task is typically language recognition (namely to recognize whether x is in some fixed language L) we will, more 6 Bellare, Goldreich, Sudan generally, consider promise problems (A; B) consisting of a set A of "positive" instances and a set B of "negative" instances <ref> [ESY] </ref>. A languages L is identified with the promise problem (L; L). Of interest in the applications are various parameters of the system. The completeness probability c = c (n) and the soundness probability s = s (n) are defined in the usual ways. <p> We will ask that all functions measuring complexity (e.g. the query complexity q = q (n)) be admissible. In defining complexity classes we will consider promise problems rather than languages. (This convention is adopted since approximation problems are easily cast as promise problems.) Following Even et. al. <ref> [ESY] </ref>, a promise problem is a pair of disjoint sets (A; B), the first being the set of "positive" instances and the second the set of "negative" instances.
Reference: [Fe1] <author> U. Feige. </author> <title> Randomized graph products, chromatic numbers, and the Lovasz theta function. </title> <booktitle> Proceedings of the 27th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1995, </year> <pages> pp. 635-640. </pages>
Reference-contexts: There is not even a heuristic algorithm that is conjectured to do better. (The Lovasz Theta function had been conjectured to approximate the Max Clique size within p N but this conjecture was disproved by Feige <ref> [Fe1] </ref>.) Prior to 1991, no non-approximability results on Max Clique were known. In 1991 the connection to proofs was made by Feige et. al. [FGLSS]. <p> We also note that Furer's reduction is randomized while the rest are deterministic. Reductions among Max Clique Problems. Next we present an invariance of the Gap Clique problem with respect to shifting of the gaps. The following result has also been independently observed by Feige <ref> [Fe1] </ref>, where he uses a randomized graph product to show the result. Our description uses the properties of fpcp and its equivalence to clique approximation. Theorem 8.12 Let k; * 1 ; * 2 be real numbers such that k 1 and 0 * 1 &lt; * 2 1.
Reference: [Fe2] <author> U. Feige. </author> <title> Set Cover. A threshold of ln n for approximating set cover. </title> <booktitle> In Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 314-318, </pages> <year> 1996. </year>
Reference-contexts: By now, tight results are known for central problems such as Min-Set-Cover (cf., <ref> [LuYa, BGLR, Fe2] </ref>), Max-Clique (cf., [H1, H2]), Min-Coloring ([FeKi2]), and Max-3SAT (cf., [H3]). The latter results were obtained subsequently to the current work, and while building on it. Amortized free-bits and Max-Clique. The most intriguing problem left open by our work has been resolved by H-astad [H1, H2].
Reference: [FeGo] <author> U. Feige and M. Goemans. </author> <title> Approximating the value of two prover proof systems, with application to Max-2SAT and Max-DICUT. </title> <booktitle> Proceedings of the Third Israel Symposium on Theory and Computing Systems, IEEE, </booktitle> <year> 1995, </year> <pages> pp. 182-189. </pages>
Reference-contexts: 2m 3m free-bits [BeSu] 8 Bellare, Goldreich, Sudan Problem Approx Non-Approx Factor Due to Our Factor Previous Factor Assumption Max3SAT 1:258 [Yan, GoWi2, TSSW] 1:038 1 + 1 72 [BeSu] P 6= NP MaxE3SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max2SAT 1:075 <ref> [GoWi2, FeGo] </ref> 1:013 1 + 1 504 (implied [BeSu]) P 6= NP MaxSAT 2 folklore 1 + 1 7 P 6= NP MaxCUT 1:139 [GoWi2] 1:014 unspecified [ALMSS] P 6= NP MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] <p> Recall that the latter is approximable within 2-o (1) [BaEv2, MoSp]. Our results for MaxCUT and Max2SAT show that it is infeasible to find a solution with value which is only a factor of 1.01 from optimal. This may be contrasted with the recent results of <ref> [GoWi2, FeGo] </ref> which shows that solutions which are within 1.14 and 1.075, respectively, of the optimum are obtainable in polynomial time. <p> Factor HARD to Approx. Factor Tight? Factor Due to Our Factor NEW Factor Assumption Max3SAT 1 + 1 7 + * [KaZw] (new) 1 + 1 26 1 + 1 MaxE3SAT 1 + 1 7 folklore 1 + 1 26 1 + 1 Max2SAT 1:075 <ref> [GoWi2, FeGo] </ref> 1:013 1:047 [H3] P 6= NP No MaxSAT 2 folklore 1 + 1 7 * 2 * [H3] P 6= NP Yes MaxCUT 1:139 [GoWi2] 1:014 1:062 [H3] P 6= NP No MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 1 + 1 Max-Clique N 1o (1) <p> This problem is particularly interesting because it has been the focus of recent improvements in the approximation factor attainable in polynomial-time. Specifically, Goemans and Williamson [GoWi2] exhibited a polynomial time algorithm achieving an approximation factor of 1 0:878 1:139, and subsequently Feige and Goemans <ref> [FeGo] </ref> exhibited an algorithm achieving 1 0:931 1:074. Non-approximability results for Max-SNP problems begin with [ALMSS] who proved that there exists a constant * &gt; 0 such that Gap-3SAT 1;1* is NP-hard.
Reference: [FGLSS] <author> U. Feige, S. Goldwasser, L. Lov asz, S. Safra, and M. Szegedy. </author> <title> Interactive proofs and the hardness of approximating cliques. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 43, No. 2, </volume> <year> 1996, </year> <pages> pp. 268-292. </pages>
Reference-contexts: Gap problems can be similarly defined for all the other optimization problems we consider. From now on, we discuss approximation in terms of these gap problems. The connection: Making gaps from proofs. The FGLSS-reduction <ref> [FGLSS] </ref> is a reduction of a promise problem (A; B) to Gap-MaxClique c;s for some appropriate c; s defined by the reduction. <p> First, one exhibits an appropriate proof system for NP. Then one applies the FGLSS reduction. The factor indicated hard depends on the Free Bits in PCP 7 proof system parameters. A key element in getting better results has been the distilling of appropriate pcp-parameters. The sequence of works <ref> [FGLSS, ArSa, ALMSS, BGLR, FeKi1, BeSu] </ref> lead us through a sequence of parameters: query complexity, free-bit complexity and, finally, for the best known results, amortized free-bit complexity. <p> Feige, Goldwasser, Lovasz, Safra and Szegedy <ref> [FGLSS] </ref> showed that NP has probabilistically checkable proofs of poly-logarithmic randomness and query complexity; namely, NP PCP 1;1=2 [r; q], where r (n) = q (n) = O (log n log log n). A hardness of approximation result based on interactive proofs was first proved by Condon [Con]. <p> A hardness of approximation result based on interactive proofs was first proved by Condon [Con]. The breakthrough PCP connection to approximation was made by Feige, Goldwasser, Lovasz, Safra and Szegedy <ref> [FGLSS] </ref>. <p> After the work of <ref> [FGLSS] </ref> the field took off in two major directions. One was to extend the interactive proof approach to prove the non-approximability of other optimization problems. Direct reductions from proofs were used to show the hardness of quadratic programming [BeRo, FeLo], Max3SAT [ALMSS], set cover [LuYa], and other problems [Be]. <p> The model underlying what are now known as "probabilistically checkable proofs" is the "oracle model" of Fortnow, Rompel and Sipser [FRS], introduced as an equivalent version (with respect to language recognition power) of the multi-prover model of Ben-Or, Goldwasser, Kilian and Wigderson [BGKW]. Interestingly, as shown by <ref> [BFLS, FGLSS] </ref>, this framework can be applied in a meaningful manner also to languages in NP. <p> Babai et. al. [BFLS] suggested a model in which the inputs are encoded in a special (polynomial-time computable and decodable) error-correcting code and the verifier works in poly-logarithmic time. Here we follow the model of Feige et. al. <ref> [FGLSS] </ref> where the verifier is probabilistic polynomial-time (as usual) and one considers finer complexity measures such as the query and randomness complexity. The FGLSS-reduction (cf., [FGLSS]), stated in terms of the query complexity (number of binary queries), randomness complexity and error probability of the proof system, has focused attention on the <p> Here we follow the model of Feige et. al. <ref> [FGLSS] </ref> where the verifier is probabilistic polynomial-time (as usual) and one considers finer complexity measures such as the query and randomness complexity. The FGLSS-reduction (cf., [FGLSS]), stated in terms of the query complexity (number of binary queries), randomness complexity and error probability of the proof system, has focused attention on the above model and these parameters. The class PCP 1;1=2 [r; q] was made explicit by [ArSa]. <p> If the reduction is deterministic we omit the subscript of "R," or, sometimes, for emphasis, replace it by a subscript of "D." An example is the randomized FGLSS transformation <ref> [FGLSS, BeSc, Zuc] </ref>. <p> In 1991 the connection to proofs was made by Feige et. al. <ref> [FGLSS] </ref>. The FGLSS reduction says that PCP 1;e [ coins = r; query = q ] Karp reduces to Gap-MaxClique c;s via a reduction running in time poly (2 r+q ), and with the gap c=s being a function of (r; q and) the error e. <p> However, as a consequence of some results from this paper, we are able to remove the assumptions made by the earlier papers and hence present those results in a simpler form. See Section 8.4 for details. Free Bits in PCP 29 Due to Factor Assumption <ref> [FGLSS] </ref> 2 log 1* N for any * &gt; 0 NP 6 e P [ArSa] 2 p [ALMSS] N * for some * &gt; 0 P 6= NP [BGLR] N 1=30 NP 6= coRP [BGLR] N 1=25 NP 6 coR e P [FeKi1] N 1=15 NP 6= coRP [BeSu] N 1=6 <p> 0 (1) NP K R Gap-ChromNum c;s for c (N )=s (N ) = N 1 (2) NP K D Gap-ChromNum c;s for c (N )=s (N ) = N 1 78 Part II Proofs and Approximation: Potential and Limitations 8 The reverse connection and its consequences Feige et al. <ref> [FGLSS] </ref> describe a procedure which takes a verifier V , and an input x and constructs a graph, which we denote G V (x), whose vertices correspond to possible accepting transcripts in V 's computation and edges corresponding to consistent/non-conflicting computations. <p> Each of the two parts of Proposition 8.7 shows that the well-known method of obtaining clique-approximation results from efficient pcp systems (cf., <ref> [FGLSS, BeSc, Zuc, FeKi1, BeSu] </ref>) is "complete" in the sense that if clique-approximation can be shown NP-hard then this can be done via this method. The precise statement is given in Theorems 8.10 and 8.11 (below). As a preparatory step, we first provide an easier-to-use form of the above proposition. <p> Furthermore, in case the proof system is of perfect completeness, we have c (N ) = N f=(1+f+*) and s (N ) = N (1+f)=(1+f+*) . Proof: We first amplify the gap of the pcp-verifier (cf., Corollary 11.3) and then by apply the bare FGLSS-reduction (see Theorem 8.1 and <ref> [FGLSS] </ref>) to the amplified verifier. Specifically, for any problem in FPCP [log; f ], we first obtain K R FPCP 1;2 t [(1 + *) t; f t], where t (n) = fl log 2 n (with the constant fl determined by the constant * &gt; 0). <p> The condition regarding the distance of the code is essential since the task is easy with respect to the identity map (which is a code of distance 1). We remark that testing "closeness" to codewords with respect to codes of large distance is essential in all known pcp constructions <ref> [BFLS, FGLSS, ArSa, ALMSS, BGLR, FeKi1, BeSu] </ref>. The absolute distance between two words w; u 2 f0; 1g n , denoted (w; u), is the number of bits on which w and u disagree.
Reference: [FeKi1] <author> U. Feige and J. Kilian. </author> <title> Two prover protocols Low error at affordable rates. </title> <booktitle> Proceedings of the 26th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1994, </year> <pages> pp. 172-183. </pages>
Reference-contexts: First, one exhibits an appropriate proof system for NP. Then one applies the FGLSS reduction. The factor indicated hard depends on the Free Bits in PCP 7 proof system parameters. A key element in getting better results has been the distilling of appropriate pcp-parameters. The sequence of works <ref> [FGLSS, ArSa, ALMSS, BGLR, FeKi1, BeSu] </ref> lead us through a sequence of parameters: query complexity, free-bit complexity and, finally, for the best known results, amortized free-bit complexity. <p> Indeed, if there is an NP-hard Max 3SAT gap problem with certain focus error queries free-bits previous related result 3 queries 0:85 3 2 error 72 73 via MaxSAT [BeSu] 2 free-bits 0.794 O (1) 2 error 1/2 1 2 11 7 32 queries (24 on average) <ref> [FeKi1] </ref> amortized free-bits O (2 m ) 2 3m 2m 3m free-bits [BeSu] 8 Bellare, Goldreich, Sudan Problem Approx Non-Approx Factor Due to Our Factor Previous Factor Assumption Max3SAT 1:258 [Yan, GoWi2, TSSW] 1:038 1 + 1 72 [BeSu] P 6= NP MaxE3SAT 1 + 1 7 folklore 1 + 1 <p> They presented new proof systems minimizing query complexity and exploited a slightly improved version of the FGLSS-reduction due to [BeSc, Zuc] to get a N 1=30 hardness of approximation factor for Max Clique. Feige and Kilian <ref> [FeKi1] </ref>, however, observed that one should work with free-bits, and noted that the free-bit complexity of the system of [BGLR] was 14, yielding a N 1=15 hardness factor. Bellare and Sudan then suggested the notion of amortized free-bits. 14 Bellare, Goldreich, Sudan Problem EASY to Approx. Factor HARD to Approx. <p> This served to focus attention on the roles of various parameters, both in reductions and in constructions. Also they introduced the consideration of average query complexity, the first in a sequence of parameter changes towards doing better for clique. Free bits are implicit in <ref> [FeKi1] </ref> and formalized in [BeSu]. Amortized free bits are introduced in [BeSu] but formalized a little better here. Proof sizes were considered in [BFLS, PoSp]. <p> Query complexity minimization. One seeks results of the form NP = PCP 1;1=2 [ coins = log ; query = q ; query av = q av ] : (4) 20 Bellare, Goldreich, Sudan Due to q q av [ALMSS] some constant some constant [BGLR] 36 29 <ref> [FeKi1] </ref> 32 24 This paper 11 10.9 logarithmic randomness; that is, results of the form of Eq. (4). This was originally done for deriving NP-hardness results for the approximation of MaxClique, but subsequent work has indicated that other parameters actually govern this application. <p> The two-prover proofs of Lapidot-Shamir and Feige-Lovasz [LaSh, FeLo] had poly-logarithmic randomness and answer sizes, so [ALMSS] used a modification of these, in the process increasing the number of provers to a constant much larger than two. The later constructions of few-prover proofs of <ref> [BGLR, Tar, FeKi1] </ref> lead to better non-approximability results. Bellare and Sudan [BeSu] identified some extra features of constant prover proofs whose presence they showed could be exploited to further increase the non-approximability factors. These features are captured in their definition of canonical verifiers (cf. Section 3.4). <p> Bellare and Sudan [BeSu] identified some extra features of constant prover proofs whose presence they showed could be exploited to further increase the non-approximability factors. These features are captured in their definition of canonical verifiers (cf. Section 3.4). But the proof systems of <ref> [FeKi1] </ref> that had worked above no longer sufficed| they are not canonical. So instead [BeSu] used (a slight modification of) the proofs of [LaSh, FeLo], thereby incurring poly-logarithmic randomness and answer sizes, so that the assumptions in their non-approximability results pertain to quasi-polynomial time classes. (Alternatively they modify the [FeKi1] system <p> of <ref> [FeKi1] </ref> that had worked above no longer sufficed| they are not canonical. So instead [BeSu] used (a slight modification of) the proofs of [LaSh, FeLo], thereby incurring poly-logarithmic randomness and answer sizes, so that the assumptions in their non-approximability results pertain to quasi-polynomial time classes. (Alternatively they modify the [FeKi1] system to a canonical three-prover one, but then incur a decrease in the non-approximability factors due to having more provers). <p> which implies the ex Free Bits in PCP 21 Due to Provers Coins Answer size Canonical? Can be made canonical? [LaSh, FeLo] 2 polylog polylog No Yes [BeSu] [ALMSS] poly (* 1 ) log polylog No ? [BGLR] 4 log polyloglog No ? [Tar] 3 log O (1) No ? <ref> [FeKi1] </ref> 2 log O (1) No At cost of one more prover [BeSu] [Raz] 2 log O (1) Yes (NA) istence of a two-provers proof system with logarithmic randomness and constant answer size [Raz]. <p> Due to Assuming Factor Technique [ALMSS] P 6= NP some constant NP PCP 1;1=2 [ log; O (1) ]; Reduction of this to Max3SAT. [BGLR] e P 6= N e P 94=93 Framework; better analyses; uses proof systems of [LaSh, FeLo]. [BGLR] P 6= NP 113=112 New four-prover proof systems. <ref> [FeKi1] </ref> P 6= NP 94=93 New two-prover proof systems. [BeSu] e P 6= N e P 66=65 Canonicity and some optimizations. [BeSu] P 6= NP 73=72 Canonicity and some optimizations. <p> They also introduced a framework for better analysis, and improved some previous analyses; we exploit in particular their better analyses of linearity testing (cf. Section 3.5) and of Freivalds's matrix multiplication test (cf. Lemma 3.16). The improvement of Feige and Kilian <ref> [FeKi1] </ref> was obtained via new proof systems; that of [BeSu] by use of the canonicity property of constant prover proofs and some optimizations. (See Section 2.2.3 for a discussion of the role of constant-prover proofs in this context). <p> This leads to their hardness results shown in Figure 7. However, significantly reducing the (average) number of bits queried seemed hard. However, as observed by Feige and Kilian, the performance of the FGLSS reduction actually depends on the free-bit complexity which may be significantly smaller than the query complexity <ref> [FeKi1] </ref>. Namely, the factor in the above mentioned reduction is N 1=(1+f) where f is the free-bit complexity. They observed that the proof system of [BGLR] has free-bit complexity 14, yielding a N 1=15 hardness of approximation factor. The notion of amortized free-bits was introduced in [BeSu]. <p> Bits in PCP 29 Due to Factor Assumption [FGLSS] 2 log 1* N for any * &gt; 0 NP 6 e P [ArSa] 2 p [ALMSS] N * for some * &gt; 0 P 6= NP [BGLR] N 1=30 NP 6= coRP [BGLR] N 1=25 NP 6 coR e P <ref> [FeKi1] </ref> N 1=15 NP 6= coRP [BeSu] N 1=6 P 6= NP [BeSu] N 1=4 NP 6 coR e P This paper N 1=4 P 6= NP This paper N 1=3 NP 6= coRP approximate within N ffi for some ffi &gt; 0. But, again, ffi is very small. <p> This means that even the (modified) [LaSh, FeLo] type proofs won't suffice for us. We could use the three-prover modification of <ref> [FeKi1] </ref> but the cost would wipe out our gain. Luckily this discussion is moot since we can use the recent result of Raz [Raz] to provide us with a canonical two-prover proof having logarithmic randomness, constant answer size, and any constant error. This makes an ideal starting point. <p> Each of the two parts of Proposition 8.7 shows that the well-known method of obtaining clique-approximation results from efficient pcp systems (cf., <ref> [FGLSS, BeSc, Zuc, FeKi1, BeSu] </ref>) is "complete" in the sense that if clique-approximation can be shown NP-hard then this can be done via this method. The precise statement is given in Theorems 8.10 and 8.11 (below). As a preparatory step, we first provide an easier-to-use form of the above proposition. <p> The condition regarding the distance of the code is essential since the task is easy with respect to the identity map (which is a code of distance 1). We remark that testing "closeness" to codewords with respect to codes of large distance is essential in all known pcp constructions <ref> [BFLS, FGLSS, ArSa, ALMSS, BGLR, FeKi1, BeSu] </ref>. The absolute distance between two words w; u 2 f0; 1g n , denoted (w; u), is the number of bits on which w and u disagree.
Reference: [FeKi2] <author> U. Feige and J. Kilian. </author> <title> Zero-knowledge and the chromatic number. </title> <booktitle> Proceedings of the 11th Annual Conference on Computational Complexity, IEEE, </booktitle> <year> 1996. </year>
Reference-contexts: N 1o (1) [BoHa] N 1 5 * N 1* <ref> [FeKi2] </ref> coRP 6= NP Yes * &gt; 0 is an arbitrarily small constant. They constructed proof systems achieving amortized free-bit complexity three, and in thus obtained a N 1=4 hardness for Max Clique assuming NP 6 coR e P. <p> Improved 2-free-bits proofs and Min-VC. The above-mentioned proof system of H-astad [H3] uses two (non-amortized) free-bits, and so NP FPCP 1*;0:5 [log; 2], for every * &gt; 0. This sets the non-approximability bound for Min Vertex-Cover at 7 6 *. Chromatic Number. Feige and Kilian <ref> [FeKi2] </ref> have introduced a new approach to showing hardness of approximability of ChromNum, based on a new measure of proof checking complexity called the covering complexity. <p> But more recently an even more efficient reduction has become available, namely that of Furer [Fu]. This reduction achieves h (*) = *=(2 *), and thereby we get our N 1=5 hardness. Following the appearance of our results, Feige and Kilian <ref> [FeKi2] </ref> have introduced a new approach to showing hardness of approximability of ChromNum. See discussion in Section 1.5. Randomized and de-randomized error reduction. As mentioned above, randomized and de-randomized error reduction techniques play an important role in obtaining the best Clique hardness results via the FGLSS method.
Reference: [FeLo] <author> U. Feige and L. Lov asz. </author> <title> Two-prover one round proof systems: Their power and their problems. </title> <booktitle> Proceedings of the 24th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1992, </year> <pages> pp. 733-744. </pages>
Reference-contexts: After the work of [FGLSS] the field took off in two major directions. One was to extend the interactive proof approach to prove the non-approximability of other optimization problems. Direct reductions from proofs were used to show the hardness of quadratic programming <ref> [BeRo, FeLo] </ref>, Max3SAT [ALMSS], set cover [LuYa], and other problems [Be]. The earlier work of Papadimitriou and Yannakakis introducing the class MaxSNP [PaYa] now came into play; by reduction from Max3SAT it implied hardness of approximation for any MaxSNP-hard problem. <p> Constant-prover proofs have been instrumental in the derivation of non-approximability results in several ways. One of these is that they are a good starting point for reductions| examples of such are reductions of two-prover proofs to quadratic programming <ref> [BeRo, FeLo] </ref> and set cover [LuYa]. However, it is a different aspect of constant prover proofs that is of more direct concern to us. This aspect is the use of constant-prover proof systems as the penultimate step of the recursion, and begins with [ALMSS]. <p> The available constant-prover proof systems appear in Figure 5 and are discussed below. Throughout this discussion we consider proof systems obtaining an arbitrary small constant error probability. The two-prover proofs of Lapidot-Shamir and Feige-Lovasz <ref> [LaSh, FeLo] </ref> had poly-logarithmic randomness and answer sizes, so [ALMSS] used a modification of these, in the process increasing the number of provers to a constant much larger than two. The later constructions of few-prover proofs of [BGLR, Tar, FeKi1] lead to better non-approximability results. <p> These features are captured in their definition of canonical verifiers (cf. Section 3.4). But the proof systems of [FeKi1] that had worked above no longer sufficed| they are not canonical. So instead [BeSu] used (a slight modification of) the proofs of <ref> [LaSh, FeLo] </ref>, thereby incurring poly-logarithmic randomness and answer sizes, so that the assumptions in their non-approximability results pertain to quasi-polynomial time classes. (Alternatively they modify the [FeKi1] system to a canonical three-prover one, but then incur a decrease in the non-approximability factors due to having more provers). <p> A breakthrough result in this area is Raz's Parallel Repetition Theorem which implies the ex Free Bits in PCP 21 Due to Provers Coins Answer size Canonical? Can be made canonical? <ref> [LaSh, FeLo] </ref> 2 polylog polylog No Yes [BeSu] [ALMSS] poly (* 1 ) log polylog No ? [BGLR] 4 log polyloglog No ? [Tar] 3 log O (1) No ? [FeKi1] 2 log O (1) No At cost of one more prover [BeSu] [Raz] 2 log O (1) Yes (NA) istence <p> Similarly for the normalized version. 26 Bellare, Goldreich, Sudan Due to Assuming Factor Technique [ALMSS] P 6= NP some constant NP PCP 1;1=2 [ log; O (1) ]; Reduction of this to Max3SAT. [BGLR] e P 6= N e P 94=93 Framework; better analyses; uses proof systems of <ref> [LaSh, FeLo] </ref>. [BGLR] P 6= NP 113=112 New four-prover proof systems. [FeKi1] P 6= NP 94=93 New two-prover proof systems. [BeSu] e P 6= N e P 66=65 Canonicity and some optimizations. [BeSu] P 6= NP 73=72 Canonicity and some optimizations. <p> But we don't take advantage of this fact.) In addition we need answer sizes of log log n as opposed to the O (log n) of previous methods, for reasons explained below. This means that even the (modified) <ref> [LaSh, FeLo] </ref> type proofs won't suffice for us. We could use the three-prover modification of [FeKi1] but the cost would wipe out our gain.
Reference: [FRS] <author> L. Fortnow, J. Rompel and M. Sipser. </author> <title> On the power of multiprover interactive protocols. </title> <journal> Theoretical Computer Science, </journal> <volume> Vol. 134, No. 2, </volume> <year> 1994, </year> <pages> pp. 545-557. 117 </pages>
Reference-contexts: Interactive proofs were introduced by Goldwasser, Micali and Rackoff [GMR] and Babai [Bab]. Ben-Or, Goldwasser, Kilian and Wigderson [BGKW] extended these ideas to define a notion of multi-prover interactive proofs. Fortnow, Rompel and Sipser <ref> [FRS] </ref> showed that the class, MIP, of languages possessing multi-prover interactive proofs equals the class of languages which have (using todays terms) probabilistically checkable proofs (of unrestricted, and thus polynomial, randomness and query complexity). <p> The model underlying what are now known as "probabilistically checkable proofs" is the "oracle model" of Fortnow, Rompel and Sipser <ref> [FRS] </ref>, introduced as an equivalent version (with respect to language recognition power) of the multi-prover model of Ben-Or, Goldwasser, Kilian and Wigderson [BGKW]. Interestingly, as shown by [BFLS, FGLSS], this framework can be applied in a meaningful manner also to languages in NP. <p> Employing the FRS-method <ref> [FRS] </ref> to any PCP (log,O (1))-system for NP (e.g., [ALMSS]) one gets a canonical verifier which is ffi-good for some ffi &lt; 1. (Roughly, the method is to take the given pcp system, send all queries to one oracle, and, as a check, a random one of them to the other
Reference: [Fu] <author> M. F urer. </author> <title> Improved hardness results for approximating the chromatic number. </title> <booktitle> Proceedings of the 36th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1995, </year> <pages> pp. 414-421. </pages>
Reference-contexts: 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 coR ~ P Number N 1 1 7 <ref> [Fu] </ref> coRP 6= NP N 7 N 14 [BeSu] P 6= NP we show are hard to achieve (Non-Approx). <p> The conclusion for Max Clique follows, of course, from the FGLSS-reduction and the first proof system listed above. The conclusion for the Chromatic Number follows from a recent reduction of Furer <ref> [Fu] </ref>, which in turn builds on reductions in [LuYa, KLS, BeSu]. (Furer's work and ours are contemporaneous and thus we view the N 1=5 hardness result as jointly due to both papers.) The improvements for the MaxSNP problems are perhaps more significant than the Max Clique one: We see hardness results <p> Our N 1=3 hardness for Clique would yield, via this, a N 1=7 hardness for the chromatic number. But more recently an even more efficient reduction has become available, namely that of Furer <ref> [Fu] </ref>. This reduction achieves h (*) = *=(2 *), and thereby we get our N 1=5 hardness. Following the appearance of our results, Feige and Kilian [FeKi2] have introduced a new approach to showing hardness of approximability of ChromNum. See discussion in Section 1.5. Randomized and de-randomized error reduction. <p> Using the FGLSS-construction on this system, the claim of Part (2) follows. Combining the above with a recent reduction of Furer <ref> [Fu] </ref>, we get Theorem 7.12 For any * &gt; 0 (1) NP K R Gap-ChromNum c;s for c (N )=s (N ) = N 1 (2) NP K D Gap-ChromNum c;s for c (N )=s (N ) = N 1 78 Part II Proofs and Approximation: Potential and Limitations 8 The <p> However now we know that FPCP and Max Clique are equivalent, so we can go back and rephrase the old statements. Thus results of <ref> [LuYa, KLS, BeSu, Fu] </ref> can be summarized as: For every ff; *; fl &gt; 0, Gap-MaxClique N ff1 ;N *1 K R Gap-ChromNum N (*+fl) ;N h (ff) , where (1) h (ff) = minf 1 6 ; ff 54ff g [LuYa]. (2) h (ff) = minf 1 11 ; ff
Reference: [FGMSZ] <author> M. F urer, O. Goldreich, Y. Mansour, M. Sipser, and S. Zachos. </author> <title> On completeness and soundness in interactive proof systems. </title> <booktitle> In Advances in Computing Research: a research annual, Vol. 5 (Randomness and Computation, </booktitle> <editor> S. Micali, ed.), </editor> <year> 1989, </year> <pages> pp. 429-442. </pages>
Reference-contexts: Specifically, we first reduce the error of the interactive proof by parallel repetition, next transform it into an Arthur-Merlin interactive proof [GS], and finally transform it into an Arthur-Merlin interactive proof of perfect completeness <ref> [FGMSZ] </ref>. We stress that all the transformations maintain the number of rounds upto a constant and that the constant-round Arthur-Merlin hierarchy collapses to one-round [Bab].
Reference: [GJ1] <author> M. Garey and D. Johnson. </author> <title> The complexity of near optimal graph coloring. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 23, No. 1, </volume> <year> 1976, </year> <pages> pp. 43-49. </pages>
Reference-contexts: 2 k [log; f k], provided that the original system has at least 2 k accepting configurations per each possible sequence of coin-tosses. (This condition is satisfied in many natural pcp systems, even for k = f .) 1.3 History Early work in non-approximability includes that of Garey and Johnson <ref> [GJ1] </ref> showing that it is NP-hard to approximate the chromatic factor within a factor less than two. The indication of higher factors, and results for other problems, had to wait for the interactive proof approach. Interactive proofs were introduced by Goldwasser, Micali and Rackoff [GMR] and Babai [Bab]. <p> They then showed that NP FPCP [polylog; 3]. This lead to a N 1=4 hardness factor assuming NP 6= coR e P. See Section 7 for our results. Chromatic Number. The first hardness result for the chromatic number is due to Garey and Johnson <ref> [GJ1] </ref>. They showed that if P 6= NP then there is no polynomial time algorithm that can achieve a factor less than 2. This remained the best result until the connection to proof systems, and the above mentioned results, emerged.
Reference: [GJ2] <author> M. Garey and D. Johnson. </author> <title> Computers and Intractability: A guide to the theory of NP-completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1979. </year>
Reference-contexts: See Section 4.3 for our results. Vertex cover. There is a simple polynomial time algorithm to approximate MinVC in unweighted graphs within a factor of 2. The algorithm, due to F. Gavril (cf. <ref> [GJ2] </ref>), consists of taking all vertices which appear in a maximal matching of the graph. For weighted graphs, Bar-Yehuda and Even [BaEv1] and Hochbaum [Hoc], gave algorithms achieving the same approximation factor. <p> Then, for each x 2 L this graph has a vertex cover of density at most *, whereas for x 62 L this graph has no vertex cover of density 2*. Using Gavril's approximation algorithm (cf. <ref> [GJ2] </ref>), these two cases are distinguishable in polynomial-time and so the third claim follows. Proof of Proposition 10.9, Part (4): Let L 2 PCP 1;s [poly; 0] and V be a verifier demonstrating this fact.
Reference: [GJS] <author> M. Garey, D. Johnson and L. Stockmeyer. </author> <title> Some simplified NP-complete graph problems. </title> <journal> Theoretical Computer Science, </journal> <volume> Vol. 1, </volume> <year> 1976, </year> <pages> pp. 237-267. </pages>
Reference-contexts: For MaxE3SAT, which is also Max-SNP complete, a very simple algorithm achieves an approximation of 8=7 1:143 (where 7=8 is the expected fraction of clauses satisfied by a uniformly chosen assignment). Max2SAT is also Max-SNP complete <ref> [GJS, PaYa] </ref>. This problem is particularly interesting because it has been the focus of recent improvements in the approximation factor attainable in polynomial-time. <p> The improvement of Feige and Kilian [FeKi1] was obtained via new proof systems; that of [BeSu] by use of the canonicity property of constant prover proofs and some optimizations. (See Section 2.2.3 for a discussion of the role of constant-prover proofs in this context). Garey, Johnson and Stockmeyer <ref> [GJS] </ref> had provided, as early as 1976, a reduction of Max3SAT to Max2SAT which showed that if the former is non-approximable within (k + 1)=k then the latter Free Bits in PCP 27 is non-approximable within (7k + 1)=(7k).
Reference: [GoWi1] <author> M. Goemans and D. Williamson. </author> <title> New 3/4-approximation algorithms for the maximum satisfiablity problem. </title> <journal> SIAM Journal on Discrete Mathematics, </journal> <volume> Vol. 7, No. 4, </volume> <year> 1994, </year> <pages> pp. 656-666. </pages>
Reference-contexts: Max3SAT is the canonical Max-SNP complete problem [PaYa]. A polynomial-time algorithm due to Yannakakis [Yan] approximates it to within a factor of 4=3 &lt; 1:334 (see Goemans and Williamson <ref> [GoWi1] </ref> for an alternate algorithm). Currently the best known polynomial-time algorithm for Max3SAT achieves a factor of 1:258 (and is due to Trevisan et. al. [TSSW] which in turn build on Goemans and Williamson [GoWi2]).
Reference: [GoWi2] <author> M. Goemans and D. Williamson. </author> <title> Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 42, No. 6, </volume> <year> 1995, </year> <pages> pp. 1115-1145. </pages>
Reference-contexts: via MaxSAT [BeSu] 2 free-bits 0.794 O (1) 2 error 1/2 1 2 11 7 32 queries (24 on average) [FeKi1] amortized free-bits O (2 m ) 2 3m 2m 3m free-bits [BeSu] 8 Bellare, Goldreich, Sudan Problem Approx Non-Approx Factor Due to Our Factor Previous Factor Assumption Max3SAT 1:258 <ref> [Yan, GoWi2, TSSW] </ref> 1:038 1 + 1 72 [BeSu] P 6= NP MaxE3SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max2SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP MaxSAT 2 folklore 1 + 1 7 P 6= NP <p> 2m 3m free-bits [BeSu] 8 Bellare, Goldreich, Sudan Problem Approx Non-Approx Factor Due to Our Factor Previous Factor Assumption Max3SAT 1:258 [Yan, GoWi2, TSSW] 1:038 1 + 1 72 [BeSu] P 6= NP MaxE3SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max2SAT 1:075 <ref> [GoWi2, FeGo] </ref> 1:013 1 + 1 504 (implied [BeSu]) P 6= NP MaxSAT 2 folklore 1 + 1 7 P 6= NP MaxCUT 1:139 [GoWi2] 1:014 unspecified [ALMSS] P 6= NP MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] <p> 1:038 1 + 1 72 [BeSu] P 6= NP MaxE3SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max2SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP MaxSAT 2 folklore 1 + 1 7 P 6= NP MaxCUT 1:139 <ref> [GoWi2] </ref> 1:014 unspecified [ALMSS] P 6= NP MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP <p> Recall that the latter is approximable within 2-o (1) [BaEv2, MoSp]. Our results for MaxCUT and Max2SAT show that it is infeasible to find a solution with value which is only a factor of 1.01 from optimal. This may be contrasted with the recent results of <ref> [GoWi2, FeGo] </ref> which shows that solutions which are within 1.14 and 1.075, respectively, of the optimum are obtainable in polynomial time. <p> Factor HARD to Approx. Factor Tight? Factor Due to Our Factor NEW Factor Assumption Max3SAT 1 + 1 7 + * [KaZw] (new) 1 + 1 26 1 + 1 MaxE3SAT 1 + 1 7 folklore 1 + 1 26 1 + 1 Max2SAT 1:075 <ref> [GoWi2, FeGo] </ref> 1:013 1:047 [H3] P 6= NP No MaxSAT 2 folklore 1 + 1 7 * 2 * [H3] P 6= NP Yes MaxCUT 1:139 [GoWi2] 1:014 1:062 [H3] P 6= NP No MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 1 + 1 Max-Clique N 1o (1) <p> [KaZw] (new) 1 + 1 26 1 + 1 MaxE3SAT 1 + 1 7 folklore 1 + 1 26 1 + 1 Max2SAT 1:075 [GoWi2, FeGo] 1:013 1:047 [H3] P 6= NP No MaxSAT 2 folklore 1 + 1 7 * 2 * [H3] P 6= NP Yes MaxCUT 1:139 <ref> [GoWi2] </ref> 1:014 1:062 [H3] P 6= NP No MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 1 + 1 Max-Clique N 1o (1) [BoHa] N 1 3 * N 1* [H2] coRP 6= NP Yes N 4 * N 2 * [H2] P 6= NP No Chromatic No. <p> Currently the best known polynomial-time algorithm for Max3SAT achieves a factor of 1:258 (and is due to Trevisan et. al. [TSSW] which in turn build on Goemans and Williamson <ref> [GoWi2] </ref>). For MaxE3SAT, which is also Max-SNP complete, a very simple algorithm achieves an approximation of 8=7 1:143 (where 7=8 is the expected fraction of clauses satisfied by a uniformly chosen assignment). Max2SAT is also Max-SNP complete [GJS, PaYa]. <p> Max2SAT is also Max-SNP complete [GJS, PaYa]. This problem is particularly interesting because it has been the focus of recent improvements in the approximation factor attainable in polynomial-time. Specifically, Goemans and Williamson <ref> [GoWi2] </ref> exhibited a polynomial time algorithm achieving an approximation factor of 1 0:878 1:139, and subsequently Feige and Goemans [FeGo] exhibited an algorithm achieving 1 0:931 1:074. <p> See Section 4.2.2 for our results. Max Cut. In 1976, Sahni and Gonzales [SaGo] gave a simple 2-approximation algorithm for this problem. Recently, in a breakthrough result, Goemans and Williamson <ref> [GoWi2] </ref> gave a new algorithm which achieves a ratio of 1 0:878 = 1:139 for this problem. On the other hand, [PaYa] give an approximation preserving reduction from Max3SAT to MaxCUT.
Reference: [GMW] <author> O. Goldreich, S. Micali, and A. Wigderson. </author> <title> Proofs that yield nothing but their validity, or all languages in NP have zero-knowledge proof systems. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 38, No. 1, </volume> <month> July </month> <year> 1991, </year> <pages> pp. 691-729. </pages>
Reference-contexts: to indicate that for every s &lt; 1, FPCP 1;s [poly; 0] coNP (2) FPCP 1;s [poly; 1] PSPACE (3) It seems that FPCP 1;1=2 [poly; 0] is not contained in BPP, since Quadratic Non-Residuosity and Graph Non-Isomorphism belong to the former class. (Specifically, the interactive proofs of [GMR] and <ref> [GMW] </ref> can be viewed as a pcp system with polynomial randomness, query complexity 1 and free-bit complexity 0.) Thus, it seems that the obvious observation PCP 1;s [poly; 1] AM (for every s &lt; 1, where AM stands for one round Arthur-Merlin games), would also be hard to improve upon. <p> First indication to the power of interactive proof systems was given in <ref> [GMW] </ref>, where it was shown that interactive proofs exist for Graph Non-Isomorphism (whereas this language is not known to be in NP). <p> Using a poly-logarithmic-space decision procedure for satisfiability of 2CNF formulae 14 , we can decide if x is satisfiable using poly (jxj)-space. Part (5) (i.e., FPCP 1;s [poly; 1] PSPACE) follows. Proof of Proposition 10.9, Part (6): We merely note that the interactive proof presented in <ref> [GMW] </ref> for Graph Non-Isomorphism 15 constitute a 1-query pcp system with perfect completeness and soundness bound 1 2 . Furthermore, the query made by the verify has a unique acceptable answer and thus the free-bit complexity of this system is zero. <p> Furthermore, the query made by the verify has a unique acceptable answer and thus the free-bit complexity of this system is zero. The same holds for the interactive proof presented in [GMR] for Quadratic Non-Residuosity QNR, which is actually the inspiration to the proof in <ref> [GMW] </ref>. 10.4 Query complexity versus free-bit complexity The following proposition quantifies the intuition that not all queries are "undetermined" (i.e., that the free-bit complexity is lower than the query complexity).
Reference: [GMR] <author> S. Goldwasser, S. Micali, and C. Rackoff. </author> <title> The knowledge complexity of interactive proofs. </title> <journal> SIAM J. Computing, </journal> <volume> Vol 18, No. 1, </volume> <year> 1989, </year> <pages> pp. 186-208. </pages>
Reference-contexts: of interest to indicate that for every s &lt; 1, FPCP 1;s [poly; 0] coNP (2) FPCP 1;s [poly; 1] PSPACE (3) It seems that FPCP 1;1=2 [poly; 0] is not contained in BPP, since Quadratic Non-Residuosity and Graph Non-Isomorphism belong to the former class. (Specifically, the interactive proofs of <ref> [GMR] </ref> and [GMW] can be viewed as a pcp system with polynomial randomness, query complexity 1 and free-bit complexity 0.) Thus, it seems that the obvious observation PCP 1;s [poly; 1] AM (for every s &lt; 1, where AM stands for one round Arthur-Merlin games), would also be hard to improve <p> The indication of higher factors, and results for other problems, had to wait for the interactive proof approach. Interactive proofs were introduced by Goldwasser, Micali and Rackoff <ref> [GMR] </ref> and Babai [Bab]. Ben-Or, Goldwasser, Kilian and Wigderson [BGKW] extended these ideas to define a notion of multi-prover interactive proofs. <p> Proof of Proposition 10.8, Part (1): We first observe that a 1-query pcp system is actually a one-round interactive proof system (cf., <ref> [GMR] </ref>). (The completeness and soundness bounds are as in the pcp system.) Using well-known transformations we obtain the claimed result. <p> For systems with two-sided error probability, we knew that they can recognize N P languages using zero free-bits see below. 13 We note, however, that the more relaxed notion of free-bits may be less relevant to proving hardness of approximation results. 103 The same holds for QNR ("Quadratic Non-Residuosity" (cf., <ref> [GMR] </ref>)) the set of integer pairs (x; N ) so that x is a quadratic non-residue modulo N . Proof of Proposition 10.9, Part (3): The first claim of Part 3 is justified by Theorem 5.4. <p> Furthermore, the query made by the verify has a unique acceptable answer and thus the free-bit complexity of this system is zero. The same holds for the interactive proof presented in <ref> [GMR] </ref> for Quadratic Non-Residuosity QNR, which is actually the inspiration to the proof in [GMW]. 10.4 Query complexity versus free-bit complexity The following proposition quantifies the intuition that not all queries are "undetermined" (i.e., that the free-bit complexity is lower than the query complexity).
Reference: [GS] <author> S. Goldwasser and M. Sipser. </author> <title> Private coins versus public coins in interactive proof systems. </title> <booktitle> Proceedings of the 18th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1986, </year> <pages> pp. 59-68. </pages>
Reference-contexts: Specifically, we first reduce the error of the interactive proof by parallel repetition, next transform it into an Arthur-Merlin interactive proof <ref> [GS] </ref>, and finally transform it into an Arthur-Merlin interactive proof of perfect completeness [FGMSZ]. We stress that all the transformations maintain the number of rounds upto a constant and that the constant-round Arthur-Merlin hierarchy collapses to one-round [Bab].
Reference: [H1] <author> J. H -astad. </author> <title> Testing of the long code and hardness for clique. </title> <booktitle> Proceedings of the 28th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1996, </year> <pages> pp. 11-19. </pages>
Reference-contexts: In retrospect, our lower bounds justify H-astad's two deviations from these techniques; specifically, his relaxation of the codeword test <ref> [H1] </ref> and his relaxation of the projection test [H2]. Specifically, H-astad [H1, H2] has constructed a pcp system (for NP) of amortized free-bit complexity *, 8* &gt; 0. This was done in two stages/papers. In his first paper [H1], H-astad builds on the framework presented in the current work but introduces <p> In retrospect, our lower bounds justify H-astad's two deviations from these techniques; specifically, his relaxation of the codeword test [H1] and his relaxation of the projection test [H2]. Specifically, H-astad <ref> [H1, H2] </ref> has constructed a pcp system (for NP) of amortized free-bit complexity *, 8* &gt; 0. This was done in two stages/papers. <p> two deviations from these techniques; specifically, his relaxation of the codeword test <ref> [H1] </ref> and his relaxation of the projection test [H2]. Specifically, H-astad [H1, H2] has constructed a pcp system (for NP) of amortized free-bit complexity *, 8* &gt; 0. This was done in two stages/papers. In his first paper [H1], H-astad builds on the framework presented in the current work but introduces a relaxed codeword test which is conducted within amortized free-bit complexity *. In his second paper [H2], H-astad abandons the current framework and utilizes a relaxed projection test which is conducted within amortized free-bit complexity *. <p> In retrospect, both sets of assumptions could be by-passed as done by H-astad <ref> [H1, H2] </ref>.) 1.5 Subsequent work Our prophecy by which the PCP approach is leading to tight non-approximability results is in the process of materializing (see Figure 3). By now, tight results are known for central problems such as Min-Set-Cover (cf., [LuYa, BGLR, Fe2]), Max-Clique (cf., [H1, H2]), Min-Coloring ([FeKi2]), and Max-3SAT <p> by-passed as done by H-astad <ref> [H1, H2] </ref>.) 1.5 Subsequent work Our prophecy by which the PCP approach is leading to tight non-approximability results is in the process of materializing (see Figure 3). By now, tight results are known for central problems such as Min-Set-Cover (cf., [LuYa, BGLR, Fe2]), Max-Clique (cf., [H1, H2]), Min-Coloring ([FeKi2]), and Max-3SAT (cf., [H3]). The latter results were obtained subsequently to the current work, and while building on it. Amortized free-bits and Max-Clique. The most intriguing problem left open by our work has been resolved by H-astad [H1, H2]. <p> such as Min-Set-Cover (cf., [LuYa, BGLR, Fe2]), Max-Clique (cf., <ref> [H1, H2] </ref>), Min-Coloring ([FeKi2]), and Max-3SAT (cf., [H3]). The latter results were obtained subsequently to the current work, and while building on it. Amortized free-bits and Max-Clique. The most intriguing problem left open by our work has been resolved by H-astad [H1, H2]. He proved our conjecture (cf., [BGS2]) by which, for every * &gt; 0, it is the case that NP FPCP [log; *]. The Long-Code, introduced in this work, plays a pivotal role in H-astad's work. He also uses the idea of folding. <p> By modifying our proof systems so as to preserve the amortized free-bit complexity and achieve low covering complexity, they proved a that approximating ChromNum within N 1=3 is hard unless NP = coRP. They were able to similarly modify H-astad's proof systems <ref> [H1, H2] </ref> and thereby improve the hard factor to N 1* , for any * &gt; 0. Gadgets. Another research direction, suggested in early versions of this work [BGS2], was taken on by Trevisan et. al. [TSSW] who initiated a systematic study of the construction of gadgets. <p> Our original motivation in proving these lower bounds was to indicate that a paradigm shift is required in order to improve over our PCP systems of amortized free-bit complexity 2 (for NP). In retrospect, the paradigm shifts have amounted to the relaxation of the codeword test in <ref> [H1] </ref> and to the relaxation of the projection test in [H2]. Thus, our lower bounds may be considered as a justification for these (somewhat unnatural) relaxations. <p> H-astad's relaxation of the codeword test is different, yet it also suffices for the purpose of constructing PCP systems of amortized free-bit complexity 1 (for NP) <ref> [H1] </ref>. The lower bound on the complexity of the projection test seems more robust.
Reference: [H2] <author> J. H -astad. </author> <title> Clique is hard to approximate within n 1* . Proceedings of the 37th Symposium on Foundations of Computer Science, </title> <publisher> IEEE, </publisher> <year> 1996, </year> <pages> pp. 627-636. </pages>
Reference-contexts: In retrospect, our lower bounds justify H-astad's two deviations from these techniques; specifically, his relaxation of the codeword test [H1] and his relaxation of the projection test <ref> [H2] </ref>. Specifically, H-astad [H1, H2] has constructed a pcp system (for NP) of amortized free-bit complexity *, 8* &gt; 0. This was done in two stages/papers. <p> In retrospect, our lower bounds justify H-astad's two deviations from these techniques; specifically, his relaxation of the codeword test [H1] and his relaxation of the projection test [H2]. Specifically, H-astad <ref> [H1, H2] </ref> has constructed a pcp system (for NP) of amortized free-bit complexity *, 8* &gt; 0. This was done in two stages/papers. <p> This was done in two stages/papers. In his first paper [H1], H-astad builds on the framework presented in the current work but introduces a relaxed codeword test which is conducted within amortized free-bit complexity *. In his second paper <ref> [H2] </ref>, H-astad abandons the current framework and utilizes a relaxed projection test which is conducted within amortized free-bit complexity *. <p> No MaxSAT 2 folklore 1 + 1 7 * 2 * [H3] P 6= NP Yes MaxCUT 1:139 [GoWi2] 1:014 1:062 [H3] P 6= NP No MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 1 + 1 Max-Clique N 1o (1) [BoHa] N 1 3 * N 1* <ref> [H2] </ref> coRP 6= NP Yes N 4 * N 2 * [H2] P 6= NP No Chromatic No. N 1o (1) [BoHa] N 1 5 * N 1* [FeKi2] coRP 6= NP Yes * &gt; 0 is an arbitrarily small constant. <p> [H3] P 6= NP Yes MaxCUT 1:139 [GoWi2] 1:014 1:062 [H3] P 6= NP No MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 1 + 1 Max-Clique N 1o (1) [BoHa] N 1 3 * N 1* <ref> [H2] </ref> coRP 6= NP Yes N 4 * N 2 * [H2] P 6= NP No Chromatic No. N 1o (1) [BoHa] N 1 5 * N 1* [FeKi2] coRP 6= NP Yes * &gt; 0 is an arbitrarily small constant. <p> In retrospect, both sets of assumptions could be by-passed as done by H-astad <ref> [H1, H2] </ref>.) 1.5 Subsequent work Our prophecy by which the PCP approach is leading to tight non-approximability results is in the process of materializing (see Figure 3). By now, tight results are known for central problems such as Min-Set-Cover (cf., [LuYa, BGLR, Fe2]), Max-Clique (cf., [H1, H2]), Min-Coloring ([FeKi2]), and Max-3SAT <p> by-passed as done by H-astad <ref> [H1, H2] </ref>.) 1.5 Subsequent work Our prophecy by which the PCP approach is leading to tight non-approximability results is in the process of materializing (see Figure 3). By now, tight results are known for central problems such as Min-Set-Cover (cf., [LuYa, BGLR, Fe2]), Max-Clique (cf., [H1, H2]), Min-Coloring ([FeKi2]), and Max-3SAT (cf., [H3]). The latter results were obtained subsequently to the current work, and while building on it. Amortized free-bits and Max-Clique. The most intriguing problem left open by our work has been resolved by H-astad [H1, H2]. <p> such as Min-Set-Cover (cf., [LuYa, BGLR, Fe2]), Max-Clique (cf., <ref> [H1, H2] </ref>), Min-Coloring ([FeKi2]), and Max-3SAT (cf., [H3]). The latter results were obtained subsequently to the current work, and while building on it. Amortized free-bits and Max-Clique. The most intriguing problem left open by our work has been resolved by H-astad [H1, H2]. He proved our conjecture (cf., [BGS2]) by which, for every * &gt; 0, it is the case that NP FPCP [log; *]. The Long-Code, introduced in this work, plays a pivotal role in H-astad's work. He also uses the idea of folding. <p> By modifying our proof systems so as to preserve the amortized free-bit complexity and achieve low covering complexity, they proved a that approximating ChromNum within N 1=3 is hard unless NP = coRP. They were able to similarly modify H-astad's proof systems <ref> [H1, H2] </ref> and thereby improve the hard factor to N 1* , for any * &gt; 0. Gadgets. Another research direction, suggested in early versions of this work [BGS2], was taken on by Trevisan et. al. [TSSW] who initiated a systematic study of the construction of gadgets. <p> In retrospect, the paradigm shifts have amounted to the relaxation of the codeword test in [H1] and to the relaxation of the projection test in <ref> [H2] </ref>. Thus, our lower bounds may be considered as a justification for these (somewhat unnatural) relaxations. In particular, the lower bound on the complexity of the codeword test relies on the particular interpretation of `closeness' used above (i.e., being at distance less than half the distance of the code). <p> H-astad's relaxation of the codeword test is different, yet it also suffices for the purpose of constructing PCP systems of amortized free-bit complexity 1 (for NP) [H1]. The lower bound on the complexity of the projection test seems more robust. Yet, as shown by H-astad in <ref> [H2] </ref>, the projection requirements can be by-passed as well, yielding pcp systems of amortized free-bit complexity tending to 0. 9.1 The tasks Our definitions of the various tasks/tests are quite minimal and do not necessarily suffice for PCP applications. <p> Since the latter task is in N L, we are done. (Actually, 2SAT is complete for coN L; see [JLL].) 12 The conjecture was stated for systems with perfect completeness, and has been subsequently proven by H-astad <ref> [H2] </ref> (who proved that NP = FPCP 1 [ log; * ], for every * &gt; 0). <p> The discussion is anyhow moot otherwise. 17 In retrospect, there is no reason to remove this assumption as it has been proven to hold in <ref> [H2] </ref>. However, this was not known at the time the current work was done. 18 Here we use the observation that the FGLSS-reduction works also for amortized average free-bit complexity. 106 11 Transformations of FPCP Systems We present several useful transformations which can be applied to pcp systems.
Reference: [H3] <author> J. H -astad. </author> <title> Getting optimal in-approximability results. </title> <booktitle> Proceedings of the 29th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1997, </year> <pages> pp. 1-10. </pages>
Reference-contexts: Factor HARD to Approx. Factor Tight? Factor Due to Our Factor NEW Factor Assumption Max3SAT 1 + 1 7 + * [KaZw] (new) 1 + 1 26 1 + 1 MaxE3SAT 1 + 1 7 folklore 1 + 1 26 1 + 1 Max2SAT 1:075 [GoWi2, FeGo] 1:013 1:047 <ref> [H3] </ref> P 6= NP No MaxSAT 2 folklore 1 + 1 7 * 2 * [H3] P 6= NP Yes MaxCUT 1:139 [GoWi2] 1:014 1:062 [H3] P 6= NP No MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 1 + 1 Max-Clique N 1o (1) [BoHa] N 1 3 <p> Max3SAT 1 + 1 7 + * [KaZw] (new) 1 + 1 26 1 + 1 MaxE3SAT 1 + 1 7 folklore 1 + 1 26 1 + 1 Max2SAT 1:075 [GoWi2, FeGo] 1:013 1:047 <ref> [H3] </ref> P 6= NP No MaxSAT 2 folklore 1 + 1 7 * 2 * [H3] P 6= NP Yes MaxCUT 1:139 [GoWi2] 1:014 1:062 [H3] P 6= NP No MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 1 + 1 Max-Clique N 1o (1) [BoHa] N 1 3 * N 1* [H2] coRP 6= NP Yes N 4 * N 2 * [H2] <p> + 1 26 1 + 1 MaxE3SAT 1 + 1 7 folklore 1 + 1 26 1 + 1 Max2SAT 1:075 [GoWi2, FeGo] 1:013 1:047 <ref> [H3] </ref> P 6= NP No MaxSAT 2 folklore 1 + 1 7 * 2 * [H3] P 6= NP Yes MaxCUT 1:139 [GoWi2] 1:014 1:062 [H3] P 6= NP No MinVC 2 o (1) [BaEv2, MoSp] 1 + 1 15 1 + 1 Max-Clique N 1o (1) [BoHa] N 1 3 * N 1* [H2] coRP 6= NP Yes N 4 * N 2 * [H2] P 6= NP No Chromatic No. <p> By now, tight results are known for central problems such as Min-Set-Cover (cf., [LuYa, BGLR, Fe2]), Max-Clique (cf., [H1, H2]), Min-Coloring ([FeKi2]), and Max-3SAT (cf., <ref> [H3] </ref>). The latter results were obtained subsequently to the current work, and while building on it. Amortized free-bits and Max-Clique. The most intriguing problem left open by our work has been resolved by H-astad [H1, H2]. <p> Improved 3-query proofs and Max-SNP. Another challenge, one we even did not dare state, was achieved as well: H-astad <ref> [H3] </ref> has recently obtained optimal non-approximability results to MaxSNP problems such as Max-E3-SAT. Furthermore, he has improved over all our non-approximability results for MaxSNP problems, obtaining non-approximability factors of 22=21 and 17=16 for Max-2-SAT and Max-CUT, respectively. <p> Furthermore, he has improved over all our non-approximability results for MaxSNP problems, obtaining non-approximability factors of 22=21 and 17=16 for Max-2-SAT and Max-CUT, respectively. Underlying these results is a new proof system for NP which yields NP PCP 1*;0:5 [log; 3], for any * &gt; 0. In addition, H-astad <ref> [H3] </ref> shows that NP is contained in PCP 1;0:75+* [log; 3] (and it follows that NP PCP 1;0:5 [log; 9]). The Long-Code plays a pivotal role in all these proof systems. Improved 2-free-bits proofs and Min-VC. The above-mentioned proof system of H-astad [H3] uses two (non-amortized) free-bits, and so NP FPCP <p> In addition, H-astad <ref> [H3] </ref> shows that NP is contained in PCP 1;0:75+* [log; 3] (and it follows that NP PCP 1;0:5 [log; 9]). The Long-Code plays a pivotal role in all these proof systems. Improved 2-free-bits proofs and Min-VC. The above-mentioned proof system of H-astad [H3] uses two (non-amortized) free-bits, and so NP FPCP 1*;0:5 [log; 2], for every * &gt; 0. This sets the non-approximability bound for Min Vertex-Cover at 7 6 *. Chromatic Number. <p> This would imply a hardness factor of 3 2 *. (3) NP FPCP 1*;* [log; 1] for every * &gt; 0. This would imply a hardness factor of 2 *. Recall that FPCP 1;s [log; 1] P, for every s &lt; 1, whereas NP FPCP 1*;0:5 [log; 2] <ref> [H3] </ref>. It will be interesting (though of no application for MinVC) to know whether NP FPCP 1;0:5+* [log; 2]. 16 Bellare, Goldreich, Sudan Perfect versus imperfect completeness. H-astad's work [H3] is indeed the trigger for the last question and similarly we wonder whether NP PCP 1;0:5+* [log; 3]. <p> Recall that FPCP 1;s [log; 1] P, for every s &lt; 1, whereas NP FPCP 1*;0:5 [log; 2] <ref> [H3] </ref>. It will be interesting (though of no application for MinVC) to know whether NP FPCP 1;0:5+* [log; 2]. 16 Bellare, Goldreich, Sudan Perfect versus imperfect completeness. H-astad's work [H3] is indeed the trigger for the last question and similarly we wonder whether NP PCP 1;0:5+* [log; 3]. Non-perfect completeness seems to be useful in [H3], but it is to be seen if this is inherent. <p> H-astad's work <ref> [H3] </ref> is indeed the trigger for the last question and similarly we wonder whether NP PCP 1;0:5+* [log; 3]. Non-perfect completeness seems to be useful in [H3], but it is to be seen if this is inherent. Similar issues arise with respect to some results in the current work (e.g., see our transformations for increasing acceptance probability of proof systems). De-randomization. <p> SNPinner , the verifier V 2inner works with functions/oracles A that are folded twice | once across (h; 0) and once across ( 1; 1). 8 Furthermore, there seems to be little hope that the former approach can ever yield an improvement over the better bounds subsequently obtained by H-astad <ref> [H3] </ref>. 63 The Enhanced RMB Test. Again, A: F l ! is the object being tested, and the test take additional inputs or parameters f 1 ; f 2 2 F l . <p> Using Theorem 4.6 (Part 3), NP K D Gap-2SAT c;s for some c &gt; 0:9 and s &lt; 73 74 c, and NP PCP c;s [log; 2] follows. Remark 10.4 The ratio c=s has been subsequently increased to (10=9) *, for any * &gt; 0 (cf., <ref> [TSSW, H3] </ref>). Proof of Proposition 10.3, Part (5): The result for general verifiers follows from Lemma 4.11 and the fact that MaxSAT can be approximated to within a 0:795 = 0:75 + 0:18 4 factor in polynomial-time 100 (cf., [TSSW]).
Reference: [Hoc] <author> D. Hochbaum. </author> <title> Efficient algorithms for the stable set, vertex cover and set packing problems. </title> <journal> Discrete Applied Mathematics, </journal> <volume> Vol 6, </volume> <year> 1983, </year> <pages> pp. 243-254. </pages>
Reference-contexts: There is a simple polynomial time algorithm to approximate MinVC in unweighted graphs within a factor of 2. The algorithm, due to F. Gavril (cf. [GJ2]), consists of taking all vertices which appear in a maximal matching of the graph. For weighted graphs, Bar-Yehuda and Even [BaEv1] and Hochbaum <ref> [Hoc] </ref>, gave algorithms achieving the same approximation factor. The best known algorithm today achieves a factor only slightly better, namely 2 (log log jV j)=(2 log jV j) [BaEv2, MoSp].
Reference: [ImZu] <author> R. Impagliazzo and D. Zuckerman. </author> <title> How to recycle random bits. </title> <booktitle> Proceedings of the 30th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1989, </year> <pages> pp. 248-253. </pages>
Reference-contexts: On the other hand, it is easy to see that that random bits can be recycled for error-reduction via the standard techniques of <ref> [AKS, CW, ImZu] </ref>. The consequence was the first NP-hardness result for Max Clique approximation. <p> The de-randomized error reduction method consists of applying general, de-randomized, error-reduction techniques to the proof system setting. 3 The best method knows as the "Expander Walk" technique is due to Ajtai, Komlos and Szemeredi [AKS] (see also <ref> [CW, ImZu] </ref>).
Reference: [JLL] <author> N. Jones, Y. Lien and W. Laaser. </author> <title> New problems complete for non-deterministic log space. </title> <journal> Math. Systems Theory, </journal> <volume> Vol. 10, </volume> <year> 1976, </year> <pages> pp. 1-17. 118 </pages>
Reference-contexts: The latter checking reduces to guessing the variable for which a conflicting assignment is implied and verifying the conflict via s-t directed connectivity. Since the latter task is in N L, we are done. (Actually, 2SAT is complete for coN L; see <ref> [JLL] </ref>.) 12 The conjecture was stated for systems with perfect completeness, and has been subsequently proven by H-astad [H2] (who proved that NP = FPCP 1 [ log; * ], for every * &gt; 0).
Reference: [Kah] <author> N. Kahale. </author> <title> Eigenvalues and expansion of regular graphs. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 42, No. 5, </volume> <year> 1995, </year> <pages> pp. 1091-1106. </pages>
Reference-contexts: It is well-known that a random walk of length t in an expander avoids a set of density with probability at most ( + d ) t (cf., <ref> [AKS, Kah] </ref>). Thus, as a preparation step, we reduce the error probability of the pcp system to p = d 2 d This is done using the trivial reduction of Proposition 11.1.
Reference: [KKLP] <author> V. Kann, S. Khanna, J. Lagergren and A. Panconesi. </author> <title> On the hardness of approximating MAX k-CUT and its dual. </title> <institution> Technical Report of the Department of Numerical Analysis and Computing Science, Royal Institute of Technology, Stockholm, TRITA-NA-P9505, </institution> <year> 1995. </year>
Reference: [KMS] <author> D. Karger, R. Motwani and M. Sudan. </author> <title> Approximate graph coloring by semidefinite programming. </title> <booktitle> Proceedings of the 35th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1994, </year> <pages> pp. 2-13. </pages>
Reference-contexts: He showed that a polynomial-time N 1* -factor approximation algorithm for Max Clique implies a polynomial time algorithm to color a three colorable graph with O (log N ) colors [Bl], which is much better than currently known <ref> [KMS] </ref>. But perhaps N 1o (1) is the best possible. Resolving the approximation complexity of this basic problem seems, in any case, to be worth some effort. Gaps in clique size.
Reference: [KaZw] <author> H. Karloff and U. </author> <title> Zwick. </title> <booktitle> A 7/8-eps approximation algorithm for MAX 3SAT? To appear in Proceedings of the 38th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1997. </year>
Reference-contexts: Bellare and Sudan then suggested the notion of amortized free-bits. 14 Bellare, Goldreich, Sudan Problem EASY to Approx. Factor HARD to Approx. Factor Tight? Factor Due to Our Factor NEW Factor Assumption Max3SAT 1 + 1 7 + * <ref> [KaZw] </ref> (new) 1 + 1 26 1 + 1 MaxE3SAT 1 + 1 7 folklore 1 + 1 26 1 + 1 Max2SAT 1:075 [GoWi2, FeGo] 1:013 1:047 [H3] P 6= NP No MaxSAT 2 folklore 1 + 1 7 * 2 * [H3] P 6= NP Yes MaxCUT 1:139 [GoWi2]
Reference: [Kar] <author> R. Karp. </author> <title> Reducibility among combinatorial problems. Complexity of Computer Computations, </title> <editor> Miller and Thatcher (eds.), </editor> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: We've followed the common tradition regarding the names of polynomial-time reductions: many-to-one reductions are called Karp-reductions whereas (polynomial-time) Turing reductions are called Cook-reductions. This terminology is somewhat unfair towards Levin whose work on NP-completeness [Lev] was independent of those of Cook [Coo] and Karp <ref> [Kar] </ref>. Actually, the reductions considered by Levin are more restricted as they also efficiently transform the corresponding NP-witnesses (this is an artifact of Levin's desire to treat search problems rather than decision problems).
Reference: [KLS] <author> S. Khanna, N. Linial and S. Safra. </author> <title> On the hardness of approximating the chromatic number. </title> <booktitle> Proceedings of the Second Israel Symposium on Theory and Computing Systems, IEEE, </booktitle> <year> 1993, </year> <pages> pp. 250-260. </pages>
Reference-contexts: The conclusion for Max Clique follows, of course, from the FGLSS-reduction and the first proof system listed above. The conclusion for the Chromatic Number follows from a recent reduction of Furer [Fu], which in turn builds on reductions in <ref> [LuYa, KLS, BeSu] </ref>. (Furer's work and ours are contemporaneous and thus we view the N 1=5 hardness result as jointly due to both papers.) The improvements for the MaxSNP problems are perhaps more significant than the Max Clique one: We see hardness results for MaxSNP problems that are comparable to the <p> But, again, ffi is very small. Improvements to ffi were derived both by improvements to * and improvements to the function h used by the reduction. A subsequent reduction of Khanna, Linial and Safra <ref> [KLS] </ref> is simpler but in fact slightly less efficient, having h (*) = *=(5 + *). A more efficient reduction is given by [BeSu] they present a reduction obtaining h (*) = *=(3 2*). <p> However now we know that FPCP and Max Clique are equivalent, so we can go back and rephrase the old statements. Thus results of <ref> [LuYa, KLS, BeSu, Fu] </ref> can be summarized as: For every ff; *; fl &gt; 0, Gap-MaxClique N ff1 ;N *1 K R Gap-ChromNum N (*+fl) ;N h (ff) , where (1) h (ff) = minf 1 6 ; ff 54ff g [LuYa]. (2) h (ff) = minf 1 11 ; ff
Reference: [LaSh] <author> D. Lapidot and A. Shamir. </author> <title> Fully parallelized multi-prover protocols for NEXP-time. </title> <booktitle> Proceedings of the 32nd Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1991, </year> <pages> pp. 13-18. </pages>
Reference-contexts: The available constant-prover proof systems appear in Figure 5 and are discussed below. Throughout this discussion we consider proof systems obtaining an arbitrary small constant error probability. The two-prover proofs of Lapidot-Shamir and Feige-Lovasz <ref> [LaSh, FeLo] </ref> had poly-logarithmic randomness and answer sizes, so [ALMSS] used a modification of these, in the process increasing the number of provers to a constant much larger than two. The later constructions of few-prover proofs of [BGLR, Tar, FeKi1] lead to better non-approximability results. <p> These features are captured in their definition of canonical verifiers (cf. Section 3.4). But the proof systems of [FeKi1] that had worked above no longer sufficed| they are not canonical. So instead [BeSu] used (a slight modification of) the proofs of <ref> [LaSh, FeLo] </ref>, thereby incurring poly-logarithmic randomness and answer sizes, so that the assumptions in their non-approximability results pertain to quasi-polynomial time classes. (Alternatively they modify the [FeKi1] system to a canonical three-prover one, but then incur a decrease in the non-approximability factors due to having more provers). <p> A breakthrough result in this area is Raz's Parallel Repetition Theorem which implies the ex Free Bits in PCP 21 Due to Provers Coins Answer size Canonical? Can be made canonical? <ref> [LaSh, FeLo] </ref> 2 polylog polylog No Yes [BeSu] [ALMSS] poly (* 1 ) log polylog No ? [BGLR] 4 log polyloglog No ? [Tar] 3 log O (1) No ? [FeKi1] 2 log O (1) No At cost of one more prover [BeSu] [Raz] 2 log O (1) Yes (NA) istence <p> Similarly for the normalized version. 26 Bellare, Goldreich, Sudan Due to Assuming Factor Technique [ALMSS] P 6= NP some constant NP PCP 1;1=2 [ log; O (1) ]; Reduction of this to Max3SAT. [BGLR] e P 6= N e P 94=93 Framework; better analyses; uses proof systems of <ref> [LaSh, FeLo] </ref>. [BGLR] P 6= NP 113=112 New four-prover proof systems. [FeKi1] P 6= NP 94=93 New two-prover proof systems. [BeSu] e P 6= N e P 66=65 Canonicity and some optimizations. [BeSu] P 6= NP 73=72 Canonicity and some optimizations. <p> But we don't take advantage of this fact.) In addition we need answer sizes of log log n as opposed to the O (log n) of previous methods, for reasons explained below. This means that even the (modified) <ref> [LaSh, FeLo] </ref> type proofs won't suffice for us. We could use the three-prover modification of [FeKi1] but the cost would wipe out our gain.
Reference: [Lau] <author> C. Lautemann. </author> <title> BPP and the polynomial hierarchy. </title> <journal> Information Processing Letters, </journal> <volume> Vol. 17, No. 4, </volume> <year> 1983, </year> <pages> pp. 215-217. </pages>
Reference-contexts: Instead, we use two alternative implementations, which yield the two parts of the proposition. In both implementations the free-bit complexity increases by log 2 m and the soundness bound increases by a factor of m. The first implementation employs a technique introduced by Lautemann (in the context of BPP) <ref> [Lau] </ref>. Using a randomized reduction, we supply the new verifier with a sequence of m possible "shifts" that it may effect. The new verifier selects one random-pad for the original verifier and generates m shifts of this pad.
Reference: [Lev] <author> L. Levin. </author> <title> Universal'nye perebornye zadachi (universal search problems : in russian). </title> <journal> Problemy Peredachi Informatsii, </journal> <volume> Vol. 9, No. 3, </volume> <year> 1973, </year> <pages> pp. 265-266. </pages>
Reference-contexts: We've followed the common tradition regarding the names of polynomial-time reductions: many-to-one reductions are called Karp-reductions whereas (polynomial-time) Turing reductions are called Cook-reductions. This terminology is somewhat unfair towards Levin whose work on NP-completeness <ref> [Lev] </ref> was independent of those of Cook [Coo] and Karp [Kar]. Actually, the reductions considered by Levin are more restricted as they also efficiently transform the corresponding NP-witnesses (this is an artifact of Levin's desire to treat search problems rather than decision problems).
Reference: [LPS] <author> A. Lubotzky, R. Phillips and P. Sarnak. </author> <title> Explicit Expanders and the Ramanujan Conjectures. </title> <booktitle> Proceedings of the 18th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1986, </year> <pages> pp. 240-246. </pages>
Reference-contexts: This transformation is analogous to the well-known transformation of Berman and Schnitger [BeSc]. Alternatively, using a known deterministic amplification method based on <ref> [AKS, LPS] </ref> one can transform FPCP 1; 1 2 [log; f ] into FPCP 1;2 k [log +2k; k f ]. (To the best of our knowledge this transformation has never appeared with a full proof.) Both alternatives are important ingredients in transforming pcp results into clique in-approximability results via the <p> Thus the Ramanujan Expander of Lubotzky, Phillips and Sarnak <ref> [LPS] </ref> play an important role yielding 2 (cf. <p> Specifically, using a degree d expander graph with second eigenvalue 109 yields a factor of log 2 d 1+log 2 . Thus, it is essential to use Ramanujan graphs <ref> [LPS] </ref> in order to obtain the claimed constant of 2 + *. Proof of Proposition 11.4: For simplicity assume s = 1=2. The idea is to use a "pseudorandom" sequence generated by a random walk on an expander graph in order to get error reduction at moderate randomness cost. <p> The idea is to use a "pseudorandom" sequence generated by a random walk on an expander graph in order to get error reduction at moderate randomness cost. Specifically, we will use a Ramanujan expander graph of constant degree d and second eigenvalue 2 p d (cf., <ref> [LPS] </ref>). The constant d will be determined so that d &gt; 2 4+ 8 * (and d &lt; 2 6+ 8 * ).
Reference: [LuYa] <author> C. Lund and M. Yannakakis. </author> <title> On the hardness of approximating minimization problems. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 41, No.5, </volume> <year> 1994, </year> <pages> pp. 960-981. </pages>
Reference-contexts: The conclusion for Max Clique follows, of course, from the FGLSS-reduction and the first proof system listed above. The conclusion for the Chromatic Number follows from a recent reduction of Furer [Fu], which in turn builds on reductions in <ref> [LuYa, KLS, BeSu] </ref>. (Furer's work and ours are contemporaneous and thus we view the N 1=5 hardness result as jointly due to both papers.) The improvements for the MaxSNP problems are perhaps more significant than the Max Clique one: We see hardness results for MaxSNP problems that are comparable to the <p> After the work of [FGLSS] the field took off in two major directions. One was to extend the interactive proof approach to prove the non-approximability of other optimization problems. Direct reductions from proofs were used to show the hardness of quadratic programming [BeRo, FeLo], Max3SAT [ALMSS], set cover <ref> [LuYa] </ref>, and other problems [Be]. The earlier work of Papadimitriou and Yannakakis introducing the class MaxSNP [PaYa] now came into play; by reduction from Max3SAT it implied hardness of approximation for any MaxSNP-hard problem. Also, reductions from Max Clique lead to hardness results for the chromatic number [LuYa] and other problems <p> [ALMSS], set cover <ref> [LuYa] </ref>, and other problems [Be]. The earlier work of Papadimitriou and Yannakakis introducing the class MaxSNP [PaYa] now came into play; by reduction from Max3SAT it implied hardness of approximation for any MaxSNP-hard problem. Also, reductions from Max Clique lead to hardness results for the chromatic number [LuYa] and other problems [Zuc]. The other direction was to increase factors, and reduce assumptions, for existing hardness of approximation results. This involves improving the efficiency of the underlying proof systems and/or the efficiency of the reductions. <p> By now, tight results are known for central problems such as Min-Set-Cover (cf., <ref> [LuYa, BGLR, Fe2] </ref>), Max-Clique (cf., [H1, H2]), Min-Coloring ([FeKi2]), and Max-3SAT (cf., [H3]). The latter results were obtained subsequently to the current work, and while building on it. Amortized free-bits and Max-Clique. The most intriguing problem left open by our work has been resolved by H-astad [H1, H2]. <p> Constant-prover proofs have been instrumental in the derivation of non-approximability results in several ways. One of these is that they are a good starting point for reductions| examples of such are reductions of two-prover proofs to quadratic programming [BeRo, FeLo] and set cover <ref> [LuYa] </ref>. However, it is a different aspect of constant prover proofs that is of more direct concern to us. This aspect is the use of constant-prover proof systems as the penultimate step of the recursion, and begins with [ALMSS]. <p> In all reductions ffi (*) = minfh (*); h (0:5)g, for some function h. The bigger h, the better the reduction. The first reduction, namely that of Lund and Yannakakis <ref> [LuYa] </ref>, obtained h (*) = *=(5 4*). Via the Max Clique hardness results of [ArSa, ALMSS] this implies the chromatic number is hard to 2 Actually all the reductions presented here, make assumptions regarding the structure of the graph and hence do not directly yield the hardness results stated here. <p> We stress that the ratio c (N) s (N) does not remain invariant. 86 Rephrasing reductions from Max Clique to Chromatic Number. Starting with the work of Lund and Yannakakis <ref> [LuYa] </ref>, there have been several works on showing the hardness of approximating the Chromatic number, which reduce the Max Clique problem to the Chromatic number problem: see Section 2.4.3 for a description. <p> However now we know that FPCP and Max Clique are equivalent, so we can go back and rephrase the old statements. Thus results of <ref> [LuYa, KLS, BeSu, Fu] </ref> can be summarized as: For every ff; *; fl &gt; 0, Gap-MaxClique N ff1 ;N *1 K R Gap-ChromNum N (*+fl) ;N h (ff) , where (1) h (ff) = minf 1 6 ; ff 54ff g [LuYa]. (2) h (ff) = minf 1 11 ; ff <p> Thus results of [LuYa, KLS, BeSu, Fu] can be summarized as: For every ff; *; fl &gt; 0, Gap-MaxClique N ff1 ;N *1 K R Gap-ChromNum N (*+fl) ;N h (ff) , where (1) h (ff) = minf 1 6 ; ff 54ff g <ref> [LuYa] </ref>. (2) h (ff) = minf 1 11 ; ff (3) h (ff) = minf 1 4 ; ff 32ff g [BeSu]. (4) h (ff) = minf 1 3 ; ff We note that it is an open problem whether one can get a reduction in which h (ff) ! 1
Reference: [LFKN] <author> C. Lund, L. Fortnow, H. Karloff, and N. Nisan. </author> <title> Algebraic Methods for Interactive Proof Systems. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 39, No. 4, </volume> <year> 1992, </year> <pages> pp 859-868. </pages>
Reference-contexts: However, the real breakthrough came with the result of Lund, Fortnow, Karloff and Free Bits in PCP 13 Nisan <ref> [LFKN] </ref> who used algebraic methods to show that all coNP languages (and actually, all languages in P #P ) have interactive proof systems. These techniques were used by Shamir [Sha] to show that IP = PSPACE.
Reference: [McSl] <author> F. MacWilliams and N. Sloane. </author> <title> The theory of error-correcting codes. </title> <publisher> North-Holland, </publisher> <year> 1981. </year>
Reference-contexts: To do so we need the following Lemma. It is the counterpart of a claim in [BGLR, Lemma 3.5] and will be used in the same way. The lemma is derived from a coding theory bound which is slight extension of bounds in <ref> [McSl, Ch. 7] </ref> (see Appendix). Lemma 3.11 Suppose 0 ffi 1=2 and A: F l ! . Then there are at most 1=(4ffi 2 ) codewords that have distance less than 1=2 ffi from A.
Reference: [MoRa] <author> R. Motwani and P. Raghavan. </author> <title> Randomized algorithms. </title> <publisher> Cambridge University Press, </publisher> <year> 1995. </year>
Reference-contexts: Let t def = C=L. Then, E [ P v2S v ] = t. Using a multiplicative Chernoff bound <ref> [MoRa] </ref>, we get Pr [ 8v 2 S : l (v) 6= i ] = Pr X v = 0 &lt; 2 t 2 Call the i th layer bad if no vertex of S is placed in it. <p> Clearly, the i 's are mutually independent and each equals 1 with probability ffi s. Using a multiplicative Chernoff Bound (cf. <ref> [MoRa, Theorem 4.3] </ref>), the probability that a random R is bad (for x w.r.t. ) is bounded by " m X i (1 + *) ms &lt; 2 (* 2 ms) Thus, by the choice of m, the probability that a random R is bad for x, with respect to any
Reference: [MoSp] <author> B. Monien and E. Speckenmeyer. </author> <title> Ramsey numbers and an approximation algorithm for the vertex cover problem. </title> <journal> Acta Informatica, </journal> <volume> Vol. 22, No. 1, </volume> <year> 1985, </year> <pages> pp. 115-123. </pages>
Reference-contexts: + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max2SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP MaxSAT 2 folklore 1 + 1 7 P 6= NP MaxCUT 1:139 [GoWi2] 1:014 unspecified [ALMSS] P 6= NP MinVC 2 o (1) <ref> [BaEv2, MoSp] </ref> 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 <p> We are obtaining the first explicit and reasonable non-approximability factor for Max2SAT, MaxCUT and minimum Vertex Cover. Recall that the latter is approximable within 2-o (1) <ref> [BaEv2, MoSp] </ref>. Our results for MaxCUT and Max2SAT show that it is infeasible to find a solution with value which is only a factor of 1.01 from optimal. <p> 1 7 folklore 1 + 1 26 1 + 1 Max2SAT 1:075 [GoWi2, FeGo] 1:013 1:047 [H3] P 6= NP No MaxSAT 2 folklore 1 + 1 7 * 2 * [H3] P 6= NP Yes MaxCUT 1:139 [GoWi2] 1:014 1:062 [H3] P 6= NP No MinVC 2 o (1) <ref> [BaEv2, MoSp] </ref> 1 + 1 15 1 + 1 Max-Clique N 1o (1) [BoHa] N 1 3 * N 1* [H2] coRP 6= NP Yes N 4 * N 2 * [H2] P 6= NP No Chromatic No. <p> For weighted graphs, Bar-Yehuda and Even [BaEv1] and Hochbaum [Hoc], gave algorithms achieving the same approximation factor. The best known algorithm today achieves a factor only slightly better, namely 2 (log log jV j)=(2 log jV j) <ref> [BaEv2, MoSp] </ref>.
Reference: [PaYa] <author> C. Papadimitriou and M. Yannakakis. </author> <title> Optimization, approximation, and complexity classes. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> Vol. 43, </volume> <year> 1991, </year> <pages> pp. 425-440. 119 </pages>
Reference-contexts: Direct reductions from proofs were used to show the hardness of quadratic programming [BeRo, FeLo], Max3SAT [ALMSS], set cover [LuYa], and other problems [Be]. The earlier work of Papadimitriou and Yannakakis introducing the class MaxSNP <ref> [PaYa] </ref> now came into play; by reduction from Max3SAT it implied hardness of approximation for any MaxSNP-hard problem. Also, reductions from Max Clique lead to hardness results for the chromatic number [LuYa] and other problems [Zuc]. <p> This immediately implied the NP-hardness of approximating Max Clique within N * , for some * &gt; 0. Furthermore, it also implied that Max-3-Sat is NP-hard to approximate to within some constant factor [ALMSS] and so is any MaxSNP-hard problem <ref> [PaYa] </ref>. The second stage of this enterprise started with the work of Bellare, Goldwasser, Lund and Russell [BGLR]. <p> This paper P 6= NP 27=26 Long code and new proof systems. 2.4.3 History of approximability results for these problems Satisfiability problems. Max3SAT is the canonical Max-SNP complete problem <ref> [PaYa] </ref>. A polynomial-time algorithm due to Yannakakis [Yan] approximates it to within a factor of 4=3 &lt; 1:334 (see Goemans and Williamson [GoWi1] for an alternate algorithm). <p> For MaxE3SAT, which is also Max-SNP complete, a very simple algorithm achieves an approximation of 8=7 1:143 (where 7=8 is the expected fraction of clauses satisfied by a uniformly chosen assignment). Max2SAT is also Max-SNP complete <ref> [GJS, PaYa] </ref>. This problem is particularly interesting because it has been the focus of recent improvements in the approximation factor attainable in polynomial-time. <p> Max Cut. In 1976, Sahni and Gonzales [SaGo] gave a simple 2-approximation algorithm for this problem. Recently, in a breakthrough result, Goemans and Williamson [GoWi2] gave a new algorithm which achieves a ratio of 1 0:878 = 1:139 for this problem. On the other hand, <ref> [PaYa] </ref> give an approximation preserving reduction from Max3SAT to MaxCUT. Combined with [ALMSS] this shows that there exists a constant ff &gt; 1 such that approximating MaxCUT within a factor of ff is NP-hard. <p> The version of MinVC in which one restricts attention to graphs of degree bounded by a constant B, is Max-SNP complete for suitably large B <ref> [PaYa] </ref>. In particular they provide a reduction from Max3SAT. Combined with [ALMSS] this implies the existence of a constant ffi &gt; 0 such that approximating MinVC within a factor of 1 + ffi is hard unless P = NP. No explicit value of ffi has been stated until now. <p> No explicit value of ffi has been stated until now. Indeed, the value that could be derived, even using the best existing non-approximability results for Max3SAT, will be very small, because of the cost of the reduction of <ref> [PaYa] </ref>, which first reduces Max3SAT to its bounded version using expanders, and then reduces this to MinVC-B. See Section 5.2 for our results. Max Clique. <p> As for c 0 , it equals 3ff 1 +2ff 2 3m 1 +2m 2 &gt; 0:6. 5 Free bits and vertex cover It is known that approximating the minimum vertex cover of a graph to within a 1 + * factor is hard, for some * &gt; 0 <ref> [PaYa, ALMSS] </ref>. However, we do not know of any previous attempt to provide a lower bound for *.
Reference: [Pet] <author> E. Petrank. </author> <title> The Hardness of Approximations: Gap Location. </title> <institution> TR-754, Department of Computer Science, Technion - Israel Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: In fact, even using our new Max3SAT result we would only get only a hardness factor of 185=184. See Section 4.2 for our results. Linear equations. The MaxLinEq problem is known to be Max-SNP complete (see [BrNa] or <ref> [Pet] </ref>). We remark that the problem of maximizing the number of satisfiable equations should not be confused with the "complementary" problem of minimizing the number of violated constraints, investigated by Arora et. al. [ABSS].
Reference: [PoSp] <author> A. Polishchuk and D. Spielman. </author> <title> Nearly-linear size holographic proofs. </title> <booktitle> Proceedings of the 26th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1994, </year> <pages> pp. 194-203. </pages>
Reference-contexts: Free bits are implicit in [FeKi1] and formalized in [BeSu]. Amortized free bits are introduced in [BeSu] but formalized a little better here. Proof sizes were considered in <ref> [BFLS, PoSp] </ref>. We consider them here for a different reason they play an important role in that the randomized FGLSS reduction [BeSc, Zuc] depends actually on this parameter (rather than on the randomness complexity).
Reference: [Raz] <author> R. Raz. </author> <title> A parallel repetition theorem. </title> <booktitle> Proceedings of the 27th Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1995, </year> <pages> pp. 447-456. </pages>
Reference-contexts: As in all recent constructions of efficient pcp's our construction also relies on the use of recursive construction of verifiers, introduced by Arora and Safra [ArSa]. We have the advantage of being able to use, at the outer level, the recent verifier of Raz <ref> [Raz] </ref>, which was not available to previous authors. The inner level verifier relies on the use of a "good" encoding scheme. <p> Answer size Canonical? Can be made canonical? [LaSh, FeLo] 2 polylog polylog No Yes [BeSu] [ALMSS] poly (* 1 ) log polylog No ? [BGLR] 4 log polyloglog No ? [Tar] 3 log O (1) No ? [FeKi1] 2 log O (1) No At cost of one more prover [BeSu] <ref> [Raz] </ref> 2 log O (1) Yes (NA) istence of a two-provers proof system with logarithmic randomness and constant answer size [Raz]. Furthermore, this proof system is canonical. 2.3 Reductions between problems and classes We will consider reductions between promise problems. <p> log polylog No ? [BGLR] 4 log polyloglog No ? [Tar] 3 log O (1) No ? [FeKi1] 2 log O (1) No At cost of one more prover [BeSu] <ref> [Raz] </ref> 2 log O (1) Yes (NA) istence of a two-provers proof system with logarithmic randomness and constant answer size [Raz]. Furthermore, this proof system is canonical. 2.3 Reductions between problems and classes We will consider reductions between promise problems. <p> and non-approximability results 3 The Long Code and its machinery 3.1 New PCPs and Hardness Results Overview and guidemap The starting point for all our proof systems is a two-prover proof system achieving arbitrarily small but fixed constant error with logarithmic randomness and constant answer size, as provided by Raz <ref> [Raz] </ref>. This proof system has the property that the answer of the second prover is supposed to be a predetermined function of the answer of the first prover. <p> We then define 36 a canonical inner verifier. Recursion is captured by an appropriate definition of a composed verifier whose attributes we relate to those of the original verifiers in Theorem 3.12. The specific outer verifier we will use is one obtained by a recent work of Raz <ref> [Raz] </ref>. We will construct various inner verifiers based on the long code and the tests in Section 3.5 and Section 7.1. Theorem 3.12 will be used ubiquitously to combine the two. Comparison with previous work. <p> This means that even the (modified) [LaSh, FeLo] type proofs won't suffice for us. We could use the three-prover modification of [FeKi1] but the cost would wipe out our gain. Luckily this discussion is moot since we can use the recent result of Raz <ref> [Raz] </ref> to provide us with a canonical two-prover proof having logarithmic randomness, constant answer size, and any constant error. This makes an ideal starting point. To simplify the definitions below we insisted on constant answer size and two provers from the start. <p> Then for every * &gt; 0 there exist positive integers l; l 1 and c such that there exists an (l; l 1 )-canonical outer verifier which is *-good for L and uses randomness r (n) = c log 2 n. Actually, Raz's Theorem <ref> [Raz] </ref> enables one to assert that l; l 1 and c are O (log * 1 ); but we will not need this fact.
Reference: [SaGo] <author> S. Sahni and T. Gonzales. </author> <title> P-complete approximation problems. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 23, </volume> <year> 1976, </year> <pages> pp. 555-565. </pages>
Reference-contexts: See Section 4.2.2 for our results. Max Cut. In 1976, Sahni and Gonzales <ref> [SaGo] </ref> gave a simple 2-approximation algorithm for this problem. Recently, in a breakthrough result, Goemans and Williamson [GoWi2] gave a new algorithm which achieves a ratio of 1 0:878 = 1:139 for this problem. On the other hand, [PaYa] give an approximation preserving reduction from Max3SAT to MaxCUT.
Reference: [Sha] <author> A. Shamir. </author> <title> IP=PSPACE. </title> <journal> Journal of the ACM, </journal> <volume> Vol. 39, No. 4, </volume> <year> 1992, </year> <pages> pp. 869-877. </pages>
Reference-contexts: However, the real breakthrough came with the result of Lund, Fortnow, Karloff and Free Bits in PCP 13 Nisan [LFKN] who used algebraic methods to show that all coNP languages (and actually, all languages in P #P ) have interactive proof systems. These techniques were used by Shamir <ref> [Sha] </ref> to show that IP = PSPACE. A central result that enabled the connection to hardness of approximation is that of Babai, Fortnow and Lund [BFL]. They showed that the class MIP equals the class NEXP (i.e., languages recognizable in non-deterministic exponential time).
Reference: [Tar] <author> G. Tardos. </author> <title> Multi-prover encoding schemes and three prover proof systems. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> Vol. 53, No. 2, </volume> <month> October </month> <year> 1996, </year> <pages> pp. 251-260. </pages>
Reference-contexts: The two-prover proofs of Lapidot-Shamir and Feige-Lovasz [LaSh, FeLo] had poly-logarithmic randomness and answer sizes, so [ALMSS] used a modification of these, in the process increasing the number of provers to a constant much larger than two. The later constructions of few-prover proofs of <ref> [BGLR, Tar, FeKi1] </ref> lead to better non-approximability results. Bellare and Sudan [BeSu] identified some extra features of constant prover proofs whose presence they showed could be exploited to further increase the non-approximability factors. These features are captured in their definition of canonical verifiers (cf. Section 3.4). <p> this area is Raz's Parallel Repetition Theorem which implies the ex Free Bits in PCP 21 Due to Provers Coins Answer size Canonical? Can be made canonical? [LaSh, FeLo] 2 polylog polylog No Yes [BeSu] [ALMSS] poly (* 1 ) log polylog No ? [BGLR] 4 log polyloglog No ? <ref> [Tar] </ref> 3 log O (1) No ? [FeKi1] 2 log O (1) No At cost of one more prover [BeSu] [Raz] 2 log O (1) Yes (NA) istence of a two-provers proof system with logarithmic randomness and constant answer size [Raz].
Reference: [Ta-S] <author> A. Ta-Shma. </author> <title> A Note on PCP vs. </title> <journal> MIP. Information Processing Letters, </journal> <volume> Vol. 58, No. 3, </volume> <year> 1996, </year> <pages> pp. 135-140. </pages>
Reference-contexts: This justifies the bound on the soundness probability of V 0 . Containments of PCP systems in MIP systems are more problematic. The reader is referred to a paper by Ta-Shma <ref> [Ta-S] </ref>.
Reference: [TSSW] <author> L. Trevisan, G. Sorkin, M. Sudan and D. Williamson. Gadgets, </author> <title> approximation and linear programming. </title> <booktitle> Proceedings of the 37th Symposium on Foundations of Computer Science, IEEE, </booktitle> <year> 1996, </year> <pages> pp. 617-626. </pages>
Reference-contexts: via MaxSAT [BeSu] 2 free-bits 0.794 O (1) 2 error 1/2 1 2 11 7 32 queries (24 on average) [FeKi1] amortized free-bits O (2 m ) 2 3m 2m 3m free-bits [BeSu] 8 Bellare, Goldreich, Sudan Problem Approx Non-Approx Factor Due to Our Factor Previous Factor Assumption Max3SAT 1:258 <ref> [Yan, GoWi2, TSSW] </ref> 1:038 1 + 1 72 [BeSu] P 6= NP MaxE3SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max2SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP MaxSAT 2 folklore 1 + 1 7 P 6= NP <p> They were able to similarly modify H-astad's proof systems [H1, H2] and thereby improve the hard factor to N 1* , for any * &gt; 0. Gadgets. Another research direction, suggested in early versions of this work [BGS2], was taken on by Trevisan et. al. <ref> [TSSW] </ref> who initiated a systematic study of the construction of gadgets. In particular, they showed that the gadgets we have used in our reductions to the MaxSAT problems were optimal, and constructed better (and optimal) gadgets for reduction to MaxCUT. Weights. <p> A polynomial-time algorithm due to Yannakakis [Yan] approximates it to within a factor of 4=3 &lt; 1:334 (see Goemans and Williamson [GoWi1] for an alternate algorithm). Currently the best known polynomial-time algorithm for Max3SAT achieves a factor of 1:258 (and is due to Trevisan et. al. <ref> [TSSW] </ref> which in turn build on Goemans and Williamson [GoWi2]). For MaxE3SAT, which is also Max-SNP complete, a very simple algorithm achieves an approximation of 8=7 1:143 (where 7=8 is the expected fraction of clauses satisfied by a uniformly chosen assignment). Max2SAT is also Max-SNP complete [GJS, PaYa]. <p> Several questions regarding the ff=fi ratios achievable by 3-SAT and 2-SAT gadgets were posed. Answers were subsequently provided in <ref> [TSSW] </ref>, which undertakes a general study of the construction of optimal gadgets. Proof of Lemma 4.9: We use the gadgets presented in Figure 10 and Figure 11. <p> Using Theorem 4.6 (Part 3), NP K D Gap-2SAT c;s for some c &gt; 0:9 and s &lt; 73 74 c, and NP PCP c;s [log; 2] follows. Remark 10.4 The ratio c=s has been subsequently increased to (10=9) *, for any * &gt; 0 (cf., <ref> [TSSW, H3] </ref>). Proof of Proposition 10.3, Part (5): The result for general verifiers follows from Lemma 4.11 and the fact that MaxSAT can be approximated to within a 0:795 = 0:75 + 0:18 4 factor in polynomial-time 100 (cf., [TSSW]). <p> Proof of Proposition 10.3, Part (5): The result for general verifiers follows from Lemma 4.11 and the fact that MaxSAT can be approximated to within a 0:795 = 0:75 + 0:18 4 factor in polynomial-time 100 (cf., <ref> [TSSW] </ref>). The (tedious) proof of the non-adaptive case can be found in earlier versions of this paper [BGS2]. The paper of Trevisan et. al. [TSSW] contains a stronger result which holds for all verifiers; that is, PCP 1;0:367 [log; 3] = P. <p> follows from Lemma 4.11 and the fact that MaxSAT can be approximated to within a 0:795 = 0:75 + 0:18 4 factor in polynomial-time 100 (cf., <ref> [TSSW] </ref>). The (tedious) proof of the non-adaptive case can be found in earlier versions of this paper [BGS2]. The paper of Trevisan et. al. [TSSW] contains a stronger result which holds for all verifiers; that is, PCP 1;0:367 [log; 3] = P. The latter result (i.e., PCP 1;0:367 [log; 3] = P) is weaker than what can be proven for MIP proof systems (see next corollary).
Reference: [Yan] <author> M. Yannakakis, </author> <title> On the approximation of maximum satisfiability. </title> <journal> Journal of Algorithms, </journal> <volume> Vol. 17, </volume> <year> 1994, </year> <pages> pp. 475-502. </pages>
Reference-contexts: via MaxSAT [BeSu] 2 free-bits 0.794 O (1) 2 error 1/2 1 2 11 7 32 queries (24 on average) [FeKi1] amortized free-bits O (2 m ) 2 3m 2m 3m free-bits [BeSu] 8 Bellare, Goldreich, Sudan Problem Approx Non-Approx Factor Due to Our Factor Previous Factor Assumption Max3SAT 1:258 <ref> [Yan, GoWi2, TSSW] </ref> 1:038 1 + 1 72 [BeSu] P 6= NP MaxE3SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max2SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP MaxSAT 2 folklore 1 + 1 7 P 6= NP <p> This paper P 6= NP 27=26 Long code and new proof systems. 2.4.3 History of approximability results for these problems Satisfiability problems. Max3SAT is the canonical Max-SNP complete problem [PaYa]. A polynomial-time algorithm due to Yannakakis <ref> [Yan] </ref> approximates it to within a factor of 4=3 &lt; 1:334 (see Goemans and Williamson [GoWi1] for an alternate algorithm). Currently the best known polynomial-time algorithm for Max3SAT achieves a factor of 1:258 (and is due to Trevisan et. al. [TSSW] which in turn build on Goemans and Williamson [GoWi2]).
Reference: [Zuc] <author> D. Zuckerman. </author> <title> On unapproximable versions of NP-complete problems. </title> <journal> SIAM J. on Computing, </journal> <volume> Vol. 25, No. 6, </volume> <year> 1996, </year> <pages> pp. 1293-1304. 120 </pages>
Reference-contexts: For the best results one typically uses a randomized form of this reduction due to <ref> [BeSc, Zuc] </ref> and it is this that we will assume henceforth. A NP-hard gap problem is obtained roughly as follows. First, one exhibits an appropriate proof system for NP. Then one applies the FGLSS reduction. The factor indicated hard depends on the Free Bits in PCP 7 proof system parameters. <p> The earlier work of Papadimitriou and Yannakakis introducing the class MaxSNP [PaYa] now came into play; by reduction from Max3SAT it implied hardness of approximation for any MaxSNP-hard problem. Also, reductions from Max Clique lead to hardness results for the chromatic number [LuYa] and other problems <ref> [Zuc] </ref>. The other direction was to increase factors, and reduce assumptions, for existing hardness of approximation results. This involves improving the efficiency of the underlying proof systems and/or the efficiency of the reductions. The first stage of this enterprise started with the work of Arora and Safra [ArSa]. <p> They presented new proof systems minimizing query complexity and exploited a slightly improved version of the FGLSS-reduction due to <ref> [BeSc, Zuc] </ref> to get a N 1=30 hardness of approximation factor for Max Clique. Feige and Kilian [FeKi1], however, observed that one should work with free-bits, and noted that the free-bit complexity of the system of [BGLR] was 14, yielding a N 1=15 hardness factor. <p> Free bits are implicit in [FeKi1] and formalized in [BeSu]. Amortized free bits are introduced in [BeSu] but formalized a little better here. Proof sizes were considered in [BFLS, PoSp]. We consider them here for a different reason they play an important role in that the randomized FGLSS reduction <ref> [BeSc, Zuc] </ref> depends actually on this parameter (rather than on the randomness complexity). The discussion of previous proof systems is coupled with the discussion of Max Clique in Section 2.4.3. We conclude the current section, by discussing two somewhat related topics: query minimization and constant-prover proof systems. Query complexity minimization. <p> If the reduction is deterministic we omit the subscript of "R," or, sometimes, for emphasis, replace it by a subscript of "D." An example is the randomized FGLSS transformation <ref> [FGLSS, BeSc, Zuc] </ref>. <p> The number of queries was unspecified, but indicated to be 10 4 , so * 10 4 . Later work has focused on reducing the constant value of * in the exponent. In later work a slightly tighter form of the FGLSS reduction due to <ref> [BeSc, Zuc] </ref> has been used. <p> An alternative description is given by Zuckerman <ref> [Zuc] </ref>. Another alternative description, carried out in the proof system, is presented in Section 11. <p> Each of the two parts of Proposition 8.7 shows that the well-known method of obtaining clique-approximation results from efficient pcp systems (cf., <ref> [FGLSS, BeSc, Zuc, FeKi1, BeSu] </ref>) is "complete" in the sense that if clique-approximation can be shown NP-hard then this can be done via this method. The precise statement is given in Theorems 8.10 and 8.11 (below). As a preparatory step, we first provide an easier-to-use form of the above proposition.
References-found: 86

