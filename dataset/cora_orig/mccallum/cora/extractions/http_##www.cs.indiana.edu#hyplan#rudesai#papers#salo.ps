URL: http://www.cs.indiana.edu/hyplan/rudesai/papers/salo.ps
Refering-URL: http://www.cs.indiana.edu/hyplan/rudesai/research.html
Root-URL: http://www.cs.indiana.edu
Email: rudesai@indiana.edu rbp1@lanl.gov  
Title: SALO: COMBINING SIMULATED ANNEALING AND LOCAL OPTIMIZATION FOR EFFICIENT GLOBAL OPTIMIZATION  
Author: Rutvik Desai Rajendra Patil 
Affiliation: Indiana University Los Alamos National Laboratory  
Date: 233-237, June 1996  
Note: In Proceedings of the 9th Florida AI Research Symposium (FLAIRS-96), Key West, FL, pp.  
Abstract: Simulated annealing is an established method for global optimization. Perhaps its most salient feature is the statistical promise to deliver a globally optimal solution. In this work, we propose a technique which attempts to combine the robustness of annealing in rugged terrain with the efficiency of local optimization methods in simple search spaces. On a variety of benchmark functions, the proposed method seems to clearly outperform a parallel genetic algorithm and adaptive simulated annealing, two popular and powerful optimization techniques. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Kirkpatrick, C. D. Gelatt, and M. P. Vecchi, </author> <title> Optimization by Simulated Annealing, </title> <journal> Science, </journal> <volume> 220, </volume> <pages> pp. 671-680, </pages> <year> 1983. </year>
Reference-contexts: In situations where the space of parameters cannot be searched exhaustively and the evaluation function cannot be subjected to analytical methods, as is often the case in real world problems, heuristic methods have to be used. We sketch two such methods below. 1.1 Simulated Annealing Simulated annealing (SA) <ref> [1] </ref> is an optimization technique inspired from Monte Carlo methods in statistical mechanics. It attempts to avoid local optima by probabilistically taking nonlocally optimal steps in the search space. The probability of taking such steps decreases with the temperature of the system, which in turn decreases with time.
Reference: [2] <author> L. Ingber, </author> <title> Simulated Annealing: Practice versus Theory, </title> <journal> J. Mathl. Comput. Modelling, </journal> <volume> 8, </volume> <pages> pp. 29-57, </pages> <year> 1993. </year>
Reference-contexts: This ensures a uniform sampling of the search space, which is reassuring when little is known about the nature of the space. Attempts to speed up SA, such as simulated quenching (SQ), usually trade this promise off with the gain in efficiency <ref> [2] </ref>. Below we argue that SALO maintains the statistical promise of SA. Employing SALO for finding the optimal point of a function f can be viewed as using ordinary SA on a transformed function f.
Reference: [3] <author> S. Geman and D. Geman, </author> <title> Stochastic Relaxation, Gibbs Distribution an the Bayesian Restoration in Images, </title> <journal> IEEE Trans. Patt. Anal. Mac. Int., </journal> <volume> 6, </volume> <pages> pp. 721-741, </pages> <year> 1984. </year>
Reference-contexts: SA is able deal with evaluation functions with quite arbitrary degrees of nonlinearities, discontinuities and stochasticity and can process quite arbitrary boundary conditions and constraints imposed on these evaluation functions [1,2]. It has been shown <ref> [3] </ref> that with a large enough initial temperature and a proper temperature schedule, SA guarantees a globally optimal solution.
Reference: [4] <author> L. Ingber, </author> <title> Adaptive Simulated Annealing (ASA) [ftp.caltech.edu:/pub/ingber/asa.Z], Software package documentation, </title> <year> 1995. </year>
Reference-contexts: It has been shown [3] that with a large enough initial temperature and a proper temperature schedule, SA guarantees a globally optimal solution. We use Adaptive Simulated Annealing (ASA) <ref> [4] </ref>, an implementation of a method known as Very Fast Simulated Re-annealing (VFSR) [5] which permits a very fast temperature annealing schedule, as our basic SA algorithm. 1.2 Local Optimization A local optimizer or a hill climber is very efficient method for optimization in simple, unimodal spaces.
Reference: [5] <author> L. Ingber, </author> <title> Very Fast Simulated Re-Annealing, </title> <journal> J. Mathl. Comput. Modelling, </journal> <volume> 12, </volume> <pages> pp. 967-973, </pages> <year> 1989. </year>
Reference-contexts: It has been shown [3] that with a large enough initial temperature and a proper temperature schedule, SA guarantees a globally optimal solution. We use Adaptive Simulated Annealing (ASA) [4], an implementation of a method known as Very Fast Simulated Re-annealing (VFSR) <ref> [5] </ref> which permits a very fast temperature annealing schedule, as our basic SA algorithm. 1.2 Local Optimization A local optimizer or a hill climber is very efficient method for optimization in simple, unimodal spaces.
Reference: [6] <author> P. Winston, </author> <booktitle> Artificial Intelligence, </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, third edition, </address> <year> 1992. </year>
Reference-contexts: But in most real-world problems, it easily gets trapped in nonoptimal regions, mainly because of (1) local optima; (2) flat surfaces or (3) ridges <ref> [6] </ref>.
Reference: [7] <author> D. Yuret, </author> <title> From Genetic Algorithms to Efficient Optimization, </title> <type> MS Thesis, </type> <institution> Dept. of Electrical Engineering and Computer Science, MIT, </institution> <year> 1994. </year>
Reference-contexts: But in most real-world problems, it easily gets trapped in nonoptimal regions, mainly because of (1) local optima; (2) flat surfaces or (3) ridges [6]. The local optimization algorithm proposed in <ref> [7] </ref> attempts to tackle some of these problems while trying to maintain the efficiency by employing following ideas: (1) Adjust the size of the probing steps to suit the nature of the terrain, shrinking when probes do poorly and growing when probes do well. (2) Keep track of the directions of
Reference: [8] <author> K. De Jong, </author> <title> An Analysis of the Behavior of a Class of Genetic Adaptive Systems, </title> <type> PhD Thesis, </type> <institution> University of Michigan, Diss. Abstr. </institution> <note> Int. 36(10), 5140B, University Microfilms No 76-9381, </note> <year> 1975. </year>
Reference-contexts: EXPERIMENTAL RESULTS For comparing the performance of SALO with other methods, we use several well-known benchmark problems, listed below. These problems represent various characteristic terrain found in real-world problems, e.g., unimodal/multi-modal, with/without plateaus and ridges, high/low dimensional. f1: Sphere model <ref> [8] </ref> This is a smooth, unimodal, symmetric function and does not have any problems of ridges, plateaus or foothills. The performance on this function is a measure of the general efficiency of the optimization algorithm. f2: Rosenbrock's function [8] This is a unimodal and bi-quadratic function with a very narrow ridge <p> problems, e.g., unimodal/multi-modal, with/without plateaus and ridges, high/low dimensional. f1: Sphere model <ref> [8] </ref> This is a smooth, unimodal, symmetric function and does not have any problems of ridges, plateaus or foothills. The performance on this function is a measure of the general efficiency of the optimization algorithm. f2: Rosenbrock's function [8] This is a unimodal and bi-quadratic function with a very narrow ridge which runs around a parabola and has a very sharp tip. The progress of many algorithms is very slow because they are unable to discover a good search direction. f3: Step function [8] This function contains flat regions <p> optimization algorithm. f2: Rosenbrock's function <ref> [8] </ref> This is a unimodal and bi-quadratic function with a very narrow ridge which runs around a parabola and has a very sharp tip. The progress of many algorithms is very slow because they are unable to discover a good search direction. f3: Step function [8] This function contains flat regions joined by steep slopes. It is a representative of plateau problems with linear and discontinuous properties.
Reference: [9] <author> D. J. Ackley, </author> <title> An Empirical Study of Bit Vector Function Optimization, in Genetic Algorithms and Simulated Annealing, edited by L. </title> <address> Davis, London, </address> <publisher> Pitman, </publisher> <pages> pp. 194-200, </pages> <year> 1987. </year>
Reference-contexts: It is a representative of plateau problems with linear and discontinuous properties. Flat regions do not give any information as to which direction is favorable. f4: Plateau function <ref> [9] </ref> This function has a large number of flat regions whose value gradually decreases towards the global minimum near the origin. f5: Sines function transformation f after applying a local optimizer f x x i n ( ) r - = = 1 f x x x x i i i
Reference: [10] <author> T. B@ck, F. Hoffmeister, and H.-P. Schwefel, </author> <title> Applications of Evolutionary Algorithms, Report of the Systems Analysis Research Group SYS-2/92, </title> <institution> University of Dortmund, Department of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: The problem optimization algorithms face with this function is the peak response of about five orders of magnitude greater than the minimum in the neighborhood of the minimum. f7: Rastrigin's function This is a scaleable, multimodal function made from the sphere model by modulating it with Acos (2x i ) <ref> [10] </ref>. Far away from the origin this functions looks like the sphere model, but with smaller x i the effect of the modulation grows and dominates the shape.
Reference: [11] <author> P. Ross, </author> <title> About PGA v2.7, </title> <institution> Dept. of AI, University of Edinburgh, </institution> <year> 1994. </year>
Reference-contexts: The number of function evaluations reported are the averages of 10 independent runs for each method. Lack of consistent convergence is indicated by a ?. n indicates the number of dimensions of the function. The multi-population genetic algorithm used was PGA v2.7 <ref> [11] </ref>. The following parameters were used in PGA: number of populations 5, number of individuals in each population 20, number of bit per variable 16, selection type rank selection, mutation rate 0.005, crossover rate 1.0, crossover type two-point crossover, and migration interval 10.
References-found: 11

