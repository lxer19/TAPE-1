URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/dayne/www/ps/webwatcher-aaais95.ps.Z
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/dayne/www/cv.html
Root-URL: 
Title: WebWatcher: A Learning Apprentice for the World Wide Web  
Author: Robert Armstrong, Dayne Freitag, Thorsten Joachims, and Tom Mitchell 
Date: March 19, 1997  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: We describe an information seeking assistant for the world wide web. This agent, called WebWatcher, interactively helps users locate desired information by employing learned knowledge about which hyperlinks are likely to lead to the target information. Our primary focus to date has been on two issues: (1) organizing WebWatcher to provide interactive advice to Mosaic users while logging their successful and unsuccessful searches as training data, and (2) incorporating machine learning methods to automatically acquire knowledge for selecting an appropriate hyperlink given the current web page viewed by the user and the user's information goal. We describe the initial design of WebWatcher, and the results of our preliminary learning experiments. 
Abstract-found: 1
Intro-found: 1
Reference: [Mitchell et al., 1985] <author> T. Mitchell, S. Mahadevan, and L. Steinberg, </author> <title> "LEAP: A Learning Apprentice for VLSI Design," </title> <booktitle> Ninth International Joint Conference on Artificial Intelligence, </booktitle> <month> August </month> <year> 1985. </year>
Reference-contexts: This paper presents the initial design and implementation of an agent called WebWatcher that is intended to assist users both by interactively advising them as they traverse web links in search of information, and by searching autonomously on their behalf. In interactive mode, WebWatcher acts as a learning apprentice <ref> [Mitchell et al., 1985; Mitchell et. al., 1994] </ref>, providing interactive advice to the Mosaic user regarding which hyperlinks to follow next, then learning by observing the user's reaction to this advice as well as the eventual success or failure of the user's actions.
Reference: [Mitchell et. al., 1994] <author> T.M. Mitchell, R. Caruana, D. Freitag, J. McDermott, and D. Zabowski, </author> <title> "Experience with a Learning Personal Assistant," </title> <journal> Communications of the ACM, </journal> <volume> Vol. 37, No. 7, </volume> <pages> pp. 81-91, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: This paper presents the initial design and implementation of an agent called WebWatcher that is intended to assist users both by interactively advising them as they traverse web links in search of information, and by searching autonomously on their behalf. In interactive mode, WebWatcher acts as a learning apprentice <ref> [Mitchell et al., 1985; Mitchell et. al., 1994] </ref>, providing interactive advice to the Mosaic user regarding which hyperlinks to follow next, then learning by observing the user's reaction to this advice as well as the eventual success or failure of the user's actions.
Reference: [Salton and McGill, 1983] <author> G. Salton and M.J. McGill, </author> <title> Introduction to Modern Information Retrieval, </title> <publisher> McGraw-Hill, Inc., </publisher> <year> 1983. </year>
Reference-contexts: We have experimented with a variety of representations that re-represent the arbitrary-length text associated with pages, links, and goals as a fixed-length feature vector. This idea is common within information retrieval retrieval systems <ref> [Salton and McGill, 1983] </ref>. It offers the advantage that the information in an arbitrary amount of text is summarized in a fixed length feature vector compatible with current machine learning methods. It also carries the disadvantage that much information is lost by this re-representation. <p> ; :::; p n are the individual probabilities, and I is the set of indexes for which a bit is set in a given test vector, then the probability that the corresponding link was followed is determined by 1 i2I (1 p i ). * TFIDF with cosine similarity measure <ref> [Salton and McGill, 1983; Lang, 1995] </ref> is a method developed in information retrieval. In the general case at first a vector V of words is created. In our experiments it is already given by the representation described above.
Reference: [Lang, 1995] <author> K. Lang, NewsWeeder: </author> <title> Learning to Filter Netnews, </title> <note> to be submitted to the International Conference on Machine Learning, </note> <year> 1995 </year>
Reference-contexts: ; :::; p n are the individual probabilities, and I is the set of indexes for which a bit is set in a given test vector, then the probability that the corresponding link was followed is determined by 1 i2I (1 p i ). * TFIDF with cosine similarity measure <ref> [Salton and McGill, 1983; Lang, 1995] </ref> is a method developed in information retrieval. In the general case at first a vector V of words is created. In our experiments it is already given by the representation described above.
Reference: [Littlestone, 1988] <author> N. Littlestone, </author> <title> "Learning quickly when irrelevant attributes abound," </title> <journal> Machine Learning, </journal> <volume> 2:4, </volume> <pages> pp. 285-318 </pages> . 
Reference-contexts: possible learning approaches and to determine the level of competence achievable by a learning agent, we applied the following four methods to training data 2 The appendix lists the words selected by this procedure using one of our training sets. collected by WebWatcher during 30 information search sessions: * Winnow <ref> [Littlestone, 1988] </ref> learns a boolean con cept represented as a single linear threshold function of the instance features. Weights for this threshold function are learned using a multiplicative update rule. In our experiments we enriched the original 530 attributes by a transformation.
Reference: [Quinlan, 1993] <author> J.R. Quinlan, C4.5: </author> <title> Programs for Machine Learning, </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: were selected by first gathering every distinct word that occurred over the training set, then ranking these according to their mutual information with respect to correctly classifying the training data, and finally choosing the top N words in this ranking. 2 Mutual information is a common statistical measure (see, e.g., <ref> [Quinlan, 1993] </ref>) of the degree to which an individual feature (in this case a word) can correctly classify the observed data. about the current P age, Link, and Goal. 3.3 What Learning Method Should be Used? The task of the learner is to learn the general function U serChoice?, given a
References-found: 6

