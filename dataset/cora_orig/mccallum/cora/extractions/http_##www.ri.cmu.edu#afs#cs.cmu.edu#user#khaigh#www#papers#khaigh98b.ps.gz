URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/khaigh/www/papers/khaigh98b.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/khaigh/www/papers/khaigh98b.abstract.html
Root-URL: 
Email: khaigh@cs.cmu.edu  mmv@cs.cmu.edu  
Title: Planning, Execution and Learning in a Robotic Agent  
Author: Karen Zita Haigh Manuela M. Veloso 
Address: Pittsburgh PA 15213-3891  
Affiliation: Computer Science Department Carnegie Mellon University  
Web: http://www.cs.cmu.edu/~khaigh  http://www.cs.cmu.edu/~mmv  
Abstract: This paper presents the complete integrated planning, executing and learning robotic agent Rogue. We describe Rogue's task planner that interleaves high-level task planning with real world robot execution. It supports multiple, asynchronous goals, suspends and interrupts tasks, and monitors and compensates for failure. We present a general approach for learning situation-dependent rules from execution, which correlates environmental features with learning opportunities, thereby detecting patterns and allowing planners to predict and avoid failures. We present two implementations of the general learning approach, in the robot's path planner, and in the task planner. We present empirical data to show the effectiveness of Rogue's novel learning approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Baroglio, C., Giordana, A., Kaiser, M., Nuttin, M., and Piola, R. </author> <title> Learning controllers for industrial robots. </title> <journal> Machine Learning, </journal> <volume> 23 </volume> <pages> 221-249, </pages> <year> 1996. </year>
Reference-contexts: The task planner is described in more detail elsewhere [6]. Learning Learning has been applied to robotics problems in a variety of manners. Common applications include map learning and localization (e.g. [9; 10; 22]), or learning operational parameters for better actuator control (e.g. <ref> [1; 2; 15] </ref>). Instead of improving low-level actuator con In Parallel: 1. Rogue receives a task request from a user, and adds the information to prodigy4.0's state. 2. Rogue requests a plan from prodigy4.0.
Reference: [2] <author> Bennett, S. W. and DeJong, G. F. </author> <title> Real-world robotics: Learning to plan for robust execution. </title> <journal> Machine Learning, </journal> <volume> 23 </volume> <pages> 121-161, </pages> <year> 1996. </year>
Reference-contexts: The task planner is described in more detail elsewhere [6]. Learning Learning has been applied to robotics problems in a variety of manners. Common applications include map learning and localization (e.g. [9; 10; 22]), or learning operational parameters for better actuator control (e.g. <ref> [1; 2; 15] </ref>). Instead of improving low-level actuator con In Parallel: 1. Rogue receives a task request from a user, and adds the information to prodigy4.0's state. 2. Rogue requests a plan from prodigy4.0.
Reference: [3] <author> Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. </author> <title> Classification and Regression Trees. </title> <address> (Pacific Grove, CA: </address> <publisher> Wadsworth & Brooks/Cole), </publisher> <year> 1984. </year>
Reference-contexts: The events matrix is grown incrementally; most recent data is appended at the bottom. By using incremental learning, Rogue can notice and respond to changes on a continuous basis. Learning. We selected regression trees <ref> [3] </ref> as our learning mechanism because they can handle continuous values, and form disjunctive hypotheses. A regression tree is created for each event, in which features are splits and costs are learned values.
Reference: [4] <author> Goodwin, R. </author> <title> Meta-Level Control for Decision-Theoretic Planners. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1996. </year>
Reference-contexts: The path planner uses a modified A* algorithm on a topological map that has additional metric information <ref> [4] </ref>. The map is a graph with nodes and arcs representing rooms, corridors, doors and lobbies. Rogue demonstrates the ability to learn situation-dependent costs for the path planner's arcs. Learning appropriate arc-cost functions will allow the path planner to avoid troublesome areas of the environment when appropriate.
Reference: [5] <author> Haigh, K. Z. </author> <title> Learning Situation-Dependent Planning Knowledge from Uncertain Robot Execution Data. </title> <type> PhD thesis, </type> <institution> Computer Science Department, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1998. </year>
Reference-contexts: This paper presents our work extending the high-level reasoning capabilities of a real robot in two ways: * by adding a high-level task planner that interleaves planning with execution, and * by adding the ability to learn from real execution to improve planning. We have developed Rogue <ref> [5; 6; 7] </ref> which forms the task planning and learning modules for a real mobile robot, Xavier (see Figure 1). <p> Rogue effectively enables the communication between Xavier, prodigy and the user. The planning and execution capabilities of Rogue form the foundation for a complete, learning, autonomous agent. Rogue's planner-independent learning approach processes the robot's execution data with the goal of improving planning <ref> [5; 7] </ref>. It applies to two planners: Xavier's path planner, and the task planner. Rogue learns situation-dependent rules that affect the planners' decisions. Our approach relies on direct examination of the robot's execution traces to identify situations in which the planner's behaviour needs to change. <p> Rule 1 is used to select tasks between those times involving rooms "less than" 5314: : : namely room 5312. The prefer-reject control rule (rule 2) is used to prefer tasks other than those involving room 5316. Additional experiments are presented elsewhere <ref> [5] </ref>. They demonstrate that, by learning search control rules from execution experience, Rogue helps the task planner predict and avoid failures when executing. In this way, the overall system becomes more efficient and effective at accomplishing tasks.
Reference: [6] <author> Haigh, K. Z. and Veloso, M. M. </author> <title> Interleaving planning and robot execution for asynchronous user requests. </title> <booktitle> Autonomous Robots, </booktitle> <year> 1997. </year> <note> In press. </note>
Reference-contexts: This paper presents our work extending the high-level reasoning capabilities of a real robot in two ways: * by adding a high-level task planner that interleaves planning with execution, and * by adding the ability to learn from real execution to improve planning. We have developed Rogue <ref> [5; 6; 7] </ref> which forms the task planning and learning modules for a real mobile robot, Xavier (see Figure 1). <p> Rogue provides a setup where users can post tasks for which the planner generates appropriate plans, delivers them to the robot, monitors their execution, and learns from feedback about execution performance. Rogue's task planner is built upon the prodigy4.0 planning and learning system <ref> [6; 23] </ref>. The task planner generates and executes plans for multiple interacting goals which arrive asynchronously and whose task structure is not known a priori. The task planner interleaves tasks and reasons about task priority and task compatibility. <p> Rogue controls the execution of a real robot to accomplish tasks in the real world. The complete interleaved planning and execution cycle is shown in Table 1. The task planner is described in more detail elsewhere <ref> [6] </ref>. Learning Learning has been applied to robotics problems in a variety of manners. Common applications include map learning and localization (e.g. [9; 10; 22]), or learning operational parameters for better actuator control (e.g. [1; 2; 15]). Instead of improving low-level actuator con In Parallel: 1.
Reference: [7] <author> Haigh, K. Z. and Veloso, M. M. </author> <title> Learning situation-dependent costs: Improving planning from probabilistic robot execution. </title> <booktitle> In Proceedings of the Second International Conference on Autonomous Agents, 1998. </booktitle> <address> (Menlo Park, CA: </address> <publisher> AAAI Press). In Press. </publisher>
Reference-contexts: This paper presents our work extending the high-level reasoning capabilities of a real robot in two ways: * by adding a high-level task planner that interleaves planning with execution, and * by adding the ability to learn from real execution to improve planning. We have developed Rogue <ref> [5; 6; 7] </ref> which forms the task planning and learning modules for a real mobile robot, Xavier (see Figure 1). <p> Rogue effectively enables the communication between Xavier, prodigy and the user. The planning and execution capabilities of Rogue form the foundation for a complete, learning, autonomous agent. Rogue's planner-independent learning approach processes the robot's execution data with the goal of improving planning <ref> [5; 7] </ref>. It applies to two planners: Xavier's path planner, and the task planner. Rogue learns situation-dependent rules that affect the planners' decisions. Our approach relies on direct examination of the robot's execution traces to identify situations in which the planner's behaviour needs to change. <p> Features are hand-picked by the designers, and are extracted from the robot, the environment, and the task. Rogue extracts arc traversal events and environmental features from the massive, continuous, probabilistic 1 More information about learning for the path planner can be found elsewhere <ref> [7] </ref>. execution traces, and then evaluates the events accord-ing to the cost function. The learning algorithm then creates the situation-dependent arc costs. The execution traces are provided by the robot's navigation module. Navigation is done using Partially Observable Markov Decision Process models [19].
Reference: [8] <author> Klingspor, V., Morik, K. J., and Rieger, A. D. </author> <title> Learning concepts from sensor data of a mobile robot. </title> <journal> Machine Learning, </journal> <volume> 23 </volume> <pages> 305-332, </pages> <year> 1996. </year>
Reference-contexts: Table 1: The complete planning and execution cycle in Rogue. Note that Steps 1 to 3 execute in parallel. trol, our work focusses at the planning stages of the system. A few other researchers have explored this area as well, learning and correcting action models (e.g. <ref> [8; 14] </ref>), or learning costs and applicability of actions (e.g. [11; 17; 21]). Our work falls into the latter category. In some situations, it is enough to learn that a particular action has a certain average probability or cost. However, actions may have different costs under different conditions.
Reference: [9] <author> Koenig, S. and Simmons, R. G. </author> <title> Passive distance learning for robot navigation. </title> <booktitle> In Machine Learning: Proceedings of the Thirteenth International Conference (ICML96), </booktitle> <pages> pages 266-274, </pages> <address> 1996. (San Mateo, CA: </address> <publisher> Mor-gan Kaufmann). </publisher>
Reference-contexts: The complete interleaved planning and execution cycle is shown in Table 1. The task planner is described in more detail elsewhere [6]. Learning Learning has been applied to robotics problems in a variety of manners. Common applications include map learning and localization (e.g. <ref> [9; 10; 22] </ref>), or learning operational parameters for better actuator control (e.g. [1; 2; 15]). Instead of improving low-level actuator con In Parallel: 1. Rogue receives a task request from a user, and adds the information to prodigy4.0's state. 2. Rogue requests a plan from prodigy4.0.
Reference: [10] <author> Kortenkamp, D. and Weymouth, T. </author> <title> Topological mapping for mobile robots using a combination of sonar and vision sensing. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <pages> pages 979-984, </pages> <address> 1994. (Menlo Park, CA: </address> <publisher> AAAI Press). </publisher>
Reference-contexts: The complete interleaved planning and execution cycle is shown in Table 1. The task planner is described in more detail elsewhere [6]. Learning Learning has been applied to robotics problems in a variety of manners. Common applications include map learning and localization (e.g. <ref> [9; 10; 22] </ref>), or learning operational parameters for better actuator control (e.g. [1; 2; 15]). Instead of improving low-level actuator con In Parallel: 1. Rogue receives a task request from a user, and adds the information to prodigy4.0's state. 2. Rogue requests a plan from prodigy4.0.
Reference: [11] <author> Lindner, J., Murphy, R. R., and Nitz, E. </author> <title> Learning the expected utility of sensors and algorithms. </title> <booktitle> In IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems, </booktitle> <pages> pages 583-590. </pages> <address> (New York, NY: </address> <publisher> IEEE Press), </publisher> <year> 1994. </year>
Reference-contexts: Note that Steps 1 to 3 execute in parallel. trol, our work focusses at the planning stages of the system. A few other researchers have explored this area as well, learning and correcting action models (e.g. [8; 14]), or learning costs and applicability of actions (e.g. <ref> [11; 17; 21] </ref>). Our work falls into the latter category. In some situations, it is enough to learn that a particular action has a certain average probability or cost. However, actions may have different costs under different conditions.
Reference: [12] <author> Nilsson, N. J. </author> <title> Shakey the robot. </title> <type> Technical Report 323, </type> <institution> AI Center, SRI International, </institution> <address> Menlo Park, CA, </address> <year> 1984. </year>
Reference-contexts: Introduction In complex, dynamic domains, a robot's knowledge about the environment will rarely be complete and correct. Since Shakey the robot <ref> [12] </ref>, researchers have been trying to build autonomous robots that are capable of planning and executing high-level tasks, as well as learning from the analysis of execution experience.
Reference: [13] <author> O'Sullivan, J., Haigh, K. Z., and Armstrong, G. D. </author> <type> Xavier. </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> April </month> <year> 1997. </year> <note> Manual, Version 0.3, unpublished internal report. Available via http://www.cs.cmu.edu/- ~Xavier/. </note>
Reference-contexts: Xavier is a mobile robot being developed at Carnegie Mellon University <ref> [13; 18] </ref>. It is built on an RWI B24 base and includes bump sensors, a laser range finder, sonars, a color camera and a speech board. The software controlling Xavier includes both reactive and de liberative behaviours.
Reference: [14] <author> Pearson, D. J. </author> <title> Learning Procedural Planning Knowledge in Complex Environments. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, University of Michigan, </institution> <address> Ann Arbor, MI, </address> <year> 1996. </year>
Reference-contexts: Table 1: The complete planning and execution cycle in Rogue. Note that Steps 1 to 3 execute in parallel. trol, our work focusses at the planning stages of the system. A few other researchers have explored this area as well, learning and correcting action models (e.g. <ref> [8; 14] </ref>), or learning costs and applicability of actions (e.g. [11; 17; 21]). Our work falls into the latter category. In some situations, it is enough to learn that a particular action has a certain average probability or cost. However, actions may have different costs under different conditions.
Reference: [15] <author> Pomerleau, D. A. </author> <title> Neural network perception for mobile robot guidance. </title> <address> (Dordrecht, Netherlands: </address> <publisher> Kluwer Academic), </publisher> <year> 1993. </year>
Reference-contexts: The task planner is described in more detail elsewhere [6]. Learning Learning has been applied to robotics problems in a variety of manners. Common applications include map learning and localization (e.g. [9; 10; 22]), or learning operational parameters for better actuator control (e.g. <ref> [1; 2; 15] </ref>). Instead of improving low-level actuator con In Parallel: 1. Rogue receives a task request from a user, and adds the information to prodigy4.0's state. 2. Rogue requests a plan from prodigy4.0.
Reference: [16] <author> Rabiner, L. R. and Juang, B. H. </author> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 6(3) </volume> <pages> 4-16, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: The execution trace in particular does not contain arc traversals. We therefore need to extract the traversed arc sequence from the Markov state distributions of the navigation module. The first step in this process is to calculate likely sequences of Markov states. We use Viterbi's algorithm <ref> [16] </ref> to calculate the most likely transitions from state to state. Then, from high probability states throughout the trace, we reconstruct sequences that are likely to reflect the robot's actual trajectory.
Reference: [17] <author> Shen, W.-M. </author> <title> Autonomous Learning from the Environment. </title> <address> (New York, NY: </address> <publisher> Computer Science Press), </publisher> <year> 1994. </year>
Reference-contexts: Note that Steps 1 to 3 execute in parallel. trol, our work focusses at the planning stages of the system. A few other researchers have explored this area as well, learning and correcting action models (e.g. [8; 14]), or learning costs and applicability of actions (e.g. <ref> [11; 17; 21] </ref>). Our work falls into the latter category. In some situations, it is enough to learn that a particular action has a certain average probability or cost. However, actions may have different costs under different conditions.
Reference: [18] <author> Simmons, R., Goodwin, R., Haigh, K. Z., Koenig, S., and O'Sullivan, J. </author> <title> A layered architecture for office delivery robots. </title> <booktitle> In Proceedings of the First International Conference on Autonomous Agents, </booktitle> <pages> pages 245-252, </pages> <address> 1997. (New York, NY: </address> <publisher> ACM Press). </publisher>
Reference-contexts: Xavier is a mobile robot being developed at Carnegie Mellon University <ref> [13; 18] </ref>. It is built on an RWI B24 base and includes bump sensors, a laser range finder, sonars, a color camera and a speech board. The software controlling Xavier includes both reactive and de liberative behaviours. <p> The software controlling Xavier includes both reactive and de liberative behaviours. Much of the software can be classified into five layers: Obstacle Avoidance, Navigation, Path Planning, Task Planning (provided by Rogue), and the User Interface. The underlying architecture is described in more detail by Simmons et al. <ref> [18] </ref>. Rogue provides a setup where users can post tasks for which the planner generates appropriate plans, delivers them to the robot, monitors their execution, and learns from feedback about execution performance. Rogue's task planner is built upon the prodigy4.0 planning and learning system [6; 23].
Reference: [19] <author> Simmons, R. and Koenig, S. </author> <title> Probabilistic robot navigation in partially observable environments. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pages 1080-1087, </pages> <address> 1995. (San Mateo, CA: </address> <publisher> Morgan Kaufmann). </publisher>
Reference-contexts: The learning algorithm then creates the situation-dependent arc costs. The execution traces are provided by the robot's navigation module. Navigation is done using Partially Observable Markov Decision Process models <ref> [19] </ref>. The execution trace includes observed features of the environment as well as the probability distribution over the Markov states at each time step. Event Identification. Identifying the planner's arc traversal events from this execution trace is challenging because they contain a massive, continuous stream of uncertain data.
Reference: [20] <author> Stone, P. and Veloso, M. M. </author> <title> User-guided interleaving of planning and execution. </title> <booktitle> In New Directions in AI Planning, </booktitle> <pages> pages 103-112. </pages> <address> (Amsterdam, Netherlands: </address> <publisher> IOS Press), </publisher> <year> 1996. </year>
Reference-contexts: The task planner is based on prodigy4.0 [23], a domain-independent nonlinear state-space planner that uses means-ends analysis and backward chaining to reason about multiple goals and multiple alternative operators to achieve the goals. It has been extended to support real-world execution of its symbolic actions <ref> [20] </ref>. Task requests arrive from users at any time. Rogue incorporates the information into prodigy4.0's state description, and then prodigy4.0 extends the current plan to incorporate the new task.
Reference: [21] <author> Tan, M. </author> <title> Cost-sensitive robot learning. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1991. </year>
Reference-contexts: Note that Steps 1 to 3 execute in parallel. trol, our work focusses at the planning stages of the system. A few other researchers have explored this area as well, learning and correcting action models (e.g. [8; 14]), or learning costs and applicability of actions (e.g. <ref> [11; 17; 21] </ref>). Our work falls into the latter category. In some situations, it is enough to learn that a particular action has a certain average probability or cost. However, actions may have different costs under different conditions.
Reference: [22] <author> Thrun, S. </author> <title> A Bayesian approach to landmark discovery in mobile robot navigation. </title> <type> Technical Report CMU-CS-96-122, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1996. </year>
Reference-contexts: The complete interleaved planning and execution cycle is shown in Table 1. The task planner is described in more detail elsewhere [6]. Learning Learning has been applied to robotics problems in a variety of manners. Common applications include map learning and localization (e.g. <ref> [9; 10; 22] </ref>), or learning operational parameters for better actuator control (e.g. [1; 2; 15]). Instead of improving low-level actuator con In Parallel: 1. Rogue receives a task request from a user, and adds the information to prodigy4.0's state. 2. Rogue requests a plan from prodigy4.0.
Reference: [23] <author> Veloso, M. M., Carbonell, J., Perez, M. A., Borrajo, D., Fink, E., and Blythe, J. </author> <title> Integrating planning and learning: The prodigy architecture. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 7(1) </volume> <pages> 81-120, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Rogue provides a setup where users can post tasks for which the planner generates appropriate plans, delivers them to the robot, monitors their execution, and learns from feedback about execution performance. Rogue's task planner is built upon the prodigy4.0 planning and learning system <ref> [6; 23] </ref>. The task planner generates and executes plans for multiple interacting goals which arrive asynchronously and whose task structure is not known a priori. The task planner interleaves tasks and reasons about task priority and task compatibility. <p> The task planner is based on prodigy4.0 <ref> [23] </ref>, a domain-independent nonlinear state-space planner that uses means-ends analysis and backward chaining to reason about multiple goals and multiple alternative operators to achieve the goals. It has been extended to support real-world execution of its symbolic actions [20]. Task requests arrive from users at any time.
References-found: 23

