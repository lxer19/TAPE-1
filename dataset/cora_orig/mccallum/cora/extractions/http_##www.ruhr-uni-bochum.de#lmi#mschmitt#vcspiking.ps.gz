URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/vcspiking.ps.gz
Refering-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Root-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Email: mschmitt@lmi.ruhr-uni-bochum.de  
Title: VC Dimension Bounds for Networks of Spiking Neurons  
Author: Michael Schmitt 
Address: Ruhr-Universitat Bochum, D-44780 Bochum, Germany  
Affiliation: Lehrstuhl Mathematik und Informatik, Fakultat fur Mathematik  
Abstract: We calculate bounds on the VC dimension and pseudo dimension for networks of spiking neurons. The connections between network nodes are parameterized by transmission delays and synaptic weights. We provide bounds in terms of network depth and number of connections that are almost linear. For networks with few layers this yields better bounds than previously established results for networks of unrestricted depth. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Gerstner. </author> <title> Spiking neurons. </title> <editor> In W. Maass and C. M. Bishop (eds.), </editor> <title> Pulsed Neural Networks. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1999, </year> <note> to app. </note>
Reference-contexts: The functions h d i ;w i approximate the so-called postsynaptic potentials of biological neurons and are used for implementations of pulsed neural networks in analog VLSI such as described by [6]. Further discussions of this type of neuron model can be found in the surveys by Gerstner <ref> [1] </ref> and Maass [3]. We consider computations of spiking neurons based on two types of coding.
Reference: [2] <author> W. Maass. </author> <title> Networks of spiking neurons: </title> <booktitle> The third generation of neural network models. Neural Networks 10, </booktitle> <pages> pp. 1659-1671, </pages> <year> 1997. </year>
Reference-contexts: More precisely, the firing time t v of v satisfies t v = minft : P v (t) v g. The neuron model that we consider here is a simple version of a leaky integrate-and-fire neuron and has been introduced by Maass <ref> [2] </ref>. The functions h d i ;w i approximate the so-called postsynaptic potentials of biological neurons and are used for implementations of pulsed neural networks in analog VLSI such as described by [6].
Reference: [3] <author> W. Maass. </author> <title> Computing with spiking neurons. </title> <editor> In W. Maass and C. M. Bishop (eds.), </editor> <title> Pulsed Neural Networks. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1999, </year> <note> to app. </note>
Reference-contexts: Further discussions of this type of neuron model can be found in the surveys by Gerstner [1] and Maass <ref> [3] </ref>. We consider computations of spiking neurons based on two types of coding. Binary coding is used to represent Boolean values assuming that a neuron fires at some fixed time if it encodes a 1, and that it does not fire at all if it encodes a 0.
Reference: [4] <author> W. Maass and M. Schmitt. </author> <title> On the complexity of learning for a spiking neuron. </title> <booktitle> In Proc. 10th COLT, </booktitle> <pages> pp. 54-61, </pages> <publisher> ACM, </publisher> <address> New York, </address> <year> 1997. </year>
Reference-contexts: Recent theoretical results have shown that the computational power and learning capabilities for a spiking neuron with adjustable delays are significantly higher compared to neuron models that only have weights as programmable parameters <ref> [4, 7] </ref>. <p> We refer to this sequence as the time course of the membrane potential of v. For n connections the sequence is known to require not more than 2n different subsets as members <ref> [4] </ref>. The networks of spiking neurons that we study here are feedforward networks where the connectivity is defined in terms of a directed acyclic graph. Since we focus on the computation of scalar-valued functions we assume that there is just one node, the output node, that has no outgoing connection. <p> Hence, for all s 2 S a number of at most m2 b C hyperplanes partition the parameter domain of weights and thresholds of level nodes into regions that yield identical sequences of symbolic outputs on S. It is well known (see, e.g., <ref> [4] </ref>) that h hyperplanes partition IR n into at most 2 (eh=n) n different regions. (Here, e denotes the base of the natural logarithm.) Applying this to the regions of delay parameters bounded by at most m (2 b C ) 2 hyperplanes of the form (1), and, for each of <p> It was argued in <ref> [4] </ref> that a function computed by a spiking neuron using binary coding of the inputs can also be computed by such a network using analog coding at the expense of adding one extra input node.
Reference: [5] <author> W. Maass and M. Schmitt. </author> <title> On the complexity of computing and learning with networks of spiking neurons. </title> <booktitle> In Proc. 5th Int. Symp. Artificial Intelligence and Mathematics, </booktitle> <address> http://rutcor.rutgers.edu/~amai/, 1998. </address>
Reference-contexts: Moreover, by exhibiting networks with quadratic VC dimension the number of training examples that an algorithm needs for adapting the delays of a network of spiking neurons has been shown to grow at least quadratically in the number of adjustable delays <ref> [5] </ref>. (For details concerning the relationship between VC dimension and learnability we refer the reader to Vidyasagar [8].) This result, however, uses networks of unrestricted depth. <p> This even holds if the delays are the only programmable parameters of the network and all weights and thresholds remain fixed. Proof. (Sketch) It was shown in <ref> [5] </ref> that for any m 2 IN a module M m can be constructed that extracts and removes the most significant bit from an m-bit binary number that is given to the module in analog coding.
Reference: [6] <author> A. F. Murray. </author> <title> Pulse-based computation in VLSI neural networks. </title> <editor> In W. Maass and C. M. Bishop (eds.), </editor> <title> Pulsed Neural Networks. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1999, </year> <note> to app. </note>
Reference-contexts: The functions h d i ;w i approximate the so-called postsynaptic potentials of biological neurons and are used for implementations of pulsed neural networks in analog VLSI such as described by <ref> [6] </ref>. Further discussions of this type of neuron model can be found in the surveys by Gerstner [1] and Maass [3]. We consider computations of spiking neurons based on two types of coding.
Reference: [7] <author> M. Schmitt. </author> <title> Complexity of Boolean computations for a spiking neuron. </title> <editor> In L. Niklasson, M. Boden, and T. Ziemke (eds.), </editor> <booktitle> Proc. 8th ICANN, </booktitle> <volume> vol. 2, </volume> <pages> pp. 585-590, </pages> <publisher> Springer, </publisher> <address> London, </address> <year> 1998. </year>
Reference-contexts: Recent theoretical results have shown that the computational power and learning capabilities for a spiking neuron with adjustable delays are significantly higher compared to neuron models that only have weights as programmable parameters <ref> [4, 7] </ref>.
Reference: [8] <author> M. Vidyasagar. </author> <title> A Theory of Learning and Generalization. Communications and Control Engineering. </title> <publisher> Springer, </publisher> <address> London, </address> <year> 1997. </year>
Reference-contexts: the number of training examples that an algorithm needs for adapting the delays of a network of spiking neurons has been shown to grow at least quadratically in the number of adjustable delays [5]. (For details concerning the relationship between VC dimension and learnability we refer the reader to Vidyasagar <ref> [8] </ref>.) This result, however, uses networks of unrestricted depth. Here, we analyze the VC dimension and pseudo dimension for networks of spiking neurons in terms of the depth (or number of layers) and the number of connections of the network.
References-found: 8

