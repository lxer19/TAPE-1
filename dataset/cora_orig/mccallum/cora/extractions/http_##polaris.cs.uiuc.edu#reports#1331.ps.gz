URL: http://polaris.cs.uiuc.edu/reports/1331.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Statement Re-ordering for DOACROSS Loops  
Author: Ding-Kai Chen Pen-Chung Yew 
Keyword: Compiler Optimization, Data Dependence, Doacross Execution, Redundant Synchronization Elimination, Statement Re-ordering.  
Address: Urbana, Illinois, 61820  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Note: Submitted to ICPP '94  This work was supported in part by the National Science Foundation under Grant Nos. NSF MIP-8920891, NSF MIP-9307910, and the U.S. Department of Energy, Grant No. DOE DE-FG02-85ER25001.  
Email: fdkchen,yewg@csrd.uiuc.edu  
Date: January 17, 1994  
Abstract: In this paper, we propose a new statement re-ordering algorithm for DOACROSS loops that overcomes some of the problems in the previous schemes. The new algorithm uses a hierarchical approach to locate strongly dependent statement groups and to order these groups considering critical dependences. A new optimization problem, dependence covering maximization, which was not discussed before is also introduced. It is shown that this optimization problem is NP-complete, and a heuristic algorithm is incorporated in our algorithm. Run-time complexity analysis is given for both algorithms. This new statement re-ordering scheme, combined with the dependence covering maximization, can be an important compiler optimization to parallelize loop structures for large scale coarse and fine grain parallelism. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Allen and K. Kennedy. </author> <title> Automatic transformation of Fortran program to vector form. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: Loop-carried dependences can be categorized as lexically forward and lexically backward. Vector and SIMD machines can handle DOACROSS loops with only lexically forward dependences <ref> [1, 15, 22, 14] </ref>. One advantage of the MIMD (multiple-instruction-multiple-data) machines is it allows loops with backward loop-carried dependences to be handled by DOACROSS execution (i.e., by delaying consecutive iterations to satisfy backward dependences).
Reference: [2] <author> R. Allen, K. Kennedy, C. Porterfield, and J. Warren. </author> <title> Conversion of control dependence to data dependence. </title> <booktitle> In 10th Annual ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 177-189, </pages> <month> Jan. </month> <year> 1983. </year>
Reference-contexts: Another kind of dependences, control dependences, occur when the result of an expression alters the course of execution, and later statements are said to be control-dependent on the expression. There are techniques that transform cross-iteration control dependences into data dependences <ref> [2] </ref>. Therefore, in this paper we focus only on data dependences. Dependences can also be classified as loop-independent or loop-carried. Basically, loop-independent dependences occur between statement instances within one iteration while loop-carried dependences occur between statement instances of different iterations.
Reference: [3] <author> R. Anderson, A. Munshi, and B. Simons. </author> <title> A scheduling problem arising from loop parallelization on mimd machines. </title> <booktitle> In 3rd Aegean Workshop on Computing, AWOC 88,Corfu,Greece, </booktitle> <pages> pages 124-133, </pages> <month> June/July </month> <year> 1988. </year>
Reference-contexts: In fact, anomalies could occur where the optimized order actually has a longer delay. A similar method was also proposed by Simons et. al. <ref> [3, 19] </ref>. In their first phase, statements are level-sorted into 6 partitions considering only the loop-independent dependences. The source and the sink statements of a loop-independent dependences are placed in different partitions. <p> of all the loop-carried dependences, which limits the strategies that can be used in the second phase. 3.2 Hierarchical Scheduling ANew Approach Because it is not adequate to preserve all forward dependences [8], and it is difficult to determine which loop-carried dependences should become forward dependences in the first phase <ref> [3] </ref>, our strategy is to postpone making the decision for each loop-carried dependence until it is necessary.
Reference: [4] <author> D.-K. Chen. </author> <title> MaxPar: An execution driven simulator for studying parallel systems. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1989. </year> <note> Also available as CSRD tech report no. 917. </note>
Reference-contexts: These graphs are all from the inner loops of several subroutines of the Eispack (a set of mathematics library routines developed at Argonne National Laboratory to solve eigenvalues and eigenvectors [20]). We randomly chose one test driver routine chtest.f and used a source-level performance analysis tool, MaxPar <ref> [4, 5] </ref>, to obtain loop-carried flow dependence information. 4 A dependence graph from each inner loop was constructed from its intermediate form by an experimental optimizing compiler developed under EPG-sim environment [17] and was augmented with the loop-carried dependence information mentioned above.
Reference: [5] <author> D.-K. Chen and P.-C. Yew. </author> <title> An empirical study of DOACROSS loops. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 620-632. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1991. </year> <note> Also available as CSRD tech report no. 1140. </note>
Reference-contexts: These graphs are all from the inner loops of several subroutines of the Eispack (a set of mathematics library routines developed at Argonne National Laboratory to solve eigenvalues and eigenvectors [20]). We randomly chose one test driver routine chtest.f and used a source-level performance analysis tool, MaxPar <ref> [4, 5] </ref>, to obtain loop-carried flow dependence information. 4 A dependence graph from each inner loop was constructed from its intermediate form by an experimental optimizing compiler developed under EPG-sim environment [17] and was augmented with the loop-carried dependence information mentioned above.
Reference: [6] <author> D.-K. Chen and P.-C. Yew. </author> <title> Redundant synchronization elimination for DOACROSS loops. </title> <booktitle> In 1994 Int'l. Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Another related optimization problem, which has been overlooked in the past, is to reduce the amount of synchronization through statement re-ordering. By making more dependences covered by others, more synchronization becomes redundant and therefore can be eliminated without affecting the correctness of the execution <ref> [12, 10, 13, 6] </ref>. This problem should be considered along with the delay minimization problem mentioned above. Although there is a trade-off between parallelism and synchronization overhead, we shall make parallelism our main optimization goal. The reduced synchronization will be achieved without compromising the parallelism available. <p> Suppose the theorem is true for n = k. We need to show that the theorem is 2 We need to construct the control path graph (CPG) to determine whether the dependence covering does occur <ref> [6] </ref>. 12 13 also true for n = k + 1. We select an edge e from the dependence graph whose source SCC src has the smallest sequence number. If there is a tie, the one whose sink SCC sink has the largest sequence number is chosen.
Reference: [7] <author> R. Cytron. </author> <title> Doacross: Beyond vectorization for multiprocessors. </title> <booktitle> In Int'l. Conf. on Parallel Processing, </booktitle> <pages> pages 836-845, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Many architectures and optimization compilers are aimed at efficient execution of parallel loops. One kind of DO loop, the DOALL loops, do not have loop-carried dependences, and it is relatively easy to obtain the desired speedup. On the other hand, it is much harder to parallelize DOACROSS loops <ref> [15, 16, 8, 7] </ref>, which contain loop-carried dependences. 1 They require the consideration of not only the scheduling and the load-balancing issues, but also the synchronization issues, which are usually much more difficult. Loop-carried dependences can be categorized as lexically forward and lexically backward.
Reference: [8] <author> R. G. Cytron. </author> <title> Compile-time Scheduling and Optimization for Asychronous Machines. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1984. </year>
Reference-contexts: Many architectures and optimization compilers are aimed at efficient execution of parallel loops. One kind of DO loop, the DOALL loops, do not have loop-carried dependences, and it is relatively easy to obtain the desired speedup. On the other hand, it is much harder to parallelize DOACROSS loops <ref> [15, 16, 8, 7] </ref>, which contain loop-carried dependences. 1 They require the consideration of not only the scheduling and the load-balancing issues, but also the synchronization issues, which are usually much more difficult. Loop-carried dependences can be categorized as lexically forward and lexically backward. <p> It is very important in DOACROSS loops to order the statements for the minimal delay (or the maximal overlap) between iterations. As has been proven by Cytron <ref> [8] </ref>, the optimization problem of statement re-ordering is NP-complete. Hence, in practice, algorithms using heuristics have to be used. Another related optimization problem, which has been overlooked in the past, is to reduce the amount of synchronization through statement re-ordering. <p> the innermost loops, it can be extended to multiply-nested loops and is discussed in Section 5. 3 Re-ordering Strategies 3.1 Previous Works It has been shown that even for a simpler dependence graph with only loop-carried dependences (hence, no restriction on its statement ordering), the delay minimization problem is NP-complete <ref> [8] </ref>. The heuristics algorithms previously proposed basically have two phases. In the first phase, the graph is partitioned and the partitions are then ordered. In the second phase, statements within each partition are ordered. <p> In the second phase, statements within each partition are ordered. The idea is to determine the partition order in the first phase such that, during the local optimization in the second phase, no loop-independent dependences will be violated. In the method proposed by Cytron <ref> [8] </ref> in the partitioning phase, it forms antichain-rows by level-sorting the dependence graph considering only forward dependences. The source and the sink of a forward dependence are placed in different partitions. <p> In summary, the two-phase algorithms try to preserve loop-independent dependences in the first phase without a proper consideration of all the loop-carried dependences, which limits the strategies that can be used in the second phase. 3.2 Hierarchical Scheduling ANew Approach Because it is not adequate to preserve all forward dependences <ref> [8] </ref>, and it is difficult to determine which loop-carried dependences should become forward dependences in the first phase [3], our strategy is to postpone making the decision for each loop-carried dependence until it is necessary. <p> In such cases, the dependence distance vectors will have more than one component (instead of the scalar d used in Equation (1) in page 4). It is shown that to determine the optimal delays for each loop level requires using the simplex algorithm to solve a linear programming problem <ref> [8] </ref>. Our problem is slightly different in that we try to determine an order to minimize the delays. Therefore, the goal is to minimize the statement distance for critical backward loop-carried dependences. They are usually indicated by small dependence distance vectors.
Reference: [9] <author> M. Garey and D. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. W.H. </title> <publisher> Freeman, </publisher> <year> 1979. </year>
Reference-contexts: A smaller number of inversions implies higher covering potential. Our optimization problem can thus be reduced to finding an order such that the number of inversions in the ordered source vector is minimized. This is equivalent to solving the following special case of the QUADRATIC ASSIGNMENT PROBLEM <ref> [9] </ref> with the cost ij being the number of inversions in (sv i ; sv j ) which is constructed from the source vectors of SCC i and j: Problem: Assume we have non-negative integer costs cost ij , 1 i; j n and distances d kl , 1 k; l <p> Proof: The dependence covering maximization problem tries to minimize the inversions in the ordered source vector, which is equivalent to solving the following subproblem of the QUADRATIC ASSIGNMENT PROBLEM <ref> [9] </ref> with the cost ij being 21 the number of inversions in (sv i ; sv j ) where sv i and sv j are source vectors of SCC i and j: INSTANCE: Non-negative integer costs cost ij , 1 i; j n and distance d kl , d kl = <p> We transform the following FEEDBACK ARC SET problem <ref> [9] </ref> to our quadratic assignment problem: INSTANCE: Directed graph G = (V; A), positive integer K j A j.
Reference: [10] <author> V. Krothapalli and P. Sadayappan. </author> <title> Removal of redundant dependences in doacross loops with constant dependences. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 281-290, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Another related optimization problem, which has been overlooked in the past, is to reduce the amount of synchronization through statement re-ordering. By making more dependences covered by others, more synchronization becomes redundant and therefore can be eliminated without affecting the correctness of the execution <ref> [12, 10, 13, 6] </ref>. This problem should be considered along with the delay minimization problem mentioned above. Although there is a trade-off between parallelism and synchronization overhead, we shall make parallelism our main optimization goal. The reduced synchronization will be achieved without compromising the parallelism available.
Reference: [11] <author> D. J. Kuck. </author> <title> The Structure of Computers and Computations, volume I, chapter 2, page 139. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year> <month> 23 </month>
Reference-contexts: Next, we find all of the strongly connected components (SCCs) in the dependence graph. They are also called -blocks in <ref> [11, 15] </ref>. After they are ordered to maximize the dependence covering, statements within each SCC are then ordered. Using the same heuristics, we try to determine, in each SCC, the strongly connected sub-structures (PSCCs), which consist of closely related statements in terms of their dependences.
Reference: [12] <author> Z. Li and W. Abu-Sufah. </author> <title> On reducing data synchronization in multiprocessed loops. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(1):105-109, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: Another related optimization problem, which has been overlooked in the past, is to reduce the amount of synchronization through statement re-ordering. By making more dependences covered by others, more synchronization becomes redundant and therefore can be eliminated without affecting the correctness of the execution <ref> [12, 10, 13, 6] </ref>. This problem should be considered along with the delay minimization problem mentioned above. Although there is a trade-off between parallelism and synchronization overhead, we shall make parallelism our main optimization goal. The reduced synchronization will be achieved without compromising the parallelism available.
Reference: [13] <author> S. Midkiff and D. Padua. </author> <title> A comparison of four synchronization optimization techniques. </title> <booktitle> In Int'l. Conf. on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 9-16, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Another related optimization problem, which has been overlooked in the past, is to reduce the amount of synchronization through statement re-ordering. By making more dependences covered by others, more synchronization becomes redundant and therefore can be eliminated without affecting the correctness of the execution <ref> [12, 10, 13, 6] </ref>. This problem should be considered along with the delay minimization problem mentioned above. Although there is a trade-off between parallelism and synchronization overhead, we shall make parallelism our main optimization goal. The reduced synchronization will be achieved without compromising the parallelism available.
Reference: [14] <author> D. Padua and M. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Comm. of ACM, </journal> <pages> pages 1184-1201, </pages> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: Loop-carried dependences can be categorized as lexically forward and lexically backward. Vector and SIMD machines can handle DOACROSS loops with only lexically forward dependences <ref> [1, 15, 22, 14] </ref>. One advantage of the MIMD (multiple-instruction-multiple-data) machines is it allows loops with backward loop-carried dependences to be handled by DOACROSS execution (i.e., by delaying consecutive iterations to satisfy backward dependences).
Reference: [15] <author> D. A. Padua. </author> <title> Multiprocessors: Discussion of Some Theoretical and Practical Problems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Univ. of Illinois at Urbana-Champaign, </institution> <month> Oct. </month> <year> 1979. </year>
Reference-contexts: Many architectures and optimization compilers are aimed at efficient execution of parallel loops. One kind of DO loop, the DOALL loops, do not have loop-carried dependences, and it is relatively easy to obtain the desired speedup. On the other hand, it is much harder to parallelize DOACROSS loops <ref> [15, 16, 8, 7] </ref>, which contain loop-carried dependences. 1 They require the consideration of not only the scheduling and the load-balancing issues, but also the synchronization issues, which are usually much more difficult. Loop-carried dependences can be categorized as lexically forward and lexically backward. <p> Loop-carried dependences can be categorized as lexically forward and lexically backward. Vector and SIMD machines can handle DOACROSS loops with only lexically forward dependences <ref> [1, 15, 22, 14] </ref>. One advantage of the MIMD (multiple-instruction-multiple-data) machines is it allows loops with backward loop-carried dependences to be handled by DOACROSS execution (i.e., by delaying consecutive iterations to satisfy backward dependences). <p> Next, we find all of the strongly connected components (SCCs) in the dependence graph. They are also called -blocks in <ref> [11, 15] </ref>. After they are ordered to maximize the dependence covering, statements within each SCC are then ordered. Using the same heuristics, we try to determine, in each SCC, the strongly connected sub-structures (PSCCs), which consist of closely related statements in terms of their dependences. <p> One of them is loop alignment <ref> [15] </ref>, which changes the distance of the dependences. Figure 8 gives a loop alignment example. The original dependences among statement instances are shown in Figure 8a, while the dependences of the aligned loop are in Figure 8b. In the original loop, no statement re-ordering can improve the loop-level parallelism.
Reference: [16] <author> D. A. Padua, D. J. Kuck, and D. H. Lawrie. </author> <title> High-speed multiprocessors and compilation techniques. </title> <journal> IEEE Trans. on Computers, </journal> <volume> c-29(9):763-776, </volume> <month> September </month> <year> 1980. </year>
Reference-contexts: Many architectures and optimization compilers are aimed at efficient execution of parallel loops. One kind of DO loop, the DOALL loops, do not have loop-carried dependences, and it is relatively easy to obtain the desired speedup. On the other hand, it is much harder to parallelize DOACROSS loops <ref> [15, 16, 8, 7] </ref>, which contain loop-carried dependences. 1 They require the consideration of not only the scheduling and the load-balancing issues, but also the synchronization issues, which are usually much more difficult. Loop-carried dependences can be categorized as lexically forward and lexically backward.
Reference: [17] <author> D. K. Poulsen and P.-C. Yew. </author> <title> Execution-driven tools for parallel simulation of parallel architectures and applications. </title> <booktitle> In Supercomputing '91, </booktitle> <pages> pages 860-869, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: We randomly chose one test driver routine chtest.f and used a source-level performance analysis tool, MaxPar [4, 5], to obtain loop-carried flow dependence information. 4 A dependence graph from each inner loop was constructed from its intermediate form by an experimental optimizing compiler developed under EPG-sim environment <ref> [17] </ref> and was augmented with the loop-carried dependence information mentioned above. Finally, the dependence graphs were used as inputs to the two algorithms described in Section 3.1 and to the new hierarchical scheduling algorithm. The preliminary results for improved parallelism are shown in Table 1.
Reference: [18] <author> E. Reingold, J. Nievergelt, and N. Deo. </author> <title> Combinatorial Algorithms: Theory and Practice. </title> <publisher> Prentice-Hall Inc., </publisher> <year> 1977. </year>
Reference-contexts: A strongly connected component (SCC) in a directed graph is a maximal subgraph such that each node in this subgraph has a directed path to any other node in the same subgraph <ref> [18] </ref>. 2.2 DOACROSS Execution DOACROSS execution for the loops with loop-carried dependences is achieved by requiring the instances of the sink statements in later iterations to wait for the completion of the source statements in earlier iterations, usually with explicit synchronization between source and sink statements.
Reference: [19] <author> B. Simons and A. Munshi. </author> <title> Scheduling loops on processors: Algorithms and complexity. </title> <journal> SIAM J. of Computing, </journal> <volume> 19(4) </volume> <pages> 728-741, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: In fact, anomalies could occur where the optimized order actually has a longer delay. A similar method was also proposed by Simons et. al. <ref> [3, 19] </ref>. In their first phase, statements are level-sorted into 6 partitions considering only the loop-independent dependences. The source and the sink statements of a loop-independent dependences are placed in different partitions.
Reference: [20] <author> B. T. Smith, J. M. Boyle, J. J. Dongarra, B. S. Garbow, Y. Ikebe, V. C. Klema, and C. B. Moler. </author> <title> Matrix eigensystem routines - eispack guide. </title> <address> Heidelberg, </address> <year> 1976. </year>
Reference-contexts: These graphs are all from the inner loops of several subroutines of the Eispack (a set of mathematics library routines developed at Argonne National Laboratory to solve eigenvalues and eigenvectors <ref> [20] </ref>).
Reference: [21] <author> R.E. Tarjan. </author> <title> Depth first search and linear graph algorithms. </title> <journal> SIAM J. Comupting, </journal> <volume> 1(2), </volume> <year> 1972. </year>
Reference-contexts: They 8 are ordered and the process proceeds recursively. 3.2.1 PSCC Identification PSCCs are strongly connected subgraphs within a strongly connected graph such that they partition the original graph. In the main algorithm, we use Tarjan's <ref> [21] </ref> well-known algorithm to determine SCCs. However, given a strongly connected graph G, we cannot use the same algorithm to find PSCC because we will obtain the same graph as G.
Reference: [22] <author> M. J. Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1982. </year>
Reference-contexts: Loop-carried dependences can be categorized as lexically forward and lexically backward. Vector and SIMD machines can handle DOACROSS loops with only lexically forward dependences <ref> [1, 15, 22, 14] </ref>. One advantage of the MIMD (multiple-instruction-multiple-data) machines is it allows loops with backward loop-carried dependences to be handled by DOACROSS execution (i.e., by delaying consecutive iterations to satisfy backward dependences).
References-found: 22

