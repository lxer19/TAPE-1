URL: ftp://ftp.cse.ucsc.edu/pub/ml/ucsc-crl-94-16.ps.Z
Refering-URL: http://www.cse.ucsc.edu/~mark/ftp-ml-root/ExpertBibDir/expbib.html
Root-URL: http://www.cse.ucsc.edu
Title: Exponentiated Gradient Versus Gradient Descent for Linear Predictors  
Author: Jyrki Kivinen Manfred K. Warmuth 
Address: Santa Cruz, CA 95064 USA  
Affiliation: Baskin Center for Computer Engineering Information Sciences University of California, Santa Cruz  
Date: June 21, 1994  
Pubnum: UCSC-CRL-94-16  
Abstract: We consider two algorithm for on-line prediction based on a linear model. The algorithms are the well-known Gradient Descent (GD) algorithm and a new algorithm, which we call EG . They both maintain a weight vector using simple updates. For the GD algorithm, the update is based on subtracting the gradient of the squared error made on a prediction. The EG algorithm uses the components of the gradient in the exponents of factors that are used in updating the weight vector multiplicatively. We present worst-case loss bounds for EG and compare them to previously known bounds for the GD algorithm. The bounds suggest that the losses of the algorithms are in general incomparable, but EG has a much smaller loss if only a few components of the input are relevant for the predictions. We have performed experiments, which show that our worst-case upper bounds are quite tight already on simple artificial data. 
Abstract-found: 1
Intro-found: 1
Reference: [Ama94] <author> S. Amari. </author> <title> Information geometry of the EM and em algorithms for neural networks. </title> <type> Technical Report METR 94-4, </type> <institution> University of Tokyo, </institution> <address> Tokyo, </address> <year> 1994. </year> <note> To appear in Neural Networks. </note>
Reference-contexts: This use of a distance measure for obtaining worst-case loss bounds was pioneered by Littlestone's analysis of Winnow [Lit89b], which also employs a variant of the relative entropy. Amari's <ref> [Ama94] </ref> approach in using the relative entropy for deriving neural network learning algorithms is similar to the first use 3 we have here for the distance measure. <p> For small values of , the value p is slightly less that L (y; s x), so the weight vector is made to make only a small corrective movement. This approach to updating a weight vector is similar to the methods introduced by Amari <ref> [Ama94] </ref> for more general neural network learning problems. It will soon become apparent that our algorithms do not actually solve the problem of minimizing U (w). They are based on making a simple approximation that results in a closed formula for w.
Reference: [BGV92] <author> B. E. Boser, I. M. Guyon, and V. N. Vapnik. </author> <title> A training algorithm for optimal margin classifiers. </title> <booktitle> In Proc. 5th Workshop on Computational Learning Theory, </booktitle> <pages> pages 144-152. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: Then a linear prediction algorithm could actually use a linear combination of the basis functions as its predictor. As an example, we might include all the O (N q ) products of up to q original input variables <ref> [BGV92] </ref>. Assuming that the input variables are in the range [1; 1], this does not increase the L 1 norms of the instances. <p> negative. 49 target and expanded instances. 9.6 Expanding the instances Our next experiment illustrates the use of linear function learning to learn nonlinear target functions by the means of expanding the instances in such a way that the target function becomes linear for the expanded inputs (see Boser et al. <ref> [BGV92] </ref>). For example, let B (x; q), for q = 1; 2; 3; : : :, be a vector that has as its components all monomials over the variables x i , up to degree k.
Reference: [CBFH + 93] <author> N. Cesa-Bianchi, Y. Freund, D. P. Helmbold, D. Haussler, R. E. Schapire, and M. K. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> In Proc. 25th Symposium on the Theory of Computing, </booktitle> <pages> pages 382-391. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: More recently, there has been some work on using an arbitrary finite comparison class P = f p 1 ; : : : ; p N g. Predictors from a finite class are often called experts <ref> [CBFH + 93] </ref>. Note that a finite comparison class can be considered as a comparison class with a very restricted set of linear predictors. <p> Such bounds are even tighter than those of the form (1.2). However, for the absolute loss such bounds were not obtained. Vovk [Vov90] and Littlestone and Warmuth [LW94] had bounds of the form (1.1) for the absolute loss. Later Cesa-Bianchi et al. <ref> [CBFH + 93] </ref> showed how these bounds could be improved to the form (1.2) by a careful choice of certain parameters in Vovk's algorithm. <p> However, if such estimates are not known before the trial sequence begins, it is in some situations possible to use an iterative method, commonly known as the doubling technique <ref> [CBFH + 93, CBLW93] </ref>, for obtaining increasingly accurate estimates as the trial sequence proceeds and modifying the 20 learning rate accordingly. This leads to bounds of the form (5.3), but with slightly worse constant coefficients. <p> More sophisticated conversion methods are given by Cesa-Bianchi et al. <ref> [CBFH + 93] </ref> and Littlestone [Lit89a]. 9 Experimental and theoretical comparison of the algorithms 9.1 Comparison of the worst-case upper bounds In this subsection we compare the worst-case upper bounds given for GD and EG in Theorems 5.3 and 5.9. <p> In practice, these quantities are usually not known, and some other methods must be used to obtain a good learning rate. If only one of the parameters is unknown, there are strategies for guessing its value with increasing accuracy <ref> [CBLW93, CBFH + 93] </ref>. These strategies sometimes lead to loss bounds of the form (5.3), but with the coefficient c 1 and c 2 somewhat larger than the ones obtained in Theorems 5.3 and 5.9 when good values of the parameters are known.
Reference: [CBLW93] <author> N. Cesa-Bianchi, P. Long, and M. Warmuth. </author> <title> Worst-case quadratic loss bounds for on-line prediction of linear functions by gradient descent. </title> <type> Technical Report UCSC-CRL-93-36, </type> <institution> University of California, Santa Cruz, </institution> <year> 1993. </year> <note> An extended abstract appeared in COLT '93. </note>
Reference-contexts: We have succeeded in the proofs only for the square loss (y t ^y t ) 2 , although the basic ideas of this paper can be phrased for general loss functions. The immediate predecessors of this work are the papers by Cesa-Bianchi et al. <ref> [CBLW93] </ref> and Littlestone et al. [LLW91]. Cesa-Bianchi et al. consider the Gradient Descent algorithm, or the GD algorithm, for linear predictions. This algorithm is also known as the Widrow-Hoff algorithm and the Least Mean Squares algorithm. It is also one of the main algorithms used in this paper. <p> We now discuss the actual worst-case bounds we can obtain for the GD and EG algorithms. For the GD algorithm, the bounds we cite were already given by Cesa-Bianchi et al. <ref> [CBLW93] </ref>. These include bounds of both the forms (1.1) and (1.2). For the EG algorithm, we give new bounds that are strictly better than those obtained by Littlestone et al. [LLW91] for their algorithm. <p> For the GD algorithm, setting the learning rate t suitably results in the bound Loss (GD; S) 2 Loss (u; S) + jjujj 2 2 max jjx t jj 2 that holds for all vectors u 2 R N and all trial sequences S <ref> [CBLW93] </ref>. To make the coefficient in front of Loss (u; S) equal to 1 and thus obtain a bound of the form (1.2), the algorithm needs before the first trial reasonably good estimates of some characteristics of the whole trial sequence. <p> This leads to a bound that is similar to (1.3) but has slightly larger constant coefficients <ref> [CBLW93] </ref>. For the EG algorithm, it is necessary to give as a parameter an upper bound U for the L 1 norm of the vectors u of the comparison class. <p> The algorithm GD (s; ) has many names, including the Widrow-Hoff algorithm and the Least Mean Square (LMS) algorithm <ref> [CBLW93, WS85] </ref>. The update for GD (s; ) is simply w t+1 = w t 2 t (^y t y t )x t : (3:1) The start vector s of the algorithm can be arbitrary. Typically one would choose s = 0. <p> The method is an abstraction of the proof method employed by Littlestone [Lit89b, Lit91] and others <ref> [LLW91, CBLW93] </ref>. In the subsequent subsections, we show how this basic idea can be applied to the specific algorithms introduced in Section 3. We have succeeded in this application only for the square loss function, but we hope it could also be applicable to other loss functions. <p> However, if such estimates are not known before the trial sequence begins, it is in some situations possible to use an iterative method, commonly known as the doubling technique <ref> [CBFH + 93, CBLW93] </ref>, for obtaining increasingly accurate estimates as the trial sequence proceeds and modifying the 20 learning rate accordingly. This leads to bounds of the form (5.3), but with slightly worse constant coefficients. <p> The weaker bounds can be achieved without any additional knowledge. 5.2 Worst-case loss bounds for GD In this subsection we give a streamlined version of the worst-case analysis of the GD algorithm. The analysis was originally presented by Cesa-Bianchi et al. <ref> [CBLW93] </ref>. We start by bounding the loss of the algorithm at a single trial in terms of the loss of a comparison vector u at that trial and the progress of the algorithm towards u. <p> Then either Loss (u + ; S) = K or Loss (u ; S) = K. Since Loss L (A; S) y 2 , we get the stated bound. 2 The special case p = q = 2 of Theorem 6.1 was noted already by Cesa-Bianchi et al. <ref> [CBLW93] </ref>. The lower bound given in Theorem 6.1 for this case coincides with the upper bound given in Theorem 5.3 for the GD algorithm. Hence, the GD algorithm has the best obtainable worst case loss bound. <p> Hence, the GD algorithm has the best obtainable worst case loss bound. Note that in Theorem 6.1, K cannot be made arbitrarily large without also making the absolute value of the outcome arbitrarily large. The following lower bound, also from Cesa-Bianchi et al. <ref> [CBLW93] </ref>, shows that if the number N of dimensions can be arbitrarily large, then again the loss bound for GD is the best possible, even if range of the outcomes is restricted. <p> EG algorithm, which we call the EGM algorithm, has the update rule w t+1;i = w t;i r t;i =( P j w t;j r t;j ) where r t;i = exp (2 t ( ^ y y t ) M t;i ) : (7:3) It has been previously shown <ref> [CBLW93, SW94] </ref> that the GDM algorithm has a loss bound similar to that of GD. Recall that the norm jjAjj 2 for a matrix A is defined as jjAjj 2 = max f jjAxjj 2 j jjxjj 2 = 1 g. <p> In practice, these quantities are usually not known, and some other methods must be used to obtain a good learning rate. If only one of the parameters is unknown, there are strategies for guessing its value with increasing accuracy <ref> [CBLW93, CBFH + 93] </ref>. These strategies sometimes lead to loss bounds of the form (5.3), but with the coefficient c 1 and c 2 somewhat larger than the ones obtained in Theorems 5.3 and 5.9 when good values of the parameters are known.
Reference: [CLR90] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: As noted in Section 4, the algorithms can be motivated by applying a distance measure to the weight vectors maintained by the algorithms. These distance measures will also be useful in proving worst-case loss bounds. They have a role similar to that of potential functions in amortized algorithm analysis <ref> [CLR90] </ref>. <p> Different distance functions lead to radically different algorithms. This framework has been adapted recently [HSW94] to an unsupervised setting. 3. The distance function also serves in a second role as a potential function in proving worst-case loss bounds by amortized analysis <ref> [CLR90] </ref>. The bounds are first expressed as a function of the learning rate and various norms of the instances and target vectors, as well as the loss of the target vector. Good loss bounds are then obtained by carefully tuning the learning rate.
Reference: [DLR77] <author> A. P. Dempster, N. M. Laird, and D. B Rubin. </author> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B39:1-38, </volume> <year> 1977. </year>
Reference-contexts: It has also been noticed that in applying the exponentiated gradient update to a certain unsupervised learning problem [HSW94], the approximation given here leads to a generalization of the Expectation Maximization algorithm <ref> [DLR77] </ref>. Note that the update rule (3.4) maintains the invariant P N i=1 w t;j = 1. However, it may make some of the weights w t+1;i zero or negative.
Reference: [Hay91] <author> S. Haykin. </author> <title> Adaptive Filter Theory. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1991. </year> <note> (Second edition.) </note>
Reference-contexts: However, the bounds always hold. This is in contrast to more common approaches where statistical assumptions about the distribution of the instances and the dependence of the outcomes on the instances are used in order to derive probabilistic loss bounds for the prediction algorithm <ref> [WS85, Hay91] </ref>. The research reported in this paper was inspired by Littlestone [Lit89b, Lit88], who proved worst-case bounds for the case when the comparison class consists of Boolean monomials, or more generally linear threshold functions.
Reference: [Hay93] <author> S. Haykin. </author> <title> Neural Networks: a Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: The distance term in the minimized function U is also somewhat analogous to regularization terms used in neural network algorithms to avoid overfitting <ref> [Hay93] </ref>. We now discuss the actual worst-case bounds we can obtain for the GD and EG algorithms. For the GD algorithm, the bounds we cite were already given by Cesa-Bianchi et al. [CBLW93]. These include bounds of both the forms (1.1) and (1.2).
Reference: [Hin86] <author> G. E. Hinton. </author> <title> Learning distributed representations of concepts. </title> <booktitle> In Proc. 8th Annual Conference of the Cognitive Science Society, </booktitle> <address> Amherst, MA, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: This class of algorithms also includes a basic variant of weight decay, where an additional jjw t jj 2 2 error term is used as a penalty for large weights <ref> [Hin86] </ref>. 45 upper bounds, for instances x t 2 f 1; 1 g 100 and target u = (1; 1; 1; 0; : : :; 0), and noise rate 0:2. upper bounds, with u = (1; 0; : : :; 0) as the target, and rows of a 256 by 256
Reference: [HSW94] <author> D. Helmbold, Y. Singer, and M. K. Warmuth. </author> <title> Maximum-likelihood estimation of mixture proportions using penalty functions. </title> <type> Unpublished manuscript, </type> <year> 1994. </year>
Reference-contexts: However, we shall see in Subsection 4.4 that the update rule (3.4) also has another motivation that is not based on approximating (3.2). It has also been noticed that in applying the exponentiated gradient update to a certain unsupervised learning problem <ref> [HSW94] </ref>, the approximation given here leads to a generalization of the Expectation Maximization algorithm [DLR77]. Note that the update rule (3.4) maintains the invariant P N i=1 w t;j = 1. However, it may make some of the weights w t+1;i zero or negative. <p> In a slightly different scenario it has been noticed that the 2 measure can also be used to motivate a generalization of the EM optimization method <ref> [HSW94] </ref>. In summary, we have seen that there are two different ways of arriving at the same approximated EG algorithm. First, one can approximate the exponential function in the update rule of EG. <p> We introduce a common framework for deriving learning algorithms based on the tradeoff between the distance traveled from the current weight vector and a loss function. Different distance functions lead to radically different algorithms. This framework has been adapted recently <ref> [HSW94] </ref> to an unsupervised setting. 3. The distance function also serves in a second role as a potential function in proving worst-case loss bounds by amortized analysis [CLR90].
Reference: [HW92] <author> D. P. Helmbold and M. K. Warmuth. </author> <title> Some weak learning results. </title> <booktitle> In Proc. 5th Workshop on Computational Learning Theory, </booktitle> <pages> pages 399-412. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: If the start vector of the algorithm is equal to u and the learning rate is positive, then the expected loss of the second hypothesis w 2 obviously is higher than that of the initial hypothesis w 1 = u. 40 We conclude this section by presenting a simple method <ref> [HW92] </ref> that can be used for proving expected instantaneous loss bounds for all algorithms and distributions.
Reference: [KSS92] <author> M. J. Kearns, R. E. Schapire, and L. M. Sellie. </author> <title> Toward efficient agnostic learning. </title> <booktitle> In Proc. 5th Workshop on Computational Learning Theory, </booktitle> <pages> pages 341-352. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year> <month> 53 </month>
Reference-contexts: To set a reasonable goal, we measure the performance of the algorithm against the performances of predictors from some fixed comparison class P . (The comparison class is analogous to the touchstone class of the agnostic PAC model of learning <ref> [KSS92] </ref>.) The algorithm is required to perform well if at least one predictor from the comparison class performs well.
Reference: [KW93] <author> J. Kivinen and M. Warmuth. </author> <title> Using experts for predicting continuous outcomes. </title> <booktitle> European Conference on Computational Learning Theory, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: Some of these results assumed that the outcomes y t must be in f 0; 1 g and were generalized for continuous-valued outcomes y t 2 [0; 1] by Kivinen and Warmuth <ref> [KW93] </ref>. In this paper, we consider proving bounds of the form (1.2) for a comparison class of general linear predictors, rather than only predictors that choose one of the N components of the instance. <p> a simpler situation, where the learner is not trying to learn a linear function but merely to pick out the best single component of the instances for predicting the outcomes, we have been able to use a similar approach to prove bounds for a very general class of loss functions <ref> [KW93] </ref>. As noted in Section 4, the algorithms can be motivated by applying a distance measure to the weight vectors maintained by the algorithms. These distance measures will also be useful in proving worst-case loss bounds.
Reference: [Lit88] <author> N. Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: The research reported in this paper was inspired by Littlestone <ref> [Lit89b, Lit88] </ref>, who proved worst-case bounds for the case when the comparison class consists of Boolean monomials, or more generally linear threshold functions. <p> This family includes the Perceptron algorithm for thresholded linear functions, the GD algorithm for linear functions, the standard back-propagation algorithm for multilayer sigmoid networks, and the Linear Least Squares algorithm for fitting a line to data points. The new family includes, respectively, the Winnow algorithm <ref> [Lit88] </ref>, the EG algorithm, the exponentiated back-propagation algorithm, and an algorithm for fitting a line to data points so that the relative entropy of the coefficient vector is minimized. The new family uses a new bias, which 52 favors sparse weight vectors.
Reference: [Lit89a] <author> N. Littlestone. </author> <title> From on-line to batch learning. </title> <booktitle> In Proc. 2nd Workshop on Computational Learning Theory, </booktitle> <pages> pages 269-284, </pages> <address> San Mateo, CA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: More sophisticated conversion methods are given by Cesa-Bianchi et al. [CBFH + 93] and Littlestone <ref> [Lit89a] </ref>. 9 Experimental and theoretical comparison of the algorithms 9.1 Comparison of the worst-case upper bounds In this subsection we compare the worst-case upper bounds given for GD and EG in Theorems 5.3 and 5.9.
Reference: [Lit89b] <author> N. Littlestone. </author> <title> Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. </title> <type> PhD thesis, Technical Report UCSC-CRL-89-11, </type> <institution> University of California, Santa Cruz, </institution> <year> 1989. </year>
Reference-contexts: The research reported in this paper was inspired by Littlestone <ref> [Lit89b, Lit88] </ref>, who proved worst-case bounds for the case when the comparison class consists of Boolean monomials, or more generally linear threshold functions. <p> This use of a distance measure for obtaining worst-case loss bounds was pioneered by Littlestone's analysis of Winnow <ref> [Lit89b] </ref>, which also employs a variant of the relative entropy. Amari's [Ama94] approach in using the relative entropy for deriving neural network learning algorithms is similar to the first use 3 we have here for the distance measure. <p> The method is an abstraction of the proof method employed by Littlestone <ref> [Lit89b, Lit91] </ref> and others [LLW91, CBLW93]. In the subsequent subsections, we show how this basic idea can be applied to the specific algorithms introduced in Section 3.
Reference: [Lit91] <author> N. Littlestone. </author> <title> Redundant noisy attributes, attribute errors, and linear threshold learning using Winnow. </title> <booktitle> In Proc. 4th Workshop on Computational Learning Theory, </booktitle> <pages> pages 147-156, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The method is an abstraction of the proof method employed by Littlestone <ref> [Lit89b, Lit91] </ref> and others [LLW91, CBLW93]. In the subsequent subsections, we show how this basic idea can be applied to the specific algorithms introduced in Section 3.
Reference: [LLW91] <author> N. Littlestone, P. M. Long, and M. K. Warmuth. </author> <title> On-line learning of linear functions. </title> <booktitle> In Proc. 23rd Symposium on the Theory of Computing, </booktitle> <pages> pages 465-475. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1991. </year> <note> To appear in Journal of Computational Complexity. </note>
Reference-contexts: The immediate predecessors of this work are the papers by Cesa-Bianchi et al. [CBLW93] and Littlestone et al. <ref> [LLW91] </ref>. Cesa-Bianchi et al. consider the Gradient Descent algorithm, or the GD algorithm, for linear predictions. This algorithm is also known as the Widrow-Hoff algorithm and the Least Mean Squares algorithm. It is also one of the main algorithms used in this paper. <p> We also introduce a new on-line prediction algorithm, which we call the Exponentiated Gradient algorithm, or the EG, algorithm. The EG algorithm is closely related to the algorithm given by Littlestone et al. <ref> [LLW91] </ref>. The EG algorithm also has a weight vector w t and predicts with ^y t = w t x t . <p> For the GD algorithm, the bounds we cite were already given by Cesa-Bianchi et al. [CBLW93]. These include bounds of both the forms (1.1) and (1.2). For the EG algorithm, we give new bounds that are strictly better than those obtained by Littlestone et al. <ref> [LLW91] </ref> for their algorithm. In particular, we also have bounds of the form (1.2), whereas Littlestone et al. [LLW91] had only bounds of the form (1.1). <p> These include bounds of both the forms (1.1) and (1.2). For the EG algorithm, we give new bounds that are strictly better than those obtained by Littlestone et al. <ref> [LLW91] </ref> for their algorithm. In particular, we also have bounds of the form (1.2), whereas Littlestone et al. [LLW91] had only bounds of the form (1.1). The importance of considering both the algorithms GD and EG comes from the fact that for these algorithms, the constants hidden by the notation in (1.1) and (1.2) are quite different. <p> For other examples of reductions of this type, see Littlestone et al. <ref> [LLW91] </ref>. 11 Algorithm EG L (U; (s + ; s ); ) Parameters: L: a loss function from R fi R to [0; 1), U : the total weight of the weight vectors, s + and s : a pair of start vectors in [0; 1] N , with P N <p> The method is an abstraction of the proof method employed by Littlestone [Lit89b, Lit91] and others <ref> [LLW91, CBLW93] </ref>. In the subsequent subsections, we show how this basic idea can be applied to the specific algorithms introduced in Section 3. We have succeeded in this application only for the square loss function, but we hope it could also be applicable to other loss functions. <p> Similar bounds were earlier proven by Littlestone et al. <ref> [LLW91] </ref> for their algorithm, which is related to ours but does not have an analogous derivation. Our bounds are lower than those of Littlestone et al. In particular, we have bounds of the form (5.3), which seem unobtainable for the algorithm of Littlestone et al. <p> We consider the upper bounds for the simpler EG algorithm, from which the bounds for EG are obtained via a reduction. If we were able to improve the bounds for EG, then an improvement for EG would automatically follow. The following result of Littlestone et al. <ref> [LLW91] </ref> shows that in the case Loss (u; S) = 0, a factor ln N in the loss of the algorithm cannot be avoided. For simplicity, we consider only the case x t 2 [0; 1] N . <p> We could easily generalize result for the more general algorithm EG when it is applied to matrices. In the noise-free case K = 0, similar results were given by Littlestone et al. <ref> [LLW91] </ref>. The reduction could also be applied to the GD algorithm to obtain a result almost equivalent with Theorem 7.1. The difference is that the reduction technique, illustrated in the following proof, requires the learning rate to be the same at all trials. <p> More generally, it can be shown that no algorithm that uses weight vectors of the form w t+1 = P t a t x t can have smaller loss in this situation <ref> [LLW91] </ref>.
Reference: [Lue84] <author> D. G. Luenberger. </author> <title> Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1984. </year>
Reference-contexts: at s x instead of at w x to obtain the more easily implementable update rule w i = s i @L (y; z) (x i avg (x)) : (4:3) The algorithm GP L that uses the update rule (4.3) is known in the literature as the Gradient Projection algorithm <ref> [Lue84] </ref>. Note that the algorithm EG L maintains the additional condition that the weights are positive.
Reference: [LW94] <author> N. Littlestone and M. K. Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108(2) </volume> <pages> 212-261, </pages> <year> 1994. </year>
Reference-contexts: Such bounds are even tighter than those of the form (1.2). However, for the absolute loss such bounds were not obtained. Vovk [Vov90] and Littlestone and Warmuth <ref> [LW94] </ref> had bounds of the form (1.1) for the absolute loss. Later Cesa-Bianchi et al. [CBFH + 93] showed how these bounds could be improved to the form (1.2) by a careful choice of certain parameters in Vovk's algorithm.
Reference: [Roy63] <author> H. Royden. </author> <title> Real Analysis. </title> <publisher> Macmillan, </publisher> <address> New York, NY, </address> <year> 1963. </year>
Reference-contexts: Note that L p and L q are dual norms if 1=p + 1=q = 1 <ref> [Roy63] </ref>. Hence, the L 1 norm used for the comparison vectors and the L 1 norm used for the instances in the bounds for the EG algorithm are dual. <p> If the norms L p and L q are dual, then the Cauchy-Schwartz Inequality can be generalized to show that jjujj p U and jjxjj q X together imply ju xj U X <ref> [Roy63] </ref>. Theorem 6.1: Let p; q 2 R + [ f 1 g. Let A be an arbitrary on-line prediction algorithm, and let K, U , and X be arbitrary positive reals.
Reference: [SW94] <author> R. E. Schapire and M. K. Warmuth. </author> <title> On the worst-case analysis of temporal-difference learing algorithms. </title> <booktitle> In Proc. 11th International Conference on Machine Learning, </booktitle> <year> 1994. </year>
Reference-contexts: EG algorithm, which we call the EGM algorithm, has the update rule w t+1;i = w t;i r t;i =( P j w t;j r t;j ) where r t;i = exp (2 t ( ^ y y t ) M t;i ) : (7:3) It has been previously shown <ref> [CBLW93, SW94] </ref> that the GDM algorithm has a loss bound similar to that of GD. Recall that the norm jjAjj 2 for a matrix A is defined as jjAjj 2 = max f jjAxjj 2 j jjxjj 2 = 1 g. <p> The upper bound of Theorem 7.1 can be shown to be tight <ref> [SW94] </ref>. We now give a similar result for the EGM algorithm. The proof is based on a reduction that allows us to apply directly the upper bound given in Theorem 5.8 for the EG algorithm.
Reference: [Vov90] <author> V. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proc. 3rd Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: The number of dimensions is now the number of experts, i.e., the size N of the original comparison class. Vovk <ref> [Vov90] </ref> proved that for a large class of loss functions, a simple algorithm achieves bounds of the form Loss L (A; S) inf u2U Loss L (u; S) + c log N , where the constant c depends only on the loss function. <p> Such bounds are even tighter than those of the form (1.2). However, for the absolute loss such bounds were not obtained. Vovk <ref> [Vov90] </ref> and Littlestone and Warmuth [LW94] had bounds of the form (1.1) for the absolute loss. Later Cesa-Bianchi et al. [CBFH + 93] showed how these bounds could be improved to the form (1.2) by a careful choice of certain parameters in Vovk's algorithm.
Reference: [WS85] <author> B. Widrow and S. Stearns. </author> <title> Adaptive Signal Processing. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1985. </year> <month> 54 </month>
Reference-contexts: However, the bounds always hold. This is in contrast to more common approaches where statistical assumptions about the distribution of the instances and the dependence of the outcomes on the instances are used in order to derive probabilistic loss bounds for the prediction algorithm <ref> [WS85, Hay91] </ref>. The research reported in this paper was inspired by Littlestone [Lit89b, Lit88], who proved worst-case bounds for the case when the comparison class consists of Boolean monomials, or more generally linear threshold functions. <p> The algorithm GD (s; ) has many names, including the Widrow-Hoff algorithm and the Least Mean Square (LMS) algorithm <ref> [CBLW93, WS85] </ref>. The update for GD (s; ) is simply w t+1 = w t 2 t (^y t y t )x t : (3:1) The start vector s of the algorithm can be arbitrary. Typically one would choose s = 0.
References-found: 24

