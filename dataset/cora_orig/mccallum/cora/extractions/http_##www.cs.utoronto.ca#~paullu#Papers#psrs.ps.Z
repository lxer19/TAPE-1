URL: http://www.cs.utoronto.ca/~paullu/Papers/psrs.ps.Z
Refering-URL: http://www.umiacs.umd.edu/research/EXPAR/papers/3549/node10.html
Root-URL: 
Title: On the Versatility of Parallel Sorting by Regular Sampling  
Author: Xiaobo Li Paul Lu Jonathan Schaeffer John Shillington Pok Sze Wong Hanmao Shi 
Address: Edmonton, Alberta Canada T6G 2H1  Waterloo, Ontario Canada N2L 3G1  
Affiliation: Department of Computing Science University of Alberta  Department of Computer Science University of Waterloo  
Abstract: Parallel sorting algorithms have already been proposed for a variety of multiple instruction streams, multiple data streams (MIMD) architectures. These algorithms often exploit the strengths of the particular machine to achieve high performance. In many cases, however, the existing algorithms cannot achieve comparable performance on other architectures. Parallel Sorting by Regular Sampling (PSRS) is an algorithm that is suitable for a diverse range of MIMD architectures. It has good load balancing properties, modest communication needs and good memory locality of reference. If there are no duplicate keys, PSRS guarantees to balance the work among the processors within a factor of two of optimal in theory, regardless of the data value distribution, and within a few percent of optimal in practice. This paper presents new theoretical and empirical results for PSRS. The theoretical analysis of PSRS is extended to include a lower bound and a tighter upper bound on the work done by a processor. The effect of duplicate keys is addressed analytically and shown that, in practice, it is not a concern. In addition, the issues of oversampling and undersampling the data are introduced and analyzed. Empirically, PSRS has been implemented on four diverse MIMD architectures and a network of workstations. On all of the machines, for both random and application-generated data sets, the algorithm achieves good results. PSRS is not necessarily the best parallel sorting algorithm for any specific machine. But PSRS will achieve good performance on a wide spectrum of machines before any strengths of the architecture are exploited. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Abali, F. Ozguner, and A. Bataineh. </author> <title> Load Balanced Sort on Hypercube Multiprocessors. </title> <booktitle> 5th Distributed Memory Computing Conference, </booktitle> <pages> pages 230-236, </pages> <year> 1990. </year>
Reference-contexts: Given the extensive literature on parallel sorting, it is not surprising that PSRS is similar to other algorithms. For example, PSRS is similar to the balanced bin sort [22] and 4 5 the load balanced sort <ref> [1] </ref>. However, both have different approaches to load balancing than PSRS. The balanced bin sort's heuristic is also based on sampling, and results in an upper bound of 3n p items that must be merged by a single processor. The bound is inferior to PSRS'.
Reference: [2] <author> R.C. Dubes and A.K. Jain. </author> <title> Clustering Techniques: The User's Dilemma. </title> <journal> Pattern Recognition, </journal> <volume> 8 </volume> <pages> 247-260, </pages> <year> 1976. </year>
Reference-contexts: Since the 2n p term dominates the bound, duplicates do not cause problems until individual keys are repeated O ( n p ) times. This point is illustrated empirically by running PSRS on some application-generated data (the IMOX data set, utilized extensively in image processing and pattern recognition <ref> [2] </ref>) that contains a high percentage of duplicate items. 3. New empirical results. The original implementation of the algorithm was on a virtual memory, demand-paged Myrias SPS-2 with 64 processors. It achieved a 44-fold 1 It is assumed that there is no computationally inexpensive way of removing duplicates. <p> Most application-generated data, however, does not fit into this simple model. To further illustrate the versatility of PSRS, the algorithm has been tested on a nonrandom, nonuniform value distribution data set from a real application area. This section reports on the results obtained using the IMOX data set <ref> [2] </ref>, used extensively in image processing and pattern recognition. A pattern matrix, derived from the Munson hand-printed Fortran character set (available from the IEEE Computer Society), consists of 192 binary-coded (24 by 24) handwritten characters from several authors.
Reference: [3] <author> K. Birman et al. </author> <title> The ISIS System Manual, Version 2.1. </title> <type> Technical Report TR-91-1185, </type> <institution> Computer Science Department, Cornell University, </institution> <year> 1991. </year>
Reference-contexts: For sufficiently large problems, the program still deadlocks. To avoid this, the data is packaged into smaller pieces and is then sent using multiple synchronized messages. Obviously, the extra synchronization adversely affects performance. The second implementation uses the ISIS package from Cornell (version 2.1) <ref> [3] </ref>. It is claimed that ISIS provides communication performance as fast as remote procedure calls. However, our version of ISIS requires 30% of each packet as overhead for control information, thus increasing the number of packets needed and decreasing communication throughput performance.
Reference: [4] <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1986. </year>
Reference: [5] <author> R.S. Francis and I.D. Mathieson. </author> <title> A Benchmark Parallel Sort for Shared Memory Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(12) </volume> <pages> 1619-1626, </pages> <year> 1988. </year>
Reference: [6] <author> B.C. Gorda, K.H. Warren, and E.D. Brooks III. </author> <title> Programming in PCP. </title> <type> Technical Report UCRL-MA-107029, </type> <institution> Lawrence Livermore National Laboratory, </institution> <year> 1988. </year>
Reference-contexts: Since accessing shared memory is more expensive than local memory, the memory access intensive operations of sequential sorting and merging are performed in local memory. In effect, shared memory is only used to communicate samples, pivots and data partitions. The program uses the PCP pre-processor to express the parallelism <ref> [6, 8] </ref>. The iPSC (Intel Personal Super Computer) is a family of loosely-coupled distributed memory multiprocessors based on the hypercube architecture. Each node of the iPSC/2-386 contains an Intel 80386 processor and 8 megabytes of RAM. Communication between nodes is done via message passing over the hypercube nearest neighbor links.
Reference: [7] <author> J.S. Huang and Y.C. Chow. </author> <title> Parallel Sorting and Data Partitioning by Sampling. </title> <booktitle> COMPSAC, </booktitle> <pages> pages 627-631, </pages> <year> 1983. </year>
Reference: [8] <author> E.D. Brooks III. PCP: </author> <title> A Parallel Extension of C that is 99% Fat Free. </title> <type> Technical Report UCRL-99673, </type> <institution> Lawrence Livermore National Laboratory, </institution> <year> 1988. </year>
Reference-contexts: Since accessing shared memory is more expensive than local memory, the memory access intensive operations of sequential sorting and merging are performed in local memory. In effect, shared memory is only used to communicate samples, pivots and data partitions. The program uses the PCP pre-processor to express the parallelism <ref> [6, 8] </ref>. The iPSC (Intel Personal Super Computer) is a family of loosely-coupled distributed memory multiprocessors based on the hypercube architecture. Each node of the iPSC/2-386 contains an Intel 80386 processor and 8 megabytes of RAM. Communication between nodes is done via message passing over the hypercube nearest neighbor links.
Reference: [9] <author> Intel. </author> <title> iPSC/2 Programmer's Reference Manual. </title> <type> Technical report, </type> <institution> Intel Scientific Computers, Beaverton, Oregon, </institution> <year> 1989. </year>
Reference-contexts: Since the memory of the iPSC is distributed, all sorting and merging is performed in local memory. Data samples, pivots and data partitions are communicated between processors by messages. The program uses the standard Intel message passing libraries <ref> [9] </ref>. The LAN implementations of PSRS use Sun 4/20 workstations connected by a single Ethernet with 10 megabit/second bandwidth. There are two different LAN implementations, each using a different message passing communications package.
Reference: [10] <author> P.P. Li and Y.W. Tung. </author> <title> Parallel Sorting on the Symult 2010. </title> <booktitle> 5th Distributed Memory Conference, </booktitle> <pages> pages 224-229, </pages> <year> 1990. </year>
Reference: [11] <author> T.A. Marsland, T. Breitkreutz, and S. Sutphen. </author> <title> A Network Multi-processor for Experiments in Parallelism. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3(3) </volume> <pages> 203-219, </pages> <year> 1991. </year>
Reference-contexts: The LAN implementations of PSRS use Sun 4/20 workstations connected by a single Ethernet with 10 megabit/second bandwidth. There are two different LAN implementations, each using a different message passing communications package. The first implementation uses the Network Multiprocessor Package (NMP) <ref> [11] </ref>, a locally produced library that provides a friendly high-level interface to sockets and TCP/IP. For large sorting problems, it was discovered that the partitions in Phase 3 fill all of the available message buffers, resulting in deadlock.
Reference: [12] <author> M.J. Quinn. </author> <title> Parallel Sorting Algorithms for Tightly Coupled Multiprocessors. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 349-357, </pages> <year> 1988. </year>
Reference: [13] <author> D. Rotem, N. Santoro, and J. Sidney. </author> <title> Distributed Sorting. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 34(4) </volume> <pages> 372-376, </pages> <year> 1985. </year>
Reference: [14] <author> S. </author> <title> Seidel and L.R. Ziegler. Sorting on Hypercubes. In M.T. </title> <editor> Heath, editor, </editor> <booktitle> Hypercube Multiprocessors, </booktitle> <pages> pages 285-291. </pages> <publisher> SIAM, </publisher> <year> 1987. </year>
Reference: [15] <author> H. Shi. </author> <title> Parallel Sorting on Multiprocessor Computers. </title> <type> Master's thesis, </type> <institution> University of Alberta, </institution> <year> 1990. </year> <month> 29 </month>
Reference-contexts: These algorithms often exploit the strengths of the architecture, while trying to minimize the effects of the weaknesses. Unfortunately, these algorithms usually do not generalize well to other MIMD machines. Parallel Sorting by Regular Sampling (PSRS) is a new MIMD parallel sorting algorithm <ref> [15, 16] </ref>. The regular sampling load balancing heuristic distinguishes PSRS from other parallel sorting algorithms. Regular sampling has good analytical and empirical properties. Also, PSRS has modest communication needs and exhibits good per task locality of reference, reducing memory and communication contention. <p> Given p processors and n keys to sort, the ideal case is that each processor works on n p data items. It has been proven that in PSRS, no processor has to work on more than 2n p data items if n p 3 , assuming no duplicate keys <ref> [15, 16] </ref>. In this paper, a lower bound (L = n p 2 + p 1) and a tighter upper bound (U = 2n p L) on the amount of work done by each processor is proven. Other results on the load balancing bounds are also presented. 2. <p> It achieved a 44-fold 1 It is assumed that there is no computationally inexpensive way of removing duplicates. Techniques such as adding secondary keys may require substantial modifications to the data. 2 speedup while sorting 8,000,000 four-byte integers <ref> [15, 16] </ref>. This paper describes the performance of PSRS on a BBN TC2000 (shared memory), an Intel iPSC/2-386, an iPSC/860 (distributed memory with hypercube interconnections), and a network of workstations (distributed memory with a LAN interconnection). <p> Given n data items 3 (indices 1, 2, 3, ..., n) and p processors (1, 2, 3, ..., p), PSRS consists of four phases. Note that this description differs slightly from that in <ref> [15, 16] </ref>. Refer to Figure 1 for an example, with n = 36, p = 3 and the keys 0, 1, 2, ..., 35. For brevity, let = b p 2 c and w = n 1. Phase One: Sort Local Data. <p> On a shared memory architecture, all information is communicated through shared memory. In particular, Phase 3 reduces to reading and writing partitions from and to shared memory. For pseudo-code and more details, please refer to <ref> [15, 16] </ref>. Intuitively, the notion of a regular sample to estimate the value distribution of the keys is appealing. By sampling the locally sorted blocks of all the processors, and not just a subset, the entire data array is represented. <p> The regular sample is the key, non-cosmetic difference between PSRS and similar sorting algorithms. 3 Time Complexity and Load Balancing For all architectures, the time complexity of PSRS is asymptotic to O ( n p log n) when n p 3 , which is cost optimal <ref> [15, 16] </ref>. Another important concern in parallel sorting is load balancing. In Phase 1, the workload is evenly distributed to all processors. Phase 2 is largely sequential. Phase 3 depends on the communication properties of the computer. This section concentrates on the load balancing issue in Phase 4. <p> The Intel hypercube has dedicated nearest neighbor communication connections. The special communication links of the hypercube allow for fast data transfer between many different processors and with different communication patterns. It is interesting to note that the more sophisticated pattern of communication in Phase 3, as described in <ref> [15, 16] </ref>, was not implemented. Instead of explicitly scheduling and synchronizing the messages so as to take advantage of the dedicated hypercube links, the implementation simply iterated through all the node numbers in Phase 3. <p> RDFA is a measure of how evenly a load is balanced among processors during the merge in Phase 4 <ref> [15, 16] </ref>.
Reference: [16] <author> H. Shi and J. Schaeffer. </author> <title> Parallel Sorting by Regular Sampling. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14(4) </volume> <pages> 361-372, </pages> <year> 1992. </year>
Reference-contexts: These algorithms often exploit the strengths of the architecture, while trying to minimize the effects of the weaknesses. Unfortunately, these algorithms usually do not generalize well to other MIMD machines. Parallel Sorting by Regular Sampling (PSRS) is a new MIMD parallel sorting algorithm <ref> [15, 16] </ref>. The regular sampling load balancing heuristic distinguishes PSRS from other parallel sorting algorithms. Regular sampling has good analytical and empirical properties. Also, PSRS has modest communication needs and exhibits good per task locality of reference, reducing memory and communication contention. <p> Given p processors and n keys to sort, the ideal case is that each processor works on n p data items. It has been proven that in PSRS, no processor has to work on more than 2n p data items if n p 3 , assuming no duplicate keys <ref> [15, 16] </ref>. In this paper, a lower bound (L = n p 2 + p 1) and a tighter upper bound (U = 2n p L) on the amount of work done by each processor is proven. Other results on the load balancing bounds are also presented. 2. <p> It achieved a 44-fold 1 It is assumed that there is no computationally inexpensive way of removing duplicates. Techniques such as adding secondary keys may require substantial modifications to the data. 2 speedup while sorting 8,000,000 four-byte integers <ref> [15, 16] </ref>. This paper describes the performance of PSRS on a BBN TC2000 (shared memory), an Intel iPSC/2-386, an iPSC/860 (distributed memory with hypercube interconnections), and a network of workstations (distributed memory with a LAN interconnection). <p> Given n data items 3 (indices 1, 2, 3, ..., n) and p processors (1, 2, 3, ..., p), PSRS consists of four phases. Note that this description differs slightly from that in <ref> [15, 16] </ref>. Refer to Figure 1 for an example, with n = 36, p = 3 and the keys 0, 1, 2, ..., 35. For brevity, let = b p 2 c and w = n 1. Phase One: Sort Local Data. <p> On a shared memory architecture, all information is communicated through shared memory. In particular, Phase 3 reduces to reading and writing partitions from and to shared memory. For pseudo-code and more details, please refer to <ref> [15, 16] </ref>. Intuitively, the notion of a regular sample to estimate the value distribution of the keys is appealing. By sampling the locally sorted blocks of all the processors, and not just a subset, the entire data array is represented. <p> The regular sample is the key, non-cosmetic difference between PSRS and similar sorting algorithms. 3 Time Complexity and Load Balancing For all architectures, the time complexity of PSRS is asymptotic to O ( n p log n) when n p 3 , which is cost optimal <ref> [15, 16] </ref>. Another important concern in parallel sorting is load balancing. In Phase 1, the workload is evenly distributed to all processors. Phase 2 is largely sequential. Phase 3 depends on the communication properties of the computer. This section concentrates on the load balancing issue in Phase 4. <p> The keys are randomly generated and uniformly distributed. Tables 1, 2, 3 and 4 show the corresponding real execution times. Note that the number of processors available varies between the machines. The BBN TC2000 speedups in Figure 2 reinforce the positive conclusions from previous experiments with the Myrias SPS-2 <ref> [16] </ref>. 13 Sorting Times Sizes (in seconds) 1PE 2PEs 4PEs 8PEs 16PEs 32PEs 64PEs 200,000 2.71 2.22 1.12 0.60 0.32 0.24 - 800,000 15.97 9.49 4.85 2.51 1.31 0.75 0.75 2,000,000 46.52 - 3.41 1.83 1.32 8,000,000 203.86 - 7.47 4.29 Table 1: Sorting times for BBN TC2000, uniform distribution 14 <p> The Intel hypercube has dedicated nearest neighbor communication connections. The special communication links of the hypercube allow for fast data transfer between many different processors and with different communication patterns. It is interesting to note that the more sophisticated pattern of communication in Phase 3, as described in <ref> [15, 16] </ref>, was not implemented. Instead of explicitly scheduling and synchronizing the messages so as to take advantage of the dedicated hypercube links, the implementation simply iterated through all the node numbers in Phase 3. <p> RDFA is a measure of how evenly a load is balanced among processors during the merge in Phase 4 <ref> [15, 16] </ref>.
Reference: [17] <author> T. Tang. </author> <title> Parallel Sorting on the Hypercube Concurrent Processor. </title> <booktitle> 5th Distributed Memory Conference, </booktitle> <pages> pages 237-240, </pages> <year> 1990. </year>
Reference: [18] <author> P.J. Varman and K. Doshi. </author> <title> Sorting with Linear Speedup on a Pipelined Hypercube. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(1) </volume> <pages> 97-103, </pages> <year> 1992. </year>
Reference: [19] <author> B. Wagar. Hyperquicksort: </author> <title> A Fast Sorting Algorithm for Hypercubes. In M.T. </title> <editor> Heath, editor, </editor> <booktitle> Hypercube Multiprocessors, </booktitle> <pages> pages 292-299. </pages> <publisher> SIAM, </publisher> <year> 1987. </year>
Reference-contexts: First, the sorting of the samples by the lone processor in Phase 2 can itself be parallelized. Because PSRS is best suited to sorting large numbers of keys, it is probably not well suited for a parallel Phase 2. A different parallel sorting algorithm, hyperquicksort <ref> [19] </ref> for example, may be better suited for sorting the small data sets of Phase 2. Similarly, the process of gathering the samples can be implemented as a parallel binary tree merge, but at the cost of additional message and synchronization overheads.
Reference: [20] <author> L.M. Wegner. </author> <title> Sorting a Distributed File in a Network. </title> <journal> Computer Networks, </journal> <volume> 8 </volume> <pages> 451-461, </pages> <year> 1984. </year>
Reference: [21] <author> M. Wheat and D.J. Evans. </author> <title> An Efficient Parallel Sorting Algorithm For Shared Memory Multiprocessors. </title> <journal> Parallel Computing, </journal> <volume> 18 </volume> <pages> 91-102, </pages> <year> 1992. </year>
Reference: [22] <author> Y. Won and S. Sahni. </author> <title> A Balanced Bin Sort for the Hypercube Multicomputers. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 435-448, </pages> <year> 1988. </year> <month> 30 </month>
Reference-contexts: Given the extensive literature on parallel sorting, it is not surprising that PSRS is similar to other algorithms. For example, PSRS is similar to the balanced bin sort <ref> [22] </ref> and 4 5 the load balanced sort [1]. However, both have different approaches to load balancing than PSRS. The balanced bin sort's heuristic is also based on sampling, and results in an upper bound of 3n p items that must be merged by a single processor.
References-found: 22

