URL: ftp://dimacs.rutgers.edu/pub/dimacs/TechnicalReports/TechReports/1995/95-35.ps.gz
Refering-URL: http://dimacs.rutgers.edu/TechnicalReports/1995.html
Root-URL: http://www.cs.rutgers.edu
Email: mayoraz@rutcor.rutgers.edu  aviolat@dma.epfl.ch  
Title: Constructive Training Methods for Feedforward Neural Networks with Binary Weights  
Author: by Eddy Mayoraz Frederic Aviolat 
Note: National Science Foundation, grant 20-5637.88. DIMACS is a cooperative project of Rutgers University, Princeton University, AT&T Bell Laboratories and Bellcore. DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives support from the New Jersey Commission on Science and Technology.  
Address: P.O. Box 5062, New Brunswick, NJ 08903-5062,  Switzerland,  
Affiliation: RUTCOR-Rutgers University's Center for Operations Research,  Operations Research, Department of Mathematics, Swiss Federal Institute of Technology, Lausanne,  
Abstract: DIMACS Technical Report 95-35 August 1995 
Abstract-found: 1
Intro-found: 1
Reference: [AB91] <author> Noga Alon and Jehoshua Bruck. </author> <title> Explicit constructions of depth-2 majority circuits for comparison and addition. </title> <type> Technical Report RJ 8300 (75661), </type> <institution> IBM Research Division, Almaden Research Center, </institution> <address> 650 Harry Road, San Jose, CA 95120-6099, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: It is worth noting that COMPARISON is a linearly separable function that requires integer weights growing exponentially in n. It has been shown however that a depth 2 and polynomial size majority network can compute COMPARISON <ref> [AB91] </ref>. Figures 3 and 4 show the average size of the networks produced by 10 runs versus the input size n for complete tasks.
Reference: [AG93] <author> Edoardo Amaldi and Bertrand Guenin. </author> <title> Constructive methods for designing compact feedforward networks of threshold units. </title> <type> Technical report, </type> <institution> Swiss Federal Institute of Technology, Department of Mathematics, </institution> <year> 1993. </year>
Reference-contexts: The second series of experiments will regard the generalization performances of the networks built with our algorithms. Results will be compared to those obtained with the classical constructive algorithms, such as the tiling algorithm, the partial task inversion algorithm and the shift algorithm <ref> [AG93] </ref>. 7.1 Synthesis of Boolean functions Let f be a Boolean function IB n ! IB. We consider tasks of the form T = f (a k ; f (a k )) j a k 2 IB n g, containing all the examples of the known function f . <p> Figure 5 shows the average size of the obtained networks and the average percentage of incorrect classifications, over 25 trainings. Performances of the tiling, the partial task inversion and the shift algorithms are also plotted. The second function we used to test generalization is the 3-SIMILARITY function (proposed in <ref> [AG93] </ref>). The input vector is partitioned into two pieces, and the output of the function will be +1 when at most 3 corresponding components of the two pieces differ. It can be formally defined as f (x) = +1 if and only if fi 2 +i g fi 3.
Reference: [AM91] <author> K. Asanovic and N. Morgan. </author> <title> Experimental determination of precision requirements for back-propagation training of artificial neural networks. </title> <booktitle> In Proceedings of 2nd International Conference on Microelectronics for Neural Networks, </booktitle> <pages> pages 9-15, </pages> <address> Munich, </address> <year> 1991. </year>
Reference: [Avi93] <author> Frederic Aviolat. Les reseaux de neurones artificiels multicouches a poids bi-naires: </author> <title> etude de complexite et algorithmes constructifs. </title> <type> Master's thesis, </type> <institution> Ecole Polytechnique Federale de Lausanne, Departement de Mathematiques, </institution> <month> March </month> <year> 1993. </year>
Reference: [BH89] <author> Eric B. Baum and David Haussler. </author> <title> What size net gives valid generalization ? Neural Computation, </title> <booktitle> 1(1) </booktitle> <pages> 151-160, </pages> <year> 1989. </year>
Reference: [dBZN94] <author> F. d'Alche Buc, D. Zwierski, and J.-P. Nadal. </author> <title> Trio learning: a new strategy for building hybrid neural trees. </title> <booktitle> Int. Journ. of Neural Systems, </booktitle> <volume> 5 </volume> <pages> 259-274, </pages> <year> 1994. </year>
Reference: [Def95] <author> Guillaume Deffuant. </author> <title> An algorithm for building regularized piecewise linear discrimination surfaces: The perceptron membrane. </title> <journal> Neural Computation, </journal> <volume> 7(2) </volume> <pages> 380-398, </pages> <month> March </month> <year> 1995. </year>
Reference: [DG88] <author> R. M. Debenham and S. C. J. Garth. </author> <title> Investigation into the effect of numerical resolution on the performance of back-propagation. In Neural Networks from Models to Applications, </title> <booktitle> Proceedings of n'Euro 88, </booktitle> <pages> pages 752-755, </pages> <address> Paris, </address> <year> 1988. </year>
Reference: [DH73] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1973. </year> <month> - 19 </month> - 
Reference-contexts: Following the objective function used in the well known "perceptron algorithm" for minimizing the number of mistakes in a task (see <ref> [Ros58, DH73] </ref>), our local search procedure will minimize the cost function c (w; w 0 ) defined in (5) when applied to the (t + 1) th unit in layer L c (w; w 0 ) = k misc. v k X k misc. (v k where "misc." refers to the
Reference: [Fie94] <author> Emile Fiesler. </author> <title> Comparative bibliography of ontogenic neural networks. </title> <editor> In Maria Marinaro and Pietro G. Morasso, editors, </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks (ICANN 94), </booktitle> <volume> volume 1, </volume> <pages> pages 793-796, </pages> <address> London, U.K., 1994. </address> <publisher> Springer-Verlag. </publisher>
Reference: [Fre90] <author> Marcus Frean. </author> <title> The upstart algorithm: A method for constructing and training feedforward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2(2) </volume> <pages> 198-209, </pages> <year> 1990. </year>
Reference: [Gal86] <author> Stephen I. Gallant. </author> <title> Three constructive algorithms for network learning. </title> <booktitle> In Proceedings of the 8th Annual Conference of Cognitive Science Society, </booktitle> <pages> pages 652-660, </pages> <address> Amherst, MA, </address> <month> August </month> <year> 1986. </year>
Reference: [Glo89] <author> Fred Glover. </author> <title> "tabu search" part I. </title> <journal> ORSA J. Computing, </journal> <volume> 1(3) </volume> <pages> 190-206, </pages> <year> 1989. </year>
Reference: [GM90] <author> M. Golea and M. Marchand. </author> <title> A growth algorithm for neural network decision trees. </title> <journal> Europhysics Letters, </journal> <volume> 12(3) </volume> <pages> 205-210, </pages> <year> 1990. </year>
Reference-contexts: It could also be expected that optimizing two neurons together still improves the results. Comparing our best algorithms with those for continuous weighted units, we observe that their performances are fair and even better than the tiling algorithm. In <ref> [GM90] </ref>, some similar experiments have been carried out using decision tree algorithms to construct - 13 - versus the input size n. networks with continuous weights.
Reference: [HB91] <author> J. L. Holt and T. E. Baker. </author> <title> Back-propagation simulations using limited precision calculations. </title> <booktitle> In Proceedings of IJCNN'91, pages II: </booktitle> <pages> 121-126, </pages> <address> Seattle, </address> <year> 1991. </year>
Reference: [HdW91] <author> Alain Hertz and Dominique de Werra. </author> <title> The tabu search metaheuristic: How we used it. </title> <journal> Annals of Math. and Artificial Intelligence, </journal> <volume> 1 </volume> <pages> 111-121, </pages> <year> 1991. </year>
Reference: [HH93] <author> Jordan L. Holt and Jenq-Neng Hwang. </author> <title> Finite precision error analysis of neural network hardware implementations. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 42(3) </volume> <pages> 281-290, </pages> <month> March </month> <year> 1993. </year>
Reference: [HHP90] <author> P. W. Hollis, J. S. Harper, and J. J. Paulos. </author> <title> The effects of precision constraints in a back-propagation learning network. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 363-373, </pages> <year> 1990. </year>
Reference: [HMP + 87] <author> A. Hajnal, W. Maass, P. Pudlak, M. Szegedy, and G. Turan. </author> <title> Threshold circuits of bounded depth. </title> <booktitle> In Proceedings of 28 th IEEE FOCS Symposium, </booktitle> <pages> pages 99-110, </pages> <year> 1987. </year>
Reference: [Jud90] <author> J. S. Judd. </author> <title> Neural Network Design and the Complexity of Learning. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1990. </year>
Reference: [May91] <author> Eddy Mayoraz. </author> <title> On the power of networks of majority functions. </title> <editor> In A. Pri-eto, editor, </editor> <booktitle> Lecture Notes in Computer Science 540, </booktitle> <pages> pages 78-85. </pages> <address> IWANN'91, </address> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Figures 3 and 4 show the average size of the networks produced by 10 runs versus the input size n for complete tasks. For small input sizes, the algorithms constructed the smallest known majority networks able to compute the PARITY function exactly <ref> [May91] </ref>, except the abnormality of the method optimizing 2 neurons, for n = 3.
Reference: [May93a] <author> Eddy Mayoraz. </author> <title> Feedforward Boolean Networks with Discrete Weights: Computational Power and Training. </title> <type> PhD thesis, </type> <institution> Swiss Federal Institute of Technology, Department of Mathematics, </institution> <year> 1993. </year> <month> - 20 </month> - 
Reference: [May93b] <author> Eddy Mayoraz. </author> <title> On the power of democratic networks. </title> <type> Technical Report ORWP 93/2, </type> <institution> Swiss Federal Institute of Technology, Department of Mathematics, </institution> <year> 1993. </year> <note> to appear in SIAM J. on Discr. Math. </note>
Reference-contexts: The cost function of equation (5) is obtained from equation (6) by using a linear penalty, i.e. by setting d = 1 in (7). Some experiments have also been carried out with a quadratic penalty (d = 2). 6.2 Back-Forth is forward In <ref> [May93b] </ref> it has been shown that any Boolean function can be computed by a majority network of depth 2. So, in principle a single hidden layer is always sufficient. <p> This allows us to imagine that such an algorithm could be a useful tool in the search for new constructions of other important Boolean functions. We proved in <ref> [May93b] </ref> that any Boolean function f can be computed by a majority network with a single hidden layer. However, we were not able to find in the present work a - 18 - constructive algorithm with convergence guarantees and restricted to one hidden layer.
Reference: [MN89] <author> M. Mezard and J.-P. Nadal. </author> <title> Learning in feedforward layered networks: the tiling algorithm. </title> <journal> J. Phys. A: Math. Gen., </journal> <volume> 22 </volume> <pages> 2191-2203, </pages> <year> 1989. </year>
Reference: [MR94] <author> Eddy Mayoraz and Vincent Robert. </author> <title> Maximizing the robustness of a linear threshold classifier with discrete weights. Network: </title> <booktitle> Computation in Neural Systems, </booktitle> <volume> 5(2) </volume> <pages> 299-315, </pages> <month> May </month> <year> 1994. </year>
Reference: [Nad89] <author> J.-P. Nadal. </author> <title> Study of a growth algorithm for a feedforward network. </title> <journal> International J. of Neural Systems, </journal> <volume> 1(1) </volume> <pages> 55-59, </pages> <year> 1989. </year>
Reference: [RCE82] <author> D. L. Reilly, L. N. Cooper, and C. Elbaum. </author> <title> A neural model for category learning. </title> <journal> Biological Cybernetics, </journal> <volume> 45 </volume> <pages> 35-41, </pages> <year> 1982. </year>
Reference: [Ree93] <author> R. Reed. </author> <title> Pruning algorithms|A survey. </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> 4(5) </volume> <pages> 740-747, </pages> <month> September </month> <year> 1993. </year>
Reference: [Ros58] <author> F. Rosenblatt. </author> <title> The perceptron: a probabilistic model for information storage and organization in the brain. </title> <journal> Psychological Review, </journal> <volume> 63 </volume> <pages> 386-408, </pages> <year> 1958. </year>
Reference-contexts: Following the objective function used in the well known "perceptron algorithm" for minimizing the number of mistakes in a task (see <ref> [Ros58, DH73] </ref>), our local search procedure will minimize the cost function c (w; w 0 ) defined in (5) when applied to the (t + 1) th unit in layer L c (w; w 0 ) = k misc. v k X k misc. (v k where "misc." refers to the
Reference: [SB91] <author> Kai-Yeung Siu and Jehoshua Bruck. </author> <title> On the power of threshold circuits with small weights. </title> <journal> SIAM J. Disc. Math., </journal> <volume> 4(3) </volume> <pages> 423-435, </pages> <year> 1991. </year>
Reference: [SD88] <author> J. Sietsma and R. J. Dow. </author> <title> Neural net pruning|Why and how. </title> <booktitle> In IEEE International Conference on Neural Networks, </booktitle> <pages> pages 325-333. </pages> <publisher> IEEE, </publisher> <address> New York, </address> <year> 1988. </year>
Reference: [SN90] <author> J. A. Sirat and J.-P. Nadal. </author> <title> Neural trees: a new tool for classification. </title> <journal> Network, </journal> <volume> 1 </volume> <pages> 423-438, </pages> <year> 1990. </year>
Reference: [WHR90] <author> Andreas S. Weigend, Bernardo A. Huberman, and David E. Rumelhart. </author> <title> Predicting future: a connectionist approach. </title> <journal> International J. of Neural Systems, </journal> <volume> 1(3) </volume> <pages> 193-209, </pages> <year> 1990. </year>
References-found: 33

