URL: http://ortega.cs.ucdavis.edu/pub/Theses/Kannappan.MS.ps
Refering-URL: http://ortega.cs.ucdavis.edu/Pubs.html
Root-URL: http://www.cs.ucdavis.edu
Title: DRAFT-2.0  
Author: Narana Kannappan 
Date: March 26, 1997  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. D. McDysan and L. D. Spohn, </author> <title> ATM: Theory and Application. </title> <journal> McGraw-Hill Series on Computer Communications, </journal> <year> 1994. </year>
Reference-contexts: ATM is a multiplexing and switching standard for broadband integrated networks and can be broadly characterized by the following salient features: 1. ATM is a cell switching technology. In ATM, all data is sent in fixed size cells of length 53 bytes <ref> [1] </ref>. 2. ATM provides statistical multiplexing gains by multiplexing cells from various different sources and users [1]. 1.1. ATM AS A HIGH SPEED NETWORK 3 3. ATM can provide Quality of Service (QoS) guarantees. This is achieved by the network and the source establishing a "traffic contract". <p> ATM is a cell switching technology. In ATM, all data is sent in fixed size cells of length 53 bytes <ref> [1] </ref>. 2. ATM provides statistical multiplexing gains by multiplexing cells from various different sources and users [1]. 1.1. ATM AS A HIGH SPEED NETWORK 3 3. ATM can provide Quality of Service (QoS) guarantees. This is achieved by the network and the source establishing a "traffic contract".
Reference: [2] <author> P. Newman, </author> <title> "Traffic management for ATM local area networks," </title> <journal> IEEE Network, </journal> <pages> pp. 44-50, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: Among the different traffic types, data traffic has the fastest growth rate. A notable feature of data traffic is that the applications that generate them are incapable of predicting their own bandwidth requirements <ref> [2] </ref>. Also, the applications are usually interfaced to the communication network via the operating system, which will have to service several other applications in general. Therefore, it is unrealistic to expect data applications to precisely know their bandwidth requirements and participate in negotiating with the network during connection set up. <p> In an ATM switch, each output port is a pool of bandwidth that needs to be shared dynamically between all connections currently requiring the best effort service. There are three proposed solutions: 1) use large buffers and let the higher layer protocols deal with congestion <ref> [2] </ref>, 2) use a loss mechanism at the switch [5], and 3) use a feedback mechanism from the network that conveys to the sources the information about the available bandwidth and require the sources to adjust accordingly. <p> INTERACTION OF TCP AND ATM FLOW CONTROL MECHANISM 4 The second solution is an improvement to the plain UBR service. Here, when the switch buffer overflows during congestion, the cells are selectively dropped so as to minimize the higher layer packet retransmission <ref> [2, 6] </ref>. The third solution is incorporated in a service called Available Bit Rate (ABR) service [3, 4, 7, 8]. Here, a scheme has been worked out to periodically provide the sources with congestion information from the network. <p> We end this chapter with a clear and concise description of the problems that we address in this thesis. 3.1 ATM Network Services The Internet Protocol (IP) provides best effort service <ref> [2] </ref>. That is IP datagrams, for example, are not guaranteed to be delivered within a specified time interval, or even delivered at all. In the event of congestion, the network simply discards the packet and end-to-end reliability is added as needed at the transport layer (e.g. TCP). <p> Some of the key differences are the following: * Unlike voice and video, for data applications it is difficult to precisely specify what the QoS are and quantify them <ref> [2] </ref>. * Non-real time data applications, like e-mail and ftp, can typically adapt to time varying bandwidth and tolerate unpredictable cell delays. * Low priority data traffic can be given the left over bandwidth after allocating for higher priority voice and video. <p> The rate based scheme for ABR in the ATM LAN environment was first proposed by <ref> [2] </ref>. Here the network would give backward congestion notification to sources via special Resource Management (RM) cells, which were essentially negative feedback, that is the sources would respond to the feedback signals by reducing their rates.
Reference: [3] <author> F. Bonomi and K. Fendick, </author> <title> "The rate-based flow control framework for the available bit rate ATM service," </title> <journal> IEEE Network, </journal> <pages> pp. 25-39, </pages> <month> March/April </month> <year> 1995. </year>
Reference-contexts: Since the traffic characteristics are unknown, an explicit guarantee of service cannot be given. Rather, data applications require a service, usually called as "best effort service", that dynamically shares the available bandwidth between all the active users <ref> [3, 4] </ref>. In an ATM switch, each output port is a pool of bandwidth that needs to be shared dynamically between all connections currently requiring the best effort service. <p> Here, when the switch buffer overflows during congestion, the cells are selectively dropped so as to minimize the higher layer packet retransmission [2, 6]. The third solution is incorporated in a service called Available Bit Rate (ABR) service <ref> [3, 4, 7, 8] </ref>. Here, a scheme has been worked out to periodically provide the sources with congestion information from the network. <p> Although it did not provide zero cell loss, it guaranteed a low cell loss by providing larger buffers without per VC queuing. Fairness and responsiveness could be optionally realized at the cost of additional complexity. Thus, it gave more architectural flexibility and addressed scalability <ref> [3] </ref> and was formally adopted by the ATM forum in September 1994. 3.2.
Reference: [4] <author> N. Yin and Hluchyj, </author> <title> "On closed loop rate control for ATM cell relay networks," </title> <booktitle> IEEE Infocom, </booktitle> <pages> pp. 35-49, </pages> <year> 1994. </year>
Reference-contexts: Since the traffic characteristics are unknown, an explicit guarantee of service cannot be given. Rather, data applications require a service, usually called as "best effort service", that dynamically shares the available bandwidth between all the active users <ref> [3, 4] </ref>. In an ATM switch, each output port is a pool of bandwidth that needs to be shared dynamically between all connections currently requiring the best effort service. <p> Here, when the switch buffer overflows during congestion, the cells are selectively dropped so as to minimize the higher layer packet retransmission [2, 6]. The third solution is incorporated in a service called Available Bit Rate (ABR) service <ref> [3, 4, 7, 8] </ref>. Here, a scheme has been worked out to periodically provide the sources with congestion information from the network. <p> Here the network would give backward congestion notification to sources via special Resource Management (RM) cells, which were essentially negative feedback, that is the sources would respond to the feedback signals by reducing their rates. This was slightly modified to the WAN environment <ref> [4] </ref> where the congested nodes set the EFCI bit in the forward cells and the DES gave back positive feedback through RM cells to the SES.
Reference: [5] <author> A. Romanow and S. Floyd, </author> <title> "Dynamics of TCP Traffic over ATM Networks," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 13, </volume> <pages> pp. 633-641, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: There are three proposed solutions: 1) use large buffers and let the higher layer protocols deal with congestion [2], 2) use a loss mechanism at the switch <ref> [5] </ref>, and 3) use a feedback mechanism from the network that conveys to the sources the information about the available bandwidth and require the sources to adjust accordingly. The first solution resulted in a class of service called Unspecified Bit Rate (UBR). <p> The higher layer protocols are expected to provide reliability by retransmitting the packet. In this scheme, a lot of bandwidth available to the best-effort service is occupied by cells belonging to packets already corrupted by a cell loss and hence is wasted <ref> [5] </ref>. 1.2. INTERACTION OF TCP AND ATM FLOW CONTROL MECHANISM 4 The second solution is an improvement to the plain UBR service. Here, when the switch buffer overflows during congestion, the cells are selectively dropped so as to minimize the higher layer packet retransmission [2, 6]. <p> For example, the cell loss at the ATM level must be kept below a minimum; otherwise, 1.3. HIGHLIGHT OF RESULTS 5 it would result in unnecessary packet drops at the TCP level affecting throughput <ref> [5] </ref>. Therefore, in addition to studying the control mechanisms at the ATM level and the TCP level, it is important to analyze their interaction. <p> The open-loop approach is specified in the UBR (Unspecified Bit Rate) service which is explained below. It is worthwhile to reemphasize that data traffic differ considerably from their voice and video counterparts and use of open-loop feed back is questionable <ref> [5] </ref>. <p> One possible mechanism is to have large buffers in the switches and let the higher layer protocols handle congestion. But as simulation results show <ref> [5] </ref>, this approach does not yield adequate performance because if the loss mechanism simply discards arriving cells (simple packet discard), each discarded cell is likely to belong to a different packet thereby increasing number of packet drops. <p> ATM NETWORK SERVICES 23 Under random cell drops the higher layer throughput decreases rapidly with packet size. Decreasing switch buffer size results in more cell drops and hence lower the throughput. A more sophisticated loss mechanism is the Early Packet Discard (EPD) proposed by <ref> [5] </ref>. Here, after the switch buffer size reaches a congestion threshold, the remaining buffer space is used to accommodate cells belonging to already queued up packets, while any arriving cell that belongs to fresh packets is dropped, thus reducing higher-layer packet drops and hence retransmissions. <p> Since an ATM cell is very small (53 bytes) in comparison to TCP packets (typically several kilobytes), the contribution of ATM cell loss to the higher layer TCP packet loss is more, which in turn directly affects the throughput <ref> [5] </ref>. shows the results for low round-trip delays of 0.018ms and 0.18ms, whereas Figure 5.2 shows the result for high round-trip delays of 1.8ms and 18ms. <p> This results in a higher throughput for larger packet sizes. The above results are consistent with some of the similar results that have been reported in the literature. Specifically, the impact of the packet size for different switch buffer sizes have been reported in <ref> [5] </ref>, to which our results corroborated. The observations with regards to the various round-trip delays are new and, to the best of our knowledge, have not yet been reported in as much detail as in this thesis. 5.1. <p> Our simulation analysis suggest that ABR is most appropriate in the LAN environment with a low round-trip delay. For wide area networks, further research is required to determine methods which combine intelligent cell dropping schemes <ref> [5, 23] </ref> with ABR control. 5.4 Fairness Issues The fairness criteria in the 10-TCP configuration is that each source must get an equal share of the bottleneck link bandwidth. We measure this by the fraction of the aggregate throughput transmitted by each source. <p> While it may not be possible to avoid cell loss completely, the manner in which the cells are dropped, i.e., the discard policy, can help in improving TCP throughput. In this study we have considered the drop-from-tail discard policy. There are more intelligent policies like Early Packet Discard (EPD) <ref> [5] </ref>, and Drop-From-Front [23] that attempt to improve the impact of cell loss on packet loss. The effect of such policies, which could be resource intensive, on the TCP throughput and fairness needs to be studied.
Reference: [6] <author> T. Chen, S. Liu, and V. Samalam, </author> <title> "The available bit rate service for data in ATM networks," </title> <journal> IEEE communications Magazine, </journal> <pages> pp. 56-71, </pages> <month> May </month> <year> 1996. </year> <title> A comprehensive survey of all the rate based schemes. </title>
Reference-contexts: INTERACTION OF TCP AND ATM FLOW CONTROL MECHANISM 4 The second solution is an improvement to the plain UBR service. Here, when the switch buffer overflows during congestion, the cells are selectively dropped so as to minimize the higher layer packet retransmission <ref> [2, 6] </ref>. The third solution is incorporated in a service called Available Bit Rate (ABR) service [3, 4, 7, 8]. Here, a scheme has been worked out to periodically provide the sources with congestion information from the network. <p> This is often referred to as open-loop-control. The CBR (Constant Bit Rate) and VBR (Variable Bit Rate) services that support video and voice traffic fall into this category <ref> [6] </ref>. There are two approaches for supporting TCP/IP like data traffic over an ATM network. The open-loop approach is specified in the UBR (Unspecified Bit Rate) service which is explained below. <p> For data applications for which it is difficult to specify the requirements, a new ABR (Available Bit Rate) service based on closed-loop-control has been defined by the ATM forum. A comparison of CBR, VBR, ABR, and UBR services is summarized 3.1. ATM NETWORK SERVICES 22 in Table 3.1 <ref> [6] </ref>. In the following sections we describe the UBR and ABR services for transporting TCP/IP traffic. <p> Another mechanism is to drop the cells from front as suggested in [23]. 3.1.2 TCP/IP Traffic with ABR Service The ABR service relies on closed-loop feedback signals from the network. The salient features of the ABR services are summarized below <ref> [6] </ref>: 1. All ABR connections will share the "available bandwidth", that is, whatever band width that is in excess of CBR/VBR traffic. 2. Since the "available bandwidth" changes randomly the share of each ABR con nection is dynamic and may diminish to a specified minimum cell rate (MCR). 3. <p> Thus, ABR can guarantee a minimum cell loss rate unlike UBR. The rest of this section further details the operation of ABR. Initially (until September 1994), two orthogonal feedback mechanisms were being considered for ABR: 1)the end-to-end rate-based control <ref> [6] </ref> and 2)the link-by-link credit-based control [24]. The credit based scheme was based on the motivation that ABR service had to guarantee zero cell loss [24].
Reference: [7] <author> J. Bennett and D. T. Jardins, </author> <title> "Failure modes of the baseline rate based congestion control plan." ATM Forum Contribution 94/0512, </title> <month> July </month> <year> 1994. </year>
Reference-contexts: Here, when the switch buffer overflows during congestion, the cells are selectively dropped so as to minimize the higher layer packet retransmission [2, 6]. The third solution is incorporated in a service called Available Bit Rate (ABR) service <ref> [3, 4, 7, 8] </ref>. Here, a scheme has been worked out to periodically provide the sources with congestion information from the network. <p> constructed: * The network used EFCI to convey congestion information (in the forward direc tion) * The DES returned negative feedback through RM cells to the SES * The SES rate adjustment followed and additive increase and multiplicative de crease (at periodic update intervals) policy As negative feedback was undesirable <ref> [7] </ref>, it was changed to positive in the next version of the rate control scheme called Proportional Rate Control Algorithm (PRCA) [25]. Further, RM cells were generated at a rate proportional to the source rate, limiting the RM cell traffic to a fixed percentage of the total.
Reference: [8] <author> A. Charny, Adams, and R. Jain, </author> <title> "Congestion control with explicit rate indication," </title> <booktitle> ICC, </booktitle> <pages> pp. 1954-1963, </pages> <year> 1995. </year>
Reference-contexts: Here, when the switch buffer overflows during congestion, the cells are selectively dropped so as to minimize the higher layer packet retransmission [2, 6]. The third solution is incorporated in a service called Available Bit Rate (ABR) service <ref> [3, 4, 7, 8] </ref>. Here, a scheme has been worked out to periodically provide the sources with congestion information from the network. <p> The use of EFCI bit in user cells was not consistent with the original use. The rate increase scheme would result in exponential increase of the SES rate, rather than linear, meaning lower rate sources acquire bandwidth more slowly resulting in unfairness. Meanwhile explicit feedback rate control was suggested <ref> [8] </ref>. Here the SES sent a steady stream of RM cells which the DES looped back to have an end-to-end feedback loop. The switches in the path modified the RM cells to log the explicit rate that it could support.
Reference: [9] <author> W. R. Stevens, </author> <title> TCP/IP Illustrated Volume 1. </title> <publisher> Addison-Wesley Professional Computing Series, </publisher> <month> November </month> <year> 1994. </year> <note> BIBLIOGRAPHY 91 </note>
Reference-contexts: The sources are expected to adapt their rates according to an algorithm which works on the feedback information received from the network. 1.2 Interaction of TCP and ATM Flow Control Mecha nism The Transmission Control Protocol (TCP) <ref> [9, 10] </ref> provides a reliable stream transmission service. It provides reliability by using a fundamental technique called positive acknowledgment with retransmission. <p> Actually, to increase efficiency, TCP uses the sliding window protocol which is more complex in that it allows the sender to transmit multiple packets before waiting for an acknowledgment <ref> [9] </ref>. This technique defines a window-size at the sender that constrains the number of unacknowledged segments at the sender. If the window-size is well-tuned, TCP sends packets just at the rate at which the network can accept them, thus maximizing efficiency. <p> In this chapter, we provide a detailed review of the TCP flow and congestion control techniques, both of which are based on the dynamic sliding window protocol <ref> [9, 12] </ref>. This is explained in Sections 2.1 and 2.2. We then review some of the TCP enhancements that have been proposed. <p> Specifically, we discuss the Explicit Congestion Notification (ECN) and Selective Acknowledgment (SACK) techniques, that have been proposed to improve the efficiency of TCP. 2.1 TCP Flow Control Flow control must be employed whenever there is a mismatch between the speeds of the sender and the receiver <ref> [9] </ref>. In the stop-and-wait protocol, the sender must wait for an acknowledgment of a packet that has been sent, before it can send the next packet. Thus, at any time the sender can have at most one outstanding unacknowledged packet. <p> Thus, at any time the sender can have at most one outstanding unacknowledged packet. TCP uses a different form of flow control called the dynamic sliding window protocol 2.1. TCP FLOW CONTROL 9 <ref> [9, 10] </ref>. It allows the sender to have multiple outstanding unacknowledged packets 1 . At connection setup the receiver and sender agree on a maximum advertised window size, W 2 , that corresponds to the maximum number of acknowledgments that can be outstanding at the sender at any time [9]. <p> It allows the sender to have multiple outstanding unacknowledged packets 1 . At connection setup the receiver and sender agree on a maximum advertised window size, W 2 , that corresponds to the maximum number of acknowledgments that can be outstanding at the sender at any time <ref> [9] </ref>. During data transfer, the receiver sends to the sender two fields (ack ,win) as part of the TCP header. <p> Here, the receiver advertised window size, W, is 6 segments (a-b). protocol. The state of the window corresponds to the case when the sender has sent six segments 1 Much of the discussion on TCP flow and congestion control is extracted from <ref> [9, 10] </ref>, which together form a very good source for the TCP protocol. 2 Typically, W is in bytes. For sake of simplicity we will denote W in terms of the number of TCP segments, with the segment size fixed. 2.1. <p> The slow start algorithm defines a new window variable referred to as the congestion window, or cwnd in short <ref> [9, 12] </ref>. When a new connection is established, cwnd is initially set to one. Each time the sender receives an acknowledgment, it increments cwnd by one, up to a maximum of the advertised window size. <p> Early TCP implementations followed a go-back-n model [18, 19] using cumulative positive acknowledgment and required a retransmit timer expiration to re-send data lost during transport. These TCP implementations did not minimize network congestion. The 4.3BSD Tahoe release <ref> [9] </ref> added a number of new algorithms and refinements to earlier implementations, like the Slow-Start, Congestion Avoidance and Fast Retransmit that significantly improved the TCP performance.
Reference: [10] <author> C. Douglas, </author> <title> Internetworking with TCP/IP, </title> <booktitle> Vol 1: Principles,Protocols, and Architecture. </booktitle> <publisher> Prentice Hall Inc.,, </publisher> <year> 1995. </year>
Reference-contexts: The sources are expected to adapt their rates according to an algorithm which works on the feedback information received from the network. 1.2 Interaction of TCP and ATM Flow Control Mecha nism The Transmission Control Protocol (TCP) <ref> [9, 10] </ref> provides a reliable stream transmission service. It provides reliability by using a fundamental technique called positive acknowledgment with retransmission. <p> TCP is a general purpose protocol that can be adapted for use with different underlying delivery systems and since its viability has been demonstrated on a large scale, it forms a good base for a range of data applications <ref> [10, 11] </ref>. Although TCP traffic is typically tolerant to variable delays and packet drops, some minimum QoS will have to be guaranteed when TCP is implemented on ATM. For example, the cell loss at the ATM level must be kept below a minimum; otherwise, 1.3. <p> Thus, at any time the sender can have at most one outstanding unacknowledged packet. TCP uses a different form of flow control called the dynamic sliding window protocol 2.1. TCP FLOW CONTROL 9 <ref> [9, 10] </ref>. It allows the sender to have multiple outstanding unacknowledged packets 1 . At connection setup the receiver and sender agree on a maximum advertised window size, W 2 , that corresponds to the maximum number of acknowledgments that can be outstanding at the sender at any time [9]. <p> Here, the receiver advertised window size, W, is 6 segments (a-b). protocol. The state of the window corresponds to the case when the sender has sent six segments 1 Much of the discussion on TCP flow and congestion control is extracted from <ref> [9, 10] </ref>, which together form a very good source for the TCP protocol. 2 Typically, W is in bytes. For sake of simplicity we will denote W in terms of the number of TCP segments, with the segment size fixed. 2.1.
Reference: [11] <author> D. Borman, R. Braden, and V. Jacobson, </author> <title> "TCP extensions for high performance," </title> <type> tech. rep., </type> <institution> Internet Engineering Task Force (RFC 1323), </institution> <year> 1992. </year>
Reference-contexts: TCP is a general purpose protocol that can be adapted for use with different underlying delivery systems and since its viability has been demonstrated on a large scale, it forms a good base for a range of data applications <ref> [10, 11] </ref>. Although TCP traffic is typically tolerant to variable delays and packet drops, some minimum QoS will have to be guaranteed when TCP is implemented on ATM. For example, the cell loss at the ATM level must be kept below a minimum; otherwise, 1.3. <p> Reno uses the first strategy while Tahoe uses the second. Several transport protocols have provided for selective acknowledgments of received data like NETBLT, XTP, RDP, and VMTP. The first proposals for adding SACK to TCP [14], [15] were later removed from the TCP RFC pending further research <ref> [11] </ref>. However, since then various algorithms for SACK have been proposed to the IETF and the current proposal being considered can be found in [20].
Reference: [12] <author> V. Jacobson, </author> <title> "Congestion avoidance and control," </title> <journal> Computer communications Review, </journal> <volume> vol. vol. 18, </volume> <pages> pp. 314-329, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: In this thesis, we study the interaction between TCP and ATM flow control mechanisms by analyzing how the above parameters affect the performance of TCP. 1.3 Highlight of Results TCP is a very complex protocol which has evolved over last 20 years of changes <ref> [12, 13, 14, 15] </ref>. UBR is a simple protocol; it does not provide any flow control. ABR, on the other hand, provides at the minimum a feedback based flow control. <p> In this chapter, we provide a detailed review of the TCP flow and congestion control techniques, both of which are based on the dynamic sliding window protocol <ref> [9, 12] </ref>. This is explained in Sections 2.1 and 2.2. We then review some of the TCP enhancements that have been proposed. <p> Even though the receiver can handle a window size of W, this naive approach can reduce the throughput of a TCP connection drastically because of buffer overflows in a router feeding an intermediate bottleneck link. The slow start algorithm proposed in <ref> [12] </ref> constraints TCP to "ramp up" slowly. 2.2.1 Slow Start The principle behind slow start is that the rate at which packets must be given to the network must equal the rate at which the acknowledgments are received. <p> The slow start algorithm defines a new window variable referred to as the congestion window, or cwnd in short <ref> [9, 12] </ref>. When a new connection is established, cwnd is initially set to one. Each time the sender receives an acknowledgment, it increments cwnd by one, up to a maximum of the advertised window size. <p> The congestion window cwnd can be increased exponentially as in the slow start phase (Section 2.2.1) or linearly as in the congestion avoidance phase. The congestion 2.2. TCP CONGESTION CONTROL 14 avoidance algorithm defines another variable called the "slow start threshold" which is denoted by ssthresh <ref> [12] </ref>. If the congestion window cwnd is less than ssthresh the source performs slow start, else it performs congestion avoidance. In the congestion avoidance phase, cwnd is incremented by 1=cwnd each time an ACK is received.
Reference: [13] <author> V. Jacobson, </author> <title> "Modified TCP congestion avoidance algorithm." end2end-interest mailing list, </title> <month> April </month> <year> 1990. </year> <title> Describes the fast retransmit and fast recovery algorithms. </title>
Reference-contexts: In this thesis, we study the interaction between TCP and ATM flow control mechanisms by analyzing how the above parameters affect the performance of TCP. 1.3 Highlight of Results TCP is a very complex protocol which has evolved over last 20 years of changes <ref> [12, 13, 14, 15] </ref>. UBR is a simple protocol; it does not provide any flow control. ABR, on the other hand, provides at the minimum a feedback based flow control. <p> Detecting congestion based on timer expiry can be long both because the round trip time can be long and also because of large timer granularities [17] in the host machine. Another means of detection congestion called Fast Retransmit, introduced in <ref> [13] </ref>, supplement the timer expiry method. In this method, three or more duplicate ACKs received in a row is taken as a strong indication that a segment has been lost and congestion has occured. <p> The TCP-Tahoe version of the Fast Retransmit drastically cuts down the sender window size, and hence the rate (Equation 2.1), even if a single packet is dropped. The 4.3BSD Reno release of TCP retained the enhancements incorporated to TCP-Tahoe, but modified the Fast Retransmit operation as suggested in <ref> [13] </ref> to include Fast Recovery. 2.3. PROPOSED EXTENSIONS TO TCP 15 TCP-Reno Implementation In TCP-Reno a sender reacts to a Fast Retransmit after receiving an initial threshold of dup ACKs. This is followed by the Fast Recovery algorithm.
Reference: [14] <author> R. Braden and V. Jacobson, </author> <title> "TCP extensions for long-delay paths," </title> <type> tech. rep., </type> <institution> Internet Engineering Task Force (RFC 1072), </institution> <year> 1988. </year>
Reference-contexts: In this thesis, we study the interaction between TCP and ATM flow control mechanisms by analyzing how the above parameters affect the performance of TCP. 1.3 Highlight of Results TCP is a very complex protocol which has evolved over last 20 years of changes <ref> [12, 13, 14, 15] </ref>. UBR is a simple protocol; it does not provide any flow control. ABR, on the other hand, provides at the minimum a feedback based flow control. <p> Reno uses the first strategy while Tahoe uses the second. Several transport protocols have provided for selective acknowledgments of received data like NETBLT, XTP, RDP, and VMTP. The first proposals for adding SACK to TCP <ref> [14] </ref>, [15] were later removed from the TCP RFC pending further research [11]. However, since then various algorithms for SACK have been proposed to the IETF and the current proposal being considered can be found in [20].
Reference: [15] <author> R. Braden and V. Jacobson, </author> <title> "TCP extensions for high-speed paths," </title> <type> tech. rep., </type> <institution> Internet Engineering Task Force (RFC 1185), </institution> <year> 1990. </year> <title> [16] "The ns simulator, </title> <institution> lawrence berkeley laboratories: </institution> <note> http://www-nrg.ee.lbl.gov/ns." </note>
Reference-contexts: In this thesis, we study the interaction between TCP and ATM flow control mechanisms by analyzing how the above parameters affect the performance of TCP. 1.3 Highlight of Results TCP is a very complex protocol which has evolved over last 20 years of changes <ref> [12, 13, 14, 15] </ref>. UBR is a simple protocol; it does not provide any flow control. ABR, on the other hand, provides at the minimum a feedback based flow control. <p> Reno uses the first strategy while Tahoe uses the second. Several transport protocols have provided for selective acknowledgments of received data like NETBLT, XTP, RDP, and VMTP. The first proposals for adding SACK to TCP [14], <ref> [15] </ref> were later removed from the TCP RFC pending further research [11]. However, since then various algorithms for SACK have been proposed to the IETF and the current proposal being considered can be found in [20].
Reference: [17] <author> S. Floyd, </author> <title> "TCP and explicit congestion notification," </title> <type> tech. rep., </type> <institution> Lawrence Berkeley Laboratory, </institution> <address> One Cyclotron Road, Berkeley, CA 94704, </address> <year> 1995. </year>
Reference-contexts: The gain g is set to 1=8 and the gain for the deviation h is set to be 1=4. Detecting congestion based on timer expiry can be long both because the round trip time can be long and also because of large timer granularities <ref> [17] </ref> in the host machine. Another means of detection congestion called Fast Retransmit, introduced in [13], supplement the timer expiry method. In this method, three or more duplicate ACKs received in a row is taken as a strong indication that a segment has been lost and congestion has occured. <p> This new algorithm prevents the "pipe" from going empty after Fast Retransmit, thereby avoiding the need to Slow-Start to refill it after a single packet loss. 2.3 Proposed Extensions to TCP In this section we discuss two extensions that has been proposed for TCP, namely, Explicit Congestion Notification (ECN) <ref> [17] </ref> and Selective Acknowledgments (SACK) 2.3. PROPOSED EXTENSIONS TO TCP 16 [20]. 2.3.1 Explicit Congestion Notification Future routers are likely to have the capability to detect incipient network congestion before their buffers overflow causing a packet drop. <p> PROPOSED EXTENSIONS TO TCP 17 and is an ineffective and unfair fix for congestion. Another form of explicit congestion notification are using RED (Random Early Detection) gateways introduced in <ref> [17] </ref>. These gateways monitor the average queue size and during congestion use a probabilistic algorithm to choose which arriving packets to mark (e.g.,to drop, or to set the ECN field in the packet header). <p> If in twice the last Round Trip Time the fraction of packets with their congestion bits set is greater than half, the source multiplicatively decreases its window size. Otherwise it increases its window size additively. As pointed out in <ref> [17] </ref>, the main advantage of ECN is in avoiding unnecessary delay for packets from low-bandwidth delay-sensitive TCP connections. This advantage will be most pronounced in a highly-congested network where a high frequency of packet drops is required to control congestion. <p> It may be worthwhile to determine if the CHAPTER 6. CONCLUSIONS AND FUTURE WORK 70 rate-based signals provided to TCP similar to that proposed in ECN <ref> [17] </ref> can improve performance of TCP when it is implemented over ABR. Another aspect that needs to be studied is the impact of providing Explicit Rate information to the sources.
Reference: [18] <author> D. Bertsakas and R. Gallager, </author> <title> Data Networks. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, N.J. 07632, </address> <year> 1987. </year>
Reference-contexts: In the congestion avoidance phase, cwnd is incremented by 1=cwnd each time an ACK is received. This ensures that cwnd is increased at most by one segment each round trip time and is thus an additive increase unlike the slow start. Early TCP implementations followed a go-back-n model <ref> [18, 19] </ref> using cumulative positive acknowledgment and required a retransmit timer expiration to re-send data lost during transport. These TCP implementations did not minimize network congestion. <p> Thus, the network must be designed such that the source rates satisfy the max-min criteria <ref> [18] </ref>. We would like to study how fair the available bandwidth would be distributed amongst the sources when different ATM parameters are chosen. There are several parameters at the TCP level and ATM layer that would affect the above metrics.
Reference: [19] <author> W. Stallings, </author> <title> Data and Computer Communications. </title> <publisher> MacMillan Publications, </publisher> <year> 1991. </year>
Reference-contexts: In the congestion avoidance phase, cwnd is incremented by 1=cwnd each time an ACK is received. This ensures that cwnd is increased at most by one segment each round trip time and is thus an additive increase unlike the slow start. Early TCP implementations followed a go-back-n model <ref> [18, 19] </ref> using cumulative positive acknowledgment and required a retransmit timer expiration to re-send data lost during transport. These TCP implementations did not minimize network congestion.
Reference: [20] <author> M. Mathis, M. Jamshid, F. Sally, and A. Romanow, </author> <title> "TCP selective acknowledgment options," </title> <type> tech. rep., Internet Draft, </type> <year> 1996. </year>
Reference-contexts: PROPOSED EXTENSIONS TO TCP 16 <ref> [20] </ref>. 2.3.1 Explicit Congestion Notification Future routers are likely to have the capability to detect incipient network congestion before their buffers overflow causing a packet drop. If these routers could notify about the impending congestion to the sources explicitly, then the sources can react to the congestion much earlier. <p> The first proposals for adding SACK to TCP [14], [15] were later removed from the TCP RFC pending further research [11]. However, since then various algorithms for SACK have been proposed to the IETF and the current proposal being considered can be found in <ref> [20] </ref>. In this thesis we will focus on the TCP-Tahoe implementation and study its performance when it is implemented over an ATM network running UBR or ABR services. 2.3.
Reference: [21] <author> K. K. Ramakrishnan and R. Jain, </author> <title> "A binary feedback scheme for congestion avoidance in computer systems," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 8, no. 2, </volume> <pages> pp. 158-181, </pages> <year> 1990. </year>
Reference-contexts: The Source Quench messages can be generated at a controlled rate if the RED gateway uses such a probabilistic algorithm. This could be a more effective and fair fix for congestion. In the DECbit scheme <ref> [21] </ref>, a gateway monitors the average queue length and detects congestion if it is greater than one. It then sets a congestion bit in the outgoing packet and the destination sets the congestion bit in the ACK back to the source. <p> some of the different kinds of information that the network could convey to the end systems and the manner in which they could react to the feedback signals. 3.2 Evolution of the Current Rate Based Framework One of the earliest rate based window flow control appeared in the DECnet protocol <ref> [21] </ref> where explicit congestion notification was passed in the forward direction through acknowledgments. The rate based scheme for ABR in the ATM LAN environment was first proposed by [2].
Reference: [22] <author> K. Fall and S. Floyd, </author> <title> "Simulation-based comparisons of tahoe, renoe, and SACK TCP," </title> <type> tech. rep., </type> <institution> Lawrence Berkeley Laboratory, </institution> <year> 1996. </year> <note> BIBLIOGRAPHY 92 </note>
Reference-contexts: A selective acknowledgement option allows receivers to additionally report non-sequential data they have received. When coupled with a selective retransmission policy implemented in TCP senders, considerable savings can be achieved <ref> [22] </ref>, because with SACK, a sender has a better idea of exactly which packets have been successfully delivered as compared with comparable protocols without SACK. Given such information, a sender can avoid unnecessary delays and retransmissions, resulting in improved throughput.
Reference: [23] <author> T. V. Lakshman, A. Neidhardt, and T. J. Ott, </author> <title> "The drop from front strategy in TCP and in TCP over ATM," </title> <booktitle> IEEE Infocom, </booktitle> <month> March </month> <year> 1996. </year>
Reference-contexts: This improves the throughput and the fairness with respect to different connections. Another mechanism is to drop the cells from front as suggested in <ref> [23] </ref>. 3.1.2 TCP/IP Traffic with ABR Service The ABR service relies on closed-loop feedback signals from the network. The salient features of the ABR services are summarized below [6]: 1. <p> Our simulation analysis suggest that ABR is most appropriate in the LAN environment with a low round-trip delay. For wide area networks, further research is required to determine methods which combine intelligent cell dropping schemes <ref> [5, 23] </ref> with ABR control. 5.4 Fairness Issues The fairness criteria in the 10-TCP configuration is that each source must get an equal share of the bottleneck link bandwidth. We measure this by the fraction of the aggregate throughput transmitted by each source. <p> In this study we have considered the drop-from-tail discard policy. There are more intelligent policies like Early Packet Discard (EPD) [5], and Drop-From-Front <ref> [23] </ref> that attempt to improve the impact of cell loss on packet loss. The effect of such policies, which could be resource intensive, on the TCP throughput and fairness needs to be studied.
Reference: [24] <author> H. T. Kung and R. Morris, </author> <title> "Credit-based flow control for ATM networks," </title> <journal> IEEE Network, </journal> <pages> pp. 40-48, </pages> <month> March/April </month> <year> 1995. </year>
Reference-contexts: Thus, ABR can guarantee a minimum cell loss rate unlike UBR. The rest of this section further details the operation of ABR. Initially (until September 1994), two orthogonal feedback mechanisms were being considered for ABR: 1)the end-to-end rate-based control [6] and 2)the link-by-link credit-based control <ref> [24] </ref>. The credit based scheme was based on the motivation that ABR service had to guarantee zero cell loss [24]. <p> Initially (until September 1994), two orthogonal feedback mechanisms were being considered for ABR: 1)the end-to-end rate-based control [6] and 2)the link-by-link credit-based control <ref> [24] </ref>. The credit based scheme was based on the motivation that ABR service had to guarantee zero cell loss [24]. In this scheme, the upstream node in each link could transmit a cell only when it had a credit update cell, that the down stream node released only if it had enough buffer space.
Reference: [25] <author> A. Barnhart, </author> <title> "Baseline performance using PRCA rate-control." ATM Forum/94-0597, </title> <month> July </month> <year> 1994. </year>
Reference-contexts: through RM cells to the SES * The SES rate adjustment followed and additive increase and multiplicative de crease (at periodic update intervals) policy As negative feedback was undesirable [7], it was changed to positive in the next version of the rate control scheme called Proportional Rate Control Algorithm (PRCA) <ref> [25] </ref>. Further, RM cells were generated at a rate proportional to the source rate, limiting the RM cell traffic to a fixed percentage of the total. The SES generated one user cell with EFCI=0 for every N rm user cells with EFCI=1.
Reference: [26] <author> L. Roberts, </author> <title> "Enhanced PRCA (proportional rate control algorithm)." ATM Forum/94-0735R1, </title> <month> August </month> <year> 1994. </year>
Reference-contexts: The switches in the path modified the RM cells to log the explicit rate that it could support. The ideas of PRCA and explicit rate were combined into a single scheme called EPRCA <ref> [26] </ref> by interpreting the explicit rate as dynamic upper bound on the rate that the SES calculated using the previous PRCA. That is, the allowed cell rate, A cr =min (PRCA rate, Explicit rate).
Reference: [27] <author> G. Nada, A. Koenig, and D. Su, </author> <title> The NIST ATM Network Simulator. Operation and Programming. </title> <type> NIST, </type> <institution> Computer Systems Laboratory, Gaithersburg, MD 20899, </institution> <month> March </month> <year> 1995. </year>
Reference-contexts: Further, since we wanted to investigate different rate control algorithms at the ATM level ( UBR, ABR with different implementations), making modifications to the simulator had to be easy. We looked into three options, ns from Lawrence Berkeley Laboratories [16], netsim from National Institute of Standards and Technology <ref> [27] </ref>, and OPNet. OPNet was not chosen as it was too expensive. Although netsim was good in modeling the ATM part it did not detail TCP very well. ns detailed TCP well but simulated only TCP networks.
Reference: [28] <author> H. S. Charles and M. K. Chandy, </author> <title> Computer Systems Performance Modeling. </title> <publisher> Pren-tice Hall Inc., </publisher> <address> Englewood Cliffs NJ 07632, </address> <year> 1985. </year>
Reference-contexts: In chapter 5 we present and explain the results that we obtained from our simulation experiments. 4.1. THE NS SIMULATOR 37 4.1 The ns Simulator The ns simulator is a discrete event driven simulator <ref> [28] </ref>. Events are generated by the simulation elements and are time-stamped. There is a global event list that contains all events in chronological order. A scheduler services the head of the list (i.e., the event that is earliest in time) by delivering it to the relevant event handler.
Reference: [29] <author> S. Floyd and V. Jacobson, </author> <title> "Random early detection gateways for congestion avoidance," </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> vol. 1, no. 4, </volume> <pages> pp. 197-413, </pages> <year> 1993. </year>
Reference-contexts: Links: 1. drop-tail:- A simple FIFO queue which dorps the last packet in the queue when the queue overflows. 2. red:- A red link is a random-early drop queue <ref> [29] </ref>. 3. cbq:- A cbq link is for class-based queueing. As explained earlier, ns is an event driven simulator. There is a global "event queue" that contains all the events in chronological order and each event is "handled" one after the other.
Reference: [30] <author> B. B. Welch, </author> <title> Practical Programming in Tcl and Tk. </title> <publisher> Prentice Hall, </publisher> <year> 1995. </year>
Reference-contexts: The vanilla tclsh comes with its own set of commands but can be extended to support user defined commands that can be linked to C (or C++) programs <ref> [30] </ref>. The tcl is a clean and efficient interface which hides the complexity of the simulator core. For example, all the interactions with the interpreter is via the tcl procedure "ns". The tcl command "ns node" calls the constructor (C++) for the object "node" and 4.1.
References-found: 29

