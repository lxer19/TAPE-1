URL: http://www.cs.brown.edu/people/milos/research/dx-98.ps
Refering-URL: http://www.cs.brown.edu/people/milos/papers.html
Root-URL: http://www.cs.brown.edu/
Email: milos@cs.brown.edu  hamish@medg.lcs.mit.edu  
Title: Planning Medical Therapy Using Partially Observable Markov Decision Processes.  
Author: Milos Hauskrecht Hamish Fraser 
Date: 1910  
Address: Box  Providence, RI 02912-1210  750 Washington Street Boston, MA 02111  
Affiliation: Computer Science Department,  Brown University  Tufts-New England Medical Center  
Abstract: Diagnosis of a disease and its treatment are not separate, one-shot activities. Instead they are very often dependent and interleaved over time, mostly due to uncertainty about the underlying disease, uncertainty associated with the response of a patient to the treatment and varying cost of different treatment and diagnostic (investigative) procedures. The framework particularly suitable for modeling such a complex therapy decision process is Partially observable Markov decision process (POMDP). Unfortunately the problem of finding the optimal therapy within the standard POMDP framework is also computationally very costly. In this paper we investigate various structural extensions of the standard POMDP framework and approximation methods which allow us to simplify model construction process for larger therapy problems and to solve them faster. A therapy problem we target specifically is the management of patients with ischemic heart disease. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K.J. Astrom. </author> <title> Optimal control of Markov decision processes with incomplete state estimation. </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 10, </volume> <pages> pp. 174-205, </pages> <year> 1965. </year>
Reference-contexts: To model accurately the complex decision process that combines diagnostic and treatment steps we need a framework that is expressive enough to capture all relevant features of the problem. A framework particularly suitable for such a problem is Partially observable Markov decision process (POMDP) <ref> [1] </ref> [11]. POMDP represents a controlled process, similar to MDP [2] [10], but with two sources of uncertainty: uncertainty related to the dynamics of the control process (same as MDPs), and uncertainty associated with the partial observability of the process by a control agent. <p> process (POMDP) describes a stochastic control process with partially observable states and formally corresponds to a 6-tuple (S; A; fi; T; O; R), where S is a finite set of states; A is a finite set of actions; fi is a finite set of observations; T : S fiAfiS ! <ref> [0; 1] </ref> is a set of transition probabilities between states that describe the dynamic behavior of the modeled environment; O : S fi A fi fi ! [0; 1] stand for a set of observation probabilities that describe the relationship among observations, states and actions; and R : S fi A <p> finite set of states; A is a finite set of actions; fi is a finite set of observations; T : S fiAfiS ! <ref> [0; 1] </ref> is a set of transition probabilities between states that describe the dynamic behavior of the modeled environment; O : S fi A fi fi ! [0; 1] stand for a set of observation probabilities that describe the relationship among observations, states and actions; and R : S fi A fi S ! R denotes a reward (cost) model that assigns rewards to state transitions and models payoffs associated with such transitions. 1 Given a POMDP, the
Reference: [2] <author> E.R. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference-contexts: A framework particularly suitable for such a problem is Partially observable Markov decision process (POMDP) [1] [11]. POMDP represents a controlled process, similar to MDP <ref> [2] </ref> [10], but with two sources of uncertainty: uncertainty related to the dynamics of the control process (same as MDPs), and uncertainty associated with the partial observability of the process by a control agent. <p> In POMDPs process states are hidden and decisions could be based only on observations seen and past actions performed. This makes a large difference when the optimal policy for all possible situations a decision-maker may encounter should be found. While in perfectly observable MDPs (see <ref> [2] </ref>, [10]) one works with a finite number of states that are always known, and the optimal policy can be found efficiently using dynamic programming techniques, in POMDPs underlying states are not known with certainty and POMDP must be first transformed into an information-state MDP, most often into a belief-state MDP. <p> The infinite number of possible belief states is the main problem complicating the optimization task. Value (expected discounted reward) of the optimal policy 1 Note that costs can be treated as negative rewards. for a belief state satisfies the Bellman's equation <ref> [2] </ref>: V fl (b) = max R (b; a) + fl o2fi where b stands for the belief state, R (b; a) denotes an ex pected one step reward for a state b and an action a and equals: R (b; a) = s 0 2S s2S and t is an <p> The optimal action for a belief state is then obtained easily as: fl (b) = arg max R (b; a) + fl o2fi The optimal value function in 1 could be approximated for all possible belief states using value iteration strategy <ref> [2] </ref>. In this strategy, we define an i-th step approximation as: V i (b) = max R (b; a) + fl o2fi (2) starting from some initial value function V 0 . The sequence of value function approximations converges to the optimal (fixed point) solution.
Reference: [3] <author> C. Boutlier, D. Poole. </author> <title> Computing Optimal Policies for Partially Observable Decision Processes. </title> <booktitle> In the Proceedings of AAAI-96, </booktitle> <pages> pp. 1168-1175, </pages> <year> 1996. </year>
Reference-contexts: We note that other structural refinements of a value function description are possible as well. For example, Boutilier and Poole <ref> [3] </ref> proposed and explored a method that uses com pact representations of linear vectors defining piecewise linear and convex value function. 4.4 Exploiting additional state constraints One drawback of the factored state representation is that it does not provide means for restricting combinations of state variable values that are not possible. <p> The second method tested is the fast informed bound method and it was computed in about 3 minutes. For every stage, the table shows a set of current observations and a list of possi 4 Decision trees were used also in <ref> [3] </ref> to represent linear vectors of piecewise linear and convex value functions more compactly. In fact, the ideas of separation of perfectly observable components, exploitation of additional state constraints and compact representations of linear vectors could be combined to refine the value function description. <p> In our we work we showed how to simplify the problem-solving by considering only a relevant subset of state variables, by differentiating observable and hidden variables, and by using additional hierarchical state variable con-traints. Other structural approaches (see e.g. <ref> [3] </ref>) or combinations of structural approaches are possible. However, structural improvements are not always sufficient to adequately speed-up the problem-solving process. In such cases approximation methods that trade-off the solution quality for speed need to be employed.
Reference: [4] <author> A. R. Cassandra. </author> <title> Exact and Approximate Algorithms for Partially Observable Markov Decision Processes. </title> <type> Ph. D. Thesis. </type> <institution> Brown University, Providence, RI, </institution> <year> 1997. </year>
Reference-contexts: Then fixed point equation 1 holds and either exact [11] <ref> [4] </ref> or approximate value iteration techniques [6] developed for the standard POMDPs could be applied. However, we can also take advantage of the additional structure present in our IHD model.
Reference: [5] <author> T. Dean, K. </author> <title> Kanazawa. The model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3), </volume> <pages> pp. 142-150, </pages> <year> 1989. </year>
Reference-contexts: We start with the issue of modeling the structure. 3.3 Model of dynamics To model the dynamics of the IHD disease process more compactly we used a hierarchical refinement of the dynamic belief network (see <ref> [5] </ref>, [8]) in figure 1. In this model a state of the patient is described using a set of state variables, which can be either hidden or observable.
Reference: [6] <author> M. Hauskrecht. </author> <title> Planning and control in stochastic domains with imperfect information. </title> <address> MIT-LCS-TR-738, </address> <year> 1997. </year>
Reference-contexts: Although computable, the computational cost for doing so is enormous and only small belief-state MDPs could be solved exactly or to arbitrary small precision in practice. This naturally leads to various approximation methods that try to calculate good solution fast (see [9], <ref> [6] </ref>). <p> Then fixed point equation 1 holds and either exact [11] [4] or approximate value iteration techniques <ref> [6] </ref> developed for the standard POMDPs could be applied. However, we can also take advantage of the additional structure present in our IHD model. In the following we will present three structural improvements based on [6]: the ability to reduce the number of state variables we need to consider during planning, <p> fixed point equation 1 holds and either exact [11] [4] or approximate value iteration techniques <ref> [6] </ref> developed for the standard POMDPs could be applied. However, we can also take advantage of the additional structure present in our IHD model. In the following we will present three structural improvements based on [6]: the ability to reduce the number of state variables we need to consider during planning, the ability to separate perfectly and partially observable state components, and an additional model of state constraints. 4.1 Process state variables The first improvement stems from the fact that not all variables representing the state <p> However, when a hybrid information state space is used, the value function can be represented as a collection of piecewise linear and convex value functions of smaller complexity, one function for each combination of perfectly observable process state variable values <ref> [6] </ref>. <p> Table 1 illustrates a sequence of recommendations for a single patient case with a follow-up obtained for two of the best-performing methods from <ref> [6] </ref>. The first method is the incremental linear vector method with 15 incremental linear vector update cycles. Using this method, the value function for all possible belief states was computed off-line in about 30 minutes on SPARC-10 in Lucid Common Lisp. <p> EKG ischemia: negative medication 483.11 447.85 decreased ventr. func.: true no action 485.15 450.04 acute MI: false stress-test 486.32 448.75 coronary artery visual: not available angiogram 496.38 458.87 stress test results: not available CABG 661.98 610.81 history CABG: false history PTCA: true Table 1: Patient case with a follow-up from <ref> [6] </ref>. Recommendations for each state (in bold) are based on the value function approximation computed by the incremental linear vector method (method 1) and the fast informed bound method (method 2). Note that both methods suggest the same action choices. for the stress test procedure. <p> However, structural improvements are not always sufficient to adequately speed-up the problem-solving process. In such cases approximation methods that trade-off the solution quality for speed need to be employed. To do this for the IHD we applied various approximation techniques developed in <ref> [6] </ref>. Using structured approach and approximations we were able to built an IHD model of moderate complexity and to solve it. The solutions obtained for the current model are promising and confirmed that POMDPs could provide a useful framework for the IHD domain.
Reference: [7] <author> M. Hauskrecht. </author> <title> Combining perfectly and partially observable Markov decision processes. under preparation, </title> <year> 1998. </year>
Reference-contexts: The value function for i-th step approximation can be rewritten as <ref> [7] </ref>: o d V i (b d ) denotes a partial value function for a vector o d and equals: V i (b d ) = max i j=1 where o d i is a set of linear vectors defining V o d i .
Reference: [8] <author> U. Kjaerulff. </author> <title> A computational scheme for reasoning in dynamic probabilistic networks. </title> <booktitle> In the Proceedings of UAI-92, </booktitle> <pages> pp. 121-129, </pages> <year> 1992. </year>
Reference-contexts: We start with the issue of modeling the structure. 3.3 Model of dynamics To model the dynamics of the IHD disease process more compactly we used a hierarchical refinement of the dynamic belief network (see [5], <ref> [8] </ref>) in figure 1. In this model a state of the patient is described using a set of state variables, which can be either hidden or observable.
Reference: [9] <author> W.S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observed Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28, </volume> <pages> pp. 47-66, </pages> <year> 1991. </year>
Reference-contexts: Although computable, the computational cost for doing so is enormous and only small belief-state MDPs could be solved exactly or to arbitrary small precision in practice. This naturally leads to various approximation methods that try to calculate good solution fast (see <ref> [9] </ref>, [6]).
Reference: [10] <author> M. Puterman. </author> <title> Markov Decision Processes. </title> <publisher> John Wiley and Sons, </publisher> <year> 1994. </year>
Reference-contexts: A framework particularly suitable for such a problem is Partially observable Markov decision process (POMDP) [1] [11]. POMDP represents a controlled process, similar to MDP [2] <ref> [10] </ref>, but with two sources of uncertainty: uncertainty related to the dynamics of the control process (same as MDPs), and uncertainty associated with the partial observability of the process by a control agent. <p> In POMDPs process states are hidden and decisions could be based only on observations seen and past actions performed. This makes a large difference when the optimal policy for all possible situations a decision-maker may encounter should be found. While in perfectly observable MDPs (see [2], <ref> [10] </ref>) one works with a finite number of states that are always known, and the optimal policy can be found efficiently using dynamic programming techniques, in POMDPs underlying states are not known with certainty and POMDP must be first transformed into an information-state MDP, most often into a belief-state MDP.
Reference: [11] <author> R.D. Smallwood, E.J. Sondik. </author> <title> The optimal control of partially observable processes over a finite horizon. </title> <journal> Operations Research, </journal> <volume> 21, </volume> <pages> pp. 1071-1088, </pages> <year> 1973. </year>
Reference-contexts: To model accurately the complex decision process that combines diagnostic and treatment steps we need a framework that is expressive enough to capture all relevant features of the problem. A framework particularly suitable for such a problem is Partially observable Markov decision process (POMDP) [1] <ref> [11] </ref>. POMDP represents a controlled process, similar to MDP [2] [10], but with two sources of uncertainty: uncertainty related to the dynamics of the control process (same as MDPs), and uncertainty associated with the partial observability of the process by a control agent. <p> The sequence of value function approximations converges to the optimal (fixed point) solution. The important property of the sequence is that value functions are piecewise linear and convex <ref> [11] </ref>. This makes it possible to compute the update in finite time for the complete belief space. Although computable, the computational cost for doing so is enormous and only small belief-state MDPs could be solved exactly or to arbitrary small precision in practice. <p> Then fixed point equation 1 holds and either exact <ref> [11] </ref> [4] or approximate value iteration techniques [6] developed for the standard POMDPs could be applied. However, we can also take advantage of the additional structure present in our IHD model. <p> and a belief over all possible combinations of values of hidden process variables (b d ). 4.3 Value function decomposition It is well known that for the standard belief space MDP the value function one would obtain after a finite number of value iteration updates is piecewise linear and convex <ref> [11] </ref>. However, when a hybrid information state space is used, the value function can be represented as a collection of piecewise linear and convex value functions of smaller complexity, one function for each combination of perfectly observable process state variable values [6].
Reference: [12] <author> J.A. Tatman, R.D. Schachter. </author> <title> Dynamic programming and influence diagrams. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 20(2), </volume> <pages> pp. 365-379, </pages> <year> 1990. </year>
Reference-contexts: Note that two structured models (dynamics and costs) can be represented together using dynamic influence diagrams framework <ref> [12] </ref>. The cost associated with a patient state, R (s 0 ), can be further broken down into state variable costs.
Reference: [13] <author> J.B. Wong et.al. </author> <title> Myocardial revascularization for chronic stable angina. </title> <journal> Annals of Internal Medicine, </journal> <volume> 113 (1), </volume> <pages> pp. 852-871, </pages> <year> 1990. </year>
Reference-contexts: Thus in a course of patient management one needs to carefully evaluate the benefit of diagnostic (investigative) and treatment steps with regard to the overall global objective, the well being of a patient. An example of such a problem is the management of patients with ischemic heart disease (IHD) <ref> [13] </ref>, and we will focus on it in our work. To model accurately the complex decision process that combines diagnostic and treatment steps we need a framework that is expressive enough to capture all relevant features of the problem. <p> An example of such a problem, with complex temporal stochastic dependencies, and costs associated with both treatment and investigative procedures, is the problem of management of patients with ischemic heart disease (IHD) (see e.g. <ref> [13] </ref>). 3.1 Management of the ischemic heart disease Ischemic heart disease is caused by an imbalance between the supply and demand of oxygen by a heart muscle.
References-found: 13

