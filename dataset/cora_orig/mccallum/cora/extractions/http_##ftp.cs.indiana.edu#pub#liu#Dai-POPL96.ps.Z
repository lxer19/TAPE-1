URL: http://ftp.cs.indiana.edu/pub/liu/Dai-POPL96.ps.Z
Refering-URL: http://www.cs.indiana.edu/hyplan/liu.html
Root-URL: http://www.cs.indiana.edu
Email: fyanhong,stoller,ttg@cs.cornell.edu  
Title: Discovering Auxiliary Information for Incremental Computation  
Author: Yanhong A. Liu Scott D. Stoller Tim Teitelbaum 
Address: Ithaca, New York 14853  
Affiliation: Department of Computer Science, Cornell University,  
Abstract: This paper presents program analyses and transformations that discover a general class of auxiliary information for any incremental computation problem. Combining these techniques with previous techniques for caching intermediate results, we obtain a systematic approach that transforms non-incremental programs into efficient incremental programs that use and maintain useful auxiliary information as well as useful intermediate results. The use of auxiliary information allows us to achieve a greater degree of incrementality than otherwise possible. Applications of the approach include strength reduction in optimizing compilers and finite differencing in transformational programming. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. Addison-Wesley Series in Computer Science. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers <ref> [1, 2, 15, 20, 60] </ref>, transformational program development [7, 17, 47, 49, 59], and interactive systems [4, 5, 9, 19, 27, 33, 53, 54]. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs.
Reference: [2] <author> F. E. Allen, J. Cocke, and K. Kennedy. </author> <title> Reduction of operator strength. </title> <editor> In S. S. Muchnick and N. D. Jones, editors, </editor> <title> Program Flow Analysis, </title> <booktitle> chapter 3, </booktitle> <pages> pages 79-101. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: Efficient iterative computation relies on effective use of state, i.e., computing the result of each iteration using stored results of previous iterations. This is why strength reduction <ref> [2] </ref> and related techniques [48] are crucial for performance. Given a program f and an input change operation , a program f 0 that computes f (x y) efficiently by using the result of the previous computation of f (x) is called an incremental version of f under . <p> Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers <ref> [1, 2, 15, 20, 60] </ref>, transformational program development [7, 17, 47, 49, 59], and interactive systems [4, 5, 9, 19, 27, 33, 53, 54]. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. <p> Some approaches to incremental computation have exploited specific kinds of auxiliary information, e.g., auxiliary arithmetic associated with some classical strength-reduction rules <ref> [2] </ref>, dynamic mappings maintained by finite differencing rules for aggregate primitives in SETL [48] and INC [64], and auxiliary data structures for problems with certain properties like stable decomposition [52]. However, until now, systematic discovery of auxiliary information for arbitrary programs has been a subject completely open for study. <p> For static incremental attribute evaluation algorithms [34, 35], where no auxiliary information is needed, the approach can cache intermediate results and maintain them automatically [40]. 11 Strength reduction <ref> [2, 15, 60] </ref> is a traditional compiler op-timization technique that aims at computing each iteration incrementally based on the result of the previous iteration. Basically, a fixed set of strength-reduction rules for primitive operators like times and plus are used.
Reference: [3] <author> B. Alpern, R. Hoover, B. Rosen, P. Sweeney, and K. Zadeck. </author> <title> Incremental evaluation of computational circuits. </title> <booktitle> In Proceedings of the 1st Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 32-42, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1990. </year>
Reference: [4] <author> R. Bahlke and G. Snelting. </author> <title> The PSG system: From formal language definitions to interactive programming environments. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 547-576, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers [1, 2, 15, 20, 60], transformational program development [7, 17, 47, 49, 59], and interactive systems <ref> [4, 5, 9, 19, 27, 33, 53, 54] </ref>. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. <p> Here, we take a closer look at related work on discovering auxiliary information for incremental computation. Interactive systems and reactive systems often use incremental algorithms to achieve fast response time <ref> [4, 5, 9, 19, 27, 33, 53, 54] </ref>. Since explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover, the general approach in this paper provides a systematic method for developing particular incremental algorithms.
Reference: [5] <author> R. A. Ballance, S. L. Graham, and M. L. Van De Van-ter. </author> <title> The Pan language-based editing system. </title> <journal> ACM Transactions on Software Engineering and Methodology, </journal> <volume> 1(1) </volume> <pages> 95-127, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers [1, 2, 15, 20, 60], transformational program development [7, 17, 47, 49, 59], and interactive systems <ref> [4, 5, 9, 19, 27, 33, 53, 54] </ref>. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. <p> Here, we take a closer look at related work on discovering auxiliary information for incremental computation. Interactive systems and reactive systems often use incremental algorithms to achieve fast response time <ref> [4, 5, 9, 19, 27, 33, 53, 54] </ref>. Since explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover, the general approach in this paper provides a systematic method for developing particular incremental algorithms.
Reference: [6] <author> F. L. Bauer, B. Moller, H. Partsch, and P. Pepper. </author> <title> Formal program construction by transformations| computer-aided, </title> <journal> intuition-guided programming. IEEE Transactions on Software Engineering, </journal> <volume> 15(2) </volume> <pages> 165-180, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: It helps avoid the kind of errors reported and corrected in [8]. Other work on transformational programming for improving program efficiency, including the extension technique in [17], the transformation of recursive functional programs in the CIP project <ref> [11, 6, 49] </ref>, and the finite differencing of functional programs in the semi-automatic program development system KIDS [59], can also be further automated with our systematic approach. In conclusion, incremental computation has widespread applications throughout computing.
Reference: [7] <author> R. S. Bird. </author> <title> The promotion and accumulation strategies in transformational programming. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(4) </volume> <pages> 487-504, </pages> <month> October </month> <year> 1984. </year> <month> 12 </month>
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers [1, 2, 15, 20, 60], transformational program development <ref> [7, 17, 47, 49, 59] </ref>, and interactive systems [4, 5, 9, 19, 27, 33, 53, 54]. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. <p> We even discover that an unnecessary shift is done in [45]. Thus, a systematic approach such as ours is desirable not only for automating designs and guaranteeing correctness, but also for reducing costs. 6.2 Transformational programming: path sequence problem This example is from <ref> [7] </ref>. Given a directed acyclic graph, and a string whose elements are vertices in the graph, the problem is to compute the length of the longest subsequence in the string that forms a path in the graph. <p> We focus on the second half of the example, where an exponential-time recursive solution is improved (incorrectly in <ref> [7] </ref>, correctly in [8]). The function llp defined below computes the desired length. The input string is given explicitly as the argument to llp. <p> The approach in this paper may be used for systematically constructing induction steps [36] and strengthening induction hypotheses. The promotion and accumulation strategies are proposed by Bird <ref> [7, 8] </ref> as general methods for achieving efficient transformed programs. Promotion attempts to derive a program that defines f (cons (a; x)) in terms of f (x), and accumulation generalizes a definition by including an extra argument. <p> Thus, promotion can be regarded as deriving incremental programs, and accumulation as identifying appropriate intermediate results or auxiliary information. Bird illustrates these strategies with two examples. However, we can discern no systematic steps being followed in <ref> [7] </ref>. As demonstrated with the path sequence problem, our approach can be regarded as a systematic formulation of the promotion and accumulation strategies. It helps avoid the kind of errors reported and corrected in [8].
Reference: [8] <author> R. S. Bird. </author> <title> Addendum: The promotion and accumula-tion strategies in transformational programming. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 490-492, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: We focus on the second half of the example, where an exponential-time recursive solution is improved (incorrectly in [7], correctly in <ref> [8] </ref>). The function llp defined below computes the desired length. The input string is given explicitly as the argument to llp. <p> The approach in this paper may be used for systematically constructing induction steps [36] and strengthening induction hypotheses. The promotion and accumulation strategies are proposed by Bird <ref> [7, 8] </ref> as general methods for achieving efficient transformed programs. Promotion attempts to derive a program that defines f (cons (a; x)) in terms of f (x), and accumulation generalizes a definition by including an extra argument. <p> However, we can discern no systematic steps being followed in [7]. As demonstrated with the path sequence problem, our approach can be regarded as a systematic formulation of the promotion and accumulation strategies. It helps avoid the kind of errors reported and corrected in <ref> [8] </ref>.
Reference: [9] <author> P. Borras and D. Clement. </author> <title> CENTAUR: The system. </title> <booktitle> In Proceedings of the ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, </booktitle> <pages> pages 14-24, </pages> <address> Boston, Mas-sachusetts, </address> <month> November </month> <year> 1988. </year> <note> Published as SIGPLAN Notices, 24(2). </note>
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers [1, 2, 15, 20, 60], transformational program development [7, 17, 47, 49, 59], and interactive systems <ref> [4, 5, 9, 19, 27, 33, 53, 54] </ref>. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. <p> Here, we take a closer look at related work on discovering auxiliary information for incremental computation. Interactive systems and reactive systems often use incremental algorithms to achieve fast response time <ref> [4, 5, 9, 19, 27, 33, 53, 54] </ref>. Since explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover, the general approach in this paper provides a systematic method for developing particular incremental algorithms.
Reference: [10] <author> R. S. Boyer and J. S. Moore. </author> <title> A Computational Logic. </title> <booktitle> ACM Monograph Series. </booktitle> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: This corresponds to discovering appropriate auxiliary information and deriving incremental programs that maintain such information. Work on loop invariants stressed mental tools for programming, rather than mechanical assistance, so no systematic procedures were proposed. Induction and generalization <ref> [10, 44] </ref> are the logical foundations for recursive calls and iterative loops in deductive program synthesis [42] and constructive logics [16]. These corpora have for the most part ignored the efficiency of the programs derived, and the resulting programs "are often wantonly wasteful of time and space" [43]. <p> In contrast, the approach in this paper is particularly concerned with the efficiency of the derived programs. Moreover, we can see that induction, whether course-of-value induction [36], structural induction <ref> [10, 12] </ref>, or well-founded induction [10, 44], enables derived programs to use results of previous iterations in each iteration, and generalization [10, 44] enables derived programs to use appropriate auxiliary information by strengthening induction hypotheses, just like strengthening loop invariants. <p> In contrast, the approach in this paper is particularly concerned with the efficiency of the derived programs. Moreover, we can see that induction, whether course-of-value induction [36], structural induction [10, 12], or well-founded induction <ref> [10, 44] </ref>, enables derived programs to use results of previous iterations in each iteration, and generalization [10, 44] enables derived programs to use appropriate auxiliary information by strengthening induction hypotheses, just like strengthening loop invariants. <p> Moreover, we can see that induction, whether course-of-value induction [36], structural induction [10, 12], or well-founded induction <ref> [10, 44] </ref>, enables derived programs to use results of previous iterations in each iteration, and generalization [10, 44] enables derived programs to use appropriate auxiliary information by strengthening induction hypotheses, just like strengthening loop invariants. The approach in this paper may be used for systematically constructing induction steps [36] and strengthening induction hypotheses.
Reference: [11] <author> M. Broy. </author> <title> Algebraic methods for program construction: The project CIP. </title> <editor> In P. Pepper, editor, </editor> <booktitle> Program Transformation and Programming Environments, volume 8 of NATO Advanced Science Institutes Series F: Computer and System Sciences, </booktitle> <pages> pages 199-222. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year> <booktitle> Proceedings of the NATO Advanced Research Workshop on Program Transformation and Programming Environments, </booktitle> <editor> directed by F. L. Bauer and H. Remus, </editor> <address> Munich, Germany, </address> <month> September </month> <year> 1983. </year>
Reference-contexts: It helps avoid the kind of errors reported and corrected in [8]. Other work on transformational programming for improving program efficiency, including the extension technique in [17], the transformation of recursive functional programs in the CIP project <ref> [11, 6, 49] </ref>, and the finite differencing of functional programs in the semi-automatic program development system KIDS [59], can also be further automated with our systematic approach. In conclusion, incremental computation has widespread applications throughout computing.
Reference: [12] <author> R. M. Burstall. </author> <title> Proving properties of programs by structural induction. </title> <journal> The Computer Journal, </journal> <volume> 12(1) </volume> <pages> 41-48, </pages> <year> 1969. </year>
Reference-contexts: In contrast, the approach in this paper is particularly concerned with the efficiency of the derived programs. Moreover, we can see that induction, whether course-of-value induction [36], structural induction <ref> [10, 12] </ref>, or well-founded induction [10, 44], enables derived programs to use results of previous iterations in each iteration, and generalization [10, 44] enables derived programs to use appropriate auxiliary information by strengthening induction hypotheses, just like strengthening loop invariants.
Reference: [13] <author> R. M. Burstall and J. Darlington. </author> <title> A transformation system for developing recursive programs. </title> <journal> Journal of the ACM, </journal> <volume> 24(1) </volume> <pages> 44-67, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: The integrated computation is usually more efficient; so is its incremental version. 8 We do not describe the integration in detail. Basically, it uses traditional transformation techniques <ref> [13] </ref> like those used in tupling tactic [21, 50, 14].
Reference: [14] <author> W.-N. Chin. </author> <title> Towards an automated tupling strategy. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on PEPM, </booktitle> <address> Copenhagen, Denmark, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The integrated computation is usually more efficient; so is its incremental version. 8 We do not describe the integration in detail. Basically, it uses traditional transformation techniques [13] like those used in tupling tactic <ref> [21, 50, 14] </ref>.
Reference: [15] <author> J. Cocke and K. Kennedy. </author> <title> An algorithm for reduction of operator strength. </title> <journal> Communications of the ACM, </journal> 20(11) 850-856, November 1977. 
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers <ref> [1, 2, 15, 20, 60] </ref>, transformational program development [7, 17, 47, 49, 59], and interactive systems [4, 5, 9, 19, 27, 33, 53, 54]. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. <p> For static incremental attribute evaluation algorithms [34, 35], where no auxiliary information is needed, the approach can cache intermediate results and maintain them automatically [40]. 11 Strength reduction <ref> [2, 15, 60] </ref> is a traditional compiler op-timization technique that aims at computing each iteration incrementally based on the result of the previous iteration. Basically, a fixed set of strength-reduction rules for primitive operators like times and plus are used.
Reference: [16] <author> R. L. Constable et al. </author> <title> Implementing Mathematics with the Nuprl Proof Development System. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1986. </year>
Reference-contexts: In that work, a strength-reduced program was manually discovered and then proved correct using Nuprl <ref> [16] </ref>. Here, we show how our method can automatically derive the strength reductions. This is of particular interest in light of the recent Pentium chip flaw [24]. <p> Work on loop invariants stressed mental tools for programming, rather than mechanical assistance, so no systematic procedures were proposed. Induction and generalization [10, 44] are the logical foundations for recursive calls and iterative loops in deductive program synthesis [42] and constructive logics <ref> [16] </ref>. These corpora have for the most part ignored the efficiency of the programs derived, and the resulting programs "are often wantonly wasteful of time and space" [43]. In contrast, the approach in this paper is particularly concerned with the efficiency of the derived programs.
Reference: [17] <author> N. Dershowitz. </author> <title> The Evolution of Programs, </title> <booktitle> volume 5 of Progress in Computer Science. </booktitle> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1983. </year>
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers [1, 2, 15, 20, 60], transformational program development <ref> [7, 17, 47, 49, 59] </ref>, and interactive systems [4, 5, 9, 19, 27, 33, 53, 54]. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. <p> As demonstrated with the path sequence problem, our approach can be regarded as a systematic formulation of the promotion and accumulation strategies. It helps avoid the kind of errors reported and corrected in [8]. Other work on transformational programming for improving program efficiency, including the extension technique in <ref> [17] </ref>, the transformation of recursive functional programs in the CIP project [11, 6, 49], and the finite differencing of functional programs in the semi-automatic program development system KIDS [59], can also be further automated with our systematic approach. In conclusion, incremental computation has widespread applications throughout computing.
Reference: [18] <author> E. W. Dijkstra. </author> <title> A Discipline of Programming. </title> <booktitle> Prentice-Hall Series in Automatic Computation. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1976. </year>
Reference-contexts: In general, such rules apply only to very-high-level languages like SETL; our method applies also to lower-level languages like Lisp. Maintaining and strengthening loop invariants has been advocated by Dijkstra, Gries, and others <ref> [18, 25, 26, 56] </ref> for almost two decades as a standard strategy for developing loops. In order to produce efficient programs, loop invariants need to be maintained by the derived programs in an incremental fashion.
Reference: [19] <author> V. Donzeau-Gouge, G. Huet, G. Kahn, and B. Lang. </author> <title> Programming environments based on structure editor: The Mentor experience. </title> <editor> In D. R. Barstow, H. E. Shrobe, and E. Sandewall, editors, </editor> <booktitle> Interactive Programming Environments, </booktitle> <pages> pages 128-140. </pages> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers [1, 2, 15, 20, 60], transformational program development [7, 17, 47, 49, 59], and interactive systems <ref> [4, 5, 9, 19, 27, 33, 53, 54] </ref>. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. <p> Here, we take a closer look at related work on discovering auxiliary information for incremental computation. Interactive systems and reactive systems often use incremental algorithms to achieve fast response time <ref> [4, 5, 9, 19, 27, 33, 53, 54] </ref>. Since explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover, the general approach in this paper provides a systematic method for developing particular incremental algorithms.
Reference: [20] <author> J. Earley. </author> <title> High level iterators and a method for automatically designing data structure representation. </title> <journal> Journal of Computer Languages, </journal> <volume> 1 </volume> <pages> 321-342, </pages> <year> 1976. </year>
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers <ref> [1, 2, 15, 20, 60] </ref>, transformational program development [7, 17, 47, 49, 59], and interactive systems [4, 5, 9, 19, 27, 33, 53, 54]. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs.
Reference: [21] <author> M. S. Feather. </author> <title> A system for assisting program transformation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(1) </volume> <pages> 1-20, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: The integrated computation is usually more efficient; so is its incremental version. 8 We do not describe the integration in detail. Basically, it uses traditional transformation techniques [13] like those used in tupling tactic <ref> [21, 50, 14] </ref>.
Reference: [22] <author> J. Field and T. Teitelbaum. </author> <title> Incremental reduction in the lambda calculus. </title> <booktitle> In Proceedings of the ACM '90 Conference on LFP, </booktitle> <pages> pages 307-322, </pages> <year> 1990. </year>
Reference: [23] <author> I. Flores. </author> <booktitle> The Logic of Computer Arithmetic. Prentice-Hall International Series in Electrical Engineering. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1963. </year>
Reference-contexts: Here, we show how our method can automatically derive the strength reductions. This is of particular interest in light of the recent Pentium chip flaw [24]. The initial specification of the l-bit non-restoring binary integer square root algorithm <ref> [23, 45] </ref>, which is exact for perfect squares and off by at most 1 for other integers, is m := 2 l1 for i := l 2 downto 0 do p := n m 2 ; if p &gt; 0 then m := m + 2 i else if p &lt;
Reference: [24] <author> J. Glanz. </author> <title> Mathematical logic flushes out the bugs in chip designs. </title> <journal> Science, </journal> <volume> 267 </volume> <pages> 332-333, </pages> <month> January 20, </month> <year> 1995. </year>
Reference-contexts: In that work, a strength-reduced program was manually discovered and then proved correct using Nuprl [16]. Here, we show how our method can automatically derive the strength reductions. This is of particular interest in light of the recent Pentium chip flaw <ref> [24] </ref>.
Reference: [25] <editor> D. Gries. </editor> <booktitle> The Science of Programming. Texts and Monographs in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: In general, such rules apply only to very-high-level languages like SETL; our method applies also to lower-level languages like Lisp. Maintaining and strengthening loop invariants has been advocated by Dijkstra, Gries, and others <ref> [18, 25, 26, 56] </ref> for almost two decades as a standard strategy for developing loops. In order to produce efficient programs, loop invariants need to be maintained by the derived programs in an incremental fashion.
Reference: [26] <author> D. Gries. </author> <title> A note on a standard strategy for developing loop invariants and loops. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 2 </volume> <pages> 207-214, </pages> <year> 1984. </year>
Reference-contexts: In general, such rules apply only to very-high-level languages like SETL; our method applies also to lower-level languages like Lisp. Maintaining and strengthening loop invariants has been advocated by Dijkstra, Gries, and others <ref> [18, 25, 26, 56] </ref> for almost two decades as a standard strategy for developing loops. In order to produce efficient programs, loop invariants need to be maintained by the derived programs in an incremental fashion. <p> In order to produce efficient programs, loop invariants need to be maintained by the derived programs in an incremental fashion. To make a loop more efficient, the strategy of strengthening a loop invariant, often by introducing fresh variables, is proposed <ref> [26] </ref>. This corresponds to discovering appropriate auxiliary information and deriving incremental programs that maintain such information. Work on loop invariants stressed mental tools for programming, rather than mechanical assistance, so no systematic procedures were proposed.
Reference: [27] <author> A. N. Habermann and D. Notkin. </author> <title> Gandalf: Software development environments. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-12(12):1117-1127, </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers [1, 2, 15, 20, 60], transformational program development [7, 17, 47, 49, 59], and interactive systems <ref> [4, 5, 9, 19, 27, 33, 53, 54] </ref>. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. <p> Here, we take a closer look at related work on discovering auxiliary information for incremental computation. Interactive systems and reactive systems often use incremental algorithms to achieve fast response time <ref> [4, 5, 9, 19, 27, 33, 53, 54] </ref>. Since explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover, the general approach in this paper provides a systematic method for developing particular incremental algorithms.
Reference: [28] <author> R. Hoover. Alphonse: </author> <title> Incremental computation as a programming abstraction. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on PLDI, </booktitle> <pages> pages 261-272, </pages> <address> California, </address> <month> June </month> <year> 1992. </year>
Reference: [29] <author> S. Horwitz and T. Teitelbaum. </author> <title> Generating editing environments based on relations and attributes. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 577-608, </pages> <month> October </month> <year> 1986. </year>
Reference: [30] <author> F. Jalili and J. H. Gallier. </author> <title> Building friendly parsers. </title> <booktitle> In Conference Record of the 9th Annual ACM Symposium on POPL, </booktitle> <pages> pages 196-206, </pages> <address> Albuquerque, New Mexico, </address> <month> January </month> <year> 1982. </year>
Reference: [31] <author> N. D. Jones, C. K. Gomard, and P. Sestoft. </author> <title> Partial Evaluation and Automatic Program Generation. </title> <publisher> Pren-tice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1993. </year>
Reference-contexts: Although this forward dependence analysis is equivalent to binding time analysis, the application here is different from that in partial evaluation <ref> [31] </ref>. In partial evaluation, the goal is to obtain a residual program that is specialized on a given set of static arguments and takes only the dy namic arguments, while here, we construct a program that computes only on the "static" arguments.
Reference: [32] <author> N. D. Jones, P. Sestoft, and H. Stndergaard. </author> <title> An experiment in partial evaluation: The generation of a compiler generator. </title> <editor> In J.-P. Jouannaud, editor, </editor> <booktitle> Rewriting Techniques and Applications, volume 202 of Lecture Notes in Computer Science, </booktitle> <pages> pages 124-140, </pages> <address> Dijon, France, May 1985. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: Forward dependence analysis. First, we use a forward dependence analysis to identify subcomputations of f 0 8 (x; y; r) that depend only on x and r. The analysis is in the same spirit as binding-time analysis <ref> [32, 37] </ref> for partial evaluation, if we regard the arguments corresponding to x and r as static and the rest as dynamic. We compute the following sets, called forward dependency sets, directly.
Reference: [33] <author> G. E. Kaiser. </author> <title> Incremental dynamic semantics for language-based programming environments. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(2) </volume> <pages> 168-193, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers [1, 2, 15, 20, 60], transformational program development [7, 17, 47, 49, 59], and interactive systems <ref> [4, 5, 9, 19, 27, 33, 53, 54] </ref>. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. <p> Here, we take a closer look at related work on discovering auxiliary information for incremental computation. Interactive systems and reactive systems often use incremental algorithms to achieve fast response time <ref> [4, 5, 9, 19, 27, 33, 53, 54] </ref>. Since explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover, the general approach in this paper provides a systematic method for developing particular incremental algorithms.
Reference: [34] <author> U. Kastens. </author> <title> Ordered attributed grammars. </title> <journal> Acta Infor-matica, </journal> <volume> 13(3) </volume> <pages> 229-256, </pages> <year> 1980. </year>
Reference-contexts: For example, for the dynamic incremental attribute evaluation algorithm in [55], the characteristic graph is a kind of auxiliary information that would be discovered following the general principles underlying our approach. For static incremental attribute evaluation algorithms <ref> [34, 35] </ref>, where no auxiliary information is needed, the approach can cache intermediate results and maintain them automatically [40]. 11 Strength reduction [2, 15, 60] is a traditional compiler op-timization technique that aims at computing each iteration incrementally based on the result of the previous iteration.
Reference: [35] <author> T. Katayama. </author> <title> Translation of attribute grammars into procedures. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(3) </volume> <pages> 345-369, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: For example, for the dynamic incremental attribute evaluation algorithm in [55], the characteristic graph is a kind of auxiliary information that would be discovered following the general principles underlying our approach. For static incremental attribute evaluation algorithms <ref> [34, 35] </ref>, where no auxiliary information is needed, the approach can cache intermediate results and maintain them automatically [40]. 11 Strength reduction [2, 15, 60] is a traditional compiler op-timization technique that aims at computing each iteration incrementally based on the result of the previous iteration.
Reference: [36] <author> S. C. Kleene. </author> <title> Introduction to Metamathematics. </title> <publisher> Van Nostrand, </publisher> <address> New York, </address> <year> 1952. </year> <booktitle> Tenth reprint, </booktitle> <publisher> Wolters-Noordhoff Publishing, Groningen and North-Holland Publishing Company, </publisher> <address> Amsterdam, </address> <year> 1991. </year>
Reference-contexts: In contrast, the approach in this paper is particularly concerned with the efficiency of the derived programs. Moreover, we can see that induction, whether course-of-value induction <ref> [36] </ref>, structural induction [10, 12], or well-founded induction [10, 44], enables derived programs to use results of previous iterations in each iteration, and generalization [10, 44] enables derived programs to use appropriate auxiliary information by strengthening induction hypotheses, just like strengthening loop invariants. <p> The approach in this paper may be used for systematically constructing induction steps <ref> [36] </ref> and strengthening induction hypotheses. The promotion and accumulation strategies are proposed by Bird [7, 8] as general methods for achieving efficient transformed programs.
Reference: [37] <author> J. Launchbury. </author> <title> Projections for specialisation. </title> <booktitle> In Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 299-315. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year> <month> 13 </month>
Reference-contexts: Forward dependence analysis. First, we use a forward dependence analysis to identify subcomputations of f 0 8 (x; y; r) that depend only on x and r. The analysis is in the same spirit as binding-time analysis <ref> [32, 37] </ref> for partial evaluation, if we regard the arguments corresponding to x and r as static and the rest as dynamic. We compute the following sets, called forward dependency sets, directly.
Reference: [38] <author> Y. A. Liu. CACHET: </author> <title> An interactive, incremental--attribution-based program transformation system for deriving incremental programs. </title> <booktitle> In Proceedings of the 10th Knowledge-Based Software Engineering Conference, </booktitle> <address> Boston, Massachusetts, November 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: For example, the method has been used to improve imperative programs with arrays for the local neighborhood problems in image processing [39]. A prototype system, CACHET <ref> [38] </ref>, based on our approach is under development.
Reference: [39] <author> Y. A. Liu. </author> <title> Incremental Computation: A Semantics-Based Systematic Transformational Approach. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> January </month> <year> 1996. </year> <note> To appear as Cornell Technical Report, </note> <month> October, </month> <year> 1995. </year>
Reference-contexts: Although our approach is presented in terms of a first-order functional language with strict semantics, the underlying principles are general and apply to other languages as well. For example, the method has been used to improve imperative programs with arrays for the local neighborhood problems in image processing <ref> [39] </ref>. A prototype system, CACHET [38], based on our approach is under development.
Reference: [40] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Caching intermediate results for program improvement. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on PEPM, </booktitle> <pages> pages 190-201, </pages> <address> La Jolla, California, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Caching, maintaining, and exploiting intermediate results of the computation f (x). * P3. Discovering, computing, maintaining, and exploiting auxiliary information about x, i.e., information not computed by f (x). Our current approaches to problems P1 and P2 are described in [41] and <ref> [40] </ref>, respectively. In this paper, we address issue P3 for the first time and contribute: * A novel proposal for finding auxiliary information. * A comprehensive methodology for deriving incremen tal programs that addresses all three subproblems. <p> The computation of f (x y) is symbolically transformed to avoid re-performing these subcomputations by replacing them with corresponding retrievals. This efficient way of computing f (x y) is captured in the definition of f 0 (x; y; r). P2. In <ref> [40] </ref>, we gave a method, called cache-and-prune, for statically transforming programs to cache all intermediate results useful for incremental computation. <p> null (x) then 0 else car (x) + sum (cdr (x)) prod (x) = if null (x) then 1 else car (x) fl prod (cdr (x)) In order to use also intermediate results of f 0 (x) to compute f 0 (x y) possibly faster, we use the approach in <ref> [40] </ref> to cache useful intermediate results of f 0 and obtain a program that incrementally computes the return value and maintains these intermediate results. <p> Step 3 constructs a function f 0 that computes only the candidate auxiliary information in f 0 8 . 3.1 Step A.1: Caching all intermediate re sults Extending f 0 to cache all intermediate results uses the transformations in Stage I of <ref> [40] </ref>. It first performs a straightforward extension transformation to embed all intermediate results in the final return value and then performs adminis trative simplifications. Certain improvements to the extension transformation are suggested, although not given, in [40] to avoid caching redundant intermediate results, i.e., values of function applications that are already <p> f 0 to cache all intermediate results uses the transformations in Stage I of <ref> [40] </ref>. It first performs a straightforward extension transformation to embed all intermediate results in the final return value and then performs adminis trative simplifications. Certain improvements to the extension transformation are suggested, although not given, in [40] to avoid caching redundant intermediate results, i.e., values of function applications that are already embedded in the values of their enclosing computations, since these omitted values can be re 3 If sum (x) = r, then sum 0 (y; r) = sum (cons (y; x)). <p> These improvements also benefit the modified version of this extension transformation used in Step A.3. We first briefly describe the extension transformation in <ref> [40] </ref>; then, we describe an embedding analysis that leads to the desired improvements to the extension transformation. Extension transformation. <p> ) &gt;; 2nd ( r ); 7th ( r ); (12) Clearly, cmp 0 (y; r ) computes cmp (cons (y; x)) in only O (1) time. 4.3 Step B.3: Pruning To prune f 0 and f 0 0 , we use the analyses and transformations in Stage III of <ref> [40] </ref>. A backward dependence analysis determines the components of r and subcomputations of f 0 0 whose values are useful in computing 0 ( f 0 0 (x; y; r )), which is the value of f 0 . A pruning transformation replaces use less computations with . <p> Automatic space analysis and the trade-off between time and space are problems open for study. Suppose Step B.1 projects out the original value using 1st. With the above condition, in a similar way to <ref> [40] </ref>, we can show that, if f 0 (x) = r, then 1st ( e f 0 (x)) = r and t ( e f 0 (x)) t (f 0 (x)) (13) and if f 0 (x y) = r 0 and e f 0 (x) = er, then 1st ( <p> For static incremental attribute evaluation algorithms [34, 35], where no auxiliary information is needed, the approach can cache intermediate results and maintain them automatically <ref> [40] </ref>. 11 Strength reduction [2, 15, 60] is a traditional compiler op-timization technique that aims at computing each iteration incrementally based on the result of the previous iteration. Basically, a fixed set of strength-reduction rules for primitive operators like times and plus are used.
Reference: [41] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Systematic derivation of incremental programs. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 24(1) </volume> <pages> 1-39, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Caching, maintaining, and exploiting intermediate results of the computation f (x). * P3. Discovering, computing, maintaining, and exploiting auxiliary information about x, i.e., information not computed by f (x). Our current approaches to problems P1 and P2 are described in <ref> [41] </ref> and [40], respectively. In this paper, we address issue P3 for the first time and contribute: * A novel proposal for finding auxiliary information. * A comprehensive methodology for deriving incremen tal programs that addresses all three subproblems. <p> The modular components complement one another to form a comprehensive principled approach for incremental computation and therefore also for efficient iterative computation generally. Although the entire approach seems complex, each module or step is simple. We summarize here the essence of our methods: P1. In <ref> [41] </ref>, we gave a systematic transformational approach for deriving an incremental version f 0 of a program f under an input change . <p> Our primary goal is to improve the asymptotic running time of the incremental computation. We attempt to save space by maintaining only information useful for achieving this. Given a program f 0 and an input change operation , we use the approach in <ref> [41] </ref> to derive an incremental version f 0 0 of f 0 under , such that, if f 0 (x) = r, then whenever f 0 (x y) returns a value, f 0 0 (x; y; r) returns the same value and is asymptotically at least as fast. 2 For example, <p> Note that some of the parameters of f 0 0 may be dead and eliminated <ref> [41] </ref>. 2 cmp (x) = sum (odd (x)) prod (even (x)) | compare sum of odd and product of even positions of list x o d d (x) = if null (x) then nil else cons (car (x); even (cdr (x))) even (x) = if null (x) then nil else odd <p> It uses analyses and transformations similar to those in <ref> [41] </ref> that derive an incremental program f 0 0 (x; y; r), by expanding subcomputations of f 0 (x y) depending on both x and y and replacing those depending only on x by retrievals from r when possible. <p> Our goal here is not to quickly retrieve values from r, but to find potentially useful auxiliary information, i.e., subcom-putations depending on x (and r) but not y whose values can not be retrieved from r. Thus, time considerations in <ref> [41] </ref> are dropped here but are picked up after Step A.3, as discussed in Section 5. In particular, in [41], a recursive application of a function f is replaced by an application of an incremental version f 0 only if a fast retrieval from some cached result of the previous computation <p> Thus, time considerations in <ref> [41] </ref> are dropped here but are picked up after Step A.3, as discussed in Section 5. In particular, in [41], a recursive application of a function f is replaced by an application of an incremental version f 0 only if a fast retrieval from some cached result of the previous computation can be used as the argument for the parameter of f 0 that corresponds to a cached result. <p> For example, if an incremental version f 0 (x; y; r) is introduced to compute f (x y) incrementally for r = f (x), then in <ref> [41] </ref>, a function application f (g (x)h (y)) is replaced by an application of f 0 only if some fast retrieval p (r) for the value of f (g (x)) can be used as the argument for the parameter r of f 0 (x; y; r), in which case the application <p> It is easy to see that, in this case, f (g (x)) becomes a piece of candidate auxiliary information. Since the functions obtained from this step may be different from the incremental functions f 0 obtained in <ref> [41] </ref>, we denote them by f 8 . <p> 2 ; u 2 ; sum (v 2 ); prod (v 1 ) &gt; (11) and the projection 0 ( r ) = 1st ( r ). 4.2 Step B.2: Incrementalization To derive an incremental version f 0 0 of f 0 under , we can use the method in <ref> [41] </ref>, as sketched in Section 1. Depending on the power expected from the derivation, the method can be made semi-automatic or fully automatic.
Reference: [42] <author> Z. Manna and R. Waldinger. </author> <title> A deductive approach to program synthesis. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 2(1) </volume> <pages> 90-121, </pages> <month> January </month> <year> 1980. </year>
Reference-contexts: Work on loop invariants stressed mental tools for programming, rather than mechanical assistance, so no systematic procedures were proposed. Induction and generalization [10, 44] are the logical foundations for recursive calls and iterative loops in deductive program synthesis <ref> [42] </ref> and constructive logics [16]. These corpora have for the most part ignored the efficiency of the programs derived, and the resulting programs "are often wantonly wasteful of time and space" [43]. In contrast, the approach in this paper is particularly concerned with the efficiency of the derived programs.
Reference: [43] <author> Z. Manna and R. Waldinger. </author> <title> Fundamentals of deductive program synthesis. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(8) </volume> <pages> 674-704, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: These corpora have for the most part ignored the efficiency of the programs derived, and the resulting programs "are often wantonly wasteful of time and space" <ref> [43] </ref>. In contrast, the approach in this paper is particularly concerned with the efficiency of the derived programs.
Reference: [44] <author> Z. Manna and R. Waldinger. </author> <booktitle> The Deductive Foundations of Computer Programming. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1993. </year>
Reference-contexts: This corresponds to discovering appropriate auxiliary information and deriving incremental programs that maintain such information. Work on loop invariants stressed mental tools for programming, rather than mechanical assistance, so no systematic procedures were proposed. Induction and generalization <ref> [10, 44] </ref> are the logical foundations for recursive calls and iterative loops in deductive program synthesis [42] and constructive logics [16]. These corpora have for the most part ignored the efficiency of the programs derived, and the resulting programs "are often wantonly wasteful of time and space" [43]. <p> In contrast, the approach in this paper is particularly concerned with the efficiency of the derived programs. Moreover, we can see that induction, whether course-of-value induction [36], structural induction [10, 12], or well-founded induction <ref> [10, 44] </ref>, enables derived programs to use results of previous iterations in each iteration, and generalization [10, 44] enables derived programs to use appropriate auxiliary information by strengthening induction hypotheses, just like strengthening loop invariants. <p> Moreover, we can see that induction, whether course-of-value induction [36], structural induction [10, 12], or well-founded induction <ref> [10, 44] </ref>, enables derived programs to use results of previous iterations in each iteration, and generalization [10, 44] enables derived programs to use appropriate auxiliary information by strengthening induction hypotheses, just like strengthening loop invariants. The approach in this paper may be used for systematically constructing induction steps [36] and strengthening induction hypotheses.
Reference: [45] <author> J. O'Leary, M. Leeser, J. Hickey, and M. Aagaard. </author> <title> Non-restoring integer square root: A case study in design by principled optimization. </title> <editor> In R. Kumar and T. Kropf, editors, </editor> <booktitle> Proceedings of TPCD '94: the 2nd International Conference on Theorem Provers in Circuit Design| Theory, Practice and Experience, volume 901 of Lecture Notes in Computer Science, </booktitle> <pages> pages 52-71, </pages> <address> Bad Herrenalb (Black Forest), Germany, September 1994. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: Other applications include optimizing compilers and transformational programming. This section presents an example for each of these two applications. The examples are based on problems in VLSI design and graph algorithms, respectively. 6.1 Strength reduction in optimizing com pilers: binary integer square root This example is from <ref> [45] </ref>, where a specification of a non-restoring binary integer square root algorithm is transformed into a VLSI circuit design and implementation. In that work, a strength-reduced program was manually discovered and then proved correct using Nuprl [16]. Here, we show how our method can automatically derive the strength reductions. <p> Here, we show how our method can automatically derive the strength reductions. This is of particular interest in light of the recent Pentium chip flaw [24]. The initial specification of the l-bit non-restoring binary integer square root algorithm <ref> [23, 45] </ref>, which is exact for perfect squares and off by at most 1 for other integers, is m := 2 l1 for i := l 2 downto 0 do p := n m 2 ; if p &gt; 0 then m := m + 2 i else if p &lt; <p> We even discover that an unnecessary shift is done in <ref> [45] </ref>. Thus, a systematic approach such as ours is desirable not only for automating designs and guaranteeing correctness, but also for reducing costs. 6.2 Transformational programming: path sequence problem This example is from [7].
Reference: [46] <author> B. Paige and J. T. Schwartz. </author> <title> Expression continuity and the formal differentiation of algorithms. </title> <booktitle> In Conference Record of the 4th Annual ACM Symposium on POPL, </booktitle> <pages> pages 58-71, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: Finite differencing <ref> [46, 47, 48] </ref> generalizes strength reduction to set-theoretic expressions for systematic program development. Basically, rules are manually developed for differentiating set expressions. For continuous expressions, our method can derive such rules directly using properties of primitive set operations.
Reference: [47] <author> R. Paige. </author> <title> Transformational programming|applications to algorithms and systems. </title> <booktitle> In Conference Record of the 10th Annual ACM Symposium on POPL, </booktitle> <pages> pages 73-87, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers [1, 2, 15, 20, 60], transformational program development <ref> [7, 17, 47, 49, 59] </ref>, and interactive systems [4, 5, 9, 19, 27, 33, 53, 54]. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. <p> Finite differencing <ref> [46, 47, 48] </ref> generalizes strength reduction to set-theoretic expressions for systematic program development. Basically, rules are manually developed for differentiating set expressions. For continuous expressions, our method can derive such rules directly using properties of primitive set operations.
Reference: [48] <author> R. Paige and S. Koenig. </author> <title> Finite differencing of computable expressions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(3) </volume> <pages> 402-454, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: Efficient iterative computation relies on effective use of state, i.e., computing the result of each iteration using stored results of previous iterations. This is why strength reduction [2] and related techniques <ref> [48] </ref> are crucial for performance. Given a program f and an input change operation , a program f 0 that computes f (x y) efficiently by using the result of the previous computation of f (x) is called an incremental version of f under . <p> Some approaches to incremental computation have exploited specific kinds of auxiliary information, e.g., auxiliary arithmetic associated with some classical strength-reduction rules [2], dynamic mappings maintained by finite differencing rules for aggregate primitives in SETL <ref> [48] </ref> and INC [64], and auxiliary data structures for problems with certain properties like stable decomposition [52]. However, until now, systematic discovery of auxiliary information for arbitrary programs has been a subject completely open for study. <p> Finite differencing <ref> [46, 47, 48] </ref> generalizes strength reduction to set-theoretic expressions for systematic program development. Basically, rules are manually developed for differentiating set expressions. For continuous expressions, our method can derive such rules directly using properties of primitive set operations.
Reference: [49] <author> H. A. Partsch. </author> <title> Specification and Transformation of Programs|A Formal Approach to Software Development. Texts and Monographs in Computer Science. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers [1, 2, 15, 20, 60], transformational program development <ref> [7, 17, 47, 49, 59] </ref>, and interactive systems [4, 5, 9, 19, 27, 33, 53, 54]. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. <p> It helps avoid the kind of errors reported and corrected in [8]. Other work on transformational programming for improving program efficiency, including the extension technique in [17], the transformation of recursive functional programs in the CIP project <ref> [11, 6, 49] </ref>, and the finite differencing of functional programs in the semi-automatic program development system KIDS [59], can also be further automated with our systematic approach. In conclusion, incremental computation has widespread applications throughout computing.
Reference: [50] <author> A. Pettorossi. </author> <title> A powerful strategy for deriving efficient programs by transformation. </title> <booktitle> In Proceedings of the ACM '84 Symposium on LFP, </booktitle> <address> Austin, Texas, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: The integrated computation is usually more efficient; so is its incremental version. 8 We do not describe the integration in detail. Basically, it uses traditional transformation techniques [13] like those used in tupling tactic <ref> [21, 50, 14] </ref>.
Reference: [51] <author> L. L. Pollock and M. L. Soffa. </author> <title> Incremental global reoptimization of programs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 14(2) </volume> <pages> 173-200, </pages> <month> April </month> <year> 1992. </year>
Reference: [52] <author> W. Pugh and T. Teitelbaum. </author> <title> Incremental computation via function caching. </title> <booktitle> In Conference Record of the 16th Annual ACM Symposium on POPL, </booktitle> <pages> pages 315-328, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Some approaches to incremental computation have exploited specific kinds of auxiliary information, e.g., auxiliary arithmetic associated with some classical strength-reduction rules [2], dynamic mappings maintained by finite differencing rules for aggregate primitives in SETL [48] and INC [64], and auxiliary data structures for problems with certain properties like stable decomposition <ref> [52] </ref>. However, until now, systematic discovery of auxiliary information for arbitrary programs has been a subject completely open for study. Auxiliary information is, by definition, useful information about x that is not computed by f (x). Where, then, can one find it? The key insight of our proposal is: A.
Reference: [53] <author> S. P. Reiss. </author> <title> An approach to incremental compilation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '84 Symposium on Compiler Construction, </booktitle> <pages> pages 144-156, </pages> <address> Montreal, Canada, </address> <month> June </month> <year> 1984. </year> <note> Published as SIGPLAN Notices, 19(6). </note>
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers [1, 2, 15, 20, 60], transformational program development [7, 17, 47, 49, 59], and interactive systems <ref> [4, 5, 9, 19, 27, 33, 53, 54] </ref>. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. <p> Here, we take a closer look at related work on discovering auxiliary information for incremental computation. Interactive systems and reactive systems often use incremental algorithms to achieve fast response time <ref> [4, 5, 9, 19, 27, 33, 53, 54] </ref>. Since explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover, the general approach in this paper provides a systematic method for developing particular incremental algorithms.
Reference: [54] <author> T. Reps and T. Teitelbaum. </author> <title> The Synthesizer Generator: A System for Constructing Language-Based Editors. Texts and Monographs in Computer Science. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers [1, 2, 15, 20, 60], transformational program development [7, 17, 47, 49, 59], and interactive systems <ref> [4, 5, 9, 19, 27, 33, 53, 54] </ref>. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. <p> Here, we take a closer look at related work on discovering auxiliary information for incremental computation. Interactive systems and reactive systems often use incremental algorithms to achieve fast response time <ref> [4, 5, 9, 19, 27, 33, 53, 54] </ref>. Since explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover, the general approach in this paper provides a systematic method for developing particular incremental algorithms.
Reference: [55] <author> T. Reps, T. Teitelbaum, and A. Demers. </author> <title> Incremental context-dependent analysis for language-based editors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 449-477, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: Since explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover, the general approach in this paper provides a systematic method for developing particular incremental algorithms. For example, for the dynamic incremental attribute evaluation algorithm in <ref> [55] </ref>, the characteristic graph is a kind of auxiliary information that would be discovered following the general principles underlying our approach.
Reference: [56] <author> J. C. Reynolds. </author> <title> The Craft of Programming. </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: In general, such rules apply only to very-high-level languages like SETL; our method applies also to lower-level languages like Lisp. Maintaining and strengthening loop invariants has been advocated by Dijkstra, Gries, and others <ref> [18, 25, 26, 56] </ref> for almost two decades as a standard strategy for developing loops. In order to produce efficient programs, loop invariants need to be maintained by the derived programs in an incremental fashion.
Reference: [57] <author> M. Rosendahl. </author> <title> Automatic complexity analysis. </title> <booktitle> In Proceedings of the 4th International Conference on FPCA, </booktitle> <pages> pages 144-156, </pages> <address> London, U.K., </address> <month> September </month> <year> 1989. </year>
Reference-contexts: This can be checked after Step A.3. We guarantee this condition by simply dropping pieces of candidate auxiliary information for which it can not be confirmed. Standard constructions for mechanical time analysis <ref> [57, 62] </ref> can be used, although further study is needed. Automatic space analysis and the trade-off between time and space are problems open for study. Suppose Step B.1 projects out the original value using 1st.
Reference: [58] <author> B. G. Ryder and M. C. Paull. </author> <title> Incremental data flow analysis algorithms. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(1) </volume> <pages> 1-50, </pages> <month> January </month> <year> 1988. </year>
Reference: [59] <author> D. R. Smith. KIDS: </author> <title> A semiautomatic program development system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(9) </volume> <pages> 1024-1043, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers [1, 2, 15, 20, 60], transformational program development <ref> [7, 17, 47, 49, 59] </ref>, and interactive systems [4, 5, 9, 19, 27, 33, 53, 54]. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. <p> Other work on transformational programming for improving program efficiency, including the extension technique in [17], the transformation of recursive functional programs in the CIP project [11, 6, 49], and the finite differencing of functional programs in the semi-automatic program development system KIDS <ref> [59] </ref>, can also be further automated with our systematic approach. In conclusion, incremental computation has widespread applications throughout computing. This paper proposes a systematic approach for discovering a general class of auxiliary information for incremental computation.
Reference: [60] <author> B. Steffen, J. Knoop, and O. Ruthing. </author> <title> Efficient code motion and an adaption to strength reduction. </title> <booktitle> In Proceedings of the 4th International Joint Conference on TAPSOFT, volume 494 of Lecture Notes in Computer Science, </booktitle> <pages> pages 394-415, </pages> <address> Brighton, U.K., 1991. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: Incremental computation is a fundamental issue relevant throughout computer software, e.g., optimizing compilers <ref> [1, 2, 15, 20, 60] </ref>, transformational program development [7, 17, 47, 49, 59], and interactive systems [4, 5, 9, 19, 27, 33, 53, 54]. Numerous techniques for incremental computation have been developed, e.g., [2, 3, 22, 28, 29, 30, 41, 48, 51, 52, 55, 58, 61, 64]. Deriving incremental programs. <p> For static incremental attribute evaluation algorithms [34, 35], where no auxiliary information is needed, the approach can cache intermediate results and maintain them automatically [40]. 11 Strength reduction <ref> [2, 15, 60] </ref> is a traditional compiler op-timization technique that aims at computing each iteration incrementally based on the result of the previous iteration. Basically, a fixed set of strength-reduction rules for primitive operators like times and plus are used.
Reference: [61] <author> R. S. Sundaresh and P. Hudak. </author> <title> Incremental computation via partial evaluation. </title> <booktitle> In Conference Record of the 18th Annual ACM Symposium on POPL, </booktitle> <pages> pages 1-13, </pages> <month> January </month> <year> 1991. </year>
Reference: [62] <author> B. Wegbreit. </author> <title> Mechanical program analysis. </title> <journal> Communications of the ACM, </journal> <volume> 18(9) </volume> <pages> 528-538, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: This can be checked after Step A.3. We guarantee this condition by simply dropping pieces of candidate auxiliary information for which it can not be confirmed. Standard constructions for mechanical time analysis <ref> [57, 62] </ref> can be used, although further study is needed. Automatic space analysis and the trade-off between time and space are problems open for study. Suppose Step B.1 projects out the original value using 1st.
Reference: [63] <author> M. Weiser. </author> <title> Program slicing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(4):352-357, </volume> <month> July </month> <year> 1984. </year>
Reference-contexts: In this respect, the resulting program obtained here is similar to the slice obtained from forward slicing <ref> [63] </ref>. However, our forward dependence analysis finds parts of a program that depend only on certain information, while forward slicing finds parts of a program that depend possibly on certain information. Furthermore, our resulting program also returns all inter mediate results on the arguments of interest.
Reference: [64] <author> D. M. Yellin and R. E. Strom. INC: </author> <title> A language for incremental computations. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 211-236, </pages> <month> April </month> <year> 1991. </year> <month> 14 </month>
Reference-contexts: Some approaches to incremental computation have exploited specific kinds of auxiliary information, e.g., auxiliary arithmetic associated with some classical strength-reduction rules [2], dynamic mappings maintained by finite differencing rules for aggregate primitives in SETL [48] and INC <ref> [64] </ref>, and auxiliary data structures for problems with certain properties like stable decomposition [52]. However, until now, systematic discovery of auxiliary information for arbitrary programs has been a subject completely open for study. Auxiliary information is, by definition, useful information about x that is not computed by f (x).
References-found: 64

