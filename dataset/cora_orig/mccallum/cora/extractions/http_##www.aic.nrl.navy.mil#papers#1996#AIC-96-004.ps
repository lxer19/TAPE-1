URL: http://www.aic.nrl.navy.mil/papers/1996/AIC-96-004.ps
Refering-URL: http://www.aic.nrl.navy.mil/~breslow/index.html
Root-URL: 
Email: breslow@aic.nrl.navy.mil  
Title: Greedy Utile Suffix Memory for Reinforcement Learning with Perceptually-Aliased States  
Author: Leonard A. Breslow 
Keyword: reinforcement learning, perceptual aliasing, instance-based learning  
Note: NCARAI Technical Report No. AIC-96-004.  
Address: (Code 5510)  Washington, DC 20375, USA  
Affiliation: Navy Center for Applied Research in AI  Naval Research Laboratory  
Abstract: Reinforcement learning agents are faced with the problem of perceptual aliasing when two or more states are perceptually identical but require different actions. To address this problem, several researchers have incorporated memory of preceding events into the definition of states to distinguish perceptually-aliased states. Recently, McCallum (1995b) has offered Utile Suffix Memory (USM), an instance-based algorithm using a tree to store instances and to represent states for reinforcement learning. USM's use of online instance-based state learning permits state definitions to be updated quickly based on the latest results of reinforcement learning. The use of a fringe (an extension of the tree to a prespecified depth below the `real' tree) provides the algorithm a limited degree of lookahead capability. However, the use of a fringe incurs a large cost in terms of tree size and provides only limited lookahead capability. We introduce a modification of USM, Greedy Utile Suffix Memory (GUSM) to address these concerns. GUSM uses a positive criterion for splitting a leaf node, similar to that used in USM, based on the immediate advantage of node splitting and resultant tree expansion. In addition, GUSM includes a negative split criterion, based on the inadequacy of a leaf node and the state it represents for determining the next correct action to take. GUSM is able to solve problems on which USM will sometimes fail (i.e., depending on the setting of the fringe depth parameter). In addition, GUSM usually produces trees that contain fewer nodes overall (i.e., real + fringe nodes) and solves problems as rapidly as USM. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W., Kibler, D., & Albert, M. K. </author> <year> (1991). </year> <title> Instance-Based Learning Algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66. </pages>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and regression trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group. </publisher>
Reference: <author> Chrisman, L. </author> <year> (1992). </year> <title> Reinforcement Learning with Perceptual Aliasing: The Perceptual Distinc-tions Approach. </title> <booktitle> In Proc. Tenth National Conference on Artificial Intelligence. </booktitle>
Reference: <author> McCallum, R. A. </author> <year> (1992a). </year> <title> First Results with Utile Distinction Memory for Reinforcement Learning. </title> <type> Technical Report 446, </type> <institution> University of Rochester Computer Science Department. </institution>
Reference: <author> McCallum, R. A. </author> <year> (1992b). </year> <title> Using Transitional Proximity for Faster Reinforcement Learning. </title> <booktitle> The Proceedings of the Ninth International Machine Learning Conference (ML-92), </booktitle> <address> Aberdeen, Scotland. </address>
Reference: <author> McCallum, R. A. </author> <year> (1993). </year> <title> Overcoming Incomplete Perception with Utile Distinction Memory. </title> <booktitle> The Proceedings of the Tenth Internation Machine Learning Conference (ML-93), </booktitle> <address> Amherst, MA. </address>
Reference: <author> McCallum, R. A. </author> <year> (1994a). </year> <title> First Results with Instance-Based State Identification for Reinforcement Learning. </title> <type> Technical Report 502, </type> <institution> University of Rochester Computer Science Department. </institution>
Reference: <author> McCallum, R. A. </author> <year> (1994b). </year> <title> Reduced Training Time for Reinforcement Learning with Hidden State. </title> <booktitle> The Proceedings of the Eleventh International Machine Learning Conference (ML-94), </booktitle> <address> New Brunswick, NJ. </address>
Reference: <author> McCallum, R. A. </author> <year> (1995a). </year> <title> Instance-Based State Identification for Reinforcement Learning. </title> <publisher> NIPS95. </publisher>
Reference: <author> McCallum, R. A. </author> <year> (1995b). </year> <title> Instance-Based Utile Distinctions for Reinforcement Learning. </title> <booktitle> Proc. of the Twelfth International Machine Learning Conference (ML-95). </booktitle>
Reference: <author> Quinlan, J. R. </author> <year> (1987). </year> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27, </volume> <pages> 221-234. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for machine learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ron, R., Singer, Y., & Tishby, N. </author> <year> (1994). </year> <title> Learning Probablistic Automata with Variable Memory Length. </title> <booktitle> In Proc. of the Seventh Annual ACM Conference on COLT, </booktitle> <address> New Brunswick, NJ. </address>
Reference-contexts: As a compromise between these two approaches, McCallum (1995b) introduced Utile Suffix Memory (USM). Like NSM, USM is an instance-based algorithm. Like the o*ine learning approaches, USM uses statistical tests to determine the relevance of history information. USM uses a prediction suffix tree (PST) <ref> (Ron et al., 1994) </ref> to organize event instances into states based on their past histories. The agent constructs the tree progressively from the top down during the course of its problem solving and learning.
Reference: <author> Siegel, S. </author> <year> (1956). </year> <title> Nonparametric Statistics for the Behavioral Sciences. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY. </address>
Reference-contexts: USM has been shown to learn even in the presence of noise. The reader is referred to McCallum (1995b) for further details of the USM algorithm. 3 Greedy Utile Suffix Memory 2 The statistical test is the Kolmogorov-Smirnov nonparametric two-sample test <ref> (Siegel, 1956) </ref>. Two disadvantages of the use of a fixed-depth fringe are (1) the increased size of the resulting tree and (2) the limitations of the lookahead offered by the fringe as a means to overcome horizon effects in tree expansion.
Reference: <author> Watkins, J. C. H., & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning. </journal> <volume> 8(3). </volume> <pages> 279-292. </pages>
Reference-contexts: USM was tested with fringe depths d f set to 1 and to 2 (the USM-1 and USM-2 conditions, respectively). McCallum (1995b) reports using USM with several reinforcement learning methods. We have chosen one of these methods, Q-learning <ref> (Watkins and Dayan, 1992) </ref>, with fi = :25 and fl = :33. Results reported are averages over 20 runs of each algorithm. Each run consists of a series of trial blocks in each of which the agent is randomly assigned to a different starting position in the maze.
Reference: <author> Whitehead, S., & Ballard, D. H. </author> <year> (1991). </year> <title> Learning to perceive and act by trial and error. </title> <journal> Machine Learning. </journal> <volume> 7(1), </volume> <pages> 45-83. </pages>
Reference-contexts: 1 Introduction Reinforcement learning agents are reactive systems that learn to map situations, or states, into actions so as to maximize future discounted reward. Such agents are faced with the problem of perceptual aliasing when two or more states are perceptually identical but require different actions <ref> (Whitehead and Ballard, 1991) </ref>. Purely reactive policies do not produce optimal performance in such situations. For instance, a robot attempting to locate the mail room in an office building may reach an intersection that is perceptually identical to other intersections in the building.
References-found: 16

