URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1993/tr-93-068.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1993.html
Root-URL: http://www.icsi.berkeley.edu
Title: Lexical Modeling in a Speaker Independent Speech Understanding System  
Phone: 1-510-642-4274 FAX 1-510-643-7684  
Author: Charles Clayton Wooters 
Date: November 1993  
Address: I 1947 Center Street Suite 600 Berkeley, California 94704  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-93-068  
Abstract: Over the past 40 years, significant progress has been made in the fields of speech recognition and speech understanding. Current state-of-the-art speech recognition systems are capable of achieving word-level accuracies of 90% to 95% on continuous speech recognition tasks using 5000 words. Even larger systems, capable of recognizing 20,000 words are just now being developed. Speech understanding systems have recently been developed that perform fairly well within a restricted domain. While the size and performance of modern speech recognition and understanding systems are impressive, it is evident to anyone who has used these systems that the technology is primitive compared to our own human ability to understand speech. Some of the difficulties hampering progress in the fields of speech recognition and understanding stem from the many sources of variation that occur during human communication. One of the sources of variation that occurs in human communication is the different ways that words can be pronounced. There are many causes of pronunciation variation, such as: the phonetic environment in which the word occurs, the dialect of the speaker, the speaker's age, the speaker's gender, and the speaking rate. Some researchers have shown improvements in speech recognition performance on a read-speech task through the use of explicit pronunciation modeling, while others have not shown any significant improvements. This thesis presents an algorithm for the construction of models that attempt to capture the variation that occurs in the pronunciations of words in spontaneous (i.e., non-read) speech. A technique for developing alternate pronunciations of words and then estimating 
Abstract-found: 1
Intro-found: 1
Reference: <author> BAHL, L. R., R. BAKIS, P. S. COHEN, A. G. COLE, F. JELINEK, B. L. LEWIS, & R. </author> <title> L. </title>
Reference: <author> MERCER. </author> <year> 1980. </year> <title> Further results on the recognition of a continuously read natural corpus. </title> <booktitle> In Proceedings Int'l Conference on Acoustics Speech and Signal Processing. </booktitle>
Reference: <author> BAKER, J. K., </author> <year> 1975. </year> <title> Stochastic modeling as a means of automatic speech recognition. </title> <institution> Carnegie Mellon University dissertation. </institution>
Reference: <author> BAKIS, R. </author> <year> 1976. </year> <title> Continuous speech recognition via centisecond acoustic states. </title> <booktitle> In 91st Meeting Acoustical Society of America, </booktitle> <address> Washington, DC. </address>
Reference-contexts: this section we describe the two approaches that we have used to model phoneme duration. 4.1.1 Context-independent Duration Models In a Hidden Markov Model (HMM) speech recognition system such as the one we are using, phonemes are represented as a sequence of states linked together with transitions (see Figure 4.1) <ref> (Bakis 1976) </ref>.
Reference: <author> BAUM, L. E. </author> <year> 1972. </year> <title> An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes. Inequalities 3.1-8. </title> , & <editor> T. PETRIE. </editor> <year> 1966. </year> <title> Statistical inference for probabilistic functions of finite state Markov chains. </title> <journal> The Annals of Mathematical Statistics 36.1554-1563. </journal> , <note> T. </note> <author> PITRIE, G. SOULES, & N. WEISS. </author> <year> 1970. </year> <title> A maximization technique occurring in the statistical analysis of probabilistic functions in Markov chains. </title> <journal> The Annals of Mathematical Statistics 41.164-171. </journal>
Reference: <author> BERGER, J. O. </author> <year> 1985. </year> <title> Statistical decision theory and Bayesian analysis. </title> <address> New York: </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: In order to maximize the posterior probability of the model, we need to offset any drop in the likelihood with a term that favors smaller models. Stolcke & Omohundro (1993b) found that by using a Dirichlet conjugate prior <ref> (Berger 1985) </ref> over the emission and transition probabilities of the model, they could produce an implicit bias towards smaller models. The use of a Dirichlet prior corresponds to adding a number of virtual samples to the actual samples for the purposes of estimating the most-likely parameter settings.

Reference: <author> BRIDLE, J. S. </author> <year> 1990. </year> <title> Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In Neurocomputing: Algorithms, architectures and applications, </title> <editor> ed. by F. Fogelman Soulie & J. Herault, </editor> <booktitle> NATO ASI, </booktitle> <pages> 227-236. </pages>
Reference-contexts: However, to ensure that the outputs sum to 1, the BeRP MLP uses a softmax <ref> (Bridle 1990) </ref> output function: g k (x n ) = i=1 exp f i (x n ) The input layer consists of 9 frames of input speech data.
Reference: <author> BUTZBERGER, J., H. MURVEIT, E. SHRIBERG, & P. PRICE. </author> <year> 1992. </year> <title> Spontaneous speech effects in large vocabulary speech recognition applications. </title> <booktitle> In Proceedings DARPA Speech and Natural Language Workshop, </booktitle> <pages> 339-343. </pages>
Reference-contexts: Therefore, if we wish to model pronunciation variation for the purposes of automatic speech recognition/understanding, we must ensure that we model these variations from the point of view of the particular recognition system being used. Considering the wide range of variation that occurs in natural spontaneous speech <ref> (Butzberger et al. 1992) </ref>, it seems obvious that word models that allow more than one pronunciation for a word should be better than models that allow only a single pronunciation per word. <p> Automatically deriving meaning from a string of words is a very difficult task even when the string of words is a grammatical sentence. It is even more difficult when the string of words is ill-formed due to errors by the recognizer. In fact, normal spontaneous speech is not grammatical <ref> (Butzberger et al. 1992) </ref>, so even if the recognizer were perfect, the system must still be able to understand ungrammatical input. An important component for any speech understanding system is the natural language backend.
Reference: <author> CHIGIER, B., & H. C. LEUNG. </author> <year> 1992. </year> <title> The effects of signal representations, phonetic classification techniques, and the telephone network. </title> <booktitle> In Proceedings Int'l. Conf. on Spoken Language Precessing, </booktitle> <volume> volume 1, </volume> <pages> 97-100, </pages> <address> Banff, Canada. </address>
Reference-contexts: We begin with a discussion of one of the most common parametric representation for speech recognition - Mel-cepstrum. Next we discuss a technique which, in some applications, has been shown <ref> (Chigier & Leung 1992) </ref> to give improved performance over Mel-cepstrum on clean speech Perceptual Linear Prediction (PLP) (Hermansky 1990).
Reference: <author> CHOW, Y. L., M. O. DUNHAM, O. KIMBALL, M. KRASNER, F. KUBALA, J. MAKHOUL, S. ROUCOS, & R. M. SCHWARTZ. </author> <year> 1987. </year> <title> BYBLOS: the BBN continuous speech recognition system. </title> <booktitle> In Proceedings Int'l Conference on Acoustics Speech and Signal Processing, </booktitle> <pages> 89-92. </pages> , <note> R. </note> <author> M. SCHWARTZ, S. ROUCOS, O. KIMBALL, P. PRICE, F. KUBALA, M. DUNHAM, M. KRASNER, & J. MAKHOUL. </author> <year> 1986. </year> <title> The role of word-dependent coarticulatory effects in a phoneme-based speech recognition system. </title> <booktitle> In Proceedings Int'l Conference on Acoustics Speech and Signal Processing. </booktitle>
Reference: <author> COHEN, M., </author> <year> 1989. </year> <title> Phonological structures for speech recognition. </title> <institution> University of California, </institution> <note> Berkeley dissertation. </note> , <author> H. FRANCO, N. MORGAN, D. RUMELHART, & V. ABRASH. </author> <year> 1992. </year> <title> Hybrid Neural Network/Hidden Markov Model continuous speech recognition. </title> <booktitle> In Proc. Int'l Conf. on Spoken Lang. Processing, </booktitle> <address> Banff, Canada. </address>
Reference-contexts: Such a recognizer will perform poorly if it is used without modification to recognize speech obtained from a different task, such as one which uses non-read (i.e., spontaneous) speech. Other researchers <ref> (Cohen 1989) </ref> have pointed out the need to optimize models of phonological variation with respect to a particular speech recognition system. We cannot presume that all speech recognition systems hear speech in the same way. <p> Some researchers (Lee 1989) have not shown any improvements in recognition performance through the use of explicit modeling of multiple pronunciations. Others <ref> (Cohen 1989) </ref> have demonstrated significant improvements in performance on large-vocabulary speaker-independent recognition systems. 4 Thanks to Anita Liang for this example. 3 The construction of word models that attempt to capture the variation that occurs in the pronunciation of a word introduces many difficulties. <p> Despite this seemingly obvious advantage, there has not been clear evidence that the use of multiple-pronunciation word models can improve the performance of speech recognition systems. Some researchers (Lee 1989) have not shown any improvements in recognition performance through the use of multiple-pronunciation word models. Others <ref> (Cohen 1989) </ref> have demonstrated significant improvements in performance on large-vocabulary speaker-independent recognition systems. The construction of a model that attempts to capture the variation that occurs in the pronunciation of a word introduces many difficulties. <p> For example, how does one derive alternate pronunciations for a word? Another difficulty arises when trying to represent the fact that certain pronunciations are more likely than others. Additionally, other researchers <ref> (Cohen 1989) </ref> have pointed out the need to optimize models of phonological variation with respect to a particular speech recognition system.
Reference: <author> COHEN, P. S., & P. L. MERCER. </author> <year> 1975. </year> <title> The phonological component of an automatic speech recognition system. In Speech recognition, </title> <editor> ed. by R. Reddy, </editor> <address> 275-320. New York: </address> <publisher> Academic Press. DARPA. </publisher> <year> 1992. </year> <booktitle> Proceedings of the DARPA speech and natural language workshop. </booktitle>
Reference-contexts: : : : : : : : : : : : : : : : 50 4.5.1 Pronunciation Modeling : : : : : : : : 50 4.5.2 Loosephones : : : : : : : : : : : : : : 59 One of the early techniques <ref> (Cohen & Mercer 1975) </ref> that was used to model the variation in the pronunciations of words used a network of allophones to represent alternate pronunciations. The allophone networks were constructed by applying a set of phonological rules to a dictionary of words. The phonological rules were constructed by expert linguists.
Reference: <author> DAVIS, K. H., R. BIDDULPH, & S. BALASHEK. </author> <year> 1952. </year> <title> Automatic recognition of spoken digits. </title> <journal> JASA 24.637-642. </journal>
Reference-contexts: So, even more beneficial than a speech recognition system would be a system that could understand what a person is saying. One of the early speech recognition systems was developed in 1952 at AT&T Bell Laboratories <ref> (Davis et al. 1952) </ref>. This system recognized the ten digits when spoken in isolation over a telephone line by a single individual with an accuracy of 97% to 99%.
Reference: <author> DAVIS, S. B., & P. MERMELSTEIN. </author> <year> 1980. </year> <title> Comparison of parametric representations of monosyllabic word recognition in continuously spoken sentences. </title> <journal> IEEE Transactions on Acoustics, Speech, </journal> <note> and Signal Processing 28.357-366. 72 DEMPSTER, </note> <author> A. P., N. M. LAIRD, & D. B. RUBIN. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B 34.1-38. </journal>
Reference-contexts: Mel-scaled cepstral coefficients are identical to normal cepstral coefficients except that the frequency axis has been warped to approximate the frequency scale of human hearing. That is, the frequency bands are spaced linearly below 1000 Hz and logarithmically above 1000 Hz (Zwicker 1961). This has been shown <ref> (Davis & Mermelstein 1980) </ref> to give improved speech recognition performance over cepstra calculated from linearly-spaced frequency bands. 3.2.2 PLP One approach to dealing with the problem of recognizing speech from multiple speakers is to use an analysis technique which is effective at preserving linguistic information while suppressing speaker-dependent variations.
Reference: <author> EDWARDS, HAROLD T. </author> <year> 1992. </year> <title> Applied phonetics: The sounds of American English. </title> <publisher> Singular Publishing Group Inc. </publisher>
Reference: <author> FERGUSON, J. D. </author> <year> 1980. </year> <title> Variable duration models for speech. In Symposium on the Application of Hidden Markov Models to Text and Speech, </title> <type> 143-179. </type>
Reference: <author> FRAZER, N. M., & G. N. GILBERT. </author> <year> 1991. </year> <title> Simulating speech systems. </title> <booktitle> Computer Speech and Language 5.81-99. </booktitle>
Reference: <author> G. D. FORNEY, JR. </author> <year> 1973. </year> <title> The Viterbi algorithm. </title> <booktitle> Proc. IEEE 61.268-78. </booktitle>
Reference: <author> GOODINE, D., S. SENEFF, L. HIRSCHMAN, & M. PHILLIPS. </author> <year> 1991. </year> <title> Full integration of speech and language understanding in the MIT spoken language system. </title> <booktitle> In Proceedings of Eurospeech 91, 2426, </booktitle> <address> Genova, Italy. </address>

Reference: <author> HIRSCH, H. G., </author> <year> 1993. </year> <title> Personal Communication. </title> , <editor> P. MEYER, & H. W. RUEHL. </editor> <year> 1991. </year> <title> Improved speech recognition using high-pass filtering of subband envelopes. </title> <booktitle> In Proceedings European Conf. on Speech Communication and Technology. (EUROSPEECH), </booktitle> <pages> 413-416, </pages> <address> Genoa. </address>
Reference-contexts: The recordings for all of the data collection sessions were made in a semi-quiet office with no attempt to suppress any environmental noise in the room. A signal-to-noise ratio of 42.25 dB was calculated <ref> (Hirsch 1993) </ref> for this data using the National Institute of Standards and Technology signal-to-noise ratio estimation software.
Reference: <author> HOCHBERG, M., & H. SILVERMAN. </author> <year> 1993. </year> <title> Constraining model duration variance in HMM-based connected-speech recognition. </title> <booktitle> In Proceedings Eurospeech. </booktitle>
Reference: <author> JELINEK, F. </author> <year> 1976. </year> <title> Continuous speech recognition by statistical methods. </title> <booktitle> Proceedings of the IEEE 64.532-556. </booktitle> , & <editor> R. L. MERCER. </editor> <year> 1980. </year> <title> Interpolated estimation of markov source parameters from sparse data. In Pattern recognition in practice, </title> <editor> ed. by E. S. Gelsema & L. N. Kanal, </editor> <address> 381-397. Amsterdam, The Netherlands: </address> <publisher> North-Holland Publishing Company. 73 JUANG, </publisher> <editor> B. H., & L. R. RABINER. </editor> <year> 1990. </year> <title> The segmental K-Means algorithm for estimating parameters of Hidden Markov Models. </title> <journal> IEEE Trans. on Acoustics, Speech, and Signal Processing 38.1639-41. </journal>
Reference: <author> JURAFSKY, D. </author> <year> 1992b. </year> <title> An on-line computational model of human sentence interpretation. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> 302-308, </pages> <address> San Jose, CA. </address> , <year> 1992a. </year> <title> An on-line computational model of human sentence interpretation: A theory of the representation and use of linguistic knowledge. </title> <institution> Berkeley, CA: University of California at Berkeley dissertation. </institution> , <note> C. </note> <author> WOOTERS, G. TAJCHMAN, & N. MORGAN. </author> <year> 1993. </year> <title> The Berkeley Restaurant Project: A status report at phase I. </title> <type> Technical report, </type> <institution> International Computer Science Institute, 1947 Center St., </institution> <address> Suite 600, Berkeley, CA 94704. </address> <note> to appear. </note>
Reference: <author> KAY, M. </author> <year> 1973. </year> <title> The MIND system. In Natural language processing, </title> <editor> ed. </editor> <booktitle> by Randall Rustin, </booktitle> <pages> 155-188. </pages> <address> New York: </address> <publisher> Algorithmics Press. </publisher>
Reference-contexts: The probability is the conditional probability of the non-terminal X expanding to ffi. The grammar is quite small, and currently only covers 70% of the training corpus sentences. Parser The BeRP backend uses both bottom-up and top-down chart parsers <ref> (Kay 1973) </ref> which use a simple dynamic programming algorithm to build a parse tree for each sentence that comes from the recognizer. The parse trees compute probabilities for parses and for prefixes, and build a semantic representation of each sentence on-line.
Reference: <author> KLATT, D. H. </author> <year> 1975. </year> <title> Voice onset time, friction, and aspiration in word-initial consonant clusters. </title> <journal> Journal of Speech and Hearing Research 18.686-706. </journal>
Reference: <author> KUBALA, G. F., Y. CHOW, A. DERR, M. FENG, O. KIMBALL, J. MAKHOUL, P. PRICE, J. ROHLICEK, S. ROUCOS, R. SCHWARTZ, & J. VANDEGRIFT. </author> <year> 1988. </year> <title> Continuous speech recognition results of the BYBLOS system on the DARPA 1000-word Resource Management databasement. </title> <booktitle> In Proceedings Int'l Conference on Acoustics Speech and Signal Processing, </booktitle> <pages> 291-294. </pages>
Reference: <author> KUCHERA, H., & W. N. FRANCIS. </author> <year> 1967. </year> <title> Computational analysis of present-day American English. </title> <address> Providence, Rhode Island: </address> <publisher> Brown University Press. </publisher>
Reference-contexts: Each sentence was thus recorded by seven different speakers. * 3 si sentences. Since the small set of phonetically-compact sentences could not cover all possible phonetic contexts, a set of 1,890 sentences was selected by TI from the Brown corpus 1 <ref> (Kuchera & Francis 1967) </ref>. Each speaker recorded three of these sentences; thus, all 1,890 sentences were spoken once. Recording Environment and Processing The speech was recorded digitally at a sampling rate of 20 kHz and then downsampled to 16 kHz.
Reference: <author> LEE, K. F. </author> <year> 1989. </year> <title> Automatic speech recognition: The development of the SPHINX system. </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: Despite the seemingly obvious advantage of multiple-pronunciation word models, there has not been clear evidence that the use of such models can improve the performance of speech recognition systems. Some researchers <ref> (Lee 1989) </ref> have not shown any improvements in recognition performance through the use of explicit modeling of multiple pronunciations. <p> the context of the specific phonemes to its left and right. 4 Voice onset time is the time between the release of the stop closure and the onset of vocal cord vibration for the following sound. 5 These categories of contexts are commonly used to model co-articulatory effects across phonemes <ref> (Lee 1989) </ref>. * Left phonetic context (left biphone) the duration for a phoneme is calculated only from samples of that phoneme when it occurs in the context of the specific phoneme to its left. * Right phonetic context (right biphone) the duration for a phoneme is calculated only from samples of <p> Despite this seemingly obvious advantage, there has not been clear evidence that the use of multiple-pronunciation word models can improve the performance of speech recognition systems. Some researchers <ref> (Lee 1989) </ref> have not shown any improvements in recognition performance through the use of multiple-pronunciation word models. Others (Cohen 1989) have demonstrated significant improvements in performance on large-vocabulary speaker-independent recognition systems.
Reference: <author> LEUNG, H. C., & V. W. ZUE. </author> <year> 1984. </year> <title> A procedure for automatic alignment of phonetic transcriptions with continuous speech. </title> <booktitle> In Proc. International Conference on Acoustics Speech and Signal Processing, 2.7.1-2.7.4. IEEE. </booktitle>
Reference-contexts: Recording Environment and Processing The speech was recorded digitally at a sampling rate of 20 kHz and then downsampled to 16 kHz. A Sennheiser close-talking microphone was used for all of the recordings. The speech was initially labeled using an automatic procedure <ref> (Leung & Zue 1984) </ref> and then hand-corrected by linguists. The speech was labeled at both the phonetic and the word levels. Table 2.3 presents a list of the phones used to label the TIMIT database along with their IPA equivalents and an example word containing that phone.
Reference: <author> LEVINSON, S. E. </author> <year> 1986. </year> <title> Continuously variable duration Hidden Markov Models for automatic speech recognition. </title> <note> Computer Speech and Language 29-45. </note> , <author> L. R. RABINER, & M. M. SONDHI. </author> <year> 1983. </year> <title> An introduction to the application of the theory of probabilistic functions on a Markov process to automatic speech recognition. </title> <journal> Bell system Technical Journal 62. </journal>
Reference: <author> MACKAY, D. J. C. </author> <year> 1992. </year> <title> Bayesian interpolation. </title> <booktitle> Neural Computation 4.415-447. </booktitle>
Reference-contexts: Stolcke & Omohundro (1993b) point out that This phenomenon is similar, but not identical, to the Bayesian `Occam factors' that prefer models with fewer parameters <ref> (MacKay 1992) </ref>. Since the merging process reduces the likelihood of the model, but increases the prior probability, the merging can continue as long as there is an increase in the posterior probability.
Reference: <author> MOORE, R., F. PEREIRA, & H. MURVEIT. </author> <year> 1989. </year> <title> Integrating speech and natural-language processing. </title> <booktitle> In Proceedings DARPA Speech and Natural Language Workshop, </booktitle> <pages> 243-247. </pages>
Reference: <author> MOORE, R. K., & A. MORRIS. </author> <year> 1992. </year> <title> Experiences collecting genuine spoken enquiries using WOZ techniques. </title> <booktitle> In Proceedings DARPA Speech and Natural Language Workshop, </booktitle> <address> Harriman, New York. </address> , <note> M. </note> <author> J. TOMLINSON, & A. MORRIS. </author> <year> 1991. </year> <booktitle> Whither the wizard? In Proc. ESCA Workshop on the Structure of Multimodal Dialogue, </booktitle> <address> Maratea, Italy. </address>

Reference: <author> MURVEIT, H., J. BUTZBURGER, & M. WEINTRAUB. </author> <year> 1992. </year> <title> Reduced channel dependence for speech recognition. </title> <booktitle> In Proceedings DARPA Speech and Natural Language Workshop. , & M. WEINTRAUB. </booktitle> <year> 1988. </year> <title> 1000-word speaker independent continuous-speech recognition using Hidden Markov Models. </title> <booktitle> In Proceedings Int'l Conference on Acoustics Speech and Signal Processing, </booktitle> <pages> 115-118. </pages>
Reference-contexts: The third parametric representation discussed in this section is a modification that can be applied to PLP (Hermansky et al. 1991) or Mel-cepstrum <ref> (Murveit et al. 1992) </ref> and is called RelAtive SpecTrAl (RASTA) processing. RASTA processing attempts to partially correct for the negative effects of convolutional noise, such as might be introduced by differences among communication channels. <p> We have primarily used RASTA processing on the logarithmic auditory-like spectrum of Perceptual Linear Predictive (PLP) analysis (see Section 3.2.2 above); however, other researchers <ref> (Murveit et al. 1992) </ref> have found that performing RASTA filtering on Mel-spectrum also works well. The constant factors that RASTA processing suppresses, represent convolutional noise, i.e. the distortions introduced by the relatively time-invariant frequency response of the microphone and of the communication environment.
Reference: <author> PAUL, D. B., & J. M. BAKER. </author> <year> 1992. </year> <title> The design for the Wall Street Journal-based CSR corpus. </title> <booktitle> In Proc. Fifth DARPA Speech and Natural Language Workshop, </booktitle> <pages> 357-362. </pages>
Reference-contexts: This system recognized the ten digits when spoken in isolation over a telephone line by a single individual with an accuracy of 97% to 99%. Current state-of-the-art speech recognition systems are capable of achieving accuracies of 90% to 95% on speech recognition tasks using 5000 words <ref> (Paul & Baker 1992) </ref> 1 . These systems are less constrained than the early AT&T recognizer in that they can recognize speech from more than just a single individual and they do not require speakers to pause between words. <p> The pronunciations produced from this system were used for the baseline single pronunciation experiments reported in Section 4.5. 6 LIMSI-CNRS pronunciation lexicon The LIMSI lexicon was produced primarily for the 1992 Wall Street Journal continuous speech recognition task <ref> (Paul & Baker 1992) </ref> and contains pronunciations for approximately 10,000 words. Resource Management These pronunciations were developed by SRI and are distributed by NIST as part of the Resource Management speech recognition task (Price et al. 1988). They represent the most-likely pronunciations for the 1,000 words in Resource Management. <p> Another possibility for increasing the amount of data on which the MLP is trained is to initialize the BeRP MLP from an MLP that was trained on a large amount of data, such as the 5,000,000 frames of data in the WSJ0 corpus <ref> (Paul & Baker 1992) </ref>. Better Language Modeling The bigram language model we currently use in BeRP was calculated from the stochastic context-free grammar (SCFG) (described in Section 3.4) by using the SCFG to generate several thousand sentences and then estimating the bigram probabilities from those sentences.
Reference: <author> PIERACCINI, R., & A. E. ROSENBERG. </author> <year> 1989. </year> <title> Automatic generation of phonetic units for continuous speech recognition. </title> <booktitle> In Proceedings Int'l Conference on Acoustics Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> 623-6, </pages> <address> Glasgow, Scotland. </address>
Reference: <author> PITCHER, D. </author> <year> 1989. </year> <title> Berkeley City Guide. </title> <publisher> Heyday Books. </publisher>
Reference: <author> PRICE, P. </author> <year> 1990. </year> <title> Evaluation of spoken language systems: The ATIS domain. </title> <booktitle> In Proc. Third DARPA Speech and Language Workshop, </booktitle> <pages> 91-95, </pages> <address> Hidden Valey, PA. </address> <note> 75 , W. </note> <author> FISHER, J. BERNSTEIN, & D. PALLET. </author> <year> 1988. </year> <title> The darpa 1000-word resource management database for continuous speech recognition. </title> <booktitle> In Proceedings IEEE Intl. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> 651-654, </pages> <address> New York. </address> <publisher> IEEE. </publisher>
Reference: <author> RABINER, L. R., & B. H. JUANG. </author> <year> 1986. </year> <title> An introduction to Hidden Markov Models. </title> <journal> IEEE ASSP Magazine 3.4-16. </journal>
Reference: <author> RENALS, S., N. MORGAN, H. BOURLARD, M. COHEN, H. FRANCO, C. WOOTERS, & P. KOHN. </author> <year> 1991. </year> <title> Connectionist speech recognition: Status and prospects. </title> <type> Technical Report TR-91-070, </type> <institution> International Computer Science Institute, 1947 Center St., </institution> <address> Suite 600, Berkeley, CA 94704. </address>
Reference: <author> ROBINSON, A. J., L. ALMEIDA, J.-M. BOITE, H. BOURLARD, F. FALLSIDE, M. HOCHBERG, D. KERSHAW, P. KOHN, Y. KONIG, N. MORGAN, J. P. NETO, S. RENALS, M. SAERENS, & C. WOOTERS. </author> <year> 1993. </year> <title> A neural network based, speaker independent, large vocabulary, continuous speech recognition system: The WERNICKE project. </title> <booktitle> In Proceedings Euro-pean Conf. on Speech Communication and Technology. (EUROSPEECH), </booktitle> <pages> 1941-1944, </pages> <address> Berlin, Germany. </address>
Reference-contexts: Y 0 was initially developed at ICSI and since late 1992 has undergone major enhancements (and bug fixes) at Cambridge University Engineering Department as part of an ESPRIT-funded Basic Research project the WERNICKE project <ref> (Robinson et al. 1993) </ref>. Given the emission probabilities from the RAP, Y 0 runs the time synchronous Viterbi (i.e. dynamic programming algorithm), producing a recognized string of words.
Reference: <author> RUMELHART, D. E., G. E. HINTON, & R. J. WILLIAMS. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. In Parallel distributed processing. explorations of the microstructure of cognition, </title> <editor> ed. by D. E. Rumelhart & J. L. McClelland, </editor> <volume> volume 1: </volume> <booktitle> Foundations. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: Thus, the total number of parameters (weights) used in this recognizer is (162 512) + (512 61) + 512 + 61 = 114; 749. 3.3.2 Task Adaptation MLP Targets Training an MLP using the error back-propagation training algorithm <ref> (Rumelhart et al. 1986) </ref> 2 requires that the training data be labeled. That is, each 10 msec frame of speech that is to be presented to the MLP must be labeled with one of the 61 phonemes from the lexicon.
Reference: <author> RUSSELL, M. J., & R. K. MOORE. </author> <year> 1985. </year> <title> Explicit modelling of state occupancy in Hidden Markov Models for automatic speech recognition. </title> <booktitle> In Proceedings ICASSP, </booktitle> <pages> 5-8. </pages>
Reference: <author> SCHWARTZ, R. M., Y. L. CHOW, O. KIMBALL, S. ROUCOS, M. KRASNER, & J. MAKHOUL. </author> <year> 1985. </year> <title> Context-dependent modeling for acoustic-phonetic recognition of continuous speech. </title> <booktitle> In Proceedings Int'l Conference on Acoustics Speech and Signal Processing. </booktitle> , <editor> Y. L. CHOW, S. ROUCOS, M. KRASNER, & J. MAKHOUL. </editor> <year> 1984. </year> <title> Improved Hidden Markov Modeling of phonemes for continuous speech recognition. </title> <booktitle> In Proceedings Int'l Conference on Acoustics Speech and Signal Processing. </booktitle>
Reference: <author> SENEFF, S., H. MENG, & V. ZUE. </author> <year> 1992. </year> <title> Language modelling for recognition and understanding using layered bigrams. </title> <booktitle> In Proceedings Int'l Conference on Spoken Language Processing, </booktitle> <address> I.317320, Banff, Alberta, Canada. </address> , & <publisher> V. ZUE, </publisher> <year> 1988. </year> <title> Transcription and alignment of the TIMIT database. Distributed with the TIMIT database. </title>
Reference: <author> SOLLA, S. A., E. LEVIN, & M. FLEISHER. </author> <year> 1988. </year> <title> Accelerated learning in layered neural networks. </title> <journal> Complex Systems 2.625-640. </journal>
Reference-contexts: It consists of a simple feed-forward multilayer perceptron (see Figure 3.5) trained with the backpropagation training algorithm using a relative entropy criterion <ref> (Solla et al. 1988) </ref>. If the MLP accurately estimates posterior probabilities, then the outputs of the MLP will sum to 1. However, it very often happens that the network converges to a local minimum, in which case the outputs are no longer guaranteed to sum to 1.
Reference: <author> STOLCKE, A., & S. OMOHUNDRO. </author> <year> 1993a. </year> <title> Best-first model merging for Hidden Markov Model induction. </title> <type> Technical report, </type> <institution> International Computer Science Institute, 1947 Center St. </institution> <address> Suite 600, Berkeley, CA. 76 , & S. OMOHUNDRO. </address> <year> 1993b. </year> <title> Hidden Markov Model induction by Bayesian model merging. </title> <booktitle> In Advances in neural information processing systems 5. </booktitle> <address> San Mateo, Ca.: </address> <publisher> Morgan Kaufman. </publisher>
Reference: <author> VITERBI, A. J. </author> <year> 1967. </year> <title> Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. </title> <journal> IEEE Trans. on Information Theory 13.260-269. </journal>
Reference: <author> WANG, W. S-Y. </author> <year> 1971. </year> <title> The basis of speech. In The learning of language, </title> <editor> ed. by Carroll E. Reed, </editor> <volume> chapter 7, </volume> <pages> 276-306. </pages> <address> Appleton-Centruy-Crofts. </address> . <year> 1972. </year> <title> Approaches to phonology. </title> <booktitle> In Current trends in linguistics, </booktitle> <editor> ed. by T. A. Sebeok. </editor> <booktitle> The Hague: </booktitle> <publisher> Mouton. </publisher>
Reference-contexts: Whether a particular type of variation in pronunciation is meaning-changing depends on the language. For example, in Hungarian, duration is meaningful, as illustrated by the words [u j] finger and [uq j] new, 4 and Estonian maintains a three-way duration contrast <ref> (Wang 1971) </ref>: kalas kalpas kalqas in the fish shore he poured When presented with these Hungarian or Estonian words, a native speaker of American English may not be able to discriminate between them because duration is not distinctive in English.
Reference: <author> WERBOS, P. J., </author> <year> 1974. </year> <title> Beyond regression: New tools for prediction and analysis in the behavioral sciences. </title> <address> Cambridge, MA: </address> <publisher> Harvard University dissertation. </publisher>
Reference: <author> ZUE, V., J. GLASS, D. GOODINE, H. LEUNG, M. PHILLIPS, & S. SENEFF. </author> <year> 1990. </year> <title> The VOYAGER speech understanding system: Preliminary development and evaluation. </title> <booktitle> In Proceedings Int'l Conference on Acoustics Speech and Signal Processing, </booktitle> <pages> 73-76, </pages> <address> Albuquerque, New Mexico. </address>
Reference-contexts: While these error rates are much higher than for tasks involving read speech (as opposed to spontaneous speech), they are comparable to the initial results obtained at the Massachusetts Institute of Technology for their spontaneous speech understanding system Voyager <ref> (Zue et al. 1990) </ref>. These results are not as good as current state-of-the-art speech understanding systems for a couple of reasons. First, we are using an unsmoothed bigram grammar which is fairly constraining and second, we are using monophone sub-word models.
Reference: <author> ZWICKER, E. </author> <year> 1961. </year> <title> Subdivision of the audible frequency range into critical bands (fre-quenzgruppen). </title> <journal> JASA 33(2).248. </journal> <volume> 77 </volume>
Reference-contexts: Mel-scaled cepstral coefficients are identical to normal cepstral coefficients except that the frequency axis has been warped to approximate the frequency scale of human hearing. That is, the frequency bands are spaced linearly below 1000 Hz and logarithmically above 1000 Hz <ref> (Zwicker 1961) </ref>.
References-found: 52

