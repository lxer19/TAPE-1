URL: http://www.cis.upenn.edu/~daes/papers/aaai98.ps.gz
Refering-URL: http://www.cis.upenn.edu/~daes/papers.html
Root-URL: 
Email: fgrove,daleg@research.nj.nec.com  
Title: Boosting in the limit: Maximizing the margin of learned ensembles  
Author: Adam J. Grove and Dale Schuurmans 
Date: July 1998.  
Note: To appear in Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-98), Madison, WI,  
Address: 4 Independence Way Princeton NJ 08540, USA  
Affiliation: NEC Research Institute  
Abstract: The minimum margin of an ensemble classifier on a given training set is, roughly speaking, the smallest vote it gives to any correct training label. Recent work has shown that the Adaboost algorithm is particularly effective at producing ensembles with large minimum margins, and theory suggests that this may account for its success at reducing generalization error. We note, however, that the problem of finding good margins is closely related to linear programming, and we use this connection to derive and test new LPboosting algorithms that achieve better minimum margins than Adaboost. However, these algorithms do not always yield better generalization performance. In fact, more often the opposite is true. We report on a series of controlled experiments which show that no simple version of the minimum-margin story can be complete. We conclude that the crucial question as to why boosting works so well in practice, and how to further improve upon it, remains mostly open. Some of our experiments are interesting for another reason: we show that Adaboost sometimes does overfiteventually. This may take a very long time to occur, however, which is perhaps why this phenomenon has gone largely unnoticed. 
Abstract-found: 1
Intro-found: 1
Reference: [BK97] <author> E. Bauer and R. Kohavi. </author> <title> An empirical comparison of voting classification algorithms: bagging, boosting, and variants. http:// robotics. </title> <publisher> stanford. </publisher> <address> edu/ users/ ronnyk, </address> <year> 1997. </year>
Reference-contexts: However, there is a growing body of empirical evidence that suggests Adaboost is remarkably effective at reducing the test set error of several well-known learning algorithms, often significantly and across a variety of do mains (more or less robustly, but with occasional exceptions) <ref> [FS96a, Qui96, MO97, BK97] </ref>. This raises a central question of the field: why is boosting so successful at improving the generalization performance of already carefully designed learning algorithms? One thing that is clear is that boosting's success cannot be directly attributed to a notion of variance reduction [Bre96b, BK97]. <p> This raises a central question of the field: why is boosting so successful at improving the generalization performance of already carefully designed learning algorithms? One thing that is clear is that boosting's success cannot be directly attributed to a notion of variance reduction <ref> [Bre96b, BK97] </ref>. This mystery is further compounded by the observation that Adaboost's generalization error often continues to decrease even after it has achieved perfect accuracy on the training set.
Reference: [Bre96a] <author> L. Breiman. </author> <title> Arcing classifiers. </title> <type> Technical report, </type> <institution> Statistics Department, </institution> <address> U. C. Berkeley, </address> <year> 1996. </year> <note> http:// www. stat. berke-ley. edu/ users/ breiman. </note>
Reference-contexts: 1 Introduction Recently, there has been great interest in ensemble methods for learning classifiers, and in particular in boosting [FS97] (or arcing <ref> [Bre96a] </ref>) algorithms. These methods take a given base learning algorithm and repeatedly apply it to reweighted versions of the original training data, producing a collection of hypotheses h 1 ; :::; h b which are then combined in a final aggregate classifier via a weighted linear vote.
Reference: [Bre96b] <author> L. Breiman. </author> <title> Bias, variance, and arcing classifiers. </title> <type> Technical report, </type> <institution> Statistics Department, </institution> <address> U. C. Berkeley, </address> <year> 1996. </year>
Reference-contexts: This raises a central question of the field: why is boosting so successful at improving the generalization performance of already carefully designed learning algorithms? One thing that is clear is that boosting's success cannot be directly attributed to a notion of variance reduction <ref> [Bre96b, BK97] </ref>. This mystery is further compounded by the observation that Adaboost's generalization error often continues to decrease even after it has achieved perfect accuracy on the training set.
Reference: [Bre97a] <author> L. Breiman. </author> <title> Arcing the edge. </title> <type> Technical report, </type> <institution> Statistics Department, </institution> <address> U. C. Berkeley, </address> <year> 1997. </year>
Reference-contexts: When we boost well beyond the range of previously reported experiments, margins may improve for a long timebut beyond some point, gen 1 We note that <ref> [Bre97a] </ref> reports a single experiment that corroborates this point, but as noted in [Bre97b], there is some question as to whether this controlled for all relevant factors. eralization error often deteriorates simultaneously. This is additional evidence against any simple version of the minimum margin story. <p> It has already been observed <ref> [Bre97a] </ref> that this maximization problem can be formulated as a linear program. 6 Here we quickly re-demonstrate this formulation, because it is the starting point and basis to our work. <p> This procedure can provably achieve the optimal margin over the entire base hypothesis space. From the work of <ref> [Bre97a, FS96b] </ref> it is known that the dual of the previous linear program has a very natural interpretation in our setting. (For a review of the standard concepts of primality and duality in LP see [Van96, Lue84].) In the dual problem we maintain a weight u i for each training example, <p> This remarkable fact, an immediate consequence of duality theory, also appears in <ref> [Bre97a, Bre97b] </ref> and [FS96b].
Reference: [Bre97b] <author> L. Breiman. </author> <title> Prediction games and arcing algorithms. </title> <type> Technical report, </type> <institution> Statistics Department, </institution> <address> U. C. Berkeley, </address> <year> 1997. </year>
Reference-contexts: When we boost well beyond the range of previously reported experiments, margins may improve for a long timebut beyond some point, gen 1 We note that [Bre97a] reports a single experiment that corroborates this point, but as noted in <ref> [Bre97b] </ref>, there is some question as to whether this controlled for all relevant factors. eralization error often deteriorates simultaneously. This is additional evidence against any simple version of the minimum margin story. <p> This quantity is in the range [1; 1] and is positive iff the weighted ensemble classifies x i correctly. It is 1 when there is a unanimous vote for the correct label. The definition just given can be found in [SFBL97] and <ref> [Bre97b] </ref>. However, we instead concentrate on a different, but similar, quantity defined by m i = v i;y i P 1). <p> Recently Breiman has proven a similar generalization theorem <ref> [Bre97b] </ref>, which speaks only about the minimum marginand thereby obtains even stronger bounds. <p> In fact, as we see in Section 5, it generally does significantly better. Importantly, this uses the same ensemble as Adaboost and so completely controls for expressive power (which otherwise can be a problem; see <ref> [Bre97b] </ref>). To the extent to which minimum margins really determine generalization error, we should expect this to improve generalization performance. But as we see in Section 5, this expectation is not realized empirically. <p> This remarkable fact, an immediate consequence of duality theory, also appears in <ref> [Bre97a, Bre97b] </ref> and [FS96b].
Reference: [Chv83] <author> V. Chvatal. </author> <title> Linear Programming. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Second, and also related to the dual LP formulation, we note the existence of the ellipsoid algorithm for solving linear programs, famous because it was the first guaranteed polynomial time algorithm for LP <ref> [Kha79, Chv83] </ref>. But it has another interesting property that it does not need to see the explicit constraint matrix. Instead, one only needs an oracle whichgiven any proposed assignment to the variableswill produce a violated constraint if one exists.
Reference: [CV95] <author> C. Cortes and V. Vapnik. </author> <title> Support-vector networks. </title> <booktitle> Machine Learning, </booktitle> <address> 20:27397, </address> <year> 1995. </year>
Reference-contexts: It therefore seems possible that this idea could lead to another boosting algorithm in the style of DualLP-boost, but with perhaps different convergence properties and (at each step) vastly superior computational complexity. Finally, the idea of minimizing margins is reminiscent of the idea of support vector machines <ref> [CV95] </ref>. There, however, one tries to find a linear combination that achieves the best worst separation in the sense of Euclidean (i.e., L 2 ) distance, as opposed to the L 1 notion used to define margins (in a straightforward way but one which we do not formalize here).
Reference: [FS96a] <author> Y. Freund and R. Schapire. </author> <title> Experiments with a new boosting algorithm. </title> <booktitle> In ICML-96, </booktitle> <pages> pages 148156, </pages> <year> 1996. </year>
Reference-contexts: However, there is a growing body of empirical evidence that suggests Adaboost is remarkably effective at reducing the test set error of several well-known learning algorithms, often significantly and across a variety of do mains (more or less robustly, but with occasional exceptions) <ref> [FS96a, Qui96, MO97, BK97] </ref>. This raises a central question of the field: why is boosting so successful at improving the generalization performance of already carefully designed learning algorithms? One thing that is clear is that boosting's success cannot be directly attributed to a notion of variance reduction [Bre96b, BK97]. <p> We can also explicitly solve the LP for this space to determine the optimal ensemble (according to the minimum margin criterion). But most importantly, <ref> [FS96a] </ref> have shown that the benefits of Adaboost are particularly decisive for FindAttrTest. <p> The second learning method we considered is a version of Quinlan's decision tree learning algorithm C4.5 [Qui93]. 9 This is at the other end of the spectrum of base learners in that it produces hypotheses from a very expressive space. Adaboost is generally effective for C4.5 but, as <ref> [FS96a] </ref> point out, the gains are not as dramatic and in fact there is sometimes significant deterioration [Qui96].
Reference: [FS96b] <author> Y. Freund and R. Schapire. </author> <title> Game theory, on-line prediction and boosting. </title> <booktitle> In COLT-96, </booktitle> <pages> pages 325332, </pages> <year> 1996. </year>
Reference-contexts: a linear programming problem simply by conjecturing a lower bound, m, on the minimum value and choosing (m; w) to maximize m subject to w z i m, 5 These results can be extended to infinite base hypothesis spaces to appealing to the standard VC dimension bounds. 6 See also <ref> [FS96b] </ref>, which predates the margin terminology and also casts the definitions in terms of game theory rather than linear programming, but otherwise makes the same point. <p> This procedure can provably achieve the optimal margin over the entire base hypothesis space. From the work of <ref> [Bre97a, FS96b] </ref> it is known that the dual of the previous linear program has a very natural interpretation in our setting. (For a review of the standard concepts of primality and duality in LP see [Van96, Lue84].) In the dual problem we maintain a weight u i for each training example, <p> This remarkable fact, an immediate consequence of duality theory, also appears in [Bre97a, Bre97b] and <ref> [FS96b] </ref>. <p> It is in this sense that such a result, if true, would strengthen <ref> [FS96b] </ref>. runs. Here we consider a single train/test split for each problem. The horizontal axis measures boosting rounds on a logarithmic scale. At the top we show training and test error, and below we plot the corresponding minimum margin over the training set.
Reference: [FS97] <author> Y. Freund and R. Schapire. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <journal> Journal of Computer and Systems Sciences, </journal> <volume> 55(1), </volume> <year> 1997. </year>
Reference-contexts: 1 Introduction Recently, there has been great interest in ensemble methods for learning classifiers, and in particular in boosting <ref> [FS97] </ref> (or arcing [Bre96a]) algorithms. These methods take a given base learning algorithm and repeatedly apply it to reweighted versions of the original training data, producing a collection of hypotheses h 1 ; :::; h b which are then combined in a final aggregate classifier via a weighted linear vote. <p> j (x i ) 6= y i , u i := u i =(2* j ) else, u i := u i =(2 (1 * j )) end return h = (h 1 ; :::; h b ), w = (w 1 ; :::; w b ), b procedure, Adaboost <ref> [FS97] </ref>, computes them in a particular way: at each round j, the example weights for the next round j + 1 are adjusted so that the most recent base hypothesis only obtains error rate 1/2 on the reweighted training set (Figure 1). <p> nice theoretical guarantee about training performance: if the base learner can always find a hypothesis with error bounded strictly below 1=2 for any reweighting of the training data, then Adaboost is guaranteed to produce a final aggregate hypothesis with zero training set error after a finite number of boosting rounds <ref> [FS97] </ref>. Of course, this only addresses training error, and there is no real reason from this to believe that Adaboost should generalize well to unseen test examples.
Reference: [Kha79] <author> L. </author> <title> Khachian. A polynomial algorithm in linear programming. </title> <journal> Doklady Adademiia Nauk SSSR, </journal> <note> 244:109396, 1979. (In Russian) Cited by [Chv83]. </note>
Reference-contexts: Second, and also related to the dual LP formulation, we note the existence of the ellipsoid algorithm for solving linear programs, famous because it was the first guaranteed polynomial time algorithm for LP <ref> [Kha79, Chv83] </ref>. But it has another interesting property that it does not need to see the explicit constraint matrix. Instead, one only needs an oracle whichgiven any proposed assignment to the variableswill produce a violated constraint if one exists.
Reference: [KSD96] <author> R. Kohavi, D. Sommerfield, and J. Dougherty. </author> <title> Data mining using MLC++: A machine learning library in C++. </title> <booktitle> In Tools with Artificial Intelligence. IEEE Computer Society, </booktitle> <year> 1996. </year> <note> http:// www. sgi. com/ technology /mlc. </note>
Reference: [Lue84] <author> D. Luenberger. </author> <title> Linear and Nonlinear Programming. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1984. </year>
Reference-contexts: From the work of [Bre97a, FS96b] it is known that the dual of the previous linear program has a very natural interpretation in our setting. (For a review of the standard concepts of primality and duality in LP see <ref> [Van96, Lue84] </ref>.) In the dual problem we maintain a weight u i for each training example, and a constraint P i u i z ij s for each hypothesis (i.e., column in Z). Here, s is the conjectured bound on the dual objective.
Reference: [MD97] <author> D. Margineantu and T. Dietterich. </author> <title> Pruning adaptive boosting. </title> <booktitle> In ICML-97, </booktitle> <pages> pages 211218, </pages> <year> 1997. </year>
Reference-contexts: But a benefit of maximizing L 1 rather than L 2 margins is that L 1 has a much stronger tendency to produce sparse weight vec tors. This can yield smaller representations of the learned ensemble, which can be an important consideration in practice <ref> [MD97] </ref>. In fact our experiments support this. We often find that LP-Adaboost, for instance, ends up giving zero (or negligible) weight to many of the hypothesis in the ensemble and so the effective ensemble size is smaller.
Reference: [MM] <author> C. Merz and P. Murphy. </author> <title> UCI repository of machine learning databases. </title> <note> http:// www. ics. uci. edu/ mlearn/ MLRepository. html. </note>
Reference: [MO97] <author> R. Maclin and D. Opitz. </author> <title> An empirical evaluation of bagging and boosting. </title> <booktitle> In AAAI-97, </booktitle> <pages> pages 546551, </pages> <year> 1997. </year>
Reference-contexts: However, there is a growing body of empirical evidence that suggests Adaboost is remarkably effective at reducing the test set error of several well-known learning algorithms, often significantly and across a variety of do mains (more or less robustly, but with occasional exceptions) <ref> [FS96a, Qui96, MO97, BK97] </ref>. This raises a central question of the field: why is boosting so successful at improving the generalization performance of already carefully designed learning algorithms? One thing that is clear is that boosting's success cannot be directly attributed to a notion of variance reduction [Bre96b, BK97].
Reference: [Qui93] <author> J. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Mor-gan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: But most importantly, [FS96a] have shown that the benefits of Adaboost are particularly decisive for FindAttrTest. The second learning method we considered is a version of Quinlan's decision tree learning algorithm C4.5 <ref> [Qui93] </ref>. 9 This is at the other end of the spectrum of base learners in that it produces hypotheses from a very expressive space. Adaboost is generally effective for C4.5 but, as [FS96a] point out, the gains are not as dramatic and in fact there is sometimes significant deterioration [Qui96]. <p> Adaboost. When C4.5 is used as the base learner, LP 9 We hedge by saying a version of C4.5 because it is in fact a re-implementation of this program, based closely on the presentation in <ref> [Qui93] </ref>. However, it produces identical output in direct comparisons. Our program corresponds to all C4.5's default settings including pruning. Adaboost always improves Adaboost's margins, by an average of about 0:1. Similarly, there is a fairly consistent increase in the margins with FindAttrTest; generally around 0:05 with greater relative increase.
Reference: [Qui96] <author> J. Quinlan. Bagging, </author> <title> boosting, </title> <booktitle> and C4.5. In AAAI-96, </booktitle> <pages> pages 725730, </pages> <year> 1996. </year>
Reference-contexts: However, there is a growing body of empirical evidence that suggests Adaboost is remarkably effective at reducing the test set error of several well-known learning algorithms, often significantly and across a variety of do mains (more or less robustly, but with occasional exceptions) <ref> [FS96a, Qui96, MO97, BK97] </ref>. This raises a central question of the field: why is boosting so successful at improving the generalization performance of already carefully designed learning algorithms? One thing that is clear is that boosting's success cannot be directly attributed to a notion of variance reduction [Bre96b, BK97]. <p> Adaboost is generally effective for C4.5 but, as [FS96a] point out, the gains are not as dramatic and in fact there is sometimes significant deterioration <ref> [Qui96] </ref>. In Tables 4 and 5 we present the statistics discussed, along with the average minimum margins obtained over 100 runs. (Note that the base learners invariably obtain margins of 1.) First, consider the behavior of LP-Adaboost vs. Adaboost.
Reference: [SFBL97] <author> R. Schapire, Y. Freund, P. Bartlett, and W. Lee. </author> <title> Boosting the margin: a new explanation for the effectiveness of voting methods. </title> <booktitle> In ICML-97, </booktitle> <pages> pages 322330, </pages> <year> 1997. </year>
Reference-contexts: What more information could it possibly be obtaining from the training data? If we could explain boosting's real-world success satisfactorily, we might hope to construct better procedures based upon that explanation. Recent progress in understanding this issue has been made by <ref> [SFBL97] </ref> who appeal to the notion of margins. Rather than focus exclusively on classification error, Schapire et al. consider the strength of the votes given to the correct class labels. <p> They observe that even after zero training error has been achieved, Adaboost tends to increase the vote it gives to the correct class label (relative to the next strongest label vote), and they posit this as an explanation for why Adaboost's test set error continues to decrease. <ref> [SFBL97] </ref> examines the effect of Adaboost on the distribution of margins as a whole. <p> This suggests the more concrete hypothesis that the size of the minimum (worst) margin is the key to generalization performance, and that Adaboost's success is due to its ability to increase this minimum. Supporting this conjecture are two theoretical results, also from <ref> [SFBL97] </ref>: (1) in the limit, Adaboost is guaranteed to achieve a minimum margin that is at least half the best possible, and (2) given that a minimum margin of &gt; 0 can be achieved, then there is a O (1=) bound on generalization error that holds independently of the size of <p> This quantity is in the range [1; 1] and is positive iff the weighted ensemble classifies x i correctly. It is 1 when there is a unanimous vote for the correct label. The definition just given can be found in <ref> [SFBL97] </ref> and [Bre97b]. However, we instead concentrate on a different, but similar, quantity defined by m i = v i;y i P 1). <p> tirely, and from this point use the term margin always to 2 We note that it easy to generalize our results to handle the case where hypotheses map examples to distributions over L. 3 Breiman has sometimes called this second quantity the edge. 4 However, see the extended version of <ref> [SFBL97] </ref>, available at www.research.att.com/schapire. refer to m i = v i;y i P y6=y i v i;y . The reader should remain aware of this subtle terminological distinction. As discussed in the introduction, there is some recent and important theory involving the margin. [SFBL97] show a result bounding generalization error in <p> 4 However, see the extended version of <ref> [SFBL97] </ref>, available at www.research.att.com/schapire. refer to m i = v i;y i P y6=y i v i;y . The reader should remain aware of this subtle terminological distinction. As discussed in the introduction, there is some recent and important theory involving the margin. [SFBL97] show a result bounding generalization error in terms of the margin distribution achieved on the training set. <p> However, the only a priori theoretical connection to Adaboost we know of involves the minimum margin (i.e., fl = sup f : f () = 0g): <ref> [SFBL97] </ref> show that Adaboost achieves at least half of the best possible minimum (see Section 6 for more discussion). Recently Breiman has proven a similar generalization theorem [Bre97b], which speaks only about the minimum marginand thereby obtains even stronger bounds. <p> Of course, neither of these results is likely to be accurate in predicting the actual errors achieved in par ticular real-world problems (among other reasons, because of the O () formulation in <ref> [SFBL97] </ref>); perhaps their real importance is in suggesting the qualitative effect of the minimum margin achieved all else being equal. 3 Maximizing margins: A Linear Program The recent theoretical results just discussed suggest that, beyond minimizing training-set error, we should attempt to make the minimum margin as large as possible. <p> Since all else is controlled for in this experiment, this seems to decisively refute any simple version of the minimum margin story. (Of course, this immediately raises the question of whether there is some other property of the margin distribution which is more predictive of generalization performance; see <ref> [SFBL97] </ref>.) Next, consider the behavior of DualLPboost with Find-AttrTest. With one exception (explained below) this yields even better margins.
Reference: [Van96] <author> R. Vanderbei. </author> <title> Linear Programming: Foundations and Extensions. </title> <publisher> Kluwer, </publisher> <address> Boston, </address> <year> 1996. </year>
Reference-contexts: From the work of [Bre97a, FS96b] it is known that the dual of the previous linear program has a very natural interpretation in our setting. (For a review of the standard concepts of primality and duality in LP see <ref> [Van96, Lue84] </ref>.) In the dual problem we maintain a weight u i for each training example, and a constraint P i u i z ij s for each hypothesis (i.e., column in Z). Here, s is the conjectured bound on the dual objective. <p> Here, s is the conjectured bound on the dual objective. The dual linear program then is to choose (s; u) to minimize s subject to P and u i = 1, u i 0 <ref> [Van96, p70] </ref>. Notice that these constraints have a natural interpretation.
References-found: 20

