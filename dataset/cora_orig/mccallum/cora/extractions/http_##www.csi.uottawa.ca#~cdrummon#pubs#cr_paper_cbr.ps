URL: http://www.csi.uottawa.ca/~cdrummon/pubs/cr_paper_cbr.ps
Refering-URL: http://www.csi.uottawa.ca/~cdrummon/pubs.html
Root-URL: 
Email: cdrummon@csi.uottawa.ca  
Title: Using a Case Base of Surfaces to Speed-Up Reinforcement Learning  
Author: Chris Drummond 
Address: Ottawa, Ontario, Canada, K1N 6N5  
Affiliation: Department of Computer Science, University of Ottawa  
Abstract: This paper demonstrates the exploitation of certain vision processing techniques to index into a case base of surfaces. The surfaces are the result of reinforcement learning and represent the optimum choice of actions to achieve some goal from anywhere in the state space. This paper shows how strong features that occur in the interaction of the system with its environment can be detected early in the learning process. Such features allow the system to identify when an identical, or very similar, task has been solved previously and to retrieve the relevant surface. This results in an orders of magnitude increase in learning rate. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <editor> Agnar Aamodt and Enric Plaza (1994) Case-Based Reasoning: </editor> <title> Foundational Issues, Methodological Variations, and System Approaches. </title> <journal> AICom Artificial Intelligence Communications V. </journal> <volume> 7 No. </volume> <pages> 1 pp 39-37 </pages>
Reference-contexts: 1 Introduction One important research issue for case based learning is its combination with other learning methods. As Aamodt and Plaza <ref> [1] </ref> point out, generally the machine learning community aims to produce "a coherent framework, where each learning method fulfills a specific and distinct role in the system." This paper discusses one such approach, combining case based and reinforcement learning.
Reference: 2. <author> Aha, D. W., and Salzberg, S. L. </author> <year> (1993). </year> <title> Learning to catch: Applying nearest neighbor algorithms to dynamic control tasks. </title> <booktitle> Proc. Fourth International Workshop on Artificial Intelligence and Statistics. </booktitle> <pages> pp 363-368 </pages>
Reference-contexts: But what of other situations where objects with well defined boundaries are not present, an important situation for this type of learning? In Herve et al's paper [5] on robot arm control, there are also strong features, the singularities in the arm's work space. In Aha and Salzberg's paper <ref> [2] </ref> learning is particularly successful where the control surface is smooth, but encounters difficulties in the area of strong non-linearities. These examples demonstrate that even in spaces that do not contain well defined objects strong non-linearities occur.
Reference: 3. <author> C. H. Chin and C. R. </author> <title> Dyer (1986) Model-based recognition in Robot Vision. </title> <booktitle> Computing surveys V. </booktitle> <pages> 18 No 1 pp 67-108 </pages>
Reference-contexts: This connection will become stronger in future work, as discussed in the previous section, where the combination and modification of cases should have much in common with case based planning. Last but not least, is the connection with object recognition in vision research <ref> [14, 3] </ref>. In many ways model bases are similar to case bases.
Reference: 4. <author> Kristian J. </author> <title> Hammond (1990) Case-Based Planning: A Framework for Planning from Experience. </title> <journal> The Journal of Cognitive Science V. </journal> <volume> 14 no. </volume> <pages> 3 </pages>
Reference-contexts: Instead, it is the result of a complete learning episode, so the method should be complementary to these other approaches. This work is also related to case based planning <ref> [4, 18] </ref>, through the general connection of reinforcement learning and planning. This connection will become stronger in future work, as discussed in the previous section, where the combination and modification of cases should have much in common with case based planning.
Reference: 5. <editor> Jean-Yves Herve and Rajeev Sharma And Peter Cucka (1991) The Geometry of Visual Coordination. </editor> <booktitle> Proc. Ninth National Conf. on Artificial Intelligence pp 732-737 </booktitle>
Reference-contexts: But what of other situations where objects with well defined boundaries are not present, an important situation for this type of learning? In Herve et al's paper <ref> [5] </ref> on robot arm control, there are also strong features, the singularities in the arm's work space. In Aha and Salzberg's paper [2] learning is particularly successful where the control surface is smooth, but encounters difficulties in the area of strong non-linearities.
Reference: 6. <author> Frederic Leymarie and Martin D. Levine. </author> <title> (1993) Tracking Deformable Objects in the Plane Using an Active Contour Model. </title> <journal> IEEE Trans. Pattern Analysis And Machine Intelligence V. </journal> <volume> 15 No. </volume> <pages> 6 pp 617-634 </pages>
Reference-contexts: When the snake reaches the top of the ridge it will tend to oscillate around an equilibrium position, by limiting the step size the process can be brought into a stationary state. A more detailed mathematical treatment of this approach is given in <ref> [6] </ref>. Fig. 3. Fitting the Snake 2.3 Constructing and Using the Case Base As discussed at the beginning of this section, the case is exactly the surface produced by reinforcement learning.
Reference: 7. <author> R. A. McCallum, </author> <year> (1995). </year> <title> Instance-based utile distinctions for reinforcement learning. </title> <booktitle> Proc. Twelfth International Conf. on Machine Learning. </booktitle> <pages> pp 387-395 </pages>
Reference-contexts: The second issue is addressed by Moore and Atkeson [11] who keep a queue of "interesting" instances. These are updated most frequently to improve the learning rate. The third issue is addressed by McCallum <ref> [7] </ref> who uses trees which expand the state representation to include prior states removing ambiguity due to hidden states.
Reference: 8. <author> R. A. </author> <title> McCallum (1995). Instance-based state identification for reinforcement learning. </title> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <pages> pp 377-384 </pages>
Reference-contexts: These are updated most frequently to improve the learning rate. The third issue is addressed by McCallum [7] who uses trees which expand the state representation to include prior states removing ambiguity due to hidden states. In further work <ref> [8] </ref> he uses a single representation to address both the hidden state problem and the general problem of representing a large state space by using a case base of state sequences associated with various trajectories.
Reference: 9. <author> Staphane Mallat and Sifen Zhong (1992). </author> <title> Characterization of Signals from Multi-scale Edges. </title> <journal> IEEE Trans. Pattern Analysis And Machine Intelligence V. </journal> <volume> 14 No. </volume> <pages> 7 pp 710-732 </pages>
Reference-contexts: This interaction can be represented by a surface, a multi-dimensional function. For the notion of strong features to be effective, they must largely dictate the shape of this surface. If the features differ by a small amount one would expect the surface to differ by a small amount. Research <ref> [9] </ref> has shown that a function can be accurately reconstructed from a record of its steep slopes. In this work, tasks producing similar features result in the same surfaces, at some qualitative abstraction.
Reference: 10. <institution> David Marr (1982) Vision : a computational investigation into the human representation and processing of visual information. </institution> <address> W.H. </address> <publisher> Freeman </publisher>
Reference-contexts: In this work, tasks producing similar features result in the same surfaces, at some qualitative abstraction. This paper takes much of its insight from work done in vision and is closely related to Marr's early work <ref> [10] </ref>. The features are essentially Marr's zero crossing points. A popular technique in object recognition, the snake [14], will be used to locate and characterise these features.
Reference: 11. <author> A. W. Moore and C. G. </author> <title> Atkeson (1993) Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time. Machine Learning, </title> <publisher> V. </publisher> <pages> 13 pp 103-130 </pages>
Reference-contexts: Sheppard and Salzberg [13] also use learnt instances but they are carefully selected by a genetic algorithm. The second issue is addressed by Moore and Atkeson <ref> [11] </ref> who keep a queue of "interesting" instances. These are updated most frequently to improve the learning rate. The third issue is addressed by McCallum [7] who uses trees which expand the state representation to include prior states removing ambiguity due to hidden states.
Reference: 12. <editor> Jing Peng (1995) Efficient Memory-Based Dynamic Programming. </editor> <booktitle> Proc. Twelfth International Conf. of Machine Learning pp 438-446 </booktitle>
Reference-contexts: Their use has been to address a number of issues: the economical representation of the state space, prioritising states for updating and dealing with hidden state. The first issue is addressed by Peng <ref> [12] </ref> and Tadepalli [17] who use learnt instances combined with linear regression over a set of neighbouring points. Sheppard and Salzberg [13] also use learnt instances but they are carefully selected by a genetic algorithm.
Reference: 13. <author> John W. Sheppard and Steven L. </author> <title> Salzberg (1996) A teaching strategy for memory-based control. </title> <note> To appear in AI Review, special issue on Lazy Learning. </note>
Reference-contexts: The first issue is addressed by Peng [12] and Tadepalli [17] who use learnt instances combined with linear regression over a set of neighbouring points. Sheppard and Salzberg <ref> [13] </ref> also use learnt instances but they are carefully selected by a genetic algorithm. The second issue is addressed by Moore and Atkeson [11] who keep a queue of "interesting" instances. These are updated most frequently to improve the learning rate.
Reference: 14. <author> P. Suetens and P. Fua and A. </author> <title> Hanson (1992) Computational strategies for object recognition. </title> <journal> Computing surveys V. </journal> <volume> 24 No. </volume> <pages> 1 pp 5-61 </pages>
Reference-contexts: This paper takes much of its insight from work done in vision and is closely related to Marr's early work [10]. The features are essentially Marr's zero crossing points. A popular technique in object recognition, the snake <ref> [14] </ref>, will be used to locate and characterise these features. One important difference in this research, is that recognising the task is not an end in itself but is a means of initialising another learning process. <p> This connection will become stronger in future work, as discussed in the previous section, where the combination and modification of cases should have much in common with case based planning. Last but not least, is the connection with object recognition in vision research <ref> [14, 3] </ref>. In many ways model bases are similar to case bases.
Reference: 15. <author> R.S. </author> <title> Sutton (1988) Learning to Predict by the Methods of Temporal Differences. </title> <booktitle> Machine Learning V. </booktitle> <pages> 3 pp 9-44 </pages>
Reference-contexts: This problem has been the focus of much work in the reinforcement learning community. To address it an action is evaluated not by waiting until the final outcome of the task, but rather by correlating it to the evaluation of the best action in the next state <ref> [15] </ref>. For the robot learning task, at each state an estimate of the distance to the goal could be kept. This estimate would be the result of the best local action, one moving the robot to the new state with the smallest estimate of distance to goal.
Reference: 16. <author> R.S. </author> <title> Sutton (1990) Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> Proc. Seventh International Conf. on Machine Learning pp 216-224 </booktitle>
Reference-contexts: This forms a surface over the state space. Figure 2a shows such a surface produced by the learning process using the environment of figure 1. Fig. 2. a) A Surface b) Its Gradient Vector The two algorithms used most frequently in reinforcement learning are Q--learning [19] and TD () <ref> [16] </ref>. This research adopts the Q-learning algorithm, the simpler of the two. This algorithm assumes the world is a discrete Markov process thus both states and actions actions are discrete.
Reference: 17. <author> P. Tadepalli and D. </author> <title> Ok (1996) Scaling up Average Reward Reinforcement Learning by Approximating the Domain Models and the Value Function. </title> <booktitle> Proc. Thirteenth International Conf. of Machine Learning pp 471-479 </booktitle>
Reference-contexts: Their use has been to address a number of issues: the economical representation of the state space, prioritising states for updating and dealing with hidden state. The first issue is addressed by Peng [12] and Tadepalli <ref> [17] </ref> who use learnt instances combined with linear regression over a set of neighbouring points. Sheppard and Salzberg [13] also use learnt instances but they are carefully selected by a genetic algorithm. The second issue is addressed by Moore and Atkeson [11] who keep a queue of "interesting" instances.
Reference: 18. <author> Manuela M. Veloso and Jaime G. </author> <title> Carbonell (1993) Derivational Analogy in PRODIGY: Automating Case Acquisition, storage and Utilization. </title> <booktitle> Machine Learning V. </booktitle> <volume> 10 No. </volume> <pages> 3 pp 249-278 </pages>
Reference-contexts: Instead, it is the result of a complete learning episode, so the method should be complementary to these other approaches. This work is also related to case based planning <ref> [4, 18] </ref>, through the general connection of reinforcement learning and planning. This connection will become stronger in future work, as discussed in the previous section, where the combination and modification of cases should have much in common with case based planning.
Reference: 19. <author> Christopher J.C.H. </author> <title> Watkins and Peter Dayan (1992) Technical Note:Q-Learning Machine Learning V. 8 No 3-4 pp 279-292 This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: This forms a surface over the state space. Figure 2a shows such a surface produced by the learning process using the environment of figure 1. Fig. 2. a) A Surface b) Its Gradient Vector The two algorithms used most frequently in reinforcement learning are Q--learning <ref> [19] </ref> and TD () [16]. This research adopts the Q-learning algorithm, the simpler of the two. This algorithm assumes the world is a discrete Markov process thus both states and actions actions are discrete. <p> The surface discussed in previous paragraphs and shown in the figures represents this maximum value. Q t+1 s;a + ff (r + flmax a 0 Q t s 0 ;a 0 ) (1) Watkins and Dayan <ref> [19] </ref> proved that this will converge to the optimal value with certain constraints on the reward and the learning rate ff. The optimal solution is to take the action with the greatest value in any state.
References-found: 19

