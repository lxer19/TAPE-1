URL: http://www.cs.cmu.edu/afs/cs/user/yiming/www/courses/bibliography/papers/cmu-cs-97-127.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/user/yiming/www/courses/bibliography/papers/
Root-URL: 
Email: yiming@cs.cmu.edu  
Title: An Evaluation of Statistical Approaches to Text Categorization  
Author: Yiming Yang 
Note: This research was supported in part by NIH grant LM-05714 and by NSF grant IRI9314992. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of NIH or the U.S. Government.  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: April 10, 1997  
Pubnum: CMU-CS-97-127  
Abstract: This paper is a comparative study of text categorization methods. Fourteen methods are investigated, based on previously published results and newly obtained results from additional experiments. Corpus biases in commonly used document collections are examined using the performance of three classifiers. Problems in previously published experiments are analyzed, and the results of flawed experiments are excluded from the cross-method evaluation. As a result, eleven out of the fourteen methods are remained. A k-nearest neighbor (kNN) classifier was chosen for the performance baseline on several collections; on each collection, the performance scores of other methods were normalized using the score of kNN. This provides a common basis for a global observation on methods whose results are only available on individual collections. Widrow-Hoff, k-nearest neighbor, neural networks and the Linear Least Squares Fit mapping are the top-performing classifiers, while the Rocchio approaches had relatively poor results compared to the other learning methods. KNN is the only learning method that has scaled to the full domain of MEDLINE categories, showing a graceful behavior when the target space grows from the level of one hundred categories to a level of tens of thousands. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Apte, F. Damerau, and S. Weiss. </author> <title> Towards language independent automated learning of text categorization models. </title> <booktitle> In Proceedings of the 17th Annual ACM/SIGIR conference, </booktitle> <year> 1994. </year>
Reference-contexts: A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 18], nearest neighbor classifiers [3, 19], Bayes belief networks [14, 9], decision trees [5, 9, 11], rule learning algorithms <ref> [1, 15, 12] </ref>, neural networks [15] and inductive learning techniques [2, 8]. With more and more methods available, cross-method evaluation becomes increasingly important. However, without an unified methodology of empirical validation, an objective comparison is difficult. The most serious problem is the lack of standard data collections. <p> It is not clear whether these unlabelled documents are all negative instances of the categories in consideration, or that they are unlabelled simply as an oversight. Apte et al. run a rule learning algorithm, SWAP-1, on the same set of documents after removing the unlabelled documents <ref> [1] </ref>. <p> a bag of words), and the SMART system [13] is used as the search engine. 3 Collection Analysis 3.1 Two corpora The Reuters corpus, a collection of newswire stories from 1987 to 1991, is commonly used for text categorization research, starting from an early evaluation of the CONSTRUE expert system <ref> [6, 9, 1, 15, 12, 2] </ref> 1 . This collection is 1 A newly refined version named Reuters-21578 is available through Lewis' home page http://www.research.att.com/ ~ lewis. 2 split into training and test sets when used to evaluate various learning systems. <p> Given a classifier, the values of r; p; e and f often depend on internal parameter tuning; there is a trade-off between recall and precision in general. A commonly used measure in method comparison <ref> [9, 1, 15, 12] </ref> is the break-even point (BrkEvn) of recall and precision, i.e., when r and p are tuned to be equal. <p> The Reuters Apte set has the densest column where the results of eight systems are available. Although the document counts reported by different researchers are somewhat inconsistent <ref> [1, 2] </ref> 4 , the differences are relatively small compared to the size of the corpus (i.e., at most 21 miscounted out of over ten thousands training documents, and at most 7 miscounted out of over three thousands of test documents), so the impact of such differences on the evaluation results <p> This observation raises a question with respect to a claim about the particular advantage of rule learning in text categorization. The claim was based on context-sensitivity, i.e., the power in capturing term combinations <ref> [1, 2] </ref>. It seems that the methods which do not explicitly identify term combinations but use the context implicitly (such as in WH, NNets, kNN and LLSF) performed at least as well. <p> The performance of a classifier depends strongly on the choice of data used for evaluation. Using a seriously problematic collection [8], comparing categorization methods without analyzing collection differences <ref> [1] </ref>, and drawing conclusion based on the results of flawed experiments [2] raise questions about the validity of some published evaluations. These problems need to be addressed to clarify of the confusions among researchers, and to prevent the repetition of similar mistakes. <p> Rocchio had a relatively poor performance, on the other hand. All the learning methods outperformed WORD, the non-learning method. However, the differences between some learning methods are not as large as previously claimed <ref> [1, 2] </ref>. It is not evident in the collected results that non-linear models are better than linear models, or that more sophisticated methods outperform simpler ones. Conclusive statements on the strengths and weaknesses of different models requires further research. 4.
Reference: [2] <author> William W. Cohen and Yoram Singer. </author> <title> Context-sensitive learning metods for text categorization. </title> <booktitle> In SIGIR '96: Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1996. </year> <pages> 307-315. </pages>
Reference-contexts: A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 18], nearest neighbor classifiers [3, 19], Bayes belief networks [14, 9], decision trees [5, 9, 11], rule learning algorithms [1, 15, 12], neural networks [15] and inductive learning techniques <ref> [2, 8] </ref>. With more and more methods available, cross-method evaluation becomes increasingly important. However, without an unified methodology of empirical validation, an objective comparison is difficult. The most serious problem is the lack of standard data collections. <p> SWAP-1, an inductive learning algorithm for classification using rules in Disjunctive Normal Form (DNF)[1]. 5. A neural network approach (NNets) to classification [15]. 6. CHARADE, a DNF rule learning system for classification by I. Moulinier [12]. 7. RIPPER, a DNF rule learning system for classification by W. Cohen <ref> [2] </ref>. 8. Rocchio, a vector space model for classification where a training set of documents are used to construct a prototype vector for each category, and category ranking given a document is based on a similarity comparison between the document vector and the category vectors [8]. 9. <p> An exponentiated gradient (EG) inductive learning algorithm which approximates a least squares fit [8]. 10. The Widrow-Hoff (WH) inductive learning algorithm which approximates a least squares fit [8]. 11. Sleeping Experts (EXPERTS), an inductive learning system using n-gram phrases in classification <ref> [2] </ref>. 12. LLSF, a linear least squares fit (LLSF) approach to classification [18]. A single regression model is used for ranking multiple categories given a test document. <p> a bag of words), and the SMART system [13] is used as the search engine. 3 Collection Analysis 3.1 Two corpora The Reuters corpus, a collection of newswire stories from 1987 to 1991, is commonly used for text categorization research, starting from an early evaluation of the CONSTRUE expert system <ref> [6, 9, 1, 15, 12, 2] </ref> 1 . This collection is 1 A newly refined version named Reuters-21578 is available through Lewis' home page http://www.research.att.com/ ~ lewis. 2 split into training and test sets when used to evaluate various learning systems. <p> This raises a serious question as to whether or not these unlabelled documents should be included in the test set, and treated as negative instances of all categories, as they were handled in the previous experiments <ref> [9, 2] </ref>. The following analysis addresses this question. Assume the test set has 58% unlabelled documents, and suppose that all of the unlabelled documents should be assigned categories but are erroneously unlabelled. <p> A commonly used measure in method comparison [9, 1, 15, 12] is the break-even point (BrkEvn) of recall and precision, i.e., when r and p are tuned to be equal. Another common measure <ref> [12, 8, 2] </ref> is called the F -measure, defined to be: F fi (r; p) = fi 2 p + r where fi is the parameter allowing differential weighting of p and r. <p> The Reuters Apte set has the densest column where the results of eight systems are available. Although the document counts reported by different researchers are somewhat inconsistent <ref> [1, 2] </ref> 4 , the differences are relatively small compared to the size of the corpus (i.e., at most 21 miscounted out of over ten thousands training documents, and at most 7 miscounted out of over three thousands of test documents), so the impact of such differences on the evaluation results <p> Cohen concluded EXPERTS the best performer ever reported on the Lewis set without an explanation on its mysterious insensitivity to the large change in test documents <ref> [2] </ref>. This is suspicious because the inclusion of a large amounts of incorrectly labelled documents in the test set should decrease the performance of a good classifier, as analyzed in Section 3. <p> This observation raises a question with respect to a claim about the particular advantage of rule learning in text categorization. The claim was based on context-sensitivity, i.e., the power in capturing term combinations <ref> [1, 2] </ref>. It seems that the methods which do not explicitly identify term combinations but use the context implicitly (such as in WH, NNets, kNN and LLSF) performed at least as well. <p> The performance of a classifier depends strongly on the choice of data used for evaluation. Using a seriously problematic collection [8], comparing categorization methods without analyzing collection differences [1], and drawing conclusion based on the results of flawed experiments <ref> [2] </ref> raise questions about the validity of some published evaluations. These problems need to be addressed to clarify of the confusions among researchers, and to prevent the repetition of similar mistakes. Providing information and analysis on these problems is a major effort in this study. 2. <p> Rocchio had a relatively poor performance, on the other hand. All the learning methods outperformed WORD, the non-learning method. However, the differences between some learning methods are not as large as previously claimed <ref> [1, 2] </ref>. It is not evident in the collected results that non-linear models are better than linear models, or that more sophisticated methods outperform simpler ones. Conclusive statements on the strengths and weaknesses of different models requires further research. 4.
Reference: [3] <author> R.H. Creecy, B.M. Masand, S.J. Smith, and D.L. Waltz. </author> <title> Trading mips and memory for knowledge engineering: classifying census returns on the connection machine. </title> <journal> Comm. ACM, </journal> <volume> 35 </volume> <pages> 48-63, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Text categorization is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 18], nearest neighbor classifiers <ref> [3, 19] </ref>, Bayes belief networks [14, 9], decision trees [5, 9, 11], rule learning algorithms [1, 15, 12], neural networks [15] and inductive learning techniques [2, 8]. With more and more methods available, cross-method evaluation becomes increasingly important.
Reference: [4] <editor> Ed. DK Harman. </editor> <booktitle> Overview of the Third Text REtrieval Conference (TREC-3). </booktitle> <institution> US Government Printing Office, </institution> <address> Washington, DC, </address> <year> 1995. </year>
Reference-contexts: It would be ideal if a universal test collection were shared by all the text categorization researchers, or if a controlled evaluation of a wide range of categorization methods were conducted, similar to the Text Retrieval Conference for document retrieval <ref> [4] </ref>. The reality, however, is still far from the ideal. Cross-method comparisons have often been attempted but only for two or three methods.
Reference: [5] <author> N. Fuhr, S. Hartmanna, G. Lustig, M. Schwantner, and K. Tzeras. </author> <title> Air/x a rule-based multistage indexing systems for large subject fields. In 606-623, editor, </title> <booktitle> Proceedings of RIAO'91, </booktitle> <year> 1991. </year> <month> 9 </month>
Reference-contexts: 1 Introduction Text categorization is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models <ref> [5, 18] </ref>, nearest neighbor classifiers [3, 19], Bayes belief networks [14, 9], decision trees [5, 9, 11], rule learning algorithms [1, 15, 12], neural networks [15] and inductive learning techniques [2, 8]. With more and more methods available, cross-method evaluation becomes increasingly important. <p> 1 Introduction Text categorization is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 18], nearest neighbor classifiers [3, 19], Bayes belief networks [14, 9], decision trees <ref> [5, 9, 11] </ref>, rule learning algorithms [1, 15, 12], neural networks [15] and inductive learning techniques [2, 8]. With more and more methods available, cross-method evaluation becomes increasingly important. However, without an unified methodology of empirical validation, an objective comparison is difficult.
Reference: [6] <author> P.J. Hayes and S. P. Weinstein. Construe/tis: </author> <title> a system for content-based indexing of a database of new stories. </title> <booktitle> In Second Annual Conference on Innovative Applications of Artificial Intelligence, </booktitle> <year> 1990. </year>
Reference-contexts: However, without an unified methodology of empirical validation, an objective comparison is difficult. The most serious problem is the lack of standard data collections. Even when a shared collection is chosen, there are still many ways to introduce inconsistency. For example, the commonly used Reuters newswire corpus <ref> [6] </ref> has at least four different versions, depending on how the training/test sets were divided, and what categories are included or excluded in the evaluation. <p> These methods are outlined below; the data sets and the result comparability will be analyzed in the next section. 1. CONSTRUE, an expert system consisting of manually developed categorization rules for Reuters news stories <ref> [6] </ref>. 2. Decision tree (DTree) algorithms for classification [9, 11]. 3. A naive Bayes model (NaiveBayes) for classification where word independence is assumed in category prediction [9, 10]. Table 1. <p> a bag of words), and the SMART system [13] is used as the search engine. 3 Collection Analysis 3.1 Two corpora The Reuters corpus, a collection of newswire stories from 1987 to 1991, is commonly used for text categorization research, starting from an early evaluation of the CONSTRUE expert system <ref> [6, 9, 1, 15, 12, 2] </ref> 1 . This collection is 1 A newly refined version named Reuters-21578 is available through Lewis' home page http://www.research.att.com/ ~ lewis. 2 split into training and test sets when used to evaluate various learning systems.
Reference: [7] <author> W. Hersh, C. Buckley, T.J. Leone, and D. Hickman. Ohsumed: </author> <title> an interactive retrieval evaluation and new large text collection for research. </title> <booktitle> In 17th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'94), </booktitle> <pages> pages 192-201, </pages> <year> 1994. </year>
Reference-contexts: Section 6 concludes the findings. 2 Categorization Methods The intention here is to integrate available results from individual experiments into a global evaluation. Two commonly used corpora, the Reuters news story collection [9] and the OHSUMED bibliographical document collection <ref> [7] </ref> are chosen for this purpose. Fourteen categorization methods are investigated, including eleven methods which were previously evaluated using these corpora, and three methods which were newly evaluated by this author. Not all of the results are directly comparable because different versions or subsets of these corpora were used.
Reference: [8] <author> David D. Lewis, Robert E. Schapire, James P. Callan, and Ron Papka. </author> <title> Training algorithms for linear text classifiers. </title> <booktitle> In SIGIR '96: Proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1996. </year> <pages> 298-306. </pages>
Reference-contexts: A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 18], nearest neighbor classifiers [3, 19], Bayes belief networks [14, 9], decision trees [5, 9, 11], rule learning algorithms [1, 15, 12], neural networks [15] and inductive learning techniques <ref> [2, 8] </ref>. With more and more methods available, cross-method evaluation becomes increasingly important. However, without an unified methodology of empirical validation, an objective comparison is difficult. The most serious problem is the lack of standard data collections. <p> Cohen [2]. 8. Rocchio, a vector space model for classification where a training set of documents are used to construct a prototype vector for each category, and category ranking given a document is based on a similarity comparison between the document vector and the category vectors <ref> [8] </ref>. 9. An exponentiated gradient (EG) inductive learning algorithm which approximates a least squares fit [8]. 10. The Widrow-Hoff (WH) inductive learning algorithm which approximates a least squares fit [8]. 11. Sleeping Experts (EXPERTS), an inductive learning system using n-gram phrases in classification [2]. 12. <p> a training set of documents are used to construct a prototype vector for each category, and category ranking given a document is based on a similarity comparison between the document vector and the category vectors <ref> [8] </ref>. 9. An exponentiated gradient (EG) inductive learning algorithm which approximates a least squares fit [8]. 10. The Widrow-Hoff (WH) inductive learning algorithm which approximates a least squares fit [8]. 11. Sleeping Experts (EXPERTS), an inductive learning system using n-gram phrases in classification [2]. 12. LLSF, a linear least squares fit (LLSF) approach to classification [18]. <p> category, and category ranking given a document is based on a similarity comparison between the document vector and the category vectors <ref> [8] </ref>. 9. An exponentiated gradient (EG) inductive learning algorithm which approximates a least squares fit [8]. 10. The Widrow-Hoff (WH) inductive learning algorithm which approximates a least squares fit [8]. 11. Sleeping Experts (EXPERTS), an inductive learning system using n-gram phrases in classification [2]. 12. LLSF, a linear least squares fit (LLSF) approach to classification [18]. A single regression model is used for ranking multiple categories given a test document. <p> The OHSUMED collection has been used with the full range of categories (14,321 MeSH categories actually occurred) in some experiments [17], or with a subset of categories in the heart disease sub-domain (HD, 119 categories) in other experiments <ref> [8] </ref>. 3.2 Different versions Table 1 lists the different versions or subsets of Reuters and OHSUMED. Each is referred as a "set" or "collection", and labelled for reference. <p> A commonly used measure in method comparison [9, 1, 15, 12] is the break-even point (BrkEvn) of recall and precision, i.e., when r and p are tuned to be equal. Another common measure <ref> [12, 8, 2] </ref> is called the F -measure, defined to be: F fi (r; p) = fi 2 p + r where fi is the parameter allowing differential weighting of p and r. <p> It is also worth asking whether there is something else, beyond the core theory, which contributed to the good performance. In the WH experiment on "HD big", Lewis used a "pocketing" strategy to select a subset of training instances from a large pool <ref> [8] </ref>. This is similar or equivalent to a sampling strategy which divides available training instances into small chunks, examines one chunk at a time using a validation set, and adds a new chunk to the selected ones only if it improves the performance on the validation set. <p> The performance of a classifier depends strongly on the choice of data used for evaluation. Using a seriously problematic collection <ref> [8] </ref>, comparing categorization methods without analyzing collection differences [1], and drawing conclusion based on the results of flawed experiments [2] raise questions about the validity of some published evaluations. These problems need to be addressed to clarify of the confusions among researchers, and to prevent the repetition of similar mistakes.
Reference: [9] <author> D.D. Lewis and M. Ringuette. </author> <title> Comparison of two learning algorithms for text categorization. </title> <booktitle> In Proceedings of the Third Annual Symposium on Document Analysis and Information Retrieval (SDAIR'94), </booktitle> <year> 1994. </year>
Reference-contexts: 1 Introduction Text categorization is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 18], nearest neighbor classifiers [3, 19], Bayes belief networks <ref> [14, 9] </ref>, decision trees [5, 9, 11], rule learning algorithms [1, 15, 12], neural networks [15] and inductive learning techniques [2, 8]. With more and more methods available, cross-method evaluation becomes increasingly important. However, without an unified methodology of empirical validation, an objective comparison is difficult. <p> 1 Introduction Text categorization is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 18], nearest neighbor classifiers [3, 19], Bayes belief networks [14, 9], decision trees <ref> [5, 9, 11] </ref>, rule learning algorithms [1, 15, 12], neural networks [15] and inductive learning techniques [2, 8]. With more and more methods available, cross-method evaluation becomes increasingly important. However, without an unified methodology of empirical validation, an objective comparison is difficult. <p> Lewis and Ringuette used this corpus to evaluate a decision tree approach and a naive Bayes classifier, where they included a large portion of unlabelled documents (47% in the training set, and 58% in the test set) <ref> [9] </ref>. It is not clear whether these unlabelled documents are all negative instances of the categories in consideration, or that they are unlabelled simply as an oversight. Apte et al. run a rule learning algorithm, SWAP-1, on the same set of documents after removing the unlabelled documents [1]. <p> Section 6 concludes the findings. 2 Categorization Methods The intention here is to integrate available results from individual experiments into a global evaluation. Two commonly used corpora, the Reuters news story collection <ref> [9] </ref> and the OHSUMED bibliographical document collection [7] are chosen for this purpose. Fourteen categorization methods are investigated, including eleven methods which were previously evaluated using these corpora, and three methods which were newly evaluated by this author. <p> These methods are outlined below; the data sets and the result comparability will be analyzed in the next section. 1. CONSTRUE, an expert system consisting of manually developed categorization rules for Reuters news stories [6]. 2. Decision tree (DTree) algorithms for classification <ref> [9, 11] </ref>. 3. A naive Bayes model (NaiveBayes) for classification where word independence is assumed in category prediction [9, 10]. Table 1. <p> CONSTRUE, an expert system consisting of manually developed categorization rules for Reuters news stories [6]. 2. Decision tree (DTree) algorithms for classification [9, 11]. 3. A naive Bayes model (NaiveBayes) for classification where word independence is assumed in category prediction <ref> [9, 10] </ref>. Table 1. <p> a bag of words), and the SMART system [13] is used as the search engine. 3 Collection Analysis 3.1 Two corpora The Reuters corpus, a collection of newswire stories from 1987 to 1991, is commonly used for text categorization research, starting from an early evaluation of the CONSTRUE expert system <ref> [6, 9, 1, 15, 12, 2] </ref> 1 . This collection is 1 A newly refined version named Reuters-21578 is available through Lewis' home page http://www.research.att.com/ ~ lewis. 2 split into training and test sets when used to evaluate various learning systems. <p> This raises a serious question as to whether or not these unlabelled documents should be included in the test set, and treated as negative instances of all categories, as they were handled in the previous experiments <ref> [9, 2] </ref>. The following analysis addresses this question. Assume the test set has 58% unlabelled documents, and suppose that all of the unlabelled documents should be assigned categories but are erroneously unlabelled. <p> Given a classifier, the values of r; p; e and f often depend on internal parameter tuning; there is a trade-off between recall and precision in general. A commonly used measure in method comparison <ref> [9, 1, 15, 12] </ref> is the break-even point (BrkEvn) of recall and precision, i.e., when r and p are tuned to be equal.
Reference: [10] <author> I. Moulinier. </author> <title> Une approche de la categorisation de textes par lapprentissage symbolique. </title> <type> In PhD thesis, </type> <institution> Uni-versite Pierre et Marie Curie (Paris 6), </institution> <year> 1996. </year>
Reference-contexts: CONSTRUE, an expert system consisting of manually developed categorization rules for Reuters news stories [6]. 2. Decision tree (DTree) algorithms for classification [9, 11]. 3. A naive Bayes model (NaiveBayes) for classification where word independence is assumed in category prediction <ref> [9, 10] </ref>. Table 1.
Reference: [11] <author> I. Moulinier. </author> <title> Is learning bias an issue on the text categorization problem? In Technical report, </title> <institution> LAFORIA-LIP6, Universite Paris VI, </institution> <note> page (to appear), </note> <year> 1997. </year>
Reference-contexts: 1 Introduction Text categorization is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 18], nearest neighbor classifiers [3, 19], Bayes belief networks [14, 9], decision trees <ref> [5, 9, 11] </ref>, rule learning algorithms [1, 15, 12], neural networks [15] and inductive learning techniques [2, 8]. With more and more methods available, cross-method evaluation becomes increasingly important. However, without an unified methodology of empirical validation, an objective comparison is difficult. <p> These methods are outlined below; the data sets and the result comparability will be analyzed in the next section. 1. CONSTRUE, an expert system consisting of manually developed categorization rules for Reuters news stories [6]. 2. Decision tree (DTree) algorithms for classification <ref> [9, 11] </ref>. 3. A naive Bayes model (NaiveBayes) for classification where word independence is assumed in category prediction [9, 10]. Table 1. <p> Should we then conclude that SWAP-1 is better than kNN, or the opposite? More interestingly, a recent result using a DTree algorithm (via C4.5) due to Moulinier scores 79% on the Apte set <ref> [11] </ref>, which is exactly the same as the SWAP-1 result. How should this be interpreted? To make the point clear, the Lewis set should not be used for text categorization evaluation unless the status of the unlabelled documents is resolved.
Reference: [12] <author> I. Moulinier, G. Raskinis, and J. Ganascia. </author> <title> Text categorization: a symbolic approach. </title> <booktitle> In Proceedings of the Fifth Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <year> 1996. </year>
Reference-contexts: A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 18], nearest neighbor classifiers [3, 19], Bayes belief networks [14, 9], decision trees [5, 9, 11], rule learning algorithms <ref> [1, 15, 12] </ref>, neural networks [15] and inductive learning techniques [2, 8]. With more and more methods available, cross-method evaluation becomes increasingly important. However, without an unified methodology of empirical validation, an objective comparison is difficult. The most serious problem is the lack of standard data collections. <p> SWAP-1, an inductive learning algorithm for classification using rules in Disjunctive Normal Form (DNF)[1]. 5. A neural network approach (NNets) to classification [15]. 6. CHARADE, a DNF rule learning system for classification by I. Moulinier <ref> [12] </ref>. 7. RIPPER, a DNF rule learning system for classification by W. Cohen [2]. 8. <p> a bag of words), and the SMART system [13] is used as the search engine. 3 Collection Analysis 3.1 Two corpora The Reuters corpus, a collection of newswire stories from 1987 to 1991, is commonly used for text categorization research, starting from an early evaluation of the CONSTRUE expert system <ref> [6, 9, 1, 15, 12, 2] </ref> 1 . This collection is 1 A newly refined version named Reuters-21578 is available through Lewis' home page http://www.research.att.com/ ~ lewis. 2 split into training and test sets when used to evaluate various learning systems. <p> Given a classifier, the values of r; p; e and f often depend on internal parameter tuning; there is a trade-off between recall and precision in general. A commonly used measure in method comparison <ref> [9, 1, 15, 12] </ref> is the break-even point (BrkEvn) of recall and precision, i.e., when r and p are tuned to be equal. <p> A commonly used measure in method comparison [9, 1, 15, 12] is the break-even point (BrkEvn) of recall and precision, i.e., when r and p are tuned to be equal. Another common measure <ref> [12, 8, 2] </ref> is called the F -measure, defined to be: F fi (r; p) = fi 2 p + r where fi is the parameter allowing differential weighting of p and r.
Reference: [13] <author> G. Salton. </author> <title> Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Pennsylvania, </address> <year> 1989. </year>
Reference-contexts: A simple, non-learning method which ranks categories for a document based on word matching (WORD) between the document and category names. The conventional Vector Space Model is used for representing documents and category names (each name is treated as a bag of words), and the SMART system <ref> [13] </ref> is used as the search engine. 3 Collection Analysis 3.1 Two corpora The Reuters corpus, a collection of newswire stories from 1987 to 1991, is commonly used for text categorization research, starting from an early evaluation of the CONSTRUE expert system [6, 9, 1, 15, 12, 2] 1 . <p> Since the behavior of a single classifier may lead to biased conclusions, the multiple and fundamentally different classifiers were used instead. All the systems produces a ranked list of candidate categories given a document. The conventional 11-point average precision <ref> [13] </ref> was used to measure the goodness of category ranking. WORD and kNN were tested on all the collections, while LLSF was only tested on the smaller collections due to computational limitations. The HD sets were examined together with the OHSUMED superset instead of being examined separately. <p> For a collection of test documents, the category ranking for each document is evaluated first, then the performance scores are averaged across documents. The conventional 11-point average precision is used to measure the performance of a classifier on a collection of documents <ref> [13] </ref>. 4.2 Evaluation of binary classification Performance measures in binary classification can be defined using a two-way contingency table (Table 2).
Reference: [14] <author> K. Tzeras and S. Hartman. </author> <title> Automatic indexing based on bayesian inference networks. </title> <booktitle> In Proc 16th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'93), </booktitle> <pages> pages 22-34, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction Text categorization is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 18], nearest neighbor classifiers [3, 19], Bayes belief networks <ref> [14, 9] </ref>, decision trees [5, 9, 11], rule learning algorithms [1, 15, 12], neural networks [15] and inductive learning techniques [2, 8]. With more and more methods available, cross-method evaluation becomes increasingly important. However, without an unified methodology of empirical validation, an objective comparison is difficult.
Reference: [15] <author> E. Wiener, J.O. Pedersen, </author> <title> and A.S. Weigend. A neural network approach to topic spotting. </title> <booktitle> In Proceedings of the Fourth Annual Symposium on Document Analysis and Information Retrieval (SDAIR'95), </booktitle> <year> 1995. </year>
Reference-contexts: A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 18], nearest neighbor classifiers [3, 19], Bayes belief networks [14, 9], decision trees [5, 9, 11], rule learning algorithms <ref> [1, 15, 12] </ref>, neural networks [15] and inductive learning techniques [2, 8]. With more and more methods available, cross-method evaluation becomes increasingly important. However, without an unified methodology of empirical validation, an objective comparison is difficult. The most serious problem is the lack of standard data collections. <p> A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 18], nearest neighbor classifiers [3, 19], Bayes belief networks [14, 9], decision trees [5, 9, 11], rule learning algorithms [1, 15, 12], neural networks <ref> [15] </ref> and inductive learning techniques [2, 8]. With more and more methods available, cross-method evaluation becomes increasingly important. However, without an unified methodology of empirical validation, an objective comparison is difficult. The most serious problem is the lack of standard data collections. <p> SWAP-1, an inductive learning algorithm for classification using rules in Disjunctive Normal Form (DNF)[1]. 5. A neural network approach (NNets) to classification <ref> [15] </ref>. 6. CHARADE, a DNF rule learning system for classification by I. Moulinier [12]. 7. RIPPER, a DNF rule learning system for classification by W. Cohen [2]. 8. <p> a bag of words), and the SMART system [13] is used as the search engine. 3 Collection Analysis 3.1 Two corpora The Reuters corpus, a collection of newswire stories from 1987 to 1991, is commonly used for text categorization research, starting from an early evaluation of the CONSTRUE expert system <ref> [6, 9, 1, 15, 12, 2] </ref> 1 . This collection is 1 A newly refined version named Reuters-21578 is available through Lewis' home page http://www.research.att.com/ ~ lewis. 2 split into training and test sets when used to evaluate various learning systems. <p> In both sets, a continuous chunk of documents (the early ones) are used for training, and the remaining chunk of documents (the later ones) are used for testing. The PARC set is drawn from the CONSTRUE set by eliminating the unlabelled documents and some rare categories <ref> [15] </ref>. Instead of taking continuous chunks of documents for training and testing, it uses a different partition. The collection is sliced into many subsets using non-overlapping time windows. The odd subsets are used for training, and the even subsets are used for testing. <p> Given a classifier, the values of r; p; e and f often depend on internal parameter tuning; there is a trade-off between recall and precision in general. A commonly used measure in method comparison <ref> [9, 1, 15, 12] </ref> is the break-even point (BrkEvn) of recall and precision, i.e., when r and p are tuned to be equal.
Reference: [16] <author> Y. Yang. </author> <title> Expert network: Effective and efficient learning from human decisions in text categorization and retrieval. </title> <booktitle> In 17th Ann Int ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'94), </booktitle> <pages> pages 13-22, </pages> <year> 1994. </year>
Reference-contexts: A single regression model is used for ranking multiple categories given a test document. The input variables in the model are unique terms (words or phrases) in the training documents, and the output variables are unique categories of the training documents. 13. kNN, a k-nearest neighbor classifier <ref> [16] </ref>. Given an arbitrary input document, the system ranks its nearest neighbors among training documents, and uses the categories of the k top-ranking neighbors to predict the categories of the input document.
Reference: [17] <author> Y. Yang. </author> <title> An evaluation of a statistical approaches to medline indexing. </title> <booktitle> In Proceedings of the 1996 Annual Full Symposium of the American Medical Informatics Association (1996 AMIA), </booktitle> <pages> pages 358-362, </pages> <year> 1996. </year>
Reference-contexts: The documents were manually indexed using subject categories (Medical Subject Headings, or MeSH; about 18,000 categories defined) in the National Library of Medicine. The OHSUMED collection has been used with the full range of categories (14,321 MeSH categories actually occurred) in some experiments <ref> [17] </ref>, or with a subset of categories in the heart disease sub-domain (HD, 119 categories) in other experiments [8]. 3.2 Different versions Table 1 lists the different versions or subsets of Reuters and OHSUMED. Each is referred as a "set" or "collection", and labelled for reference.
Reference: [18] <author> Y. Yang and C.G. Chute. </author> <title> A linear least squares fit mapping method for information retrieval from natural language texts. </title> <booktitle> In Proceedings of the 14th International Conference on Computational Linguistics (COLING 92), </booktitle> <pages> pages 447-453, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Text categorization is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models <ref> [5, 18] </ref>, nearest neighbor classifiers [3, 19], Bayes belief networks [14, 9], decision trees [5, 9, 11], rule learning algorithms [1, 15, 12], neural networks [15] and inductive learning techniques [2, 8]. With more and more methods available, cross-method evaluation becomes increasingly important. <p> The Widrow-Hoff (WH) inductive learning algorithm which approximates a least squares fit [8]. 11. Sleeping Experts (EXPERTS), an inductive learning system using n-gram phrases in classification [2]. 12. LLSF, a linear least squares fit (LLSF) approach to classification <ref> [18] </ref>. A single regression model is used for ranking multiple categories given a test document. The input variables in the model are unique terms (words or phrases) in the training documents, and the output variables are unique categories of the training documents. 13. kNN, a k-nearest neighbor classifier [16].
Reference: [19] <author> Y. Yang and C.G. Chute. </author> <title> An example-based mapping method for text categorization and retrieval. </title> <journal> ACM Transaction on Information Systems (TOIS), </journal> <pages> pages 253-277, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Text categorization is the problem of assigning predefined categories to free text documents. A growing number of statistical learning methods have been applied to this problem in recent years, including regression models [5, 18], nearest neighbor classifiers <ref> [3, 19] </ref>, Bayes belief networks [14, 9], decision trees [5, 9, 11], rule learning algorithms [1, 15, 12], neural networks [15] and inductive learning techniques [2, 8]. With more and more methods available, cross-method evaluation becomes increasingly important.
Reference: [20] <author> Y. Yang and J.P. Pedersen. </author> <title> Feature selection in statistical learning of text categorization. </title> <booktitle> In The Fourteenth International Conference on Machine Learning, page (to apprear), </booktitle> <year> 1997. </year> <month> 10 </month>
Reference-contexts: About 1-2% improvements in average precision and break-even point were observed in both kNN and LLSF when an 85% vocabulary reduction was applied. Several word selection criteria were tested, including information gain, mutual information, a 2 statistic and document frequency <ref> [20] </ref>. The best results (using the 2 statistic) were included in Table 3. Aggressive vocabulary reduction was not used in WORD because it would reduce the chance of word-based matching between documents and category names.
References-found: 20

