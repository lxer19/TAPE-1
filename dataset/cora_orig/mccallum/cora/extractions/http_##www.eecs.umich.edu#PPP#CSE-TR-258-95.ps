URL: http://www.eecs.umich.edu/PPP/CSE-TR-258-95.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Email: gabandah@eecs.umich.edu  
Title: Modeling the Communication and Computation Performance of the IBM SP2  
Author: Gheith A. Abandah Advisor: Edward S. Davidson 
Date: May 8, 1995  
Address: 1301 Beal Avenue Ann Arbor, MI 48109-2122  
Affiliation: Advanced Computer Architecture Laboratory Department of Electrical Engineering and Computer Science University of Michigan  
Pubnum: (313) 936-2917  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. H. Mangione-Smith, S. G. Abraham, and E. S. Davidson, </author> <title> "A performance comparison of the IBM RS/6000 and the Astronautics ZS-1," </title> <journal> IEEE Computer, </journal> <volume> vol. 24, </volume> <pages> pp. 39-46, </pages> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Although we have characterized the distance effect on the SP1 and the SP2 communication times, we do not include the distance in their communication models for simplicity. 1.3 Computation Modeling Our approach for modeling the computation performance of the processor nodes is through using a Machine-Application Performance Bound (MA Bound) <ref> [1, 2, 3, 4] </ref>. The MA Bound of an application is an upper bound for the performance that can be achieved for the application on a certain machine. The MA Bound is found by counting the essential operations in the application. <p> The returning data arrive at the DCU, which forwards the data to the ICU on the instruction reload bus. 5.3 Machine-Application Bound Model The Machine-Application (MA) Bound <ref> [1, 2, 3, 4] </ref> gives an upper bound on the performance that can be achieved on a certain machine for a certain application. The measured 46 CHAPTER 5.
Reference: [2] <author> W. H. Mangione-Smith, T. P. Shih, S. G. Abraham, and E. S. Davidson, </author> <title> "Approaching a machine-application bound in delivered performance on scientific code," </title> <journal> IEEE Proceedings, </journal> <volume> vol. 81, </volume> <pages> pp. 1166-1178, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: Although we have characterized the distance effect on the SP1 and the SP2 communication times, we do not include the distance in their communication models for simplicity. 1.3 Computation Modeling Our approach for modeling the computation performance of the processor nodes is through using a Machine-Application Performance Bound (MA Bound) <ref> [1, 2, 3, 4] </ref>. The MA Bound of an application is an upper bound for the performance that can be achieved for the application on a certain machine. The MA Bound is found by counting the essential operations in the application. <p> The returning data arrive at the DCU, which forwards the data to the ICU on the instruction reload bus. 5.3 Machine-Application Bound Model The Machine-Application (MA) Bound <ref> [1, 2, 3, 4] </ref> gives an upper bound on the performance that can be achieved on a certain machine for a certain application. The measured 46 CHAPTER 5.
Reference: [3] <author> E. L. Boyd, W. Azeem, H. H. Lee, T. P. Shih, S. H. Hung, and E. S. Davidson, </author> <title> "A hierarchical approach to modeling and improving the performance of scientific applications on the KSR1," </title> <booktitle> in Inter. Conf. on Parallel Processing, </booktitle> <volume> vol. 3, </volume> <pages> pp. 188-192, </pages> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: Although we have characterized the distance effect on the SP1 and the SP2 communication times, we do not include the distance in their communication models for simplicity. 1.3 Computation Modeling Our approach for modeling the computation performance of the processor nodes is through using a Machine-Application Performance Bound (MA Bound) <ref> [1, 2, 3, 4] </ref>. The MA Bound of an application is an upper bound for the performance that can be achieved for the application on a certain machine. The MA Bound is found by counting the essential operations in the application. <p> The returning data arrive at the DCU, which forwards the data to the ICU on the instruction reload bus. 5.3 Machine-Application Bound Model The Machine-Application (MA) Bound <ref> [1, 2, 3, 4] </ref> gives an upper bound on the performance that can be achieved on a certain machine for a certain application. The measured 46 CHAPTER 5.
Reference: [4] <author> E. L. Boyd, G. A. Abandah, H.-H. Lee, and E. S. Davidson, </author> <title> "Modeling computation and communication performance of parallel scientific applications: A case study of the IBM SP2," in Suprcomputing, </title> <type> (Draft), </type> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Although we have characterized the distance effect on the SP1 and the SP2 communication times, we do not include the distance in their communication models for simplicity. 1.3 Computation Modeling Our approach for modeling the computation performance of the processor nodes is through using a Machine-Application Performance Bound (MA Bound) <ref> [1, 2, 3, 4] </ref>. The MA Bound of an application is an upper bound for the performance that can be achieved for the application on a certain machine. The MA Bound is found by counting the essential operations in the application. <p> The two systems have similar Synchronization time. These models are useful for programmers and engineers involved in writing high performance message-passing applications. We have successfully used these models to analyze and tune a Finite Element Application <ref> [4] </ref>. And we have used them to study the performance of six broadcast algorithms (see chapter 3). These models can also be used to do comparisons between MPP's. <p> The returning data arrive at the DCU, which forwards the data to the ICU on the instruction reload bus. 5.3 Machine-Application Bound Model The Machine-Application (MA) Bound <ref> [1, 2, 3, 4] </ref> gives an upper bound on the performance that can be achieved on a certain machine for a certain application. The measured 46 CHAPTER 5. <p> This model gives an upper bound on the performance that can be achieved for a certain application. This section tries to answer the question of how much of this bound is achievable. This is done by a case study of a commercial application (FEMC) <ref> [4] </ref> that uses the finite element method, and of the Livermore kernels [19]. 5.5.1 Finite Element Method Application (FEMC) Some FEMC key routines were studied using the performance models developed in this chapter, the results of the analysis for two routines are presented here. <p> And we have discussed how this model can guide the effort of tuning the computational performance of an application. 59 60 CHAPTER 6. CONCLUSIONS We have applied the computation and communication performance models to some of the FEMC routines <ref> [4] </ref>, and we have seen that the modeled time ranges between 25% and 80% of the measured time.
Reference: [5] <author> D. Baily et al., </author> <title> "The NAS parallel benchmarks," </title> <type> Tech. Rep. </type> <institution> RNR-94-07, NASA Ames Research Center, Mail Stop T27A-1, Moffett Field, </institution> <address> CA 94035-1000, </address> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: The available models either do not cover all the important communication patterns, or do not give information that is directly usable by a programmer tuning a message-passing application. Most of the work in developing performance models for message passing multi-computers is centered around benchmarks. The NAS Parallel Benchmarks (NPB) <ref> [5] </ref> are developed to study the performance of parallel supercomputers. These benchmarks consist of five parallel kernels and three simulated application benchmarks. Together they mimic the computation and data movement characteristics of large scale computational fluid dynamics applications.
Reference: [6] <author> R. Hockney et al., </author> <title> "Public international benchmarks for parallel computers," </title> <type> Tech. Rep. 1, </type> <institution> PARKBENCH Committee, </institution> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: The NAS Parallel Benchmarks (NPB) [5] are developed to study the performance of parallel supercomputers. These benchmarks consist of five parallel kernels and three simulated application benchmarks. Together they mimic the computation and data movement characteristics of large scale computational fluid dynamics applications. Another benchmark suite is the PARKBENCH <ref> [6] </ref>, this suite contains low level benchmarks for measuring basic computer characteristics, kernel benchmarks to test typical scientific subroutines, and compact applications to test complete problems. Benchmarks like NPB are useful for comparing different machines, but they do not separate computation performance from communication performance.
Reference: [7] <author> C. B. Stunkel et al., </author> <title> "The SP2 communication subsystem," </title> <type> tech. rep., </type> <institution> IBM Thomas J. Watson Research Center, </institution> <address> P.O. Box 218, Yorktown Heights, NY 10598, </address> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: Finally, section 2.11 states the conclusions of this chapter. 2.2 IBM SP1 and SP2 The IBM Scalable POWERparallel Systems SP1 and SP2 connect RISC System/6000 processors via the communication subsystem <ref> [7] </ref>. This subsystem is based upon a low latency high bandwidth switching network called the High Performance Switch. The SP1 systems offer switch connectivity from 8 to 64 POWER nodes, the SP2 systems offer switch connectivity from 4 to 128 POWER or POWER2 nodes [8].
Reference: [8] <author> S. W. White and S. Dhawan, "POWER2: </author> <title> next generation of the RISC System/6000 family," </title> <journal> IBM J. Research and Development, </journal> <volume> vol. 38, </volume> <pages> pp. 493-502, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: This subsystem is based upon a low latency high bandwidth switching network called the High Performance Switch. The SP1 systems offer switch connectivity from 8 to 64 POWER nodes, the SP2 systems offer switch connectivity from 4 to 128 POWER or POWER2 nodes <ref> [8] </ref>. The SP1/SP2 networks are bidirectional multistage interconnection networks (MIN's). In a bidirectional MIN each communication link comprises two channels which carry data in opposite directions. MIN's are capable of scaling bisection bandwidth linearly with the number of nodes while maintaining a fixed number of communication ports per switching element. <p> Section 5.7 develops a methodology for performance improvement. Conclusions drawn from this study are presented in section 5.8. 5.2 The IBM POWER2 RISC System/6000 The IBM POWER2 RISC System/6000 processor <ref> [8] </ref> is the second generation of IBM's implementation of the POWER Instruction Set Architecture, it was introduced in late 1993. The first generation, the POWER processor, was introduced in 1990.
Reference: [9] <editor> C. B. Stunkel et al., </editor> <booktitle> "Architecture and implementation of Vulcan," in The 8th International Parallel Processing Symposium, </booktitle> <address> (Cancun, Mexico), </address> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: As soon as the output port is free, packet transfer resumes. An SP1/SP2 system is composed of frames containing up to 16 processors and one switch board assembly that implements the two stage network building block. The switching element of the board is the Vulcan switch chip <ref> [9] </ref>. The Switch operates at 40 MHz, providing 2.2. IBM SP1 AND SP2 7 peak bandwidth of 40 megabytes per second over both byte-wide channels of each communication link. The Vulcan switch chip contains 8 receiver modules and 8 transmitter modules, an unbuffered crossbar, and the central queue.
Reference: [10] <author> IBM, </author> <title> IBM AIX Parallel Environment, Parallel Programming Subroutine Reference, </title> <editor> 2.0 ed., </editor> <month> June </month> <year> 1994. </year>
Reference-contexts: The SP2 system used for these experiments has 32 POWER2 Thin nodes operating at 66.7 MHz, with 32 KB instruction cache and 64 KB data cache. The IBM SP2 supports three message passing libraries, MPL <ref> [10] </ref>, PMVe [11], and MPI. PVMe is IBM's implementation of the popular PVM [12] on SP2, the current version is 3.2.6. The IBM PVMe is compatible with PVM, but its internal structure is different. The IBM PVMe does not interface directly with the TCP/IP to perform data communication between processors.
Reference: [11] <author> IBM, </author> <title> IBM AIX PVMe User's Guide and Subroutine Reference, </title> <editor> 1.0 ed., </editor> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: The SP2 system used for these experiments has 32 POWER2 Thin nodes operating at 66.7 MHz, with 32 KB instruction cache and 64 KB data cache. The IBM SP2 supports three message passing libraries, MPL [10], PMVe <ref> [11] </ref>, and MPI. PVMe is IBM's implementation of the popular PVM [12] on SP2, the current version is 3.2.6. The IBM PVMe is compatible with PVM, but its internal structure is different. The IBM PVMe does not interface directly with the TCP/IP to perform data communication between processors.
Reference: [12] <author> A. Geist et al., </author> <title> PVM 3 User's Guide and Reference Manual. </title> <institution> Oak Ridge National Laboratory, Oak Ridge, Tennessee 37831, </institution> <month> Sept. </month> <year> 1994. </year> <note> ORNL/TM-12187. 61 62 BIBLIOGRAPHY </note>
Reference-contexts: The SP2 system used for these experiments has 32 POWER2 Thin nodes operating at 66.7 MHz, with 32 KB instruction cache and 64 KB data cache. The IBM SP2 supports three message passing libraries, MPL [10], PMVe [11], and MPI. PVMe is IBM's implementation of the popular PVM <ref> [12] </ref> on SP2, the current version is 3.2.6. The IBM PVMe is compatible with PVM, but its internal structure is different. The IBM PVMe does not interface directly with the TCP/IP to perform data communication between processors. <p> Chapter 4 PVM Comparison 4.1 Introduction PVM is a popular message-passing library, it is available on many multicomputer systems. PVM stands for Parallel Virtual Machine <ref> [12] </ref>. It is a software package that allows heterogeneous network of parallel and serial computers to appear as a single concurrent computational resource (a virtual machine).
Reference: [13] <author> R. Hockney, </author> <title> "Performance parameters and benchmarking of supercomputers," </title> <journal> Parallel Computing, </journal> <volume> vol. 17, </volume> <pages> pp. 1111-1130, </pages> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: The discontinuities are at 217 + 232i Bytes; where i 2 f0; 1; 2; : : :g The Point-to-point communication time often can be expressed by a few simple parameters <ref> [13] </ref>; r 1 is the asymptotic transfer rate in MBytes/sec, and t 0 is the asymptotic zero message length latency in microseconds.
Reference: [14] <institution> Convex Computer Corporation, </institution> <address> P.O. Box 833851, Richardson, TX 75083-3851, </address> <note> Convex Exemplar Programming Guide, </note> <editor> x3.0.0.2 ed., </editor> <month> June </month> <year> 1994. </year>
Reference-contexts: These models can also be used to do comparisons between MPP's. We were able to conclude interesting results (see chapter 4) by developing performance models for the PVM message-passing library on the IBM SP2 and the Convex SPP-1000 <ref> [14] </ref>. 20 CHAPTER 2. COMMUNICATION MODELING Chapter 3 Broadcast Algorithms 3.1 Introduction In message-passing applications, collective communication patterns can be time consuming. In this chapter we present a study for six broadcast algorithms implemented on the IBM SP2. <p> Finally, section 4.9 states the conclusions of this chapter. 31 32 CHAPTER 4. PVM COMPARISON 4.2 Convex SPP-1000 An SPP-1000 system (also called the Convex Exemplar) consists of 1 to 16 Hypernodes <ref> [14] </ref>. Each hypernode contains 4 Functional Blocks, each functional block contains 1 or 2 HP PA-RISC 7100 processors, memory, and some control devices, see figures 4.1 and 4.2. The functional blocks communicate across the hypernodes via four CTI (Convex Toroidal Interconnect) rings.
Reference: [15] <author> Q. F. Stout and B. Wagar, </author> <title> "Intensive hypercube communication," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 10, </volume> <pages> pp. 167-181, </pages> <year> 1990. </year>
Reference-contexts: BROADCAST ALGORITHMS approximated by the following equation:- t 1m (n; p) = T 1m (n; p 2) + T pp (n) 3.4 Recursive Doubling Broadcast (rd) This algorithm was introduced for hypercube multi-computers <ref> [15] </ref>, it has dlog 2 pe stages, in stage i processors 0; 1; : : : ; 2 i 1 send to processors 2 i ; 2 i + 1; : : : ; 2 i+1 1.
Reference: [16] <author> J. I. Barreh, R. T. Golla, L. B. Arimilli, and P. J. Jordan, </author> <title> "POWER2 instruction cache unit," </title> <journal> IBM J. Research and Development, </journal> <volume> vol. 38, </volume> <pages> pp. 537-544, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: The POWER2 processor philosophy of achieving higher performance is through using more powerful instructions and wider issue, while giving the clock rate a secondary degree of importance. chip module that contains 8 chips, partitioned in the following units:- 5.3. MACHINE-APPLICATION BOUND MODEL 45 * Instruction Cache Unit (ICU) <ref> [16] </ref>, this unit contains the instruction cache, it prefetches the instructions form the cache and places them in the instruction buffers. The ICU control logic decodes the instructions in the buffers, executes the branch and condition-register instructions and dispatches the remaining instructions to the FXU and FPU.
Reference: [17] <author> D. J. Shippy and T. W. Griffith, </author> <title> "POWER2 fixed-point, data cache, and storage control units," </title> <journal> IBM J. Research and Development, </journal> <volume> vol. 38, </volume> <pages> pp. 503-524, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: The ICU can fetch eight instructions per cycle form the I-cache. It can dispatch six instructions per cycle: two internally and four externally to the FXU and FPU. * Fixed-Point Unit (FXU) <ref> [17] </ref>, performs all storage references, integer arithmetic, and logical operations. The FXU contains the general-purpose registers, two fixed-point execution units, the data cache directory, and the data TLB. <p> Thin nodes has dual double-word interface and can load up to 2 FPRs per cycle. * Data Cache Unit (DCU) <ref> [17] </ref>, is a four-way set-associative dual-ported D-cache that consists of four identical chips. <p> cache chips generate two single-word data buses to the FXU, two quad-word buses to the FPU (two double-word buses in the Thin node), a 4-word instruction reload bus to the ICU, and a 2-word System I/O (SIO) bus to the I/O subsystem for DMA data. * Storage Control Unit (SCU) <ref> [17] </ref>, handles the main memory references for I-cache and D-cache misses. When a data cache miss occurs, the FXU arbitrates for the processor bus (PBUS).
Reference: [18] <author> T. N. Hicks, R. E. Fry, and P. E. Harvey, </author> <title> "POWER2 floating-point unit: Architecture and implementation," </title> <journal> IBM J. Research and Development, </journal> <volume> vol. 38, </volume> <pages> pp. 525-536, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: The FXU contains the general-purpose registers, two fixed-point execution units, the data cache directory, and the data TLB. The two execution units enable the FXU to execute a total of two instructions per cycle. * Floating-Point Unit (FPU) <ref> [18] </ref>, includes the floating-point registers (FPRs) and two double-precision execution units. The two units allow it to execute two floating-point instructions per cycle. The FPU supports the compound multiply-add instruction, where an execution unit performs two operations with the same latency as a single multiply or add instruction.
Reference: [19] <author> F. H. McMahon, </author> <title> "The Livermore Fortran kernels: A computer test of the numerical performance range," </title> <type> Tech. Rep. </type> <institution> UCRL-53745, Lawrence Livermore Nat. Lab., Livermore, </institution> <address> CA, </address> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: Divide and square root operations are weighted by a factor of four to reflect their complexity with respect to adds and multiplies (this is a common practice in scientific benchmarks <ref> [19] </ref>). CPF = t l =(f a + f m + 2f ma + 4f div + 4f sqrt ) The number of processor cycles, t l , equals the time of the slowest (busiest) functional unit. <p> This section tries to answer the question of how much of this bound is achievable. This is done by a case study of a commercial application (FEMC) [4] that uses the finite element method, and of the Livermore kernels <ref> [19] </ref>. 5.5.1 Finite Element Method Application (FEMC) Some FEMC key routines were studied using the performance models developed in this chapter, the results of the analysis for two routines are presented here.
Reference: [20] <author> K. Gallivan, D. Gannon, W. Jalby, A. Malony, and H. Wijshoff, </author> <title> "Experimentally characterizing the behavior of multiprocessor memory systems: A case study," </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> vol. 16, </volume> <pages> pp. 216-223, </pages> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: See subsection 5.4.1 for more details on these regions, and see subsection 5.4.2 for more details on the estimation of the cache misses. 5.4.1 Finding the Memory Access Parameters This subsection describes the experiments <ref> [20] </ref> used to characterize the behavior of the POWER2 memory system. Two experiments were done, the results are shown in figures 5.2 and 5.3. These figures show the time per access operation in clock cycles.
Reference: [21] <author> R. C. Agarwal, F. G. Gustavson, and M. Zubair, </author> <title> "Exploiting functional parallelism of POWER2 to design high-performance numerical algorithms," </title> <journal> IBM J. Research and Development, </journal> <volume> vol. 38, </volume> <pages> pp. 563-576, </pages> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: It yields codes that make better 5.7. WHAT CAN WE DO TO MAKE IT HIGHER? 55 exploitation of the POWER2 functional parallelism, and it reduces or eliminates the effect of control overheads. Algorithmic Prefetching <ref> [21] </ref> This technique can be applied in POWER2 processor because it has two cache ports. This technique is useful in hiding load miss latency.
Reference: [22] <author> E. Anderson et al., </author> <title> "MPI: a message-passing interface standard," </title> <type> tech. rep., Message Passing Interface Forum, </type> <month> May </month> <year> 1994. </year>
Reference-contexts: We plan to enhance our models to include the effects of load imbalance, and enhance our techniques for estimating the number of essential cache misses. The MPI <ref> [22] </ref> message-passing library is widely accepted as the standard library for writing message-passing applications. It is becoming available for many multi-computer systems, and is becoming regarded as the message-passing library of choice for writing portable applications.
References-found: 22

