URL: http://www.cs.washington.edu/research/smt/papers/tlp2ilp.public.ps
Refering-URL: http://www.cs.washington.edu/homes/eggers/Research/multithread.html
Root-URL: 
Email: -jlo,eggers,levy-@cs.washington.edu  -emer,stamm-@vssad.enet.dec.com  tullsen@cs.ucsd.edu  
Title: Converting Thread-Level Parallelism to Instruction-Level Parallelism via Simultaneous Multithreading  
Author: Jack L. Lo, Susan J. Eggers, Joel S. Emer Henry M. Levy, Rebecca L. Stamm and Dean M. Tullsen 
Address: Box 352350  Seattle, WA 98195-2350  HLO2-3/J3 77 Reed Road Hudson, MA 01749  9500 Gilman Drive  La Jolla, CA 92093-0114  
Affiliation: Dept. of Computer Science and Engineering  University of Washington  Digital Equipment Corporation  Dept. of Computer Science and Engineering  University of California, San Diego  
Abstract: A version of this paper will appear in ACM Transactions on Computer Systems, August 1997. Permission to make digital copies of part or all of this work for personal or classroom use is grantedwithout fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Abstract To achieve high performance, contemporary computer systems rely on two forms of parallelism: instruction-level parallelism (ILP) and thread-level parallelism (TLP). Wide-issue superscalar processors exploit ILP by executing multiple instructions from a single program in a single cycle. Multiprocessors (MP) exploit TLP by executing different threads in parallel on different processors. Unfortunately, both parallel-processing styles statically partition processor resources, thus preventing them from adapting to dynamically-changing levels of ILP and TLP in a program. With insufficient TLP, processors in an MP will be idle; with insufficient ILP, multiple-issue hardware on a superscalar is wasted. This paper explores parallel processing on an alternative architecture, simultaneous multithreading (SMT), which allows multiple threads to compete for and share all of the processors resources every cycle. The most compelling reason for running parallel applications on an SMT processor is its ability to use thread-level parallelism and instruction-level parallelism interchangeably. By permitting multiple threads to share the processors functional units simultaneously, the processor can use both ILP and TLP to accommodate variations in parallelism. When a program has only a single thread, all of the SMT processors resources can be dedicated to that thread; when more TLP exists, this parallelism can compensate for a lack of 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal. </author> <title> Performance tradeoffs in multithreaded processors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(5):525539, </volume> <month> September </month> <year> 1992. </year>
Reference-contexts: For unop-timized programs, however, misses were more significant and resulted in performance degradations. Weber and Gupta [35] also studied the effects of cache interference in conventional multithreaded architectures, and found increases in inter-thread misses that are comparable to our results. Agarwal <ref> [1] </ref> and Saavedra-Barrera, et al., [25] used analytic models for studying the efficiency of multithreaded processors. Both models included factors for cache interference which they correlated with the results obtained by Weber and Gupta.
Reference: [2] <author> C. Beckmann and C. Polychronopoulos. </author> <title> Microarchitecture support for dynamic scheduling of acyclic task graphs. </title> <booktitle> In 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 140148, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Their simulations do not include caches or TLBs. Prasadh and Wu [23], as well as Keckler and Dally [15], have proposed architectures in which VLIW operations from multiple threads are dynamically interleaved onto a processor. The architectures described by Govindarajan, et al. [11], Gunther [13], and Beckmann and Polychronopoulos <ref> [2] </ref> partition issue bandwidth among threads, and only one instruction can be issued from each thread per cycle. These architectures lack flexible resource sharing, which contributes to resource waste when only a single thread is running.
Reference: [3] <author> J. Boyle, et al. </author> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart, and Winston, Inc., </publisher> <year> 1987. </year>
Reference-contexts: 2: Benchmark suite 9 resource partitioning, a multiprocessor typically cannot get good single-thread performance; SMT can, by taking better advantage of ILP, even in sequential sections. 3 Five of our benchmarks are explicitly-parallel programs from the SPLASH-2 suite [38], which are built on the Argonne National Laboratories parallel macro library <ref> [3] </ref>. Tomcatv and hydro2d from SPEC92 [7], as well as shallow and linpack, are implicitly-parallel programs for which we use the SUIF compiler [37] to extract loop-level parallelism.
Reference: [4] <author> B. Calder and D. Grunwald. </author> <title> Fast and accurate instruction fetch and branch prediction. </title> <booktitle> In 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 211, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: These values are the minimum latencies from when the source operands are ready to when the result becomes ready for a dependent instruction. 10 prediction, we use a 256-entry, 4-way set associative branch target buffer and a 2K x 2-bit pattern history table <ref> [4] </ref>. These structures are shared by all running threads (even if less than 8 are executing), allowing more exible and therefore higher utilization. Most importantly, these structures are fully available even if only a single thread is executing.
Reference: [5] <institution> IBM Corp. RISC System/6000 model J50. </institution> <note> http://www.rs6000.ibm.com/hardware/entrprise/j50.html. </note>
Reference: [6] <author> G. Daddis, Jr. and H. Torng. </author> <title> The concurrent execution of multiple instruction streams on superscalar processors. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages I:7683, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: These architectures lack flexible resource sharing, which contributes to resource waste when only a single thread is running. Studies by Daddis and Torng <ref> [6] </ref>, Prasadh and Wu [23], and Yamamoto, et al. [40][39], as well as our previous work [31][30], also examined simultaneous multithreading architectures, but looked at multiprogrammed workloads, rather than parallel applications. The simultaneous multithreading in the study by Li and Chu [16] was based on an analytic model of performance.
Reference: [7] <author> K. Dixit. </author> <title> New CPU benchmark suites from SPEC. </title> <booktitle> In COMPCON 92 digest of papers, </booktitle> <pages> pages 305310, </pages> <year> 1992. </year>
Reference-contexts: Tomcatv and hydro2d from SPEC92 <ref> [7] </ref>, as well as shallow and linpack, are implicitly-parallel programs for which we use the SUIF compiler [37] to extract loop-level parallelism. SUIF generates transformed C output files that contain calls to a parallel runtime library to create threads and execute loop iterations in parallel.
Reference: [8] <author> J. Edmondson and P. Rubinfield. </author> <title> An overview of the 21164 AXP microprocessor. </title> <booktitle> In Hot Chips VI, </booktitle> <pages> pages 18, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: For both the SMT and MP architectures, the simulator takes unmodified Alpha executables and uses emulation-based, instruction-level simulation to model the processor pipelines, the TLBs, and the entire memory hierarchy. The instruction latencies for the functional units are similar to those of the DEC Alpha 21164 <ref> [8] </ref> and are listed in Table 3. The memory hierarchy in our processor consists of three levels of cache, with sizes, latencies, and bandwidth characteristics as shown in Table 4.
Reference: [9] <author> M. Fillo, S. Keckler, W. Dally, N. Carter, A. Chang, Y. Gurevich, and W. Lee. </author> <title> The M-Machine multicomputer. </title> <booktitle> In 28th 24 Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 146156, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: The simultaneous multithreading in the study by Li and Chu [16] was based on an analytic model of performance. Several other architectures have been designed to exploit multiple levels of program parallelism. The M-Machine <ref> [9] </ref>, MISC [33][32], and Multiscalar [28] architectures all require significant compiler support (or hand-coding) to extract maximum performance. In all three designs, execution resources are partitioned in a manner that prevents them from being dynamically shared by all threads.
Reference: [10] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5:587616, </volume> <year> 1988. </year>
Reference: [11] <author> R. Govindarajan, S. Nemawarkar, and P. LeNir. </author> <title> Design and performance evaluation of a multithreaded architecture. </title> <booktitle> In First IEEE Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 298307, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Their simulations do not include caches or TLBs. Prasadh and Wu [23], as well as Keckler and Dally [15], have proposed architectures in which VLIW operations from multiple threads are dynamically interleaved onto a processor. The architectures described by Govindarajan, et al. <ref> [11] </ref>, Gunther [13], and Beckmann and Polychronopoulos [2] partition issue bandwidth among threads, and only one instruction can be issued from each thread per cycle. These architectures lack flexible resource sharing, which contributes to resource waste when only a single thread is running.
Reference: [12] <author> M. Gulati and N. Bagherzadeh. </author> <title> Performance study of a multithreaded superscalar microprocessor. </title> <booktitle> In Second International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 291301, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: In the parallel applications used in this study, all threads work together toward completing the entire program, so speedup becomes the critical metric. Other studies Several variants of simultaneous multithreading have been studied. Gulati and Bagherzadeh <ref> [12] </ref> implemented simultaneous multithreading as an extension to a superscalar processor and measured speedups for a set of parallelized programs. In contrast to our processor model, their base processor was a 4-issue machine with fewer functional units, which limited the speedups they obtained when using additional threads.
Reference: [13] <author> B. Gunther. </author> <title> Superscalar performance in a multithreaded microprocessor. </title> <type> Ph.D. thesis, </type> <institution> University of Tasmania, </institution> <month> Decem-ber </month> <year> 1993. </year>
Reference-contexts: Their simulations do not include caches or TLBs. Prasadh and Wu [23], as well as Keckler and Dally [15], have proposed architectures in which VLIW operations from multiple threads are dynamically interleaved onto a processor. The architectures described by Govindarajan, et al. [11], Gunther <ref> [13] </ref>, and Beckmann and Polychronopoulos [2] partition issue bandwidth among threads, and only one instruction can be issued from each thread per cycle. These architectures lack flexible resource sharing, which contributes to resource waste when only a single thread is running.
Reference: [14] <author> H. Hirata, K. Kimura, S. Nagamine, Y. Mochizuki, A. Nishimura, Y. Nakase, and T. Nishizawa. </author> <title> An elementary processor architecture with simultaneous instruction issuing from multiple threads. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 136145, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In contrast to our processor model, their base processor was a 4-issue machine with fewer functional units, which limited the speedups they obtained when using additional threads. Hirata, et al., <ref> [14] </ref> proposed an architecture that dynamically packs instructions from different streams. They evaluated the performance benefits of their architecture by parallelizing a ray-tracing application. Their simulations do not include caches or TLBs.
Reference: [15] <author> S. W. Keckler and W. J. Dally. </author> <title> Processor coupling: Integrating compile time and runtime scheduling for parallelism. </title> <booktitle> In 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 202213, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Hirata, et al., [14] proposed an architecture that dynamically packs instructions from different streams. They evaluated the performance benefits of their architecture by parallelizing a ray-tracing application. Their simulations do not include caches or TLBs. Prasadh and Wu [23], as well as Keckler and Dally <ref> [15] </ref>, have proposed architectures in which VLIW operations from multiple threads are dynamically interleaved onto a processor. The architectures described by Govindarajan, et al. [11], Gunther [13], and Beckmann and Polychronopoulos [2] partition issue bandwidth among threads, and only one instruction can be issued from each thread per cycle.
Reference: [16] <author> Y. Li and W. Chu. </author> <title> The effects of STEF in finely parallel multithreaded processors. </title> <booktitle> In First IEEE Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 318325, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Studies by Daddis and Torng [6], Prasadh and Wu [23], and Yamamoto, et al. [40][39], as well as our previous work [31][30], also examined simultaneous multithreading architectures, but looked at multiprogrammed workloads, rather than parallel applications. The simultaneous multithreading in the study by Li and Chu <ref> [16] </ref> was based on an analytic model of performance. Several other architectures have been designed to exploit multiple levels of program parallelism. The M-Machine [9], MISC [33][32], and Multiscalar [28] architectures all require significant compiler support (or hand-coding) to extract maximum performance.
Reference: [17] <author> P. Lowney, S. Freudenberger, T. Karzes, W. Lichtenstein, R. Nix, J. ODonnell, and J. Ruttenberg. </author> <title> The Multiflow trace scheduling compiler. </title> <journal> Journal of Supercomputing, </journal> <volume> 7(1-2):51142, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: In each application, all threads share the same address space, but each thread has its own private data and stack, which are stored in a distinct location in the address space. For all programs in our workload, we use the Multiflow trace scheduling compiler <ref> [17] </ref> to generate DEC Alpha object files. The compiler generates highly-optimized code using aggressive static scheduling, loop unrolling, and other ILP-exposing optimizations, so that single-thread performance is maximized.
Reference: [18] <author> B. Nayfeh and K. Olukotun. </author> <title> Exploring the design space for a shared-cache multiprocessor. </title> <booktitle> In 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 166175, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Both models included factors for cache interference which they correlated with the results obtained by Weber and Gupta. None of the studies included the effects of bank contention in their results, since they did not use multiple banks. Nayfeh and Olukotun <ref> [18] </ref> investigated the benefits of a shared cluster cache on a single-chip multiprocessor. They found that inter-thread interference can cause degradation for multiprogrammed workloads. For parallel applications, a shared cache could actually obtain superlinear speedups in some cases because of prefetching effects.
Reference: [19] <author> B. A. Nayfeh, L. Hammond, and K. Olukotun. </author> <title> Evaluation of design alternatives for a multiprocessor microprocessor. </title> <booktitle> In 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 6777, </pages> <month> May </month> <year> 1996. </year>
Reference: [20] <author> K. Olukotun, B. A. Nayfeh, L. Hammond, K. Wilson, and K. Chang. </author> <title> The case for a single-chip multiprocessor. </title> <booktitle> In Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2 11, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: As chip densities increase, single-chip multiprocessing will be possible, and some architects have already begun to investigate this use of chip real estate <ref> [20] </ref>. An SMT processor and a small-scale, on-chip multiprocessor have many similarities: e.g., both have large numbers of registers and functional units, on-chip caches, and the ability to issue multiple instructions each cycle. <p> A primary goal was to expose scheduling and allocation to the compiler. On their machine intrathread parallelism was avoided; parallelism was instead expressed only between threads. Olukotun, et al., <ref> [20] </ref> advocates the design of single-chip multiprocessors, instead of wider-issue superscalars, to exploit thread-level parallelism. Cache effects of multithreading has been the subject of several studies. Yamamoto, et al., and Gulati and Bagherzadeh both found that the cache miss rates in simultaneous multithreading processors increased when more threads were used.
Reference: [21] <author> S. Palacharla, N. P. Jouppi, and J. E. Smith. </author> <title> Complexity-effective superscalar processors. </title> <booktitle> In 24th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: The additional pipe stage requires an extra level of bypass logic, but the number of stages has a smaller impact on the complexity and delay of this logic (O (n)) than the issue width (O (n 2 )) <ref> [21] </ref>. Our previous study [30] contains more details regarding the effects of the two-stage register read/write pipelines on the architecture and performance.
Reference: [22] <author> A. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> Ph.D. thesis, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference: [23] <author> R. Prasadh and C.-L. Wu. </author> <title> A benchmark evaluation of a multi-threaded RISC processor architecture. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages I:8491, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Hirata, et al., [14] proposed an architecture that dynamically packs instructions from different streams. They evaluated the performance benefits of their architecture by parallelizing a ray-tracing application. Their simulations do not include caches or TLBs. Prasadh and Wu <ref> [23] </ref>, as well as Keckler and Dally [15], have proposed architectures in which VLIW operations from multiple threads are dynamically interleaved onto a processor. <p> These architectures lack flexible resource sharing, which contributes to resource waste when only a single thread is running. Studies by Daddis and Torng [6], Prasadh and Wu <ref> [23] </ref>, and Yamamoto, et al. [40][39], as well as our previous work [31][30], also examined simultaneous multithreading architectures, but looked at multiprogrammed workloads, rather than parallel applications. The simultaneous multithreading in the study by Li and Chu [16] was based on an analytic model of performance.
Reference: [24] <author> R.Thekkath and S. Eggers. </author> <title> The effectiveness of multiple hardware contexts. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 328337, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: In addition, our study somewhat overstates the amount of inter-thread interference, because we have not applied compiler optimizations (such as cache tiling [10][22]) to minimize interference by reducing the size of the working sets. Thekkath and Eggers <ref> [24] </ref> found that for traditional multithreaded architectures, programmer or compiler-based locality optimizations can significantly reduce inter-thread interference. <p> Cache effects of multithreading has been the subject of several studies. Yamamoto, et al., and Gulati and Bagherzadeh both found that the cache miss rates in simultaneous multithreading processors increased when more threads were used. Neither quantified the direct effect from inter-thread interference, however. Thekkath and Eggers <ref> [24] </ref> examined the effectiveness of multiple contexts on conventional, coarse-grained multi-threaded architectures. They found that cache interference between threads varied depending on the benchmark. For locality-optimized programs, the total number of misses remained fairly constant as the number of contexts was increased.
Reference: [25] <author> R. H. Saavedra-Barrera, D. E. Culler, and T. von Eicken. </author> <title> Analysis of multithreaded architectures for parallel computing. </title> <booktitle> In 2nd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 169178, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: For unop-timized programs, however, misses were more significant and resulted in performance degradations. Weber and Gupta [35] also studied the effects of cache interference in conventional multithreaded architectures, and found increases in inter-thread misses that are comparable to our results. Agarwal [1] and Saavedra-Barrera, et al., <ref> [25] </ref> used analytic models for studying the efficiency of multithreaded processors. Both models included factors for cache interference which they correlated with the results obtained by Weber and Gupta. None of the studies included the effects of bank contention in their results, since they did not use multiple banks.
Reference: [26] <institution> Silicon Graphics, Inc. </institution> <note> The Onyx system family. http://www.sgi.com/Products/hardware/Onyx/Products/sys_lineup.ht-ml. </note>
Reference: [27] <author> M. Slater. </author> <title> SuperSPARC premiers in SPARCstation 10. </title> <type> Microprocessor Report, </type> <pages> pages 1113, </pages> <month> May </month> <year> 1992. </year>
Reference: [28] <author> G. S. Sohi, S. E. Breach, and T. Vijaykumar. </author> <title> Multiscalar processors. </title> <booktitle> In 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 414425, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The simultaneous multithreading in the study by Li and Chu [16] was based on an analytic model of performance. Several other architectures have been designed to exploit multiple levels of program parallelism. The M-Machine [9], MISC [33][32], and Multiscalar <ref> [28] </ref> architectures all require significant compiler support (or hand-coding) to extract maximum performance. In all three designs, execution resources are partitioned in a manner that prevents them from being dynamically shared by all threads.
Reference: [29] <author> Sun Microsystems, Inc. </author> <title> Ultra HPC Series Overview. </title> <address> http://www.sun.com/hpc/products/index.html. </address>
Reference: [30] <author> D. M. Tullsen, S. J. Eggers, J. S. Emer, H. M. Levy, J. L. Lo, and R. L. Stamm. </author> <title> Exploiting choice: Instruction fetch and issue on an implementable simultaneous multithreading processor. </title> <booktitle> In 23rd Annual International Symposium on Com 25 puter Architecture, </booktitle> <pages> pages 191202, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: the results of our previous work [31]<ref> [30] </ref>. In [31], we used a multiprogrammed workload to assess the potential of SMT on a high-level architectural model and favorably compared total instruction throughput on an SMT to several alternatives: a superscalar processor, a fine-grained multithreaded processor, and a single-chip, shared-memory multiprocessor. In [30], we presented a micro-architectural design that demonstrated that this potential can be realized in an implementable SMT processor. <p> On each cycle, the fetch mechanism selects up to two threads (among threads not already incurring I-cache misses) and fetches up to four instructions from each thread (the 2.4 scheme from <ref> [30] </ref>). The total fetch bandwidth of 8 instructions is therefore equivalent to that required for an 8-wide superscalar processor, and only two I-cache ports are required. Additional logic, however, is necessary in the SMT to prioritize thread selection. Thread priorities are assigned using the icount feedback technique [30], which favors threads <p> 2.4 scheme from <ref> [30] </ref>). The total fetch bandwidth of 8 instructions is therefore equivalent to that required for an 8-wide superscalar processor, and only two I-cache ports are required. Additional logic, however, is necessary in the SMT to prioritize thread selection. Thread priorities are assigned using the icount feedback technique [30], which favors threads that are using processor resources most effectively. Under icount, highest priority is given to the threads that have the least number of instructions in the decode, renaming, and queue pipeline stages. <p> The additional pipe stage requires an extra level of bypass logic, but the number of stages has a smaller impact on the complexity and delay of this logic (O (n)) than the issue width (O (n 2 )) [21]. Our previous study <ref> [30] </ref> contains more details regarding the effects of the two-stage register read/write pipelines on the architecture and performance. <p> Finally, parallel applications are a natural workload for traditional parallel architectures, and therefore serve as a fair basis for comparing SMT and multiprocessors. For the sake of comparison, in Section 7, we will also briefly compare our parallel results with the multiprogrammed results from <ref> [30] </ref>. 3.2 Workload Our workload of parallel programs includes both explicitly and implicitly-parallel applications (Table 2). Many multi processor studies look only at the parallel portions of a program for measuring speedups; we look at the execution of the entire program, including the sequential portions, for two reasons. <p> As indicated in the previous section, SMT may exhibit different behavior for parallel applications when compared to a multiprogrammed workload because of different degrees of inter-thread interference. Table 14 compares the SMT throughput results from this study (parallel workload) with the multipro-grammed results in <ref> [30] </ref>. Note that because the applications themselves are different, and the simulation methodology is slightly different (mainly a 2MB L3 cache vs. an 8MB L3), we can only make rough comparisons and highlight some differences. <p> Parallel applications experience less inter-thread interference in the branch hardware, because the distinct threads tend to exhibit similar branching behavior. Table 15 compares the parallel applications of this study with the multiprogrammed workload in our previous study <ref> [30] </ref>, in terms of branch and jump misprediction rates. The data indicates that minimal branch prediction interference occurs in parallel applications, relative to a multiprogrammed workload. Parallel applications also suffer from less inter-thread interference in the memory system, compared to multipro-grammed applications. <p> The data indicates that minimal branch prediction interference occurs in parallel applications, relative to a multiprogrammed workload. Parallel applications also suffer from less inter-thread interference in the memory system, compared to multipro-grammed applications. For the sake of comparison, Table 16 presents resource contention statistics from <ref> [30] </ref>. Although contention occurs in the parallel applications, it is much less significant than in multiprogrammed workloads. This means that the use of multithreading to expose parallelism is truly useful, as the latency tolerance has the ability to hide additional latencies introduced by cache and branch interference.
Reference: [31] <author> D. M. Tullsen, S. J. Eggers, and H. M. Levy. </author> <title> Simultaneous multithreading: Maximizing on-chip parallelism. </title> <booktitle> In 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 392403, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: This paper explores parallel processing on a simultaneous multithreading architecture. Our investigation of parallel program performance on an SMT is motivated by the results of our previous work <ref> [31] </ref>[30]. In [31], we used a multiprogrammed workload to assess the potential of SMT on a high-level architectural model and favorably compared total instruction throughput on an SMT to several alternatives: a superscalar processor, a fine-grained multithreaded processor, and a single-chip, shared-memory multiprocessor. <p> These results demonstrate that SMT is particularly well-suited for parallel applications, especially in comparison to multiprocessors. In Tullsen, et al., <ref> [31] </ref>, we compared the performance of SMT and various multiprocessor configurations and found that SMT outperforms an MP with comparable hardware for a multiprogrammed workload.
Reference: [32] <author> G. Tyson and M. Farrens. </author> <title> Techniques for extracting instruction level parallelism on MIMD architectures. </title> <booktitle> In 26th International Symposium on Microarchitecture, </booktitle> <pages> pages 128137, </pages> <month> December </month> <year> 1993. </year>
Reference: [33] <author> G. Tyson, M. Farrens, and A. R. Pleszkun. MISC: </author> <title> A multiple instruction stream computer. </title> <booktitle> In 25th International Symposium on Microarchitecture, </booktitle> <pages> pages 193196, </pages> <month> December </month> <year> 1992. </year>
Reference: [34] <author> J. Vasell. </author> <title> A fine-grain threaded abstract machine. </title> <booktitle> In 1994 International Conference on Parallel Architectures and Compilation Techniques, </booktitle> <pages> pages 1524, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: The M-Machine [9], MISC [33][32], and Multiscalar [28] architectures all require significant compiler support (or hand-coding) to extract maximum performance. In all three designs, execution resources are partitioned in a manner that prevents them from being dynamically shared by all threads. The S-TAM architecture <ref> [34] </ref> exploits both instruction-level and thread-level parallelism by statically allocating threads to processors, and dynamically allocating each thread to a functional unit. A primary goal was to expose scheduling and allocation to the compiler. On their machine intrathread parallelism was avoided; parallelism was instead expressed only between threads.
Reference: [35] <author> W. Weber and A. Gupta. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: preliminary results. </title> <booktitle> In 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 273280, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: They found that cache interference between threads varied depending on the benchmark. For locality-optimized programs, the total number of misses remained fairly constant as the number of contexts was increased. For unop-timized programs, however, misses were more significant and resulted in performance degradations. Weber and Gupta <ref> [35] </ref> also studied the effects of cache interference in conventional multithreaded architectures, and found increases in inter-thread misses that are comparable to our results. Agarwal [1] and Saavedra-Barrera, et al., [25] used analytic models for studying the efficiency of multithreaded processors.
Reference: [36] <author> K. M. Wilson, K. Olukotun, and M. Rosenblum. </author> <title> Increasing cache port efficiency for dynamic superscalar microprocessors. </title> <booktitle> In 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 147157, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: We found, however, that when using longer cache lines, the gains due to better spatial locality outweighed the costs associated with the increase in L1 bank contention. Furthermore, some of the contention may be reduced by using recently-proposed architectural techniques to use cache ports more efficiently <ref> [36] </ref>.) In order to measure the performance degradation due to inter-thread bank conflicts, we modeled an L1 cache that ignored inter-thread bank conflicts, while still accounting for same-thread bank conflicts.
Reference: [37] <author> R. Wilson, R. French, C. Wilson, S. Amarasinghe, J. Anderson, S. Tjiang, S.-W. Liao, C.-W. Tseng, M. Hall, M. Lam, and J. Hennessy. </author> <title> SUIF: An infrastructure for research on parallelizing and optimizing compilers. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 29(12):3137, </volume> <month> December </month> <year> 1994. </year>
Reference-contexts: Tomcatv and hydro2d from SPEC92 [7], as well as shallow and linpack, are implicitly-parallel programs for which we use the SUIF compiler <ref> [37] </ref> to extract loop-level parallelism. SUIF generates transformed C output files that contain calls to a parallel runtime library to create threads and execute loop iterations in parallel.
Reference: [38] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2436, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: C explicit Molecular dynamics N-body problem, using 3-d spatial data structure Table 2: Benchmark suite 9 resource partitioning, a multiprocessor typically cannot get good single-thread performance; SMT can, by taking better advantage of ILP, even in sequential sections. 3 Five of our benchmarks are explicitly-parallel programs from the SPLASH-2 suite <ref> [38] </ref>, which are built on the Argonne National Laboratories parallel macro library [3]. Tomcatv and hydro2d from SPEC92 [7], as well as shallow and linpack, are implicitly-parallel programs for which we use the SUIF compiler [37] to extract loop-level parallelism.
Reference: [39] <author> W. Yamamoto and M. Nemirovsky. </author> <title> Increasing superscalar performance through multistreaming. </title> <booktitle> In IFIP WG10.3 Working Conference on Parallel Architectures and Compilation Techniques (PACT 95), </booktitle> <pages> pages 4958, </pages> <month> June </month> <year> 1995. </year>
Reference: [40] <author> W. Yamamoto, M. J. Serrano, A. R. Talcott, R. C. Wood, and M. Nemirovsky. </author> <title> Performance estimation of multi-streamed, superscalar processors. </title> <booktitle> In Twenty-Seventh Hawaii International Conference on System Sciences, </booktitle> <pages> pages I:195 204, </pages> <month> January </month> <year> 1994. </year>
Reference: [41] <author> K. C. Yeager. </author> <title> The MIPS R10000 superscalar microprocessor. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 2840, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: In the rest of this section, we will describe all three of these processor architectures. 2.1 Base processor architecture The base processor is a sophisticated, out-of-order superscalar processor with a dynamic scheduling core similar to the MIPS R10000 <ref> [41] </ref>. Figure 2 illustrates the organization of this processor, while Figure 3 (top) shows its processor pipeline. On each cycle, the processor fetches a block of instructions from the instruction cache.
References-found: 41

