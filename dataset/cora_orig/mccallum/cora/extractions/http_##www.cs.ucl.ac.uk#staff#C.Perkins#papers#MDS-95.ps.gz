URL: http://www.cs.ucl.ac.uk/staff/C.Perkins/papers/MDS-95.ps.gz
Refering-URL: http://www.cs.ucl.ac.uk/staff/C.Perkins/papers/
Root-URL: http://www.cs.ucl.ac.uk
Title: Reliability Models for Hard Real-Time Systems  
Author: CS Perkins AM Tyrrell 
Keyword: Real-time system, Recovery block, Markov model, completion probability profile.  
Address: Heslington, York, YO1 5DD, UK.  
Affiliation: Department of Electronics, University of York  
Abstract: We present a new reliability model for hard real-time systems. This is an extended Markov model, derived from an analysis of the generic properties of hard real-time systems subject to a simple random-fault model. Our model permits analysis of the run-time behaviour of a system, in order to derive the probability profiles of the system's completion/failure times. The model is applied to the analysis of a simple sequential recovery block system, and illustrative examples based on this system are provided. The paper concludes with a discussion of the application of such accurate completion profile information to the design of embedded software systems. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. Arlat, L. Kanoun, and J.-C. Laprie. </author> <title> Dependability evaluation of software fault-tolerance. </title> <booktitle> In Proceedings 18th International Symposium on Fault- Tolerant Computing. IEEE, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: The reliability models which have been developed in the literature may be split into two groups: Functional models which describe the system from a time-independent viewpoint, and dynamic models which describe the run-time behaviour of a system. Time-independent models <ref> [1, 9, 18, 21] </ref> are typically based around a Markov-chain or other probabilistic process which is used to describe the behaviour of the system either neglecting information about execution time or providing a partial ordering of events only.
Reference: 2. <author> S. Balaji, L. Jenkins, L.M. Patnaik, and P.S. Goel. </author> <title> Workload redistribution for fault-tolerance in a hard real-time distributed computing system. </title> <booktitle> In 19th International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 366-373. </pages> <publisher> IEEE, </publisher> <year> 1989. </year>
Reference-contexts: Much of the research conducted with hard real-time systems has focussed on scheduling problems <ref> [2, 8, 16, 22, 24] </ref>, and typically requires knowledge of the execution time bounds of a process to enable efficient schedules to be calculated. With the introduction of fault-tolerant procedures, the execution time bounds of the system will change.
Reference: 3. <author> A. Csenki. </author> <title> Reliability analysis of recovery blocks with nested clusters of failure points. </title> <journal> IEEE Transactions on Reliability, </journal> <volume> 42(1) </volume> <pages> 34-43, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: A generic acceptance test will be fallible, that is, it will not correctly classify all systems, and will take a finite amount of time. For reasons of simplicity and tractability of the analysis, the test modelled here will, however, be assumed infallible <ref> [3] </ref>, and will take unit time. The study of systems with fallible acceptance tests is the subject of current research. This combined alternate and acceptance test model is illustrated in figure 7. Several such systems may be combined in order to model a complete recovery block.
Reference: 4. <author> B. Dimitrov, Z. Khalil, N. Kolev, and P. Petrov. </author> <title> On the optimal total processing time using checkpoints. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(5) </volume> <pages> 436-442, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The timing properties of a hard real-time system are as important as its functional properties in ensuring correct operation and, unfortunately, this class of model is not able to describe this with sufficient rigour. In contrast, time-dependent models are much less well developed <ref> [4, 7] </ref>. Although some work has been conducted into finding algorithms to derive the mean execution time of a set of processes in the presence of failure [20], there has been little work undertaken to determine the probability distribution of the system's execution time.
Reference: 5. <author> European Space Agency. </author> <title> Software reliability modeling study, </title> <month> February </month> <year> 1988. </year> <note> Invitation to tender AO/1-2039/87/NL/IW. </note>
Reference-contexts: an a priori unrealistic hypothesis, turns out to be satisfactory." It is further noted that a study made of the reliability logs of Tandem systems [6] provides evidence for this claim, as indeed does the work of Musa at Bell Labs [14, 15], and that of the European Space Agency <ref> [5] </ref>, where it is noted that "Software failure is a process that appears to the observer to be random, therefore the term reliability is meaningful when applied to a system which includes software and the process can be modelled as stochastic." It can therefore be seen that the random-fault model as
Reference: 6. <author> J. N. Gray. </author> <title> Why do computers stop and what can be done about it? In Proceedings 5th Symposium on Reliability in Distributed Software and Database Systems, </title> <address> pages 3-12, Los Angeles, </address> <month> January </month> <year> 1986. </year>
Reference-contexts: It is considered that such an assumption is not unrealistic, indeed it is the basis for a number of other models [12, 13, 14], and certain experimental data <ref> [6, 15] </ref> has been collected which appears to confirm the validity of this approach. <p> The work conducted by Laprie [11] also lends support to this, when it is noted that "...the constancy of the hazard rates, although it is an a priori unrealistic hypothesis, turns out to be satisfactory." It is further noted that a study made of the reliability logs of Tandem systems <ref> [6] </ref> provides evidence for this claim, as indeed does the work of Musa at Bell Labs [14, 15], and that of the European Space Agency [5], where it is noted that "Software failure is a process that appears to the observer to be random, therefore the term reliability is meaningful when
Reference: 7. <author> A. Grnarov, J. Arlat, and A. Avizienis. </author> <title> On the performance of software fault-tolerance strategies. </title> <booktitle> In Proceedings of the 10th International Symposium on Fault- Tolerant Computing. IEEE, </booktitle> <year> 1980. </year>
Reference-contexts: The timing properties of a hard real-time system are as important as its functional properties in ensuring correct operation and, unfortunately, this class of model is not able to describe this with sufficient rigour. In contrast, time-dependent models are much less well developed <ref> [4, 7] </ref>. Although some work has been conducted into finding algorithms to derive the mean execution time of a set of processes in the presence of failure [20], there has been little work undertaken to determine the probability distribution of the system's execution time. <p> This section will propose a scheme by which this can be accomplished. A number of experimental studies have been conducted into the failure characteristics of software systems [10, 15]. These studies, together with theoretical results such as those presented in <ref> [7, 11, 12, 13, 14] </ref> seem to indicate that it is possible to achieve a reasonably accurate prediction of the failure characteristics of a software system using very simple models, and indeed, it has often been proposed that a random-fault model will suffice.
Reference: 8. <author> D. Haban and K. G. Shin. </author> <title> Application of real-time monitoring to scheduling tasks with random execution times. </title> <journal> IEEE Transactions on software engineering, </journal> <volume> 16(12), </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: Much of the research conducted with hard real-time systems has focussed on scheduling problems <ref> [2, 8, 16, 22, 24] </ref>, and typically requires knowledge of the execution time bounds of a process to enable efficient schedules to be calculated. With the introduction of fault-tolerant procedures, the execution time bounds of the system will change.
Reference: 9. <author> B. E. Helvik. </author> <title> Modelling the influence of unreliable software in distributed computer systems. </title> <booktitle> In Digest of papers : 18th International symposium on fault-tolerant computing, </booktitle> <pages> pages 136-141. </pages> <publisher> IEEE, </publisher> <year> 1988. </year>
Reference-contexts: The reliability models which have been developed in the literature may be split into two groups: Functional models which describe the system from a time-independent viewpoint, and dynamic models which describe the run-time behaviour of a system. Time-independent models <ref> [1, 9, 18, 21] </ref> are typically based around a Markov-chain or other probabilistic process which is used to describe the behaviour of the system either neglecting information about execution time or providing a partial ordering of events only.
Reference: 10. <author> J. C. Knight and N. G. Leveson. </author> <title> An experimental evaluation of the assumption of independence in multiversion programming. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-12(1):96-109, </volume> <month> January </month> <year> 1986. </year> <month> 20 </month>
Reference-contexts: This section will propose a scheme by which this can be accomplished. A number of experimental studies have been conducted into the failure characteristics of software systems <ref> [10, 15] </ref>.
Reference: 11. <author> J.-C. Laprie. </author> <title> Dependability evaluation of software systems in operation. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(4):701-714, </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: This section will propose a scheme by which this can be accomplished. A number of experimental studies have been conducted into the failure characteristics of software systems [10, 15]. These studies, together with theoretical results such as those presented in <ref> [7, 11, 12, 13, 14] </ref> seem to indicate that it is possible to achieve a reasonably accurate prediction of the failure characteristics of a software system using very simple models, and indeed, it has often been proposed that a random-fault model will suffice. <p> It is considered that such an assumption is not unrealistic, indeed it is the basis for a number of other models [12, 13, 14], and certain experimental data [6, 15] has been collected which appears to confirm the validity of this approach. The work conducted by Laprie <ref> [11] </ref> also lends support to this, when it is noted that "...the constancy of the hazard rates, although it is an a priori unrealistic hypothesis, turns out to be satisfactory." It is further noted that a study made of the reliability logs of Tandem systems [6] provides evidence for this claim,
Reference: 12. <author> J.-C. Laprie and K. Kanoun. </author> <title> X-Ware reliability and availability modeling. </title> <journal> IEEE Transactions of Software Engineering, </journal> <volume> 18(2) </volume> <pages> 130-147, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: This section will propose a scheme by which this can be accomplished. A number of experimental studies have been conducted into the failure characteristics of software systems [10, 15]. These studies, together with theoretical results such as those presented in <ref> [7, 11, 12, 13, 14] </ref> seem to indicate that it is possible to achieve a reasonably accurate prediction of the failure characteristics of a software system using very simple models, and indeed, it has often been proposed that a random-fault model will suffice. <p> It is considered that such an assumption is not unrealistic, indeed it is the basis for a number of other models <ref> [12, 13, 14] </ref>, and certain experimental data [6, 15] has been collected which appears to confirm the validity of this approach.
Reference: 13. <author> B. Littlewood. </author> <title> Stochastic reliability-growth: A model for fault-removal in computer programs and hardware designs. </title> <journal> IEEE Transactions on Reliability, </journal> <volume> R-30(4):313-320, </volume> <month> October </month> <year> 1981. </year>
Reference-contexts: This section will propose a scheme by which this can be accomplished. A number of experimental studies have been conducted into the failure characteristics of software systems [10, 15]. These studies, together with theoretical results such as those presented in <ref> [7, 11, 12, 13, 14] </ref> seem to indicate that it is possible to achieve a reasonably accurate prediction of the failure characteristics of a software system using very simple models, and indeed, it has often been proposed that a random-fault model will suffice. <p> at different times. 4 It can readily be seen that, for all but the simplest of systems, the input space is so large, and the interactions which occur are so subtle and complex, that it is effectively impossible to predict the path the system will take through its input space <ref> [13] </ref>. From the above arguments, it seems reasonable to model the system's path through its inputs as a random-walk in a multi-dimensional space. This is transformed by the system to provide a path through the output space which is necessarily also modelled as a random walk. <p> It is considered that such an assumption is not unrealistic, indeed it is the basis for a number of other models <ref> [12, 13, 14] </ref>, and certain experimental data [6, 15] has been collected which appears to confirm the validity of this approach.
Reference: 14. <author> J.D. Musa. </author> <title> Validity of execution-time theory of software reliability. </title> <journal> IEEE Transactions on Reliability, </journal> <volume> R-28(3):181-191, </volume> <month> August </month> <year> 1979. </year>
Reference-contexts: This section will propose a scheme by which this can be accomplished. A number of experimental studies have been conducted into the failure characteristics of software systems [10, 15]. These studies, together with theoretical results such as those presented in <ref> [7, 11, 12, 13, 14] </ref> seem to indicate that it is possible to achieve a reasonably accurate prediction of the failure characteristics of a software system using very simple models, and indeed, it has often been proposed that a random-fault model will suffice. <p> It is considered that such an assumption is not unrealistic, indeed it is the basis for a number of other models <ref> [12, 13, 14] </ref>, and certain experimental data [6, 15] has been collected which appears to confirm the validity of this approach. <p> "...the constancy of the hazard rates, although it is an a priori unrealistic hypothesis, turns out to be satisfactory." It is further noted that a study made of the reliability logs of Tandem systems [6] provides evidence for this claim, as indeed does the work of Musa at Bell Labs <ref> [14, 15] </ref>, and that of the European Space Agency [5], where it is noted that "Software failure is a process that appears to the observer to be random, therefore the term reliability is meaningful when applied to a system which includes software and the process can be modelled as stochastic." It
Reference: 15. <author> J.D. Musa. </author> <title> Software reliability data. </title> <type> Technical report, </type> <institution> Bell Telephone Laboratories, </institution> <month> January </month> <year> 1980. </year> <note> Report obtainable from DACS, </note> <institution> Rome Air Development Centre, Rome, </institution> <address> New York. </address>
Reference-contexts: This section will propose a scheme by which this can be accomplished. A number of experimental studies have been conducted into the failure characteristics of software systems <ref> [10, 15] </ref>. <p> It is considered that such an assumption is not unrealistic, indeed it is the basis for a number of other models [12, 13, 14], and certain experimental data <ref> [6, 15] </ref> has been collected which appears to confirm the validity of this approach. <p> "...the constancy of the hazard rates, although it is an a priori unrealistic hypothesis, turns out to be satisfactory." It is further noted that a study made of the reliability logs of Tandem systems [6] provides evidence for this claim, as indeed does the work of Musa at Bell Labs <ref> [14, 15] </ref>, and that of the European Space Agency [5], where it is noted that "Software failure is a process that appears to the observer to be random, therefore the term reliability is meaningful when applied to a system which includes software and the process can be modelled as stochastic." It
Reference: 16. <author> C. Y. Park. </author> <title> Predicting program execution times by analysing static and dynamic program paths. </title> <booktitle> Real-Time Systems, </booktitle> <volume> 5 </volume> <pages> 31-62, </pages> <year> 1993. </year>
Reference-contexts: Much of the research conducted with hard real-time systems has focussed on scheduling problems <ref> [2, 8, 16, 22, 24] </ref>, and typically requires knowledge of the execution time bounds of a process to enable efficient schedules to be calculated. With the introduction of fault-tolerant procedures, the execution time bounds of the system will change.
Reference: 17. <author> J.L. Peterson. </author> <title> Petri net theory and the modeling of systems. </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: The underlying network model borrows a number of concepts from Petri net theory <ref> [17] </ref>, not least the notation used. Despite the notational similarities, however, this is primarily a Markov model, not a Petri net system. The basis of the model comprises a multi-graph consisting of a set of places and a set of transitions connected by directed arcs.
Reference: 18. <author> G. Pucci. </author> <title> A new approach to the modeling of recovery block structures. </title> <journal> IEEE Transactions on software engineering, </journal> <volume> 18(2) </volume> <pages> 159-167, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: The reliability models which have been developed in the literature may be split into two groups: Functional models which describe the system from a time-independent viewpoint, and dynamic models which describe the run-time behaviour of a system. Time-independent models <ref> [1, 9, 18, 21] </ref> are typically based around a Markov-chain or other probabilistic process which is used to describe the behaviour of the system either neglecting information about execution time or providing a partial ordering of events only.
Reference: 19. <author> B. Randell. </author> <title> System structure for software fault tolerance. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-1:220-231, </volume> <month> June </month> <year> 1975. </year>
Reference-contexts: In particular, it may be used to derive a schedule which takes into account the probability distribution of the system's execution time; allowing the reliability/performance trade off to be made explicit. 6 Application to Recovery Block Systems The recovery block <ref> [19] </ref> is a technique which uses multiple versions of a program block to attempt to ensure success in the presence of system failures. The first version is known as the primary and the second and subsequent versions are known as alternates. The primary is executed, and an acceptance test evaluated.
Reference: 20. <author> A. Ranganathan and S. Upadhyaya. </author> <title> Performance evaluation of rollback-recovery techniques in computer programs. </title> <journal> IEEE Transactions on Reliability, </journal> <volume> 42(2) </volume> <pages> 220-226, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: In contrast, time-dependent models are much less well developed [4, 7]. Although some work has been conducted into finding algorithms to derive the mean execution time of a set of processes in the presence of failure <ref> [20] </ref>, there has been little work undertaken to determine the probability distribution of the system's execution time.
Reference: 21. <author> R. K. Scott, J. W. Gault, and D. F. McAllister. </author> <title> Fault-tolerant software reliability modeling. </title> <journal> IEEE Transactions of Software Engineering, </journal> <volume> SE-13(5):582-592, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: The reliability models which have been developed in the literature may be split into two groups: Functional models which describe the system from a time-independent viewpoint, and dynamic models which describe the run-time behaviour of a system. Time-independent models <ref> [1, 9, 18, 21] </ref> are typically based around a Markov-chain or other probabilistic process which is used to describe the behaviour of the system either neglecting information about execution time or providing a partial ordering of events only.
Reference: 22. <author> T. Shepard and J. A. M. Gagne. </author> <title> A pre-run-time scheduling algorithm for hard real-time systems. </title> <journal> IEEE Transactions on Software engineering, </journal> <volume> 17(7), </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: Much of the research conducted with hard real-time systems has focussed on scheduling problems <ref> [2, 8, 16, 22, 24] </ref>, and typically requires knowledge of the execution time bounds of a process to enable efficient schedules to be calculated. With the introduction of fault-tolerant procedures, the execution time bounds of the system will change.
Reference: 23. <author> L. Takacs. </author> <title> Stochastic processes. </title> <publisher> Methuen, </publisher> <year> 1962. </year>
Reference-contexts: It can therefore be seen that the random-fault model as applied to software and combined hardware-software systems provides a reasonable fit with experimental data with a relatively simple theoretical background. 3 Underlying Mathematical Model The underlying mathematical model detailed here is a stochastic model, derived primarily from Markov chain theory <ref> [23] </ref>, with modifications to allow for simple process interactions. The underlying network model borrows a number of concepts from Petri net theory [17], not least the notation used. Despite the notational similarities, however, this is primarily a Markov model, not a Petri net system.
Reference: 24. <author> J. Xu and D. L. Parnas. </author> <title> On satisfying timing constraints in hard-real-time systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 19(1) </volume> <pages> 70-84, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Much of the research conducted with hard real-time systems has focussed on scheduling problems <ref> [2, 8, 16, 22, 24] </ref>, and typically requires knowledge of the execution time bounds of a process to enable efficient schedules to be calculated. With the introduction of fault-tolerant procedures, the execution time bounds of the system will change.
References-found: 24

