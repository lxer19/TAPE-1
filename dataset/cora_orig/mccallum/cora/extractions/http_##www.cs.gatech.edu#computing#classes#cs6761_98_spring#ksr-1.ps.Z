URL: http://www.cs.gatech.edu/computing/classes/cs6761_98_spring/ksr-1.ps.Z
Refering-URL: http://www.cs.gatech.edu/computing/classes/cs6761_98_spring/hw1.html
Root-URL: 
Title: Scalability Study of the KSR-1  
Author: Umakishore Ramachandran Gautam Shah S. Ravikumar Jeyakumar Muthukumarasamy 
Keyword: Key Words: Shared memory multiprocessors, Scalability, Synchronization, Ring interconnect, Latency, NAS bench marks, Performance  
Address: Atlanta, GA 30332  
Affiliation: College of Computing Georgia Institute of Technology  
Note: Appeared in Parallel Computing, Vol  This work is supported in part by an NSF PYI Award MIP-9058430 and an NSF Grant MIP-9200005. A preliminary version  
Email: e-mail: rama@cc.gatech.edu  
Phone: Phone: (404) 894-5136  
Date: 22, 1996, 739-759  
Abstract: Scalability of parallel architectures is an interesting area of current research. Shared memory parallel programming is attractive stemming from its relative ease in transitioning from sequential programming. However, there has been concern in the architectural community regarding the scalability of shared memory parallel architectures owing to the potential for large latencies for remote memory accesses. KSR-1 is a commercial shared memory parallel architecture, and the scalability of KSR-1 is the focus of this research. The study is conducted using a range of experiments spanning latency measurements, synchronization, and analysis of parallel algorithms for three computational kernels and an application. The key conclusions from this study are as follows: The communication network of KSR-1, a pipelined unidirectional ring, is fairly resilient in supporting simultaneous remote memory accesses from several processors. The multiple communication paths realized through this pipelining help in the efficient implementation of tournament-style barrier synchronization algorithms. Parallel algorithms that have fairly regular and contiguous data access patterns scale well on this architecture. The architectural features of KSR-1 such as the poststore and prefetch are useful for boosting the performance of parallel applications. The sizes of the caches available at each node may be too small for efficiently implementing large data structures. The network does saturate when there are simultaneous remote memory accesses from a fully populated (32 node) ring. of this paper appeared in the International Conference on Parallel Processing, 1993, Vol I:237-240 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. E. Anderson. </author> <title> The performance of spin lock alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: The hardware primitive essentially serializes all lock requests regardless of whether they are shared or exclusive. We have implemented a simple read-write lock using the KSR-1 exclusive lock primitive. Our algorithm is a modified version of Anderson's ticket lock <ref> [1] </ref>. A shared data structure can be acquired in read-shared mode or in a write-exclusive mode. Lock requests are granted tickets atomically using the get sub page primitive. Consecutive read lock requests are combined by allowing them to get the same ticket.
Reference: [2] <author> D. H. Bailey, E. Barszcz, and J. T. Barton et al. </author> <title> The NAS parallel benchmarks summary and preliminary results. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 158-65, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: As can be seen from Figure 5 the same performance trends continue for the larger system as for the 32-node system. 3.3 NAS Benchmark Suite The Numerical Aerodynamic Simulation (NAS) parallel benchmark <ref> [3, 2] </ref> consists of five kernels and three applications which are considered to be representative of several scientific and numerical applications. We have implemented three of the five kernels and one application on the KSR-1 as part of our scalability study.
Reference: [3] <author> David Bailey, John Barton, Thomas Lasinski, and Horst Simon. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report Report RNR-91-002, </type> <institution> NAS Systems Division, Applied Research Branch, NASA Ames Research Center, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: As can be seen from Figure 5 the same performance trends continue for the larger system as for the 32-node system. 3.3 NAS Benchmark Suite The Numerical Aerodynamic Simulation (NAS) parallel benchmark <ref> [3, 2] </ref> consists of five kernels and three applications which are considered to be representative of several scientific and numerical applications. We have implemented three of the five kernels and one application on the KSR-1 as part of our scalability study.
Reference: [4] <author> S. R. Breit and G. Shah. </author> <title> Implementing the NAS parallel benchmarks on the KSR-1. </title> <booktitle> In Parallel CFD, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Table 3 shows the performance of the application on the KSR-1. The table shows the time it takes to perform each iteration in the application (it takes 400 such steps to run the benchmark). Details of the application performance can be found in <ref> [4] </ref>. The results show that the application is scalable on the KSR-1. However, it is instructive to observe the optimizations that were necessary to get to this level of performance.
Reference: [5] <author> Thinking Machines Corporation. </author> <title> The Connection Machine CM-5 Technical Summary, </title> <month> October </month> <year> 1991. </year>
Reference-contexts: Thus there is a tendency to believe that message passing architectures may be more scalable than its architectural rival as is evident from the number of commercially available message passing machines <ref> [5, 11, 15] </ref>. Yet, there is considerable interest in the architectural community toward realizing scalable shared memory multiprocessors. Indeed the natural progression from sequential programming to shared memory style parallel programming is one of the main reasons for such a trend.
Reference: [6] <author> Steve Breit et al. </author> <title> Implementation of EP, SP and BT on the KSR-1. </title> <type> KSR internal report, </type> <month> September </month> <year> 1992. </year>
Reference-contexts: The first one is the Embarrassingly Parallel (EP) kernel, which evaluates integrals by means of pseudorandom trials and is used in many Monte-Carlo simulations. As the name suggests, it is highly suited for parallel machines, since there is virtually no communication among the parallel tasks. Our implementation <ref> [6] </ref> showed linear speedup, and given the limited communication requirements of this kernel, this result was 12 not surprising.
Reference: [7] <author> Steve Frank. </author> <type> Personal communication, </type> <month> January </month> <year> 1993. </year>
Reference-contexts: This result is surprising considering the software overhead (maintaining the queue of requesters) in our queue-based lock algorithm. The result can be partly explained due to the interaction of the operating system with the application as follows <ref> [7] </ref>. While threads of a parallel program can be bound to distinct processors, there is no guarantee that they will be co-scheduled. More importantly, from the point of view of our results, the timer interrupts on the different processors are not synchronized.
Reference: [8] <author> John L. Gustafson. </author> <title> Reevaluating Amdahl's law. </title> <journal> Communications of the ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: A drawback with this definition is that an architecture would be considered non-scalable if an algorithm running on it has a large sequential part. There have been several recent attempts at refining this notion and define new scalability metrics (see for instance <ref> [12, 8, 14] </ref>).
Reference: [9] <author> D. P. Helmbold and C. E. McDowell. </author> <title> Modelling speedup (n) greater than n. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(2):.250-6, </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: Figure 8 shows the corresponding speedup curve for the algorithm. Up to about 4 processors, the insufficient sizes of the sub-cache and local-cache inhibits achieving very good speedups. However, notice that relative to the 4 processor performance the 8 and 16 processor executions exhibit superunitary 7 <ref> [9] </ref> speedup. This superunitary speedup can be explained by the fact that the amount of data that each processor has to deal with fits in the respective local-caches, and as we observed earlier the algorithm design ensures that there is very limited synchronization among the processors.
Reference: [10] <author> D. Hensgen, R. Finkel, and U. Manber. </author> <title> Two algorithms for barrier synchronization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(1) </volume> <pages> 1-17, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: In each round a total of P messages are exchanged. The communication pattern for these message exchanges is such that (see Reference <ref> [10] </ref> for details) after the log 2 P rounds are over all the processors are aware of barrier completion. Algorithm 4 (labeled tournament in Figure 4) is a tournament barrier (another tree-style algorithm similar to Algorithm 2) in which the winner in each round is determined statically.
Reference: [11] <author> Intel Corporation, </author> <title> Beaverton, Oregon. Touchstone Delta System User's Guide, </title> <year> 1991. </year>
Reference-contexts: Thus there is a tendency to believe that message passing architectures may be more scalable than its architectural rival as is evident from the number of commercially available message passing machines <ref> [5, 11, 15] </ref>. Yet, there is considerable interest in the architectural community toward realizing scalable shared memory multiprocessors. Indeed the natural progression from sequential programming to shared memory style parallel programming is one of the main reasons for such a trend.
Reference: [12] <author> Alan H. Karp and Horace P. Flatt. </author> <title> Measuring parallel processor performance. </title> <journal> Communications of the ACM, </journal> <volume> 33(5) </volume> <pages> 539-543, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: A drawback with this definition is that an architecture would be considered non-scalable if an algorithm running on it has a large sequential part. There have been several recent attempts at refining this notion and define new scalability metrics (see for instance <ref> [12, 8, 14] </ref>). <p> Table 1 gives the speedup, efficiency, and the measured serial fraction 6 <ref> [12] </ref> for the CG algorithm on the KSR-1. Figure 8 shows the corresponding speedup curve for the algorithm. Up to about 4 processors, the insufficient sizes of the sub-cache and local-cache inhibits achieving very good speedups.
Reference: [13] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: While this reason may explain why the software algorithm may do as well as the system lock, it is still unclear why it does better as can be seen in the results. 3.2.2 Barriers We have implemented five barrier synchronization algorithms <ref> [13] </ref> on the KSR-1. In our implementation, we have aligned (whenever possible) mutually exclusive parts of shared data structures on separate cache lines so that there is no false sharing. The results are shown in Figure 4. There are two steps to barrier synchronization: arrival and completion. <p> We do observe it to perform better than the counter algorithm. But since this algorithm requires mutual exclusion lock for the fetch and operation, the performance degrades quickly as the number of processors is increased. However, modifying the completion notification (as suggested in <ref> [13] </ref>) by spinning on a global wakeup flag (instead of tree-based notification) set by the last arriving processor produces remarkable performance enhancement (see line labeled tree (M) in Figure 4). <p> Its performance is almost similar to that of the dynamic-tree barrier with global wakeup flag. Although it is stated that the MCS algorithm may be the best for large-scale cache coherent multiprocessors in Reference <ref> [13] </ref>, the detrimental effects discussed above and our results on the KSR-1 indicate that the tournament (M) algorithm is better than the MCS (M) algorithm. <p> communication paths available in the architecture and due to the false sharing inherent in the MCS algorithm. 10 3.2.3 Comparison with Other Architectures It is illustrative to compare the performance of the barriers on the KSR-1 with their performance on the Sequent Symmetry and the BBN Butterfly reported in Reference <ref> [13] </ref>. The Symmetry is a bus-based shared memory multiprocessor with invalidation-based coherent caches. The Butterfly is a distributed shared memory multiprocessor with a butterfly multistage interconnection network, and no caches. The counter algorithm performs the best on the Symmetry. <p> So the determinant here in deciding the winner is the number of rounds of communication (or the critical path for the arrival + wakeup tree) as pointed out 11 by Mellor-Crummey and Scott <ref> [13] </ref>. Thus the dissemination algorithm does the best ((log 2 P ) rounds of communication), followed by the tournament algorithm (2log 2 P rounds of communication), and then the MCS algorithm ((log 4 P + log 2 P ) rounds of communication).
Reference: [14] <author> D. Nussbaum and A. Agarwal. </author> <title> Scalability of parallel machines. </title> <journal> Communications of the ACM, </journal> <volume> 34(3) </volume> <pages> 56-61, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: A drawback with this definition is that an architecture would be considered non-scalable if an algorithm running on it has a large sequential part. There have been several recent attempts at refining this notion and define new scalability metrics (see for instance <ref> [12, 8, 14] </ref>).
Reference: [15] <author> J. F. Palmer and G. Fox. </author> <title> The NCUBE family of high-performance parallel computer systems. </title> <booktitle> In Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 847-51 vol.1, </pages> <year> 1988. </year>
Reference-contexts: Thus there is a tendency to believe that message passing architectures may be more scalable than its architectural rival as is evident from the number of commercially available message passing machines <ref> [5, 11, 15] </ref>. Yet, there is considerable interest in the architectural community toward realizing scalable shared memory multiprocessors. Indeed the natural progression from sequential programming to shared memory style parallel programming is one of the main reasons for such a trend.
Reference: [16] <institution> Kendall Square Research. KSR1 Principles of Operations, </institution> <year> 1992. </year>
Reference-contexts: Concluding remarks are presented in Section 4. 2 Architecture of the KSR-1 The KSR-1 is a 64-bit cache-only memory architecture (COMA) based on an interconnection of a hierarchy of rings <ref> [16, 17] </ref>. The ring (see Figure 1) at the lowest level in the hierarchy can contain up to 32 processors. These leaf rings connect to rings of higher bandwidth through a routing unit (ARD).
Reference: [17] <institution> Kendall Square Research. Technical summary, </institution> <year> 1992. </year> <month> 22 </month>
Reference-contexts: Concluding remarks are presented in Section 4. 2 Architecture of the KSR-1 The KSR-1 is a 64-bit cache-only memory architecture (COMA) based on an interconnection of a hierarchy of rings <ref> [16, 17] </ref>. The ring (see Figure 1) at the lowest level in the hierarchy can contain up to 32 processors. These leaf rings connect to rings of higher bandwidth through a routing unit (ARD).
References-found: 17

