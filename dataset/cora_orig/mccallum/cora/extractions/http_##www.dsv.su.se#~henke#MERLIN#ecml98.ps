URL: http://www.dsv.su.se/~henke/MERLIN/ecml98.ps
Refering-URL: http://www.dsv.su.se/~henke/MERLIN/MERLIN.html
Root-URL: 
Email: henke@dsv.su.se  
Phone: Tel: +46-8-16 16 16  
Title: Predicate Invention and Learning from Positive Examples Only  
Author: Henrik Bostrom 
Address: Electrum 230, 164 40 Kista, Sweden  
Affiliation: Dept. of Computer and Systems Sciences Stockholm University  
Abstract: Previous bias shift approaches to predicate invention are not applicable to learning from positive examples only, if a complete hypothesis can be found in the given language, as negative examples are required to determine whether new predicates should be invented or not. One approach to this problem is presented, MERLIN 2.0, which is a successor of a system in which predicate invention is guided by sequences of input clauses in SLD-refutations of positive and negative examples w.r.t. an overly general theory. In contrast to its predecessor which searches for the minimal finite-state automaton that can generate all positive and no negative sequences, MERLIN 2.0 uses a technique for inducing Hidden Markov Models from positive sequences only. This enables the system to invent new predicates without being triggered by negative examples. Another advantage of using this induction technique is that it allows for incremental learning. Experimental results are presented comparing MERLIN 2.0 with the positive only learning framework of Progol 4.2 and comparing the original induction technique with a new version that produces deterministic Hidden Markov Models. The results show that predicate invention may indeed be both necessary and possible when learning from positive examples only as well as it can be beneficial to keep the induced model deterministic.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Bain M. and Muggleton S., </author> <title> "Non-Monotonic Learning", </title> <editor> in Muggleton S. (ed.), </editor> <booktitle> Inductive Logic Programming, </booktitle> <publisher> Academic Press (1992) 145-161 </publisher>
Reference-contexts: 1 Introduction Bias shift approaches to predicate invention (e.g. <ref> [15, 6, 8, 1, 16] </ref>) introduce new predicates whenever the learning method fails to produce a consistent hypothesis in the given language [13].
Reference: 2. <author> Bar-Hillel Y., Perles M. and Shamir E., </author> <title> "On formal properties of simple phrase structure grammars", </title> <journal> Zeitschrift fur Phonetik, Sprachwissenschaft und Kommunika-tionsforschung, </journal> <volume> 14, 1, </volume> <publisher> Akademie Verlag, </publisher> <address> Berlin (1961) 143-172 </address>
Reference-contexts: together with the program in section 1, the above procedure produces the following proof grammar (p=1; R), where R is the following set of rules: p=1 ! c1 p=1 ! c3 p=1 3.3 Deriving the Intersection The intersection of a context-free language and a regular language is a context-free language <ref> [2] </ref>. In [5], a derivation of the algorithm in [2] is presented, which finds a context-free grammar that represents the intersection of a proof grammar and an NFA. <p> procedure produces the following proof grammar (p=1; R), where R is the following set of rules: p=1 ! c1 p=1 ! c3 p=1 3.3 Deriving the Intersection The intersection of a context-free language and a regular language is a context-free language <ref> [2] </ref>. In [5], a derivation of the algorithm in [2] is presented, which finds a context-free grammar that represents the intersection of a proof grammar and an NFA.
Reference: 3. <author> Baum L., Petrie T, Soules G. and Weiss N., </author> <title> "A maximization technique occurring in the statistical analysis of probabilistic functions in Markov chains", </title> <journal> The Annals of Mathematical Statistics 41 (1970) 164-171 </journal>
Reference-contexts: x: X p (q I ! q 1 )p (q 1 " 1 )p (q 1 ! q 2 ) : : : p (q l " l )p (q l ! q F ) 2.2 Induction of Hidden Markov Models Traditional HMM estimation is based on the Baum-Welch algorithm <ref> [3] </ref>, which assumes a certain topology and adjusts the parameters so as to maximise the model likelihood on the given samples.
Reference: 4. <author> Biermann A. W. and Feldman J. </author> <title> A.,"On the Synthesis of Finite-State Machines from Samples of Their Behavior", </title> <note> IEEE Transactions on Computers 21 (1972) 592-597 </note>
Reference-contexts: There are a number of possible directions for future research. One is to experiment with the approach using other techniques for inducing finite-state automata from positive examples only (e.g. <ref> [4] </ref>). Another direction is to investigate extensions to the technique for inducing Hidden Markov Models, including techniques for finding a good global prior weight, using other search techniques than Hill-climbing and allowing negative examples.
Reference: 5. <author> Bostrom H., </author> <title> "Theory-Guided Induction of Logic Programs by Inference of Regular Languages", </title> <booktitle> Proc. of the 13th International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann (1996) 46-53 </publisher>
Reference-contexts: In this work we present one approach to this problem, the system MERLIN 2.0 2 . The system is a successor of MERLIN 1.0 <ref> [5] </ref>, which uses an overly general theory to find SLD-refutations 3 of positive and negative examples, and then searches for the minimal finite-state automaton that can generate all sequences of input clauses in the SLD-refutations of the positive examples and none of the sequences in the SLD-refutations of the negative examples. <p> Below, we show how to produce such a grammar by an example (for algorithmic details see <ref> [5] </ref>). <p> In <ref> [5] </ref>, a derivation of the algorithm in [2] is presented, which finds a context-free grammar that represents the intersection of a proof grammar and an NFA. <p> This is achieved using the procedure presented in <ref> [5] </ref>. <p> The set of negative examples were generated in the same way as for the previous domain, resulting in a set of 128 examples in total (of which 50% are positive). The fourth theory and example set that were investigated were taken from <ref> [5] </ref>, where the target predicate turing (M) represents a sequence of movements of a Turing machine, where each move is on the form (Read; W rite; M ove) and the positive examples correspond to movements of a Turing machine performing addition.
Reference: 6. <author> Kijsirikul B., Numao M. and Shimura M., </author> <title> "Discrimination-based constructive induction of logic programs", </title> <booktitle> Proceedings of the 10th National Conference on Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann (1992) 44-49 </publisher>
Reference-contexts: 1 Introduction Bias shift approaches to predicate invention (e.g. <ref> [15, 6, 8, 1, 16] </ref>) introduce new predicates whenever the learning method fails to produce a consistent hypothesis in the given language [13].
Reference: 7. <author> Lewis H. R. and Papadimitriou C. H., </author> <title> Elements of the Theory of Computation, </title> <publisher> Prentice-Hall (1981) </publisher>
Reference-contexts: Models We first give a definition of Hidden Markov Models adopted from [14] and then briefly present the technique for inducing Hidden Markov Models that was introduced in [14]. 2.1 Hidden Markov Models Hidden Markov Models (HMMs) can be viewed as a stochastic generalisation of the non-deterministic finite automata (NFAs) <ref> [7] </ref>. As NFAs, HMMs accept (or generate) strings over the alphabet by non-deterministic walks between the initial and final states. In addition, HMMs also assign probabilities to the strings they generate, computed from the probabilities of individual transitions and emissions. These concepts are defined formally below. Definition 1.
Reference: 8. <author> Lapointe S., Ling, C. and Matwin S., </author> <title> "Constructive Inductive Logic Programming", </title> <booktitle> Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <address> Mor-gan Kaufmann (1993) 1030-1036 </address>
Reference-contexts: 1 Introduction Bias shift approaches to predicate invention (e.g. <ref> [15, 6, 8, 1, 16] </ref>) introduce new predicates whenever the learning method fails to produce a consistent hypothesis in the given language [13].
Reference: 9. <author> Lloyd J. W., </author> <booktitle> Foundations of Logic Programming, (2nd edition), </booktitle> <publisher> Springer-Verlag (1987) </publisher>
Reference-contexts: In section three it is shown how 2 The system can be obtained from http://www.dsv.su.se/ML/MERLIN.html 3 Familiarity with logic programming terminology is assumed <ref> [9] </ref>. this technique is incorporated in MERLIN 2.0. In section four, we present exper-imental results comparing the system with positive only learning in Progol 4.2 [10, 11] as well as comparing the deterministic and non-deterministic versions of the technique for induction of Hidden Markov Models.
Reference: 10. <author> Muggleton S., </author> <title> "Inverse entailment and Progol", </title> <booktitle> New Generation Computing 13 (1995) 245-286 </booktitle>
Reference-contexts: In section three it is shown how 2 The system can be obtained from http://www.dsv.su.se/ML/MERLIN.html 3 Familiarity with logic programming terminology is assumed [9]. this technique is incorporated in MERLIN 2.0. In section four, we present exper-imental results comparing the system with positive only learning in Progol 4.2 <ref> [10, 11] </ref> as well as comparing the deterministic and non-deterministic versions of the technique for induction of Hidden Markov Models.
Reference: 11. <author> Muggleton S., </author> <title> "Learning from positive data", </title> <booktitle> Proc. of the Sixth International Workshop on Inductive Logic Programming (1996) </booktitle>
Reference-contexts: In section three it is shown how 2 The system can be obtained from http://www.dsv.su.se/ML/MERLIN.html 3 Familiarity with logic programming terminology is assumed [9]. this technique is incorporated in MERLIN 2.0. In section four, we present exper-imental results comparing the system with positive only learning in Progol 4.2 <ref> [10, 11] </ref> as well as comparing the deterministic and non-deterministic versions of the technique for induction of Hidden Markov Models.
Reference: 12. <author> Muggleton S., </author> <title> "Stochastic Logic Programs", Advances in Inductive Logic Programming (Ed. </title> <editor> L. De Raedt), </editor> <publisher> IOS Press (1996) 254-264 </publisher>
Reference-contexts: A third direction is to actually use the parameters which are set by the induction algorithm in order to induce stochastic logic programs <ref> [12] </ref>. Acknowledgements This work has been supported by the European Community ESPRIT Long Term Research Project no. 20237 Inductive Logic Program ming II and the Swedish Research Council for Engineering Sciences (TFR).
Reference: 13. <author> Stahl I., </author> <title> "Predicate Invention in Inductive Logic Programming", Advances in Inductive Logic Programming (Ed. </title> <editor> L. De Raedt), </editor> <publisher> IOS Press (1996) 34-47 </publisher>
Reference-contexts: 1 Introduction Bias shift approaches to predicate invention (e.g. [15, 6, 8, 1, 16]) introduce new predicates whenever the learning method fails to produce a consistent hypothesis in the given language <ref> [13] </ref>. This means that as long as it is possible to formulate a complete hypothesis in the given language (i.e. such that all positive examples are covered), negative examples are necessary for detecting inconsistency, and thus for inventing new predicates 1 . 1 Other approaches to predicate invention according to [13] <p> <ref> [13] </ref>. This means that as long as it is possible to formulate a complete hypothesis in the given language (i.e. such that all positive examples are covered), negative examples are necessary for detecting inconsistency, and thus for inventing new predicates 1 . 1 Other approaches to predicate invention according to [13] fall outside the scope of this work. These are reformulation approaches, which uses predicate invention for optimising a theory w.r.t. size or compression, and transformation approaches, which base their decision on introducing new predicates on the operationality or efficiency of the induced hypothesis.
Reference: 14. <author> Stolcke A. and Omohundro S., </author> <title> "Best-first Model Merging for Hidden Markov Model Induction", </title> <booktitle> TR-94-003, International Computer Science Institute, </booktitle> <address> Berkeley, CA (1994) </address>
Reference-contexts: In the above example, this would result in a hypothesis identical to the overly general theory. In order to overcome this problem, MERLIN 2.0 uses a technique for inducing Hidden Markov Models from positive sequences only <ref> [14] </ref>, that instead of minimising the size of the resulting automaton maximises the posterior probability. This enables MERLIN 2.0 to invent new predicates without being triggered by negative examples. <p> Finally, in section five we give concluding remarks and point out directions for future research. 2 Induction of Hidden Markov Models We first give a definition of Hidden Markov Models adopted from <ref> [14] </ref> and then briefly present the technique for inducing Hidden Markov Models that was introduced in [14]. 2.1 Hidden Markov Models Hidden Markov Models (HMMs) can be viewed as a stochastic generalisation of the non-deterministic finite automata (NFAs) [7]. <p> Finally, in section five we give concluding remarks and point out directions for future research. 2 Induction of Hidden Markov Models We first give a definition of Hidden Markov Models adopted from <ref> [14] </ref> and then briefly present the technique for inducing Hidden Markov Models that was introduced in [14]. 2.1 Hidden Markov Models Hidden Markov Models (HMMs) can be viewed as a stochastic generalisation of the non-deterministic finite automata (NFAs) [7]. As NFAs, HMMs accept (or generate) strings over the alphabet by non-deterministic walks between the initial and final states. <p> However, as we are primarily interested in finding the topology, and not the parameters, the technique in <ref> [14] </ref> is more appropriate than the former, as it in contrast to the former can be used for finding the HMM with maximimal posterior probability of the structure 4 . We first introduce the concept of posterior probability of an HMM structure according to [14] and then present their Best-first merging <p> not the parameters, the technique in <ref> [14] </ref> is more appropriate than the former, as it in contrast to the former can be used for finding the HMM with maximimal posterior probability of the structure 4 . We first introduce the concept of posterior probability of an HMM structure according to [14] and then present their Best-first merging algorithm for finding an HMM with maximal posterior probability. Finally, we present an extension to the algorithm that forces the induced HMM to be deterministic. <p> where B (ff 1 ; : : : ; ff n ) is the n-dimensional Beta function: B (ff 1 ; : : : ; ff n ) = (ff 1 + + ff n ) Best-first merging Below we present the incremental version of the Best-first merging algorithm in <ref> [14] </ref>. It takes as input a sequence of samples, incorporates them one by one into the current model and after each incorporation uses Hill-Climbing (with look-ahead) to find a new current model with maximal posterior probability by merging states. After all samples are processed, the current model is returned. <p> We first present the theories and example sets that were used in the experiments and then the experimental results. 9 The lookahead was set to 5 for the non-deterministic version and to 1 for the deterministic version. Following <ref> [14] </ref>, a logarithmic version of Bayes' law was used in the implementation including a global prior weight , giving log P (M S )+ log P (XjM S ) as the quantity to be maximised. was set to 0.30 for the non-deterministic version and to 0.25 for the deterministic version. <p> The set of positive examples consisted of instances of p (L) where L contained up to 33 elements representing the regular expression ac fl a [ bc fl b (this target was also used in <ref> [14] </ref>). The set of negative examples were generated in the same way as for the previous domain, resulting in a set of 128 examples in total (of which 50% are positive).
Reference: 15. <author> Wirth R. and O'Rorke P., </author> <title> "Constraints on Predicate Invention", </title> <booktitle> Proceedings of the 8th International Workshop on Machine Learning , Morgan Kaufmann (1991) 457-461 </booktitle>
Reference-contexts: 1 Introduction Bias shift approaches to predicate invention (e.g. <ref> [15, 6, 8, 1, 16] </ref>) introduce new predicates whenever the learning method fails to produce a consistent hypothesis in the given language [13].
Reference: 16. <author> Wrobel S., </author> <title> "Concept Formation During Interactive Theory Revision", Machine Learning Journal 14 (1994) 169-192 This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: 1 Introduction Bias shift approaches to predicate invention (e.g. <ref> [15, 6, 8, 1, 16] </ref>) introduce new predicates whenever the learning method fails to produce a consistent hypothesis in the given language [13].
References-found: 16

