URL: ftp://synapse.cs.byu.edu/pub/papers/rudolph.lia96.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Title: Word Perfect Corp. LIA: A Location-Independent Transformation for ASOCS Adaptive Algorithm 2  
Author: George L. Rudolph and Tony R. Martinez 
Address: Provo, Utah 84602  
Affiliation: Computer Science Department, Brigham Young University,  
Note: To appear in International Journal of Neural Systems This research is funded in part by grants from Novell Inc. and  
Abstract: Most Artificial Neural Networks (ANNs) have a fixed topology during learning, and often suffer from a number of shortcomings as a result. ANNs that use dynamic topologies have shown ability to overcome many of these problems. Adaptive Self Organizing Concurrent Systems (ASOCS) are a class of learning models with inherently dynamic topologies. This paper introduces Location-Independent Transformations (LITs) as a general strategy for implementing learning models that use dynamic topologies efficiently in parallel hardware. A LIT creates a set of location-independent nodes, where each node computes its part of the network output independent of other nodes, using local information. This type of transformation allows efficient support for adding and deleting nodes dynamically during learning. In particular, this paper presents the Location - Independent ASOCS (LIA) model as a LIT for ASOCS Adaptive Algorithm 2. The description of LIA gives formal definitions for LIA algorithms. Because LIA implements basic ASOCS mechanisms, these definitions provide a formal description of basic ASOCS mechanisms in general, in addition to LIA. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. S. Almasi, A. Gottlieb, </author> <title> Highly Parallel Computing , The Benjamin/Cummings Publishing Company, </title> <publisher> Inc, </publisher> <address> Redwood City, CA, </address> <year> 1989. </year>
Reference-contexts: Although some neurocomputers could potentially support dynamic topologies more directly in hardware, rather than in software, they currently do not. Of course, general parallel machines, like the Connection Machine [9] and the CRAY <ref> [1] </ref>, can simulate the desired dynamics in software, but these machines are not optimized for neural computation. LIT supports general classes of ANNs and dynamic topologies in an efficient parallel hardware implementation. LIT maps the original network into a hierarchical, parallel network of Location-Independent nodes (LI-nodes).
Reference: [2] <author> P. T. Baffes, J. M. Zelle, </author> <title> Growing Layers of Perceptrons: Introducing the Extentron Algorithm, </title> <booktitle> Proceedings of 1992 IEEE/INNS International Joint Conference on Neural NetworksBaltimore, </booktitle> <volume> Vol. 2, </volume> <pages> pp 4979-4984, </pages> <year> 1992. </year>
Reference: [3] <author> T. Denoeux, R. Lengelle, </author> <title> Initializing Back Propagation Networks With Prototypes Neural Networks, </title> <journal> Vol. </journal> <volume> 6, #3, </volume> <pages> pp 351-363, </pages> <publisher> Pergamon Press Ltd, </publisher> <address> New York, </address> <year> 1993. </year>
Reference: [4] <author> S. Fahlmann, </author> <title> Faster-Learning Variations on BackPropagation: An Empirical Study, </title> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pp 38-51, </pages> <year> 1988. </year>
Reference: [5] <author> S. Fahlmann, C. Lebiere, </author> <booktitle> The Cascade-Correlation Learning Architecture, Advances in Neural Information Processing 2, </booktitle> <pages> pp 524-532, </pages> <publisher> Morgan Kaufmann Publishers: </publisher> <address> Los Altos, CA. </address>
Reference: [6] <author> Farhat, N., D. Psaltis, A. Prata, and E. Paek. </author> <title> Optical Implementation of the Hopfield Model. </title> <journal> Applied Optics, </journal> <volume> Vol. 24, #10. </volume> <month> pp.1469-1475 . </month> <year> 1985. </year>
Reference: [7] <author> Graf, H., L. Jackel, W. Hubbard. </author> <title> VLSI Implementation of a Neural Network Model. In Artificial Neural Networks: Electronic Implementations, </title> <publisher> Nelson Morgan, Ed. </publisher> <pages> pp. 34-42. </pages> <year> 1990. </year>
Reference: [8] <author> D. Hammerstrom, W. Henry, M. Kuhn. </author> <title> Neurocomputer System for Neural-Network Applications. In Parallel Digital Implementations of Neural Networks . K. </title> <editor> Przytula, V. Prasanna, Eds. </editor> <publisher> Prentice-Hall, Inc. </publisher> <year> 1991. </year>
Reference-contexts: More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs <ref> [8] </ref>, [18], [23]. Although some neurocomputers could potentially support dynamic topologies more directly in hardware, rather than in software, they currently do not.
Reference: [9] <author> W. D. Hillis. </author> <title> The Connection Machine. </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs [8], [18], [23]. Although some neurocomputers could potentially support dynamic topologies more directly in hardware, rather than in software, they currently do not. Of course, general parallel machines, like the Connection Machine <ref> [9] </ref> and the CRAY [1], can simulate the desired dynamics in software, but these machines are not optimized for neural computation. LIT supports general classes of ANNs and dynamic topologies in an efficient parallel hardware implementation. LIT maps the original network into a hierarchical, parallel network of Location-Independent nodes (LI-nodes).
Reference: [10] <author> E. Karnin, </author> <title> A Simple Procedure for Pruning BackPropagation Trained Neural Networks, </title> <journal> IEEE Transactions On Neural Networks, </journal> <volume> Vol. 1, #2, </volume> <pages> pp 239-242, </pages> <month> June, </month> <year> 1990. </year>
Reference: [11] <author> C. Lee, </author> <title> A Simple Procedure for Pruning BackPropagation Trained Neural Networks, </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 6, #3, </volume> <pages> pp 385-392. </pages>
Reference: [12] <author> T. Martinez, J.J. Vidal. </author> <title> Adaptive Parallel Logic Networks. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 5, #1. </volume> <pages> pp. 26-58. </pages> <year> 1988. </year>
Reference-contexts: The eight which do not are those which contain any b j that is Notrelated, is Discriminated, or is Concordant Overlap. With the exception of Notrelated, which should be obvious, the others are discussed in more detail in <ref> [12] </ref> and other ASOCS literature. Of the remaining seven subsets: four would contain any possible inconsistent b j , those being Discordant Subset, Equal, Superset or Overlap; three would contain any instance that could possibly be used to minimize W t , those being Concordant Subset, Equal or Superset. <p> The reason for doing self-deletion at this point in the algorithm is that any nodes which self-delete may potentially be reallocated during DVA, if 12 necessary. This makes more efficient use of the nodes. This differs from the AA2 algorithm of <ref> [12] </ref>. There, self-deletion takes place after DVA, in order to take advantage of any preexisting Pnode hierarchies for node combination in adding properly discriminated instances to the network. Note that self-deletion effectively deletes redundant instances and totally contradicted instances.
Reference: [13] <author> T. Martinez, D. Campbell, </author> <title> A Self-Organizing Binary Decision Tree for Incrementally Defined Rule Based Systems. </title> <journal> IEEE Trans. on Systems, Man and Cybernetics, </journal> <volume> Vol. 21, #5. </volume> <pages> pp. 1231-1237, </pages> <year> 1991. </year>
Reference: [14] <author> T. Martinez, D. Campbell, </author> <title> A Self-Adjusting Dynamic Logic Module, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 11, #4, </volume> <pages> pp 303-313. </pages> <year> 1991. </year>
Reference-contexts: Current research is demonstrating the use of dynamic topologies in overcoming these problems [2-5], [10-11], <ref> [14] </ref>, [16-17], [19]. A dynamic topology is one which allows adding and deleting both nodes and weighted connections during learning. 2 Early ANN hardware implementations are modelspecific, and are intended to support static topologies [6-7], [16]. <p> Transform the ANN 2. Embed the LI-nodes in a tree arbitrary number of hidden layers L INo d es m As suggested above, LIT is a generalization of the authors work on a strategy for implementing in hardware the ASOCS class of models [12-14], in particular AA2 <ref> [14] </ref>, which have inherently dynamic topologies. The initial results of that work were the Location - Independent ASOCS model (LIA) [20] and the Priority ASOCS model [15], [24]. <p> The new nodes R is the same as R j . end 6. Add a t to the network. 6.1 Allocate a free node. 6.2 Store a t in the new node. end (learning) 4.3 Optional Network Minimizations As in <ref> [14] </ref>, the learning algorithm above omits optional minimization procedures that may lead to a smaller network.
Reference: [15] <author> T. Martinez, D. Campbell, B. Hughes. </author> <title> Priority ASOCS, </title> <journal> Journal of Artificial Neural Networks, </journal> <volume> Vol. 1, #3. </volume> <pages> pp. 403-429. </pages> <year> 1994. </year>
Reference-contexts: The initial results of that work were the Location - Independent ASOCS model (LIA) [20] and the Priority ASOCS model <ref> [15] </ref>, [24]. While the underlying mechanisms of ASOCS may differ significantly from those common among ANNs, the overall goals of ASOCS are similar to other such learning models, as is the need to support dynamic topologies. Hence, the authors have applied and expanded the initial strategy to more common ANNs.
Reference: [16] <author> C. Mead, </author> <title> Analog VLSI and Neural Systems, </title> <publisher> Addison-Wesley Publishing Company, Inc, </publisher> <year> 1991. </year>
Reference-contexts: A dynamic topology is one which allows adding and deleting both nodes and weighted connections during learning. 2 Early ANN hardware implementations are modelspecific, and are intended to support static topologies [6-7], <ref> [16] </ref>. More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs [8], [18], [23]. Although some neurocomputers could potentially support dynamic topologies more directly in hardware, rather than in software, they currently do not.
Reference: [17] <author> S. Odri, D. Petrovacki, G. Krstonosic, </author> <title> Evolutional Development of a Multilevel Neural Network, </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 6, #4. </volume> <pages> pp 583-595. </pages> <publisher> Pergamon Press Ltd, </publisher> <address> New York, </address> <year> 1993. </year>
Reference: [18] <author> Ramacher, U., W. Raab, J. Anlauf, U. Hachmann, J. Beichter, N. Brls, M. Weiling, E. Schneider, R. Mnner, J. </author> <title> Gl. </title> <booktitle> Multiprocessor and Memory Architecture of the 21 Neurocomputer SYNAPSE-1. Proceedings, World Congress on Neural Networks 1993, </booktitle> <volume> Vol. 4. </volume> <pages> pp. 775-778. </pages> <publisher> INNS Press, </publisher> <year> 1993. </year>
Reference-contexts: More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs [8], <ref> [18] </ref>, [23]. Although some neurocomputers could potentially support dynamic topologies more directly in hardware, rather than in software, they currently do not.
Reference: [19] <author> D. Reilly, L. Cooper, C. </author> <title> Erlbaum, A Neural Model Category Learning, </title> <journal> Biological Cybernetics, </journal> <volume> Vol. 45, </volume> <pages> pp 35-41, </pages> <year> 1982. </year>
Reference-contexts: Current research is demonstrating the use of dynamic topologies in overcoming these problems [2-5], [10-11], [14], [16-17], <ref> [19] </ref>. A dynamic topology is one which allows adding and deleting both nodes and weighted connections during learning. 2 Early ANN hardware implementations are modelspecific, and are intended to support static topologies [6-7], [16].
Reference: [20] <author> G. Rudolph, T. Martinez, </author> <title> An Efficient Static Topology for Modeling ASOCS, Artificial Neural Networks, </title> <editor> Editors, Kohonen et al, </editor> <address> pp 279-734, </address> <publisher> Elsevier Publishers, North Holland, </publisher> <year> 1991. </year>
Reference-contexts: The initial results of that work were the Location - Independent ASOCS model (LIA) <ref> [20] </ref> and the Priority ASOCS model [15], [24]. While the underlying mechanisms of ASOCS may differ significantly from those common among ANNs, the overall goals of ASOCS are similar to other such learning models, as is the need to support dynamic topologies.
Reference: [21] <author> G. Rudolph, T. Martinez, </author> <title> A Transformation for Implementing Localist Neural Networks, </title> <note> To appear in Journal of Neural, Parallel and Scientific Computations, </note> <year> 1995. </year>
Reference-contexts: Hence, the authors have applied and expanded the initial strategy to more common ANNs. This led to the development of LITs for counterpropagation, competitive learning <ref> [21] </ref>, and backpropagation with an arbitrary number of layers [22]. LITs can potentially support a broad set of ANNs, thus allowing one efficient implementation strategy to support both inherently dynamic ANNs and variations of ANNs with dynamic extensions (such as backpropagation).
Reference: [22] <author> Rudolph G., Martinez, T. R. </author> <title> A Transformation for Implementing Multilayer Distributed FeedForward Neural Networks, </title> <note> Submitted, </note> <year> 1995. </year>
Reference-contexts: Hence, the authors have applied and expanded the initial strategy to more common ANNs. This led to the development of LITs for counterpropagation, competitive learning [21], and backpropagation with an arbitrary number of layers <ref> [22] </ref>. LITs can potentially support a broad set of ANNs, thus allowing one efficient implementation strategy to support both inherently dynamic ANNs and variations of ANNs with dynamic extensions (such as backpropagation).
Reference: [23] <author> Shams, S. </author> <title> Dream MachineA Platform for Efficient Implementation of Neural Networks with Arbitrarily Complex Interconnect Structures. </title> <type> Technical Report CENG 92-23 . PhD Dissertation, </type> <institution> USC, </institution> <year> 1992. </year>
Reference-contexts: More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs [8], [18], <ref> [23] </ref>. Although some neurocomputers could potentially support dynamic topologies more directly in hardware, rather than in software, they currently do not.
Reference: [24] <author> Stout, M., G. Rudolph, T.R. Martinez, L. Salmon, </author> <title> A VLSI Implementation of a Parallel, Self-Organizing Learning Model, </title> <booktitle> Proceedings of the International Conference on Pattern Recognition (1994) 373-376. </booktitle>
Reference-contexts: The initial results of that work were the Location - Independent ASOCS model (LIA) [20] and the Priority ASOCS model [15], <ref> [24] </ref>. While the underlying mechanisms of ASOCS may differ significantly from those common among ANNs, the overall goals of ASOCS are similar to other such learning models, as is the need to support dynamic topologies. Hence, the authors have applied and expanded the initial strategy to more common ANNs. <p> These views are conceptually correct, intending primarily to show how the nodes implement conjunctions of variables and the required mapping of inputs to outputs. However, Stout et. al. found these high-level views to be too simplistic and somewhat misleading, when attempting to design actual implementations <ref> [24] </ref>. The critical issue is that such a view ignores the control circuitry (switches, memory, etc.) needed to set up or preprocess and maintain the inputs to the AND-gates. <p> The "matching" function used in LIA and PASOCS models is essentially an equivalence function extended to handle * values in the instances. This view was more convenient for the VLSI implementation of PASOCS in <ref> [24] </ref>. 3.2. LIA Learning Mode Let W 0 be the null, or empty instance set, which corresponds to a network that stores no instances. <p> The diagram uses only single-variable comparisons, one comparison per transition. The comparison can also be done in parallel blocks depending on the width of the data path, as in <ref> [24] </ref>. 11 -- = -- = -- = , -- S is the starting state; all other symbols represent states and /or single-variable comparisons as follows: is subset is equal = is superset is overlap -- is discriminated using only single variable comparisons. <p> This section begins with definitions for one type of pairwise minimizations based on certain one-difference concordant instances, which were defined for the original ASOCS models. The definition of the term "one-difference" differs here and in <ref> [24] </ref> from previous ASOCS usage in slight, but fairly significant, ways.
References-found: 24

