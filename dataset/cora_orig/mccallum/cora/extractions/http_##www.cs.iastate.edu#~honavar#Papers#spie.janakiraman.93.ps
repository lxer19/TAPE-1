URL: http://www.cs.iastate.edu/~honavar/Papers/spie.janakiraman.93.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/publist.html
Root-URL: 
Title: Adaptive Learning Rate Selection for Backpropagation Networks  
Author: Jayathi Janakiraman Vasant Honavar 
Date: September 11, 1993  
Address: 226 Atanasoff Hall  Ames Iowa 50011-1040  
Affiliation: Department of Computer Science  Iowa State University  
Abstract: Backpropagation is a supervised learning algorithm for training multi-layer neural networks for function approximation and pattern classification by minimizing a suitably defined error metric (e.g., the mean square error between the desired and actual outputs of the network for a training set) using gradient descent. It does this by calculating the partial derivative of the overall error and changing each weight by a small amount (determined by the learning rate) in a direction that is expected to reduce the error. Despite its success on a number of real-world problems, backpropagation can be very slow (it requires hundreds of passes (epochs) through the training set). Also, its performance is extremely sensitive to the choice of parameters such as the learning rate. The mathematical considerations that go into the derivation of backpropagation require that the learning rate be as small as possible. On the other hand, in order reduce the number training epochs required to learn the desired input-output mapping, it is desirable to make the weight changes as large as possible without causing the error to increase. Carefully designed experiments with a number of different data sets show that the number of epochs required for learning is very sensitive to the choice of the (constant) learning rate parameter. The learning rate that works best for one data set may not work so well for a different data set. Furthermore, our simulations with the iris data set clearly demonstrate that the best learning rate for the same problem can be rather sensitive to the particular random choice of training examples as well as the initial weight settings! It is therefore desirable to have an algorithm that can change the learning rate dynamically so that it is close to optimal (in terms of reducing the error as much as possible given the fl This research was partially supported by the Iowa State University College of Liberal Arts and Sciences. Some of the resuls in this paper were presented in SPIE '93 conference at Orlando, Florida and the World Congress on Neural Networks, Portland, Oregon. The authors are grateful to Dr. Michael Pazzani for making the electronic versions of the data sets used in this study available through the University of California (Irvine) machine learning repository. 
Abstract-found: 1
Intro-found: 1
Reference: [Balakrishnan & Honavar, 1992] <author> Balakrishnan, K., & Honavar, V., </author> <title> "Improving Convergence of Backpropagation by Handling Flat-spots in the Output Layer", </title> <booktitle> In: Proceedings of the Second International Conference on Artificial Neural Networks, </booktitle> <address> Brighton, U.K., </address> <year> 1992. </year>
Reference-contexts: In [Fahlman, 1988], several methods to combat the flat-spot problem are examined; adding an offset to the derivative of the sigmoid function to prevent it from approaching zero gave the best results. Recently, a different solution for the flat-spot problem has been proposed and experimentally evaluated <ref> [Balakrishnan & Honavar, 1992] </ref>. A common modification to the backpropogation algorithm which helps alleviate the step-size problem is to use a momentum term [Rumelhart et al., 1986]. In this paper we have tried a different approach to solve the step-size problem. <p> An alternative solution to the flat spots problem has been explored in <ref> [Balakrishnan & Honavar, 1992] </ref>. 4 Experimental methodology The primary objective of the experiments discussed in this section are to compare the performance of the adaptive eta modification to the back-propagation proposed in this paper witha number of variants of standard back-propagation (e.g, adding momentum, flatspot elimination etc) on a variety of
Reference: [Parekh & Balakrishnan & Honavar, 1992] <author> Parekh, R., & Balakrish nan, K., & Honavar, V., </author> <title> "An Empirical Comparison of Flat-Spot Elimination Tec hniques in Back-Propogation Networks", </title> <booktitle> inProceedings of the Conference on Artificial Neural Networks, </booktitle> <address> Houston, TX, </address> <year> 1992. </year>
Reference-contexts: Choice of Parameters for Experiments: Standard backprop is rather slow. Exhaustively scanning the 3-dimensional space defined by , ff, and weight-range is very time-consuming. Therefore, the choice was made as follows: We used the parameter-setting of the best-average case results reported in 10 <ref> [Parekh & Balakrishnan & Honavar, 1992] </ref> for the Iris and Soybean data-sets and the Encoder-Decoder problems. For the Audiology and Chess data sets, we ran simulations for = 0:25; 0:5; 0:8; 0:9; 1:0; 1:25 and ff = 0:25; 0:5; 0:80:9. The best-average case combination was then taken for our experiments.
Reference: [Becker & leCun, 1988] <author> Becker, S. and leCun, Y., </author> <title> "Improving the Convergence of BackPropagation Learning with Second-Order Methods", </title> <booktitle> in Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Many techniques have been proposed which explicitly calculate an approximation to the second derivative, and use this information to determine the appropriate step size <ref> [Becker & leCun, 1988, Fahlman, 1988] </ref>. These methods are quite computationally expensive compared to back-propagation.
Reference: [Fahlman, 1988] <author> Fahlman, Scott E., </author> <title> "An Empirical Study of Learnin g Speed in BackPropagation Networks",Technical Report CMU-CS-88-162, </title> <institution> Carnegie Mellon University, Computer Science Department,Pittsburgh, </institution> <address> PA, </address> <year> 1988. </year> <month> 15 </month>
Reference-contexts: Two factors contributing to the slow convergence of the backpropagation are the small step-size and flat-spots in the sigmoid function where the derivative is near zero <ref> [Fahlman, 1988, Fahlman & Lebiere, 1991] </ref>. In [Fahlman, 1988], several methods to combat the flat-spot problem are examined; adding an offset to the derivative of the sigmoid function to prevent it from approaching zero gave the best results. <p> Two factors contributing to the slow convergence of the backpropagation are the small step-size and flat-spots in the sigmoid function where the derivative is near zero [Fahlman, 1988, Fahlman & Lebiere, 1991]. In <ref> [Fahlman, 1988] </ref>, several methods to combat the flat-spot problem are examined; adding an offset to the derivative of the sigmoid function to prevent it from approaching zero gave the best results. Recently, a different solution for the flat-spot problem has been proposed and experimentally evaluated [Balakrishnan & Honavar, 1992]. <p> Many techniques have been proposed which explicitly calculate an approximation to the second derivative, and use this information to determine the appropriate step size <ref> [Becker & leCun, 1988, Fahlman, 1988] </ref>. These methods are quite computationally expensive compared to back-propagation. <p> This may cause the unit to be "stuck" in the flat spot for several iterations. One solution to the flat spots problem is to ensure that the sigmoid-prime function does not approache zero. As proposed in <ref> [Fahlman, 1988] </ref>, this can be accomplished by simply adding a constant 0.1 to the sigmoid-prime function before using it to scale the error. <p> This keeps the weight changes for an output unit that is far from its target value from approaching zero; the weight changes for units near their targets still reach zero due to the proximity of the output to the target value. Significant speedups are reported in <ref> [Fahlman, 1988] </ref> using this sigmoid-prime with offset function. <p> Threshold and margin metric <ref> [Fahlman, 1988] </ref> The network is said to have correctly classified a training pattern if the actual network output is within a spec ified threshold for each of the components of the output vector. <p> The 11 percent speedup is calculated relative to BP rule with momentum added (we don't report results for BP without the momentum term because there is general consensus in the literature that the addition of momentum term substantially speeds up the convergence of BP <ref> [Fahlman, 1988] </ref>). 5.1 Modification Interactions Toy Data Sets: Rule ff os Ave.
Reference: [Fahlman & Lebiere, 1991] <author> Fahlman, Scott E., and Lebiere, </author> <title> Christi an, "The Cascade--Correlation Learning Architecture", </title> <type> Technical Report CMU-CS-90-100, </type> <institution> Carnegie Mellon University, Computer Science Department,Pittsburgh, </institution> <address> PA, </address> <year> 1991. </year>
Reference-contexts: Two factors contributing to the slow convergence of the backpropagation are the small step-size and flat-spots in the sigmoid function where the derivative is near zero <ref> [Fahlman, 1988, Fahlman & Lebiere, 1991] </ref>. In [Fahlman, 1988], several methods to combat the flat-spot problem are examined; adding an offset to the derivative of the sigmoid function to prevent it from approaching zero gave the best results.
Reference: [Knight, 1989] <author> Knight, Kevin, </author> <title> "A Gentle Introduction to Subsymbolic Computation: Connectionism for the A. I. </title> <type> Researcher", Technical Report CMU-CS-89-150, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <year> 1989. </year>
Reference: [Press et al., 1988] <author> Press, W. H., Flannery, B. P., Teukolsky, S. A., & Vetterling, W. T., </author> <title> Numerical Recipies in C, </title> <publisher> Cambridge University Press, </publisher> <address> New York, NY, </address> <year> 1988. </year>
Reference-contexts: Also, when using an approximation to the second order derivative, any difference between the approximation and the true derivative introduces some error into the weight update and it is well-known that any numerical estimation of higher-order derivatives is more error-prone than the lower order derivatives <ref> [Press et al., 1988] </ref>. 3.1.2 Momentum The use of a momentum term ff [Rumelhart et al., 1986] | a constant which determines the effect of past weight changes on the current direction of movement in weight space often helps alleviate the step size problem.
Reference: [Rumelhart et al., 1986] <author> Rumelhart, D. E., Hinton, G. E., and Wil liams, R. J. </author> <title> "Learning Internal Representations by Error Propagation", </title> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. I, </volume> <editor> Rumelhart D. E., and McClelland, J. L. (Eds.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: Recently, a different solution for the flat-spot problem has been proposed and experimentally evaluated [Balakrishnan & Honavar, 1992]. A common modification to the backpropogation algorithm which helps alleviate the step-size problem is to use a momentum term <ref> [Rumelhart et al., 1986] </ref>. In this paper we have tried a different approach to solve the step-size problem. Our approach is to dynamically adjust the learning rate at the end of each epoch, based on a heuristically guided local search. <p> exprimental methodology and the Data-sets used; Section 5 discusses the experiment results and their implications; Section 6 briefly addresses the computational complexity of the proposed modification; Section 7 concludes with a summary and directions for future research. 3 2 Backpropagation Learning Algorithm The backpropagation learning algorithm, or generalized delta rule <ref> [Rumelhart et al., 1986] </ref>, is a supervised learning algorithm for feedforward neural networks. A feedforward neural network consists of two or more layers of processing units. The lowest layer is the input layer, the uppermost layer is the output layer and the intermediate layers, if any, are called hidden layers. <p> order derivative, any difference between the approximation and the true derivative introduces some error into the weight update and it is well-known that any numerical estimation of higher-order derivatives is more error-prone than the lower order derivatives [Press et al., 1988]. 3.1.2 Momentum The use of a momentum term ff <ref> [Rumelhart et al., 1986] </ref> | a constant which determines the effect of past weight changes on the current direction of movement in weight space often helps alleviate the step size problem. The momentum term ff simply adds a fraction of the previous weight change to the current weight adjustment value.
Reference: [Shavlik et al., 1991] <author> Shavlik, Jude W., Mooney, Raymond J, and Towell, Geoffrey G., </author> <title> "Symbolic and Neural Learning Algorithms: An Experimental Comparison", </title> <journal> Machine Learning, </journal> <volume> Volume 6, </volume> <year> 1991. </year>
Reference-contexts: this section are to compare the performance of the adaptive eta modification to the back-propagation proposed in this paper witha number of variants of standard back-propagation (e.g, adding momentum, flatspot elimination etc) on a variety of datasets which have been used by other researchers for experiments reported in the literature <ref> [Yang & Honavar, 1991, Shavlik et al., 1991] </ref>. 4.1 Data-Sets used The data sets chosen are listed below along with the network architecture used and a description of the problem. <p> The output pattern isa 17-bit string with one bit set corresponding to the correct disease diagnosis. This is the same data set as used by <ref> [Shavlik et al., 1991] </ref>. Audiology Data: 87-11-24, It consists of 200 examples of the various disorders associated with the ear. It is from the Baylor College of Medicine. Since there were 24 categories possible, we had 24 ouput nodes. <p> Threshold and margin metric [Fahlman, 1988] The network is said to have correctly classified a training pattern if the actual network output is within a spec ified threshold for each of the components of the output vector. Highest output metric <ref> [Shavlik et al., 1991] </ref> Count an example as correctly classified if the node with the highest output corresponds to the correct category. Learning is said to be complete when a high percentage of the training examples are correctly classified.
Reference: [Yang & Honavar, 1991] <author> Yang, Jihoon, and Honavar, </author> <title> Vasant, </title> <booktitle> "Exper iments with the Cascade-Correlation Algorithm",Proceedings of the 4th UNB Artificial Intelligence Symposium, </booktitle> <address> Fredericton, NB, Canada, </address> <year> 1991. </year> . <title> Rule ff os Ave. </title> <address> Epochs % Converged Test % Speedup % BP + m 1.00 0.0 0.0 34.70 10.15 100 94.90 0 BP + o 0.25 0.0 0.1 30.0 0.0 100 94.00 13.5 BP + d,m,o 1.0 0.8 0.1 39.50 12.03 100 94.71 -13.8 Table 6: </address> <booktitle> Modification Interactions for the Chess data set 17 </booktitle>
Reference-contexts: this section are to compare the performance of the adaptive eta modification to the back-propagation proposed in this paper witha number of variants of standard back-propagation (e.g, adding momentum, flatspot elimination etc) on a variety of datasets which have been used by other researchers for experiments reported in the literature <ref> [Yang & Honavar, 1991, Shavlik et al., 1991] </ref>. 4.1 Data-Sets used The data sets chosen are listed below along with the network architecture used and a description of the problem. <p> two thirds of the dataset was chosen at random for training and the remainder was used as a test set to study generalization; the encoding of the dataset as well as the network architecture (i.e, the number of hidden units) was chosen to correspond exactly to the values reported in <ref> [Yang & Honavar, 1991] </ref>. 8-bit Encoder: 8-3-8, One of the eight input bits is turned on, and the system must learn to turn on the corresponding output bit. <p> Each example is a distributed representation of four plant attributes and the corresponding target output, where each output bit represents a diferent species. It is exactly the same data set used by <ref> [Yang & Honavar, 1991] </ref>. Two thirds of the data was chosen randomly as the training set, and the other one third was used as the test set to check the generalization performance of the rule. Soybean Classification: 208-23-17, This data set is also a real-world classification problem. <p> Audiology Data: 87-11-24, It consists of 200 examples of the various disorders associated with the ear. It is from the Baylor College of Medicine. Since there were 24 categories possible, we had 24 ouput nodes. The number of hidden units was picked up from <ref> [Yang & Honavar, 1991] </ref> so that we could make direct comparisons. Chess Data: 73-8-2, This data set consists of 3196 examples of a "king and rook vs king and pawn" end game which can be classified into two categories win and not win. <p> Each example has 36 attributes (of which one is three-valued and the rest are two-valued ). Thus, the original data-set was encoded with 73 input nodes and 2 output nodes. The number of hidden nodes were used as reported for standard BP in <ref> [Yang & Honavar, 1991] </ref>.
References-found: 10

