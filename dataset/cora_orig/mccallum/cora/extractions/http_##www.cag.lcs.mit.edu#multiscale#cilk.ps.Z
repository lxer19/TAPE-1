URL: http://www.cag.lcs.mit.edu/multiscale/cilk.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/multiscale/
Root-URL: 
Title: Cilk: An Efficient Multithreaded Runtime System  
Author: Robert D. Blumofe Christopher F. Joerg Bradley C. Kuszmaul Charles E. Leiserson Keith H. Randall Yuli Zhou 
Address: 545 Technology Square Cambridge, MA 02139  
Affiliation: MIT Laboratory for Computer Science  
Abstract: Cilk (pronounced silk) is a C-based runtime system for multi-threaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the work and critical path of a Cilk computation can be used to accurately model performance. Consequently, a Cilk programmer can focus on reducing the work and critical path of his computation, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of fully strict (well-structured) programs, the Cilk scheduler achieves space, time, and communication bounds all within a constant factor of optimal. The Cilk runtime system currently runs on the Connection Machine CM5 MPP, the Intel Paragon MPP, the Silicon Graphics Power Challenge SMP, and the MIT Phish network of workstations. Applications written in Cilk include protein folding, graphic rendering, backtrack search, and the ?Socrates chess program, which won third prize in the 1994 ACM International Computer Chess Championship. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Thomas E. Anderson, Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 95-109, </pages> <address> Pacific Grove, California, </address> <month> October </month> <year> 1991. </year>
Reference: [2] <author> Guy E. Blelloch. </author> <title> Programming parallel algorithms. </title> <booktitle> In Proceedings of the 1992 Dartmouth Institute for Advanced Graduate Studies (DAGS) Symposium on Parallel Computation, </booktitle> <pages> pages 11-18, </pages> <address> Hanover, New Hampshire, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Many of these applications pose problems for more traditional parallel environments, such as message passing [38] and data parallel <ref> [2, 20] </ref>, because of the unpredictability of the dynamic workloads on processors. Analytically, we prove that for fully strict (well-structured) programs, Cilk's work-stealing scheduler achieves execution space, time, and communication bounds all within a constant factor of optimal. <p> Cilk provides a programming model in which work and critical path are observable quantities, and it delivers guaranteed performance as a function of these quantities. Work and critical path have been used in the theory community for years to analyze parallel algorithms [26]. Blelloch <ref> [2] </ref> has developed a performance model for data-parallel computations based on these same two abstract measures. He cites many advantages to such a model over machine-based models. Cilk provides a similar performance model for the domain of asynchronous, multithreaded computation.
Reference: [3] <author> Robert D. Blumofe and Charles E. Leiserson. </author> <title> Scheduling mul-tithreaded computations by work stealing. </title> <booktitle> In Proceedings of the 35th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 356-368, </pages> <address> Santa Fe, New Mexico, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the scheduler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work-stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree. <p> Cilk's strategy is for thieves to choose victims at random <ref> [3, 27, 37] </ref>. At runtime, each processor maintains a local ready queue to hold ready closures. Each closure has an associated level, which corresponds to the number of spawn's (but not spawn next's) on the path from the root of the spawn tree. <p> This bound is existentially optimal to within a constant factor <ref> [3] </ref>. Time With P processors, the expected execution time, including scheduling overhead, is bounded by T P = O (T 1 =P + T 1 ). <p> Gray, curved edges represent the additional edges in D 0 that do not also belong to D. additive term in both cases. Proofs of these bounds use generalizations of the techniques developed in <ref> [3] </ref>. We defer complete proofs and give outlines here. The space bound follows from the busy-leaves property which characterizes the allocated closures at all times during the execution. <p> We now give the theorems bounding execution time and communication cost. Proofs for these theorems generalize the results of <ref> [3] </ref> for a more restricted model of multithreaded computation. As in [3], these proofs assume a communication model in which messages are delayed only by contention at destination processors, but no assumptions are made about the order in which contending messages are delivered [31]. <p> We now give the theorems bounding execution time and communication cost. Proofs for these theorems generalize the results of <ref> [3] </ref> for a more restricted model of multithreaded computation. As in [3], these proofs assume a communication model in which messages are delayed only by contention at destination processors, but no assumptions are made about the order in which contending messages are delivered [31]. The bounds given by these theorems assume that no thread has more than one successor thread. <p> The bounds given by these theorems assume that no thread has more than one successor thread. The proofs of these theorems are analogous to the proofs of Theorems 12 and 13 in <ref> [3] </ref>. We show that certain critical threads are likely to be executed after only a modest number of steal requests, and that executing a critical thread guarantees progress on the critical path of the dag. <p> Furthermore, for any * &gt; 0, the execution time is T P = O (T 1 =P + T 1 + lg P + lg (1=*)) with probability at least 1 *. Proof: This proof is just a straightforward application of the techniques in <ref> [3] </ref>, using our Lemma 2 as a substitute for Lemma 9 in [3]. Because the critical threads are first in the work-stealing order, they are likely to be stolen (or executed locally) after a modest number of steal requests. This fact can be shown formally using a delay sequence argument. <p> Proof: This proof is just a straightforward application of the techniques in <ref> [3] </ref>, using our Lemma 2 as a substitute for Lemma 9 in [3]. Because the critical threads are first in the work-stealing order, they are likely to be stolen (or executed locally) after a modest number of steal requests. This fact can be shown formally using a delay sequence argument. <p> Furthermore, for any * &gt; 0, the communication cost is O ((T 1 + lg (1=*))P S max ) with probability at least 1 *. Proof: This proof is exactly analogous to the proof of Theorem 13 in <ref> [3] </ref>. We observe that at most O (T 1 P ) steal attempts occur in an execution, and all communication costs can be associated with one of these steal requests such that at most O (S max ) communication is associated with each steal request.
Reference: [4] <author> Robert D. Blumofe and David S. Park. </author> <title> Scheduling large-scale parallel computations on networks of workstations. </title> <booktitle> In Proceedings of the Third International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 96-105, </pages> <address> San Francisco, Califor-nia, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: A Cilk program is preprocessed to C and then linked with a runtime library to run on the Connection Machine CM5 MPP, the Intel Paragon MPP, the Silicon Graphics Power Challenge SMP, or the MIT Phish <ref> [4] </ref> network of workstations. In this paper, we focus on the Connection Machine CM5 implementation of Cilk. The Cilk scheduler on the CM5 is written in about 30 pages of C, and it performs communication among processors using the Strata [6] active-message library.
Reference: [5] <author> Richard P. Brent. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> Journal of the ACM, </journal> <volume> 21(2) </volume> <pages> 201-206, </pages> <month> April </month> <year> 1974. </year>
Reference-contexts: The Cilk scheduler uses work stealing [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] to achieve execution time very near to the sum of these two measures. Off-line techniques for computing such efficient schedules have been known for a long time <ref> [5, 16, 17] </ref>, but this efficiency has been difficult to achieve on-line in a distributed environment while simultaneously using small amounts of space and communication. We demonstrate the efficiency of the Cilk scheduler both empirically and analytically.
Reference: [6] <author> Eric A. Brewer and Robert Blumofe. Strata: </author> <title> A multi-layer communications library. </title> <note> Technical Report to appear, </note> <institution> MIT Laboratory for Computer Science. </institution> <note> Available as ftp://ftp.lcs.mit.edu /pub/supertech/strata/strata.tar.Z. </note>
Reference-contexts: In this paper, we focus on the Connection Machine CM5 implementation of Cilk. The Cilk scheduler on the CM5 is written in about 30 pages of C, and it performs communication among processors using the Strata <ref> [6] </ref> active-message library. The remainder of this paper is organized as follows. Section 2 describes Cilk's runtime data structures and the C language extensions that are used for programming. Section 3 describes the work-stealing scheduler. Section 4 documents the performance of several Cilk applications.
Reference: [7] <author> F. Warren Burton and M. Ronan Sleep. </author> <title> Executing functional programs on a virtual tree of processors. </title> <booktitle> In Proceedings of the 1981 Conference on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 187-194, </pages> <address> Portsmouth, New Hampshire, </address> <month> October </month> <year> 1981. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the scheduler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work-stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [8] <author> Martin C. Carlisle, Anne Rogers, John H. Reppy, and Laurie J. Hendren. </author> <title> Early experiences with Olden. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year>
Reference: [9] <author> Rohit Chandra, Anoop Gupta, and John L. Hennessy. </author> <title> COOL: An object-based language for parallel programming. </title> <journal> IEEE Computer, </journal> <volume> 27(8) </volume> <pages> 13-26, </pages> <month> August </month> <year> 1994. </year>
Reference: [10] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. Lazowska, Henry M. Levy, and Richard J. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the Twelfth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <address> Litchfield Park, Arizona, </address> <month> December </month> <year> 1989. </year>
Reference: [11] <author> Eric C. Cooper and Richard P. Draves. </author> <title> C Threads. </title> <type> Technical Report CMU-CS-88-154, </type> <institution> School of Computer Science, Carnegie-Mellon University, </institution> <month> June </month> <year> 1988. </year>
Reference: [12] <author> David E. Culler, Anurag Sah, Klaus Erik Schauser, Thorsten von Eicken, and John Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled threaded abstract machine. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-175, </pages> <address> Santa Clara, California, </address> <month> April </month> <year> 1991. </year>
Reference: [13] <author> Rainer Feldmann, Peter Mysliwietz, and Burkhard Monien. </author> <title> Studying overheads in massively parallel min/max-tree evaluation. </title> <booktitle> In Proceedings of the Sixth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 94-103, </pages> <address> Cape May, New Jersey, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the scheduler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work-stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [14] <author> Raphael Finkel and Udi Manber. </author> <title> DIBa distributed implementation of backtracking. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(2) </volume> <pages> 235-256, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the scheduler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work-stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [15] <author> Vincent W. Freeh, David K. Lowenthal, and Gregory R. An-drews. </author> <title> Distributed Filaments: Efficient fine-grain parallelism on a cluster of workstations. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 201-213, </pages> <address> Monterey, California, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the scheduler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work-stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [16] <author> R. L. Graham. </author> <title> Bounds for certain multiprocessing anomalies. </title> <journal> The Bell System Technical Journal, </journal> <volume> 45 </volume> <pages> 1563-1581, </pages> <month> November </month> <year> 1966. </year>
Reference-contexts: The Cilk scheduler uses work stealing [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] to achieve execution time very near to the sum of these two measures. Off-line techniques for computing such efficient schedules have been known for a long time <ref> [5, 16, 17] </ref>, but this efficiency has been difficult to achieve on-line in a distributed environment while simultaneously using small amounts of space and communication. We demonstrate the efficiency of the Cilk scheduler both empirically and analytically.
Reference: [17] <author> R. L. Graham. </author> <title> Bounds on multiprocessing timing anomalies. </title> <journal> SIAM Journal on Applied Mathematics, </journal> <volume> 17(2) </volume> <pages> 416-429, </pages> <month> March </month> <year> 1969. </year>
Reference-contexts: The Cilk scheduler uses work stealing [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] to achieve execution time very near to the sum of these two measures. Off-line techniques for computing such efficient schedules have been known for a long time <ref> [5, 16, 17] </ref>, but this efficiency has been difficult to achieve on-line in a distributed environment while simultaneously using small amounts of space and communication. We demonstrate the efficiency of the Cilk scheduler both empirically and analytically.
Reference: [18] <author> Michael Halbherr, Yuli Zhou, and Chris F. Joerg. </author> <title> MIMD-style parallel programming with continuation-passing threads. </title> <booktitle> In Proceedings of the 2nd International Workshop on Massive Parallelism: Hardware, Software, and Applications, </booktitle> <address> Capri, Italy, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: Recent information about Cilk is maintained on the World Wide Web in page http://theory.lcs.mit.edu/cilk. Acknowledgments We gratefully acknowledge the inspiration of Michael Halbherr, now of the Boston Consulting Group in Zurich, Switzerland. Mike's PCM runtime system <ref> [18] </ref> developed at MIT was the precursor of Cilk, and many of the design decisions in Cilk are owed to him. We thank Shail Aditya and Sivan Toledo of MIT and Larry Rudolph of Hebrew University for helpful discussions.
Reference: [19] <author> Robert H. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> to achieve execution time very near to the sum of these two measures. <p> Each Cilk thread leaves the C runtime stack empty when it completes. Thus, Cilk can run on top of a vanilla C runtime system. A common alternative <ref> [19, 25, 32, 34] </ref> is to support a programming style in which a thread suspends whenever it discovers that required values have not yet been computed, resuming when the values become available. <p> Other features include various abilities to override the scheduler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work-stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [20] <author> W. Hillis and G. Steele. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Many of these applications pose problems for more traditional parallel environments, such as message passing [38] and data parallel <ref> [2, 20] </ref>, because of the unpredictability of the dynamic workloads on processors. Analytically, we prove that for fully strict (well-structured) programs, Cilk's work-stealing scheduler achieves execution space, time, and communication bounds all within a constant factor of optimal.
Reference: [21] <author> Wilson C. Hsieh, Paul Wang, and William E. Weihl. </author> <title> Computation migration: Enhancing locality for distributed-memory parallel systems. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 239-248, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference: [22] <author> Suresh Jagannathan and Jim Philbin. </author> <title> A customizable substrate for concurrent languages. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 55-67, </pages> <address> San Francisco, California, </address> <month> June </month> <year> 1992. </year>
Reference: [23] <author> Chris Joerg and Bradley C. Kuszmaul. </author> <title> Massively parallel chess. </title> <booktitle> In Proceedings of the Third DIMACS Parallel Implementation Challenge, </booktitle> <institution> Rutgers University, </institution> <address> New Jersey, </address> <month> October </month> <year> 1994. </year> <note> Available as ftp://theory.lcs.mit.edu /pub/cilk/dimacs94.ps.Z. </note>
Reference-contexts: Modifying the serial code to imitate the Cilk decomposition improved its performance. Timings for the improved version are given in the table. * ?Socrates is a parallel chess program that uses the Jamboree search algorithm <ref> [23, 29] </ref> to parallelize a minmax tree search.
Reference: [24] <author> L. V. Kal e. </author> <title> The Chare kernel parallel programming system. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, Volume II: Software, </booktitle> <pages> pages 17-25, </pages> <month> August </month> <year> 1990. </year>
Reference: [25] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Concertefficient run-time support for concurrent object-oriented programming languages on stock hardware. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 598-607, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Each Cilk thread leaves the C runtime stack empty when it completes. Thus, Cilk can run on top of a vanilla C runtime system. A common alternative <ref> [19, 25, 32, 34] </ref> is to support a programming style in which a thread suspends whenever it discovers that required values have not yet been computed, resuming when the values become available. <p> For example, the fib program executed over 17 million threads but migrated only 6170 (24.10 per processor) when run with 256 processors. Taking advantage of this property, other researchers <ref> [25, 32] </ref> have developed techniques for implementing spawns such that when the child thread executes on the same processor as its parent, the cost of the spawn operation is roughly equal the cost of a C function call. We hope to incorporate such techniques into future implementations of Cilk.
Reference: [26] <author> Richard M. Karp and Vijaya Ramachandran. </author> <title> Parallel algorithms for shared-memory machines. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer ScienceVolume A: Algorithms and Complexity, chapter 17, </booktitle> <pages> pages 869-941. </pages> <publisher> MIT Press, </publisher> <address> Cam-bridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Cilk provides a programming model in which work and critical path are observable quantities, and it delivers guaranteed performance as a function of these quantities. Work and critical path have been used in the theory community for years to analyze parallel algorithms <ref> [26] </ref>. Blelloch [2] has developed a performance model for data-parallel computations based on these same two abstract measures. He cites many advantages to such a model over machine-based models. Cilk provides a similar performance model for the domain of asynchronous, multithreaded computation.
Reference: [27] <author> Richard M. Karp and Yanjun Zhang. </author> <title> Randomized parallel algorithms for backtrack search and branch-and-bound computation. </title> <journal> Journal of the ACM, </journal> <volume> 40(3) </volume> <pages> 765-789, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the scheduler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work-stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree. <p> Cilk's strategy is for thieves to choose victims at random <ref> [3, 27, 37] </ref>. At runtime, each processor maintains a local ready queue to hold ready closures. Each closure has an associated level, which corresponds to the number of spawn's (but not spawn next's) on the path from the root of the spawn tree.
Reference: [28] <author> David A. Kranz, Robert H. Halstead, Jr., and Eric Mohr. Mul-T: </author> <title> A high-performance parallel Lisp. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 81-90, </pages> <address> Portland, Oregon, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the scheduler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work-stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [29] <author> Bradley C. Kuszmaul. </author> <title> Synchronized MIMD Computing. </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1994. </year> <note> Available as MIT Laboratory for Computer Science Technical Report MIT/LCS/TR-645 or ftp://theory.lcs.mit.edu /pub/bradley/phd.ps.Z. </note>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the scheduler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work-stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree. <p> Modifying the serial code to imitate the Cilk decomposition improved its performance. Timings for the improved version are given in the table. * ?Socrates is a parallel chess program that uses the Jamboree search algorithm <ref> [23, 29] </ref> to parallelize a minmax tree search. <p> On the other hand, the fib program has low efficiency, because the threads are so short: fib does almost nothing besides spawn and send argument. Despite it's long threads, the ?Socrates program has low efficiency, because its parallel Jamboree search algorithm <ref> [29] </ref> is based on speculatively searching subtrees that are not searched by a serial algorithm. Consequently, as we increase the number of processors, the program executes more threads and, hence, does more work.
Reference: [30] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Dou-glas, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, W. Daniel Hillis, Bradley C. Kuszmaul, Margaret A. St. Pierre, David S. Wells, Monica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The network architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <address> San Diego, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: We also present empirical evidence from experiments run on a CM5 supercomputer to document the efficiency of our work-stealing scheduler. The CM5 is a massively parallel computer based on 32MHz SPARC processors with a fat-tree interconnection network <ref> [30] </ref>. The applications are described below: * fib is the same as was presented in Section 2, except that the second recursive spawn is replaced by a tail call that avoids the scheduler.
Reference: [31] <author> Pangfeng Liu, William Aiello, and Sandeep Bhatt. </author> <title> An atomic model for message-passing. </title> <booktitle> In Proceedings of the Fifth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 154-163, </pages> <address> Velen, Germany, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: As in [3], these proofs assume a communication model in which messages are delayed only by contention at destination processors, but no assumptions are made about the order in which contending messages are delivered <ref> [31] </ref>. The bounds given by these theorems assume that no thread has more than one successor thread. The proofs of these theorems are analogous to the proofs of Theorems 12 and 13 in [3].
Reference: [32] <author> Eric Mohr, David A. Kranz, and Robert H. Halstead, Jr. </author> <title> Lazy task creation: A technique for increasing the granularity of parallel programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 264-280, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Each Cilk thread leaves the C runtime stack empty when it completes. Thus, Cilk can run on top of a vanilla C runtime system. A common alternative <ref> [19, 25, 32, 34] </ref> is to support a programming style in which a thread suspends whenever it discovers that required values have not yet been computed, resuming when the values become available. <p> For example, the fib program executed over 17 million threads but migrated only 6170 (24.10 per processor) when run with 256 processors. Taking advantage of this property, other researchers <ref> [25, 32] </ref> have developed techniques for implementing spawns such that when the child thread executes on the same processor as its parent, the cost of the spawn operation is roughly equal the cost of a C function call. We hope to incorporate such techniques into future implementations of Cilk.
Reference: [33] <author> Rishiyur S. Nikhil. </author> <title> A multithreaded implementation of Id using P-RISC graphs. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Languages and Compilers for Parallel Computing, number 768 in Lecture Notes in Computer Science, </booktitle> <pages> pages 390-405, </pages> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year> <note> Springer-Verlag. </note>
Reference: [34] <author> Rishiyur S. Nikhil. Cid: </author> <title> A parallel, shared-memory C for distributed-memory machines. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> to achieve execution time very near to the sum of these two measures. <p> Each Cilk thread leaves the C runtime stack empty when it completes. Thus, Cilk can run on top of a vanilla C runtime system. A common alternative <ref> [19, 25, 32, 34] </ref> is to support a programming style in which a thread suspends whenever it discovers that required values have not yet been computed, resuming when the values become available. <p> Other features include various abilities to override the scheduler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work-stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [35] <author> Vijay S. Pande, Christopher F. Joerg, Alexander Yu Grosberg, and Toyoichi Tanaka. </author> <title> Enumerations of the hamiltonian walks on a cubic sublattice. </title> <journal> Journal of Physics A, </journal> <volume> 27, </volume> <year> 1994. </year>
Reference-contexts: The Cilk program is based on serial code by R. Sargent of the MIT Media Laboratory. Thread length was enhanced by serializing the bottom 7 levels of the search tree. * pfold is a protein-folding program <ref> [35] </ref> written in conjunction with V. Pande of MIT's Center for Material Sciences and Engineering. This program finds hamiltonian paths in a three-dimensional grid using backtrack search. It was the first program to enumerate all hamiltonian paths in a 3 fi 4 fi 4 grid.
Reference: [36] <author> Martin C. Rinard, Daniel J. Scales, and Monica S. Lam. </author> <title> Jade: A high-level, machine-independent language for parallel programming. </title> <journal> Computer, </journal> <volume> 26(6) </volume> <pages> 28-38, </pages> <month> June </month> <year> 1993. </year>
Reference: [37] <author> Larry Rudolph, Miriam Slivkin-Allalouf, and Eli Upfal. </author> <title> A simple load balancing scheme for task allocation in parallel machines. </title> <booktitle> In Proceedings of the Third Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 237-245, </pages> <address> Hilton Head, South Carolina, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Cilk's strategy is for thieves to choose victims at random <ref> [3, 27, 37] </ref>. At runtime, each processor maintains a local ready queue to hold ready closures. Each closure has an associated level, which corresponds to the number of spawn's (but not spawn next's) on the path from the root of the spawn tree.
Reference: [38] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: To date, the applications we have programmed include protein folding, graphic rendering, backtrack search, and the ?Socrates chess program, which won third prize in the 1994 ACM International Computer Chess Championship. Many of these applications pose problems for more traditional parallel environments, such as message passing <ref> [38] </ref> and data parallel [2, 20], because of the unpredictability of the dynamic workloads on processors. Analytically, we prove that for fully strict (well-structured) programs, Cilk's work-stealing scheduler achieves execution space, time, and communication bounds all within a constant factor of optimal.
Reference: [39] <author> Andrew S. Tanenbaum, Henri E. Bal, and M. Frans Kaashoek. </author> <title> Programming a distributed system using shared objects. </title> <booktitle> In Proceedings of the Second International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 5-12, </pages> <address> Spokane, Wash-ington, </address> <month> July </month> <year> 1993. </year>
Reference: [40] <author> Mark T. Vandevoorde and Eric S. Roberts. WorkCrews: </author> <title> An abstraction for controlling parallelism. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(4) </volume> <pages> 347-366, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: With P processors, the execution time cannot be less than T 1 =P or less than T 1 . The Cilk scheduler uses work stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> to achieve execution time very near to the sum of these two measures. <p> Other features include various abilities to override the scheduler's decisions, including on which processor a thread should be placed and how to pack and unpack data when a closure is migrated from one processor to another. 3 The Cilk work-stealing scheduler Cilk's scheduler uses the technique of work-stealing <ref> [3, 7, 13, 14, 15, 19, 27, 28, 29, 34, 40] </ref> in which a processor (the thief) who runs out of work selects another processor (the victim) from whom to steal work, and then steals the shallowest ready thread in the victim's spawn tree.
Reference: [41] <author> I-Chen Wu and H. T. Kung. </author> <title> Communication complexity for parallel divide-and-conquer. </title> <booktitle> In Proceedings of the 32nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 151-162, </pages> <address> San Juan, Puerto Rico, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Communication The expected number of bytes communicated during a P -processor execution is O (T 1 P S max ), where S max denotes the largest size of any closure. This bound is existentially optimal to within a constant factor <ref> [41] </ref>.
References-found: 41

