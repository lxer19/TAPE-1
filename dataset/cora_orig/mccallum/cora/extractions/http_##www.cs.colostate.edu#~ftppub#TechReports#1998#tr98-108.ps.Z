URL: http://www.cs.colostate.edu/~ftppub/TechReports/1998/tr98-108.ps.Z
Refering-URL: http://www.cs.colostate.edu/~ftppub/
Root-URL: 
Phone: Phone: (970) 491-5792 Fax: (970) 491-2466  
Title: Visualisation System for the Image Understanding Environment  
Author: Karthik Balasubramaniam 
Web: WWW: http://www.cs.colostate.edu  
Address: Fort Collins, CO 80523-1873  
Affiliation: Computer Science Department Colorado State University  
Date: June 19, 1998  
Note: A 3D  
Pubnum: Technical Report CS-98-108  
Abstract: Computer Science Technical Report 
Abstract-found: 1
Intro-found: 1
Reference: [App68] <author> Arthur Appel. </author> <title> Some techniques for shading machine rendering of solids. </title> <booktitle> SJCC, </booktitle> <pages> pages 37-45, </pages> <year> 1968. </year>
Reference-contexts: A sample scene is rendered by each method and the generated images are compared to illustrate the differences between them. 3.1 Pinhole Ray Tracing The traditional ray tracing technique <ref> [App68, Gla89, Wil94b] </ref> uses a pinhole model as shown in Figure 6. The basic idea is to simulate the illumination-reflection process in the 3D world.
Reference: [BDHR91] <author> J. Ross Beveridge, Bruce A. Draper, Al Hanson, and Ed Riseman. </author> <title> Issues Central to a Useful Image Understanding Environment. </title> <booktitle> In The 20th AIPR Workshop, </booktitle> <pages> pages 21 - 30, </pages> <address> McLean, VA, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Section 6 describes the software architecture of the 3DCAMS system. 3DCAMS has been developed within the DARPA Image Understanding Environment (IUE) <ref> [Mea92, BDHR91] </ref>. The IUE is a C++ software development environment for computer vision and imaging research. 3DCAMS uses the IUE class libraries for representing and manipulating its spatial objects, and for 3D transformation operations. <p> Methods: - coplanar-calibration: Implements coplanar calibration using Tsai's algorithm as discussed earlier. - noncoplanar-calibration: Implements noncoplanar calibration using Tsai's algorithm as discussed earlier. 6.2 The Image Understanding Environment (IUE) The Image Understanding Environment (IUE) is DARPA's object oriented software development environment specifically designed for research in image understanding (IU) <ref> [Mea92, BDHR91, BDHR91, KLH + 94, KHL95, Joh96] </ref>. It's main purpose is the facilitation of sharing of research amongst the IU community. The IUE aims to achieve this goal by providing a standard set of C++ libraries for IU applications, and a standard data exchange mechanism.
Reference: [BW64] <author> Max Born and Emil Wolf. </author> <title> Principles of Optics. </title> <publisher> MacMillan, </publisher> <year> 1964. </year>
Reference-contexts: The lens must be treated as a thick lens. Thick lens approximations and equations may be found in standard optics textbooks <ref> [BW64, JW57, Lai91] </ref>. I summarise the salient concepts. The behaviour of a thick lens is modelled by its focal points and principal planes, which are illustrated in Figure 4. Paraxial rays approaching from object space converge at the secondary focal point F 00 after refraction through the lens.
Reference: [Coo86] <author> Robert L. Cook. </author> <title> Stochastic sampling in computer graphics. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 5(1) </volume> <pages> 51-72, </pages> <month> jan </month> <year> 1986. </year>
Reference-contexts: This method integrates depth of field with visible surface determination, and hence gives a more accurate solution to the depth of field problem than Potmesil's approach. Cook <ref> [CPC84, Coo86] </ref> originally named this technique distributed ray tracing.
Reference: [CPC84] <author> R.L. Cook, T. Porter, and L. Carpenter. </author> <title> Distributed ray tracing. </title> <journal> Computer Graphics, </journal> <volume> 18 </volume> <pages> 137-145, </pages> <year> 1984. </year>
Reference-contexts: The view of 3D object space is different from different points on the lens surface, and thus precludes a correct treatment of the hidden surface problem by a postprocessing approach. 3.4 Thin-Lens Ray Tracing A better solution to the depth of field problem was presented by Cook <ref> [CPC84] </ref>. Depth of field occurs because the lens is a finite size, with a nonzero aperture. For any point on the image plane, the computation of final perceived colour needs to integrate contributions from different points on the lens. <p> This method integrates depth of field with visible surface determination, and hence gives a more accurate solution to the depth of field problem than Potmesil's approach. Cook <ref> [CPC84, Coo86] </ref> originally named this technique distributed ray tracing.
Reference: [Fau93] <author> O.D. Faugeras. </author> <title> Three-Dimensional Computer Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: a wide-angle effect i.e, serves to widen the field of view. 18 (a) Model rendered using concavo-convex lens (b) Model rendered using double Gauss lens 4 Camera Calibration Camera calibration is the process of determining a camera's geometric and optical constants and it's pose in a 3D world coordinate system <ref> [Fau93, Kan93] </ref>. This process is essential for obtaining a mathematical model 19 of the camera's imaging process, which in turn allows one to * project 3D points in object space to 2D points in an image. * make 3D inferences from 2D points in an image (e.g. stereo).
Reference: [FD82] <author> J. D. Foley and A. Van Dam. </author> <title> Fundamentals of Interactive Computer Graphics. </title> <booktitle> The Systems Programming Series. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts, </address> <year> 1982. </year>
Reference-contexts: There is the notion of an abstract camera model, which is a simplification of the physical camera, reduced to a convenient mathematical model. Examples of abstract models include the pinhole model <ref> [FD82] </ref>, thin-lens and thick-lens approximations [JW57], and photogrammetric models [STH80]. These models usually lead to convenient closed form equations representing the imaging process. Then, there is the notion of the physical camera model, which typically involves physical simulation based on principles from optics and radiometry.
Reference: [Gla89] <author> Andrew S. Glassner. </author> <title> An Introduction to Ray Tracing. </title> <publisher> Academic Press, </publisher> <year> 1989. </year>
Reference-contexts: A sample scene is rendered by each method and the generated images are compared to illustrate the differences between them. 3.1 Pinhole Ray Tracing The traditional ray tracing technique <ref> [App68, Gla89, Wil94b] </ref> uses a pinhole model as shown in Figure 6. The basic idea is to simulate the illumination-reflection process in the 3D world. <p> This is followed by a discussion on reconstruction filtering, which deals with the computation of a final value using the multiple samples obtained. 3.9.1 Jittering a Uniform Grid This is a straightforward technique that is easy to implement, and is widely used <ref> [Wat92, Gla89, Wil94b] </ref>. A pixel is subdivided into a regular grid of subpixels. The centre of each subpixel is jittered, or displaced by a random offset.
Reference: [Joh96] <author> John Dolan and Charlie Kohl and Richard Lerner and Joseph Mundy and Terrance Boult and J. Ross Beveridge. </author> <title> Solving Diverse Image Understanding Problems Using the Image Understanding Environment. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 1481 - 1504, </pages> <address> Los Altos, CA, </address> <month> February </month> <year> 1996. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Methods: - coplanar-calibration: Implements coplanar calibration using Tsai's algorithm as discussed earlier. - noncoplanar-calibration: Implements noncoplanar calibration using Tsai's algorithm as discussed earlier. 6.2 The Image Understanding Environment (IUE) The Image Understanding Environment (IUE) is DARPA's object oriented software development environment specifically designed for research in image understanding (IU) <ref> [Mea92, BDHR91, BDHR91, KLH + 94, KHL95, Joh96] </ref>. It's main purpose is the facilitation of sharing of research amongst the IU community. The IUE aims to achieve this goal by providing a standard set of C++ libraries for IU applications, and a standard data exchange mechanism.
Reference: [JW57] <author> F.A. Jenkins and H.E. White. </author> <title> Fundamentals of Optics. </title> <publisher> McGraw-Hill, </publisher> <year> 1957. </year>
Reference-contexts: There is the notion of an abstract camera model, which is a simplification of the physical camera, reduced to a convenient mathematical model. Examples of abstract models include the pinhole model [FD82], thin-lens and thick-lens approximations <ref> [JW57] </ref>, and photogrammetric models [STH80]. These models usually lead to convenient closed form equations representing the imaging process. Then, there is the notion of the physical camera model, which typically involves physical simulation based on principles from optics and radiometry. <p> As shall be seen, these issues are relevant in accurately modelling the imaging process. Bearing these issues in mind, the discussion now shifts to the role of lenses in image formation. 2.2 Thin Lens Model A thin lens may be defined <ref> [JW57] </ref> as one whose thickness is small in comparison with the distances generally associated with its optical properties. Such distances are, for example, the primary and secondary focal lengths and object and image distances. Figure 2 illustrates the notion of focal points. <p> The distance between the centre of a lens and either of its focal points is its focal length. Image formation is illustrated in Figure 3. The position of the image of an object is given by <ref> [JW57] </ref> 1 = s 1 where s is the object distance, s 0 is the image distance, and f is the focal length, all measured from the centre of the lens. <p> The lens must be treated as a thick lens. Thick lens approximations and equations may be found in standard optics textbooks <ref> [BW64, JW57, Lai91] </ref>. I summarise the salient concepts. The behaviour of a thick lens is modelled by its focal points and principal planes, which are illustrated in Figure 4. Paraxial rays approaching from object space converge at the secondary focal point F 00 after refraction through the lens. <p> Similarly, paraxial rays from image space converge at the primary focal point F . 4 In either case, the intersection of incident and emergent rays defines a principal plane. There are thus two principal planes, primary and secondary, at which refraction is assumed to occur. It may be shown <ref> [JW57] </ref> that 1 + s 00 = f 1 Image formation is illustrated in Figure 5. The distances s and s 00 are measured from the principal points H and H 00 respectively. <p> This follows from the fact that the principal planes have unit positive lateral magnification with respect to each other. In other words, each is an upright identical image of the other. There are formulae <ref> [JW57] </ref> which may be used to compute the location of the principal planes, and the focal points. The image location may then be determined as illustrated in 5. <p> The ray is then directed towards the image, or conjugate point x, of x 00 . Ray tracing then proceeds in the usual manner. The problem of analytically finding principal plane representations for combinations of thick lenses is one of considerable difficulty <ref> [JW57] </ref>. Here ray tracing comes to the rescue. As illustrated in Figure 9, paraxial rays may be used to determine the location of the principal planes. One simply has to compute the intersection of the incident and emergent rays for this purpose. <p> The resulting defect is known as spherical aberration. Spherical aberration is measured as the distance by which the focus is displaced along the optical axis (Figure 11). The deviation of lens behaviour from Gaussian optics also gives rise to other types of lens aberration such as coma, and astigmatism <ref> [JW57] </ref>. I use results from lens aberration theory to model spherical aberration for thin lenses. <p> One may compute the thin lens focal length for this lens using the Lens Makers' Formula <ref> [JW57] </ref>: 1 = (n 1) 1 r 2 (20) where r 1 and r 2 are the radii of the lens surfaces from left to right, n is the refractive index of the lens medium, and f is the focal length.
Reference: [Kan93] <author> Kenichi Kanatani. </author> <title> Geometric Computation for Machine Vision. </title> <publisher> Oxford Science Publications, </publisher> <year> 1993. </year>
Reference-contexts: a wide-angle effect i.e, serves to widen the field of view. 18 (a) Model rendered using concavo-convex lens (b) Model rendered using double Gauss lens 4 Camera Calibration Camera calibration is the process of determining a camera's geometric and optical constants and it's pose in a 3D world coordinate system <ref> [Fau93, Kan93] </ref>. This process is essential for obtaining a mathematical model 19 of the camera's imaging process, which in turn allows one to * project 3D points in object space to 2D points in an image. * make 3D inferences from 2D points in an image (e.g. stereo).
Reference: [KHL95] <author> C. Kohl, J.J. Hunter, and C. Loiselle. </author> <title> Towards a unified image understanding environment. </title> <booktitle> Proc. 5th International Conference on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: Methods: - coplanar-calibration: Implements coplanar calibration using Tsai's algorithm as discussed earlier. - noncoplanar-calibration: Implements noncoplanar calibration using Tsai's algorithm as discussed earlier. 6.2 The Image Understanding Environment (IUE) The Image Understanding Environment (IUE) is DARPA's object oriented software development environment specifically designed for research in image understanding (IU) <ref> [Mea92, BDHR91, BDHR91, KLH + 94, KHL95, Joh96] </ref>. It's main purpose is the facilitation of sharing of research amongst the IU community. The IUE aims to achieve this goal by providing a standard set of C++ libraries for IU applications, and a standard data exchange mechanism.
Reference: [KLH + 94] <author> C. Kohl, R. Lerner, A. Hough, C. Loiselle, J.Dolan, M. Friedman, and M. Roubentchik. </author> <title> A stellar application of the image understanding environment: Solar feature extraction. </title> <booktitle> Proc. ARPA IUW, </booktitle> <year> 1994. </year>
Reference-contexts: Methods: - coplanar-calibration: Implements coplanar calibration using Tsai's algorithm as discussed earlier. - noncoplanar-calibration: Implements noncoplanar calibration using Tsai's algorithm as discussed earlier. 6.2 The Image Understanding Environment (IUE) The Image Understanding Environment (IUE) is DARPA's object oriented software development environment specifically designed for research in image understanding (IU) <ref> [Mea92, BDHR91, BDHR91, KLH + 94, KHL95, Joh96] </ref>. It's main purpose is the facilitation of sharing of research amongst the IU community. The IUE aims to achieve this goal by providing a standard set of C++ libraries for IU applications, and a standard data exchange mechanism.
Reference: [KMH95] <author> Craig Kolb, Don Mitchell, and Pat Hanrahan. </author> <title> A realistic camera model for computer graphics. </title> <journal> Computer Graphics, </journal> <volume> 29 </volume> <pages> 317-324, </pages> <year> 1995. </year>
Reference-contexts: In the following section the focus shifts from abstract modelling to physical modelling. A physical simulation of a lens system may be achieved by tracing ray paths through the lenses <ref> [KMH95] </ref>, applying some of the principles discussed in the earlier sections. The number of lenses in most lens systems usually being small, this should prove to be comparable in expense to regular ray tracing. <p> The column titled R:I: denotes the refractive index of the material on the side of the surface facing image space. The last entry is the diameter of each lens element, or its aperture. The lens system in Table 1 is an example of a double-Gauss lens <ref> [KMH95, Smi92] </ref>, this version being used in 35mm cameras. This lens system is shown in Figure 9. 3.6 Ray Tracing a Lens System A robust and accurate method of simulating image formation by lenses is to trace light rays through the system [KMH95]. <p> This lens system is shown in Figure 9. 3.6 Ray Tracing a Lens System A robust and accurate method of simulating image formation by lenses is to trace light rays through the system <ref> [KMH95] </ref>. The propagation of a ray through a lens system involves both finding the point of intersection between the ray and the surface and the refraction of the ray as it crosses the interface between the two media (usually glass and air). <p> front: p = intersection (R; E i ) if (p is outside aperture of E i ) R is blocked, discard R else test for refraction compute new direction of R 11 3.7 Using a Thick Lens Approximation Cook's distribution raytracing may be extended to use a thick lens approximation <ref> [KMH95] </ref> instead of a thin lens approximation. Figure 10 illustrates the procedure for using a thick lens approximation with distribution ray tracing. The thick lens image formation principles discussed earlier are put to use here. <p> Shirley [Shi91] suggests mapping concentric squares to concentric circles. This mapping is illustrated in Figure 13, and for one wedge of the square, is defined by: x 0 = 2x 1; y 0 = 2y 1; r = y 0 ; = y 0 (18) An alternative mapping <ref> [KMH95] </ref> takes subrectangles [0; x] fi [0; 1] to a chord on the disc with area proportional to x, as illustrated in Figure 14. 14 3.9.4 Reconstruction Filtering The reconstruction filter interpolates samples (obtained using an appropriate supersampling pattern such as the ones described above) to recreate a continuous image (signal).
Reference: [Lai91] <author> M. Laikin. </author> <title> Lens Design. </title> <publisher> Marcel Dekker, </publisher> <year> 1991. </year>
Reference-contexts: The lens must be treated as a thick lens. Thick lens approximations and equations may be found in standard optics textbooks <ref> [BW64, JW57, Lai91] </ref>. I summarise the salient concepts. The behaviour of a thick lens is modelled by its focal points and principal planes, which are illustrated in Figure 4. Paraxial rays approaching from object space converge at the secondary focal point F 00 after refraction through the lens.
Reference: [Mea92] <editor> J. Mundy and et. al. </editor> <booktitle> The image understanding environments program. In Proceedings: IEEE 1992 Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 185 - 214, </pages> <address> San Mateo, CA, January 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Section 6 describes the software architecture of the 3DCAMS system. 3DCAMS has been developed within the DARPA Image Understanding Environment (IUE) <ref> [Mea92, BDHR91] </ref>. The IUE is a C++ software development environment for computer vision and imaging research. 3DCAMS uses the IUE class libraries for representing and manipulating its spatial objects, and for 3D transformation operations. <p> Methods: - coplanar-calibration: Implements coplanar calibration using Tsai's algorithm as discussed earlier. - noncoplanar-calibration: Implements noncoplanar calibration using Tsai's algorithm as discussed earlier. 6.2 The Image Understanding Environment (IUE) The Image Understanding Environment (IUE) is DARPA's object oriented software development environment specifically designed for research in image understanding (IU) <ref> [Mea92, BDHR91, BDHR91, KLH + 94, KHL95, Joh96] </ref>. It's main purpose is the facilitation of sharing of research amongst the IU community. The IUE aims to achieve this goal by providing a standard set of C++ libraries for IU applications, and a standard data exchange mechanism.
Reference: [Mit87] <author> D.P. Mitchell. </author> <title> Generating antialiased images at low sampling densities. </title> <journal> Computer Graphics, </journal> <volume> 21 </volume> <pages> 65-72, </pages> <year> 1987. </year>
Reference-contexts: In this technique, the sampling pattern is random, but with the constraint that no two sample points may be closer than a certain minimum distance (each point is treated as an inviolable disc) <ref> [Rip77, Mit87] </ref>. This point distribution has the characteristic of concentrating aliasing noise energy in the higher frequencies, thus facilitating its removal through low pass filtering [Mit91]. This concept is illustrated in Figure 12. The ! x and ! y axes denote the axes in the 2D frequency domain. <p> The reconstruction and low pass filters can be combined into a single filter <ref> [Mit87] </ref>. One such reconstruction 15 filter is the Gaussian filter given by e d 2 (19) where d is the distance from the centre of the sampling region to the centre of the pixel, and w=1.5 is the filter width, beyond which the filter is set to zero.
Reference: [Mit91] <author> D.P. Mitchell. </author> <title> Spectrally optimal sampling for distribution ray tracing. </title> <journal> Computer Graphics, </journal> <volume> 25 </volume> <pages> 157-164, </pages> <year> 1991. </year>
Reference-contexts: This point distribution has the characteristic of concentrating aliasing noise energy in the higher frequencies, thus facilitating its removal through low pass filtering <ref> [Mit91] </ref>. This concept is illustrated in Figure 12. The ! x and ! y axes denote the axes in the 2D frequency domain. The spectrum F of the sampling pattern S will be in the form of a spike at the origin surrounded by a noisy distribution.
Reference: [PC81] <author> M. Potmesil and I. Chakravarthy. </author> <title> A lens and aperture camera model for synthetic image gener ation. </title> <journal> Computer Graphics, </journal> <volume> 15 </volume> <pages> 297-305, </pages> <year> 1981. </year> <month> 39 </month>
Reference-contexts: This effect is known as depth of field. Depth of field can be an unwanted artifact in real-world photography, but it is an effect that must be taken into consideration for accurate camera modelling and image synthesis. 3.3 A Post-Processing Approach Potmesil <ref> [PC81] </ref> simulated depth of field using a postprocessing technique. The 3D scene is first rendered with a pinhole camera model. All objects are thus in sharp focus. Later, each imaged object point is convolved with a filter whose size is the circle of confusion. <p> From 8, one may see that points, on a plane that is a distance s from the lens, will focus at an image plane distance s 0 , given by s 0 = s f The diameter of the circle of confusion is given by <ref> [PC81] </ref> C = f jd where d is the diameter of the lens. An examination of this equation reveals something that one's geometric intuition readily confirms. As the image plane is moved away from the position given by s 0 f , the circle of confusion grows larger.
Reference: [Rip77] <author> B.D. Ripley. </author> <title> Modeling spatial patterns. </title> <journal> Journal of the Royal Statistical Society., </journal> <volume> B-39:172-212, </volume> <year> 1977. </year>
Reference-contexts: In this technique, the sampling pattern is random, but with the constraint that no two sample points may be closer than a certain minimum distance (each point is treated as an inviolable disc) <ref> [Rip77, Mit87] </ref>. This point distribution has the characteristic of concentrating aliasing noise energy in the higher frequencies, thus facilitating its removal through low pass filtering [Mit91]. This concept is illustrated in Figure 12. The ! x and ! y axes denote the axes in the 2D frequency domain.
Reference: [Shi91] <author> P. Shirley. </author> <title> Discrepancy as a measure for sample distributions. </title> <booktitle> Eurographics, </booktitle> <pages> pages 183-193, </pages> <year> 1991. </year>
Reference-contexts: The obvious mapping is r = u; = 2v (17) This mapping, however, exhibits severe distortion. One requires a mapping that takes uniformly distributed points on the square to uniformly distributed points on the disc. Shirley <ref> [Shi91] </ref> suggests mapping concentric squares to concentric circles.
Reference: [Smi92] <author> W.J. Smith. </author> <title> Modern Lens Design. </title> <publisher> McGraw Hill, </publisher> <year> 1992. </year>
Reference-contexts: The column titled R:I: denotes the refractive index of the material on the side of the surface facing image space. The last entry is the diameter of each lens element, or its aperture. The lens system in Table 1 is an example of a double-Gauss lens <ref> [KMH95, Smi92] </ref>, this version being used in 35mm cameras. This lens system is shown in Figure 9. 3.6 Ray Tracing a Lens System A robust and accurate method of simulating image formation by lenses is to trace light rays through the system [KMH95].
Reference: [STH80] <author> C. Slama, C. Theurer, and S. Henriksen. </author> <title> Manual of Photogrammetry, Fourth Edition. </title> <journal> American Society of Photogrammetry, </journal> <year> 1980. </year>
Reference-contexts: There is the notion of an abstract camera model, which is a simplification of the physical camera, reduced to a convenient mathematical model. Examples of abstract models include the pinhole model [FD82], thin-lens and thick-lens approximations [JW57], and photogrammetric models <ref> [STH80] </ref>. These models usually lead to convenient closed form equations representing the imaging process. Then, there is the notion of the physical camera model, which typically involves physical simulation based on principles from optics and radiometry.
Reference: [Tsa87] <author> R.Y. Tsai. </author> <title> A versatile camera calibration technique for high-accuracy 3d machine vision using off-the-shelf tv cameras and lenses. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> 4 </volume> <pages> 323-344, </pages> <year> 1987. </year>
Reference-contexts: The process of calibration determines a best-fit abstract model given a set of 3D-2D point correspondences. Section 4 on camera calibration focuses on a widely used calibration technique; Tsai's algorithm <ref> [Tsa87] </ref>. A significant component of this thesis work is a 3D visualisation system, which allows a user to interact with different types of camera models. A suite of camera modelling, camera manipulation and rendering routines are implemented in an interactive 3D visualisation system, 3DCAMS (3D Camera Modeller and Simulator). <p> Sensor element parameters are usually known reliably. The information is normally supplied by manufacturers of CCD cameras. For instance, the Photometrics Star I camera [Wil95] has d x = d y = 0:023 mm/sensor-element. Tsai's algorithm <ref> [Tsa87] </ref>, calibrates a camera represented by this model, sometimes referred to as Tsai's camera model [Wil94a]. <p> r 2 y w + r 3 z w + T x (40) y Y + d y Y 1 r 2 = f r 7 x w + r 8 y w + r 9 z w + T z 4.2 The Algorithm This section summarises Tsai's algorithm from <ref> [Tsa87] </ref>. The following section deals with the radial alignment constraint which is the distinguishing idea of this technique. <p> A detailed proof may be found in <ref> [Tsa87] </ref>. 4.2.2 Calibration Using a Coplanar Set of Points It is convenient to use a coplanar set of data points. This arises from the ease of physically fabricating a calibration target containing coplanar data points, whose world coordinates are accurately known. The calibration procedure may be outlined as follows: 1. <p> Details may be found in <ref> [Tsa87] </ref>. 2. Compute effective focal length, distortion coefficient, and z translation. (a) Compute approximate f and T z by ignoring lens distortion. <p> (c) Compute (r 1 ; :::; r 9 ; T x ; T y ; s x ) from the previously determined s x T y r 2 ; s x T y T x ; r 4 T y ; r 6 The detailed derivation may be found in <ref> [Tsa87] </ref>. 2.
Reference: [Wat92] <editor> C.D. Watkins. Photorealism and Ray Tracing in C. M and T, </editor> <year> 1992. </year>
Reference-contexts: This is followed by a discussion on reconstruction filtering, which deals with the computation of a final value using the multiple samples obtained. 3.9.1 Jittering a Uniform Grid This is a straightforward technique that is easy to implement, and is widely used <ref> [Wat92, Gla89, Wil94b] </ref>. A pixel is subdivided into a regular grid of subpixels. The centre of each subpixel is jittered, or displaced by a random offset.
Reference: [Wil94a] <author> R.G. Wilson. </author> <title> Modeling and Calibration of Automated Zoom Lenses. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon Uniiversity, </institution> <year> 1994. </year>
Reference-contexts: The information is normally supplied by manufacturers of CCD cameras. For instance, the Photometrics Star I camera [Wil95] has d x = d y = 0:023 mm/sensor-element. Tsai's algorithm [Tsa87], calibrates a camera represented by this model, sometimes referred to as Tsai's camera model <ref> [Wil94a] </ref>.
Reference: [Wil94b] <author> Nicholas Wilt. </author> <title> Object Oriented Ray Tracing. </title> <publisher> John Wiley and Sons, </publisher> <year> 1994. </year>
Reference-contexts: A sample scene is rendered by each method and the generated images are compared to illustrate the differences between them. 3.1 Pinhole Ray Tracing The traditional ray tracing technique <ref> [App68, Gla89, Wil94b] </ref> uses a pinhole model as shown in Figure 6. The basic idea is to simulate the illumination-reflection process in the 3D world. <p> This is followed by a discussion on reconstruction filtering, which deals with the computation of a final value using the multiple samples obtained. 3.9.1 Jittering a Uniform Grid This is a straightforward technique that is easy to implement, and is widely used <ref> [Wat92, Gla89, Wil94b] </ref>. A pixel is subdivided into a regular grid of subpixels. The centre of each subpixel is jittered, or displaced by a random offset.
Reference: [Wil95] <author> R.G. Willson. </author> <title> Freeware Implementation of Roger Tsai's Calibration Algorithm. </title> <note> http://www.cs.cmu.edu/People/rgw/TsaiCode.html, 1995. </note>
Reference-contexts: This arises due to a variety of factors, such as timing mismatches between acquisition and scanning hardware, or timing imprecision of sampling. Sensor element parameters are usually known reliably. The information is normally supplied by manufacturers of CCD cameras. For instance, the Photometrics Star I camera <ref> [Wil95] </ref> has d x = d y = 0:023 mm/sensor-element. Tsai's algorithm [Tsa87], calibrates a camera represented by this model, sometimes referred to as Tsai's camera model [Wil94a]. <p> Basic coplanar calibration requires at least five data points. Basic noncoplanar calibration requires at least seven data points. Nonlinear optimization for these camera calibration routines is performed using a modified Levenberg-Marquardt algorithm <ref> [Wil95] </ref>. To accurately estimate radial lens distortion and image centre parameters, the calibration data should be distributed broadly across the field of view. The distribution of data points should, if possible, span the range of depths that one expects [Wil95]. <p> these camera calibration routines is performed using a modified Levenberg-Marquardt algorithm <ref> [Wil95] </ref>. To accurately estimate radial lens distortion and image centre parameters, the calibration data should be distributed broadly across the field of view. The distribution of data points should, if possible, span the range of depths that one expects [Wil95]. To be able to separate the effects of f and T z on the image there needs to be adequate perspective distortion in the calibration data. <p> For adequate perspective distortion, the distance between the calibration points nearest and farthest from the camera should be on the same scale as the distance between the calibration points and the camera <ref> [Wil95] </ref>. This applies both to coplanar and noncoplanar calibration. Experiments indicate that calibration is sensitive to these data distribution factors, especially so when a modest number of data points are used. 26 5 User's Overview of 3DCAMS 3DCAMS is an interactive 3D visualisation system. <p> An interactive camera calibration component based on Tsai's algorithm is included in the system. The calibration routines are based on freely available code <ref> [Wil95] </ref>. For basic coplanar calibration using Tsai's technique, at least 5 correspondences must be supplied. This will not allow recovery of scaling s x .
Reference: [Yel83] <author> J.I. Yellott. </author> <title> Spectral consequences of photoreceptor sampling in the rhesus retina. </title> <journal> Science, </journal> <volume> 221 </volume> <pages> 382-385, </pages> <year> 1983. </year>
Reference-contexts: Frequencies higher than the Nyquist limit cause aliasing. When a uniform sampling grid is used, these high frequencies appear as objectionable artifacts. If the sample grid is nonuniform, the high frequencies may appear as noise, which is less objectionable than aliasing artifacts <ref> [Yel83] </ref>. Techniques for nonuniform stochastic (probabilistic) sampling are described below.
References-found: 29

