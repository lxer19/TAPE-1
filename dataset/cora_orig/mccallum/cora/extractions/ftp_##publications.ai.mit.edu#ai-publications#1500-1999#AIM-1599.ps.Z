URL: ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1599.ps.Z
Refering-URL: http://www.mpik-tueb.mpg.de/people/personal/bs/svm.html
Root-URL: 
Title: Comparing Support Vector Machines with Gaussian Kernels to Radial Basis Function Classifiers  
Author: B. Scholkopf, K. Sung, C. Burges, F. Girosi, P. Niyogi, T. Poggio, V. Vapnik 
Note: This publication can be retrieved by anonymous ftp to publications.ai.mit.edu. Copyright c Massachusetts Institute of Technology, 1996  
Date: 1599 December, 1996  142  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES  
Pubnum: A.I. Memo No.  C.B.C.L. Paper No.  
Abstract: The Support Vector (SV) machine is a novel type of learning machine, based on statistical learning theory, which contains polynomial classifiers, neural networks, and radial basis function (RBF) networks as special cases. In the RBF case, the SV algorithm automatically determines centers, weights and threshold such as to minimize an upper bound on the expected test error. The present study is devoted to an experimental comparison of these machines with a classical approach, where the centers are determined by k-means clustering and the weights are found using error backpropagation. We consider three machines, namely a classical RBF machine, an SV machine with Gaussian kernel, and a hybrid system with the centers determined by the SV method and the weights trained by error backpropagation. Our results show that on the US postal service database of handwritten digits, the SV machine achieves the highest test accuracy, followed by the hybrid approach. The SV approach is thus not only theoretically well-founded, but also superior in a practical application. This report describes research done at the Center for Biological and Computational Learning, the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology, and at AT&T Bell Laboratories (now AT&T Research, and Lucent Technologies Bell Laboratories). Support for the Center is provided in part by a grant from the National Science Foundation under contract ASC-9217041. BS thanks the M.I.T. for hospitality during a three-week visit in March 1995, where this work was started. At the time of the study, BS, CB, and VV were with AT&T Bell Laboratories, NJ; KS, FG, PN, and TP were with the Massachusetts Institute of Technology. KS is now with the Department of Information Systems and Computer Science at the National University of Singapore, Lower Kent Ridge Road, Singapore 0511; CB and PN are with Lucent Technologies, Bell Laboratories, NJ; VV is with AT&T Research, NJ. BS was supported by the Studienstiftung des deutschen Volkes; CB was supported by ARPA under ONR contract number N00014-94-C-0186. We thank A. Smola for useful discussions. Please direct correspondence to Bernhard Scholkopf, bs@mpik-tueb.mpg.de, Max-Planck-Institut fur biologische Kybernetik, Spemannstr. 38, 72076 Tubingen, Germany. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Aizerman, M.; Braverman, E.; and Rozonoer, L. </author> <year> 1964. </year> <title> Theoretical foundations of the potential function method in pattern recognition learning. </title> <journal> Automation and Remote Control, </journal> <volume> 25 </volume> <pages> 821-837. </pages>
Reference: [2] <author> Boser, B. E.; Guyon, I. M.; and Vapnik, V. </author> <year> 1992. </year> <title> A training algorithm for optimal margin classifiers. </title> <booktitle> Fifth Annual Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh ACM 144-152. </address>
Reference: [3] <author> Burges, C. J. C. </author> <year> 1996. </year> <title> Simplified Support Vector Decision Rules. </title> <booktitle> 13th International Conference on Machine Learning. </booktitle>
Reference: [4] <author> Burges, C.J.C., and Scholkopf, B. </author> <year> 1996. </year> <title> Improving the Accuracy and Speed of Support Vector Machines. To appear in: </title> <booktitle> Advances in Neural Information Processing Systems 9. </booktitle>
Reference: [5] <author> Cortes, C.; and Vapnik, V. </author> <year> 1995. </year> <title> Support Vector Networks. </title> <journal> Machine Learning, </journal> <volume> 20:273 - 297. </volume>
Reference: [6] <author> Le Cun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., Jackel, L. J. </author> <year> 1989. </year> <title> Backpropagation applied to handwritten zip code recognition. </title> <booktitle> Neural Computation 1: </booktitle> <volume> 541 - 551. </volume>
Reference: [7] <author> Lloyd, S. P. </author> <year> 1982. </year> <title> Least squares quantization in PCM. </title> <journal> IEEE Trans. </journal> <note> Information Theory IT-28:129 - 137. </note>
Reference: [8] <author> Poggio, T., & Girosi, F. </author> <year> 1990. </year> <title> Networks for approximation and learning. </title> <journal> Proc. IEEE, </journal> <volume> 78:1481 - 1497. </volume>
Reference: [9] <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> 1986. </year> <title> Learning representations by back-propagating errors. </title> <booktitle> Nature, </booktitle> <address> 323:533 - 536. </address>
Reference: [10] <author> Scholkopf, B.; Burges, C.; and Vapnik, V. </author> <year> 1995. </year> <title> Extracting support data for a given task. </title> <editor> In: Fayyad, U. M., and Uthurusamy, R. (eds.): </editor> <booktitle> Proceedings, First International Conference on Knowledge Discovery and Data Mining, </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA. </address>
Reference: [11] <author> Scholkopf, B., Burges, C.J.C., Vapnik, V. </author> <year> 1996. </year> <title> Incorporating Invariances in Support Vector Learning Machines. </title> <editor> In C. von der Malsburg, W. von Seelen, J. C. Vorbruggen, and B. Sendhoff, editors, </editor> <booktitle> Artificial Neural Networks | ICANN'96, </booktitle> <pages> pages 47 - 52, </pages> <address> Berlin. </address> <booktitle> (Springer Lecture Notes in Computer Science, </booktitle> <volume> Vol. </volume> <pages> 1112.) </pages>
Reference: [12] <author> Sung, K. </author> <year> 1995. </year> <title> Learning and Example Selection for Object and Pattern Detection. </title> <type> Ph.D. Thesis, </type> <institution> Mas-sachusetts Institute of Technology. </institution>
Reference: [13] <author> Vapnik, V. </author> <year> 1979. </year> <title> Estimation of Dependences Based on Empirical Data, [in Russian] Nauka, Moscow; English translation: </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference: [14] <author> Vapnik, V. </author> <year> 1995. </year> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer Verlag, </publisher> <address> New York. </address> <month> 6 </month>
References-found: 14

