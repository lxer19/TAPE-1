URL: ftp://ftp.cs.toronto.edu/pub/zoubin/factorial.ps.Z
Refering-URL: http://www.cs.utoronto.ca/~zoubin/
Root-URL: 
Email: zoubin@psyche.mit.edu  
Title: Factorial Learning and the EM Algorithm  
Author: Zoubin Ghahramani 
Note: In G. Tesauro, D.S. Touretzky and T.K. Leen (eds.), Advances in Neural Information Processing Systems 7. pp. 617-624. MIT Press,  
Address: Cambridge, MA 02139  Cambridge, MA, 1995.  
Affiliation: Department of Brain Cognitive Sciences Massachusetts Institute of Technology  
Abstract: Many real world learning problems are best characterized by an interaction of multiple independent causes or factors. Discovering such causal structure from the data is the focus of this paper. Based on Zemel and Hinton's cooperative vector quantizer (CVQ) architecture, an unsupervised learning algorithm is derived from the Expectation-Maximization (EM) framework. Due to the combinatorial nature of the data generation process, the exact E-step is computationally intractable. Two alternative methods for computing the E-step are proposed: Gibbs sampling and mean-field approximation, and some promising empirical results are presented. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ackley, D., Hinton, G., and Sejnowski, T. </author> <year> (1985). </year> <title> A learning algorithm for Boltzmann machines. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 147-169. </pages>
Reference-contexts: This constant partition function results in desirable properties, such as the lack of a Boltzmann machine-like sleep phase (Neal, 1992), which we will exploit in the learning algorithm. The system described by equation (1) 1 can be thought of as a special form of the Boltzmann machine <ref> (Ackley et al., 1985) </ref>. Expanding out the quadratic term we see that there are pairwise interaction terms between every unit. The evaluation of the partition function (3) tells us that when y is unclamped the quadratic term can be integrated out and therefore all s i are independent.
Reference: <author> Barlow, H. </author> <year> (1989). </year> <title> Unsupervised learning. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 295-311. </pages>
Reference: <author> Baum, L., Petrie, T., Soules, G., and Weiss, N. </author> <year> (1970). </year> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> The Annals of Mathematical Statistics, </journal> <volume> 41 </volume> <pages> 164-171. </pages>
Reference-contexts: Within each vector quantizer a competitive learning mechanism operates to select an appropriate vector code to describe the input. The CVQ is related to algorithms based on mixture models, such as soft competitive clustering, mixtures of experts (Jordan and Jacobs, 1994), and hidden Markov models <ref> (Baum et al., 1970) </ref>, in that each vector quantizer in the CVQ is itself a mixture model. However, it generalizes this notion by allowing the mixture models to cooperate in describing features in the data set, thereby creating a distributed representations of the mixture components.
Reference: <author> Dempster, A., Laird, N., and Rubin, D. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statistical Society Series B, </journal> <volume> 39 </volume> <pages> 1-38. </pages>
Reference-contexts: In this paper we first formulate the factorial learning problem in the framework of statistical physics (section 2). Through this formalism, we derive a novel learning algorithm for the CVQ based on the Expectation-Maximization (EM) algorithm <ref> (Dempster et al., 1977) </ref> (section 3). The exact EM algorithm is intractable for this and related factorial learning problems|however, a tractable mean-field approximation can be derived. <p> This chicken-and-egg problem can be solved by iterating between computing the expectation of the hidden causes given the current weights and maximizing the likelihood of the weights given these expected causes|the two steps forming the basis of the Expectation-Maximization (EM) algorithm <ref> (Dempster et al., 1977) </ref>.
Reference: <author> Geman, S. and Geman, D. </author> <year> (1984). </year> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 721-741. </pages>
Reference-contexts: As with Boltzmann machines, the CVQ architecture lends itself well to Gibbs sampling <ref> (Geman and Geman, 1984) </ref>. Starting from a clamped observable y and a random setting of the hidden units fs j g, the setting of each vector is updated in turn stochastically according to its conditional distribution s i ~ p (s i jy; fs j g j6=i ; W ).
Reference: <author> Ghahramani, Z. </author> <year> (1995). </year> <title> Factorial learning and the EM algorithm. </title> <publisher> MIT Computational Cognitive Science TR 9501. </publisher>
Reference-contexts: Simulation results on the Gibbs and mean-field EM algorithms for factorial HMMs are also promising <ref> (Ghahramani, 1995) </ref>. 7 mean-field EM algorithms for the lines data. Each data point shows the mean squared training error averaged over 10 runs of 20 EM steps, with standard error bars.
Reference: <author> Hinton, G. and Zemel, R. </author> <year> (1994). </year> <title> Autoencoders, minimum description length, and Helmholtz free energy. </title> <editor> In Cowan, J., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <publisher> Morgan Kaufmanm Publishers, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Jordan, M. and Jacobs, R. </author> <year> (1994). </year> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214. </pages>
Reference-contexts: Within each vector quantizer a competitive learning mechanism operates to select an appropriate vector code to describe the input. The CVQ is related to algorithms based on mixture models, such as soft competitive clustering, mixtures of experts <ref> (Jordan and Jacobs, 1994) </ref>, and hidden Markov models (Baum et al., 1970), in that each vector quantizer in the CVQ is itself a mixture model.
Reference: <author> McCullagh, P. and Nelder, J. </author> <year> (1989). </year> <title> Generalized Linear Models. </title> <publisher> Chapman & Hall, London. </publisher>
Reference-contexts: For models in which the observable is a monotonic differentiable function of P i.e. generalized linear models, least squares estimates of the weights for the M-step can be obtained iteratively by the method of scoring <ref> (McCullagh and Nelder, 1989) </ref>. 3.1 E-step: Exact The difficulty arises in the E-step of the algorithm.
Reference: <author> Neal, R. </author> <year> (1992). </year> <title> Connectionist learning of belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 56:71 113. </volume>
Reference-contexts: This constant partition function results in desirable properties, such as the lack of a Boltzmann machine-like sleep phase <ref> (Neal, 1992) </ref>, which we will exploit in the learning algorithm. The system described by equation (1) 1 can be thought of as a special form of the Boltzmann machine (Ackley et al., 1985). Expanding out the quadratic term we see that there are pairwise interaction terms between every unit.
Reference: <author> Parisi, G. </author> <year> (1988). </year> <title> Statistical Field Theory. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA. </address>
Reference-contexts: These estimated expectations are then used in the E-step. 3.3 E-step: Mean-field approximation Although Gibbs sampling is generally much more efficient than exact calculations, it too can be computationally demanding. A more promising approach is to approximate the intractable system with a tractable mean-field approximation <ref> (Parisi, 1988) </ref>, and perform the E-step calculation on this approximation.
Reference: <author> Redlich, A. </author> <year> (1993). </year> <title> Supervised factorial learning. </title> <journal> Neural Computation, </journal> <volume> 5 </volume> <pages> 750-766. </pages>
Reference: <author> Rissanen, J. </author> <year> (1989). </year> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, Singa pore. </publisher>
Reference-contexts: The goal of factorial learning is to invert this data generation process, discovering a representation that will both parsimoniously describe the data and reflect its underlying causes. A recent approach to factorial learning uses the Minimum Description Length (MDL) principle <ref> (Rissanen, 1989) </ref> to extract a compact representation of the input (Zemel, 1993; Hinton and Zemel, 1994). This has resulted in a learning architecture called Cooperative Vector Quantization (CVQ), in which a set of vector quantiz-ers cooperates to reproduce the input.
Reference: <author> Saund, E. </author> <year> (1995). </year> <title> A multiple cause mixture model for unsupervised learning. </title> <journal> Neural Computation, </journal> <volume> 7(1) </volume> <pages> 51-71. </pages>
Reference: <author> Zemel, R. </author> <year> (1993). </year> <title> A minimum description length framework for unsupervised learning. </title> <type> Ph.D. Thesis, </type> <institution> Dept. of Computer Science, University of Toronto, Toronto, Canada. </institution> <month> 8 </month>
References-found: 15

