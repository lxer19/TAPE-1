URL: http://www.icsi.berkeley.edu/~bilmes/papers/icmc92_paper.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/~bilmes/papers/
Root-URL: http://www.icsi.berkeley.edu
Email: &lt;bilmes@amt.mit.edu&gt;  
Title: A Model for Musical Rhythm  
Author: Jeff Bilmes 
Address: Cambridge, MA 02139  
Affiliation: The MIT Media Laboratory Massachusetts Institute of Technology  
Abstract: At least four elements characterize musical rhythm: 1) metric content, a quantized attribute; 2) ametric phrases, those unrelated to any tactus; 3) tempo variation, change in the number of beats per second; and 4) event shifting, time deviations from a steady beat. This deconstruction provides a means to represent timing features from different musical styles. Both tempo variation and event shift information can operate at different levels of the metric hierarchy found in music. This representation will facilitate the analysis and production of musical style to the extent that it is rhythmically expressed. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, D. P., & Bilmes, J. A. </author> <year> (1991). </year> <title> Concurrent real-time music in C++. </title> <booktitle> In USENIX C++ Conference Proceedings Washington D.C. </booktitle>
Reference-contexts: Current Work/Conclusion It is possible to disassemble musical rhythm into four separate components: a metric hierarchy, two forms of timing functions that operate in cooperation with the hierarchy, and an amorphous structure. We are currently adding constructs to specify the metric hierarchy into a C++ based music system <ref> (Anderson & Bilmes, 1991) </ref>. We are also adding to the system primitives that specify tempo variation and event shift functions and primitives that situate the functions at domains specified by hierarchical levels.
Reference: <author> Anderson, D. P., & Kuivila, R. </author> <year> (1989). </year> <title> Continuous abstractions for discrete event languages. </title> <note> Computer Music Journal 13(3). </note>
Reference-contexts: Functions describing tempo variation have been discovered that correspond closely to real musical performances. Some examples are time maps (Jaffe, 1985), time deformations <ref> (Anderson & Kuivila, 1989) </ref>, sentic curves (Clines, 1977), and force model constructs (Feldman, Epstein, & Richards, pres). <p> In the case where intra-measure domain values arise (e.g., one and a half measures), output values are computed for non-integer inputs. A tempo variation function's range, or output, is a tempo-scaling factor. The function itself is used for 1) a time deformation-style transformation <ref> (Anderson & Kuivila, 1989) </ref> that takes a time duration as input and produces as output a deformed time duration.
Reference: <author> Clines, M. </author> <year> (1977). </year> <title> Sentics, The Touch of Emotion. </title> <publisher> Doubleday Anchor, </publisher> <address> New York. </address>
Reference-contexts: Functions describing tempo variation have been discovered that correspond closely to real musical performances. Some examples are time maps (Jaffe, 1985), time deformations (Anderson & Kuivila, 1989), sentic curves <ref> (Clines, 1977) </ref>, and force model constructs (Feldman, Epstein, & Richards, pres). One important difference between Western classical music and ethnic music (such as African) or modern music (such as African-American jazz) is that expressive timing in the former can often be described using only tempo variation functions.
Reference: <author> Desain, P., & Honing, H. </author> <year> (1991). </year> <title> Tempo curves considered harmful. </title> <booktitle> In ICMC Proceedings Montreal, </booktitle> <address> Canada. </address>
Reference-contexts: A performer does not use the same time shift values when playing a piece at different tempos. Thus, event shift functions must also, at times, be functions of instantaneous tempo. An event shift function of two variables <ref> (Desain & Honing, 1991) </ref> sufficiently models this situation. Most metrical rhythmic figures can be represented using these models. However, we need to gather timing data from real performances by quality musicians to discover the functions operating at each level in the metric hierarchy.
Reference: <author> Feldman, J., Epstein, D., & Richards, W. </author> ( <title> in pres.). Force dynamics of tempo change in music. Music Perception, </title> <publisher> forthcoming. </publisher>
Reference: <author> Fraisse, P. </author> <year> (1983). </year> <editor> Rhythm and tempo. In Deutch, D. (Ed.), </editor> <booktitle> The Psychology of Music. </booktitle> <publisher> Academic Press. </publisher>
Reference-contexts: The metric content of musical rhythm is the perceptual relation of successive rhythmic events to an evenly-spaced time grid. The time grid determines the "beat" within which all musical events are heard. Our perception of rhythm, sometimes referred to as subjective rhythmization <ref> (Fraisse, 1983) </ref>, is a psychological linking of sequential event stimuli. For the effect to occur, the event inter-onset times must be bounded in time, ranging from about 120 msec to about 2 sec. Traditional Western musical notation is one example of a model which represents the metric content.
Reference: <author> Jaffe, D. </author> <year> (1985). </year> <title> Ensemble timing in computer music. </title> <journal> CMJ, </journal> <volume> 9 (4), </volume> <pages> 38-48. </pages>
Reference-contexts: Functions describing tempo variation have been discovered that correspond closely to real musical performances. Some examples are time maps <ref> (Jaffe, 1985) </ref>, time deformations (Anderson & Kuivila, 1989), sentic curves (Clines, 1977), and force model constructs (Feldman, Epstein, & Richards, pres).
Reference: <author> Jordan, M. </author> <year> (1989). </year> <title> Serial order: A parallel, distributed processing approach. </title> <editor> In Elman, J. L. & Rumelhart D. E. (Ed.), </editor> <booktitle> Advances in Connectionist Theory: Speech. </booktitle> <address> Hillsdale: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: However, these functions often might not be continuous since an event shift function is undefined during musical periods without note events. Time sequence learning algorithms, including connectionist approaches <ref> (Jordan, 1989) </ref> and statistical clustering analysis, look promising for learning and representing such functions. 5. Current Work/Conclusion It is possible to disassemble musical rhythm into four separate components: a metric hierarchy, two forms of timing functions that operate in cooperation with the hierarchy, and an amorphous structure.
Reference: <author> Richards, W. </author> <year> (1988). </year> <title> Natural Computation. </title> <publisher> MIT Press. </publisher>
Reference-contexts: We need, however, more psychophysical experiments to discover the functions that describe tempo variation and event shifting, and to learn how to use these functions for classification. Ultimately, as noted by Richards <ref> (Richards, 1988, pages 307-308) </ref>, discovering functions such as those described above might also provide insight into the more general issue of temporal perception.
References-found: 9

