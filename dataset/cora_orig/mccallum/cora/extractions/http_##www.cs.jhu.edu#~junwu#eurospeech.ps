URL: http://www.cs.jhu.edu/~junwu/eurospeech.ps
Refering-URL: http://www.cs.jhu.edu/~junwu/publication.html
Root-URL: http://www.cs.jhu.edu/~junwu/
Email: fjunwu,sanjeevg@mail.clsp.jhu.edu  
Title: COMBINING NONLOCAL, SYNTACTIC AND N-GRAM DEPENDENCIES IN LANGUAGE MODELING  
Author: Jun Wu Sanjeev Khudanpur 
Address: Baltimore, MD 21218, USA  
Affiliation: Center for Language and Speech Processing, Johns Hopkins University  
Abstract: A new language model is presented which incorporates local N-gram dependencies with two important sources of long-range dependencies: the syntactic structure and the topic of a sentence. These dependencies or constraints are integrated using the maximum entropy method. Substantial improvements are demonstrated over a trigram model in both perplexity and speech recognition accuracy on the Switchboard task. It is shown that topic dependencies are most useful in predicting words which are semantically related by the subject matter of the conversation. Syntactic dependencies on the other hand are found to be most helpful in positions where the best predictors of the following word are not within N-gram range due to an intervening phrase or clause. It is also shown that these two methods individually enhance an N-gram model in complementary ways and the overall improvement from their combination is nearly additive. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. R. Bellegarda, </author> <title> "Exploiting Both Local and Global Constraints for Multispan Statistical Language Modeling," </title> <booktitle> in Proc. ICASSP'98, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 677-680, </pages> <month> May 12-15, </month> <year> 1998. </year>
Reference-contexts: In language modeling, a more accurate expectation of the probability of these topic relevant words may be estimated via determination of semantic content of the document, e.g., by topic detection. Several models that combine topic-related information with N-gram models have been studied, e.g., in <ref> [8, 1, 5, 3, 4, 11, 9, 12, 13, 14] </ref>. Most of them [8, 5, 9, 13, 14] exploit these differences for language modeling by constructing separate N-gram models for each individual topic.
Reference: [2] <author> C. Chelba and F. Jelinek, </author> <title> "Exploiting Syntactic Structure for Language Modeling," </title> <booktitle> in Proc. COLING-ACL'98, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 225-231, </pages> <month> Aug. </month> <pages> 10-14, </pages> <year> 1998. </year>
Reference-contexts: The approach proposed in [11] takes the advantage of the ME method by incorporating the topic information with N-grams in a unified model. The benefits of using syntactic structure for language modeling have been shown in <ref> [2] </ref>. Syntactic structure for a sentence hypothesised by a speech recognizer is developed in a left-to-right manner and is used to assign a probabilistic (syntactic) score for each word in the hypotheses. An interpolated model is constructed in [2] by combining the syntactic score and the N-gram score. <p> benefits of using syntactic structure for language modeling have been shown in <ref> [2] </ref>. Syntactic structure for a sentence hypothesised by a speech recognizer is developed in a left-to-right manner and is used to assign a probabilistic (syntactic) score for each word in the hypotheses. An interpolated model is constructed in [2] by combining the syntactic score and the N-gram score. Perplexity reduction on the Wall Street Journal corpus have been demonstrated using this model. The topic and the structure of a sentence provide two different kinds of long-range dependencies. <p> The former captures the semantic correlation between words over a very long range while the latter one contains the syntactic dependencies of words within a sentence. The model presented in this paper starts from the work in [11] and <ref> [2] </ref>, and aims to create a language model that has the advantages of both the topic model and the syntactic model. The ME method is used to bring these diverse information sources to bear in a unified composite language model. <p> Furthermore, parsers that generate syntactic structures are usually designed to work on correct sentences, and they must be modified to parse erroneous partial hypotheses in speech recognition. We parse all sentences in the training data by the left-to-right parser presented in <ref> [2] </ref>. This parser generates a stack S i of parse trees T i for a sentence prefix W i 1 = w 1 ; w 2 ; :::; w i at time i. <p> preceding the contract NP the contract ended with a loss of 7 cents NNDT VBD IN DT NN IN CD NNS h h w w w after ii-1i-2 VP ended i-1i-2 word "after" in Figure 1, for instance, are "contract" and "ended." Details about the parser may be found in <ref> [2] </ref>. We assume that the immediate history (w i2 , w i1 ) and last two head words h i2 , h i1 of the partial parse T i carry most of the useful information. Of course some syntactic information will be lost under this assumption. <p> The results show the benefits of integrating various sources of information under the ME framework in improving language modeling. More gains from syntactic structure is expected by using the non-terminal labels in the partial parse <ref> [2] </ref>. Work is in progress to incorporate this information in the language model. All experiments in this paper are based on N-best rescoring, and we will also use this method in lattice rescoring in the future.
Reference: [3] <author> S. F. Chen et al, </author> <title> "Topic Adaptation for Language Modeling Using Unnormalized Exponential Models," </title> <booktitle> in Proc. ICASSP'98, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 681-684, </pages> <month> May 12-15, </month> <year> 1998. </year>
Reference-contexts: In language modeling, a more accurate expectation of the probability of these topic relevant words may be estimated via determination of semantic content of the document, e.g., by topic detection. Several models that combine topic-related information with N-gram models have been studied, e.g., in <ref> [8, 1, 5, 3, 4, 11, 9, 12, 13, 14] </ref>. Most of them [8, 5, 9, 13, 14] exploit these differences for language modeling by constructing separate N-gram models for each individual topic. <p> This results in fragmentation of the training data and therefore may hurt the discriminating power of a well-trained global N-gram model. Some use an idea similar to ours but build the exponential model <ref> [3, 4] </ref> or cache-like model [12]. The approach proposed in [11] takes the advantage of the ME method by incorporating the topic information with N-grams in a unified model. The benefits of using syntactic structure for language modeling have been shown in [2].
Reference: [4] <author> S. F. Chen et al, </author> <title> "Topic Adaptation for Language Modeling Using Unnormalized Exponential Models," </title> <booktitle> in Proc. ICASSP'99, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 681-684, </pages> <month> March 14-19, </month> <year> 1999. </year>
Reference-contexts: In language modeling, a more accurate expectation of the probability of these topic relevant words may be estimated via determination of semantic content of the document, e.g., by topic detection. Several models that combine topic-related information with N-gram models have been studied, e.g., in <ref> [8, 1, 5, 3, 4, 11, 9, 12, 13, 14] </ref>. Most of them [8, 5, 9, 13, 14] exploit these differences for language modeling by constructing separate N-gram models for each individual topic. <p> This results in fragmentation of the training data and therefore may hurt the discriminating power of a well-trained global N-gram model. Some use an idea similar to ours but build the exponential model <ref> [3, 4] </ref> or cache-like model [12]. The approach proposed in [11] takes the advantage of the ME method by incorporating the topic information with N-grams in a unified model. The benefits of using syntactic structure for language modeling have been shown in [2].
Reference: [5] <author> P. Clarkson and A. Robinson, </author> <title> "Language Model Adaptation Using Mixtures and an Exponentially Decaying Cache," </title> <booktitle> in Proc. ICASSP'97, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 799-802, </pages> <month> Apr. </month> <pages> 21-25, </pages> <year> 1997. </year>
Reference-contexts: In language modeling, a more accurate expectation of the probability of these topic relevant words may be estimated via determination of semantic content of the document, e.g., by topic detection. Several models that combine topic-related information with N-gram models have been studied, e.g., in <ref> [8, 1, 5, 3, 4, 11, 9, 12, 13, 14] </ref>. Most of them [8, 5, 9, 13, 14] exploit these differences for language modeling by constructing separate N-gram models for each individual topic. <p> Several models that combine topic-related information with N-gram models have been studied, e.g., in [8, 1, 5, 3, 4, 11, 9, 12, 13, 14]. Most of them <ref> [8, 5, 9, 13, 14] </ref> exploit these differences for language modeling by constructing separate N-gram models for each individual topic. This results in fragmentation of the training data and therefore may hurt the discriminating power of a well-trained global N-gram model.
Reference: [6] <author> J. N. Darroch and D. Ratcliff, </author> <title> "Generalized Iterative Scaling for Log-Linear Models," </title> <journal> Annals Math. Stats., </journal> <volume> Vol. 43, </volume> <year> 1972. </year>
Reference: [7] <author> S. Della Pietra, V. Della Pietra, J. Lafferty, </author> <title> "Inducing Features of Random Fields", </title> <publisher> CMU-CS-95-144. </publisher>
Reference-contexts: The parameters ('s) of the model are trained by the improved iterative scaling algorithm <ref> [7] </ref>. The heavy computational load in the training procedure is distributed to many computers by a parallel training method described in [11]. 4. EXPERIMENTAL RESULTS Perplexity and recognition results on the Switchboard corpus are presented in this section.
Reference: [8] <author> Radu Florian and David Yarowsky, </author> <title> "Dynamic nonlocal language modeling via hierarchical topic-based adaptation," </title> <note> To appear in Proceedings of ACL99, </note> <year> 1999. </year>
Reference-contexts: In language modeling, a more accurate expectation of the probability of these topic relevant words may be estimated via determination of semantic content of the document, e.g., by topic detection. Several models that combine topic-related information with N-gram models have been studied, e.g., in <ref> [8, 1, 5, 3, 4, 11, 9, 12, 13, 14] </ref>. Most of them [8, 5, 9, 13, 14] exploit these differences for language modeling by constructing separate N-gram models for each individual topic. <p> Several models that combine topic-related information with N-gram models have been studied, e.g., in [8, 1, 5, 3, 4, 11, 9, 12, 13, 14]. Most of them <ref> [8, 5, 9, 13, 14] </ref> exploit these differences for language modeling by constructing separate N-gram models for each individual topic. This results in fragmentation of the training data and therefore may hurt the discriminating power of a well-trained global N-gram model. <p> The method has been presented in [11] and we review it here for completeness. In the training procedure, each of the 2200 conversation sides in the language model training corpus is represented by a TF-IDF 2 vector <ref> [8] </ref>. These vectors are then clustered into 67 topics using a K-means procedure and the centroid of each cluster is computed. Cosine similarity is used as the distance between vectors during clustering.
Reference: [9] <author> R. Iyer and M. Ostendorf, </author> <title> "Modeling Long Range Dependencies in Language," </title> <booktitle> in Proc. ICSLP'96, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 236-239, </pages> <month> Oct. </month> <pages> 3-6, </pages> <year> 1996. </year>
Reference-contexts: In language modeling, a more accurate expectation of the probability of these topic relevant words may be estimated via determination of semantic content of the document, e.g., by topic detection. Several models that combine topic-related information with N-gram models have been studied, e.g., in <ref> [8, 1, 5, 3, 4, 11, 9, 12, 13, 14] </ref>. Most of them [8, 5, 9, 13, 14] exploit these differences for language modeling by constructing separate N-gram models for each individual topic. <p> Several models that combine topic-related information with N-gram models have been studied, e.g., in [8, 1, 5, 3, 4, 11, 9, 12, 13, 14]. Most of them <ref> [8, 5, 9, 13, 14] </ref> exploit these differences for language modeling by constructing separate N-gram models for each individual topic. This results in fragmentation of the training data and therefore may hurt the discriminating power of a well-trained global N-gram model.
Reference: [10] <author> E.T. Jaynes, </author> <title> "On the Rationale of Maximum Entropy Methods," </title> <booktitle> Proceedings of IEEE, </booktitle> <volume> Vol. 70, </volume> <pages> pp. 939-952, </pages> <month> Sep. </month> <year> 1982. </year>
Reference: [11] <author> S. Khudanpur and J. Wu, </author> <title> "A Maximum Entropy Language Model to Integrate N-Grams and Topic Dependencies for Conversational Speech Recognition", </title> <booktitle> Proceedings of ICASSP'99, </booktitle> <pages> pp. 553-556 </pages>
Reference-contexts: In language modeling, a more accurate expectation of the probability of these topic relevant words may be estimated via determination of semantic content of the document, e.g., by topic detection. Several models that combine topic-related information with N-gram models have been studied, e.g., in <ref> [8, 1, 5, 3, 4, 11, 9, 12, 13, 14] </ref>. Most of them [8, 5, 9, 13, 14] exploit these differences for language modeling by constructing separate N-gram models for each individual topic. <p> This results in fragmentation of the training data and therefore may hurt the discriminating power of a well-trained global N-gram model. Some use an idea similar to ours but build the exponential model [3, 4] or cache-like model [12]. The approach proposed in <ref> [11] </ref> takes the advantage of the ME method by incorporating the topic information with N-grams in a unified model. The benefits of using syntactic structure for language modeling have been shown in [2]. <p> The former captures the semantic correlation between words over a very long range while the latter one contains the syntactic dependencies of words within a sentence. The model presented in this paper starts from the work in <ref> [11] </ref> and [2], and aims to create a language model that has the advantages of both the topic model and the syntactic model. The ME method is used to bring these diverse information sources to bear in a unified composite language model. <p> EXPLOITING NON-LOCAL DEPENDENCIES 2.1. Topic Feature Selection and Topic Detection We use the long range history of an utterance to assign a topic to a utterance and then use this topic to predict words in that utterance during speech recognition. The method has been presented in <ref> [11] </ref> and we review it here for completeness. In the training procedure, each of the 2200 conversation sides in the language model training corpus is represented by a TF-IDF 2 vector [8]. <p> Although the performance of the topic sensitive language model will be influenced by the quality of this topic assignment, it has been shown in <ref> [11] </ref> that little degradation in performance is caused by recognition and classification errors. We assign a fresh topic to each test utterance to allow for changes in topic as the conversation progresses. This has been shown to yield better recognition accuracy than topic assignment based on the entire conversation [11]. 2.2. <p> in <ref> [11] </ref> that little degradation in performance is caused by recognition and classification errors. We assign a fresh topic to each test utterance to allow for changes in topic as the conversation progresses. This has been shown to yield better recognition accuracy than topic assignment based on the entire conversation [11]. 2.2. Exploiting Syntactic Features Although it is widely acknowledged that the syntactic structure of a sentence should be helpful in predicting words, many challenges must be met to integrate syntactic structure into a language model. First, the structure of statistical language models and that of syntax are totally different. <p> The parameters ('s) of the model are trained by the improved iterative scaling algorithm [7]. The heavy computational load in the training procedure is distributed to many computers by a parallel training method described in <ref> [11] </ref>. 4. EXPERIMENTAL RESULTS Perplexity and recognition results on the Switchboard corpus are presented in this section. The training text for all language models comprises about 2.2 million words in the form of 1100 conversations on about 70 different topics.
Reference: [12] <author> R. Kneser et al, </author> <title> "Language Model Adaptation Using Dynamic Marginals," </title> <booktitle> in Proc. EUROSPEECH'97, </booktitle> <volume> Vol. 4, </volume> <pages> pp. 1971-1974, </pages> <month> Sept. </month> <pages> 22-25, </pages> <year> 1997. </year>
Reference-contexts: In language modeling, a more accurate expectation of the probability of these topic relevant words may be estimated via determination of semantic content of the document, e.g., by topic detection. Several models that combine topic-related information with N-gram models have been studied, e.g., in <ref> [8, 1, 5, 3, 4, 11, 9, 12, 13, 14] </ref>. Most of them [8, 5, 9, 13, 14] exploit these differences for language modeling by constructing separate N-gram models for each individual topic. <p> This results in fragmentation of the training data and therefore may hurt the discriminating power of a well-trained global N-gram model. Some use an idea similar to ours but build the exponential model [3, 4] or cache-like model <ref> [12] </ref>. The approach proposed in [11] takes the advantage of the ME method by incorporating the topic information with N-grams in a unified model. The benefits of using syntactic structure for language modeling have been shown in [2].
Reference: [13] <author> M. Mahajan, D. Befferman, X.D. Huang, </author> <title> "Improved Topic-Dependent Language Modeling Using Information Retrieval Techniques", </title> <booktitle> in Proc. ICASSP'99, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 541-544, </pages> <month> March 15-19, </month> <year> 1999. </year>
Reference-contexts: In language modeling, a more accurate expectation of the probability of these topic relevant words may be estimated via determination of semantic content of the document, e.g., by topic detection. Several models that combine topic-related information with N-gram models have been studied, e.g., in <ref> [8, 1, 5, 3, 4, 11, 9, 12, 13, 14] </ref>. Most of them [8, 5, 9, 13, 14] exploit these differences for language modeling by constructing separate N-gram models for each individual topic. <p> Several models that combine topic-related information with N-gram models have been studied, e.g., in [8, 1, 5, 3, 4, 11, 9, 12, 13, 14]. Most of them <ref> [8, 5, 9, 13, 14] </ref> exploit these differences for language modeling by constructing separate N-gram models for each individual topic. This results in fragmentation of the training data and therefore may hurt the discriminating power of a well-trained global N-gram model.
Reference: [14] <author> S. C. Martin et al, </author> <title> "Adaptive Topic-Dependent Language Modeling Using Word-Based Varigrams," </title> <journal> Proc. EUROSPEECH'97, </journal> <volume> Vol. 3, </volume> <pages> pp. 1447-1450, </pages> <month> Sept. </month> <pages> 22-25, </pages> <year> 1997. </year>
Reference-contexts: In language modeling, a more accurate expectation of the probability of these topic relevant words may be estimated via determination of semantic content of the document, e.g., by topic detection. Several models that combine topic-related information with N-gram models have been studied, e.g., in <ref> [8, 1, 5, 3, 4, 11, 9, 12, 13, 14] </ref>. Most of them [8, 5, 9, 13, 14] exploit these differences for language modeling by constructing separate N-gram models for each individual topic. <p> Several models that combine topic-related information with N-gram models have been studied, e.g., in [8, 1, 5, 3, 4, 11, 9, 12, 13, 14]. Most of them <ref> [8, 5, 9, 13, 14] </ref> exploit these differences for language modeling by constructing separate N-gram models for each individual topic. This results in fragmentation of the training data and therefore may hurt the discriminating power of a well-trained global N-gram model.
References-found: 14

