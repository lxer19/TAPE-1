URL: http://www.cs.caltech.edu./~ps/papers/taubin/sig95.ps
Refering-URL: http://www.ugcs.caltech.edu/~jtr/software/174c/
Root-URL: http://www.cs.caltech.edu
Title: A Signal Processing Approach To Fair Surface Design  
Author: Gabriel Taubin 
Keyword: CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Picture/image generation display algorithms; I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling curve, surface, solid, and object representations; J.6 [Computer Applications]: Computer-Aided Engineering computer-aided design General Terms: Algorithms, Graphics.  
Affiliation: IBM T.J.Watson Research Center  
Abstract: In this paper we describe a new tool for interactive free-form fair surface design. By generalizing classical discrete Fourier analysis to two-dimensional discrete surface signals functions defined on polyhedral surfaces of arbitrary topology -, we reduce the problem of surface smoothing, or fairing, to low-pass filtering. We describe a very simple surface signal low-pass filter algorithm that applies to surfaces of arbitrary topology. As opposed to other existing optimization-based fairing methods, which are computationally more expensive, this is a linear time and space complexity algorithm. With this algorithm, fairing very large surfaces, such as those obtained from volumetric medical data, becomes affordable. By combining this algorithm with surface subdivision methods we obtain a very effective fair surface design technique. We then extend the analysis, and modify the algorithm accordingly, to accommodate different types of constraints. Some constraints can be imposed without any modification of the algorithm, while others require the solution of a small associated linear system of equations. In particular, vertex location constraints, vertex normal constraints, and surface normal discontinuities across curves embedded in the surface, can be imposed with this technique. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C.L. Bajaj and Ihm. </author> <title> Smoothing polyhedra using implicit algebraic splines. </title> <journal> Computer Graphics, </journal> <pages> pages 79-88, </pages> <month> July </month> <year> 1992. </year> <note> (Proceedings SIGGRAPH'92). </note>
Reference-contexts: Unless these large surfaces are first simplified [29, 13, 11], or re-meshed using far fewer faces [35], methods based on patch technology, whether parametric [28, 22, 10, 20, 19] or implicit <ref> [1, 23] </ref>, are not acceptable either.
Reference: [2] <author> H. Baker. </author> <title> Building surfaces of evolution: The weaving wall. </title> <journal> International Journal of Computer Vision, </journal> <volume> 3 </volume> <pages> 51-71, </pages> <year> 1989. </year>
Reference-contexts: 1 INTRODUCTION The signal processing approach described in this paper was originally motivated by the problem of how to fair large polyhedral surfaces of arbitrary topology, such as those extracted from volumetric medical data by iso-surface construction algorithms <ref> [21, 2, 11, 15] </ref>, or constructed by integration of multiple range images [36]. <p> i=1 because K u i = k i u i , to define a low-pass filter we need to find a polynomial such that f (k i ) N 1 for low frequencies, and f (k i ) N 0 for high frequencies in the region of interest k 2 <ref> [0; 2] </ref>. Our choice is f (k) = (1 k)(1 k) (7) where 0 &lt; , and is a new negative scale factor such that &lt; . <p> The value of k PB is k PB = 1 &gt; 0 : (9) The graph of the transfer function f (k) N displays a typical low-pass filter shape in the region of interest k 2 <ref> [0; 2] </ref>. The pass-band region extends from k = 0 to k = k PB , where f (k) N 1. As k increases from k = k PB to k = 2, the transfer function decreases to zero. The faster the transfer function decreases in this region, the better. <p> Of course we want to minimize N , the number of iterations. To do so, must be chosen as large as possible, while keeping jf (k)j &lt; 1 for k PB &lt; k 2 (if jf (k)j 1 in <ref> [k PB ; 2] </ref>, the filter will enhance high frequencies instead of attenuating them). In some of the examples, we have chosen so that f (1) = f (2). For k PB &lt; 1 this choice of ensures a stable and fast filter.
Reference: [3] <author> P. Borrel. </author> <title> Simple constrained deformations for geometric modeling and interactive design. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 13(2) </volume> <pages> 137-155, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: To do so, the values x 1 ; : : : ; x m in equation (10) must be replaced by the desired final values of the faired signal at the corresponding vertices. As in in the Free-form deformation approaches of Hsu, Hughes, and Kaufman [14] and Borrel <ref> [3] </ref>, instead of moving control points outside the surface, surfaces can be deformed here by pulling one or more vertices.
Reference: [4] <author> E. Catmull and J. Clark. </author> <title> Recursively generated B-spline surfaces on arbitrary topological meshes. </title> <booktitle> Computer Aided Design, </booktitle> <volume> 10 </volume> <pages> 350-355, </pages> <year> 1978. </year>
Reference-contexts: Note the shrinkage effect. (D) Five non-shrinking smoothing steps (k PB = 0:1 and = 0:6307) of this paper. (B),(C), and (D) are the surfaces obtained after two levels of refinement and smoothing. Surfaces are flat-shaded to enhance the faceting effect. subdivision schemes <ref> [8, 4, 12] </ref> are rigid, in the sense that they have no free parameters that influence the behavior of the algorithm as it progresses trough the subdivision process. By using our fairing algorithm in conjunction with subdivision steps, we achieve more flexibility in the design process. <p> By using our fairing algorithm in conjunction with subdivision steps, we achieve more flexibility in the design process. In this way our fairing algorithm can be seen as a complement of the existing subdivision strategies. In the subdivision surfaces of Catmull and Clark <ref> [4, 12] </ref> and Loop [18, 6], the subdivision process involves two steps. A refinement step, where a new surface with more vertices and faces is created, and a smoothing step, where the vertices of the new sur face are moved.
Reference: [5] <author> G. Celniker and D. Gossard. </author> <title> Deformable curve and surface finite--elements for free-form shape design. </title> <journal> Computer Graphics, </journal> <pages> pages 257-266, </pages> <month> July </month> <year> 1991. </year> <note> (Proceedings SIGGRAPH'91). </note>
Reference-contexts: The faired surface has exactly the same number of vertices and faces as the original one. However, our signal processing formulation results in much less expensive computations. In these variational formulations <ref> [5, 24, 38, 12] </ref>, after finite element discretization, the problem is often reduced to the solution of a large sparse linear system, or a more expensive global optimization problem. Large sparse linear systems are solved using iterative methods [9], and usually result in quadratic time complexity algorithms.
Reference: [6] <author> T.D. DeRose, M. Lounsbery, and J. Warren. </author> <title> Multiresolution analysis for surfaces of arbitrary topological type. </title> <type> Technical Report 93-10-05, </type> <institution> Department of Computer Science and Enginnering, University of Washington, </institution> <address> Seattle, </address> <year> 1993. </year>
Reference-contexts: By using our fairing algorithm in conjunction with subdivision steps, we achieve more flexibility in the design process. In this way our fairing algorithm can be seen as a complement of the existing subdivision strategies. In the subdivision surfaces of Catmull and Clark [4, 12] and Loop <ref> [18, 6] </ref>, the subdivision process involves two steps. A refinement step, where a new surface with more vertices and faces is created, and a smoothing step, where the vertices of the new sur face are moved.
Reference: [7] <author> M. </author> <title> Do Carmo. Differential Geometry of Curves and Surfaces. </title> <publisher> Prentice Hall, </publisher> <year> 1976. </year>
Reference-contexts: known that, for a curvature continuous surface, if the curve fl is let to shrink to to the point v i , the integral converges to the mean curvature (v i ) of the surface at the point v i times the normal vector N i at the same point <ref> [7] </ref> lim 1 Z (v v i ) dl (v) = (v i )N i : Because of this fact, we can define the vector v i as the normal vector to the polyhedral surface at v i .
Reference: [8] <author> D. Doo and M. Sabin. </author> <title> Behaviour of recursive division surfaces near extraordinary points. </title> <booktitle> Computer Aided Design, </booktitle> <volume> 10 </volume> <pages> 356-360, </pages> <year> 1978. </year>
Reference-contexts: Note the shrinkage effect. (D) Five non-shrinking smoothing steps (k PB = 0:1 and = 0:6307) of this paper. (B),(C), and (D) are the surfaces obtained after two levels of refinement and smoothing. Surfaces are flat-shaded to enhance the faceting effect. subdivision schemes <ref> [8, 4, 12] </ref> are rigid, in the sense that they have no free parameters that influence the behavior of the algorithm as it progresses trough the subdivision process. By using our fairing algorithm in conjunction with subdivision steps, we achieve more flexibility in the design process.
Reference: [9] <author> G. Golub and C.F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> John Hopkins University Press, 2nd. </publisher> <address> edition, </address> <year> 1989. </year>
Reference-contexts: In these variational formulations [5, 24, 38, 12], after finite element discretization, the problem is often reduced to the solution of a large sparse linear system, or a more expensive global optimization problem. Large sparse linear systems are solved using iterative methods <ref> [9] </ref>, and usually result in quadratic time complexity algorithms. In our case, the problem of surface fairing is reduced to sparse matrix multiplication instead, a linear time complexity operation. The paper is organized as follows. <p> Although many functions of one variable can be evaluated in matrices <ref> [9] </ref>, we will only consider polynomials here. For example, in the case of Gaussian smoothing the transfer function is f (k) = 1 k. <p> The columns that constitute C (1) must be chosen so that C (1) become non-singular, and as well conditioned as possible. In practice this can be done using Gauss elimination with full pivoting <ref> [9] </ref>, but for the sake of simplicity, we will assume here that C (1) is composed of the first m columns of C. We decompose signals in the same way. x (1) denotes here the first m components, and x (2) the last n m components, of the signal x.
Reference: [10] <author> G. Greiner and H.P. Seidel. </author> <title> Modeling with triangular b-splines. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 14(2) </volume> <pages> 56-60, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Unless these large surfaces are first simplified [29, 13, 11], or re-meshed using far fewer faces [35], methods based on patch technology, whether parametric <ref> [28, 22, 10, 20, 19] </ref> or implicit [1, 23], are not acceptable either.
Reference: [11] <author> A. Gueziec and R. Hummel. </author> <title> The wrapper algorithm: Surface extraction and simplification. </title> <booktitle> In IEEE Workshop on Biomedical Image Analysis, </booktitle> <pages> pages 204-213, </pages> <address> Seattle, WA, </address> <month> June 24-25 </month> <year> 1994. </year>
Reference-contexts: 1 INTRODUCTION The signal processing approach described in this paper was originally motivated by the problem of how to fair large polyhedral surfaces of arbitrary topology, such as those extracted from volumetric medical data by iso-surface construction algorithms <ref> [21, 2, 11, 15] </ref>, or constructed by integration of multiple range images [36]. <p> Unless these large surfaces are first simplified <ref> [29, 13, 11] </ref>, or re-meshed using far fewer faces [35], methods based on patch technology, whether parametric [28, 22, 10, 20, 19] or implicit [1, 23], are not acceptable either.
Reference: [12] <author> M. Halstead, M. Kass, and T. DeRose. </author> <title> Efficient, fair interpolation using catmull-clark surface. </title> <institution> Computer Graphics,pages 35-44, </institution> <month> August </month> <year> 1993. </year> <note> (Proceedings SIGGRAPH'93). </note>
Reference-contexts: Since most existing algorithms based on fairness norm optimization <ref> [37, 24, 12, 38] </ref> are prohibitively expensive for very large surfaces a million vertices is not unusual in medical images -, we decided to look for new algorithms with linear time and space complexity [31]. <p> The faired surface has exactly the same number of vertices and faces as the original one. However, our signal processing formulation results in much less expensive computations. In these variational formulations <ref> [5, 24, 38, 12] </ref>, after finite element discretization, the problem is often reduced to the solution of a large sparse linear system, or a more expensive global optimization problem. Large sparse linear systems are solved using iterative methods [9], and usually result in quadratic time complexity algorithms. <p> Note the shrinkage effect. (D) Five non-shrinking smoothing steps (k PB = 0:1 and = 0:6307) of this paper. (B),(C), and (D) are the surfaces obtained after two levels of refinement and smoothing. Surfaces are flat-shaded to enhance the faceting effect. subdivision schemes <ref> [8, 4, 12] </ref> are rigid, in the sense that they have no free parameters that influence the behavior of the algorithm as it progresses trough the subdivision process. By using our fairing algorithm in conjunction with subdivision steps, we achieve more flexibility in the design process. <p> By using our fairing algorithm in conjunction with subdivision steps, we achieve more flexibility in the design process. In this way our fairing algorithm can be seen as a complement of the existing subdivision strategies. In the subdivision surfaces of Catmull and Clark <ref> [4, 12] </ref> and Loop [18, 6], the subdivision process involves two steps. A refinement step, where a new surface with more vertices and faces is created, and a smoothing step, where the vertices of the new sur face are moved. <p> The problem of shrinkage can be solved by a global operation. If the amount of shrinkage can be predicted in closed form, the skeleton surface can be expanded before the subdivision process is applied. This is what Halstead, Kass, and DeRose <ref> [12] </ref> do. They show how to modify the skeleton mesh so that the subdivision surface associated with the modified skeleton interpolates the vertices of the original skeleton. The subdivision surfaces of Halstead, Kass, and DeRose interpolate the vertices of the original skeleton, and are curvature continuous.
Reference: [13] <author> H. Hoppe, T. DeRose, T. Duchamp, J. McDonald, and W. Stuetzle. </author> <title> Mesh optimization. </title> <journal> Computer Graphics, </journal> <pages> pages 19-26, </pages> <month> August </month> <year> 1993. </year> <note> (Proceedings SIGGRAPH'93). </note>
Reference-contexts: Unless these large surfaces are first simplified <ref> [29, 13, 11] </ref>, or re-meshed using far fewer faces [35], methods based on patch technology, whether parametric [28, 22, 10, 20, 19] or implicit [1, 23], are not acceptable either.
Reference: [14] <author> W.M. Hsu, J.F. Hughes, and H. Kaufman. </author> <title> Direct manipulation of freeform deformations. </title> <journal> Computer Graphics, </journal> <pages> pages 177-184, </pages> <month> July </month> <year> 1992. </year> <note> (Proceedings SIGGRAPH'92). </note>
Reference-contexts: To do so, the values x 1 ; : : : ; x m in equation (10) must be replaced by the desired final values of the faired signal at the corresponding vertices. As in in the Free-form deformation approaches of Hsu, Hughes, and Kaufman <ref> [14] </ref> and Borrel [3], instead of moving control points outside the surface, surfaces can be deformed here by pulling one or more vertices.
Reference: [15] <author> A.D. Kalvin. </author> <title> Segmentation and Surface-Based Modeling of Objects in Three-Dimensional Biomedical Images. </title> <type> PhD thesis, </type> <address> New York University, New York, </address> <month> March </month> <year> 1991. </year>
Reference-contexts: 1 INTRODUCTION The signal processing approach described in this paper was originally motivated by the problem of how to fair large polyhedral surfaces of arbitrary topology, such as those extracted from volumetric medical data by iso-surface construction algorithms <ref> [21, 2, 11, 15] </ref>, or constructed by integration of multiple range images [36].
Reference: [16] <author> M. Kass, A. Witkin, and D. Terzopoulos. Snakes: </author> <title> active contour models. </title> <journal> International Journal of Computer Vision, </journal> <volume> 1(4) </volume> <pages> 321-331, </pages> <year> 1988. </year>
Reference-contexts: As in the fairness norm optimization methods and physics-based deformable models <ref> [16, 34, 30, 26] </ref>, our approach is to move the vertices of the polyhedral surface without changing the connectivity of the faces. The faired surface has exactly the same number of vertices and faces as the original one. However, our signal processing formulation results in much less expensive computations.
Reference: [17] <author> T. Lindeberg. </author> <title> Scale-space for discrete signals. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(3) </volume> <pages> 234-254, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: A more detailed analysis of other filter methodologies is beyond the scope of this paper, and will be done elsewhere [33]. Perhaps the most popular convolution-based smoothing method for parameterized curves is the so-called Gaussian filtering method, associated with scale-space theory <ref> [39, 17] </ref>. In its simplest form, it can be described by the following formula x i = x i + x i ; (4) where 0 &lt; &lt; 1 is a scale factor (for &lt; 0 and 1 the algorithm enhances high frequencies instead of attenuating them).
Reference: [18] <author> C. </author> <title> Loop. Smooth subdivision surfaces based on triangles. </title> <type> Master's thesis, </type> <institution> Dept. of Mathematics, University of Utah, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: By using our fairing algorithm in conjunction with subdivision steps, we achieve more flexibility in the design process. In this way our fairing algorithm can be seen as a complement of the existing subdivision strategies. In the subdivision surfaces of Catmull and Clark [4, 12] and Loop <ref> [18, 6] </ref>, the subdivision process involves two steps. A refinement step, where a new surface with more vertices and faces is created, and a smoothing step, where the vertices of the new sur face are moved.
Reference: [19] <author> C. </author> <title> Loop. A G 1 triangular spline surface of arbitrary topological type. </title> <booktitle> Computer Aided Geometric Design, </booktitle> <volume> 11 </volume> <pages> 303-330, </pages> <year> 1994. </year>
Reference-contexts: Unless these large surfaces are first simplified [29, 13, 11], or re-meshed using far fewer faces [35], methods based on patch technology, whether parametric <ref> [28, 22, 10, 20, 19] </ref> or implicit [1, 23], are not acceptable either.
Reference: [20] <author> C. </author> <title> Loop. Smooth spline surfaces over irregular meshes. </title> <journal> Computer Graphics, </journal> <pages> pages 303-310, </pages> <month> July </month> <year> 1994. </year> <note> (Proceedings SIGGRAPH'94). </note>
Reference-contexts: Unless these large surfaces are first simplified [29, 13, 11], or re-meshed using far fewer faces [35], methods based on patch technology, whether parametric <ref> [28, 22, 10, 20, 19] </ref> or implicit [1, 23], are not acceptable either.
Reference: [21] <author> W. Lorenson and H. Cline. </author> <title> Marching cubes: A high resolution 3d surface construction algorithm. </title> <journal> Computer Graphics, </journal> <pages> pages 163-169, </pages> <month> July </month> <year> 1987. </year> <note> (Proceedings SIGGRAPH). </note>
Reference-contexts: 1 INTRODUCTION The signal processing approach described in this paper was originally motivated by the problem of how to fair large polyhedral surfaces of arbitrary topology, such as those extracted from volumetric medical data by iso-surface construction algorithms <ref> [21, 2, 11, 15] </ref>, or constructed by integration of multiple range images [36].
Reference: [22] <author> M. Lounsbery, S. Mann, and T. DeRose. </author> <title> Parametric surface interpolation. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 12(5) </volume> <pages> 45-52, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Unless these large surfaces are first simplified [29, 13, 11], or re-meshed using far fewer faces [35], methods based on patch technology, whether parametric <ref> [28, 22, 10, 20, 19] </ref> or implicit [1, 23], are not acceptable either.
Reference: [23] <author> J. Menon. </author> <title> Constructive shell representations for freeform surfaces and solids. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 14(2) </volume> <pages> 24-36, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Unless these large surfaces are first simplified [29, 13, 11], or re-meshed using far fewer faces [35], methods based on patch technology, whether parametric [28, 22, 10, 20, 19] or implicit <ref> [1, 23] </ref>, are not acceptable either.
Reference: [24] <author> H.P. Moreton and C.H. Sequin. </author> <title> Functional optimization for fair surface design. </title> <journal> Computer Graphics, </journal> <pages> pages 167-176, </pages> <month> July </month> <year> 1992. </year> <note> (Proceedings SIGGRAPH'92). </note>
Reference-contexts: Since most existing algorithms based on fairness norm optimization <ref> [37, 24, 12, 38] </ref> are prohibitively expensive for very large surfaces a million vertices is not unusual in medical images -, we decided to look for new algorithms with linear time and space complexity [31]. <p> The faired surface has exactly the same number of vertices and faces as the original one. However, our signal processing formulation results in much less expensive computations. In these variational formulations <ref> [5, 24, 38, 12] </ref>, after finite element discretization, the problem is often reduced to the solution of a large sparse linear system, or a more expensive global optimization problem. Large sparse linear systems are solved using iterative methods [9], and usually result in quadratic time complexity algorithms.
Reference: [25] <author> J. Oliensis. </author> <title> Local reproducible smoothing without shrinkage. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 15(3) </volume> <pages> 307-312, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: This can be written in matrix form as x = (I K) x : (5) It is well known though, that Gaussian filtering produces shrinkage, and this is so because the Gaussian kernel is not a low-pass filter kernel <ref> [25] </ref>. To define a low-pass filter, the matrix I K must be replaced by some other function f (K ) of the matrix K. Our non-shrinking fairing algorithm, described in the next section, is one particularly efficient choice.
Reference: [26] <author> A. Pentland and S. Sclaroff. </author> <title> Closed-form solutions for physically based shape modeling and recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(7) </volume> <pages> 715-729, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: As in the fairness norm optimization methods and physics-based deformable models <ref> [16, 34, 30, 26] </ref>, our approach is to move the vertices of the polyhedral surface without changing the connectivity of the faces. The faired surface has exactly the same number of vertices and faces as the original one. However, our signal processing formulation results in much less expensive computations.
Reference: [27] <author> E. Seneta. </author> <title> Non-Negative Matrices, An Introduction to Theory and Applications. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1973. </year>
Reference: [28] <author> L.A. Shirman and C.H. Sequin. </author> <title> Local surface interpolation with bezier patches. </title> <booktitle> Computer Aided Geometric Design, </booktitle> <volume> 4 </volume> <pages> 279-295, </pages> <year> 1987. </year>
Reference-contexts: Unless these large surfaces are first simplified [29, 13, 11], or re-meshed using far fewer faces [35], methods based on patch technology, whether parametric <ref> [28, 22, 10, 20, 19] </ref> or implicit [1, 23], are not acceptable either.
Reference: [29] <author> W.J. Shroeder, A. Zarge, and W.E. Lorensen. </author> <title> Decimation of triangle meshes. </title> <journal> Computer Graphics, </journal> <pages> pages 65-70, </pages> <year> 1992. </year> <note> (Proceedings SIGGRAPH'92). </note>
Reference-contexts: Unless these large surfaces are first simplified <ref> [29, 13, 11] </ref>, or re-meshed using far fewer faces [35], methods based on patch technology, whether parametric [28, 22, 10, 20, 19] or implicit [1, 23], are not acceptable either.
Reference: [30] <author> R.S. Szeliski, D. Tonnesen, and D. Terzopoulos. </author> <title> Modeling surfaces of arbitrary topology with dynamic particles. </title> <booktitle> In Proceedings, IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 82-87, </pages> <address> New York, NY, </address> <month> June 15-17 </month> <year> 1993. </year>
Reference-contexts: As in the fairness norm optimization methods and physics-based deformable models <ref> [16, 34, 30, 26] </ref>, our approach is to move the vertices of the polyhedral surface without changing the connectivity of the faces. The faired surface has exactly the same number of vertices and faces as the original one. However, our signal processing formulation results in much less expensive computations.
Reference: [31] <author> G. Taubin. </author> <title> Curve and surface smoothing without shrinkage. </title> <type> Technical Report RC-19536, </type> <institution> IBM Research, </institution> <month> April </month> <year> 1994. </year> <note> (also in Proceedings ICCV'95). </note>
Reference-contexts: Since most existing algorithms based on fairness norm optimization [37, 24, 12, 38] are prohibitively expensive for very large surfaces a million vertices is not unusual in medical images -, we decided to look for new algorithms with linear time and space complexity <ref> [31] </ref>. Unless these large surfaces are first simplified [29, 13, 11], or re-meshed using far fewer faces [35], methods based on patch technology, whether parametric [28, 22, 10, 20, 19] or implicit [1, 23], are not acceptable either. <p> In terms of future work, we plan to investigate how this approach can be extended to provide alternatives solutions for other important graphics and modeling problems that are usually formulated as variational problems, such as surface reconstruction or surface fitting problems solved with physics-based deformable models. Some related papers <ref> [31, 32] </ref> can be retrieved from the IBM web server (http://www.watson.ibm.com:8080).
Reference: [32] <author> G. Taubin. </author> <title> Estimating the tensor of curvature of a surface from a polyhedral approximation. </title> <type> Technical Report RC-19860, </type> <institution> IBM Research, </institution> <month> December </month> <year> 1994. </year> <note> (also in Proceedings ICCV'95). </note>
Reference-contexts: In terms of future work, we plan to investigate how this approach can be extended to provide alternatives solutions for other important graphics and modeling problems that are usually formulated as variational problems, such as surface reconstruction or surface fitting problems solved with physics-based deformable models. Some related papers <ref> [31, 32] </ref> can be retrieved from the IBM web server (http://www.watson.ibm.com:8080).
Reference: [33] <author> G. Taubin, T. Zhang, and G. Golub. </author> <title> Optimal polyhedral surface smoothing as filter design. </title> <note> (in preparation). </note>
Reference-contexts: This is what a low-pass filter does. We will only consider here low-pass filters implemented as a convolution. A more detailed analysis of other filter methodologies is beyond the scope of this paper, and will be done elsewhere <ref> [33] </ref>. Perhaps the most popular convolution-based smoothing method for parameterized curves is the so-called Gaussian filtering method, associated with scale-space theory [39, 17]. <p> Faster algorithms can be achieved by choosing other polynomial transfer functions, but the analysis of the filter design problem is beyond the scope of this paper, and will be treated elsewhere <ref> [33] </ref>. However, as a rule of thumb, the filter based on the second degree polynomial transfer function of equation (7) can be designed by first choosing a values of k PB .
Reference: [34] <author> D. Terzopoulos and K. Fleischer. </author> <title> Deformable models. </title> <journal> The Visual Computer, </journal> <volume> 4 </volume> <pages> 306-311, </pages> <year> 1988. </year>
Reference-contexts: As in the fairness norm optimization methods and physics-based deformable models <ref> [16, 34, 30, 26] </ref>, our approach is to move the vertices of the polyhedral surface without changing the connectivity of the faces. The faired surface has exactly the same number of vertices and faces as the original one. However, our signal processing formulation results in much less expensive computations.
Reference: [35] <author> G. Turk. </author> <title> Re-tiling polygonal surfaces. </title> <journal> Computer Graphics, </journal> <pages> pages 55-64, </pages> <month> July </month> <year> 1992. </year> <note> (Proceedings SIGGRAPH'92). </note>
Reference-contexts: Unless these large surfaces are first simplified [29, 13, 11], or re-meshed using far fewer faces <ref> [35] </ref>, methods based on patch technology, whether parametric [28, 22, 10, 20, 19] or implicit [1, 23], are not acceptable either.
Reference: [36] <author> G. Turk and M. Levoy. </author> <title> Zippered polygon meshes from range data. </title> <journal> Computer Graphics, </journal> <pages> pages 311-318, </pages> <month> July </month> <year> 1994. </year> <note> (Proceedings SIG-GRAPH'94). </note>
Reference-contexts: 1 INTRODUCTION The signal processing approach described in this paper was originally motivated by the problem of how to fair large polyhedral surfaces of arbitrary topology, such as those extracted from volumetric medical data by iso-surface construction algorithms [21, 2, 11, 15], or constructed by integration of multiple range images <ref> [36] </ref>. Since most existing algorithms based on fairness norm optimization [37, 24, 12, 38] are prohibitively expensive for very large surfaces a million vertices is not unusual in medical images -, we decided to look for new algorithms with linear time and space complexity [31].
Reference: [37] <author> W. Welch and A. Witkin. </author> <title> Variational surface modeling. </title> <journal> Computer Graphics, </journal> <pages> pages 157-166, </pages> <month> July </month> <year> 1992. </year> <note> (Proceedings SIGGRAPH'92). </note>
Reference-contexts: Since most existing algorithms based on fairness norm optimization <ref> [37, 24, 12, 38] </ref> are prohibitively expensive for very large surfaces a million vertices is not unusual in medical images -, we decided to look for new algorithms with linear time and space complexity [31].
Reference: [38] <author> W. Welch and A. Witkin. </author> <title> Free-form shape design using triangulated surfaces. </title> <journal> Computer Graphics, </journal> <pages> pages 247-256, </pages> <month> July </month> <year> 1994. </year> <note> (Proceedings SIGGRAPH'94). </note>
Reference-contexts: Since most existing algorithms based on fairness norm optimization <ref> [37, 24, 12, 38] </ref> are prohibitively expensive for very large surfaces a million vertices is not unusual in medical images -, we decided to look for new algorithms with linear time and space complexity [31]. <p> The faired surface has exactly the same number of vertices and faces as the original one. However, our signal processing formulation results in much less expensive computations. In these variational formulations <ref> [5, 24, 38, 12] </ref>, after finite element discretization, the problem is often reduced to the solution of a large sparse linear system, or a more expensive global optimization problem. Large sparse linear systems are solved using iterative methods [9], and usually result in quadratic time complexity algorithms. <p> Then we concentrate on the applications of this algorithm to interactive free-form fair surface design. As Welch and Witkin <ref> [38] </ref>, in section 3 we design more detailed fair surfaces by combining our fairing algorithm with subdivision techniques. In section 4 we modify our fairing algorithm to accommodate different kinds of constraints. <p> In practice, since the number of faces grows very fast, only a few levels of subdivision are computed. Once the faces are smaller than the resolution of the display, it is not necessary to continue. As Welch and Witkin <ref> [38] </ref>, we are not interested in the limit surfaces, but rather in using subdivision and smoothing steps as tools to design fair polyhedral surfaces in an interactive environment. The classical A C smoothing steps. (A) Skeleton surface. (B) One Gaussian smoothing step ( = 0:5).
Reference: [39] <author> A.P. Witkin. </author> <title> Scale-space filtering. </title> <booktitle> In Proceedings, 8th. International Joint Conference on Artificial Intelligence (IJCAI), </booktitle> <pages> pages 1019-1022, </pages> <address> Karlsruhe, Germany, </address> <month> August </month> <year> 1983. </year>
Reference-contexts: A more detailed analysis of other filter methodologies is beyond the scope of this paper, and will be done elsewhere [33]. Perhaps the most popular convolution-based smoothing method for parameterized curves is the so-called Gaussian filtering method, associated with scale-space theory <ref> [39, 17] </ref>. In its simplest form, it can be described by the following formula x i = x i + x i ; (4) where 0 &lt; &lt; 1 is a scale factor (for &lt; 0 and 1 the algorithm enhances high frequencies instead of attenuating them).
Reference: [40] <author> C.T. Zahn and R.Z. Roskies. </author> <title> Fourier descriptors for plane closed curves. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 21(3) </volume> <pages> 269-281, </pages> <month> March </month> <year> 1972. </year>
Reference-contexts: This is what the method of Fourier descriptors, which dates back to the early 60's, does <ref> [40] </ref>. Our approach to extend Fourier analysis to signals defined on polyhedral surfaces of arbitrary topology is based on the observation that the classical Fourier transform of a signal can be seen as the decomposition of the signal into a linear combination of the eigenvectors of the Laplacian operator.
References-found: 40

