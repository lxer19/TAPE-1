URL: http://wol.ra.phy.cam.ac.uk/mackay/itprnn/book.ps.gz
Refering-URL: http://wol.ra.phy.cam.ac.uk/mackay/itprnn/course.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Information Theory, Inference, and Learning Algorithms  
Author: David J.C. MacKay 
Date: 1996, 1997, 1998  1.6.1 August 31, 1998  
Note: c fl1995,  Draft  
Abstract-found: 0
Intro-found: 1
Reference: <author> Adler, S. L. </author> <title> (1981) Over-relaxation method for the Monte-Carlo evaluation of the partition function for multiquadratic actions. </title> <journal> Physical Review D-Particles and Fields 23 (12): </journal> <pages> 2901-2904. </pages>
Reference: <author> Aiyer, S. V. B., </author> <title> (1991) Solving Combinatorial Optimization Problems Using Neural Networks. </title> <institution> Cambridge University Engineering Department dissertation. </institution> <address> CUED/F-INFENG/TR 89. </address>
Reference: <author> Amit, D. J., Gutfreund, H., and Sompolinsky, H. </author> <title> (1985) Storing infinite numbers of patterns in a spin glass model of neural networks. </title> <journal> Phys. Rev. Lett. </journal> <volume> 55: </volume> <pages> 1530. </pages>
Reference: <author> Angel, J. R. P., Wizinowich, P., Lloyd-Hart, M., and Sandler, D. </author> <title> (1990) Adaptive optics for array telescopes using neural-network techniques. </title> <booktitle> Nature 348: </booktitle> <pages> 221-224. </pages>
Reference-contexts: These networks have been successfully applied to real-world tasks as varied as pronouncing English text (Sejnowski, 1986b) and focussing multiple-mirror telescopes <ref> (Angel et al., 1990) </ref>. 22.3 Neural network learning as inference The neural network learning process above can be given the following probabilistic interpretation. [Here we duplicate and generalize the discussion of chapter 19.] The error function is interpreted as minus the log likelihood for a noise model: P (Djw; fi; H)
Reference: <author> Berger, J. </author> <title> (1985) Statistical Decision theory and Bayesian Analysis. </title> <publisher> Springer. </publisher>
Reference: <author> Berrou, C., Glavieux, A., and Thitimajshima, P. </author> <title> (1993) Near Shannon limit error-correcting coding and decoding: </title> <booktitle> Turbo-codes. In Proc. 1993 IEEE International Conference on Communications, Geneva, </booktitle> <pages> Switzerland , pp. 1064-1070. </pages>
Reference: <author> Bishop, C. M. </author> <title> (1995) Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press. </publisher>
Reference: <author> Blahut, R. E. </author> <booktitle> (1987) Principles and Practice of Information Theory. </booktitle> <address> New York: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Box, G. E. P., and Tiao, G. C. </author> <title> (1973) Bayesian inference in statistical analysis. </title> <publisher> Addison-Wesley. </publisher>
Reference: <author> Bretthorst, G. </author> <title> (1988) Bayesian spectrum analysis and parameter estimation. Springer. </title> <note> Also available at bayes.wustl.edu. </note>
Reference: <author> Bridle, J. S. </author> <title> (1989) Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. In Neuro-computing: algorithms, architectures and applications, </title> <editor> ed. by F. Fougelman-Soulie and J. Herault. </editor> <publisher> Springer-Verlag. </publisher>
Reference-contexts: In this case it is appropriate to use a `softmax' network <ref> (Bridle, 1989) </ref> having coupled outputs which sum to one and are interpreted as class probabilities y i = P (t i = 1jx; w; A).
Reference: <author> Copas, J. B. </author> <title> (1983) Regression, prediction and shrinkage (with discussion). </title> <editor> J. R. </editor> <volume> Statist.Soc B 45 (3): </volume> <pages> 311-354. </pages>
Reference-contexts: The best fit parameters w MP often give over-confident predictions. A non-Bayesian approach to this problem is to down-weight all predictions uniformly, by an empirically determined factor <ref> (Copas, 1983) </ref>. This is not ideal, since intuition suggests the strength of the predictions at B should be downweighted more than those at A. A Bayesian viewpoint helps us to understand the cause of the problem, and provides a straightforward solution that is demonstrably superior to this ad hoc procedure.
Reference: <author> Cover, T. M. </author> <title> (1965) Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. </title> <journal> IEEE Transactions on Electronic Computers 14: </journal> <pages> 326-334. </pages> <address> c fl David J.C. </address> <note> MacKay: Draft 1.6.1. August 31, 19981110001101111110 447 BIBLIOGRAPHY Cover, </note> <editor> T. M., and Thomas, J. A. </editor> <booktitle> (1991) Elements of Information Theory. </booktitle> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Cowles, M. K., and Carlin, B. P. </author> <title> (1996) Markov-chain Monte-Carlo convergence diagnostics | a comparative review. </title> <journal> Journal of the American Statistical Association 91 (434): </journal> <pages> 883-904. </pages>
Reference: <author> Davey, M. C., and MacKay, D. J. C., </author> <title> (1997) Good codes over GF (q) based on very sparse matrices. </title> <note> In preparation. </note>
Reference: <author> Dayan, P., Hinton, G. E., Neal, R. M., and Zemel, R. S. </author> <title> (1995) The Helmholtz machine. </title> <booktitle> Neural Computation 7 (5): </booktitle> <pages> 889-904. </pages>
Reference: <author> Deco, G., and Obradovic, D. </author> <title> (1996) An Information-Theoretic Approach to Neural Computation. </title> <publisher> Springer. </publisher>
Reference: <author> Divsilar, D., and Pollara, F. </author> <title> (1995) On the design of turbo codes. </title> <type> Technical Report TDA 42-123, </type> <institution> Jet Propulsion Laboratory, Pasadena. </institution>
Reference: <author> Duda, R., and Hart, P. </author> <title> (1973) Pattern Classification and Scene Analysis. </title> <publisher> Wiley. </publisher>
Reference: <author> Elias, P. </author> <title> (1975) Universal codeword sets and representations of the integers. </title> <journal> IEEE Transactions on Information Theory 21 (2): </journal> <pages> 194-203. </pages>
Reference: <author> Gallager, R. G. </author> <title> (1962) Low density parity check codes. </title> <journal> IRE Trans. Info. Theory IT-8: </journal> <pages> 21-28. </pages>
Reference: <author> Gallager, R. G. </author> <title> (1963) Low Density Parity Check Codes. </title> <booktitle> Number 21 in Research monograph series. </booktitle> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference: <author> Gilks, W., and Wild, P. </author> <title> (1992) Adaptive rejection sampling for Gibbs sampling. </title> <journal> Applied Statistics 41: </journal> <pages> 337-348. </pages>
Reference: <author> Gilks, W. R., Richardson, S., and Spiegelhalter, D. J. </author> <title> (1996) Markov Chain Monte Carlo in Practice. </title> <publisher> Chapman and Hall. </publisher>
Reference: <author> Goldie, C. M., and Pinch, R. G. E. </author> <title> (1991) Communication theory. </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference: <author> Golomb, S. W., Peile, R. E., and Scholtz, R. A. </author> <title> (1994) Basic Concepts in Information Theory and Coding: The Adventures of Secret Agent 00111 . New York: </title> <publisher> Plenum Press. </publisher>
Reference: <author> Green, P. J. </author> <title> (1995) Reversible jump Markov chain Monte Carlo computation and Bayesian model determination. </title> <journal> Biometrika 82: </journal> <pages> 711-732. </pages>
Reference: <author> Hamming, R. W. </author> <title> (1986) Coding and Information Theory. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <address> 2 edition. </address>
Reference: <author> Hamming, R. W. </author> <booktitle> (1991) The Art of Probability. </booktitle> <address> Redwood City, California: </address> <publisher> Addison Wesley. </publisher>
Reference: <author> Hertz, J., Krogh, A., and Palmer, R. G. </author> <title> (1991) Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley. </publisher>
Reference: <author> Hinton, G. E., Dayan, P., Frey, B. J., and Neal, R. M. </author> <title> (1995) The wake-sleep algorithm for unsupervised neural networks. </title> <booktitle> Science 268 (5214): </booktitle> <pages> 1158-1161. </pages>
Reference: <author> Hinton, G. E., and Ghahramani, Z., </author> <title> (1997) Generative models for discovering sparse distributed representations. </title> <note> submitted to Proc. Roy. Soc. </note> <author> c fl David J.C. MacKay. </author> <note> Draft 1.6.1. August 31, 1998448 1110001110000000 BIBLIOGRAPHY Hinton, </note> <author> G. E., and Sejnowski, T. J. </author> <title> (1986) Learning and relearning in Boltzmann machines. In Parallel Distributed Processing, </title> <editor> ed. by D. E. Rumelhart and J. E. </editor> <booktitle> McClelland, </booktitle> <pages> pp. 282-317. </pages> <address> Cambridge Mass.: </address> <publisher> MIT Press. </publisher>
Reference: <author> Hinton, G. E., and van Camp, D. </author> <title> (1993) Keeping neural networks simple by minimizing the description length of the weights. </title> <booktitle> In Proc. 6th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pp. 5-13. </pages> <publisher> ACM Press, </publisher> <address> New York, NY. </address>
Reference: <author> Hinton, G. E., and Zemel, R. S. </author> <year> (1994) </year> <month> Autoencoders, </month> <title> minimum description length and Helmholtz free energy. </title> <booktitle> In Advances in Neural Information Processing Systems 6 , ed. </booktitle> <editor> by J. D. Cowan, G. Tesauro, and J. Alspector, </editor> <address> San Mateo, California. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Jeffreys, H. </author> <title> (1939) Theory of Probability. </title> <institution> Oxford Univ. </institution> <note> Press. 3rd edition reprinted in paperback 1985. </note>
Reference: <author> Jensen, F. V. </author> <title> (1996) An Introduction to Bayesian Networks. </title> <publisher> London: UCL press. </publisher>
Reference: <author> Kschischang, F. R., and Sorokine, V. </author> <title> (1995) On the trellis structure of block codes. </title> <journal> IEEE Trans. in Inform. </journal> <volume> Theory 41 (6): </volume> <pages> 1924-1937. </pages>
Reference: <author> Lauritzen, S. L. </author> <title> (1996) Graphical Models. Number 17 in Oxford Statistical Science. </title> <publisher> Oxford: Clarendon Press. </publisher>
Reference: <author> MacKay, D. J. C., </author> <title> (1991) Bayesian Methods for Adaptive Models. </title> <institution> California Institute of Technology dissertation. </institution>
Reference: <author> MacKay, D. J. C. </author> <title> (1992a) The evidence framework applied to classification networks. </title> <booktitle> Neural Computation 4 (5): </booktitle> <pages> 698-714. </pages>
Reference-contexts: MacKay. Draft 1.6.1. August 31, 1998374 1110001011101100 19.5: Implementing inference with Gaussian approximations (a) (b) evaluated numerically. In (b) the functions (a; s 2 ) and (a; s 2 ) defined in the text are shown as a function of a for s 2 = 4. From <ref> (MacKay, 1992a) </ref>. (a) (b) (a) A projection of the Gaussian approximation onto the (w 1 ; w 2 ) plane of weight space. The one- and two-standard-deviation contours are shown. Also shown are the trajectory of the optimizer, and the Monte Carlo method's samples. <p> The integral of a sigmoid times a Gaussian cannot be solved analytically, but there are easy ways to handle such one-dimensional integrals; a simple numerical approximation to this integral <ref> (MacKay, 1992a) </ref> is: (a MP ; s 2 ) ' (a MP ; s 2 ) f ((s)a MP ) (19.31) with = 1= p 1 + s 2 =8.
Reference: <author> MacKay, D. J. C. </author> <title> (1992b) A practical Bayesian framework for backpropagation networks. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 448-472. </pages>
Reference: <author> MacKay, D. J. C. </author> <title> (1994) Bayesian non-linear modelling for the prediction competition. </title> <journal> In ASHRAE Transactions, </journal> <volume> V.100, </volume> <pages> Pt.2 , pp. 1053-1062, </pages> <address> Atlanta Georgia. ASHRAE. </address>
Reference: <author> MacKay, D. J. C. </author> <title> (1995) Free energy minimization algorithm for decoding and cryptanalysis. </title> <journal> Electronics Letters 31 (6): </journal> <pages> 446-447. </pages>
Reference: <author> MacKay, D. J. C., and Neal, R. M. </author> <title> (1996) Near Shannon limit performance of low density parity check codes. </title> <journal> Electronics Letters 32 (18): 1645-1646. Reprinted Electronics Letters, </journal> <volume> 33(6) </volume> <pages> 457-458, </pages> <month> March </month> <year> 1997. </year>
Reference: <author> Marinari, E., and Parisi, G. </author> <title> (1992) Simulated tempering anew Monte-Carlo scheme. </title> <journal> Europhysics Letters 19 (6): </journal> <pages> 451-458. </pages>
Reference: <author> McEliece, R. J. </author> <title> (1977) The Theory of Information and Coding: A Mathematical Framework for Communication. </title> <address> Reading, Mass.: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> McEliece, R. J., Posner, E. C., Rodemich, E. R., and Venkatesh, S. S. </author> <title> (1987) The capacity of the hopfield associative memory. </title> <journal> IEEE Transactions on Information Theory 33 (4): </journal> <pages> 461-482. </pages>
Reference: <author> Mosteller, F., and Wallace, D. L. </author> <title> (1984) Applied Bayesian and Classical Inference. The case of The Federalist papers. </title> <publisher> Springer. </publisher>
Reference: <author> Neal, R. M. </author> <title> (1993) Probabilistic inference using Markov chain Monte Carlo methods. </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Dept. of Computer Science, University of Toronto. c fl David J.C. </institution> <note> MacKay: Draft 1.6.1. August 31, 19981110001110000010 449 BIBLIOGRAPHY Neal, </note> <author> R. M. </author> <title> (1995) Suppressing random walks in Markov chain Monte Carlo using ordered overrelaxation. </title> <type> Technical Report 9508, </type> <institution> Dept. of Statistics, University of Toronto. </institution>
Reference: <author> Neal, R. M. </author> <title> (1996) Bayesian Learning for Neural Networks. </title> <booktitle> Number 118 in Lecture Notes in Statistics. </booktitle> <address> New York: </address> <publisher> Springer. </publisher>
Reference-contexts: The sort of functions that we obtain depend on the values of bias , in and out <ref> (Neal, 1996) </ref> (see captions of figures 22.2 & 22.3). As the weights and biases are made bigger we obtain more complex functions with more features and a greater sensitivity to the input variable.
Reference: <author> Neal, R. M. </author> <title> (1997) Markov chain Monte Carlo methods based on `slicing' the density function. </title> <type> Technical Report 9722, </type> <institution> Dept. of Statistics, Univ. of Toronto. </institution>
Reference: <author> Neal, R. M. </author> <title> (1998) Annealed importance sampling. </title> <type> Technical Report 9805, </type> <institution> Dept. of Statistics, Univ. of Toronto. </institution>
Reference: <author> Neal, R. M., and Hinton, G. E. </author> <title> (1993) A new view of the EM algorithm that justifies incremental and other variants. </title> <journal> Biometrika. </journal> <note> submitted. </note>
Reference: <author> O Ruanaidh, J. J. K., and Fitzgerald, W. J. </author> <title> (1996) Numerical Bayesian methods applied to signal processing. </title> <booktitle> Statistics and computing. </booktitle> <address> New York: </address> <publisher> Springer. </publisher>
Reference: <author> Patrick, J. D., and Wallace, C. S. </author> <title> (1982) Stone circle geometries: an information theory approach. In Archaeoastronomy in the Old World , ed. </title> <editor> by D. C. </editor> <booktitle> Heggie, </booktitle> <pages> pp. 231-264. </pages> <publisher> Cambridge Univ. Press. </publisher>
Reference: <author> Pearl, J. </author> <title> (1988) Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Propp, J. G., and Wilson, D. B. </author> <title> (1996) Exact sampling with coupled Markov chains and applications to statistical mechanics. Random Structures and Algorithms 9 (1-2): </title> <type> 223-252. </type>
Reference: <author> Ripley, B. D. </author> <title> (1995) Pattern Recognition and Neural Networks. </title> <publisher> Cambridge. </publisher>
Reference: <author> Rissanen, J. </author> <title> (1978) Modeling by shortest data description. </title> <type> Automatica 14: </type> <pages> 465-471. </pages>
Reference: <author> Rissanen, J., and Langdon, G. G. </author> <title> (1981) Universal modeling and coding. </title> <journal> IEEE Trans. Info. </journal> <volume> Theory 27 (1): </volume> <pages> 12-23. </pages>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <title> (1986) Learning representations by back-propagating errors. </title> <booktitle> Nature 323: </booktitle> <pages> 533-536. </pages>
Reference: <editor> Rumelhart, D. E., and McClelland, J. E. </editor> <booktitle> (1986) Parallel Distributed Processing. </booktitle> <address> Cambridge Mass.: </address> <publisher> MIT Press. </publisher>
Reference: <author> Schwarz, G. </author> <title> (1978) Estimating the dimension of a model. </title> <journal> Ann. Stat. </journal> <volume> 6 (2): </volume> <pages> 461-464. </pages>
Reference: <author> Sejnowski, T. J. </author> <title> (1986a) Higher order Boltzmann machines. In Neural networks for computing, </title> <editor> ed. by J. </editor> <booktitle> Denker, </booktitle> <pages> pp. 398-403, </pages> <address> New York. </address> <publisher> American Institute of Physics. </publisher>
Reference-contexts: instinct might be to create models which directly capture higher order correlations, such as: P 0 (xjW; V; : : :) = Z 0 exp @ 2 ij 1 X V ijk x i x j x k + : : : A : (21.23) Such `higher order Boltzmann machines' <ref> (Sejnowski, 1986a) </ref> are equally easy to simulate using stochastic updates, and the learning rule for the higher order parameters v ijk is equivalent to the learning rule for w ij . Exercise 21.3: Derive the gradient of the log likelihood with respect to v ijk . c fl David J.C.
Reference: <author> Sejnowski, T. J. </author> <year> (1986b) </year> <month> Nettalk: </month> <title> A neural network that learns to read aloud. </title> <editor> In ? , ed. by ?, p. ?, ? ? Shannon, C. E. </editor> <title> (1993) The best detection of pulses. In Collected Papers, </title> <editor> ed. by N. J. A. Sloane and A. D. </editor> <booktitle> Wyner, </booktitle> <pages> pp. 148-150. </pages> <address> New York: </address> <publisher> IEEE Press. </publisher>
Reference-contexts: Rumelhart et al. (1986) that multilayer perceptrons can be trained, by gradient descent on M (w), to discover solutions to non-trivial problems such as deciding whether an image is symmetric or not. These networks have been successfully applied to real-world tasks as varied as pronouncing English text <ref> (Sejnowski, 1986b) </ref> and focussing multiple-mirror telescopes (Angel et al., 1990). 22.3 Neural network learning as inference The neural network learning process above can be given the following probabilistic interpretation. [Here we duplicate and generalize the discussion of chapter 19.] The error function is interpreted as minus the log likelihood for a
Reference: <author> Shannon, C. E., and Weaver, W. </author> <title> (1949) The Mathematical Theory of Communication. </title> <institution> Urbana: Univ. of Illinois Press. </institution>
Reference: <author> Sivia, D. S. </author> <title> (1996) Data Analysis. </title> <publisher> A Bayesian Tutorial . Oxford University Press. </publisher>
Reference: <author> Tanner, M. A. </author> <title> (1996) Tools for Statistical Inference: Methods for the Exploration of Posterior Distributions and Likelihood Functions. </title> <booktitle> Springer Series in Statistics. </booktitle> <publisher> Springer Verlag, 3rd edition. </publisher> <address> c fl David J.C. </address> <note> MacKay. Draft 1.6.1. August 31, 1998450 1110001110000100 BIBLIOGRAPHY Thomas, </note> <author> A., Spiegelhalter, D. J., and Gilks, W. R. </author> <title> (1992) BUGS: A program to perform Bayesian inference using Gibbs sampling. In Bayesian Statistics 4 , ed. </title> <editor> by J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. </editor> <volume> Smith, </volume> <pages> pp. 837-842. </pages> <publisher> Oxford: Clarendon Press. </publisher>
Reference: <author> Wallace, C., and Boulton, D. </author> <title> (1968) An information measure for classification. </title>
Reference: <editor> Comput. J. </editor> <volume> 11 (2): </volume> <pages> 185-194. </pages>
Reference: <author> Wallace, C. S., and Freeman, P. R. </author> <title> (1987) Estimation and inference by compact coding. </title> <journal> J. R. Statist. Soc. </journal> <volume> B 49 (3): </volume> <pages> 240-265. </pages>
Reference: <author> Witten, I. H., Neal, R. M., and Cleary, J. G. </author> <title> (1987) Arithmetic coding for data compression. </title> <booktitle> Communications of the ACM 30 (6): </booktitle> <pages> 520-540. </pages> <address> c fl David J.C. </address> <note> MacKay: Draft 1.6.1. August 31, 19981110001110000110 451 </note>
References-found: 72

