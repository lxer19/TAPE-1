URL: http://arch.cs.ucdavis.edu/~chong/250C/sm-mp/udm.ps
Refering-URL: http://arch.cs.ucdavis.edu/~chong/250C/sm-mp/
Root-URL: http://www.cs.ucdavis.edu
Title: UDM: User Direct Messaging for General-Purpose Multiprocessing  
Author: Kenneth Mackenzie, John Kubiatowicz, Matthew Frank, Walter Lee, Anant Agarwal and M. Frans Kaashoek 
Date: March 8, 1996  
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: User Direct Messaging (UDM) allows user-level, processor-to-processor messaging to coexist with general multiprogramming and virtual memory. Direct messaging, where processors launch and receive messages in tens of cycles directly via network interface FIFOs as opposed to indirectly via memory, offers high message bandwidth and low delivery latency by avoiding memory delay and buffer management overhead. However, user-level direct messaging implementations to date are limited in that they operate only in single-user machines or with strict gang scheduling. In this paper, we develop a messaging approach for protected, direct delivery with a single, unified user interface but with an underlying implementation that provides two delivery cases: a fast, common case corresponding to direct user access to hardware queues and a second case using virtual buffering that is invoked transparently when required by the demands of multiprogramming, virtual memory or user intransigence. The paper lays out a simple, efficient messaging model for user direct messaging that allows both user interrupts and user polling by explicitly incorporating atomicity. The paper then identifies two mechanisms that enable the model to map to a fast, hardware path: a revocable interrupt disable mechanism in hardware permits the user to block the network in a limited way and an overflow control scheme used in the virtual buffer case allows all buffer management overhead to be avoided in the fast path. Experiments with real and synthetic applications on an existing, single-user machine, Alewife, and a new, simulated, multi-user machine, FUGU, show that the cost of the fast case is within a few cycles of the cost of unprotected, kernel messaging and indicate that the fast case can indeed be expected to be the common case under ordinary conditions.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Kenneth Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture (ISCA'95), </booktitle> <pages> pages 2-13, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: On the other hand, systems supporting fine-grain message passing are currently either single-user machines, at best resorting to hard partitioning or strict gang scheduling to permit multiprogramming <ref> [17, 6, 1, 11] </ref> or use alternate techniques that generally add restrictions or overhead (see related work in Section 6). <p> processor and messages are logically and physically passed from processor to processor with no traversal of the 1 We focus in this paper on the UDM communication mechanism; our system also supports complementary communication mechanisms including DMA for bulk transfer and hardware-synthesized messages for accelerating shared memory. 2 memory system <ref> [17, 6, 11, 8, 1] </ref>. Low overhead and latency are achieved by avoiding the memory system, so that message overheads scale with processor performance rather than with memory performance. FUGU, operating at 20MHz.
Reference: [2] <author> Brian N. Bershad, Stefan Savage, Przemysaw Pardyak, Emin Gun Sirer, Marc E. Fiuczynski, David Becker, Craig Chambers, and Susan Eggers. </author> <title> Extensibility, Safety and Performance in the SPIN Operating System. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: This approach recognizes that compilers and application writers are in the best position to evaluate the needs of individual applications; hence, the current trend toward exporting hardware facilities directly to user level <ref> [2, 7] </ref>. While operating systems research has been addressing the construction of complex systems, multiprocessor research has focused on raw performance, primarily because the users of parallel computers have been concerned more with performance than with usability. <p> Messaging through memory is popular 19 because it avoids processor modifications and decouples the delivery of the message data from the delivery of the message event. * In a system with kernel messages, flexibility may be recovered by contriving to safety-check and then download user-provided message handlers into the kernel <ref> [2, 7] </ref>. Several previous multicomputers have provided direct messaging. The Mosaic [17] and J-machine [6] multicomputers are single-user machines. The J-machine provides two levels of network priorities and the ability to relaunch incoming messages from memory transparently.
Reference: [3] <author> Matthias A. Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, Edward W. Felten, and Jonathan Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings 21st Annual International Symposium on Computer Architecture (ISCA'94), </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: However, combining communication performance with protection and other usability features, such as virtual memory, taken for granted in modern operating systems has not been straightforward. Memory-based communication such as shared-memory or bulk, memory-to-memory message passing can use the same memory protection mechanisms as are used in uniprocessors <ref> [3, 22] </ref>. On the other hand, systems supporting fine-grain message passing are currently either single-user machines, at best resorting to hard partitioning or strict gang scheduling to permit multiprogramming [17, 6, 1, 11] or use alternate techniques that generally add restrictions or overhead (see related work in Section 6). <p> Packets can be sorted into request and reply networks because the message protocols are controlled by the kernel-level coprocessor. FUGU mostly uses one network with mixed kernel and user traffic, but occasional resorts to use of the second network to avoid deadlock. SHRIMP <ref> [3] </ref> and Hamlyn [22] implement protected user-level message passing through a remote-write model. Communication is possible between pairs of virtual pages. Protection is provided by memory mapping but pages must be pre-negotiated and pinned.
Reference: [4] <author> Eric Brewer, Fred Chong, Lok Liu, Shamik Sharma, and John Kubiatowicz. </author> <title> Remote Queues: Exposing Message Queues for Optimization and Atomicity. </title> <booktitle> In Proceedings of the Symposium on Parallel Algorithms and Architectures (SPAA'95). ACM, </booktitle> <year> 1995. </year> <month> 21 </month>
Reference-contexts: The current Alewife system emulates the atomicity system using the system timer and a reserved flags register in the processor at a cost of approximately 30 extra cycles per interrupt handler <ref> [4] </ref>. The Alewife software system provides a particularly aggressive use of extremely lightweight threads: interrupt handlers execute as general threads that can block and that possess their own stacks. To achieve this speed, we take advantage of the existence of two or more independent register sets. <p> Table 4 details the cost of sending and receiving messages in FUGU at user level, in Alewife at user level and at kernel level in either machine. The Alewife cycles breakdowns come from <ref> [4] </ref>, verified by microbenchmarks run on the Alewife hardware. The kernel and FUGU cycle counts are made from simulator traces. The send cost is for a blocking send operation. The interrupt-based receive cost represents the basic fast path cost.
Reference: [5] <author> John Chapin, Mendel Rosenblum, Scott Devine, Tirthankar Lahiri, Dan Teodosiu, and Anoop Gupta. Hive: </author> <title> Fault Containment for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: FLASH [10] uses a kernel-level coprocessor for message handling including shared-memory protocol messages. The coprocessor implements synchronization primitives and provides user messaging through memory. Kernel messages serviced by the main processor are also implemented at greater cost <ref> [5] </ref>. Unlike FUGU, Flash does not provide general-purpose, user-level messaging except through memory. FLASH includes two networks but for a different reason than FUGU. FLASH uses its two networks for request and reply packets to avoid deadlock.
Reference: [6] <author> William J. Dally et al. </author> <title> The J-Machine: A Fine-Grain Concurrent Computer. </title> <booktitle> In Proceedings of the IFIP (International Federation for Information Processing), 11th World Congress, </booktitle> <pages> pages 1147-1153, </pages> <address> New York, 1989. </address> <publisher> Elsevier Science Publishing. </publisher>
Reference-contexts: On the other hand, systems supporting fine-grain message passing are currently either single-user machines, at best resorting to hard partitioning or strict gang scheduling to permit multiprogramming <ref> [17, 6, 1, 11] </ref> or use alternate techniques that generally add restrictions or overhead (see related work in Section 6). <p> processor and messages are logically and physically passed from processor to processor with no traversal of the 1 We focus in this paper on the UDM communication mechanism; our system also supports complementary communication mechanisms including DMA for bulk transfer and hardware-synthesized messages for accelerating shared memory. 2 memory system <ref> [17, 6, 11, 8, 1] </ref>. Low overhead and latency are achieved by avoiding the memory system, so that message overheads scale with processor performance rather than with memory performance. FUGU, operating at 20MHz. <p> Several previous multicomputers have provided direct messaging. The Mosaic [17] and J-machine <ref> [6] </ref> multicomputers are single-user machines. The J-machine provides two levels of network priorities and the ability to relaunch incoming messages from memory transparently. The CM-5 multicomputer [11] provides direct, user-level messaging and allows multiprogramming via strict gang scheduling.
Reference: [7] <author> Dawson R. Engler, M. Frans Kaashoek, and Jr. James O'Toole. Exokernel: </author> <title> An Operating System Architecture for Application-Level Resource Management. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: This approach recognizes that compilers and application writers are in the best position to evaluate the needs of individual applications; hence, the current trend toward exporting hardware facilities directly to user level <ref> [2, 7] </ref>. While operating systems research has been addressing the construction of complex systems, multiprocessor research has focused on raw performance, primarily because the users of parallel computers have been concerned more with performance than with usability. <p> The GID stamp and check are also currently implemented in software. We account for these differences in the results by explicitly counting the costs in cycles of the software emulation. The FUGU operating system, Glaze, is a custom multiuser operating system based on the Aegis Exokernel <ref> [7] </ref> and is under active development. The operating system supports multiprogramming, virtual memory, messages and user-level threads. Glaze implements the UDM model including virtual buffering used in response to GID mismatches and page faults, 5 although message timeouts are currently fatal. <p> Messaging through memory is popular 19 because it avoids processor modifications and decouples the delivery of the message data from the delivery of the message event. * In a system with kernel messages, flexibility may be recovered by contriving to safety-check and then download user-provided message handlers into the kernel <ref> [2, 7] </ref>. Several previous multicomputers have provided direct messaging. The Mosaic [17] and J-machine [6] multicomputers are single-user machines. The J-machine provides two levels of network priorities and the ability to relaunch incoming messages from memory transparently.
Reference: [8] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A Tightly-Coupled Processor-Network Interface. </title> <booktitle> In Fifth Inter-nataional Architectural Support for Programming Languages and Operating Systems (ASPLOS V), </booktitle> <address> Boston, </address> <month> October </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: processor and messages are logically and physically passed from processor to processor with no traversal of the 1 We focus in this paper on the UDM communication mechanism; our system also supports complementary communication mechanisms including DMA for bulk transfer and hardware-synthesized messages for accelerating shared memory. 2 memory system <ref> [17, 6, 11, 8, 1] </ref>. Low overhead and latency are achieved by avoiding the memory system, so that message overheads scale with processor performance rather than with memory performance. FUGU, operating at 20MHz.
Reference: [9] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In Proceedings of the International Supercomputing Conference (ISC) 1993, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year> <note> IEEE. Also as MIT/LCS TM-498, </note> <month> December </month> <year> 1992. </year>
Reference-contexts: The injection operation is atomic in that messages are committed to the network in their entirety; no partial packets are ever seen by the communication substrate <ref> [9] </ref>. Message injection can thus be viewed in the following fashion: send (header, handler, word0, word1, : : : ) If resource contention prevents the network from accepting a given message, the corresponding send operation blocks until successful. <p> Further discussion of buffering is deferred to Section 3.3. Send and Receive. The send operation of the abstract model is decomposed into a two-phase process of describe and launch, as used in <ref> [9] </ref>.
Reference: [10] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA) 1994, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: For instance, *T demultiplexes messages into several receive queues, and the receive queues are implemented as a part of the processor register set. The CS-2 coprocessor primarily demultiplexes messages into memory but allows some user-level processing of messages, albeit with limits in speed and functionality [16]. FLASH <ref> [10] </ref> uses a kernel-level coprocessor for message handling including shared-memory protocol messages. The coprocessor implements synchronization primitives and provides user messaging through memory. Kernel messages serviced by the main processor are also implemented at greater cost [5]. Unlike FUGU, Flash does not provide general-purpose, user-level messaging except through memory.
Reference: [11] <author> Charles E. Leiserson, Aahil S. Abuhamdeh, and David C. Douglas et al. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In The Fourth Annual ACM Symposium on Parallel Algorithms and Architectures. ACM, </booktitle> <year> 1992. </year>
Reference-contexts: On the other hand, systems supporting fine-grain message passing are currently either single-user machines, at best resorting to hard partitioning or strict gang scheduling to permit multiprogramming <ref> [17, 6, 1, 11] </ref> or use alternate techniques that generally add restrictions or overhead (see related work in Section 6). <p> processor and messages are logically and physically passed from processor to processor with no traversal of the 1 We focus in this paper on the UDM communication mechanism; our system also supports complementary communication mechanisms including DMA for bulk transfer and hardware-synthesized messages for accelerating shared memory. 2 memory system <ref> [17, 6, 11, 8, 1] </ref>. Low overhead and latency are achieved by avoiding the memory system, so that message overheads scale with processor performance rather than with memory performance. FUGU, operating at 20MHz. <p> Several previous multicomputers have provided direct messaging. The Mosaic [17] and J-machine [6] multicomputers are single-user machines. The J-machine provides two levels of network priorities and the ability to relaunch incoming messages from memory transparently. The CM-5 multicomputer <ref> [11] </ref> provides direct, user-level messaging and allows multiprogramming via strict gang scheduling. Typhoon [15], *T [13] and the Meiko CS-2 [16] provide coprocessors with the ability to run user code. Typhoon's coprocessor is protected by hard gang scheduling in the manner of the CM-5.
Reference: [12] <author> Kevin Lew, Kirk Johnson, and Frans Kaashoek. </author> <title> A Case Study of Shared-Memory and Message-Passing Implementations of Parallel Breadth-First Search: The Triangle Puzzle. </title> <booktitle> In Third DIMACS International Algorithm Implementation Challenge Workshop, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: The application, Tpuz, performs an exhaustive search of board positions in the triangle puzzle <ref> [12] </ref> a simple game. The application operates in stages separated by barriers, where at each stage processors exchange many unacknowledged messages with one another. The messaging in Tpuz can be characterized by T handler ' 170 instructions and T interhandler ' 950 instructions.
Reference: [13] <author> R. S. Nikhil, G. M. Papadopoulos, and Arvind. </author> <title> *T: A Multithreaded Massively Parallel Architecture. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167. </pages> <publisher> ACM, </publisher> <year> 1992. </year>
Reference-contexts: The Mosaic [17] and J-machine [6] multicomputers are single-user machines. The J-machine provides two levels of network priorities and the ability to relaunch incoming messages from memory transparently. The CM-5 multicomputer [11] provides direct, user-level messaging and allows multiprogramming via strict gang scheduling. Typhoon [15], *T <ref> [13] </ref> and the Meiko CS-2 [16] provide coprocessors with the ability to run user code. Typhoon's coprocessor is protected by hard gang scheduling in the manner of the CM-5.
Reference: [14] <author> John K. Ousterhout. </author> <title> Scheduling Techniques for Concurrent Systems. </title> <booktitle> In 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pages 22-30, </pages> <year> 1982. </year>
Reference-contexts: Second, an application must operate correctly despite scheduling uncertainty introduced by multiprogramming and virtual memory. Finally, the application should achieve essentially the same performance as on a single-user machine provided the system scheduler succeeds at co-scheduling (gang scheduling, but only loosely and on demand <ref> [14, 18] </ref>) the processes in the application.
Reference: [15] <author> Steve K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture (ISCA) 1994, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: The Mosaic [17] and J-machine [6] multicomputers are single-user machines. The J-machine provides two levels of network priorities and the ability to relaunch incoming messages from memory transparently. The CM-5 multicomputer [11] provides direct, user-level messaging and allows multiprogramming via strict gang scheduling. Typhoon <ref> [15] </ref>, *T [13] and the Meiko CS-2 [16] provide coprocessors with the ability to run user code. Typhoon's coprocessor is protected by hard gang scheduling in the manner of the CM-5.
Reference: [16] <author> Klaus E. Schauser and Chris J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In Proceedings of the 9th International Symposium on Parallel Processing, </booktitle> <year> 1995. </year>
Reference-contexts: The J-machine provides two levels of network priorities and the ability to relaunch incoming messages from memory transparently. The CM-5 multicomputer [11] provides direct, user-level messaging and allows multiprogramming via strict gang scheduling. Typhoon [15], *T [13] and the Meiko CS-2 <ref> [16] </ref> provide coprocessors with the ability to run user code. Typhoon's coprocessor is protected by hard gang scheduling in the manner of the CM-5. <p> For instance, *T demultiplexes messages into several receive queues, and the receive queues are implemented as a part of the processor register set. The CS-2 coprocessor primarily demultiplexes messages into memory but allows some user-level processing of messages, albeit with limits in speed and functionality <ref> [16] </ref>. FLASH [10] uses a kernel-level coprocessor for message handling including shared-memory protocol messages. The coprocessor implements synchronization primitives and provides user messaging through memory. Kernel messages serviced by the main processor are also implemented at greater cost [5].
Reference: [17] <author> C.L. Seitz, N.J. Boden, J. Seizovic, and W.K. Su. </author> <title> The Design of the Caltech Mosaic C Multicomputer. </title> <booktitle> In Research on Integrated Systems Symposium Proceedings, </booktitle> <pages> pages 1-22, </pages> <address> Cambridge, MA, 1993. </address> <publisher> MIT Press. </publisher>
Reference-contexts: On the other hand, systems supporting fine-grain message passing are currently either single-user machines, at best resorting to hard partitioning or strict gang scheduling to permit multiprogramming <ref> [17, 6, 1, 11] </ref> or use alternate techniques that generally add restrictions or overhead (see related work in Section 6). <p> processor and messages are logically and physically passed from processor to processor with no traversal of the 1 We focus in this paper on the UDM communication mechanism; our system also supports complementary communication mechanisms including DMA for bulk transfer and hardware-synthesized messages for accelerating shared memory. 2 memory system <ref> [17, 6, 11, 8, 1] </ref>. Low overhead and latency are achieved by avoiding the memory system, so that message overheads scale with processor performance rather than with memory performance. FUGU, operating at 20MHz. <p> Several previous multicomputers have provided direct messaging. The Mosaic <ref> [17] </ref> and J-machine [6] multicomputers are single-user machines. The J-machine provides two levels of network priorities and the ability to relaunch incoming messages from memory transparently. The CM-5 multicomputer [11] provides direct, user-level messaging and allows multiprogramming via strict gang scheduling.
Reference: [18] <author> Patrick G. Sobalvarro and William E. Weihl. </author> <title> Demand-based Coscheduling of Parallel Jobs on Multipro-grammed Multiprocessors. </title> <booktitle> In Lecture Notes in Computer Science, number 949, </booktitle> <address> Santa Barbara, </address> <year> 1995. </year> <title> Springer Verlag. Workshop on Parallel Job Scheduling, </title> <booktitle> IPPS '95. </booktitle>
Reference-contexts: Second, an application must operate correctly despite scheduling uncertainty introduced by multiprogramming and virtual memory. Finally, the application should achieve essentially the same performance as on a single-user machine provided the system scheduler succeeds at co-scheduling (gang scheduling, but only loosely and on demand <ref> [14, 18] </ref>) the processes in the application.
Reference: [19] <author> Chandramohan A. Thekkath, Henry M. Levy, and Edward D. Lazowska. </author> <title> Efficient Support for Multicomputing on ATM Networks. </title> <institution> UW-CSE 93-04-03, University of Washington, </institution> <address> Seattle, WA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Communication is possible between pairs of virtual pages. Protection is provided by memory mapping but pages must be pre-negotiated and pinned. Work on parallel processing on networks of workstations seeks to identify mechanisms to accelerate communication while retaining protection. Thekkath's RMA model <ref> [19] </ref> separates data transfer from notification in a network of workstations. The data transfer can be protected by memory mapping mechanisms. The uNET system [20] provides support for short, user-level messages in a network of workstations by demultiplexing messages into pre-negotiated, per-process receive buffers in memory.
Reference: [20] <author> Thorsten von Eicken, Anindya Basu, Vineet Buch, and Werner Vogels. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles. ACM, </booktitle> <month> December </month> <year> 1995. </year> <month> 22 </month>
Reference-contexts: Work on parallel processing on networks of workstations seeks to identify mechanisms to accelerate communication while retaining protection. Thekkath's RMA model [19] separates data transfer from notification in a network of workstations. The data transfer can be protected by memory mapping mechanisms. The uNET system <ref> [20] </ref> provides support for short, user-level messages in a network of workstations by demultiplexing messages into pre-negotiated, per-process receive buffers in memory. The arrival of messages is detected by polling. uNet uses the ATM VCI field in a manner similar to FUGU's GID field.
Reference: [21] <author> Thorsten von Eicken, David Culler, Seth Goldstein, and Klaus Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: The model is similar to Active Messages <ref> [21] </ref> but explicitly defines and recognizes atomic sections in which the user is given (apparent) privilege to disable message arrival interrupts for polling and interrupt handler management. The result is fast, low-overhead, user-level messaging that smoothly integrates interrupts and polling. The implementation utilizes a combination of hardware and software mechanisms. <p> Messaging Model. A message is a variable-length sequence of words. Two of these words are specialized: the first is an implementation-dependent routing header which specifies the destination of the message; the second is an optional handler address, as used in Active Messages <ref> [21] </ref>. Remaining words represent the data payload and are unconstrained. The semantics of messaging are asynchronous and unacknowledged. At the source, messages are injected into the network at any rate up to and including the rate at which the network will accept them. <p> To our knowledge UDM is the first general-purpose communication model that provides efficient protected communication for fine-grained messages and directly supports a wide range of communication styles. The UDM interface is similar to Active Messages <ref> [21] </ref>. The chief focus of Active Messages is that, since an active message is within one domain, it can carry a raw pointer to code to be executed on the receive side, thus permitting many message types to be crafted for situations.
Reference: [22] <author> John Wilkes. </author> <title> Hamlyn An Interface for Sender-Based Communications. </title> <type> Department technical report HPL-OSR-92-13, </type> <institution> HP Labs OS Research, </institution> <month> November </month> <year> 1992. </year> <month> 23 </month>
Reference-contexts: However, combining communication performance with protection and other usability features, such as virtual memory, taken for granted in modern operating systems has not been straightforward. Memory-based communication such as shared-memory or bulk, memory-to-memory message passing can use the same memory protection mechanisms as are used in uniprocessors <ref> [3, 22] </ref>. On the other hand, systems supporting fine-grain message passing are currently either single-user machines, at best resorting to hard partitioning or strict gang scheduling to permit multiprogramming [17, 6, 1, 11] or use alternate techniques that generally add restrictions or overhead (see related work in Section 6). <p> Packets can be sorted into request and reply networks because the message protocols are controlled by the kernel-level coprocessor. FUGU mostly uses one network with mixed kernel and user traffic, but occasional resorts to use of the second network to avoid deadlock. SHRIMP [3] and Hamlyn <ref> [22] </ref> implement protected user-level message passing through a remote-write model. Communication is possible between pairs of virtual pages. Protection is provided by memory mapping but pages must be pre-negotiated and pinned. Work on parallel processing on networks of workstations seeks to identify mechanisms to accelerate communication while retaining protection.
References-found: 22

