URL: http://www-robotics.usc.edu/~maja/publications/iros98.ps.gz
Refering-URL: http://www-robotics.usc.edu/~maja/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: michaudf@gel.usherb.ca mataric@cs.usc.edu  
Title: Learning from History for Adaptive Mobile Robot Control  
Author: Fran~cois Michaud Maja J Mataric 
Address: Sherbrooke (Quebec Canada) J1K 2R1 Los Angeles, CA 90089-0781  
Affiliation: Dept. Elect. Comp. Engineering Computer Science Department Universite de Sherbrooke University of Southern California  
Abstract: Learning in the mobile robot domain is a very challenging task, especially in non-stationary conditions. This paper presents an approach that allows a robot to learn a model of its interactions with its operating environment in order to manage them according to the experienced dynamics. The robot is initially given a set of "behavior-producing" modules to choose from, and the algorithm provides a means of making that choice intelligently and dynamically. The approach is validated using a vision- and sonar-based Pioneer I robot in non-stationary conditions, in the context of a multi-robot foraging task. Results show the effectiveness of the approach in taking advantage of any regularities experienced in the world, leading to fast and adaptable specialization for the learning robot. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. A. Brooks. </author> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> RA-2(1):14-23, </volume> <month> March </month> <year> 1986. </year>
Reference-contexts: This approach bypasses complete autonomy (by giving up tabula rasa learning techniques [3]) in favor of incorporating a bias that accelerates the learning process to achieve worthwhile performance rapidly. To do so, the control policy can be decomposed into "behavior-producing" modules, also called behaviors <ref> [1] </ref>. This approach has been praised for its robustness and simplicity of construction. In a typical behavior-based system, the constituent behaviors are designed parsimoniously, executed in parallel, and prioritized using some fixed or flexible arbitration mechanism [7]. <p> The actual duration of behavior use depends on discrete sensory and temporal events encoded in rules, and on commands issued by other behaviors subsuming it. Note that the organization follows the Subsumption Architecture <ref> [1] </ref> with the difference that the behaviors that are allowed to issue outputs (i.e., activated behaviors) change dynamically.
Reference: [2] <author> R. A. Brooks. </author> <title> MARS: Multiple Agency Reactivity System. </title> <type> Tech. Report, </type> <note> IS Robotics, </note> <year> 1996. </year>
Reference-contexts: The robot is programmed using MARS, a language for programming multiple concurrent processes and behaviors <ref> [2] </ref>. The experiments were conducted in an enclosed 12'fi11' rectangular pen containing pink blocks and a home region, marked with a green cylinder.
Reference: [3] <author> L. P. Kaelbling, M. L. Littman, and A. W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: This approach bypasses complete autonomy (by giving up tabula rasa learning techniques <ref> [3] </ref>) in favor of incorporating a bias that accelerates the learning process to achieve worthwhile performance rapidly. To do so, the control policy can be decomposed into "behavior-producing" modules, also called behaviors [1]. This approach has been praised for its robustness and simplicity of construction. <p> two Pioneers or more R1s in the pen did not influence the ability of the learning robot to find stable strategies; the dynamics of the interactions had a more significant effect. 6 Related work Our algorithm learns at the behavior selection level (i.e., decides which behavior's output should be executed <ref> [3] </ref>). Past approaches to learning behavior selection [4, 5, 6] used sensory inputs as the selection criterion. In contrast, our approach uses the stored history of past behavior use. <p> In contrast, our approach uses the stored history of past behavior use. Our history-based approach was inspired by [8], whose algorithm partitions the state space from raw sensory experiences and learns a variable-width window that serves simultaneously as a model of the environment and a finite-memory policy <ref> [3] </ref>. In our case, the algorithm learns a finite-memory strategy of behaviors use, since our tree representation uses behaviors as an abstraction.
Reference: [4] <author> P. Maes and R. A. Brooks. </author> <title> Learning to coordinate behaviors. </title> <booktitle> In Proc. National Conf. on Artificial Intelligence (AAAI), </booktitle> <volume> Vol. 2, </volume> <pages> pp. 796-802, </pages> <year> 1990. </year>
Reference-contexts: Past approaches to learning behavior selection <ref> [4, 5, 6] </ref> used sensory inputs as the selection criterion. In contrast, our approach uses the stored history of past behavior use.
Reference: [5] <author> S. Mahadevan and J. Connell. </author> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55 </volume> <pages> 311-365, </pages> <year> 1992. </year>
Reference-contexts: Past approaches to learning behavior selection <ref> [4, 5, 6] </ref> used sensory inputs as the selection criterion. In contrast, our approach uses the stored history of past behavior use.
Reference: [6] <author> M. J. Mataric. </author> <title> Reinforcement learning in the multi-robot domain. </title> <booktitle> Autonomous Robots, </booktitle> <volume> 4(1) </volume> <pages> 73-83, </pages> <year> 1997. </year>
Reference-contexts: Past approaches to learning behavior selection <ref> [4, 5, 6] </ref> used sensory inputs as the selection criterion. In contrast, our approach uses the stored history of past behavior use.
Reference: [7] <author> M. J. Mataric. </author> <title> Behavior-based control: Examples from navigation, learning, and group behavior. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <pages> 9(2-3), </pages> <year> 1997. </year>
Reference-contexts: This approach has been praised for its robustness and simplicity of construction. In a typical behavior-based system, the constituent behaviors are designed parsimoniously, executed in parallel, and prioritized using some fixed or flexible arbitration mechanism <ref> [7] </ref>. A fixed and parsimonious behavior set, however, does not allow a robot to easily adapt to changes in the environment, thus resulting in diminished flexibility. The ability to adapt to changing dynamics, however, is especially important for robots operating in unpredictable and non-stationary environments.
Reference: [8] <author> A. K. McCallum. </author> <title> Learning to use selective attention and short-term memory in sequential tasks. </title> <booktitle> In Proc. 4th Int'l Conf. on Simulation of Adaptive Behavior, </booktitle> <pages> pp. 315-324, </pages> <address> Cape Cod, </address> <year> 1996. </year>
Reference-contexts: Past approaches to learning behavior selection [4, 5, 6] used sensory inputs as the selection criterion. In contrast, our approach uses the stored history of past behavior use. Our history-based approach was inspired by <ref> [8] </ref>, whose algorithm partitions the state space from raw sensory experiences and learns a variable-width window that serves simultaneously as a model of the environment and a finite-memory policy [3].
Reference: [9] <author> F. Michaud, G. Lachiver, and C. T. Le Dinh. </author> <title> A new control architecture combining reactivity, deliberation and motivation for situated autonomous agent. </title> <booktitle> In Proc. 4th Int'l Conf. on Simulation of Adaptive Behavior, </booktitle> <pages> pp. 245-254, </pages> <address> Cape Cod, </address> <year> 1996. </year>
Reference-contexts: In our case, the algorithm learns a finite-memory strategy of behaviors use, since our tree representation uses behaviors as an abstraction. The development of our algorithm was derived from a general control architecture for intelligent agent <ref> [9] </ref>, which is based on dynamic selection of behaviors. 7 Summary and conclusions The goal of our approach is to enable a robot to learn and utilize the interaction dynamics with its environment.
References-found: 9

