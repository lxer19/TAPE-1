URL: http://L2R.cs.uiuc.edu/~danr/Papers/partialJ.ps.gz
Refering-URL: http://L2R.cs.uiuc.edu/~danr/publications.html
Root-URL: http://www.cs.uiuc.edu
Email: roni@dcs.ed.ac.uk  danr@cs.uiuc.edu  
Title: Learning to Reason with a Restricted View  
Author: Roni Khardon Dan Roth 
Address: The King's Buildings Edinburgh EH9 3JZ Scotland  1304 W. Springfield Ave. Urbana, IL 61801 USA  
Affiliation: Department of Computer Science University of Edinburgh  Department of Computer Science University of Illinois at Urbana-Champaign  
Note: Submitted for publication  
Abstract: The Learning to Reason framework combines the study of Learning and Reasoning into a single task. Within it, learning is done specifically for the purpose of reasoning with the learned knowledge. Computational considerations show that this is a useful paradigm; in some cases learning and reasoning problems that are intractable when studied separately become tractable when performed as a task of Learning to Reason. In this paper we study Learning to Reason problems where the interaction with the world supplies the learner only partial information in the form of partial assignments. Several natural interpretations of partial assignments are considered and learning and reasoning algorithms using these are developed. The results presented exhibit a tradeoff between learnability, the strength of the oracles used in the interface, and the range of reasoning queries the learner is guaranteed to answer correctly.
Abstract-found: 1
Intro-found: 1
Reference: <author> Angluin, D. </author> <year> 1988. </year> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <month> April. </month>
Reference-contexts: We also show that on-line L2R algorithms exist for queries in k-CNF. Finally, we show that with stronger oracles, either a membership oracle <ref> (Angluin, 1988) </ref> for partial observations or entailment queries (Frazier and Pitt, 1993), L2R is possible for the same classes of queries discussed in (Khardon and Roth, 1994a).
Reference: <author> Ben-David, S. and E. Dichterman. </author> <year> 1993. </year> <title> Learning with restricted focus of attention. </title> <booktitle> In Proc. of the Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 287-296. </pages> <publisher> ACM Press, </publisher> <address> New York, NY. </address>
Reference: <author> Blum, A. </author> <year> 1992. </year> <title> Learning boolean functions in an infinite attribute space. </title> <journal> Machine Learning, </journal> <volume> 9(4) </volume> <pages> 373-386, </pages> <month> October. </month>
Reference-contexts: The dependence of the mistake bound can be reduced in a different way, which holds for all the interpretations. This is done by converting the algorithm A-OL-NIMP to an attribute-efficient algorithm using the Winnow algorithm (Littlestone, 1988), similar to what is done in <ref> (Blum, 1992) </ref>. The resulting mistake bound is of the form log m N (W ). The knowledge representation used by the new algorithm is not a list of models any more.
Reference: <author> Blum, A., L. Hellerstein, and N. Littlestone. </author> <year> 1991. </year> <title> Learning in the presence of finitely or infinitely many irrelevant attribute. </title> <booktitle> In Proc. of the Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 155-166. </pages>
Reference: <author> Bshouty, N. H. </author> <year> 1995. </year> <title> Exact learning via the monotone theory. </title> <journal> Information and Computation, </journal> <volume> 123(1) </volume> <pages> 146-153. </pages>
Reference-contexts: A model based approach becomes feasible if one can make correct inferences when working with a small subset of models. Some results along this line have been obtained (Kautz, Kearns, and Selman, 1995; Khardon and Roth, 1994b). In particular, previous work in (Khardon and Roth, 1994b), using ideas from <ref> (Bshouty, 1995) </ref>, identified a small set of models, called the set of characteristic models of W , that supports correct reasoning. We briefly describe some of the relevant results we need, culminating in Theorem 4.1 that identifies the models needed for the algorithm MBR to be correct and efficient.
Reference: <author> Cook, S. A. </author> <year> 1971. </year> <title> The complexity of theorem proving procedures. </title> <booktitle> In 3rd annual ACM Symposium of the Theory of Computing, </booktitle> <pages> pages 151-158. </pages>
Reference-contexts: In particular, if W is an arbitrary Boolean circuit, the learning problem is known to be hard (independent of the representation (Kearns and Valiant, 1994)), and the reasoning problem is also hard, regardless of the class of queries <ref> (Cook, 1971) </ref>. However, by restricting the queries to k-CNF we show that one can learn to reason.
Reference: <author> De Raedt, L. </author> <year> 1997. </year> <title> Logical settings for concept learning. </title> <journal> Artificial Inteligence, </journal> <volume> 95(1) </volume> <pages> 187-201. </pages>
Reference-contexts: In this sense, hence, ILP algorithms use a fixed knowledge representation to learn to reason about such clauses. Some of our results can therefore be phrased in terms of ILP. Using terminology from <ref> (De Raedt, 1997) </ref> these imply for example that k-CNF is learnable from entailment. While in this paper we concentrate on deductive reasoning, the learning to reason approach should be seen in a more general context and can be applied for a variety of tasks. <p> However, by restricting the queries to k-CNF we show that one can learn to reason. Finally, we note that in ILP terminology <ref> (De Raedt, 1997) </ref> the above shows that k-CNF is learnable from entailment. 6.4 Using Stronger Oracles The results of Section 4.2 suggest that the only way to improve the results we have shown so far, and reason with classes of queries that are wider than k-CNF, is to use total models
Reference: <author> Frazier, M. and L. Pitt. </author> <year> 1993. </year> <title> Learning from entailment: An application to propositional Horn sentences. </title> <booktitle> In Proc. of the International Conference on Machine Learning, </booktitle> <pages> pages 120-127. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We also show that on-line L2R algorithms exist for queries in k-CNF. Finally, we show that with stronger oracles, either a membership oracle (Angluin, 1988) for partial observations or entailment queries <ref> (Frazier and Pitt, 1993) </ref>, L2R is possible for the same classes of queries discussed in (Khardon and Roth, 1994a). In fact, our proofs show that these oracles are strong enough to enable the learner to complete partial observations into total ones and learn in this way. <p> On the other hand, EQ with partial assignments has more freedom than EQ with total assignments when choosing the counterexamples and is therefore weaker (since in addition to total assignments it can supply partial assignments). The next oracles, introduced in <ref> (Frazier and Pitt, 1993) </ref>, can be thought of as using the reasoning process itself as a source for examples. An "entailment example oracle" similar to those was also used in (Greiner and Schuurmans, 1992).
Reference: <author> Greiner, R., A. Grove, and A. Kogan. </author> <year> 1996. </year> <title> Exploiting the omission of irrelevant data. </title> <booktitle> In Proc. of the International Conference on Machine Learning, </booktitle> <pages> pages 216-224. </pages> <note> 19 Greiner, </note> <author> R., A. Grove, and D. Roth. </author> <year> 1996. </year> <title> Learning active classifiers. </title> <booktitle> In Proc. of the International Conference on Machine Learning, </booktitle> <pages> pages 207-215. </pages>
Reference: <author> Greiner, R. and D. Schuurmans. </author> <year> 1992. </year> <title> Learning useful Horn approximations. </title> <booktitle> In Proc. of the International Conference on the Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 383-392. </pages>
Reference-contexts: The next oracles, introduced in (Frazier and Pitt, 1993), can be thought of as using the reasoning process itself as a source for examples. An "entailment example oracle" similar to those was also used in <ref> (Greiner and Schuurmans, 1992) </ref>.
Reference: <author> Kautz, H., M. Kearns, and B. Selman. </author> <year> 1995. </year> <title> Horn approximations of empirical data. </title> <journal> Artificial Intelligence, </journal> <volume> 74 </volume> <pages> 129-145. </pages>
Reference: <author> Kearns, M.J. and L.G. Valiant. </author> <year> 1994. </year> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <journal> Journal of the ACM, </journal> <volume> 41(1) </volume> <pages> 67-95. </pages>
Reference-contexts: In particular, if W is an arbitrary Boolean circuit, the learning problem is known to be hard (independent of the representation <ref> (Kearns and Valiant, 1994) </ref>), and the reasoning problem is also hard, regardless of the class of queries (Cook, 1971). However, by restricting the queries to k-CNF we show that one can learn to reason.
Reference: <author> Khardon, R. </author> <year> 1996. </year> <title> Learning to take actions. </title> <booktitle> In Proc. of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 787-792. </pages>
Reference: <author> Khardon, R. </author> <year> 1998. </year> <title> Learning first order universal Horn expressions. </title> <booktitle> In Proc. of the Annual ACM Workshop on Computational Learning Theory. </booktitle> <publisher> Forthcoming. </publisher>
Reference-contexts: One major difference between these frameworks is that work in ILP is mainly concerned with first order logic representations while we have discussed propositional logic. Further exploration of these relations and their possible implications is an interesting direction for future work; some preliminary ideas are reported in <ref> (Khardon, 1998) </ref>. Acknowledgments We are grateful to Moti Frances, Wolfgang Maass, Les Valiant and anonymous referees for their useful comments on earlier versions of this paper.
Reference: <author> Khardon, R. and D. Roth. </author> <year> 1994a. </year> <title> Learning to reason. </title> <booktitle> In Proc. of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 682-687. </pages> <note> To appear in Journal of the ACM. </note>
Reference-contexts: In some sense, within this framework, the learner acquires "domain knowledge" that, in turn, is used for reasoning about the domain. This framework, introduced in <ref> (Khardon and Roth, 1994a) </ref>, follows standard models of learning (Valiant, 1984; Angluin, 1988) in the way it models the interaction of the learner with its environment in the process of learning. <p> Namely, for a question ff presented to it, the learner must answer correctly whether W j= ff or W 6j= ff by using its representation KB. It was shown in <ref> (Khardon and Roth, 1994a) </ref> that the Learning to Reason framework offers efficient solutions in cases where the separate learning and reasoning problems are intractable. <p> The goal of the learner is to reason about this partially observable world by answering reasoning questions. As in <ref> (Khardon and Roth, 1994a) </ref>, it is shown that in some cases Learning to Reason can be achieved even though the traditionally phrased reasoning problem is intractable, and when the traditionally phrased learning problem learning a representation of the world (a concept) is intractable. <p> We also show that on-line L2R algorithms exist for queries in k-CNF. Finally, we show that with stronger oracles, either a membership oracle (Angluin, 1988) for partial observations or entailment queries (Frazier and Pitt, 1993), L2R is possible for the same classes of queries discussed in <ref> (Khardon and Roth, 1994a) </ref>. In fact, our proofs show that these oracles are strong enough to enable the learner to complete partial observations into total ones and learn in this way. <p> with respect to the same class of queries, and that the stronger oracles, P M Q and entailment oracles, can be used to support reasoning with respect to all common queries. 6.1 A Sampling Approach We first show that a simple sampling approach to reasoning, analyzed for total models in <ref> (Khardon and Roth, 1994a) </ref>, works for partial assignments as well. The main difference is that, for partial assignments, the problem of model evaluation may be computationally hard (Claim 3.2). Therefore we have to restrict attention to queries that can be evaluated in polynomial time. <p> We use a Learning to Reason result that was established in the context of total models. We show that the same task can be performed using the appropriate partial assignments oracles. Recall that by Q C we denote the class of all common queries. Theorem 6.7 <ref> (Khardon and Roth, 1994a) </ref> There is a MB-L2R algorithm, Ex-L2R-DNF, for the reasoning problem (F ; Q C ), The algorithm interacts with the oracles RQC (W; Q C ) and M Q (W ) restricted to total models, maintains a model based representation G W (where W is the hidden <p> As argued in <ref> (Khardon and Roth, 1994a) </ref> this algorithm can be used as a learning to reason algorithm for the class of Horn queries and arbitrary W (and is polynomial for a certain class of functions discussed below). <p> The computational advantages of L2R were explored and discussed. For example, for the case of common queries, similar to <ref> (Khardon and Roth, 1994a) </ref>, the results in this paper show that while learning DNF is still an open problem, one can learn to reason for domains with a polynomial size DNF.
Reference: <author> Khardon, R. and D. Roth. </author> <year> 1994b. </year> <title> Reasoning with models. </title> <booktitle> In Proc. of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 1148-1153. </pages>
Reference-contexts: A model based approach becomes feasible if one can make correct inferences when working with a small subset of models. Some results along this line have been obtained (Kautz, Kearns, and Selman, 1995; Khardon and Roth, 1994b). In particular, previous work in <ref> (Khardon and Roth, 1994b) </ref>, using ideas from (Bshouty, 1995), identified a small set of models, called the set of characteristic models of W , that supports correct reasoning. <p> the clauses in ff, deduce that f 6j= ff; Otherwise, f j= ff. where min b (W ) = fz j z 2 W; such that 8y 2 W; z 6&gt; b yg: Using these definitions we can identify the models that are needed for the algorithm MBR: Theorem 4.1 <ref> (Khardon and Roth, 1994b) </ref> Let W 2 F , ff 2 G and let B be a basis for G. <p> random partial assignments can be used to reason with respect to *-fair queries, the oracle RQ restricted to k-CNF queries can 5 This can be traced to the fact that h j= W B lub , where W B lub is the least upper bound of W in Q C <ref> (Khardon and Roth, 1994b) </ref>. We give an argument from first principles to avoid using the notion of least upper bounds. 18 be used to reason with respect to the same class of queries, and stronger oracles can be used to support reasoning with respect to all common queries.
Reference: <author> Khardon, R. and D. Roth. </author> <year> 1997. </year> <title> Default and relevance in model based reasoning. </title> <journal> Artificial Intelligence, 97(1-2):169-193. </journal>
Reference: <author> Littlestone, N. </author> <year> 1988. </year> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318. </pages>
Reference-contexts: The dependence of the mistake bound can be reduced in a different way, which holds for all the interpretations. This is done by converting the algorithm A-OL-NIMP to an attribute-efficient algorithm using the Winnow algorithm <ref> (Littlestone, 1988) </ref>, similar to what is done in (Blum, 1992). The resulting mistake bound is of the form log m N (W ). The knowledge representation used by the new algorithm is not a list of models any more.
Reference: <author> Moses, Y. and M. Tennenholtz. </author> <year> 1996. </year> <title> Off-line reasoning for on-line efficiency: knowledge bases. </title> <journal> Artificial Intelligence, </journal> <volume> 83(2) </volume> <pages> 229-239. </pages>
Reference-contexts: Since the number of disjunctions of size k is bounded by 3 k n k and is therefore polynomial for constant k, this leads to a polynomial algorithm. These observations have been used before <ref> (Moses and Tennenholtz, 1996) </ref> where off-line compilation of knowledge bases into this form is suggested.
Reference: <author> Muggleton, S. and L. De Raedt. </author> <year> 1994. </year> <title> Inductive logic programming: Theory and methods. </title> <journal> Journal of Logic Programming, </journal> <volume> 20 </volume> <pages> 629-679. </pages>
Reference-contexts: In fact, our proofs show that these oracles are strong enough to enable the learner to complete partial observations into total ones and learn in this way. Our framework is related to work in Inductive Logic Programming (ILP) <ref> (Muggleton and De Raedt, 1994) </ref> where examples for learning are often given in the form of clauses implied by the learned program. In this sense, hence, ILP algorithms use a fixed knowledge representation to learn to reason about such clauses.
Reference: <author> Naor, J. and M. Naor. </author> <year> 1993. </year> <title> Small-bias probability spaces: Efficient constructions and applications. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22(4) </volume> <pages> 838-856, </pages> <month> August. </month>
Reference-contexts: So the lazy model based algorithm will answer "No" due to z 0 , and is therefore correct. 3 The basis B H k , of size O (n k ) is sufficient, but there is a smaller basis, composed of a (n; k)-universal set <ref> (Naor and Naor, 1993) </ref>. 8 x 1 x 2 x 1 x 3 x 1 x 4 x 2 x 3 x 2 x 4 x 3 x 4 fl 2 = 0101 01 00 0 1 10 1 1 0 1 fl 4 = 0100 0 1 0 0 0
Reference: <author> Reiter, R. </author> <year> 1987. </year> <title> Nonmonotonic reasoning. </title> <booktitle> In Annual Reviews of Computer Science. </booktitle> <publisher> Annual Reviews Inc., </publisher> <pages> pages 147-188. </pages>
Reference-contexts: In particular, a different treatment of partial information is taken in (Valiant, 1995; Roth, 1995), where the effect of partially specified queries is discussed. A similar learning to reason approach is developed there, supporting several aspects of non-monotonic reasoning <ref> (Reiter, 1987) </ref> which have proved difficult to 2 capture in other frameworks.
Reference: <author> Roth, D. </author> <year> 1995. </year> <title> Learning to reason: The non-monotonic case. </title> <booktitle> In Proc. of the International Joint Conference of Artificial Intelligence, </booktitle> <pages> pages 1178-1184. </pages>
Reference: <author> Schuurmans, D. and R. Greiner. </author> <year> 1994. </year> <title> Learning default concepts. </title> <booktitle> In Proceedings of the Tenth Canadian Conference on Artificial Intelligence (CSCSI-94), </booktitle> <pages> pages 519-523. </pages>
Reference: <author> Selman, B. and H. Kautz. </author> <year> 1996. </year> <title> Knowledge compilation and theory approximation. </title> <journal> Journal of the ACM, </journal> <volume> 43(2) </volume> <pages> 193-224, </pages> <month> March. </month>
Reference: <author> Valiant, L. G. </author> <year> 1984. </year> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November. </month> <note> 20 Valiant, </note> <author> L. G. </author> <year> 1995. </year> <title> Rationality. </title> <booktitle> In Workshop on Computational Learning Theory, </booktitle> <pages> pages 3-14, </pages> <month> July. 21 </month>
References-found: 26

