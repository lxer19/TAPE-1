URL: http://www.cs.utah.edu/~wilson/papers/sc.ps.gz
Refering-URL: http://www.cs.utah.edu/~wilson/publications.html
Root-URL: 
Title: Dynamic Computation Migration in DSM Systems  
Author: Wilson C. Hsieh, M. Frans Kaashoek, William E. Weihl 
Keyword: computation migration, data migration, replication, coherence  
Abstract: Dynamic computation migration is the runtime choice between computation and data migration. Dynamic computation migration speeds up access to concurrent data structures with unpredictable read/write patterns. This paper describes the design, implementation, and evaluation of dynamic computation migration in a multithreaded distributed shared-memory system, MCRL. Two policies are studied, STATIC and REPEAT. Both migrate computation for writes. STATIC migrates data for reads, while REPEAT maintains a limited history of accesses and sometimes migrates computation for reads. On a concurrent, distributed B-tree with 50% lookups and 50% inserts, STATIC improves performance by about 17% on both Alewife and the CM-5. REPEAT generally performs better than STATIC. With 80% lookups and 20% inserts, REPEATimproves performance by 23% on Alewife, and by 46% on the CM-5. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, R. Bianchini, D. Chaiken, K.L. Johnson, D. Kranz, J. Kubiatowicz, B.H. Lim, K. Mackenzie, and D. Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> June 22-24, </month> <year> 1995. </year>
Reference-contexts: Locking happens implicitly with the CRL access functions, which acquire regions in either read-mode (shared) or write-mode (exclusive). MCRL extends CRL in two ways: first, it provides support for multithreaded programs; second, it provides support for dynamic computation migration. MCRL runs on the MIT Alewife machine <ref> [1] </ref> and on Thinking Machines' CM-5. Alewife is an experimental shared-memory multiprocessor that supports high-performance message passing. Each of the 32 processors is a 20 MHz SPARC-like processor with a 64Kbyte cache and 8Mbytes of physical memory. The machine achieves a peak network bandwidth of 18 Mbytes/second.
Reference: [2] <author> J. Aspnes, M. Herlihy, and N. Shavit. </author> <title> Counting Networks. </title> <journal> Journal of the ACM, </journal> <volume> 41(5) </volume> <pages> 1020-1048, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: This decision is effective, because it allows for replication. 4.3 Counting Network A counting network is a distributed data structure that supports shared counting, as well as producer/consumer buffering and barrier synchronization <ref> [2, 9] </ref>. The simplest data structure for shared counting is a counter protected by a lock. Under such a scheme, a thread acquires the lock 7 the STATIC and REPEAT policies.
Reference: [3] <author> R. Bayer and E.M. McCreight. </author> <title> Organization and Maintenance of Large Ordered Indexes. </title> <journal> Acta Informatica, </journal> <volume> 1(3) </volume> <pages> 173-189, </pages> <year> 1972. </year>
Reference-contexts: Despite the small size of the counting network nodes (eight words), the cost of maintaining coherence is still more expensive than moving computation. Our experiments confirm our earlier results [11], where we demonstrated that write accesses should migrate computation. 4.4 B-tree A B-tree <ref> [3] </ref> is a data structure used to represent a dictionary, which is a dynamic set that supports the operations insert, delete, and lookup. The basic structure and implementation of a B-tree is similar to that of a balanced binary tree.
Reference: [4] <author> M.C. Carlisle and A. Rogers. </author> <title> Software Caching and Computation Migration in Olden. </title> <booktitle> In Proceedings of the 5th Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 29-38, </pages> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Thirty-two processors access a three-level B-tree. 5 Related Work The work described in this paper builds upon our earlier work, in which we explored the use of static computation migration [11]. The Olden project <ref> [4] </ref> has also explored static computation migration. Rogers and Carlisle have investigated language and compiler support for statically determining when to migrate computation or data.
Reference: [5] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Techniques for Reducing Consistency-Related Communication in Distributed Shared-Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(3) </volume> <pages> 205-243, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: A static decision between computation migration and data migration does not work for truly dynamic data structures. DSM systems typically use only one protocol for maintaining the coherence of data, whereas the access patterns for data vary widely. The Munin project <ref> [5] </ref> provides various different coherence protocols that can be used for different types of data. As in other DSM systems, Munin restricts its attention to the migration of data, and does not allow for the migration of computation. Munin does provide support for migratory data in the form of RPC.
Reference: [6] <author> R. Chandra, A. Gupta, and J.L. Hennessy. </author> <title> COOL: An Object-Based Language for Parallel Programming. </title> <journal> IEEE Computer, </journal> <volume> 27(8) </volume> <pages> 13-26, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: As in other DSM systems, Munin restricts its attention to the migration of data, and does not allow for the migration of computation. Munin does provide support for migratory data in the form of RPC. Several systems, such as Mercury [8] and COOL <ref> [6] </ref>, use cache-conscious scheduling, which is a scheduling policy designed to increase cache reuse. The use of computation migration can increase global cache effectiveness, because by avoiding replication it reduces pressure on the cache.
Reference: [7] <author> J.S. Chase, F.G. Amador, E.D. Lazowska, H.M. Levy, and R.J. Littlefield. </author> <title> The Amber System: Parallel Programming on a Network of Multiprocessors. </title> <booktitle> In Proceedings of the 12th Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: William Weihl, 130 Lytton Avenue, Palo Alto, CA 94301, weihl@pa.dec.com, http://www.research.digital.com/SRC/staff/weihl/bio.html, fax: 415-853-2104. This paper is a condensed version of the first author's dissertation [10]. 1 (b) illustrates the state of the system after data migration; (c) illustrates the state after computation migration. and Amber <ref> [7] </ref>) in that RPC systems do not migrate pre-existing computation. Computation migration is also distinct from thread migration, as only part of a thread is migrated. Data migration is the migration of data to a thread that accesses it.
Reference: [8] <author> R.J. Fowler and L.I. Kontothanassis. </author> <title> Improving Processor and Cache Locality in Fine-Grain Parallel Computations using Object-Affinity Scheduling and Continuation Passing (Revised). </title> <type> Technical Report 411, </type> <institution> University of Rochester Computer Science Department, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: As in other DSM systems, Munin restricts its attention to the migration of data, and does not allow for the migration of computation. Munin does provide support for migratory data in the form of RPC. Several systems, such as Mercury <ref> [8] </ref> and COOL [6], use cache-conscious scheduling, which is a scheduling policy designed to increase cache reuse. The use of computation migration can increase global cache effectiveness, because by avoiding replication it reduces pressure on the cache.
Reference: [9] <author> M. Herlihy, B.H. Lim, and N. Shavit. </author> <title> Scalable Concurrent Counting. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 13(4) </volume> <pages> 343-364, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: This decision is effective, because it allows for replication. 4.3 Counting Network A counting network is a distributed data structure that supports shared counting, as well as producer/consumer buffering and barrier synchronization <ref> [2, 9] </ref>. The simplest data structure for shared counting is a counter protected by a lock. Under such a scheme, a thread acquires the lock 7 the STATIC and REPEAT policies.
Reference: [10] <author> W.C. Hsieh. </author> <title> Dynamic Computation Migration in Distributed Shared Memory Systems. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> September </month> <year> 1995. </year> <note> Available as MIT/LCS/TR-665. </note>
Reference-contexts: Frans Kaashoek, 545 Technology Square, Cambridge, MA 02139, kaashoek@lcs.mit.edu, http://www.pdos.lcs.mit.edu/kaashoek, fax: 617-258-8682. William Weihl, 130 Lytton Avenue, Palo Alto, CA 94301, weihl@pa.dec.com, http://www.research.digital.com/SRC/staff/weihl/bio.html, fax: 415-853-2104. This paper is a condensed version of the first author's dissertation <ref> [10] </ref>. 1 (b) illustrates the state of the system after data migration; (c) illustrates the state after computation migration. and Amber [7]) in that RPC systems do not migrate pre-existing computation. Computation migration is also distinct from thread migration, as only part of a thread is migrated.
Reference: [11] <author> W.C. Hsieh, P. Wang, and W.E. Weihl. </author> <title> Computation Migration: Enhancing Locality for Distributed-Memory Parallel Systems. </title> <booktitle> In Proceedings of the 4th Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 239-248, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year> <month> 15 </month>
Reference-contexts: Performance measurements demonstrate that migrating computations for some accesses is better than always migrating data, and that choosing dynamically whether to migrate data or computation is better yet for dynamic data structures with unpredictable access patterns. Computation migration <ref> [11, 15] </ref> is the partial migration of active threads. Under computation migration, a currently executing thread has some of its state migrated to remote data that it accesses. Computation migration is distinct from RPC (which was explored in systems such as Emerald [13] Technical paper. <p> Two approaches have been used to implement computation migration. In the first approach, the compiler generates calls into the runtime system, which manages migration. This approach was used in the Olden system [15]. In the second approach, the compiler manages migration more directly. We took this approach in Prelude <ref> [11] </ref>. MCRL is designed for the latter approach, but does not provide compiler support. Compiler support should not be difficult to add, although it would require some language restrictions similar to those in Olden. 3 Dynamic Computation Migration This section describes the mechanics of dynamic computation migration. <p> Computation migration does not incur any coherence overhead, whereas data migration must incur coherence overhead on every access. Despite the small size of the counting network nodes (eight words), the cost of maintaining coherence is still more expensive than moving computation. Our experiments confirm our earlier results <ref> [11] </ref>, where we demonstrated that write accesses should migrate computation. 4.4 B-tree A B-tree [3] is a data structure used to represent a dictionary, which is a dynamic set that supports the operations insert, delete, and lookup. <p> Thirty-two processors access a three-level B-tree. 5 Related Work The work described in this paper builds upon our earlier work, in which we explored the use of static computation migration <ref> [11] </ref>. The Olden project [4] has also explored static computation migration. Rogers and Carlisle have investigated language and compiler support for statically determining when to migrate computation or data.
Reference: [12] <author> K.L. Johnson, M.F. Kaashoek, and D.A. Wallach. </author> <title> CRL: High-Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 213-228, </pages> <address> Copper Mountain, CO, </address> <month> December 3-6, </month> <year> 1995. </year> <note> http://www.pdos.lcs.mit.edu/crl. </note>
Reference-contexts: This paper evaluates dynamic computation migration in the context of MCRL, a software distributed shared-memory (DSM) system. 2 We have made the following contributions: * We have built a multithreaded distributed shared-memory system called MCRL that supports dynamic computation migration. MCRL extends the CRL system <ref> [12] </ref>. MCRL runs on a 32-node MIT Alewife machine and on a 128-node CM-5. We will concentrate on our Alewife results, as Alewife's communication costs are very low. <p> Section 5 discusses related work, and Section 6 discusses future work. Section 7 summarizes the results and conclusions of this paper. 2 MCRL We have implemented dynamic computation migration in the MCRL software distributed shared-memory system. MCRL extends CRL <ref> [12] </ref>, a SPMD distributed shared-memory system. CRL provides a global name space for regions, which are programmer-defined blocks of memory. CRL replicates regions using a fixed-home, directory-based, sequentially consistent cache-coherence protocol. Locking happens implicitly with the CRL access functions, which acquire regions in either read-mode (shared) or write-mode (exclusive).
Reference: [13] <author> E. Jul, H. Levy, N. Hutchison, and A. Black. </author> <title> Fine-Grained Mobility in the Emerald System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 109-133, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Computation migration [11, 15] is the partial migration of active threads. Under computation migration, a currently executing thread has some of its state migrated to remote data that it accesses. Computation migration is distinct from RPC (which was explored in systems such as Emerald <ref> [13] </ref> Technical paper. Contact information: Wilson Hsieh, University of Washington, Box 352350, Seattle, WA 98195, whsieh@cs.washington.edu, http://www.cs.washington.edu/homes/whsieh, fax: 206-543-2969. Frans Kaashoek, 545 Technology Square, Cambridge, MA 02139, kaashoek@lcs.mit.edu, http://www.pdos.lcs.mit.edu/kaashoek, fax: 617-258-8682. William Weihl, 130 Lytton Avenue, Palo Alto, CA 94301, weihl@pa.dec.com, http://www.research.digital.com/SRC/staff/weihl/bio.html, fax: 415-853-2104.
Reference: [14] <author> P.L. Lehman and S.B. Yao. </author> <title> Efficient Locking for Concurrent Operations on B-Trees. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 6(4) </volume> <pages> 650-670, </pages> <month> December </month> <year> 1981. </year>
Reference-contexts: Unlike a binary tree, the maximum number of children for each B-tree node is not constrained to two; it can be much larger. The B-tree implementation we use is a simplified version of one of the concurrent, distributed algorithms described by Wang <ref> [14, 19] </ref>. The form of these B-trees is a variant of the classic B-tree: keys are stored only in the leaves, and all nodes and leaves contain pointers to the node to their right. The values of the keys range from 0 to 1,000,000.
Reference: [15] <author> A. Rogers, M. Carlisle, J. Reppy, and L. Hendren. </author> <title> Supporting Dynamic Data Structures on Distributed-Memory Machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 17(2) </volume> <pages> 233-263, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Performance measurements demonstrate that migrating computations for some accesses is better than always migrating data, and that choosing dynamically whether to migrate data or computation is better yet for dynamic data structures with unpredictable access patterns. Computation migration <ref> [11, 15] </ref> is the partial migration of active threads. Under computation migration, a currently executing thread has some of its state migrated to remote data that it accesses. Computation migration is distinct from RPC (which was explored in systems such as Emerald [13] Technical paper. <p> Two approaches have been used to implement computation migration. In the first approach, the compiler generates calls into the runtime system, which manages migration. This approach was used in the Olden system <ref> [15] </ref>. In the second approach, the compiler manages migration more directly. We took this approach in Prelude [11]. MCRL is designed for the latter approach, but does not provide compiler support.
Reference: [16] <author> J.P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Both of these multithreaded data structures involve graph traversal, although their read/write characteristics are very different. The B-tree is more dynamic than the data structures used in the SPLASH benchmarks <ref> [16] </ref>: the pattern of accesses to a particular object cannot be predicted statically. 4.1 Migration Latency We examine how the size of data affects the relative costs of moving data and computation on Alewife, which has a high-performance communication network. <p> On the CM-5, it improves performance by 46%. Dynamic computation migration is useful for reducing coherence costs in applications that do not have statically known (or predictable) data access patterns. Most scientific applications, such as those in the well-known SPLASH benchmark suite <ref> [16] </ref>, have communication patterns that are very predictable: they have relatively static data and thread layout. Future parallel applications are unlikely to have such characteristics. Although network performance continues to improve, sending messages will remain significantly more expensive than local computation.
Reference: [17] <author> C.A. Thekkath and H.M. Levy. </author> <title> Limits to Low-Latency Communication on High-Speed Networks. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 179-203, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: We ran with version 7.4.0 Final of the CMOST operating system, and version 3.3 of the CMMD message-passing library. Messages are received by polling, because interrupts are slow on the CM-5. The communication performance of the CM-5 is on a par with today's high-performance distributed systems such as FRPC <ref> [17] </ref> and U-Net [18], but is lower than Alewife's. Dynamic computation migration improves performance more on the CM-5 than on Alewife, because the communication costs that are removed are greater. We will concentrate on the Alewife results, which represent a lower bound on the benefits of computation migration.
Reference: [18] <author> T. von Eicken, A. Basu, V. Buch, and W. Vogels. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 40-53, </pages> <address> Copper Mountain, CO, </address> <month> December 3-6, </month> <year> 1995. </year>
Reference-contexts: Messages are received by polling, because interrupts are slow on the CM-5. The communication performance of the CM-5 is on a par with today's high-performance distributed systems such as FRPC [17] and U-Net <ref> [18] </ref>, but is lower than Alewife's. Dynamic computation migration improves performance more on the CM-5 than on Alewife, because the communication costs that are removed are greater. We will concentrate on the Alewife results, which represent a lower bound on the benefits of computation migration.
Reference: [19] <author> P. Wang. </author> <title> An In-Depth Analysis of Concurrent B-Tree Algorithms. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> January </month> <year> 1991. </year> <note> Available as MIT/LCS/TR-496. 16 </note>
Reference-contexts: Unlike a binary tree, the maximum number of children for each B-tree node is not constrained to two; it can be much larger. The B-tree implementation we use is a simplified version of one of the concurrent, distributed algorithms described by Wang <ref> [14, 19] </ref>. The form of these B-trees is a variant of the classic B-tree: keys are stored only in the leaves, and all nodes and leaves contain pointers to the node to their right. The values of the keys range from 0 to 1,000,000.
References-found: 19

