URL: http://ciir.cs.umass.edu/info/psfiles/tepubs/jair2_soderland.ps
Refering-URL: http://cobar.cs.umass.edu/pubfiles/te.html
Root-URL: 
Email: soderlan@cs.umass.edu  lehnert@cs.umass.edu  
Title: Wrap-Up: a Trainable Discourse Module for Information Extraction  
Author: Stephen Soderland Wendy Lehnert 
Address: Amherst, MA 01003-4610  
Affiliation: Department of Computer Science, University of Massachusetts  
Note: Journal of Artificial Intelligence Research 2 (1994) 131-158 Submitted 4/94; published 12/94  
Abstract: The vast amounts of on-line text now available have led to renewed interest in information extraction (IE) systems that analyze unrestricted text, producing a structured representation of selected information from the text. This paper presents a novel approach that uses machine learning to acquire knowledge for some of the higher level IE processing. Wrap-Up is a trainable IE discourse component that makes intersentential inferences and identifies logical relations among information extracted from the text. Previous corpus-based approaches were limited to lower level processing such as part-of-speech tagging, lexical disambiguation, and dictionary construction. Wrap-Up is fully trainable, and not only automatically decides what classifiers are needed, but even derives the feature set for each classifier automatically. Performance equals that of a partially trainable discourse module requiring manual customization for each domain.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ayuso, D., Boisen, S., Fox, H., Gish, H., Ingria, R., & Weischedel, R. </author> <year> (1992). </year> <title> BBN: Description of the PLUM System as Used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference, </booktitle> <pages> 169-176. </pages> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Brent, M. </author> <year> (1993). </year> <title> Robust Acquisition of Subcategorization Frames. </title> <booktitle> In Proceeding of the Association for Computational Linguistics. </booktitle>
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> A Case-Based Approach to Knowledge Acquisition for Domain-Specific Sentence Analysis. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 798-803. </pages>
Reference-contexts: Weischedel (1993) has used corpus-based probabilities both for part-of-speech tagging and to guide parsing. Collocation data has been used for lexical disambiguation by Hindle (1989), Brent (1993), and others. Examples from a training corpus have driven both part-of-speech and semantic tagging <ref> (Cardie, 1993) </ref> and dictionary construction (Riloff, 1993). c fl1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. Soderland and Lehnert This paper describes Wrap-Up (Soderland & Lehnert, 1994), the first system to automatically acquire domain knowledge for the higher level processing associated with discourse analysis.
Reference: <author> Church, K. </author> <year> (1988). </year> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing of the ACL, </booktitle> <pages> 136-143. </pages>
Reference: <author> Cost, S., & Salzberg, S. </author> <year> (1993). </year> <title> A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features. </title> <journal> Machine Learning, </journal> <volume> 10(1), </volume> <pages> 57-78. </pages>
Reference: <author> DeRose, S. </author> <year> (1988). </year> <title> Grammatical Category Disambiguation by Statistical Optimization. </title> <journal> Computational Linguistics, </journal> <volume> 14(1), </volume> <pages> 31-39. </pages>
Reference: <author> Dolan, C. P., Goldman, S. R., Cuda, T. V., & Nakamura, A. M. </author> <year> (1991). </year> <title> Hughes Trainable Text Skimmer: Description of the TTS System as used for MUC-3. </title> <booktitle> In Proceedings of the Third Message Understanding Conference, </booktitle> <pages> 155-162. </pages> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Dunning, T. </author> <year> (1993). </year> <title> Accurate Methods for the Statistics of Surprise and Coincidence. </title> <journal> Computational Linguistics, </journal> <volume> 19(1), </volume> <pages> 61-74. </pages>
Reference: <author> Grosz, B., & Sidner C. </author> <year> (1986). </year> <title> Attention, intention and the structure of discourse. </title> <journal> Computational Linguistics, </journal> <volume> 12(3), </volume> <pages> 175-204. </pages>
Reference: <author> Hindle, D. </author> <year> (1989). </year> <title> Acquiring Disambiguation Rules from Text. </title> <booktitle> In Proceeding of the Association for Computational Linguistics, </booktitle> <pages> 118-125. </pages>
Reference: <author> Hobbs, J. </author> <year> (1978). </year> <title> Resolving Pronoun References. </title> <journal> Lingua, </journal> <volume> 44(4), </volume> <pages> 311-338. </pages>
Reference-contexts: Other work on discourse has often involved tracking shifts in topic and in the speaker/writer's goals (Grosz & Sidner, 1986; Liddy et al., 1993) or in resolving anaphoric references <ref> (Hobbs, 1978) </ref>. Discourse processing in an IE system may concern itself with some of these issues, but only as a means to its main objective of transforming bits and pieces of extracted information into a coherent representation.
Reference: <author> Jacobs, P., Krupka, G., Rau, L., Mauldin, M., Mitamura, T., Kitani, T., Sider, I., & Childs, L. </author> <year> (1993). </year> <title> GE-CMU: Description of the SHOGUN System used for MUC-5. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference, </booktitle> <pages> 109-120. </pages> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: IE systems are knowledge-based, however, and must be individually tailored to the information needs of each application. Some research laboratories have focused on sophisticated user interfaces to ease the burden of knowledge acquisition. GE's NLToolset is an example of this approach <ref> (Jacobs et al., 1993) </ref>, while BBN typifies systems that combine user input with corpus-based statistics (Ayuso et al., 1993). The University of Massachusetts has been moving in the direction of machine learning to create a fully trainable IE system.
Reference: <author> Lehnert, W. </author> <year> (1990). </year> <title> Symbolic/Subsymbolic Sentence Analysis: Exploiting the Best of Two Worlds. </title> <booktitle> Advances in Connectionist and Neural Computation Theory. vol. </booktitle> <address> 1.. Norwood, NJ: </address> <publisher> Ablex Publishing, </publisher> <pages> 151-158. </pages>
Reference: <author> Lehnert, W., Cardie, C., Fisher, D., McCarthy, J., Riloff, E., & Soderland, S. </author> <year> (1992). </year> <title> University of Massachusetts: Description of the CIRCUS System as Used for MUC-4. </title> <booktitle> In Proceedings of the Fourth Message Understanding Conference, </booktitle> <pages> 282-288. </pages> <publisher> Morgan Kaufmann Publishers. </publisher> <editor> 157 Soderland and Lehnert Lehnert, W., McCarthy, J., Soderland, S., Riloff, E., Cardie, C., Peterson, J., Feng, F., Dolan, C., & Goldman, S. </editor> <year> (1993). </year> <title> UMass/Hughes: Description of the CIRCUS System as Used for MUC-5. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference, </booktitle> <pages> 257-259. </pages> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Liddy, L., McVearry, K., Paik, W., Yu, E., & McKenna, M. </author> <year> (1993). </year> <title> Development, Implementation, and Testing of a Discourse Model for Newspaper Texts. </title> <booktitle> In Proceedings of the Human Language Technology Workshop, </booktitle> <pages> 159-164. </pages> <publisher> Morgan Kaufmann Publishers. MUC-3. </publisher> <year> (1991). </year> <booktitle> Proceedings of the Third Message Understanding Conference. </booktitle> <publisher> Morgan Kauf-mann Publishers. MUC-4. </publisher> <year> (1992). </year> <booktitle> Proceedings of the Fourth Message Understanding Conference. </booktitle> <publisher> Morgan Kaufmann Publishers. </publisher> <address> MUC-5. </address> <year> (1993). </year> <booktitle> Proceedings of the Fifth Message Understanding Conference. </booktitle> <publisher> Morgan Kauf-mann Publishers. </publisher>
Reference: <author> Quinlan, J.R. </author> <year> (1986). </year> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: Wrap-Up uses supervised learning to induce a set of classifiers from a training corpus of representative texts, where each text is accompanied by hand-coded target output. We implemented Wrap-Up with the ID3 decision tree algorithm <ref> (Quinlan, 1986) </ref>, although other machine learning algorithms could have been selected. Wrap-Up is a fully trainable system and is unique in that it not only decides what classifiers are needed for the domain, but automatically derives the feature set for each classifier. <p> If the classifier returns "positive", Wrap-Up adds a pointer between these two objects in the output to indicate that the equipment was used for that lithography process. The ID3 decision tree algorithm <ref> (Quinlan, 1986) </ref> was used in these experiments, although any machine learning classifier could be plugged into the Wrap-Up architecture. A vector space approach might seem appropriate, but its performance would depend on the weights assigned to each feature (Salton et al., 1975). <p> The a priori probability of a pointer from lithography to equipment in the training corpus was 34%, with 282 positive and 539 negative training instances. ID3 uses an information gain metric to select the most effective feature to partition the training instances <ref> (p.89-90, Quinlan, 1986) </ref>, in this case choosing equipment type as the test at the root of this tree. This feature alone is sufficient to classify instances with equipment type such as modular equipment, radiation source, or etching system, which have only negative instances.
Reference: <author> Riloff, E. </author> <year> (1993). </year> <title> Automatically Constructing a Dictionary for Information Extraction Tasks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 811-816. </pages>
Reference-contexts: Weischedel (1993) has used corpus-based probabilities both for part-of-speech tagging and to guide parsing. Collocation data has been used for lexical disambiguation by Hindle (1989), Brent (1993), and others. Examples from a training corpus have driven both part-of-speech and semantic tagging (Cardie, 1993) and dictionary construction <ref> (Riloff, 1993) </ref>. c fl1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. Soderland and Lehnert This paper describes Wrap-Up (Soderland & Lehnert, 1994), the first system to automatically acquire domain knowledge for the higher level processing associated with discourse analysis. <p> Including less reliable patterns increases coverage but does so at the expense of spurious extraction. The more specific pattern "developed with assistance from X" is reliable, but was missed by the dictionary construction tool <ref> (Riloff, 1993) </ref>. For many of the domain objects, such as equipment, devices, and microchip fabrication processes, the set of possible objects is predefined and a list of keywords that refer to these objects can be created. <p> In 1991 a purely hand-crafted UMass system had the highest performance of any site in the MUC-3 evaluation. The following year UMass ran both a hand-crafted system and an alternate system that replaced a key component with output from AutoSlog, a trainable dictionary construction tool <ref> (Riloff, 1993) </ref>. The AutoSlog variant exhibited performance levels comparable to a dictionary based on 1500 hours of manual coding.
Reference: <author> Salton, G., Wong, A., & Yang, C.S. </author> <year> (1975). </year> <title> A vector space model for automatic indexing. </title> <journal> Correspondences of the ACM, </journal> <volume> 18(11), </volume> <pages> 613-620. </pages>
Reference-contexts: The ID3 decision tree algorithm (Quinlan, 1986) was used in these experiments, although any machine learning classifier could be plugged into the Wrap-Up architecture. A vector space approach might seem appropriate, but its performance would depend on the weights assigned to each feature <ref> (Salton et al., 1975) </ref>. It is hard to see a principled way to assign weights to the heterogeneous features used in Wrap-Up's classifiers (see Section 3.3), since some features encode attributes of the domain objects and others encode linguistic context or relative position in the text.
Reference: <author> Soderland, S., & Lehnert, W. </author> <year> (1994). </year> <title> Corpus-Driven Knowledge Acquisition for Discourse Analysis. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 827-832. </pages>
Reference-contexts: Examples from a training corpus have driven both part-of-speech and semantic tagging (Cardie, 1993) and dictionary construction (Riloff, 1993). c fl1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. Soderland and Lehnert This paper describes Wrap-Up <ref> (Soderland & Lehnert, 1994) </ref>, the first system to automatically acquire domain knowledge for the higher level processing associated with discourse analysis. Wrap-Up uses supervised learning to induce a set of classifiers from a training corpus of representative texts, where each text is accompanied by hand-coded target output.
Reference: <author> Weischedel, R., Meteer, M., Schwartz, R., Ramshaw, L., & Palmucci, J. </author> <year> (1993). </year> <title> Coping with Ambiguity and Unknown Words Through Probabilistic Models. </title> <journal> Computational Linguistics, </journal> <volume> 19(2), </volume> <pages> 359-382. </pages>
Reference: <author> Will, C. </author> <year> (1993). </year> <title> Comparing human and machine performance for natural language information extraction: Results for English microelectronics from the MUC-5 evaluation. </title> <booktitle> In Proceedings of the Fifth Message Understanding Conference, </booktitle> <pages> 53-67. </pages> <publisher> Morgan Kauf-mann Publishers. </publisher> <pages> 158 </pages>
Reference-contexts: A human performance study for this task found that experienced analysts agreed with each other on only 80% on their text interpretations in this domain <ref> (Will, 1993) </ref>. World knowledge is also needed about the relationships possible between domain objects. A lithography process may be linked to stepper equipment, but steppers are never used in layering, etching, or packaging processes.
References-found: 21

