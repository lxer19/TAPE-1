URL: http://www.cs.dartmouth.edu/~jasonliu/courses/sim188/biblio/drs96.ps.gz
Refering-URL: http://www.cs.dartmouth.edu/~jasonliu/courses/sim188/notes-15.html
Root-URL: http://www.cs.dartmouth.edu
Title: FUNCTIONAL ALGORITHM SIMULATION OF THE FAST MULTIPOLE METHOD: ARCHITECTURAL IMPLICATIONS  
Author: MARIOS D. DIKAIAKOS and ANNE ROGERS and KENNETH STEIGLITZ 
Keyword: Functional Algorithm Simulation. N-Body Problem. Performance Modeling.  
Address: Box 351580, Seattle, WA 98195, U.S.A.  Princeton, NJ 08544, U.S.A.  Princeton, NJ 08544, U.S.A.  
Affiliation: Departments of Astronomy and Computer Science-Engineering, University of Washington, ASTRONOMY,  Department of Computer Science, Princeton University  Department of Computer Science, Princeton University  
Note: Parallel Processing Letters c World Scientific Publishing Company  
Abstract: Functional Algorithm Simulation is a methodology for predicting the computation and communication characteristics of parallel algorithms for a class of scientific problems, without actually performing the expensive numerical computations involved. In this paper, we use Functional Algorithm Simulation to study the parallel Fast Multipole Method (FMM), which solves the N-body problem. Functional Algorithm Simulation provides us with useful information regarding communication patterns in the algorithm, the variation of available parallelism during different algorithmic phases, and upper bounds on available speedups for different problem sizes. Furthermore, it allows us to predict the performance of the FMM on message-passing multiprocessors with topologies such as cliques, hypercubes, rings, and multirings, over a wider range of problem sizes and numbers of processors than would be feasible by direct simulation. Our simulations show that an implementation of the FMM on low-cost, scalable ring or multiring architectures can attain satisfactory performance. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> L. Greengard and V. Rokhlin, </author> <title> A fast algorithm for particle simulation, </title> <journal> Journal of Computational Physics, </journal> <volume> 73 (1987), </volume> <pages> 325-348. </pages>
Reference-contexts: FAST runs on uniprocessor workstations and has been used to evaluate a number of interesting and important scientific al 1 gorithms [1,8,10] mapped onto message-passing multiprocessors. In this paper we present a case-study conducted with FAST for the parallel Fast Multipole Method <ref> [1] </ref>, which solves the N-Body problem in two dimensions. We use the information derived with FAST to evaluate and analyze the relative performance of the algorithm on different interconnection topologies. Performance is critical in parallel computing. <p> They include cycles-per-instruction counts, clock-speeds, communication bandwidth, communication overhead, etc. In the current version of FAST, we used hardware parameters characteristic of INTEL's iPSC/2 and iPSC/860 multiprocessors. 3. Functional Algorithm Simulation of the FMM 3.1. The Fast Multipole Method The Fast Multipole Method (FMM) <ref> [1] </ref> solves the N-body problem, i.e., it calculates the forces exerted on each particle by the whole ensemble of particles lying in a 2- or 3-dimensional data space. These forces determine new locations for the particles in each small time-step. Forces can be either gravitational or coulombic.
Reference: 2. <author> J.P. Singh, J.L. Hennessy and A. Gupta, </author> <title> Implications of Hierarchical N-body Methods for Multiprocessor Architecture, </title> <institution> CSL-TR-92-506, Computer Systems Lab, Stanford University, </institution> <year> 1992. </year>
Reference: 3. <author> R. Cypher, A. Ho, S. Konstantinidou and P. Messina, </author> <title> Architectural Requirements of Parallel Scientific Applications with Explicit Communication, </title> <booktitle> in Proc., 20th Annual 11 International Symposium on Computer Architecture, </booktitle> <address> San Diego, CA., </address> <month> May </month> <year> 1993, </year> <pages> 2-13. </pages>
Reference: 4. <author> S. Reinhardt, M. Hill, J. Larus, A. Lebeck, J. Lewis and D. Wood, </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers, </title> <type> 1122, </type> <institution> Computer Sciences Dept., University of Wisconsin-Madison, </institution> <month> Nov. </month> <year> 1992. </year>
Reference: 5. <author> M. D. Dikaiakos, </author> <title> FAST: A Functional Algorithm Simulation Testbed, </title> <type> Ph.D. Thesis, </type> <institution> Princeton University, </institution> <year> 1994. </year>
Reference-contexts: This profile reveals characteristics inherent to the algorithm at hand and is not influenced by partitioning and mapping to a specific multiprocessor. Experiments with differ 7 ent clustering heuristics led to similar results and conclusions (see <ref> [5] </ref>). Figure 4 presents such a profile for a parallel execution of the Fast Multipole Method on a problem instance with 15,000 particles distributed according to the uniform distribution, fifteen particles per quadtree-leaf, and a ten coefficient approximation.
Reference: 6. <author> D. Culler, R. Karp, D. Patterson et al, </author> <title> LogP: Towards a Realistic Model of Parallel Computation, </title> <booktitle> in Proc., Fourth ACM Sigplan Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: It is basically a method for approximately simulating real parallel executions. It can also be considered as an accurate simulation of a theoretical model that accounts for communication costs and limited communication bandwidth, such as LogP <ref> [6] </ref>. The common algorithmic property necessary for Functional Algorithm Simulation to apply is the ability to determine the set of expensive calculations and data exchanges from input information, at the initialization phase of the algorithm, before the actual numerical computations take place.
Reference: 7. <author> J. Barnes and P. Hut, </author> <title> A Hierarchical O(N)log(N) force-calculation algorithm, </title> <booktitle> Nature, </booktitle> <month> 87 </month> <year> (1990) </year> <month> 161-170. </month>
Reference: 8. <author> A.W. Appel, </author> <title> An efficient program for many-body simulation, </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <month> 6 </month> <year> (1986) </year> <month> 85-103. </month>
Reference: 9. <author> T. Chan, </author> <title> Hierarchical Algorithms and Architectures for Parallel Scientific Computing, </title> <booktitle> Proc., International Conference on Supercomputing, </booktitle> <address> Amsterdam, Netherlands, </address> <month> September </month> <year> 1990, </year> <pages> 318-329. </pages>
Reference: 10. <author> C. Lin and L. Snyder, </author> <title> A Portable Implementation of SIMPLE, </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20 (1991), </volume> <pages> 363-401. </pages>
Reference-contexts: By not doing the numerical calculations, FAST achieves significant savings in terms of processor cycles and disk space. For example, a Functional Algorithm Simulation of an instance of the SIMPLE computational fluid dynamics benchmark <ref> [10] </ref> took 0.63 secs to complete on a Sparcstation. The same instance took 9.8 secs to run on one iPSC/2 node.
Reference: 11. <author> V. Sarkar, </author> <title> Partitioning and Scheduling Parallel Programs for Multiprocessors, </title> <publisher> (MIT Press, </publisher> <year> 1989). </year>
Reference-contexts: These groups correspond to the basic computational blocks of the algorithm. Send/Receive primitives correspond to data-dependences between IR-operations and represent the data-flow. In the second phase of the front-end, a parser transforms the Intermediate Representation into a weighted task-flow graph which follows the Macro-Dataflow model of computation <ref> [11] </ref>. Task-nodes in the graph contain a number of Intermediate Representation primitives. Their "boundaries" are defined by Send and Receive primitives occurring in the IR. The tasks start executing upon receipt of all incoming data and continue to completion without interruption. Upon completion their results are forwarded to adjacent nodes. <p> The resulting graph is called the parallel-execution graph and is subsequently passed through clustering, a stage that seeks to minimize the communication overhead of the parallel execution, without sacrificing parallelism [11,16]. Clustering is NP-Complete <ref> [11] </ref>; therefore, FAST provides several different clustering heuristics [15]. After clustering, FAST performs a mapping of the clustered parallel-execution graph onto a message-passing architecture with a number of processors specified by the user. The mapping problem is also NP-complete [11]. <p> Clustering is NP-Complete <ref> [11] </ref>; therefore, FAST provides several different clustering heuristics [15]. After clustering, FAST performs a mapping of the clustered parallel-execution graph onto a message-passing architecture with a number of processors specified by the user. The mapping problem is also NP-complete [11]. To map the task-clusters onto processors, FAST includes another set of heuristics [15]. <p> Figure 4 presents such a profile for a parallel execution of the Fast Multipole Method on a problem instance with 15,000 particles distributed according to the uniform distribution, fifteen particles per quadtree-leaf, and a ten coefficient approximation. In this case we used a clustering technique combining heuristics from <ref> [11] </ref> and [16]. Profiles from non-uniform distributions have similar shape. From these plots it is clear that the parallel execution has three phases: a short phase at the beginning is defined by a large number of active tasks indicating a high degree of available parallelism.
Reference: 12. <author> B. Boothe, </author> <title> Fast Accurate Simulation of Large Shared Memory Multiprocessors, </title> <institution> UCB/CSD 92/682 Computer Science Division, University of California at Berke-ley. </institution>
Reference-contexts: The same instance took 9.8 secs to run on one iPSC/2 node. Considering that exact simulation is approximately 100 to 1000 times slower than actual execution <ref> [12] </ref>, we deduce that Functional Algorithm Simulation decreases the simulation time by two to three orders of magnitude.
Reference: 13. <author> M. Dikaiakos, A. Rogers and K. Steiglitz, </author> <title> Functional Algorithm Simulation: Implementation and Experiments, </title> <institution> TR-429-93, Dept. of Computer Science, Princeton University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Section 3 describes the Fast Multipole Method and the study of the FMM with FAST; additionally, it presents an assessment of the parallel performance of the FMM on different interconnection networks. Finally, Section 4 summarizes our results and conclusions. 2. Functional Algorithm Simulation Functional Algorithm Simulation <ref> [13] </ref> models and evaluates parallel executions by reproducing the skeletons of parallel computations and using them to extract their basic computation and communication patterns. It is basically a method for approximately simulating real parallel executions.
Reference: 14. <author> M. Dikaiakos, A. Rogers, and K. Steiglitz, </author> <title> FAST: A Functional Algorithm Simulation Testbed, </title> <booktitle> in Proc. of the International Workshop on Modeling, Analysis and Simulation of Computer and Telecommunications Systems - MASCOTS'94 (IEEE Press 1994), </booktitle> <pages> 142-146. </pages>
Reference: 15. <author> M. Dikaiakos, A. Rogers and K. Steiglitz, </author> <title> A Comparison of Techniques Used for Mapping Parallel Algorithms to Message-Passing Multiprocessors, </title> <booktitle> in Proc. of the 6th IEEE Symposium on Parallel and Distributed Processing, </booktitle> <address> Dallas, TX, Oct. 1994, </address> <publisher> IEEE Computer Press, </publisher> <pages> 434-442. </pages>
Reference-contexts: The resulting graph is called the parallel-execution graph and is subsequently passed through clustering, a stage that seeks to minimize the communication overhead of the parallel execution, without sacrificing parallelism [11,16]. Clustering is NP-Complete [11]; therefore, FAST provides several different clustering heuristics <ref> [15] </ref>. After clustering, FAST performs a mapping of the clustered parallel-execution graph onto a message-passing architecture with a number of processors specified by the user. The mapping problem is also NP-complete [11]. To map the task-clusters onto processors, FAST includes another set of heuristics [15]. <p> FAST provides several different clustering heuristics <ref> [15] </ref>. After clustering, FAST performs a mapping of the clustered parallel-execution graph onto a message-passing architecture with a number of processors specified by the user. The mapping problem is also NP-complete [11]. To map the task-clusters onto processors, FAST includes another set of heuristics [15].
Reference: 16. <author> A. Gerasoulis and S. Venugopal and T. Yang, </author> <title> Clustering Task Graphs for Message Passing Architectures, </title> <booktitle> in Proc. of the 1990 International Conference on Supercomputing, </booktitle> <pages> 447-457 </pages>
Reference-contexts: In this case we used a clustering technique combining heuristics from [11] and <ref> [16] </ref>. Profiles from non-uniform distributions have similar shape. From these plots it is clear that the parallel execution has three phases: a short phase at the beginning is defined by a large number of active tasks indicating a high degree of available parallelism.
Reference: 17. <author> M. Dikaiakos and J. Stadel, </author> <title> A Performance Study of Cosmological Simulations on Message-Passing and Shared-Memory Multiprocessors. </title> <note> (Submitted for Publication, </note> <month> March </month> <year> 1995). </year> <month> 12 </month>
References-found: 17

