URL: ftp://may.cs.ucla.edu/pub/papers/iosim-sc97.PS
Refering-URL: http://may.cs.ucla.edu/papers/
Root-URL: http://www.cs.ucla.edu
Title: Parallel Simulation of Parallel File Systems and I/O Programs  
Author: Rajive Bagrodia, Stephen Docy, and Andy Kahn 
Keyword: 94-C-0273, "Scalable Systems Software Measurement and Evaluation"  
Note: This work was supported by the Advanced Research Projects Agency, ARPA/CSTO, under Contract  
Address: Los Angeles, CA  
Affiliation: Computer Science Department University of California  
Date: August 14th, 1997  
Web: F-30602  
Abstract: Efficient I/O implementations can have a significant impact on the performance of parallel applications. This paper describes the design and implementation of PIOSIM, a parallel simulation library for MPI-IO programs. The simulator can be used to predict the performance of existing MPI-IO programs as a function of architectural characteristics, caching algorithms, and alternative implementations of collective I/O operations. This paper describes the simulator and presents the results of a number of performance studies to evaluate the impact of the preceding factors on a set of MPI-IO benchmarks, including programs from the NAS benchmark suite. 
Abstract-found: 1
Intro-found: 1
Reference: [BBB94] <author> Sandra Johnson Baylor, Caroline Benveniste, and Leo J. Beolhouwer. </author> <title> A methodology for evaluating parallel I/O performance for massively parallel processors. </title> <booktitle> In Proceedings of the 27th Annual Simulation Symposium, </booktitle> <pages> pages 31-40, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Traditional caching does no collective I/O optimizations, since I/O requests are served as they arrive. These three methods were implemented and compared using the STARFISH [Kot96] simulator, which is based on Proteus [BDCW91], a parallel architecture simulation engine. In <ref> [BBB94] </ref>, a hybrid methodology for evaluating the performance of parallel I/O subsystems was done. PIOS, a trace-driven I/O simulator, is used to calculate the performance of the I/O system for a subset of the problem to be evaluated, while an analytical model was used for the remainder.
Reference: [BBH96] <author> Sandra Johnson Baylor, Caroline Benveniste, and Yarsun Hsu. </author> <title> Performance evaluation of a massively parallel I/O subsystem. </title> <editor> In Ravi Jain, John Werth, and James C. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems, volume 362 of The Kluwer International Series in Engineering and Computer Science, chapter 13, </booktitle> <pages> pages 293-311. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: PIOS, a trace-driven I/O simulator, is used to calculate the performance of the I/O system for a subset of the problem to be evaluated, while an analytical model was used for the remainder. PIOS uses a synthetically generated workload and models the Vulcan network <ref> [BBH96] </ref>. Panda [SCJ + 95] provides a high-level collective I/O library interface.
Reference: [BDCW91] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl. PROTEUS: </author> <title> A High-Performance Parallel-Architecture Simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA 02139, </address> <year> 1991. </year>
Reference-contexts: To reduce simulation times, most simulators typically use direct execution to simulate the local portions of their code and use models only for the communication and input/output events. Even with direct execution, sequential simulation of large parallel programs can be very time consuming <ref> [BDCW91, DGH91, CDJ + 91] </ref>. This has lead to a variety of attempts to use parallel execution to reduce simulation times for models that simulate parallel programs [LW96, RHL + 93, DHN94]. <p> Traditional caching does no collective I/O optimizations, since I/O requests are served as they arrive. These three methods were implemented and compared using the STARFISH [Kot96] simulator, which is based on Proteus <ref> [BDCW91] </ref>, a parallel architecture simulation engine. In [BBB94], a hybrid methodology for evaluating the performance of parallel I/O subsystems was done.
Reference: [BHS + 95] <author> D. Bailey, T. Harris, W. Saphir, R. v.d. Wijngaart, A. Woo, and M. Yarrow. </author> <title> The nas parallel benchmarks 2.0. </title> <type> Technical report nas-95-020, </type> <institution> NASA Ames Research Center, Moffet Field, </institution> <address> CA 94035-1000, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: (e.g., number of I/O nodes, message passing latency), file system characteristics (e.g., data caching and partitioning), and secondary storage device characteristics (e.g., seek time, data transfer time) * performance of the parallel execution of the simulator for three benchmark applications in cluding the BTIO benchmark from the NAS benchmark suite <ref> [BHS + 95] </ref> * performance of the benchmark applications as a function of alternative implementations for collective I/O operations including Two-Phase I/O [dBC93] * performance of the benchmark applications as a function of alternative caching strategies including cooperative caching [DWAP94] The rest of the paper is organized as follows: the next <p> The workload for our experiments was provided by using three MPI/MPI-IO programs plus a synthetic benchmark. The first two programs are from the NAS BTIO Parallel I/O Benchmarks v0.1 2 . The BTIO benchmark extends the original BT benchmark <ref> [BHS + 95] </ref> by using MPI-IO to write data to a file at regular time intervals. The first BTIO program is the "simple" benchmark, where only non-collective I/O and primitive MPI datatypes are used.
Reference: [CDJ + 91] <author> R.G. Covington, S. Dwarkadas, J.R. Jump, J.B. Sinclair, and S. Madala. </author> <title> The efficient simulation of parallel computer systems. </title> <journal> International Journal in Computer Simulation, </journal> <volume> 1 </volume> <pages> 31-58, </pages> <year> 1991. </year>
Reference-contexts: To reduce simulation times, most simulators typically use direct execution to simulate the local portions of their code and use models only for the communication and input/output events. Even with direct execution, sequential simulation of large parallel programs can be very time consuming <ref> [BDCW91, DGH91, CDJ + 91] </ref>. This has lead to a variety of attempts to use parallel execution to reduce simulation times for models that simulate parallel programs [LW96, RHL + 93, DHN94].
Reference: [CF96] <author> Peter F. Corbett and Dror G. Feitelson. </author> <title> The Vesta parallel file system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 14(3) </volume> <pages> 225-264, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: All disk access is performed by the ionodes, which each have their own secondary storage device (SSD) and manage a subset of the file system's total data. In this manner, the parallelism of the user application can be preserved when performing I/O. Some parallel file systems <ref> [CFP + 95, CF96] </ref> even allow user processes to configure the underlying I/O parallelism to match their intended access pattern, further enhancing performance. When properly tuned, the use of file system caching can drastically reduce the number of I/O requests which must wait to be serviced by the SSD. <p> are optimized in two ways: 1) duplicate requests are eliminated, and 2) numerous small requests which lie contiguously are combined into one, large request. 3.2 PFS-SIM The basic structure and functionality of PFS-SIM is taken from the Vesta parallel file system, a highly scalable, experimental file system developed by IBM <ref> [CF96] </ref>. Many of Vesta's features have been included in the design of PFS-SIM, most notably, the use of an interface which allows user applications to configure the parallelism actually used to perform I/O.
Reference: [CFP + 95] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, George S. Almasi, Sandra John-son Baylor, Anthony S. Bolmarcich, Yarsun Hsu, Julian Satran, Marc Snir, Robert Colao, Brian Herr, Joseph Kavaky, Thomas R. Morgan, and Anthony Zlotek. </author> <title> Parallel file systems for the IBM SP computers. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 222-248, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: All disk access is performed by the ionodes, which each have their own secondary storage device (SSD) and manage a subset of the file system's total data. In this manner, the parallelism of the user application can be preserved when performing I/O. Some parallel file systems <ref> [CFP + 95, CF96] </ref> even allow user processes to configure the underlying I/O parallelism to match their intended access pattern, further enhancing performance. When properly tuned, the use of file system caching can drastically reduce the number of I/O requests which must wait to be serviced by the SSD.
Reference: [CGL97] <author> Toni Cortes, Sergi Girona, and Jesus Labarta. </author> <title> Avoiding the cache-coherence problem in a parallel/distributed file system. </title> <booktitle> In Proceedings of the High-Performace Computing and Networking, </booktitle> <pages> pages 860-869, </pages> <month> April </month> <year> 1997. </year>
Reference-contexts: This may not be desirable since it is non-standard, which is one of the reasons why something such as MPI-IO has been proposed. [DWAP94] presented a number of cooperative caching techniques, but only within the framework of a network environment. <ref> [CGL97] </ref> extended cooperative caching to include parallel machines by allowing the speed of the interconnection network to be varied. However, all their results were obtained using the Sprite network workload, which may not be very representative of typical parallel machine workloads. [CGL97] also introduced a cooperative caching strategy which eliminates the <p> but only within the framework of a network environment. <ref> [CGL97] </ref> extended cooperative caching to include parallel machines by allowing the speed of the interconnection network to be varied. However, all their results were obtained using the Sprite network workload, which may not be very representative of typical parallel machine workloads. [CGL97] also introduced a cooperative caching strategy which eliminates the need for coherency mechanisms by coordinating the contents of one hundred percent of the system's file cache. 7 Acknowledgements All the data presented in this paper was collected on the IBM-SP2 at UCLA's Office of Academic Computing, granted to UCLA by
Reference: [dBC93] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <booktitle> In Proceedings of the IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 56-70, </pages> <address> Newport Beach, CA, </address> <year> 1993. </year> <note> Also published in Computer Architecture News 21(5), </note> <month> December </month> <year> 1993, </year> <pages> pages 31-38. </pages>
Reference-contexts: seek time, data transfer time) * performance of the parallel execution of the simulator for three benchmark applications in cluding the BTIO benchmark from the NAS benchmark suite [BHS + 95] * performance of the benchmark applications as a function of alternative implementations for collective I/O operations including Two-Phase I/O <ref> [dBC93] </ref> * performance of the benchmark applications as a function of alternative caching strategies including cooperative caching [DWAP94] The rest of the paper is organized as follows: the next section presents an overview of the subset of MPI-IO and the parallel file system that is simulated. <p> Thus node grouping attempts to avoid flooding the system with too many simultaneous requests. Its utility is heavily dependent on the application [Nit92]. * Two-Phase I/O: I/O is done in two phases <ref> [dBC93] </ref>. In the first phase, processes arrange their requests according to a conforming distribution. This is a permutation of the data across 1 See http://www.research.ibm.com/people/p/prost/sections/mpiio.html 4 the processes so that it coincides as much as possible with the underlying file layout.
Reference: [DGH91] <author> H. Davis, S. R. Goldschmidt, and Hennessey. </author> <title> Multiprocessor simulation and tracing using Tango. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing (ICPP'91), </booktitle> <pages> pages II99-II107, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: To reduce simulation times, most simulators typically use direct execution to simulate the local portions of their code and use models only for the communication and input/output events. Even with direct execution, sequential simulation of large parallel programs can be very time consuming <ref> [BDCW91, DGH91, CDJ + 91] </ref>. This has lead to a variety of attempts to use parallel execution to reduce simulation times for models that simulate parallel programs [LW96, RHL + 93, DHN94].
Reference: [DHN94] <author> P. Dickens, P. Heidelberger, and D. Nicol. </author> <title> A distributed memory lapse: Parallel simulation of message-passing programs. </title> <booktitle> In Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 32-38, </pages> <month> July </month> <year> 1994. </year> <month> 15 </month>
Reference-contexts: Even with direct execution, sequential simulation of large parallel programs can be very time consuming [BDCW91, DGH91, CDJ + 91]. This has lead to a variety of attempts to use parallel execution to reduce simulation times for models that simulate parallel programs <ref> [LW96, RHL + 93, DHN94] </ref>. Most of the existing parallel program simulators are used to evaluate the performance of the memory hierarchy, interconnection network, or processor architecture. To the best of our knowledge, none of the existing parallel simulators have been used to evaluate parallel I/O systems.
Reference: [DWAP94] <author> M.D. Dahlin, R.Y. Wang, T.E. Anderson, and D.A. Patterson. </author> <title> Cooperative Caching: Using Remote Client Memory to Improve File System Performance. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: applications in cluding the BTIO benchmark from the NAS benchmark suite [BHS + 95] * performance of the benchmark applications as a function of alternative implementations for collective I/O operations including Two-Phase I/O [dBC93] * performance of the benchmark applications as a function of alternative caching strategies including cooperative caching <ref> [DWAP94] </ref> The rest of the paper is organized as follows: the next section presents an overview of the subset of MPI-IO and the parallel file system that is simulated. <p> Unfortunately, choosing the correct cache configuration for peak performance is no mean feat. This is particularly true of parallel file systems, which offer a wide variety of techniques for managing the many caches spread across the ionodes and cnodes. One technique with much potential is cooperative caching. Presented in <ref> [DWAP94] </ref> 1 as a set of high performance caching algorithms for use within a network file system, cooperative caching attempts to improve performance through better management of multiple client and server caches. <p> This is very similar to physically moving some of the cnode cache to each of the ionodes. The penalty for this is a reduced hit rate at each cnode's local cache. * Optimized globally managed caching: For the network environment in <ref> [DWAP94] </ref>, the tradeoff of a reduced local hit rate for a higher global hit rate resulted in peak cache perfor 5 mance occurring when 80% of each cnode's cache was coordinated. <p> Unfortunately, it may not produce as much performance as a true implementation of disk-directed I/O might. Also, Panda provides its own API to access its libraries. This may not be desirable since it is non-standard, which is one of the reasons why something such as MPI-IO has been proposed. <ref> [DWAP94] </ref> presented a number of cooperative caching techniques, but only within the framework of a network environment. [CGL97] extended cooperative caching to include parallel machines by allowing the speed of the interconnection network to be varied.
Reference: [For93] <author> MPI Forum. </author> <title> Mpi: A message passing interface. </title> <booktitle> In Proceedings of 1993 Supercomputing Conference, </booktitle> <address> Portland, Washington, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: For users requiring maximum I/O performance, the pairing of a high performance parallel file system with a language-level parallel I/O library, such as MPI-IO, is a natural choice. MPI is a standard message passing library that provides a number of point-to-point and collective communication primitives <ref> [For93] </ref>. MPI-IO is a proposed extension to MPI that would incorporate parallel I/O constructs [MPI96].
Reference: [Kot96] <author> David Kotz. </author> <title> Tuning STARFISH. </title> <type> Technical Report PCS-TR96-296, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: Traditional caching does no collective I/O optimizations, since I/O requests are served as they arrive. These three methods were implemented and compared using the STARFISH <ref> [Kot96] </ref> simulator, which is based on Proteus [BDCW91], a parallel architecture simulation engine. In [BBB94], a hybrid methodology for evaluating the performance of parallel I/O subsystems was done.
Reference: [Kot97] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 15(1) </volume> <pages> 41-74, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: For a simple matrix multiplication example, the predicted execution time decreased from about five hours to 4 minutes for a relatively small matrix with 5000 elements. 6 Related Work Three approaches to collective I/O are discussed in <ref> [Kot97] </ref>: traditional caching, two-phase I/O, and disk-directed I/O. Traditional caching does no collective I/O optimizations, since I/O requests are served as they arrive. These three methods were implemented and compared using the STARFISH [Kot96] simulator, which is based on Proteus [BDCW91], a parallel architecture simulation engine.
Reference: [LW96] <author> U. Legedza and W. E. Weihl. </author> <title> Reducing synchronization overhead in parallel simulation. </title> <booktitle> In 10th Workshop on Parallel and Distributed Simulation (PADS96), </booktitle> <pages> pages 86-95, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Even with direct execution, sequential simulation of large parallel programs can be very time consuming [BDCW91, DGH91, CDJ + 91]. This has lead to a variety of attempts to use parallel execution to reduce simulation times for models that simulate parallel programs <ref> [LW96, RHL + 93, DHN94] </ref>. Most of the existing parallel program simulators are used to evaluate the performance of the memory hierarchy, interconnection network, or processor architecture. To the best of our knowledge, none of the existing parallel simulators have been used to evaluate parallel I/O systems.
Reference: [MPI96] <author> MPI-IO: </author> <title> a parallel file I/O interface for MPI. The MPI-IO Committee, </title> <month> April </month> <year> 1996. </year> <note> Version 0.5. See WWW http://lovelace.nas.nasa.gov/MPI-IO/mpi-io-report.0.5.ps. </note>
Reference-contexts: MPI is a standard message passing library that provides a number of point-to-point and collective communication primitives [For93]. MPI-IO is a proposed extension to MPI that would incorporate parallel I/O constructs <ref> [MPI96] </ref>.
Reference: [Nit92] <author> Bill Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Thus node grouping attempts to avoid flooding the system with too many simultaneous requests. Its utility is heavily dependent on the application <ref> [Nit92] </ref>. * Two-Phase I/O: I/O is done in two phases [dBC93]. In the first phase, processes arrange their requests according to a conforming distribution. <p> Although node grouping has previously been shown to yield upto 8-times improvement <ref> [Nit92] </ref>, it has performed very poorly for the NAS 2 A newer version, v0.2, of the NAS BTIO Parallel I/O Benchmarks is available.
Reference: [NK96] <author> Nils Nieuwejaar and David Kotz. </author> <title> The Galley parallel file system. </title> <booktitle> In Proceedings of the 10th ACM International Conference on Supercomputing, </booktitle> <pages> pages 374-381, </pages> <address> Philadelphia, PA, May 1996. </address> <publisher> ACM Press. </publisher>
Reference-contexts: We have included simple models based on seek time, rotational latency, and data transfer rate as well as a higly detailed model developed at Dartmouth <ref> [NK96] </ref>. Since we restrict our attention in to parallel I/O simulations, the interested reader is referred to [Pra96] for information on MPISIM. The I/O simulator has been designed to be both modular and extensible: it is relatively easy to replace individual modules at each of the preceding levels.
Reference: [oM96] <author> PMPIO-A Portable Implementation of MPI-IO. Samuel a. fineberg and parkson wong and bill nitzberg and chris kuszmaul. </author> <booktitle> In Proceedings of the Sixth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 188-195. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: Consequently, PIO-SIM maintains its own internal structure to represent datatypes. This structure consists of a list of base offsets and the length (number of bytes) of valid data at that offset. This structure is similar to I/O lists, as found in PMPIO, NAS's implementation of MPI-IO <ref> [oM96] </ref>, as well as type maps, as found in the MPI-IO implementation for the NEC Cenju-3 [SPB96]. We will refer to this structure simply as an IOL. In MPI-IO, opening a file is a collective operation. Upon invocation, each target LP creates an IOL to represent its filetype (ftype IOL).
Reference: [Pra96] <author> Sundeep Prakash. </author> <title> Performance Prediction of Parallel Programs. </title> <type> Ph.d. dissertation, </type> <institution> Computer Science Dept, UCLA, </institution> <address> Los Angeles, CA, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: MPISIM is a multi-threaded MPI simulator that was developed for this purpose <ref> [Pra96] </ref>. * PIO-SIM: simulates the individual and collective I/O constructs provided by MPI-IO. These constructs include creating, opening, closing and deleting a file; most data access (read/write) operations; and a local datatype constructor introduced as part of the MPI-IO specification. <p> We have included simple models based on seek time, rotational latency, and data transfer rate as well as a higly detailed model developed at Dartmouth [NK96]. Since we restrict our attention in to parallel I/O simulations, the interested reader is referred to <ref> [Pra96] </ref> for information on MPISIM. The I/O simulator has been designed to be both modular and extensible: it is relatively easy to replace individual modules at each of the preceding levels.
Reference: [RHL + 93] <author> S. K. Reinhardt, M. D. Hill, J. R. Larus, A. R. Lebeck, J. C. Lewis, and D. A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Even with direct execution, sequential simulation of large parallel programs can be very time consuming [BDCW91, DGH91, CDJ + 91]. This has lead to a variety of attempts to use parallel execution to reduce simulation times for models that simulate parallel programs <ref> [LW96, RHL + 93, DHN94] </ref>. Most of the existing parallel program simulators are used to evaluate the performance of the memory hierarchy, interconnection network, or processor architecture. To the best of our knowledge, none of the existing parallel simulators have been used to evaluate parallel I/O systems.
Reference: [SCJ + 95] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-directed collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <address> San Diego, CA, December 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: PIOS, a trace-driven I/O simulator, is used to calculate the performance of the I/O system for a subset of the problem to be evaluated, while an analytical model was used for the remainder. PIOS uses a synthetically generated workload and models the Vulcan network [BBH96]. Panda <ref> [SCJ + 95] </ref> provides a high-level collective I/O library interface. It implements server-directed I/O, which is disk-directed I/O at the logical file level, rather than the physical disk level. 13 This method provides a high-level of portability by avoiding the difficult details of utilizing specific attributes for each underlying filesystem.
Reference: [SPB96] <author> Darren Sanders, Yoonho Park, and Maciej Brodowicz. </author> <title> Implementation and performance of MPI-IO file access using MPI datatypes. </title> <type> Technical Report UH-CS-96-12, </type> <institution> University of Houston, </institution> <month> November </month> <year> 1996. </year> <month> 16 </month>
Reference-contexts: This structure is similar to I/O lists, as found in PMPIO, NAS's implementation of MPI-IO [oM96], as well as type maps, as found in the MPI-IO implementation for the NEC Cenju-3 <ref> [SPB96] </ref>. We will refer to this structure simply as an IOL. In MPI-IO, opening a file is a collective operation. Upon invocation, each target LP creates an IOL to represent its filetype (ftype IOL). Afterwards, each target LP exchanges its ftype IOL with all other target LP's.
References-found: 24

