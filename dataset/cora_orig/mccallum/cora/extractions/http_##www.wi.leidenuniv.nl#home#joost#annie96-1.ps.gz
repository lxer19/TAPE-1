URL: http://www.wi.leidenuniv.nl/home/joost/annie96-1.ps.gz
Refering-URL: http://www.wi.leidenuniv.nl/home/joost/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail s.haring@cs.ruu.nl  
Phone: 2  
Title: TUNING IMAGE FILTERS USING FEED-FORWARD NETWORKS  
Author: S. HARING J.N. KOK and M.A. VIERGEVER 
Address: P.O. Box 80089, 3508 TB Utrecht, The Netherlands  
Affiliation: 1 Department of Computer Science, Utrecht University  Department of Computer Science, Leiden University 3 Image Center Utrecht, Utrecht University Hospital  
Abstract: We present a method to automatically tune filters that are used to extract features from image data. In a standard image processing approach filters are fixed. The filters are embedded in a feed-forward network architecture. Tuning of the filter-parameters is achieved using the back-propagated error terms that are available for the input units of such a network. Details are provided for the Gaussian filter and its derivatives, but the approach is also applicable to Gabor filters, wavelets, etc. We will show that the proposed algorithm can find optimal parameter settings for filters|given a specific task.
Abstract-found: 1
Intro-found: 1
Reference: <author> Florack, L. M. J., B. M. ter Haar Romeny, J. J. Koenderink, and M. A. </author> <month> Viergever </month> <year> (1993). </year> <title> Cartesian differential invariants in scale-space. </title> <journal> Journal of Mathematical Image and Vision. </journal>
Reference-contexts: And an edge-detector will only extract desired edges if a proper scale is applied. We will consider two scaled feature-extractors, which follow from scale space theory <ref> (Florack, ter Haar Romeny, Koenderink, and Viergever 1993) </ref>: (i) blurring an image|this happens by convolving the image with a Gaussian; (ii) edge-detection| this is done by convolving the image with derivatives of a Gaussian. For both feature-extractors we will derive learning-rules for scale .
Reference: <author> Freeman, W. and E. </author> <title> Adelson (1991). The design and use of streerable filters. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 13, </journal> <pages> 891-906. </pages>
Reference-contexts: The last row shows how two automatically tuned scales outperform the manually tuned scales. Methods that are related to ours are, amongst others, the Adaptive-Subspace som (assom) (Kohonen 1995), steerable filters <ref> (Freeman and Adelson 1991) </ref>, Radial Basis Function (rbf) networks (Moody and Darken 1989), and the Neocognitron (Fukushima 1980). The assom is also used to tune filter parameters but it is an unsupervised approach based on the Self-Organizing Map.
Reference: <author> Fukushima, K. </author> <year> (1980). </year> <title> Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. </title> <booktitle> Biological Cybernetics 36, </booktitle> <pages> 193-202. </pages>
Reference-contexts: The last row shows how two automatically tuned scales outperform the manually tuned scales. Methods that are related to ours are, amongst others, the Adaptive-Subspace som (assom) (Kohonen 1995), steerable filters (Freeman and Adelson 1991), Radial Basis Function (rbf) networks (Moody and Darken 1989), and the Neocognitron <ref> (Fukushima 1980) </ref>. The assom is also used to tune filter parameters but it is an unsupervised approach based on the Self-Organizing Map.
Reference: <author> Kohonen, T. </author> <year> (1995). </year> <title> The adaptive-subspace som (assom) and its use for the implementation of invariant feature detextion. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, </booktitle> <pages> pp. 3-10. </pages>
Reference-contexts: The third row shows the applied scale; in the last row the percentage of correctly classifed pixels in test images is presented. The last row shows how two automatically tuned scales outperform the manually tuned scales. Methods that are related to ours are, amongst others, the Adaptive-Subspace som (assom) <ref> (Kohonen 1995) </ref>, steerable filters (Freeman and Adelson 1991), Radial Basis Function (rbf) networks (Moody and Darken 1989), and the Neocognitron (Fukushima 1980). The assom is also used to tune filter parameters but it is an unsupervised approach based on the Self-Organizing Map.
Reference: <author> Moody, J. and C. </author> <month> Darken </month> <year> (1989). </year> <title> Fast learning in networks of locally-tuned processing units. </title> <booktitle> Neural Computation 1, </booktitle> <pages> 281-294. </pages>
Reference-contexts: The last row shows how two automatically tuned scales outperform the manually tuned scales. Methods that are related to ours are, amongst others, the Adaptive-Subspace som (assom) (Kohonen 1995), steerable filters (Freeman and Adelson 1991), Radial Basis Function (rbf) networks <ref> (Moody and Darken 1989) </ref>, and the Neocognitron (Fukushima 1980). The assom is also used to tune filter parameters but it is an unsupervised approach based on the Self-Organizing Map.
Reference: <author> S.Haring, M.A.Viergever, and J. </author> <title> Kok (1994). Kohonen networks for multiscale image segmentation. </title> <booktitle> Image and Vision Computing 12, </booktitle> <pages> 339-344. </pages>
Reference-contexts: In the intended segmentation in which the various objects are depicted. We apply a multi-scale (or multi-resolution) technique; the input image is convolved with various Gaussians with a different scale. In previous work we tuned the scales of the Gaussians manually <ref> (S.Haring, M.A.Viergever, and Kok 1994) </ref>. With the algorithm proposed here, we can tune the scales automatically. Table 1 reveals that the automatically found scales yield better segmentation results than fixed scales. Furthermore, we need less Gaussians than before, to get these results.
References-found: 6

