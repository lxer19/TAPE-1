URL: http://www.cs.tamu.edu/people/ravi/JPDC98.ps.gz
Refering-URL: http://www.cs.tamu.edu/people/ravi/
Root-URL: http://www.cs.tamu.edu
Email: E-mail: fravi, hwang, bhuyang@cs.tamu.edu  
Title: Design and Analysis of Static Memory Management Policies for CC-NUMA Multiprocessors  
Author: R. Iyer, H.J.Wang, and L.N.Bhuyan 
Keyword: memory management, interconnection networks, execution-based simulation, scientific applications, shared-memory multiprocessor.  
Note: This research has been supported by NSF grant MIP 9622740.  
Address: College Station, TX 77843-3112, USA.  
Affiliation: Department of Computer Science Texas A&M University  
Abstract: The primary bottleneck of CC-NUMA architectures remains in the remote memory access that has latencies several magnitudes higher than the local cache access. Designing effective data allocation policies that provide local memory data access and limit the need to access remote memories remains a challenge. We characterize the performance of three existing memory management techniques, namely, buddy, round-robin, and first-touch policies. With existing memory management schemes, we find several cases where requests from different processors arrive at the same memory simultaneously. This gives rise to bulky replies (in the form of data blocks) from the same memory. The cause of this bulkiness lies in the distribution of scientific data in sizes of a power of 2 distributed over a number of memory modules, also a power of 2. To alleviate this problem, we present two new memory management policies called skew-mapping and prime-mapping policies. By utilizing the properties of skewing and prime, the new memory management designs considerably improve the application performance of CC-NUMA multiprocessors. Interconnection network performance depends heavily on the memory access patterns of the workload. Using these existing and new memory management policies, we re-evaluate the performance of a multistage interconnection network (MIN). Limited earlier work in the area of CC-NUMA memory management has assumed constant network delays and thus ignored the impact of switch design on performance. We use accurate crossbar switch models in conjunction with the changing memory access patterns caused by the memory management policies and the application data access behavior. Our results effectively present the performance benefits of different memory management techniques based on the sharing patterns of applications. Applications with a low degree of sharing benefit from the data locality provided by first-touch. However, several applications with significant sharing degrees as well as those with single processor initialization routines benefit highly from the intelligent distribution of data provided by skew-mapping and prime-mapping schemes. Improvements due to the new schemes are found to be as high as 35% in stall time. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. P. Larowe and C. S. Ellis, </author> <title> "Experimental Comparisons of Memory Management Policies for NUMA Multiprocessors," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 9, no. 4, </volume> <pages> pp. 319-363, </pages> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Related work in this area can be divided into two categories. The first category has been the system level performance evaluation of memory management policies <ref> [1, 2, 3] </ref>. Most of these studies focus on distributed shared memory systems without hardware cache coherence. Recently the effect of different policies was studied for CC-NUMA systems in [4]. This work presented significant data on the OS/hardware support required for dynamic page migration and replication policies.
Reference: [2] <author> C. Scheurich and M. Dubois, </author> <title> "Dynamic Page Migration in Multiprocessors with Distributed Global Memory," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 38, no. 8, </volume> <month> Aug. </month> <year> 1989. </year> <month> 35 </month>
Reference-contexts: Related work in this area can be divided into two categories. The first category has been the system level performance evaluation of memory management policies <ref> [1, 2, 3] </ref>. Most of these studies focus on distributed shared memory systems without hardware cache coherence. Recently the effect of different policies was studied for CC-NUMA systems in [4]. This work presented significant data on the OS/hardware support required for dynamic page migration and replication policies.
Reference: [3] <author> J. Ramanathan and L. M. Ni, </author> <title> "Critical Factors in NUMA memory management," </title> <booktitle> In Proc of the 11th Conference on Distributed Computing Systems, </booktitle> <pages> pp. 500-507, </pages> <address> Arlington , Texas, </address> <month> May </month> <year> 1991, </year> <journal> IEEE Computer Society. </journal>
Reference-contexts: Related work in this area can be divided into two categories. The first category has been the system level performance evaluation of memory management policies <ref> [1, 2, 3] </ref>. Most of these studies focus on distributed shared memory systems without hardware cache coherence. Recently the effect of different policies was studied for CC-NUMA systems in [4]. This work presented significant data on the OS/hardware support required for dynamic page migration and replication policies.
Reference: [4] <author> B. Vergese, et al., </author> <title> "Operating System Support for improving data locality on CC-NUMA Computer Servers," </title> <booktitle> Proc. ASPLOS-VII, </booktitle> <pages> pp. 279-289, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: The first category has been the system level performance evaluation of memory management policies [1, 2, 3]. Most of these studies focus on distributed shared memory systems without hardware cache coherence. Recently the effect of different policies was studied for CC-NUMA systems in <ref> [4] </ref>. This work presented significant data on the OS/hardware support required for dynamic page migration and replication policies. However, their results also indicate that dynamic memory management policies improve the performance of parallel applications (the SPLASH workload) by only 4% over the static schemes.
Reference: [5] <author> D.T. Shapiro, </author> <title> "Theoretical limitations on the Efficient Use of Parallel Memories," </title> <journal> IEEE Transa-tions on Computers, </journal> <volume> vol. c-27, no. 5, </volume> <pages> pp. 421-428, </pages> <month> May </month> <year> 1978. </year>
Reference-contexts: It can be specified as a function that maps logical pages onto memories such that the required pages could be accessed conflict-free. Several general classes of skewing memory data accesses have been investigated and charaterized by previous researchers <ref> [5, 6, 7] </ref>, but not yet been developed for CC-NUMA memory management. The prime-mapping scheme is based on the allocation of data pages to memories according to a prime number.
Reference: [6] <author> D.T. Harper III, </author> <title> "Block, Multistride Vector, and FFT Accesses in Parallel Memory Systems," </title> <journal> IEEE Transations on Parallel and Distributed Systems, </journal> <volume> vol. 2, no. 1, </volume> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: It can be specified as a function that maps logical pages onto memories such that the required pages could be accessed conflict-free. Several general classes of skewing memory data accesses have been investigated and charaterized by previous researchers <ref> [5, 6, 7] </ref>, but not yet been developed for CC-NUMA memory management. The prime-mapping scheme is based on the allocation of data pages to memories according to a prime number.
Reference: [7] <author> A. Deb, </author> <title> "Multiskewing-A Noval Technique for Optimal Parallel Memory Access," </title> <journal> IEEE Transa-tions on Parallel and Distributed Systems, </journal> <volume> vol. 7, no. 6, </volume> <pages> pp. 595-604, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: It can be specified as a function that maps logical pages onto memories such that the required pages could be accessed conflict-free. Several general classes of skewing memory data accesses have been investigated and charaterized by previous researchers <ref> [5, 6, 7] </ref>, but not yet been developed for CC-NUMA memory management. The prime-mapping scheme is based on the allocation of data pages to memories according to a prime number.
Reference: [8] <author> D.H. Lawrie and C.R.Vora, </author> <title> "The Prime Memory System for Array Access," </title> <journal> IEEE Transations on Computers, </journal> <volume> vol. c-31, no. 5, </volume> <pages> pp. 435-442, </pages> <month> May </month> <year> 1982. </year>
Reference-contexts: The prime-mapping scheme is based on the allocation of data pages to memories according to a prime number. The use of a prime number for effective distribution of data accesses has been studied in a few papers <ref> [8, 9] </ref>. Lawrie [8] described a memory system designed for parallel array access which based on the use of a prime number of memories in SIMD computers. Recently, Yang [9] presented a prime-mapped cache in the vector processing environment. <p> The prime-mapping scheme is based on the allocation of data pages to memories according to a prime number. The use of a prime number for effective distribution of data accesses has been studied in a few papers [8, 9]. Lawrie <ref> [8] </ref> described a memory system designed for parallel array access which based on the use of a prime number of memories in SIMD computers. Recently, Yang [9] presented a prime-mapped cache in the vector processing environment.
Reference: [9] <author> Q. Yang, </author> <title> "Introducing a New Cache Design into Vector Computers," </title> <journal> IEEE Transations on Computers, </journal> <volume> vol. c-42, no. 12, </volume> <pages> pp. 1411-1424, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: The prime-mapping scheme is based on the allocation of data pages to memories according to a prime number. The use of a prime number for effective distribution of data accesses has been studied in a few papers <ref> [8, 9] </ref>. Lawrie [8] described a memory system designed for parallel array access which based on the use of a prime number of memories in SIMD computers. Recently, Yang [9] presented a prime-mapped cache in the vector processing environment. <p> The use of a prime number for effective distribution of data accesses has been studied in a few papers [8, 9]. Lawrie [8] described a memory system designed for parallel array access which based on the use of a prime number of memories in SIMD computers. Recently, Yang <ref> [9] </ref> presented a prime-mapped cache in the vector processing environment. The memory access logic of the new allocation schemes is similar to those of the existing policies, resulting in no additional delay for memory accesses.
Reference: [10] <author> L.N. Bhuyan, Q. Yang and D.P. Agrawal, </author> <title> "Performance of Multiprocessor Interconnection Networks," </title> <journal> Computer, </journal> <volume> vol. 22, no. 2, pp.25-37, </volume> <month> Feb. </month> <year> 1989. </year>
Reference-contexts: We identify the effect of these policies on different components of processor stall times while executing scientific applications. The second category of related work is the performance evaluation of interconnection networks. Performance evaluation of interconnection networks (IN's) has been an active area of research for a long 1 time <ref> [10, 11, 12] </ref>. These studies were conducted with a synthetic workload that is more suitable for a message passing or networking environment. Workload in a CC-NUMA environment is characterized by unsymmetric bulky messages due to cache coherence, synchronization and memory management.
Reference: [11] <author> W. J. Dally, </author> <title> "Virtual-Channel Flow Control," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 3, no. 2, </volume> <pages> pp. 194-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: We identify the effect of these policies on different components of processor stall times while executing scientific applications. The second category of related work is the performance evaluation of interconnection networks. Performance evaluation of interconnection networks (IN's) has been an active area of research for a long 1 time <ref> [10, 11, 12] </ref>. These studies were conducted with a synthetic workload that is more suitable for a message passing or networking environment. Workload in a CC-NUMA environment is characterized by unsymmetric bulky messages due to cache coherence, synchronization and memory management.
Reference: [12] <author> L. N. Bhuyan, et al., </author> <title> "Performance of Multistage Bus Networks for a Distributed Shared Memory Multiprocessor," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 8, no. 1, </volume> <pages> pp. 82-95, </pages> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: We identify the effect of these policies on different components of processor stall times while executing scientific applications. The second category of related work is the performance evaluation of interconnection networks. Performance evaluation of interconnection networks (IN's) has been an active area of research for a long 1 time <ref> [10, 11, 12] </ref>. These studies were conducted with a synthetic workload that is more suitable for a message passing or networking environment. Workload in a CC-NUMA environment is characterized by unsymmetric bulky messages due to cache coherence, synchronization and memory management.
Reference: [13] <author> A. Kumar and L.N. Bhuyan, </author> <title> "Evaluating virtual channels for cache coherent shared memory multiprocessors," </title> <booktitle> ACM International Conference on Supercomuting, </booktitle> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: The aim of this paper is to re-evaluate the performance of an IN with realistic application data accesses and memory management policies governing the placement of the data blocks. Execution-based evaluations for mesh interconnection networks have been reported recently <ref> [13, 14] </ref> to test the effectiveness of virtual channels. While these studies provide useful data, they do not explore different memory management techniques as we do. We consider a multistage interconnection network (MIN) in this paper, similar to the one employed in Butterfly and Cedar multiprocessors [15].
Reference: [14] <author> L.N.Bhuyan, H.Wang, R.Iyer, and A.Kumar, </author> <title> "Impact of Switch Design on the Application Performance of Cache-Coherent Multiprocessors," </title> <booktitle> In Proceedings of IPPS/SPDP'98, </booktitle> <address> Orlando, FL, </address> <pages> pp. 466-475, </pages> <month> April </month> <year> 1998. </year>
Reference-contexts: The aim of this paper is to re-evaluate the performance of an IN with realistic application data accesses and memory management policies governing the placement of the data blocks. Execution-based evaluations for mesh interconnection networks have been reported recently <ref> [13, 14] </ref> to test the effectiveness of virtual channels. While these studies provide useful data, they do not explore different memory management techniques as we do. We consider a multistage interconnection network (MIN) in this paper, similar to the one employed in Butterfly and Cedar multiprocessors [15]. <p> Unlike Cedar, we employ a NUMA organization with one network (like Butterfly) that is used for both forward and backward (reply) messages. We evaluate two different switch design alternatives (simple wormhole (SWH) and buffered virtual channel (BVC)) <ref> [14] </ref> for the MIN. To evaluate the different memory management techniques in conjunction with different switch architectures, we have significantly modified our CC-NUMA simulator based on Proteus [16]. We developed virtual memory support in the Proteus simulator to evaluate the memory management techniques. <p> scheme is provided in the SGI Origin system. 4 2.2 The New Memory Management Policies For round-robin policies, we find that requests from different processors go to the same memory at the same time giving rise to bulk of replies (in the form of data blocks) from the same memory <ref> [14] </ref>. The cause of this bulky response lies in the distribution of scientific data size of a power of 2 distributed over a number of memory modules, also a power of 2. This phenomenon builds up a large delay at the network interface and considerably affects the performance. <p> We have modified the simulator extensively to exactly model the MIN with wormhole routing. We have also incorporated different switch architectures with virtual channels and multi-flitbuffers <ref> [14] </ref>. For this work, all the virtual memory support needed to simulate the data allocation policies in Section 2 have been implemented. The architecture under consideration can be modeled as processor, cache, memory, network interface and the network, as shown in Figure 4a. <p> Flits from only one message can occupy the buffers at one virtual channel at a time. There are no buffers on the output side to avoid an unnecessary delay. We call such a design as buffered virtual channel (BVC) switch, as shown in Figure 4d <ref> [14] </ref>. The buffered virtual channel switch (BVC32) used in this study has a buffer size of 32 flits and 2 virtual channels. Selection among the virtual channels in the BVC switch is done using a modified version of round-robin arbitration policy to cut down the message latency.
Reference: [15] <author> J. Torrellas and Z. Zheng, </author> <title> "The Performance of the Cedar Multistage Switching Network," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 8, no. 4, </volume> <pages> pp. 321-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: While these studies provide useful data, they do not explore different memory management techniques as we do. We consider a multistage interconnection network (MIN) in this paper, similar to the one employed in Butterfly and Cedar multiprocessors <ref> [15] </ref>. Unlike Cedar, we employ a NUMA organization with one network (like Butterfly) that is used for both forward and backward (reply) messages. We evaluate two different switch design alternatives (simple wormhole (SWH) and buffered virtual channel (BVC)) [14] for the MIN. <p> For first-touch, the similiar phenomenon is seen in four out of six applications. The same situation was observed in Cedar network <ref> [15] </ref> where there was a large delay at the input of the backward network.
Reference: [16] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl, "Proteus: </author> <title> A High-Performance Parallel-Architecture Simulator," </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: We evaluate two different switch design alternatives (simple wormhole (SWH) and buffered virtual channel (BVC)) [14] for the MIN. To evaluate the different memory management techniques in conjunction with different switch architectures, we have significantly modified our CC-NUMA simulator based on Proteus <ref> [16] </ref>. We developed virtual memory support in the Proteus simulator to evaluate the memory management techniques. We have also incorporated detailed switch models and wormhole routing with virtual channels in the network to accurately interpret the effect of network latency on application stall time. <p> These reordered pages are allocated to processors P 0 , P 1 , P 2 , and P 3 using i mod 5 again. 3 Simulator Design Our simulator is based on Proteus <ref> [16] </ref> that implemented MIN using an analytical model. We have modified the simulator extensively to exactly model the MIN with wormhole routing. We have also incorporated different switch architectures with virtual channels and multi-flitbuffers [14].
Reference: [17] <author> L. M. Censier and P. Feautrier, </author> <title> "A New Solution to Coherence Problems in Multicache Systems," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-27, no. 12, </volume> <pages> pp. 1112-1118, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: We have also incorporated detailed switch models and wormhole routing with virtual channels in the network to accurately interpret the effect of network latency on application stall time. We have modified the directory based cache coherence protocol <ref> [17] </ref> in the simulator to correctly account for the possible out-of-order messages due to the presence of virtual channels in the MIN. Our simulation results show that the performance of memory management techniques depends on the application sharing pattern. <p> Multiple input channels can request for the same output channel in this phase. This conflict is resolved in the second step. A similar arbitration policy is used in the Intel Cavallino switch [22]. 3.2 Cache Coherence And Synchronization We implemented the full-map directory-based cache coherence protocol <ref> [17] </ref> with some modifications for evaluation in this paper. In this scheme, each shared memory block is assigned to a node, called home node, which maintains the directory entries for that block. Each entry in the directory is a bit-vector of same length as the number of nodes.
Reference: [18] <author> D. Lenoski, et. al., </author> <title> "The DASH Prototype: Logic Overhead and Performance," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 4, no. 1, </volume> <pages> pp. 41-61, </pages> <month> Jan </month> <year> 1993. </year>
Reference-contexts: The distribution of the pages is extremely uniform thus smoothening out the traffic generated in the network and in most cases making sure that simultaneous access to different data from different processors are interleaved among the different nodes. This scheme is used in the DASH <ref> [18] </ref>, HP/Exemplar [19], and SGI Origin [20] multiprocessors. However, the scheme does not place data intelligently enough to reduce the number of remote accesses. Since the distribution is regular in most scientific applications, many processors tend to access multiple pages on the same node in a given time interval. <p> The switch latency or delay is considered to be 4 processor cycles. The SGI Spider and Cavallino [21, 22] both have 4 cycles of latency. The page size is considered to be 1, 2, or 4 Kbytes. The interface delay is 20 cycles similar to DASH <ref> [18] </ref>. 3.1 Network/Switch Architecture The schematic of the network is shown in Figure 4b. It is a multistage interconnection network (MIN) employing 2x2 switches. In general, an NxN MIN consists of log 2 N stages of 2x2 switches with N/2 such switches per stage.
Reference: [19] <author> G. Astfalk and T. Brewer., </author> <title> "An Overview of the HP/Convex Exemplar Hardware.," </title> <address> http://www.convex.com/tech cache/ps/hw ov.ps. </address> <month> 36 </month>
Reference-contexts: The distribution of the pages is extremely uniform thus smoothening out the traffic generated in the network and in most cases making sure that simultaneous access to different data from different processors are interleaved among the different nodes. This scheme is used in the DASH [18], HP/Exemplar <ref> [19] </ref>, and SGI Origin [20] multiprocessors. However, the scheme does not place data intelligently enough to reduce the number of remote accesses. Since the distribution is regular in most scientific applications, many processors tend to access multiple pages on the same node in a given time interval.
Reference: [20] <author> J. Laudon and D. Lenoski, </author> <title> "The SGI Origin: A ccNUMA Highly Scalable Server," </title> <booktitle> Proceedings of 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 241-251, </pages> <year> 1997. </year>
Reference-contexts: This scheme is used in the DASH [18], HP/Exemplar [19], and SGI Origin <ref> [20] </ref> multiprocessors. However, the scheme does not place data intelligently enough to reduce the number of remote accesses. Since the distribution is regular in most scientific applications, many processors tend to access multiple pages on the same node in a given time interval.
Reference: [21] <author> M. Galles, </author> <title> "Scalable Pipelined Interconnect for Distributed Endpoint Routing: The SGI SPIDER Chip," </title> <booktitle> Proc. Symp. High Performance Interconnects (Hot Interconnects 4), </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: The switch parameters are explained in detail in section 3.1. A flit size of 16 bits was considered which is same as the width of the link. The switch latency or delay is considered to be 4 processor cycles. The SGI Spider and Cavallino <ref> [21, 22] </ref> both have 4 cycles of latency. The page size is considered to be 1, 2, or 4 Kbytes. The interface delay is 20 cycles similar to DASH [18]. 3.1 Network/Switch Architecture The schematic of the network is shown in Figure 4b. <p> In virtual cut-through, the message is buffered when it is blocked. We use these switching techniques because they give better performance than packet switching and because they are used in the design of current multiprocessor switches (SPIDER <ref> [21] </ref>, CAVALLINO [22]). There are two types of messages that are transmitted over the MIN. One is a control message that consists of read, write and invalidation signals that are short and 4 flits long. <p> The input synchronization takes one to two cycles and it takes one more cycle for transmission. As a result, the switch delay is about 4 cycles, which is similar to the time taken in SGI Spider and Intel Cavallino switches <ref> [21, 22] </ref>. Virtual channels (VC's)[11] are used in wormhole networks to avoid deadlocks and to improve link utilization and network throughput. Whenever there is a blocking of a message, another message can be routed through other VCs.
Reference: [22] <author> J. Carbonaro and F. Verhoorn, "Cavallino: </author> <title> The Teraflops Router and NIC," </title> <booktitle> Proc. Symp. High Performance Interconnects (Hot Interconnects 4), </booktitle> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: The switch parameters are explained in detail in section 3.1. A flit size of 16 bits was considered which is same as the width of the link. The switch latency or delay is considered to be 4 processor cycles. The SGI Spider and Cavallino <ref> [21, 22] </ref> both have 4 cycles of latency. The page size is considered to be 1, 2, or 4 Kbytes. The interface delay is 20 cycles similar to DASH [18]. 3.1 Network/Switch Architecture The schematic of the network is shown in Figure 4b. <p> In virtual cut-through, the message is buffered when it is blocked. We use these switching techniques because they give better performance than packet switching and because they are used in the design of current multiprocessor switches (SPIDER [21], CAVALLINO <ref> [22] </ref>). There are two types of messages that are transmitted over the MIN. One is a control message that consists of read, write and invalidation signals that are short and 4 flits long. <p> The input synchronization takes one to two cycles and it takes one more cycle for transmission. As a result, the switch delay is about 4 cycles, which is similar to the time taken in SGI Spider and Intel Cavallino switches <ref> [21, 22] </ref>. Virtual channels (VC's)[11] are used in wormhole networks to avoid deadlocks and to improve link utilization and network throughput. Whenever there is a blocking of a message, another message can be routed through other VCs. <p> Multiple input channels can request for the same output channel in this phase. This conflict is resolved in the second step. A similar arbitration policy is used in the Intel Cavallino switch <ref> [22] </ref>. 3.2 Cache Coherence And Synchronization We implemented the full-map directory-based cache coherence protocol [17] with some modifications for evaluation in this paper. In this scheme, each shared memory block is assigned to a node, called home node, which maintains the directory entries for that block.
Reference: [23] <author> J. P. Singh, W.-D. Weber, and A. Gupta, </author> <title> "SPLASH: Stanford Parallel Applications for Shared-Memory," </title> <journal> ACM SIGARCH Computer Architecture News, </journal> <volume> vol. 20, no. 1, </volume> <pages> pp. 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The 9 overhead due to contention on synchronization variables is significant for some applications, such as MP3D <ref> [23] </ref>. The network latency also plays an important role on the synchronization overhead. In this paper, we do not discuss the synchronization overhead associated with any of the simulations. <p> In rest of the log 2 P stages, every data point is shared by two processors. During these stages, instead of using two separate input and output array, we interleave these arrays to avoid large number of conflict misses. MP3D <ref> [23] </ref> is a three-dimensional particle simulator used in rarefied fluid flow simulation. We used 25000 molecules with the default geometry provided with SPLASH [23] which uses a 14 fi 24 fi 4 (2646-cell) space containing a single flat sheet placed at an angle to the free stream. <p> During these stages, instead of using two separate input and output array, we interleave these arrays to avoid large number of conflict misses. MP3D <ref> [23] </ref> is a three-dimensional particle simulator used in rarefied fluid flow simulation. We used 25000 molecules with the default geometry provided with SPLASH [23] which uses a 14 fi 24 fi 4 (2646-cell) space containing a single flat sheet placed at an angle to the free stream.
Reference: [24] <author> S.Chodnekar, et al, </author> <title> "Toward a Communication Characterization Methodology for Parallel Applications," </title> <booktitle> The Third International Symposium on High-Performance Computer Architecture, </booktitle> <address> San Antonio, Texas, </address> <pages> pp. 310-319, </pages> <month> February </month> <year> 1997. </year> <month> 37 </month>
Reference-contexts: While observing the memory access patterns, we find that FFT, MP3D and LU have temporal access patterns that fit CDFs of known distributions like exponential or hyper-exponential distribution. Similar observations were made in a recent paper for a particular memory management policy <ref> [24] </ref>. However FWA and MATMUL applications exhibit jumps in the inter-arrival times. For matrix multiplication, and a probability of close to 60% of a think time higher than 375 processor cycles.
References-found: 24

