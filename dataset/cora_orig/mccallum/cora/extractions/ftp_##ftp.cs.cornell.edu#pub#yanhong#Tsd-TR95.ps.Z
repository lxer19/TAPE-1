URL: ftp://ftp.cs.cornell.edu/pub/yanhong/Tsd-TR95.ps.Z
Refering-URL: http://www.cs.cornell.edu/faculty/home/tt/vita/vita.html
Root-URL: 
Title: Incremental Computation for Transformational Software Development  
Author: Yanhong A. Liu Tim Teitelbaum 
Date: March 1995  
Abstract: Given a program f and an input change , we wish to obtain an incremental program that computes f(x y) efficiently by making use of the value of f(x), the intermediate results computed in computing f(x), and auxiliary information about x that can be inexpensively maintained. Obtaining such incremental programs is an essential part of the transformational-programming approach to software development and enhancement. This paper presents a systematic approach that discovers a general class of useful auxiliary information, combines it with useful intermediate results, and obtains an efficient incremental program that uses and maintains these intermediate results and auxiliary information. We give a number of examples from list processing, VLSI circuit design, image processing, etc.
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> IEEE Standard Glossary of software engineering terminology. IEEE Standard 729. </institution> <year> 1983. </year>
Reference-contexts: 1 Introduction Software engineering is the systematic approach to the development, operation, maintenance, and retirement of software <ref> [1] </ref>. The transformational-programming approach to software engineering advocates the use of formal source-to-source transformations to reduce programming labor, improve program reliability, and upgrade program performance [35, 37]. During development, semantics-preserving transformations refine correct high-level specifications and inefficient programs into executable code and more efficient programs.
Reference: [2] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. Addison-Wesley series in Computer Science. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: This problem has received much attention in the transformational programming literature [7, 12, 35, 37, 45], where it is commonly known as finite differencing. However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers <ref> [2, 3, 11, 14, 46] </ref>, interactive systems like editors [6, 13, 43] and programming environments [5, 9, 13, 19, 24, 25, 41, 42], dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation.
Reference: [3] <author> F. E. Allen, J. Cocke, and K. Kennedy. </author> <title> Reduction of operator strength. </title> <editor> In S. Muchnick and N. Jones, editors, </editor> <booktitle> Program Flow Analysis, </booktitle> <pages> pages 79-101. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: This problem has received much attention in the transformational programming literature [7, 12, 35, 37, 45], where it is commonly known as finite differencing. However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers <ref> [2, 3, 11, 14, 46] </ref>, interactive systems like editors [6, 13, 43] and programming environments [5, 9, 13, 19, 24, 25, 41, 42], dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation. <p> Some methods in incremental computation exploit certain kinds of auxiliary information, e.g., auxiliary arithmetic associated with some classical strength-reduction rules <ref> [3] </ref>, dynamic mappings maintained by finite differencing rules for aggregate primitives in SETL [36] and INC [52], and auxiliary data structures for problems with certain properties like stable decomposition [40] and other decomposition properties [15]. <p> For static incremental attribute evaluation algorithms [26, 27], where no auxiliary information is needed, our approach can cache intermediate results and maintain them automatically [30]. Strength reduction <ref> [3, 11, 46] </ref> is a traditional compiler optimization technique that aims at computing each iteration incrementally based on the result of the previous iteration. Basically, a fixed set of strength-reduction rules for primitive operators like multiplication and addition are used on induction variables and region constants.
Reference: [4] <author> B. Alpern, R. Hoover, B. Rosen, P. Sweeney, and K. Zadeck. </author> <title> Incremental evaluation of computational circuits. </title> <booktitle> In Proceedings of the First Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 32-42, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1990. </year>
Reference: [5] <author> R. Bahlke and G. Snelting. </author> <title> The PSG system: From formal language definitions to interactive programming environments. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 547-576, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors [6, 13, 43] and programming environments <ref> [5, 9, 13, 19, 24, 25, 41, 42] </ref>, dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation. <p> Related work on incremental computation that exploits intermediate results and auxiliary information is summarized in Section 1. Here, we take a closer look at related work in discovering auxiliary information for incremental computation. 12 Interactive systems and reactive systems often use various incremental algorithms to achieve fast response time <ref> [5, 6, 9, 13, 19, 24, 41, 42] </ref>. Explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover. Our approach provides a general and systematic approach for developing particular incremental algorithms.
Reference: [6] <author> R. A. Ballance, S. L. Graham, and M. L. Van De Vanter. </author> <title> The Pan language-based editing system. </title> <journal> ACM Transactions on Software Engineering and Methodology, </journal> <volume> 1(1) </volume> <pages> 95-127, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors <ref> [6, 13, 43] </ref> and programming environments [5, 9, 13, 19, 24, 25, 41, 42], dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation. <p> Related work on incremental computation that exploits intermediate results and auxiliary information is summarized in Section 1. Here, we take a closer look at related work in discovering auxiliary information for incremental computation. 12 Interactive systems and reactive systems often use various incremental algorithms to achieve fast response time <ref> [5, 6, 9, 13, 19, 24, 41, 42] </ref>. Explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover. Our approach provides a general and systematic approach for developing particular incremental algorithms.
Reference: [7] <author> R. S. Bird. </author> <title> The promotion and accumulation strategies in transformational programming. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(4) </volume> <pages> 487-504, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: During maintenance, performance bottlenecks are eliminated using similar techniques. One of the most important techniques used to improve program performance involves incrementally updating results of computations as their parameters change rather than computing the results from scratch. This problem has received much attention in the transformational programming literature <ref> [7, 12, 35, 37, 45] </ref>, where it is commonly known as finite differencing. <p> Thus the whole program takes only O (n 2 ) time. 11 6.3 Path Sequence Problem This example is taken from <ref> [7] </ref>. Given a directed acyclic graph, and a string whose elements are vertices in the graph, the problem is to compute the length of the longest subsequence in the string that forms a path in the graph. <p> We focus on the second half where an exponential-time recursive solution is improved (incorrectly in <ref> [7] </ref> but corrected in [8]). A program llp is defined to compute the desired length. <p> Also, in general, Paige's rules apply only to very high-level languages like set expressions; our method applies also to standard languages like Lisp. The promotion and accumulation strategies are proposed by Bird <ref> [7, 8] </ref> as general methods for achieving efficient transformed programs. Promotion attempts to derive a program that defines f (cons (a; x)) in terms of f (x), and accumulation generalizes a definition by including an extra argument.
Reference: [8] <author> R. S. Bird. </author> <title> Addendum: The promotion and accumulation strategies in transformational programming. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 490-492, </pages> <month> July </month> <year> 1985. </year> <month> 13 </month>
Reference-contexts: We focus on the second half where an exponential-time recursive solution is improved (incorrectly in [7] but corrected in <ref> [8] </ref>). A program llp is defined to compute the desired length. <p> Also, in general, Paige's rules apply only to very high-level languages like set expressions; our method applies also to standard languages like Lisp. The promotion and accumulation strategies are proposed by Bird <ref> [7, 8] </ref> as general methods for achieving efficient transformed programs. Promotion attempts to derive a program that defines f (cons (a; x)) in terms of f (x), and accumulation generalizes a definition by including an extra argument. <p> Thus, promotion can be formulated as deriving incremental programs, and accumulation as identifying appropriate intermediate results or auxiliary information. However, we can discern no systematic steps being followed in Bird's derivation; such derivations are error-prone <ref> [8] </ref>. As demonstrated with the path sequence problem, our approach can be regarded as a systematic formulation of the promotion and accumulation strategies.
Reference: [9] <author> P. Borras and D. Clement. </author> <title> CENTAUR: The system. </title> <booktitle> In Proceedings of the ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, </booktitle> <pages> pages 14-24, </pages> <address> Boston, Massachusetts, </address> <month> November </month> <year> 1988. </year> <note> Published as SIGPLAN Notices, 24(2). </note>
Reference-contexts: However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors [6, 13, 43] and programming environments <ref> [5, 9, 13, 19, 24, 25, 41, 42] </ref>, dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation. <p> Related work on incremental computation that exploits intermediate results and auxiliary information is summarized in Section 1. Here, we take a closer look at related work in discovering auxiliary information for incremental computation. 12 Interactive systems and reactive systems often use various incremental algorithms to achieve fast response time <ref> [5, 6, 9, 13, 19, 24, 41, 42] </ref>. Explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover. Our approach provides a general and systematic approach for developing particular incremental algorithms.
Reference: [10] <author> S. Ceri, M. A. W. Houtsma, A. M. Keller, and P. Samarati. </author> <title> Achieving incremental consistency among autonomous replicated databases. </title> <journal> IFIP Transactions A [Computer Science and Technology], </journal> <volume> A-25:223-237, </volume> <year> 1993. </year>
Reference-contexts: However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors [6, 13, 43] and programming environments [5, 9, 13, 19, 24, 25, 41, 42], dynamic systems like distributed databases <ref> [10, 29] </ref> and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation.
Reference: [11] <author> J. Cocke and K. Kennedy. </author> <title> An algorithm for reduction of operator strength. </title> <journal> Communications of the ACM, </journal> 20(11) 850-856, November 1977. 
Reference-contexts: This problem has received much attention in the transformational programming literature [7, 12, 35, 37, 45], where it is commonly known as finite differencing. However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers <ref> [2, 3, 11, 14, 46] </ref>, interactive systems like editors [6, 13, 43] and programming environments [5, 9, 13, 19, 24, 25, 41, 42], dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation. <p> For static incremental attribute evaluation algorithms [26, 27], where no auxiliary information is needed, our approach can cache intermediate results and maintain them automatically [30]. Strength reduction <ref> [3, 11, 46] </ref> is a traditional compiler optimization technique that aims at computing each iteration incrementally based on the result of the previous iteration. Basically, a fixed set of strength-reduction rules for primitive operators like multiplication and addition are used on induction variables and region constants.
Reference: [12] <author> N. Dershowitz. </author> <title> The evolution of programs, </title> <booktitle> volume 5 of Progress in Computer Science. </booktitle> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1983. </year>
Reference-contexts: During maintenance, performance bottlenecks are eliminated using similar techniques. One of the most important techniques used to improve program performance involves incrementally updating results of computations as their parameters change rather than computing the results from scratch. This problem has received much attention in the transformational programming literature <ref> [7, 12, 35, 37, 45] </ref>, where it is commonly known as finite differencing. <p> However, we can discern no systematic steps being followed in Bird's derivation; such derivations are error-prone [8]. As demonstrated with the path sequence problem, our approach can be regarded as a systematic formulation of the promotion and accumulation strategies. Other work in transformational program development, including the extension technique <ref> [12] </ref>, the differencing strategy in CIP [37], and the finite differencing of functional programs in KIDS [45], can be further automated with our systematic approach. These techniques are indispensable for developing efficient programs.
Reference: [13] <author> V. Donzeau-Gouge, G. Huet, G. Kahn, and B. Lang. </author> <title> Programming environments based on structure editor: the Mentor experience. </title> <editor> In D. R. Barstow, H. E. Shrobe, and E. Sandewall, editors, </editor> <booktitle> Interactive Programming Environments, </booktitle> <pages> pages 128-140. </pages> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors <ref> [6, 13, 43] </ref> and programming environments [5, 9, 13, 19, 24, 25, 41, 42], dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation. <p> However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors [6, 13, 43] and programming environments <ref> [5, 9, 13, 19, 24, 25, 41, 42] </ref>, dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation. <p> Related work on incremental computation that exploits intermediate results and auxiliary information is summarized in Section 1. Here, we take a closer look at related work in discovering auxiliary information for incremental computation. 12 Interactive systems and reactive systems often use various incremental algorithms to achieve fast response time <ref> [5, 6, 9, 13, 19, 24, 41, 42] </ref>. Explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover. Our approach provides a general and systematic approach for developing particular incremental algorithms.
Reference: [14] <author> J. Earley. </author> <title> High level iterators and a method for automatically designing data structure representation. </title> <journal> Journal of Computer Languages, </journal> <volume> 1 </volume> <pages> 321-342, </pages> <year> 1976. </year>
Reference-contexts: This problem has received much attention in the transformational programming literature [7, 12, 35, 37, 45], where it is commonly known as finite differencing. However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers <ref> [2, 3, 11, 14, 46] </ref>, interactive systems like editors [6, 13, 43] and programming environments [5, 9, 13, 19, 24, 25, 41, 42], dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation.
Reference: [15] <author> D. Eppstein, Z. Galil, G. F. Italiano, and A. Nissenzweig. </author> <title> Sparsification a technique for speeding up dynamic graph algorithms. </title> <booktitle> In Proceedings of the 33rd Annual IEEE Symposium on FOCS, </booktitle> <address> Pittsburgh, Pennsylvania, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: computation exploit certain kinds of auxiliary information, e.g., auxiliary arithmetic associated with some classical strength-reduction rules [3], dynamic mappings maintained by finite differencing rules for aggregate primitives in SETL [36] and INC [52], and auxiliary data structures for problems with certain properties like stable decomposition [40] and other decomposition properties <ref> [15] </ref>. However, until now, the systematic discovery of auxiliary information for general problems has been a subject left completely open for study. Methods for dealing with incremental programs, intermediate results, and auxiliary information together have also been needed. This Paper.
Reference: [16] <author> J. Field and T. Teitelbaum. </author> <title> Incremental reduction in the lambda calculus. </title> <booktitle> In Proceedings of the ACM '90 Conference on LFP, </booktitle> <pages> pages 307-322, </pages> <year> 1990. </year>
Reference-contexts: Author's address: Department of Computer Science, Cornell University, Ithaca, NY 14853. Email: yanhong,tt@cs.cornell.edu 1 Most methods in incremental computation exploit caching intermediate results of certain kinds, e.g., annotating parse trees with attributes [43], dynamically caching results of function calls [39, 40], saving results of intermediate reductions at appointed places <ref> [16] </ref>, maintaining the change-detailing network of INC [52], keeping residual programs for certain input partitions [47], and maintaining intermediate results and dependencies identified by Alphonse annotations [20]. In [30], we have given a cache-and-prune method to statically transform programs to cache all kinds of intermediate results needed for incremental computation.
Reference: [17] <author> I. Flores. </author> <booktitle> The logic of computer arithmetic. Prentice-Hall international series in electrical engineering. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1963. </year>
Reference-contexts: This is of particular interest in the context of the recent Pentium chip flaw [18]. The initial specification of the l-bit binary integer square root algorithm uses the non-restoring method <ref> [17, 32] </ref>, which is exact for perfect squares and off by at most 1 for other integers: m := 2 l1 for i := l 2 downto 0 do p := n m 2 ; if p &gt; 0 then m := m + 2 i else if p &lt; 0
Reference: [18] <author> J. Glanz. </author> <title> Mathematical logic flushes out the bugs in chip designs. </title> <journal> Science, </journal> <volume> 267 </volume> <pages> 332-333, </pages> <month> January 20 </month> <year> 1995. </year>
Reference-contexts: We show how our method can automate the derivation of the finite differencing transformations that are manually discovered and verified in [32]. This is of particular interest in the context of the recent Pentium chip flaw <ref> [18] </ref>.
Reference: [19] <author> A. N. Habermann and D. Notkin. </author> <title> Gandalf: Software development environments. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-12(12):1117-1127, </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors [6, 13, 43] and programming environments <ref> [5, 9, 13, 19, 24, 25, 41, 42] </ref>, dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation. <p> Related work on incremental computation that exploits intermediate results and auxiliary information is summarized in Section 1. Here, we take a closer look at related work in discovering auxiliary information for incremental computation. 12 Interactive systems and reactive systems often use various incremental algorithms to achieve fast response time <ref> [5, 6, 9, 13, 19, 24, 41, 42] </ref>. Explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover. Our approach provides a general and systematic approach for developing particular incremental algorithms.
Reference: [20] <author> R. Hoover. Alphonse: </author> <title> Incremental computation as a programming abstraction. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on PLDI, </booktitle> <pages> pages 261-272, </pages> <address> California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: kinds, e.g., annotating parse trees with attributes [43], dynamically caching results of function calls [39, 40], saving results of intermediate reductions at appointed places [16], maintaining the change-detailing network of INC [52], keeping residual programs for certain input partitions [47], and maintaining intermediate results and dependencies identified by Alphonse annotations <ref> [20] </ref>. In [30], we have given a cache-and-prune method to statically transform programs to cache all kinds of intermediate results needed for incremental computation.
Reference: [21] <author> F. Jalili and J. H. Gallier. </author> <title> Building friendly parsers. </title> <booktitle> In Conference Record of the 9th Annual ACM Symposium on POPL, </booktitle> <pages> pages 196-206, </pages> <address> Albuquerque, New Mexico, </address> <month> January </month> <year> 1982. </year>
Reference: [22] <author> N. D. Jones, C. K. Gomard, and P. Sestoft. </author> <title> Partial Evaluation and Automatic Program Generation. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1993. </year>
Reference-contexts: Then it optimizes f together with 0 . The goal is to merge the computation of auxiliary information f naturally into the computation of intermediate results f , as opposed to two disjoint parallel computations. This involves transformations used in partial evaluation <ref> [22, 33] </ref>, including introducing functions to compute function applications, unfolding, simplifying primitive function applications, driving, replacing recursive applications with introduced functions, etc.
Reference: [23] <author> N. D. Jones, P. Sestoft, and H. Stndergaard. </author> <title> An experiment in partial evaluation: The generation of a compiler generator. </title> <editor> In J.-P. Jouannaud, editor, </editor> <booktitle> Rewriting Techniques and Applications, </booktitle> <pages> pages 124-140, </pages> <address> Dijon, France, </address> <month> May </month> <year> 1985. </year> <note> Springer-Verlag. LNCS 202. </note>
Reference-contexts: When this is true, Col acts like Ext; otherwise, Col directly collects the intermediate results without having to build the value of the original computation into the final return value. Our dependence analysis is similar to binding-time analysis for partial evaluation <ref> [23, 28] </ref> if we regard arguments x and r of f 00 (x; y; r) as static and y as dynamic.
Reference: [24] <author> G. E. Kaiser. </author> <title> Incremental dynamic semantics for language-based programming environments. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(2) </volume> <pages> 168-193, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors [6, 13, 43] and programming environments <ref> [5, 9, 13, 19, 24, 25, 41, 42] </ref>, dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation. <p> Related work on incremental computation that exploits intermediate results and auxiliary information is summarized in Section 1. Here, we take a closer look at related work in discovering auxiliary information for incremental computation. 12 Interactive systems and reactive systems often use various incremental algorithms to achieve fast response time <ref> [5, 6, 9, 13, 19, 24, 41, 42] </ref>. Explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover. Our approach provides a general and systematic approach for developing particular incremental algorithms.
Reference: [25] <author> S. M. Kaplan and G. E. Kaiser. </author> <title> Incremental attribute evaluation in distributed language-based environments. </title> <booktitle> In Conference Record of the ACM Symposium on PODC, </booktitle> <address> Calgary, Canada, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors [6, 13, 43] and programming environments <ref> [5, 9, 13, 19, 24, 25, 41, 42] </ref>, dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation.
Reference: [26] <author> U. Kastens. </author> <title> Ordered attributed grammars. </title> <journal> Acta Informatica, </journal> <volume> 13(3) </volume> <pages> 229-256, </pages> <year> 1980. </year>
Reference-contexts: Our approach provides a general and systematic approach for developing particular incremental algorithms. For example, for the dynamic incremental attribute evaluation algorithm in [43], the characteristic graph is auxiliary information that would be discovered following our principle. For static incremental attribute evaluation algorithms <ref> [26, 27] </ref>, where no auxiliary information is needed, our approach can cache intermediate results and maintain them automatically [30]. Strength reduction [3, 11, 46] is a traditional compiler optimization technique that aims at computing each iteration incrementally based on the result of the previous iteration.
Reference: [27] <author> T. Katayama. </author> <title> Translation of attribute grammars into procedures. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(3) </volume> <pages> 345-369, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: Our approach provides a general and systematic approach for developing particular incremental algorithms. For example, for the dynamic incremental attribute evaluation algorithm in [43], the characteristic graph is auxiliary information that would be discovered following our principle. For static incremental attribute evaluation algorithms <ref> [26, 27] </ref>, where no auxiliary information is needed, our approach can cache intermediate results and maintain them automatically [30]. Strength reduction [3, 11, 46] is a traditional compiler optimization technique that aims at computing each iteration incrementally based on the result of the previous iteration.
Reference: [28] <author> J. Launchbury. </author> <title> Projections for specialisation. </title> <booktitle> In Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 299-315. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year>
Reference-contexts: When this is true, Col acts like Ext; otherwise, Col directly collects the intermediate results without having to build the value of the original computation into the final return value. Our dependence analysis is similar to binding-time analysis for partial evaluation <ref> [23, 28] </ref> if we regard arguments x and r of f 00 (x; y; r) as static and y as dynamic.
Reference: [29] <author> E. Levy and A. Silberschatz. </author> <title> Incremental recovery in main memory database systems. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 4(6) </volume> <pages> 529-540, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors [6, 13, 43] and programming environments [5, 9, 13, 19, 24, 25, 41, 42], dynamic systems like distributed databases <ref> [10, 29] </ref> and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation.
Reference: [30] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Caching intermediate results for program improvement. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on on PEPM, </booktitle> <address> La Jolla, California, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: In <ref> [30] </ref>, we have given a cache-and-prune method to statically transform programs to cache all kinds of intermediate results needed for incremental computation. <p> Then, we merge such information with intermediate results of f (x) and decide which of them are needed for efficient incremental computation and how they can be used (in a fashion similar to the cache-and-prune method in <ref> [30] </ref>). As a result, we derive a program ~ f 0 that uses the needed intermediate results and auxiliary information to compute f incrementally, while incrementally maintaining these results and information at the same time. Our approach is modular: each component performs relatively independent analyses and transformations. <p> In particular, those that depend on y are not useful information about x. A forward dependence analysis and a collection transformation are needed to identify and collect subcomputations that depend only on certain arguments. * Although caching intermediate results wouldn't increase the asymptotic computation time <ref> [30] </ref>, we must consider the cost of maintaining the collected auxiliary information. Let ~ f 0 be the resulting incremental program that computes the value of f and maintains the needed intermediate results and auxiliary information. <p> Sometimes, this can be guaranteed by simply checking that computing auxiliary information from scratch is at least as fast as computing f from scratch. Three-Phase Transformation. The overall approach consists of three phases, corresponding to the three-stage method for caching intermediate results in <ref> [30] </ref>. However, Phase I is extended here to discover and compute candidate auxiliary information as well. * Phase I constructs a program f that caches all intermediate results and candidate auxiliary information for computing f incrementally under . <p> It has four steps: Step 1 constructs program f , an extended version of f , such that f (x) returns the values of all intermediate results used in computing f (x), as in <ref> [30] </ref>. Step 2 derives program f 00 , an incrementalized version of f on x y, using a modified method of [31] to expose subcomputations that do not depend on y. <p> This phase uses methods in <ref> [30] </ref>. 3 Running Example We detail our method with the aid of a running example, cmp, given in Figure 1 as a set of first-order recursive equations with call-by-value semantics. <p> Caching All Intermediate Results Step 1 extends the program f to cache all intermediate results using the local, structure-preserving extension transformation Ext in <ref> [30] </ref>. Basically, Ext extends a computation to return the values of all nontrivial intermediate computations, i.e., it introduces names for the values of intermediate computations, and builds up data structures for these values together with the values of the original computation. 4 Example. <p> Basically, a backward dependence analysis is used to identify subcomputations of f 0 whose values are used in computing the value of f , a pruning transformation is used to replace unneeded computations with , and finally, the resulting programs are optimized by eliminating the components <ref> [30] </ref>. Example. <p> For example, for the dynamic incremental attribute evaluation algorithm in [43], the characteristic graph is auxiliary information that would be discovered following our principle. For static incremental attribute evaluation algorithms [26, 27], where no auxiliary information is needed, our approach can cache intermediate results and maintain them automatically <ref> [30] </ref>. Strength reduction [3, 11, 46] is a traditional compiler optimization technique that aims at computing each iteration incrementally based on the result of the previous iteration. Basically, a fixed set of strength-reduction rules for primitive operators like multiplication and addition are used on induction variables and region constants.
Reference: [31] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Systematic derivation of incremental programs. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 24(1) </volume> <pages> 1-39, </pages> <year> 1995. </year>
Reference-contexts: Moreover, certain auxiliary information about x may need to be discovered and maintained as well. Numerous techniques for incremental computation have been developed, e.g., [3, 4, 16, 20, 21, 24, 31, 36, 38, 40, 43, 44, 47, 52]. In <ref> [31] </ref>, we give a systematic transformational approach for deriving an incremental program f 0 from a given program f and an input change . <p> This paper presents a systematic approach that discovers a general class of auxiliary information for any incremental computation problem. More importantly, it combines discovering auxiliary information with obtaining incremental programs and caching intermediate results. Basically, we obtain candidate auxiliary information by transforming f (x y) (using a revision of <ref> [31] </ref>) and collecting all subcomputations that depend on the previous input x. Then, we merge such information with intermediate results of f (x) and decide which of them are needed for efficient incremental computation and how they can be used (in a fashion similar to the cache-and-prune method in [30]). <p> collecting candidate auxiliary information from the incremental computation of f on x y, we collect it in the incremental computation of f on x y, where f is f extended to cache all intermediate results. * In transforming the computation of f (x y), we modify the incrementalization method of <ref> [31] </ref> to expose more subcomputations that do not depend on y, especially subcomputations whose values can not be retrieved from the cached result of f (x). * Not all computations performed in the transformed computation of f (x y) are candidate auxiliary information. <p> Step 2 derives program f 00 , an incrementalized version of f on x y, using a modified method of <ref> [31] </ref> to expose subcomputations that do not depend on y. <p> and f to return the value of f , the intermediate results, and the candidate auxiliary information, and 0 projects the value of f out of the value of f . * Phase II derives program f 0 , an incremental version of f under , using the approach in <ref> [31] </ref>. * Phase III generates program ~ f, a pruned version of f , such that ~ f (x) returns ( r ), where r is the re turn value of f (x), and ( r ) projects out 0 ( r ) and other components of r on which 0 <p> Note that some of the parameters of f 0 0 may be dead and eliminated <ref> [31] </ref>. 3 cmp (x) = sum (odd (x)) prod (even (x)) | compare sum of odd and product of even positions o d d (x) = if null (x) then nil else cons (car (x); even (cdr (x))) even (x) = if null (x) then nil else odd (cdr (x)) sum <p> Exposing Auxiliary Information by Incrementalization Step 2 transforms f (x y) to expose subcomputations depending on x but not y using transformations similar to the incrementalization in <ref> [31] </ref> for obtaining the incremental version f 0 (x; y; r). However, our present motivation differs in that we are not interested here in whether the values of subcomputations can be efficiently retrieved from the cached value of f . <p> Such a derivation method is given in <ref> [31] </ref>, and, depending on the power one expects from the derivation, the method can be made semi-automatic or fully-automatic. Example.
Reference: [32] <author> J. O'Leary, M. Leeser, J. Hickey, and M. Aagaard. </author> <title> Non-restoring integer square root: A case study in design by principled optimization. </title> <booktitle> In Proceedings of the 2nd International Conference on Theorem Provers in Circuit Design | Theory, Practice and Experience, </booktitle> <address> Bad Herrenalb (Black Forest), Germany, </address> <month> September </month> <year> 1994. </year> <note> Springer-Verlag. LNCS. 14 </note>
Reference-contexts: Further study is needed in these areas. Implementation. A prototype system, CACHET, has been implemented for most of the transformations used in our approach. The implementation will be described elsewhere. 6 Examples 6.1 Binary Integer Square Root In <ref> [32] </ref>, a specification of binary integer square root algorithm is transformed into a VLSI circuit. We show how our method can automate the derivation of the finite differencing transformations that are manually discovered and verified in [32]. <p> The implementation will be described elsewhere. 6 Examples 6.1 Binary Integer Square Root In <ref> [32] </ref>, a specification of binary integer square root algorithm is transformed into a VLSI circuit. We show how our method can automate the derivation of the finite differencing transformations that are manually discovered and verified in [32]. This is of particular interest in the context of the recent Pentium chip flaw [18]. <p> This is of particular interest in the context of the recent Pentium chip flaw [18]. The initial specification of the l-bit binary integer square root algorithm uses the non-restoring method <ref> [17, 32] </ref>, which is exact for perfect squares and off by at most 1 for other integers: m := 2 l1 for i := l 2 downto 0 do p := n m 2 ; if p &gt; 0 then m := m + 2 i else if p &lt; 0 <p> Following our systematic approach, we have even eliminated an extra shift done in <ref> [32] </ref>. 6.2 Local Neighborhood Problems In image processing, computing information about local neighborhoods is common [49, 51, 53, 54]. A simple but typical example is the local summation problem [51, 53]: given an n-by-n image, compute, for each pixel, the local sum of its m-by-m neighborhood.
Reference: [33] <author> F. G. Pagan. </author> <title> Partial computation and the construction of language processors. </title> <publisher> Prentice Hall Software Series. Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1991. </year>
Reference-contexts: Then it optimizes f together with 0 . The goal is to merge the computation of auxiliary information f naturally into the computation of intermediate results f , as opposed to two disjoint parallel computations. This involves transformations used in partial evaluation <ref> [22, 33] </ref>, including introducing functions to compute function applications, unfolding, simplifying primitive function applications, driving, replacing recursive applications with introduced functions, etc.
Reference: [34] <author> B. Paige and J. T. Schwartz. </author> <title> Expression continuity and the formal differentiation of algorithms. </title> <booktitle> In Conference Record of the 4th Annual ACM Symposium on POPL, </booktitle> <pages> pages 58-71, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: Our method can be viewed as a principled strength reduction technique not limited to a fixed set of pre-known rules, even allowing such rules to be derived and justified when necessary, as shown in the integer square root example. Finite differencing, proposed by Paige <ref> [34, 35, 36] </ref>, can be regarded as a generalization of strength reduction to set-theoretic expressions for systematic program development. Basically, a set of rules are manually developed for differentiating set expressions. For continuous expressions, our method can derive such rules directly using properties of primitive set operations.
Reference: [35] <author> R. Paige. </author> <title> Transformational programming applications to algorithms and systems. </title> <booktitle> In Conference Record of the 10th Annual ACM Symposium on POPL, </booktitle> <pages> pages 73-87, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: 1 Introduction Software engineering is the systematic approach to the development, operation, maintenance, and retirement of software [1]. The transformational-programming approach to software engineering advocates the use of formal source-to-source transformations to reduce programming labor, improve program reliability, and upgrade program performance <ref> [35, 37] </ref>. During development, semantics-preserving transformations refine correct high-level specifications and inefficient programs into executable code and more efficient programs. During maintenance, performance bottlenecks are eliminated using similar techniques. <p> During maintenance, performance bottlenecks are eliminated using similar techniques. One of the most important techniques used to improve program performance involves incrementally updating results of computations as their parameters change rather than computing the results from scratch. This problem has received much attention in the transformational programming literature <ref> [7, 12, 35, 37, 45] </ref>, where it is commonly known as finite differencing. <p> Our method can be viewed as a principled strength reduction technique not limited to a fixed set of pre-known rules, even allowing such rules to be derived and justified when necessary, as shown in the integer square root example. Finite differencing, proposed by Paige <ref> [34, 35, 36] </ref>, can be regarded as a generalization of strength reduction to set-theoretic expressions for systematic program development. Basically, a set of rules are manually developed for differentiating set expressions. For continuous expressions, our method can derive such rules directly using properties of primitive set operations.
Reference: [36] <author> R. Paige and S. Koenig. </author> <title> Finite differencing of computable expressions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(3) </volume> <pages> 402-454, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: Some methods in incremental computation exploit certain kinds of auxiliary information, e.g., auxiliary arithmetic associated with some classical strength-reduction rules [3], dynamic mappings maintained by finite differencing rules for aggregate primitives in SETL <ref> [36] </ref> and INC [52], and auxiliary data structures for problems with certain properties like stable decomposition [40] and other decomposition properties [15]. However, until now, the systematic discovery of auxiliary information for general problems has been a subject left completely open for study. <p> Our method can be viewed as a principled strength reduction technique not limited to a fixed set of pre-known rules, even allowing such rules to be derived and justified when necessary, as shown in the integer square root example. Finite differencing, proposed by Paige <ref> [34, 35, 36] </ref>, can be regarded as a generalization of strength reduction to set-theoretic expressions for systematic program development. Basically, a set of rules are manually developed for differentiating set expressions. For continuous expressions, our method can derive such rules directly using properties of primitive set operations.
Reference: [37] <author> H. A. Partsch. </author> <title> Specification and Transformation of Programs A Formal Approach to Software Development. Texts and Monographs in Computer Science. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Software engineering is the systematic approach to the development, operation, maintenance, and retirement of software [1]. The transformational-programming approach to software engineering advocates the use of formal source-to-source transformations to reduce programming labor, improve program reliability, and upgrade program performance <ref> [35, 37] </ref>. During development, semantics-preserving transformations refine correct high-level specifications and inefficient programs into executable code and more efficient programs. During maintenance, performance bottlenecks are eliminated using similar techniques. <p> During maintenance, performance bottlenecks are eliminated using similar techniques. One of the most important techniques used to improve program performance involves incrementally updating results of computations as their parameters change rather than computing the results from scratch. This problem has received much attention in the transformational programming literature <ref> [7, 12, 35, 37, 45] </ref>, where it is commonly known as finite differencing. <p> As demonstrated with the path sequence problem, our approach can be regarded as a systematic formulation of the promotion and accumulation strategies. Other work in transformational program development, including the extension technique [12], the differencing strategy in CIP <ref> [37] </ref>, and the finite differencing of functional programs in KIDS [45], can be further automated with our systematic approach. These techniques are indispensable for developing efficient programs.
Reference: [38] <author> L. L. Pollock and M. L. Soffa. </author> <title> Incremental global reoptimization of programs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 14(2) </volume> <pages> 173-200, </pages> <month> April </month> <year> 1992. </year>
Reference: [39] <author> W. Pugh. </author> <title> An improved cache replacement strategy for function caching. </title> <booktitle> In Proceedings of the ACM '88 Conference on LFP, </booktitle> <pages> pages 269-276, </pages> <year> 1988. </year>
Reference-contexts: N00014-92-J-1973. Author's address: Department of Computer Science, Cornell University, Ithaca, NY 14853. Email: yanhong,tt@cs.cornell.edu 1 Most methods in incremental computation exploit caching intermediate results of certain kinds, e.g., annotating parse trees with attributes [43], dynamically caching results of function calls <ref> [39, 40] </ref>, saving results of intermediate reductions at appointed places [16], maintaining the change-detailing network of INC [52], keeping residual programs for certain input partitions [47], and maintaining intermediate results and dependencies identified by Alphonse annotations [20].
Reference: [40] <author> W. Pugh and T. Teitelbaum. </author> <title> Incremental computation via function caching. </title> <booktitle> In Conference Record of the 16th Annual ACM Symposium on POPL, </booktitle> <pages> pages 315-328, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: N00014-92-J-1973. Author's address: Department of Computer Science, Cornell University, Ithaca, NY 14853. Email: yanhong,tt@cs.cornell.edu 1 Most methods in incremental computation exploit caching intermediate results of certain kinds, e.g., annotating parse trees with attributes [43], dynamically caching results of function calls <ref> [39, 40] </ref>, saving results of intermediate reductions at appointed places [16], maintaining the change-detailing network of INC [52], keeping residual programs for certain input partitions [47], and maintaining intermediate results and dependencies identified by Alphonse annotations [20]. <p> Some methods in incremental computation exploit certain kinds of auxiliary information, e.g., auxiliary arithmetic associated with some classical strength-reduction rules [3], dynamic mappings maintained by finite differencing rules for aggregate primitives in SETL [36] and INC [52], and auxiliary data structures for problems with certain properties like stable decomposition <ref> [40] </ref> and other decomposition properties [15]. However, until now, the systematic discovery of auxiliary information for general problems has been a subject left completely open for study. Methods for dealing with incremental programs, intermediate results, and auxiliary information together have also been needed. This Paper.
Reference: [41] <author> S. P. Reiss. </author> <title> Graphical program development with PECAN program development systems. </title> <booktitle> In Proceedings of the ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, </booktitle> <pages> pages 144-156, </pages> <address> Montreal, Canada, </address> <month> June </month> <year> 1984. </year>
Reference-contexts: However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors [6, 13, 43] and programming environments <ref> [5, 9, 13, 19, 24, 25, 41, 42] </ref>, dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation. <p> Related work on incremental computation that exploits intermediate results and auxiliary information is summarized in Section 1. Here, we take a closer look at related work in discovering auxiliary information for incremental computation. 12 Interactive systems and reactive systems often use various incremental algorithms to achieve fast response time <ref> [5, 6, 9, 13, 19, 24, 41, 42] </ref>. Explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover. Our approach provides a general and systematic approach for developing particular incremental algorithms.
Reference: [42] <author> T. Reps and T. Teitelbaum. </author> <title> The Synthesizer Generator: A System for Constructing Language-Based Editors. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors [6, 13, 43] and programming environments <ref> [5, 9, 13, 19, 24, 25, 41, 42] </ref>, dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation. <p> Related work on incremental computation that exploits intermediate results and auxiliary information is summarized in Section 1. Here, we take a closer look at related work in discovering auxiliary information for incremental computation. 12 Interactive systems and reactive systems often use various incremental algorithms to achieve fast response time <ref> [5, 6, 9, 13, 19, 24, 41, 42] </ref>. Explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover. Our approach provides a general and systematic approach for developing particular incremental algorithms.
Reference: [43] <author> T. Reps, T. Teitelbaum, and A. Demers. </author> <title> Incremental context-dependent analysis for language-based editors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 449-477, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors <ref> [6, 13, 43] </ref> and programming environments [5, 9, 13, 19, 24, 25, 41, 42], dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation. <p> N00014-92-J-1973. Author's address: Department of Computer Science, Cornell University, Ithaca, NY 14853. Email: yanhong,tt@cs.cornell.edu 1 Most methods in incremental computation exploit caching intermediate results of certain kinds, e.g., annotating parse trees with attributes <ref> [43] </ref>, dynamically caching results of function calls [39, 40], saving results of intermediate reductions at appointed places [16], maintaining the change-detailing network of INC [52], keeping residual programs for certain input partitions [47], and maintaining intermediate results and dependencies identified by Alphonse annotations [20]. <p> Explicit incremental algorithms are hard to write and appropriate auxiliary information is hard to discover. Our approach provides a general and systematic approach for developing particular incremental algorithms. For example, for the dynamic incremental attribute evaluation algorithm in <ref> [43] </ref>, the characteristic graph is auxiliary information that would be discovered following our principle. For static incremental attribute evaluation algorithms [26, 27], where no auxiliary information is needed, our approach can cache intermediate results and maintain them automatically [30].
Reference: [44] <author> B. G. Ryder and M. C. Paull. </author> <title> Incremental data flow analysis algorithms. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(1) </volume> <pages> 1-50, </pages> <month> January </month> <year> 1988. </year>
Reference: [45] <author> D. R. Smith. KIDS: </author> <title> A semiautomatic program development system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(9) </volume> <pages> 1024-1043, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: During maintenance, performance bottlenecks are eliminated using similar techniques. One of the most important techniques used to improve program performance involves incrementally updating results of computations as their parameters change rather than computing the results from scratch. This problem has received much attention in the transformational programming literature <ref> [7, 12, 35, 37, 45] </ref>, where it is commonly known as finite differencing. <p> As demonstrated with the path sequence problem, our approach can be regarded as a systematic formulation of the promotion and accumulation strategies. Other work in transformational program development, including the extension technique [12], the differencing strategy in CIP [37], and the finite differencing of functional programs in KIDS <ref> [45] </ref>, can be further automated with our systematic approach. These techniques are indispensable for developing efficient programs.
Reference: [46] <author> B. Steffen, J. Knoop, and O. Ruthing. </author> <title> Efficient code motion and an adaption to strength reduction. </title> <booktitle> In Proceedings of the 4th International Joint Conference on TAPSOFT, </booktitle> <pages> pages 394-415, </pages> <address> Brighton, UK, 1991. </address> <publisher> Springer-Verlag. LNCS 494. </publisher>
Reference-contexts: This problem has received much attention in the transformational programming literature [7, 12, 35, 37, 45], where it is commonly known as finite differencing. However, general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers <ref> [2, 3, 11, 14, 46] </ref>, interactive systems like editors [6, 13, 43] and programming environments [5, 9, 13, 19, 24, 25, 41, 42], dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing [49, 51, 53, 54]. Incremental Computation. <p> For static incremental attribute evaluation algorithms [26, 27], where no auxiliary information is needed, our approach can cache intermediate results and maintain them automatically [30]. Strength reduction <ref> [3, 11, 46] </ref> is a traditional compiler optimization technique that aims at computing each iteration incrementally based on the result of the previous iteration. Basically, a fixed set of strength-reduction rules for primitive operators like multiplication and addition are used on induction variables and region constants.
Reference: [47] <author> R. S. Sundaresh and P. Hudak. </author> <title> Incremental computation via partial evaluation. </title> <booktitle> In Conference Record of the 18th Annual ACM Symposium on POPL, </booktitle> <pages> pages 1-13, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Most methods in incremental computation exploit caching intermediate results of certain kinds, e.g., annotating parse trees with attributes [43], dynamically caching results of function calls [39, 40], saving results of intermediate reductions at appointed places [16], maintaining the change-detailing network of INC [52], keeping residual programs for certain input partitions <ref> [47] </ref>, and maintaining intermediate results and dependencies identified by Alphonse annotations [20]. In [30], we have given a cache-and-prune method to statically transform programs to cache all kinds of intermediate results needed for incremental computation.
Reference: [48] <author> A. Varma and S. Chalasani. </author> <title> An incremental algorithm for TDM switching assignments in satellite and terrestrial networks. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 10(2) </volume> <pages> 364-377, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: general techniques of incremental computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors [6, 13, 43] and programming environments [5, 9, 13, 19, 24, 25, 41, 42], dynamic systems like distributed databases [10, 29] and real-time systems <ref> [48] </ref>, and image processing [49, 51, 53, 54]. Incremental Computation.
Reference: [49] <author> J. Webb. </author> <title> Steps towards architecture-independent image processing. </title> <booktitle> IEEE Computer, </booktitle> <month> February </month> <year> 1992. </year>
Reference-contexts: computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors [6, 13, 43] and programming environments [5, 9, 13, 19, 24, 25, 41, 42], dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing <ref> [49, 51, 53, 54] </ref>. Incremental Computation. Given a program f and an input change operation , a program f 0 that computes the result of f (x y) efficiently by making use of the value of f (x) is called an incremental version of f under . <p> Following our systematic approach, we have even eliminated an extra shift done in [32]. 6.2 Local Neighborhood Problems In image processing, computing information about local neighborhoods is common <ref> [49, 51, 53, 54] </ref>. A simple but typical example is the local summation problem [51, 53]: given an n-by-n image, compute, for each pixel, the local sum of its m-by-m neighborhood.
Reference: [50] <author> M. Weiser. </author> <title> Program slicing. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(4):352-357, </volume> <month> July </month> <year> 1984. </year>
Reference-contexts: However, binding time analysis helps obtain a residual program that takes only the dynamic one as arguments, while forward dependence analysis helps obtain a program that computes only on the static arguments. Our resulting program is similar to the resulting slice obtained from forward slicing <ref> [50] </ref>. However, forward slicing finds parts of a program that depend possibly on certain information, while our analysis finds parts of a program that depend only on certain information. Example.
Reference: [51] <author> W. M. Wells, III. </author> <title> Efficient synthesis of Gaussian filters by cascaded uniform filters. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 8(2) </volume> <pages> 234-239, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors [6, 13, 43] and programming environments [5, 9, 13, 19, 24, 25, 41, 42], dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing <ref> [49, 51, 53, 54] </ref>. Incremental Computation. Given a program f and an input change operation , a program f 0 that computes the result of f (x y) efficiently by making use of the value of f (x) is called an incremental version of f under . <p> Following our systematic approach, we have even eliminated an extra shift done in [32]. 6.2 Local Neighborhood Problems In image processing, computing information about local neighborhoods is common <ref> [49, 51, 53, 54] </ref>. A simple but typical example is the local summation problem [51, 53]: given an n-by-n image, compute, for each pixel, the local sum of its m-by-m neighborhood. <p> Following our systematic approach, we have even eliminated an extra shift done in [32]. 6.2 Local Neighborhood Problems In image processing, computing information about local neighborhoods is common [49, 51, 53, 54]. A simple but typical example is the local summation problem <ref> [51, 53] </ref>: given an n-by-n image, compute, for each pixel, the local sum of its m-by-m neighborhood. The naive algorithm takes O (n 2 m 2 ) time, while an efficient algorithm using dynamic programming takes O (n 2 ) time.
Reference: [52] <author> D. M. Yellin and R. E. Strom. INC: </author> <title> A language for incremental computations. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 211-236, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Email: yanhong,tt@cs.cornell.edu 1 Most methods in incremental computation exploit caching intermediate results of certain kinds, e.g., annotating parse trees with attributes [43], dynamically caching results of function calls [39, 40], saving results of intermediate reductions at appointed places [16], maintaining the change-detailing network of INC <ref> [52] </ref>, keeping residual programs for certain input partitions [47], and maintaining intermediate results and dependencies identified by Alphonse annotations [20]. In [30], we have given a cache-and-prune method to statically transform programs to cache all kinds of intermediate results needed for incremental computation. <p> Some methods in incremental computation exploit certain kinds of auxiliary information, e.g., auxiliary arithmetic associated with some classical strength-reduction rules [3], dynamic mappings maintained by finite differencing rules for aggregate primitives in SETL [36] and INC <ref> [52] </ref>, and auxiliary data structures for problems with certain properties like stable decomposition [40] and other decomposition properties [15]. However, until now, the systematic discovery of auxiliary information for general problems has been a subject left completely open for study.
Reference: [53] <author> R. Zabih. </author> <title> Individuating unknown objects by combining motion and stereo. </title> <type> Ph.D. Thesis, </type> <institution> Department of Computer Science, Stanford University, Stanford, California, </institution> <year> 1994. </year>
Reference-contexts: computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors [6, 13, 43] and programming environments [5, 9, 13, 19, 24, 25, 41, 42], dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing <ref> [49, 51, 53, 54] </ref>. Incremental Computation. Given a program f and an input change operation , a program f 0 that computes the result of f (x y) efficiently by making use of the value of f (x) is called an incremental version of f under . <p> Following our systematic approach, we have even eliminated an extra shift done in [32]. 6.2 Local Neighborhood Problems In image processing, computing information about local neighborhoods is common <ref> [49, 51, 53, 54] </ref>. A simple but typical example is the local summation problem [51, 53]: given an n-by-n image, compute, for each pixel, the local sum of its m-by-m neighborhood. <p> Following our systematic approach, we have even eliminated an extra shift done in [32]. 6.2 Local Neighborhood Problems In image processing, computing information about local neighborhoods is common [49, 51, 53, 54]. A simple but typical example is the local summation problem <ref> [51, 53] </ref>: given an n-by-n image, compute, for each pixel, the local sum of its m-by-m neighborhood. The naive algorithm takes O (n 2 m 2 ) time, while an efficient algorithm using dynamic programming takes O (n 2 ) time.
Reference: [54] <author> R. Zabih and J. Woodfill. </author> <title> Non-parametric local transforms for computing visual correspondence. </title> <editor> In J.-O. Eklundh, editor, </editor> <booktitle> 3rd European Conference on Computer Vision, </booktitle> <pages> pages 151-158. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year> <note> LNCS 801. </note>
Reference-contexts: computation have far broader application throughout software, e.g., loop optimizations in optimizing compilers [2, 3, 11, 14, 46], interactive systems like editors [6, 13, 43] and programming environments [5, 9, 13, 19, 24, 25, 41, 42], dynamic systems like distributed databases [10, 29] and real-time systems [48], and image processing <ref> [49, 51, 53, 54] </ref>. Incremental Computation. Given a program f and an input change operation , a program f 0 that computes the result of f (x y) efficiently by making use of the value of f (x) is called an incremental version of f under . <p> Following our systematic approach, we have even eliminated an extra shift done in [32]. 6.2 Local Neighborhood Problems In image processing, computing information about local neighborhoods is common <ref> [49, 51, 53, 54] </ref>. A simple but typical example is the local summation problem [51, 53]: given an n-by-n image, compute, for each pixel, the local sum of its m-by-m neighborhood.
References-found: 54

