URL: ftp://ftp.cse.ucsc.edu/pub/tr/ucsc-crl-97-06.ps.Z
Refering-URL: ftp://ftp.cse.ucsc.edu/pub/tr/README.html
Root-URL: http://www.cse.ucsc.edu
Title: Coping with Memory Latency Restructuring General-Purpose Programs to cope with Memory Latency Interim Project Report  
Author: Dirk Coldewey 
Address: Santa Cruz, CA 95064  
Affiliation: Board of Studies in Computer and Information Sciences University of California, Santa Cruz  
Date: March 20, 1997  
Pubnum: UCSC-CRL-97-06  
Abstract: The widening gap between microprocessor speeds and DRAM has spawned a number of approaches for tolerating the resulting latency of memory accesses. Four common approaches are software controlled prefetching, multi-threading, non-blocking loads, and relaxed consistency models. Most of these methods have been evaluated only on array based codes, although prefetching has also shown to be effective for instruction cache prefetching in operating system codes that have been hand-optimized for cache performance. Many of these methods suffer from considerable runtime overhead, scalability issues, or only work well for a small problem domain. This thesis proposes to show that a method of code transformation to support a combination of prefetching and coarse-grain multithreading can significantly reduce the effects of memory latency with less runtime overhead than currently proposed forms of prefetching, yet that performs well for both array-based codes as well as a broad class of pointer-based data structures found in commercial applications. 
Abstract-found: 1
Intro-found: 1
Reference: [AHH88] <author> Anant Agarwal, John Hennessy, and Mark Horowitz. </author> <title> Cache Performance of Operating System and Multiprogramming Work-loads. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(4) </volume> <pages> 393-431, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: Clark showed that cache performance is significantly lower than application only traces would indicate [Cla83]. Using traces of VAX memory references, Agarwal et al showed that the OS could be responsible for over 50% of the cache miss rate <ref> [AHH88] </ref>. Torrellas et al showed that the OS significantly affects cache performance, stalling the processors for 17% to 21% of their non-idle time for a mix of user-level and commercial data base workloads [TGH92].
Reference: [AI87] <author> Arvind and Robert A. Ian-nucci. </author> <title> Two Fundamental Issues in Multi-processing. Technical Report MIT Computation Structures Group Memo 226-6, </title> <publisher> MIT, </publisher> <month> May </month> <year> 1987. </year> <booktitle> Conference on Parallel Processing in Science and Engineering. </booktitle>
Reference-contexts: Dataflow 3 2. Related Work A general-purpose approach to hiding latency has been the holy grail of large-scale computing since memory was identified as a performance bottleneck. This was addressed as one of the two fundamental issues in multipocessing by Arvind and Ian-nucci <ref> [AI87] </ref>. The widening gap between microprocessors and memory speeds is becoming an increasingly important factor in system performance in uniprocessors as well. 2.1 Multithreading The Denelcor HEP and its successor, the Horizon were among the earliest architectures to employ multithreading to hide memory latency [Jor85]. <p> Dataflow processors rely on low-cost context switching as a method of hiding memory latency and synchronization overhead <ref> [AI87] </ref>. Dataflow architectures restrict their application domain to dataflow graphs, which avoid all but data dependencies. This allows them to maximize the amount of parallelism exploitable by the hardware, right down to the instruction level.
Reference: [ALKK90] <author> A. Agarwal, B-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In 17th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 104-114. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference-contexts: Synchronization occurred through full/empty tags for each memory word and register. Parallelism had to be explicitly specified in HEP Fortran [Smi81]. Several incarnations of multithreading architectures have evolved since then, including April <ref> [ALKK90] </ref> and more recently an interleaved multiple context processor [LGH94]. April performs a context switch only when a memory request cannot be satisfied by the cache or local memory, or when explicitly requested, such as for a failed synchronization attempt. <p> A thread has associated with it a certain amount of state at the very least the program counter and register values. To reduce the amount of overhead required to save the context of a thread prior to performing a context switch, some architectures provide multiple contexts in hardware <ref> [ALKK90] </ref> [LGH94]. entry point: S (X,Y); finished; entry point: prefetch data (&X [0],count); prefetch data (&Y [0],count); prefetch instructions (cont,fini); context_switch; cont: S (X,Y); fini: ... Prefetching Code Block Schema. <p> The simple context switch statement hides the fact that, at the very least, the continuation address must be communicated to the thread control software. The overhead of saving thread context may be addressed by providing multiple contexts in hardware <ref> [ALKK90] </ref> [LGH94]. Second, there must be another block of code that is ready to run in order to keep the processor busy while the data for the current block is being prefetched. The other block should not suffer significant cache conflicts with the data being prefetched.
Reference: [CB93] <author> J. Bradley Chen and Brian N. Bershad. </author> <title> The Impact of Operating System Structure on Memory System Performance. </title> <journal> Operating Systems Review, </journal> <volume> 27(5), </volume> <month> December </month> <year> 1993. </year>
Reference-contexts: When OS interference with the applications' cached working sets is factored in, this number reached up to 25% 1 . Chen and Bershad observed that OS data and instruction locality is considerably worse than that found in applications <ref> [CB93] </ref>. Maynard et al noted that the branching behavior of operating system codes tend to be significantly more random than that of applications, causing more cache lines to be touched. This results in a much larger footprint and higher cache miss rate [MDO94].
Reference: [CB94] <author> Tien-Fu Chen and Jean-Loup Baer. </author> <title> A Performance Study of Software and Hardware Data Prefetching Schemes. </title> <booktitle> In Proceedings of the 21st Annual International Symposium of Computer Architecture, </booktitle> <pages> pages 223-232, </pages> <year> 1994. </year>
Reference-contexts: Block prefetch-ing loads multiple cache lines of contiguous memory with a single command. Chen and Baer showed that the instruction overhead could be significant, ranging from 0:9% to 8:6% of execution time for the mix of scientific and numerical applications that they evaluated, even when block prefetching was supported <ref> [CB94] </ref>. Part of the overhead comes from the prefetch instructions. Additionally, the index values of each prefetched array element must be computed twice: once when the array element is prefetched, and again when the array element is actually used. <p> Borrowing an example from Chen and Baer <ref> [CB94] </ref>, consider the code transformation in figure 2.2. The loop is split into a prolog, a steady state loop, and an epilogue. The prolog prefetches the data for the first for iterations of the steady state loop. <p> This example assumes that the cache line holds two array elements, so the loop is unrolled to avoid unnecessary prefetches, which can cause additional pressure on the registers <ref> [CB94] </ref>. In general, prefetches are scheduled d m e e loop iterations ahead, where e is the estimated execution time of the loop and m is the memory latency. <p> Chen and Baer showed that, even when block prefetching is used, the instruction overhead imposed by software pipelin ing can be significant, ranging from 0:9% to 8:6% for the mix of scientific and numerical applications that they evaluated <ref> [CB94] </ref>. The number of prefetch commands is not the only overhead associated with software controlled pipelining. Another form of overhead comes from having to recalculate loop index values multiple times once for the prefetch, and again when the data is used.
Reference: [Cla83] <author> D. Clark. </author> <title> Cache Performance in the VAX-11/780. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 1(1) </volume> <pages> 24-37, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: A substantial part of the application path-length tends to fall in the operating system kernel. The execution paths tend to have fewer loop iterations and more non-loop branches than technical codes. Clark showed that cache performance is significantly lower than application only traces would indicate <ref> [Cla83] </ref>. Using traces of VAX memory references, Agarwal et al showed that the OS could be responsible for over 50% of the cache miss rate [AHH88].
Reference: [Col94] <author> Dirk Coldewey. </author> <title> A Data Flow Approach to Thread Caching. </title> <type> Technical report, </type> <institution> UCSC, </institution> <address> Santa Cruz, CA, </address> <year> 1994. </year>
Reference-contexts: The memory subsystem simulator also allows for modeling of memory refresh events and static column DRAM setup and access time. The back end includes a programmable access processor to support decoupled access/execute (DAE) architectures [JRH + 94] [SWP86] [Wul92] in combination with featherweight multithreading <ref> [Col94] </ref>. The integrated memory and cache system make it possible to collect numerous statistics on cache behavior, both with and without prefetching.
Reference: [Ell85] <author> John R. Ellis. Bulldog: </author> <title> A Compiler for VLIW Architectures. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <address> Boston, MA, </address> <year> 1985. </year>
Reference-contexts: John Ellis pointed out that, in spite of the promises of scalability and ready access to program parallelism, no commercially viable general-purpose architecture at the dataflow end of the spectrum had been introduced by 4 2. Related Work 1985 <ref> [Ell85] </ref>.
Reference: [FJ94] <author> Keith I. Farkas and Normann P. Jouppi. </author> <title> Complexity/Performance Tradeoffs with Non-Blocking Loads. </title> <booktitle> In IEEE Computer Architecture, </booktitle> <pages> pages 211-222, </pages> <year> 1994. </year>
Reference-contexts: Farkas al used a trace scheduling compiler retargeted to an architecture supporting non-blocking loads to increase the distance between a load instruction and the first use of the corresponding target register <ref> [FJ94] </ref> 2 . The branching behavior of non-numerical codes makes it unlikely that 1 Prefetches to specialized prefetch buffers also suf fer from this problem [KL91] [Jou90]. 2 It is conceivable that the available distance may be further constrained by a trend towards multiple-issue and VLIW architectures.
Reference: [GHD + 91] <author> V.G. Graf, J.E. Hoch, G.S. David-son, V.P. Holmes, D.M. Davenport, and K.M. Steele. </author> <title> The Epsilon Project. </title> <booktitle> In Advanced Topics in Data-Flow Computing. </booktitle> <publisher> Pren-tice Hall, </publisher> <year> 1991. </year>
Reference-contexts: Provide the address of the instruction at which execution can continue once (sufficient) data has arrived to the context switch mechanism. This address is referred to as the continuation. continuation, in accordance with Graf <ref> [GHD + 91] </ref>. 3. Perform a context switch to some other piece of code for which data should be ready, if necessary. This scheme presupposes a runtime system that coordinates the context switches.
Reference: [GLL + 90] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory Consistency and Event Ordering in scalable shared-memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 15-25, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: At times when sequential consistency is essential, specialized operations ensure that the state of memory is consistent throughout the memory hierarchy. Release consistency is the most relaxed memory consistency model, requiring that synchroniza tion occur via specialized acquire and release operations to acquire and release synchronization variables <ref> [GLL + 90] </ref>. Mowry studied the impact of employing release consistency instead of sequential consistency on the Stanford DASH Multiprocessor in conjunction with software-controlled prefetch-ing, and found that it could have a dramatic impact on performance [Mow94]. 3.1. Coarse Grain Multithreading 7 3.
Reference: [Got91] <author> Israel Gottlieb. </author> <title> Work Distribution of in the DSDF Architecture. </title> <booktitle> In Advanced Topics in Data-Flow Computing, </booktitle> <pages> pages 381-382. </pages> <publisher> Pren-tic Hall, </publisher> <address> Englewood Cliffs, New Jersey 07632, </address> <year> 1991. </year>
Reference-contexts: Large sequences of serial code can be identified in most dataflow graphs, and the most efficient execution of serial code occurs on a serial processor equipped with optimizations resulting from 30 years of engineering experience <ref> [Got91] </ref>. John Ellis pointed out that, in spite of the promises of scalability and ready access to program parallelism, no commercially viable general-purpose architecture at the dataflow end of the spectrum had been introduced by 4 2. Related Work 1985 [Ell85].
Reference: [Gus92] <author> David Gustafson. </author> <title> The Scalable Coherent Interface. </title> <journal> IEEE Micro, </journal> <volume> 12(1) </volume> <pages> 10-12, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Recent high end systems SMP systems can be configured with 12 to 20 or more processors. Distributed memory multiprocessor systems that provide a global memory abstraction, including non-uniform memory architectures (NUMA) such as DASH [LLG + 92] and systems based on Scalable Coherent Interconnect <ref> [Gus92] </ref>, such as the Convex Exemplar suffer the additional overhead of a global communication network that can extend memory access times for remote memory to hundreds of clock cycles.
Reference: [Han93] <author> Jim Handy. </author> <title> The Cache Memory Book. </title> <publisher> Academic Press, Inc., </publisher> <address> San Diego, CA 92101, </address> <year> 1993. </year>
Reference-contexts: Faster processor cycle times can restrict the size of the primary cache, resulting in a trend towards putting both a small primary cache and a larger secondary cache directly on the processor chip of high speed processors such as the Alpha <ref> [Han93] </ref>. Shared-memory multiprocessor systems, now commonplace in the commercial marketplace, further exacerbate the problem by requiring processors to compete for the bus in order to satisfy requests to main memory, introducing additional latency due to bus contention and cache interference.
Reference: [HP90] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1990. </year> <title> 26 4. Approach </title>
Reference-contexts: 1.1. Technical Workloads 1 1. Problem Statement While the performance of microprocessors has doubled roughly every three years, DRAM performance has not kept pace, growing at a rate of roughly 22% every three years <ref> [HP90] </ref>. Consequently the performance of memory is becoming a limiting factor in system performance.
Reference: [Ian88] <author> Robert A. </author> <title> Iannucci. Toward a Dataflow/Von Neuman Hybrid Architecture. </title> <booktitle> In Proc. 15th Annual Symp. on Computer Architecture, </booktitle> <volume> volume 16, </volume> <pages> pages 131-140. </pages> <publisher> IEEE, </publisher> <year> 1988. </year>
Reference-contexts: This allows them to maximize the amount of parallelism exploitable by the hardware, right down to the instruction level. Iannucci considered processor architectures to constitute a continuum, with von Neumann architectures occupying one end of the spectrum and dataflow processors at the other end <ref> [Ian88] </ref>. He recognized that it is possible to pick a point on this continuum that represented a much coarser granularity of parallelism than at the instruction level, and proposed a hybrid architecture between von Neumann and Dataflow.
Reference: [IBM90] <author> IBM. </author> <title> POWER Processor Architecture. </title> <institution> IBM Corporation, Advanced Workstation Division, Austin, Texas, </institution> <year> 1990. </year>
Reference-contexts: Some RISC processors will go so far as to reorder the memory requests so that the word that generated the miss will be satisfied first <ref> [IBM90] </ref>. In this instance, the hardware is essentially guessing that a temporally proximate future memory reference will fall within the same cache line. <p> The simulator currently supports two types of traces: those generated by the back end of MINT itself, and PatchWrx traces [SP95]. It provides support for a multilevel cache hierarchy and a pipelined memory subsystem based on the RS/6000 <ref> [IBM90] </ref>, but can be extended to model arbitrary pipelined memory subsystems. Parameters include bus width, memory interleave factor, number of outstanding requests on the bus, and the minimum distance between them.
Reference: [Jor85] <author> Harry F. Jordan. </author> <title> HEP: Achitec-ture, Programming and Performance. In Parallel MIMD Computation: </title> <booktitle> HEP Supercomputer and Its Applications, </booktitle> <pages> pages 1-40. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Mas-sachusetts, </address> <year> 1985. </year>
Reference-contexts: The widening gap between microprocessors and memory speeds is becoming an increasingly important factor in system performance in uniprocessors as well. 2.1 Multithreading The Denelcor HEP and its successor, the Horizon were among the earliest architectures to employ multithreading to hide memory latency <ref> [Jor85] </ref>. The HEP consisted of a 16-element execution pipeline that performed a context switch at every clock cycle.
Reference: [Jou90] <author> Norman P. Jouppi. </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully-Associative Cache and Prefetch Buffers. </title> <booktitle> In Proc. 17th Annual Symp. on Computer Architecture, </booktitle> <volume> volume 18, </volume> <pages> pages 364-373. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference-contexts: Some modern RISC processors such as UltraSparc prefetch instructions for several cache lines beyond an instruction that generates a miss in the instruction cache. Jouppi's proposed multi-stream buffers were a step in the evolution of prefetching sequential operands <ref> [Jou90] </ref>. Stream buffers automatically prefetch additional cache lines, starting at the initial miss target. His measurements indicate that instruction streams break their sequential access pattern by the time the sixth successive cache line is prefetched. <p> The branching behavior of non-numerical codes makes it unlikely that 1 Prefetches to specialized prefetch buffers also suf fer from this problem [KL91] <ref> [Jou90] </ref>. 2 It is conceivable that the available distance may be further constrained by a trend towards multiple-issue and VLIW architectures. <p> In a uniprocessor, this problem is solved by ensuring that the results of store operations are visible to the coherence mechanisms of the memory hierarchy, either by making the write buffer (which is usually small) fully set associative as in victim caches <ref> [Jou90] </ref>, or by using an write-allocate cache policy that ensures that the caches always reflect the values in the store buffers. In a multiprocessor, sequential consistency can be maintained by ensuring that all store operations stall until the main memory is updated.
Reference: [JRH + 94] <author> Lizy Kurian John, Vinod Reddy, Paul T. Hulina, , and Lee D. Coraor. </author> <title> Program Balance and its Impact on High Performance RISC Architectures. </title> <booktitle> In Proceedings of the First IEEE Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 370-379, </pages> <month> Jan-uary </month> <year> 1994. </year>
Reference-contexts: The memory subsystem simulator also allows for modeling of memory refresh events and static column DRAM setup and access time. The back end includes a programmable access processor to support decoupled access/execute (DAE) architectures <ref> [JRH + 94] </ref> [SWP86] [Wul92] in combination with featherweight multithreading [Col94]. The integrated memory and cache system make it possible to collect numerous statistics on cache behavior, both with and without prefetching.
Reference: [KL91] <author> A.C. Klaiber and H.M. Levy. </author> <title> An Architecture for Software-Controlled Data Prefetching. </title> <booktitle> In Proceedings of the 18th Interrna-tional Symposium on Computer Architecture, </booktitle> <pages> pages 43-53, </pages> <year> 1991. </year>
Reference-contexts: Software controlled prefetch-ing has proven effective in hiding a significant amount of latency in array based data structures where future address references are readily computed from the surrounding loop variables [Mow94] <ref> [KL91] </ref> [Por89]. Software controlled prefetching incurs the overhead of separate prefetch instructions for each prefetched cache line. Block prefetch-ing loads multiple cache lines of contiguous memory with a single command. <p> Klaiber and Levy extended Porterfield's work to fetch into a separate fetch buffer more than a single iteration ahead <ref> [KL91] </ref>. Mowry noted that, in addition to representing an inefficient use of chip area, their approach makes nonbinding prefetches difficult [Mow94]. The non-binding property of prefetches is essential to ensuring program correctness in a multiprocessor environment. <p> The branching behavior of non-numerical codes makes it unlikely that 1 Prefetches to specialized prefetch buffers also suf fer from this problem <ref> [KL91] </ref> [Jou90]. 2 It is conceivable that the available distance may be further constrained by a trend towards multiple-issue and VLIW architectures.
Reference: [KV94] <author> P. Krishnan and Jeffrey Scott Vitter. </author> <title> Optimal Prediction for Prefetching in the Worst Case. </title> <booktitle> In SODA 94, </booktitle> <year> 1994. </year>
Reference-contexts: For each of the instructions selected to be preceded by a prefetch, it is possible to trace the addresses that missed, allowing evaluation of off-line and on-line prediction algorithms <ref> [KV94] </ref> [VK91]. 4.2.1 Remaining Infrastructure Work The model currently makes several assumptions: while it does model finite prefetch issue queues, it currently assumes infinite write queues. The modeling of prefetch issue queues and write queues is essential to accurately measure the stall times.
Reference: [LGH94] <author> James Laudon, Anoop Gupta, and Mark Horowitz. </author> <booktitle> Architectural and Implementation Tradeoffs in the Design of Multiple-Context Processors. In Mul-tithreaded Computer Architecture, </booktitle> <pages> pages 167-200. </pages> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> Mas-sachusetts 02061, </address> <year> 1994. </year>
Reference-contexts: Synchronization occurred through full/empty tags for each memory word and register. Parallelism had to be explicitly specified in HEP Fortran [Smi81]. Several incarnations of multithreading architectures have evolved since then, including April [ALKK90] and more recently an interleaved multiple context processor <ref> [LGH94] </ref>. April performs a context switch only when a memory request cannot be satisfied by the cache or local memory, or when explicitly requested, such as for a failed synchronization attempt. The expense of a context switch can be high because it requires the execution pipelines to complete processing. <p> A thread has associated with it a certain amount of state at the very least the program counter and register values. To reduce the amount of overhead required to save the context of a thread prior to performing a context switch, some architectures provide multiple contexts in hardware [ALKK90] <ref> [LGH94] </ref>. entry point: S (X,Y); finished; entry point: prefetch data (&X [0],count); prefetch data (&Y [0],count); prefetch instructions (cont,fini); context_switch; cont: S (X,Y); fini: ... Prefetching Code Block Schema. <p> The simple context switch statement hides the fact that, at the very least, the continuation address must be communicated to the thread control software. The overhead of saving thread context may be addressed by providing multiple contexts in hardware [ALKK90] <ref> [LGH94] </ref>. Second, there must be another block of code that is ready to run in order to keep the processor busy while the data for the current block is being prefetched. The other block should not suffer significant cache conflicts with the data being prefetched.
Reference: [LLG + 92] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica S. Lam. </author> <title> The Stan-ford DASH Multiprocessor. </title> <booktitle> Computer, </booktitle> <pages> pages 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Recent high end systems SMP systems can be configured with 12 to 20 or more processors. Distributed memory multiprocessor systems that provide a global memory abstraction, including non-uniform memory architectures (NUMA) such as DASH <ref> [LLG + 92] </ref> and systems based on Scalable Coherent Interconnect [Gus92], such as the Convex Exemplar suffer the additional overhead of a global communication network that can extend memory access times for remote memory to hundreds of clock cycles.
Reference: [LRW91] <author> Monica S. Lam, Edward E. Roth-berg, and Michael E. Wolf. </author> <title> The Cache Performance and Optimizations of Blocked Algorithms. </title> <booktitle> In Proc. Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: The non-binding property of prefetches is essential to ensuring program correctness in a multiprocessor environment. Lam et al proposed algorithms to predict cache misses in nested loops in order to optimize blocking algorithms for various cache geometries to maximize cache reuse <ref> [LRW91] </ref>. Wolf incorporated these ideas into a compiler that performed automatic parallelization and locality optimizations via loop slicing [Wol92]. Mowry extended Lam and Wolf's algorithms to incorporate prefetching in the form of software pipelining. The salient features of this approach are that Wolf's algorithms perform reasonably well at predicting misses.
Reference: [MDO94] <author> Ann Marie Grizzaffi Manyard, Colette M. Donnelly, , and Bret R. Olszewski. </author> <title> Contrasting Characteristics and Cache Performance of Technical and Multi-User Commercial Workloads. </title> <booktitle> In ASPLOS-VI Proceedings, </booktitle> <pages> pages 145-155. </pages> <publisher> ACM Press, </publisher> <month> November </month> <year> 1994. </year>
Reference-contexts: goal of the proposed approach is to reduce the insruction overhead of software controlled prefetching when applied to array based codes. 1.2 Commercial Workloads Recent measurements on commercial work-loads have shown their system resource usage characteristics to be considerably different than those of the commonly used scientific and engineering benchmarks <ref> [MDO94] </ref>. Commercial applications such as the TPC benchmark suites distributed by the Transaction Processing Performance Council run in multi-user environments. Unlike technical workloads, they tend to consist of many user processes exhibiting short run-lengths 2 1. Problem Statement because of frequent I/O operations. <p> Maynard et al noted that the branching behavior of operating system codes tend to be significantly more random than that of applications, causing more cache lines to be touched. This results in a much larger footprint and higher cache miss rate <ref> [MDO94] </ref>. They also found that some commercial applications, and database applications in particular, exhibit caching behavior very similar to that of the OS. In summary, commercial codes exhibit considerably less data locality and instruction reuse, and both data and instruction cache miss rates are substantially higher. <p> Related Work even a VLIW compiler can find sufficient intra-thread parallelism to keep the processor busy without resorting to speculative execution [TXD94] <ref> [MDO94] </ref>. Given the poor instruction cache performance of commercial workloads, any benefits derived from speculative execution is likely to evaporate quickly with the increased instruction cache miss rate. 2.5 Relaxed Consistency Models Memory consistency models have a significant impact on performance in multiprocessor designs. <p> Determine the data that will be referenced within the block and place prefetch commands that prefetch the required data into cache at the beginning of the block. This information might be gathered through profiling or some other means of gathering cache statistics [SP95] <ref> [MDO94] </ref>. Compilers can 8 3. Proposed Solution /* start of block */ ... if ( funky () ) foo (Y); ... /* end of block */ the block. identify references that are likely to miss in the data cache for many array based codes [Mow94]. 2. <p> These timers can be used to en force a scheduling discipline that prevents time critical activities from expiring if an insufficient number of events have accumulated to trigger processing. Because commercial applications exhibit significant instruction cache miss rates [TXD94] <ref> [MDO94] </ref> [Xia96], this approach benefits the implementation even in the absence of prefetching hardware because it guarantees, at the very least, instruction cache reuse by forcing temporal locality of instruction references.
Reference: [Mow94] <author> Todd C. Mowry. </author> <title> Tolerating Latency through Software-Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1994. </year>
Reference-contexts: Software controlled prefetch-ing has proven effective in hiding a significant amount of latency in array based data structures where future address references are readily computed from the surrounding loop variables <ref> [Mow94] </ref> [KL91] [Por89]. Software controlled prefetching incurs the overhead of separate prefetch instructions for each prefetched cache line. Block prefetch-ing loads multiple cache lines of contiguous memory with a single command. <p> Klaiber and Levy extended Porterfield's work to fetch into a separate fetch buffer more than a single iteration ahead [KL91]. Mowry noted that, in addition to representing an inefficient use of chip area, their approach makes nonbinding prefetches difficult <ref> [Mow94] </ref>. The non-binding property of prefetches is essential to ensuring program correctness in a multiprocessor environment. Lam et al proposed algorithms to predict cache misses in nested loops in order to optimize blocking algorithms for various cache geometries to maximize cache reuse [LRW91]. <p> Chen and Baer comment on the potential for prefetched data to be displaced before they can be used or to interfere with the working set, although Mowry tended to double the prefetch distance in his experiments without suffering significantly from interference effects <ref> [Mow94] </ref>. 2.4 Non-blocking Loads Non-blocking loads are a special form of prefetching. Non-blocking loads are register load instructions that allow processing to proceed upon a data cache miss until the target register is actually referenced. This approach has a number of severe limitations. Loads are non-blocking but not non-binding. <p> This means that the compiler must ensure that no stores occur to the address for which a load has previously been initiated <ref> [Mow94] </ref>. A further problem arises from the need to find sufficient work between register loads to keep the processor busy. This issue is universal to all approaches to hiding read latency. <p> Mowry studied the impact of employing release consistency instead of sequential consistency on the Stanford DASH Multiprocessor in conjunction with software-controlled prefetch-ing, and found that it could have a dramatic impact on performance <ref> [Mow94] </ref>. 3.1. Coarse Grain Multithreading 7 3. Proposed Solution Software controlled prefetching has proven effective at hiding memory latency in many loop intensive codes. We would like to investigate means of hiding latency for a more general class of programs. <p> Compilers can 8 3. Proposed Solution /* start of block */ ... if ( funky () ) foo (Y); ... /* end of block */ the block. identify references that are likely to miss in the data cache for many array based codes <ref> [Mow94] </ref>. 2. Provide the address of the instruction at which execution can continue once (sufficient) data has arrived to the context switch mechanism. This address is referred to as the continuation. continuation, in accordance with Graf [GHD + 91]. 3. <p> For software pipelin-ing, scheduling is a natural result of the looping mechanism, but our target codes are not constrained to loop intensive codes. Mowry showed that a victim cache can be effective in hiding some of the spurious cache evictions due to conflicts <ref> [Mow94] </ref>, but clearly the currently executing context should not be a loop intensive blocked algorithm that effectively utilizes most of the cache before yielding to another thread. <p> Recall that, in order to hide the memory latency of references in loop-based codes, prefetches may have to occur multiple iterations in advance. This is problematic for several reasons. Some pointers may generate memory exceptions. This problem is solved by making prefetches non-excepting <ref> [Mow94] </ref>. A larger problem is that the address of the nth element requires that the addresses of the pointers to the next field of each preceding element in the linked list be resolved.
Reference: [NA89] <author> Rishiyur S. Nikhil and Arvind. </author> <title> Can dataflow subsume von Neu-mann computing? In Proc. </title> <booktitle> 16th International Symposium on Computer Architecture, </booktitle> <pages> pages 262-272, </pages> <year> 1989. </year>
Reference-contexts: Thus if the processor supports n simultaneous threads, only 1=n pipeline bubbles will occur for each thread that blocks on a cache miss. 2.2 Dataflow The idea of multithreading was taken a step further in the MIT Tagged-Token Dataflow Architecture, where synchronization occurred at the instruction level <ref> [NA89] </ref>. Dataflow processors rely on low-cost context switching as a method of hiding memory latency and synchronization overhead [AI87]. Dataflow architectures restrict their application domain to dataflow graphs, which avoid all but data dependencies.
Reference: [Pfi95] <author> Gregory E. Pfister. </author> <title> In Search of Clusters. </title> <publisher> Prentice Hall, Inc., </publisher> <address> Upper Saddle River, New Jersey, </address> <year> 1995. </year> <note> 4.2. Infrastructure 27 </note>
Reference-contexts: It has the unfortunate side effect of disallowing buffering of write operations, thus introducing considerable latency into the system. Relaxed consistency permits a more arbitrary interleaving of read and write requests, based on the observation that it possible to get away with it almost all of the time <ref> [Pfi95] </ref>. At times when sequential consistency is essential, specialized operations ensure that the state of memory is consistent throughout the memory hierarchy.
Reference: [Por89] <author> A. K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> Technical Report COMP TR 89-93, </type> <institution> Rice University, </institution> <month> May </month> <year> 1989. </year> <note> Cited in Klai91. </note>
Reference-contexts: Software controlled prefetch-ing has proven effective in hiding a significant amount of latency in array based data structures where future address references are readily computed from the surrounding loop variables [Mow94] [KL91] <ref> [Por89] </ref>. Software controlled prefetching incurs the overhead of separate prefetch instructions for each prefetched cache line. Block prefetch-ing loads multiple cache lines of contiguous memory with a single command. <p> Porterfield first took advantage of compile time information to predict future references and prefetched all array references in inner loops a single iteration ahead <ref> [Por89] </ref>. He then refined his algorithm to take into account dependence information and to estimate the number of loop iterations before the loop began accessing data that would no longer fit into the cache.
Reference: [Smi81] <author> Burton J. Smith. </author> <title> Architecture and Applications of the HEP Multiprocessor Computer System. In Tutorial on Supercomputers, page 425. </title> <publisher> IEEE Press, </publisher> <year> 1981. </year>
Reference-contexts: Synchronization occurred through full/empty tags for each memory word and register. Parallelism had to be explicitly specified in HEP Fortran <ref> [Smi81] </ref>. Several incarnations of multithreading architectures have evolved since then, including April [ALKK90] and more recently an interleaved multiple context processor [LGH94].
Reference: [SP95] <author> Richard L. Sites and Sharon E. </author> <title> Perl. PatchWrx ADynamic Execution Tracing Tool. </title> <institution> Technical Report Systems Research Center, Digital Equipment Corporation, </institution> <month> October </month> <year> 1995. </year> <note> Submitted for Publication. </note>
Reference-contexts: This restriction does preclude many important data structures, such as binary trees. Strategies for prefetching such data structures are discussed in section 3.2. Blocks that experience the highest instruction and data cache miss rates are identified using cache profiling <ref> [SP95] </ref>, or simply by means of educated guesses on the part of the programmer. The generic schema for tranforming these code blocks to perform prefetching is as follows: 1. <p> Determine the data that will be referenced within the block and place prefetch commands that prefetch the required data into cache at the beginning of the block. This information might be gathered through profiling or some other means of gathering cache statistics <ref> [SP95] </ref> [MDO94]. Compilers can 8 3. Proposed Solution /* start of block */ ... if ( funky () ) foo (Y); ... /* end of block */ the block. identify references that are likely to miss in the data cache for many array based codes [Mow94]. 2. <p> The back end has been modified to generate and read address traces instead of relying on simulation to generate cache usage statistics. The simulator currently supports two types of traces: those generated by the back end of MINT itself, and PatchWrx traces <ref> [SP95] </ref>. It provides support for a multilevel cache hierarchy and a pipelined memory subsystem based on the RS/6000 [IBM90], but can be extended to model arbitrary pipelined memory subsystems. Parameters include bus width, memory interleave factor, number of outstanding requests on the bus, and the minimum distance between them.
Reference: [SWP86] <author> J.E. Smith, S. Weiss, and N.Y. Pang. </author> <title> A Simulation Study of De-coupled Architecture Computers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(8), </volume> <month> August </month> <year> 1986. </year> <note> Cited in John94. </note>
Reference-contexts: The memory subsystem simulator also allows for modeling of memory refresh events and static column DRAM setup and access time. The back end includes a programmable access processor to support decoupled access/execute (DAE) architectures [JRH + 94] <ref> [SWP86] </ref> [Wul92] in combination with featherweight multithreading [Col94]. The integrated memory and cache system make it possible to collect numerous statistics on cache behavior, both with and without prefetching.
Reference: [TGH92] <author> Josep Torrellas, Anoop Gupta, and John Hennessy. </author> <title> Characterizing the Caching and Synchronization Performance of a Multiprocessor Operating System. </title> <booktitle> In ASPLOS-V Proceedings, </booktitle> <pages> pages 162-174, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Torrellas et al showed that the OS significantly affects cache performance, stalling the processors for 17% to 21% of their non-idle time for a mix of user-level and commercial data base workloads <ref> [TGH92] </ref>. When OS interference with the applications' cached working sets is factored in, this number reached up to 25% 1 . Chen and Bershad observed that OS data and instruction locality is considerably worse than that found in applications [CB93].
Reference: [TXD94] <author> Josep Torrellas, Chun Xia, and Russell Daigle. </author> <title> Optimizing Instruction Cache Performance for Operating System Intensive Loads. </title> <booktitle> In Proceedings of the First IEEE Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 360-369, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: They also found that some commercial applications, and database applications in particular, exhibit caching behavior very similar to that of the OS. In summary, commercial codes exhibit considerably less data locality and instruction reuse, and both data and instruction cache miss rates are substantially higher. Torrellas <ref> [TXD94] </ref> and Xia [Xia96] found that careful manual restructuring of the basic blocks of operating system codes could substantially reduce the instruction cache miss rate. <p> Related Work even a VLIW compiler can find sufficient intra-thread parallelism to keep the processor busy without resorting to speculative execution <ref> [TXD94] </ref> [MDO94]. Given the poor instruction cache performance of commercial workloads, any benefits derived from speculative execution is likely to evaporate quickly with the increased instruction cache miss rate. 2.5 Relaxed Consistency Models Memory consistency models have a significant impact on performance in multiprocessor designs. <p> These timers can be used to en force a scheduling discipline that prevents time critical activities from expiring if an insufficient number of events have accumulated to trigger processing. Because commercial applications exhibit significant instruction cache miss rates <ref> [TXD94] </ref> [MDO94] [Xia96], this approach benefits the implementation even in the absence of prefetching hardware because it guarantees, at the very least, instruction cache reuse by forcing temporal locality of instruction references.
Reference: [VF93] <author> Jack E. Veenstra and Robert J. Fowler. </author> <title> MINT Tutorial and User Manual. </title> <type> Technical Report Technical Report 452, </type> <institution> University of Rochester, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: This shotgun approach allows us to show the level of performance improvement under a broad range of cache pollution scenarios. 4.2 Infrastructure I have constructed an evaluation environment built around the MINT MIPS R4000 simulator front end <ref> [VF93] </ref>. MINT executes code compiled to run on a MIPS R4000 processor. System calls are passed on to the underlying operating system. The back end has been modified to generate and read address traces instead of relying on simulation to generate cache usage statistics.
Reference: [VK91] <author> Jeffrey Scott Vitter and P. Kr-ishnan. </author> <title> Optimal Prefetching via Dat Compression. </title> <booktitle> In Proceedings of the 32nd Annual Symposium on Foundations of Computer Science, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: For each of the instructions selected to be preceded by a prefetch, it is possible to trace the addresses that missed, allowing evaluation of off-line and on-line prediction algorithms [KV94] <ref> [VK91] </ref>. 4.2.1 Remaining Infrastructure Work The model currently makes several assumptions: while it does model finite prefetch issue queues, it currently assumes infinite write queues. The modeling of prefetch issue queues and write queues is essential to accurately measure the stall times.
Reference: [Wol92] <author> Michael Edward Wolf. </author> <title> Improving Locality and Parallelism in Nested Loops. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1992. </year>
Reference-contexts: Lam et al proposed algorithms to predict cache misses in nested loops in order to optimize blocking algorithms for various cache geometries to maximize cache reuse [LRW91]. Wolf incorporated these ideas into a compiler that performed automatic parallelization and locality optimizations via loop slicing <ref> [Wol92] </ref>. Mowry extended Lam and Wolf's algorithms to incorporate prefetching in the form of software pipelining. The salient features of this approach are that Wolf's algorithms perform reasonably well at predicting misses. These algorithms are applicable to loop-intensive numerical codes, and do not address other data structures and algorithms.
Reference: [Wul92] <author> W.A. Wulf. </author> <title> Evaluation of the Wm Architecture. </title> <booktitle> In Proceedings of the 19th Annual International Symposium of Computer Architecture, </booktitle> <pages> pages 382-390, </pages> <month> May </month> <year> 1992. </year> <note> Cited in John94. </note>
Reference-contexts: The memory subsystem simulator also allows for modeling of memory refresh events and static column DRAM setup and access time. The back end includes a programmable access processor to support decoupled access/execute (DAE) architectures [JRH + 94] [SWP86] <ref> [Wul92] </ref> in combination with featherweight multithreading [Col94]. The integrated memory and cache system make it possible to collect numerous statistics on cache behavior, both with and without prefetching.
Reference: [Xia96] <author> Chun Xia. </author> <title> Exploiting Multiprocessor Memory Hierarchies for Operating Systems. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Urbana, Illi-nois, </institution> <year> 1996. </year>
Reference-contexts: They also found that some commercial applications, and database applications in particular, exhibit caching behavior very similar to that of the OS. In summary, commercial codes exhibit considerably less data locality and instruction reuse, and both data and instruction cache miss rates are substantially higher. Torrellas [TXD94] and Xia <ref> [Xia96] </ref> found that careful manual restructuring of the basic blocks of operating system codes could substantially reduce the instruction cache miss rate. <p> In practical terms, this means that blocks must have a small number of entry and exit points. Xia showed that many of the frequently executed blocks of operating system codes meet this criterion <ref> [Xia96] </ref>. For the time being we also impose a second restriction, requiring that all instructions that perform long latency data references within the block must have a high probability of being executed. This restriction makes it possible to avoid issuing unnecessary prefetches. <p> These timers can be used to en force a scheduling discipline that prevents time critical activities from expiring if an insufficient number of events have accumulated to trigger processing. Because commercial applications exhibit significant instruction cache miss rates [TXD94] [MDO94] <ref> [Xia96] </ref>, this approach benefits the implementation even in the absence of prefetching hardware because it guarantees, at the very least, instruction cache reuse by forcing temporal locality of instruction references.
References-found: 40

