URL: http://www.cs.duke.edu/~jsv/Papers/BGV96.Simple_Mergesort.ps.gz
Refering-URL: http://www.cs.duke.edu/~jsv/Papers/catalog/node17.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: CS-1996-15 Simple Randomized Mergesort on Parallel Disks 1  
Author: Rakesh Barve Edward F. Grove Jeffrey Scott Vitter 
Date: October 1996  
Address: 27708-0129  
Affiliation: Department of Computer Science Duke University Durham, North Carolina  
Abstract-found: 0
Intro-found: 1
Reference: [AP94] <author> Alok Aggarwal and C. Greg Plaxton. </author> <title> Optimal parallel sorting in multi-level storage. </title> <booktitle> Proc. Fifth Annual ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 659-668, </pages> <year> 1994. </year>
Reference-contexts: Vitter and Shriver [VS94] give further intuition regarding the difficulty of mergesorting on parallel disks. In Greed Sort [NV90], the trick of "approximate merging" is used to circumvent this difficulty. Aggarwal and Plaxton <ref> [AP94] </ref> use the Sharesort technique that does repeated merging with accompanying overhead. Recently, Pai et al [PSV94] considered the average-case performance of a simple merging scheme for R = D sorted runs, one run on each disk.
Reference: [AV88] <author> Alok Aggarwal and Jeffrey S. Vitter. </author> <title> The input/output complexity of sorting and related problems. </title> <journal> Communications of the ACM, </journal> <volume> 31(9) </volume> <pages> 1116-1127, </pages> <year> 1988. </year>
Reference-contexts: One way to alleviate the effects of the I/O bottleneck is to use parallel disk systems [HGK + 94, PGK88, Uni89, GS84, Mag87]. Aggarwal and Vitter <ref> [AV88] </ref>, generalizing initial work done by Floyd [Flo72] and Hong and Kung [HK81], laid the foundation for I/O algorithms by studying the I/O complexity of sorting and related problems. The model they studied [AV88] considers an internal memory of size M and I/O reads or writes that each result in a <p> Aggarwal and Vitter <ref> [AV88] </ref>, generalizing initial work done by Floyd [Flo72] and Hong and Kung [HK81], laid the foundation for I/O algorithms by studying the I/O complexity of sorting and related problems. The model they studied [AV88] considers an internal memory of size M and I/O reads or writes that each result in a transfer of D blocks, where each block is comprised of B contiguous records, from or to disks.
Reference: [BGV96] <author> Rakesh D. Barve, Edward F. Grove, and Jeffrey Scott Vitter. </author> <title> Simple randomized mergesort on parallel disks. </title> <booktitle> Proc. of the 8th ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 109-118, </pages> <year> 1996. </year>
Reference-contexts: Intuitively, independence increases the expected maximum occupancy. In <ref> [BGV96] </ref>, our approach was to use the well known bounds for the classical maximum occupancy to bound dependent maximum occupancies; however, the proof of the bound we gave was erroneous. We do believe, however, that the bound is correct, as we conjecture above.
Reference: [Flo72] <author> R. W. Floyd. </author> <title> Permuting information in idealized two-level storage. </title> <editor> In R. Miller and J. Thatcher, editors, </editor> <booktitle> Complexity of Computer Computations, </booktitle> <pages> pages 105-109. </pages> <publisher> Plenum, </publisher> <year> 1972. </year>
Reference-contexts: One way to alleviate the effects of the I/O bottleneck is to use parallel disk systems [HGK + 94, PGK88, Uni89, GS84, Mag87]. Aggarwal and Vitter [AV88], generalizing initial work done by Floyd <ref> [Flo72] </ref> and Hong and Kung [HK81], laid the foundation for I/O algorithms by studying the I/O complexity of sorting and related problems.
Reference: [GK81] <author> Daniel H. Greene and Donald E. Knuth. </author> <title> Mathematics for the Analysis of Algorithms. </title> <publisher> Birkhauser, </publisher> <address> Boston., </address> <year> 1981. </year>
Reference-contexts: which gives us G X (z) = 1jD 1 D jz n j For 0 m 1jD n j = C, the coefficient PrfX = mg of z m in G X (z) is also the residue at z = 0 of the complex analytic function G X (z)=z m+1 <ref> [GK81] </ref>. Moreover, z = 0 is the only pole of G X (z)=z m+1 enclosed in a circle of radius P centered at z = 0 in the complex plane, where P is any positive number.
Reference: [GS84] <author> D. Gifford and A. Spector. </author> <title> The TWA reservation system. </title> <journal> Comm. ACM, </journal> <volume> 27(7) </volume> <pages> 650-665, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: One way to alleviate the effects of the I/O bottleneck is to use parallel disk systems <ref> [HGK + 94, PGK88, Uni89, GS84, Mag87] </ref>. Aggarwal and Vitter [AV88], generalizing initial work done by Floyd [Flo72] and Hong and Kung [HK81], laid the foundation for I/O algorithms by studying the I/O complexity of sorting and related problems.
Reference: [HGK + 94] <author> L. Hellerstein, G. Gibson, R. M. Karp, R. H. Katz, and D. A. Patterson. </author> <title> Coding techniques for handling failures in large disk arrays. </title> <journal> Algorithmica, </journal> <pages> 12(2-3), </pages> <year> 1994. </year>
Reference-contexts: One way to alleviate the effects of the I/O bottleneck is to use parallel disk systems <ref> [HGK + 94, PGK88, Uni89, GS84, Mag87] </ref>. Aggarwal and Vitter [AV88], generalizing initial work done by Floyd [Flo72] and Hong and Kung [HK81], laid the foundation for I/O algorithms by studying the I/O complexity of sorting and related problems.
Reference: [HK81] <author> J. W. Hong and H. T. Kung. </author> <title> I/O complexity: The red-blue pebble game. </title> <booktitle> Proc. 13th Annual ACM Symp. on Theory of Computation, </booktitle> <pages> pages 326-333, </pages> <month> may </month> <year> 1981. </year>
Reference-contexts: One way to alleviate the effects of the I/O bottleneck is to use parallel disk systems [HGK + 94, PGK88, Uni89, GS84, Mag87]. Aggarwal and Vitter [AV88], generalizing initial work done by Floyd [Flo72] and Hong and Kung <ref> [HK81] </ref>, laid the foundation for I/O algorithms by studying the I/O complexity of sorting and related problems.
Reference: [Knu73] <author> D. E. Knuth. </author> <title> The Art of Computer Programming, Volume 3: Sorting and Searching. </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: 1 Introduction The classical problem of sorting and related processing is reported to consume roughly 20 percent of computing resources in large-scale installations <ref> [Knu73, LV85] </ref>. In the light of the rapidly increasing gap between processor speeds and disk memory access times, popularly referred to as the I/O bottleneck , the specific problem of external memory sorting assumes particular importance. <p> Techniques like replacement selection <ref> [Knu73] </ref> can produce roughly N=M runs each of length about M . Then, in a series of passes over the file, R sorted runs of records are repeatedly merged together until the file is sorted. <p> Our algorithm uses a generalization of the forecasting technique <ref> [Knu73] </ref> in order to carry out parallel prefetching of appropriate disk blocks during merging. Randomization is used only while choosing, for each input run, the disk on which to place the first block of the run; the remaining blocks of that run are cyclically striped across the disks. <p> This enables us to begin each run on a uniformly random disk. We can also format each block of every run so as to implant some future information as explained below. This is a generalization of the forecasting technique of <ref> [Knu73] </ref>. We denote the ith block of a run r as b r;i and the smallest (first) key in block b r;i as k r;i . <p> Implementing internal merge processing and the I/O schedule as concurrent modules makes the overlapping of CPU computation and I/O operations an easier task. Internal merge processing can be implemented using a variety of techniques; we refer the reader to <ref> [Knu73] </ref>. In this section, we focus on the I/O scheduling algorithm. We first discuss issues pertaining to management of internal memory and maintaining the FDS data structure during the merge.
Reference: [KSC78] <author> V. F. Kolchin, B. A. Sevastyanov, and V. P. Chistyakov. </author> <title> Random Allocations. </title> <publisher> Winston & Sons, </publisher> <address> Washington, </address> <year> 1978. </year>
Reference-contexts: This memory management scheme enables us to obtain a handle on SRM's performance. We are able to relate SRM's performance to a combinatorial problem we call the dependent maximum occupancy problem, which is a generalization of the classical maximum occupancy problem <ref> [KSC78, VF90] </ref>. Our main theorem below gives expressions for the I/O performance of SRM for three patterns of growth rate between the number M=B of blocks in internal memory and the number D of disks in the parallel disk system. <p> In the classical occupancy problem of parameters fN b ; Dg, N b balls are thrown into D bins independently each with uniform probability of 1=D of falling into any bin. We denote the asymptotically tight expression for the expectation of the maximum number of balls in any bin <ref> [VF90, KSC78] </ref> by C (N b ; D). In the dependent occupancy problem, we consider D bins but instead of balls, we consider C chains of balls, such that the total number of balls summed over the C chains, is N b . <p> As mentioned earlier, the leading terms in our upper bounds for this quantity are the same as those in the well-known bounds for C (N b ; D) in the classical occupancy problem <ref> [KSC78] </ref>. We conjecture that the expected maximum classical occupancy C (N b ; D) is an upper bound for the maximum dependent occupancies. <p> In this paper, we instead derive a direct bound on the expectation of the maximum dependent occupancy via interesting analytical and asymptotic techniques, and as a result our proof is independent of the classical occupancy bounds of <ref> [KSC78] </ref>. <p> The upper bounds we prove for the expectation of maximum occupancy in the dependent occupancy problem are precisely the asymptotically tight bounds for the classical maximum occupancy problem derived in <ref> [KSC78] </ref>. In fact, our proof for the upper bound 16 7. PROBABILISTIC ANALYSIS on dependent expected maximum occupancy constitutes an alternate approach to obtaining the same asymptotically leading terms as those in [KSC78] for the classical expected maximum occupancy. (The proof in [KSC78] also serves as a lower bound on classical <p> the dependent occupancy problem are precisely the asymptotically tight bounds for the classical maximum occupancy problem derived in <ref> [KSC78] </ref>. In fact, our proof for the upper bound 16 7. PROBABILISTIC ANALYSIS on dependent expected maximum occupancy constitutes an alternate approach to obtaining the same asymptotically leading terms as those in [KSC78] for the classical expected maximum occupancy. (The proof in [KSC78] also serves as a lower bound on classical expected maximum occupancy but our techniques can be modified to do the same.) Proof of Theorem 2: The given dependency problem X 0 might involve some chains that are longer than D <p> for the classical maximum occupancy problem derived in <ref> [KSC78] </ref>. In fact, our proof for the upper bound 16 7. PROBABILISTIC ANALYSIS on dependent expected maximum occupancy constitutes an alternate approach to obtaining the same asymptotically leading terms as those in [KSC78] for the classical expected maximum occupancy. (The proof in [KSC78] also serves as a lower bound on classical expected maximum occupancy but our techniques can be modified to do the same.) Proof of Theorem 2: The given dependency problem X 0 might involve some chains that are longer than D in length.
Reference: [LV85] <author> E. E. Lindstrom and J. S. Vitter. </author> <title> The design and analysis of bucketsort for bubble memory secondary storage. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34:218-233, </volume> <month> March </month> <year> 1985. </year>
Reference-contexts: 1 Introduction The classical problem of sorting and related processing is reported to consume roughly 20 percent of computing resources in large-scale installations <ref> [Knu73, LV85] </ref>. In the light of the rapidly increasing gap between processor speeds and disk memory access times, popularly referred to as the I/O bottleneck , the specific problem of external memory sorting assumes particular importance.
Reference: [Mag87] <author> N. B. Maginnis. </author> <title> Store more, spend less: </title> <journal> Mid-range options abound. </journal> <volume> Computer-world, </volume> <pages> pages 71-82, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: One way to alleviate the effects of the I/O bottleneck is to use parallel disk systems <ref> [HGK + 94, PGK88, Uni89, GS84, Mag87] </ref>. Aggarwal and Vitter [AV88], generalizing initial work done by Floyd [Flo72] and Hong and Kung [HK81], laid the foundation for I/O algorithms by studying the I/O complexity of sorting and related problems.
Reference: [NV90] <author> M. H. Nodine and J. S. Vitter. </author> <title> Large-scale sorting in parallel memories. </title> <booktitle> In Proc. 3rd ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 29-39, </pages> <year> 1990. </year> <month> 27 </month>
Reference-contexts: MAIN RESULTS external sorting on parallel disks <ref> [NV90, NV93] </ref>. In DSM, the disks are coordinated so that in each parallel read and write, the locations of the blocks accessed on each disk are the same, which has the logical effect of sorting with D 0 = 1 disk and block size B 0 = DB. <p> However, this memory management is difficult to achieve since the order in which the data in various disk blocks will participate in the merge process is input dependent and unknown. Vitter and Shriver [VS94] give further intuition regarding the difficulty of mergesorting on parallel disks. In Greed Sort <ref> [NV90] </ref>, the trick of "approximate merging" is used to circumvent this difficulty. Aggarwal and Plaxton [AP94] use the Sharesort technique that does repeated merging with accompanying overhead.
Reference: [NV93] <author> M. H. Nodine and J. S. Vitter. </author> <title> Deterministic distribution sort in shared and distributed memory multiprocessors. </title> <booktitle> Proc. 5th Annual ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 120-129, </pages> <year> 1993. </year>
Reference-contexts: MAIN RESULTS external sorting on parallel disks <ref> [NV90, NV93] </ref>. In DSM, the disks are coordinated so that in each parallel read and write, the locations of the blocks accessed on each disk are the same, which has the logical effect of sorting with D 0 = 1 disk and block size B 0 = DB.
Reference: [PGK88] <author> D. A. Patterson, G. Gibson, and R. H. Katz. </author> <title> A case for redundant arrays of inexpensive disks (raid). </title> <booktitle> Proc. 1988 ACM-SIGMOD Conf. on Management of Data, </booktitle> <pages> pages 109-116, </pages> <year> 1988. </year>
Reference-contexts: One way to alleviate the effects of the I/O bottleneck is to use parallel disk systems <ref> [HGK + 94, PGK88, Uni89, GS84, Mag87] </ref>. Aggarwal and Vitter [AV88], generalizing initial work done by Floyd [Flo72] and Hong and Kung [HK81], laid the foundation for I/O algorithms by studying the I/O complexity of sorting and related problems.
Reference: [PSV94] <author> V. S. Pai, A. A. Schaffer, and P. J. Varman. </author> <title> Markov analysis of multiple-disk prefetching strategies for external merging. </title> <journal> Theoretical Computer Science, </journal> <volume> 128(2) </volume> <pages> 211-239, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Vitter and Shriver [VS94] give further intuition regarding the difficulty of mergesorting on parallel disks. In Greed Sort [NV90], the trick of "approximate merging" is used to circumvent this difficulty. Aggarwal and Plaxton [AP94] use the Sharesort technique that does repeated merging with accompanying overhead. Recently, Pai et al <ref> [PSV94] </ref> considered the average-case performance of a simple merging scheme for R = D sorted runs, one run on each disk. They use an approximate model of average-case inputs and require that the internal memory be sufficiently large. <p> Thus in this exposition we choose to overlook this issue and not apply the ceiling function, thus obtaining simplified expressions for I/O complexity. 5 The mergesort algorithm by Pai et al <ref> [PSV94] </ref> uses significantly more I/Os and internal memory. In their scheme, the internal memory size needs to be (D 2 B) to attain an efficient merging algorithm. The other aspect of our analysis deals with the practical merit of our SRM mergesort algorithm on existing parallel disk systems.
Reference: [RW94] <author> C. Ruemmler and J. Wilkes. </author> <title> An introduction to disk drive modeling. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 17-28, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: In this paper we adopt the simpler and more restrictive model in which D = D 0 and develop algorithms that are near-optimal even for this more restrictive model. A more detailed discussion of disk models and characteristics appears in <ref> [RW94] </ref>. The problem of external sorting in the D-disk model has been extensively studied. Algorithms have been developed that use an asymptotically optimal fi ( N DB log (M=B) ) number of I/O operations 1 , as well as doing an optimal amount of work in internal memory.
Reference: [Uni89] <institution> University of California at Berkeley. Massive Information Storage, Management, and Use (NSF Institutional Infrastructure Proposal), </institution> <month> January </month> <year> 1989. </year> <note> Technical Report No. UCB/CSD 89/493. </note>
Reference-contexts: One way to alleviate the effects of the I/O bottleneck is to use parallel disk systems <ref> [HGK + 94, PGK88, Uni89, GS84, Mag87] </ref>. Aggarwal and Vitter [AV88], generalizing initial work done by Floyd [Flo72] and Hong and Kung [HK81], laid the foundation for I/O algorithms by studying the I/O complexity of sorting and related problems.
Reference: [VF90] <author> J. S. Vitter and Ph. Flajolet. </author> <title> Average-case analysis of algorithms and data structures. </title> <editor> In Jan van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A: Algorithms and Complexity, chapter 9, </booktitle> <pages> pages 431-524. </pages> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference-contexts: This memory management scheme enables us to obtain a handle on SRM's performance. We are able to relate SRM's performance to a combinatorial problem we call the dependent maximum occupancy problem, which is a generalization of the classical maximum occupancy problem <ref> [KSC78, VF90] </ref>. Our main theorem below gives expressions for the I/O performance of SRM for three patterns of growth rate between the number M=B of blocks in internal memory and the number D of disks in the parallel disk system. <p> In the classical occupancy problem of parameters fN b ; Dg, N b balls are thrown into D bins independently each with uniform probability of 1=D of falling into any bin. We denote the asymptotically tight expression for the expectation of the maximum number of balls in any bin <ref> [VF90, KSC78] </ref> by C (N b ; D). In the dependent occupancy problem, we consider D bins but instead of balls, we consider C chains of balls, such that the total number of balls summed over the C chains, is N b .
Reference: [VS94] <author> J. S. Vitter and E. A. M. Shriver. </author> <title> Algorithms for parallel memory I: Two-level memories. </title> <journal> Algorithmica, </journal> <volume> 12(2-3):110-147, </volume> <year> 1994. </year>
Reference-contexts: The model they studied [AV88] considers an internal memory of size M and I/O reads or writes that each result in a transfer of D blocks, where each block is comprised of B contiguous records, from or to disks. Subsequently, Vitter and Shriver <ref> [VS94] </ref> considered a realistic D-disk two-level memory model in which secondary memory is partitioned into D physically distinct and independent disk drives or read-write heads that can simultaneously transmit a block of data, with the requirement that M 2DB. In the D-disk two-level memory hierarchy [VS94], there are two sources of <p> Subsequently, Vitter and Shriver <ref> [VS94] </ref> considered a realistic D-disk two-level memory model in which secondary memory is partitioned into D physically distinct and independent disk drives or read-write heads that can simultaneously transmit a block of data, with the requirement that M 2DB. In the D-disk two-level memory hierarchy [VS94], there are two sources of parallelism. First, as in traditional I/O systems, records are transferred concurrently in blocks of B contiguous records. <p> However, this memory management is difficult to achieve since the order in which the data in various disk blocks will participate in the merge process is input dependent and unknown. Vitter and Shriver <ref> [VS94] </ref> give further intuition regarding the difficulty of mergesorting on parallel disks. In Greed Sort [NV90], the trick of "approximate merging" is used to circumvent this difficulty. Aggarwal and Plaxton [AP94] use the Sharesort technique that does repeated merging with accompanying overhead. <p> In the the realistic case that D = O (B), SRM merges an optimal number (namely, R = fi (M=B)) of runs together at a time. For simplicity in the exposition, we assume that D = O (B). (We can use the partial striping technique of <ref> [VS94] </ref> to enforce the assumption if needed.) There are two aspects to our analysis of the algorithm|one theoretical and one practical.
Reference: [VV96] <author> D. E. Vengroff and J. S. Vitter. </author> <title> I/O-efficient computation: The tpie approach. </title> <booktitle> In Proceedings of the Goddard Conference on Mass Storage Systems and Technologies, </booktitle> <month> September </month> <year> 1996. </year>
Reference-contexts: However, DSM is often more efficient in practice than the previously developed asymptotically optimal sorting algorithms, since the latter algorithms have larger overheads <ref> [VV96] </ref>. When the amount M of internal memory is small or the number D of disks is large, our SRM method is clearly superior to DSM.
References-found: 21

