URL: http://www.cs.berkeley.edu/~christos/ir.ps
Refering-URL: http://www.cs.berkeley.edu/~christos/
Root-URL: 
Email: Email: christos@cs.berkeley.edu.  Email: pragh@almaden.ibm.com.  Email: htamaki@cs.meiji.ac.jp.  Email: vempala@math.mit.edu.  
Title: Latent Semantic Indexing: A Probabilistic Analysis  
Author: Christos H. Papadimitriou Prabhakar Raghavan Hisao Tamaki Santosh Vempala 
Address: Berkeley, Berkeley, CA 94720.  650 Harry Road, San Jose, CA 95120.  Tokyo, Japan.  Cambridge, MA 02139.  
Affiliation: Computer Science Division, U. C.  IBM Almaden Research Center,  Computer Science Department, Meiji University,  Department of Mathematics, M.I.T.,  
Date: November 14, 1997  
Abstract: Latent semantic indexing (LSI) is an information retrieval technique based on the spectral analysis of the term-document matrix, whose empirical success had heretofore been without rigorous prediction and explanation. We prove that, under certain conditions, LSI does succeed in capturing the underlying semantics of the corpus and achieves improved retrieval performance. We also propose the technique of random projection as a way of speeding up LSI. We complement our theorems with encouraging experimental results. We also argue that our results may be viewed in a more general framework, as a theoretical basis for the use of spectral methods in a wider class of applications such as collaborative filtering. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. W. Berry, S. T. Dumais, and G. W. O'Brien. </author> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> 37(4), </volume> <year> 1995, </year> <pages> 573-595, </pages> <year> 1995. </year>
Reference-contexts: Queries are also projected and processed in this low-dimensional space. This results not only in great savings in storage and query time (at the expense of some considerable preprocessing), but also, according to empirical evidence reported in the literature, to improved information retrieval <ref> [1, 7, 8] </ref>. Indeed, it has been repeatedly reported that LSI outperforms, with regard to precision and recall in standard collections and query workloads, more conventional vector-based methods, and that it does address the problems of polysemy and synonymy.
Reference: [2] <author> M. W. Berry, T. Do, G. W. O'Brien, V. Krishna, and S. Varadhan. </author> <note> SVDPACKC (Version 1.0) User's Guide. </note> <institution> University of Tennessee, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Results from experiments with different size-parameters are also similar in spirit. In this and the other experiments reported here, we used SVDPACKC <ref> [2] </ref> for singular value decomposition. Synonymy We end this section with a brief discussion of synonymy in the context of LSI. Let us consider a simple model in which two terms have identical co-occurrences (this generalizes synonymy, as it also applies to pairs of terms such as supply-demand and war-peace).
Reference: [3] <author> E. Brewer. </author> <type> Invited talk, </type> <year> 1997 </year> <month> PODS/SIGMOD, </month> <year> 1997. </year>
Reference-contexts: Another important change is the dramatic expansion of the scope of information retrieval, with the advent of multimedia, the internet, and globalized information; database concepts and some theory have started to find fertile ground there (see for example <ref> [9, 3, 17] </ref>, as well a record number of information retrieval papers in the 1997 SIGMOD Proceedings) 1 Secondly, the techniques employed in information retrieval have become more mathematical and sophisticated, more plausibly amenable to analytical treatment.
Reference: [4] <author> S. Chakrabarti, B. Dom, D. Gibson, J. Kleinberg, P. Raghavan and S. Rajagopalan. </author> <title> "Mining information networks through spectral methods". In preparation, </title> <institution> IBM Almaden Research Center, </institution> <year> 1997. </year>
Reference-contexts: Finally, the advent of the web has enabled powerful new applications such as collaborative filtering (also known as target or personalized reccomendation systems) that can be tackled using techniques inspired in part by information retrieval <ref> [4] </ref>; more on this in Section 6. IR and LSI The complexity of information retrieval is best illustrated by the two nasty classical problems of synonymy (missing documents with references to "automobile" when querying on "car") and polysemy (retrieving documents about the internet when querying on "surfing"). <p> Furthermore, spectral analysis of a similar graph-theoretic model of the world-wide web has been shown experimentally to succeed in identifying topics and to substantially increase precision and recall in web searches [17], as well as in databases of law decisions, service logs and patents <ref> [4] </ref>. Finally, it is becoming clear that spectral techniques and their theoretical analysis may prove to be key methodologies in many other domains of current interest, such as data mining (using spectral techniques to discover correlations in relational databases [13]) and collaborative filtering (personalizing subscriber preferences and interests) [4]. <p> and patents <ref> [4] </ref>. Finally, it is becoming clear that spectral techniques and their theoretical analysis may prove to be key methodologies in many other domains of current interest, such as data mining (using spectral techniques to discover correlations in relational databases [13]) and collaborative filtering (personalizing subscriber preferences and interests) [4]. The rows and columns of A could in general be, instead of terms and documents, consumers and products, viewers and movies, or components and systems. We conclude this section with a brief description of a promising alternative, graph-theoretic, 12 corpus model.
Reference: [5] <author> D.M. Cvetkovic, M. Doob, and H. Sachs, </author> <title> Spectra of Graphs, </title> <publisher> Academic Press, </publisher> <year> 1979. </year>
Reference-contexts: Intuitively, the matrix B T i B i is essentially the adjacency matrix of a random bipartite multigraph and then, from the standard theory of spectra of graphs <ref> [5] </ref>, we have that 0 i = i ! 0 with probability 1 as t ! 0 and jC i j ! 1.
Reference: [6] <author> S. Deerwester, S. T. Dumais, T.K. Landauer, G.W. Furnas, and R.A. Harshman. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the Society for Information Science, </journal> <volume> 41(6), </volume> <pages> 391-407, </pages> <year> 1990. </year>
Reference-contexts: Latent Semantic Indexing <ref> [6] </ref> is an information retrieval method which attempts to capture this hidden structure by using techniques from linear algebra.
Reference: [7] <author> S.T. Dumais, G.W. Furnas, T.K. Landauer, and S. Deerwester. </author> <title> Using latent semantic analysis to improve information retrieval. </title> <booktitle> In Proceedings of CHI'88: Conference on Human Factors in Computing, </booktitle> <address> New York: </address> <publisher> ACM, </publisher> <pages> 281-285, </pages> <year> 1988. </year>
Reference-contexts: Queries are also projected and processed in this low-dimensional space. This results not only in great savings in storage and query time (at the expense of some considerable preprocessing), but also, according to empirical evidence reported in the literature, to improved information retrieval <ref> [1, 7, 8] </ref>. Indeed, it has been repeatedly reported that LSI outperforms, with regard to precision and recall in standard collections and query workloads, more conventional vector-based methods, and that it does address the problems of polysemy and synonymy.
Reference: [8] <author> S.T. Dumais. </author> <title> Improving the retrieval of information from external sources. Behavior Research Methods, </title> <journal> Instruments and Computers, </journal> <volume> 23(2), </volume> <pages> 229-236, </pages> <year> 1991. </year>
Reference-contexts: Queries are also projected and processed in this low-dimensional space. This results not only in great savings in storage and query time (at the expense of some considerable preprocessing), but also, according to empirical evidence reported in the literature, to improved information retrieval <ref> [1, 7, 8] </ref>. Indeed, it has been repeatedly reported that LSI outperforms, with regard to precision and recall in standard collections and query workloads, more conventional vector-based methods, and that it does address the problems of polysemy and synonymy.
Reference: [9] <author> R. Fagin. </author> <title> Combining fuzzy information from multiple sources. </title> <booktitle> Proc. 1996 PODS, </booktitle> <pages> pp. 216-223, </pages> <year> 1996. </year>
Reference-contexts: Another important change is the dramatic expansion of the scope of information retrieval, with the advent of multimedia, the internet, and globalized information; database concepts and some theory have started to find fertile ground there (see for example <ref> [9, 3, 17] </ref>, as well a record number of information retrieval papers in the 1997 SIGMOD Proceedings) 1 Secondly, the techniques employed in information retrieval have become more mathematical and sophisticated, more plausibly amenable to analytical treatment.
Reference: [10] <author> C. Faloutsos and D. Oard. </author> <title> A survey of information retrieval and filtering methods. </title> <type> Technical Report, </type> <institution> University of Maryland Computer Science Dept., College Park, MD. </institution>
Reference-contexts: Evidently, very little theory can be built on this basis. However, an increasing volume of applied database research (see for instance the 1997 SIGMOD Proceedings) is focusing on methods for managing text. (See <ref> [10, 20] </ref> for surveys on information retrieval, including discussions of the technique that is the focus of this paper, from database and theoretical points of view, respectively; [21, 22] are classical texts on the subject of information retrieval.) However, the field of information retrieval has been evolving in directions that bring
Reference: [11] <author> P. Frankl and H. Maehara. </author> <title> The Johnson-Lindenstrauss Lemma and the Sphericity of some graphs, </title> <journal> J. Comb. Theory B 44 (1988), </journal> <pages> 355-362. </pages>
Reference-contexts: This is exactly what one would expect from a method that claims to bring out the hidden semantics of the corpus. 9 5 LSI by random projection A result by Johnson and Lindenstrauss <ref> [11, 18] </ref> states that if points in a vector space are projected to a random subspace of suitably high dimension, then the distances between the points are approximately preserved. <p> Another way to view this result is that random projection gives us a fast way to approximate the eigenspace (eigenvalues, eigenvectors) of a matrix. We first state the Johnson-Lindenstrauss lemma. Lemma 2 (Johnson and Lindenstrauss, see <ref> [11, 18] </ref>.) Let v 2 R n be a unit vector, let H be a random l-dimensional subspace through the origin, and let the random variable X denote the square of the length of the projection of v onto H.
Reference: [12] <author> N. </author> <title> Fuhr "Probabilistic models of information retrieval," </title> <journal> Computer Journal, </journal> <volume> 35, 3, </volume> <pages> pp. 244-255, </pages> <year> 1992. </year>
Reference-contexts: It remains to be seen in what way it improves these retrieval capabilities. 3 The probabilistic corpus model There are many useful formal models of IR in the literature, and probability plays a major role in many of them |see for instance the surveys and comparisons in <ref> [12, 21, 23] </ref>. The approach in this body of work is to formulate information retrieval as a problem of learning the concept of "relevance" that relates documents and queries. The corpus and its correlations plays no central role. In contrast, our focus is on the probabilistic properties of the corpus.
Reference: [13] <author> D. Gibson, J.M. Kleinberg and P. Raghavan. </author> <title> "Using nonlinear dynamical systems to mine categorical data". </title> <note> Submitted for publication, </note> <year> 1997. </year>
Reference-contexts: Finally, it is becoming clear that spectral techniques and their theoretical analysis may prove to be key methodologies in many other domains of current interest, such as data mining (using spectral techniques to discover correlations in relational databases <ref> [13] </ref>) and collaborative filtering (personalizing subscriber preferences and interests) [4]. The rows and columns of A could in general be, instead of terms and documents, consumers and products, viewers and movies, or components and systems.
Reference: [14] <author> G. Golub and C. Reinsch. </author> <title> Handbook for matrix computation II, Linear Algebra. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: How good is this approximation? The following well-known theorem gives us some idea (the subscript F denotes the Frobenius norm). Theorem 1 (Eckart and Young, see <ref> [14] </ref>.) Among all n fi m matrices C of rank at most k, A k is the one that minimizes kA Ck 2 F = i;j (A i;j C i;j ) 2 .
Reference: [15] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> London, </address> <year> 1989. </year>
Reference-contexts: Then, U 0 k = U k R+G for some k fi k orthonormal matrix R and some n fi k matrix G with kGk 2 O (*). The proof of this lemma, given in the appendix, relies on a theorem of Stewart <ref> [15] </ref> about perturbing a symmetric matrix. Let C = (U; T ; D) be a corpus model. We call C pure if each document involves only a single topic.
Reference: [16] <author> W. Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables, </title> <journal> Journal of the American Statistical Association 58 13-30, </journal> <year> 1963. </year>
Reference-contexts: Then we can estimate, for each term, P s minfp s =2; p s *g with probability at least 1 1 2 t using the independence of the x j 's via a simple application of Chernoff-Hoeffding bound <ref> [16] </ref>. Using this we lower bound the weight of the cut (S; S): ( i2S X x j ) i2S s (minfp s =2; p s *g) 7 which is ( jSj jT i j ) with high probability by a second application of the Chernoff-Hoeffding bound.
Reference: [17] <author> J. Kleinberg, </author> <title> "Authoritative sources in a hyperlinked environment," </title> <booktitle> Proc. ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1998, </year> <note> to appear. Also available as IBM Research Report RJ 10076(91892) May 1997. </note>
Reference-contexts: Another important change is the dramatic expansion of the scope of information retrieval, with the advent of multimedia, the internet, and globalized information; database concepts and some theory have started to find fertile ground there (see for example <ref> [9, 3, 17] </ref>, as well a record number of information retrieval papers in the 1997 SIGMOD Proceedings) 1 Secondly, the techniques employed in information retrieval have become more mathematical and sophisticated, more plausibly amenable to analytical treatment. <p> There are several specific technical issues to be pursued. Can Theorem 2 be extended to a model where documents could belong to several topics, or to one where term occurrences are not independent? Also, does LSI address polysemy (as spectral techniques of slightly different kind to, see <ref> [17] </ref>)? We have seen some evidence that it does handle synonymy. Theory should ideally go beyond the ex post facto justification of methods and explanation of positive phenomena, it should point the way to new ways of exploiting them and improving them. <p> Spectral techniques are not confined to the vector-space model, neither to the strict context of information retrieval. Furthermore, spectral analysis of a similar graph-theoretic model of the world-wide web has been shown experimentally to succeed in identifying topics and to substantially increase precision and recall in web searches <ref> [17] </ref>, as well as in databases of law decisions, service logs and patents [4].
Reference: [18] <author> W. B. Johnson and J. Lindenstrauss. </author> <title> Extensions of Lipshitz mapping into Hilbert space, </title> <journal> Contemp. Math. </journal> <volume> 26 (1984), </volume> <pages> 189-206. </pages>
Reference-contexts: This is exactly what one would expect from a method that claims to bring out the hidden semantics of the corpus. 9 5 LSI by random projection A result by Johnson and Lindenstrauss <ref> [11, 18] </ref> states that if points in a vector space are projected to a random subspace of suitably high dimension, then the distances between the points are approximately preserved. <p> Another way to view this result is that random projection gives us a fast way to approximate the eigenspace (eigenvalues, eigenvectors) of a matrix. We first state the Johnson-Lindenstrauss lemma. Lemma 2 (Johnson and Lindenstrauss, see <ref> [11, 18] </ref>.) Let v 2 R n be a unit vector, let H be a random l-dimensional subspace through the origin, and let the random variable X denote the square of the length of the projection of v onto H.
Reference: [19] <author> M. Jerrum and A. Sinclair. </author> <title> Approximating the permanent. </title> <journal> Siam J. Comp. </journal> <volume> 18, </volume> <pages> pp. 1149-1178, </pages> <year> (1989). </year>
Reference-contexts: Below we give a formal justification of this by showing that a quantity that captures this property, the conductance <ref> [19] </ref> (equivalently, expansion) of B T i B i is high. The conductance of an undirected edge-weighted graph G = (V; E) is min P minfjSj; jSjg Let x 1 ; x 2 ; : : : ; x t be random documents picked from the topic T i . <p> Suppose that documents are nodes in a graph, and weights on the edges capture conceptual proximity of two documents (for example, this distance matrix could be derived from, or in fact coincide with, AA T ). Then a topic is defined implicitly as a subgraph with high conductance <ref> [19] </ref>, a concept of connectivity which seems very appropriate in this context.
Reference: [20] <author> P. Raghavan. </author> <title> Information retrieval algorithms: a survey. </title> <booktitle> Proceedings of the ACM Symposium on Discrete Algorithms, </booktitle> <year> 1997. </year>
Reference-contexts: Evidently, very little theory can be built on this basis. However, an increasing volume of applied database research (see for instance the 1997 SIGMOD Proceedings) is focusing on methods for managing text. (See <ref> [10, 20] </ref> for surveys on information retrieval, including discussions of the technique that is the focus of this paper, from database and theoretical points of view, respectively; [21, 22] are classical texts on the subject of information retrieval.) However, the field of information retrieval has been evolving in directions that bring
Reference: [21] <author> C. J. </author> <title> van Rijsbergen Information Retrieval Butterworths, </title> <address> London 1979. </address>
Reference-contexts: However, an increasing volume of applied database research (see for instance the 1997 SIGMOD Proceedings) is focusing on methods for managing text. (See [10, 20] for surveys on information retrieval, including discussions of the technique that is the focus of this paper, from database and theoretical points of view, respectively; <ref> [21, 22] </ref> are classical texts on the subject of information retrieval.) However, the field of information retrieval has been evolving in directions that bring it closer to databases. Information retrieval systems are increasingly being built on relational (or object-relational) database systems, rather than on flat text and index files. <p> It remains to be seen in what way it improves these retrieval capabilities. 3 The probabilistic corpus model There are many useful formal models of IR in the literature, and probability plays a major role in many of them |see for instance the surveys and comparisons in <ref> [12, 21, 23] </ref>. The approach in this body of work is to formulate information retrieval as a problem of learning the concept of "relevance" that relates documents and queries. The corpus and its correlations plays no central role. In contrast, our focus is on the probabilistic properties of the corpus.
Reference: [22] <author> G. Salton and M. McGill. </author> <title> Introduction to modern information retrieval. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: However, an increasing volume of applied database research (see for instance the 1997 SIGMOD Proceedings) is focusing on methods for managing text. (See [10, 20] for surveys on information retrieval, including discussions of the technique that is the focus of this paper, from database and theoretical points of view, respectively; <ref> [21, 22] </ref> are classical texts on the subject of information retrieval.) However, the field of information retrieval has been evolving in directions that bring it closer to databases. Information retrieval systems are increasingly being built on relational (or object-relational) database systems, rather than on flat text and index files.
Reference: [23] <author> H. R. Turtle and W. B. </author> <title> Croft "A comparison of text retrieval methods," </title> <journal> The Computer Journal, </journal> <volume> 35, 3, </volume> <pages> pp. 279-289, </pages> <year> 1992. </year>
Reference-contexts: It remains to be seen in what way it improves these retrieval capabilities. 3 The probabilistic corpus model There are many useful formal models of IR in the literature, and probability plays a major role in many of them |see for instance the surveys and comparisons in <ref> [12, 21, 23] </ref>. The approach in this body of work is to formulate information retrieval as a problem of learning the concept of "relevance" that relates documents and queries. The corpus and its correlations plays no central role. In contrast, our focus is on the probabilistic properties of the corpus.

References-found: 23

