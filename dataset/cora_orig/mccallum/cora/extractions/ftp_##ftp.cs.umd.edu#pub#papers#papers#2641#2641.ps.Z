URL: ftp://ftp.cs.umd.edu/pub/papers/papers/2641/2641.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Lanczos and Linear Systems  
Author: G. W. Stewart 
Note: 20742. This work was supported in part by the Air Force Office of Scientific Research under Contract AFOSR-87-0188.  
Address: College Park, MD  
Affiliation: Department of Computer Science and Institute for Advanced Computer Studies, University of Maryland,  
Date: March 1991 CS-TR 2641  
Pubnum: UMIACS-TR-91-47  
Abstract: Lanczos's major contributions to the numerical solution of linear equations are contained in two papers: "An Iteration Method for the Solution of the Eigenvalue Problem of Linear Differential and Integral Operators" and "Solutions of Linear Equations by Minimized Iterations," the second of which contains the method of conjugate gradients. In this note we retrace Lanczos's journey from Krylov sequences to conjugate gradients. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Greenbaum. </author> <title> Behavior of slightly perturbed lanczos conjugate gradient recurrences. </title> <journal> Linear Algebra and Its Applications, </journal> <volume> 113 </volume> <pages> 7-63, </pages> <year> 1989. </year>
Reference-contexts: It is interesting that they approach the method from different angles. We have already followed Lanczos's journey from Krylov sequences to conjugate gradients. Stiefel 5 The most complete error analysis of the method to date is that of Greenbaum <ref> [1] </ref>. Lanczos and Linear Systems 7 derives the method from minimizing properties of polynomial iterations. Hestenes gives no indication of how he discovered the method; he simply writes down the formulas and proves that they work.
Reference: [2] <author> M. R. Hestenes. </author> <title> Iterative methods for solving linear equations. </title> <type> Report 52-9, </type> <institution> NAML, </institution> <year> 1951. </year> <journal> Reprinted in Journal of Opeimization Theory and Applications, </journal> <volume> Volume 11, </volume> <pages> pp. 323-334, </pages> <year> 1973. </year>
Reference-contexts: There remains the question of who should be credited with the discovery of the method of conjugate gradients. Lanczos, Hestenes <ref> [2] </ref>, and Stiefel [9] each published the method at about the same time in single-authored papers. It is interesting that they approach the method from different angles. We have already followed Lanczos's journey from Krylov sequences to conjugate gradients.
Reference: [3] <author> M. R. Hestenes and E. </author> <title> Stiefel. Methods of conjugate gradients for solving linear systems. </title> <journal> Journal of Research of the National Bureau of Standards, </journal> <volume> 49 </volume> <pages> 409-436, </pages> <year> 1952. </year>
Reference-contexts: initial approximation y 0 = 0. 4 Lanczos goes on to point out that the process can be started with any residual and that the sequence of residual vectors is given by r k+1 = k ^p k+1 . 4 The name "conjugate gradients" is due to Hestenes and Stiefel <ref> [3] </ref>. The gradients are the residuals, which are actual gradients of an associated minimization problem.
Reference: [4] <author> A. S. </author> <title> Householder. The Theory of Matrices in Numerical Analysis. </title> <publisher> Dover Publishing, </publisher> <address> New York, </address> <year> 1964. </year> <note> Originally published by Ginn Blaisdell. </note>
Reference-contexts: In a footnote, Lanczos cites Krylov's paper and adds, "On the basis of the reviews of these papers in the Zentralblatt, the author believes that the two methods coincide only in the point of departure. The author has not, however, read these Russian papers." Householder <ref> [4] </ref> gives a detailed treatment of Krylov's method and its relation to other algorithms for the algebraic eigenvalue problem. 2 The conventions introduced here will be used throughout this survey.
Reference: [5] <author> A. N. Krylov. O Cislennom resenii uravnenija, kotorym v techniceskih vo-prasah opredeljajutsja castoy malyh kolebani material'nyh. Izv. </author> <title> Adad. </title> <journal> Nauk SSSR otd. Mat. </journal> <volume> Estest., </volume> <pages> pages 491-539, </pages> <year> 1931. </year> <note> Cited in [4]. </note>
Reference-contexts: ; 1) T such that K m g = 0: To say the same thing, if we set g (x) = g 0 + g 1 x + + g m1 x m1 + x m ; (3) then the vector ^g = g (A)b is zero. 2 1 Krylov <ref> [5] </ref> used this sequence, as Lanczos will do, to compute the minimal polynomial of A. In a footnote, Lanczos cites Krylov's paper and adds, "On the basis of the reviews of these papers in the Zentralblatt, the author believes that the two methods coincide only in the point of departure.
Reference: [6] <author> C. </author> <title> Lanczos. An iteration method for the solution of the eigenvalue problem of linear differential and integral operators. </title> <journal> Journal of Research of the National Bureau of Standards, </journal> <volume> 45 </volume> <pages> 255-282, </pages> <year> 1950. </year> <title> 8 Lanczos and Linear Systems </title>
Reference-contexts: Introduction Lanczos's major contributions to the numerical solution of linear equations are contained in two papers: "An Iteration Method for the Solution of the Eigenvalue Problem of Linear Differential and Integral Operators" and "Solutions of Linear Equations by Minimized Iterations" <ref> [6, 7] </ref>. The first paper is the usual reference the Lanczos algorithm, and as such it has been surveyed elsewhere in this collection.
Reference: [7] <author> C. </author> <title> Lanczos. Solutions of linear equations by minimized iterations. </title> <journal> Journal of Research of the National Bureau of Standards, </journal> <volume> 49 </volume> <pages> 33-53, </pages> <year> 1952. </year>
Reference-contexts: Introduction Lanczos's major contributions to the numerical solution of linear equations are contained in two papers: "An Iteration Method for the Solution of the Eigenvalue Problem of Linear Differential and Integral Operators" and "Solutions of Linear Equations by Minimized Iterations" <ref> [6, 7] </ref>. The first paper is the usual reference the Lanczos algorithm, and as such it has been surveyed elsewhere in this collection.
Reference: [8] <author> N. Levinson. </author> <title> The Wiener RMS (Root Mean Square) error criterion in filter design and prediction. </title> <journal> Journal of Mathematics and Physics, </journal> <volume> 25 </volume> <pages> 261-278, </pages> <year> 1946. </year>
Reference-contexts: After repeating the characterization of the polynomial p k (x) as minimizing the length of ^p k , he goes on to assert that among all monic polynomials 3 Lanczos's progressive algorithm also provides the wherewithal for a fast Hankel solver in the spirit of Levinson's fast Toeplitz solver <ref> [8] </ref>. It is unlikely that Lanczos knew of Levinson's work. Although Lanczos is not one for searching the literature, he is punctilious about acknowledging priorities. Lanczos and Linear Systems 5 q k (x) minimizes the length of ^q k =q k (0).
Reference: [9] <author> E. </author> <title> Stiefel. Uber einige Methoden der Relaxationsrechnung. </title> <journal> Zeitschrift fur angewandte Mathematik und Physik, </journal> <volume> 3 </volume> <pages> 1-33, </pages> <year> 1952. </year>
Reference-contexts: There remains the question of who should be credited with the discovery of the method of conjugate gradients. Lanczos, Hestenes [2], and Stiefel <ref> [9] </ref> each published the method at about the same time in single-authored papers. It is interesting that they approach the method from different angles. We have already followed Lanczos's journey from Krylov sequences to conjugate gradients.
References-found: 9

