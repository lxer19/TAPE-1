URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/reports/98/tr1372.ps.Z
Refering-URL: http://www.cs.wisc.edu/~fischer/ftp/pub/tech-reports/reports/98/
Root-URL: http://www.cs.wisc.edu
Email: qjacobso@ece.wisc.edu cao@cs.wisc.edu  
Title: Potential and Limits of Web Prefetching Between Low-Bandwidth Clients and Proxies  
Author: Quinn Jacobson Pei Cao 
Affiliation: Department of Electrical and Computer Engineering Department of Computer Science University of Wisconsin-Madison University of Wisconsin-Madison  
Abstract: The majority of the Internet population access the World Wide Web via dial-up modem connections. Studies have shown that limited modem bandwidth is the main contributing factor to the Web access latency perceived by the users. In this paper, we investigate one approach to reduce the user-perceived latency: pre-pushing from the proxy to the browsers. The approach takes advantage of the idle time between user Web requests and uses prediction algorithms to predict what document a user might reference next. It then relies on proxies to send ("push") the documents to the user. Using existing modem Web access traces, we evaluate the potential of the technique at reducing user latency, examine the design of prediction algorithms and measure their accuracy as well as overhead, and evaluate the latency reduction of pre-push schemes using the algorithms. Our results show that with perfect predictors, proxy-based Web pre-pushing with a 256K-byte pre-push buffer at the browser side can reduce latency by over 20%. Our results also show that prefix-based prediction algorithms works well for predicting user behavior. Proxy-based web pre-pushing driven by real prediction mechanisms can reduce latency by nearly 10%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. C. Bell, J. C. Cleary, and I. H. Witten. </author> <title> Text Compression. </title> <publisher> Prentice Hall Advanced Reference Series, </publisher> <year> 1990. </year>
Reference-contexts: The actual prediction algorithm is based on the Prediction-by-Partial-Match (PPM) data compressor <ref> [1, 6] </ref>. A m-th order predictor uses the context of the past m references to predict the next set of URLs.
Reference: [2] <author> Azer Bestavros and Carlos Cunha. </author> <title> Server-initiated document dissemination for the www. IEEE Data Engineering Bulletin, </title> <month> September </month> <year> 1996. </year> <pages> Page 15 </pages>
Reference-contexts: The prediction algorithm they used is also based on the PPM data compressor, but with order of 1. The study shows that prefetching from Web servers to individual clients can reduce client latency by as high as 45%, at the expense of doubling the network traffic. Bestavros el al <ref> [2] </ref> presents a model for speculative dissemination of World Wide Web documents. The work shows that reference patterns observed at a Web server can be used as an effective source of information to drive prefetching. They reached similar conclusions in [17].
Reference: [3] <author> Pei Cao, Edward W. Felten, Anna R. Karlin, and Kai Li. </author> <title> A study of integrated prefetching and caching strategies. </title> <booktitle> In Proc. 1995 ACM SIG-METRICS, </booktitle> <pages> pages 188-197, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: In this paper, we use recent, large-scale modem client traces to explore the design space and evaluate the tradeoffs in pre-pushing. Outside the Web contexts, prefetching has been studied extensively in file systems and memory systems. Several studies investigate application-controlled prefetching in the file system contexts <ref> [3, 4, 13, 19] </ref>, where an application gives prediction of what it might access next. Other studies also investigate the speculative approach to file prefetching [9, 20, 18]. The prediction algorithms used there are also derived from Page 2 data compressors, while the compressor and the pa-rameters vary.
Reference: [4] <author> Pei Cao, Edward W. Felten, Anna R. Karlin, and Kai Li. </author> <title> Implementation and performance of integrated application-controlled file caching, prefetching and disk scheduling. </title> <booktitle> In TOCS, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: In this paper, we use recent, large-scale modem client traces to explore the design space and evaluate the tradeoffs in pre-pushing. Outside the Web contexts, prefetching has been studied extensively in file systems and memory systems. Several studies investigate application-controlled prefetching in the file system contexts <ref> [3, 4, 13, 19] </ref>, where an application gives prediction of what it might access next. Other studies also investigate the speculative approach to file prefetching [9, 20, 18]. The prediction algorithms used there are also derived from Page 2 data compressors, while the compressor and the pa-rameters vary.
Reference: [5] <author> Carlos Cunha and Carlos F. B. Jaccoud. </author> <title> Determining www user's next access and its application to pre-fetching. </title> <booktitle> In Proceedings of ISCC'97: The Second IEEE Symposium on Computers and Communications, </booktitle> <month> July </month> <year> 1997. </year> <note> URL: http://www.cs.bu.edu/students/alumni/ carro/Home.html. </note>
Reference-contexts: Bestavros el al [2] presents a model for speculative dissemination of World Wide Web documents. The work shows that reference patterns observed at a Web server can be used as an effective source of information to drive prefetching. They reached similar conclusions in [17]. Finally, Cuhan and Bestavros <ref> [5] </ref> uses a collection of Web client traces and studies how effectively a user's future Web accesses can be predicted from his or her past Web accesses. They show that a number of models work well and can be used in prefetching. <p> Thus, it is difficult to estimate from Web server traces the true user idle time that can be exploited to push documents to users over the modem lines. Lastly, Cuhan and Bestavros <ref> [5] </ref> uses the trace of client requests that are not filtered by the browser cache; in practice, a proxy only sees requests after they are filtered by the browser cache.
Reference: [6] <author> Kenneth M. Curewitz, P. Krishnan, and Jef-frey Scott Vitter. </author> <title> Practical prefetching via data compression. </title> <booktitle> In Proceedings of SIGMOD'93, </booktitle> <pages> pages 257-266, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The prediction algorithms used there are also derived from Page 2 data compressors, while the compressor and the pa-rameters vary. Most of the algorithms, including the one studies here, are inspired by an important early study by Krishnan and Vitter demonstrating the relationship between data compression and prediction <ref> [21, 6] </ref>. Their study shows that if the source behind a reference string can be modelled as an M-order Markov source, then the compressor-driven prediction algorithm will converge to the optimal predictor given a long enough reference string. <p> Their study shows that if the source behind a reference string can be modelled as an M-order Markov source, then the compressor-driven prediction algorithm will converge to the optimal predictor given a long enough reference string. Our algorithms extend the algorithms described in [21] and <ref> [6] </ref> by considering prediction depth of more than 1, that is, not only predicting which documents might be used next, but also predicting which documents will be used after the immediate next ones. <p> The actual prediction algorithm is based on the Prediction-by-Partial-Match (PPM) data compressor <ref> [1, 6] </ref>. A m-th order predictor uses the context of the past m references to predict the next set of URLs. <p> Their impacts on the predictor's performance are quantitatively analyzed in Section 6. The algorithm is similar and yet different from previous proposed prefetching algorithms. Papadu-manta and Mogul [17] studies the same algorithm, but with m always equal to 1. Pk and Vitter <ref> [6] </ref> also studies the PPM-based algorithm as one of their prefetching algorithms, but always have l equal to 1. The reason for considering algorithms with m &gt; 1 is that more context can help improve the accuracy of the prediction.
Reference: [7] <author> Fred Douglis and Gideon Glass. </author> <title> Performance of caching proxies. </title> <type> Private Communication, </type> <year> 1997. </year>
Reference-contexts: For example, studies using commercial ISP traces shows that even if a Web proxy caching system is employed and has perfect performance, the client latency can only be reduced by 3% to 4% <ref> [7] </ref>. Other studies have shown that when image distillation techniques is used to reduce the document sizes, the client latency can be improved by a factor of 4 to 5, further demonstrating that the slow modem links are the bottleneck. <p> assume that the transfer of the document from the Internet to the head-end machine and the transfer of the document from the head-end machine to the modem client can be overlapped, and since packets typically arrive at the modem bank faster than they can be sent to the modem clients <ref> [7] </ref>, we estimate the client-seen latency to be (T 1 T 0 ) plus the document size divided by the modem bandwidth 1 . Except in our analysis of user idle times, we always assume the existence of a proxy between the modem clients and the Internet.
Reference: [8] <author> Steven Gribble and Eric Brewer. </author> <note> Ucb home IP HTTP traces. Available at http://www.cs.berkeley.edu/ grib-ble/traces/index.html, </note> <month> June </month> <year> 1997. </year>
Reference-contexts: For a detailed description of the trace see <ref> [8] </ref>. The traces are collected at a packet sniffing machine placed at the head-end of the UC Berkeley HomeIP modem bank. Thus, it cor Page 3 responds to requests that would be seen by a proxy between the modem clients and the Internet.
Reference: [9] <author> Jim Griffioen and Randy Appleton. </author> <title> Reducing file system latency using a predictive approach. </title> <booktitle> In Conference Proceedings of the USENIX Summer 1994 Technical Conference, </booktitle> <pages> pages 197-208, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: Several studies investigate application-controlled prefetching in the file system contexts [3, 4, 13, 19], where an application gives prediction of what it might access next. Other studies also investigate the speculative approach to file prefetching <ref> [9, 20, 18] </ref>. The prediction algorithms used there are also derived from Page 2 data compressors, while the compressor and the pa-rameters vary.
Reference: [10] <author> Wcol Group. </author> <note> Www collector the prefetch-ing proxy server for www. http://shika.aist-nara.ac.jp/products/wcol/wcol.html, 1997. </note>
Reference-contexts: Thus, it is not clear whether the conclusions from this particular study are applicable to our investigation. As proxies become more widespread, a number of studies investigated prefetching between Web servers and proxies <ref> [14, 16, 10, 11] </ref>. <p> The technique requires cooperation from the Web servers. Furthermore, since the server traces do not capture each user's idle time, the study did not evaluate the client latency reduction resulted from the technique. Wcol <ref> [10] </ref> is a proxy software that prefetches documents from the Web servers. It parses HTML files and prefetches links and embedded images. The proxy, however, does not pre-push the documents to the clients.
Reference: [11] <author> James Gwertzman and Margo Seltzer. </author> <title> The case for geographical push-caching. </title> <booktitle> In Proceedings of the Fifth Workshop on Hot Topics in Operating Systems, </booktitle> <year> 1995. </year>
Reference-contexts: Thus, it is not clear whether the conclusions from this particular study are applicable to our investigation. As proxies become more widespread, a number of studies investigated prefetching between Web servers and proxies <ref> [14, 16, 10, 11] </ref>. <p> Wcol [10] is a proxy software that prefetches documents from the Web servers. It parses HTML files and prefetches links and embedded images. The proxy, however, does not pre-push the documents to the clients. Finally, Gwertzman and Seltzer <ref> [11, 12] </ref> discussed a technique called Geographical Push-Caching where a Web server selectively sends its documents to the caches that are closest to its clients. The focus of the study is on deriving reasonably accurate network topology information and using the information to select caches.
Reference: [12] <author> James Gwertzman and Margo Seltzer. </author> <title> An analysis of geographical push-caching. </title> <address> http://www.eecs.harvard.edu/vino/web/server.cache/ icdcs.ps, </address> <year> 1997. </year>
Reference-contexts: Wcol [10] is a proxy software that prefetches documents from the Web servers. It parses HTML files and prefetches links and embedded images. The proxy, however, does not pre-push the documents to the clients. Finally, Gwertzman and Seltzer <ref> [11, 12] </ref> discussed a technique called Geographical Push-Caching where a Web server selectively sends its documents to the caches that are closest to its clients. The focus of the study is on deriving reasonably accurate network topology information and using the information to select caches.
Reference: [13] <author> Tracy Kimbrel, Pei Cao, Edward W. Felten, Anna R. Karlin, and Kai Li. </author> <title> Integrated parallel prefetching and caching. </title> <type> Technical Report TR-502-95, </type> <institution> Princeton University, Department of Computer Science, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: In this paper, we use recent, large-scale modem client traces to explore the design space and evaluate the tradeoffs in pre-pushing. Outside the Web contexts, prefetching has been studied extensively in file systems and memory systems. Several studies investigate application-controlled prefetching in the file system contexts <ref> [3, 4, 13, 19] </ref>, where an application gives prediction of what it might access next. Other studies also investigate the speculative approach to file prefetching [9, 20, 18]. The prediction algorithms used there are also derived from Page 2 data compressors, while the compressor and the pa-rameters vary.
Reference: [14] <author> Thomas M. Kroeger, Darrell D. E. Long, and Jeffrey C. Mogul. </author> <title> Exploring the bounds of web latency reduction from caching and prefetching. </title> <booktitle> In Proceedings of USENIX Symposium on Internet Technology and Systems, </booktitle> <month> December </month> <year> 1997. </year>
Reference-contexts: Thus, it is not clear whether the conclusions from this particular study are applicable to our investigation. As proxies become more widespread, a number of studies investigated prefetching between Web servers and proxies <ref> [14, 16, 10, 11] </ref>. <p> Thus, it is not clear whether the conclusions from this particular study are applicable to our investigation. As proxies become more widespread, a number of studies investigated prefetching between Web servers and proxies [14, 16, 10, 11]. Kroeger el al <ref> [14] </ref> investigates the performance limits of prefetching between Web servers and proxies, and shows that combining perfect caching and perfect prefetching at the proxies can at best reduce the client latency by 60% for relatively high-bandwidth clients (i.e. those that accesses the Web through a LAN instead of modems).
Reference: [15] <author> Tong Sau Loon and Vaduvur Bharghavan. </author> <title> Alleviating the latency and bandwidth problems in www browsing. </title> <booktitle> In Proceedings of the 1997 USENIX Symposium on Internet Technology and Systems, </booktitle> <month> December </month> <year> 1997. </year> <note> URL: http://timely.crhc.uiuc.edu/. </note>
Reference-contexts: The focus of the study is on deriving reasonably accurate network topology information and using the information to select caches. In terms of prefetching between proxies and clients, Loon and Bharghavan <ref> [15] </ref> has proposed the same idea as we described here. However, the study did not address the potential and limits of the approach, or which prediction algorithms are the most successful. Rather, the study presents a design and an implementation of a proxy system that performs the prepush-ing.
Reference: [16] <author> Evangelos P. Markatos and Catherine E. Chron-aki. </author> <title> A top-10 approach to prefetching on the web. </title> <type> Technical report, Technical Report No. 173, </type> <address> ICS-FORTH, Heraklion, Crete, Greece, </address> <month> August </month> <year> 1996. </year> <note> URL http://www.ics.forth.gr/proj/arch-vlsi/www.html. </note>
Reference-contexts: Thus, it is not clear whether the conclusions from this particular study are applicable to our investigation. As proxies become more widespread, a number of studies investigated prefetching between Web servers and proxies <ref> [14, 16, 10, 11] </ref>. <p> Markatos and Chronaki <ref> [16] </ref> proposes that Web servers regularly push their most popular documents to Web proxies, which then push those documents to the clients. They evaluate the performance of the strategy using several Web server traces and find that the technique can anticipate more than 40% of a client's request.
Reference: [17] <author> Venkata N. Padmanabhan and Jeffrey C. Mogul. </author> <title> Using predictive prefetching to improve world wide web latency. </title> <journal> ACM SIGCOMM Computer Communication Review, </journal> <month> July </month> <year> 1996. </year>
Reference-contexts: Early studies have mostly focused on prefetch-ing between Web servers and clients, since relatively few proxies were deployed then. Padmanabhan and Mogul <ref> [17] </ref> analyze the latency reduction and network traffic of prefetching using Web server traces and trace-driven simulation. The prediction algorithm they used is also based on the PPM data compressor, but with order of 1. <p> Bestavros el al [2] presents a model for speculative dissemination of World Wide Web documents. The work shows that reference patterns observed at a Web server can be used as an effective source of information to drive prefetching. They reached similar conclusions in <ref> [17] </ref>. Finally, Cuhan and Bestavros [5] uses a collection of Web client traces and studies how effectively a user's future Web accesses can be predicted from his or her past Web accesses. They show that a number of models work well and can be used in prefetching. <p> We call it the "search depth." * t: the threshold used to weed out candidates. Their impacts on the predictor's performance are quantitatively analyzed in Section 6. The algorithm is similar and yet different from previous proposed prefetching algorithms. Papadu-manta and Mogul <ref> [17] </ref> studies the same algorithm, but with m always equal to 1. Pk and Vitter [6] also studies the PPM-based algorithm as one of their prefetching algorithms, but always have l equal to 1.
Reference: [18] <author> Mark Palmer and Stanley B. Zdonik. </author> <title> Fido: A cache that learns to fetch. </title> <booktitle> In Proceedings of the 17th International Conference on Very Large Data Bases, </booktitle> <pages> pages 255-264, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Several studies investigate application-controlled prefetching in the file system contexts [3, 4, 13, 19], where an application gives prediction of what it might access next. Other studies also investigate the speculative approach to file prefetching <ref> [9, 20, 18] </ref>. The prediction algorithms used there are also derived from Page 2 data compressors, while the compressor and the pa-rameters vary.
Reference: [19] <author> R. Hugo Patterson, Garth A. Gibson, Eka Gint-ing, Daniel Stodolsky, and Jim Zelenka. </author> <title> Informed prefetching and caching. </title> <booktitle> In Proceedings of 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: In this paper, we use recent, large-scale modem client traces to explore the design space and evaluate the tradeoffs in pre-pushing. Outside the Web contexts, prefetching has been studied extensively in file systems and memory systems. Several studies investigate application-controlled prefetching in the file system contexts <ref> [3, 4, 13, 19] </ref>, where an application gives prediction of what it might access next. Other studies also investigate the speculative approach to file prefetching [9, 20, 18]. The prediction algorithms used there are also derived from Page 2 data compressors, while the compressor and the pa-rameters vary.
Reference: [20] <author> Carl D. Tait and Dan Duchamp. </author> <title> Detection and exploitation of file working sets. </title> <type> Technical Report CUCS-050-90, </type> <institution> Computer Science Department, Columbia University, </institution> <year> 1990. </year>
Reference-contexts: Several studies investigate application-controlled prefetching in the file system contexts [3, 4, 13, 19], where an application gives prediction of what it might access next. Other studies also investigate the speculative approach to file prefetching <ref> [9, 20, 18] </ref>. The prediction algorithms used there are also derived from Page 2 data compressors, while the compressor and the pa-rameters vary.
Reference: [21] <author> Jeffrey Scott Vitter and P. Krishnan. </author> <title> Optimal prefetching via data compression. </title> <booktitle> In Proceedings of the 32nd Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 121-130, </pages> <note> also appears as Brown Univ. Tech. Rep. No. CS-91-46, October 1991. Page 16 </note>
Reference-contexts: The prediction algorithms used there are also derived from Page 2 data compressors, while the compressor and the pa-rameters vary. Most of the algorithms, including the one studies here, are inspired by an important early study by Krishnan and Vitter demonstrating the relationship between data compression and prediction <ref> [21, 6] </ref>. Their study shows that if the source behind a reference string can be modelled as an M-order Markov source, then the compressor-driven prediction algorithm will converge to the optimal predictor given a long enough reference string. <p> Their study shows that if the source behind a reference string can be modelled as an M-order Markov source, then the compressor-driven prediction algorithm will converge to the optimal predictor given a long enough reference string. Our algorithms extend the algorithms described in <ref> [21] </ref> and [6] by considering prediction depth of more than 1, that is, not only predicting which documents might be used next, but also predicting which documents will be used after the immediate next ones.
References-found: 21

