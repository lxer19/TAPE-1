URL: http://robotics.stanford.edu/users/sahami/papers-dir/kdd96-disc.ps
Refering-URL: http://robotics.stanford.edu/users/sahami/papers.html
Root-URL: http://www.cs.stanford.edu
Email: ronnyk@sgi.com  sahami@cs.stanford.edu  
Title: Error-Based and Entropy-Based Discretization of Continuous Features  
Author: Ron Kohavi 
Address: 2011 N. Shoreline Blvd Mountain View, CA 94043-1389  Gates Building 1A, Room 126  Stanford, CA 94305-9010  
Affiliation: Data Mining and Visualization Silicon Graphics, Inc.  Mehran Sahami  Computer Science Department Stanford University  
Abstract: We present a comparison of error-based and entropy-based methods for discretization of continuous features. Our study includes both an extensive empirical comparison as well as an analysis of scenarios where error minimization may be an inappropriate discretization criterion. We present a discretization method based on the C4.5 decision tree algorithm and compare it to an existing entropy-based discretization algorithm, which employs the Minimum Description Length Principle, and a recently proposed error-based technique. We evaluate these discretization methods with respect to C4.5 and Naive-Bayesian classifiers on datasets from the UCI repository and analyze the computational complexity of each method. Our results indicate that the entropy-based MDL heuristic outperforms error minimization on average. We then analyze the shortcomings of error-based approaches in comparison to entropy-based methods. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Apte, C., and Hong, S. </author> <year> 1996. </year> <title> Predicting equity returns from security data. In Advances in Knowledge Discovery and Data Mining. </title> <publisher> AAAI Press and the MIT Press. </publisher> <pages> chapter 22, 541-569. </pages>
Reference: <author> Auer, P.; Holte, R.; and Maass, W. </author> <year> 1995. </year> <title> Theory and applications of agnostic pac-learning with small decision trees. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth Int. Conference. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Hence, we still use an entropy-based metric (gain-ratio), but use a different criterion for the number of intervals, i.e., determined by pruning as opposed to Fayyad and Irani's stopping criteria. The error-based discretization we compare has been proposed by Maass (1994) and used in the T2 algorithm <ref> (Auer, Holte, & Maass 1995) </ref>. Given a number of intervals, k, the method constructs the optimal discretization of a continuous feature with respect to classification error in polynomial time. <p> We refer to this algorithm as ErrorMin . The maximum number of intervals k is a user-set parameter. This method has been implemented as part of the T2 induction algorithm <ref> (Auer, Holte, & Maass 1995) </ref> which induces one or two level decision trees. T2 cir-cumvented the difficulty of providing a good justification for the value of k by simply setting k to be the number of classes plus one.
Reference: <author> Catlett, J. </author> <year> 1991. </year> <title> On changing continuous attributes into ordered discrete attributes. </title> <editor> In Kodratoff, Y., ed., </editor> <booktitle> Proceedings of the European Working Session on Learning, </booktitle> <pages> 164-178. </pages> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Cost, S., and Salzberg, S. </author> <year> 1993. </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <booktitle> Machine Learning 10(1) </booktitle> <pages> 57-78. </pages>
Reference: <author> Dougherty, J.; Kohavi, R.; and Sahami, M. </author> <year> 1995. </year> <title> Supervised and unsupervised discretization of continuous features. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth Int. Conference, </booktitle> <pages> 194-202. </pages> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: The reason for this problem is that ErrorMin will never create two adjacent intervals with the same majority class. We explore the impact of this phenomenon in an artificial example. As reported in previous work <ref> (Dougherty, Kohavi, & Sahami 1995) </ref>, any form of discretization produced large improvements over the Naive-Bayesian algorithm with the normality assumption for continuous variables.
Reference: <author> Fayyad, U. M., and Irani, K. B. </author> <year> 1993. </year> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the 13th Int. Joint Conference on Artificial Intelligence, </booktitle> <pages> 1022-1027. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Good, I. J. </author> <year> 1965. </year> <title> The Estimation of Probabilities: An Essay on Modern Bayesian Methods. </title> <publisher> M.I.T. Press. </publisher>
Reference-contexts: Dougherty, Kohavi, & Sahami (1995) provided an initial comparison of uniform binning, the discretiza-tion method proposed by Holte (1993), and an entropy based method proposed by Fayyad & Irani (1993) using two induction algorithms: C4.5 (Quinlan 1993) and a Naive-Bayesian classifier <ref> (Good 1965) </ref>. Since that study reported that the entropy-based discretization method was the most promising method, we compare that method to two other methods: C4.5-based dis-cretization and error-based discretization.
Reference: <author> Holte, R. C. </author> <year> 1993. </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine Learning 11 </booktitle> <pages> 63-90. </pages>
Reference: <author> John, G.; Kohavi, R.; and Pfleger, K. </author> <year> 1994. </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh Int. Conference, </booktitle> <pages> 121-129. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Dynamic methods conduct a search through the space of possible k values for all features simultaneously, thereby capturing interdependencies in feature discretization. During the course of this study, we looked at dynamic versions of several discretization methods, using the wrapper approach <ref> (John, Kohavi, & Pfleger 1994) </ref> as a means of searching through the space of the number of discretization intervals for all variables simultaneously. We found no significant improvement in employing dynamic discretization over its static counterpart.
Reference: <author> Kohavi, R.; John, G.; Long, R.; Manley, D.; and Pfleger, K. </author> <year> 1994. </year> <title> MLC++: A machine learning library in C++. </title> <booktitle> In Tools with Artificial Intelligence, </booktitle> <pages> 740-743. </pages> <publisher> IEEE Computer Society Press. </publisher> <address> http://www.sgi.com/Technology/mlc. </address>
Reference-contexts: The probabilities for nominal features are estimated using counts and a Gaussian distribution is assumed for continuous features (in the undiscretized cases). The Naive-Bayesian classifier used in our experiments is the one implemented in MLC ++ <ref> (Kohavi et al. 1994) </ref>. Discretization Algorithms We focus on two discretization methods using entropy and a recently developed error-based discretization method. These methods are described below. A comprehensive review of the existing discretization literature is found in Dougherty, Kohavi, & Sahami (1995). <p> Dynamic methods conduct a search through the space of possible k values for all features simultaneously, thereby capturing interdependencies in feature discretization. During the course of this study, we looked at dynamic versions of several discretization methods, using the wrapper approach <ref> (John, Kohavi, & Pfleger 1994) </ref> as a means of searching through the space of the number of discretization intervals for all variables simultaneously. We found no significant improvement in employing dynamic discretization over its static counterpart.
Reference: <author> Maass, W. </author> <year> 1994. </year> <title> Efficient agnostic PAC-learning with simple hypotheses. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> 67-75. </pages>
Reference: <author> Murphy, P. M., and Aha, D. W. </author> <year> 1996. </year> <note> UCI repository of machine learning databases. http://www.ics.uci.edu/~mlearn. </note>
Reference-contexts: Results We begin by presenting the experimental results and then analyze them. Empirical Findings Table 1 shows the datasets we chose for our comparison. We chose 17 datasets from the UCI repository <ref> (Murphy & Aha 1996) </ref> such that each had at least one continuous feature. We used 10-fold cross-validation to determine error rates for the application of each discretization and induction method pair to each dataset.
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <address> Los Altos, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Dougherty, Kohavi, & Sahami (1995) provided an initial comparison of uniform binning, the discretiza-tion method proposed by Holte (1993), and an entropy based method proposed by Fayyad & Irani (1993) using two induction algorithms: C4.5 <ref> (Quinlan 1993) </ref> and a Naive-Bayesian classifier (Good 1965). Since that study reported that the entropy-based discretization method was the most promising method, we compare that method to two other methods: C4.5-based dis-cretization and error-based discretization. <p> In light of our empirical findings, we analyze situations in which error-based discretization may be inappropriate. Methods We briefly describe the induction algorithms and dis-cretization methods we compare. Induction Algorithms In our experimental study, we test different discretiza-tion methods as applied to C4.5 and Naive-Bayesian classifiers. C4.5 <ref> (Quinlan 1993) </ref> is a state-of-the-art top-down decision tree induction algorithm. When we discretize features, we declare them nominal, thus C4.5 does a multi-way split on all possible thresholds. The Naive-Bayesian induction algorithm computes the posterior probability of the classes given the data, assuming independence between the features for each class.
References-found: 13

