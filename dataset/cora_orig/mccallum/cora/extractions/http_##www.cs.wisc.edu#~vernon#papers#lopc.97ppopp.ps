URL: http://www.cs.wisc.edu/~vernon/papers/lopc.97ppopp.ps
Refering-URL: http://www.cs.wisc.edu/~vernon/papers.html
Root-URL: 
Email: fmfrank,agarwalg@lcs.mit.edu  vernon@cs.wisc.edu  
Title: LoPC: Modeling Contention in Parallel Algorithms  
Author: Matthew I. Frank Anant Agarwal Mary K. Vernon 
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  Computer Sciences Department University of Wisconsin-Madison  
Abstract: Parallel algorithm designers need computational models that take first order system costs into account, but are also simple enough to use in practice. This paper introduces the LoPC model, which is inspired by the LogP model but accounts for contention for message processing resources in parallel algorithms on a multiprocessor or network of workstations. LoPC takes the L, o and P parameters directly from the LogP model and uses them to predict the cost of contention, C. This paper defines the LoPC model and derives the general form of the model for parallel applications that communicate via active messages. Model modifications for systems that implement coherent shared memory abstractions are also discussed. We carry out the analysis for two important classes of applications that have irregular communication. In the case of parallel applications with homogeneous all-to-any communication, such as sparse matrix computations, the analysis yields a simple rule of thumb and insight into contention costs. In the case of parallel client-server algorithms, the LoPC analysis provides a simple and accurate calculation of the optimal allocation of nodes between clients and servers. The LoPC estimates for these applications are shown to be accurate when compared against event driven simulation and against a sparse matrix computation on the MIT Alewife multiprocessor. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Vikram S. Adve. </author> <title> Analyzing the Behavior and Performance of Parallel Programs. </title> <type> PhD thesis, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: They find, as do the studies with more relaxed resource constraints, that for several algorithms contention is bounded by a constant factor. One goal of the LoPC model is to correctly predict the constant factors that are of concern to applications programmers. Finally, two recent works <ref> [1, 16] </ref> propose analytic models to predict parallel program performance, including communication and contention costs. The thesis work by Adve [1] models a parallel program with a deterministic task graph and uses mean value analysis to predict mean task execution times, including contention. <p> One goal of the LoPC model is to correctly predict the constant factors that are of concern to applications programmers. Finally, two recent works [1, 16] propose analytic models to predict parallel program performance, including communication and contention costs. The thesis work by Adve <ref> [1] </ref> models a parallel program with a deterministic task graph and uses mean value analysis to predict mean task execution times, including contention.
Reference: [2] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk L. Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Kenneth Macken-zie, and Donald Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: If additional requests arrive while the atomic handler is running, they are queued in a hardware FIFO. When the first handler finishes, the processor is again interrupted for each additional message in the queue. The Alewife machine <ref> [2] </ref>, used to validate the analyses in this paper, provides hardware network input queues that can hold up to 512 bytes of data. <p> A number of existing machines provide efficient support for communication with active messages <ref> [2, 9, 14, 20, 24, 28, 32] </ref>. We make two further assumptions in the LoPC model that appear to result in very little loss in accuracy yet great gain in simplicity. First, we assume that the hardware message buffers at the nodes are infinitely large.
Reference: [3] <author> Albert Alexandrov, Mihai Ionescu, Klaus E. Schauser, and Chris Scheiman. LogGP: </author> <title> Incorporating Long Messages into the LogP Model. </title> <booktitle> In Proceedings of the SPAA'95, </booktitle> <pages> pages 95-105, </pages> <address> Santa Bar-bara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The model can be generalized in fairly straightforward ways to include multiple CPUs per node, message co-processors, and asynchronous send operations. Alexandrov et al have extended LogP to model much longer messages <ref> [3] </ref> and we believe that such extensions can also be incorporated into LoPC. Validating such models is the subject of future research.
Reference: [4] <author> Yonathan Bard. </author> <title> Some Extensions to Multiclass Queueing Network Analysis. </title> <editor> In M. Arato, A. Butrimenko, and E. Gelenbe, editors, </editor> <booktitle> Performance of Computer Systems. </booktitle> <publisher> North-Holland, </publisher> <year> 1979. </year>
Reference-contexts: To remove this recursion on the number of customers in the system, we use an approximation to the arrival theorem, due to Bard <ref> [4] </ref>, which assumes that the average queue length at request arrival time is approximately equal to the average queue length. This approximation will overestimate the average observed queue lengths and response times, and underestimate throughput. However, the error diminishes asymptotically as the number of threads, N , increases.
Reference: [5] <author> Eric A. Brewer and Bradley C. Kuszmaul. </author> <title> How to Get Good Performance from the CM-5 Data Network. </title> <booktitle> In Proceedings of the 1994 International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Regular communication patterns can also demonstrate contention. Brewer and Kuszmal <ref> [5] </ref> measured the communication costs in regular, all-to-all communication patterns carefully designed on the CM-5 to interleave message arrivals across processors so as to avoid contention. They discovered that the pattern quickly became virtually random, largely due to small variances in the interconnect.
Reference: [6] <author> Raymond M. Bryant, Anthony E. Krzesinski, M. Seetha Lakshmi, and K. Mani Chandy. </author> <title> The MVA Priority Approximation. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(4) </volume> <pages> 335-359, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: In addition, once the computation thread does resume, additional request messages may arrive, interrupting the computation thread. We compute the average total time for the thread computation including handler interrupts, R w , by using the BKT preempt-resume priority approximation <ref> [6, 7, 13] </ref>: R w = 1 U q We use the BKT approximation because it is more accurate than the simpler shadow server approximation, and it yields a simpler result than the Chandy-Lakshmi priority approximation [6, 13]. <p> computation including handler interrupts, R w , by using the BKT preempt-resume priority approximation [6, 7, 13]: R w = 1 U q We use the BKT approximation because it is more accurate than the simpler shadow server approximation, and it yields a simpler result than the Chandy-Lakshmi priority approximation <ref> [6, 13] </ref>. The set of equations (1) through (7) completely characterize the execution time of a compute/request cycle, including contention for processor resources, for algorithms with homogeneous all-to-any communication.
Reference: [7] <author> Raymond M. Bryant, Anthony E. Krzesinski, and P. Teunissen. </author> <title> The MVA Pre-empt Resume Priority Approximation. </title> <booktitle> In Proceedings of the 1983 ACM Sigmetrics Conference, </booktitle> <pages> pages 12-27, </pages> <year> 1983. </year>
Reference-contexts: In addition, once the computation thread does resume, additional request messages may arrive, interrupting the computation thread. We compute the average total time for the thread computation including handler interrupts, R w , by using the BKT preempt-resume priority approximation <ref> [6, 7, 13] </ref>: R w = 1 U q We use the BKT approximation because it is more accurate than the simpler shadow server approximation, and it yields a simpler result than the Chandy-Lakshmi priority approximation [6, 13].
Reference: [8] <author> Men-Chow Chiang and Guri Sohi. </author> <title> Evaluating Design Choices for Shared Bus Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(3) </volume> <pages> 297-317, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In this case we simply model R w as W . One other change is required if the shared memory algorithm includes non-blocking (or asynchronous) communication. Approximate MVA techniques for modeling such asynchronous communication [17] are well-known and have been employed in validated AMVA models of shared memory architectures <ref> [8, 21, 31] </ref>. Otherwise, the shared-memory model for the specified class of algorithms is the same as the message-passing model.
Reference: [9] <author> Derek Chiou, Boon S. Ang, Arvind, Michael J. Beckerle, Andy Boughton, Robert Greiner, James E. Hicks, and James C. Hoe. StarT-NG: </author> <title> Delivering Seamless Parallel Computing. </title> <booktitle> In Proceedings of the EURO-PAR '95, </booktitle> <pages> pages 101-116, </pages> <address> Stockholm, Sweden, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: A number of existing machines provide efficient support for communication with active messages <ref> [2, 9, 14, 20, 24, 28, 32] </ref>. We make two further assumptions in the LoPC model that appear to result in very little loss in accuracy yet great gain in simplicity. First, we assume that the hardware message buffers at the nodes are infinitely large.
Reference: [10] <author> David Culler, Richard Karp, David Patterson, Abhijit Sahay, Klaus Erik Schauser, Eunice Santos, Ramesh Subramonian, and Thorsten von Eicken. </author> <title> LogP: Towards a Realistic Model of Parallel Computation. </title> <booktitle> In Proceedings of the 4th Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 1-12, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Coherent shared-memory systems also often exhibit irregular communication because the home-node for each coherence unit is found using a simple hash function. The LogP model <ref> [10] </ref> has been successful at accurately modeling and optimizing tightly synchronized algorithms with regular, ordered communication patterns on active-message based systems. <p> The model can be extended to include analysis of contention in the network, as noted in Section 3.2. However, a number of researchers have found that for many real applications contention in current interconnection networks accounts for only a minimal portion of total runtime <ref> [10, 18, 26] </ref>. Furthermore, for the algorithms investigated in this paper we found that network contention is insignificant.
Reference: [11] <author> Andrea C. Dusseau, David E. Culler, Klaus Erik Schauser, and Richard P. Martin. </author> <title> Fast Parallel Sorting Under LogP: Experience with the CM-5. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 7(8) </volume> <pages> 791-805, </pages> <year> 1996. </year>
Reference-contexts: For example, Dusseau et al used LogP to analyze a variety of sorting algorithms with irregular communication patterns <ref> [11] </ref>. They found that some of their models underestimated execution time and attributed the difference to contention costs. <p> Lewandowski [22] successfully used LogP to analyze a parallel branch and bound algorithm with a relatively small amount of communication relative to processing. Dusseau et al <ref> [11] </ref> compared LogP analyses of a variety of parallel sorting algorithms with implementations of those algorithms running on a CM-5 with Active Messages. For those algorithms with irregular communication patterns, radix sort and sample sort, they found that their LogP models underestimate communication costs and attribute the difference to contention.
Reference: [12] <author> Cynthia Dwork, Maurice Herlihy, and Orli Waarts. </author> <title> Contention in Shared Memory Algorithms. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 174-183, </pages> <address> San Diego, California, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: They also find that as message overhead increases application runtime increases by a factor greater than predicted by a simple contention free model. Contention has also been studied in a more formal framework. For example, Dwork et al <ref> [12] </ref> and Gibbons et al [15] have extended the PRAM model, traditionally used for parallel algorithm complexity analysis, to account for contention.
Reference: [13] <author> Derek L. Eager and John N. Lipscomb. </author> <title> The AMVA Priority Approximation. Performance Evaluation, </title> <booktitle> 8(3) </booktitle> <pages> 173-193, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: In addition, once the computation thread does resume, additional request messages may arrive, interrupting the computation thread. We compute the average total time for the thread computation including handler interrupts, R w , by using the BKT preempt-resume priority approximation <ref> [6, 7, 13] </ref>: R w = 1 U q We use the BKT approximation because it is more accurate than the simpler shadow server approximation, and it yields a simpler result than the Chandy-Lakshmi priority approximation [6, 13]. <p> computation including handler interrupts, R w , by using the BKT preempt-resume priority approximation [6, 7, 13]: R w = 1 U q We use the BKT approximation because it is more accurate than the simpler shadow server approximation, and it yields a simpler result than the Chandy-Lakshmi priority approximation <ref> [6, 13] </ref>. The set of equations (1) through (7) completely characterize the execution time of a compute/request cycle, including contention for processor resources, for algorithms with homogeneous all-to-any communication.
Reference: [14] <author> Marco Fillo, Stephen W. Keckler, William J. Dally, Nicholas P. Carter, Andrew Chang, Yevgeny Gurevich, and Whay S. Lee. </author> <title> The M-Machine Multicomputer. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <address> Ann Arbor, MI, </address> <year> 1995. </year>
Reference-contexts: A number of existing machines provide efficient support for communication with active messages <ref> [2, 9, 14, 20, 24, 28, 32] </ref>. We make two further assumptions in the LoPC model that appear to result in very little loss in accuracy yet great gain in simplicity. First, we assume that the hardware message buffers at the nodes are infinitely large.
Reference: [15] <author> Phillip B. Gibbons, Yossi Matias, and Vijaya Ramachandran. </author> <title> The QRQW PRAM: Accounting for Contention in Parallel Algorithms. </title> <booktitle> In Proceedings of the Fifth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 638-648, </pages> <address> Philadelphia, Pennsylvania, </address> <month> 23-25 January </month> <year> 1994. </year> <note> To appear SIAM Journal on Computing. </note>
Reference-contexts: They also find that as message overhead increases application runtime increases by a factor greater than predicted by a simple contention free model. Contention has also been studied in a more formal framework. For example, Dwork et al [12] and Gibbons et al <ref> [15] </ref> have extended the PRAM model, traditionally used for parallel algorithm complexity analysis, to account for contention. However this extended PRAM model assumes that each individual memory location has a queue associated with it, whereas LoPC makes the more realistic assumption that queues are associated with message processing resources.
Reference: [16] <author> Karim Harzallah and Kenneth C. Sevcik. </author> <title> Predicting Application Behavior in Large Scale Shared-memory Multiprocessors. </title> <booktitle> In Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: They find, as do the studies with more relaxed resource constraints, that for several algorithms contention is bounded by a constant factor. One goal of the LoPC model is to correctly predict the constant factors that are of concern to applications programmers. Finally, two recent works <ref> [1, 16] </ref> propose analytic models to predict parallel program performance, including communication and contention costs. The thesis work by Adve [1] models a parallel program with a deterministic task graph and uses mean value analysis to predict mean task execution times, including contention. <p> The thesis work by Adve [1] models a parallel program with a deterministic task graph and uses mean value analysis to predict mean task execution times, including contention. The work by Harzallah and Sevcik <ref> [16] </ref> breaks parallel program execution into phases and uses mean value analysis to predict the execution time of each phase, including contention.
Reference: [17] <author> Philip Heidelberger and Kishor S. Trivedi. </author> <title> Queueing Network Models for Parallel Processing with Asynchronous Tasks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-31(11):1099-1109, </volume> <month> November </month> <year> 1982. </year>
Reference-contexts: In this case we simply model R w as W . One other change is required if the shared memory algorithm includes non-blocking (or asynchronous) communication. Approximate MVA techniques for modeling such asynchronous communication <ref> [17] </ref> are well-known and have been employed in validated AMVA models of shared memory architectures [8, 21, 31]. Otherwise, the shared-memory model for the specified class of algorithms is the same as the message-passing model. <p> Ongoing work with LoPC includes analysis of further parallel applications and classes of applications, as well as extending the model, using a technique pioneered by Heidelberger and Trivedi <ref> [17] </ref>, to model non-blocking requests such as those that occur in shared memory systems. With this extension we plan to use LoPC to evaluate cost-performance tradeoffs between shared-memory and message-passing communication primitives.
Reference: [18] <author> Chris Holt, Mark Heinrich, Jaswinder Pal Singh, Edward Rothberg, and John Hennessy. </author> <title> The Effects of Latency, Occupancy, and Bandwidth in Distributed Shared Memory Multiprocessors. </title> <type> Technical Report CSL-TR-95-660, </type> <institution> Stanford Computer Systems Laboratory, </institution> <month> Jan-uary </month> <year> 1995. </year>
Reference-contexts: For example, Dusseau et al used LogP to analyze a variety of sorting algorithms with irregular communication patterns [11]. They found that some of their models underestimated execution time and attributed the difference to contention costs. Furthermore, Holt et al <ref> [18] </ref> used LogP as a framework for an experimental study of contention in memory controllers for shared memory; for a variety of SPLASH benchmark applications and a variety of controller speeds and network latencies they find that contention in the memory controller dominates the costs of handler service time and network <p> The model can be extended to include analysis of contention in the network, as noted in Section 3.2. However, a number of researchers have found that for many real applications contention in current interconnection networks accounts for only a minimal portion of total runtime <ref> [10, 18, 26] </ref>. Furthermore, for the algorithms investigated in this paper we found that network contention is insignificant. <p> For those algorithms with irregular communication patterns, radix sort and sample sort, they found that their LogP models underestimate communication costs and attribute the difference to contention. Holt et al <ref> [18] </ref> have performed an empirical study of the sensitivity of several of the SPLASH benchmarks, running on coherent shared memory machines, to the L, o and P parameters of the LogP model. They found that application performance is highly dependent on the cost of contention in the message coprocessor.
Reference: [19] <author> S.S. Lavenberg and M. Reiser. </author> <title> Stationary State Probabilities of Arrival Instants for Closed Queueing Networks with Multiple Types of Customers. </title> <journal> Journal of Applied Probability, </journal> <month> December </month> <year> 1980. </year>
Reference-contexts: Terms related to request handlers have subscript q; terms related to reply handlers have subscript y. to find the average number of messages waiting for service at each node and to compute the total system throughput. The cornerstone of Mean Value Analysis, the Arrival Theorem <ref> [19, 29] </ref>, states that for a broad class of queueing networks the average queue length observed by an arriving customer is equal to the average steady state queue length in a network with the arriving customer removed.
Reference: [20] <author> Charles E. Leiserson, Zahi S. Abuhamdeh, David C. Douglas, Carl R. Feynman, Mahesh N. Ganmukhi, Jeffrey V. Hill, Daniel Hillis, Bradley C. Kuszmal, Margaret A. St. Pierre, David S. Wells, Mon-ica C. Wong, Shaw-Wen Yang, and Robert Zak. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <journal> The Journal of Parallel and Distributed Computing, </journal> <volume> 33(2) </volume> <pages> 145-158, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: A number of existing machines provide efficient support for communication with active messages <ref> [2, 9, 14, 20, 24, 28, 32] </ref>. We make two further assumptions in the LoPC model that appear to result in very little loss in accuracy yet great gain in simplicity. First, we assume that the hardware message buffers at the nodes are infinitely large.
Reference: [21] <author> Scott Leutenegger and Mary K. Vernon. </author> <title> A Mean Value Performance Analysis of a New Multiprocessor Architecture. </title> <booktitle> In Proceedings of the 1988 ACM SIGMETRICS Conference on Measurment and Modeling of Computer Systems, </booktitle> <pages> pages 167-176, </pages> <address> Santa Fe, New Mexico, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: In this case we simply model R w as W . One other change is required if the shared memory algorithm includes non-blocking (or asynchronous) communication. Approximate MVA techniques for modeling such asynchronous communication [17] are well-known and have been employed in validated AMVA models of shared memory architectures <ref> [8, 21, 31] </ref>. Otherwise, the shared-memory model for the specified class of algorithms is the same as the message-passing model.
Reference: [22] <author> Gary Lewandowski. </author> <title> LogP Analysis of Parallel Branch and Bound Communication. </title> <journal> Submitted to IEEE Transactions on Parallel and Distributed Systems, </journal> <month> April </month> <year> 1994. </year>
Reference-contexts: The contention-free analysis also underestimates the number of nodes that should be allocated to the servers for optimal throughput. 6 Related Work As noted in Section 1, several studies have used LogP as a framework for studying applications with asynchronous communication patterns. Lewandowski <ref> [22] </ref> successfully used LogP to analyze a parallel branch and bound algorithm with a relatively small amount of communication relative to processing. Dusseau et al [11] compared LogP analyses of a variety of parallel sorting algorithms with implementations of those algorithms running on a CM-5 with Active Messages.
Reference: [23] <author> Pangfeng Liu, William Aiello, and Sandeep Bhatt. </author> <title> An Atomic Model for Message-Passing. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 154-163, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: However this extended PRAM model assumes that each individual memory location has a queue associated with it, whereas LoPC makes the more realistic assumption that queues are associated with message processing resources. A study by Liu et al <ref> [23] </ref> models more restricted message passing systems in which there is only a single, finite length, queue per processor. They find, as do the studies with more relaxed resource constraints, that for several algorithms contention is bounded by a constant factor.
Reference: [24] <author> Kenneth Mackenzie, John Kubiatowicz, Matthew Frank, Walter Lee, Anant Agarwal, and M. Frans Kaashoek. UDM: </author> <title> User Direct Messaging for General-Purpose Multiprocessing. </title> <type> Technical Memo MIT-LCS-TM-556, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: A number of existing machines provide efficient support for communication with active messages <ref> [2, 9, 14, 20, 24, 28, 32] </ref>. We make two further assumptions in the LoPC model that appear to result in very little loss in accuracy yet great gain in simplicity. First, we assume that the hardware message buffers at the nodes are infinitely large.
Reference: [25] <author> Richard P. Martin, Amin M. Vahdat, David E. Culler, and Thomas E. Anderson. </author> <title> Effects of Communication Latency, Overhead, and Bandwidth in a Cluster Architecture. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: They found that application performance is highly dependent on the cost of contention in the message coprocessor. As in our study, they found that their applications were not sensitive to the g, (gap), parameter of the LogP model. A recent study by Martin et al <ref> [25] </ref> finds similar empirical results for a variety of fine grain message passing benchmarks running on a network of workstations. They also find that as message overhead increases application runtime increases by a factor greater than predicted by a simple contention free model.
Reference: [26] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM SIGMETRICS Conference on Measurment and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <address> Santa Clara, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The model can be extended to include analysis of contention in the network, as noted in Section 3.2. However, a number of researchers have found that for many real applications contention in current interconnection networks accounts for only a minimal portion of total runtime <ref> [10, 18, 26] </ref>. Furthermore, for the algorithms investigated in this paper we found that network contention is insignificant.
Reference: [27] <author> M. Reiser and S.S. Lavenberg. </author> <title> Mean Value Analysis of Closed Mul-tichain Queueing Networks. </title> <type> Report RC-7023, </type> <institution> IBM T.J. Watson Research Center, </institution> <month> March </month> <year> 1978. </year>
Reference-contexts: Thus, for many applications the service time distributions for handlers will be much closer to a constant distribution. This section discusses how to extend the model with an approximation, due to Reiser and Lavenberg <ref> [27] </ref>, to account for arbitrary handler service time distributions, with squared coefficient of variation given by C 2 For most systems it will be appropriate to assume either C 2 o = 0 or 2 When a message arrives at a given node, there is a probability that it will find
Reference: [28] <author> Steven L. Scott. </author> <title> Synchronization and Communication in the T3E Multiprocessor. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: The original LogP paper also notes that the model underestimates the cost of regular all-to-all communication on the CM-5 unless extra barriers are inserted to resynchronize the communication pattern. However, low-latency barriers like those on the CM-5 are very expensive relative to other hardware components <ref> [28] </ref>. Few, if any, current generation multiprocessors or NOWs implement this feature. The goal of this paper is to create a new model for analyzing parallel algorithms, LoPC, that provides algorithm running times that include accurate predictions of contention costs. <p> A number of existing machines provide efficient support for communication with active messages <ref> [2, 9, 14, 20, 24, 28, 32] </ref>. We make two further assumptions in the LoPC model that appear to result in very little loss in accuracy yet great gain in simplicity. First, we assume that the hardware message buffers at the nodes are infinitely large.
Reference: [29] <author> K. C. Sevcik and I. Mitrani. </author> <title> The Distribution of Queuing Network States at Input and Output Instants. </title> <journal> Journal of the ACM, </journal> <volume> 28(2) </volume> <pages> 358-371, </pages> <month> April </month> <year> 1981. </year>
Reference-contexts: Terms related to request handlers have subscript q; terms related to reply handlers have subscript y. to find the average number of messages waiting for service at each node and to compute the total system throughput. The cornerstone of Mean Value Analysis, the Arrival Theorem <ref> [19, 29] </ref>, states that for a broad class of queueing networks the average queue length observed by an arriving customer is equal to the average steady state queue length in a network with the arriving customer removed.
Reference: [30] <author> Leslie G. Valiant. </author> <title> A Bridging Model for Parallel Computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: The goal of this paper is to create a new model for analyzing parallel algorithms, LoPC, that provides algorithm running times that include accurate predictions of contention costs. LoPC is inspired by LogP and, like LogP, is motivated by Valiant's observation <ref> [30] </ref> that the parallel computing community requires models that accurately account for both important algorithmic operations and realistic costs for hardware primitives.
Reference: [31] <author> Mary K. Vernon, Edward D. Lazowska, and John Zahorjan. </author> <title> An Accurate and Efficient Performance Analysis Technique for Multiprocessor Snooping Cache-Consistency Protocols. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 308-315, </pages> <address> Honolulu, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: In this case we simply model R w as W . One other change is required if the shared memory algorithm includes non-blocking (or asynchronous) communication. Approximate MVA techniques for modeling such asynchronous communication [17] are well-known and have been employed in validated AMVA models of shared memory architectures <ref> [8, 21, 31] </ref>. Otherwise, the shared-memory model for the specified class of algorithms is the same as the message-passing model.
Reference: [32] <author> Thorsten von Eicken, Anindya Basu, Vineet Buch, and Werner Vogels. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 40-53, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: A number of existing machines provide efficient support for communication with active messages <ref> [2, 9, 14, 20, 24, 28, 32] </ref>. We make two further assumptions in the LoPC model that appear to result in very little loss in accuracy yet great gain in simplicity. First, we assume that the hardware message buffers at the nodes are infinitely large.
Reference: [33] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Shauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year> <note> ACM Sigarch. </note>
Reference-contexts: 1 Introduction Light-weight user-level message passing paradigms, like Active Messages <ref> [33] </ref>, are an increasingly popular tool for writing parallel applications. To design effective algorithms, programmers need a simple cost model that accurately reflects first-order system overheads. <p> The Alewife machine [2], used to validate the analyses in this paper, provides hardware network input queues that can hold up to 512 bytes of data. This type of communication model using messages, called Active Messages <ref> [33] </ref>, is general enough to implement more complex communication and synchronization protocols, which we believe makes it a good basis for algorithm analysis in modern parallel systems. A number of existing machines provide efficient support for communication with active messages [2, 9, 14, 20, 24, 28, 32].
References-found: 33

