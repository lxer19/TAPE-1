URL: http://www.cs.cornell.edu/Info/People/ronitt/PAP/learndist.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/ronitt/papers.html
Root-URL: http://www.cs.cornell.edu
Title: On the Learnability of Discrete Distributions (extended abstract)  
Author: Michael Kearns Yishay Mansour Dana Ron Ronitt Rubinfeld Robert E. Schapire Linda Sellie 
Affiliation: AT&T Bell Laboratories  Tel-Aviv University  Hebrew University  Cornell University  AT&T Bell Laboratories  University of Chicago  
Date: May, 1994.  
Note: Appearing in the Proceedings of the 26th Annual ACM Symposium on Theory of Computing,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Naoki Abe and Manfred K. Warmuth. </author> <title> On the computational complexity of approximating distributions by probabilistic automata. </title> <booktitle> Machine Learning, </booktitle> <address> 9(2-3):205-260, </address> <year> 1992. </year>
Reference-contexts: Our results highlight the importance of distinguishing between two rather different types of representations for a probability distribution D. The first representation, called an evaluator for D, takes as input any vector ~y 2 f0; 1g n , and outputs the real number D [~y] 2 <ref> [0; 1] </ref>, that is, the weight that ~y is given under D. The second and usually less demanding representation, called a generator for D, takes as input a string of truly random bits, and outputs a vector ~y 2 f0; 1g n that is distributed according to D. <p> If E is a circuit with n input bits, we say that E is an *-good evaluator for D if KL (DjjE) *, where KL (DjjE) denotes the Kullback-Leibler divergence of D and the distribution on f0; 1g n defined by the mapping E : f0; 1g n ! <ref> [0; 1] </ref>. We are now ready to define our learning protocol. Let D n be a distribution class. <p> with the linear dependencies of our hypothesis, the probability of generation is 1=2 ` , otherwise it is 0. 5 Learning Mixtures of Hamming Balls A Hamming ball distribution over f0; 1g n is defined by a center vector ~x 2 f0; 1g n and a corruption probability p 2 <ref> [0; 1] </ref>. The distribution h~x; pi generates a vector ~y 2 f0; 1g n in the following simple way: for each bit 1 i n, y i = x i with probability 1 p, and y i = x i with probability p. <p> Each distribution D 2 HB k n is parameterized by k triples D = (h~x 1 ; p 1 ; q 1 i; : : : ; h~x k ; p k ; q k i), where ~x i 2 f0; 1g n , p i 2 <ref> [0; 1] </ref>, and q i 2 [0; 1]. The q i are additionally constrained by P k i=1 q i = 1. <p> 2 HB k n is parameterized by k triples D = (h~x 1 ; p 1 ; q 1 i; : : : ; h~x k ; p k ; q k i), where ~x i 2 f0; 1g n , p i 2 <ref> [0; 1] </ref>, and q i 2 [0; 1]. The q i are additionally constrained by P k i=1 q i = 1. <p> At each step, an outgoing transition is chosen at random according to its associated probability, and the f0; 1g label of the chosen transition is the next output bit. The resulting distribution has both polynomial-size generators and evaluators. Abe and Warmuth <ref> [1] </ref> showed that it is hard in a representation dependent sense to learn a probabilistic automaton defined over a large alphabet. Here, we give evidence for the representation independent intractability of learning PFA n with an evaluator (even when the alphabet has cardinality two). <p> The construction is simple, 9 and based on quadratic residues. For any n, each distribu-tion in the class HC n will be defined by a tuple hp; q; r; zi. Here p and q are n=4-bit primes (let N = p q), r 2 <ref> [0; 1] </ref>, and z 2 Z fl N is any element such that z 6= x 2 mod N for all x 2 Z fl N (that is, z is a quadratic non-residue).
Reference: [2] <author> Shai Ben-David, Benny Chor, Oded Goldreich, and Michael Luby. </author> <title> On the theory of average case complexity. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 44(2) </volume> <pages> 193-219, </pages> <year> 1992. </year>
Reference-contexts: For our results it turns out to be crucial to distinguish between distributions that can be only generated efficiently, and distributions that can be both generated and evaluated efficiently. Similar distinctions have been made before in the context of average-case complexity <ref> [2, 13] </ref>. We now make these notions precise. We start by defining an efficient generator. Definition 1 Let D n be a class of distributions over f0; 1g n .
Reference: [3] <author> Avrim Blum, Merrick Furst, Michael Kearns, and Richard J. Lipton. </author> <title> Cryptographic primitives based on hard learning problems. </title> <booktitle> In Pre-Proceedings of CRYPTO '93, </booktitle> <pages> pages 24.1-24.10, </pages> <year> 1993. </year>
Reference-contexts: Thus we prove our theorem under the following conjecture, for which some evidence has been provided in recent papers <ref> [18, 3] </ref>. Conjecture 15 (Noisy Parity Assumption) There is a constant 0 &lt; &lt; 1 2 such that there is no efficient algorithm for learning parity functions under the uniform distribution in the PAC model with classification noise rate .
Reference: [4] <author> L. Blum, M. Blum, and M. Shub. </author> <title> A simple unpredictable pseudo-random number generator. </title> <journal> SIAM Journal on Computing, </journal> <volume> 15(2) </volume> <pages> 364-383, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: The generated output is (y; N ) 2 f0; 1g n . It is easy to verify that HC n has both polynomial-size generators and evaluators. Theorem 18 The class HC n is efficiently learnable with a generator, and under the Quadratic Residue Assumption <ref> [4] </ref> is not efficiently learnable with an evaluator. Proof: (Sketch) The hardness of learning with an evaluator is straightforward and omitted.
Reference: [5] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24(6) </volume> <pages> 377-380, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: This powerful principle goes by the name Occam's Razor, and it can be verified for many learning models, including our distribution learning model <ref> [5, 6, 21, 14] </ref>. In the distribution-free PAC model, the converse to Oc-cam's Razor can be shown to hold as well [11, 22].
Reference: [6] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: This powerful principle goes by the name Occam's Razor, and it can be verified for many learning models, including our distribution learning model <ref> [5, 6, 21, 14] </ref>. In the distribution-free PAC model, the converse to Oc-cam's Razor can be shown to hold as well [11, 22]. <p> are interesting only in the computationally bounded setting; without computational constraints, they hold trivially.) This should be contrasted with the fact that for many distributions, it is possible to prove an (1=*) lower bound on the number of examples any learning algorithm must see when learning under those specific distributions <ref> [6] </ref>.
Reference: [7] <author> V. Chvatal. </author> <title> A greedy heuristic for the set covering problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 4(3) </volume> <pages> 233-235, </pages> <year> 1979. </year>
Reference-contexts: By applying the greedy algorithm, we obtain a subcollection of at most k log m candidate centers that covers S <ref> [7] </ref>. Let us assume without loss of generality that this subcollection is simply f~x 0 1 ; : : : ; ~x 0 Our hypothesis distribution is this subcollection, with corruption probability p and uniform mixture coefficients, that is, q i = 1=(k log m).
Reference: [8] <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: One of its nicest properties is that it upper bounds other natural distance measures such as the L 1 distance: L 1 (D; ^ D) = ~y2f0;1g n Thus it can be shown <ref> [8] </ref> that we always have 2 ln 2 KL (Djj ^ D) L 1 (D; ^ D): It is also easily verified that if D is any distribution over f0; 1g n and U is the uniform distribution, then KL (DjjU ) n (since we can always encode each output of <p> pn+d (1 p) n (pn+d) = nH (p) + d log p 1 (Here, H (p) = p log (p) (1 p) log (1 p) is the standard binary entropy function.) Furthermore, it can be shown that the expected log-loss of the target distribution is lower bounded by nH (p) <ref> [8] </ref>.
Reference: [9] <author> Richard O. Duda and Peter E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: Nearest-neighbor approaches to the unsupervised learning problem often arise in the nonparametric setting. While we obviously cannot do justice to these areas here, the books of Duda and Hart <ref> [9] </ref> and Vapnik [25] provide excellent overviews and introductions to the pattern recognition work, as well as many pointers for further reading. See also Izenman's recent survey article [16]. Roughly speaking, our work departs from the traditional statistical and pattern recognition approaches in two ways.
Reference: [10] <author> Paul Fischer and Hans Ulrich Simon. </author> <title> On learning ring-sum-expansions. </title> <journal> SIAM Journal on Computing, </journal> <volume> 21(1) </volume> <pages> 181-192, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Theorem 8 The class PARITY n is efficiently exactly learnable with a generator and evaluator. Proof: The learning algorithm uses as a subroutine an algorithm for learning parity functions in the PAC model <ref> [10, 15] </ref> by solving a system of linear equations over the field of integers modulo 2.
Reference: [11] <author> Yoav Freund. </author> <title> An improved boosting algorithm and its implications on learning complexity. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 391-398, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: This powerful principle goes by the name Occam's Razor, and it can be verified for many learning models, including our distribution learning model [5, 6, 21, 14]. In the distribution-free PAC model, the converse to Oc-cam's Razor can be shown to hold as well <ref> [11, 22] </ref>.
Reference: [12] <author> Oded Goldreich, Shafi Goldwasser, and Silvio Micali. </author> <title> How to construct random functions. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 33(4) </volume> <pages> 792-807, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: For learning with a generator, it is only for the powerful class of all polynomial-size circuit generators that we can prove hardness; the proof relies on the strong properties of pseudo-random functions <ref> [12] </ref>. 6.1 Hardness of Learning Probabilistic Finite Automata with an Evaluator We define a class of distributions PFA n over f0; 1g n generated by probabilistic finite automata. <p> In the following theorem, we show that POLY n is not efficiently learnable with a generator. The construction uses the strong properties of pseudo-random functions <ref> [12] </ref>. Theorem 17 If there exists a one-way function, POLY n is not efficiently learnable with an evaluator or with a generator. Proof: (Sketch) We use the construction of small circuits indistinguishable from truly random functions due to Gol-dreich, Goldwasser and Micali [12]. <p> construction uses the strong properties of pseudo-random functions <ref> [12] </ref>. Theorem 17 If there exists a one-way function, POLY n is not efficiently learnable with an evaluator or with a generator. Proof: (Sketch) We use the construction of small circuits indistinguishable from truly random functions due to Gol-dreich, Goldwasser and Micali [12].
Reference: [13] <author> Yuri Gurevich. </author> <title> Average case completeness. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 42(3) </volume> <pages> 346-398, </pages> <year> 1991. </year>
Reference-contexts: For our results it turns out to be crucial to distinguish between distributions that can be only generated efficiently, and distributions that can be both generated and evaluated efficiently. Similar distinctions have been made before in the context of average-case complexity <ref> [2, 13] </ref>. We now make these notions precise. We start by defining an efficient generator. Definition 1 Let D n be a class of distributions over f0; 1g n .
Reference: [14] <author> David Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Information and Computation, </journal> <volume> 100(1) </volume> <pages> 78-150, </pages> <year> 1992. </year>
Reference-contexts: To analyze our performance, we will take the standard approach of comparing the log-loss of our hypothesis on S to the log-loss of the target distribution on S <ref> [14] </ref>. We define the log-loss by loss (D; S) = P ~y2S log D [~y] where D [~y] denotes the probability ~y is generated by the distribution D. <p> Eventually we shall use the fact that for a sufficiently large sample, the difference between the log-loss of our hypothesis and the log-loss of the target gives an upper bound on the Kullback-Leibler divergence <ref> [14] </ref>. Note that since our hypothesis centers O ( p cover the sample S, and each hypothesis center is given mix ture coefficient 1=(k log m), our hypothesis assigns probabil ity at least 1 p p n pn+O pn log (m=ffi) to every vector in S. <p> Thus, provided m is sufficiently large for the uniform con vergence of the expected log-losses to the true log-losses <ref> [14] </ref>, we are ensured that the expected log-loss of our hypothesis exceeds that of the target by at most O pn log (m=ffi) log p 1 and this is by definition an upper bound on the Kullback-Leibler divergence. <p> This powerful principle goes by the name Occam's Razor, and it can be verified for many learning models, including our distribution learning model <ref> [5, 6, 21, 14] </ref>. In the distribution-free PAC model, the converse to Oc-cam's Razor can be shown to hold as well [11, 22].
Reference: [15] <author> David Helmbold, Robert Sloan, and Manfred K. Warmuth. </author> <title> Learning integer lattices. </title> <journal> SIAM Journal on Computing, </journal> <volume> 21(2) </volume> <pages> 240-266, </pages> <year> 1992. </year>
Reference-contexts: Theorem 8 The class PARITY n is efficiently exactly learnable with a generator and evaluator. Proof: The learning algorithm uses as a subroutine an algorithm for learning parity functions in the PAC model <ref> [10, 15] </ref> by solving a system of linear equations over the field of integers modulo 2.
Reference: [16] <author> Alan Julian Izenman. </author> <title> Recent developments in nonparamet-ric density estimation. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 86(413) </volume> <pages> 205-224, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: While we obviously cannot do justice to these areas here, the books of Duda and Hart [9] and Vapnik [25] provide excellent overviews and introductions to the pattern recognition work, as well as many pointers for further reading. See also Izenman's recent survey article <ref> [16] </ref>. Roughly speaking, our work departs from the traditional statistical and pattern recognition approaches in two ways. First, we place explicit emphasis on the computational complexity of distribution learning. <p> It is interesting to note as an aside that many of the standard statistical algorithms (such as the nearest-neighbor and kernel-based algorithms surveyed by Izenman <ref> [16] </ref>) also involve the memorization of the entire sample. We now make a concrete proposal for a counterexample to the converse of Occam's Razor for learning with a generator.
Reference: [17] <author> Jeff Kahn, Nathan Linial, and Alex Samorodintsky. Inclusion-exclusion: </author> <title> exact and approximate. </title> <type> Manuscript, </type> <year> 1993. </year>
Reference-contexts: Naively, we would explicitly compute the sizes of all 2 k+1 intersections. We can obtain an improved bound by using a new result of Kahn, Linial, and Samorodintsky <ref> [17] </ref> which shows that the sizes of all 2 k+1 intersections are in fact uniquely determined by the sizes of all intersections of at most 2 log k + 2 sets. More formally: Lemma 6 (Implicit in Kahn et al. [17]) Let fT i g m i=1 and i g m <p> by using a new result of Kahn, Linial, and Samorodintsky <ref> [17] </ref> which shows that the sizes of all 2 k+1 intersections are in fact uniquely determined by the sizes of all intersections of at most 2 log k + 2 sets. More formally: Lemma 6 (Implicit in Kahn et al. [17]) Let fT i g m i=1 and i g m i=1 be two families of sets, where T i ; T 0 i [`].
Reference: [18] <author> Michael Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 392-401, </pages> <year> 1993. </year>
Reference-contexts: Thus we prove our theorem under the following conjecture, for which some evidence has been provided in recent papers <ref> [18, 3] </ref>. Conjecture 15 (Noisy Parity Assumption) There is a constant 0 &lt; &lt; 1 2 such that there is no efficient algorithm for learning parity functions under the uniform distribution in the PAC model with classification noise rate .
Reference: [19] <author> Michael Kearns and Ming Li. </author> <title> Learning in the presence of malicious errors. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22(4) </volume> <pages> 807-837, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: We can then use the greedy heuristic for the partial cover problem <ref> [19] </ref> and conduct a similar analysis. (Theorem 10) In contrast to the covering approach taken in the algorithm of Theorem 10, the algorithm of the following theorem uses an equation-solving technique.
Reference: [20] <author> Michael Kearns and Leslie G. Valiant. </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata. </title> <booktitle> In Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 433-444, </pages> <month> May </month> <year> 1989. </year> <note> To appear, Journal of the Association for Computing Machinery. </note>
Reference-contexts: PAC model, we should distinguish between representation dependent hardness results, in which the intractability is the result of demanding that the learning algorithm output a hypothesis of certain syntactic form, and representation independent hardness results, in which a learning problem is shown hard regardless of the form of the hypothesis <ref> [20] </ref> and thus is inherently hard. While we seek only results of the second type, we must still specify whether it is learning with an evaluator or learning with a generator that is hard, or both.
Reference: [21] <author> Michael J. Kearns and Robert E. Schapire. </author> <title> Efficient distribution-free learning of probabilistic concepts. </title> <booktitle> In 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 382-391, </pages> <month> October </month> <year> 1990. </year> <note> To appear, Journal of Computer and System Sciences. </note>
Reference-contexts: This powerful principle goes by the name Occam's Razor, and it can be verified for many learning models, including our distribution learning model <ref> [5, 6, 21, 14] </ref>. In the distribution-free PAC model, the converse to Oc-cam's Razor can be shown to hold as well [11, 22].
Reference: [22] <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference-contexts: A function class with similar properties in the PAC model (that is, a class that is PAC learnable only if the hypothesis memorizes the training sample) provably does not exist <ref> [22] </ref>. Thus this construction is of some philosophical interest, since it is the first demonstration of a natural learning model in which the converse to Occam's Razor | namely, that efficient learning implies efficient compression | may fail. <p> This powerful principle goes by the name Occam's Razor, and it can be verified for many learning models, including our distribution learning model [5, 6, 21, 14]. In the distribution-free PAC model, the converse to Oc-cam's Razor can be shown to hold as well <ref> [11, 22] </ref>.
Reference: [23] <author> L. G. Valiant. </author> <title> The complexity of enumeration and reliability problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 8(3) </volume> <pages> 410-421, </pages> <year> 1979. </year>
Reference-contexts: In other words, OR k n does not have polynomial-size evaluators, unless #P P=poly. Proof: We use the fact that exactly counting the number of satisfying assignments to a monotone 2-CNF formula is a #P -complete problem <ref> [23] </ref>. The circuit C n will have inputs x 1 ; : : : ; x n that will correspond to the variables of a monotone 2-CNF formula, and also inputs z i;j for each possible monotone clause (x i _ x j ).
Reference: [24] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: 1 Introduction and History We introduce and investigate a new model of learning probability distributions from independent draws. Our model is inspired by the popular Probably Approximately Correct (PAC) model for learning boolean functions from labeled examples <ref> [24] </ref>, in the sense that we emphasize efficient and approximate learning, and we study the learnability of restricted classes of target distributions. <p> Our approach is directly influenced by the popular PAC model for learning boolean functions from labeled examples <ref> [24] </ref>, in that we assume the unknown target distribution is chosen from a known class of distributions that are characterized by some simple computational device for generating independent observations or outputs.
Reference: [25] <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <year> 1982. </year> <month> 10 </month>
Reference-contexts: Nearest-neighbor approaches to the unsupervised learning problem often arise in the nonparametric setting. While we obviously cannot do justice to these areas here, the books of Duda and Hart [9] and Vapnik <ref> [25] </ref> provide excellent overviews and introductions to the pattern recognition work, as well as many pointers for further reading. See also Izenman's recent survey article [16]. Roughly speaking, our work departs from the traditional statistical and pattern recognition approaches in two ways.
References-found: 25

