URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92197-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Automatic Software Cache Coherence through Vectorization  
Author: Ervan Darnell John M. Mellor-Crummey Ken Kennedy 
Address: Houston, TX 77251-1892  
Affiliation: Computer Science Department Rice University  
Abstract: Access latency in large-scale shared-memory multiprocessors is a concern since most (if not all) memory is one or more hops away through an interconnection network. Providing processors with one or more levels of cache is an accepted way to reduce the average access latency; however, in a multiprocessor, cached values must be kept coherent for the multiprocessor to support the abstraction of a shared global memory. There is no generally accepted hardware solution to provide cache coherence for large-scale shared-memory multiprocessors. Software coherence strategies offer scalability with current hardware. In this paper we examine a compiler-based software strategy for maintaining cache coherence that relies on dependence analysis and a vectorization algorithm to insert cache control directives. Experiments on the BBN TC2000 for a pair of numerical problems show that the run-time cost of coherence using our strategy is less than that for previously proposed compiler-based software methods and suggest that it should compare favorably with proposed hardware schemes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Allen and K. Kennedy. </author> <title> Automatic translation of FORTRAN programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> Oct. </month> <year> 1987. </year>
Reference-contexts: DO I=1,N DO K=1,N Inv A (I,F (J),K) 4 Coherence Through Vectorization Conceptually, our approach starts with the solution generated using the CKM method and applies vectorization <ref> [1] </ref> to aggregate cache control operations and move them as far as possible, toward either the fork or the join. The resulting invalidates often resemble those created by the FSI method. We refer to our strategy as Coherence Through Vectorization (CTV).
Reference: [2] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> In Proc. of the 1990 International Conference on Supercomputing/Computer Architecture News, </booktitle> <pages> pages 1-6, </pages> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year> <journal> Proceedings published as ACM SIGARCH Computer Architecture News, </journal> <volume> 18 (3), </volume> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: With respect to any particular processor, most (if not all) memory is one or more hops away through the interconnect. For this reason, access latency is an important concern. Approaches for coping with access latency in shared-memory multiprocessors include multithreading <ref> [2] </ref> and caching. In this paper we focus on using caching to reduce access latency. Caching can reduce the impact of network latency on average access latency by reducing the number of accesses that must traverse the interconnect.
Reference: [3] <author> L. M. Censier and P. Feautrier. </author> <title> A new solution to coherence problems in multicache systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> Dec. </month> <year> 1978. </year>
Reference-contexts: Snoopy cache schemes, which monitor accesses broadcast on the processor-memory interconnect, are now in common use for small scale systems [14, 16]; however, snoopy schemes are problematic for large-scale machines because such machines cannot be based on a single, central broadcast medium for lack of sufficient bandwidth. Directory schemes <ref> [3, 11, 17] </ref>, in which a directory entry associated with each memory location indicates which processors have cached values for that location, seem more promising for large-scale systems. However, directories can require large amounts of additional storage and directory maintenance operations may substantially increase network traffic.
Reference: [4] <author> H. Cheong and A. Veidenbaum. </author> <title> Compiler-directed cache management for multiprocessors. </title> <journal> Computer, </journal> <volume> 23(6) </volume> <pages> 39-47, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: However, directories can require large amounts of additional storage and directory maintenance operations may substantially increase network traffic. Others researchers suggest that caches include version number based support for coherence <ref> [4, 12] </ref>. Drawbacks to these schemes include dedication of precious cache real-estate to version numbers (decreasing the amount of useful data that the cache can hold), and the additional hardware complexity. <p> Similar observations apply for the CWE. 3.1 Related Work Here we examine two previously proposed software schemes in terms of the aforementioned trade-offs in the positioning of coherence operations. The first is Cheong and Veiden-baum's fast selective invalidation <ref> [4] </ref> (hereafter referred to as the FSI method). The second is a method proposed by Cytron, Karlovsky, and McAuliffe [5] (hereafter referred to as the CKM method). Both methods were developed for programs with fork-join parallelism expressed in the form of parallel loops. <p> This is useful for FSI only if it applies to every reference in a loop. CKM can apply it on a reference by reference basis. Cheong and Viedenbaum's simulations of the FSI method <ref> [4] </ref> show that it has a good CRE, approaching 100%, but its CWE is always 0%. There is no data on how the CKM method fairs but one would expect it to usually have a lower CRE and a CWE slightly above 0%.
Reference: [5] <author> R. Cytron, S. Karlovsky, and K. McAuliffe. </author> <title> Automatic management of programmable caches. </title> <booktitle> In Proc. of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 229-238, </pages> ?, <month> Aug. </month> <year> 1988. </year>
Reference-contexts: This dependence is carried by the J loop and the carrying level of the dependence is 2. Dependences that are not carried by loops are said to be loop independent. For programs with parallel constructs, a dependence is called processor crossing <ref> [5] </ref> when the statement endpoints of a dependence may execute on different processors. Processor crossing dependences indicate when a value may be accessed by more than one processor. <p> The first is Cheong and Veiden-baum's fast selective invalidation [4] (hereafter referred to as the FSI method). The second is a method proposed by Cytron, Karlovsky, and McAuliffe <ref> [5] </ref> (hereafter referred to as the CKM method). Both methods were developed for programs with fork-join parallelism expressed in the form of parallel loops. Evictions and cache line sizes greater than one word were not considered. Both methods issue updates immediately after their corresponding write.
Reference: [6] <author> E. Darnell, J. M. Mellor-Crummey, and K. Kennedy. </author> <title> Automatic software cache coherence through vector-ization. </title> <type> Technical Report CRPC-TR92197, </type> <institution> Computer Science Department, Rice University, </institution> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: If the architecture does not permit such an allocation, some of the parallelism will have to sacrificed for a completely automatic technique. This must be handled before the coherence graph is built. A more detailed treatment of the algorithm can be found in <ref> [6] </ref>. 4.3 Example Consider a simple matrix multiply (figure 4) where the outer loop is parallel.
Reference: [7] <author> J. J. Donagrra, I. S. Duff, D. C. Sorenson, and H. A. van der Vorst. </author> <title> Solving Linear Systems on Vector and Shared Memory Computers. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <year> 1991. </year>
Reference-contexts: The LU decomposition is a partial pivot blocked right Processors R l t v S e d p 0 0 0 0 0 Uncached CKM OptFSI FSI OptCTV CTV No Coherence looking algorithm <ref> [7] </ref>. The blocking factor for all tests was 10. That is close to optimal for all measured test cases. The no coherence case produces numerous floating point exceptions due to both division by zero and overflow errors. Trapping these exceptions skews the results.
Reference: [8] <author> D. Kuck. </author> <title> The Structure of Computers and Computations, Volume 1. </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1978. </year>
Reference-contexts: There are four types of data dependence <ref> [8, 9] </ref>: True (flow) dependence S 1 writes a memory location that S 2 later reads. Anti dependence S 1 reads a memory location that S 2 later writes. Output dependence S 1 writes a memory location that S 2 later writes.
Reference: [9] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> Jan. </month> <year> 1981. </year>
Reference-contexts: There are four types of data dependence <ref> [8, 9] </ref>: True (flow) dependence S 1 writes a memory location that S 2 later reads. Anti dependence S 1 reads a memory location that S 2 later writes. Output dependence S 1 writes a memory location that S 2 later writes.
Reference: [10] <author> L. Lamport. </author> <title> How to make a multiprocessor that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9), </volume> <month> Sept. </month> <year> 1979. </year>
Reference-contexts: To avoid changing the semantics of a program execution through the use of caches, memory must retain the appearance of sequential consistency <ref> [10] </ref>. By itself, caching hardware for uniprocessors is not sufficient for multiprocessors since uniprocessor caches have no knowledge of other processors' actions. Most approaches to the cache coherence problem have focused on hardware mechanisms to maintain coherence.
Reference: [11] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the dash multiprocessor. </title> <booktitle> 17th International Symposium on Computer Architecture/Computer Architecture News, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year> <journal> Special issue of Computer Architecture News, </journal> <volume> 18(2), </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: Snoopy cache schemes, which monitor accesses broadcast on the processor-memory interconnect, are now in common use for small scale systems [14, 16]; however, snoopy schemes are problematic for large-scale machines because such machines cannot be based on a single, central broadcast medium for lack of sufficient bandwidth. Directory schemes <ref> [3, 11, 17] </ref>, in which a directory entry associated with each memory location indicates which processors have cached values for that location, seem more promising for large-scale systems. However, directories can require large amounts of additional storage and directory maintenance operations may substantially increase network traffic.
Reference: [12] <author> S. Min and J. Baer. </author> <title> A timestamp-based cache coherence scheme. </title> <booktitle> In Proc. of the 1989 International Conference on Parallel Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 23-32, </pages> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: However, directories can require large amounts of additional storage and directory maintenance operations may substantially increase network traffic. Others researchers suggest that caches include version number based support for coherence <ref> [4, 12] </ref>. Drawbacks to these schemes include dedication of precious cache real-estate to version numbers (decreasing the amount of useful data that the cache can hold), and the additional hardware complexity.
Reference: [13] <author> S. Min, J. Baer, and H. Kim. </author> <title> An efficient caching support for critical sections in large-scale shared-memory multiprocessors. </title> <booktitle> In Proc. of the 1990 International Conference on Supercomputing/Computer Architecture News, </booktitle> <pages> pages 4-47, </pages> <month> June </month> <year> 1990. </year> <journal> Special issue of Computer Architecture News, </journal> <volume> 18(3), </volume> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: Most approaches to the cache coherence problem have focused on hardware mechanisms to maintain coherence. Unfortunately, the overhead of maintaining coherence in hardware can be high; scaling systems based on hardware coherence is a difficult problem <ref> [13] </ref>. Snoopy cache schemes, which monitor accesses broadcast on the processor-memory interconnect, are now in common use for small scale systems [14, 16]; however, snoopy schemes are problematic for large-scale machines because such machines cannot be based on a single, central broadcast medium for lack of sufficient bandwidth.
Reference: [14] <author> A. Osterhaug, </author> <title> editor. Guide to Parallel Programming on Sequent Computer Systems. </title> <publisher> Sequent Technical Publications, </publisher> <address> San Diego, CA, </address> <year> 1989. </year>
Reference-contexts: Unfortunately, the overhead of maintaining coherence in hardware can be high; scaling systems based on hardware coherence is a difficult problem [13]. Snoopy cache schemes, which monitor accesses broadcast on the processor-memory interconnect, are now in common use for small scale systems <ref> [14, 16] </ref>; however, snoopy schemes are problematic for large-scale machines because such machines cannot be based on a single, central broadcast medium for lack of sufficient bandwidth.
Reference: [15] <institution> Parallel Computing Forum. PCF Fortran, </institution> <month> Mar. </month> <year> 1990. </year> <note> Working Draft. </note>
Reference-contexts: Such imprecision does not affect correctness as long as the region covered by the invalidate subsumes the data accessed by the read. This leads to a tradeoff affecting potential reuse of cached values. placement of invalidates. All program examples in this paper use Parallel Computing Forum (PCF) Fortran <ref> [15] </ref> syntax; PCF Fortran is an emerging standard for shared-memory parallel Fortran. The PARALLEL construct corresponds to a fork-join pair that specifies a block of code to be executed on every processor. PDO is a worksharing construct; iterations of a PDO are partitioned among the processors for execution.
Reference: [16] <author> D. Schanin. </author> <title> The design and development of a very high speed system bus the encore multimax nanobus. </title> <booktitle> In Proceedings of the Fall Joint Computer Conference, </booktitle> <pages> pages 410-418, </pages> <month> Nov. </month> <year> 1986. </year>
Reference-contexts: Unfortunately, the overhead of maintaining coherence in hardware can be high; scaling systems based on hardware coherence is a difficult problem [13]. Snoopy cache schemes, which monitor accesses broadcast on the processor-memory interconnect, are now in common use for small scale systems <ref> [14, 16] </ref>; however, snoopy schemes are problematic for large-scale machines because such machines cannot be based on a single, central broadcast medium for lack of sufficient bandwidth.
Reference: [17] <author> J. Willis, A. Sanderson, and C. Hill. </author> <title> Cache coherence in systems with parallel communication channels & many processors. </title> <booktitle> In Supercomputing '90, </booktitle> <pages> pages 554-563, </pages> <year> 1990. </year>
Reference-contexts: Snoopy cache schemes, which monitor accesses broadcast on the processor-memory interconnect, are now in common use for small scale systems [14, 16]; however, snoopy schemes are problematic for large-scale machines because such machines cannot be based on a single, central broadcast medium for lack of sufficient bandwidth. Directory schemes <ref> [3, 11, 17] </ref>, in which a directory entry associated with each memory location indicates which processors have cached values for that location, seem more promising for large-scale systems. However, directories can require large amounts of additional storage and directory maintenance operations may substantially increase network traffic.
References-found: 17

