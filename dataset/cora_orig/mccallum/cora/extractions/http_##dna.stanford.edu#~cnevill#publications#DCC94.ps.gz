URL: http://dna.stanford.edu/~cnevill/publications/DCC94.ps.gz
Refering-URL: http://dna.stanford.edu/~cnevill/resume.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email cgn@waikato.ac.NZ  email ihw@waikato.ac.NZ  Email maulsby@cpsc.UCalgary.CA  
Phone: Telephone +64 7 8384021;  Telephone +64 7 8384246;  
Title: Compression by Induction of Hierarchical Grammars  
Author: Craig G. Nevill-Manning Ian H. Witten David L. Maulsby 
Address: Hamilton, New Zealand  Hamilton, New Zealand  Calgary, Calgary T2N 1N4, Canada  
Affiliation: Computer Science, University of Waikato,  Computer Science, University of Waikato,  Computer Science, University of  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Bell, T.C., Cleary, J.G. and Witten, I.H. </author> <title> (1990) Text Compression. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: This will not necessarily produce the smallest grammar possible. To do this would require finding two things: the best set of productions, and the best order in which they should be applied. The latter is called optimal parsing, and can be implemented by a dynamic programming algorithm <ref> [1] </ref>. Parsing is dependant upon the set of productions, or dictionary, and this technique builds a dictionary and performs parsing simultaneously. Unfortunately, the selection of the best dictionary can be shown to be NP-complete, [4, 7]. <p> It has been shown that adaptive modeling is at least as good as sending a static model and then sending the sequence relative to that model <ref> [1] </ref>. The present technique represents an interesting variation: rather than describing some characteristics of the sequence, the model describes the sequence exactly . No more information is needed once the model has been sent; the decoder simply expands the first rule in the grammar, and continues recursively. <p> Similarly, macro schemes will not achieve any compression, as no phrase appears more than once. The compression performance of the scheme was measured on the Calgary corpus. This corpus contains fourteen files ranging in content from English text to object programs <ref> [1] </ref>. The sizes of the files after being compressed by this technique are given in the column labeled Grammar in Table 1. Results are also shown for Unix compress [1], a standard macro scheme, LZFG [3], an excellent macro scheme, and a high-performance context scheme, PPMC [6]. <p> This corpus contains fourteen files ranging in content from English text to object programs <ref> [1] </ref>. The sizes of the files after being compressed by this technique are given in the column labeled Grammar in Table 1. Results are also shown for Unix compress [1], a standard macro scheme, LZFG [3], an excellent macro scheme, and a high-performance context scheme, PPMC [6]. The second grammar induction column, labeled Adaptive, is discussed 6 below. Compression rates in bits per character are given for each scheme on each file.
Reference: [2] <author> Chan, P.K. </author> <title> Machine Learning in Molecular Biology Sequence Analysis, </title> <institution> CUCS-011-91, Department of Computer Science, Columbia University. </institution>
Reference: [3] <author> Fiala, E.R., Greene, D.H. </author> <title> (1989) Data compression with finite windows, </title> <journal> Communications of the ACM, </journal> <volume> 32(4) 490505 </volume>
Reference-contexts: This corpus contains fourteen files ranging in content from English text to object programs [1]. The sizes of the files after being compressed by this technique are given in the column labeled Grammar in Table 1. Results are also shown for Unix compress [1], a standard macro scheme, LZFG <ref> [3] </ref>, an excellent macro scheme, and a high-performance context scheme, PPMC [6]. The second grammar induction column, labeled Adaptive, is discussed 6 below. Compression rates in bits per character are given for each scheme on each file.
Reference: [4] <author> Fraenkel, A.S. Mor, M. and Perl, Y. </author> <title> (1983) Is text compression by prefixes and suffixes practical?, </title> <journal> Acta Informatica, </journal> <volume> (20) 371389. </volume>
Reference-contexts: The latter is called optimal parsing, and can be implemented by a dynamic programming algorithm [1]. Parsing is dependant upon the set of productions, or dictionary, and this technique builds a dictionary and performs parsing simultaneously. Unfortunately, the selection of the best dictionary can be shown to be NP-complete, <ref> [4, 7] </ref>. It is not known how much better this technique would perform if an optimal dictionary could be constructed and optimal parsing performed using this dictionary, but the process would be at least NP-complete.
Reference: [5] <author> Manzara, L.C., Witten, I.H., James, M., </author> <title> 1992 On the entropy of music: an experiment with Bach Chorale melodies, </title> <note> Leonardo Music Journal 2(1) 8188 </note>
Reference: [6] <author> Moffat, A. </author> <year> (1990), </year> <title> Implementing the PPM data compression scheme, </title> <journal> IEEE Transactions on Communications COM-38(11), 19171921 </journal>
Reference-contexts: The sizes of the files after being compressed by this technique are given in the column labeled Grammar in Table 1. Results are also shown for Unix compress [1], a standard macro scheme, LZFG [3], an excellent macro scheme, and a high-performance context scheme, PPMC <ref> [6] </ref>. The second grammar induction column, labeled Adaptive, is discussed 6 below. Compression rates in bits per character are given for each scheme on each file. The average rate is shown at the bottom of the column for each scheme.
Reference: [7] <author> Storer, J.A. and Szymanski, T.G. </author> <title> (1978) The macro model for data compression, </title> <booktitle> 10th Annual Symposium of the Theory of Computing , 3039. </booktitle>
Reference-contexts: The latter is called optimal parsing, and can be implemented by a dynamic programming algorithm [1]. Parsing is dependant upon the set of productions, or dictionary, and this technique builds a dictionary and performs parsing simultaneously. Unfortunately, the selection of the best dictionary can be shown to be NP-complete, <ref> [4, 7] </ref>. It is not known how much better this technique would perform if an optimal dictionary could be constructed and optimal parsing performed using this dictionary, but the process would be at least NP-complete.
Reference: [8] <author> Conklin, D., Witten, I.H. </author> <year> (1990), </year> <title> Prediction and Entropy of Music, </title> <type> Technical report 91/457/41 (Calgary: </type> <institution> Department of Computer Science, Univ. of Calgary). </institution>
Reference: [9] <author> Witten, I.H., Mo, D. </author> <year> (1993) </year> <month> TELS: </month> <title> Learning text editing tasks from examples, Watch what I do: programming by demonstration, </title> <editor> A. Cypher (Ed.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <month> 182203 </month>
Reference: [10] <author> Witten, I.H., Neal, R., Cleary, J.G. </author> <title> (1987) Arithmetic coding for data compression, </title> <journal> Communications of the ACM 30(6), </journal> <volume> 520540 </volume>
Reference-contexts: First, the complete grammar is formed by processing the entire sequence. The grammar can be viewed as a sequence of terminals, nonterminals and end-of-rule markers. This sequence can be coded efficiently using a zero-order model together with an arithmetic coder <ref> [10] </ref>. Recall that every digram in the grammar is unique. This implies that each symbol is unique in the context of the preceding one, and so models of higher order cannot contribute any useful predictions. Similarly, macro schemes will not achieve any compression, as no phrase appears more than once.
Reference: [11] <author> Ziv, J., Lempel, A. </author> <title> (1977) A universal algorithm for sequential data compression, </title> <journal> IEEE Transactions on Information Theory IT-23 (3), </journal> <volume> 337343 </volume>
Reference-contexts: Compress performs poorly relative to the other methods, but is a practical and widely used macro scheme. Macro methods achieve compression by replacing repeated sequences with references to earlier occurrences, either by explicit reference to a segment of the sequence already transmitted in the case of LZ77 <ref> [11] </ref>, or by referring to a phrase in a list of phrases extracted from the sequence in the case of LZ78 [12]. LZFG is a macro scheme based on the same principle as compress, but with improved selection of phrases and correspondingly improved compression performance.
Reference: [12] <author> Ziv, J., Lempel, </author> <title> A.(1978) Compression of individual sequences via variable-rate coding, </title> <journal> IEEE Transactions on Information Theory IT-24 (5), </journal> <volume> 530536 </volume>
Reference-contexts: methods achieve compression by replacing repeated sequences with references to earlier occurrences, either by explicit reference to a segment of the sequence already transmitted in the case of LZ77 [11], or by referring to a phrase in a list of phrases extracted from the sequence in the case of LZ78 <ref> [12] </ref>. LZFG is a macro scheme based on the same principle as compress, but with improved selection of phrases and correspondingly improved compression performance. It represents the best general macro method.
References-found: 12

