URL: http://www.cs.utexas.edu/users/neeraj/doc/NN/proj/nnreport.ps
Refering-URL: http://www.cs.utexas.edu/users/neeraj/resume.html
Root-URL: 
Email: E-mail: neeraj@cs.utexas.edu  E-mail: imadhava@ece.utexas.edu  
Title: Evolutionary Learning of the Crossover Operator  
Author: Neeraj Garg Indumathi Madhavan 
Date: December 11, 1997  
Address: Austin, Texas 78712  Austin, Texas 78712  
Affiliation: Computer Science Dept. University of Texas at Austin  Electrical Engineering Dept. University of Texas at Austin  
Abstract: 1 Abstract 
Abstract-found: 1
Intro-found: 1
Reference: <author> Haykin, S. </author> <year> (1994). </year> <title> Neural Networks: A Comprehensive Foundation. </title> <address> New York: </address> <publisher> Macmillan. </publisher>
Reference-contexts: Both the model and the Decision-Maker are feed-forward backpropogation neural networks using sigmoid neurons. The system is trained by alternating between two phases: Model Learning and Decision-Maker training. In the model learning mode, no changes are made to the Decision-Maker, but the Model uses backpropogation <ref> (Haykin 1994) </ref> to minimize (r R) 2 . In essence, the Model is learning to predict how the environment will respond to the decisions made by the Decision-Maker. If the Model learns its task well, R=r for every input situation.
Reference: <author> John Hertz, A. K., and Palmer, R. G. </author> <year> (1991). </year> <title> Introduction to the Theory of Neural Computation. </title> <address> New York: </address> <publisher> Addison Wesley. </publisher>
Reference-contexts: Under such circumstances, it may be useful to use a neural network that learns the description of the search space, and is thus able to produce a better result. 4 Architecture and Implemen tation The architecture of the system <ref> (John Hertz and Palmer 1991) </ref> is shown in figure 5. It is very much similar to the reinforcement learning architecture described earlier.
Reference: <author> Michalewicz, Z. </author> <year> (1996). </year> <title> Genetic Algorithms + Data Structures = Evolution Programs. </title> <address> New York: </address> <publisher> Springer. </publisher>
Reference-contexts: They are parallel in nature, like neural networks. These algorithms operate by maintaining and modifying the characteristics of a population of solutions (individuals) over a large number of generations. This process is designed to produce to produce successive populations having an increasing number of individuals with desirable characteristics <ref> (Michalewicz 1996) </ref>. Like nature's solution, the process is probabilistic but not completely random. Genetic algorithms have been developed which perform optimization with techniques modeled on biological genetics and natural selection. Optimization is performed on a set of strings, where each string is composed of a sequence of characters.
Reference: <author> Patrick Goetz, S. K., and Miikulainen, R. </author> <year> (1996). </year> <title> On--line adaptation of a signal predistorter through dual reinforcement learning. </title> <booktitle> In Proceedings of the Thirteenth International Conference ICML '96. </booktitle>
Reference-contexts: to genetic algorithms: 2.2 The Model and Critic architecture for Reinforcement Learning One of the approaches to reinforcement learning involves modelling the environment with an auxilliary network, which can then be used to produce a target for each output of the main network instead of just a global reinforcement signal <ref> (Patrick Goetz and Miikulainen 1996) </ref>. The Decision-Maker receives input from the environment and generates an output signal. The environment delivers a reinforcement signal r in response to this output.
References-found: 4

