URL: ftp://ftp.idsia.ch/pub/techrep/IDSIA-69-96.ps.gz
Refering-URL: http://www.idsia.ch/techrep.html
Root-URL: http://www.idsia.ch/techrep.html
Email: juergen,marco,jieyu@idsia.ch  
Title: SIMPLE PRINCIPLES OF METALEARNING  difference between time and space is that you can't reuse time.  
Author: J urgen Schmidhuber Jieyu Zhao Marco Wiering Merrick Furst 
Note: The biggest  
Date: June 27, 1996  
Web: http://www.idsia.ch  
Address: Corso Elvezia 36, CH-6900-Lugano, Switzerland  
Affiliation: IDSIA,  
Pubnum: Technical Report IDSIA-69-96  
Abstract: The goal of metalearning is to generate useful shifts of inductive bias by adapting the current learning strategy in a "useful" way. Our learner leads a single life during which actions are continually executed according to the system's internal state and current policy (a modifiable, probabilistic algorithm mapping environmental inputs and internal states to outputs and new internal states). An action is considered a learning algorithm if it can modify the policy. Effects of learning processes on later learning processes are measured using reward/time ratios. Occasional backtracking enforces success histories of still valid policy modifications corresponding to histories of lifelong reward accelerations. The principle allows for plugging in a wide variety of learning algorithms. In particular, it allows for embedding the learner's policy modification strategy within the policy itself (self-reference). To demonstrate the principle's feasibility in cases where conventional reinforcement learning fails, we test it in complex, non-Markovian, changing environments ("POMDPs"). One of the tasks involves more than 10 13 states, two learners that both cooperate and compete, and strongly delayed reinforcement signals (initially separated by more than 300,000 time steps). 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G. </author> <year> (1989). </year> <title> Connectionist approaches for control. </title> <type> Technical Report COINS 89-89, </type> <institution> University of Massachusetts, </institution> <address> Amherst MA 01003. </address>
Reference: <author> Berry, D. A. and Fristedt, B. </author> <year> (1985). </year> <title> Bandit Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, London. </publisher>
Reference: <author> Boddy, M. and Dean, T. L. </author> <year> (1994). </year> <title> Deliberation scheduling for problem solving in time-constrained environments. </title> <journal> Artificial Intelligence, </journal> <volume> 67 </volume> <pages> 245-285. </pages>
Reference: <author> Caruana, R., Silver, D. L., Baxter, J., Mitchell, T. M., Pratt, L. Y., and Thrun, S. </author> <year> (1995). </year> <title> Learning to learn: </title> <booktitle> knowledge consolidation and transfer in inductive systems. Workshop held at NIPS-95, </booktitle> <address> Vail, CO, </address> <note> see http://www.cs.cmu.edu/afs/user/caruana/pub/transfer.html. </note>
Reference: <author> Chaitin, G. </author> <year> (1969). </year> <title> On the length of programs for computing finite binary sequences: statistical considerations. </title> <journal> Journal of the ACM, </journal> <volume> 16 </volume> <pages> 145-159. </pages>
Reference: <author> Cliff, D. and Ross, S. </author> <year> (1994). </year> <title> Adding temporary memory to ZCS. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 3 </volume> <pages> 101-150. </pages>
Reference: <author> Cramer, N. L. </author> <year> (1985). </year> <title> A representation for the adaptive generation of simple sequential programs. </title> <editor> In Grefenstette, J., editor, </editor> <booktitle> Proceedings of an International Conference on Genetic Algorithms and Their Applications, </booktitle> <address> Hillsdale NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Dickmanns, D., Schmidhuber, J., and Winklhofer, A. </author> <year> (1987). </year> <title> Der genetische Algorithmus: Eine Imple-mentierung in Prolog. </title> <institution> Fortgeschrittenenpraktikum, Institut fur Informatik, Lehrstuhl Prof. Radig, Technische Universitat Munchen. </institution>
Reference: <author> Gittins, J. C. </author> <year> (1989). </year> <title> Multi-armed Bandit Allocation Indices. Wiley-Interscience series in systems and optimization. </title> <publisher> Wiley, </publisher> <address> Chichester, NY. </address> <note> 20 Greiner, </note> <author> R. </author> <year> (1996). </year> <title> PALO: A probabilistic hill-climbing algorithm. </title> <journal> Artificial Intelligence, </journal> <volume> 83(2). </volume>
Reference: <author> Jaakkola, T., Singh, S. P., and Jordan, M. I. </author> <year> (1995). </year> <title> Reinforcement learning algorithm for partially observable Markov decision problems. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Kaelbling, L. </author> <year> (1993). </year> <title> Learning in Embedded Systems. </title> <publisher> MIT Press. </publisher>
Reference: <author> Kaelbling, L., Littman, M., and Cassandra, A. </author> <year> (1995). </year> <title> Planning and acting in partially observable stochastic domains. </title> <type> Technical report, </type> <institution> Brown University, Providence RI. </institution>
Reference: <author> Kolmogorov, A. </author> <year> (1965). </year> <title> Three approaches to the quantitative definition of information. </title> <journal> Problems of Information Transmission, </journal> <volume> 1 </volume> <pages> 1-11. </pages>
Reference: <author> Koza, J. R. </author> <year> (1992). </year> <title> Genetic evolution and co-evolution of computer programs. </title> <editor> In Langton, C., Taylor, C., Farmer, J. D., and Rasmussen, S., editors, </editor> <booktitle> Artificial Life II, </booktitle> <pages> pages 313-324. </pages> <publisher> Addison Wesley Publishing Company. </publisher>
Reference: <author> Kumar, P. R. and Varaiya, P. </author> <year> (1986). </year> <title> Stochastic Systems: Estimation, Identification, and Adaptive Control. </title> <publisher> Prentice Hall. </publisher>
Reference: <author> Lenat, D. </author> <year> (1983). </year> <title> Theory formation by heuristic search. </title> <journal> Machine Learning, </journal> <volume> 21. </volume>
Reference: <author> Levin, L. A. </author> <year> (1973). </year> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266. </pages>
Reference: <author> Levin, L. A. </author> <year> (1984). </year> <title> Randomness conservation inequalities: Information and independence in mathematical theories. </title> <journal> Information and Control, </journal> <volume> 61 </volume> <pages> 15-37. </pages>
Reference: <author> Li, M. and Vitanyi, P. M. B. </author> <year> (1993). </year> <title> An Introduction to Kolmogorov Complexity and its Applications. </title> <publisher> Springer. </publisher>
Reference: <author> Littman, M. </author> <year> (1994). </year> <title> Memoryless policies: Theoretical limitations and practical results. </title> <editor> In D. Cliff, P. Husbands, J. A. M. and Wilson, S. W., editors, </editor> <booktitle> Proc. of the International Conference on Simulation of Adaptive Behavior: From Animals to Animats 3, </booktitle> <pages> pages 297-305. </pages> <publisher> MIT Press/Bradford Books. </publisher>
Reference-contexts: Figure 5.3 shows a partially observable environment (POE) with 600 fi 800 fields (or pixels). The POE has many more fields and obstacles than POEs used by previous authors working on POMDPs. For instance, McCallum's maze has only 23 free fields (McCallum, 1995), and Littman et al.'s biggest problem <ref> (Littman, 1994) </ref> involves less than 1000 states. There are two IS-based agents A and B. Each has circular shape and a diameter of 30 pixel widths. At a given time, each is rotated in one of eight different directions.
Reference: <author> McCallum, R. A. </author> <year> (1995). </year> <title> Instance-based utile distinctions for reinforcement learning with hidden state. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 387-395. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: Environment. Figure 5.3 shows a partially observable environment (POE) with 600 fi 800 fields (or pixels). The POE has many more fields and obstacles than POEs used by previous authors working on POMDPs. For instance, McCallum's maze has only 23 free fields <ref> (McCallum, 1995) </ref>, and Littman et al.'s biggest problem (Littman, 1994) involves less than 1000 states. There are two IS-based agents A and B. Each has circular shape and a diameter of 30 pixel widths. At a given time, each is rotated in one of eight different directions.
Reference: <author> Ring, M. B. </author> <year> (1994). </year> <title> Continual Learning in Reinforcement Environments. </title> <type> PhD thesis, </type> <institution> University of Texas at Austin, Austin, Texas 78712. </institution>
Reference: <author> Rosenbloom, P. S., Laird, J. E., and Newell, A. </author> <year> (1993). </year> <title> The SOAR Papers. </title> <publisher> MIT Press. </publisher>
Reference: <author> Russell, S. and Wefald, E. </author> <year> (1991). </year> <title> Principles of Metareasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 49 </volume> <pages> 361-395. </pages>
Reference: <author> Schmidhuber, J. </author> <year> (1987). </year> <title> Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... </title> <type> hook. </type> <institution> Institut fur Informatik, Technische Universitat Munchen. </institution>
Reference: <author> Schmidhuber, J. </author> <year> (1991). </year> <title> Reinforcement learning in Markovian and non-Markovian environments. </title> <editor> In Lipp-man, D. S., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 500-506. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schmidhuber, J. </author> <year> (1993). </year> <title> A self-referential weight matrix. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> pages 446-451. </pages> <publisher> Springer. </publisher>
Reference: <author> Schmidhuber, J. </author> <year> (1994). </year> <title> On learning how to learn learning strategies. </title> <type> Technical Report FKI-198-94, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen. </institution> <note> Revised January 1995. </note>
Reference: <author> Schmidhuber, J. </author> <year> (1995). </year> <title> Discovering solutions with low Kolmogorov complexity and high generalization capability. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 488-496. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: The learner makes use of an assembler-like programming language similar to but not quite as general as the one in <ref> (Schmidhuber, 1995) </ref>. It has n addressable work cells with addresses ranging from 0 to n 1. The variable, real-valued contents of the work cell with address k are denoted c k . Processes in the external environment occasionally write inputs into certain work cells.
Reference: <author> Schmidhuber, J. </author> <year> (1996). </year> <title> A general method for incremental self-improvement and multi-agent learning in unrestricted environments. </title> <editor> In Yao, X., editor, </editor> <booktitle> Evolutionary Computation: Theory and Applications. </booktitle> <publisher> Scientific Publ. Co., Singapore. </publisher>
Reference-contexts: IP and work cells together represent the system's internal state I (see section 2). For each value j in I, there is an assembler-like instruction b j with n j integer-valued parameters. See <ref> (Schmidhuber, 1996) </ref> for a related, illustrative figure.
Reference: <author> Solomonoff, R. </author> <year> (1964). </year> <title> A formal theory of inductive inference. Part I. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22. </pages>
Reference: <author> Solomonoff, R. </author> <year> (1986). </year> <title> An application of algorithmic probability to problems in artificial intelligence. </title> <editor> In Kanal, L. N. and Lemmer, J. F., editors, </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 473-491. </pages> <publisher> Elsevier Science Publishers. 21 Sutton, </publisher> <editor> R. S. </editor> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference: <author> Utgoff, P. </author> <year> (1986). </year> <title> Shift of bias for inductive concept learning. </title> <booktitle> In Machine Learning, </booktitle> <volume> volume 2. </volume> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA. </address>
Reference: <author> Watanabe, O. </author> <year> (1992). </year> <title> Kolmogorov complexity and computational complexity. </title> <booktitle> EATCS Monographs on Theoretical Computer Science, </booktitle> <publisher> Springer. </publisher>
Reference: <author> Watkins, C. J. C. H. and Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292. </pages>
Reference-contexts: this issue, we add Q-learning to the instruction list to be used by IS: b 13 : Q-learning (w 1 ) | with probability w 1 200fln ops , keep executing actions according to the Q-table until there is non-zero reinforcement, and update the Q-table according to standard Q-learning rules <ref> (Watkins and Dayan, 1992) </ref>. Otherwise, execute only one single action according to the current Q-table. Interestingly, this combination leads to even slightly better results near system death (see Figure 5.3). Essentially, the system learns when to trust the Q-table. Stack size.
Reference: <author> Wiering, M. and Schmidhuber, J. </author> <year> (1996). </year> <title> Solving POMDPs with Levin search and EIRA. </title> <editor> In Saitta, L., editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address> <note> To appear. </note>
Reference: <author> Williams, R. J. </author> <year> (1992). </year> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 229-256. </pages>

References-found: 37

