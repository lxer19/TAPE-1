URL: http://www.cs.ucsb.edu/TRs/techreports/TRCS94-14.ps
Refering-URL: http://www.cs.ucsb.edu/TRs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: What Price Replication?  
Author: M. L. Liu D. Agrawal A. El Abbadi 
Keyword: replication control, transaction response time, throughput, data availability.  
Date: May 25, 1994  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California  
Abstract: Replicated data is employed in distributed databases to enhance data availability. However, the benefit of data availability is only realized at the cost of elaborate algorithms which hide the underlying complexity of maintaining multiple copies of a single data item. The concern about the performance impact of replica control is part of the reasons why replication, although extensively researched, has yet to receive wide acceptance in practice. This paper makes use of a simulation model to explore the performance tradeoffs of data replication. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Agrawal and A. Bernstein. </author> <title> A nonblocking quorum consensus protocol for replicated data. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(2), </volume> <month> April </month> <year> 1991. </year>
Reference-contexts: Drawback (ii) is true of all algorithms which addresses network partitioning. Finally, drawback (iii) has been addressed by some enhancements to QC which allow dynamic reconfiguration of the quorums. As a replica control protocol, QC has received much attention from researchers <ref> [1, 9, 16, 18, 19, 29, 28, 27, 36, 20, 35] </ref> . Its implementation, however, is not widespread.
Reference: [2] <author> R. Agrawal, M. Carey, and M. Livny. </author> <title> The Performance of Alternative Strategies for Dealing with Deadlocks in Database Management Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(12):1348-1363, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: Transactions emanate from a client site, which is then known as the coordinator site of the transaction. Each client site has disk storage (for logs) and one or more CPUs, but contains no data partition. in <ref> [2] </ref>. Each module is briefly described as follows: 1. Transactions Generator: This is a global module which generates transactions and distributes them to the client sites. Transactions are generated independently of the processing at the client sites. 2. <p> Deadlock avoidance is implemented by using a timeout interval based on the following heuristic <ref> [2] </ref> : T imeoutInterval = (G) + k fl (G) where (G) is the average lock request response time, (G) is the standard deviation of the response time, and k is a weighting factor. Hence T imeoutInterval is dynamically adjusted to reflect on-line estimation of lock request time.
Reference: [3] <author> P.A. Alsberg and J.D.Day. </author> <title> A principle for resilient sharing of distributed resources. </title> <booktitle> Proceedings of the Second International Conference on Software Engineering, </booktitle> <month> October </month> <year> 1976. </year>
Reference-contexts: With failures, however, writing all copies can cause indefinite blocking, which is unacceptable in practice. Hence the write-all approach is modified to write all copies available to the transaction coordinator. The most commonly known protocol of this genre is the primary copy protocol <ref> [3, 32] </ref>. The protocol is designed for systems where a primary copy of each data object is located at one site, while secondary copies of the object are distributed among other sites.
Reference: [4] <author> P. Bernstein, V. Hadzilacos, and N. Goodman. </author> <title> Concurrency Control and Recovery in Database Systems. </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference-contexts: The difficulty lies in keeping the copies consistent with each other in the face of system failures while at the same time maximizing the data availability. The algorithms which address these problems are called replica control algorithms <ref> [4] </ref>. Although replica control has been the subject of intensive research for quite some time now, it has yet to fulfill its promise in practical applications. In the current state of distributed database technology, data replication, if implemented at all, is typically enforced by the read-one, write all protocol. <p> A user interacts with a database through the use of transactions. A transaction is an execution of a program which contains a sequence of data-accessing operations <ref> [4] </ref>. A distributed database system is a collection of sites each of which contains a database and the sites are connected by a communication network. <p> To preserve data integrity, it is necessary for all participating sites to consistently perform a single logical action, either commit or abort. This consistency is enforced by an atomic commitment protocol <ref> [4, 15] </ref>. In addition, concurrency control protocol, for example, two-phase locking, ensures that the interleaved execution of operations from different transactions is correct, that is, serialized [4, 15]. In a replicated database, a data item may be stored redundantly so that copies of the item are distributed on different sites. <p> This consistency is enforced by an atomic commitment protocol <ref> [4, 15] </ref>. In addition, concurrency control protocol, for example, two-phase locking, ensures that the interleaved execution of operations from different transactions is correct, that is, serialized [4, 15]. In a replicated database, a data item may be stored redundantly so that copies of the item are distributed on different sites. <p> This property, known as one-copy equivalence, is the correctness criteria for transaction execution on a replicated database system <ref> [4] </ref>. On a system with replicated data, each Read (x) is translated by the transaction coordinator into Read (x A 1 ), ..., Read (x A m ), where x A 1 is the copy of data item x at site A 1 . <p> Similarly, each Write (x) is translated into Write (x B 1 ), ..., Write (x B n ) <ref> [4] </ref>. The composition of the read set R = fA 1 ; :::; A m g and the write set W = fB 1 ; :::; B n g depends on each protocol. <p> It also does not require special treatment to recover copies, as outdated copies will not have the latest version number and so will not be read but will be over-written. The QC algorithm has some well-known drawbacks, some of which that have been cited <ref> [4] </ref> are: (i) It requires multiple reads for each read; (ii) It requires a large number of copies (2n + 1) to tolerate n site failures; and (iii) All copies of x must be known in advance (to determine the weights and hence quorum configurations). <p> To simulate caching, a hit rate of 80% is assumed for disk accesses. 6 * Sites exchange messages at an overhead of 0.001 time units for message propagation, and 0.004 time units for message processing. * At each site, strict two-phase locking is applied to individual pages for concurrency control <ref> [12, 4] </ref>. Deadlock avoidance is implemented by using a timeout interval based on the following heuristic [2] : T imeoutInterval = (G) + k fl (G) where (G) is the average lock request response time, (G) is the standard deviation of the response time, and k is a weighting factor.
Reference: [5] <author> B. Bhargava, A. Helal, and K. Friesen. </author> <title> Analyzing Availability of Replicated Database Systems. </title> <journal> International Journal in Computer Simulation, </journal> <volume> 1(4) </volume> <pages> 345-436, </pages> <year> 1991. </year>
Reference-contexts: In [7] Carey and Livny compared the performances of concurrency control algorithms in replicated systems. P^aris, Long, and Glockner evaluated consistency algorithms for replicated files in [17]. Our work is in the same vein as the SETH experiment described in <ref> [5] </ref>. However, we differ in that our model contains details of concurrency control, atomic commitment, and two different replica control protocols, and our model simulates failures.
Reference: [6] <author> B. Bhargava, P. Noll, and D. Sabo. </author> <title> An Experimental Analysis of Replicated Copy Control During Site Failure and Recovery. </title> <booktitle> In Proceedings of the fourth Internation Conference on Data Engineering, </booktitle> <pages> pages 82-91, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Most existing works are analytical in nature. However, there is a volume of work based on experimentation. Pu, Noe, and Proudfoot studied a technique of regeneration of replicated object [30] using an implementation (Eden). In <ref> [6] </ref>, Bhargava, Noll, and Sabo observed the overhead of a replica control protocol and the effects of failures on data availability on the prototype system RAID. Simulation has also been used in a number of existing works.
Reference: [7] <author> M. Carey and M. Livny. </author> <title> Conflict Detection Tradeoffs for Replicated Data. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 16(4) </volume> <pages> 703-746, </pages> <year> 1991. </year>
Reference-contexts: In [6], Bhargava, Noll, and Sabo observed the overhead of a replica control protocol and the effects of failures on data availability on the prototype system RAID. Simulation has also been used in a number of existing works. In <ref> [7] </ref> Carey and Livny compared the performances of concurrency control algorithms in replicated systems. P^aris, Long, and Glockner evaluated consistency algorithms for replicated files in [17]. Our work is in the same vein as the SETH experiment described in [5].
Reference: [8] <author> D. </author> <title> Comer. </title> <publisher> Internetworking with TCP/IP . Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: Such a list, which we term an estimated uplist, may be maintained by having the sites periodically exchanging status verification messages, as practiced by hosts on the Internet and on the Tandem non-stop system <ref> [9, 8] </ref>. In our model, we assume that such a list is available to the transaction coordinator with no additional overhead. If any site senses a failure among the server sites, a recovery process is initiated whereby data objects whose primary sites have failed may be reassigned new primary sites.
Reference: [9] <author> D. Davcev and W. Burkhard. </author> <title> Consistency and Recovery Control for Replicated Files. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 87-96, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: Drawback (ii) is true of all algorithms which addresses network partitioning. Finally, drawback (iii) has been addressed by some enhancements to QC which allow dynamic reconfiguration of the quorums. As a replica control protocol, QC has received much attention from researchers <ref> [1, 9, 16, 18, 19, 29, 28, 27, 36, 20, 35] </ref> . Its implementation, however, is not widespread. <p> Such a list, which we term an estimated uplist, may be maintained by having the sites periodically exchanging status verification messages, as practiced by hosts on the Internet and on the Tandem non-stop system <ref> [9, 8] </ref>. In our model, we assume that such a list is available to the transaction coordinator with no additional overhead. If any site senses a failure among the server sites, a recovery process is initiated whereby data objects whose primary sites have failed may be reassigned new primary sites.
Reference: [10] <author> A. El Abbadi, D. Skeen, and F. Cristian. </author> <title> A efficient, fault-tolerant protocol for replicated data management. </title> <booktitle> Proceedings 4th ACM Symposium on the Principle of Database Systems, </booktitle> <pages> pages 240-251, </pages> <month> March </month> <year> 1985. </year>
Reference-contexts: On the other hand, the Harp distributed file system [25] uses the concept of view changes, first introduced in <ref> [10] </ref>, to maintain a consistent view of the organization among each individual groups of copies. In general, an elaborate algorithm is required for the primary copy protocol to guarantee a consistent view of the replica configuration.
Reference: [11] <author> A. El Abbadi and S. Toueg. </author> <title> Maintaining Availability in Partitioned Replicated Databases. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 14(2) </volume> <pages> 264-290, </pages> <year> 1989. </year>
Reference-contexts: QC is an elegant algorithm which provides one-copy-equivalence even in the presence of network partitioning. It does so without requiring complicated management of network configurations among the replica sites which is required for other replica control algorithms such as virtual partitions <ref> [11] </ref> or Primary Copy [32, 25]. It also does not require special treatment to recover copies, as outdated copies will not have the latest version number and so will not be read but will be over-written.
Reference: [12] <author> K. P. Eswaran, J. N. Gray, R. A. Lorie, and I. L. Traiger. </author> <title> The Notions of Consistency and Predicate Locks in a Database System. </title> <journal> Communications of the ACM, </journal> 19(11) 624-633, November 1976. 
Reference-contexts: To simulate caching, a hit rate of 80% is assumed for disk accesses. 6 * Sites exchange messages at an overhead of 0.001 time units for message propagation, and 0.004 time units for message processing. * At each site, strict two-phase locking is applied to individual pages for concurrency control <ref> [12, 4] </ref>. Deadlock avoidance is implemented by using a timeout interval based on the following heuristic [2] : T imeoutInterval = (G) + k fl (G) where (G) is the average lock request response time, (G) is the standard deviation of the response time, and k is a weighting factor.
Reference: [13] <author> Domenico Ferrari. </author> <title> Computer Systems Performance Evaluation. </title> <publisher> Prentice Hall, </publisher> <year> 1978. </year>
Reference-contexts: Statistics for individual transactions are carried in the data fields of the objects for the transactions, and are accumulated using statistic procedures provided by MODSIM II. In our experiments, the means of the desired measurements are obtained by using the method of batch means <ref> [22, 13, 21] </ref>.
Reference: [14] <author> D. K. Gifford. </author> <title> Weighted Voting for Replicated Data. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 16(4) </volume> <pages> 150-159, </pages> <year> 1979. </year>
Reference-contexts: The algorithm calls for a two-phased protocol similar to the two-phase commit protocol to attain consensus among the sites or copies regarding the state of the copy sites. 3.2 The Quorum Consensus Protocol In the quorum consensus (QC) algorithm <ref> [14] </ref>, a non-negative weight is assigned to each copy of a replicated object x. A read threshold RT and a write threshold W T are also defined for x such that both 2 fl W T and (RT + W T ) exceed the total weight of all copies.
Reference: [15] <author> J. Gray and A. Reuter. </author> <title> Transaction Processing: Concepts and Techniques. </title> <publisher> Morgan Kaufman, </publisher> <year> 1993. </year>
Reference-contexts: To preserve data integrity, it is necessary for all participating sites to consistently perform a single logical action, either commit or abort. This consistency is enforced by an atomic commitment protocol <ref> [4, 15] </ref>. In addition, concurrency control protocol, for example, two-phase locking, ensures that the interleaved execution of operations from different transactions is correct, that is, serialized [4, 15]. In a replicated database, a data item may be stored redundantly so that copies of the item are distributed on different sites. <p> This consistency is enforced by an atomic commitment protocol <ref> [4, 15] </ref>. In addition, concurrency control protocol, for example, two-phase locking, ensures that the interleaved execution of operations from different transactions is correct, that is, serialized [4, 15]. In a replicated database, a data item may be stored redundantly so that copies of the item are distributed on different sites.
Reference: [16] <author> M. Herlihy. </author> <title> Dynamic Quorum Adjustment for Partitioned Data. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 12(2) </volume> <pages> 170-194, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Drawback (ii) is true of all algorithms which addresses network partitioning. Finally, drawback (iii) has been addressed by some enhancements to QC which allow dynamic reconfiguration of the quorums. As a replica control protocol, QC has received much attention from researchers <ref> [1, 9, 16, 18, 19, 29, 28, 27, 36, 20, 35] </ref> . Its implementation, however, is not widespread.
Reference: [17] <author> D.D.E. Long J.-F. P^aris and A. Glockner. </author> <title> A realistic evaluation of consistency algorithms for replicated files. </title> <booktitle> Proceedings 21st Annual Simulation, </booktitle> <pages> pages 121-130, </pages> <year> 1988. </year> <month> 17 </month>
Reference-contexts: Simulation has also been used in a number of existing works. In [7] Carey and Livny compared the performances of concurrency control algorithms in replicated systems. P^aris, Long, and Glockner evaluated consistency algorithms for replicated files in <ref> [17] </ref>. Our work is in the same vein as the SETH experiment described in [5]. However, we differ in that our model contains details of concurrency control, atomic commitment, and two different replica control protocols, and our model simulates failures.
Reference: [18] <author> S. Jajodia and D. Mutchler. </author> <title> Dynamic Voting. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 227-238, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: Drawback (ii) is true of all algorithms which addresses network partitioning. Finally, drawback (iii) has been addressed by some enhancements to QC which allow dynamic reconfiguration of the quorums. As a replica control protocol, QC has received much attention from researchers <ref> [1, 9, 16, 18, 19, 29, 28, 27, 36, 20, 35] </ref> . Its implementation, however, is not widespread.
Reference: [19] <author> S. Jajodia and D. Mutchler. </author> <title> Dynamic Voting Algorithms for Maintaining the Consistency of a Replicated Database. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 15(2) </volume> <pages> 230-280, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Drawback (ii) is true of all algorithms which addresses network partitioning. Finally, drawback (iii) has been addressed by some enhancements to QC which allow dynamic reconfiguration of the quorums. As a replica control protocol, QC has received much attention from researchers <ref> [1, 9, 16, 18, 19, 29, 28, 27, 36, 20, 35] </ref> . Its implementation, however, is not widespread.
Reference: [20] <author> A. Kumar. </author> <title> Performance Analysis of a Hierarchical Quorum Consensus Algorithm for Replicated Objects. </title> <booktitle> In Proceedings of the Tenth International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Drawback (ii) is true of all algorithms which addresses network partitioning. Finally, drawback (iii) has been addressed by some enhancements to QC which allow dynamic reconfiguration of the quorums. As a replica control protocol, QC has received much attention from researchers <ref> [1, 9, 16, 18, 19, 29, 28, 27, 36, 20, 35] </ref> . Its implementation, however, is not widespread. <p> Another example is that under QC there are schemes for using bystanders (or witnesses) in lieu of data copies to reduce overhead [27, 28]. Also, under both PC and QC, the copies can be organized in a multi-level hierarchy <ref> [20] </ref> to reduce the amount of message overhead (however, such a technique is not applicable to our model where the number of copies is small.) The key point is that there are many techniques which can be explored to improve the performance of protocols for replicated databases. 14 - Transaction Rate
Reference: [21] <author> S. Lavenberg. </author> <title> Computer Performance Modeling Handbook. </title> <publisher> Academic Press, </publisher> <year> 1983. </year>
Reference-contexts: Statistics for individual transactions are carried in the data fields of the objects for the transactions, and are accumulated using statistic procedures provided by MODSIM II. In our experiments, the means of the desired measurements are obtained by using the method of batch means <ref> [22, 13, 21] </ref>.
Reference: [22] <author> Averill M. Law and David Kelton. </author> <title> Simulation Modeling and Analysis. </title> <publisher> McGraw Hill, </publisher> <year> 1991. </year>
Reference-contexts: Statistics for individual transactions are carried in the data fields of the objects for the transactions, and are accumulated using statistic procedures provided by MODSIM II. In our experiments, the means of the desired measurements are obtained by using the method of batch means <ref> [22, 13, 21] </ref>.
Reference: [23] <author> M. Liu, A. Agrawal, and A. El Abbadi. </author> <title> A simple and efficient implementation of the quorum consensus protocol. </title> <booktitle> In Submission, </booktitle> <year> 1994. </year>
Reference-contexts: Although the basic idea is for the coordinator to dispatch an operation to a set of the copy sites such that responses are received from at least a quorum, it is not clear how this should be implemented. In <ref> [23] </ref>, we describe four implementation approaches to gather a quorum for each operation. We start with the most naive approach of dispatching an operation to all copies, and subsequently propose alternative approaches, ending with the one which offers both efficiency and high data availability.
Reference: [24] <author> M. L. Liu. </author> <title> The Design of Distributed Database Systems in the Presence of Failures. </title> <type> PhD thesis, </type> <institution> The University of California, Santa Barbara, </institution> <year> 1994. </year> <note> In preparation. </note>
Reference-contexts: After-image logging is assumed for transaction recovery. A detailed description of the model can be found in <ref> [24] </ref>. The next subsection provides an overview of the model's attributes. 4.2 Attributes of the Model Choosing the parameter settings is a difficult issue in simulating a distributed database system. Due to the large number of parameters, it is not possible to vary all of them in our experiments.
Reference: [25] <author> B. Oki and B. Liskov. Viewstamped Replicataion: </author> <title> A New Primary Copy Method to Support Highly-Available Distributed Systems. </title> <booktitle> Proceedings of the Seventh Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 8-17, </pages> <year> 1988. </year>
Reference-contexts: The primary-copy protocol is perhaps the most widely accepted replica control protocol in practice such as Distributed INGRES [33, 32], as well as in research prototypes such as the Harp Distributed File System <ref> [25] </ref>. The basic appeal of the protocol is in its conceptual simplicity, and, in environments where the primary copy can be chosen to be the closest to the transaction coordinator, the protocol yields the best response time for read operations. <p> Based on the up list and a pre-defined ordering among the sites, a group of sites can consistently determine which of the copies of a data object is the current primary copy. On the other hand, the Harp distributed file system <ref> [25] </ref> uses the concept of view changes, first introduced in [10], to maintain a consistent view of the organization among each individual groups of copies. In general, an elaborate algorithm is required for the primary copy protocol to guarantee a consistent view of the replica configuration. <p> QC is an elegant algorithm which provides one-copy-equivalence even in the presence of network partitioning. It does so without requiring complicated management of network configurations among the replica sites which is required for other replica control algorithms such as virtual partitions [11] or Primary Copy <ref> [32, 25] </ref>. It also does not require special treatment to recover copies, as outdated copies will not have the latest version number and so will not be read but will be over-written. <p> However, the update is logged to allow the failed copies to be updated upon the recovery of its host site. Our model assumes that such recovery actions take place in background mode. 7 For maintaining a consistent view of the replica configurations, we adopted the view-change algorithm out-lined in <ref> [25] </ref> with the following stipulations: Whereas [25] envisions a view for the servers associated with each individual data item, we maintained one single view, that is, a global view, for all servers. (Note a view is different from the estimated uplist provided by the underlying system.) This approach is more suitable <p> Our model assumes that such recovery actions take place in background mode. 7 For maintaining a consistent view of the replica configurations, we adopted the view-change algorithm out-lined in <ref> [25] </ref> with the following stipulations: Whereas [25] envisions a view for the servers associated with each individual data item, we maintained one single view, that is, a global view, for all servers. (Note a view is different from the estimated uplist provided by the underlying system.) This approach is more suitable for our model, where the copies <p> In addition, each operation incurs concurrency control overhead at each copy site contacted. As with PC, though to a lesser extent, the inclusion of the replica sites add to the overhead during the commit phase. It should be noted that there exists at least one implementation of replicated data <ref> [25] </ref> which claims to outperform a non-replicated, conventional system in the absence of failures. This superiority in performance, however, is achieved by using a special underlying communication support (a reliable communication buffer, among other features) which replaces conventional event logging with a special message exchange facility.
Reference: [26] <author> M. T. Ozsu and P. Valduriez. </author> <title> Distributed Database Systems: Where Are We Now? Computer, </title> <booktitle> 24(8) </booktitle> <pages> 68-78, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: In the current state of distributed database technology, data replication, if implemented at all, is typically enforced by the read-one, write all protocol. More complicated, less restrictive replica control protocols, though a popular topic for research, are not implemented in any widely-used systems <ref> [26] </ref>. A major reason for this lack of acceptance is that the performance impact of these protocols cannot be easily quantified, as very little existing performance figures of commercial database systems which support copies are available.
Reference: [27] <author> J. F. P^aris. </author> <title> Voting with Witnesses: A Consistency Scheme for Replicated Files. </title> <booktitle> In Proceedings of the Sixth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 606-612, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Drawback (ii) is true of all algorithms which addresses network partitioning. Finally, drawback (iii) has been addressed by some enhancements to QC which allow dynamic reconfiguration of the quorums. As a replica control protocol, QC has received much attention from researchers <ref> [1, 9, 16, 18, 19, 29, 28, 27, 36, 20, 35] </ref> . Its implementation, however, is not widespread. <p> As has already been mentioned, the PC protocol implemented on the Harp distributed file system makes use of special hardware to reduce message and logging overhead. Another example is that under QC there are schemes for using bystanders (or witnesses) in lieu of data copies to reduce overhead <ref> [27, 28] </ref>.
Reference: [28] <author> J. F. P^aris. </author> <title> Voting with Bystanders. </title> <booktitle> In Proceedings of the Ninth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 394-401, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Drawback (ii) is true of all algorithms which addresses network partitioning. Finally, drawback (iii) has been addressed by some enhancements to QC which allow dynamic reconfiguration of the quorums. As a replica control protocol, QC has received much attention from researchers <ref> [1, 9, 16, 18, 19, 29, 28, 27, 36, 20, 35] </ref> . Its implementation, however, is not widespread. <p> As has already been mentioned, the PC protocol implemented on the Harp distributed file system makes use of special hardware to reduce message and logging overhead. Another example is that under QC there are schemes for using bystanders (or witnesses) in lieu of data copies to reduce overhead <ref> [27, 28] </ref>.
Reference: [29] <author> J. F. P^aris and D. E. </author> <title> Long. Efficient Dynamic Voting Algorithms. </title> <booktitle> In Proceedings of the Fourth IEEE International Conference on Data Engineering, </booktitle> <pages> pages 268-275, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Drawback (ii) is true of all algorithms which addresses network partitioning. Finally, drawback (iii) has been addressed by some enhancements to QC which allow dynamic reconfiguration of the quorums. As a replica control protocol, QC has received much attention from researchers <ref> [1, 9, 16, 18, 19, 29, 28, 27, 36, 20, 35] </ref> . Its implementation, however, is not widespread.
Reference: [30] <author> C. Pu, J. D. Noe, and A. Proudfoot. </author> <title> Regeneration of Replicated Objects: A Technique and its Eden Implementation. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(7) </volume> <pages> 936-945, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Most existing works are analytical in nature. However, there is a volume of work based on experimentation. Pu, Noe, and Proudfoot studied a technique of regeneration of replicated object <ref> [30] </ref> using an implementation (Eden). In [6], Bhargava, Noll, and Sabo observed the overhead of a replica control protocol and the effects of failures on data availability on the prototype system RAID. Simulation has also been used in a number of existing works.
Reference: [31] <author> J. Seguin, G. Sergeant, and P. Wilms. </author> <title> A majority consensus algorithm for the consistency of duplicated and distributed information. </title> <booktitle> Proc. IEEE Int. Conf. Distributed Computing Systems, </booktitle> <pages> pages 617-624, </pages> <year> 1979. </year>
Reference-contexts: As the weights assigned to the copies can be seen as "votes" from them, the algorithm is also known as the Voting algorithm. In its simplest form, a quorum is a majority set, in which case the algorithm is known as Majority Voting <ref> [31, 34] </ref>. QC is an elegant algorithm which provides one-copy-equivalence even in the presence of network partitioning. It does so without requiring complicated management of network configurations among the replica sites which is required for other replica control algorithms such as virtual partitions [11] or Primary Copy [32, 25].
Reference: [32] <author> M. Stonebraker. </author> <title> Concurrency Control and Consistency of Multiple Copies of Data in Distributed INGRESS. </title> <journal> IEEE Transactions on Software Engineering, </journal> <pages> pages 188-194, </pages> <month> May </month> <year> 1979. </year>
Reference-contexts: With failures, however, writing all copies can cause indefinite blocking, which is unacceptable in practice. Hence the write-all approach is modified to write all copies available to the transaction coordinator. The most commonly known protocol of this genre is the primary copy protocol <ref> [3, 32] </ref>. The protocol is designed for systems where a primary copy of each data object is located at one site, while secondary copies of the object are distributed among other sites. <p> Unavailable secondary copies receive the update on a deferred basis. If a primary copy fails, a secondary copy is chosen to assume the role of the primary copy. The primary-copy protocol is perhaps the most widely accepted replica control protocol in practice such as Distributed INGRES <ref> [33, 32] </ref>, as well as in research prototypes such as the Harp Distributed File System [25]. <p> In other implementations, the protocol has been enhanced to tolerate network partitioning by maintaining a systemwide consistent view of the replica configuration. In INGRES <ref> [32] </ref>, the configuration is maintained system-wide in a ` data structure called an `up list". Based on the up list and a pre-defined ordering among the sites, a group of sites can consistently determine which of the copies of a data object is the current primary copy. <p> QC is an elegant algorithm which provides one-copy-equivalence even in the presence of network partitioning. It does so without requiring complicated management of network configurations among the replica sites which is required for other replica control algorithms such as virtual partitions [11] or Primary Copy <ref> [32, 25] </ref>. It also does not require special treatment to recover copies, as outdated copies will not have the latest version number and so will not be read but will be over-written. <p> Thus, each server site in the model serves as the primary for multiple files as well as the secondary for other files. The replica configuration for each data item is assumed to be known to all active sites. The deferred update scheme described in <ref> [32] </ref> is employed in our implementation. During the execution of a transaction, the coordinator forwards each operation to the primary site of the target data item. <p> The global view in fact corresponds to the uplist described in <ref> [32] </ref> and is used accordingly. For a given data item, the primary server is the server in the current uplist which has the lowest site number among the servers for the data item.
Reference: [33] <author> M. StoneBraker and E. Neuhold. </author> <title> A distributed data base version of ingres. </title> <booktitle> Proceedings 2nd Berkeley Workshop on Distributed Data Bases and Computer Networks, </booktitle> <month> May </month> <year> 1977. </year>
Reference-contexts: Unavailable secondary copies receive the update on a deferred basis. If a primary copy fails, a secondary copy is chosen to assume the role of the primary copy. The primary-copy protocol is perhaps the most widely accepted replica control protocol in practice such as Distributed INGRES <ref> [33, 32] </ref>, as well as in research prototypes such as the Harp Distributed File System [25].
Reference: [34] <author> R. H. Thomas. </author> <title> A Majority Consensus Approach to Concurrency Control for Multiple Copy Databases. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 4(2) </volume> <pages> 180-209, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: As the weights assigned to the copies can be seen as "votes" from them, the algorithm is also known as the Voting algorithm. In its simplest form, a quorum is a majority set, in which case the algorithm is known as Majority Voting <ref> [31, 34] </ref>. QC is an elegant algorithm which provides one-copy-equivalence even in the presence of network partitioning. It does so without requiring complicated management of network configurations among the replica sites which is required for other replica control algorithms such as virtual partitions [11] or Primary Copy [32, 25].
Reference: [35] <author> Z. Tong and R. Y. Kain. </author> <title> Vote Assignments in Weighted Voting Mechanisms. </title> <booktitle> In Proceedings of the Seventh Symposium on Reliable Distributed Systems, </booktitle> <pages> pages 138-143, </pages> <month> October </month> <year> 1988. </year> <month> 18 </month>
Reference-contexts: Drawback (ii) is true of all algorithms which addresses network partitioning. Finally, drawback (iii) has been addressed by some enhancements to QC which allow dynamic reconfiguration of the quorums. As a replica control protocol, QC has received much attention from researchers <ref> [1, 9, 16, 18, 19, 29, 28, 27, 36, 20, 35] </ref> . Its implementation, however, is not widespread.
Reference: [36] <author> R. van Rennesse and A. Tanenbaum. </author> <title> Voting with Ghosts. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computing Systems, </booktitle> <pages> pages 456-462, </pages> <month> June </month> <year> 1988. </year> <month> 19 </month>
Reference-contexts: Drawback (ii) is true of all algorithms which addresses network partitioning. Finally, drawback (iii) has been addressed by some enhancements to QC which allow dynamic reconfiguration of the quorums. As a replica control protocol, QC has received much attention from researchers <ref> [1, 9, 16, 18, 19, 29, 28, 27, 36, 20, 35] </ref> . Its implementation, however, is not widespread.
References-found: 36

