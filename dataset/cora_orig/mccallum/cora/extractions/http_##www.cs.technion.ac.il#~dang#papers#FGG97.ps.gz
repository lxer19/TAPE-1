URL: http://www.cs.technion.ac.il/~dang/papers/FGG97.ps.gz
Refering-URL: http://www.cs.technion.ac.il/~dang/
Root-URL: 
Email: nir@cs.berkeley.edu  dang@cs.technion.ac.il  MOISES GOLDSZMIDT moises@erg.sri.com  
Title: Bayesian Network Classifiers.  
Author: NIR FRIEDMAN DAN GEIGER Editor: Gregory Provan 
Address: 387 Soda Hall, University of California, Berkeley, CA 94720  Israel, 32000  333 Ravenswood Ave., Menlo Park, CA 94025  
Affiliation: Computer Science Division,  Technion, Computer Science Department, Haifa,  SRI International,  
Note: 1-36 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Abstract: Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state-of-the-art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we evaluate approaches for inducing classifiers from data, based on the theory of learning Bayesian networks. Bayesian networks are factored representations of probability distributions that generalize the naive Bayesian classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that are characteristic of naive Bayes. We experimentally tested these approaches, using benchmark problems from the University of California at Irvine repository, and compared them to C4.5, naive Bayes, and wrapper-based feature selection methods. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Binder, J., D. Koller, S. Russell, and K. </author> <title> Kanazawa (1997). Adaptive probabilistic networks with hidden variables. </title> <booktitle> Machine Learning (this volume). </booktitle>
Reference-contexts: Moreover, to evaluate the optimal choice of parameters for a candidate network structure, we must perform nonlinear optimization using either EM (Lauritzen, 1995) or gradient descent <ref> (Binder, Koller, Russell, and Kanazawa, 1997) </ref>. The problem of selecting the best structure is usually intractable in the presence of missing values. Several recent works (Geiger, Heckerman, and Meek, 1996; Chick-ering and Heckerman, 1996) examined approximation to the marginal score that can be evaluated efficiently.
Reference: <author> Bouckaert, R. R. </author> <year> (1994). </year> <title> Properties of Bayesian network learning algorithms. </title> <editor> In R. Lopez de Mantaras and D. Poole (Eds.), </editor> <booktitle> Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI '94), </booktitle> <pages> 102-109. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Buntine, W. </author> <year> (1991). </year> <title> Theory refinement on Bayesian networks. </title> <editor> In B. D. D'Ambrosio, P. Smets, and P. P. Bonissone (Eds.), </editor> <booktitle> Proceedings of the Seventh Annual Conference on Uncertainty Artificial Intelligence (UAI '92), </booktitle> <pages> 52-60. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Buntine, W. </author> <year> (1996). </year> <title> A guide to the literature on learning probabilistic networks from data. </title> <journal> IEEE Trans. on Knowledge and Data Engineering 8, </journal> <pages> 195-210. </pages>
Reference: <author> Cestnik, B. </author> <year> (1990). </year> <title> Estimating probabilities: a crucial task in machine learning. </title> <editor> In L. </editor> <address> C. </address>
Reference-contexts: Since I (X; Y) I (X; Y [ Z), we immediately derive LL (BjD) LL (B 0 jD). 34 N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT Notes 1. TAN structures were called "Bayesian conditional trees" in (Geiger, 1992). 2. An alternative notion of smoothing was investigated by <ref> (Cestnik, 1990) </ref> in the context of learning Naive Bayesian classifiers. 3. The choice of 5 folds is based on the recommendations of (Kohavi, 1995).
Reference: <editor> Aiello (Ed.), </editor> <booktitle> Proceedings of the 9th European Conference on Artificial Intelligence (ECAI 90), </booktitle> <pages> 147-149. </pages> <address> London: </address> <publisher> Pitman. </publisher>
Reference: <author> Chickering, D. M. and D. </author> <title> Heckerman (1996). Efficient approximations for the marginal likelihood of incomplete data given a Bayesian network. </title> <editor> In E. Horvits and F. Jenssen (Eds.), </editor> <booktitle> Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence (UAI '96), </booktitle> <pages> 158-168. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Chow, C. K. and C. N. </author> <title> Liu (1968). Approximating discrete probability distributions with dependence trees. </title> <booktitle> IEEE Transaction on Information Theory 14, </booktitle> <pages> 462-467. </pages>
Reference-contexts: Transform the resulting undirected tree to a directed one by choosing a root variable and setting the direction of all edges to be outward from it. Chow and Liu prove that this procedure finds the tree that maximizes the likelihood given D. Theorem 1 <ref> (Chow and Liu, 1968) </ref> Let D be a collection of N instances of X 1 ; : : : ; X n . The Construct-Tree procedure constructs a tree B T that maximizes LL (B T jD) and has time complexity O (n 2 N ). <p> Indeed, the construction of a set of trees that minimizes the log likelihood score was the original method used by Chow and Liu to build classifiers for a script-letters recognition task <ref> (Chow and Liu, 1968) </ref>. In fact, they reported that in their experiments the error rate of this method was less than half that of naive Bayes. Thus, we apply the algorithm in Theorem 1 separately to the attributes that correspond to each value of the class variable. <p> Thus, we apply the algorithm in Theorem 1 separately to the attributes that correspond to each value of the class variable. This results in a multinet where each network is a tree. Corollary 1 <ref> (Chow and Liu, 1968) </ref> Let D be a collection of N instances of C; A 1 ; : : : ; A n . There is a procedure of time complexity O (n 2 N ) which constructs a multinet consisting of trees that maximizes log likelihood.
Reference: <author> Cooper, G. F. and E. </author> <title> Herskovits (1992). A Bayesian method for the induction of probabilistic networks from data. </title> <booktitle> Machine Learning 9, </booktitle> <pages> 309-347. </pages>
Reference: <author> Cormen, T. H., C. E. Leiserson, and R. L. </author> <title> Rivest (1990). Introduction to Algorithms. </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: There are well-known algorithms to solving MWST of time complexity O (n 2 log n) where n is the number of vertices in the graph <ref> (Cormen, Leiserson, and Rivest, 1990) </ref>. The Construct-Tree procedure of Chow and Liu consists of the following steps: 1.
Reference: <author> Cover, T. M. and J. A. </author> <title> Thomas (1991). Elements of Information Theory. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: Hence, we need to maximize the term X I ^ P D (A i ; A (i) ; C) + i;(i)=0 We simplify this term by using the identity known as the chain law for mutual information <ref> (Cover and Thomas, 1991) </ref>: I P (X; Y; Z) = I P (X; Z) + I P (X; YjZ): Hence, we can rewrite the (8) above as X I ^ P D (A i ; C) + i;(i)&gt;0 Note that the first summand is not affected by the choice of (i).
Reference: <author> Dawid, A. P. </author> <year> (1976). </year> <title> Properties of diagnostic data distributions. </title> <type> Biometrics 32, </type> <pages> 647-658. </pages>
Reference: <author> DeGroot, M. H. </author> <year> (1970). </year> <title> Optimal Statistical Decisions. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Domingos, P. and M. </author> <title> Pazzani (1996). Beyond independence: Conditions for the optimality of the simple Bayesian classifier. </title> <editor> In L. Saitta (Ed.), </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> 105-112. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dougherty, J., R. Kohavi, and M. </author> <title> Sahami (1995). Supervised and unsupervised discretization of continuous features. </title> <editor> In A. Prieditis and S. Russell (Eds.), </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning. </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Duda, R. O. and P. E. </author> <title> Hart (1973). Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Ezawa, K. J. and T. </author> <month> Schuermann </month> <year> (1995). </year> <title> Fruad/uncollectable debt detection using a Bayesian network based learning system: A rare binary outcome with mixed data structures. </title> <editor> In P. Besnard and S. Hanks (Eds.), </editor> <booktitle> Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI '95), </booktitle> <pages> 157-166. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fayyad, U. M. and K. B. </author> <title> Irani (1993). Multi-interval discretization of continuous-valued attributes for classification learning. </title> <editor> In R. Bajcsy (Ed.), </editor> <booktitle> Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI '93), </booktitle> <pages> 1022-1027. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. BAYESIAN NETWORK CLASSIFIERS 35 Friedman, </publisher> <editor> J. </editor> <year> (1997a). </year> <title> On bias, variance, 0/1 loss, and the curse-of-dimensionality. Data Mining and Knowledge Discovery 1. </title> <publisher> in press. </publisher>
Reference: <author> Friedman, N. </author> <year> (1997b). </year> <title> Learning Bayesian networks in the presence of missing values and hidden variables. </title> <editor> In D. Fisher (Ed.), </editor> <booktitle> Proceedings of the Fourteenth International Conference on Machine Learning. </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. in press. </publisher>
Reference: <author> Friedman, N. and M. </author> <title> Goldszmidt (1996a). Building classifiers using Bayesian networks. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI '96), </booktitle> <pages> 1277-1284. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Friedman, N. and M. </author> <title> Goldszmidt (1996b). Discretization of continuous attributes while learning Bayesian networks. </title> <editor> In L. Saitta (Ed.), </editor> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning. </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Friedman, N. and M. </author> <title> Goldszmidt (1996c). Learning Bayesian networks with local structure. </title>
Reference: <editor> In E. Horvits and F. Jenssen (Eds.), </editor> <booktitle> Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence (UAI '96). </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Geiger, D. </author> <year> (1992). </year> <title> An entropy-based learning algorithm of Bayesian conditional trees. </title> <editor> In D. Dubois, M. P. Wellman, B. D. D'Ambrosio, and P. Smets (Eds.), </editor> <booktitle> Proceedings of the Eighth Annual Conference on Uncertainty Artificial Intelligence (UAI '92), </booktitle> <pages> 92-97. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Then, X i 0 X i for all i. Since I (X; Y) I (X; Y [ Z), we immediately derive LL (BjD) LL (B 0 jD). 34 N. FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT Notes 1. TAN structures were called "Bayesian conditional trees" in <ref> (Geiger, 1992) </ref>. 2. An alternative notion of smoothing was investigated by (Cestnik, 1990) in the context of learning Naive Bayesian classifiers. 3. The choice of 5 folds is based on the recommendations of (Kohavi, 1995).
Reference: <author> Geiger, D. and D. </author> <title> Heckerman (1996). Knowledge representation and inference in similarity networks and Bayesian multinets. </title> <booktitle> Artificial Intelligence 82, </booktitle> <pages> 45-74. </pages>
Reference: <author> Geiger, D., D. Heckerman, and C. </author> <title> Meek (1996). Asymptotic model selection for directed graphs with hidden variables. </title> <editor> In E. Horvits and F. Jenssen (Eds.), </editor> <booktitle> Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence (UAI '96). </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Heckerman, D. </author> <year> (1991). </year> <title> Probabilistic Similarity Networks. </title> <address> Cambirdge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Heckerman, D. </author> <year> (1995). </year> <title> A tutorial on learning Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research. </institution>
Reference-contexts: On the other hand, if the number of instances is small, then the prior dominates. In the context of learning Bayesian networks, we can use a different Dirichlet prior for each distribution of X i given a particular value of its parents; see <ref> (Heckerman, 1995) </ref>.
Reference: <author> Heckerman, D. and D. </author> <title> Geiger (1995). Learning Bayesian networks: a unification for discrete and gaussian domains. </title> <editor> In P. Besnard and S. Hanks (Eds.), </editor> <booktitle> Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI '95), </booktitle> <pages> 274-284. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: On the other hand, if the number of instances is small, then the prior dominates. In the context of learning Bayesian networks, we can use a different Dirichlet prior for each distribution of X i given a particular value of its parents; see <ref> (Heckerman, 1995) </ref>.
Reference: <author> Heckerman, D., D. Geiger, and D. M. </author> <title> Chickering (1995). Learning Bayesian networks: The combination of knowledge and statistical data. </title> <booktitle> Machine Learning 20, </booktitle> <pages> 197-243. </pages>
Reference-contexts: On the other hand, if the number of instances is small, then the prior dominates. In the context of learning Bayesian networks, we can use a different Dirichlet prior for each distribution of X i given a particular value of its parents; see <ref> (Heckerman, 1995) </ref>.
Reference: <author> John, G., R. Kohavi, and K. </author> <month> Pfleger </month> <year> (1994). </year> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> 121-129. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: All of the datasets are from the U. C. Irvine repository (Murphy and Aha, 1995), with the exception of "mofn-3-7-10" and "corral". These two artificial datasets were used for the evaluation of feature subset selection methods by <ref> (John, Kohavi, and Pfleger, 1994) </ref>. The accuracy of each classifier is based on the percentage of successful predictions on the test sets of each dataset. <p> These two artificial datasets were used for the evaluation of feature subset selection methods by (John, Kohavi, and Pfleger, 1994). The accuracy of each classifier is based on the percentage of successful predictions on the test sets of each dataset. We used the MLC++ system <ref> (Kohavi, John, Long, Manley, and Pfleger, 1994) </ref> to estimate the prediction accuracy for each classifier as well as the variance of this accuracy.
Reference: <author> John, G. H. and P. </author> <title> Langley (1995). Estimating continuous distributions in Bayesian classifiers. </title>
Reference: <editor> In P. Besnard and S. Hanks (Eds.), </editor> <booktitle> Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI '95), </booktitle> <pages> 338-345. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R. </author> <year> (1995). </year> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection. </title> <editor> In C. S. Mellish (Ed.), </editor> <booktitle> Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI '95), </booktitle> <pages> 1137-1143. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT Notes 1. TAN structures were called "Bayesian conditional trees" in (Geiger, 1992). 2. An alternative notion of smoothing was investigated by (Cestnik, 1990) in the context of learning Naive Bayesian classifiers. 3. The choice of 5 folds is based on the recommendations of <ref> (Kohavi, 1995) </ref>.
Reference: <author> Kohavi, R., G. John, R. Long, D. Manley, and K. </author> <month> Pfleger </month> <year> (1994). </year> <title> MLC++: A machine learning library in C++. </title> <booktitle> In Proceedings of the Sixth International Conference on Tools with Artificial Intelligence, </booktitle> <pages> 740-743. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: All of the datasets are from the U. C. Irvine repository (Murphy and Aha, 1995), with the exception of "mofn-3-7-10" and "corral". These two artificial datasets were used for the evaluation of feature subset selection methods by <ref> (John, Kohavi, and Pfleger, 1994) </ref>. The accuracy of each classifier is based on the percentage of successful predictions on the test sets of each dataset. <p> These two artificial datasets were used for the evaluation of feature subset selection methods by (John, Kohavi, and Pfleger, 1994). The accuracy of each classifier is based on the percentage of successful predictions on the test sets of each dataset. We used the MLC++ system <ref> (Kohavi, John, Long, Manley, and Pfleger, 1994) </ref> to estimate the prediction accuracy for each classifier as well as the variance of this accuracy.
Reference: <author> Kononenko, I. </author> <year> (1991). </year> <title> Semi-naive Bayesian classifier. </title> <editor> In Y. Kodratoff (Ed.), </editor> <booktitle> Proceedings of the Sixth European Working Session on Learning, </booktitle> <pages> 206-219. </pages> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Work in the second category (Kononenko, 1991; Pazzani, 1995; Ezawa and Schuer-mann, 1995) are closer in spirit to our proposal, since they attempt to improve the predictive accuracy by removing some of the independence assumptions. The sem-inaive Bayesian classifier <ref> (Kononenko, 1991) </ref> is a model of the form P (C; A 1 ; : : : ; A n ) = P (C) P (A 1 jC) P (A k jC) (9) where A 1 ; : : : ; A k are pairwise disjoint clusters of attributes.
Reference: <author> Kullback, S. and R. A. </author> <month> Leibler </month> <year> (1951). </year> <title> On information and sufficiency. </title> <journal> Annals of Mathematical Statistics 22, </journal> <pages> 76-86. </pages>
Reference-contexts: This reading suggests that by maximizing the log likelihood we are minimizing the description of D. Another way of viewing this optimization process is to use cross-entropy (also known as the Kullback-Leibler divergence <ref> (Kullback and Leibler, 1951) </ref>). Cross entropy is a measure of distance between two probability distributions. Formally, D (P (X)jjQ (X)) = x2Val (X) P (x) : (A.1) One information-theoretic interpretation of cross-entropy is the average redundancy incurred in encoding when we use a wrong probability measure.
Reference: <author> Lam, W. and F. </author> <title> Bacchus (1994). Learning Bayesian belief networks. An approach based on the MDL principle. </title> <booktitle> Computational Intelligence 10, </booktitle> <pages> 269-293. </pages>
Reference: <author> Langley, P., W. Iba, and K. </author> <title> Thompson (1992). An analysis of Bayesian classifiers. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI '92), </booktitle> <pages> 223-228. </pages> <address> Menlo Park, CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: This is in fact the definition of naive Bayes commonly found in the literature <ref> (Langley, Iba, and Thompson, 1992) </ref>. The problem of learning a Bayesian network can be informally stated as follows. Given a training set D = fu 1 ; : : : ; u N g of instances of U, find a network B that best matches D.
Reference: <author> Langley, P. and S. </author> <title> Sage (1994). Induction of selective Bayesian classifiers. </title> <editor> In R. Lopez de Mantaras and D. Poole (Eds.), </editor> <booktitle> Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence (UAI '94), </booktitle> <pages> 399-406. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher> <address> 36 N. </address> <note> FRIEDMAN, </note> <author> D. GEIGER, AND M. GOLDSZMIDT Lauritzen, S. L. </author> <year> (1995). </year> <title> The EM algorithm for graphical association models with missing data. </title> <journal> Computational Statistics and Data Analysis 19, </journal> <pages> 191-201. </pages>
Reference: <author> Lewis, P. M. </author> <year> (1959). </year> <title> Approximating probability distributions to reduce storage requirements. </title> <booktitle> Information and Control 2, </booktitle> <pages> 214-225. </pages>
Reference: <author> Murphy, P. M. and D. W. </author> <note> Aha (1995). UCI repository of machine learning databases. http:// www.ics.uci.edu/~mlearn/MLRepository.html. </note>
Reference-contexts: We ran this experiment on 25 datasets, 23 of which were from the U. C. Irvine repository <ref> (Murphy and Aha, 1995) </ref>. Section 5 describes in detail the experimental setup, evaluation methods, and results. As can be seen from the results shown in Figure 2, the classifier based on unrestricted networks performed significantly better than naive Bayes on six datasets, and performed significantly worse on six datasets. <p> Moreover, in some datasets they have better accuracy than TAN and Chow's and Liu's classifier. 5. Experimental Methodology and Results We ran our experiments on the 25 datasets listed in Table 1. All of the datasets are from the U. C. Irvine repository <ref> (Murphy and Aha, 1995) </ref>, with the exception of "mofn-3-7-10" and "corral". These two artificial datasets were used for the evaluation of feature subset selection methods by (John, Kohavi, and Pfleger, 1994).
Reference: <author> Pazzani, M. J. </author> <year> (1995). </year> <title> Searching for dependencies in Bayesian classifiers. </title> <editor> In D. Fisher and H. Lenz (Eds.), </editor> <booktitle> Proceedings of the fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <address> Fort Lauderdale, FL. </address>
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This example raises the following question: can we improve the performance of naive Bayesian classifiers by avoiding unwarranted (by the data) assumptions about independence? In order to tackle this problem effectively, we need an appropriate language and efficient machinery to represent and manipulate independence assertions. Bayesian networks provide both <ref> (Pearl, 1988) </ref>. Bayesian networks are directed acyclic graphs that allow efficient and effective representation of the joint probability distribution over a set of random variables. Each vertex in the graph represents a random variable, and edges represent direct correlations between the variables. <p> As we show, this approximation is optimal, in a precise sense; moreover, we can learn TAN classifiers in polynomial time. This result extends a well-known result by Chow and Liu (1968) (see also <ref> (Pearl, 1988) </ref>) for learning tree-structured Bayesian networks. Finally, we also examine a generalization of these models based on the idea that correlations among attributes may vary according to the specific instance of the class variable. Thus, instead of one TAN model we have a collection of networks as the classifier. <p> We base our definition of relevant attributes on the notion of a Markov blanket. The Markov blanket of a variable X consists of X's parents, X's children, and the parents of X's children in a given network structure G <ref> (Pearl, 1988) </ref>. This BAYESIAN NETWORK CLASSIFIERS 9 (solid line, x-axis) to naive Bayes (dashed line, y-axis). <p> FRIEDMAN, D. GEIGER, AND M. GOLDSZMIDT The procedure for learning these edges is based on a well known method by Chow and Liu (1968), for learning tree-like Bayesian networks (see also <ref> (Pearl, 1988, pp. 387-390) </ref>). We start by reviewing Chow's and Liu's result.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The complete list of results for the smoothed version of naive Bayes is reported in Table 3. Given that TAN performs better than naive Bayes and that naive Bayes is comparable to C4.5 <ref> (Quinlan, 1993) </ref>, a state-of-the-art decision tree learner, we may infer that TAN should perform rather well in comparison to C4.5. To confirm this inference, we performed experiments comparing TAN to C4.5, and to the selective naive Bayesian classifier (Langley and Sage, 1994; John, Kohavi, and Pfleger, 1994). <p> the paper: NB: the naive Bayesian classifier BN: unrestricted Bayesian networks learned with the MDL score TAN s : TAN networks learned according to Theorem 2, with smoothed parameters C+L s : Chow and Liu method|Bayesian multinets learned according to Theo rem 1|with smoothed parameters C4.5: the decision-tree classifier of <ref> (Quinlan, 1993) </ref> SNB: the selective naive Bayesian classifier , a wrapper-based feature selection ap plied to naive Bayes, using the implementation of John, Kohavi, and Pfleger (1994) In the previous sections we discussed these results in some detail. We now summarize the highlights.
Reference: <author> Ripley, B. D. </author> <year> (1996). </year> <title> Pattern recognition and neural networks. </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference-contexts: In general, neither of these approaches dominates the other <ref> (Ripley, 1996) </ref>. The naive Bayesian classifier and the extensions we have evaluated belongs to the sampling paradigm. Although the unrestricted Bayesian networks (described in Section 3) do not strictly belong in either paradigm, they are closer in spirit to the sampling paradigm. 6.3.
Reference: <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data description. </title> <type> Automatica 14, </type> <pages> 465-471. </pages>
Reference-contexts: An-in depth discussion of the pros and cons of each scoring function is beyond the scope of this paper. From now on, we concentrate on the MDL scoring function. The MDL principle <ref> (Rissanen, 1978) </ref> casts learning in terms of data compression. Roughly speaking, the goal of the learner is to find a model that facilitates the shortest description of the original data.
Reference: <author> Singh, M. and G. M. </author> <title> Provan (1995). A comparison of induction algorithms for selective and nonselective Bayesian classifiers. </title> <editor> In A. Prieditis and S. Russell (Eds.), </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning. </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Spiegelhalter, D. J., A. P. Dawid, S. L. Lauritzen, and R. G. </author> <title> Cowell (1993). Bayesian analysis in expert systems. </title> <booktitle> Statistical Science 8, </booktitle> <pages> 219-283. </pages>
Reference: <author> Suzuki, J. </author> <year> (1993). </year> <title> A construction of Bayesian networks from databases based on an MDL scheme. </title>
Reference: <editor> In D. Heckerman and A. Mamdani (Eds.), </editor> <booktitle> Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence (UAI '93), </booktitle> <pages> 266-273. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 51

