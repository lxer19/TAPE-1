URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1991/tr-91-030.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1991.html
Root-URL: http://www.icsi.berkeley.edu
Title: PROBABILITY ESTIMATION BY FEED-FORWARD NETWORKS IN CONTINUOUS SPEECH RECOGNITION  
Phone: 1-415-642-4274 FAX 1-415-643-7684  
Author: Steve Renals, Nelson Morgan and Herve Bourlard 
Note: L&H Speechproducts, Ieper, B-8900, Belgium  
Date: 28 August 1991  
Address: I 1947 Center Street Suite 600 Berkeley, California 94704  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-91-030  
Abstract: We review the use of feed-forward networks as estimators of probability densities in hidden Markov modelling. In this paper we are mostly concerned with radial basis functions (RBF) networks. We note the isomorphism of RBF networks to tied mixture density estimators; additionally we note that RBF networks are trained to estimate posteriors rather than the likelihoods estimated by tied mixture density estimators. We show how the neural network training should be modified to resolve this mismatch. We also discuss problems with discriminative training, particularly the problem of dealing with unlabelled training data and the mismatch between model and data priors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, and Robert L. Mercer. </author> <title> Maximum mutual information estimation of hidden Markov model parameters for speech recognition. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 49-52, </pages> <address> Tokyo, </address> <year> 1986. </year>
Reference-contexts: It is possible to attempt a global optimisation in which all the parameters of the HMM are optimised simultaneously according to some discriminative criterion. Such an approach was first proposed by Bahl et al. <ref> [1] </ref> who presented a training scheme for continuous HMMs in which the mutual information between the acoustic evidence and the word sequence was maximised using gradient descent.
Reference: [2] <author> Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. </author> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-5:179-190, </volume> <year> 1983. </year>
Reference-contexts: A set of initial state probabilities must also be specified. The transition probabilities and the parameters of the output PDFs are frequently estimated using a maximum likelihood training procedure, the forward-backward algorithm (see e.g. <ref> [2] </ref>). This procedure is optimal if the true model is in the space of models being searched 1 . However, this is not the case for speech recognition. What is desired is not the best possible model of each class, but the best set of models for discrimination between classes.
Reference: [3] <author> Jerome R. Bellegarda and David Nahamoo. </author> <title> Tied mixture continuous parameter modeling for continuous speech recognition. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> 38 </volume> <pages> 2033-2045, </pages> <year> 1990. </year>
Reference-contexts: We survey this problem and discuss some possible solutions. TIED MIXTURE HMM Tied mixture density (or semi-continuous) HMMs have proven to be powerful PDF estimators in continuous speech recognition <ref> [13, 3] </ref>. This method may be regarded as intermediate between discrete vector-quantised methods and separate continuous PDF estimates for each state. If a unified formalism for both discrete and continuous HMMs is adopted, then tied mixture density modelling may be regarded as an interpolation between discrete and continuous modelling [3]. <p> This method may be regarded as intermediate between discrete vector-quantised methods and separate continuous PDF estimates for each state. If a unified formalism for both discrete and continuous HMMs is adopted, then tied mixture density modelling may be regarded as an interpolation between discrete and continuous modelling <ref> [3] </ref>. Essentially, tied mixture modelling has a single "codebook" of Gaussians shared by all output PDFs. Each of these PDFs has its own set of mixture coefficients used to combine the individual Gaussians.
Reference: [4] <author> Yoshua Bengio, Renato de Mori, Giovammi Flammia, and Ralf Kompe. </author> <title> Global optimization of a neural network hidden Markov model hybrid. </title> <type> Technical Report TR-SOCS-90.22, </type> <institution> McGill University School of Computer Science, </institution> <year> 1990. </year>
Reference-contexts: This is not desirable as it assumes uniform priors rather than those specified by the language model. Initial work in using global optimisation methods for continuous speech recognition has been performed by Bridle [7] and Bengio <ref> [4] </ref>; both of these involved training the parameters of the HMM by a maximum likelihood process, using the "alphanets" method to optimise the input parameters via some (linear or non-linear) transform.
Reference: [5] <author> H. Bourlard and N. Morgan. </author> <title> A continuous speech recognition system embedding MLP into HMM. </title> <editor> In David S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 413-416. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1990. </year>
Reference-contexts: This computation may be efficiently performed using a dynamic programming algorithm. When used at recognition time this is referred to as Viterbi decoding. 1 And if some other conditions are satisfied [11]. 1 We have used discriminatively trained classifiers to estimate the output PDFs <ref> [5, 14, 17] </ref>. It may be shown that a "1-from-n" classifier trained using a relative entropy (or a least mean squares) objective function outputs the posterior probabilities, P (q l jx), of each class given the input data [6]. <p> In this method after an initial network training on labelled data and Viterbi segmentation, the targets used in training the unlabelled data are updated by performing a Viterbi segmentation after each epoch of discriminative training. Such an approach has been referred to as embedded MLP <ref> [5] </ref> or connectionist Viterbi training [12]. It should be noted that the transition probabilities are still optimised by a maximum likelihood criterion (or the Viterbi approximation to it).
Reference: [6] <author> H. Bourlard and C. J. Wellekens. </author> <title> Links between Markov models and multilayer perceptrons. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-12:1167-1178, </volume> <year> 1990. </year>
Reference-contexts: It may be shown that a "1-from-n" classifier trained using a relative entropy (or a least mean squares) objective function outputs the posterior probabilities, P (q l jx), of each class given the input data <ref> [6] </ref>. However, the likelihoods P (xjq l ) are required; the prior probabilities, p (q l ) are given by the allowable sentence models constructed from the basic HMMs using a phone-structured lexicon and the language model. <p> It should be noted that the transition probabilities are still optimised by a maximum likelihood criterion (or the Viterbi approximation to it). It may be proved that performing a Viterbi segmentation using posterior local probabilities will also 6 result in a global optimisation <ref> [6] </ref>: however, there is a mismatch between model and data priors here (see next section). It is possible to attempt a global optimisation in which all the parameters of the HMM are optimised simultaneously according to some discriminative criterion. <p> PROBLEMS WITH DISCRIMINATIVE TRAINING It has been shown, both theoretically and in practice, that the training and recognition procedures used with standard HMMs remain valid for posterior probabilities <ref> [6] </ref>. Why then do we replace these posterior probabilities with likelihoods? The answer to this problem lies in a mismatch between the prior probabilities given by the training data and those imposed by the topology of the HMMs. Choosing the HMM topology also amounts to fixing the priors.
Reference: [7] <author> J. S. Bridle and L. Dodd. </author> <title> An alphanet aproach to optimising input transformations for continuous speech recognition. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 277-280, </pages> <address> Toronto, </address> <year> 1991. </year>
Reference-contexts: This is not desirable as it assumes uniform priors rather than those specified by the language model. Initial work in using global optimisation methods for continuous speech recognition has been performed by Bridle <ref> [7] </ref> and Bengio [4]; both of these involved training the parameters of the HMM by a maximum likelihood process, using the "alphanets" method to optimise the input parameters via some (linear or non-linear) transform.
Reference: [8] <author> John S. Bridle. Alpha-nets: </author> <title> a recurrent neural network architecture with a hidden Markov model interpretation. </title> <journal> Speech Communication, </journal> <volume> 9 </volume> <pages> 83-92, </pages> <year> 1990. </year>
Reference-contexts: Such an approach was first proposed by Bahl et al. [1] who presented a training scheme for continuous HMMs in which the mutual information between the acoustic evidence and the word sequence was maximised using gradient descent. More recently, Bridle introduced the "alphanet" representation <ref> [8] </ref> of HMMs, in which the computation of the HMM "forward" probabilities a jt = P (X t 1 , q (t) = j) is performed by the forward dynamics of a recurrent network. Alphanets may be discriminatively trained by min-imising a relative entropy objective function.
Reference: [9] <author> John S. Bridle. </author> <title> Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. </title> <editor> In David S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 2, </volume> <pages> pages 211-217. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1990. </year>
Reference-contexts: Bridle has demonstrated that minimising this error function is equivalent to maximising the mutual information between the acoustic evidence and HMM state sequence <ref> [9] </ref>. If we wish to interpret the weights as mixture coefficients, then we must ensure that they are non-negative and sum to 1.
Reference: [10] <author> D. S. Broomhead and David Lowe. </author> <title> Multi-variable functional interpolation and adaptive networks. </title> <journal> Complex Systems, </journal> <volume> 2 </volume> <pages> 321-355, </pages> <year> 1988. </year>
Reference-contexts: RADIAL BASIS FUNCTIONS The radial basis functions (RBF) network was originally introduced as a means of function interpolation <ref> [16, 10] </ref>. A set of K approximating functions, k (x) is constructed from a set of J basis functions f (x): k (x) = j=1 This equation defines a RBF network with J RBFs (hidden units) and K outputs. The output units here are linear, with weights a kj .
Reference: [11] <author> Peter F. Brown. </author> <title> The Acoustic-Modelling Problem in Automatic Speech Recognition. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1987. </year>
Reference-contexts: This computation may be efficiently performed using a dynamic programming algorithm. When used at recognition time this is referred to as Viterbi decoding. 1 And if some other conditions are satisfied <ref> [11] </ref>. 1 We have used discriminatively trained classifiers to estimate the output PDFs [5, 14, 17].
Reference: [12] <author> Michael A. Franzini, Kai-Fu Lee, and Alex Waibel. </author> <title> Connectionist Viterbi train-ing: a new hybrid method for continuous speech recognition. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 425-428, </pages> <address> Albuquerque, </address> <year> 1990. </year>
Reference-contexts: Such an approach has been referred to as embedded MLP [5] or connectionist Viterbi training <ref> [12] </ref>. It should be noted that the transition probabilities are still optimised by a maximum likelihood criterion (or the Viterbi approximation to it).
Reference: [13] <author> X. D. Huang and M. A. Jack. </author> <title> Semi-continuous hidden Markov models for speech signals. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 3 </volume> <pages> 239-251, </pages> <year> 1989. </year>
Reference-contexts: We survey this problem and discuss some possible solutions. TIED MIXTURE HMM Tied mixture density (or semi-continuous) HMMs have proven to be powerful PDF estimators in continuous speech recognition <ref> [13, 3] </ref>. This method may be regarded as intermediate between discrete vector-quantised methods and separate continuous PDF estimates for each state. If a unified formalism for both discrete and continuous HMMs is adopted, then tied mixture density modelling may be regarded as an interpolation between discrete and continuous modelling [3].
Reference: [14] <author> N. Morgan, H. Hermansky, H. Bourlard, C. Wooters, and P. Kohn. </author> <title> Continuous speech recognition using PLP analysis with multi-layer perceptrons. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 49-52, </pages> <address> Toronto, </address> <year> 1991. </year>
Reference-contexts: This computation may be efficiently performed using a dynamic programming algorithm. When used at recognition time this is referred to as Viterbi decoding. 1 And if some other conditions are satisfied [11]. 1 We have used discriminatively trained classifiers to estimate the output PDFs <ref> [5, 14, 17] </ref>. It may be shown that a "1-from-n" classifier trained using a relative entropy (or a least mean squares) objective function outputs the posterior probabilities, P (q l jx), of each class given the input data [6].
Reference: [15] <author> Douglas B. Paul, James K. Baker, and Janet M. Baker. </author> <title> On the interaction between true source, training and testing language models. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 569-572, </pages> <address> Toronto, </address> <year> 1991. </year>
Reference-contexts: Unfortunately this results in a huge number of parameters that would require an unrealistic amount of training data to estimate them significantly. This problem has also been raised in the context of language modelling <ref> [15] </ref>. Since the ideal theoretical solution is not accessible in practice, it is usually better to dispose of the poor estimate of the priors obtained using the training data, replacing them with "prior" phonological or syntactic knowledge. <p> It maybe that this mismatch is responsible for the lack of robustness of discriminative training (compared with pure maximum likelihood training) in vocabulary independent speech recognition tasks <ref> [15] </ref>. The assumption of model correctness used to generate the labels may have the effect of further embedding specifics of the training data into the final models. CONCLUSION We have a defined a feed-forward network that estimates Gaussian mixture densities using a discriminative training criterion.
Reference: [16] <author> M. J. D. Powell. </author> <title> Radial basis functions for multi-variable interpolation: a review. </title> <type> Technical Report DAMPT/NA12, </type> <institution> Dept. of Applied Mathematics and Theoretical Physics, University of Cambridge, </institution> <year> 1985. </year>
Reference-contexts: RADIAL BASIS FUNCTIONS The radial basis functions (RBF) network was originally introduced as a means of function interpolation <ref> [16, 10] </ref>. A set of K approximating functions, k (x) is constructed from a set of J basis functions f (x): k (x) = j=1 This equation defines a RBF network with J RBFs (hidden units) and K outputs. The output units here are linear, with weights a kj .
Reference: [17] <author> Steve Renals, David McKelvie, and Fergus McInnes. </author> <title> A comparative study of continuous speech recognition using neural networks and hidden Markov models. </title> <booktitle> In Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 369-372, </pages> <address> Toronto, </address> <year> 1991. </year> <month> 10 </month>
Reference-contexts: This computation may be efficiently performed using a dynamic programming algorithm. When used at recognition time this is referred to as Viterbi decoding. 1 And if some other conditions are satisfied [11]. 1 We have used discriminatively trained classifiers to estimate the output PDFs <ref> [5, 14, 17] </ref>. It may be shown that a "1-from-n" classifier trained using a relative entropy (or a least mean squares) objective function outputs the posterior probabilities, P (q l jx), of each class given the input data [6]. <p> The covariance matrix is frequently assumed to be diagonal 3 . Such a network has been used for HMM output probability estimation in continuous speech recognition <ref> [17] </ref> and an isomorphism to tied-mixture HMMs was noted. However, there is a mismatch between the posterior probabilities estimated by the network and the likelihoods required for the HMM decoding. Previously this was resolved by dividing the outputs by the relative frequencies of each state.
References-found: 17

