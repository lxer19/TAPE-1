URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/VysGroKro95.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail: vytas.vysn@nis.mii.lt  e-mail: groen@fwi.uva.nl, krose@fwi.uva.nl  
Title: Orthogonal incremental learning of a feedforward network  
Author: Vytautas Vysniauskas Frans C.A. Groen, Ben J.A. Krose 
Address: Akademijos 4, 2600 Vilnius, Lithuania  Kruislaan 403, 1098 SJ Amsterdam, the Netherlands  
Affiliation: Institute of Mathematics and Informatics  University of Amsterdam Faculty of Mathematics, Computer Science, Physics Astronomy  
Abstract: Orthogonal incremental learning (OIL) is a new approach of incremental training for a feedforward network with a single hidden layer. OIL is based on the idea to describe the output weights (but not the hidden nodes) as a set of orthogonal basis functions. Hidden nodes are treated as the orthogonal representation of the network in the output weights domain. We proved that a separate training of hidden nodes does not conflict with previously optimized nodes and is described by a special relationship orthogonal backpropagation (OBP) rule. An advantage of OIL over existing algorithms is extremely fast learning. This approach can be also easily extended to build-up incrementally an arbitrary function as a linear composition of adjustable functions which are not necessarily orthogonal. OIL has been tested on `two-spirals' and `Net Talk' benchmark problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Barmann and F. B. Konig. </author> <title> On a class of efficient algorithms for neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 139-144, </pages> <year> 1992. </year>
Reference-contexts: Yet another approach is an attempt to avoid high dimensional search in the whole space of parameters by exploiting a partial, iterative optimization approach when only a part of weights is trained while the rest part of the network weights is "frozen". Some work has been done [2], <ref> [1] </ref>, [9] in attempt to optimize repeatedly the network layer by layer involving a linear optimization technique, that yields considerably faster learning.
Reference: [2] <author> S. Barton. </author> <title> A matrix metod for optimizing a neural network. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 450-459, </pages> <year> 1991. </year>
Reference-contexts: Yet another approach is an attempt to avoid high dimensional search in the whole space of parameters by exploiting a partial, iterative optimization approach when only a part of weights is trained while the rest part of the network weights is "frozen". Some work has been done <ref> [2] </ref>, [1], [9] in attempt to optimize repeatedly the network layer by layer involving a linear optimization technique, that yields considerably faster learning.
Reference: [3] <author> R. Battiti. </author> <title> First- and Second-Order Methods for Learning: Between Steepest Descent and Newton's Method. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 141-166, </pages> <year> 1992. </year>
Reference-contexts: It has been proven [14] that the asymptotic rate of convergence of the backpropagation algorithm is very slow, on the order of 1=t at best. Numerical optimization technique offers a rich and robust set of methods which can be applied in an attempt to improve learning rates <ref> [3] </ref>.
Reference: [4] <author> E. B. Baum and K. J. Lang. </author> <title> Constructing hidden units using examples and queries. </title> <editor> In R. P. Lippman, S. J. Hanson, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 904-910. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference: [5] <author> B. Cheng and D. M. Titterington. </author> <title> Neural networks: A review form a statistical perspective. </title> <journal> Statistical Science, </journal> <volume> 9(1) </volume> <pages> 2-54, </pages> <year> 1994. </year>
Reference-contexts: For example, to fulfil this condition in Cascade Correlation, an additional criterion is used to correlate the output of a new node with the network output error. Recently an incremental, iterative optimization idea for a single hidden layer network with a linear output was reported by Andrew Barron in <ref> [5] </ref>. The main emphasis was to avoid peculiarities of nonlinear minimization by properly chosen objective function and to ensure consistent optimization of a hidden node by introducing an additional constraint on the input weights of the hidden node that resembles the correlation constraint in the Cascade Correlation. <p> Our approach presented in this paper is focused on a single node optimization idea but differs from <ref> [5] </ref> and [6] that the representation of hidden nodes in the network is properly chosen to ensure automatically a consistent optimization of a node without any additional constraint.
Reference: [6] <author> S. E. Fahlman and Christian Lebiere. </author> <title> The cascade-correlation learning architecture. </title> <type> Technical Report CMU-CS-90-100, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213, </address> <month> February, </month> <year> 1990. </year>
Reference-contexts: A challenging (and computationally very attractive) step in this direction is to use a single node optimization approach, in which more nodes are added iterativelly until the desired accuracy is obtained or the limit of nodes is exceeded. An excellent example is Cascade-Correlation Architecture <ref> [6] </ref>, successively used in many applications. A central topic of the incremental learning is that a single node optimization must be performed in a consistent, "coherent" way in order do not spoil previously optimized nodes. <p> Our approach presented in this paper is focused on a single node optimization idea but differs from [5] and <ref> [6] </ref> that the representation of hidden nodes in the network is properly chosen to ensure automatically a consistent optimization of a node without any additional constraint. <p> The pool of candidates consisted from Gaussian nodes defined as follows h n (x; fw n g) = s n exp (jw n;1 x 1 + w n;2 x 2 + w n;3 j 2 ) (16) There are known several architectures to solve the problem <ref> [6] </ref>, but we are concentrated on a single hidden layer network. We used 50 hidden nodes with Gaussian activation function that yields fast learning of the network.
Reference: [7] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-366, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction With an invention of a new training technique, backward error propagation or backpropagation [12], feedforward neural networks have become very powerful tools for many diverse real world problems. In principle, a feedforward network can solve any continuous nonlinear mappings. It has been proven <ref> [7] </ref> that a feedforward network with one hidden layer and arbitrary sigmoidal activation function is capable of arbitrary accurate approximation provided sufficiently many hidden units are available. Nevertheless, the training of such networks tends to be very time-consuming. <p> For the simplicity we describe the case of a single output network. Our goal is to build-up iteratively a sequence of feedforward networks g k which converge 2 to the true function f (x) lim Z An existence of such sequence is supported by <ref> [7] </ref>, because g k represents a feedforward network with k hidden nodes.
Reference: [8] <author> E. M. Johansson, F. U. Dowla, and D. M. Goodman. </author> <title> Backpropagation learning for multilayer feed-forward neural networks using he conjugate gradient method. </title> <journal> International Journal of Neural Systems, </journal> <volume> 2(4) </volume> <pages> 291-301, </pages> <year> 1992. </year>
Reference-contexts: Numerical optimization technique offers a rich and robust set of methods which can be applied in an attempt to improve learning rates [3]. In particular, the conjugate gradient method can be easily implemented <ref> [8] </ref> and the convergence rate is comparable with computationally expensive second order methods [15]. fl It is an extended version of the paper presented in the proceedings of ICANN'95 conference (see vol. 1, p. 311) held in Paris, October 9-13.
Reference: [9] <author> F. B. Konig and F. Barmann. </author> <title> A learning algorithm for multilayered neural metworks based on linear squares problems. </title> <booktitle> Neural Networks, </booktitle> <volume> 6(1) </volume> <pages> 127-131, </pages> <year> 1993. </year>
Reference-contexts: Yet another approach is an attempt to avoid high dimensional search in the whole space of parameters by exploiting a partial, iterative optimization approach when only a part of weights is trained while the rest part of the network weights is "frozen". Some work has been done [2], [1], <ref> [9] </ref> in attempt to optimize repeatedly the network layer by layer involving a linear optimization technique, that yields considerably faster learning.
Reference: [10] <author> J. Lee. </author> <title> A novel design method for multilayer feedforward neural networks. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 885-901, </pages> <year> 1994. </year>
Reference-contexts: Another approach to improve the convergence is based on the idea to incorporate a priori knowledge by a proper initialization of weights. Many researchers have reported a significant improvement of the convergence [4],[16], <ref> [10] </ref>, [17] against commonly used random initialization technique. Yet another approach is an attempt to avoid high dimensional search in the whole space of parameters by exploiting a partial, iterative optimization approach when only a part of weights is trained while the rest part of the network weights is "frozen".
Reference: [11] <author> H. Robbins and S. Munroe. </author> <title> A stochastic approximation method. </title> <journal> Ann. Math. Statist, </journal> <volume> 22(1) </volume> <pages> 400-407, </pages> <year> 1951. </year>
Reference-contexts: Nevertheless, the training of such networks tends to be very time-consuming. The backpropagation method belongs to a class of stochastic gradient methods, known in the literature as a stochastic approximation <ref> [11] </ref>, which converges very slow, in general. It has been proven [14] that the asymptotic rate of convergence of the backpropagation algorithm is very slow, on the order of 1=t at best.
Reference: [12] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning representation by back-propagating errors. </title> <journal> Nature, </journal> <volume> 323 </volume> <pages> 533-536, </pages> <year> 1986. </year>
Reference-contexts: 1 Introduction With an invention of a new training technique, backward error propagation or backpropagation <ref> [12] </ref>, feedforward neural networks have become very powerful tools for many diverse real world problems. In principle, a feedforward network can solve any continuous nonlinear mappings.
Reference: [13] <author> T. J. Sejnowski. </author> <title> NET Talk: a parallel network that learns to read aloud. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168, </pages> <year> 1987. </year>
Reference-contexts: The main emphasis of 6 this benchmark test was to investigate the convergence rate of the learning error (ignoring the performance or the generalization issues) in the conjunction of different pool size. The second benchmark was chosen "Net Talk" problem <ref> [13] </ref>. Here main focus is payed to compare the generalization between incrementally and conventionally trained networks. 5.1 "Two spirals" problem A standard data set for the "two spirals" problem consists of 194 data points arranged on X-Y plane into two interlocking spirals (see fig.3). <p> But the "two spirals" benchmark is not a typical example to study the generalization. We address the generalization of the incrementally learned network in the second benchmark. 5.2 "Net Talk" problem The second application was `Net Talk' problem <ref> [13] </ref>. It is a benchmark of a real-word problem when a feedforward network is trained to produce the proper phonemes, given a string of letters as input. The feedforward network with 203 inputs, 26 outputs and 60 hidden nodes has about 14 thousand parameters.
Reference: [14] <author> G. Tesauro, Yu He, and Subutai Ahmad. </author> <title> Asymptotic convergence of backpropagation. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 382-391, </pages> <year> 1989. </year>
Reference-contexts: Nevertheless, the training of such networks tends to be very time-consuming. The backpropagation method belongs to a class of stochastic gradient methods, known in the literature as a stochastic approximation [11], which converges very slow, in general. It has been proven <ref> [14] </ref> that the asymptotic rate of convergence of the backpropagation algorithm is very slow, on the order of 1=t at best. Numerical optimization technique offers a rich and robust set of methods which can be applied in an attempt to improve learning rates [3].
Reference: [15] <author> P. van der Smagt. </author> <title> Minimisation methods for training feed-forward networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(1) </volume> <pages> 1-11, </pages> <year> 1994. </year>
Reference-contexts: Numerical optimization technique offers a rich and robust set of methods which can be applied in an attempt to improve learning rates [3]. In particular, the conjugate gradient method can be easily implemented [8] and the convergence rate is comparable with computationally expensive second order methods <ref> [15] </ref>. fl It is an extended version of the paper presented in the proceedings of ICANN'95 conference (see vol. 1, p. 311) held in Paris, October 9-13.
Reference: [16] <author> L. A. F. Wessels and E. Barnard. </author> <title> Avoiding false local minima by proper initialization of connections. </title> <journal> IEEE transactions on Neural Networks, </journal> <volume> 3(6) </volume> <pages> 899-905, </pages> <year> 1992. </year>
Reference: [17] <author> N. Weymaere and J. P. Martens. </author> <title> On the initialization and optimization of multilayer perceptrons. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(5) </volume> <pages> 738-751, </pages> <year> 1994. </year> <month> 11 </month>
Reference-contexts: Another approach to improve the convergence is based on the idea to incorporate a priori knowledge by a proper initialization of weights. Many researchers have reported a significant improvement of the convergence [4],[16], [10], <ref> [17] </ref> against commonly used random initialization technique. Yet another approach is an attempt to avoid high dimensional search in the whole space of parameters by exploiting a partial, iterative optimization approach when only a part of weights is trained while the rest part of the network weights is "frozen".
References-found: 17

