URL: file://ftp.cs.utexas.edu/pub/qsim/papers/Gribble-iscv-95.ps.Z
Refering-URL: http://net.cs.utexas.edu/users/qr/robotics/papers.html
Root-URL: 
Email: grib@cs.utexas.edu  
Title: Slow visual search in a fast-changing world  
Author: William S. Gribble 
Address: Austin, Texas 78712  
Affiliation: Dept. of Electrical and Computer Engineering University of Texas at Austin  
Abstract: Attention focusing mechanisms and domain-informed selection of representations can make real-time vision tasks work with limited computational power. This paper describes ongoing work in distributed real-time vision which aims to use cheap and plentiful workstations and PCs rather than special-purpose hardware. I discuss a system called Argus which is inspired by the visual routines theory of human vision. In Argus, reactive feature tracking agents maintain minimal, task-dependent descriptions of relevant image features by direct observation of the live video stream. Routines for model-based object recognition operate on these descriptions. Higher-level processing is independent of the maintenance of lower-level representations. This allows the visual subsystem to provide real-time feedback for closed-loop tasks even when high-level perceptual processing is slow compared to video frame rates. Experiments in moving-object recognition are described which demonstrate the strength of this approach in situations where the perceived scene is changing faster than high-level analysis can categorize it. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Philip E. Agre and David Chapman. Pengi: </author> <title> An implementation of a theory of activity. </title> <booktitle> In Proc. 6th National Conf. on Artificial Intelligence (AAAI-87). </booktitle> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: The description of human vision provided by visual routines theory is an excellent starting point for computational vision, and visual routines-based systems have generated substantial interest ([6], <ref> [1] </ref>). The remainder of this paper describes Argus, a distributed vision system based on an adaptation of the visual routines paradigm. Argus uses real-time feature tracking processes to maintain a limited set of intermediate representations of the scene.
Reference: [2] <author> Ronald C. Arkin and Douglas MacKenzie. </author> <title> Temporal coordination of perceptual algorithms for mobile robot navigation. </title> <journal> IEEE Trans. on Robotics and Automation, </journal> <volume> 10(3) </volume> <pages> 276-286, </pages> <year> 1994. </year>
Reference-contexts: Schema has been used in many contexts in computer vision; our usage comes most nearly from Arkin <ref> [2] </ref>, who used the term perceptual schema to describe a virtual sensor which, when activated, triggered state changes in robot motor control laws. In Argus, a visual schema is similarly a virtual sensor, "tuned" to a specific set of features and geometric constraints.
Reference: [3] <author> Dana H. Ballard. </author> <title> Animate vision. </title> <journal> Artificial Intelligence, </journal> <volume> 48 </volume> <pages> 57-86, </pages> <year> 1991. </year>
Reference-contexts: Features are tracked in image space, using a feature-centered local coordinate frame. Object-centered coordinate frames have a proved to have both aesthetic and practical advantages for vision with an active camera in a dynamic world <ref> [3] </ref>. The state information that a feature tracker reports is an oriented bounding box which encloses the feature being tracked. This bounding box is the basis for evaluation of geometric constraints on the feature. The current implementation of Argus only has tracking agents for straight edge segments.
Reference: [4] <author> Rafael C. Gonzalez and Richard E. Woods. </author> <title> Digital Image Processing. </title> <publisher> Addison-Wesley, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: The identification of geometric shapes in static images is an exhaustively studied area; well-known techniques such as the Hough transform <ref> [4] </ref> are excellent for picking parameterized models out of an image. However, the success of Hough space analysis and other modern techniques depends on large amounts of memory and compute time.
Reference: [5] <author> Gregory D. Hager and Kentaro Toyama. </author> <title> A framework for real-time window-based tracking using off-the-shelf hardware. </title> <type> Draft documentation, </type> <month> August </month> <year> 1994. </year>
Reference-contexts: The current implementation of Argus only has tracking agents for straight edge segments. Other tracker types are under development, including a color blob tracker, a corner tracker, and a parametric curve tracker. The Yale tracking system <ref> [5] </ref> already has tracking algorithms for various types of features, so adding them to Argus should be straightforward. tracker following the top edge of a white flash card with a diamond shape printed on it.
Reference: [6] <author> Ian Horswill. </author> <title> Visual routines and visual search: a real-time implementation and an automata-theoretic analysis. </title> <booktitle> In Proc. 15th Int. Joint Conf. on Artificial Intelligence (IJCAI-95). </booktitle> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: A more sophisticated scheme of region segmentation and tracking, essentially a straightforward implementation of the markers concept from visual routines theory, is under development. This will allow more robust backtracking search, as in Horswill's Jeeves system <ref> [6] </ref>. 2.5 Model maintenance and feature reac quisition In a dynamic scene, temporary occlusions, noise, or tracking error can cause feature trackers associated with an acquired object to lose their "lock" on the image feature.
Reference: [7] <author> Anne M. Triesman and Garry Gelade. </author> <title> A feature-integration theory of attention. </title> <journal> Cognitive Psychology, </journal> <volume> 12 </volume> <pages> 97-136, </pages> <year> 1980. </year>
Reference-contexts: Video-rate performance of visual tasks is very difficult. Research suggests that the human visual sense may not operate completely in "real-time", and theories of human visual performance suggest that meaningful visual feedback is possible without performing all operations at video frame rates. Experiments by Triesman <ref> [7] </ref> and others (see [9] for a thorough survey and analysis) in shape discrimination and other simple visual tasks measure human response times in the hundreds of milliseconds.
Reference: [8] <author> Shimon Ullman. </author> <title> Visual routines. </title> <journal> Cognition, </journal> <volume> 18 </volume> <pages> 97-159, </pages> <year> 1984. </year>
Reference-contexts: Clearly, in humans the completion of relatively high-level visual tasks such as precisely categorizing objects is decoupled from the general task of maintaining timely knowledge about the perceived world. Ullman's theory of visual routines <ref> [8] </ref> describes human performance of visual tasks as a sequential "program" of simple routines such as bounded activation and line following. Each routine is an operator which determines relationships between or properties of image features.
Reference: [9] <author> A. H. C. van der Heijden. </author> <title> Selective Attention in Vision. </title> <publisher> Routledge, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Video-rate performance of visual tasks is very difficult. Research suggests that the human visual sense may not operate completely in "real-time", and theories of human visual performance suggest that meaningful visual feedback is possible without performing all operations at video frame rates. Experiments by Triesman [7] and others (see <ref> [9] </ref> for a thorough survey and analysis) in shape discrimination and other simple visual tasks measure human response times in the hundreds of milliseconds. A "real-time" vision system which took hundreds of milliseconds to analyze each frame would not provide adequate feedback for performing real-world tasks, such as tracking objects.
References-found: 9

