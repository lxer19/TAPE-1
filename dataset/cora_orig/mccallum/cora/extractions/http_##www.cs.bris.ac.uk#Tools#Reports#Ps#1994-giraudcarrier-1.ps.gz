URL: http://www.cs.bris.ac.uk/Tools/Reports/Ps/1994-giraudcarrier-1.ps.gz
Refering-URL: http://www.cs.bris.ac.uk/Tools/Reports/Abstracts/1994-giraudcarrier-1.html
Root-URL: 
Email: cgc@axon.cs.byu.edu  
Title: A Reconfigurable Data Flow Machine for Implementing Functional Programming Languages  
Author: Christophe Giraud-Carrier 
Address: Provo, UT 84602  
Affiliation: Brigham Young University, Department of Computer Science,  
Abstract: Functional languages do away with the current state paradigm and achieve referential transparency. They also exhibit inherent parallelism. These qualities fit very well on top of a data-driven architecture such as a data flow machine. In this paper, we propose a fully reconfigurable data flow machine for implementing functional programming languages. The design is based on smart memories and nodes interconnected via a hypercube. Important aspects of the proposed model are described and compared with other similar attempts. Advantages of our system include massive parallelism, reconfigurability, and amenability to higher-level, graphical programming. Current limitations are identified and extensions are suggested. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dennis, J.B. </author> <title> Data Flow Supercomputers. </title> <journal> IEEE Computer, </journal> <volume> 13, </volume> <month> November </month> <year> 1980, </year> <pages> 48-56. </pages>
Reference-contexts: Specific data flow architectures of particular interest for comparison with the one presented here have been proposed by Rumbaugh [10] and Dennis <ref> [1] </ref>. The main advantages of our system over other models are (1) its ability to reconfigure on demand, and (2) its amenability to higher-level, graphical forms of programming.
Reference: [2] <author> Dennis, J.B., Gao, G.R., and Todd, K.W. </author> <title> Modeling the Weather with a Data Flow Supercomputer. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-33, </volume> <month> July </month> <year> 1984, </year> <pages> 592-603. </pages>
Reference-contexts: An example of global weather model was implemented on Dennis's architecture using the functional language VAL <ref> [2] </ref>. A 20-fold improvement in performance was reported for that weather simulation application. Since Dennis's model suffers some of the drawbacks discussed in this paper, and that our system attempts to overcome, we feel confident that our system can achieve, at least similar and probably higher-order of increases in performance.
Reference: [3] <author> Friedman, </author> <title> D.P., and Wise, D.S. Aspects of Applicative Programming for Parallel Processing. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27, </volume> <month> April </month> <year> 1978, </year> <pages> 289-296. </pages>
Reference-contexts: This property is sometimes referred to as referential transparency. It gives rise to very natural parallelism since all the arguments of a function can potentially be evaluated in parallel. Much work has been done to develop functional languages that make easy the automatic extraction of inherent parallelism. In <ref> [3] </ref> for example, Friedman and Wise propose a functional language whose semantics is aimed at using massive parallelism. The programmer is provided high-level constructs that can be automatically recognized by the compiler and translated into efficient parallel code.
Reference: [4] <author> Gurd, J.P., Kirkham, C.C., and Watson, I. </author> <title> The Manchester Prototype Data Flow Computer. </title> <journal> Communications of the ACM, </journal> <volume> 28, </volume> <month> January </month> <year> 1985, </year> <pages> 34-52. </pages>
Reference-contexts: Most data flow machines that have been proposed have been based on fixed, non-reconfigurable architectures. Many of them use a circular pipelining mechanism with several matching stores and several functional units, associated together in pairs in a ringlike architecture <ref> [4, 13] </ref>. Not only is this approach inherently less parallel, it also introduces possible bottlenecks at the functional and matching units. Of course, one could argue that there is a potential communication bottleneck in more complex structures.
Reference: [5] <author> Hillis, W.D. </author> <title> The Connection Machine. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA: </address> <year> 1985. </year>
Reference-contexts: An effective way of dynamically gaining nodes and adding them to a growing structure is found in the rendezvous mechanism. An alternative guaranteeing more locality is to use a wave allocation mechanism. A complete discussion of node interconnect and allocation in hypercubes is available in the work of Hillis <ref> [5] </ref> on the Connection Machine. Our system uses wavefronts to logically connect smart memories to nodes ensuring physical proximity, and consequently, fast transfer of data between node and smart memory. This locality is a necessary condition to the efficiency of the caching mechanism that we discuss presently. <p> The rendezvous mechanism guarantees us both. To allow efficient exchanges between nodes and high parallelism in communications, nodes are able to handle the various routing mechanisms described in Hillis's Connection Machine <ref> [5] </ref>, in particular, referral and forwarding. The basic mapping of functions to nodes is handled by the compiler. As it scans the program's source code, the compiler performs the following: 1. Each time a new atomic (i.e., built-in) function is encountered: a.
Reference: [6] <author> Hudak, P. </author> <title> Conception, Evolution, and Application of Functional Programming Languages. </title> <journal> ACM Computing Surveys, </journal> <volume> 21, </volume> <month> December </month> <year> 1989, </year> <pages> 359-411. </pages>
Reference-contexts: The programmer is provided high-level constructs that can be automatically recognized by the compiler and translated into efficient parallel code. An excellent discussion of functional programming languages concepts and evolution is available in <ref> [6] </ref>. Data flow computing is based upon data flow graphs. Elements of data are transformed as they flow through the graph. Nodes in the graph represent computation units which, from their inputs, produce an output. The output of a node serves as an input to subsequent nodes in the graph.
Reference: [7] <author> Keller, R.M., and Sleep, </author> <title> M.R. Applicative Caching. </title> <booktitle> In Proceedings of the ACM Conference on Functional Programming Language Computer Architecture, </booktitle> <year> 1981, </year> <pages> 131-140. </pages>
Reference-contexts: Recomputing vs. Caching. It is more than likely that in a large program, a function will be invoked several times with the same arguments. If the function is complex and slow to execute, we might wish to have its result made available through caching. Keller and Sleep <ref> [7] </ref> discuss important issues introduced by caching in Functional Programming. Counting on the fact that it is possible to design a cache (smart memory in the present system) such that the lookup (including data transfer) is faster than computing, we decided to cache each function type in the program.
Reference: [8] <author> Martinez, T.R. </author> <title> Smart Memory Architecture and Methods. </title> <journal> Future Generation Computer Systems, </journal> <volume> 6, </volume> <year> 1990, </year> <pages> 145-162. </pages>
Reference-contexts: INTERCONNECT Pool of Smart Memories SM ...... SM SM N Pool of Nodes N 3.2. Smart Memories The purpose of the smart memories is to hold previously computed results (i.e., cache). In our system, we use a simplified version of the more general smart memories introduced in <ref> [8] </ref>. Their internal structure is described in memory is associated to each numeric function type in the program, and is used to cache the results of computation of that function. Each descriptor contains the values of the inputs and the corresponding output.
Reference: [9] <author> Michie, D. </author> <title> Memo Functions and Machine Learning. </title> <journal> Nature, </journal> <volume> 218, </volume> <month> April </month> <year> 1968, </year> <pages> 19-22. </pages>
Reference-contexts: The output is also made available to all of the node's successors in the program's data flow graph (via rendezvous). Michie <ref> [9] </ref> was one of the first to study this issue of caching and recomputing. He argued, based on intuitive arguments, that the use of caching improves function performance.
Reference: [10] <author> Rumbaugh, </author> <title> J.E. A Data Flow Multiprocessor. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-26, </volume> <month> February </month> <year> 1977, </year> <pages> 138-146. </pages>
Reference-contexts: Rather than reproducing his results here, we refer the interested reader to his insightful paper and add our contribution to the list of proposed models in Table 1. Specific data flow architectures of particular interest for comparison with the one presented here have been proposed by Rumbaugh <ref> [10] </ref> and Dennis [1]. The main advantages of our system over other models are (1) its ability to reconfigure on demand, and (2) its amenability to higher-level, graphical forms of programming.
Reference: [11] <author> Snyder, L. </author> <title> Introduction to the Configurable Highly Parallel Computer. </title> <booktitle> Computer, </booktitle> <month> January </month> <year> 1982, </year> <pages> 47-56. </pages>
Reference-contexts: The originality, and resulting increase in efficiency, of our system lie in the distribution of the matching store among the nodes. High performance is achieved in tag-matching. c. Reconfigurability. The issue here, is between dedicated and general-purpose machines. Snyder <ref> [11] </ref> provides an interesting discussion of the issues involved in dealing with reconfiguration and proposes some possible architectures to handle them. Most data flow machines that have been proposed have been based on fixed, non-reconfigurable architectures.
Reference: [12] <author> Vegdahl, </author> <title> S.R. A Survey of Proposed Architectures for the Execution of Functional Languages. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-33, </volume> <month> December </month> <year> 1984, </year> <pages> 1050-1071. </pages>
Reference-contexts: The compiler then parses the graph and translates it into the appropriate configuration. This higher level of abstraction fits naturally in a functional programming environment. 5.2. Comparison with Other Models Vegdahl <ref> [12] </ref> developed a set of criteria with which he meaningfully compared various architectures implementing functional languages. Rather than reproducing his results here, we refer the interested reader to his insightful paper and add our contribution to the list of proposed models in Table 1.
Reference: [13] <author> Veen, A.H. </author> <title> Data Flow Machine. </title> <journal> ACM Computing Surveys, </journal> <volume> 18, </volume> <month> December </month> <year> 1986, </year> <pages> 365-396. </pages>
Reference-contexts: The output of a node serves as an input to subsequent nodes in the graph. A node fires as soon as all of its inputs are available. Data flow computing is therefore data-driven (i.e., functional). A thorough study of issues in data flow machines is in <ref> [13] </ref>. One of the challenges with data flow computing is its need for reconfigurability. Each program executed on a data flow machine requires its own configuration of nodes. <p> Most data flow machines that have been proposed have been based on fixed, non-reconfigurable architectures. Many of them use a circular pipelining mechanism with several matching stores and several functional units, associated together in pairs in a ringlike architecture <ref> [4, 13] </ref>. Not only is this approach inherently less parallel, it also introduces possible bottlenecks at the functional and matching units. Of course, one could argue that there is a potential communication bottleneck in more complex structures. <p> No attempts have been made at this point to deal with data structures. The issue of handling data structures is in fact far from having found a general solution, as can be seen from Veen's remarks on the subject <ref> [13] </ref>. However, the caching mechanism and high degree of parallelism of our machine would be very beneficial in applications dealing with large data structures, such as matrices, that need to be extensively manipulated. Given the fine granularity of parallelism implemented, structure copying is too expensive.
Reference: [14] <author> Watson, I, and Gurd, J. </author> <title> A Prototype Data Flow Computer with Token Labeling. </title> <booktitle> In Proceedings of AFIPS National Computer Conference, </booktitle> <volume> 48, </volume> <year> 1979, </year> <pages> 623-628. </pages>
Reference-contexts: Each piece of data that flows through the system is tagged with a value reflecting the invocation it belongs to. Watson and Gurd <ref> [14] </ref> proposed a similar approach on the Manchester Data Flow Prototype to allow reentrant code structures. However, their system is based upon a ring architecture with a global matching store which holds all the tokens (i.e., data items).
References-found: 14

