URL: ftp://ftp.cs.swarthmore.edu/pub/meeden/meeden.thesis.ps.Z
Refering-URL: http://www.cs.swarthmore.edu/~meeden/
Root-URL: 
Title: TOWARDS PLANNING: INCREMENTAL INVESTIGATIONS INTO ADAPTIVE ROBOT CONTROL  
Author: Lisa A. Meeden 
Degree: Submitted to the faculty of the Graduate School in partial fulfillment of the requirements for the degree Doctor of Philosophy in the  
Date: August 1994  
Affiliation: Department of Computer Science Indiana University  
Abstract-found: 0
Intro-found: 0
Reference: <author> Ackley, D. H. and Littman, M. L. </author> <year> (1990). </year> <title> Generalization and scaling in reinforcement learning. </title> <editor> In Touretsky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 550-557. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address>
Reference-contexts: Each of these methods is described in the following sections. 2. Adaptation methods for learning control 22 2.3.1 Local method: Complementary Reinforcement Back <p>- Propagation The local method is a modified version of the complementary reinforcement backpropagation (CRBP) learning algorithm <ref> (Ackley and Littman, 1990) </ref>. Back-propagation learning requires precise error measures for each output produced by a network so that gradient descent on the error can be performed. CRBP provides these exact error measures from the abstract reward and punishment signals as described below.
Reference: <author> Agre, P. E. </author> <year> (1988). </year> <title> The Dynamic Structure of Everyday Life. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science. </institution>
Reference: <author> Agre, P. E. </author> <year> (1993). </year> <title> The symbolic worldview: Reply to Vera and Simon. </title> <journal> Cognitive Science, </journal> <volume> 17(1) </volume> <pages> 61-70. </pages>
Reference-contexts: Based on this alternative view, I will offer an incremental approach to the development of plans in robots. 1.1 The deliberative processing worldview A worldview is a largely unarticulated system of vocabulary, methods, and values shared by a research community. <ref> (Agre, 1993, p 62) </ref> A worldview helps its owner interpret observations and guide further studies. It is neither right or wrong.
Reference: <author> Barto, A. G. </author> <year> (1989). </year> <title> Connectionist learning for control: An overview. </title> <type> Technical Report 89-89, </type> <institution> University of Massachusetts, Department of Computer and Information Science. </institution>
Reference-contexts: These methods suggest how new techniques for adaptive control can be developed which take full advantage of the possibilities for fabricating associative memory systems having high capacity, high speed, and the ability to usefully interpolate and extrapolate in real time" <ref> (Barto, 1989, p 31) </ref>. Barto's review was based on applications such as controlling the production of a plant, but the problem is analogous to controlling a robot. <p> Adaptation methods for learning control 17 Certainly rigorous theory is important, and a valid criticism of any research is that it proceeds in ignorance of relevant theoretical frameworks and previous research, but an experimental methodology seems necessary for developing control applications involving complex nonlinear systems. <ref> (Barto, 1989, p 30) </ref> Provably complete planners have been constructed that abide by classical planning's restrictive assumptions; David Chapman's TWEAK program is one example (Chapman, 1987). However, Chapman discovered that these restrictions made TWEAK almost useless as a real-world planner.
Reference: <author> Beer, R. D. </author> <year> (1990). </year> <title> Intelligence as Adaptive Behavior: An Experiment in Computational Neuroethology. </title> <publisher> Academic Press, Inc., </publisher> <address> San Diego, CA. </address>
Reference: <author> Beer, R. D. and Gallagher, J. C. </author> <year> (1992). </year> <title> Evolving dynamical neural networks for adaptive behavior. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 1(1) </volume> <pages> 91-122. </pages>
Reference-contexts: In contrast, because dynamical neural networks maintain state, their response to identical environmental stimuli can differ at different times. Their activity exhibits a certain "inertia" independent of their immediate environmental context. <ref> (Beer and Gallagher, 1992, p 115) </ref> Beer's dynamical neural network model shown in Figure 2.5 is quite similar to the Elman-style architecture being used as the basis for this research.
Reference: <author> Belew, R. K., McInerney, J., and Schraudolph, N. N. </author> <year> (1992). </year> <title> Evolving networks: Using genetic algorithms with connectionist learning. </title> <editor> In Langton, C. G., Taylor, C., Farmer, J. D., and Rasmussen, S., editors, </editor> <booktitle> Artificial Life II, </booktitle> <pages> pages 511-547, </pages> <address> Redwood City, CA. Addison-Welsley. </address>
Reference-contexts: By working at the level of bits, the GA has no knowledge about the semantics of a problem. This is considered an advantage because the GA's success cannot be linked to any built-in knowledge about a particular domain <ref> (Belew et al., 1992) </ref>. Although there are methods for translating real values into bit strings appropriate for GA processing, for the robot domains studied here, individuals are represented as real-coded vectors of weights. <p> Belew, McInerney, and Schraudolph did a number of experiments to test the feasibility of using a GA as a source of initial weights for gradient descent learning and found that this technique is quite effective <ref> (Belew et al., 1992) </ref>. Combining global and local adaptation methods such as the GA and CRBP is a promising avenue for further research into developing robotics controllers. But there is a caveat: the computational complexity of these hybrids can be extremely high.
Reference: <author> Blank, D. S., Meeden, L. A., and Mashall, J. B. </author> <year> (1992). </year> <title> Exploring the sym-bolic/subsymbolic continuum: A case study of RAAM. </title> <editor> In Dinsmore, J., editor, </editor> <booktitle> The Symbolic and Connectionist Paradigms: Closing the gap, </booktitle> <pages> pages 113-148. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: The ability of connectionist networks to develop their own internal representations|or hidden representations|is an extremely important property of these mechanisms. See <ref> (Blank et al., 1992) </ref> for a thorough discussion of the unique aspects of connectionist representations. One difficulty with the standard feedforward architecture is that when time is an important aspect of the problem to be modeled, it is not clear how to represent time in an efficient and useful way.
Reference: <author> Braitenberg, V. </author> <year> (1984). </year> <title> Vehicles: Experiments in Synthetic Psychology. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Brooks, R. A. </author> <year> (1986). </year> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> 2(1) </volume> <pages> 14-23. </pages>
Reference: <author> Brooks, R. A. </author> <year> (1991). </year> <title> Intelligence without representation. </title> <journal> Artificial Intelligence, </journal> <volume> 47(1) </volume> <pages> 139-159. </pages> <note> REFERENCES 108 Chalmers, </note> <author> D. J. </author> <year> (1990). </year> <title> The evolution of learning: An experiment in genetic connec-tionism. </title> <booktitle> In Proceedings of the 1990 Connectionist Summer School, </booktitle> <address> Palo Alto, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Chapman, D. </author> <year> (1987). </year> <title> Planning for conjunctive goals. </title> <journal> Artificial Intelligence, 32:333377. </journal>
Reference-contexts: that it proceeds in ignorance of relevant theoretical frameworks and previous research, but an experimental methodology seems necessary for developing control applications involving complex nonlinear systems. (Barto, 1989, p 30) Provably complete planners have been constructed that abide by classical planning's restrictive assumptions; David Chapman's TWEAK program is one example <ref> (Chapman, 1987) </ref>. However, Chapman discovered that these restrictions made TWEAK almost useless as a real-world planner. TWEAK can generate complete plans because every possible action has a set of preconditions and postconditions associated with it.
Reference: <author> Chapman, D. </author> <year> (1991). </year> <title> Vision, Instuction, and Action. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Clark, A. </author> <year> (1993). </year> <title> Associative Engines: Conectionism, Concepts, and Representational Change. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: As Clark has noted, connectionism "inverts the official classical ordering, in which a high-level understanding comes first and closely guides the search for algorithms" <ref> (Clark, 1993) </ref>. Through adaptation, the system's solution to the task emerges as it discovers the key features of the problem space rather than simply being the product of the designer's understanding of the domain.
Reference: <author> Cliff, D., Harvey, I., and Husbands, P. </author> <year> (1993). </year> <title> Explorations in evolutionary robotics. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 2(1) </volume> <pages> 73-110. </pages>
Reference-contexts: There are two types of connections between units: inhibitory and excitatory. An 2. Adaptation methods for learning control 32 inhibitory link is considered to be infinitely negative and so has veto power over the units it is connected to <ref> (Cliff et al., 1993) </ref>. Infinitely negative connections may seem to be too strong a constraint, but the designers of this model claim that there is a similar phenomena found in invertebrate nervous systems.
Reference: <author> Dennis, S. and Wiles, J. </author> <year> (1993). </year> <title> Integrating learning into models of human memory: The hebbian recurrent memory. </title> <booktitle> In Proceedings of the Fifteenth Annual Meeting of the Cognitive Science Society, </booktitle> <pages> pages 394-399, </pages> <address> Hillsdale, NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: To this end, the controller should be able to save and generalize over its own protoplans. Figure 6.1 shows a proposed architecture for the on-line storage and retrieval of protoplans. This architecture was 6. Conclusion 105 inspired by work on Hebbian Recurrent Networks <ref> (Dennis and Wiles, 1993) </ref>. In this architecture, a protoplan memory can be updated on every time step to reflect the system's ever changing summary of the current situation. In addition, a generalized version of this summary can be used to affect the current action decision.
Reference: <author> Elman, J. L. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-212. </pages>
Reference-contexts: Elman contends that "a better approach would be to represent time implicitly rather than explicitly. That is, we represent time by the effect it has on processing and not as an additional dimension of the input" <ref> (Elman, 1990, p 180) </ref>. To this end, a number of researchers have suggested recurrent architectures and new learning algorithms for them (Mozer, 1989; Pearlmutter, 1989; Williams and Zipser, 1989).
Reference: <author> Firby, R. J. </author> <year> (1989). </year> <title> Adaptive Execution in Complex Dynamic Worlds. </title> <type> PhD thesis, </type> <institution> Yale University, Department of Computer Science. </institution> <type> Technical Report YALEU/CSD/RR 672. </type>
Reference-contexts: For example, consider the type of goal Friby employed in his reactive action packages <ref> (Firby, 1989) </ref>. His system controlled a delivery truck that could be attacked by roving enemies.
Reference: <author> Fitzpatrick, M. J. and Grefenstette, J. J. </author> <year> (1988). </year> <title> Genetic algorithms in noisy envi-ronments. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 101-120. </pages>
Reference-contexts: However it is better to obtain quick, rough estimates of fitness and to allow the GA to consider many candidate solutions than it is to attempt to obtain highly accurate evaluations with a smaller population size <ref> (Fitzpatrick and Grefenstette, 1988) </ref>. GAs have proved to be quite adept at finding good solutions in very noisy environments. Processing in GAs is measured in terms of generations. A generation is completed when a new population has been created through reproduction within the old population.
Reference: <author> Gasser, M. </author> <year> (1993). </year> <title> The structure grounding problem. </title> <booktitle> In Proceedings of the Fifteenth Annual Meeting of the Cognitive Science Society, </booktitle> <pages> pages 149-152, </pages> <address> Hillsdale, NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Goldberg, D. E. </author> <year> (1989). </year> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: Instead they are primarily viewed as search-based function optimizers. Goldberg provides a good introduction to both types of applications <ref> (Goldberg, 1989) </ref>. Genetic algorithms are based on the theory of natural selection in evolution. GAs work on a population of individuals, where each individual represents a possible solution to the given problem.
Reference: <author> Griffin, D. R. </author> <year> (1984). </year> <title> Animal Thinking. </title> <publisher> Harvard University Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: This belief biases our observations of other agents' behavior: Anticipation and planning are of course impossible to observe directly in another person or animal, but indications of their likelihood are often 1. Reconsidering planning 12 observable. <ref> (Griffin, 1984, p 37) </ref> Griffin argues that complex, structured behavior suggests that planning is taking place. As an example of what he means by structured behavior, he offers a description of the behavior of an assassin bug that lives in the tropical rain forests. <p> Another termite seizes the corpse as part of a normal behavior pattern of devouring dead siblings. The assassin bug pulls this second termite out and eats it as well. In one case an assassin bug was observed to eat 31 termites in this manner <ref> (Griffin, 1984, p 123) </ref>. A key contribution of the situated action view is that it questions this implicit belief in the centrality of planning. If insects like the assassin bug can produce highly structured behavior, then structured behavior may be the result of much simpler mechanisms than planning.
Reference: <author> Harnad, S. </author> <year> (1990). </year> <title> The symbol grounding problem. </title> <journal> Pysica D, </journal> <volume> 42 </volume> <pages> 335-346. </pages>
Reference-contexts: can the semantic interpretation of a formal symbol system be made intrinsic to the system rather than parasitic on the meanings in our heads? How can the meaning of the meaningless symbol tokens, manipulated solely on the bases of their (arbitrary) shape, be grounded in anything but other meaningless symbols? <ref> (Harnad, 1990, p 335) </ref> One solution to this problem is to construct symbols from the ground up, basing them directly on nonsymbolic, perceptual features.
Reference: <author> Harp, S. A., Samad, T., and Guha, A. </author> <year> (1989). </year> <title> Towards the genetic synthesis of neu-ral networks. </title> <editor> In Schaffer, J. D., editor, </editor> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> pages 360-369, </pages> <address> Palo Alto, CA. </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher> <address> REFERENCES 109 Harvey, I. </address> <year> (1993a). </year> <title> Evolutionary robotics and SAGA: The case for hill crawling and tournament selection. </title> <editor> In Langton, C. G., editor, </editor> <booktitle> Artificial Life III, </booktitle> <address> Redwood City, CA. Addison-Welsley. </address>
Reference: <author> Harvey, I. </author> <year> (1993b). </year> <title> Issues in evolutionary robotics. </title> <editor> In Meyer, J.-A., Roitblat, H., and Wilson, S., editors, </editor> <booktitle> Proceedings of the second international conference on the simulation of adaptive behavior, </booktitle> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Yet by fixing on a particular architecture we have bounded the set of possible solutions. Harvey, Husbands, and Cliff take the stance of limiting designer bias one step further by suggesting that the architecture itself should be allowed to evolve over time <ref> (Harvey, 1993b) </ref>. They argue that determining an appropriate network architecture for a particular task is not always easy. For instance, one must determine what hidden layer size will be large enough to allow the task to be learned but small enough to produce successful generalization. <p> In response to this, they claim that "with an evolutionary approach it may not be necessary to analyze how it works, but rather one should assess how good is the behavior it elicits" <ref> (Harvey, 1993b) </ref>. Ultimately I agree with Harvey, Husbands, and Cliff that the neural structure itself must be evolved. <p> Noise must be taken into account at all levels. 3. The simulation can be calibrated by testing adapted architectures in the real robot. 4. A range of unstructured, dynamic environments should be used to ensure ro <br>- bustness. <ref> (Harvey, 1993b) </ref> 4. Comparing local and global reinforcement methods 62 In constructing carbot's simulator, Harvey's first three suggestions were followed (with respect to the fourth suggestion of employing a range of environments, only the playpen was used although a number of different configurations were tried).
Reference: <author> Husbands, P., Harvey, I., and Cliff, D. </author> <year> (1993). </year> <title> Analysing recurrent dynamical net-works evolved for robot control. </title> <booktitle> In Proceedings of the Third IEE International Conference on Neural Networks. </booktitle> <publisher> IEE Press. </publisher>
Reference-contexts: Then time series plots of sensor, neuron, and motor activities were used to further eliminate units that were deemed to play no role in the behavior. In this way a network with 13 units and 34 connections was reduced to 8 units and 21 connections <ref> (Husbands et al., 1993) </ref>. Despite their initial success, as the size and complexity of these amorphous structures grows it will become increasingly difficult to analyze their mechanisms.
Reference: <author> Jordan, M. I. </author> <year> (1989). </year> <title> Serial order: A parallel, distributed processing approach. </title> <editor> In Elman, J. L. and Rumelhart, D. E., editors, </editor> <booktitle> Advances in Connectionist Theory: Speech. </booktitle> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference: <author> Kaelbling, L. P. </author> <year> (1993). </year> <title> Learning in Embedded Systems. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: There are two standard approaches to alleviating this problem. The first is to improve the sensors themselves. The second, as Kaelbling describes, is "to extend the system's inputs to include the last k percepts for some value of k" <ref> (Kaelbling, 1993) </ref>. She further points out though, that as k gets large, the input space grows and dramatically increases the complexity of the overall control task.
Reference: <author> Laird, J. E., Newell, A., and Rosenbloom, P. S. </author> <year> (1987). </year> <title> Soar: An architecture for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33 </volume> <pages> 1-64. </pages>
Reference-contexts: Typically models colored by this worldview begin their inquiry from the top, positing highly structured symbolic representations of knowledge that are manipulated according to rules of logic to produce behavior. One instantiation of the deliberative processing view is Soar|a suggested architecture for general intelligence <ref> (Laird et al., 1987) </ref>. I will focus on Soar because it is representative of the deliberative style of processing and it has been successfully applied to the widest range of problems to date. Soar embodies two hypotheses fundamental to the deliberative processing world- view. <p> A second assumption is that problem spaces are the basic organizational unit of all goal-directed behavior. A problem space consists of a set of possible states in a particular domain and a set of primitive operators that transform one state into another. Figure 1.1, adapted from <ref> (Laird et al., 1987, p 6) </ref>, depicts the problem space for a particular problem called the eight puzzle. Planning for a task in Soar is seen as searching through a problem space for an appropriate sequence of operators that will transform the initial state to a desired goal state. <p> Factoring implies both that the aspects are encoded as distinct attributes and that the operators are sensitive only to the relevant attributes and not to the irrelevant attributes. <ref> (Laird et al., 1987, p 56) </ref> Chunking is a powerful method for efficiently recombining consistent and existing knowledge. Allowing Soar to actually extend its sphere of knowledge beyond the boundary of the original relevant features is a major new step.
Reference: <author> Laird, J. E. and Rosenbloom, P. S. </author> <year> (1993). </year> <title> Integrating execution, planning, and learning in Soar for external environments. </title> <editor> In Rosenbloom, P. S., Laird, J. E., and Newell, A., editors, </editor> <booktitle> The Soar Papers: Research on Integrated Intelligence, </booktitle> <volume> volume 2, </volume> <pages> pages 1036-1043. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Thus they do not involve direct execution in a real external environment and they safely ignore many of the issues inherent to such environments. <ref> (Laird and Rosenbloom, 1993) </ref> Let's posit a very simple robot and consider in detail how a model based on the deliberative processing worldview would be applied to the problem of controlling this robot.
Reference: <author> Laird, J. E., Yager, E. S., Tuck, C. M., and Hucka, M. </author> <year> (1989). </year> <title> Learning in teleautonomous systems using Soar. </title> <editor> In Rodriquez, G. and Seraji, H., editors, </editor> <booktitle> Proceedings of the NASA Conference on Space Telerobotics, </booktitle> <volume> volume 3, </volume> <pages> pages 415424, </pages> <address> Pasedena, CA. </address> <institution> NASA Jet Propulsion Laboratory, California Institute of Technology. </institution>
Reference-contexts: This reconsideration is implemented by "creating a dummy operator and making preferences that make it both better and worse than the other operators" <ref> (Laird et al., 1989, p 421) </ref>. This dummy operator will now be valid in every situation, and because it is better and worse than all other decisions an impasse is created. <p> Laird acknowledges that this is a "brute-force technique to learn new features" and "although it may not be considered the most elegant or complex machine learning technique, it allows the human to easily correct the system" <ref> (Laird et al., 1989, p 422) </ref>. A system that depends on human intervention cannot be truly autonomous, but Laird argues that the human element could easily be eliminated by allowing the system to engage in experimentation.
Reference: <author> Martin, F. </author> <year> (1992). </year> <title> Mini board 2.0 technical reference. </title> <publisher> MIT Media Lab, </publisher> <address> Cambridge MA 02139. </address>
Reference-contexts: and weaknesses are revealed by varying the presence of the goal and the immediacy of feedback. 4.1 Carbot|an autonomous robot 4.1.1 The vehicle Carbot is a modified toy car (6 inches wide, 9 inches long, and 4 inches high) controlled by a programmable mini-board (designed at MIT by Fred Martin <ref> (Martin, 1992) </ref>). This board allows remote or on-board control, although the on-board memory is quite small. Because of this carbot was tethered to a PC during execution. Carbot was inexpensive to build, primarily because it makes use of primitive sensors|no lasers, video, or sonar.
Reference: <author> Meeden, L. A., McGraw, G., and Blank, D. </author> <year> (1993). </year> <title> Emergence of control and planning in an autonomous vehicle. </title> <booktitle> In Proceedings of the Fifteenth Annual Meeting of the Cognitive Science Society, </booktitle> <pages> pages 735-740, </pages> <address> Hillsdale, NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Carbot's task within this environment is just as before. At the reactive level it must perform rudimentary navigation by avoiding walls and continually moving. See <ref> (Meeden et al., 1993) </ref> for a detailed description of factors that affect a network's performance when learning this type of simple reactive task. At the goal level, carbot must again either seek or avoid the light depending on the current goal.
Reference: <author> Mozer, M. C. </author> <year> (1989). </year> <title> A focused beck-propagation algorithm for temporal pattern recognition. </title> <journal> Complex Systems, </journal> <volume> 3 </volume> <pages> 349-381. </pages>
Reference: <author> Newell, A. </author> <year> (1980). </year> <title> Physical symbol systems. </title> <journal> Cognitive Science, </journal> <volume> 4 </volume> <pages> 135-183. </pages> <note> REFERENCES 110 Newell, </note> <author> A. and Simon, H. A. </author> <year> (1972). </year> <title> Computer science as empirical inquiry: Symbols and search. </title> <journal> Communications of the ACM, </journal> 19:113-126. 
Reference-contexts: Symbols that designate the situations to be attained (including that it is to be attained, under what conditions, etc.) appear to be the only candidate for doing this. <ref> (Newell, 1980) </ref> A redefinition of plans as dynamic, context-dependent entities should in turn lead to a redefinition of goals, since the two are so inextricably linked. This chapter explores the use of goals with connectionist controllers and will provide some insights into the further development of the planning model.
Reference: <author> Norman, D. A. </author> <year> (1993a). </year> <title> Approaches to the study of intelligence. </title> <editor> In Rosenbloom, P. S., Laird, J. E., and Newell, A., editors, </editor> <booktitle> The Soar Papers: Research on Integrated Intelligence, </booktitle> <volume> volume 2, </volume> <pages> pages 793-812. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Perception and action are seen as auxiliary functions. Soar has impressively mimicked human problem solving abilities for numerous tasks, including the eight puzzle, tic-tac-toe, towers of Hanoi, missionaries and cannibals, algebraic equations, logical syllogisms, and blocks world problems <ref> (Norman, 1993a) </ref>.
Reference: <author> Norman, D. A. </author> <year> (1993b). </year> <title> Cognition in the head and in the world: An introdution to the special issue on situated action. </title> <journal> Cognitive Science, </journal> <volume> 17(1) </volume> <pages> 1-6. </pages>
Reference-contexts: It is neither right or wrong. Instead, it has to be judged on how useful it is to the pursuit of science and to a shared understanding of the phe <br>- nomena. <ref> (Norman, 1993b, p 5) </ref> As Agre and Norman suggest, a worldview provides a filter through which observations are made. This filter enhances the significance of some phenomena while diminishing the importance of others, narrowing the sphere of inquiry to a particular focus.
Reference: <author> Pearlmutter, B. A. </author> <year> (1989). </year> <title> Learning state space trajectories in recurrent neural net-works. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 263-269. </pages>
Reference: <author> Pfeifer, R. and Verschure, P. </author> <year> (1992a). </year> <title> Beyond rationalism: Symbols, patterns and behavior. Connection Science, </title> <publisher> 4(3-4):313-325. </publisher>
Reference-contexts: Deliberative processing has been the dominant worldview for both Cognitive Science and Artificial Intelligence. Others have referred to this view as rationalism 1. Reconsidering planning 3 to indicate that cognition is explained in terms of logical structures <ref> (Pfeifer and Ver- schure, 1992a) </ref>. The deliberative or rationalist view has led researchers to concentrate on abstract tasks that more easily lend themselves to a high-level, symbolic formulation, especially problems related to what have been been considered the uniquely human aspects of intelligence|consciousness, language, and planning.
Reference: <author> Pfeifer, R. and Verschure, P. F. </author> <year> (1992b). </year> <title> Designing efficiently navigating non-goaldirected robots. </title> <editor> In Meyer, J., Roitblat, H., and S., W., editors, </editor> <booktitle> From Animals to Animats. </booktitle>
Reference-contexts: Pfeifer and Ver- schure have discussed several reasons why reactivity should be the sole basis for autonomous agent design <ref> (Pfeifer and Verschure, 1992b) </ref>. In their view, memory use is linked to goal directedness and plan following, which they suggest may be unnecessary for producing structured behavior. Traditionally, goal-directed systems resort to plans, which in turn have required symbolic world models.
Reference: <author> Ram, A. and Santamaria, J. C. </author> <year> (1993). </year> <title> A multistrategy case-based and reinforcement learning approach to self-improving reactive control systems for autonomous robtic navigation. </title> <booktitle> In Proceedings of the Second International Workshop on Multistrategy learning. </booktitle>
Reference: <author> Rich, E. </author> <year> (1983). </year> <booktitle> Artificial Intelligence. </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York, NY. </address>
Reference-contexts: Only then is the plan executed. Elaine Rich notes that "for problems such as the eight-puzzle, the distinction between planning and doing is unimportant" <ref> (Rich, 1983, p 249) </ref>. This is because the world of the eight-puzzle is stationary. The computer does not physically push any of the tiles around or interact with the real world in any way.
Reference: <author> Rumelhart, D., Hinton, G., and Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In McClelland, J. and Rumelhart, D., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume I, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Rumelhart, Hinton, and Williams, who popularized this algorithm, note that "although our learning results do not guarantee that we can find a solution for all solvable problems, our analyses and results show that as a practical matter, the error propagation scheme leads to solutions in virtually every case <ref> (Rumelhart et al., 1986, page 361, emphasis in the original) </ref>.
Reference: <author> Schnepf, U. </author> <year> (1991). </year> <title> Robot ethology: A proposal for the research into intelligent autonomous systems. </title> <editor> In Meyer, J.-A. and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats, </booktitle> <pages> pages 465-474, </pages> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Smolensky, P. </author> <year> (1988). </year> <title> On the proper treatment of connectionism. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 11 </volume> <pages> 1-74. </pages>
Reference: <author> Suchman, L. A. </author> <year> (1987). </year> <title> Plans and Situated Action. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference: <author> Verschure, P. F., Krose, B. K., and Pfeifer, R. </author> <year> (1992). </year> <title> Distributed adaptive control: the self-organization of structured behavior. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 9 </volume> <pages> 181-196. </pages> <note> REFERENCES 111 Waltz, </note> <author> D. L. </author> <year> (1991). </year> <title> Eight principles for building an intelligent robot. </title> <editor> In Meyer, J.-A. and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats, </booktitle> <pages> pages 462-464, </pages> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference-contexts: They propose a model based on distributed adaptive control (DAC) that can perform tasks without resorting to goals, plans, or internal world models. The DAC model, shown in Figure 2.4, is a neural network based on classical conditioning principles that learns to integrate pre-wired reflexes with sophisticated sensors <ref> (Verschure et al., 1992) </ref>. This network has no recurrent connections and is essentially feedforward except for the lateral connection between the collision and target detection groups.
Reference: <author> Whitehead, S. D. and Ballard, D. H. </author> <year> (1991). </year> <title> Learning to perceive and act. </title> <journal> Machine Learning, </journal> <volume> 7(1) </volume> <pages> 45-83. </pages>
Reference-contexts: Thus in these situations carbot will be unable to reliably avoid hitting the walls. This has been called the perceptual aliasing problem and occurs when states in the world with important differences appear the same to the system <ref> (Whitehead and Ballard, 1991) </ref>. There are two standard approaches to alleviating this problem. The first is to improve the sensors themselves. The second, as Kaelbling describes, is "to extend the system's inputs to include the last k percepts for some value of k" (Kaelbling, 1993).
Reference: <author> Whitley, C., Dominic, S., Das, R., and Anderson, C. W. </author> <year> (1993). </year> <title> Genetic reinforcement learning for neurocontrol problems. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 259-284. </pages>
Reference-contexts: According to Whitely, "genetic algorithms are capable of performing a global search of a space because they can rely on hyperplane sampling to guide the search instead of searching along the gradient of a function" as back-propagation does <ref> (Whitley et al., 1993) </ref>. Combining genetic algorithms and connectionist networks There are many interesting possibilities for applying genetic algorithms to connectionist networks. <p> None of these trials has converged on a final solution since the fitness is still climbing, and one trial is significantly slower than other two. Both types of variability can be reduced if tough performance criteria are imposed to determine when to stop the learning <ref> (Whitley et al., 1993) </ref>. In these experiments, learning time was based on the number of actions performed rather than a particular performance standard.
Reference: <author> Williams, R. J. and Zipser, D. </author> <year> (1989). </year> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 270-280. </pages>
Reference: <author> Wilson, S. W. </author> <year> (1991). </year> <title> The animat path to AI. </title> <editor> In Meyer, J.-A. and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats, </booktitle> <pages> pages 15-21, </pages> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Yamauchi, B. M. </author> <year> (1993). </year> <title> Dynamical neural networks for mobile robot control. </title>
Reference: <institution> NRL Memorandum Report AIC-033-93, Naval Research Laboratory, </institution> <address> Washington, D.C. </address>
Reference: <author> Yamauchi, B. M. and Beer, R. D. </author> <year> (1994a). </year> <title> Integrating reactive, sequential and learn-ing behavior using dynamical neural networks. </title> <booktitle> Submitted to the Third International Conference on Simulation of Adaptive Behavior. </booktitle>
Reference: <author> Yamauchi, B. M. and Beer, R. D. </author> <year> (1994b). </year> <title> Sequential behavior and learning in evolved neural networks. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 2(3) </volume> <pages> 219-246. </pages>
References-found: 55

