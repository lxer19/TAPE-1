URL: http://http.cs.berkeley.edu/~jjones/Work/ppq.ps
Refering-URL: http://http.cs.berkeley.edu/~jjones/Work/
Root-URL: http://www.cs.berkeley.edu
Title: Parallelism and Locality in Priority Queues  
Author: A. Ranade S. Cheng E. Deprit J. Jones S. Shih 
Keyword: O(P 1=d time with high probability.  
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division University of California  
Abstract: We explore two ways of incorporating parallelism into priority queues. The first is to speed up the execution of individual priority operations so that they can be performed one operation per time step, unlike sequential implementations which require O(log N ) time steps per operation for an N element heap. We give an optimal parallel implementation that uses a linear array of O(log N ) processors. Second, we consider parallel operations on the priority queue. We show that using a d-dimensional array (constant d) of P processors we can insert or delete the smallest P elements from a heap in time O(P 1=d log 11=d P ), where the number of elements in the heap is assumed to be polynomial in P . We also show a matching lower bound, based on communication complexity arguments, for a range of deterministic implementations. Finally, using randomization, we show that the time can be reduced to the optimal 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Adler and J. Byers. </author> <title> AT 2 Bounds for a Class of VLSI Problems and String Matching. </title> <booktitle> ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1994. </year>
Reference-contexts: Standard arguments consider information transfer across the bisection of the network (or smaller dimension of a VLSI chip, for AT 2 bounds). In our case these do not give the best bounds. Bilardi and Preparata [2], and recently Adler and Byers <ref> [1] </ref>, consider information transfer between smaller regions of a VLSI chip. Our argument builds upon theirs, and is similar to Cypher's use of pin limitation arguments to prove lower bounds [3].
Reference: [2] <author> G. Bilardi and F. Preparata. </author> <title> Area-Time Lower Bound Techniques with Applications to Sorting. </title> <journal> Algorithmica, </journal> <volume> 1(1) </volume> <pages> 65-91, </pages> <year> 1986. </year>
Reference-contexts: We can lower bound the time for this using a communication complexity argument. Standard arguments consider information transfer across the bisection of the network (or smaller dimension of a VLSI chip, for AT 2 bounds). In our case these do not give the best bounds. Bilardi and Preparata <ref> [2] </ref>, and recently Adler and Byers [1], consider information transfer between smaller regions of a VLSI chip. Our argument builds upon theirs, and is similar to Cypher's use of pin limitation arguments to prove lower bounds [3].
Reference: [3] <author> R. Cypher. </author> <title> Theoretical Aspects of VLSI Pin Limitations. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22(2) </volume> <pages> 356-378, </pages> <year> 1993. </year>
Reference-contexts: In our case these do not give the best bounds. Bilardi and Preparata [2], and recently Adler and Byers [1], consider information transfer between smaller regions of a VLSI chip. Our argument builds upon theirs, and is similar to Cypher's use of pin limitation arguments to prove lower bounds <ref> [3] </ref>. First, we need the notion of a q-section width of a graph, which is a natural extension of the bisection width. Definition 1 Let V (G) denote the vertex set of a graph G.
Reference: [4] <author> R. Karp and Y. Zhang. </author> <title> A randomized parallel branch-and-bound procedure. </title> <booktitle> In Proceedings of the ACM Annual Symposium on Theory of Computing, </booktitle> <pages> pages 290-300, </pages> <year> 1988. </year>
Reference-contexts: We present one such implementation, but this involves randomization. Karp and Zhang <ref> [4] </ref> use a technique similar to the following, but only approximate a priority queue. We maintain a local priority queue at each processor. For the insert operation, each processor sends its request to a randomly chosen processor, where it is inserted into the priority queue of that processor.
Reference: [5] <author> C. Kruskal. </author> <title> Searching, merging and sorting in parallel computation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(10):942-946, </volume> <year> 1983. </year>
Reference-contexts: The key observation guaranteeing correctness is that because of step 2, steps 4 and 5 do not violate the extended heap order. Time. Rearrange takes time O (h + log P ) using standard merging algorithms <ref> [5] </ref>. For delete-min, the min-path is constructed sequentially in time O (h), and steps 2 and 5 can be performed in time O (h + log P ) by using P=h processors at each level of the heap.
Reference: [6] <author> F.T. Leighton. </author> <title> Introduction to parallel algorithms and architectures. </title> <address> Morgan-Kaufman, </address> <year> 1991. </year>
Reference-contexts: We first describe the two components of the array and then explain how the parts are interconnected. 1. Cache: This is a systolic priority queue having L = (1) + log N processors <ref> [6] </ref>. It can receive requests once every 2 clock cycles from the external world at processor P 1 and return the response (for delete-min requests) in one cycle, again at processor P 1 .
Reference: [7] <author> M. Pinotti and G. Pucci. </author> <title> Parallel priority queues. </title> <journal> Information Processing Letters, </journal> <volume> 40(1) </volume> <pages> 33-40, </pages> <year> 1991. </year>
Reference-contexts: This is a mapped simulation of the P processor O (log P ) time algorithm due to Pinotti and Pucci <ref> [7] </ref> for performing P operations in parallel (Section 3). Note that brute force simulation of [7] would take time O (P 1=d log P ). 3. <p> This is a mapped simulation of the P processor O (log P ) time algorithm due to Pinotti and Pucci <ref> [7] </ref> for performing P operations in parallel (Section 3). Note that brute force simulation of [7] would take time O (P 1=d log P ). 3. We give a matching lower bound of (P 1=d log 11=d P ) for all mapped simulations of Pinotti and Pucci's algorithm (Section 6) on arrays. <p> Also, for the P processor butterfly network, we can show a lower bound of (log 2 P ) time. Notice that this lower bound can be matched simply by a brute-force (albeit ran domized) simulation of the algorithm of <ref> [7] </ref>. 4. We give a randomized priority queue implementation for a P processor d-dimensional array that performs P operations in time O (P 1=d ). This matches the bisection width lower bound applicable to all priority queue implementations. <p> As a result, the cache cannot return anything larger than x at step t, giving the contradiction. 3 P -Bandwidth Heap We next consider executing P heap operations in parallel using P processors. On the PRAM this can be done using the P -bandwidth heap of Pinotti and Pucci <ref> [7] </ref>, the operation of which we outline below. The P -bandwidth heap is similar to the sequential binary heap, except that each node holds P keys instead of just one. <p> For N polynomial in P , the total time for each parallel operation is O (log P ). 4 Array Implementation We first consider mapped simulations of <ref> [7] </ref> on 2-dimensional arrays of P processors. The P -bandwidth heap is distributed among processors as follows. We divide the mesh into square blocks of P= log P processors and number the blocks in a snakelike order, from B 1 to B log P (see Figure 2). <p> T = (log 1:5 P ), which is weaker than what we have in Corollary 2. 7 Randomized Priority Queues No mapped simulation of <ref> [7] </ref> can be faster than the lower bound given above; however, this does not rule out the possibility of faster customized implementations. We present one such implementation, but this involves randomization. Karp and Zhang [4] use a technique similar to the following, but only approximate a priority queue.
Reference: [8] <author> A.G. Ranade. </author> <title> A Framework for Analyzing Locality Issues in Parallel Computing. </title> <booktitle> In Proceedings of the International Heinz-Nixdorf Symposium on "Parallel Architectures and their Efficient Use", </booktitle> <institution> University of Paderborn, Paderborn, Ger-many, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: In this case, which we will refer to as brute-force simulation, the data structures used by the algorithm are typically distributed randomly among the processors in the network. In some cases, this can be proved to be the best possible strategy <ref> [8] </ref>, but here we use it as a baseline to evaluate sparse network algorithms. The following alternative strategy is gaining popularity. The user first writes a PRAM program, and then annotates it using directives to indicate how data structures must be stored.
Reference: [9] <author> V.N. Rao and V. Kumar. </author> <title> Concurrent access of priority queues. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(12) </volume> <pages> 1657-1665, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: We thus match the performance achieved on the PRAM by Rao and Kumar <ref> [9] </ref>, but we require only a sparse network. Our solution is not a mapped simulation but rather a customized al gorithm for the linear array. 2.
References-found: 9

