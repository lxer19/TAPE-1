URL: ftp://ftp.santafe.edu/pub/wgm/nfl.ps
Refering-URL: http://www.aic.nrl.navy.mil/~spears/yin-yang.html
Root-URL: 
Email: (dhw@santafe.edu)  (wgm@santafe.edu)  
Title: No Free Lunch Theorems for Search SFI-TR-95-02-010  
Author: David H. Wolpert William G. Macready 
Date: February 23, 1996  
Address: 1399 Hyde Park Road Santa Fe, NM, 87501  
Affiliation: The Santa Fe Institute  
Abstract: We show that all algorithms that search for an extremum of a cost function perform exactly the same, according to any performance measure, when averaged over all possible cost functions. In particular, if algorithm A outperforms algorithm B on some cost functions, then loosely speaking there must exist exactly as many other functions where B outperforms A. Starting from this we analyze a number of the other a priori characteristics of the search problem, like its geometry and its information-theoretic aspects. This analysis allows us to derive mathematical benchmarks for assessing a particular search algorithm's performance. We also investigate minimax aspects of the search problem, the validity of using characteristics of a partial search over a cost function to predict future behavior of the search algorithm on that cost function, and time-varying cost functions. We conclude with some discussion of the justifiability of biologically-inspired search methods.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.O. Berger, </author> <title> Statistical Decison Theory and Bayesian Analysis, </title> <publisher> Springer-Verlag (1985). </publisher>
Reference-contexts: Other issues to be explored involve the relation between the statistical view of search adopted in this paper and conventional statistics. In particular the field of optimal experimental design <ref> [1] </ref> and more precisely active learning [2] is concerned with the following question: There is some unknown probabilistic relationship between X and Y. I have a set of pairs of X -Y values formed by sampling that relationship (the "training set").
Reference: [2] <author> D. Cohn, </author> <title> Neural Network Exploration Using Optimal Experimental Design, </title> <publisher> MIT AI Memo. </publisher> <pages> 1491. </pages>
Reference-contexts: Other issues to be explored involve the relation between the statistical view of search adopted in this paper and conventional statistics. In particular the field of optimal experimental design [1] and more precisely active learning <ref> [2] </ref> is concerned with the following question: There is some unknown probabilistic relationship between X and Y. I have a set of pairs of X -Y values formed by sampling that relationship (the "training set").
Reference: [3] <author> T. Cover, J. Thomas, </author> <title> Elements of Information Theory, </title> <publisher> John Wiley & Sons, </publisher> <year> (1991). </year>
Reference-contexts: Call the fraction we are interested in alg (~ff; ~ fi). It turns out that alg (~ff; ~ fi) depends to leading order on the Kullback-Liebler "distance" <ref> [3] </ref> between ~ff and ~ fi.
Reference: [4] <author> M.R. Garey, D.S. Johnson, </author> <title> Computers and Intractability, </title> <publisher> Freeman (1979). </publisher>
Reference-contexts: Examples also abound in combinatorial optimization, ranging from number partitioning to graph coloring to scheduling <ref> [4] </ref>. 1 There are two common approaches to these optimization problems. The first is a sys-tematic construction of a good X value, x 0 , from good sub-solutions specifying part of x 0 . The most celebrated method of this type is the branch and bound algorithm [9].
Reference: [5] <author> J. Holland, </author> <title> Adaptation in Natural and Artificial Systems, </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, </address> <year> (1975). </year>
Reference-contexts: A second approach to optimization begins with a population of one or more complete solutions x 2 X and the associated Y values, and (tries to) iteratively improves upon those X values. There are many algorithms of this type, including hill-climbing, simulated annealing [7], and genetic algorithms <ref> [5] </ref>. Intuitively, one would expect that for this class of algorithms to work effectively, the biases in how they try to improve the population (i.e., the biases in how they search X ) must "match" those implicit in the cost function they are optimizing.
Reference: [6] <author> L. Ingber, </author> <title> Adaptive Simulated Annealing, Software package documentation, </title> <publisher> ftp.alumni.caltech.edu:/pub/ingber/asa.tar.gz. </publisher>
Reference-contexts: If a genetic algorithm outperforms simulated annealing over some class of cost functions , then over the remaining cost functions F n , simulated annealing must outperform the genetic algorithm. It should be noted that this conservation applies even if one considers "adaptive" search algorithms <ref> [6, 18] </ref> which modify their search strategy based on properties of the population of (X Y) pairs observed so far in the search, and which perform this "adaptation" without regard to any knowledge concerning salient features of f .
Reference: [7] <author> S. Kirkpatrick, C. D. Gelatt Jr., M. P. </author> <title> Vecchi, </title> <journal> Science, </journal> <volume> 220, 671, </volume> <year> (1983). </year>
Reference-contexts: A second approach to optimization begins with a population of one or more complete solutions x 2 X and the associated Y values, and (tries to) iteratively improves upon those X values. There are many algorithms of this type, including hill-climbing, simulated annealing <ref> [7] </ref>, and genetic algorithms [5]. Intuitively, one would expect that for this class of algorithms to work effectively, the biases in how they try to improve the population (i.e., the biases in how they search X ) must "match" those implicit in the cost function they are optimizing.
Reference: [8] <author> R. Kohavi, </author> <title> personal communication. Also see A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection, </title> <note> to be presented at IJCAI 1995. </note>
Reference-contexts: What's important is how much better the improvement is than you would expect it to be solely due to the "fittest can only improve" effect. That's what our measures are designed to assess. Given the recent experience in the supervised learning community <ref> [8, 13, 10] </ref>, it seems quite likely that on a significant fraction of the problems in the standard test suites, one or more of the currently popular search algorithms will fail to perform well, at least for some range of population sizes.
Reference: [9] <author> E.L. Lawler, D.E. </author> <title> Wood, </title> <journal> Operations Research, </journal> <volume> 14(4), </volume> <pages> 699-719, </pages> <year> (1966). </year> <month> 33 </month>
Reference-contexts: The first is a sys-tematic construction of a good X value, x 0 , from good sub-solutions specifying part of x 0 . The most celebrated method of this type is the branch and bound algorithm <ref> [9] </ref>. For this systematic and exhaustive approach to work in reasonable time, one must have an effective heuristic, h (n), representing the quality of sub-solutions n. There is extensive theoretical work [11] linking the cost function to the properties a heuristic must have in order to search efficiently.
Reference: [10] <author> P. Murphy, M. </author> <title> Pazzani, </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <month> 257-275 </month> <year> (1994). </year>
Reference-contexts: What's important is how much better the improvement is than you would expect it to be solely due to the "fittest can only improve" effect. That's what our measures are designed to assess. Given the recent experience in the supervised learning community <ref> [8, 13, 10] </ref>, it seems quite likely that on a significant fraction of the problems in the standard test suites, one or more of the currently popular search algorithms will fail to perform well, at least for some range of population sizes.
Reference: [11] <author> J. Pearl, </author> <title> Heuristics, intelligent search strategies for computer problem solving, </title> <publisher> Addison-Wesley, </publisher> <year> (1984). </year>
Reference-contexts: The most celebrated method of this type is the branch and bound algorithm [9]. For this systematic and exhaustive approach to work in reasonable time, one must have an effective heuristic, h (n), representing the quality of sub-solutions n. There is extensive theoretical work <ref> [11] </ref> linking the cost function to the properties a heuristic must have in order to search efficiently. A second approach to optimization begins with a population of one or more complete solutions x 2 X and the associated Y values, and (tries to) iteratively improves upon those X values.
Reference: [12] <author> Gerhard Reinelt, </author> <title> The Traveling Salesman, computational solutions for TSP applications, </title> <publisher> Springer Verlag Berlin Heidelberg (1994). </publisher>
Reference-contexts: The Traveling Salesman Problem (TSP) is an excellent example of such a situation; the best search algorithms for the TSP problem are hand-tailored for it <ref> [12] </ref>. Linear programming problems are another example; the simplex algorithm is a search algorithm specifically designed to solve cost functions of a particular type.
Reference: [13] <author> C. Schaffer, </author> <title> Conservation of Generalization: A Case Study. </title>
Reference-contexts: What's important is how much better the improvement is than you would expect it to be solely due to the "fittest can only improve" effect. That's what our measures are designed to assess. Given the recent experience in the supervised learning community <ref> [8, 13, 10] </ref>, it seems quite likely that on a significant fraction of the problems in the standard test suites, one or more of the currently popular search algorithms will fail to perform well, at least for some range of population sizes.
Reference: [14] <author> P.F. </author> <title> Stadler, </title> <journal> Europhys. Lett. </journal> <volume> 20, pp479-482, </volume> <year> (1992). </year>
Reference-contexts: I have a set of pairs of X -Y values formed by sampling that relationship (the "training set"). At what next 5 As an example, might be the set of correlated cost functions as in <ref> [14] </ref>. 31 X value should I sample the relationship to "best" help me infer the full X -Y relationship? This question of how best to conduct active learning is obviously very closely related to the search problem; future work involves seeing what results in the field of active learning can be
Reference: [15] <author> C.E.M. Strauss, D.H. Wolpert, D.R. Wolf. </author> <title> Alpha, Evidence, and the Entropic Prior in Maximum Entropy and Bayesian Methods, </title> <editor> ed. Ali Mohammed-Djafari, pp113-120, </editor> <year> (1992). </year>
Reference-contexts: However Y is defined, the normalization constant of Eq. (8) can be found by summing over all ~ff lying on the unit simplex. The details of such a calculation can be found in <ref> [15] </ref>.
Reference: [16] <author> D H. Wolpert, </author> <title> Off-training set error and a priori distinctions between learning algorithms, </title> <type> Technical Report SFI-TR-95-01-003, </type> <institution> Santa Fe Institute, </institution> <year> 1995. </year>
Reference-contexts: In short, there are no "free lunches" for effective optimization; any algorithm performs only as well as the knowledge concerning the cost function put into the cost algorithm. For this reason (and to emphasize the parallel with similar supervised learning results <ref> [16, 17] </ref>), we have dubbed our central result a "no free lunch" (NFL) theorem. To prove the NFL theorem a framework has to be developed which addresses the core aspects of search. <p> In fact, things may very well be worse than this. In supervised learning, there is a result related to the theorem above <ref> [16] </ref>.
Reference: [17] <author> D H. Wolpert, </author> <title> On Overfitting Avoidance as Bias, </title> <type> Technical Report SFI-TR-92-03-5001, </type> <institution> Santa Fe Institute, </institution> <year> 1992. </year>
Reference-contexts: In short, there are no "free lunches" for effective optimization; any algorithm performs only as well as the knowledge concerning the cost function put into the cost algorithm. For this reason (and to emphasize the parallel with similar supervised learning results <ref> [16, 17] </ref>), we have dubbed our central result a "no free lunch" (NFL) theorem. To prove the NFL theorem a framework has to be developed which addresses the core aspects of search.
Reference: [18] <author> D. Yuret, M. de la Maza, </author> <title> Dynamic Hill-Climbing: </title> <booktitle> Overcoming the limitations of optimization techniques in The Second Turkish Symposium on Artificial Intelligence and Neural Networks, </booktitle> <address> pp208-212, </address> <year> (1993). </year>
Reference-contexts: If a genetic algorithm outperforms simulated annealing over some class of cost functions , then over the remaining cost functions F n , simulated annealing must outperform the genetic algorithm. It should be noted that this conservation applies even if one considers "adaptive" search algorithms <ref> [6, 18] </ref> which modify their search strategy based on properties of the population of (X Y) pairs observed so far in the search, and which perform this "adaptation" without regard to any knowledge concerning salient features of f .
References-found: 18

