URL: ftp://ftp.cs.colorado.edu/users/mozer/papers/timeseries.ps
Refering-URL: http://www.cs.colorado.edu/~mozer/papers/timeseries.html
Root-URL: http://www.cs.colorado.edu
Title: Neural net architectures for temporal sequence processing  
Author: Michael C. Mozer 
Date: February 9, 1993  
Address: Boulder, CO 80309-0430  
Affiliation: Department of Computer Science Institute of Cognitive Science University of Colorado  
Note: To appear in: A. Weigend N. Gershenfeld (Eds.), Predicting the future and understanding the past. Redwood City, CA: Addison-Wesley Publishing.  
Abstract: I present a general taxonomy of neural net architectures for processing time-varying patterns. This taxonomy subsumes many existing architectures in the literature, and points to several promising architectures that have yet to be examined. Any architecture that processes time-varying patterns requires two conceptually distinct components: a short-term memory that holds on to relevant past events and an associator that uses the short-term memory to classify or predict. My taxonomy is based on a characterization of short-term memory models along the dimensions of form, content, and adaptability. Experiments on predicting future values of a financial time series (US dollar-Swiss franc exchange rates) are presented using several alternative memory models. The results of these experiments serve as a baseline against which more sophisticated architectures can be compared. Neural networks have proven to be a promising alternative to traditional techniques for nonlinear temporal prediction tasks (e.g., Curtiss, Brandemuehl, & Kreider, 1992; Lapedes & Farber, 1987; Weigend, Huberman, & Rumelhart, 1992). However, temporal prediction is a particularly challenging problem because conventional neural net architectures and algorithms are not well suited for patterns that vary over time. The prototypical use of neural nets is in structural pattern recognition. In such a task, a collection of features|visual, semantic, or otherwise|is presented to a network and the network must categorize the input feature pattern as belonging to one or more classes. For example, a network might be trained to classify animal species based on a set of attributes describing living creatures such as "has tail", "lives in water", or "is carnivorous"; or a network could be trained to recognize visual patterns over a two-dimensional pixel array as a letter in fA; B; . . . ; Zg. In such tasks, the network is presented with all relevant information simultaneously. In contrast, temporal pattern recognition involves processing of patterns that evolve over time. The appropriate response at a particular point in time depends not only on the current input, but potentially all previous inputs. This is illustrated in Figure 1, which shows the basic framework for a temporal prediction problem. I assume that time is quantized into discrete steps, a sensible assumption because many time series of interest are intrinsically discrete, and continuous series can be sampled at a fixed interval. The input at time t is denoted x(t). For univariate series, this input 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bachrach, J. </author> <year> (1988). </year> <title> Learning to represent state. </title> <type> Unpublished master's thesis, </type> <institution> University of Mas-sachusetts, Amherst. </institution>
Reference: <author> Ballard, D. H. </author> <year> (1986). </year> <title> Cortical connections and parallel processing: Structure and function. </title> <journal> The Behavioral and Brain Sciences, </journal> <volume> 9, </volume> <pages> 67-120. </pages>
Reference-contexts: The target output of the neural net is a prediction of the change in the series value p minutes in the future, i.e., s ( s t + p ) s ( s t). Each of the input and output quantities is represented using a variable encoding <ref> (Ballard, 1986) </ref>, which means that the input and output activities are monotonic in the represented quantity.
Reference: <author> Bengio, Y., De Mori, R., & Cardin, R. </author> <year> (1990). </year> <title> Speaker independent speech recognition with neural networks and speech knowledge. </title> <editor> In D. S. Touretzky (Ed.), </editor> <booktitle> Advances in neural network information processing systems II (pp. </booktitle> <pages> 218-225). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bengio, Y., Frasconi, P., & Simard, P. </author> <year> (1993). </year> <title> The problem of learning long-term dependencies in recurrent networks. </title> <booktitle> Proceedings of the IEEE International Conference on Neural Networks. To appear. </booktitle>
Reference: <author> Bodenhausen, U., & Waibel, A. </author> <year> (1991). </year> <title> The Tempo 2 algorithm: Adjusting time delays by supervised learning. </title> <editor> In R. P. Lippmann, J. Moody, & D. S. Touretzky (Eds.), </editor> <booktitle> Advances in neural information processing systems 3 (pp. </booktitle> <pages> 155-161). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bridle, J. </author> <year> (1990). </year> <title> Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. </title> <editor> In D. S. Touretzky (Ed.), </editor> <booktitle> Advances in neural information processing systems 2 (pp. </booktitle> <pages> 211-217). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Curtiss, P., Kreider, J., & Brandemuehl, M. </author> <year> (1992). </year> <title> Adaptive control of HVAC processes using predictive neural networks (Technical Report). </title> <institution> Boulder, CO: University of Colorado, Joint Center for Energy Management. </institution> <note> de Vries, </note> <author> B., & Principe, J. C. </author> <year> (1991). </year> <title> A theory for neural networks with time delays. </title> <editor> In R. </editor> <address> P. </address>
Reference: <editor> Lippmann, J. Moody, & D. S. Touretzky (Eds.), </editor> <booktitle> Advances in neural information processing systems 3 (pp. </booktitle> <pages> 162-168). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. Mozer 19 de Vries, </publisher> <editor> B., & Principe, J. C. </editor> <year> (1992). </year> <title> The gamma model|A new neural net model for temporal processing. </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> 565-576. </pages>
Reference: <author> Elman, J. L. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14, </volume> <pages> 179-212. </pages>
Reference: <author> Elman, J. L., & Zipser, D. </author> <year> (1988). </year> <title> Learning the hidden structure of speech. </title> <journal> Journal of the Acoustical Society of America, </journal> <volume> 83, </volume> <pages> 1615-1625. </pages>
Reference: <author> Finnoff, W., Hergert, F., & Zimmermann, H. G. </author> <year> (1992). </year> <title> Improving model selection by nonconver-gent methods. </title> <type> Unpublished manuscript. </type>
Reference: <author> Frasconi, P., Gori, M., & Soda, G. </author> <year> (1992). </year> <title> Local feedback multilayered networks. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 120-130. </pages>
Reference: <author> Geman, S., Bienenstock, E., & Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 1-58. </pages>
Reference: <author> Herz, A. V. M. </author> <year> (1991). </year> <title> Global analysis of parallel analog networks with retarded feedback. </title> <journal> Physical Review A, </journal> <volume> 44, </volume> <pages> 1415-1418. </pages>
Reference: <author> Hochreiter, J. </author> <year> (1991). </year> <type> Untitled Diploma Thesis. </type> <institution> Institute fuer Informatik, Technische Universitaet Muenchen. </institution>
Reference-contexts: This situation is true for feedforward nets as well as recurrent nets, but the problem is exacerbated 2 With careful selection of the weights, one can prevent the gradients from shrinking through time <ref> (Hochreiter, 1991) </ref>; however, this imposes serious restrictions on the sorts of memories the network can form. Mozer 12 by the deeply layered structure of the unfolded net. The result is an error surface fraught with local optima.
Reference: <author> Jordan, M. I. </author> <year> (1987). </year> <title> Attractor dynamics and parallelism in a connectionist sequential machine. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 531-546). </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Kleinfeld, D. </author> <year> (1986). </year> <title> Sequential state generation by model neural networks. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> 83, </volume> <pages> 9469-9473. </pages>
Reference: <author> Kuhn, R., & van Hemmen, J. L. </author> <year> (1991). </year> <title> Self-organizing maps and adaptive filters. </title> <editor> In E. Domany, J. L. van Hemmen, & K. Schulten (Eds.), </editor> <booktitle> Models of Neural Networks (pp. </booktitle> <pages> 213-280). </pages> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Lapedes, A., & Farber, R. </author> <year> (1987). </year> <title> Nonlinear signal processing using neural networks (Report No. </title> <institution> LA-UR-87-2662). Los Alamos, NM: Los Alamos National Laboratory. </institution>
Reference: <author> Le Cun, Y., Kanter, I., & Solla, S. A. </author> <year> (1991). </year> <title> Second order properties of error surfaces: Learning time and generalization. </title> <editor> In R. P. Lippmann, J. Moody, & D. S. Touretzky (Eds.), </editor> <booktitle> Advances in neural information processing systems 3 (pp. </booktitle> <pages> 918-924). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lincoln, W. P., & Skrzypek, J. </author> <year> (1990). </year> <title> Synergy of clustering multiple back propagation networks. </title>
Reference: <editor> In D. S. Touretzky (Ed.), </editor> <booktitle> Advances in neural information processing systems 2 (pp. </booktitle> <pages> 650-657). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <editor> Mozer 20 McClelland, J. L., & Elman, J. L. </editor> <year> (1986). </year> <title> Interactive processes in speech perception: The TRACE model. </title> <editor> In J. L. McClelland & D. E. Rumelhart (Eds.), </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition. Volume II: Psychological and biological models (pp. </booktitle> <pages> 58-121). </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The choice of number of hidden units and number of delays was based on validation testing, as described below. The various networks were trained using stochastic gradient descent, whereby the weights were updated following presentation of an entire day's data. Back propagation through time <ref> (Rumelhart, Hinton, & Williams, 1986) </ref> was used to train the recurrent connections in the TIS-0 memory.
Reference: <author> Mozer, M. C. </author> <year> (1989). </year> <title> A focused back-propagation algorithm for temporal pattern recognition. </title> <journal> Complex Systems, </journal> <volume> 3, </volume> <pages> 349-381. </pages>
Reference-contexts: The TDNN, for example, is an I-delay memory attached to a series of TI-delay memory modules. I have proposed architectures combining an I-delay memory with a TIS-exponential memory (Mozer & Soukup, 1991) and an I-delay memory with a TI-exponential memory <ref> (Mozer, 1989) </ref>. The possibilities are, of course, diverse, but the certain conclusion is that which class of memory is best depends on the domain and the task.
Reference: <author> Mozer, M. C. </author> <year> (1992). </year> <title> The induction of multiscale temporal structure. </title> <editor> In J. E. Moody, S. J. Hanson, & R. P. Lippman (Eds.), </editor> <booktitle> Advances in neural information processing systems IV (pp. </booktitle> <pages> 275-282). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mozer, M. C., & Soukup, T. </author> <year> (1991). </year> <note> CONCERT: A connectionist composer of erudite tunes. In R. </note>
Reference-contexts: Beyond the simple memory classes, hybrid schemes can also be considered. The TDNN, for example, is an I-delay memory attached to a series of TI-delay memory modules. I have proposed architectures combining an I-delay memory with a TIS-exponential memory <ref> (Mozer & Soukup, 1991) </ref> and an I-delay memory with a TI-exponential memory (Mozer, 1989). The possibilities are, of course, diverse, but the certain conclusion is that which class of memory is best depends on the domain and the task.
Reference: <editor> P. Lippmann, J. Moody, & D. S. Touretzky (Eds.), </editor> <booktitle> Advances in neural information processing systems 3 (pp. </booktitle> <pages> 789-796). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Myers, C. </author> <year> (1990). </year> <title> Learning with delayed reinforcement through attention-driven buffering (Technical Report). </title> <institution> London: Neural Systems Engineering Group, Department of Electrical Engineering, Imperial College of Science, Technology, and Medicine. </institution>
Reference-contexts: The idea underlying this approach is to discard redundant input elements in a sequence. <ref> (See Myers, 1990, and Ring, 1991, for variants of this idea.) </ref> For the most part, I have also sidestepped the issue of network dynamics, assuming instead the typical back propagation style activation dynamics.
Reference: <author> Pearlmutter, B. A. </author> <year> (1989). </year> <title> Learning state space trajectories in recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 263-269. </pages>
Reference-contexts: There is an interesting literature on temporal associative networks that seek energy minima (e.g., Herz, 1991; Kleinfeld, 1986; Sompolinsky & Kanter, 1986) and networks that operate with continuous time dynamics <ref> (Pearlmutter, 1989) </ref>. However, this work more directly addresses the issue of network architecture, not representation, and as I indicated at the outset, I have focused on representational issues.
Reference: <author> Plaut, D. C., Nowlan, S., & Hinton, G. E. </author> <year> (1986). </year> <title> Experiments on learning by back propagation (Technical report CMU-CS-86-126). </title> <institution> Pittsburgh, PA: Carnegie-Mellon University, Department of Computer Science. </institution>
Reference: <author> Ring, M. B. </author> <year> (1991). </year> <title> Incremental development of complex behaviors through automatic construction of sensory-motor hierarchies. </title> <editor> In L. Birnbaum & G. Collins (Eds.), </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop (pp. </booktitle> <pages> 343-347). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Robinson, A. J., & Fallside, F. </author> <year> (1987). </year> <title> The utility driven dynamic error propagation netowrk (Tech Report CUED/F-INFENG/TR.1). </title> <institution> Cambridge: Cambridge University, Department of Engineering. </institution>
Reference: <author> Rosenblatt, F. </author> <year> (1962). </year> <title> Principles of neurodynamics. </title> <address> New York: </address> <publisher> Spartan. </publisher>
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart & J. L. McClelland (Eds.), </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition. </booktitle> <volume> Volume I: </volume> <pages> Foundations (pp. 318-362). </pages> <address> Cambridge, MA: </address> <publisher> MIT Press/Bradford Books. </publisher>
Reference-contexts: The choice of number of hidden units and number of delays was based on validation testing, as described below. The various networks were trained using stochastic gradient descent, whereby the weights were updated following presentation of an entire day's data. Back propagation through time <ref> (Rumelhart, Hinton, & Williams, 1986) </ref> was used to train the recurrent connections in the TIS-0 memory.
Reference: <author> Rumelhart, D. E. </author> <title> (in press). Connectionist processing and learning as statistical inference. </title> <booktitle> In Y. </booktitle>
Reference: <editor> Chauvin & D. E. Rumelhart (Eds.), Backpropagation: </editor> <booktitle> Theory, architectures, and applications. </booktitle> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. Mozer 21 Schmidhuber, </publisher> <editor> J. </editor> <year> (1992). </year> <title> A fixed size storage O(n 3 ) time complexity learning algorithm for fully recurrent continually running networks. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 243-248. </pages>
Reference: <author> Schmidhuber, J. </author> <year> (1992). </year> <title> Learning unambiguous reduced sequence descriptions. </title> <editor> In J. E. Moody, S. J. Hanson, & R. P. Lippman (Eds.), </editor> <booktitle> Advances in neural information processing systems IV (pp. </booktitle> <pages> 291-298). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schmidhuber, J., Prelinger, D., & Mozer, M. C. </author> <year> (1993). </year> <title> Continuous history compression. </title> <note> Manuscript in preparation. </note>
Reference: <author> Smolensky, P. </author> <year> (1990). </year> <title> Tensor product variable binding and the representation of symbolic structures in connectionist networks. </title> <journal> Artificial Intelligence, </journal> <volume> 46, </volume> <pages> 159-216. </pages>
Reference: <author> Sompolinsky, H., & Kanter, I. </author> <year> (1986). </year> <title> Temporal association in asymmetric neural networks. </title> <journal> Physical Review Letters, </journal> <volume> 57, </volume> <pages> 2861-2864. </pages>
Reference: <author> Stornetta, W. S., Hogg, T., & Huberman, B. A. </author> <year> (1988). </year> <title> A dynamical approach to temporal pattern processing. </title> <booktitle> In Neural Information Processing Systems (pp. </booktitle> <pages> 750-759). </pages> <address> New York: </address> <publisher> American Institute of Physics. </publisher>
Reference: <author> Tank, D. W., & Hopfield, J. J. </author> <year> (1987). </year> <title> Neural computation by concentrating information in time. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> 84, </volume> <pages> 1896-1900. </pages>
Reference: <author> Unnikrishnan, K. P., Hopfield, J. J., & Tank, D. W. </author> <year> (1991). </year> <title> Connected-digit speaker-dependent speech recognition using a neural network with time-delayed connections. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 39, </volume> <pages> 698-713. </pages>
Reference: <author> Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., & Lang, K. </author> <year> (1987). </year> <title> Phoneme recognition using time-delay neural networks (Technical report TR-1-0006). </title> <institution> Japan: ATR Interpreting Telephony Research Labs. </institution>
Reference: <author> Wan, E. </author> <year> (1993). </year> <title> Finite impulse response neural networks for autoregressive time series prediction. This volume. </title>
Reference: <author> Watrous, R. L., & Shastri, L. </author> <year> (1987). </year> <title> Learning acoustic features from speech data using connectionist networks. </title> <booktitle> In Proceedings of the Ninth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 518-530). </pages> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Weigend, A. S., Huberman, B. A., & Rumelhart, D. E. </author> <year> (1990). </year> <title> Predicting the future: A connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1, </volume> <pages> 193-209. </pages>
Reference: <author> Weigend, A. S., Huberman, B. A., & Rumelhart, D. E. </author> <year> (1992). </year> <title> Predicting sunspots and exchange rates with connectionist networks. </title> <editor> In M. Casdagli & S. Eubank (Eds.), </editor> <booktitle> Nonlinear modeling and forecasting: Proceedings of the workshop on nonlinear modeling and forecasting (pp. </booktitle> <pages> 395-431). </pages> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Company. Mozer 22 Widrow, </publisher> <editor> B., & Stearns, S. D. </editor> <booktitle> (1985). Adaptive signal processing. </booktitle> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference: <author> Williams, R. J., & Zipser, D. </author> <year> (1989). </year> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 270-280. </pages>
Reference: <author> Zhang, X., & Hutchinson, J. </author> <year> (1993). </year> <title> Practical issues in nonlinear time series prediction. This volume. </title>
References-found: 49

