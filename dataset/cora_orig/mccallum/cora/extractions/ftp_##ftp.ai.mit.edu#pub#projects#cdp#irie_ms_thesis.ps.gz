URL: ftp://ftp.ai.mit.edu/pub/projects/cdp/irie_ms_thesis.ps.gz
Refering-URL: http://www.ai.mit.edu/projects/cog/Text/publications.html
Root-URL: 
Title: Robust Sound Localization: An Application of an Auditory Perception System for a Humanoid Robot  
Author: by Robert Eiichi Irie Rodney A. Brooks 
Degree: (1993) Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Master of Science at the  c Robert E. Irie, 1995 The author hereby grants to MIT permission to reproduce and to distribute copies of this thesis document in whole or in part. Signature of Author  Certified by  Professor, Department of Electrical Engineering Computer Science Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, Departmental Committee on Graduate Students  
Date: June 1995  May 24, 1995  
Affiliation: S.B., Harvard University  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: <author> AD (1994), </author> <title> Analog Devices AD1848K Parallel-Port 16-Bit SoundPort Stereo Codec, </title> <type> rev. 0 edn. </type>
Reference-contexts: The built-in analog to digital converters (ADC) include linear phase (decimation) low-pass filters with a 3-dB point at around the Nyquist frequency. 8 Only a simple single pole external low pass filter is necessary to insure anti-aliasing, simplifying the microphone pre-amplifier circuit greatly <ref> (AD 1994) </ref>. Any standard microphone can be interfaced to the system; electret condenser microphones were chosen for their small size and power requirements. board and their interactions. The audio board has been designed to be very simple to program and use. <p> Figure A-2: Audio Board: Codec Interface 58 Figure A-3: Audio Board: DMA PAL Interface 59 Figure A-4: Audio Board: DPRAM Interface 60 Figure A-5: Audio Board: 68HC11 Controller 61 A.2.2 Audio PAL State Diagram Figure A-6: Audio Board: FSM State Diagram 62 A.2.3 Codec Information Figure A-7: Codec Block Diagram <ref> (AD 1994) </ref> Figure A-8: Frequency Response of ADC (AD 1994) 63 Figure A-9: Timing Diagram for DMA accesses (AD 1994) 64 Appendix B Training Data B.1 Clap B.1.1 "Center" Direction Figure B-1: Clap: "Cen ter" Figure B-2: Time Do main Cues Figure B-3: Frequency Domain Cues 65 B.1.2 "Right" Direction Figure <p> A-3: Audio Board: DMA PAL Interface 59 Figure A-4: Audio Board: DPRAM Interface 60 Figure A-5: Audio Board: 68HC11 Controller 61 A.2.2 Audio PAL State Diagram Figure A-6: Audio Board: FSM State Diagram 62 A.2.3 Codec Information Figure A-7: Codec Block Diagram <ref> (AD 1994) </ref> Figure A-8: Frequency Response of ADC (AD 1994) 63 Figure A-9: Timing Diagram for DMA accesses (AD 1994) 64 Appendix B Training Data B.1 Clap B.1.1 "Center" Direction Figure B-1: Clap: "Cen ter" Figure B-2: Time Do main Cues Figure B-3: Frequency Domain Cues 65 B.1.2 "Right" Direction Figure B-4: Clap: "Right" Figure B-5: Time Do main <p> Board: DPRAM Interface 60 Figure A-5: Audio Board: 68HC11 Controller 61 A.2.2 Audio PAL State Diagram Figure A-6: Audio Board: FSM State Diagram 62 A.2.3 Codec Information Figure A-7: Codec Block Diagram <ref> (AD 1994) </ref> Figure A-8: Frequency Response of ADC (AD 1994) 63 Figure A-9: Timing Diagram for DMA accesses (AD 1994) 64 Appendix B Training Data B.1 Clap B.1.1 "Center" Direction Figure B-1: Clap: "Cen ter" Figure B-2: Time Do main Cues Figure B-3: Frequency Domain Cues 65 B.1.2 "Right" Direction Figure B-4: Clap: "Right" Figure B-5: Time Do main Cues Figure B-6: Frequency Domain Cues 66 B.2 Spoken "ahh"
Reference: <author> Beauchamp, K. & Yuen, C. </author> <year> (1979), </year> <title> Digital Methods for Signal Analysis, </title> <publisher> George Allen and Unwin. </publisher>
Reference-contexts: an N-point discrete time series transforms to an N 2 -point discrete Fourier Transform series (the other N 2 points of the DFT are symmetric copies and contain no additional information), the original time series is usually extended with zero-value samples to increase the resulting resolution of the DFT series <ref> (Beauchamp & Yuen 1979) </ref>. <p> Correlation Analysis Correlation analysis has been used in a variety of fields from statistics, in determining the similarities or "correlation" between two signals, to control theory, in deriving an approximate impulse response function of the plant, to signal processing, in recovering signal from noise <ref> (Beauchamp & Yuen 1979) </ref>. Another application of correlation analysis is to determine the phase difference between two identical time-shifted signals. While the left and right channel sound signals are not identical, they are similar enough to exploit this last application.
Reference: <author> Beranek, L. </author> <year> (1970), </year> <title> Acoustics, </title> <publisher> MIT. </publisher>
Reference-contexts: They have developed a neural network that performs front/back determination of pulsed sounds, without visual input, and uses differences 5 A detailed discussion of acoustic theory is beyond the scope of this thesis, and interested parties may refer to dedicated texts on acoustics. One such text is <ref> (Beranek 1970) </ref>. 6 It has been shown that the human auditory system suppresses these later arriving signals some what when determining directionality. 19 in both onset time and power spectra.
Reference: <author> Blauert, J. </author> <year> (1983), </year> <title> Spatial Hearing, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Head motions remove ambiguities from localization that may occur from using ITDs and IIDs alone. Although it has been shown that, with fully developed auditory systems, we can localize sounds without head motion <ref> (Blauert 1983) </ref>, it is also known that newborn human infants orient their heads to the general direction of sounds (Muir & Field 1979). 3 In addition, other experiments have concluded that localization 2 If we model the head as a sphere of radius 8.75cm, t sec = 255 ( + sin), <p> This is especially true for sounds that produce ambiguous localization cues, since the auditory system learns to pay attention to only those cues that are in agreement and discard irrelevant or misleading cues <ref> (Blauert 1983) </ref>. As stated earlier, vision plays a major role in this learning process, for both humans and the neural network described here; everyday sounds, including speech, are mostly transient in nature, and their production often involves some sort of motion.
Reference: <author> Bose, A. </author> <year> (1994), </year> <title> `Acoustics', 6.312 lectures. </title> <publisher> MIT. </publisher>
Reference-contexts: It has also been shown that localization of unfamiliar sounds is worse than for familiar ones, so the auditory system clearly adapts to new sounds during the life of the organism <ref> (Bose 1994) </ref>. With respect to the humanoid robot, there has been a deliberate decision not to "hard-code" or store models of the auditory system. As discussed in Section 2.1.2, psychoacoustics researchers have modeled approximately the functions of ITD and (to a lesser extent) IID cues. <p> is due primarily to repeated reflections; localization becomes difficult since the localization cues of the initial direct sound are soon corrupted by reflected sounds that arrive from all directions. 6 The major cue that must be used in the reverberant field is the onset time difference of the signal envelopes <ref> (Bose 1994) </ref>. Since this cue disappears after the start of the signal, continuous tones can not be accurately localized in reverberant fields, while clicks and other transients, with sharp onset time differences, can be localized quite well.
Reference: <author> Bregman, A. S. </author> <year> (1990), </year> <title> Auditory Scene Analysis, </title> <publisher> MIT Press. </publisher>
Reference-contexts: justification for incorporating time dependence into a neural network for sound localization; it has been noted that a sound that has been already localized recently in the past is expected or predicted to remain in the same general location, helping the human auditory system in determining the present localization angle <ref> (Bregman 1990) </ref>. 4 Note that there are two separate time dependencies at work here|changes in the localization cues throughout the duration of a particular signal, and changes in the different signals that are heard. 5 The TDNN architecture can be implemented on an existing MLP by representing each synapse of each
Reference: <author> Brooks, R. & Stein, L. A. </author> <year> (1994), </year> <title> `Building Brains for Bodies', </title> <booktitle> Autonomous Robots 1:1, </booktitle> <pages> 7-25. </pages>
Reference-contexts: and software now : : : 1 3.1 Setup 3.1.1 Cog The humanoid robot, Cog, on which this project is based is an ambitious effort lead by Professors Rodney Brooks and Lynn Stein at the MIT Artificial Intelligence Laboratory to understand human cognition by embodying intelligence in a physical manifestation <ref> (Brooks & Stein 1994) </ref>. This belief, that cognition must be rooted in a physical embodiment and can not usefully be relegated to simulation, is a notion firmly believed by all members of the group, including the author. <p> L provides a multitasking lisp environment for the development of "brain models," where the nature and organization of processing will be influenced by actual biological brains. The goal is not to build a model of an actual brain, but to take inspiration from the modular structure of brains <ref> (Brooks & Stein 1994) </ref>. 13 While the camera output is color, the frame grabbers output grayscale values. 14 Matlab is a registered trademark of The Mathworks Inc. and is an easy to use mathematics software package. 32 Chapter 4 Application Use the right tool for the right job. 1 4.1 Procedure
Reference: <author> Burgess, D. A. </author> <year> (1992), </year> <title> Techniques for Low Cost Spatial Audio, </title> <booktitle> in `Fifth Annual Symposium on User Interface Software and Technology', ACM, Monterey. (UIST '92). </booktitle>
Reference-contexts: For higher frequencies, the resolution of the ears is not fine enough to distinguish the phase difference. In this case, the onset time difference of the signal envelopes at the two ears provides a form of ITD <ref> (Burgess 1992) </ref>. Using some geometry, an approximate expression for interaural time difference can be derived 2 . Figure 2-2 shows a close correspondence between the approximate model and actual ITD.
Reference: <author> Durrant, J. D. & Lovrinic, J. H. </author> <year> (1984), </year> <title> Bases of Hearing Science, second edn, </title> <editor> Williams & Wilkins, </editor> <address> Baltimore. </address>
Reference: <author> Ferrell, C., Scassellati, B. & Binnard, M. </author> <year> (1995), </year> <title> A Robot for Natural Human-Machine Interaction, </title> <booktitle> Submitted to International Joint Conference on Artificial Intelligence. </booktitle>
Reference-contexts: The robot itself, shown in Figure 3-2, is still under development. 2 This robot 1 Anonymous colleague, 1992 Undergraduate Group Engineering Design Project 2 Refer to papers by other members of the group for a more complete description of the other subsystems <ref> (Ferrell, Scassellati & Binnard 1995, Marjanovic 1995, Williamson 1995, Matsuoka 1995) </ref>. 23 is built from the waist up, and is currently bolted to an immovable stand; as we learn more about the issues involved in embodied intelligence, more robots will be built incorporating the lessons we have learned.
Reference: <author> Gamble, E. & Rainton, D. </author> <year> (1994), </year> <title> Learning to Localize Sounds Using Vision, </title> <booktitle> ATR Human Information Processing Laboratories, </booktitle> <address> Kyoto Japan. </address>
Reference-contexts: Despite these complexities, it is important to take advantage of the complementary nature of auditory and visual information to extract features and information from the surrounding environment that would be difficult or impossible from either modality alone <ref> (Gamble & Rainton 1994) </ref>. To this end, it is crucial to have an integrated system that can tightly couple different sensory modalities, like audition and vision. <p> The system makes an association between the motor commands necessary to saccade to the light (center its image in the visual field) and the left and right power spectra of the ears <ref> (Gamble & Rainton 1994) </ref>. Another related effort is the Anthropomorphic Auditory Robot developed at Waseda University, Japan.
Reference: <author> Haykin, S. </author> <year> (1994), </year> <title> Neural Networks|A Comprehensive Foundation, </title> <publisher> Macmillan College Publishing Co., </publisher> <address> New York. </address>
Reference-contexts: Section 4.3 goes into more detail about the specific multi-layer perceptron used to localize sound. There is a large body of research dealing with applications of neural networks to signal processing. Neural networks are ideally suited for signal processing and 7 <ref> (Haykin 1994) </ref> provides an excellent introduction to the entire neural networks field and is highly recommended. 8 To simplify notation, the threshold is usually considered to be just another weight with a fixed input of -1. 20 especially for audio/visual processing for several reasons: they are nonlinear compu-tational units that can <p> The neural network implemented in this project was a standard feed-forward multi-layer perceptron with one hidden layer. Neural networks with more than two hidden layers are rarely necessary, and the bulk of neural networks research deals with MLPs with only one of two hidden layers <ref> (Haykin 1994) </ref>. Standard MLPs can only learn static maps from input to output. This is not a severe limitation for the purposes of localization, as it has been determined that the cues, acting separately, have a fixed, functional form (see Section 2.1.2). <p> In other words, the same weights are applied to a series of time-delayed inputs. 5 Training a TDNN is performed by a modified temporal backpropagation learning algorithm. TDNNs have been implemented that have better performance in recognizing isolated words than traditional hidden Markov models <ref> (Haykin 1994) </ref>. <p> work here|changes in the localization cues throughout the duration of a particular signal, and changes in the different signals that are heard. 5 The TDNN architecture can be implemented on an existing MLP by representing each synapse of each neuron in the network as a finite impulse response (FIR) filter <ref> (Haykin 1994) </ref>.
Reference: <author> Kapogiannis, E. </author> <year> (1994), </year> <title> Design of a Large Scale MIMD Computer, </title> <type> EE Master's Thesis, </type> <institution> MIT, Cambridge,MA. </institution> <month> Kno </month> <year> (1973), </year> <title> BT-1759 Performance Specification. </title>
Reference-contexts: communication between nodes and sensory hardware accomplished through the use of dual-ported static RAMs, which provide independent, asynchronous access to the same memory range through two ports. fi's features|modularity, total asynchrony, no global control or shared memory, scalability|were chosen to make the entire system have some degree of biological relevance <ref> (Kapogiannis 1994) </ref>. A new backplane is currently being designed for better performance and reliability.
Reference: <author> Knudsen, E. I. & Knudsen, P. F. </author> <year> (1985), </year> <title> `Vision Guides the Adjustment of Auditory Localization in Young Barn Owls', </title> <booktitle> Science 230, </booktitle> <pages> 545-548. </pages> <note> 71 Marjanovic, </note> <author> M. </author> <year> (1995), </year> <title> Learning Maps Between Sensorimotor Systems on a Humanoid Robot, </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology. </institution>
Reference-contexts: If, however, vision was restored but subjected to a constant error using prisms, the owls would adjust their localization such that localization errors match the induced visual error. Vision therefore provides the spatial reference for "fine-tuning" auditory localization <ref> (Knudsen & Knudsen 1985) </ref>. Auditory-visual integration is important not only for localization, but other perceptual tasks.
Reference: <author> Matsuoka, Y. </author> <year> (1995), </year> <title> Embodiment and manipulation process for a humanoid hand, </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology. </institution>
Reference-contexts: The robot itself, shown in Figure 3-2, is still under development. 2 This robot 1 Anonymous colleague, 1992 Undergraduate Group Engineering Design Project 2 Refer to papers by other members of the group for a more complete description of the other subsystems <ref> (Ferrell, Scassellati & Binnard 1995, Marjanovic 1995, Williamson 1995, Matsuoka 1995) </ref>. 23 is built from the waist up, and is currently bolted to an immovable stand; as we learn more about the issues involved in embodied intelligence, more robots will be built incorporating the lessons we have learned.
Reference: <author> Mills, A. W. </author> <year> (1972), </year> <title> Auditory Localization, </title> <editor> in J. V. Tobias, ed., </editor> <booktitle> `Foundations of Modern Auditory Theory', </booktitle> <volume> Vol. II, </volume> <publisher> Academic Press, </publisher> <address> New York, </address> <pages> pp. 303-348. </pages>
Reference-contexts: The interaural intensity difference can not be modeled as easily as ITDs. The head shadow effect can cause IIDs of up to 20dB, and the effect is very frequency dependent as shown in Figure 2-3 <ref> (Mills 1972) </ref>. Head motions remove ambiguities from localization that may occur from using ITDs and IIDs alone. <p> that newborn human infants orient their heads to the general direction of sounds (Muir & Field 1979). 3 In addition, other experiments have concluded that localization 2 If we model the head as a sphere of radius 8.75cm, t sec = 255 ( + sin), where is the azimuthal angle <ref> (Mills 1972) </ref> 3 Muir et. al. admit however that it can not be concluded that newborns actually localize sound or possess a spatial map. 16 (Mills 1972) (Mills 1972) resolution in human adults is greatest in the area directly in front of the head, so it makes sense to orient the <p> localization 2 If we model the head as a sphere of radius 8.75cm, t sec = 255 ( + sin), where is the azimuthal angle <ref> (Mills 1972) </ref> 3 Muir et. al. admit however that it can not be concluded that newborns actually localize sound or possess a spatial map. 16 (Mills 1972) (Mills 1972) resolution in human adults is greatest in the area directly in front of the head, so it makes sense to orient the head towards the sound source for better localization (Mills 1972). Vision serves as a significant non-acoustic localization cue. <p> If we model the head as a sphere of radius 8.75cm, t sec = 255 ( + sin), where is the azimuthal angle <ref> (Mills 1972) </ref> 3 Muir et. al. admit however that it can not be concluded that newborns actually localize sound or possess a spatial map. 16 (Mills 1972) (Mills 1972) resolution in human adults is greatest in the area directly in front of the head, so it makes sense to orient the head towards the sound source for better localization (Mills 1972). Vision serves as a significant non-acoustic localization cue. <p> it can not be concluded that newborns actually localize sound or possess a spatial map. 16 <ref> (Mills 1972) </ref> (Mills 1972) resolution in human adults is greatest in the area directly in front of the head, so it makes sense to orient the head towards the sound source for better localization (Mills 1972). Vision serves as a significant non-acoustic localization cue. In many cases auditory cues are ignored if they conflict with visual cues. When watching television or a movie, we perceive speech to be coming from the mouths of people instead of from speakers.
Reference: <author> Morgan, D. P. & Scofield, C. L. </author> <year> (1991), </year> <title> Neural Networks and Speech Processing, </title> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: 5 The DFT is the sampled version of the continuous short-time Fourier Transform (STFT), ex pressed as X n (e j! ) = P 1 m=1 w (nm)x (m)e j!m , within each window. ! is evaluated at N points around the complex unit circle, or ! = 2n N <ref> (Morgan & Scofield 1991) </ref>. 6 The term FFT is a misnomer; it is not a transform at all, but a collection of algorithms with which one obtains the Discrete Fourier Transform. 36 at which the DFT is computed from O (N 2 ) operations to O (N logN ) operations. <p> Filterbank-Based Cues One common representation obtained from signal spectrums consists of "banks" or groupings of passband filters, called filterbanks <ref> (Morgan & Scofield 1991) </ref>. The center frequency of the filterbanks are usually spaced logarithmically, emphasizing the low frequency end of the spectrum, especially in speech processing. <p> Research in neural networks for speech processing have developed time delay neural networks (TDNN) that take into account the time varying changes of speech characteristics in a given utterance <ref> (Morgan & Scofield 1991) </ref>. The TDNN is a MLP whose hidden and output nodes are replicated across time. In other words, the same weights are applied to a series of time-delayed inputs. 5 Training a TDNN is performed by a modified temporal backpropagation learning algorithm.
Reference: <author> Muir, D. & Field, J. </author> <year> (1979), </year> <title> `Newborn Infants Orient to Sounds', </title> <booktitle> Child Development 50, </booktitle> <pages> 431-436. </pages>
Reference-contexts: Although it has been shown that, with fully developed auditory systems, we can localize sounds without head motion (Blauert 1983), it is also known that newborn human infants orient their heads to the general direction of sounds <ref> (Muir & Field 1979) </ref>. 3 In addition, other experiments have concluded that localization 2 If we model the head as a sphere of radius 8.75cm, t sec = 255 ( + sin), where is the azimuthal angle (Mills 1972) 3 Muir et. al. admit however that it can not be concluded
Reference: <author> Press, W. H., Flannery, B. P., Teukolsky, S. A. & Vetterling, W. T. </author> <year> (1988), </year> <title> Numerical Recipes in C; The Art of Scientific Computing, first edn, </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: l [n] t r [n] () L fl [k]R [k] Thus, implementing efficient cross-correlation involves transforming the two discrete time series into their DFT representations, performing element-by-element multiplication of one DFT series with the complex conjugate of the other, and then inverse transforming the product back into the time domain <ref> (Press, Flannery, Teukolsky & Vetterling 1988) </ref>. Filterbank-Based Cues One common representation obtained from signal spectrums consists of "banks" or groupings of passband filters, called filterbanks (Morgan & Scofield 1991).
Reference: <author> Rabiner, L. & Schafer, R. </author> <year> (1978), </year> <title> Digital Processing of Speech Signals, </title> <publisher> Prentice Hall. </publisher>
Reference-contexts: Q n can therefore be interpreted as a sequence of locally weighted average values of the sequence T [x (m)] <ref> (Rabiner & Schafer 1978) </ref>. <p> Onset delay is a useful cue for high frequency sounds or complex transients. P Magnitudes Another simple to compute IID cue is the difference in the sum of magnitudes of the signals in each segment. This is an approximate measure of the short-time energy of each signal <ref> (Rabiner & Schafer 1978) </ref>.
Reference: <author> Takanishi, A., Masukawa, S., Mori, Y. & Ogawa, T. </author> <year> (1993), </year> <title> Study on Anthropomorphic Auditory Robot|Continuous Localization of a Sound Source in Horizontal Plane, in `Eleventh Japan Robot Society Arts and Science Lecture Series', Japan Robot Society. (in Japanese). TI (1993), Texas Instruments TMS320C4x User's Guide. </title> <editor> von Bekesy, G. </editor> <year> (1960), </year> <title> Experiments in Hearing, </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference-contexts: One such text is (Beranek 1970). 6 It has been shown that the human auditory system suppresses these later arriving signals some what when determining directionality. 19 in both onset time and power spectra. Results were promising, although the exper-iments were performed in an anechoic room <ref> (Takanishi, Masukawa, Mori & Ogawa 1993) </ref>. 2.2 Neural Networks 2.2.1 Introduction A comprehensive introduction to neural networks is beyond the scope of this thesis. 7 While there are several different neural network architectures and methodologies, the feed-forward multi-layer perceptron (MLP) remains the most widely used and succesful architecture.
Reference: <author> Williamson, M. </author> <year> (1995), </year> <title> Series Elastic Actuators, </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology. </institution>
Reference-contexts: The robot itself, shown in Figure 3-2, is still under development. 2 This robot 1 Anonymous colleague, 1992 Undergraduate Group Engineering Design Project 2 Refer to papers by other members of the group for a more complete description of the other subsystems <ref> (Ferrell, Scassellati & Binnard 1995, Marjanovic 1995, Williamson 1995, Matsuoka 1995) </ref>. 23 is built from the waist up, and is currently bolted to an immovable stand; as we learn more about the issues involved in embodied intelligence, more robots will be built incorporating the lessons we have learned.
Reference: <author> Yuhas, B. P., Jr., M. H. G., Sejnowski, T. J. & Jenkins, R. E. </author> <year> (1990), </year> <title> `Neural Network Models of Sensory Integration for Improved Vowel Recognition', </title> <booktitle> Proceedings of the IEEE 78(10), </booktitle> <pages> 1658-1668. 72 </pages>
Reference-contexts: Auditory-visual integration is important not only for localization, but other perceptual tasks. Speech perception also benefits from visual input; isolated word recognition in a noisy environment improved significantly when normal hearing subjects were able to see the speakers as well as hear the speech <ref> (Yuhas, Jr., Sejnowski & Jenkins 1990) </ref>. This is not surprising, since even those who have impaired hearing can learn to "lip read" and thus perceive speech mostly or solely from vision. <p> chambers and test "clicks." Applying standard neural networks architectures in novel ways will hopefully allow "simple" tasks such as localization to perform well in realistic, and therefore complex, listening environments. 2.2.2 Related Work There have been recent attempts to specifically apply neural networks in the integration of vision and audition. <ref> (Yuhas et al. 1990) </ref> explore the use of neural networks to improve speech perception, specifically the recognition of isolated vowels. One MLP was trained to estimate the spectral characteristics of the corresponding acoustic signals from visual images of the speaker's mouth. <p> In other words, what form should the inputs and outputs of the neural network take? Research in both image and sound processing have explored both extremes, from taking raw data after minimal subsampling, to pre-processing the raw data and abstracting most of the signal characteristics to very high-level representations <ref> (Yuhas et al. 1990) </ref>. An intermediate approach was taken for this project, and some amount of preprocessing on the raw sound data was performed resulting in rough localization cues from the binaural differences of various signal properties.
References-found: 23

