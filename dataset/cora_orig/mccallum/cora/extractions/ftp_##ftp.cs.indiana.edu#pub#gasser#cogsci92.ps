URL: ftp://ftp.cs.indiana.edu/pub/gasser/cogsci92.ps
Refering-URL: http://www.cs.indiana.edu/ai/Gasser/Morphophon/home.html
Root-URL: http://www.cs.indiana.edu
Email: gasser@cs.indiana.edu  
Title: Learning Distributed Representations for Syllables three reasons to prefer distributed over symbolic representations for structured
Author: Michael Gasser 
Keyword: Linguistic Structure and Distributed Representation  
Date: (1992), 396-401  
Note: What good are distributed representations? They certainly are harder to interpret directly, at least by external users of the system that creates them. And In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society  There are  
Address: Bloomington, IN 47405  
Affiliation: Departments of Computer Science and Linguistics Indiana University  
Abstract: This paper presents a connectionist model of how representations for syllables might be learned from sequences of phones. A simple recurrent network is trained to distinguish a set of words in an artificial language, which are presented to it as sequences of phonetic feature vectors. The distributed syllable representations that are learned as a side-effect of this task are used as input to other networks. It is shown that these representations encode syllable structure in a way which permits the regeneration of the phone sequences (for production) as well as systematic phonological operations on the representations. If the language sciences agree on one thing, it is the hierarchical nature of language. The importance of hierarchical, structured representations is now generally recognized for the phonological pole, where syllables and metrical units now play a major role (see, e.g., Fra-zier (1987) and Goldsmith (1990)), as well as for the syntactic/semantic pole of language and language processing. The major reason for believing in structured representations is the significance of structure-sensitive operations in language processing. A semantic inference rule may need to know where the subject of a clause is; a morphological reduplication rule may need to know where the coda (final consonant(s)) of a syllable is. Traditional symbolic representations are based crucially on the simple notion of concatenation (van Gelder, 1990). A syllable representation, for example, is a (bracketed) string of concatenated phones. Recent connectionist work offers as an alternative to this widely accepted approach distributed representations, for which it is generally impossible to isolate which elements of the representation denote which of the lower-level units comprising the structure being represented. at first blush it seems cumbersome, if not impossible, to implement structure-sensitive operations on them, operations which present no particular difficulty for symbolic representations (Fodor & Pylyshyn, 1988). Clearly distributed representations would be useless for most purposes if they were not amenable to such operations. Recently, however, it has been shown that it is possible to arrive at a set of connection weights which implements structure-sensitive operations on distributed representations. Where the representations arise on hidden layers through training, the operations on them are also implemented through training (Chalmers, 1990). Where the representations arise as a result of the application of a set of primitive operations analogous to the filling of roles in symbolic models, the operations on them can be implemented more directly (Legendre, Miyata, & Smolensky, 1991). 
Abstract-found: 1
Intro-found: 1
Reference: <author> Chalmers, D. </author> <year> (1990). </year> <title> Syntactic transformations on distributed representations. </title> <journal> Connection Science, </journal> <volume> 2, </volume> <pages> 53-62. </pages>
Reference: <author> Corina, D. P. </author> <year> (1991). </year> <title> Towards an Understanding of the Syllable: Evidence from Linguistic, Psychological, and Connectionist Investigations of Syllable Structure. </title> <type> Ph.D. thesis, </type> <institution> University of California, </institution> <address> San Diego. </address>
Reference: <author> Doutriaux, A. & Zipser, D. </author> <year> (1990). </year> <title> Unsupervised discovery of speech segments using recurrent networks. </title> <editor> In Touretzky, D., Elman, J., Sejnowski, T., & Hinton, G. (Eds.), </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School, </booktitle> <pages> pp. 303-309. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Dresher, B. E. & Kaye, J. D. </author> <year> (1990). </year> <title> A computational learning model for metrical phonology. </title> <journal> Cognition, </journal> <volume> 34, </volume> <pages> 137-195. </pages>
Reference-contexts: The hearer/learner may be learning phonology for its own sake, that is, either simply looking for regularity in the input, or looking for evidence that would allow the setting of some innate parameters <ref> (Dresher & Kaye, 1990) </ref>. 2. The hearer/learner may be attempting to map perceptual features onto representations of articulatory gestures, as in various versions of the motor theory of perception (Liberman & Mattingly, 1986). 3. The hearer/learner may learn prosodic representa tions as a side-effect of word recognition.
Reference: <author> Elman, J. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14, </volume> <pages> 179-211. </pages>
Reference: <author> Fodor, J. & Pylyshyn, Z. </author> <year> (1988). </year> <title> Connectionism and cognitive architecture: a critical analysis. </title> <journal> Cognition, </journal> <volume> 28, </volume> <pages> 3-71. </pages>
Reference: <author> Frazier, L. </author> <year> (1987). </year> <title> Structure in auditory word recognition. </title> <journal> Cognition, </journal> <volume> 25, </volume> <pages> 157-187. </pages>
Reference: <author> Gasser, M. </author> <year> (1991). </year> <title> Sequence comparison and simple recurrent networks. </title> <note> Center for Research in Language Newsletter. </note>
Reference: <author> Goldsmith, J. & Larson, G. </author> <year> (1990). </year> <title> Local modeling and syllabification. </title> <editor> In Deaton, K., Noske, M., & Ziolkowski, M. (Eds.), </editor> <booktitle> Papers from the 26th Annual Regional Meeting of the Chicago Linguistics Society: Parasession on the Syllable in Phonetics and Phonology. </booktitle> <publisher> Chicago Linguistics Society. </publisher>
Reference: <author> Goldsmith, J. </author> <year> (1990). </year> <title> Autosegmental and Metrical Phonology. </title> <publisher> Basil Blackwell, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Gupta, P. & Touretzky, D. S. </author> <year> (1991). </year> <title> Connectionist networks and linguistic theory: investigations of stress systems in language.. </title> <type> Unpublished report, </type> <institution> Carnegie-Mellon University. </institution> <note> 5 Jordan, </note> <author> M. </author> <year> (1986). </year> <title> Attractor dynamics and parallelism in a connectionist sequential machine. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. </pages> <address> 531-546 Hillsdale, New Jersey. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Legendre, G., Miyata, Y., & Smolensky, P. </author> <year> (1991). </year> <title> Distributed recursive structure processing. </title> <editor> In Lipp-mann, R. P., Moody, J. E., & Touretzky, D. S. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pp. 591-597. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Complex transformations can be performed on distributed representations in a single parallel step, rather than through a series of symbolic conses, cars, and cdrs <ref> (Legendre et al., 1991) </ref>. 3. There are relatively simple algorithms for learning the structure in distributed representations (Elman, 1990; Pollack, 1990). Most work concerned with distributed representations for structured objects has examined syntax or semantics.
Reference: <author> Liberman, A. M. & Mattingly, I. G. </author> <year> (1986). </year> <title> The motor theory of speech revised. </title> <journal> Cognition, </journal> <volume> 21, </volume> <pages> 1-36. </pages>
Reference-contexts: The hearer/learner may be attempting to map perceptual features onto representations of articulatory gestures, as in various versions of the motor theory of perception <ref> (Liberman & Mattingly, 1986) </ref>. 3. The hearer/learner may learn prosodic representa tions as a side-effect of word recognition. It is the third possibility that is pursued here. The idea that phonology emerges as the child learns to recognize and produce words is an appealing idea, and an old one.
Reference: <author> Plunkett, K. & Marchman, V. </author> <year> (1991). </year> <title> U-shaped learning and frequency effects in a multi-layered perceptron: implications for child language acquisition. </title> <journal> Cognition, </journal> <volume> 38, </volume> <pages> 1-60. </pages>
Reference: <author> Pollack, J. B. </author> <year> (1990). </year> <title> Recursive distributed representations. </title> <journal> Artificial Intelligence, </journal> <volume> 46, </volume> <pages> 77-105. </pages>
Reference-contexts: 1. Distributed representations do not necessarily increase in size as the complexity of the represented object increases. In the case of some types of representations, for example, those described in this paper, representations for objects of the same type are of fixed width <ref> (Pollack, 1990) </ref>.
Reference: <author> Port, R. </author> <year> (1990). </year> <title> Representation and recognition of temporal patterns. </title> <journal> Connection Science, </journal> <volume> 2, </volume> <pages> 151-176. </pages>
Reference-contexts: One approach to short-term memory is to give a system access to a buffer of some fixed width. This has several drawbacks, in particular the problem of how the system is to know beforehand how wide the buffer should be <ref> (Port, 1990) </ref>. An alternative is an approach that permits a system to develop its own short-term memory. This is possible in connectionist networks with recurrent connections (Elman, 1990; Jordan, 1986; Port, 1990). It is this method that is utilized in the study described here.
Reference: <author> Rumelhart, D. E., Hinton, G., & Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. & McClelland, J. L. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Volume 1, </volume> <pages> pp. 318-364. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address> <publisher> van Gelder, </publisher> <editor> T. </editor> <year> (1990). </year> <title> Compositionality: a connectionist variation on a classical theme. </title> <journal> Cognitive Science, </journal> <volume> 14, </volume> <pages> 355-384. 6 </pages>
Reference-contexts: They consist of feedforward networks supplemented with recurrent connections from the hidden and/or output layers and are trained using the familiar back-propagation learning algorithm <ref> (Rumelhart, Hinton, & Williams, 1986) </ref>. for the recognition and production tasks in the experiments described below. Earlier experiments indicated the superiority of these particular architectures over other variants of simple recurrent networks for these tasks.
References-found: 17

