URL: http://www.mtl.t.u-tokyo.ac.jp/~mohri/e/aaai94-cbr.ps.gz
Refering-URL: http://www.mtl.t.u-tokyo.ac.jp/~mohri/index-e.html
Root-URL: 
Email: fmohri,tanakag@MTL.T.u-tokyo.ac.jp  
Title: An Optimal Weighting Criterion of Case Indexing for Both Numeric and Symbolic Attributes  
Author: Takao Mohri and Hidehiko Tanaka 
Address: 7-3-1 Hongo Bunkyo-ku, Tokyo 113, Japan  
Affiliation: Information Engineering Course, Faculty of Engineering The University of Tokyo  
Abstract: Indexing of cases is an important topic for Memory-Based Reasoning(MBR). One key problem is how to assign weights to attributes of cases. Although several weighting methods have been proposed, some methods cannot handle numeric attributes directly, so it is necessary to discretize numeric values by classification. Furthermore, existing methods have no theoretical background, so little can be said about optimality. We propose a new weighting method based on a statistical technique called Quantification Method II. It can handle both numeric and symbolic attributes in the same framework. Generated attribute weights are optimal in the sense that they maximize the ratio of variance between classes to variance of all cases. Experiments on several benchmark tests show that in many cases, our method obtains higher accuracies than some other weighting methods. The results also indicate that it can distinguish relevant attributes from irrelevant ones, and can tolerate noisy data. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> 1989. </year> <title> Incremental, instance-based learning of independent and graded concept descriptions. </title> <booktitle> In Proceedings of the Sixth International Machine Learning Workshop(ML89), </booktitle> <pages> 387-391. </pages>
Reference: <author> Aha, D. W. </author> <year> 1992. </year> <title> Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms. </title> <journal> Int. J. Man-Machine Studies 36 </journal> <pages> 267-287. </pages>
Reference: <author> Cost, S., and Salzberg, S. </author> <year> 1993. </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <booktitle> Machine Learning 10 </booktitle> <pages> 57-78. </pages>
Reference-contexts: In VDM, the distance between values is calculated for every pair of values for every attribute. Each attribute is weighted by w (a; u a ). Recently, Cost and Salzberg proposed MVDM (Modified VDM) <ref> (Cost & Salzberg 1993) </ref>. MVDM omits the attribute-weighting (the term w (a; u a )) of VDM, and introduces weighting of cases. MVDM performs well for some tests including prediction of protein secondary structure and pronunciation of English text.
Reference: <author> Creecy, R. H.; Masand, B. M.; Smith, S. J.; and Waltz, D. L. </author> <year> 1992. </year> <title> Trading MIPS and memory for knowledge engineering. </title> <journal> Communications of the ACM 35(8) </journal> <pages> 48-63. </pages>
Reference-contexts: Typical Weighting Methods for Attributes Per-/Cross-Category Feature Importance Before explaining our method, let us review some typical attribute-weighting methods. Per-category feature importance and cross-category feature importance (in short, PCF/CCF) were proposed in <ref> (Creecy et al. 1992) </ref>. Both weighting methods are based on conditional probabilities. In the case of PCF/CCF, a symbolic attribute with N values is converted to a set of N binary attributes.
Reference: <author> Hayashi, C.; Suzuki, T.; and Sasaki, M., eds. </author> <year> 1992. </year> <title> Data Analysis for Comparative Social Research. </title> <publisher> North-Holland. </publisher> <pages> chapter 15, 423-458. </pages>
Reference-contexts: an index of such groups, i is an index of a case in a group, and n c is the size of each group. w a is a coefficient for the a-th attribute. countries, it may be better known as "correspondence analysis" (J.P.Benzecri et al. 1973) by J.P.Benzecri of France <ref> (Hayashi, Suzuki, & Sasaki 1992) </ref> Table 2: A Simple Example (After Quantification) class attr1 attr2 attr3 u 1 u 2 u 3 u 4 u 5 u 6 1 0 1 0 1 0 12.5 0 0 1 0 1 0 9.4 For each case, a criterion variable y c i
Reference: <author> J.P.Benzecri et al. </author> <year> 1973. </year> <note> L'analyse des donnees 1,2(paris: Dunod)(in French). </note>
Reference-contexts: Suppose c is an index of such groups, i is an index of a case in a group, and n c is the size of each group. w a is a coefficient for the a-th attribute. countries, it may be better known as "correspondence analysis" <ref> (J.P.Benzecri et al. 1973) </ref> by J.P.Benzecri of France (Hayashi, Suzuki, & Sasaki 1992) Table 2: A Simple Example (After Quantification) class attr1 attr2 attr3 u 1 u 2 u 3 u 4 u 5 u 6 1 0 1 0 1 0 12.5 0 0 1 0 1 0 9.4 For
Reference: <author> Kawaguchi, M. </author> <year> 1978. </year> <title> Introduction to Multivariate Analysis II (in Japanese). </title> <publisher> Morikita-Shuppan. </publisher>
Reference-contexts: This problem results in an eigen value problem of a N a fi N a square matrix calculated by u a of all instances. The elements of the eigen vector become w a . For detailed calcula tion, please refer to <ref> (Kawaguchi 1978) </ref>. New Classification Methods based on Quantification Method II We introduce three methods based on Quantification Method II. The first two (QM2m, QM2y) are instance based methods, and the rest is the original QM2 itself. * QM2m (QM2 + matching) Each attribute has its weight calculated by QM2.
Reference: <author> Mohri, T.; Nakamura, M.; and Tanaka, H. </author> <year> 1993. </year> <title> Weather forecasting using memory-based reasoning. </title> <booktitle> In Second International Workshop on Parallel Processing for Artificial Intelligence (PPAI-93), </booktitle> <pages> 40-45. </pages>
Reference-contexts: While per-category feature importance addresses the classes of training data, cross-category neglects them, and uses an average (in fact, summation of square) as weight. We have applied these per-/cross-category weighting methods for a weather prediction task using memory-based reasoning <ref> (Mohri, Nakamura, & Tanaka 1993) </ref>. In that paper, we show per-category importance is too sensitive to the proportion of classes, and has a tendency to answer the most frequent class too often.
Reference: <author> Murphy, P. M., and Aha, D. W. </author> <year> 1994. </year> <title> UCI repository of machine learning databases. </title> <address> Irvine, CA: </address> <institution> University of California, ftp://ics.uci.edu/pub/machine-learning-databases. </institution>
Reference-contexts: Experiments The experimental results for several benchmark data are shown in Table 3. Four data sets (vote, soybean, crx, hypo) were in the distribution floppy disk of Quin-lan's C4.5 book (Quinlan 1993). The remaining four data sets (iris, hepatitis, led, led-noise) were obtained from the Irvine Machine Learning Database <ref> (Murphy & Aha 1994) </ref>. Including our 3 methods,VDM, PCF, CCF, IB4, and C4.5 are compared. Quinlan's C4.5 is a sophisticated decision tree generation algorithm, and used by default parameters. The accuracies by pruned decision trees are used in the experimental results.
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Therefore, they have a theoretical basis and clear meaning. Experiments The experimental results for several benchmark data are shown in Table 3. Four data sets (vote, soybean, crx, hypo) were in the distribution floppy disk of Quin-lan's C4.5 book <ref> (Quinlan 1993) </ref>. The remaining four data sets (iris, hepatitis, led, led-noise) were obtained from the Irvine Machine Learning Database (Murphy & Aha 1994). Including our 3 methods,VDM, PCF, CCF, IB4, and C4.5 are compared. Quinlan's C4.5 is a sophisticated decision tree generation algorithm, and used by default parameters.
Reference: <author> Stanfill, C., and Waltz, D. </author> <year> 1986. </year> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM 29(12) </journal> <pages> 1213-1228. </pages>
Reference-contexts: Introduction Indexing of cases is an important topic for both Case-Based Reasoning (CBR) and Memory-Based Reasoning (MBR) <ref> (Stanfill & Waltz 1986) </ref>. Indexing is especially important in MBR because of the lack of case adaptation phase, and the nearest instances' classes are directly mapped to a new instance. Usually, similarity is calculated by summing up weights of matched attributes.
Reference: <author> Weiss, S. M., and Kulikowski, C. A. </author> <year> 1991. </year> <title> Computer Systems That Learn. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Including our 3 methods,VDM, PCF, CCF, IB4, and C4.5 are compared. Quinlan's C4.5 is a sophisticated decision tree generation algorithm, and used by default parameters. The accuracies by pruned decision trees are used in the experimental results. All accuracies are calculated by 10-fold cross-validation <ref> (Weiss & Kulikowski 1991) </ref>. The specifications of these benchmark data are shown in Table 4. led has 7 relevant Boolean attributes, and 17 irrelevant (randomly assigned) Boolean attributes.
References-found: 12

