URL: ftp://ftp.cag.lcs.mit.edu/pub/raw/documents/maps98.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/~barua/papers/index.html
Root-URL: 
Email: fbarua,waltg@lcs.mit.edu fsaman,agarwalg@lcs.mit.edu  
Title: Maps: A Compiler-Managed Memory System for Raw Machines  
Author: Rajeev Barua, Walter Lee, Saman Amarasinghe, Anant Agarwal 
Web: http://cag-www.lcs.mit.edu/raw  
Address: Cambridge, MA 02139, U.S.A.  
Affiliation: M.I.T. Laboratory for Computer Science  
Abstract: Microprocessors of the next decade and beyond will be built using VLSI chips employing billions of transistors. In this generation of microprocessors, achieving a high level of parallelism at a reasonable clock speed will require full distribution of machine resources. Raw architectures explore this architectural space by distributing all their processor and memory resources as a 2-D mesh of simple tiles. To provide a simple sequential programming model, a Raw architecture exposes the hardware fully and relies on the compiler or the software run-time system to achieve efficient execution while maintaining the semantics of a single instruction stream and a unified memory system. Supporting a single view of memory on top of a distributed memory architecture presents a challenging compiler problem. This paper presents a compiler system called Maps that supports a unified view of memory on a Raw architecture. Maps relies on two inter-tile interconnects: a fast, statically schedulable network and a slower dynamic network. Maps attempts to schedule the memory accesses for maximum parallelism and speed while enforcing proper dependence. It optimizes for speed in two ways: by finding accesses that can be scheduled on the static interconnect through a process called static promotion, and by minimizing dependence sequentialization for the remaining accesses. Static accesses are discovered through applications of traditional pointer and array analysis, and a new technique called modulo unrolling. Maps enforces proper dependence through a combination of explicit synchronization and a technique called software serial ordering. We have implemented Maps based on the SUIF infrastructure. This paper presents preliminary results based on compiling several programs using Maps.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Amarasinghe, J. Anderson, C. Wilson, S. Liao, B. Murphy, R. French, and M. Lam. </author> <title> Multiprocessors from a Software Perspective. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 5261, </pages> <month> June </month> <year> 1996. </year> <month> 14 </month>
Reference-contexts: In terms of execution cost, the mechanism is heavyweight, which limits the type of programs it can profitably execute to those with access patterns that require few synchronizations <ref> [1] </ref> [3]. The approach we present solves the memory coherence problem with much less hardware, and enables arbitrary sequential programs to be parallelized on distributed, scalable machines. This paper presents Maps, a compiler-managed memory system for Raw architectures.
Reference: [2] <author> R. Barua, W. Lee, S. Amarasinghe, and A. Agarwal. </author> <title> Memory Bank Disambiguation using Modulo Un--rolling for Raw Machines. </title> <booktitle> In Proceedings of the ACM/IEEE Fifth Int'l Conference on High Performance Computing(HIPC), </booktitle> <month> Dec </month> <year> 1998. </year>
Reference-contexts: Therefore, all resulting accesses can be statically promoted. This technique is always applicable for loops with array accesses having indices which are affine functions of enclosing loop induction variables. For a detailed explanation and the derivation of the unrolling factor, see <ref> [2] </ref>. 5.3 The need for dynamic references A compiler can statically promote all accesses through equivalence-class unification alone, and modulo unrolling helps improve the distribution of data during promotion. There are several reasons, however, why it is undesirable to promote all references. <p> Relative memory disambiguation [7] aims to discover whether two memory accesses never refer to the same memory location. Successful disambiguation implies that accesses can be executed in parallel. Hence, relative memory disambiguation is more closely linked to dependence and pointer analysis techniques. The modulo unrolling scheme used in Raw <ref> [2] </ref> is related to an observation made by Ellis [5]. He observes that unrolling can sometimes help disambiguate accesses, but he does not attempt to formalize the 13 observation into a theory or algorithm.
Reference: [3] <author> R. Cytron. </author> <title> Doacross: Beyond vectorization for multiprocessors. </title> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: In terms of execution cost, the mechanism is heavyweight, which limits the type of programs it can profitably execute to those with access patterns that require few synchronizations [1] <ref> [3] </ref>. The approach we present solves the memory coherence problem with much less hardware, and enables arbitrary sequential programs to be parallelized on distributed, scalable machines. This paper presents Maps, a compiler-managed memory system for Raw architectures.
Reference: [4] <author> S. Dwarkadas, A. L. Cox, and W. Zwaenepoel. </author> <title> An integrated compile-time/run-time software distributed shared memory system. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 186197, </pages> <address> Cambridge, Massachusetts, </address> <month> October 15, </month> <year> 1996. </year>
Reference-contexts: Section 5.2 will discuss a novel transformation technique for increasing equivalence classes for array accesses. illustration of the Maps compiler-managed memory system. Figure 4 (a) shows the initial code fragment, which contains five memory accesses A <ref> [4] </ref>, flp, B [x], C [y] and flq. Figure 4 (b) shows the information derived after pointer and array analysis, which includes location set lists, equivalence classes, and data dependence. <p> More importantly, it is important for such mechanisms to integrate well with the static mechanism. The next section explains how these goals are accommodated. Continuing our running example, Figure 4 (c) shows the result of static promotion. Only the A <ref> [4] </ref> reference is promoted, as it is a simple affine function. <p> Additional dependences are placed to serialize the requests assigned on the same turnstile as required by software serial ordering. on six processors P1 through P6. The dynamic requests are serialized on turnstiles 1 and 2, assigned to processors P3 and P5. The A <ref> [4] </ref> static access is placed on the processor it was promoted to, namely P4. All other computation is partitioned a manner which exploits parallelism while respecting dependences. The shaded nodes represent loads of input variables at their latest locations. <p> Due to space limitations, we do not present related work on the architectural aspects of Raw. For a detailed comparison to other architectures, see [12]. Software distributed shared memory schemes on multiprocessors (DSMs) [10] <ref> [4] </ref> are similar in spirit to Map's software approach of managing memory. They emulate in software the task of cache coherence, one which is traditionally performed by complex hardware. In contrast, Maps turns sequential accesses from a single memory image into decentralized accesses across Raw tiles.
Reference: [5] <author> J. R. Ellis. Bulldog: </author> <title> A Compiler for VLIW Architectures. </title> <type> In Ph.D Thesis, </type> <institution> Yale University, </institution> <year> 1985. </year>
Reference-contexts: In contrast, Maps turns sequential accesses from a single memory image into decentralized accesses across Raw tiles. This technique enables the parallelization of sequential programs on a distributed machine. Static promotion is related to static memory bank disambiguation, a term used by Ellis <ref> [5] </ref> for a point-to-point VLIW model. For such VLIWs, he shows that successful disambiguation allows an access to be executed through a fast front door to a memory bank, while an non-disambiguated access is sent over a slower back door. <p> Successful disambiguation implies that accesses can be executed in parallel. Hence, relative memory disambiguation is more closely linked to dependence and pointer analysis techniques. The modulo unrolling scheme used in Raw [2] is related to an observation made by Ellis <ref> [5] </ref>. He observes that unrolling can sometimes help disambiguate accesses, but he does not attempt to formalize the 13 observation into a theory or algorithm. Instead, his technique relies on user-identified array accesses and user annotations to provide the unroll factors needed.
Reference: [6] <author> W. Lee, R. Barua, D. Srikrishna, J. Babb, V. Sarkar, S. Amarasinghe, and A. Agarwal. </author> <title> Space-Time Scheduling of Instruction-Level Parallelism on a Raw Machine. </title> <booktitle> In Proceedings of the Eighth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> San Jose, California, </address> <month> October </month> <year> 1998. </year>
Reference-contexts: Using a distributed processor architecture to run general-purpose sequential programs presents several interesting research issues. The scheduling of instruction-level parallelism, which includes both a spatial and a temporal component, has been addressed in <ref> [6] </ref>. From the memory's perspective, the distributed organization presents both an opportunity and a challenge. The opportunity is to be able to use compiler knowledge to optimize the references for locality, parallelism, and efficiency. <p> The compiler consists of two main phases, Maps and the space-time scheduler. Maps uses the information provided by traditional pointer and array analysis to perform static promotion and software serial ordering. The space-time scheduler <ref> [6] </ref> parallelizes each basic block of the program across the processors, obeying dependence and serialization requirements specified by Maps. 4 Analysis techniques Throughout the memory system, the Raw compiler employs traditional analysis techniques to enhance the effectiveness of its mechanisms. The techniques include pointer analysis and array analysis.
Reference: [7] <author> G. Lowney et al. </author> <title> The Multiflow Trace Scheduling Compiler. </title> <booktitle> In Journal of Supercomputing, </booktitle> <pages> pages 51142, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: The lack of point-to-point VLIWs seems to explain the dearth of work on memory bank disambiguation for compiling for VLIWs. A different type of memory disambiguation is relevant on the more typical bus-based VLIW machines such as the Multiflow Trace <ref> [7] </ref>. Relative memory disambiguation [7] aims to discover whether two memory accesses never refer to the same memory location. Successful disambiguation implies that accesses can be executed in parallel. Hence, relative memory disambiguation is more closely linked to dependence and pointer analysis techniques. <p> The lack of point-to-point VLIWs seems to explain the dearth of work on memory bank disambiguation for compiling for VLIWs. A different type of memory disambiguation is relevant on the more typical bus-based VLIW machines such as the Multiflow Trace <ref> [7] </ref>. Relative memory disambiguation [7] aims to discover whether two memory accesses never refer to the same memory location. Successful disambiguation implies that accesses can be executed in parallel. Hence, relative memory disambiguation is more closely linked to dependence and pointer analysis techniques.
Reference: [8] <author> D. Matzke. </author> <title> Will physical scalability sabotage performance gains? Computer, </title> <booktitle> pages 3739, </booktitle> <month> Sept. </month> <year> 1997. </year>
Reference-contexts: Within the chip boundary, however, shrinking technology makes the speed of basic logic faster and faster, while improvement in fl This research is funded in part by ARPA contract # DABT63-96-C-0036 and in part by an NSF Presidential Young Investigator Award. MIT-LCS-TM-583, July 1998 1 wire speed lags behind <ref> [8] </ref>. These forces impose increasing relative penalties on complex logic and global wires. Consequently, locality and simplicity become the key principles of good hardware design. Locality suggests that processing resources be fully distributed with near neighbor connectivity and the elimination of global buses.
Reference: [9] <author> R. Rugina and M. Rinard. </author> <title> Span: A shape and pointer analysis package. </title> <type> Technical report, </type> <institution> M.I.T. LCS-TM-581, </institution> <month> June </month> <year> 1998. </year>
Reference-contexts: The techniques include pointer analysis and array analysis. This section briefly presents the information provided by these techniques. Pointer analysis is leveraged for three purposes: equivalence class unification, software serial ordering, and the minimization of dependence edges. The Raw compiler uses the SPAN, a state-of-the-art pointer analysis package <ref> [9] </ref>. It contains a notion of abstract data objects, and it associates a location set number to each data object. As output, the analysis phase marks each pointer reference with the list of location sets it can refer to, termed its location set list.
Reference: [10] <author> D. J. Scales, K. Gharachorloo, and C. A. Thekkath. </author> <title> Shasta: A low overhead, software-only approach for supporting fine-grain shared memory. </title> <booktitle> In Proceedings of the Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 174185, </pages> <address> Cambridge, Massachusetts, </address> <month> October 15, </month> <year> 1996. </year>
Reference-contexts: Due to space limitations, we do not present related work on the architectural aspects of Raw. For a detailed comparison to other architectures, see [12]. Software distributed shared memory schemes on multiprocessors (DSMs) <ref> [10] </ref> [4] are similar in spirit to Map's software approach of managing memory. They emulate in software the task of cache coherence, one which is traditionally performed by complex hardware. In contrast, Maps turns sequential accesses from a single memory image into decentralized accesses across Raw tiles.
Reference: [11] <author> M. D. Smith. </author> <title> Extending suif for machine-dependent optimizations. </title> <booktitle> In Proceedings of the First SUIF Compiler Workshop, </booktitle> <pages> pages 1425, </pages> <address> Stanford, CA, </address> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Preliminary application performance The RAWCC compiler has been evaluated on a set of regular benchmarks shown in Table 3. We compare the results of the Raw compiler with the results of a MIPS compiler provided by Machsuif <ref> [11] </ref> targeted for an R2000. Table 4 shows the speedups attained by the benchmarks for Raw machines of various sizes. For all these benchmarks, Maps is able to statically promote 100% of the accesses.
Reference: [12] <author> E. Waingold, M. Taylor, V. Sarkar, W. Lee, V. Lee, J. Kim, M. Frank, P. Finch, S. Devabhaktuni, R. Barua, J. Babb, S. Amarasinghe, and A. Agarwal. </author> <title> Baring It All to Software: The RAW Machine. </title> <booktitle> IEEE Computer, </booktitle> <month> September </month> <year> 1997. </year> <note> Also as MIT-LCS-TR-709. </note>
Reference-contexts: Accordingly, the architecture must be able to run existing sequential programs without requiring new programming methodologies. To support sequential programs, traditional processors use complicated hardware to provide features like memory reorder buffers and mechanisms to extract instruction-level parallelism, which violates the simplicity principle of hardware design. The Raw alternative <ref> [12] </ref> is to expose fully the raw hardware to the compiler so it can implement the support functions for sequential programs, thereby maintaining hardware simplicity. <p> Section 4 describes the traditional analysis techniques leveraged by Maps. Section 5 describes techniques for static promotion. Section 6 describes software serial ordering. Section 7 presents the results. Section 8 discusses the related work, and Section 9 concludes. 2 Raw architecture and memory mechanisms The Raw architecture <ref> [12] </ref> is a simple, distributed, software-exposed architecture motivated by the desire to maximize the performance per silicon area of a machine. Together, distribution and simplicity enable a fast clock, and they maximize the amount of processing resources that can fit on a chip. <p> Due to space limitations, we do not present related work on the architectural aspects of Raw. For a detailed comparison to other architectures, see <ref> [12] </ref>. Software distributed shared memory schemes on multiprocessors (DSMs) [10] [4] are similar in spirit to Map's software approach of managing memory. They emulate in software the task of cache coherence, one which is traditionally performed by complex hardware.
Reference: [13] <author> R. Wilson et al. </author> <title> SUIF: A Parallelizing and Optimizing Research Compiler. </title> <journal> SIGPLAN Notices, </journal> <volume> 29(12):3137, </volume> <month> December </month> <year> 1994. </year> <month> 15 </month>
References-found: 13

