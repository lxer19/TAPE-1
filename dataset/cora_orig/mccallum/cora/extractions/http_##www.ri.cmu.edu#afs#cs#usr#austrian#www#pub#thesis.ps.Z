URL: http://www.ri.cmu.edu/afs/cs/usr/austrian/www/pub/thesis.ps.Z
Refering-URL: http://www.ri.cmu.edu/afs/cs/usr/austrian/www/phd.html
Root-URL: 
Title: Learning and Validation of Human Control Strategies  
Author: Michael C. Nechyba CMU-RI-TR-- 
Degree: Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy  
Note: 1998 Michael C. Nechyba This research was funded in part through a National Science Foundation Graduate Research Fellowship, as well as a Department of Energy Doctoral Research Fellowship.  
Address: Pittsburgh, Pennsylvania 15213  
Affiliation: The Robotics Institute Carnegie Mellon University  
Date: May 1998  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. Albus, </author> <title> A New Approach to Manipulator Control: The Cerebellar Model Articulation Controller (Cmac), </title> <journal> Trans. ASME Journal of Dynamic Systems, Measurement, and Control, </journal> <volume> vol. 97, </volume> <pages> pp. 220-7, </pages> <year> 1975. </year>
Reference-contexts: Atkeson, et. al. [9] offer an excellent overview of locally weighted learning, while [10] addresses control-specific issues. Locally weighted regression is one instance of locally weighted learning and is similar in approach to CMAC <ref> [1] </ref> and RBF [75] neural networks in that local (linear) models are fit to nearby data. All the data is explicitly stored and organized in efficient data structures (such as k-d trees [17] or Bump trees [84], for example). <p> 2 k k ( ) k t 1= K 1 2 k k k t 1= K -= = 2 c t 180 Appendix B: 181 Appendix C: Authors Publications The following is a complete list of journal and refereed conference publications derived from this work (in reverse chronological order): <ref> [1] </ref> M. C. Nechyba and Y. Xu, Stochastic Similarity for Validating Human Control Strategy Models, to appear in IEEE Trans. on Robotics and Automation, June, 1998. [2] M. C. Nechyba and Y. Xu, On Discontinuous Human Control Strategies, to appear in Proc. IEEE Int.
Reference: [2] <author> B. D. O. Anderson and J. B. Moore, </author> <title> Optimal Filtering, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, </address> <year> 1979. </year>
Reference-contexts: A priori assumptions about the underlying functional form of the mapping we wish to learn are thus minimized. 3.1 Motivation 27 To address the second problem slow convergence with gradient-descent training algorithms we look towards extended Kalman filtering (EKF) <ref> [2] </ref>, previously confined to the area of optimal filtering. What makes EKF algorithms attractive is that, unlike backpropagation, they explicitly account for the pairwise interdependence of the weights in the neural network during training. <p> C. Nechyba and Y. Xu, Stochastic Similarity for Validating Human Control Strategy Models, to appear in IEEE Trans. on Robotics and Automation, June, 1998. <ref> [2] </ref> M. C. Nechyba and Y. Xu, On Discontinuous Human Control Strategies, to appear in Proc. IEEE Int. Conference on Robotics and Automation, May, 1998. [3] J. Song, Y. Xu, M. C. Nechyba and Y. Yam, Two Performance Measures for Evaluating Human Control Strategy, to appear in Proc. IEEE Int.
Reference: [3] <author> P. J. Antsaklis, </author> <title> guest ed., </title> <journal> Special Issue on Neural Networks in Control Systems, IEEE Control System Magazine, </journal> <volume> vol. 10, no. 3, </volume> <pages> pp. 3-87, </pages> <year> 1990. </year>
Reference-contexts: Hov-land, et. al. [50] encode human assembly skill with Hidden Markov Models. In [85], neural networks encode simple pick-and-place skill primitives from human demonstrations. 1.2.3 Neural network learning Interest and research in neural network-based learning for control has exploded in recent years. References <ref> [3, 4, 19, 52, 73, 99] </ref> provide good overviews of neural network control over a broad range of applications. Most often, the role of the neural network in control is restricted to modeling either the nonlinear plant or some nonlinear feedback controller. <p> C. Nechyba and Y. Xu, Stochastic Similarity for Validating Human Control Strategy Models, to appear in IEEE Trans. on Robotics and Automation, June, 1998. [2] M. C. Nechyba and Y. Xu, On Discontinuous Human Control Strategies, to appear in Proc. IEEE Int. Conference on Robotics and Automation, May, 1998. <ref> [3] </ref> J. Song, Y. Xu, M. C. Nechyba and Y. Yam, Two Performance Measures for Evaluating Human Control Strategy, to appear in Proc. IEEE Int. Conference on Robotics and Auto mation, May, 1998. [4] M. C. Nechyba and Y.
Reference: [4] <author> P. J. Antsaklis, </author> <title> guest ed., </title> <journal> Special Issue on Neural Networks in Control Systems, IEEE Control System Magazine, </journal> <volume> vol. 12, no. 2, </volume> <pages> pp. 8-57, </pages> <year> 1992. </year>
Reference-contexts: Hov-land, et. al. [50] encode human assembly skill with Hidden Markov Models. In [85], neural networks encode simple pick-and-place skill primitives from human demonstrations. 1.2.3 Neural network learning Interest and research in neural network-based learning for control has exploded in recent years. References <ref> [3, 4, 19, 52, 73, 99] </ref> provide good overviews of neural network control over a broad range of applications. Most often, the role of the neural network in control is restricted to modeling either the nonlinear plant or some nonlinear feedback controller. <p> IEEE Int. Conference on Robotics and Automation, May, 1998. [3] J. Song, Y. Xu, M. C. Nechyba and Y. Yam, Two Performance Measures for Evaluating Human Control Strategy, to appear in Proc. IEEE Int. Conference on Robotics and Auto mation, May, 1998. <ref> [4] </ref> M. C. Nechyba and Y. Xu, Learning and Transfer of Human Real-Time Control Strate gies, Journal of Advanced Computational Intelligence, vol. 1, no. 2, pp. 137-54, 1997. [5] M. C. Nechyba and Y.
Reference: [5] <author> H. Asada and S. Liu, </author> <title> Transfer of Human Skills to Neural Net Robot Controllers, </title> <booktitle> Proc. IEEE Int. Conf. on Robotics and Automation, </booktitle> <volume> vol. 3, </volume> <pages> pp. 2442-2447, </pages> <year> 1991. </year>
Reference-contexts: Expert linguistic rules are acquired directly from a human expert to partition the control space. For each region, a corresponding linear control law is derived from the numeric demonstration data by the human expert. In <ref> [5, 70, 106] </ref>, the same deburring robot is controlled through an associative neural network which maps process parameter features to action parameters from human control data. The proper tool feed rate is determined from the burr characteristics of the current process. <p> IEEE Int. Conference on Robotics and Auto mation, May, 1998. [4] M. C. Nechyba and Y. Xu, Learning and Transfer of Human Real-Time Control Strate gies, Journal of Advanced Computational Intelligence, vol. 1, no. 2, pp. 137-54, 1997. <ref> [5] </ref> M. C. Nechyba and Y. Xu, Human Control Strategy: Abstraction, Verification and Rep lication, IEEE Control Systems Magazine, vol. 17, no. 5, pp. 48-61, 1997. 182 Appendix C: [6] M. C. Nechyba and Y. Xu, Cascade Neural Networks with Node-Decoupled Extended Kalman Filtering, Proc. IEEE Int.
Reference: [6] <author> H. Asada and B. Yang, </author> <title> Skill Acquisition from Human Experts Through Pattern Processing of Teaching Data, </title> <booktitle> Proc. IEEE Int. Conf. on Robotics and Automation, </booktitle> <volume> vol. 3, </volume> <pages> pp. 1302-7, </pages> <year> 1989. </year>
Reference-contexts: Although fuzzy control systems are well suited for simple control tasks with few inputs and outputs, they do not scale well to the high-dimensional input spaces required in modeling human control strategy [8]. Robot learning from human experts has also been applied to a deburring robot. Asada and Yang <ref> [6] </ref> derive control rules directly from human input/output data, by associating input patterns with corresponding output actions for the deburring robot. In [125], Yang and Asada combine linguistic information and numeric input/output data for the overall control of the robot. <p> C. Nechyba and Y. Xu, Human Control Strategy: Abstraction, Verification and Rep lication, IEEE Control Systems Magazine, vol. 17, no. 5, pp. 48-61, 1997. 182 Appendix C: <ref> [6] </ref> M. C. Nechyba and Y. Xu, Cascade Neural Networks with Node-Decoupled Extended Kalman Filtering, Proc. IEEE Int. Symp. on Computational Intelligence in Robotics and Automation, vol. 1, pp. 214-9, 1997. [7] M. C. Nechyba and Y. Xu, Stochastic Similarity for Validating Human Control Strategy Models, Proc. IEEE Int.
Reference: [7] <author> T. Ash, </author> <title> Dynamic Node Creation in Backpropagation Networks, </title> <journal> Connection Science, </journal> <volume> vol. 1, no. 4, </volume> <pages> pp. 365-75, </pages> <year> 1989. </year> <note> 184 Bibliography </note>
Reference-contexts: See, for example [18, 22, 23, 44, 77, 78, 117]. In constructive algorithms, on the other hand, neural networks are initialized in some minimal configuration and additional hidden units are added as the learning requires. Ash <ref> [7] </ref>, Bartlett [13] and Hiroshe [49], for example, have all experimented with adaptive architectures where hidden units are added one at a time in a single hidden layer as the error measure fails to reduce appreciably during learning. <p> C. Nechyba and Y. Xu, Cascade Neural Networks with Node-Decoupled Extended Kalman Filtering, Proc. IEEE Int. Symp. on Computational Intelligence in Robotics and Automation, vol. 1, pp. 214-9, 1997. <ref> [7] </ref> M. C. Nechyba and Y. Xu, Stochastic Similarity for Validating Human Control Strategy Models, Proc. IEEE Int. Conf. on Robotics and Automation, vol. 1, pp. 278-83, 1997. [8] M. C. Nechyba and Y. Xu, On the Fidelity of Human Skill Models, Proc. IEEE Int.
Reference: [8] <author> K. J. strm and T. J. McAvoy, </author> <title> Intelligent Control: An Overview and Evaluation, Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, </title> <editor> D. A. White and D. A. Sofge, </editor> <booktitle> eds., </booktitle> <pages> pp. 3-34, </pages> <publisher> Multiscience Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Methods in intelligent control include fuzzy logic control, neural network control, reinforcement learning, and locally weighted learning for control. Neural networks are used to map unknown nonlinear functions and have been applied in control most commonly for dynamic system identification and parameter adaptive control <ref> [8, 73, 79] </ref>. Locally weighted learning presents an alternative to neural networks, and maps unknown functions through local statistical regression of the experimental data, rather than through a functional representation [9, 10]. In reinforcement learning, an appropriate control strategy is found t (sec) the same road and simulation parameters. <p> Although fuzzy control systems are well suited for simple control tasks with few inputs and outputs, they do not scale well to the high-dimensional input spaces required in modeling human control strategy <ref> [8] </ref>. Robot learning from human experts has also been applied to a deburring robot. Asada and Yang [6] derive control rules directly from human input/output data, by associating input patterns with corresponding output actions for the deburring robot. <p> In Chapter 4, we will then investigate the proposed learning architecture for abstracting HCS models. 3.1 Motivation In recent years, artificial neural networks have shown great promise in identifying complex nonlinear mappings from observed data and have found many applications in robotics and other nonlinear control problems <ref> [8, 73, 79] </ref>. As such, they have received a great deal of attention in the learning community. <p> IEEE Int. Symp. on Computational Intelligence in Robotics and Automation, vol. 1, pp. 214-9, 1997. [7] M. C. Nechyba and Y. Xu, Stochastic Similarity for Validating Human Control Strategy Models, Proc. IEEE Int. Conf. on Robotics and Automation, vol. 1, pp. 278-83, 1997. <ref> [8] </ref> M. C. Nechyba and Y. Xu, On the Fidelity of Human Skill Models, Proc. IEEE Int. Conference on Robotics and Automation, vol. 3, pp. 2688-93, 1996. [9] M. C. Nechyba and Y. Xu, Human Skill Transfer: Neural Networks as Learners and Teachers, Proc. IEEE Int.
Reference: [9] <author> C. G. Atkeson, A. W. Moore and S. Schaal, </author> <title> Locally Weighted Learning, </title> <journal> Artificial Intelligence Review, </journal> <volume> vol. 11, no. </volume> <pages> 1-5, pp. 11-73, </pages> <year> 1997. </year>
Reference-contexts: Locally weighted learning presents an alternative to neural networks, and maps unknown functions through local statistical regression of the experimental data, rather than through a functional representation <ref> [9, 10] </ref>. In reinforcement learning, an appropriate control strategy is found t (sec) the same road and simulation parameters. The two control strategies are quite different. control strategy #1 control strategy #2 t (sec) applied force (N) 1.2 Related work 5 through dynamic programming of a discretized state space [14]. <p> Another learning paradigm, locally weighted learning, has emerged more recently and has shown great success for a number of different control applications, ranging from devil sticking [100] to robot juggling [101]. Atkeson, et. al. <ref> [9] </ref> offer an excellent overview of locally weighted learning, while [10] addresses control-specific issues. Locally weighted regression is one instance of locally weighted learning and is similar in approach to CMAC [1] and RBF [75] neural networks in that local (linear) models are fit to nearby data. <p> Xu, Stochastic Similarity for Validating Human Control Strategy Models, Proc. IEEE Int. Conf. on Robotics and Automation, vol. 1, pp. 278-83, 1997. [8] M. C. Nechyba and Y. Xu, On the Fidelity of Human Skill Models, Proc. IEEE Int. Conference on Robotics and Automation, vol. 3, pp. 2688-93, 1996. <ref> [9] </ref> M. C. Nechyba and Y. Xu, Human Skill Transfer: Neural Networks as Learners and Teachers, Proc. IEEE Int. Conference on Intelligent Robots and Systems, vol. 3, pp. 314 9, 1995. [10] M. C. Nechyba and Y. Xu, Neural Network Approach to Control System Identification with Variable Activation Functions, Proc.
Reference: [10] <author> C. G. Atkeson, A. W. Moore and S. Schaal, </author> <title> Locally Weighted Learning for Control, </title> <journal> Artificial Intelligence Review, </journal> <volume> vol. 11, no. </volume> <pages> 1-5, pp. 75-113, </pages> <year> 1997. </year>
Reference-contexts: Locally weighted learning presents an alternative to neural networks, and maps unknown functions through local statistical regression of the experimental data, rather than through a functional representation <ref> [9, 10] </ref>. In reinforcement learning, an appropriate control strategy is found t (sec) the same road and simulation parameters. The two control strategies are quite different. control strategy #1 control strategy #2 t (sec) applied force (N) 1.2 Related work 5 through dynamic programming of a discretized state space [14]. <p> This reward represents the reinforcement signal, and is akin to learning with a critic as opposed to learning with a teacher. The reinforcement learning algorithm is expected to explore and learn a suitable control strategy over time. References [43] and <ref> [10] </ref> give some examples of reinforcement learning control for a robot manipulator and a simulated car in a hole, respectively. Schneider [102] learns the open-loop skill of throwing through a search of the parameter space which defines all possible throwing motions. <p> Another learning paradigm, locally weighted learning, has emerged more recently and has shown great success for a number of different control applications, ranging from devil sticking [100] to robot juggling [101]. Atkeson, et. al. [9] offer an excellent overview of locally weighted learning, while <ref> [10] </ref> addresses control-specific issues. Locally weighted regression is one instance of locally weighted learning and is similar in approach to CMAC [1] and RBF [75] neural networks in that local (linear) models are fit to nearby data. <p> When the model is queried for the output at a specified input vector, points in the database near that input vector are used to construct a local linear map. Locally weighted regression offers several advantages over global learning paradigms (such as neural networks) <ref> [10] </ref>. First, locally weighted regression results in smooth predicted surfaces. Second, it automatically linearizes the system around a query point by providing the local linear map. <p> IEEE Int. Conference on Robotics and Automation, vol. 3, pp. 2688-93, 1996. [9] M. C. Nechyba and Y. Xu, Human Skill Transfer: Neural Networks as Learners and Teachers, Proc. IEEE Int. Conference on Intelligent Robots and Systems, vol. 3, pp. 314 9, 1995. <ref> [10] </ref> M. C. Nechyba and Y. Xu, Neural Network Approach to Control System Identification with Variable Activation Functions, Proc. IEEE Int. Symp. on Intelligent Control, vol. 1, pp. 358-63, 1994. 183
Reference: [11] <author> S. Baluja and R. Caruana, </author> <title> Removing Genetics from the Standard Genetic Algorithm, </title> <booktitle> Proc. of the 12th Int. Conf. on Machine Learning, </booktitle> <volume> vol. 1, </volume> <pages> pp. 38-46, </pages> <year> 1995. </year>
Reference-contexts: Since, in general, we do not have an explicit representation for in terms of the model parameters , model optimization can be achieved through one of a number of different stochastic algorithms, including simultaneously perturbed stochastic approximation (SPSA) [113], population-based incremental learning (PBIL) <ref> [11] </ref> and genetic optimization [40]. Initial experiments with SPSA, for example, demonstrate that learned models of human control strategy can be improved with respect to specific performance criteria [111]. Another area of application for HCS models might be as virtual expert instructors.
Reference: [12] <author> R. Basri and D. Weinshall, </author> <title> Distance Metric Between 3D Models and 2D Images for Recognition and Classification, </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 43, no. 4, </volume> <pages> pp. 465-479, </pages> <year> 1996. </year>
Reference-contexts: Therefore, we require a stochastic similarity measure, with sufficient representational power and exibility to compare multi-dimensional, stochastic trajectories. 6.2 Stochastic similarity measure Similarity measures or metrics have been given considerable attention in computer vision <ref> [12, 20, 121] </ref>, image database retrieval [54], and 2D or 3D shape analysis [62, 107].
Reference: [13] <author> E. B. Bartlett, </author> <title> Dynamic Node Architecture Learning: An Information Theoretic Approach, </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 7, no. 1, </volume> <pages> pp. 129-40, </pages> <year> 1994. </year>
Reference-contexts: See, for example [18, 22, 23, 44, 77, 78, 117]. In constructive algorithms, on the other hand, neural networks are initialized in some minimal configuration and additional hidden units are added as the learning requires. Ash [7], Bartlett <ref> [13] </ref> and Hiroshe [49], for example, have all experimented with adaptive architectures where hidden units are added one at a time in a single hidden layer as the error measure fails to reduce appreciably during learning.
Reference: [14] <author> A. G. Barto, R. S. Sutton and C. J. Watkins, </author> <title> Learning and Sequential Decision Making, Learning and Computational Neuroscience, </title> <editor> ed. M. Gabriel and J. W. Moore, </editor> <publisher> MIT Press, Cambridge, </publisher> <pages> pp. 539-602, </pages> <year> 1990. </year>
Reference-contexts: In reinforcement learning, an appropriate control strategy is found t (sec) the same road and simulation parameters. The two control strategies are quite different. control strategy #1 control strategy #2 t (sec) applied force (N) 1.2 Related work 5 through dynamic programming of a discretized state space <ref> [14] </ref>. Below, we describe previous work relating to each of these methods. 1.2.1 Skill learning through exploration Learning skill through exploration has become a popular paradigm for acquiring robotic skills. <p> Below, we describe previous work relating to each of these methods. 1.2.1 Skill learning through exploration Learning skill through exploration has become a popular paradigm for acquiring robotic skills. In reinforcement learning <ref> [14, 76, 115, 120] </ref>, data is not given as direct input/output data points; rather data is specified by an input vector and an associated (scalar) reward from the environment. This reward represents the reinforcement signal, and is akin to learning with a critic as opposed to learning with a teacher.
Reference: [15] <author> P. H. </author> <title> Batavia, Driver Adaptive Warning Systems, </title> <type> Technical Report, </type> <institution> CMU-RI-TR-98-07, Carnegie Mellon University, </institution> <year> 1998. </year>
Reference-contexts: Data was collected from seven drivers of both genders, ranging in age from 21 to 50 in Navlab 8, a minivan equipped with RALPH, a vision-based lateral-position estimation system <ref> [15] </ref>. Each driver was asked to drive from Carnegie Mellon University, Pittsburgh, PA, 4.
Reference: [16] <author> L. E. Baum, T. Petrie, G. Soules and N. Weiss, </author> <title> A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains, </title> <journal> Ann. Mathematical Statistics, </journal> <volume> vol. 41, no. 1, </volume> <pages> pp. 164-71, </pages> <year> 1970. </year> <note> Bibliography 185 </note>
Reference-contexts: The following two HMMs are, for example, equivalent, but not identical: , (6-10) Finally, we note that for an observation sequence O of discrete symbols and an HMM , we can locally maximize using the well-known Baum-Welch algorithm (see Section 6.2.8) <ref> [94, 16] </ref>. We can also evaluate using the computationally efficient forward-backward algorithm. 6.2.2 Similarity measure Here, we derive a stochastic similarity measure, based on discrete-output HMMs. Assume that we wish to compare observation sequences from two stochastic processes and .
Reference: [17] <author> J. L. Bentley, </author> <title> Multidimensional Binary Search Trees Used for Associative Searching, </title> <journal> Communications of the ACM, </journal> <volume> vol. 19, no. 9, </volume> <pages> pp. 509-17, </pages> <year> 1975. </year>
Reference-contexts: Locally weighted regression is one instance of locally weighted learning and is similar in approach to CMAC [1] and RBF [75] neural networks in that local (linear) models are fit to nearby data. All the data is explicitly stored and organized in efficient data structures (such as k-d trees <ref> [17] </ref> or Bump trees [84], for example). When the model is queried for the output at a specified input vector, points in the database near that input vector are used to construct a local linear map.
Reference: [18] <author> N. V. Bhat and T. J. McAvoy, </author> <title> Determining Model Structure for Neural Models by Network Stripping, </title> <journal> Computers and Chemical Engineering, </journal> <volume> vol. 16, no. 4, </volume> <pages> pp. 271-81, </pages> <year> 1992. </year>
Reference-contexts: These approaches can be divided into (1) destructive and (2) constructive algorithms. In destructive algorithms, oversized feedforward models are trained first, and then, after learning has been completed, unimportant weights, based on some relevancy criteria are pruned from the network. See, for example <ref> [18, 22, 23, 44, 77, 78, 117] </ref>. In constructive algorithms, on the other hand, neural networks are initialized in some minimal configuration and additional hidden units are added as the learning requires.
Reference: [19] <author> C. M. Bishop, </author> <title> Neural Networks for Pattern Recognition, </title> <publisher> Oxford University Press, </publisher> <year> 1995. </year>
Reference-contexts: Hov-land, et. al. [50] encode human assembly skill with Hidden Markov Models. In [85], neural networks encode simple pick-and-place skill primitives from human demonstrations. 1.2.3 Neural network learning Interest and research in neural network-based learning for control has exploded in recent years. References <ref> [3, 4, 19, 52, 73, 99] </ref> provide good overviews of neural network control over a broad range of applications. Most often, the role of the neural network in control is restricted to modeling either the nonlinear plant or some nonlinear feedback controller.
Reference: [20] <author> M. Boninsegna and M. Rossi, </author> <title> Similarity Measures in Computer Vision, </title> <journal> Pattern Recognition Letters, </journal> <volume> vol. 15, no. 12, </volume> <pages> pp. 1255-60, </pages> <year> 1994. </year>
Reference-contexts: Therefore, we require a stochastic similarity measure, with sufficient representational power and exibility to compare multi-dimensional, stochastic trajectories. 6.2 Stochastic similarity measure Similarity measures or metrics have been given considerable attention in computer vision <ref> [12, 20, 121] </ref>, image database retrieval [54], and 2D or 3D shape analysis [62, 107].
Reference: [21] <author> P. J. Brockwell and R. A. Davis, </author> <title> Time Series: Theory and Methods, 2nd. </title> <editor> ed., </editor> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: One of the more well known, autoregressive-moving average (ARMA) modeling <ref> [21] </ref>, predicts the current signal based on a linear combination of previous time histories and Gaussian noise assumptions.
Reference: [22] <author> P. Burrascano, </author> <title> A Pruning Technique Maximizing Generalization, </title> <booktitle> Proc. Int. Joint Conf. on Neural Networks, </booktitle> <volume> vol. 1, </volume> <pages> pp. 347-50, </pages> <year> 1993. </year>
Reference-contexts: These approaches can be divided into (1) destructive and (2) constructive algorithms. In destructive algorithms, oversized feedforward models are trained first, and then, after learning has been completed, unimportant weights, based on some relevancy criteria are pruned from the network. See, for example <ref> [18, 22, 23, 44, 77, 78, 117] </ref>. In constructive algorithms, on the other hand, neural networks are initialized in some minimal configuration and additional hidden units are added as the learning requires.
Reference: [23] <author> G. Castellano, A. M. Fanelli and M. Pelillo, </author> <title> An Empirical Comparison of Node Pruning Methods for Layered Feed-forward Neural Networks, </title> <booktitle> Proc. Int. Joint Conf. on Neural Networks, </booktitle> <volume> vol. 1, </volume> <pages> pp. 321-6, </pages> <year> 1993. </year>
Reference-contexts: These approaches can be divided into (1) destructive and (2) constructive algorithms. In destructive algorithms, oversized feedforward models are trained first, and then, after learning has been completed, unimportant weights, based on some relevancy criteria are pruned from the network. See, for example <ref> [18, 22, 23, 44, 77, 78, 117] </ref>. In constructive algorithms, on the other hand, neural networks are initialized in some minimal configuration and additional hidden units are added as the learning requires.
Reference: [24] <author> J. P. Cater, </author> <title> Successfully Using Peak Learning Rates of 10 (and Greater) in Back-Propagation Networks with the Heuristic Learning Algorithm, </title> <booktitle> IEEE First Int. Conf. on Neural Networks, </booktitle> <volume> vol. 2, </volume> <pages> pp. 645-51, </pages> <year> 1987. </year>
Reference-contexts: Since this training method was first proposed [98], modifications to standard backpropagation, as well as other training algorithms have been suggested. An adaptive learning rate <ref> [24] </ref>, as well as an additive momentum term [87] are both somewhat effective in accelerating convergence of backpropagation in at regions of the error hyper-surface. Quickprop [31] incorporates local, second-order information in the weight-update 10 Chapter 1: Introduction algorithm to further speed up learning.
Reference: [25] <author> K. Chen and R. D. Ervin, </author> <title> Worldwide IVHS Activities: A Comparative Overview, </title> <booktitle> Proc. CONVERGENCE92 Int. Congress on Transportation Electronics, </booktitle> <pages> pp. 339-49, </pages> <year> 1992. </year> <note> 186 Bibliography </note>
Reference-contexts: In other robotic applications, we would like robots to carry out tasks which humans have traditionally performed. For example, the Intelligent Vehicle Highway System (IVHS), currently being developed through massive initiatives in the United States, Europe, and Japan <ref> [25, 26] </ref>, envisions automating much of the driving on our highways. The required automated vehicles will need significant intelligence to interact safely with variable road conditions and other traffic. Modeling human intelligence offers one way of building up the necessary skills for this type of intelligent machine.
Reference: [26] <author> W. C. Collier and R. J. Weiland, </author> <title> Smart Cars, Smart Highways, </title> <journal> IEEE Spectrum, </journal> <volume> vol. 31, No. 4, </volume> <pages> pp. 27-33, </pages> <year> 1994. </year>
Reference-contexts: In other robotic applications, we would like robots to carry out tasks which humans have traditionally performed. For example, the Intelligent Vehicle Highway System (IVHS), currently being developed through massive initiatives in the United States, Europe, and Japan <ref> [25, 26] </ref>, envisions automating much of the driving on our highways. The required automated vehicles will need significant intelligence to interact safely with variable road conditions and other traffic. Modeling human intelligence offers one way of building up the necessary skills for this type of intelligent machine.
Reference: [27] <author> G. Cybenko, </author> <title> Approximation by Superposition of a Sigmoidal Function, Mathematics of Control, Signals, </title> <journal> and Systems, </journal> <volume> vol. 2, no. 4, </volume> <pages> pp. 303-14, </pages> <year> 1989. </year>
Reference-contexts: Note that a new hidden unit receives as input connections from the input units as well as all previous hidden units (hence the name cascade). A cascade network with input units (including the bias unit), hidden units, and output units, has connections where, (3-1) Recent theorems by Cybenko <ref> [27] </ref> and Funahashi [37], which hold that standard layered neural networks are universal function approximators also hold for the cascade network topology, since any multi-layer feedforward neural network with hidden units arranged in layers, fully connected between consecutive layers, is a special case of a cascade network with hidden units and <p> We can relax these assumptions further by allowing new hidden units to have variable activation functions [80, 81]. In fact, Cybenko <ref> [27] </ref> shows that sigmoidal functions are not the only possible activation functions which allow for universal function approximation. There are other nonlinear functions, such as sine and cosine for example, which are complete in the space of n-dimensional continuous functions.
Reference: [28] <author> N. Delson and H. West, </author> <title> Robot Programming by Human Demonstration: Adaptation and Inconsistency in Constrained Motion, </title> <booktitle> Proc. IEEE Int. Conf. on Robotics and Automation, </booktitle> <volume> vol. 1, </volume> <pages> pp. 30-6, </pages> <year> 1996. </year>
Reference-contexts: Lee [65] investigates human-to-robot skill transfer through demonstration of task performance in a virtual environments. Voyles, et. al. [119] program a robot manipulator through human demonstration and abstraction of gesture primitives. Delson and West <ref> [28] </ref> and Ude [118] both learn open-loop robot trajectories from human demonstration. Skubic and Volz [109] transfer force-based assembly skills to robots from human demonstrations. Iba [53] models open-loop sensory motor skills in humans.
Reference: [29] <author> K. Deng, A. Moore and M. C. Nechyba, </author> <title> Learning to Recognize Time Series: Combining ARMA Models with Memory-Based Learning, </title> <booktitle> Proc. IEEE Int. Symp. on Computational Intelligence in Robotics and Automation, </booktitle> <volume> vol. 1, </volume> <pages> pp. 246-50, </pages> <year> 1997. </year>
Reference-contexts: We developed a real-time graphic driving simulator, with dynamic interactions of the simulated car an the environment. This has proven to be a valuable testing tool for the learning algorithms and statistical methods developed herein. Some other researchers have also used this simulator in their work <ref> [29, 110] </ref>. 9.2 Future work While this thesis provides the foundations for modeling and analyzing human control strategies, it is certainly not the first and last word on this topic it is only an important first step. <p> Most importantly, it could be used to monitor and detect potentially dangerous control behaviors on the part of the human operator, as is done, for example, in <ref> [29] </ref> with auto-regressive models. Alternatively, it could be used to monitor an apprentices control strategy during training to see whether or not his control strategy begins to approach that of the expert.
Reference: [30] <author> R. O. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Since we have already observed in Chapter 4 that a linear model is insufficient to qualitatively replicate switching, nonlinear control strategies, ARMA models may form a poor foundation upon which to develop a similarity measure. Other work has focussed on classifying temporal patterns using Bayesian statistics <ref> [30] </ref>, wavelet and spectral analysis [114], neural networks (both feedforward and recurrent) [47, 112], and Hidden Markov Models (see discussion below). Much of this work, however, analyzes only short-time trajectories or patterns, and, in many cases, generates only a binary classification, rather than a continuously valued similarity measure. <p> Consequently, we chose not include this dimension in the similarity analysis. 7.1.3 Bayes classification A legitimate question of course is whether or not a simpler statistical technique, like the Bayes optimal classifier <ref> [30] </ref>, can achieve similar positive results. <p> Now, for each vector and each class , , we can calculate <ref> [30] </ref>, , , (7-9) (i.e. the probability that vector belongs to class ), where is the prior probability of class , , (7-10) X i k,( ) m i k,( ) mm N m X i j,( ) s,( )[ ]= i j,( ) P C l k,( ) u t
Reference: [31] <author> S. E. Fahlman, </author> <title> An Empirical Study of Learning Speed in Back-Propagation Networks, </title> <type> Technical Report, </type> <institution> CMU-CS-TR-88-162, Carnegie Mellon University, </institution> <year> 1988. </year>
Reference-contexts: Ash [7], Bartlett [13] and Hiroshe [49], for example, have all experimented with adaptive architectures where hidden units are added one at a time in a single hidden layer as the error measure fails to reduce appreciably during learning. Fahlman <ref> [31, 32] </ref> proposes a cascade learning architecture, where hidden units are added in multiple cascading layers as opposed to a single hidden layer. <p> An adaptive learning rate [24], as well as an additive momentum term [87] are both somewhat effective in accelerating convergence of backpropagation in at regions of the error hyper-surface. Quickprop <ref> [31] </ref> incorporates local, second-order information in the weight-update 10 Chapter 1: Introduction algorithm to further speed up learning. Kollias and Anastassious [58] propose applying the Marquardt-Levenberg least squares optimization method, which utilizes an approximation of the Hessian matrix. <p> Second, backpropagation and other gradient descent techniques tend to converge rather slowly. Since the backpropagation algorithm adjusts one weight at a time, the current weight change in the network frequently contradicts one or more of the previous weight adjustments, leading to oscillatory behavior and convergence to poor local minima <ref> [31, 47] </ref>. To address the problem of fixed architectures in neural networks, we look towards exible cascade neural networks [32, 33]. <p> With no further significant decrease in the RMS error between the network outputs and the training data ( ), a first hidden unit is added to the network from a pool of candidate units. Using the quickprop algorithm <ref> [31] </ref> an improved version of the standard backprop algorithm these candidate units are trained independently and in parallel with different random initial weights. Again, after no more appreciable error reduction occurs, the best candidate unit is selected and installed in the network. <p> sigmoidal activation function are the Gaussian function, Bessel functions, and sinusoidal functions of various frequency [80]. 3.3 Node-decoupled extended Kalman filtering While quickprop is an improvement over the standard backpropagation algorithm for adjusting the weights in the cascade network, it can still require many iterations until satisfactory convergence is reached <ref> [31, 108] </ref>. Thus, we modify standard cascade learning by replacing the quick-prop algorithm with node-decoupled extended Kalman filtering (NDEKF), which has been shown to have better convergence properties and faster training times than gradient-descent techniques for fixed-architecture multi-layer feedforward networks [92].
Reference: [32] <author> S. E. Fahlman and C. Lebiere, </author> <title> The Cascade-Correlation Learning Architecture, </title> <type> Technical Report, </type> <institution> CMU-CS-TR-91-100, Carnegie Mellon University, </institution> <year> 1991. </year>
Reference-contexts: Ash [7], Bartlett [13] and Hiroshe [49], for example, have all experimented with adaptive architectures where hidden units are added one at a time in a single hidden layer as the error measure fails to reduce appreciably during learning. Fahlman <ref> [31, 32] </ref> proposes a cascade learning architecture, where hidden units are added in multiple cascading layers as opposed to a single hidden layer. <p> To address the problem of fixed architectures in neural networks, we look towards exible cascade neural networks <ref> [32, 33] </ref>. In cascade learning, the network topology is not fixed prior to learning, but rather adjusts dynamically as a function of learning, as hidden units are added to an initially minimal network one at a time.
Reference: [33] <author> S. E. Fahlman, L. D. Baker and J. A. Boyan, </author> <title> The Cascade 2 Learning Architecture, </title> <type> Technical Report, </type> <institution> CMU-CS-TR-96-184, Carnegie Mellon University, </institution> <year> 1996. </year>
Reference-contexts: To address the problem of fixed architectures in neural networks, we look towards exible cascade neural networks <ref> [32, 33] </ref>. In cascade learning, the network topology is not fixed prior to learning, but rather adjusts dynamically as a function of learning, as hidden units are added to an initially minimal network one at a time. <p> We believe, however, that both (1) and (2) above have a place in the learning process. Thus, for modeling human control strategy, we look towards the exible cascade learning architecture <ref> [33] </ref>, which adjusts the structure of the neural network as part of learning. <p> As originally formulated in <ref> [33] </ref>, cascade neural network training proceeds in several steps. Initially, there are no hidden units in the network, only direct input-output connections. These weights are trained first, thereby capturing any linear relationship between the inputs and outputs. <p> The training set consists of 1000 random points; the cross validation set consists of an additional 1000 random points; and our test set consists of another 2000 random points. Our second problem (B) is taken from <ref> [33] </ref>. <p> Our training, cross validation, and test sets are identical to those in <ref> [33] </ref> and consist of the following: (1) 4000 evenly spaced points are generated in the interval ; (2) 968 of those points are randomly chosen for the training set; (3) 968 are randomly chosen from the remaining 3032 points for the cross validation set; and (4) the remaining 2064 points make <p> We use a 2500-length sequence for training, another 2500-length sequence for cross validation, and another 5000-length sequence for testing. The variables , and are initialized to zero. Finally, our last two problems are taken once again from <ref> [33] </ref>. <p> We present, (3-34) as the four inputs to the neural network, while the goal of this task is to predict for . We will refer to as problem (D) and as problem (E). Our training, cross validation, and test sets are once again identical to those in <ref> [33] </ref>. The training set consists of the 500 data points from time to ; the cross validation set consists of the 500 data points from time to ; and the test set consists of the 500 points from time to . <p> Both have been shown to outperform sigmoidal networks for continuous function approximation. For problems (B), (D), and (E), taken from <ref> [33] </ref>, we follow the same procedure in training with methods , as Fahlman, et. al., follow in training with methods . In [33], Cq neural networks are allowed to grow to a maximum of 50 hidden units in 15 separate trials for each problem. <p> Both have been shown to outperform sigmoidal networks for continuous function approximation. For problems (B), (D), and (E), taken from <ref> [33] </ref>, we follow the same procedure in training with methods , as Fahlman, et. al., follow in training with methods . In [33], Cq neural networks are allowed to grow to a maximum of 50 hidden units in 15 separate trials for each problem. For each trial, the best RMS error over the test set is recorded. Equivalently sized Fq networks are also trained, for up to 60,000 epochs per trial. <p> Finally, for the cascade methods , we use eight candidate units, the same as in <ref> [33] </ref>. 3.4.2 Learning results Table 3-2 below reports the average RMS error ( ) over the test sets for problems (A) through (E). We note that in all cases, our cascade/NDEKF (Ck) approach outperforms the other three methods. <p> Standard deviations are in parentheses. Shaded cells are results taken from <ref> [33] </ref>. b. For the Fk results we report the better of the sinusoidal or sigmoidal networks (in all cases, the sinusoidal networks did better on average). c. A fixed-architecture/backprop network failed to converge for this problem, despite many experiments with different learning parameters [33]. d. <p> Shaded cells are results taken from <ref> [33] </ref>. b. For the Fk results we report the better of the sinusoidal or sigmoidal networks (in all cases, the sinusoidal networks did better on average). c. A fixed-architecture/backprop network failed to converge for this problem, despite many experiments with different learning parameters [33]. d. This is comparable to the result of 0.03 in [92] for a network with an equal number of parameters. <p> We once again look at problems (A) through (E) defined in Section 3.4.1, only now, we add Gaussian noise with standard deviation to both the inputs and outputs of the training, cross-validation and test data sets. Since these results no longer compare with those in <ref> [33] </ref>, we follow the training regime for problems (A) and (C) in the previous section (rather than for problems (B), (D) and (E)). <p> This stands in sharp contrast to more ad hoc methods such as Cq and Fq, for which the various learning parameters were tuned for each particular problem in order to achieve good results <ref> [33] </ref>. The weight-update recursion, (3-38) can be thought of as an adaptive learning rate, which lessens the need for parameter tuning in NDEKF. Thus, we need spend little time tuning either learning parameters or network architectures in this approach.
Reference: [34] <author> E. Fix and H. G. Armstrong, </author> <title> Modeling Human Performance with Neural Networks, </title> <booktitle> Proc. Int. Joint Conf. on Neural Networks, </booktitle> <volume> vol. 1, </volume> <pages> pp. 247-52, </pages> <year> 1990. </year>
Reference-contexts: Iba [53] models open-loop sensory motor skills in humans. Gin-grich, et. al. [39] argue that learning human performance models is valuable, but offer results only for simulated, known dynamic systems. Several approaches to skill learning in human driving have been implemented. In <ref> [34, 35] </ref>, neural networks are trained to mimic human behavior for a simulated, circular racetrack. The task essentially involves avoiding other computer-generated cars; no dynamics are modeled or considered in the approach. Pomerleau [89, 90] implements real-time road-following with data collected from a human driver.
Reference: [35] <author> E. Fix and H. G. Armstrong, </author> <title> Neural Network Based Human Performance Modeling, </title> <booktitle> Proc. IEEE National Aerospace and Electronics Conf., </booktitle> <volume> vol. 3, </volume> <pages> pp. 1162-5, </pages> <year> 1990. </year> <note> Bibliography 187 </note>
Reference-contexts: Iba [53] models open-loop sensory motor skills in humans. Gin-grich, et. al. [39] argue that learning human performance models is valuable, but offer results only for simulated, known dynamic systems. Several approaches to skill learning in human driving have been implemented. In <ref> [34, 35] </ref>, neural networks are trained to mimic human behavior for a simulated, circular racetrack. The task essentially involves avoiding other computer-generated cars; no dynamics are modeled or considered in the approach. Pomerleau [89, 90] implements real-time road-following with data collected from a human driver.
Reference: [36] <author> H. Friedrich, M. Kaiser and R. Dillman, </author> <title> What Can Robots Learn From Humans? Annual Reviews in Control, </title> <journal> vol. </journal> <volume> 20, </volume> <pages> pp. 167-72, </pages> <year> 1996. </year>
Reference-contexts: HMMs are trained to learn both telerobotic trajectories executed by a human operator and simple human handwriting gestures. Yamato, et. al. [123, 124] also train HMMs to recognize open-loop human actions. 1.2 Related work 7 Friedrich, et. al. <ref> [36] </ref> and Kaiser [56] review programming by demonstration and skill acquisition via human demonstration for elementary robot skills. Lee [65] investigates human-to-robot skill transfer through demonstration of task performance in a virtual environments. Voyles, et. al. [119] program a robot manipulator through human demonstration and abstraction of gesture primitives.
Reference: [37] <author> K. Funahashi, </author> <title> On the Approximate Realization of Continuous Mappings by Neural Networks, </title> <journal> Neural Net., </journal> <volume> vol. 2, no. 3, </volume> <pages> pp. 183-92, </pages> <year> 1989. </year>
Reference-contexts: A cascade network with input units (including the bias unit), hidden units, and output units, has connections where, (3-1) Recent theorems by Cybenko [27] and Funahashi <ref> [37] </ref>, which hold that standard layered neural networks are universal function approximators also hold for the cascade network topology, since any multi-layer feedforward neural network with hidden units arranged in layers, fully connected between consecutive layers, is a special case of a cascade network with hidden units and some weight connections
Reference: [38] <author> S. Geva and J. Sitte, </author> <title> A Cartpole Experiment Benchmark for Trainable Controllers, </title> <journal> IEEE Control Systems Magazine, </journal> <volume> vol. 13, no. 5, </volume> <pages> pp. 40-51, </pages> <year> 1993. </year>
Reference: [39] <author> C. G. Gingrich, D. R. Kuespert and T. J. McAvoy, </author> <title> Modeling Human Operators Using Neural Networks, </title> <journal> ISA Trans., </journal> <volume> vol. 31, no. 3, </volume> <pages> pp. 81-90, </pages> <year> 1992. </year>
Reference-contexts: Delson and West [28] and Ude [118] both learn open-loop robot trajectories from human demonstration. Skubic and Volz [109] transfer force-based assembly skills to robots from human demonstrations. Iba [53] models open-loop sensory motor skills in humans. Gin-grich, et. al. <ref> [39] </ref> argue that learning human performance models is valuable, but offer results only for simulated, known dynamic systems. Several approaches to skill learning in human driving have been implemented. In [34, 35], neural networks are trained to mimic human behavior for a simulated, circular racetrack.
Reference: [40] <author> D. E. Goldberg, </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning, </title> <publisher> Addison-Wesley, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Since, in general, we do not have an explicit representation for in terms of the model parameters , model optimization can be achieved through one of a number of different stochastic algorithms, including simultaneously perturbed stochastic approximation (SPSA) [113], population-based incremental learning (PBIL) [11] and genetic optimization <ref> [40] </ref>. Initial experiments with SPSA, for example, demonstrate that learned models of human control strategy can be improved with respect to specific performance criteria [111]. Another area of application for HCS models might be as virtual expert instructors.
Reference: [41] <author> D. Gopher, M. Weil and T. Bareket, </author> <title> The Transfer of Skill from a Computer Game Trainer to Actual Flight, </title> <booktitle> Proc. Human Factors Society 36th Annual Meeting, </booktitle> <volume> vol. 2, </volume> <pages> pp. 1285-1290, </pages> <year> 1992. </year>
Reference-contexts: It has been shown that simulated training (e.g. training on a simulation of the system rather than the real system) still improves performance once the apprentice transitions to control of the real dynamic system <ref> [41, 103] </ref>. Therefore, we would expect that this approach would prove useful, even if for safety reasons we replace the actual system by a simulation of that system during apprentice training. contribute to the learning of a single apprentice (right).
Reference: [42] <author> A. Guez and J. Selinsky, </author> <title> A Trainable Neuromorphic Controller, </title> <journal> Journal of Robotic Systems, </journal> <volume> vol. 5, no. 4, </volume> <pages> pp. 363-88, </pages> <year> 1988. </year>
Reference: [43] <author> V. Gullapalli, J. A. Franklin and H. Benbrahim, </author> <title> Acquiring Robot Skills Via Reinforcement Learning, </title> <journal> IEEE Control Systems Magazine, </journal> <volume> vol. 14, no. 1, </volume> <pages> pp. 13-24, </pages> <year> 1994. </year>
Reference-contexts: This reward represents the reinforcement signal, and is akin to learning with a critic as opposed to learning with a teacher. The reinforcement learning algorithm is expected to explore and learn a suitable control strategy over time. References <ref> [43] </ref> and [10] give some examples of reinforcement learning control for a robot manipulator and a simulated car in a hole, respectively. Schneider [102] learns the open-loop skill of throwing through a search of the parameter space which defines all possible throwing motions.
Reference: [44] <author> M. Hagiwara, </author> <title> Removal of Hidden Units and Weights for Back Propagation Networks, </title> <booktitle> Proc. Int. Joint Conf. on Neural Networks, </booktitle> <volume> vol. 1, </volume> <pages> pp. 351-54, </pages> <year> 1993. </year>
Reference-contexts: These approaches can be divided into (1) destructive and (2) constructive algorithms. In destructive algorithms, oversized feedforward models are trained first, and then, after learning has been completed, unimportant weights, based on some relevancy criteria are pruned from the network. See, for example <ref> [18, 22, 23, 44, 77, 78, 117] </ref>. In constructive algorithms, on the other hand, neural networks are initialized in some minimal configuration and additional hidden units are added as the learning requires.
Reference: [45] <author> B. Hannaford and P. Lee, </author> <title> Hidden Markov Model Analysis of Force/Torque Information in Telemanipulation, </title> <journal> Int. Journal Robotics Research, </journal> <volume> vol. 10, no. 5, </volume> <pages> pp. 528-39, </pages> <year> 1991. </year> <note> 188 Bibliography </note>
Reference-contexts: Transient sonar signals are classified with HMMs for ocean surveillance in [61]. Radons, et. al. [96] analyze 30-electrode neuronal spike activity in a monkeys visual cortex with HMMs. Hannaford and Lee <ref> [45] </ref> classify task structure in tele-operation based on HMMs. In [123, 124], HMMs are used to characterize sequential images of human actions.
Reference: [46] <author> H. Hatwal and E. C. Mikulcik, </author> <title> Some Inverse Solutions to an Automobile Path-Tracking Problem with Input Control of Steering and Brakes, Vehicle System Dynamics, </title> <journal> vol. </journal> <volume> 15, </volume> <pages> pp. 61-71, </pages> <year> 1986. </year>
Reference-contexts: The vehicle dynamics are given in (2-1) through (2-19) below (modified from <ref> [46] </ref>): steering wheelspeedometer compass map who has independent control over steering, braking, and acceleration (gas). 18 Chapter 2: Experimental design (2-1) (2-3) where describe the Cartesian position and orientation of the car; is the lateral velocity of the car; is the longitudinal velocity of the car; and is the angular velocity
Reference: [47] <author> J. Hertz, A. Krogh and R. G. Palmer, </author> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison-Wesley Publishing, </publisher> <address> Redwood City, </address> <year> 1991. </year>
Reference-contexts: Compared to static feedforward networks, the learning algorithms for recurrent networks are significantly more computationally involved, requiring relaxation of sets of differential equations <ref> [47] </ref>. Yet, Qin, et. al. [93] show similar error convergence in mapping simple dynamic systems with feedforward and recurrent networks, respectively. <p> In all cases, experimental data is usually partitioned into two random sets one for actual training, and the other for cross validation <ref> [47] </ref>; training is generally stopped once the error measure on the cross-validation set no longer decreases. 1.2.4 Locally weighted learning Neural networks have received great attention for nonlinear learning and control applications. <p> Second, backpropagation and other gradient descent techniques tend to converge rather slowly. Since the backpropagation algorithm adjusts one weight at a time, the current weight change in the network frequently contradicts one or more of the previous weight adjustments, leading to oscillatory behavior and convergence to poor local minima <ref> [31, 47] </ref>. To address the problem of fixed architectures in neural networks, we look towards exible cascade neural networks [32, 33]. <p> Other work has focussed on classifying temporal patterns using Bayesian statistics [30], wavelet and spectral analysis [114], neural networks (both feedforward and recurrent) <ref> [47, 112] </ref>, and Hidden Markov Models (see discussion below). Much of this work, however, analyzes only short-time trajectories or patterns, and, in many cases, generates only a binary classification, rather than a continuously valued similarity measure.
Reference: [48] <author> R. A. Hess, </author> <title> Human-in-the-Loop Control, The Control Handbook, </title> <editor> ed. W. S. Levine, </editor> <publisher> CRC Press, </publisher> <pages> pp. </pages> <address> 1497- 505, </address> <year> 1996. </year>
Reference-contexts: acquisition from human expert rules, and (2) skill discovery or refinement through hypothesis generation and testing. 1.2.2 Skill modeling from human data Interest in modeling human control goes all the way back to World War II, when engineers and psychologists attempted to improve the performance of pilots, gunners and bombardiers <ref> [48] </ref>. Early research in modeling human control is based on the control-theory paradigm [72], which attempts to model the human-in-the-loop as a simple feedback control system.
Reference: [49] <author> Y. Hiroshe, K. Yamashita and S. Hijiya, </author> <title> Backpropagation Algorithm Which Varies the Number of Hidden Units, </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 4, no. 1, </volume> <pages> pp. 61-6, </pages> <year> 1991. </year>
Reference-contexts: See, for example [18, 22, 23, 44, 77, 78, 117]. In constructive algorithms, on the other hand, neural networks are initialized in some minimal configuration and additional hidden units are added as the learning requires. Ash [7], Bartlett [13] and Hiroshe <ref> [49] </ref>, for example, have all experimented with adaptive architectures where hidden units are added one at a time in a single hidden layer as the error measure fails to reduce appreciably during learning.
Reference: [50] <author> G. E. Hovland, P. Sikka and B. J. MacCarragher, </author> <title> Skill Acquisition from Human Demonstration Using a Hidden Markov Model, </title> <booktitle> Proc. IEEE Int. Conf. on Robotics and Automation, </booktitle> <volume> vol. 3, </volume> <pages> pp. 2706-11, </pages> <year> 1997. </year>
Reference-contexts: Kosuge, et. al. [59] also abstract high-level assembly skill from human demonstration data. The high-level sequence of motion is decomposed into discrete state transitions, based on contact states during assembly. In each state, compliant motion control implements the corresponding low level control. Hov-land, et. al. <ref> [50] </ref> encode human assembly skill with Hidden Markov Models. In [85], neural networks encode simple pick-and-place skill primitives from human demonstrations. 1.2.3 Neural network learning Interest and research in neural network-based learning for control has exploded in recent years.
Reference: [51] <author> X. D. Huang, Y. Ariki and M. A. Jack, </author> <title> Hidden Markov Models for Speech Recognition, </title> <publisher> Edinburgh University Press, Edinburgh, </publisher> <year> 1990. </year>
Reference-contexts: As such, they have been applied for a variety of stochastic signal processing. In speech recognition, where HMMs have found their widest application, human auditory signals are analyzed as speech patterns <ref> [51, 94] </ref>. Transient sonar signals are classified with HMMs for ocean surveillance in [61]. Radons, et. al. [96] analyze 30-electrode neuronal spike activity in a monkeys visual cortex with HMMs. Hannaford and Lee [45] classify task structure in tele-operation based on HMMs. <p> If there are zero elements in a probability vector (i.e. a row of or a column of ), this methods redistributes of the total probability mass to the zero elements. Semicontinuous evaluation <ref> [51] </ref> redefines the forward algorithm. Let denote a discrete observation sequence that has been vector quantized from a sequence of real vectors , , and a VQ codebook , .
Reference: [52] <author> K. J. Hunt, et. al., </author> <title> Neural Networks for Control Systems - A Survey, </title> <journal> Automatica, </journal> <volume> vol. 28, no. 6, </volume> <pages> pp. 1083-112, </pages> <year> 1992. </year>
Reference-contexts: Hov-land, et. al. [50] encode human assembly skill with Hidden Markov Models. In [85], neural networks encode simple pick-and-place skill primitives from human demonstrations. 1.2.3 Neural network learning Interest and research in neural network-based learning for control has exploded in recent years. References <ref> [3, 4, 19, 52, 73, 99] </ref> provide good overviews of neural network control over a broad range of applications. Most often, the role of the neural network in control is restricted to modeling either the nonlinear plant or some nonlinear feedback controller.
Reference: [53] <author> W. Iba, </author> <title> Modeling the Acquisition and Improvement of Motor Skills, </title> <booktitle> Machine Learning: Proc. Eighth Int. Workshop on Machine Learning, </booktitle> <volume> vol. 1, </volume> <pages> pp. 60-64, </pages> <year> 1991. </year>
Reference-contexts: Voyles, et. al. [119] program a robot manipulator through human demonstration and abstraction of gesture primitives. Delson and West [28] and Ude [118] both learn open-loop robot trajectories from human demonstration. Skubic and Volz [109] transfer force-based assembly skills to robots from human demonstrations. Iba <ref> [53] </ref> models open-loop sensory motor skills in humans. Gin-grich, et. al. [39] argue that learning human performance models is valuable, but offer results only for simulated, known dynamic systems. Several approaches to skill learning in human driving have been implemented.
Reference: [54] <author> R. Jain, S. N. J. Murty, et. al., </author> <title> Similarity Measures for Image Databases, </title> <booktitle> in Proc. IEEE Int. Conf. on Fuzzy Systems, </booktitle> <volume> vol. 3, </volume> <pages> pp. 1247-54, </pages> <year> 1995. </year>
Reference-contexts: Therefore, we require a stochastic similarity measure, with sufficient representational power and exibility to compare multi-dimensional, stochastic trajectories. 6.2 Stochastic similarity measure Similarity measures or metrics have been given considerable attention in computer vision [12, 20, 121], image database retrieval <ref> [54] </ref>, and 2D or 3D shape analysis [62, 107].
Reference: [55] <author> B. H. Juang and L. R. Rabiner, </author> <title> A Probabilistic Distance Measure for Hidden Markov Models, </title> <journal> AT&T Technical Journal, </journal> <volume> vol. 64, no. 2, </volume> <pages> pp. 391-408, </pages> <year> 1985. </year> <note> Bibliography 189 </note>
Reference-contexts: 1 O 2 ,( ) 6.2 Stochastic similarity measure 105 (6-28) such that, , (6-29) if (a) or (b) . (6-31) The distance measure between two sets of observation sequences defined in (6-28) is closely related to the dual notion of distance between two Hidden Markov Models, as proposed in <ref> [55] </ref>. <p> ) s O 1 O 2 ,( )log 2 d O 1 O 2 ,( ) d O 2 O 1 ,( )= d O 1 O 2 ,( ) 0= l 1 l 2 ~ O 1 O 2 = O l 106 Chapter 6: Model validation (6-32) Then, <ref> [55] </ref> defines the following distance measure between two Hidden Markov Models, and : Unlike the observation sequences , the sequences are not unique, since they are stochastically generated from . Hence, is uniquely determined only in the limit as .
Reference: [56] <author> M. Kaiser, </author> <title> Transfer of Elementary Skills via Human-Robot Interaction, </title> <booktitle> Adaptive Behavior, </booktitle> <volume> vol. 5, no. </volume> <pages> 3-4, pp. 249-80, </pages> <year> 1997. </year>
Reference-contexts: HMMs are trained to learn both telerobotic trajectories executed by a human operator and simple human handwriting gestures. Yamato, et. al. [123, 124] also train HMMs to recognize open-loop human actions. 1.2 Related work 7 Friedrich, et. al. [36] and Kaiser <ref> [56] </ref> review programming by demonstration and skill acquisition via human demonstration for elementary robot skills. Lee [65] investigates human-to-robot skill transfer through demonstration of task performance in a virtual environments. Voyles, et. al. [119] program a robot manipulator through human demonstration and abstraction of gesture primitives.
Reference: [57] <author> S. B. Kang, </author> <title> Automatic Robot Instruction from Human Demonstration, </title> <type> Ph.D. Thesis, </type> <institution> The Robotics Institute, Carnegie Mellon University, </institution> <year> 1994. </year>
Reference-contexts: In [74], the authors provide a control theoretic model of human driver steering control. Finally, Pentland and Liu [86] apply HMMs towards inferring a particular drivers high-level intentions, such as turning and stopping. Other, higher level skills have also been abstracted from human performance data. Kang <ref> [57] </ref>, for example, teaches a robot assembly through human demonstration. The system observes a 8 Chapter 1: Introduction human performing a given task, recognizes the human grasp, and maps it onto an available manipulator.
Reference: [58] <author> S. Kollias and D. Anastassiou, </author> <title> An Adaptive Least Squares Algorithm for the Efficient Training of Artificial Neural Networks, </title> <journal> IEEE Trans. on Circuits and Systems, </journal> <volume> vol. 36, no. 8, </volume> <pages> pp. 1092-101, </pages> <year> 1989. </year>
Reference-contexts: Quickprop [31] incorporates local, second-order information in the weight-update 10 Chapter 1: Introduction algorithm to further speed up learning. Kollias and Anastassious <ref> [58] </ref> propose applying the Marquardt-Levenberg least squares optimization method, which utilizes an approximation of the Hessian matrix.
Reference: [59] <author> K. Kosuge, T. Fukuda and H. Asada, </author> <title> Acquisition of Human Skills for Robotic Systems, </title> <booktitle> Proc. IEEE Int. Symp. on Intelligent Control, </booktitle> <pages> pp. 469-74, </pages> <year> 1991. </year>
Reference-contexts: The system observes a 8 Chapter 1: Introduction human performing a given task, recognizes the human grasp, and maps it onto an available manipulator. In other words, a sequence of camera images, observing the human demonstration, is automatically partitioned into meaningful temporal segments. Kosuge, et. al. <ref> [59] </ref> also abstract high-level assembly skill from human demonstration data. The high-level sequence of motion is decomposed into discrete state transitions, based on contact states during assembly. In each state, compliant motion control implements the corresponding low level control.
Reference: [60] <author> U. Kramer, </author> <title> On the Application of Fuzzy Sets to the Analysis of the System-Driver-Vehicle Environment, </title> <journal> Automatica, </journal> <volume> vol. 21, no. 1, </volume> <pages> pp. 101-7, </pages> <year> 1985. </year>
Reference-contexts: In fuzzy control schemes [63, 64], human experts are asked to specify if-then control rules, with fuzzy linguistic variables (e.g. hot, cold, etc.), which they believe guide their control actions. For example, simple human-in-the-loop control models based on fuzzy modeling have been demonstrated for automobile steering <ref> [60] </ref> and ships helmsmen [116]. Although fuzzy control systems are well suited for simple control tasks with few inputs and outputs, they do not scale well to the high-dimensional input spaces required in modeling human control strategy [8].
Reference: [61] <author> A. Kundu, G. C. Chen and C. E. </author> <title> Persons, Transient Sonar Signal Classification Using Hidden Markov Models and Neural Nets, </title> <journal> IEEE Jour. Oceanic Engineering, </journal> <volume> vol. 19, no. 1, </volume> <pages> pp. 87-99, </pages> <year> 1994. </year>
Reference-contexts: As such, they have been applied for a variety of stochastic signal processing. In speech recognition, where HMMs have found their widest application, human auditory signals are analyzed as speech patterns [51, 94]. Transient sonar signals are classified with HMMs for ocean surveillance in <ref> [61] </ref>. Radons, et. al. [96] analyze 30-electrode neuronal spike activity in a monkeys visual cortex with HMMs. Hannaford and Lee [45] classify task structure in tele-operation based on HMMs. In [123, 124], HMMs are used to characterize sequential images of human actions.
Reference: [62] <author> K. Y. Kupeev and H. J. Wolfson, </author> <title> On Shape Similarity, </title> <booktitle> Proc. of 12th IAPR Int. Conf. on Pattern Recognition, </booktitle> <volume> vol. 1, </volume> <pages> pp. 227-31, </pages> <year> 1994. </year>
Reference-contexts: Therefore, we require a stochastic similarity measure, with sufficient representational power and exibility to compare multi-dimensional, stochastic trajectories. 6.2 Stochastic similarity measure Similarity measures or metrics have been given considerable attention in computer vision [12, 20, 121], image database retrieval [54], and 2D or 3D shape analysis <ref> [62, 107] </ref>.
Reference: [63] <author> C. C. Lee, </author> <title> Fuzzy Logic in Control Systems: Fuzzy Logic Controller Part I, </title> <journal> IEEE Trans. on Systems, Man and Cybernetics, </journal> <volume> vol. 20, no. 2, </volume> <pages> pp. 404-18, </pages> <year> 1990. </year>
Reference-contexts: These modeling efforts generally focussed on simple tracking tasks, where the human is most often modeled as a simple time delay in the overall human-machine system [105]. 6 Chapter 1: Introduction More recently, work has been done towards learning more advanced skills directly from humans. In fuzzy control schemes <ref> [63, 64] </ref>, human experts are asked to specify if-then control rules, with fuzzy linguistic variables (e.g. hot, cold, etc.), which they believe guide their control actions. For example, simple human-in-the-loop control models based on fuzzy modeling have been demonstrated for automobile steering [60] and ships helmsmen [116].
Reference: [64] <author> C. C. Lee, </author> <title> Fuzzy Logic in Control Systems: Fuzzy Logic Controller Part II, </title> <journal> IEEE Trans. on Systems, Man and Cybernetics, </journal> <volume> vol. 20, no. 2, </volume> <pages> pp. 419-35, </pages> <year> 1990. </year>
Reference-contexts: These modeling efforts generally focussed on simple tracking tasks, where the human is most often modeled as a simple time delay in the overall human-machine system [105]. 6 Chapter 1: Introduction More recently, work has been done towards learning more advanced skills directly from humans. In fuzzy control schemes <ref> [63, 64] </ref>, human experts are asked to specify if-then control rules, with fuzzy linguistic variables (e.g. hot, cold, etc.), which they believe guide their control actions. For example, simple human-in-the-loop control models based on fuzzy modeling have been demonstrated for automobile steering [60] and ships helmsmen [116].
Reference: [65] <author> C. Lee, </author> <title> Transferring Human Skills to Robots via Task Demonstrations in Virtual Environments, </title> <type> Ph.D. Thesis Proposal, </type> <institution> Carnegie Mellon University, </institution> <year> 1997. </year> <note> 190 Bibliography </note>
Reference-contexts: Yamato, et. al. [123, 124] also train HMMs to recognize open-loop human actions. 1.2 Related work 7 Friedrich, et. al. [36] and Kaiser [56] review programming by demonstration and skill acquisition via human demonstration for elementary robot skills. Lee <ref> [65] </ref> investigates human-to-robot skill transfer through demonstration of task performance in a virtual environments. Voyles, et. al. [119] program a robot manipulator through human demonstration and abstraction of gesture primitives. Delson and West [28] and Ude [118] both learn open-loop robot trajectories from human demonstration.
Reference: [66] <author> S. Lee and J. Chen, </author> <title> Skill Learning from Observations, </title> <booktitle> Proc. IEEE Int. Conf. on Robotics and Automation, </booktitle> <volume> vol. 4, </volume> <pages> pp. 3245-250, </pages> <year> 1994. </year>
Reference-contexts: In [5, 70, 106], the same deburring robot is controlled through an associative neural network which maps process parameter features to action parameters from human control data. The proper tool feed rate is determined from the burr characteristics of the current process. Lee and Chen <ref> [66] </ref> use feasible state transition graphs through self-organizing data clusters to abstract skill from human data. Skills are modeled as optimal sequences of one-step state transitions that transform the current state into the goal state. The approach is verified on demonstrated human Cartesian teleoperation skill.
Reference: [67] <author> S. Lee and M. H. Kim, </author> <title> Cognitive Control of Dynamic Systems, </title> <booktitle> Proc. IEEE Int. Symp. on Intelligent Control, </booktitle> <pages> pp. 455-60, </pages> <year> 1987. </year>
Reference-contexts: One of the advantages of learning human control strategy directly from human control data is that we avoid the need for this type of state space search to find a suitable control strategy. Lee and Kim <ref> [67] </ref> have proposed and verified an inductive learning scheme, where control rules are learned from examples of perception/action data through hypothesis generation and testing.
Reference: [68] <author> S. Lee and M. H. Kim, </author> <title> Learning Expert Systems for Robot Fine Motion Control, </title> <booktitle> Proc. IEEE Int. Symp. on Intelligent Control, </booktitle> <pages> pp. 534-44, </pages> <year> 1988. </year>
Reference-contexts: Lee and Kim [67] have proposed and verified an inductive learning scheme, where control rules are learned from examples of perception/action data through hypothesis generation and testing. Their learning paradigm, Expert Assisted Robot Skill Acquisition (EARSA) <ref> [68] </ref>, consists of two steps: (1) skill acquisition from human expert rules, and (2) skill discovery or refinement through hypothesis generation and testing. 1.2.2 Skill modeling from human data Interest in modeling human control goes all the way back to World War II, when engineers and psychologists attempted to improve the
Reference: [69] <author> Y. Linde, A. Buzo and R. M. Gray, </author> <title> An Algorithm for Vector Quantizer Design, </title> <journal> IEEE Trans. Communication, </journal> <volume> vol. COM-28, no. 1, </volume> <pages> pp. 84-95, </pages> <year> 1980. </year>
Reference-contexts: For a particular data set let, , (5-14) denote the normalized model input vector at time step corresponding to control action , where are defined in equation (2-26). Also, let be a matrix, whose rows are the vectors. Using the LBG VQ algorithm <ref> [69] </ref>, we generate a codebook of size that minimizes the quantization distortion defined in equation (6-67). <p> We choose the well known LBG vector quantization (VQ) algorithm <ref> [69] </ref> to perform this quantization. Figure 6-10 illustrates the algorithm, which generates code-books of size , , and can be stopped at an appropriate level of dis-cretization given the amount of available data and the complexity of the system trajectories.
Reference: [70] <author> S. Liu and H. Asada, </author> <title> Transferring Manipulative Skills to Robots: Representation and Acquisition of Tool Manipulative Skills Using a Process Dynamics Model, </title> <journal> Trans. ASME Journal of Dynamic Systems, Measurement, and Control, </journal> <volume> vol. 114, </volume> <pages> pp. 220-8, </pages> <year> 1992. </year>
Reference-contexts: Expert linguistic rules are acquired directly from a human expert to partition the control space. For each region, a corresponding linear control law is derived from the numeric demonstration data by the human expert. In <ref> [5, 70, 106] </ref>, the same deburring robot is controlled through an associative neural network which maps process parameter features to action parameters from human control data. The proper tool feed rate is determined from the burr characteristics of the current process.
Reference: [71] <author> M. C. Mackey and L. Glass, </author> <title> Oscillations and Chaos in Physiological Control Systems, </title> <journal> Science, </journal> <volume> vol. 197, no. 4300, </volume> <pages> pp. 287-9, </pages> <year> 1977. </year>
Reference-contexts: We use a 2500-length sequence for training, another 2500-length sequence for cross validation, and another 5000-length sequence for testing. The variables , and are initialized to zero. Finally, our last two problems are taken once again from [33]. Here, we want to predict the chaotic Mackey-Glass time series <ref> [71] </ref>, widely studied in the literature and described by, f ( x 0 x 20&lt; f x 1 x 2 x 3 x 4 x 5 , , , ,[ ] 1 x 3 2 + + x k ( ) 1 x k ( ) 1&lt; &lt; 38 Chapter 3:
Reference: [72] <author> D. T. McRuer and E. S. Krendel, </author> <title> Human Dynamics in Man-Machine Systems, </title> <journal> Auto-matica, </journal> <volume> vol. 16, no. 3, </volume> <pages> pp. 237-53, </pages> <year> 1980. </year>
Reference-contexts: Early research in modeling human control is based on the control-theory paradigm <ref> [72] </ref>, which attempts to model the human-in-the-loop as a simple feedback control system.
Reference: [73] <author> W. T. Miller, R. S. Sutton and P. I. Werbos, eds., </author> <title> Neural Networks For Control, </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1990. </year>
Reference-contexts: Methods in intelligent control include fuzzy logic control, neural network control, reinforcement learning, and locally weighted learning for control. Neural networks are used to map unknown nonlinear functions and have been applied in control most commonly for dynamic system identification and parameter adaptive control <ref> [8, 73, 79] </ref>. Locally weighted learning presents an alternative to neural networks, and maps unknown functions through local statistical regression of the experimental data, rather than through a functional representation [9, 10]. In reinforcement learning, an appropriate control strategy is found t (sec) the same road and simulation parameters. <p> Hov-land, et. al. [50] encode human assembly skill with Hidden Markov Models. In [85], neural networks encode simple pick-and-place skill primitives from human demonstrations. 1.2.3 Neural network learning Interest and research in neural network-based learning for control has exploded in recent years. References <ref> [3, 4, 19, 52, 73, 99] </ref> provide good overviews of neural network control over a broad range of applications. Most often, the role of the neural network in control is restricted to modeling either the nonlinear plant or some nonlinear feedback controller. <p> In Chapter 4, we will then investigate the proposed learning architecture for abstracting HCS models. 3.1 Motivation In recent years, artificial neural networks have shown great promise in identifying complex nonlinear mappings from observed data and have found many applications in robotics and other nonlinear control problems <ref> [8, 73, 79] </ref>. As such, they have received a great deal of attention in the learning community.
Reference: [74] <author> A. Modjtahedzadeh and R. A. Hess, </author> <title> A Model of Driver Steering Control Behavior for Use in Assessing Vehicle Handling Qualities, </title> <journal> Trans. ASME Journal of Dynamic Systems, Measurement, and Control, </journal> <volume> vol. 115, no. 3, </volume> <pages> pp. 456-64, </pages> <year> 1993. </year>
Reference-contexts: These preprocessed inputs include the cars yaw angle with respect to the road, the instantaneous and time-averaged road curvature, and the instantaneous and time-averaged lateral offset. Driving data is again collected from a human operator. In <ref> [74] </ref>, the authors provide a control theoretic model of human driver steering control. Finally, Pentland and Liu [86] apply HMMs towards inferring a particular drivers high-level intentions, such as turning and stopping. Other, higher level skills have also been abstracted from human performance data.
Reference: [75] <author> J. Moody and C. Darken, </author> <title> Fast Learning in Networks of Locally Tuned Processing Units, </title> <journal> Neural Computation, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 281-94, </pages> <year> 1989. </year> <note> Bibliography 191 </note>
Reference-contexts: Atkeson, et. al. [9] offer an excellent overview of locally weighted learning, while [10] addresses control-specific issues. Locally weighted regression is one instance of locally weighted learning and is similar in approach to CMAC [1] and RBF <ref> [75] </ref> neural networks in that local (linear) models are fit to nearby data. All the data is explicitly stored and organized in efficient data structures (such as k-d trees [17] or Bump trees [84], for example).
Reference: [76] <author> A. W. Moore and C. G. Atkeson, </author> <title> Prioritized Sweeping: Reinforcement Learning with Less Data and Less Real Time, </title> <journal> Machine Learning, </journal> <volume> vol 13, no. 1, </volume> <pages> pp. 103-30, </pages> <year> 1993. </year>
Reference-contexts: Below, we describe previous work relating to each of these methods. 1.2.1 Skill learning through exploration Learning skill through exploration has become a popular paradigm for acquiring robotic skills. In reinforcement learning <ref> [14, 76, 115, 120] </ref>, data is not given as direct input/output data points; rather data is specified by an input vector and an associated (scalar) reward from the environment. This reward represents the reinforcement signal, and is akin to learning with a critic as opposed to learning with a teacher.
Reference: [77] <author> M. C. Mozer and P. Smolensky, </author> <title> Skeletonization: A Technique for Trimming the Fat From a Network Via Relevance Assessment, </title> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <editor> D. S. Touretzky, ed., </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> pp. 107-15, </pages> <year> 1989. </year>
Reference-contexts: These approaches can be divided into (1) destructive and (2) constructive algorithms. In destructive algorithms, oversized feedforward models are trained first, and then, after learning has been completed, unimportant weights, based on some relevancy criteria are pruned from the network. See, for example <ref> [18, 22, 23, 44, 77, 78, 117] </ref>. In constructive algorithms, on the other hand, neural networks are initialized in some minimal configuration and additional hidden units are added as the learning requires.
Reference: [78] <author> T. M. Nabhan and A. Y. Zomaya, </author> <title> Toward Generating Neural Network Structures for Function Approximation, </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 7, no. 1, </volume> <pages> pp. 89-99, </pages> <year> 1994. </year>
Reference-contexts: These approaches can be divided into (1) destructive and (2) constructive algorithms. In destructive algorithms, oversized feedforward models are trained first, and then, after learning has been completed, unimportant weights, based on some relevancy criteria are pruned from the network. See, for example <ref> [18, 22, 23, 44, 77, 78, 117] </ref>. In constructive algorithms, on the other hand, neural networks are initialized in some minimal configuration and additional hidden units are added as the learning requires.
Reference: [79] <author> K. S. Narendra and K. Parthasarathy, </author> <title> Identification and Control of Dynamical Systems Using Neural Networks, </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 4-27, </pages> <year> 1990. </year>
Reference-contexts: Methods in intelligent control include fuzzy logic control, neural network control, reinforcement learning, and locally weighted learning for control. Neural networks are used to map unknown nonlinear functions and have been applied in control most commonly for dynamic system identification and parameter adaptive control <ref> [8, 73, 79] </ref>. Locally weighted learning presents an alternative to neural networks, and maps unknown functions through local statistical regression of the experimental data, rather than through a functional representation [9, 10]. In reinforcement learning, an appropriate control strategy is found t (sec) the same road and simulation parameters. <p> Because human control strategy is dynamic, we must map that dynamic system (i.e. the human control strategy) onto a static map. In general, we can approximate any dynamic system through the difference equation <ref> [79] </ref>, (2-23) where is some (possibly nonlinear) map, is the control vector, is the system state vector, and is a vector describing the external environment at time step k. The order of the dynamic system is given by the constants and , which may be infinite. <p> In Chapter 4, we will then investigate the proposed learning architecture for abstracting HCS models. 3.1 Motivation In recent years, artificial neural networks have shown great promise in identifying complex nonlinear mappings from observed data and have found many applications in robotics and other nonlinear control problems <ref> [8, 73, 79] </ref>. As such, they have received a great deal of attention in the learning community. <p> Our third problem (C) is taken from <ref> [79, 92] </ref>. We want to model the following dynamic system, (3-31) where, (3-32) and the input is randomly generated in the interval . We use a 2500-length sequence for training, another 2500-length sequence for cross validation, and another 5000-length sequence for testing. The variables , and are initialized to zero.
Reference: [80] <author> M. C. Nechyba and Y. Xu, </author> <title> Learning and Transfer of Human Real-Time Control Strategies, </title> <journal> Journal of Advanced Computational Intelligence, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 137-54, </pages> <year> 1997. </year>
Reference-contexts: This not only frees us from an a priori choice of network architecture, but also allows new hidden units to assume variable activation functions <ref> [80, 81] </ref>. That is, each hidden units activation function no longer need be confined to just a sig-moidal nonlinearity. <p> We can relax these assumptions further by allowing new hidden units to have variable activation functions <ref> [80, 81] </ref>. In fact, Cybenko [27] shows that sigmoidal functions are not the only possible activation functions which allow for universal function approximation. There are other nonlinear functions, such as sine and cosine for example, which are complete in the space of n-dimensional continuous functions. <p> Hence, the unit with the most appropriate activation function at that point during training is selected. Typical alternatives to the sigmoidal activation function are the Gaussian function, Bessel functions, and sinusoidal functions of various frequency <ref> [80] </ref>. 3.3 Node-decoupled extended Kalman filtering While quickprop is an improvement over the standard backpropagation algorithm for adjusting the weights in the cascade network, it can still require many iterations until satisfactory convergence is reached [31, 108]. <p> Therefore, we try two different networks for method Fk one with sigmoidal activation functions, and the other with sinusoidal activation functions. In <ref> [80] </ref>, we have shown that neural networks with sinusoidal activation functions x t ( ) 1 x t t ( ) 10 + a 0.2= b 0.1= t 17= x ( x t ( ) x t k+( ) t 200= t 699= t 5000= t 5499= 3.4 Comparison experiments 39 <p> Apprentice Expert HCS model HCS model HCS model HCS model 9.2 Future work 159 In <ref> [80] </ref>, we demonstrate the viability of applying HCS models towards human-to-human skill transfer for a simple inverted-pendulum system (see Figure 9-2).
Reference: [81] <author> M. C. Nechyba and Y. Xu, </author> <title> Neural Network Approach to Control System Identification with Variable Activation Functions, </title> <booktitle> Proc. IEEE Int. Symp. on Intelligent Control, </booktitle> <volume> vol. 1, </volume> <pages> pp. 358-63, </pages> <year> 1994. </year>
Reference-contexts: This not only frees us from an a priori choice of network architecture, but also allows new hidden units to assume variable activation functions <ref> [80, 81] </ref>. That is, each hidden units activation function no longer need be confined to just a sig-moidal nonlinearity. <p> We can relax these assumptions further by allowing new hidden units to have variable activation functions <ref> [80, 81] </ref>. In fact, Cybenko [27] shows that sigmoidal functions are not the only possible activation functions which allow for universal function approximation. There are other nonlinear functions, such as sine and cosine for example, which are complete in the space of n-dimensional continuous functions.
Reference: [82] <author> S. Neuser, J. Nijhuis, et. al., </author> <title> Neurocontrol for Lateral Vehicle Guidance, </title> <journal> IEEE Micro, </journal> <volume> vol. 13, no. 1, </volume> <pages> pp. 57-66, </pages> <year> 1993. </year>
Reference-contexts: The system has been demonstrated successfully at speeds up to 70 mi/h. Subsequently, a statistical algorithm called RALPH [88] has been developed for calculating the road curvature and lateral offset from the road median. Neuser, et. al. <ref> [82] </ref> control the steering of an autonomous vehicle through preprocessed inputs to a single-layer feed-forward neural network. These preprocessed inputs include the cars yaw angle with respect to the road, the instantaneous and time-averaged road curvature, and the instantaneous and time-averaged lateral offset.
Reference: [83] <author> D. OHare and S. Roscoe, </author> <title> Flight Deck Performance: The Human Factor, </title> <institution> Iowa State University Press, Ames, </institution> <year> 1990. </year>
Reference-contexts: Consider, for example, the tasks of teleoperating robots in remote environments or learning to y a high-performance jet. Training for both of these tasks is difficult, expensive, and time consuming for a novice <ref> [83, 104] </ref>. We can accelerate learning for the novice operator by providing on-line feedback from virtual teachers in the form of skill models, which capture the control strategies of expert operators.
Reference: [84] <author> S. Omohundro, </author> <title> Bumptrees for Efficient Function, Constraint, and Classification Learning, </title> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <editor> R. P. Lippmann, J. E. Moody and D. S. Touretzky, eds, </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> pp. 693-9, </pages> <year> 1991. </year> <note> 192 Bibliography </note>
Reference-contexts: All the data is explicitly stored and organized in efficient data structures (such as k-d trees [17] or Bump trees <ref> [84] </ref>, for example). When the model is queried for the output at a specified input vector, points in the database near that input vector are used to construct a local linear map. Locally weighted regression offers several advantages over global learning paradigms (such as neural networks) [10].
Reference: [85] <author> E. H. Park, et. al., </author> <title> Adaptive Learning of Human Motion by a Telerobot Using a Neural Network Model as a Teacher, </title> <journal> Computer and Industrial Engineering, </journal> <volume> vol. 27, </volume> <pages> pp. 453-6, </pages> <year> 1994. </year>
Reference-contexts: The high-level sequence of motion is decomposed into discrete state transitions, based on contact states during assembly. In each state, compliant motion control implements the corresponding low level control. Hov-land, et. al. [50] encode human assembly skill with Hidden Markov Models. In <ref> [85] </ref>, neural networks encode simple pick-and-place skill primitives from human demonstrations. 1.2.3 Neural network learning Interest and research in neural network-based learning for control has exploded in recent years. References [3, 4, 19, 52, 73, 99] provide good overviews of neural network control over a broad range of applications.
Reference: [86] <author> A. Pentland and A. Liu, </author> <title> Toward Augmented Control Systems, </title> <booktitle> Proc. Intelligent Vehicles, </booktitle> <volume> vol. 1, </volume> <pages> pp. 350-5, </pages> <year> 1995. </year>
Reference-contexts: Driving data is again collected from a human operator. In [74], the authors provide a control theoretic model of human driver steering control. Finally, Pentland and Liu <ref> [86] </ref> apply HMMs towards inferring a particular drivers high-level intentions, such as turning and stopping. Other, higher level skills have also been abstracted from human performance data. Kang [57], for example, teaches a robot assembly through human demonstration.
Reference: [87] <author> D. Plaut, S. Nowlan and G. Hinton, </author> <title> Experiment on Learning by Backpropagation, </title> <type> Technical Report, </type> <institution> CMU-CS-86-126, Carnegie Mellon University, </institution> <year> 1986. </year>
Reference-contexts: Since this training method was first proposed [98], modifications to standard backpropagation, as well as other training algorithms have been suggested. An adaptive learning rate [24], as well as an additive momentum term <ref> [87] </ref> are both somewhat effective in accelerating convergence of backpropagation in at regions of the error hyper-surface. Quickprop [31] incorporates local, second-order information in the weight-update 10 Chapter 1: Introduction algorithm to further speed up learning.
Reference: [88] <author> D. A. Pomerleau and T. Jochem, </author> <title> Rapidly Adapting Machine Vision for Automated Vehicle Steering, </title> <journal> IEEE Expert, </journal> <volume> vol. 11, no. 2, </volume> <pages> pp. 19-27, </pages> <year> 1996. </year>
Reference-contexts: The system has been demonstrated successfully at speeds up to 70 mi/h. Subsequently, a statistical algorithm called RALPH <ref> [88] </ref> has been developed for calculating the road curvature and lateral offset from the road median. Neuser, et. al. [82] control the steering of an autonomous vehicle through preprocessed inputs to a single-layer feed-forward neural network. <p> This representation is reasonable, since computer vision algorithms such as RALPH <ref> [88] </ref> can abstract a very similar representation from real camera images. (a) (b) G u k ( ) u k 1 ( ) u k n u 1+( ) x k ( ) x k 1 ( ) x k n x 1+( ) z k ( ), , , ,
Reference: [89] <author> D. A. Pomerleau, </author> <title> Neural Network Perception for Mobile Robot Guidance, </title> <type> Ph.D. Thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <year> 1992. </year>
Reference-contexts: Several approaches to skill learning in human driving have been implemented. In [34, 35], neural networks are trained to mimic human behavior for a simulated, circular racetrack. The task essentially involves avoiding other computer-generated cars; no dynamics are modeled or considered in the approach. Pomerleau <ref> [89, 90] </ref> implements real-time road-following with data collected from a human driver. A static feedforward neural network with a single hidden layer, ALVINN, learns to map coarsely digitized camera images of the road ahead to a desired steering direction, whose reliability is given through an input-reconstruction reliability estimator.
Reference: [90] <author> D. A. Pomerleau, </author> <title> Reliability Estimation for Neural Network Based Autonomous Driving, </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> vol. 12, no. </volume> <pages> 3-4, pp. 113-9, </pages> <year> 1994. </year>
Reference-contexts: Several approaches to skill learning in human driving have been implemented. In [34, 35], neural networks are trained to mimic human behavior for a simulated, circular racetrack. The task essentially involves avoiding other computer-generated cars; no dynamics are modeled or considered in the approach. Pomerleau <ref> [89, 90] </ref> implements real-time road-following with data collected from a human driver. A static feedforward neural network with a single hidden layer, ALVINN, learns to map coarsely digitized camera images of the road ahead to a desired steering direction, whose reliability is given through an input-reconstruction reliability estimator.
Reference: [91] <author> W. H. Press, et. al., </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing, 2nd ed., </booktitle> <publisher> Cambrige University Press, </publisher> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: For these input space parameters, the input training vectors are of length 50. Since it is impossible to visualize a 50-dimensional input space, we decompose each of the input vectors in the training set into the principal components (PCs) <ref> [91] </ref> over Grouchos entire data set, such that, 3. We will formalize this qualitative observation in Chapter 8. v init mi/h ( ) r ( . <p> 6 The Discrete Fourier Transform maps a k-length real vector to a k-length complex vector and is defined as, , where (6-44) , . (6-45) Prior to applying the Fourier transform, we filter each frame through a Hamming window in order to minimize spectral leakage caused by the data windowing <ref> [91] </ref>. The Hamming transform maps a k-length real vector to a k-length real vector and is defined as, , where (6-46) , (see Figure 6-7) (6-47) For notational convenience let . Instead of sinusoidal basis functions, the Discrete Walsh Transform decomposes a signal based on the orthonormal Walsh functions [97].
Reference: [92] <author> G. V. Puskorius and L. A. Feldkamp, </author> <title> Decoupled Extended Kalman Filter Training of Feedforward Layered Networks, </title> <booktitle> Proc. Int. Joint Conf. on Neural Networks, </booktitle> <volume> vol. 1, </volume> <pages> pp. 771-7, </pages> <year> 1991. </year>
Reference-contexts: Quickprop [31] incorporates local, second-order information in the weight-update 10 Chapter 1: Introduction algorithm to further speed up learning. Kollias and Anastassious [58] propose applying the Marquardt-Levenberg least squares optimization method, which utilizes an approximation of the Hessian matrix. Extended Kalman filtering <ref> [92, 108] </ref>, where the weights in the neural network are viewed as states in a discrete-time finite dimensional system, outperform the previously mentioned algorithms in terms of learning speed and error convergence [92]. <p> Extended Kalman filtering [92, 108], where the weights in the neural network are viewed as states in a discrete-time finite dimensional system, outperform the previously mentioned algorithms in terms of learning speed and error convergence <ref> [92] </ref>. <p> Thus, in this chapter we first review how learning proceeds in cascade neural networks. We then show how NDEKF fits seamlessly into the cascade learning framework, and how cascade learning addresses the poor local minima problem of NDEKF reported in <ref> [92] </ref>. We analyze the computational complexity of our approach and compare it to fixed-architecture training paradigms. <p> GEKFs computational complexity is , where is the number of weights in the neural network. This is prohibitive, even for moderately sized neural networks, where the weights can easily number in the thousands. To address this problem, Puskorius and Feldkamp <ref> [92] </ref>, propose node-decoupled extended Kal-man filtering (NDEKF), which considers only the pairwise interdependence of weights feeding into the same node, rather than the interdependence of all the weights in the network. <p> In this chapter we show that combining cascade neural networks with NDEKF solves the problem of poor local minima reported in <ref> [92] </ref>, and that the resulting learning architecture substantially outperforms other neural network training paradigms in learning speed and/or error convergence for learning tasks important in control problems. Below, we first describe how learning proceeds in cascade neural networks. <p> Thus, we modify standard cascade learning by replacing the quick-prop algorithm with node-decoupled extended Kalman filtering (NDEKF), which has been shown to have better convergence properties and faster training times than gradient-descent techniques for fixed-architecture multi-layer feedforward networks <ref> [92] </ref>. <p> y k i 0= + = i P k i ( ) T A k y k i f k P 0 z k i P k i n w h Q 32 Chapter 3: Cascade Neural Networks with Kalman Filtering 6) to alleviate singularity problems for error covariance matrices <ref> [92] </ref>. In (3-3) through (3-6), --s, ()s, and []s evaluate to scalars, vectors and matrices, respectively. The vector is easy to compute within the cascade framework. <p> Our third problem (C) is taken from <ref> [79, 92] </ref>. We want to model the following dynamic system, (3-31) where, (3-32) and the input is randomly generated in the interval . We use a 2500-length sequence for training, another 2500-length sequence for cross validation, and another 5000-length sequence for testing. The variables , and are initialized to zero. <p> A fixed-architecture/backprop network failed to converge for this problem, despite many experiments with different learning parameters [33]. d. This is comparable to the result of 0.03 in <ref> [92] </ref> for a network with an equal number of parameters. <p> the authors of the NDEKF algorithm note, NDEKF at times requires a small amount of redundancy in the network in terms of the total number of nodes in order to avoid poor local minima for certain problems, which [they attribute] to high effective learning rates at the onset of training <ref> [92] </ref>. Consider, for example, Figure 3-6. While the minimum for the Fk network is below that of the Cq network, its maximum is much worse than either Ck or Cq.
Reference: [93] <author> S. Qin, H. Su and T. J. McAvoy, </author> <title> Comparison of Four Neural Net Learning Methods for Dynamic System Identification, </title> <journal> IEEE Trans. on Neural Networks, </journal> <volume> vol. 3, no. 1, </volume> <pages> pp. 122-30, </pages> <year> 1992. </year>
Reference-contexts: Compared to static feedforward networks, the learning algorithms for recurrent networks are significantly more computationally involved, requiring relaxation of sets of differential equations [47]. Yet, Qin, et. al. <ref> [93] </ref> show similar error convergence in mapping simple dynamic systems with feedforward and recurrent networks, respectively.
Reference: [94] <author> L. R. Rabiner, </author> <title> A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition, </title> <journal> Proc. IEEE, </journal> <volume> vol. 77, no. 2, </volume> <pages> pp. 257-86, </pages> <year> 1989. </year> <note> Bibliography 193 </note>
Reference-contexts: As such, they have been applied for a variety of stochastic signal processing. In speech recognition, where HMMs have found their widest application, human auditory signals are analyzed as speech patterns <ref> [51, 94] </ref>. Transient sonar signals are classified with HMMs for ocean surveillance in [61]. Radons, et. al. [96] analyze 30-electrode neuronal spike activity in a monkeys visual cortex with HMMs. Hannaford and Lee [45] classify task structure in tele-operation based on HMMs. <p> Section 6.2.7 follows with methods for minimizing the detrimental effects of discretization. A discrete HMM is completely defined by the following triplet <ref> [94] </ref>, (6-8) where A is the probabilistic state transition matrix, B is the output probability matrix with L discrete output symbols , and is the n-length initial state probability distribution vector for the HMM. <p> The following two HMMs are, for example, equivalent, but not identical: , (6-10) Finally, we note that for an observation sequence O of discrete symbols and an HMM , we can locally maximize using the well-known Baum-Welch algorithm (see Section 6.2.8) <ref> [94, 16] </ref>. We can also evaluate using the computationally efficient forward-backward algorithm. 6.2.2 Similarity measure Here, we derive a stochastic similarity measure, based on discrete-output HMMs. Assume that we wish to compare observation sequences from two stochastic processes and . <p> Theoretically, the Baum-Welch algorithm guarantees that is a local maximum only. In practice, this is not a significant concern, however, since the Baum-Welch algorithm converges to near-optimal solutions for discrete-output HMMs, when the algorithm is initialized with random model parameters <ref> [94, 95] </ref>. We have verified this near-optimal convergence property experimentally in two ways. First, for a given set of observation sequences , we trained different HMMs from different initial random parameter settings. We then observed that the probabilities , , were approximately equivalent. <p> Below, we consider two parameterized post-training solutions to this singularity problem within the context of discrete-output HMMs: (1) ooring and (2) semicontinuous evaluation. Flooring <ref> [94] </ref> defines the common practice of replacing nonzero elements in the trained HMMs by some small value and then renormalizing the rows of and the columns of to satisfy the probabilistic constraints in equations (6-87) and (6-88). <p> For a complete discussion of these algorithms, please see <ref> [94] </ref>. B.1 Forward-backward algorithm The forward-backward algorithm is a computationally efficient algorithm for calculating , for a discrete-output Hidden Markov Model with states, and a discrete observation sequence , where, , , . (B-1) It is also the first step of the Baum-Welch algorithm described in Section B.2. <p> The state transition matrix is updated by, , (B-16) while the output probability distribution matrix is updated by, , , (B-17) Rabiner provides an excellent and practical introduction to the Baum-Welch algorithm <ref> [94] </ref>. Unfortunately, in [94] the final equations summarizing the Baum-Welch algorithm namely, equations (110) and (111), corresponding to equations (B-16) and (B-17) are incorrect. c t l A B p, , -= n L P l O k ( ) ( ) K k 1= a ij k i ( )a <p> The state transition matrix is updated by, , (B-16) while the output probability distribution matrix is updated by, , , (B-17) Rabiner provides an excellent and practical introduction to the Baum-Welch algorithm <ref> [94] </ref>. Unfortunately, in [94] the final equations summarizing the Baum-Welch algorithm namely, equations (110) and (111), corresponding to equations (B-16) and (B-17) are incorrect. c t l A B p, , -= n L P l O k ( ) ( ) K k 1= a ij k i ( )a ij b j
Reference: [95] <author> L. R. Rabiner, B. H. Juang, S. E. Levinson and M. M. Sondhi, </author> <title> Some Properties of Continuous Hidden Markov Model Representations, </title> <journal> AT&T Technical Journal, </journal> <volume> vol. 64, no. 6, </volume> <pages> pp. 1211-1222, </pages> <year> 1986. </year>
Reference-contexts: training HMMs with both discrete and continuous output probability distributions, and although most applications of HMMs deal with real-valued signals, discrete HMMs are preferred to continuous or semi-continuous HMMs in practice, due to their relative computational simplicity (orders of magnitude more efficient) and lesser sensitivity to initial random parameter settings <ref> [95] </ref>. In Section 6.2.5 below, we describe how we use discrete HMMs for analysis of real-valued signals by converting the data to discrete symbols through pre-processing and vector quantization. Section 6.2.7 follows with methods for minimizing the detrimental effects of discretization. <p> Theoretically, the Baum-Welch algorithm guarantees that is a local maximum only. In practice, this is not a significant concern, however, since the Baum-Welch algorithm converges to near-optimal solutions for discrete-output HMMs, when the algorithm is initialized with random model parameters <ref> [94, 95] </ref>. We have verified this near-optimal convergence property experimentally in two ways. First, for a given set of observation sequences , we trained different HMMs from different initial random parameter settings. We then observed that the probabilities , , were approximately equivalent.
Reference: [96] <author> G. Radons, J. D. Becker, B. Dulfer and J. Kruger, </author> <title> Analysis, Classification and Coding of Multielectrode Spike Trains with Hidden Markov Models, </title> <journal> Biological Cybernetics, </journal> <volume> vol. 71, no. 4, </volume> <pages> pp. 359-73, </pages> <year> 1994. </year>
Reference-contexts: As such, they have been applied for a variety of stochastic signal processing. In speech recognition, where HMMs have found their widest application, human auditory signals are analyzed as speech patterns [51, 94]. Transient sonar signals are classified with HMMs for ocean surveillance in [61]. Radons, et. al. <ref> [96] </ref> analyze 30-electrode neuronal spike activity in a monkeys visual cortex with HMMs. Hannaford and Lee [45] classify task structure in tele-operation based on HMMs. In [123, 124], HMMs are used to characterize sequential images of human actions.
Reference: [97] <author> K. R. Rao and D. F. Elliott, </author> <title> Fast Transforms: Algorithms, Analyses and Applications, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: The Hamming transform maps a k-length real vector to a k-length real vector and is defined as, , where (6-46) , (see Figure 6-7) (6-47) For notational convenience let . Instead of sinusoidal basis functions, the Discrete Walsh Transform decomposes a signal based on the orthonormal Walsh functions <ref> [97] </ref>. The first eight Walsh-ordered Walsh functions are shown in Figure 6-8 (a). In Figure 6-8 (b), we show an example of human control data which can be characterized better through the Walsh transform, rather than the Fourier transform, due to 6.
Reference: [98] <author> D. E. Rumelhart, J. L. </author> <title> McClelland and the PDP Research Group, Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </title> <booktitle> Volume 1: Foundations, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: As such, we will restrict the remainder of this discussion to feedforward models only. 1.2 Related work 9 Research into feedforward neural networks began in earnest with the publication of the backpropagation algorithm in 1986 <ref> [98] </ref>. Since then a number of different learning architectures have been proposed to adjust the structure of the feedforward neural network (i.e. the number and arrangement of hidden units) as part of learning. These approaches can be divided into (1) destructive and (2) constructive algorithms. <p> As we have already noted, the first of these was the backpropagation algorithm, which implements local gradient descent on the weights during training in order to minimize the sum-of-squared residuals. Since this training method was first proposed <ref> [98] </ref>, modifications to standard backpropagation, as well as other training algorithms have been suggested. An adaptive learning rate [24], as well as an additive momentum term [87] are both somewhat effective in accelerating convergence of backpropagation in at regions of the error hyper-surface.
Reference: [99] <author> T. Samad, </author> <title> Neurocontrol: </title> <booktitle> Concepts and Applications, Proc. IEEE Int. Conf. on Systems, Man, and Cybernetics, </booktitle> <volume> vol. 1, </volume> <pages> pp. 369-74, </pages> <year> 1992. </year>
Reference-contexts: Hov-land, et. al. [50] encode human assembly skill with Hidden Markov Models. In [85], neural networks encode simple pick-and-place skill primitives from human demonstrations. 1.2.3 Neural network learning Interest and research in neural network-based learning for control has exploded in recent years. References <ref> [3, 4, 19, 52, 73, 99] </ref> provide good overviews of neural network control over a broad range of applications. Most often, the role of the neural network in control is restricted to modeling either the nonlinear plant or some nonlinear feedback controller.
Reference: [100] <author> S. Schaal and C. G. Atkeson, </author> <title> Memory-Based Robot Learning, </title> <booktitle> Proc. IEEE Int. Conf. on Robotics and Automation, </booktitle> <volume> vol. 4, </volume> <pages> pp. 2928-33, </pages> <year> 1994. </year>
Reference-contexts: Another learning paradigm, locally weighted learning, has emerged more recently and has shown great success for a number of different control applications, ranging from devil sticking <ref> [100] </ref> to robot juggling [101]. Atkeson, et. al. [9] offer an excellent overview of locally weighted learning, while [10] addresses control-specific issues.
Reference: [101] <author> S. Schaal and C. G. Atkeson, </author> <title> Robot Juggling: Implementation of Memory-Based Learning, </title> <journal> IEEE Control Systems Magazine, </journal> <volume> vol. 14, no. 1, </volume> <pages> pp. 57-71, </pages> <year> 1994. </year>
Reference-contexts: Another learning paradigm, locally weighted learning, has emerged more recently and has shown great success for a number of different control applications, ranging from devil sticking [100] to robot juggling <ref> [101] </ref>. Atkeson, et. al. [9] offer an excellent overview of locally weighted learning, while [10] addresses control-specific issues.
Reference: [102] <author> J. G. Schneider, </author> <title> Robot Skill Learning Through Intelligent Experimentation, </title> <type> Ph.D. Thesis, </type> <institution> School of Computer Science, University of Rochester, </institution> <year> 1995. </year>
Reference-contexts: The reinforcement learning algorithm is expected to explore and learn a suitable control strategy over time. References [43] and [10] give some examples of reinforcement learning control for a robot manipulator and a simulated car in a hole, respectively. Schneider <ref> [102] </ref> learns the open-loop skill of throwing through a search of the parameter space which defines all possible throwing motions.
Reference: [103] <author> W. L. Shebilske and J. W. Regian, </author> <title> Video Games, Training, and Investigating Complex Skills, </title> <booktitle> Proc. Human Factors Society 36th Annual Meeting, </booktitle> <volume> vol. 2, </volume> <pages> pp. 1296-1300, </pages> <year> 1992. </year> <note> 194 Bibliography </note>
Reference-contexts: It has been shown that simulated training (e.g. training on a simulation of the system rather than the real system) still improves performance once the apprentice transitions to control of the real dynamic system <ref> [41, 103] </ref>. Therefore, we would expect that this approach would prove useful, even if for safety reasons we replace the actual system by a simulation of that system during apprentice training. contribute to the learning of a single apprentice (right).
Reference: [104] <author> T. B. Sheridan, </author> <title> Space Teleoperation Through Time Delay: </title> <journal> Review and Prognosis, IEEE Trans. on Robotics and Automation, </journal> <volume> vol. 9, no. 5, </volume> <pages> pp. 592-606, </pages> <year> 1993. </year>
Reference-contexts: Consider, for example, the tasks of teleoperating robots in remote environments or learning to y a high-performance jet. Training for both of these tasks is difficult, expensive, and time consuming for a novice <ref> [83, 104] </ref>. We can accelerate learning for the novice operator by providing on-line feedback from virtual teachers in the form of skill models, which capture the control strategies of expert operators.
Reference: [105] <author> T. B. Sheridan, Telerobotics, </author> <title> Automation, and Human Supervisory Control, </title> <publisher> Cambridge Press, </publisher> <address> Cambrdige, </address> <year> 1992. </year>
Reference-contexts: These modeling efforts generally focussed on simple tracking tasks, where the human is most often modeled as a simple time delay in the overall human-machine system <ref> [105] </ref>. 6 Chapter 1: Introduction More recently, work has been done towards learning more advanced skills directly from humans. In fuzzy control schemes [63, 64], human experts are asked to specify if-then control rules, with fuzzy linguistic variables (e.g. hot, cold, etc.), which they believe guide their control actions.
Reference: [106] <author> K. Shimokura and S. Liu, </author> <title> Programming Deburring Robots Based on Human Demonstration with Direct Burr Size Measurement, </title> <booktitle> Proc. IEEE Int. Conf. on Robotics and Automation, </booktitle> <volume> vol. 1, </volume> <pages> pp. 572-7, </pages> <year> 1994. </year>
Reference-contexts: Expert linguistic rules are acquired directly from a human expert to partition the control space. For each region, a corresponding linear control law is derived from the numeric demonstration data by the human expert. In <ref> [5, 70, 106] </ref>, the same deburring robot is controlled through an associative neural network which maps process parameter features to action parameters from human control data. The proper tool feed rate is determined from the burr characteristics of the current process.
Reference: [107] <author> H. Y. Shum, M. Hebert and K. </author> <title> Ikeuchi, On 3D Shape Similarity, </title> <type> Technical Report, </type> <institution> CMU-CS-95-212, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: Therefore, we require a stochastic similarity measure, with sufficient representational power and exibility to compare multi-dimensional, stochastic trajectories. 6.2 Stochastic similarity measure Similarity measures or metrics have been given considerable attention in computer vision [12, 20, 121], image database retrieval [54], and 2D or 3D shape analysis <ref> [62, 107] </ref>.
Reference: [108] <author> S. Singhal and L. Wu, </author> <title> Training Multilayer Perceptrons with the Extended Kalman Algorithm, </title> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <editor> ed. Touretzky, D. S., </editor> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> pp. 133-40, </pages> <year> 1989. </year>
Reference-contexts: Quickprop [31] incorporates local, second-order information in the weight-update 10 Chapter 1: Introduction algorithm to further speed up learning. Kollias and Anastassious [58] propose applying the Marquardt-Levenberg least squares optimization method, which utilizes an approximation of the Hessian matrix. Extended Kalman filtering <ref> [92, 108] </ref>, where the weights in the neural network are viewed as states in a discrete-time finite dimensional system, outperform the previously mentioned algorithms in terms of learning speed and error convergence [92]. <p> What makes EKF algorithms attractive is that, unlike backpropagation, they explicitly account for the pairwise interdependence of the weights in the neural network during training. By viewing the training of feedforward neural networks as an identification problem for a nonlinear dynamic system, Singhal and Wu <ref> [108] </ref> were the first to show how the EKF algorithm can be used for neural network training. While converging to better local minima in many fewer epochs than backpropagation, their global extended Kalman filtering (GEKF) approach, carries a heavy computational toll. <p> sigmoidal activation function are the Gaussian function, Bessel functions, and sinusoidal functions of various frequency [80]. 3.3 Node-decoupled extended Kalman filtering While quickprop is an improvement over the standard backpropagation algorithm for adjusting the weights in the cascade network, it can still require many iterations until satisfactory convergence is reached <ref> [31, 108] </ref>. Thus, we modify standard cascade learning by replacing the quick-prop algorithm with node-decoupled extended Kalman filtering (NDEKF), which has been shown to have better convergence properties and faster training times than gradient-descent techniques for fixed-architecture multi-layer feedforward networks [92]. <p> As we demonstrate later, the combination of cascade learning and NDEKF alleviates critical problems that each exhibits by itself, and better exploits the main strengths of both algorithms. 3.3.1 Learning architecture In general extended Kalman filtering (GEKF) <ref> [108] </ref>, an conditional error covariance matrix , which stores the interdependence of each pair of weights in a given neural network, is explicitly generated.
Reference: [109] <author> M. Skubic and R. A. Volz, </author> <title> Learning Force Sensory Patterns and Skills From Human Demonstration, </title> <booktitle> Proc. IEEE Int. Conf. on Robotics and Automation, </booktitle> <volume> vol. 1, </volume> <pages> pp. 284-90, </pages> <year> 1997. </year>
Reference-contexts: Lee [65] investigates human-to-robot skill transfer through demonstration of task performance in a virtual environments. Voyles, et. al. [119] program a robot manipulator through human demonstration and abstraction of gesture primitives. Delson and West [28] and Ude [118] both learn open-loop robot trajectories from human demonstration. Skubic and Volz <ref> [109] </ref> transfer force-based assembly skills to robots from human demonstrations. Iba [53] models open-loop sensory motor skills in humans. Gin-grich, et. al. [39] argue that learning human performance models is valuable, but offer results only for simulated, known dynamic systems.
Reference: [110] <author> J. Song, Y. Xu, M. C. Nechyba and Y. Yam, </author> <title> Two Performance Measures for Evaluating Human Control Strategy, </title> <booktitle> to appear in Proc. IEEE Int. Conference on Robotics and Automation, </booktitle> <month> May, </month> <year> 1998. </year>
Reference-contexts: We developed a real-time graphic driving simulator, with dynamic interactions of the simulated car an the environment. This has proven to be a valuable testing tool for the learning algorithms and statistical methods developed herein. Some other researchers have also used this simulator in their work <ref> [29, 110] </ref>. 9.2 Future work While this thesis provides the foundations for modeling and analyzing human control strategies, it is certainly not the first and last word on this topic it is only an important first step. <p> Models or control strategies with different skill qualities may be more or less appropriate for a given situation, depending on the specific performance requirements of an application. For example, Song, et. al. <ref> [110] </ref> have begun to examine the problem of skill evaluation, by proposing two task-specific performance criteria for the human driving task, including a (1) tight-turning and (2) obstacle avoidance criterion.
Reference: [111] <author> J. Song, Y. Xu, Y. Yam and M. C. Nechyba, </author> <title> Optimization of Human Control Strategies with Simultaneously Perturbed Stochastic Approximation, </title> <booktitle> submitted to Proc. IEEE Int. Conference on Intelligent Robots and Systems, </booktitle> <month> October, </month> <year> 1998. </year>
Reference-contexts: Initial experiments with SPSA, for example, demonstrate that learned models of human control strategy can be improved with respect to specific performance criteria <ref> [111] </ref>. Another area of application for HCS models might be as virtual expert instructors. A novice, when faced with learning an appropriate control strategy for a new task, is generally faced with two alternatives.
Reference: [112] <author> L. G. Sotelino, M. Saerens and H. Bersini, </author> <title> Classification of Temporal Trajectories by Continuous-Time Recurrent Nets, </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 7, no. 5, </volume> <pages> pp. 767-76, </pages> <year> 1994. </year> <note> Bibliography 195 </note>
Reference-contexts: Other work has focussed on classifying temporal patterns using Bayesian statistics [30], wavelet and spectral analysis [114], neural networks (both feedforward and recurrent) <ref> [47, 112] </ref>, and Hidden Markov Models (see discussion below). Much of this work, however, analyzes only short-time trajectories or patterns, and, in many cases, generates only a binary classification, rather than a continuously valued similarity measure.
Reference: [113] <author> J. C. Spall, </author> <title> Multivariate Stochastic Approximation Using a Simultaneous Perturbation Gradient Approximation, </title> <journal> IEEE Trans. on Automation Control, </journal> <volume> vol. 37, no. 3, </volume> <pages> pp. 332-41, </pages> <year> 1992. </year>
Reference-contexts: Since, in general, we do not have an explicit representation for in terms of the model parameters , model optimization can be achieved through one of a number of different stochastic algorithms, including simultaneously perturbed stochastic approximation (SPSA) <ref> [113] </ref>, population-based incremental learning (PBIL) [11] and genetic optimization [40]. Initial experiments with SPSA, for example, demonstrate that learned models of human control strategy can be improved with respect to specific performance criteria [111]. Another area of application for HCS models might be as virtual expert instructors.
Reference: [114] <author> M. Sun, G. Burk and R. J. Sclabassi, </author> <title> Measurement of Signal Similarity Using the Maxima of the Wavelet Transform, </title> <booktitle> Proc. IEEE Int. Conf. on Acoustics, Speech, and Signal Processing, </booktitle> <volume> vol. 3, </volume> <pages> pp. 583-586, </pages> <year> 1993. </year>
Reference-contexts: Other work has focussed on classifying temporal patterns using Bayesian statistics [30], wavelet and spectral analysis <ref> [114] </ref>, neural networks (both feedforward and recurrent) [47, 112], and Hidden Markov Models (see discussion below). Much of this work, however, analyzes only short-time trajectories or patterns, and, in many cases, generates only a binary classification, rather than a continuously valued similarity measure.
Reference: [115] <author> R. S. Sutton, </author> <title> Learning to Predict by the Methods of Temporal Differences, </title> <journal> Machine Learning, </journal> <volume> vol. 3, no. 1, </volume> <pages> pp. 9-44, </pages> <year> 1988. </year>
Reference-contexts: Below, we describe previous work relating to each of these methods. 1.2.1 Skill learning through exploration Learning skill through exploration has become a popular paradigm for acquiring robotic skills. In reinforcement learning <ref> [14, 76, 115, 120] </ref>, data is not given as direct input/output data points; rather data is specified by an input vector and an associated (scalar) reward from the environment. This reward represents the reinforcement signal, and is akin to learning with a critic as opposed to learning with a teacher.
Reference: [116] <author> R. Sutton and D. R. Towill, </author> <title> Modeling the Helmsan in a Ship Steering System Using Fuzzy Sets, Analysis, Design and Evaluation of Man-Machine Systems: </title> <booktitle> Selected Papers from the Third IFAC Conference, </booktitle> <volume> vol. 1, </volume> <pages> pp. 157-62, </pages> <year> 1988. </year>
Reference-contexts: For example, simple human-in-the-loop control models based on fuzzy modeling have been demonstrated for automobile steering [60] and ships helmsmen <ref> [116] </ref>. Although fuzzy control systems are well suited for simple control tasks with few inputs and outputs, they do not scale well to the high-dimensional input spaces required in modeling human control strategy [8]. Robot learning from human experts has also been applied to a deburring robot.
Reference: [117] <author> H. H. Thodberg, </author> <title> Improving Generalization of Neural Networks Through Pruning, </title> <journal> Int. Journal of Neural Systems, </journal> <volume> vol. 1, no. 4, </volume> <pages> pp. 317-26, </pages> <year> 1991. </year>
Reference-contexts: These approaches can be divided into (1) destructive and (2) constructive algorithms. In destructive algorithms, oversized feedforward models are trained first, and then, after learning has been completed, unimportant weights, based on some relevancy criteria are pruned from the network. See, for example <ref> [18, 22, 23, 44, 77, 78, 117] </ref>. In constructive algorithms, on the other hand, neural networks are initialized in some minimal configuration and additional hidden units are added as the learning requires.
Reference: [118] <author> A. Ude, </author> <title> Trajectory Generation from Noisy Positions of Object Features for Teaching Robot Paths, </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> vol. 11, no. 2, </volume> <pages> pp. 113-27, </pages> <year> 1993. </year>
Reference-contexts: Lee [65] investigates human-to-robot skill transfer through demonstration of task performance in a virtual environments. Voyles, et. al. [119] program a robot manipulator through human demonstration and abstraction of gesture primitives. Delson and West [28] and Ude <ref> [118] </ref> both learn open-loop robot trajectories from human demonstration. Skubic and Volz [109] transfer force-based assembly skills to robots from human demonstrations. Iba [53] models open-loop sensory motor skills in humans.
Reference: [119] <author> R. M. Voyles, J. D. Morrow and P. K. Khosla, </author> <title> Towards Gesture-Based Programming: Shape from Motion Primordial Learning of Sensorimotor Primitives, </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> vol. 22, no. </volume> <pages> 3-4, pp. 361-75, </pages> <year> 1997. </year>
Reference-contexts: Lee [65] investigates human-to-robot skill transfer through demonstration of task performance in a virtual environments. Voyles, et. al. <ref> [119] </ref> program a robot manipulator through human demonstration and abstraction of gesture primitives. Delson and West [28] and Ude [118] both learn open-loop robot trajectories from human demonstration. Skubic and Volz [109] transfer force-based assembly skills to robots from human demonstrations. Iba [53] models open-loop sensory motor skills in humans.
Reference: [120] <author> C. J. Watkins, </author> <title> Learning from Delayed Rewards, </title> <type> Ph.D. Thesis, </type> <institution> Kings College, University of Cambridge, </institution> <year> 1989. </year>
Reference-contexts: Below, we describe previous work relating to each of these methods. 1.2.1 Skill learning through exploration Learning skill through exploration has become a popular paradigm for acquiring robotic skills. In reinforcement learning <ref> [14, 76, 115, 120] </ref>, data is not given as direct input/output data points; rather data is specified by an input vector and an associated (scalar) reward from the environment. This reward represents the reinforcement signal, and is akin to learning with a critic as opposed to learning with a teacher.
Reference: [121] <author> M. Werman and D. Weinshall, </author> <title> Similarity and Affine Invariant Distances Between 2D Point Sets, </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 17, no. 8, </volume> <pages> pp. 810-14, </pages> <year> 1995. </year> <note> 196 Bibliography </note>
Reference-contexts: Therefore, we require a stochastic similarity measure, with sufficient representational power and exibility to compare multi-dimensional, stochastic trajectories. 6.2 Stochastic similarity measure Similarity measures or metrics have been given considerable attention in computer vision <ref> [12, 20, 121] </ref>, image database retrieval [54], and 2D or 3D shape analysis [62, 107].
Reference: [122] <author> D. A. White and D. A. Sofge, eds., </author> <title> Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches, </title> <publisher> Multiscience Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: On the other hand, the demonstrated skill in Figure 1-1 requires no high-level reasoning or abstract thought. Modeling such mental processes, of which humans are capable, requires an as-of-yet unavailable understanding of the human traits of self-awareness and consciousness. 1.2 Related work The field of intelligent control <ref> [122] </ref> has emerged from the field of classical control theory to deal with applications too complex for classical control approaches. Broadly speaking, intelligent control combines classical control theory with adaptation, learning, or active exploration.
Reference: [123] <author> J. Yamato, S. Kurakake, A. Tomono and K. Ishii, </author> <title> Human Action Recognition Using HMM with Category Separated Vector Quantization, </title> <journal> Trans. Institute of Electronics, Information and Communication Engineers D-II, </journal> <volume> vol. J77D-II, no. 7, </volume> <pages> pp. 1311-18, </pages> <year> 1994. </year>
Reference-contexts: Yang, et. al. [126, 127] implement a different state-based approach to open-loop skill learning and telerobotics using Hidden Markov Models (HMMs). HMMs are trained to learn both telerobotic trajectories executed by a human operator and simple human handwriting gestures. Yamato, et. al. <ref> [123, 124] </ref> also train HMMs to recognize open-loop human actions. 1.2 Related work 7 Friedrich, et. al. [36] and Kaiser [56] review programming by demonstration and skill acquisition via human demonstration for elementary robot skills. Lee [65] investigates human-to-robot skill transfer through demonstration of task performance in a virtual environments. <p> Transient sonar signals are classified with HMMs for ocean surveillance in [61]. Radons, et. al. [96] analyze 30-electrode neuronal spike activity in a monkeys visual cortex with HMMs. Hannaford and Lee [45] classify task structure in tele-operation based on HMMs. In <ref> [123, 124] </ref>, HMMs are used to characterize sequential images of human actions.
Reference: [124] <author> J. Yamato, J. Ohya and K. Ishii, </author> <title> REcognizing Human Action in Time-sequential Images Using Hidden Markov Models, </title> <journal> Trans. Institute of Electronics, Information, and Communication Engineers D-II, </journal> <volume> vol. J76D-II, no. 12, </volume> <pages> pp. 2556-2563, </pages> <year> 1993. </year>
Reference-contexts: Yang, et. al. [126, 127] implement a different state-based approach to open-loop skill learning and telerobotics using Hidden Markov Models (HMMs). HMMs are trained to learn both telerobotic trajectories executed by a human operator and simple human handwriting gestures. Yamato, et. al. <ref> [123, 124] </ref> also train HMMs to recognize open-loop human actions. 1.2 Related work 7 Friedrich, et. al. [36] and Kaiser [56] review programming by demonstration and skill acquisition via human demonstration for elementary robot skills. Lee [65] investigates human-to-robot skill transfer through demonstration of task performance in a virtual environments. <p> Transient sonar signals are classified with HMMs for ocean surveillance in [61]. Radons, et. al. [96] analyze 30-electrode neuronal spike activity in a monkeys visual cortex with HMMs. Hannaford and Lee [45] classify task structure in tele-operation based on HMMs. In <ref> [123, 124] </ref>, HMMs are used to characterize sequential images of human actions.
Reference: [125] <author> B. Yang and H. Asada, </author> <title> Hybrid Linguistic/Numeric Control of Deburring Robots Based on Human Skills, </title> <booktitle> Proc. IEEE Int. Conf. on Robotics and Automation, </booktitle> <volume> vol. 2, </volume> <pages> pp. 1467-74, </pages> <year> 1992. </year>
Reference-contexts: Robot learning from human experts has also been applied to a deburring robot. Asada and Yang [6] derive control rules directly from human input/output data, by associating input patterns with corresponding output actions for the deburring robot. In <ref> [125] </ref>, Yang and Asada combine linguistic information and numeric input/output data for the overall control of the robot. Expert linguistic rules are acquired directly from a human expert to partition the control space.
Reference: [126] <author> J. Yang, Y. Xu and C. S. Chen, </author> <title> Hidden Markov Model Approach to Skill Learning and its Application to Telerobotics, </title> <journal> IEEE Trans. on Robotics and Automation, </journal> <volume> vol. 10, no. 5, </volume> <pages> pp. 621-31, </pages> <year> 1994. </year>
Reference-contexts: Skills are modeled as optimal sequences of one-step state transitions that transform the current state into the goal state. The approach is verified on demonstrated human Cartesian teleoperation skill. Yang, et. al. <ref> [126, 127] </ref> implement a different state-based approach to open-loop skill learning and telerobotics using Hidden Markov Models (HMMs). HMMs are trained to learn both telerobotic trajectories executed by a human operator and simple human handwriting gestures. <p> Hannaford and Lee [45] classify task structure in tele-operation based on HMMs. In [123, 124], HMMs are used to characterize sequential images of human actions. Finally, Yang and Xu apply Hidden Markov Models to open-loop action skill learning <ref> [126] </ref> and human gesture recognition [127]. 6.2 Stochastic similarity measure 99 A Hidden Markov Model consists of a set of n states, interconnected through probabilistic transitions; each of these states has some output probability distribution associated with it.
Reference: [127] <author> J. Yang, Y. Xu and C. S. Chen, </author> <title> Human Action Learning via Hidden Markov Models, </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics Part A: Systems and Humans, </journal> <volume> vol. 27, no. 1, </volume> <pages> pp. 34-44, </pages> <year> 1997. </year>
Reference-contexts: Skills are modeled as optimal sequences of one-step state transitions that transform the current state into the goal state. The approach is verified on demonstrated human Cartesian teleoperation skill. Yang, et. al. <ref> [126, 127] </ref> implement a different state-based approach to open-loop skill learning and telerobotics using Hidden Markov Models (HMMs). HMMs are trained to learn both telerobotic trajectories executed by a human operator and simple human handwriting gestures. <p> Hannaford and Lee [45] classify task structure in tele-operation based on HMMs. In [123, 124], HMMs are used to characterize sequential images of human actions. Finally, Yang and Xu apply Hidden Markov Models to open-loop action skill learning [126] and human gesture recognition <ref> [127] </ref>. 6.2 Stochastic similarity measure 99 A Hidden Markov Model consists of a set of n states, interconnected through probabilistic transitions; each of these states has some output probability distribution associated with it.
References-found: 127

