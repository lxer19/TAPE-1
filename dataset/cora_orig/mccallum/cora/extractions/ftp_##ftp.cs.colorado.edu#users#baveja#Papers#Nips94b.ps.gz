URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/Nips94b.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: http://www.cs.colorado.edu
Email: tommi@psyche.mit.edu  singh@psyche.mit.edu  jordan@psyche.mit.edu  
Title: Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems  
Author: Tommi Jaakkola Satinder P. Singh Michael I. Jordan 
Address: Bld. E10  Cambridge, MA 02139  
Affiliation: Department of Brain and Cognitive Sciences,  Massachusetts Institute of Technology  
Abstract: Increasing attention has been paid to reinforcement learning algorithms in recent years, partly due to successes in the theoretical analysis of their behavior in Markov environments. If the Markov assumption is removed, however, neither generally the algorithms nor the analyses continue to be usable. We propose and analyze a new learning algorithm to solve a certain class of non-Markov decision problems. Our algorithm applies to problems in which the environment is Markov, but the learner has restricted access to state information. The algorithm involves a Monte-Carlo policy evaluation combined with a policy improvement method that is similar to that of Markov decision problems and is guaranteed to converge to a local maximum. The algorithm operates in the space of stochastic policies, a space which can yield a policy that performs considerably better than any deterministic policy. Although the space of stochastic policies is continuous|even for a discrete action space|our algorithm is computationally tractable. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A., and Duff, M. </author> <year> (1994). </year> <title> Monte-Carlo matrix inversion and reinforcement learning. </title> <booktitle> In Advances of Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: As we noted earlier, indirect approaches lead to computationally intractable algorithms. Our approach can be viewed as providing a generalization of the direct approach to MDP's to the case of POMDP's. 3 A MONTE-CARLO POLICY EVALUATION Advantages of Monte-Carlo methods for policy evaluation in MDP's have been reviewed recently <ref> (Barto and Duff, 1994) </ref>. Here we present a method for calculating the value of a stochastic policy that has the flavor of a Monte-Carlo algorithm.
Reference: <author> Bertsekas, D. P. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference-contexts: To motivate such an approach let us first consider a simple case where the average reward is known and generalize the well-defined MDP value function to the POMDP setting. In the Markov case the value function can be written as <ref> (cf. Bertsekas, 1987) </ref>: N!1 t=1 where s t and a t refer to the state and the action taken at the t th step respectively.
Reference: <author> Dayan, P. </author> <year> (1992). </year> <title> The convergence of TD() for general . Machine Learning, </title> <booktitle> 8, </booktitle> <pages> 341-362. </pages>
Reference: <author> Jaakkola, T., Jordan M. I., and Singh, S. P. </author> <year> (1994). </year> <title> On the convergence of stochastic iterative Dynamic Programming algorithms. </title> <booktitle> Neural Computation 6, </booktitle> <pages> 1185-1201. </pages>
Reference-contexts: Although current reinforcement learning algorithms are based on the assumption that the learning problem can be cast as Markov decision problem (MDP), many practical problems resist being treated as an MDP. Unfortunately, if the Markov assumption is removed examples can be found where current algorithms cease to perform well <ref> (Singh, Jaakkola, & Jordan, 1994) </ref>. Moreover, the theoretical analyses rely heavily on the Markov assumption. The non-Markov nature of the environment can arise in many ways. The most direct extension of MDP's is to deprive the learner of perfect information about the state of the environment.
Reference: <author> Monahan, G. </author> <year> (1982). </year> <title> A survey of partially observable Markov decision processes. </title> <journal> Management Science, </journal> <volume> 28, </volume> <pages> 1-16. </pages>
Reference: <author> Singh, S. P., Jaakkola, T., Jordan, M. I. </author> <year> (1994). </year> <title> Learning without state estimation in partially observable environments. </title> <booktitle> In Proceedings of the Eleventh Machine Learning Conference. </booktitle>
Reference-contexts: Although current reinforcement learning algorithms are based on the assumption that the learning problem can be cast as Markov decision problem (MDP), many practical problems resist being treated as an MDP. Unfortunately, if the Markov assumption is removed examples can be found where current algorithms cease to perform well <ref> (Singh, Jaakkola, & Jordan, 1994) </ref>. Moreover, the theoretical analyses rely heavily on the Markov assumption. The non-Markov nature of the environment can arise in many ways. The most direct extension of MDP's is to deprive the learner of perfect information about the state of the environment. <p> In this paper we describe an alternative approach for POMDP's that avoids the state estimation problem and works directly in the space of (stochastic) control policies. <ref> (See Singh, et al., 1994, for additional material on stochastic policies.) </ref> 2 PARTIAL OBSERVABILITY A Markov decision problem can be generalized to a POMDP by restricting the state information available to the learner. Accordingly, we define the learning problem as follows.
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Schwartz, A. </author> <year> (1993). </year> <title> A reinforcement learning method for maximizing undiscounted rewards. </title> <booktitle> In Proceedings of the Tenth Machine Learning Conference. </booktitle>
Reference: <author> Tsitsiklis J. N. </author> <year> (1994). </year> <title> Asynchronous stochastic approximation and Q-learning. </title> <booktitle> Machine Learning 16, </booktitle> <pages> 185-202. </pages>
Reference: <author> Watkins, C.J.C.H. </author> <year> (1989). </year> <title> Learning from delayed rewards. </title> <type> PhD Thesis, </type> <institution> University of Cambridge, </institution> <address> England. </address>
Reference: <author> Watkins, C.J.C.H, & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 279-292. </pages>
References-found: 11

