URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/Weigend.Srivastava_IJNS95.ps.Z
Refering-URL: http://www.stern.nyu.edu/~aweigend/Research/Papers/BEFORENYU/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: andreas@cs.colorado.edu  srivasan@cs.colorado.edu  
Title: Predicting Conditional Probability Distributions: A Connectionist Approach  
Author: Andreas S. Weigend Ashok N. Srivastava 
Address: Boulder Boulder, CO 80309-0430  Boulder Boulder, CO 80309-0529  
Affiliation: Department of Computer Science and Institute of Cognitive Science University of Colorado at  Department of Elec. Comp. Engineering and Center for Space Construction University of Colorado at  
Web: ftp://ftp.cs.colorado.edu/pub/Time-Series/MyPapers/prob-density.ps  
Note: To appear in: International Journal of Neural Systems, Vol 6 (Summer 1995). Draft:  
Abstract: Most traditional prediction techniques deliver the mean of the probability distribution (a single point). For multimodal processes, instead of predicting the mean of the probability distribution, it is important to predict the full distribution. This article presents a new connectionist method to predict the conditional probability distribution in response to an input. The main idea is to transform the problem from a regression to a classification problem. The conditional probability distribution network can perform both direct predictions and iterated predictions, a task which is specific for time series problems. We compare our method to fuzzy logic and discuss important differences, and also demonstrate the architecture on two time series. The first is the benchmark laser series used in the Santa Fe competition, a deterministic chaotic system. The second is a time series from a Markov process which exhibits structure on two time scales. The network produces multimodal predictions for this series. We compare the predictions of the network with a nearest-neighbor predictor and find that the conditional probability network is more than twice as likely a model.
Abstract-found: 1
Intro-found: 1
Reference: [Bishop, 1994] <author> Bishop, C. M. </author> <year> (1994). </year> <title> Mixture density networks. </title> <type> Technical report, </type> <institution> Aston University. </institution>
Reference-contexts: This method was introduced to the connectionist community by [Jacobs et al., 1991] in the Mixture of Experts model and applied to time series prediction by [Weigend and Mangeas, 1995]. Other connectionist mixture models are discussed in <ref> [Bishop, 1994, Neuneier et al., 1994, Taylor et al., 1995] </ref>; the case of a single Gaussian is disussed in [Nix and Weigend, 1995]. 3 Representation: Fractional Binning To produce distributions, we convert the usual regression problem to a classification problem, and classify each target value in at most two adjacent classes
Reference: [Brown, 1983] <author> Brown, R. G. </author> <year> (1983). </year> <title> Introduction to Random Signal Analysis and Kalman Filtering. </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address>
Reference-contexts: For these unimodal processes, predicting the mean is appropriate, and the Kalman filter gives the best estimate (in the least-squares sense) of y (t + 1) among all linear and nonlinear filters <ref> [Brown, 1983] </ref>.
Reference: [Fukunaga, 1972] <author> Fukunaga, K. </author> <year> (1972). </year> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: Other approaches to density estimation include kernel density estimation and mixture density estimation. In kernel density estimation, a kernel function (such as a Gaussian) is placed on each data point. The density function is then expressed as a normalized sum of these kernel functions <ref> [Fukunaga, 1972, Scott, 1992] </ref>. In mixture density estimation, the density function is expressed as a linear combination of basis functions. This method was introduced to the connectionist community by [Jacobs et al., 1991] in the Mixture of Experts model and applied to time series prediction by [Weigend and Mangeas, 1995].
Reference: [Hubner et al., 1994] <author> Hubner, U., Weiss, C. O., Abraham, N. B., and Tang, D. </author> <year> (1994). </year> <note> Lorenz-like chaos in nh 3 -fir lasers. In Weigend, </note> <editor> A. S. and Gershenfeld, N. A., editors, </editor> <title> Time Series Prediction: </title> <booktitle> Forecasting the Future and Understanding the Past, </booktitle> <pages> pages 73-104, </pages> <address> Reading, MA. </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: The data set consists of 25,000 points which are readings from an 8-bit A/D converter measuring the intensity of a far-infrared laser. The laser can be approximately described by the Lorenz equations, a set of three coupled nonlinear differential equations <ref> [Hubner et al., 1994] </ref>. 2 We divided the 25,000 data points into three sets: a training set (12,000 points, even-numbered), a validation set (12,000 points, odd-numbered), and a test set (1,000 points).
Reference: [Jacobs et al., 1991] <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. </author> <year> (1991). </year> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87. </pages>
Reference-contexts: The density function is then expressed as a normalized sum of these kernel functions [Fukunaga, 1972, Scott, 1992]. In mixture density estimation, the density function is expressed as a linear combination of basis functions. This method was introduced to the connectionist community by <ref> [Jacobs et al., 1991] </ref> in the Mixture of Experts model and applied to time series prediction by [Weigend and Mangeas, 1995].
Reference: [Neuneier et al., 1994] <author> Neuneier, R., Hergert, F., Finnoff, W., and Ormoneit, D. </author> <year> (1994). </year> <title> Estimation of conditional densities: A comparison of neural network approaches. </title> <editor> In Marinaro, M. and Morasso, P. G., editors, </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 689-692, </pages> <address> New York. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: This method was introduced to the connectionist community by [Jacobs et al., 1991] in the Mixture of Experts model and applied to time series prediction by [Weigend and Mangeas, 1995]. Other connectionist mixture models are discussed in <ref> [Bishop, 1994, Neuneier et al., 1994, Taylor et al., 1995] </ref>; the case of a single Gaussian is disussed in [Nix and Weigend, 1995]. 3 Representation: Fractional Binning To produce distributions, we convert the usual regression problem to a classification problem, and classify each target value in at most two adjacent classes <p> This example has also been used by <ref> [Neuneier et al., 1994] </ref>. This problem exhibits structure on two time scales and has a multimodal probability distribution. Consider predicting the position of a ball (with mass) sliding on a double-welled surface (with friction), as shown in noise.
Reference: [Nix and Weigend, 1995] <author> Nix, D. A. and Weigend, A. S. </author> <year> (1995). </year> <title> Local error bars for nonlinear regression and time series prediction. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7 (NIPS*94). </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Other connectionist mixture models are discussed in [Bishop, 1994, Neuneier et al., 1994, Taylor et al., 1995]; the case of a single Gaussian is disussed in <ref> [Nix and Weigend, 1995] </ref>. 3 Representation: Fractional Binning To produce distributions, we convert the usual regression problem to a classification problem, and classify each target value in at most two adjacent classes by allowing for fractional class memberships.
Reference: [Rumelhart et al., 1995] <author> Rumelhart, D. E., Durbin, R., Golden, R., and Chauvin, Y. </author> <year> (1995). </year> <title> Backpropagation: The basic theory. </title> <editor> In Chauvin, Y. and Rumelhart, D. E., editors, Backpropagation: </editor> <booktitle> Theory, Architectures, and Applications, </booktitle> <pages> pages 1-34, </pages> <address> Hillsdale, NJ. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: 1 Introduction Most traditional regression techniques deliver single point predictions, and can be shown to predict the mean of the conditional probability distribution of the output given the input <ref> [Rumelhart et al., 1995] </ref>. Although such predictions are adequate for situations with additive noise, in stochastic systems where multimodal distributions can occur, predicting the mean is inappropriate. We designed the conditional probability distribution network (cpd-net) for these systems.
Reference: [Scott, 1992] <author> Scott, D. W. </author> <year> (1992). </year> <title> Multivariate Density Estimation, Theory, Practice and Visualization. </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address>
Reference-contexts: Other approaches to density estimation include kernel density estimation and mixture density estimation. In kernel density estimation, a kernel function (such as a Gaussian) is placed on each data point. The density function is then expressed as a normalized sum of these kernel functions <ref> [Fukunaga, 1972, Scott, 1992] </ref>. In mixture density estimation, the density function is expressed as a linear combination of basis functions. This method was introduced to the connectionist community by [Jacobs et al., 1991] in the Mixture of Experts model and applied to time series prediction by [Weigend and Mangeas, 1995].
Reference: [Srivastava and Weigend, 1994] <author> Srivastava, A. N. and Weigend, A. S. </author> <year> (1994). </year> <title> Computing the probability density in connectionist regression. </title> <editor> In Marinaro, M. and Morasso, P. G., editors, </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks, Sorrento, Italy (ICANN 94), </booktitle> <pages> pages 685-688. </pages> <editor> Springer-Verlag. </editor> <booktitle> Also in Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <address> Orlando, FL (IEEE-ICNN'94), p. 3786-3789, IEEE-Press. </address>
Reference-contexts: We compared the predictions of the conditional probability network to a nearest neighbor predictor, and found that the network is more than twice as likely a model. Acknowledgments An earlier version of this article has been presented in <ref> [Srivastava and Weigend, 1994] </ref>. This material is based upon work supported by the National Science Foundation under Grant No. RIA ECS-9309786 and the National Aeronautics and Space Administration Grant No. NHGW-1388. We thank William Finnoff, Dave Rumelhart, Renjeng Su, and John Tukey for their comments and suggestions. 9
Reference: [Taylor et al., 1995] <author> Taylor, J. G., Husmeier, D., and Allen, D. </author> <year> (1995). </year> <title> A universal approximator network for learning conditional probability distributions. </title> <note> In ICANN'95 (submitted). </note>
Reference-contexts: This method was introduced to the connectionist community by [Jacobs et al., 1991] in the Mixture of Experts model and applied to time series prediction by [Weigend and Mangeas, 1995]. Other connectionist mixture models are discussed in <ref> [Bishop, 1994, Neuneier et al., 1994, Taylor et al., 1995] </ref>; the case of a single Gaussian is disussed in [Nix and Weigend, 1995]. 3 Representation: Fractional Binning To produce distributions, we convert the usual regression problem to a classification problem, and classify each target value in at most two adjacent classes
Reference: [Wang and Mendel, 1992] <author> Wang, L. and Mendel, J. M. </author> <year> (1992). </year> <title> Generating fuzzy rules by learning from examples. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics, </journal> <volume> 22(6) </volume> <pages> 1414-1427. </pages>
Reference-contexts: The following technique has been applied to time series prediction problems as well as control <ref> [Wang and Mendel, 1992] </ref>. 1. Divide the input and output spaces into fuzzy regions. This is similar to the fractional-binning procedure described in Section 3. 2. Generate fuzzy rules from the given data.
Reference: [Weigend, 1994] <author> Weigend, A. S. </author> <year> (1994). </year> <title> Paradigm change in prediction. </title> <journal> Philosophical Transactions of the Royal Society (Physical Sciences), </journal> <volume> 348 </volume> <pages> 405-420. </pages>
Reference-contexts: In forecasting, there are two ways of obtaining predictions: direct predictions and iterated predictions; their relative advantages and disadvantages are discussed for example in <ref> [Weigend, 1994] </ref>. The task influences the inputs of the cpd net: On the one hand, for direct predictions, it suffices to assign one input unit for each time-delayed value of the series.
Reference: [Weigend, 1995] <author> Weigend, A. S. </author> <year> (1995). </year> <title> Time series analysis and prediction. </title> <editor> In Smolensky, P., Mozer, M. C., and Rumelhart, D. E., editors, </editor> <booktitle> Mathematical Perspectives on Neural Networks. </booktitle> <publisher> Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: We train the network using backpropagation with no momentum. Training is stopped when the error on a held-out validation set starts increasing [Weigend et al., 1990]. * Stochastic teacher forcing for iterated predictions. We use stochastic teacher forcing to train the network for iterated predictions <ref> [Weigend, 1995] </ref>. At the beginning of training, the network inputs are given the exact, fractionally binned distributions. As training proceeds, these are replaced by the predicted distributions.
Reference: [Weigend and Gershenfeld, 1994] <author> Weigend, A. S. and Gershenfeld, N. A., </author> <title> editors (1994). Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: A subset of these data was used in the Santa Fe Time Series Prediction Competition <ref> [Weigend and Gershenfeld, 1994] </ref>. The specific architecture of the cpd-net for this task is shown in Fig. 3. The network has three fractionally binned (13 bins) inputs corresponding to y (t 2), y (t 1), and y (t), 25 sigmoid units in each hidden layer, and 13 normalized exponential units.
Reference: [Weigend et al., 1990] <author> Weigend, A. S., Huberman, B. A., and Rumelhart, D. E. </author> <year> (1990). </year> <title> Predicting the future: A connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1 </volume> <pages> 193-209. </pages>
Reference-contexts: These alternatives require the network learn the constraints from the data. 3 exponentials). We train the network using backpropagation with no momentum. Training is stopped when the error on a held-out validation set starts increasing <ref> [Weigend et al., 1990] </ref>. * Stochastic teacher forcing for iterated predictions. We use stochastic teacher forcing to train the network for iterated predictions [Weigend, 1995]. At the beginning of training, the network inputs are given the exact, fractionally binned distributions. As training proceeds, these are replaced by the predicted distributions.
Reference: [Weigend and Mangeas, 1995] <author> Weigend, A. S. and Mangeas, M. </author> <year> (1995). </year> <title> Analysis and prediction of multi-stationary time series using nonlinear gated experts. </title> <type> Technical Report CU-CS-764-95, </type> <institution> University of Colorado at Boulder, ftp://ftp.cs.colorado.edu/pub/Time-Series/MyPapers/experts.ps. </institution>
Reference-contexts: In mixture density estimation, the density function is expressed as a linear combination of basis functions. This method was introduced to the connectionist community by [Jacobs et al., 1991] in the Mixture of Experts model and applied to time series prediction by <ref> [Weigend and Mangeas, 1995] </ref>.
Reference: [Werbos and Titus, 1978] <author> Werbos, P. J. and Titus, J. </author> <year> (1978). </year> <title> An empirical test of new forecasting methods derived from a theory of intelligence: the prediction of conflict in latin america. </title> <journal> IEEE Trans. Systems, Man and Cybernetics. </journal> <volume> 10 </volume>
Reference-contexts: As training proceeds, these are replaced by the predicted distributions. This is not done abruptly, but according to an annealing schedule that gradually increases the probability of replacing the observed value by the predicted value. Note that this idea can be traced back to <ref> [Werbos and Titus, 1978] </ref>. structure of the cpd-net for iterated predictions; the output can be fed back to the input for the next prediction.
References-found: 18

