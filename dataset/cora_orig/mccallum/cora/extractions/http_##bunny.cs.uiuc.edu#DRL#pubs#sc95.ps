URL: http://bunny.cs.uiuc.edu/DRL/pubs/sc95.ps
Refering-URL: http://bunny.cs.uiuc.edu/DRL/pubs/sc95.html
Root-URL: http://www.cs.uiuc.edu
Email: fseamons,ying,pecj,jozwiak,winslettg@cs.uiuc.edu  
Title: Server-Directed Collective I/O in Panda  
Author: K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett 
Keyword: Collective I/O, Parallel I/O  
Address: Urbana, Illinois 61801  
Affiliation: Computer Science Department University of Illinois,  
Abstract: We present the architecture and implementation results for Panda 2.0, a library for input and output of multidimensional arrays on parallel and sequential platforms. Panda achieves remarkable performance levels on the IBM SP2, showing excellent scalability as data size increases and as the number of nodes increases, and provides throughputs close to the full capacity of the AIX file system on the SP2 we used. We argue that this good performance can be traced to Panda's use of server-directed i/o (a logical-level version of disk-directed i/o [Kotz94b]) to perform array i/o using sequential disk reads and writes, a very high level interface for collective i/o requests, and built-in facilities for arbitrary rearrangements of arrays during i/o. Other advantages of Panda's approach are ease of use, easy application portability, and a reliance on commodity system software. 
Abstract-found: 1
Intro-found: 1
Reference: [Bennett94] <author> R. Bennett, K. Bryant, A. Sussman, R. Das, and J. Saltz, Jovian: </author> <title> A framework for optimizing parallel I/O, </title> <booktitle> Proceedings of the 1994 Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-20. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: With a collective i/o implementation, there are more opportunities for disk i/o optimization, as one can form large orderly contiguous disk i/o requests rather than servicing disk i/o requests as they arrive in random order. Several parallel file systems and multidimensional array libraries have provided a collective i/o interface <ref> [Bennett94, Brezany95, Corbett94b, 6 Corbett95, del Rosario94, Karpovich94, Seamons94b, Seligman94] </ref>; some of them have shown that collective i/o provides high performance given appropriate language, compiler and run-time system support. Three approaches to i/o optimization are discussed in [Kotz94b]: traditional caching, two-phase i/o, and disk-directed i/o. <p> Among other approaches to collective i/o, [Galbreath93] buffers several i/o requests before issuing disk i/o requests. In this approach, each processor contributes its requests to a buffer, then the buffers are gathered together and written into or read from by a master processor. <ref> [Bennett94] </ref> gives a strategy to minimize the number of i/o requests by coalescing a large number of requests from compute nodes into one big i/o request.
Reference: [Bordawekar93] <author> R. Bordawekar, J. Miguel del Rosario, and A. Choudary, </author> <title> Design and Evaluation of Primitives for Parallel I/O, </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <year> 1993. </year>
Reference-contexts: Intel CFS [Pierce93] uses traditional caching, and [Kotz93b] shows that CFS only uses half of the raw disk bandwidth. <ref> [Bordawekar93] </ref> considers a `two-phase' access strategy for collective i/o.
Reference: [Brezany95] <author> P. Brezany, T. Mueck, and E. Schikuta, </author> <title> Language, compiler and parallel database support for I/O intensive applications, </title> <booktitle> Proceedings of the High Performance Computing and Networking 1995 Europe Conference, </booktitle> <address> Milano, Italy, May 1995, </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: With a collective i/o implementation, there are more opportunities for disk i/o optimization, as one can form large orderly contiguous disk i/o requests rather than servicing disk i/o requests as they arrive in random order. Several parallel file systems and multidimensional array libraries have provided a collective i/o interface <ref> [Bennett94, Brezany95, Corbett94b, 6 Corbett95, del Rosario94, Karpovich94, Seamons94b, Seligman94] </ref>; some of them have shown that collective i/o provides high performance given appropriate language, compiler and run-time system support. Three approaches to i/o optimization are discussed in [Kotz94b]: traditional caching, two-phase i/o, and disk-directed i/o.
Reference: [Corbett94b] <author> P. F. Corbett and D. G. Feitelson, </author> <title> Vesta file system programmer's reference. </title> <type> Technical Report Research Report RC 19898 (88058), </type> <institution> IBM T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY 10598, </address> <month> October </month> <year> 1994. </year> <note> Version 1.01. </note>
Reference-contexts: With a collective i/o implementation, there are more opportunities for disk i/o optimization, as one can form large orderly contiguous disk i/o requests rather than servicing disk i/o requests as they arrive in random order. Several parallel file systems and multidimensional array libraries have provided a collective i/o interface <ref> [Bennett94, Brezany95, Corbett94b, 6 Corbett95, del Rosario94, Karpovich94, Seamons94b, Seligman94] </ref>; some of them have shown that collective i/o provides high performance given appropriate language, compiler and run-time system support. Three approaches to i/o optimization are discussed in [Kotz94b]: traditional caching, two-phase i/o, and disk-directed i/o.
Reference: [Corbett95] <author> P. Corbett, D. Feitelson, Y. Hsu, J. Prost, M. Snir, S. Fineberg, B. Nitzberg, B. Traversat, and P. Wong. </author> <title> MPI-IO: A Parallel File I/O Interface for MPI, </title> <type> Technical Report NAS-95-002, </type> <institution> NASA Ames Research Center, </institution> <month> January </month> <year> 1995. </year>
Reference: [del Rosario94] <author> J. M. del Rosario, M. Harry, A. Choudhary, </author> <title> The Design of VIP-FS: A Virtual, Parallel File System for High Performance Parallel and Distributed Computing, </title> <type> Technical Report SCCS-628, </type> <institution> NPAC, Syracuse, </institution> <address> NY, </address> <month> May </month> <year> 1994. </year> <month> 8 </month>
Reference-contexts: This from others' work, where the arrays were striped across i/o nodes in traditional (row-major or column-major) order according to disk block size <ref> [Kotz94b, Kotz95b, del Rosario94] </ref>.) Once a server has made its plan, it assembles its first assigned chunk by sending a request for a logical sub-chunk to all the clients that hold a part of its first assigned chunk. <p> With a collective i/o implementation, there are more opportunities for disk i/o optimization, as one can form large orderly contiguous disk i/o requests rather than servicing disk i/o requests as they arrive in random order. Several parallel file systems and multidimensional array libraries have provided a collective i/o interface <ref> [Bennett94, Brezany95, Corbett94b, 6 Corbett95, del Rosario94, Karpovich94, Seamons94b, Seligman94] </ref>; some of them have shown that collective i/o provides high performance given appropriate language, compiler and run-time system support. Three approaches to i/o optimization are discussed in [Kotz94b]: traditional caching, two-phase i/o, and disk-directed i/o.
Reference: [Galbreath93] <author> N. Galbreath, W. Gropp, and D. Levine, </author> <title> Applications-Driven Parallel I/O, </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year>
Reference-contexts: Among other approaches to collective i/o, <ref> [Galbreath93] </ref> buffers several i/o requests before issuing disk i/o requests.
Reference: [Karpovich94] <author> J. F. Karpovich, A. S. Grimshaw, J. C. </author> <title> French, Extensible File Systems (ELFS): An Object-Oriented Approach to High Performance File I/O, </title> <booktitle> Proceedings of the International Conference on Object-Oriented Programming, Systems, Languages, and Applications, </booktitle> <address> Portland OR, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: With a collective i/o implementation, there are more opportunities for disk i/o optimization, as one can form large orderly contiguous disk i/o requests rather than servicing disk i/o requests as they arrive in random order. Several parallel file systems and multidimensional array libraries have provided a collective i/o interface <ref> [Bennett94, Brezany95, Corbett94b, 6 Corbett95, del Rosario94, Karpovich94, Seamons94b, Seligman94] </ref>; some of them have shown that collective i/o provides high performance given appropriate language, compiler and run-time system support. Three approaches to i/o optimization are discussed in [Kotz94b]: traditional caching, two-phase i/o, and disk-directed i/o.
Reference: [Kotz93b] <author> D. Kotz, </author> <title> Throughput of Existing Multiprocessor File Systems (An Informal Study), </title> <institution> Dartmouth PCS-TR93-190, </institution> <year> 1993. </year>
Reference-contexts: Without a high level semantic view of the collective i/o requests, the file system is not able to predict whether sequential prefetching will be useful or when to flush the file cache. Intel CFS [Pierce93] uses traditional caching, and <ref> [Kotz93b] </ref> shows that CFS only uses half of the raw disk bandwidth. [Bordawekar93] considers a `two-phase' access strategy for collective i/o.
Reference: [Kotz94a] <author> D. Kotz, and N. Nieuwejaar, </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 640-649, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Recent studies characterizing dynamic i/o patterns in scientific applications <ref> [Kotz94a, Purakayastha94, Pasquale94] </ref> show that many scientific applications have regular patterns of i/o behavior, such as physical periodicity in strided access to multidimensional arrays, or temporal periodicity in checkpoint and restart operations. In both cases all application compute nodes participate in i/o operations to access array data.
Reference: [Kotz94b] <author> D. Kotz, </author> <title> Disk-Directed I/O for MIMD Multiprocessors, </title> <booktitle> First Symposium on Operating Systems Design and Implementation, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: The cleaner view means we have a higher level semantic understanding of i/o requests, and we can use the extra semantic information to make more intelligent optimization decisions. In addition, higher-level interfaces are easier for users to master, and give their applications better portability. Disk-directed i/o <ref> [Kotz94b] </ref>, as described in the following sections, uses information about a collective i/o request that is being issued by a group of compute nodes. <p> This from others' work, where the arrays were striped across i/o nodes in traditional (row-major or column-major) order according to disk block size <ref> [Kotz94b, Kotz95b, del Rosario94] </ref>.) Once a server has made its plan, it assembles its first assigned chunk by sending a request for a logical sub-chunk to all the clients that hold a part of its first assigned chunk. <p> Three approaches to i/o optimization are discussed in <ref> [Kotz94b] </ref>: traditional caching, two-phase i/o, and disk-directed i/o. Traditional caching does not support a collective i/o interface; i/o requests are served as they arrive. Each i/o node has a file cache and prefetches, thus optimizing sequential file accesses. <p> In the second phase the compute nodes permute the data in memory until each node has the data that it needs. Disk-directed i/o for collective i/o operations is proposed in <ref> [Kotz94b] </ref>. Under this approach, compute nodes tell the i/o nodes about a collective i/o request they plan to perform. Based on this semantic information, the i/o nodes determine which compute nodes they should ask for data or send data to. <p> Based on this semantic information, the i/o nodes determine which compute nodes they should ask for data or send data to. The i/o nodes plan their data accesses so that they are able to form a large contiguous disk i/o request rather than many smaller requests. The simulation in <ref> [Kotz94b] </ref> promises good performance. [Kotz95a] compares the simulated performance using disk-directed i/o with that of traditional caching on irregular distributions of data, and shows that disk-directed i/o is never slower in the test cases. [Kotz95b] uses this approach to implement an out-of-core LU decomposition problem and shows that it is much
Reference: [Kotz95a] <author> D. Kotz, </author> <title> Expanding Potential for Disk-Directed I/O. </title> <institution> Dartmouth PCS-TR95-254, </institution> <note> submitted to SPDP '95. </note>
Reference-contexts: The i/o nodes plan their data accesses so that they are able to form a large contiguous disk i/o request rather than many smaller requests. The simulation in [Kotz94b] promises good performance. <ref> [Kotz95a] </ref> compares the simulated performance using disk-directed i/o with that of traditional caching on irregular distributions of data, and shows that disk-directed i/o is never slower in the test cases. [Kotz95b] uses this approach to implement an out-of-core LU decomposition problem and shows that it is much better than the traditional
Reference: [Kotz95b] <author> D. Kotz, </author> <title> Disk-Directed I/O for Out-of-Core Computation. </title> <note> Dartmouth TR PCS-TR95-251, submitted to HPDC '95. </note>
Reference-contexts: This from others' work, where the arrays were striped across i/o nodes in traditional (row-major or column-major) order according to disk block size <ref> [Kotz94b, Kotz95b, del Rosario94] </ref>.) Once a server has made its plan, it assembles its first assigned chunk by sending a request for a logical sub-chunk to all the clients that hold a part of its first assigned chunk. <p> The simulation in [Kotz94b] promises good performance. [Kotz95a] compares the simulated performance using disk-directed i/o with that of traditional caching on irregular distributions of data, and shows that disk-directed i/o is never slower in the test cases. <ref> [Kotz95b] </ref> uses this approach to implement an out-of-core LU decomposition problem and shows that it is much better than the traditional caching scheme. Among other approaches to collective i/o, [Galbreath93] buffers several i/o requests before issuing disk i/o requests.
Reference: [Pasquale94] <author> B. Pasquale, and G. Polyzos, </author> <title> Dynamic I/O Characterization of I/O intensive Scientific Applications, </title> <type> Technical Report No. </type> <institution> CS94-364, University of California, </institution> <address> San Diego, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Recent studies characterizing dynamic i/o patterns in scientific applications <ref> [Kotz94a, Purakayastha94, Pasquale94] </ref> show that many scientific applications have regular patterns of i/o behavior, such as physical periodicity in strided access to multidimensional arrays, or temporal periodicity in checkpoint and restart operations. In both cases all application compute nodes participate in i/o operations to access array data.
Reference: [Pierce93] <author> P. Pierce, </author> <title> A Concurrent File System for a Highly Parallel Mass Storage Subsystem, </title> <booktitle> Proceedings of the 4th Conference on Hypercube Computers and Applications, </booktitle> <address> Monterey, </address> <month> March </month> <year> 1989. </year> , <pages> pp. 155-160. </pages>
Reference-contexts: Without a high level semantic view of the collective i/o requests, the file system is not able to predict whether sequential prefetching will be useful or when to flush the file cache. Intel CFS <ref> [Pierce93] </ref> uses traditional caching, and [Kotz93b] shows that CFS only uses half of the raw disk bandwidth. [Bordawekar93] considers a `two-phase' access strategy for collective i/o.
Reference: [Purakayastha94] <author> A. Purakayastha, C. Ellis, D. Kotz, N. Nieuwejaar, and M. </author> <title> Best, Characterizing Parallel File-Access Patterns on a Large-Scale Multiprocessor, </title> <institution> Duke University Technical Report CS-1994-33, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: Recent studies characterizing dynamic i/o patterns in scientific applications <ref> [Kotz94a, Purakayastha94, Pasquale94] </ref> show that many scientific applications have regular patterns of i/o behavior, such as physical periodicity in strided access to multidimensional arrays, or temporal periodicity in checkpoint and restart operations. In both cases all application compute nodes participate in i/o operations to access array data.
Reference: [Seamons94a] <author> K. E. Seamons and M. Winslett, </author> <title> Physical Schemas for Large Multidimensional Arrays in Scientific Computing Applications, </title> <booktitle> Proceedings of the 7th International Working Conference on Scientific and Statistical Database Management, </booktitle> <address> Charlottesville, Virginia, </address> <month> September </month> <year> 1994. </year>
Reference: [Seamons94b] <author> K. E. Seamons and M. Winslett, </author> <title> An Efficient Abstract Interface for Multidimensional Array I/O, </title> <booktitle> Proceedings of Supercomputing '94, </booktitle> <address> Washington D.C., </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Panda currently supports applications that distribute arrays across all compute nodes using HPF-style BLOCK- and *-based array schemas. By default, for performance and convenience, Panda uses a disk schema that is identical to the memory schema. We refer to this as "natural chunking" <ref> [Seamons94b] </ref>. However, users may override the default by declaring any BLOCK- and *-based schema for disk; this is useful when users know how the data will be accessed in the future and wish to optimize for the future. <p> With a collective i/o implementation, there are more opportunities for disk i/o optimization, as one can form large orderly contiguous disk i/o requests rather than servicing disk i/o requests as they arrive in random order. Several parallel file systems and multidimensional array libraries have provided a collective i/o interface <ref> [Bennett94, Brezany95, Corbett94b, 6 Corbett95, del Rosario94, Karpovich94, Seamons94b, Seligman94] </ref>; some of them have shown that collective i/o provides high performance given appropriate language, compiler and run-time system support. Three approaches to i/o optimization are discussed in [Kotz94b]: traditional caching, two-phase i/o, and disk-directed i/o.

References-found: 18

