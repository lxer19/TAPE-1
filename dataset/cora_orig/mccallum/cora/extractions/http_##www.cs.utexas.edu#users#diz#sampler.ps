URL: http://www.cs.utexas.edu/users/diz/sampler.ps
Refering-URL: http://www.cs.utexas.edu/users/diz/pubs.html
Root-URL: 
Email: fi  
Title: Randomness-Optimal Oblivious Sampling takes as input the output of a defective random source and a
Author: David Zuckerman f f; g m [; ], Pr fi d i= f i Ef 
Keyword: interactive proofs.  
Note: ff &gt; 0, it uses (1 ff)(m log 1 random bits to output d poly(* 1 log 1 m) sample points 1 d 2 f0; 1g m such that for any function  dure which  extractor,  
Date: July 7, 1997  
Abstract: We present the first efficient oblivious sampler that uses an optimal number of Our proof is based on an improved extractor construction. An extractor is a proce of truly random bits, and outputs a nearly-random string. We present the first optimal We give applications to constructive leader election and reducing randomness in random bits, up to an arbitrary constant factor bigger than 1. Specifically, for any
Abstract-found: 1
Intro-found: 1
Reference: [AKS87] <author> M. Ajtai, J. Komlos, and E. Szemeredi. </author> <title> Deterministic simulation in Logspace. </title> <booktitle> In Proceedings of the 19th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 132-140, </pages> <year> 1987. </year>
Reference-contexts: The only known previous technique requires a larger constant times m +k even to amplify RP. It is based on random walks on expanders <ref> [AKS87, CW89, IZ89] </ref> (the sampler of [BGG93] builds on this). However, that uses O (k) samples, whereas ours uses poly (m; k) samples.
Reference: [AN93] <author> N. Alon and M. Naor. </author> <title> Coin-flipping games immune against linear-sized coalitions. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22 </volume> <pages> 403-417, </pages> <year> 1993. </year>
Reference-contexts: Note that such a protocol can also be used to flip a coin that has probability bounded away from 0 and 1 of being heads: the elected leader simply flips the coin and broadcasts the result. Alon and Naor <ref> [AN93] </ref> were the first to exhibit a protocol which is fi-immune for some constant fi &gt; 0. Non-constructively, [BN] improved upon [AN93] to show that there exists a fi-immune protocol for any constant fi &lt; 1=2, which is best possible [Sak89]. These protocols require a linear number of rounds. <p> Alon and Naor <ref> [AN93] </ref> were the first to exhibit a protocol which is fi-immune for some constant fi &gt; 0. Non-constructively, [BN] improved upon [AN93] to show that there exists a fi-immune protocol for any constant fi &lt; 1=2, which is best possible [Sak89]. These protocols require a linear number of rounds. <p> As suggested in [ORV94], we then recurse twice more, and are left with a problem of size o (log log M ). We can then use the non-constructive sequential protocol of <ref> [AN93] </ref>, proved optimal by [BN], which is constructive and fast enough for such a small problem size. 2 We remark that in [ORV94], the committees had size O (log M ), so their protocol had only two levels of recursion, whereas we have three. 14 6 Open Questions One open question
Reference: [AW] <author> R. Armoni and A. Wigderson. </author> <title> Pseudorandomness for space-bounded computations. </title> <type> Unpublished manuscript. </type>
Reference-contexts: The previous best bound was O (l + (q + log g) log l) random bits [BR94]. We also mention that our extractor has been used by <ref> [AW] </ref> to give pseudo-random generators for space-bounded machines, yielding a smooth tradeoff between the generators of [Nis92] and [NZ96]. 4 2 Equivalence of Samplers and Extractors In this section we show the essential equivalence of samplers and extractors.
Reference: [BGG93] <author> M. Bellare, O. Goldreich, and S. Goldwasser. </author> <title> Randomness in interactive proofs. </title> <journal> Computational Complexity, </journal> <volume> 3(4) </volume> <pages> 319-354, </pages> <year> 1993. </year>
Reference-contexts: Full Independence Yes O (m* 2 log fl 1 ) O (* 2 log fl 1 ) [CG89] Pairwise Independence Yes 2m O (* 2 fl 1 ) [Gil93] Random Walks on Expanders Yes m + O (* 2 log fl 1 ) O (* 2 log fl 1 ) <ref> [BGG93] </ref> Pair. Ind. + RW's on Expan. No 2m + O (log fl 1 ) O (* 2 log fl 1 ) [GW94] [BGG93] + Expan./Hashing No m + O (log fl 1 ) O (* 2 log fl 1 ) [BR94] Iterated Sampling Yes O (m + (log m) log <p> (* 2 fl 1 ) [Gil93] Random Walks on Expanders Yes m + O (* 2 log fl 1 ) O (* 2 log fl 1 ) <ref> [BGG93] </ref> Pair. Ind. + RW's on Expan. No 2m + O (log fl 1 ) O (* 2 log fl 1 ) [GW94] [BGG93] + Expan./Hashing No m + O (log fl 1 ) O (* 2 log fl 1 ) [BR94] Iterated Sampling Yes O (m + (log m) log fl 1 ) poly (* 1 ; log fl 1 ; log m) Here Hash-Based Extractors Yes (1 + ff)(m + log fl <p> Thus the only other sampler using a constant times the optimal number of random bits is that of <ref> [BGG93] </ref> (and that of [GW94] building upon it). That sampler also has the advantage of using an optimal number of sample points, up to a constant factor, whereas ours uses a larger but polynomial number. <p> The only known previous technique requires a larger constant times m +k even to amplify RP. It is based on random walks on expanders [AKS87, CW89, IZ89] (the sampler of <ref> [BGG93] </ref> builds on this). However, that uses O (k) samples, whereas ours uses poly (m; k) samples. Using the nice structure of the expanders of [GG81], that can also be made to run in NC. 1 Note that in this definition and future ones there are 5 parameters. <p> Is there an oblivious sampler that simultaneously uses a constant times optimal number of random bits and constant times optimal number of sample points? A non-oblivious sampler with these properties was constructed in <ref> [BGG93] </ref>. Another open question is whether the number of random bits in the sampler can be improved to m + log fl 1 , without any constant. This would correspond to constructing an extractor that extracted all ffin bits of randomness.
Reference: [Blu86] <author> M. Blum. </author> <title> Independent unbiased coin flips from a correlated biased source: a finite Markov chain. </title> <journal> Combinatorica, </journal> <volume> 6(2) </volume> <pages> 97-108, </pages> <year> 1986. </year>
Reference-contexts: The other approach is to assume that the only random sources available are defective, or weak (e.g. <ref> [Blu86, SV86] </ref>). In other words, the "random strings" they output are not uniformly random, but rather just somewhat random. In this paper we deal with both approaches. <p> The only loss is that the new sampler uses a polynomially larger number of samples. First we explain what we mean by general weak random sources. Many models of weak random sources have been studied (e.g. <ref> [Blu86, SV86, CG88, CGH + 85, CW89] </ref>). We associate a source with the probability distribution by which it outputs a string.
Reference: [BN] <author> R. Boppana and B. Narayanan. </author> <title> Perfect-information leader election with optimal resilience. </title> <type> Unpublished manuscript. </type>
Reference-contexts: Alon and Naor [AN93] were the first to exhibit a protocol which is fi-immune for some constant fi &gt; 0. Non-constructively, <ref> [BN] </ref> improved upon [AN93] to show that there exists a fi-immune protocol for any constant fi &lt; 1=2, which is best possible [Sak89]. These protocols require a linear number of rounds. <p> As suggested in [ORV94], we then recurse twice more, and are left with a problem of size o (log log M ). We can then use the non-constructive sequential protocol of [AN93], proved optimal by <ref> [BN] </ref>, which is constructive and fast enough for such a small problem size. 2 We remark that in [ORV94], the committees had size O (log M ), so their protocol had only two levels of recursion, whereas we have three. 14 6 Open Questions One open question is to improve the
Reference: [BR94] <author> M. Bellare and J. Rompel. </author> <title> Randomness-efficient oblivious sampling. </title> <booktitle> In Proceedings of the 35th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 276-287, </pages> <year> 1994. </year>
Reference-contexts: Ind. + RW's on Expan. No 2m + O (log fl 1 ) O (* 2 log fl 1 ) [GW94] [BGG93] + Expan./Hashing No m + O (log fl 1 ) O (* 2 log fl 1 ) <ref> [BR94] </ref> Iterated Sampling Yes O (m + (log m) log fl 1 ) poly (* 1 ; log fl 1 ; log m) Here Hash-Based Extractors Yes (1 + ff)(m + log fl 1 ) poly (* 1 ; log fl 1 ; m) ff &gt; 0 any constant Note that <p> On the other hand, our sampler has the advantages of being oblivious and using an optimal number of random bits up to an arbitrary constant factor bigger than 1. The previous best oblivious sampler used O (log m) random bits times the optimal <ref> [BR94] </ref>. The importance of obliviousness may not be obvious, but it is what is needed for the applications in this paper and in [BR94]. Our sampler also runs in NC. A corollary of our sampler construction is an NC algorithm for deterministic amplification. <p> The previous best oblivious sampler used O (log m) random bits times the optimal <ref> [BR94] </ref>. The importance of obliviousness may not be obvious, but it is what is needed for the applications in this paper and in [BR94]. Our sampler also runs in NC. A corollary of our sampler construction is an NC algorithm for deterministic amplification. <p> We remark that for this application to leader election, we do not need the new extractor; even that in [NZ96] would have sufficed. However, the new extractor makes this construction cleaner. Interactive Proofs with Few Random Bits Oblivious samplers were used in <ref> [BR94] </ref> to reduce the number of random bits needed for interactive proofs. Using a theorem in [BR94], our oblivious sampler implies the following. <p> However, the new extractor makes this construction cleaner. Interactive Proofs with Few Random Bits Oblivious samplers were used in <ref> [BR94] </ref> to reduce the number of random bits needed for interactive proofs. Using a theorem in [BR94], our oblivious sampler implies the following. <p> The previous best bound was O (l + (q + log g) log l) random bits <ref> [BR94] </ref>. We also mention that our extractor has been used by [AW] to give pseudo-random generators for space-bounded machines, yielding a smooth tradeoff between the generators of [Nis92] and [NZ96]. 4 2 Equivalence of Samplers and Extractors In this section we show the essential equivalence of samplers and extractors. <p> Theorems 3.14 and 3.15, * exp (ff 2 log fl N log N ) in Theorem 3.16, and * exp (ff 2 log fl m m) in Theorem 3.17. 4 Interactive Proofs with Few Random Bits To obtain our theorem about interactive proofs, we need only the following theorem from <ref> [BR94] </ref>: Theorem 4.1 [BR94] Suppose we have a (AM) 2g A (g = g (n)) proof system for L in which Arthur's messages are of length l = l (n) and Merlin's messages are of length q = q (n), and suppose we have a strong (r = r (n); l; <p> 3.15, * exp (ff 2 log fl N log N ) in Theorem 3.16, and * exp (ff 2 log fl m m) in Theorem 3.17. 4 Interactive Proofs with Few Random Bits To obtain our theorem about interactive proofs, we need only the following theorem from <ref> [BR94] </ref>: Theorem 4.1 [BR94] Suppose we have a (AM) 2g A (g = g (n)) proof system for L in which Arthur's messages are of length l = l (n) and Merlin's messages are of length q = q (n), and suppose we have a strong (r = r (n); l; s = s
Reference: [CEG95] <author> R. Canetti, G. Even, and O. Goldreich. </author> <title> Lower bounds for sampling algorithms for estimating the average. </title> <journal> Information Processing Letters, </journal> <volume> 53 </volume> <pages> 17-25, </pages> <year> 1995. </year> <month> 15 </month>
Reference-contexts: Not surprisingly, many researchers have worked on producing efficient samplers using few random bits. A summary is given below. For us, the interesting ranges of parameters are * = 1=poly (m) and fl = exp (fi (m)). Due to Method Obliv? Random Bits Sample Points <ref> [CEG95] </ref> Lower Bound Any m + log fl 1 log (O (d)) (* 2 log fl 1 ) Standard Full Independence Yes O (m* 2 log fl 1 ) O (* 2 log fl 1 ) [CG89] Pairwise Independence Yes 2m O (* 2 fl 1 ) [Gil93] Random Walks on <p> The last two paramenters refer to the quality of the sampler. 2 Non-Constructive Upper Bound Note that for small fl, our oblivious sampler uses fewer random bits than the non-explicit construction given in <ref> [CEG95] </ref>, where m + 2 log fl 1 + log log (2*) 1 random bits are used. <p> Here we rectify this situation by showing non-constructively that there is a sampler taking d samples and using m + log fl 1 log (* 2 d) + 4 random bits, for d 2 (log fl 1 )=* 2 . We achieve a better bound than <ref> [CEG95] </ref> by viewing the sampler as an extractor. Extractors for Weak Random Sources Our oblivious samplers are constructed by viewing them in the essentially equivalent form of extractors for general weak random sources. This view turns out to be quite helpful. <p> We show that there exists samplers with better parameters than achieved in <ref> [CEG95] </ref> by first showing the existence of the corresponding extractor. Proposition 2.19 Let positive n, m, ffi and * be given, and set k = ffin. Suppose d (2 ln 2)(2 mk + n k + 3)=* 2 .
Reference: [CG88] <author> B. Chor and O. Goldreich. </author> <title> Unbiased bits from sources of weak randomness and probabilistic communication complexity. </title> <journal> SIAM Journal on Computing, </journal> <volume> 17(2) </volume> <pages> 230-261, </pages> <year> 1988. </year>
Reference-contexts: The only loss is that the new sampler uses a polynomially larger number of samples. First we explain what we mean by general weak random sources. Many models of weak random sources have been studied (e.g. <ref> [Blu86, SV86, CG88, CGH + 85, CW89] </ref>). We associate a source with the probability distribution by which it outputs a string. <p> First we explain what we mean by general weak random sources. Many models of weak random sources have been studied (e.g. [Blu86, SV86, CG88, CGH + 85, CW89]). We associate a source with the probability distribution by which it outputs a string. Extending models in <ref> [CG88, SV86] </ref>, the most general model was studied in [Zuc90]: Definition 1.3 A distribution D on f0; 1g n is called a ffi-source if for all x 2 f0; 1g n , D (x) 2 ffin . <p> A block-wise ffi-source is the same as the PRB-source of <ref> [CG88] </ref> except that here the block length is allowed to vary. 9 3.2 The Construction In this section, we use the word "efficient" to mean in NC.
Reference: [CG89] <author> B. Chor and O. Goldreich. </author> <title> On the power of two-point sampling. </title> <journal> Journal of Complexity, </journal> <volume> 5 </volume> <pages> 96-106, </pages> <year> 1989. </year>
Reference-contexts: Due to Method Obliv? Random Bits Sample Points [CEG95] Lower Bound Any m + log fl 1 log (O (d)) (* 2 log fl 1 ) Standard Full Independence Yes O (m* 2 log fl 1 ) O (* 2 log fl 1 ) <ref> [CG89] </ref> Pairwise Independence Yes 2m O (* 2 fl 1 ) [Gil93] Random Walks on Expanders Yes m + O (* 2 log fl 1 ) O (* 2 log fl 1 ) [BGG93] Pair. Ind. + RW's on Expan.
Reference: [CGH + 85] <author> B. Chor, O. Goldreich, J. H-astad, J. Friedman, S. Rudich, and R. Smolensky. </author> <title> The bit extraction problem or t-resilient functions. </title> <booktitle> In Proceedings of the 26th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 396-407, </pages> <year> 1985. </year>
Reference-contexts: The only loss is that the new sampler uses a polynomially larger number of samples. First we explain what we mean by general weak random sources. Many models of weak random sources have been studied (e.g. <ref> [Blu86, SV86, CG88, CGH + 85, CW89] </ref>). We associate a source with the probability distribution by which it outputs a string.
Reference: [CL95] <author> J. Cooper and N. Linial. </author> <title> Fast perfect-information leader-election protocol with linear immunity. </title> <journal> Combinatorica, </journal> <volume> 15 </volume> <pages> 319-332, </pages> <year> 1995. </year>
Reference-contexts: Non-constructively, [BN] improved upon [AN93] to show that there exists a fi-immune protocol for any constant fi &lt; 1=2, which is best possible [Sak89]. These protocols require a linear number of rounds. The first sublinear protocol was given in <ref> [CL95] </ref>, and recently [ORV94] exhibited an O (log M ) round protocol that is fi-immune for fi a sufficiently small constant. They also showed non-constructively the existence of an O (log M ) round protocol that is fi-immune for any fixed fi &lt; 1=2.
Reference: [CW89] <author> A. Cohen and A. Wigderson. Dispersers, </author> <title> deterministic amplification, and weak random sources. </title> <booktitle> In Proceedings of the 30th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 14-19, </pages> <year> 1989. </year>
Reference-contexts: The only known previous technique requires a larger constant times m +k even to amplify RP. It is based on random walks on expanders <ref> [AKS87, CW89, IZ89] </ref> (the sampler of [BGG93] builds on this). However, that uses O (k) samples, whereas ours uses poly (m; k) samples. <p> The only loss is that the new sampler uses a polynomially larger number of samples. First we explain what we mean by general weak random sources. Many models of weak random sources have been studied (e.g. <ref> [Blu86, SV86, CG88, CGH + 85, CW89] </ref>). We associate a source with the probability distribution by which it outputs a string. <p> We rely heavily on the work of [ORV94], but modify their protocol to make the analysis even simpler. For this application, it is helpful to view the extractor graph-theoretically. The natural way to do this yields a family of highly-expanding uneven bipartite graphs, which have been called dispersers <ref> [San87, Sip88, CW89] </ref>. The dispersers constructed in this way are stronger than those that can be constructed using eigenvalues in the natural way, and have been used to give near-optimal explicit constructions for supercon-centrators, nonblocking networks, and algorithms for sorting and selecting in rounds [WZ95]. <p> We use the name approximating disperser because it is related to two types of dispersers previously defined: one corresponding to RP, and the other to BPP (see <ref> [San87, Sip88, CW89] </ref>). For the RP type, a vertex is bad if none of its neighbors lie in some fixed subset S W , jSj jW j=2.
Reference: [FM88] <author> P. Feldman and S. Micali. </author> <title> Optimal algorithms for Byzantine agreement. </title> <booktitle> In Proceedings of the 20th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 148-161, </pages> <year> 1988. </year>
Reference-contexts: The goal is to elect a leader. The difficulty is that there is a coalition of fiM players that want one of their members elected and may not follow the protocol. This problem is well-studied in the cryptographic setting, where there is a constant expected time Byzantine agreement protocol <ref> [FM88] </ref>. Here we study the full information model: the bad coalition can have infinite computational power, and thus cryptographic tools like one-way functions are useless. A protocol is fi-immune if regardless of which fiM players are dishonest, the protocol chooses an honest leader with probability bounded away from 0.
Reference: [GG81] <author> O. Gabber and Z. Galil. </author> <title> Explicit construction of linear sized superconcentrators. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 22 </volume> <pages> 407-420, </pages> <year> 1981. </year>
Reference-contexts: It is based on random walks on expanders [AKS87, CW89, IZ89] (the sampler of [BGG93] builds on this). However, that uses O (k) samples, whereas ours uses poly (m; k) samples. Using the nice structure of the expanders of <ref> [GG81] </ref>, that can also be made to run in NC. 1 Note that in this definition and future ones there are 5 parameters.
Reference: [Gil93] <author> D. Gillman. </author> <title> A chernoff bound for random walks on expander graphs. </title> <booktitle> In Proceedings of the 34th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 680-691, </pages> <year> 1993. </year>
Reference-contexts: Bits Sample Points [CEG95] Lower Bound Any m + log fl 1 log (O (d)) (* 2 log fl 1 ) Standard Full Independence Yes O (m* 2 log fl 1 ) O (* 2 log fl 1 ) [CG89] Pairwise Independence Yes 2m O (* 2 fl 1 ) <ref> [Gil93] </ref> Random Walks on Expanders Yes m + O (* 2 log fl 1 ) O (* 2 log fl 1 ) [BGG93] Pair. Ind. + RW's on Expan.
Reference: [GW94] <author> O. Goldreich and A. Wigderson. </author> <title> Tiny families of families with random properties: A quality-size trade-off for hashing. </title> <booktitle> In Proceedings of the 26th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 574-583, </pages> <year> 1994. </year>
Reference-contexts: Ind. + RW's on Expan. No 2m + O (log fl 1 ) O (* 2 log fl 1 ) <ref> [GW94] </ref> [BGG93] + Expan./Hashing No m + O (log fl 1 ) O (* 2 log fl 1 ) [BR94] Iterated Sampling Yes O (m + (log m) log fl 1 ) poly (* 1 ; log fl 1 ; log m) Here Hash-Based Extractors Yes (1 + ff)(m + log <p> Thus the only other sampler using a constant times the optimal number of random bits is that of [BGG93] (and that of <ref> [GW94] </ref> building upon it). That sampler also has the advantage of using an optimal number of sample points, up to a constant factor, whereas ours uses a larger but polynomial number.
Reference: [ILL89] <author> R. Impagliazzo, L. A. Levin, and M. Luby. </author> <title> Pseudorandom generation from one-way functions. </title> <booktitle> In Proceedings of the 21st Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 12-24, </pages> <year> 1989. </year>
Reference-contexts: The block-wise extractor in [NZ96] uses a recursive construction of hash functions. It starts with a (weak) extractor built from hash functions: E (x; h) = h (x) ffi h. The Leftover Hash Lemma of <ref> [ILL89] </ref> implies that this is an extractor. A block-wise extractor is then constructed as follows. The block-wise source has blocks with geometrically-decreasing lengths n 1 &gt; n 2 &gt; : : : &gt; n k .
Reference: [IZ89] <author> R. Impagliazzo and D. Zuckerman. </author> <title> How to recycle random bits. </title> <booktitle> In Proceedings of the 30th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 248-253, </pages> <year> 1989. </year>
Reference-contexts: The only known previous technique requires a larger constant times m +k even to amplify RP. It is based on random walks on expanders <ref> [AKS87, CW89, IZ89] </ref> (the sampler of [BGG93] builds on this). However, that uses O (k) samples, whereas ours uses poly (m; k) samples.
Reference: [Lub86] <author> M. Luby. </author> <title> A simple parallel algorithm for the maximal independent set problem. </title> <journal> SIAM Journal on Computing, </journal> <volume> 15(4) </volume> <pages> 1036-1053, </pages> <year> 1986. </year>
Reference-contexts: If the number of random bits can be sufficiently reduced (usually this means to O (log n)), then a deterministic algorithm can often be created by cycling through all possible choices for the random seed (see e.g. <ref> [Lub86] </ref>). The other approach is to assume that the only random sources available are defective, or weak (e.g. [Blu86, SV86]). In other words, the "random strings" they output are not uniformly random, but rather just somewhat random. In this paper we deal with both approaches.
Reference: [Nis92] <author> N. Nisan. </author> <title> Pseudorandom generators for space-bounded computation. </title> <journal> Combinatorica, </journal> <volume> 12(4) </volume> <pages> 449-461, </pages> <year> 1992. </year>
Reference-contexts: The previous best bound was O (l + (q + log g) log l) random bits [BR94]. We also mention that our extractor has been used by [AW] to give pseudo-random generators for space-bounded machines, yielding a smooth tradeoff between the generators of <ref> [Nis92] </ref> and [NZ96]. 4 2 Equivalence of Samplers and Extractors In this section we show the essential equivalence of samplers and extractors. It is helpful to do this by looking graph-theoretically. 2.1 Graph-Theoretic View of Samplers We now define an uneven bipartite graph that is equivalent to a sampler.
Reference: [Nis96] <author> N. Nisan. </author> <title> Extracting randomness: How and why a survey. </title> <booktitle> In Proceedings of the 11th Annual IEEE Conference on Computational Complexity, </booktitle> <pages> pages 44-58, </pages> <year> 1996. </year>
Reference-contexts: In previous definitions, E (x; y) ffi y was required to be close to uniform. Yet this stronger condition was not used in the important applications ([NZ96, WZ95, Zuc96]; see <ref> [Nis96] </ref> for a survey). We can achieve the stronger condition; however, our proofs are cleaner without it. Viewing ffi, n, and * as given, the goal is to make t as small as possible and m as large as possible. This can yield a simulation of BPP using a ffi-source. <p> Note that an approximating disperser is stronger than an RP or BPP type disperser. The dispersers we construct, and that were constructed in [Zuc90, Zuc96, NZ96, WZ95, SSZ95], are better than what can be achieved using eigenvalues and have had numerous applications (see <ref> [WZ95, Nis96] </ref>).
Reference: [NZ96] <author> N. Nisan and D. Zuckerman. </author> <title> Randomness is linear in space. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 52(1) </volume> <pages> 43-52, </pages> <year> 1996. </year>
Reference-contexts: Improving upon [Zuc90], it was shown in [Zuc96] how to simulate BPP using the output from such a source, for ffi a constant. In <ref> [NZ96] </ref>, an extractor was defined and constructed: Definition 1.4 E : f0; 1g n fif0; 1g t ! f0; 1g m is an (n; m; t; ffi; *)-extractor if, for x chosen according to any ffi-source on f0; 1g n and y chosen uniformly at random from f0; 1g t , <p> Although this work is generally more impressive than the case of constant ffi, constant ffi was enough to achieve most of the applications <ref> [NZ96, WZ95, Zuc96] </ref>. In this paper, we achieve the optimal results for constant ffi, up to constant factors. It should be noted, however, that the optimal t up to constant factors results in optimal 2 t up to polynomial factors. <p> The following is a history of work for constant ffi; the big-Oh terms hide constants related to ffi. 2 See Section 3.1 for a definition of statistical distance, or variation distance. 3 Due to t truly random bits m output bits Lower bound <ref> [NZ96] </ref> (log n + log * 1 ) dffin + te [NZ96] O ((log n + log * 1 ) log n) (n) [WZ95] O ((log n + log * 1 ) log n) (ffi ff)n, any ff &gt; 0 Here O (log n + log * 1 ) (ffi ff)n, <p> history of work for constant ffi; the big-Oh terms hide constants related to ffi. 2 See Section 3.1 for a definition of statistical distance, or variation distance. 3 Due to t truly random bits m output bits Lower bound <ref> [NZ96] </ref> (log n + log * 1 ) dffin + te [NZ96] O ((log n + log * 1 ) log n) (n) [WZ95] O ((log n + log * 1 ) log n) (ffi ff)n, any ff &gt; 0 Here O (log n + log * 1 ) (ffi ff)n, any ff &gt; 0 Thus, the main technical contribution of this <p> Applying the dispersers to appropriately pick committees in the [ORV94] protocol gives our constructive result. We remark that for this application to leader election, we do not need the new extractor; even that in <ref> [NZ96] </ref> would have sufficed. However, the new extractor makes this construction cleaner. Interactive Proofs with Few Random Bits Oblivious samplers were used in [BR94] to reduce the number of random bits needed for interactive proofs. Using a theorem in [BR94], our oblivious sampler implies the following. <p> The previous best bound was O (l + (q + log g) log l) random bits [BR94]. We also mention that our extractor has been used by [AW] to give pseudo-random generators for space-bounded machines, yielding a smooth tradeoff between the generators of [Nis92] and <ref> [NZ96] </ref>. 4 2 Equivalence of Samplers and Extractors In this section we show the essential equivalence of samplers and extractors. It is helpful to do this by looking graph-theoretically. 2.1 Graph-Theoretic View of Samplers We now define an uneven bipartite graph that is equivalent to a sampler. <p> For the BPP type, a vertex is bad if a majority of its neighbors lie outside some fixed subset S W , jSj 2jW j=3. Note that an approximating disperser is stronger than an RP or BPP type disperser. The dispersers we construct, and that were constructed in <ref> [Zuc90, Zuc96, NZ96, WZ95, SSZ95] </ref>, are better than what can be achieved using eigenvalues and have had numerous applications (see [WZ95, Nis96]). <p> In fact, by using Proposition 2.18, the tool used in [WZ95], it suffices to improve the output length to (n). Before we outline our construction, we set up the suitable framework, and outline the construction in <ref> [NZ96] </ref>, which uses ideas from [Zuc90, Zuc96]. As in [NZ96], our extractor is composed of a block-wise converter and a block-wise extractor, defined explicitly in [SZ94]. <p> In fact, by using Proposition 2.18, the tool used in [WZ95], it suffices to improve the output length to (n). Before we outline our construction, we set up the suitable framework, and outline the construction in <ref> [NZ96] </ref>, which uses ideas from [Zuc90, Zuc96]. As in [NZ96], our extractor is composed of a block-wise converter and a block-wise extractor, defined explicitly in [SZ94]. <p> The following trivial lemma, implicit in <ref> [NZ96] </ref> and explicit in [SZ94], states that together these constitute an extractor: Lemma 3.6 Given an efficient (n; (l 1 ; : : : ; l s ); t 1 ; ffi; ffi 0 ; * 1 ) block-wise converter and an efficient ((l 1 ; : : :; l s <p> We always use the same block-wise converter, but recursively build stronger block-wise extractors to get our new extractor. Our block-wise converter is from <ref> [NZ96] </ref>, which uses k-wise independence. Actually, the parameters here come from the improved analysis in the final version of [SZ94] (or the algorithm in [NZ96] using random walks on expanders). Lemma 3.7 [NZ96] There is a constant c such that the following holds. <p> We always use the same block-wise converter, but recursively build stronger block-wise extractors to get our new extractor. Our block-wise converter is from <ref> [NZ96] </ref>, which uses k-wise independence. Actually, the parameters here come from the improved analysis in the final version of [SZ94] (or the algorithm in [NZ96] using random walks on expanders). Lemma 3.7 [NZ96] There is a constant c such that the following holds. Let ffi 1=2, n, and n i , 1 i k, be such that P k i=1 n i ffin=4, and let ffi 0 = ffi=c log ffi 1 . <p> Our block-wise converter is from <ref> [NZ96] </ref>, which uses k-wise independence. Actually, the parameters here come from the improved analysis in the final version of [SZ94] (or the algorithm in [NZ96] using random walks on expanders). Lemma 3.7 [NZ96] There is a constant c such that the following holds. Let ffi 1=2, n, and n i , 1 i k, be such that P k i=1 n i ffin=4, and let ffi 0 = ffi=c log ffi 1 . <p> Then for any * there is an efficient (n; (n 1 ; n 2 ; : : : ; n k ); ck (log n + log * 1 ); ffi; ffi 0 ; *) block-wise converter. The block-wise extractor in <ref> [NZ96] </ref> uses a recursive construction of hash functions. It starts with a (weak) extractor built from hash functions: E (x; h) = h (x) ffi h. The Leftover Hash Lemma of [ILL89] implies that this is an extractor. A block-wise extractor is then constructed as follows. <p> These m k bits are then used as the t k1 truly random bits to extract m k1 bits from the block of length n k1 . We continue in this manner until we output m 1 bits from the block of length n 1 . The proof in <ref> [NZ96] </ref> shows that this procedure works (and it makes use of the fact that the procedure starts with the last block and works backwards). We will use the same recursive construction, except we replace the above weak extractor by an arbitrary extractor. <p> We claim that Z 0 is quasi-random to within P k i=1 * i . The proof of this, by induction, is basically the same as the proof in <ref> [NZ96] </ref> for the special case of this lemma when the E i correspond to the weak hash extractors described above. We include the proof here for completeness. We will prove by induction from i = k down to i = 0 the following claim, which clearly implies the lemma. <p> A first attempt would be to use the (n i ; (n i ); O (log 2 n i ); ffi; 1=poly (n i ))-extractors in <ref> [NZ96] </ref>, with varying n i , as follows. Setting n 1 = ffin=8, we let E 1 be a (ffin=8; m 1 = (n); t 1 = c log 2 n; ffi 0 ; 1=poly (n))-extractor. <p> Indeed, this is enough to get the simulation of BPP using a linear number of bits from a ffi-source, for constant ffi. Instead of building E 2 from the original <ref> [NZ96] </ref> extractor, we use the [SZ94] extractor, given in Theorem 3.2. We can now take n 2 = n 1 = ffin=8. Thus t 2 = O (log n) and m 2 = ffi log fl n n, which is bigger than t 1 = O (log 2 n). <p> This gives our result, although in some sense it seems wasteful because m 2 t 1 . Therefore, for E 1 , we use a somewhat more appropriate extractor than the <ref> [NZ96] </ref> extractor, in order to improve the dependence on ffi. This more appropriate extractor is built from the basic [SZ94] extractor given below, which is similar to the [NZ96] extractor except it has much better dependence on ffi: Theorem 3.10 [SZ94] There is a constant c such that for any fi <p> Therefore, for E 1 , we use a somewhat more appropriate extractor than the <ref> [NZ96] </ref> extractor, in order to improve the dependence on ffi. This more appropriate extractor is built from the basic [SZ94] extractor given below, which is similar to the [NZ96] extractor except it has much better dependence on ffi: Theorem 3.10 [SZ94] There is a constant c such that for any fi &gt; 0 and any parameters ffi = ffi (n) and * = *(n) with ffi 1=2 and * exp (ffi 2 n 1fi ), there is an (n; <p> Then we can construct an efficient (n; (ffi )n k; a t (); ffi; a (*() + 2 k ))- extractor. We now choose = ffi=4 and k = ffin=4, and apply Proposition 3.11 to Theorem 3.10. This yields an extractor equivalent to the <ref> [NZ96] </ref> extractor for constant ffi, but better than that for ffi = o (1).
Reference: [ORV94] <author> R. Ostrovsky, S. Rajagopalan, and U. Vazirani. </author> <title> Simple and efficient leader election in the full information model. </title> <booktitle> In Proceedings of the 26th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 234-242, </pages> <year> 1994. </year>
Reference-contexts: Non-constructively, [BN] improved upon [AN93] to show that there exists a fi-immune protocol for any constant fi &lt; 1=2, which is best possible [Sak89]. These protocols require a linear number of rounds. The first sublinear protocol was given in [CL95], and recently <ref> [ORV94] </ref> exhibited an O (log M ) round protocol that is fi-immune for fi a sufficiently small constant. They also showed non-constructively the existence of an O (log M ) round protocol that is fi-immune for any fixed fi &lt; 1=2. <p> Here we give a constructive O (log M ) round protocol that is fi-immune for any fixed fi &lt; 1=2. We also resolve another unsatisfactory feature of the <ref> [ORV94] </ref> protocol: in that protocol each player had to send polynomially many bits per round. In our protocol the players need only send log M bits per round. We rely heavily on the work of [ORV94], but modify their protocol to make the analysis even simpler. <p> We also resolve another unsatisfactory feature of the <ref> [ORV94] </ref> protocol: in that protocol each player had to send polynomially many bits per round. In our protocol the players need only send log M bits per round. We rely heavily on the work of [ORV94], but modify their protocol to make the analysis even simpler. For this application, it is helpful to view the extractor graph-theoretically. The natural way to do this yields a family of highly-expanding uneven bipartite graphs, which have been called dispersers [San87, Sip88, CW89]. <p> Applying the dispersers to appropriately pick committees in the <ref> [ORV94] </ref> protocol gives our constructive result. We remark that for this application to leader election, we do not need the new extractor; even that in [NZ96] would have sufficed. However, the new extractor makes this construction cleaner. <p> The players are synchronized between rounds: all messages sent in round i are received before any are sent in round i + 1. As in many leader election protocols, the protocol of <ref> [ORV94] </ref> works by forming committees. Each committee is a multi-set of players, and each player can be in several committees. One of these committees is selected by some protocol, and then recursively the committee selects a leader from among its own members. Each recursive level of the [ORV94] protocol is composed <p> the protocol of <ref> [ORV94] </ref> works by forming committees. Each committee is a multi-set of players, and each player can be in several committees. One of these committees is selected by some protocol, and then recursively the committee selects a leader from among its own members. Each recursive level of the [ORV94] protocol is composed of O (log M ) rounds, where in each round some of the committees are eliminated. <p> We now analyze the behavior of one recursive level. Let * &lt; 1=2fi be given. Call a committee dangerous if it has more than fi + * fraction of dishonest players. The proof of correctness in <ref> [ORV94] </ref> proceeds by showing that with high probability, a dangerous committee is not selected. 3 Note that our use of M for the number of players and N for the number of committees is the opposite of [ORV94]; however, this will correspond with the letters we use for dispersers. 13 Our <p> The proof of correctness in <ref> [ORV94] </ref> proceeds by showing that with high probability, a dangerous committee is not selected. 3 Note that our use of M for the number of players and N for the number of committees is the opposite of [ORV94]; however, this will correspond with the letters we use for dispersers. 13 Our modification of the [ORV94] protocol is to select the committees differently. We will use a disperser to select the committees. <p> that with high probability, a dangerous committee is not selected. 3 Note that our use of M for the number of players and N for the number of committees is the opposite of <ref> [ORV94] </ref>; however, this will correspond with the letters we use for dispersers. 13 Our modification of the [ORV94] protocol is to select the committees differently. We will use a disperser to select the committees. We also simplify their analysis via the following lemma: Lemma 5.1 Let there be M players and N committees. Suppose there is a p &gt; 0 such that 1. <p> At most N p=2 of the committees are dangerous. 4. N (2M ) 2=p . Then, with probability greater than 1 1=M , the protocol ends after (ln N )=p rounds and a dangerous committee is not selected. Proof. We use ideas from <ref> [ORV94] </ref>. Let s = log N 2M . By the first assumption, the number of committees left after s rounds is at least 2 s N = 2M , so the protocol has not yet terminated. Note that the first two assumptions of the lemma imply that p 1=2. <p> Adding the two probabilities gives the lemma. 2 We now modify the protocol of <ref> [ORV94] </ref> to achieve the first two preconditions of the lemma where each player uses at most log N random bits per round. Number the N 0 remaining committees 0; 1; 2; : : :; N 0 1. <p> Proof. After s = O (log M ) rounds the above protocol reduces the problem to one of size (log M ) O (1) . As suggested in <ref> [ORV94] </ref>, we then recurse twice more, and are left with a problem of size o (log log M ). We can then use the non-constructive sequential protocol of [AN93], proved optimal by [BN], which is constructive and fast enough for such a small problem size. 2 We remark that in [ORV94], <p> <ref> [ORV94] </ref>, we then recurse twice more, and are left with a problem of size o (log log M ). We can then use the non-constructive sequential protocol of [AN93], proved optimal by [BN], which is constructive and fast enough for such a small problem size. 2 We remark that in [ORV94], the committees had size O (log M ), so their protocol had only two levels of recursion, whereas we have three. 14 6 Open Questions One open question is to improve the number of samples.
Reference: [Sak89] <author> M. Saks. </author> <title> A robust noncryptographic protocol for collective coin flipping. </title> <journal> SIAM Journal on Discrete Mathematics, </journal> <volume> 6 </volume> <pages> 240-244, </pages> <year> 1989. </year>
Reference-contexts: Alon and Naor [AN93] were the first to exhibit a protocol which is fi-immune for some constant fi &gt; 0. Non-constructively, [BN] improved upon [AN93] to show that there exists a fi-immune protocol for any constant fi &lt; 1=2, which is best possible <ref> [Sak89] </ref>. These protocols require a linear number of rounds. The first sublinear protocol was given in [CL95], and recently [ORV94] exhibited an O (log M ) round protocol that is fi-immune for fi a sufficiently small constant.
Reference: [San87] <author> M. Santha. </author> <title> On using deterministic functions in probabilistic algorithms. </title> <journal> Information and Computation, </journal> <volume> 74(3) </volume> <pages> 241-249, </pages> <year> 1987. </year> <month> 16 </month>
Reference-contexts: We rely heavily on the work of [ORV94], but modify their protocol to make the analysis even simpler. For this application, it is helpful to view the extractor graph-theoretically. The natural way to do this yields a family of highly-expanding uneven bipartite graphs, which have been called dispersers <ref> [San87, Sip88, CW89] </ref>. The dispersers constructed in this way are stronger than those that can be constructed using eigenvalues in the natural way, and have been used to give near-optimal explicit constructions for supercon-centrators, nonblocking networks, and algorithms for sorting and selecting in rounds [WZ95]. <p> We use the name approximating disperser because it is related to two types of dispersers previously defined: one corresponding to RP, and the other to BPP (see <ref> [San87, Sip88, CW89] </ref>). For the RP type, a vertex is bad if none of its neighbors lie in some fixed subset S W , jSj jW j=2.
Reference: [Sip88] <author> M. Sipser. Expanders, </author> <title> randomness or time vs. space. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 36 </volume> <pages> 379-383, </pages> <year> 1988. </year>
Reference-contexts: We rely heavily on the work of [ORV94], but modify their protocol to make the analysis even simpler. For this application, it is helpful to view the extractor graph-theoretically. The natural way to do this yields a family of highly-expanding uneven bipartite graphs, which have been called dispersers <ref> [San87, Sip88, CW89] </ref>. The dispersers constructed in this way are stronger than those that can be constructed using eigenvalues in the natural way, and have been used to give near-optimal explicit constructions for supercon-centrators, nonblocking networks, and algorithms for sorting and selecting in rounds [WZ95]. <p> We use the name approximating disperser because it is related to two types of dispersers previously defined: one corresponding to RP, and the other to BPP (see <ref> [San87, Sip88, CW89] </ref>). For the RP type, a vertex is bad if none of its neighbors lie in some fixed subset S W , jSj jW j=2.
Reference: [SSZ95] <author> M. Saks, A. Srinivasan, and S. Zhou. </author> <title> Explicit dispersers with polylog degree. </title> <booktitle> In Proceedings of the 27th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 479-488, </pages> <year> 1995. </year>
Reference-contexts: Most of the recent progress in this area was made for ffi = o (1) <ref> [SZ94, SSZ95, TS96] </ref>. In particular, building upon [SZ94], Ta-Shma [TS96] gave an extractor which uses t = polylog (n; * 1 ) truly random bits to extract all ffin bits of randomness. <p> For the BPP type, a vertex is bad if a majority of its neighbors lie outside some fixed subset S W , jSj 2jW j=3. Note that an approximating disperser is stronger than an RP or BPP type disperser. The dispersers we construct, and that were constructed in <ref> [Zuc90, Zuc96, NZ96, WZ95, SSZ95] </ref>, are better than what can be achieved using eigenvalues and have had numerous applications (see [WZ95, Nis96]). <p> This brings up another important extractor question: is there an extractor which adds O (log n) truly random bits to a ffi-source with ffin = n fi and outputs n fi 0 bits, where 0 &lt; fi 0 fi &lt; 1? Saks, Srinivasan, and Zhou <ref> [SSZ95] </ref> achieved the corresponding result for RP. Ta-Shma can almost construct the extractor, but is off by a factor of a logarithm iterated any constant number of times.
Reference: [SV86] <author> M. Santha and U. V. Vazirani. </author> <title> Generating quasi-random sequences from semi-random sources. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 33 </volume> <pages> 75-87, </pages> <year> 1986. </year>
Reference-contexts: The other approach is to assume that the only random sources available are defective, or weak (e.g. <ref> [Blu86, SV86] </ref>). In other words, the "random strings" they output are not uniformly random, but rather just somewhat random. In this paper we deal with both approaches. <p> The only loss is that the new sampler uses a polynomially larger number of samples. First we explain what we mean by general weak random sources. Many models of weak random sources have been studied (e.g. <ref> [Blu86, SV86, CG88, CGH + 85, CW89] </ref>). We associate a source with the probability distribution by which it outputs a string. <p> First we explain what we mean by general weak random sources. Many models of weak random sources have been studied (e.g. [Blu86, SV86, CG88, CGH + 85, CW89]). We associate a source with the probability distribution by which it outputs a string. Extending models in <ref> [CG88, SV86] </ref>, the most general model was studied in [Zuc90]: Definition 1.3 A distribution D on f0; 1g n is called a ffi-source if for all x 2 f0; 1g n , D (x) 2 ffin .
Reference: [SZ94] <author> A. Srinivasan and D. Zuckerman. </author> <title> Computing with very weak random sources. </title> <booktitle> In Proceedings of the 35th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 264-275, </pages> <year> 1994. </year> <note> Submitted for journal publication. Revised version may be obtained from http://www.cs.utexas.edu/users/diz. </note>
Reference-contexts: Most of the recent progress in this area was made for ffi = o (1) <ref> [SZ94, SSZ95, TS96] </ref>. In particular, building upon [SZ94], Ta-Shma [TS96] gave an extractor which uses t = polylog (n; * 1 ) truly random bits to extract all ffin bits of randomness. <p> Most of the recent progress in this area was made for ffi = o (1) [SZ94, SSZ95, TS96]. In particular, building upon <ref> [SZ94] </ref>, Ta-Shma [TS96] gave an extractor which uses t = polylog (n; * 1 ) truly random bits to extract all ffin bits of randomness. <p> log n) (ffi ff)n, any ff &gt; 0 Here O (log n + log * 1 ) (ffi ff)n, any ff &gt; 0 Thus, the main technical contribution of this paper is to eliminate the ffi log fl n factor in the number of output bits of the extractor in <ref> [SZ94] </ref>. This yields the first simulation of BPP using a linear number of random bits from a ffi-source. <p> The reader is advised to ignore the lower bounds on ffi and * on the first reading, and think of ffi as a constant and * = 1=poly (n). Our construction uses and improves the construction in <ref> [SZ94] </ref>. The following is from the final version of [SZ94]: Theorem 3.2 [SZ94] There is a constant c such that for any fi &gt; 0 and any parameters ffi = ffi (n) and * = *(n) with ffi n 1=2 log fl n and * exp (ffi log fl n n <p> The reader is advised to ignore the lower bounds on ffi and * on the first reading, and think of ffi as a constant and * = 1=poly (n). Our construction uses and improves the construction in <ref> [SZ94] </ref>. The following is from the final version of [SZ94]: Theorem 3.2 [SZ94] There is a constant c such that for any fi &gt; 0 and any parameters ffi = ffi (n) and * = *(n) with ffi n 1=2 log fl n and * exp (ffi log fl n n 1fi ), there is an (n; m = ffi <p> The reader is advised to ignore the lower bounds on ffi and * on the first reading, and think of ffi as a constant and * = 1=poly (n). Our construction uses and improves the construction in <ref> [SZ94] </ref>. The following is from the final version of [SZ94]: Theorem 3.2 [SZ94] There is a constant c such that for any fi &gt; 0 and any parameters ffi = ffi (n) and * = *(n) with ffi n 1=2 log fl n and * exp (ffi log fl n n 1fi ), there is an (n; m = ffi log fl n <p> Before we outline our construction, we set up the suitable framework, and outline the construction in [NZ96], which uses ideas from [Zuc90, Zuc96]. As in [NZ96], our extractor is composed of a block-wise converter and a block-wise extractor, defined explicitly in <ref> [SZ94] </ref>. <p> The following trivial lemma, implicit in [NZ96] and explicit in <ref> [SZ94] </ref>, states that together these constitute an extractor: Lemma 3.6 Given an efficient (n; (l 1 ; : : : ; l s ); t 1 ; ffi; ffi 0 ; * 1 ) block-wise converter and an efficient ((l 1 ; : : :; l s ); m; t 2 <p> We always use the same block-wise converter, but recursively build stronger block-wise extractors to get our new extractor. Our block-wise converter is from [NZ96], which uses k-wise independence. Actually, the parameters here come from the improved analysis in the final version of <ref> [SZ94] </ref> (or the algorithm in [NZ96] using random walks on expanders). Lemma 3.7 [NZ96] There is a constant c such that the following holds. <p> We will use the same recursive construction, except we replace the above weak extractor by an arbitrary extractor. This idea was used in <ref> [SZ94] </ref>, although the following lemma was not stated explicitly, nor exploited in full. <p> Indeed, this is enough to get the simulation of BPP using a linear number of bits from a ffi-source, for constant ffi. Instead of building E 2 from the original [NZ96] extractor, we use the <ref> [SZ94] </ref> extractor, given in Theorem 3.2. We can now take n 2 = n 1 = ffin=8. Thus t 2 = O (log n) and m 2 = ffi log fl n n, which is bigger than t 1 = O (log 2 n). <p> Therefore, for E 1 , we use a somewhat more appropriate extractor than the [NZ96] extractor, in order to improve the dependence on ffi. This more appropriate extractor is built from the basic <ref> [SZ94] </ref> extractor given below, which is similar to the [NZ96] extractor except it has much better dependence on ffi: Theorem 3.10 [SZ94] There is a constant c such that for any fi &gt; 0 and any parameters ffi = ffi (n) and * = *(n) with ffi 1=2 and * exp <p> This more appropriate extractor is built from the basic <ref> [SZ94] </ref> extractor given below, which is similar to the [NZ96] extractor except it has much better dependence on ffi: Theorem 3.10 [SZ94] There is a constant c such that for any fi &gt; 0 and any parameters ffi = ffi (n) and * = *(n) with ffi 1=2 and * exp (ffi 2 n 1fi ), there is an (n; m = ffi 2 n=c log ffi 1 ; t = c
Reference: [TS96] <author> A. Ta-Shma. </author> <title> On extracting randomness from weak random sources. </title> <booktitle> In Proceedings of the 28th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 276-285, </pages> <year> 1996. </year>
Reference-contexts: Most of the recent progress in this area was made for ffi = o (1) <ref> [SZ94, SSZ95, TS96] </ref>. In particular, building upon [SZ94], Ta-Shma [TS96] gave an extractor which uses t = polylog (n; * 1 ) truly random bits to extract all ffin bits of randomness. <p> Most of the recent progress in this area was made for ffi = o (1) [SZ94, SSZ95, TS96]. In particular, building upon [SZ94], Ta-Shma <ref> [TS96] </ref> gave an extractor which uses t = polylog (n; * 1 ) truly random bits to extract all ffin bits of randomness. Although this work is generally more impressive than the case of constant ffi, constant ffi was enough to achieve most of the applications [NZ96, WZ95, Zuc96]. <p> This would correspond to constructing an extractor that extracted all ffin bits of randomness. Such an improvement would result in strong improvements in the expanders constructed in [WZ95] and in the applications there. Ta-Shma <ref> [TS96] </ref> can extract all the randomness by adding polylog truly random bits, which amounts to a quasi-polynomial number of samples. Indeed, Ta-Shma's extractor works for any amount of entropy ffin.
Reference: [WZ95] <author> A. Wigderson and D. Zuckerman. </author> <title> Expanders that beat the eigenvalue bound: Explicit construction and applications. </title> <journal> Combinatorica. </journal> <note> To appear. Revised version appears as Technical Report TR-95-21, </note> <institution> Department of Computer Sciences, The University of Texas at Austin, </institution> <month> June </month> <year> 1995. </year> <booktitle> Preliminary version in Proceedings of the 25th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 245-251, </pages> <year> 1993. </year>
Reference-contexts: This view turns out to be quite helpful. Besides giving our main construction and the non-explicit construction above, it can be used to prove an unintuitive proposition by translating a result in <ref> [WZ95] </ref> into the language of samplers. In particular, if there is an efficient sampler that uses a constant times optimal number of random bits, then there is one using the optimal number times an arbitrary constant factor bigger than one. <p> Although this work is generally more impressive than the case of constant ffi, constant ffi was enough to achieve most of the applications <ref> [NZ96, WZ95, Zuc96] </ref>. In this paper, we achieve the optimal results for constant ffi, up to constant factors. It should be noted, however, that the optimal t up to constant factors results in optimal 2 t up to polynomial factors. <p> to ffi. 2 See Section 3.1 for a definition of statistical distance, or variation distance. 3 Due to t truly random bits m output bits Lower bound [NZ96] (log n + log * 1 ) dffin + te [NZ96] O ((log n + log * 1 ) log n) (n) <ref> [WZ95] </ref> O ((log n + log * 1 ) log n) (ffi ff)n, any ff &gt; 0 Here O (log n + log * 1 ) (ffi ff)n, any ff &gt; 0 Thus, the main technical contribution of this paper is to eliminate the ffi log fl n factor in the <p> The dispersers constructed in this way are stronger than those that can be constructed using eigenvalues in the natural way, and have been used to give near-optimal explicit constructions for supercon-centrators, nonblocking networks, and algorithms for sorting and selecting in rounds <ref> [WZ95] </ref>. Applying the dispersers to appropriately pick committees in the [ORV94] protocol gives our constructive result. We remark that for this application to leader election, we do not need the new extractor; even that in [NZ96] would have sufficed. However, the new extractor makes this construction cleaner. <p> For the BPP type, a vertex is bad if a majority of its neighbors lie outside some fixed subset S W , jSj 2jW j=3. Note that an approximating disperser is stronger than an RP or BPP type disperser. The dispersers we construct, and that were constructed in <ref> [Zuc90, Zuc96, NZ96, WZ95, SSZ95] </ref>, are better than what can be achieved using eigenvalues and have had numerous applications (see [WZ95, Nis96]). <p> Note that an approximating disperser is stronger than an RP or BPP type disperser. The dispersers we construct, and that were constructed in [Zuc90, Zuc96, NZ96, WZ95, SSZ95], are better than what can be achieved using eigenvalues and have had numerous applications (see <ref> [WZ95, Nis96] </ref>). <p> Note that in such a statement, and in similar statements throughout the paper, we refer to one sampler, but we really mean a family of samplers. The corresponding proposition about extractors is: Proposition 2.18 <ref> [WZ95] </ref> If there is an efficient (n; (ffin); t; ffi; *)-extractor E, then for any ff &gt; 0 there is an efficient (n; (ffi ff)n; O (t); ffi; *)-extractor E 0 . The intuition for the proof, given in [WZ95], is as follows. <p> The corresponding proposition about extractors is: Proposition 2.18 <ref> [WZ95] </ref> If there is an efficient (n; (ffin); t; ffi; *)-extractor E, then for any ff &gt; 0 there is an efficient (n; (ffi ff)n; O (t); ffi; *)-extractor E 0 . The intuition for the proof, given in [WZ95], is as follows. When we extract randomness from x using E and truly random bits y 1 , even conditional on the output bits E (x; y 1 ) there is a lot of randomness left in x. <p> However, this is usually not in the range of interest. Thus, our goal is to improve the output length m from ffi log fl n to (ffi ff)n while not increasing t by more than a constant factor. In fact, by using Proposition 2.18, the tool used in <ref> [WZ95] </ref>, it suffices to improve the output length to (n). Before we outline our construction, we set up the suitable framework, and outline the construction in [NZ96], which uses ideas from [Zuc90, Zuc96]. <p> We also make use of the following lemma in <ref> [WZ95] </ref>, which is the general, more precise statement corresponding to Proposition 2.18. 11 Proposition 3.11 [WZ95] Fix positive integers n and k. <p> We also make use of the following lemma in <ref> [WZ95] </ref>, which is the general, more precise statement corresponding to Proposition 2.18. 11 Proposition 3.11 [WZ95] Fix positive integers n and k. Suppose that for each ffi 2 [; 1] we are given an efficient (n; m (ffi); t (ffi); ffi; *(ffi))-extractor, where t and * are non-increasing functions of ffi. Let f (ffi) = m (ffi)=(ffin), and suppose f is non-decreasing. <p> This would correspond to constructing an extractor that extracted all ffin bits of randomness. Such an improvement would result in strong improvements in the expanders constructed in <ref> [WZ95] </ref> and in the applications there. Ta-Shma [TS96] can extract all the randomness by adding polylog truly random bits, which amounts to a quasi-polynomial number of samples. Indeed, Ta-Shma's extractor works for any amount of entropy ffin.
Reference: [Zuc90] <author> D. Zuckerman. </author> <title> General weak random sources. </title> <booktitle> In Proceedings of the 31st Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 534-543, </pages> <year> 1990. </year>
Reference-contexts: Many models of weak random sources have been studied (e.g. [Blu86, SV86, CG88, CGH + 85, CW89]). We associate a source with the probability distribution by which it outputs a string. Extending models in [CG88, SV86], the most general model was studied in <ref> [Zuc90] </ref>: Definition 1.3 A distribution D on f0; 1g n is called a ffi-source if for all x 2 f0; 1g n , D (x) 2 ffin . Improving upon [Zuc90], it was shown in [Zuc96] how to simulate BPP using the output from such a source, for ffi a constant. <p> Extending models in [CG88, SV86], the most general model was studied in <ref> [Zuc90] </ref>: Definition 1.3 A distribution D on f0; 1g n is called a ffi-source if for all x 2 f0; 1g n , D (x) 2 ffin . Improving upon [Zuc90], it was shown in [Zuc96] how to simulate BPP using the output from such a source, for ffi a constant. <p> For the BPP type, a vertex is bad if a majority of its neighbors lie outside some fixed subset S W , jSj 2jW j=3. Note that an approximating disperser is stronger than an RP or BPP type disperser. The dispersers we construct, and that were constructed in <ref> [Zuc90, Zuc96, NZ96, WZ95, SSZ95] </ref>, are better than what can be achieved using eigenvalues and have had numerous applications (see [WZ95, Nis96]). <p> In fact, by using Proposition 2.18, the tool used in [WZ95], it suffices to improve the output length to (n). Before we outline our construction, we set up the suitable framework, and outline the construction in [NZ96], which uses ideas from <ref> [Zuc90, Zuc96] </ref>. As in [NZ96], our extractor is composed of a block-wise converter and a block-wise extractor, defined explicitly in [SZ94].
Reference: [Zuc96] <author> D. Zuckerman. </author> <title> Simulating BPP using a general weak random source. </title> <journal> Algorithmica, </journal> <volume> 16 </volume> <pages> 367-391, </pages> <year> 1996. </year>
Reference-contexts: Extending models in [CG88, SV86], the most general model was studied in [Zuc90]: Definition 1.3 A distribution D on f0; 1g n is called a ffi-source if for all x 2 f0; 1g n , D (x) 2 ffin . Improving upon [Zuc90], it was shown in <ref> [Zuc96] </ref> how to simulate BPP using the output from such a source, for ffi a constant. <p> Namely, given a BPP machine M which uses m random bits to achieve error :1 and an (n; m; t; ffi=2; 1=3)-extractor, cycling through all possible t-bit strings yields a simulation of BPP using n bits from a ffi-source and requiring 2 t runs of M <ref> [Zuc96] </ref>. (What is important above is that :1 + 1=3 &lt; 1=2; an exponentially small error in the simulation is achieved because the extractor is designed for a ffi=2-source rather than a ffi-source.) Thus for this purpose it is desirable to have t = O (log n) and m as large <p> Although this work is generally more impressive than the case of constant ffi, constant ffi was enough to achieve most of the applications <ref> [NZ96, WZ95, Zuc96] </ref>. In this paper, we achieve the optimal results for constant ffi, up to constant factors. It should be noted, however, that the optimal t up to constant factors results in optimal 2 t up to polynomial factors. <p> For the BPP type, a vertex is bad if a majority of its neighbors lie outside some fixed subset S W , jSj 2jW j=3. Note that an approximating disperser is stronger than an RP or BPP type disperser. The dispersers we construct, and that were constructed in <ref> [Zuc90, Zuc96, NZ96, WZ95, SSZ95] </ref>, are better than what can be achieved using eigenvalues and have had numerous applications (see [WZ95, Nis96]). <p> In fact, by using Proposition 2.18, the tool used in [WZ95], it suffices to improve the output length to (n). Before we outline our construction, we set up the suitable framework, and outline the construction in [NZ96], which uses ideas from <ref> [Zuc90, Zuc96] </ref>. As in [NZ96], our extractor is composed of a block-wise converter and a block-wise extractor, defined explicitly in [SZ94].
References-found: 34

