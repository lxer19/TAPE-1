URL: http://www-sal.cs.uiuc.edu/~pitt/Papers/experts.ps
Refering-URL: http://www-sal.cs.uiuc.edu/~pitt/
Root-URL: http://www.cs.uiuc.edu
Email: Email: dph@cse.ucsc.edu  Email: kwek@cs.wustl.edu  Email: pitt@cs.uiuc.edu  
Phone: 2  3  
Title: Learning When to Trust Which Experts  
Author: David Helmbold and Stephen Kwek and Leonard Pitt 
Address: Cruz, CA, 95064, U.S.A.,  St. Louis, MO 63130,  Urbana, IL 61801, U.S.A.,  
Affiliation: 1 University of California at Santa Cruz, Department of Computer Science, Santa  Department of Computer Science, Washington University,  University of Illinois at Urbana-Champaign, Department of Computer Science,  
Abstract: The standard model for prediction using a pool of experts has an underlying assumption that one of the experts performs well. In this paper, we show that this assumption does not take advantage of situations where both the outcome and the experts' predictions are based on some input which the learner gets to observe too. In particular, we exhibit a situation where each individual expert performs badly but collectively they perform well, and show that the traditional weighted majority techniques perform poorly. To capture this notion of `the whole is often greater than the sum of its parts', we propose an approach to measure the overall competency of a pool of experts with respect to a competency class or structure. A competency class or structure is a set of decompositions of the instance space where each expert is associated with a `competency region' in which we assume he is competent. Our goal is to perform close to the performance of a predictor who knows the best decomposition in the competency class or structure where each expert performs reasonably well in its competency region. We present both positive and negative results in our model. 
Abstract-found: 1
Intro-found: 1
Reference: [AW95] <author> P. Auer and M. K. Warmuth. </author> <title> Tracking the best disjunction. </title> <booktitle> In Proc. of the 36th Symposium on the Foundations of Comp. Sci., </booktitle> <pages> pages 312-321. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1995. </year>
Reference-contexts: Winnow2 can be used to agnostically learn disjunctions of k literals with a mistake bound of O (k (log N + M)) <ref> [Lit88, Lit91, AW95] </ref> where N is the number of variables and M is the number of mistakes made in the entire sequence of trials using the best disjunction. Thus, we can apply Winnow2 to learn h agnostically and obtained a learning algorithm that has a smaller mistake bound. Theorem 9.
Reference: [CBFHW95] <author> N. Cesa-Bianchi, Y. Freund, D. P. Helmbold, and M. K. Warmuth. </author> <title> On--line prediction and conversion strategies. </title> <booktitle> Machine Learning, </booktitle> <year> 1995. </year> <note> To appear, an extended abstract appeared in Eurocolt `93. </note>
Reference-contexts: Recently, an alternate weighting scheme, called the binomial weighting scheme, has been suggested by Cesa-Bianchi et al. <ref> [CBFHW95] </ref> and is shown to have a better mistake bound. Suppose the experts' predictions and the actual outcome depend on some input (instance) in each trial. Notice that neither the binomial weighting scheme nor the exponential weighting scheme makes use of the instance when calculating the weights of the experts.
Reference: [Lit88] <author> N. Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: We then continue to exhibit a simple learning algorithm for learning the same competency class with a smaller mistake bound. In doing so, we illustrate that identifying an optimum covering may not be the best way of learning a competency class. Both algorithms are based on Littlestone's <ref> [Lit88] </ref> Winnow algorithm for learning r-literal disjunctions which has the ability to pick up the relevant attributes quickly even when there are (exponentially) many irrelevant attributes. Our results also imply that the competency class of k-DNF formulas is identifiable and learnable when k is fixed. <p> The main idea of our algorithm for learning the competency class of disjunctions of literals is to run an algorithm, which is a hybrid of Littlestone's Winnow1 and Winnow2 algorithms <ref> [Lit88] </ref>, for each expert E i to learn its competency region C opt i in an optimum covering C opt . The learner maintains a hypothesis h i for the disjunction representing each C opt i in the optimal covering. <p> We begin by proving some simple lemmas similar to those used in proving the mistake bound of Winnow <ref> [Lit88] </ref>. Let u and v be the number of promotion steps and -elimination steps, respectively. Then the number of mistakes is simply the sum of u and v. Lemma 5. v ff 1 + ffku: Proof: Consider the sum S = i=1 j=1 S is always non-negative. <p> Winnow2 can be used to agnostically learn disjunctions of k literals with a mistake bound of O (k (log N + M)) <ref> [Lit88, Lit91, AW95] </ref> where N is the number of variables and M is the number of mistakes made in the entire sequence of trials using the best disjunction. Thus, we can apply Winnow2 to learn h agnostically and obtained a learning algorithm that has a smaller mistake bound. Theorem 9.
Reference: [Lit91] <author> N. Littlestone. </author> <title> Redundant noisy attributes, attribute errors, and linear threshold learning using Winnow. </title> <booktitle> In Proc. 4th Annu. Workshop on Com-put. Learning Theory, </booktitle> <pages> pages 147-156, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Winnow2 can be used to agnostically learn disjunctions of k literals with a mistake bound of O (k (log N + M)) <ref> [Lit88, Lit91, AW95] </ref> where N is the number of variables and M is the number of mistakes made in the entire sequence of trials using the best disjunction. Thus, we can apply Winnow2 to learn h agnostically and obtained a learning algorithm that has a smaller mistake bound. Theorem 9.
Reference: [LW94] <author> N. Littlestone and M. K. Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108(2) </volume> <pages> 212-261, </pages> <year> 1994. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: 1 Introduction 1.1 The Deficiency of the Standard Weighted Majority Techniques for Prediction Using a Pool of Experts In <ref> [LW94] </ref>, Littlestone and Warmuth study the problem of making on-line prediction using a pool of experts. In their model, the learner faces a (possibly infinite) sequence of trials, with a boolean prediction to be made in each trial, and the goal of the learner is to make few mistakes. <p> When a mistake is made, the learner simply multiplies the weights of those experts that predict wrongly by some fixed non-negative constant fi smaller than one. The following mistake bounds are obtained for the weighted majority algorithm. Theorem 1. <ref> [LW94] </ref> Given a pool of experts E , on any (possibly infinite) sequence of trials, the weighted majority algorithm makes at most a) O (log jEj + ) mistakes, if one of the experts makes at most mistakes. b) O (log (jE j=k)+m=k) mistakes, if there is a subset of k <p> We now restrict our attention to coverings with disjoint intervals. When there are k experts and the instance space has d points then there are d+1 such coverings. If the set of experts is completely competent, then the halving algorithm <ref> [LW94] </ref> can be used to learn the competency class while making at most k log dk mistakes. We refine this approach to overcome two difficulties - the halving algorithm is not generally efficient, and the experts may not be completely competent.
References-found: 5

