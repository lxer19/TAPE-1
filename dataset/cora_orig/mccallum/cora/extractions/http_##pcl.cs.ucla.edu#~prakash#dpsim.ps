URL: http://pcl.cs.ucla.edu/~prakash/dpsim.ps
Refering-URL: http://pcl.cs.ucla.edu/~prakash/
Root-URL: http://www.cs.ucla.edu
Title: Parallel Simulation of Data parallel Programs  
Author: Sundeep Prakash and Rajive Bagrodia 
Address: Los Angeles, CA 90024  
Affiliation: Computer Science Department, University of California,  
Abstract: Accurate simulations of parallel programs for large datasets can often be slow; parallel execution has been shown to offer significant potential in reducing the execution time of many discrete-event simulators. In this paper, we describe the design and implementation of a parallel simulator called DPSIM that simulates the execution of data parallel programs on contemporary message-passing parallel architectures. The simulator has been implemented on the IBM SPx using a conservative synchronization algorithm. This paper also describes the use of the simulator in evaluating the impact of architectural characteristics like processor speed and message communication latency on the performance of scientific applications including Gauss Jordan elimination and matrix multiplication.
Abstract-found: 1
Intro-found: 1
Reference: [BDCW91] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl. PRO-TEUS: </author> <title> A High-Performance Parallel-Architecture Simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cam-bridge, MA 02139, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Simulators for parallel programs can be effectively utilized to test, debug, and predict performance of parallel programs on a diverse set of parallel architectures. A variety of simulators have been designed <ref> [BDCW91, DGH91, RHL + 93, CDJ + 91, DHN94] </ref> to estimate the performance of a parallel program. Most simulators were designed to estimate the performance of asynchronous or task parallel programs. <p> Existing simulators include Proteus <ref> [BDCW91] </ref>, Tango [DGH91] and the RPPT simulation engine [CDJ + 91] among others. In Proteus, the application to be simulated is written in a superset of C, and constructs are provided to control the placement of data.
Reference: [BKM] <author> R. Bagrodia, K.M.Chandy, and M.Dhagat. </author> <title> UC: a set-based language for data parallel programming. </title> <note> To appear. </note>
Reference-contexts: Clearly parallel execution of the simulation will not incur additional synchronization overhead compared to the sequential execution and hence has significant potential for performance improvements. 4 Experiments 4.1 Simulator Our results are based on a simulator written for a compiler of the data parallel language UC <ref> [BKM] </ref> on the IBM-SP2. The compiler translates UC code into Maisie [BtL94] 4 with MPL calls for fast communication and synchronization over the HPS (high performance switch). The resulting program executes as one single-threaded process per processor.
Reference: [BtL94] <author> R. Bagrodia and Wen toh Liao. Maisie: </author> <title> A language for design of efficient discret-event simulations. </title> <journal> IEEE Transactions on Software Engineering, </journal> <month> April </month> <year> 1994. </year>
Reference-contexts: A non-deterministic communication pattern is one in which the sequence of messages that is accepted at a processor cannot be predetermined. We next present some examples of data parallel code and their equivalent SPMD code, in order to explain the communication pattern classification. We assume the following Maisie <ref> [BtL94] </ref> like syntax for message passing calls in the SPMD code: - (send to &lt; dest &gt; message &lt; msgtype &gt; f&lt; msg &gt;g;): This statement describes the send of a message &lt; msg &gt; to a process &lt; dest &gt;. &lt; msgtype &gt; is a user defined message type, and <p> Implementations of optimistic algorithms are usually more difficult because they require complex mechanisms for detection and handling of causality errors, termination detection, exception handling, and memory management. A comprehensive discussion of parallel discrete-event simulations may be found in [Fuj90, Mis86]. A simulation language called Maisie <ref> [BtL94] </ref> has been designed at UCLA to support the execution of parallel discrete-event models. Maisie is a C-based discrete-event simulation language that was designed to 7 cleanly separate a simulation model from the underlying algorithm (sequen-tial or parallel) used for the execution of the model. <p> The compiler translates UC code into Maisie <ref> [BtL94] </ref> 4 with MPL calls for fast communication and synchronization over the HPS (high performance switch). The resulting program executes as one single-threaded process per processor. The simulator is simply a modified version of the same compiler, which: Also produces Maisie code.
Reference: [CDJ + 91] <author> R.G. Covington, S. Dwarkadas, J.R. Jump, J.B. Sinclair, and S. Madala. </author> <title> The efficient simulation of parallel computer systems. </title> <journal> International Journal in Computer Simulation, </journal> <volume> 1 </volume> <pages> 31-58, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Simulators for parallel programs can be effectively utilized to test, debug, and predict performance of parallel programs on a diverse set of parallel architectures. A variety of simulators have been designed <ref> [BDCW91, DGH91, RHL + 93, CDJ + 91, DHN94] </ref> to estimate the performance of a parallel program. Most simulators were designed to estimate the performance of asynchronous or task parallel programs. <p> Existing simulators include Proteus [BDCW91], Tango [DGH91] and the RPPT simulation engine <ref> [CDJ + 91] </ref> among others. In Proteus, the application to be simulated is written in a superset of C, and constructs are provided to control the placement of data. Provided library routines are used for message passing, thread management, memory management and data collection.
Reference: [CMM + 88] <author> R.G. Covington, S. Madala, V. Mehta, J.R. Jump, and J.B. Sinclair. </author> <title> The Rice Parallel Processing Testbed. </title> <booktitle> In Proceedings of the 1988 ACM SIG-METRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1988. </year> <month> 14 </month>
Reference-contexts: When viewed as a discrete-event model, each process of the compiled program contains only three types of events: basic blocks, or code that does not involve communication with another process, send events, and receive events. Although it is possible to simulate basic blocks, most simulators use direct execution <ref> [CMM + 88, MF88] </ref> as this has been shown to be more efficient.
Reference: [CS89a] <author> K.M. Chandy and R. Sherman. </author> <title> The conditional event approach to dis-tributed simulation. </title> <booktitle> In Distributed Simulation Conference, </booktitle> <address> Miami, </address> <year> 1989. </year>
Reference-contexts: This constraint may introduce deadlocks, which are typically handled by incorporating deadlock detection [Mis86] or deadlock avoidance <ref> [Mis86, CS89a] </ref> mechanisms into the simulation algorithm. Optimistic algorithms [Jef85, CS89b] allow an LP to process messages out of order; causality errors are corrected by using rollbacks and recomputations.
Reference: [CS89b] <author> K.M. Chandy and R. Sherman. </author> <title> Space-time and simulation. </title> <booktitle> In Distributed Simulation Conference, </booktitle> <address> Miami, </address> <year> 1989. </year>
Reference-contexts: This constraint may introduce deadlocks, which are typically handled by incorporating deadlock detection [Mis86] or deadlock avoidance [Mis86, CS89a] mechanisms into the simulation algorithm. Optimistic algorithms <ref> [Jef85, CS89b] </ref> allow an LP to process messages out of order; causality errors are corrected by using rollbacks and recomputations. Implementations of optimistic algorithms are usually more difficult because they require complex mechanisms for detection and handling of causality errors, termination detection, exception handling, and memory management.
Reference: [DGH91] <author> H. Davis, S. R. Goldschmidt, and Hennessey. </author> <title> Multiprocessor simulation and tracing using Tango. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing (ICPP'91), </booktitle> <pages> pages II99-II107, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Simulators for parallel programs can be effectively utilized to test, debug, and predict performance of parallel programs on a diverse set of parallel architectures. A variety of simulators have been designed <ref> [BDCW91, DGH91, RHL + 93, CDJ + 91, DHN94] </ref> to estimate the performance of a parallel program. Most simulators were designed to estimate the performance of asynchronous or task parallel programs. <p> Existing simulators include Proteus [BDCW91], Tango <ref> [DGH91] </ref> and the RPPT simulation engine [CDJ + 91] among others. In Proteus, the application to be simulated is written in a superset of C, and constructs are provided to control the placement of data. Provided library routines are used for message passing, thread management, memory management and data collection.
Reference: [DHN94] <author> P. Dickens, P. Heidelberger, and D. Nicol. </author> <title> A distributed memory lapse: Parallel simulation of message-passing programs. </title> <booktitle> In Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 32-38, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Simulators for parallel programs can be effectively utilized to test, debug, and predict performance of parallel programs on a diverse set of parallel architectures. A variety of simulators have been designed <ref> [BDCW91, DGH91, RHL + 93, CDJ + 91, DHN94] </ref> to estimate the performance of a parallel program. Most simulators were designed to estimate the performance of asynchronous or task parallel programs. <p> A profiler is used to augment the code for direct execution. Parallel simulation engines include the Wisconsin Wind Tunnel [RHL + 93](WWT) and the Large Application Parallel Simulation Environment <ref> [DHN94] </ref> (LAPSE). WWT is a simulator of cache-coherent, shared memory computers that runs on the Thinking Machines CM-5. It provides fast parallel simulation by direct execution of local code, one host processor per target processor.
Reference: [Fuj90] <author> R. Fujimoto. </author> <title> Parallel discrete event simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 30-53, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Implementations of optimistic algorithms are usually more difficult because they require complex mechanisms for detection and handling of causality errors, termination detection, exception handling, and memory management. A comprehensive discussion of parallel discrete-event simulations may be found in <ref> [Fuj90, Mis86] </ref>. A simulation language called Maisie [BtL94] has been designed at UCLA to support the execution of parallel discrete-event models.
Reference: [Hig93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification. </title> <note> Available by anonymous ftp from titan.cs.rice.edu, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: Section 5 discusses the relationship between this and existing program simulators and section 6 discusses future work. 2 2 Compilation of Data parallel Programs Data parallel programming is defined as single threaded, global name space, loosely synchronous parallel computation <ref> [Hig93] </ref>. A data parallel program, in order to be executed on a multicomputer, is first translated into a message passing SPMD program. The SPMD program is then compiled and run on each of (a subset of) the nodes of the multicomputer.
Reference: [Jef85] <author> D. Jefferson. </author> <title> Virtual time. </title> <journal> ACM TOPLAS, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: This constraint may introduce deadlocks, which are typically handled by incorporating deadlock detection [Mis86] or deadlock avoidance [Mis86, CS89a] mechanisms into the simulation algorithm. Optimistic algorithms <ref> [Jef85, CS89b] </ref> allow an LP to process messages out of order; causality errors are corrected by using rollbacks and recomputations. Implementations of optimistic algorithms are usually more difficult because they require complex mechanisms for detection and handling of causality errors, termination detection, exception handling, and memory management.
Reference: [Mad87] <author> S. Madala. </author> <title> Concurrent c users manual. </title> <type> Tech. </type> <institution> rept. #8701, ECE Dept., Rice University, </institution> <month> October </month> <year> 1987. </year>
Reference-contexts: The simulator executes as set of Unix processes. Target architecture primitives are implemented by interactions between the processes (using shared memory and semaphores), and interactions between the processes and the memory simulator, which can have varying degrees of accuracy. In RPPT, the application is written in Concurrent C <ref> [Mad87] </ref>. The target architecture is specified in terms of processor modules and global memory modules, a process mapping which describes the logical node to physical node mapping, and a routine UserSend (a CSIM [Sch85] routine written by the user) which essentially describes the interconnection medium.
Reference: [MF88] <author> I. Mathieson and R. Francis. </author> <title> A dynamic-trace-driven simulator for evaluating parallelism. </title> <booktitle> In Proceedings of 21st Hawaii International Conference on System Sciences, </booktitle> <volume> volume 1, </volume> <pages> pages 158-166, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: When viewed as a discrete-event model, each process of the compiled program contains only three types of events: basic blocks, or code that does not involve communication with another process, send events, and receive events. Although it is possible to simulate basic blocks, most simulators use direct execution <ref> [CMM + 88, MF88] </ref> as this has been shown to be more efficient.
Reference: [Mis86] <author> J. Misra. </author> <title> Distributed discrete-event simulation. </title> <journal> ACM Computing Surveys, </journal> <volume> 18(1) </volume> <pages> 39-65, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: This constraint may introduce deadlocks, which are typically handled by incorporating deadlock detection <ref> [Mis86] </ref> or deadlock avoidance [Mis86, CS89a] mechanisms into the simulation algorithm. Optimistic algorithms [Jef85, CS89b] allow an LP to process messages out of order; causality errors are corrected by using rollbacks and recomputations. <p> This constraint may introduce deadlocks, which are typically handled by incorporating deadlock detection [Mis86] or deadlock avoidance <ref> [Mis86, CS89a] </ref> mechanisms into the simulation algorithm. Optimistic algorithms [Jef85, CS89b] allow an LP to process messages out of order; causality errors are corrected by using rollbacks and recomputations. <p> Implementations of optimistic algorithms are usually more difficult because they require complex mechanisms for detection and handling of causality errors, termination detection, exception handling, and memory management. A comprehensive discussion of parallel discrete-event simulations may be found in <ref> [Fuj90, Mis86] </ref>. A simulation language called Maisie [BtL94] has been designed at UCLA to support the execution of parallel discrete-event models.
Reference: [PB] <author> S. Prakash and R. Bagrodia. </author> <title> An adaptive synchronization method for unpredictable communication patterns in dataparallel programs. </title> <note> To appear in IPPS95. </note>
Reference-contexts: Parallel data combination: Data combination occurs in operations like reduction, prefix, suffix and combining scatter. In HPF, these operations occur as intrinsic operations. From the point of view of communication optimization, the communication patterns resulting from these operations may be classified as predictable or unpredictable <ref> [PB] </ref>. Predictable communication patterns are those in which every processor knows which other processor needs its data. Unpredictable communication patterns are those in which a processor does not know who needs its data and hence generally need costly global synchronizations to implement them.
Reference: [PDB93] <author> S. Prakash, M. Dhagat, and R. Bagrodia. </author> <title> Synchronization issues in mul-ticomputer implementation of data-parallel languages. </title> <editor> In U. Banerjee, M.Wolfe, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1993. </year>
Reference: [RHL + 93] <author> S. K. Reinhardt, M. D. Hill, J. R. Larus, A. R. Lebeck, J. C. Lewis, and D. A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proceedings of the 1993 ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Simulators for parallel programs can be effectively utilized to test, debug, and predict performance of parallel programs on a diverse set of parallel architectures. A variety of simulators have been designed <ref> [BDCW91, DGH91, RHL + 93, CDJ + 91, DHN94] </ref> to estimate the performance of a parallel program. Most simulators were designed to estimate the performance of asynchronous or task parallel programs. <p> A preprocessor is used to translate the application into a simulation program by inserting calls to UserSend and possibly a global memory simulator at the required points. A profiler is used to augment the code for direct execution. Parallel simulation engines include the Wisconsin Wind Tunnel <ref> [RHL + 93] </ref>(WWT) and the Large Application Parallel Simulation Environment [DHN94] (LAPSE). WWT is a simulator of cache-coherent, shared memory computers that runs on the Thinking Machines CM-5. It provides fast parallel simulation by direct execution of local code, one host processor per target processor.
Reference: [Sch85] <author> H. Schwetman. </author> <title> Csim : A c based process oriented simulation language. </title> <type> Technical report, </type> <institution> Microelectronics & Computer Technology Corp., Austin, </institution> <month> May </month> <year> 1985. </year> <title> This article was processed using the L a T E X macro package with LLNCS style 15 </title>
Reference-contexts: In RPPT, the application is written in Concurrent C [Mad87]. The target architecture is specified in terms of processor modules and global memory modules, a process mapping which describes the logical node to physical node mapping, and a routine UserSend (a CSIM <ref> [Sch85] </ref> routine written by the user) which essentially describes the interconnection medium. A preprocessor is used to translate the application into a simulation program by inserting calls to UserSend and possibly a global memory simulator at the required points. A profiler is used to augment the code for direct execution.
References-found: 19

