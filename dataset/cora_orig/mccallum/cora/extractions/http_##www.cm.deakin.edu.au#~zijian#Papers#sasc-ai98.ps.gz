URL: http://www.cm.deakin.edu.au/~zijian/Papers/sasc-ai98.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/publications.html
Root-URL: 
Email: fzijian,webbg@deakin.edu.au  
Title: Stochastic Attribute Selection Committees  
Author: Zijian Zheng and Geoffrey I. Webb 
Address: Geelong Victoria 3217, Australia  
Affiliation: School of Computing and Mathematics Deakin University,  
Note: To appear in Proceedings of AI-98, Berlin: Springer Verlag  Boosting is not.  
Abstract: Classifier committee learning methods generate multiple classifiers to form a committee by repeated application of a single base learning algorithm. The committee members vote to decide the final classification. Two such methods, Bagging and Boosting, have shown great success with decision tree learning. They create different classifiers by modifying the distribution of the training set. This paper studies a different approach: Stochastic Attribute Selection Committee learning of decision trees. It generates classifier committees by stochastically modifying the set of attributes but keeping the distribution of the training set unchanged. An empirical evaluation of a variant of this method, namely Sasc, in a representative collection of natural domains shows that the SASC method can significantly reduce the error rate of decision tree learning. On average Sasc is more accurate than Bagging and less accurate than Boosting, although a one-tailed sign-test fails to show that these differences are significant at a level of 0.05. In addition, it is found that, like Bagging, Sasc is more stable than Boosting in terms of less frequently obtaining significantly higher error rates than C4.5 and, when error is raised, producing lower error rate increases. Moreover, like Bagging, Sasc is amenable to parallel and distributed processing while 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ali, K.M.: </author> <title> Learning Probabilistic Relational Concept Descriptions. </title> <type> PhD. Thesis, </type> <institution> Dept of Info. and Computer Science, Univ. of California, </institution> <address> Irvine (1996). </address>
Reference-contexts: trees (Buntine 1990; Kohavi and Kunz 1997), training a committee of neural networks by manually selecting attribute subsets (Cherkauer 1996; Tumer and Ghosh 1996), learning naive Bayesian classifier committees by randomly choosing attribute subsets (Zheng 1998), and creating committees for first-order learning by adding random selection of conditions to FOIL <ref> (Ali and Pazzani 1996) </ref>. Finally, different base learning algorithms can be used for learning different classifiers in committees (Wolpert 1992). A collection of recent research in this area and reviews of related methods can be found in Chan, Stolfo, and Wolpert (1996), Dietterich (1997), and Ali (1996).
Reference: <author> Ali, K.M. and Pazzani, M.J.: </author> <title> Error reduction through learning multiple descriptions. </title> <note> Machine Learning 24 (1996) 173-202. </note>
Reference-contexts: trees (Buntine 1990; Kohavi and Kunz 1997), training a committee of neural networks by manually selecting attribute subsets (Cherkauer 1996; Tumer and Ghosh 1996), learning naive Bayesian classifier committees by randomly choosing attribute subsets (Zheng 1998), and creating committees for first-order learning by adding random selection of conditions to FOIL <ref> (Ali and Pazzani 1996) </ref>. Finally, different base learning algorithms can be used for learning different classifiers in committees (Wolpert 1992). A collection of recent research in this area and reviews of related methods can be found in Chan, Stolfo, and Wolpert (1996), Dietterich (1997), and Ali (1996).
Reference: <author> Bauer, E. and Kohavi, R.: </author> <title> An empirical comparison of voting classification algorithms: Bagging, Boosting, and variants. </title> <note> Submitted to Machine Learning (1998) (available at: http://reality.sgi.com/ronnyk/vote.ps.gz). </note>
Reference: <author> Breiman, L.: </author> <note> Bagging predictors. Machine Learning 24 (1996a) 123-140. </note>
Reference-contexts: With this type of approach, a set of classifiers is generated using a single base learning algorithm to form a committee. The committee members vote to decide the final classification. Bagging <ref> (Breiman 1996a) </ref> and Boosting (Schapire 1990; Freund 1996; Fre-und and Schapire 1996; Schapire et al. 1997), as two representative methods of this type, can significantly decrease the error rate of decision tree learning (Quinlan 1996; Freund and Schapire 1996; Bauer and Kohavi 1998). <p> The primary idea of Bagging <ref> (Breiman 1996a) </ref> is to generate a committee of classifiers with each from a bootstrap sample of the original training set. <p> The first one is using the probabilistic predictions produced by all H t s, without voting weights. The second one is using the categorical predictions provided by all H t s, without weights. This voting method corresponds to the method used in the original Bagging <ref> (Breiman 1996a) </ref>. The other two methods are the same as these two but each tree H t is given a weight ff t for voting.
Reference: <author> Breiman, L.: </author> <note> Arcing classifiers. Technical Report (available at: http://www.stat. Berkeley.EDU/users/breiman/). </note> <institution> Department of Statistics, University of California, Berkeley, </institution> <address> CA (1996b). </address> <note> 11 Buntine, </note> <author> W.: </author> <title> A Theory of Learning Classification Rules. </title> <type> PhD. Thesis, </type> <institution> School of Com--puting Science, University of Technology, </institution> <address> Sydney (1990). </address>
Reference: <author> Chan, P., Stolfo, S., and Wolpert, D. (eds): </author> <title> Working Notes of AAAI Workshop on Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms (available at http://www.cs.fit.edu/~imlm/papers.html), Portland, </title> <booktitle> Oregon (1996). </booktitle>
Reference: <author> Cherkauer, K.J.: </author> <title> Human expert-level performance on a science image analysis task by a system using combined artificial neural networks. </title> <editor> Chan, P., Stolfo, S., and Wolpert, D. </editor> <title> (eds) Working Notes of AAAI Workshop on Integrating Multiple Learned Models for Improving and Scaling Machine Learning Algorithms (available at http://www.cs.fit.edu/~imlm/papers.html), Portland, </title> <address> Oregon (1996) 15-21. </address>
Reference: <author> Dietterich, T.G. and Bakiri, G.: </author> <title> Solving multiclass learning problems via error-correcting output codes. </title> <note> Journal of AI Research 2 (1995) 263-286. </note>
Reference-contexts: This makes Bagging appropriate for parallel machine learning and datamining. While much recent attention has focused on Boosting and Bagging, other classifier committee learning approaches have also been developed, including generating multiple trees by manually changing learning parameters (Kwok and Carter 1990), error-correcting output codes <ref> (Dietterich and Bakiri 1995) </ref>, generating different classifiers by randomizing the base learning process (Dietterich and Kong 1995; Ali 1996), learning option trees (Buntine 1990; Kohavi and Kunz 1997), training a committee of neural networks by manually selecting attribute subsets (Cherkauer 1996; Tumer and Ghosh 1996), learning naive Bayesian classifier committees by
Reference: <author> Dietterich, T.G. and Kong, E.B.: </author> <title> Machine learning bias, statistical bias, and statistical variance of decision tree algorithms. </title> <type> Technical Report, </type> <institution> Dept of Computer Science, Oregon State University, Corvallis, </institution> <note> Oregon (1995) (available at ftp://ftp. cs.orst.edu/pub/tgd/papers/tr-bias.ps.gz). </note>
Reference-contexts: This makes Bagging appropriate for parallel machine learning and datamining. While much recent attention has focused on Boosting and Bagging, other classifier committee learning approaches have also been developed, including generating multiple trees by manually changing learning parameters (Kwok and Carter 1990), error-correcting output codes <ref> (Dietterich and Bakiri 1995) </ref>, generating different classifiers by randomizing the base learning process (Dietterich and Kong 1995; Ali 1996), learning option trees (Buntine 1990; Kohavi and Kunz 1997), training a committee of neural networks by manually selecting attribute subsets (Cherkauer 1996; Tumer and Ghosh 1996), learning naive Bayesian classifier committees by
Reference: <author> Dietterich, T.G.: </author> <note> Machine learning research. AI Magazine 18 (1997) 97-136. </note>
Reference: <author> Freund, Y.: </author> <title> Boosting a weak learning algorithm by majority. </title> <note> Information and Computation 121 (1996) 256-285. </note>
Reference: <author> Freund, Y. and Schapire, R.E.: </author> <title> Experiments with a new Boosting algorithm. </title> <booktitle> Proceedings of the Thirteenth International Conference on Machine Learning. </booktitle> <address> San Fran-cisco, CA: </address> <publisher> Morgan Kaufmann (1996) 148-156. </publisher>
Reference: <author> Kohavi, R. and Kunz, C.: </author> <title> Option decision trees with majority votes. </title> <booktitle> Proceedings of the Fourteenth International Conference on Machine Learning. </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann (1997) 161-169. </publisher>
Reference: <author> Kwok, S.W. and Carter, C.: </author> <title> Multiple decision trees. </title> <editor> Schachter, R.D., Levitt, T.S., Kanal, L.N., and Lemmer, J.F. </editor> <booktitle> (eds) Uncertainty in Artificial Intelligence. </booktitle> <address> Elsevier Science (1990) 327-335. </address>
Reference-contexts: This makes Bagging appropriate for parallel machine learning and datamining. While much recent attention has focused on Boosting and Bagging, other classifier committee learning approaches have also been developed, including generating multiple trees by manually changing learning parameters <ref> (Kwok and Carter 1990) </ref>, error-correcting output codes (Dietterich and Bakiri 1995), generating different classifiers by randomizing the base learning process (Dietterich and Kong 1995; Ali 1996), learning option trees (Buntine 1990; Kohavi and Kunz 1997), training a committee of neural networks by manually selecting attribute subsets (Cherkauer 1996; Tumer and Ghosh
Reference: <author> Merz, C.J. and Murphy, </author> <title> P.M.: UCI Repository of machine learning databases [http://www.ics.uci.edu/~mlearn/MLRepository.html]. Irvine, </title> <institution> CA: Univ of Cali-fornia, Dept of Info and Computer Science (1997). </institution>
Reference-contexts: We will empirically compare these four voting methods later in this section. 3.2 Experimental Domains and Methods Forty natural domains from the UCI machine learning repository <ref> (Merz and Murphy 1997) </ref> are used. They include all the domains used by Quinlan (1996) for studying Boosting and Bagging. In every domain, two stratified 10-fold cross-validations are carried out for each algorithm. The result of each algorithm in each domain reported is an average value over 20 trials.
Reference: <author> Quinlan, J.R.: C4.5: </author> <title> Program for Machine Learning. </title> <publisher> Morgan Kaufmann (1993). </publisher>
Reference-contexts: Two variants of this method which have been proposed are Dietterich and Kong's (1995) randomization trees and Ali's (1996) decision tree ensembles. The former generates decision tree committees through repeatedly applying a base learning algorithm that is derived by modifying C4.5 <ref> (Quinlan 1993) </ref> to choose randomly among the top 20 tests in terms of information gain ratio at each decision node. The latter employs a very similar technique. <p> The final section summarizes our findings. 2 SASC for Decision Tree Learning During the growth of a decision tree, at each decision node, a decision tree learning algorithm searches for the best attribute to form a test based on some test selection functions <ref> (Quinlan 1993) </ref>. The key idea of SASC is to vary the members of a decision tree committee by stochastic manipulation of the set of attributes available for selection at decision nodes. This creates decision trees that each partition the instance space differently. We use C4.5 (Quinlan 1993) with the modifications described <p> on some test selection functions <ref> (Quinlan 1993) </ref>. The key idea of SASC is to vary the members of a decision tree committee by stochastic manipulation of the set of attributes available for selection at decision nodes. This creates decision trees that each partition the instance space differently. We use C4.5 (Quinlan 1993) with the modifications described below as the base classifier learning algorithm in our stochastic attribute selection committee learning algorithm, Sasc, although any conventional decision tree learning algorithm can be used. <p> When building a decision node, by default C4.5 uses the information gain ratio to search for the best attribute to form a test <ref> (Quinlan 1993) </ref>. To force C4.5 to generate different trees using the same training set, we modify C4.5 by stochastic restriction of the attributes available for selection at a decision node. <p> Given a training set D consisting of m instances and an integer T, the number of trials, Boost builds T pruned trees over T trials by repeatedly invoking C4.5 <ref> (Quinlan 1993) </ref>. Let w t (x) denote the weight of instance x in D at trial t. At the first trial, each instance has weight 1; that is, w 1 (x) = 1 for each x. <p> In effect, this treatment is similar to using probabilistic predictions without weights discussed here. 5 This step is limited to 10 fi T times. 5 our implementation of Bagging, uses C4.5 <ref> (Quinlan 1993) </ref> as its base classifier learning algorithm. Given a committee size T and a training set D consisting of m instances, Bag generates T 1 bootstrap samples with each being created by uniformly sampling m instances from D with replacement.
Reference: <author> Quinlan, J.R.: Bagging, </author> <title> Boosting, </title> <booktitle> and C4.5. Proceedings of the 13th National Conference on Artificial Intelligence. </booktitle> <address> Menlo Park, CA: </address> <publisher> AAAI Press (1996) 725-730. </publisher>
Reference-contexts: Sasc is also compared with a Boosting algorithm, namely Boost, and a Bagging algorithm, namely Bag. 3.1 The Comparison Algorithms: BOOST and BAG Boost is our implementation of the Boosting algorithm with decision tree learning. It follows the Boosted C4.5 algorithm (AdaBoost.M1) <ref> (Quinlan 1996) </ref> but uses a new Boosting equation as shown in Equation 1, derived from Schapire et al. (1997). Given a training set D consisting of m instances and an integer T, the number of trials, Boost builds T pruned trees over T trials by repeatedly invoking C4.5 (Quinlan 1993).
Reference: <author> Schapire, R.E.: </author> <title> The strength of weak learnability. </title> <note> Machine Learning 5 (1990) 197-227. </note>
Reference: <author> Schapire, R.E., Freund, Y., Bartlett, P., and Lee, </author> <title> W.S.: Boosting the margin: A new explanation for the effectiveness of voting methods. </title> <booktitle> Proceedings of the Fourteenth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann (1997) 322-330. </publisher>
Reference: <author> Tumer, K. and Ghosh, J.: </author> <title> Error correction and error reduction in ensemble classifiers. </title> <note> Connection Science 8 (1996) 385-404. </note>
Reference: <author> Wolpert, D.H.: </author> <title> Stacked generalization. </title> <booktitle> Neural Networks 5 (1992) 241-259. </booktitle>
Reference-contexts: Finally, different base learning algorithms can be used for learning different classifiers in committees <ref> (Wolpert 1992) </ref>. A collection of recent research in this area and reviews of related methods can be found in Chan, Stolfo, and Wolpert (1996), Dietterich (1997), and Ali (1996).
Reference: <author> Zheng, Z.: </author> <title> Naive Bayesian classifier committees. </title> <booktitle> Proceedings of the 10th European Conference on Machine Learning. </booktitle> <address> Berlin: </address> <month> Springet-Verlag </month> <year> (1998) </year> <month> 196-207. </month>
Reference-contexts: different classifiers by randomizing the base learning process (Dietterich and Kong 1995; Ali 1996), learning option trees (Buntine 1990; Kohavi and Kunz 1997), training a committee of neural networks by manually selecting attribute subsets (Cherkauer 1996; Tumer and Ghosh 1996), learning naive Bayesian classifier committees by randomly choosing attribute subsets <ref> (Zheng 1998) </ref>, and creating committees for first-order learning by adding random selection of conditions to FOIL (Ali and Pazzani 1996). Finally, different base learning algorithms can be used for learning different classifiers in committees (Wolpert 1992). <p> A different random subset is selected at each node of the tree. The modified version of C4.5 is called C4.5SAS (C4.5 Stochastic Attribute Selection). For a more detailed description of the C4.5Sas algorithm, see the long version of this paper <ref> (Zheng and Webb 1998) </ref>. The only difference between C4.5Sas and C4.5 is that when growing a tree, at a decision node, C4.5Sas creates an attribute subset and uses the best attribute in it to form a test as described above. All other parts are identical for these two algorithms. <p> In this paper, a voting method that uses the probabilistic predictions produced by all committee members without voting weights is adopted. 2 With this method, each decision tree returns a distribution over classes to which the example belongs (see the long version of this paper <ref> (Zheng and Webb 1998) </ref>, for the computation of class distributions). The decision tree committee members vote by summing up the class distributions provided by all trees. The class with the highest score (sum of probabilities) wins the voting, and serves as the predicted class of Sasc for this example.
Reference: <author> Zheng, Z. and Webb, </author> <title> G.I.: Stochastic attribute selection committees. </title> <type> Technical Report (TR C98/08), </type> <institution> School of Computing and Mathematics, Deakin University, </institution> <note> Australia (1998) (available at http://www3.cm.deakin.edu.au/~zijian/Papers/ sasc-tr-C98-08.ps.gz). This article was processed using the L a T E X macro package with LLNCS style 12 </note>
Reference-contexts: different classifiers by randomizing the base learning process (Dietterich and Kong 1995; Ali 1996), learning option trees (Buntine 1990; Kohavi and Kunz 1997), training a committee of neural networks by manually selecting attribute subsets (Cherkauer 1996; Tumer and Ghosh 1996), learning naive Bayesian classifier committees by randomly choosing attribute subsets <ref> (Zheng 1998) </ref>, and creating committees for first-order learning by adding random selection of conditions to FOIL (Ali and Pazzani 1996). Finally, different base learning algorithms can be used for learning different classifiers in committees (Wolpert 1992). <p> A different random subset is selected at each node of the tree. The modified version of C4.5 is called C4.5SAS (C4.5 Stochastic Attribute Selection). For a more detailed description of the C4.5Sas algorithm, see the long version of this paper <ref> (Zheng and Webb 1998) </ref>. The only difference between C4.5Sas and C4.5 is that when growing a tree, at a decision node, C4.5Sas creates an attribute subset and uses the best attribute in it to form a test as described above. All other parts are identical for these two algorithms. <p> In this paper, a voting method that uses the probabilistic predictions produced by all committee members without voting weights is adopted. 2 With this method, each decision tree returns a distribution over classes to which the example belongs (see the long version of this paper <ref> (Zheng and Webb 1998) </ref>, for the computation of class distributions). The decision tree committee members vote by summing up the class distributions provided by all trees. The class with the highest score (sum of probabilities) wins the voting, and serves as the predicted class of Sasc for this example.
References-found: 23

