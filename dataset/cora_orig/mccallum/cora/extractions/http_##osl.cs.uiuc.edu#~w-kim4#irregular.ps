URL: http://osl.cs.uiuc.edu/~w-kim4/irregular.ps
Refering-URL: http://osl.cs.uiuc.edu/~w-kim4/publications.html
Root-URL: http://www.cs.uiuc.edu
Email: Email: f panwar j wooyoung j agha@cs.uiuc.edu  
Phone: Phone: (217) 244-7115 Fax: (217) 333-3501  
Title: Parallel Implementations of Irregular Problems Using High-level Actor Language  
Author: R. B. Panwar and W. Kim and G. A. Agha 
Web: g  
Address: Urbana, IL 61801, USA  
Affiliation: Open Systems Laboratory Department of Computer Science University of Illinois at Urbana-Champaign  
Abstract: In this paper we present performance results of several irregular parallel algorithms implemented using a high-level actor language and its runtime kernel. The problems studied require dynamic computation of the placement of objects and may create load imbalance as the computation proceeds thereby requiring dynamic load balancing. We express the algorithms as fine grained computation, which allows us to rearrange objects in various ways and to compose the same algorithm with different partitioning and distribution strategies (PDS's). The PDS's are implemented for specific data structures or algorithm structures and are reusable for different 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Actors are message driven objects which unify data and the operations modifying the data. Actors are self-contained, interactive components of a computing system and communicate by asynchronous message passing <ref> [1] </ref>. Specifying a parallel algorithm in terms of actors does not introduce unnecessary sequentiality. In particular, actor programs naturally represent all the parallelism in an ideal algorithm. Each actor has a mail address and a behavior. Mail addresses may be communicated, thereby providing a dynamic communication topology.
Reference: [2] <author> G. Agha, C. Houck, and R. Panwar. </author> <title> Distributed Execution of Actor Systems. </title> <editor> In D. Gelernter, T. Gross, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 1-17. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <note> Lecture Notes in Computer Science 589. 19 </note>
Reference-contexts: These basic actor primitives are supported as primitives in THAL. THAL is a descendent of HAL <ref> [9, 2, 3] </ref> and tailored for high-performance execution on stock-hardware distributed memory multicomputers, such as CM-5. It is desigend by careful introduction of sequentiality to the Actor model. For example, a method is executed sequentially in the order specified in its definition. <p> Since C compiler may optimize the sequential intra-method execution computation, compile-time optimizations concentrate on improving concurrent inter-method execution. In particular, it restores profitable concurrency that may be lost by language constructs, such a RPC sends <ref> [2] </ref>. The rest of efficiency comes from the runtime kernel [16], which provides a fast communication layer implemented using the CMAM messaging layer [26]. Messages to local actors are scheduled in a stack-based fashion, minimizing scheduling overhead and context switch overhead.
Reference: [3] <author> G. Agha, W. Kim, and R. Panwar. </author> <title> Actor languages for specification of parallel computations. </title> <editor> In G. E. Blelloch, K. Mani Chandy, and S. Jagannathan, editors, </editor> <booktitle> DIMACS. Series in Discrete Mathematics and Theoretical Computer Science. </booktitle> <volume> vol 18. </volume> <booktitle> Specification of Parallel Algorithms, </booktitle> <pages> pages 239-258. </pages> <publisher> American Mathematical Society, </publisher> <year> 1994. </year> <booktitle> Proceedings of DIMACS '94 Workshop. </booktitle>
Reference-contexts: These basic actor primitives are supported as primitives in THAL. THAL is a descendent of HAL <ref> [9, 2, 3] </ref> and tailored for high-performance execution on stock-hardware distributed memory multicomputers, such as CM-5. It is desigend by careful introduction of sequentiality to the Actor model. For example, a method is executed sequentially in the order specified in its definition.
Reference: [4] <author> A. Chien. </author> <title> Concurrent Aggregates: An Object-Oriented Language for Fine-Grained Message-Passing Machines. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1990. </year>
Reference-contexts: However, they differ in specific extensions to the Actor model that they employed <ref> [4, 27] </ref>. Moreover, they support different extent of flexibility. ABCL/onAP1000 uses location-dependent addresses as actor's mail address and thus does not support dynamic actor placement, such as migration.
Reference: [5] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis. </author> <title> Distributed Memory Compiler Methods for Irregular Problems Data Copy Reuse and Runtime Partitioning. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers and Run-Time Environments for Distributed Memory Machines. </title> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year>
Reference-contexts: For irregular problems, such a priori determination of the necessary data distribution is not feasible. To address this problem in some limited cases, PARTI <ref> [5] </ref> and Kali [17] transform a user-defined for loop to an inspector/executor pair. In these languages the compiler assumes whole responsibility to uncover concurrency characteristics. In many regular problems using dense matrices, compiler tools may exploit most of the useful parallelism.
Reference: [6] <author> A. Grama, V. Kumar, and A Sameh. </author> <title> n-body simulations using message passing parallel computers. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 355-360, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: The call/return mechanism described in [15] is used for computing the acceleration. Note that several bodies compute their acceleration concurrently and several messages related to different bodies are moving up/down the quadtree structure concurrently thereby resulting in a complex communication pattern. The partitioning strategies used are similar to <ref> [6] </ref>. It is assumed that the density of bodies is roughly uniform in the region being considered. The bodies are partitioned into P processors by dividing the available space into P square blocks and assigning one block to each processor (P is assumed to be a square).
Reference: [7] <author> A. Grimshaw, W. T. Strayer, and P. Narayan. </author> <title> Dynamic object-oriented parallel processing. </title> <journal> IEEE Parallel and Distributed Technology: Systems and Applications, </journal> <volume> 1(2) </volume> <pages> 33-47, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: We have also shown that additional flexibility can be supported without sacrificing too much performance efficiency [16]. 18 The desire to leverage the existing compiler technology in the design of concurrent languages has made many researchers extend C++ to get a variety of COOP languages <ref> [11, 7, 13, 18] </ref>, mainly because of the popularity and portability of C++. Although these language have their own distinguishing features, the grain size supported in these languages are not sufficiently fine so that effective overlapping of computation and communication is not easy in dynamic irregular applications.
Reference: [8] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiling Fortran-D for MIMD Distributed Memory Machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: They will be added to the paper in the final version. 17 6 Related Work Most of the previous work in specifying and optimizing placement of data in parallel programs has been based on extensions of sequential languages. In particular, Fortran-D <ref> [8] </ref> and High Performance Fortran (HPF) [19] allow explicit specification of data decomposition and distribution policies for regular problems to improve execution efficiency on distributed memory multicomputers. For irregular problems, such a priori determination of the necessary data distribution is not feasible.
Reference: [9] <author> C. Houck and G. Agha. HAL: </author> <title> A high-level actor language and its distributed implementation. </title> <booktitle> In Proceedings of th 21st International Conference on Parallel Processing (ICPP '92), </booktitle> <volume> volume II, </volume> <pages> pages 158-165, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: These basic actor primitives are supported as primitives in THAL. THAL is a descendent of HAL <ref> [9, 2, 3] </ref> and tailored for high-performance execution on stock-hardware distributed memory multicomputers, such as CM-5. It is desigend by careful introduction of sequentiality to the Actor model. For example, a method is executed sequentially in the order specified in its definition.
Reference: [10] <author> L. H. Jamieson. </author> <title> Characterizing parallel algorithms. </title> <editor> In R. J. Douglass L.H. Jamieson, D.B. Gannon, editor, </editor> <booktitle> The Characteristics of Parallel Algorithms, </booktitle> <pages> pages 65-100. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: 1 Introduction In parallel computing, the set of operations and the partial order in which they may be carried out define an ideal algorithm <ref> [10] </ref>. This ideal algorithm can be specified without introducing any unnecessary sequentiality in the code by using a form of concurrent objects, i.e., actors.
Reference: [11] <author> K. Mani Chandy and Carl Kesselman. </author> <title> CC++,: A Declarative Concurrent Object-Oriented Programming Notation. </title> <editor> In Gul Agha and Peter Wegner and Akinori Yonezawa, editor, </editor> <booktitle> Research Direction in Concurrent Object-Oriented Programming, chapter 11, </booktitle> <pages> pages 281-313. </pages> <publisher> The MIT press, </publisher> <year> 1993. </year>
Reference-contexts: We have also shown that additional flexibility can be supported without sacrificing too much performance efficiency [16]. 18 The desire to leverage the existing compiler technology in the design of concurrent languages has made many researchers extend C++ to get a variety of COOP languages <ref> [11, 7, 13, 18] </ref>, mainly because of the popularity and portability of C++. Although these language have their own distinguishing features, the grain size supported in these languages are not sufficiently fine so that effective overlapping of computation and communication is not easy in dynamic irregular applications.
Reference: [12] <author> D. Kahaner, C. Moler, and S. Nash. </author> <title> Numerical Methods and Software. </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: The workers perform the integration in their subintervals in parallel and return the values to the master where they are combined to obtained the final result. Adaptive integration techniques <ref> [12] </ref> can vary the step size used for integrating in a region based on the error present locally. Thus the work available for each worker may dynamically increase as the computation proceeds.
Reference: [13] <author> L. V. Kale and S. Krishnan. CHARM++: </author> <title> A Portable Concurrent Object Oriented System Based On C++. </title> <editor> In Andreas Paepcke, editor, </editor> <booktitle> Proceedings of OOPSLA 93'. </booktitle> <publisher> ACM Press, </publisher> <month> October </month> <year> 1993. </year> <journal> ACM SIGPLAN Notices 28(10). </journal>
Reference-contexts: We have also shown that additional flexibility can be supported without sacrificing too much performance efficiency [16]. 18 The desire to leverage the existing compiler technology in the design of concurrent languages has made many researchers extend C++ to get a variety of COOP languages <ref> [11, 7, 13, 18] </ref>, mainly because of the popularity and portability of C++. Although these language have their own distinguishing features, the grain size supported in these languages are not sufficiently fine so that effective overlapping of computation and communication is not easy in dynamic irregular applications.
Reference: [14] <author> V. Karamcheti and A. A. Chien. </author> <title> Concert Efficient Runtime Support for Concurrent Object-Oriented Programming Languages on Stock Hardware. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: However, an unaided compiler may be less successful in more general cases. In our model, actors are the unit of concurrency and all forms of parallelism is expressed explicitly in the programs themselves. ABCL/onAP1000 [25] and the Concert system <ref> [14] </ref> are similar to our work: all are based on the Actor model and rely on software technology (i.e., compiler and runtime system) for efficient execution of application programs on stock-hardware multicomputers. However, they differ in specific extensions to the Actor model that they employed [4, 27].
Reference: [15] <author> W. Kim and G. Agha. </author> <title> Compilation of a Highly Parallel Actor-Based Language. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Proceedings of the Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 1-15. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year> <note> LNCS 757. </note>
Reference-contexts: The acceleration caused by different nodes/bodies is computed and the partial results are propagated up the tree to the root and combined as they move up. The final result is returned to the body that initiated the computation. The call/return mechanism described in <ref> [15] </ref> is used for computing the acceleration. Note that several bodies compute their acceleration concurrently and several messages related to different bodies are moving up/down the quadtree structure concurrently thereby resulting in a complex communication pattern. The partitioning strategies used are similar to [6].
Reference: [16] <author> W. Kim and G. Agha. </author> <title> Efficient Support of Location Transparency in Concurrent Object-Oriented Programming Languages. </title> <booktitle> In Supercomputing '95, </booktitle> <year> 1995. </year> <note> (to appear). </note>
Reference-contexts: Since C compiler may optimize the sequential intra-method execution computation, compile-time optimizations concentrate on improving concurrent inter-method execution. In particular, it restores profitable concurrency that may be lost by language constructs, such a RPC sends [2]. The rest of efficiency comes from the runtime kernel <ref> [16] </ref>, which provides a fast communication layer implemented using the CMAM messaging layer [26]. Messages to local actors are scheduled in a stack-based fashion, minimizing scheduling overhead and context switch overhead. Actor mail addresses are implemented using real addresses in a location independent way, guaranteeing location-transparent execution of application programs. <p> Actor mail addresses are implemented using real addresses in a location independent way, guaranteeing location-transparent execution of application programs. Despite the flexibility and support for dynamic computations that THAL provides, its performance on dense, regular problems is competitive with more restrictive, static languages <ref> [16] </ref>. The fine-grained specification of the parallel algorithms allows us to rearrange the objects on the given processors in different ways therby composing the same parallel algorithm with different PDS's and obtaining different performance [23]. The PDS's themselves are designed for specific data structures or program structures. <p> We have argued that different applications may have different optimal placement strategy [23]. We have also shown that additional flexibility can be supported without sacrificing too much performance efficiency <ref> [16] </ref>. 18 The desire to leverage the existing compiler technology in the design of concurrent languages has made many researchers extend C++ to get a variety of COOP languages [11, 7, 13, 18], mainly because of the popularity and portability of C++.
Reference: [17] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling Global Name-space Parallel loops for Distibuted Execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year> <month> 20 </month>
Reference-contexts: For irregular problems, such a priori determination of the necessary data distribution is not feasible. To address this problem in some limited cases, PARTI [5] and Kali <ref> [17] </ref> transform a user-defined for loop to an inspector/executor pair. In these languages the compiler assumes whole responsibility to uncover concurrency characteristics. In many regular problems using dense matrices, compiler tools may exploit most of the useful parallelism. However, an unaided compiler may be less successful in more general cases.
Reference: [18] <author> Jenq Kuen Lee and Dennis Gannon. </author> <title> Object-oriented parallel programming experiments and results. </title> <booktitle> In Proceedings Supercomputing 91, </booktitle> <pages> pages 273-282, </pages> <year> 1991. </year>
Reference-contexts: We have also shown that additional flexibility can be supported without sacrificing too much performance efficiency [16]. 18 The desire to leverage the existing compiler technology in the design of concurrent languages has made many researchers extend C++ to get a variety of COOP languages <ref> [11, 7, 13, 18] </ref>, mainly because of the popularity and portability of C++. Although these language have their own distinguishing features, the grain size supported in these languages are not sufficiently fine so that effective overlapping of computation and communication is not easy in dynamic irregular applications.
Reference: [19] <author> David. B. Loveman. </author> <title> High Performance Fortran. Parallel & Distributed Technology, </title> <journal> Systems & Applications, </journal> <volume> 1(1) </volume> <pages> 25-42, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: They will be added to the paper in the final version. 17 6 Related Work Most of the previous work in specifying and optimizing placement of data in parallel programs has been based on extensions of sequential languages. In particular, Fortran-D [8] and High Performance Fortran (HPF) <ref> [19] </ref> allow explicit specification of data decomposition and distribution policies for regular problems to improve execution efficiency on distributed memory multicomputers. For irregular problems, such a priori determination of the necessary data distribution is not feasible.
Reference: [20] <author> K. Mani Chandy and Stephen Taylor. </author> <title> An Introduction to Parallel Programming. </title> <publisher> Jones and Bartlett Publishers, </publisher> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: The algorithm we implemented in this section uses the structure to compute the sum of elements available at each node of the tree (similar to the summation of leaves of a binary tree described in <ref> [20] </ref>). The values being summed are arrays of floating point numbers. The tree is generated as a simple binary search tree formed by inserting random keys. <p> PEs Time Time Without DLB With DLB 1 0.963 0.963 4 0.435 0.386 16 0.146 0.122 Table 5: Timing results (in seconds) for Adaptive Quadrature Algorithm. 14 4.2 Unstructured Grid Problem The unstructured grid problem <ref> [20, 21] </ref> solves a set of differential equations for a given input domain with adaptive refinement of the grid used for solving the equations. The program can be modeled using the master worker configuration discussed above.
Reference: [21] <editor> P. Mehrotra, J. Saltz, and R. Voigt, editors. </editor> <title> Unstructured Scientific Computation on Scalable Multiprocessors. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachussets, </address> <year> 1992. </year>
Reference-contexts: PEs Time Time Without DLB With DLB 1 0.963 0.963 4 0.435 0.386 16 0.146 0.122 Table 5: Timing results (in seconds) for Adaptive Quadrature Algorithm. 14 4.2 Unstructured Grid Problem The unstructured grid problem <ref> [20, 21] </ref> solves a set of differential equations for a given input domain with adaptive refinement of the grid used for solving the equations. The program can be modeled using the master worker configuration discussed above.
Reference: [22] <author> A Hierarchical O(nlogn) Force-Calculation ALgorithm. J. barnes and p. </author> <title> hut. </title> <journal> Nature, </journal> <volume> 324(4) </volume> <pages> 446-449, </pages> <year> 1986. </year>
Reference-contexts: We have implemented the Barnes-Hut algorithm <ref> [22] </ref> for solving the n-body problem. This algorithm involves the most complicated data structures and the communication structure of all the programs discussed above. The algorithm constructs a quadtree (since we assumed a two-dimensional space) such that the leaves point to actors containing information about individual bodies.
Reference: [23] <author> R. Panwar and G. Agha. </author> <title> A Methodology for Programming Scalable Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 479-487, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: As a result, different placement policies lead to different performance. In this paper, we study the effect of computation and data placement on the execution efficiency of irregular problems. We use a programming methodology <ref> [23] </ref> which uses fine grained computation, asynchronous communication and dynamic object creation. Such a methodology allows for easy expression of irregular applications. It enables programmers to compose different partitioning and distribution strategies (PDS's) with ideal algorithms to obtain different performances. <p> The fine-grained specification of the parallel algorithms allows us to rearrange the objects on the given processors in different ways therby composing the same parallel algorithm with different PDS's and obtaining different performance <ref> [23] </ref>. The PDS's themselves are designed for specific data structures or program structures. For example a concurrent implementation of the divide 4 and conquer strategy results in a binary tree structure. There are several other algorithms which explicitly or implicitly construct the binary tree data structure. <p> The Concert system does not provide constructs to specify the object placement in the desire to execute the same program on multiprocessor computer as well as single processor computer without any change. We have argued that different applications may have different optimal placement strategy <ref> [23] </ref>.
Reference: [24] <author> J. Singh, C. Holt, T. Totsuka, A. Gupta, and J. Hennessy. </author> <title> Load balancing and data locality in hierachical n-body methods. </title> <journal> Journal of Parallel and Distributed Processing, </journal> <year> 1994. </year>
Reference-contexts: the computation proceeds. 15 PEs Time Time Without DLB With DLB 1 1.117 1.118 4 0.343 0.341 16 0.181 0.131 Table 6: Timing results (in seconds) for Irregular Grid Problem 5 Irregular Problems Based on Quadtree Structure The quadtree data structure has been used for several applications in various domains <ref> [24] </ref>. We have implemented the Barnes-Hut algorithm [22] for solving the n-body problem. This algorithm involves the most complicated data structures and the communication structure of all the programs discussed above.
Reference: [25] <author> K. Taura, S. Matsuoka, and A. Yonezawa. </author> <title> An Efficient Implementation Scheme of Concurrent Object-Oriented Languages on Stock Multicomputers. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming PPOPP, </booktitle> <pages> pages 218-228, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: However, an unaided compiler may be less successful in more general cases. In our model, actors are the unit of concurrency and all forms of parallelism is expressed explicitly in the programs themselves. ABCL/onAP1000 <ref> [25] </ref> and the Concert system [14] are similar to our work: all are based on the Actor model and rely on software technology (i.e., compiler and runtime system) for efficient execution of application programs on stock-hardware multicomputers.
Reference: [26] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of International Symposium of Computer Architectures, </booktitle> <pages> pages 256-266, </pages> <year> 1992. </year>
Reference-contexts: In particular, it restores profitable concurrency that may be lost by language constructs, such a RPC sends [2]. The rest of efficiency comes from the runtime kernel [16], which provides a fast communication layer implemented using the CMAM messaging layer <ref> [26] </ref>. Messages to local actors are scheduled in a stack-based fashion, minimizing scheduling overhead and context switch overhead. Actor mail addresses are implemented using real addresses in a location independent way, guaranteeing location-transparent execution of application programs.
Reference: [27] <author> A. Yonezawa, </author> <title> editor. ABCL An Object-Oriented Concurrent System. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1990. </year>
Reference-contexts: However, they differ in specific extensions to the Actor model that they employed <ref> [4, 27] </ref>. Moreover, they support different extent of flexibility. ABCL/onAP1000 uses location-dependent addresses as actor's mail address and thus does not support dynamic actor placement, such as migration.
References-found: 27

