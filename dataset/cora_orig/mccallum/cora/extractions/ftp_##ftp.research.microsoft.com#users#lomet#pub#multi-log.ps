URL: ftp://ftp.research.microsoft.com/users/lomet/pub/multi-log.ps
Refering-URL: http://www.research.microsoft.com/users/lomet/pub/default.htm
Root-URL: http://www.research.microsoft.com
Title: Recovery for Shared Disk Systems Using Multiple Redo Logs  
Author: David B. Lomet 
Keyword: logging, database recovery, data sharing systems  
Note: c flDigital Equipment Corporation 1990. All rights reserved.  
Date: CRL 90/4 October 1,1990  
Affiliation: Digital Equipment Corporation Cambridge Research Lab  
Abstract: A new method for redo logging and recovery is described. It is designed to work in a data sharing system where multiple nodes can access common data. Particularly important is that each node can have its own log. Crash recovery is possible based on the contents of just one of these logs. Media recovery is supported via a form of merging of the logs. The method requires no time synchronization between nodes, and does not use timestamps to order the log records during the merge. The method should work with many undo recovery methods. Conventional checkpointing schemes can be adapted to the scheme, enabling the redo scan point to be advanced and the log to be truncated. Finally, the method makes possible a new paradigm for distributed DBMSs which has the potential to exploit inexpensive desktop processing and improve availability and responsiveness. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Bernstein, P., Hadzilacos, V., and Goodman, N., </author> <title> Concurrency Control and Recovery in Database Systems, </title> <publisher> Addison-Wesley, </publisher> <year> 1987. </year>
Reference: 2. <author> Bhide, A. </author> <title> An analysis of three transaction processing architectures. </title> <booktitle> Proc. of 14th VLDB, </booktitle> <address> Los Angeles, CA, </address> <month> (Aug. </month> <year> 1988) </year> <month> 339-350. </month>
Reference: 3. <author> Crus, </author> <title> R, Data recovery in IBM Database2. </title> <address> IBM Sys.J 23,2 (1984) 178-188. </address>
Reference: 4. <author> DeWitt, D., Futtersack, P., Maier, D., and Velez, F. </author> <title> A Study of Three Alternative Workstation-Server Architectures for Object-Oriented Database Systems. </title> <institution> Computer Sciences Technical Report #936 (May 1990) Univ. of Wisconsin-Madison, Madison, WI. </institution>
Reference-contexts: intrinsic physical limitations imposed by distance, this new model has some substantial advantages compared to currently popular ways of realizing distributed database systems. 18 6.1 Workstation Orientation More important than improving the performance of data sharing systems in a cluster may be that private logs facilitate workstation-oriented client-server based architectures <ref> [4] </ref>. The workstation DBMS may manage some local data itself, and can share data with host computers and perhaps other workstations. The shared data can be cached ("checked out") for long periods and over multiple transactions in the workstation. <p> With private redo logging, no data need be written back to the host at transaction end, neither database pages nor log records. Private logs can be used to advantage by all the avors of client-server architecture described in <ref> [4] </ref>. A private log at the workstation requires that the workstation have a PSTORE, e.g. a local magnetic disk. A workstation's PSTORE stably stores its private log. Further, this PSTORE might be used as a stable cache.
Reference: 5. <author> Gray, J. </author> <title> Notes on data base operating systems. </title> <note> Research Report RJ2188 (Feb 1978), </note> <institution> IBM Research Division, </institution> <address> San Jose, CA. </address>
Reference: 6. <author> Kronenberg, N., Levy, H., Strecker, W., and Merewood, R., </author> <title> The VAXcluster concept: an overview of a distributed system. </title> <journal> Digital Technical Journal No. </journal> <volume> 5, </volume> <month> (Sept </month> <year> 1987) </year> <month> 7-21. </month>
Reference-contexts: 1 Introduction 1.1 Data Sharing Systems There has been little written in the open literature on how to handle logging in system con---gurations where a number of computers all access a collection of shared disks. This con---guration is called a cluster <ref> [6] </ref> or a "shared disk system"[2,13]. A database system that exploits the con---guration such that any computer can access any of the data within a single transaction is called a "data sharing" system.
Reference: 7. <author> Mohan, C., Haderle, D., Lindsay, B., Pirahesh, H., and Schwarz, P., </author> <title> ARIES: a transaction recovery method supporting ---ne-granularity locking and partial rollbacks using Write-Ahead logging. </title> <institution> Research Report RJ6649 (Jan 1989) IBM Almaden Research Center, </institution> <address> San Jose, </address> <note> CA and ACM Trans. Database Systs. (to appear) </note>
Reference-contexts: The technique that enables multiple logs to contain recovery information for a single data item can be used with many of these methods. To show how the technique works, we describe it in the context of the "repeating history" technique <ref> [7] </ref>, which we like because of its functionality and simplicity. In particular, it permits operation logging and our method can exploit that capability. REDO is only part of system recovery. Most systems will also require an UNDO recovery capability as well. <p> We call this assumption the independent redo recovery assumption. (The ---rst published paper of which we are aware that makes this explicit is <ref> [7] </ref>.) Independent Redo Recovery Assumption: The state of a BLOCK can be transformed to a successor state during redo recovery based solely on the state of the BLOCK at the time the original operation was applied and the contents of the log record that describes the operation. <p> However, typically, a truncated part of an TLOG will continue to be required for media recovery. If so, the truncated part becomes part of the MLOG. 14 Some checkpointing schemes require that we know the safe point LSN for each BLOCK (called the RecLSN in <ref> [7] </ref>). This is used to re-calculate the TLOG safe point after the BLOCK with the oldest RecLSN is ushed to disk. The persistence of the RecLSNs across system crashes can be assured by including them in the checkpoint information. <p> Without an analysis phase, some extra work may be done during the other recovery phases. In <ref> [7] </ref>, the information in the last complete checkpoint on the TLOG is read and used to determine which blocks were dirty at checkpoint time.
Reference: 8. <author> Mohn, C. and Pirahesh, H. ARIES-RRH: </author> <title> Restricted Repeating of History in the ARIES Transaction Recovery Method. </title> <institution> Research Report RJ7342 (Feb 1990) IBM Almaden Research Center, </institution> <address> San Jose, CA. </address>
Reference-contexts: Committed or prepared transactions must have their actions redone. Transactions active at the moment of crash will be aborted, and hence the effects of their actions must eventually be removed from the stable database. Only some of their actions may need to be redone (see <ref> [8] </ref>). The set of write locks held by active or prepared transactions. These locks need to be re-acquired prior to starting N-LOG undo. They are the locks that protect data changed by these transactions. With node private TLOGs, the analysis phase can be done independently for each node. <p> The TLOG record's BSI is not equal to the BLOCK's DSI. The logged action can be ignored. 2. The TLOG record's BSI is equal to the BLOCK's DSI The appropriate "redo" activity is performed. Our redo phase method involves repeating history, at least what <ref> [8] </ref> calls restricted repeating of history. Update redo log records, starting from the redo safe point are applied, sometimes even those that belong to "loser" transactions, i.e. those that will need to subsequently be undone.
Reference: 9. <author> Mohan, C., Narang, I., and Palmer, J., </author> <title> A case study of problems in migrating to distributed computing: data base recovery using multiple logs in the shared disks environment. </title> <institution> Research Report RJ7343 (Mar 1990) IBM Almaden Research Center, </institution> <address> San Jose, CA. </address>
Reference-contexts: Note that synchronization and timestamps are required in <ref> [9] </ref>. 1.5 Organization of the Paper Section two provides background information for the remainder of the paper. Terms are de---ned and the fundamental conditions that need to be satis---ed in order to permit effective use of private logs are discussed. <p> However, it is highly desirable for SIs to be totally ordered when treating N-LOG recovery such that the ordering agrees with the time sequence of the actions logged. [LI,LSN] pairs lack such an ordering. Ordered SIs are exploited in an essential way for ONE-LOG recovery in <ref> [9] </ref>. There, the state identi---er associated with a log record is an ASI, which is the same role played by an LSN. These state identi---ers are required to be monotonically increasing values, effectively update sequence numbers. <p> Importantly, this technique did not make it possible to derive the SI for the BLOCK 8 state before the operation is executed from the ASI. While the technique is suf---cient for ONE-LOG recovery, it requires extra mechanism to permit convenient N-LOG recovery. In <ref> [9] </ref>, timestamping is used to supplement the SIs. 3.1.1 Before State Identi---ers In our technique, we include, in each log record, the precise identity of the BLOCK state that is seen BEFORE we perform a logged action. This is the "before state identi---er" or BSI. <p> That is because it has not been possible to uniquely determine, the BSI from the ASI. This is why <ref> [9] </ref> supplements the state identi---er in the log record with a timestamp, and further requires that the clocks from which the timestamps are derived be synchronized. Timestamp ordering is used to control the log merge, not the state identi---ers. There is no ambiguity in our method.
Reference: 10. <author> Rengarajan, T., Spiro, P., and Wright, W., </author> <title> High availability mechanisms of VAX DBMS software, </title> <journal> Digital Technical Journal No. </journal> <volume> 8, </volume> <month> (Feb. </month> <year> 1989), </year> <pages> 88-98. </pages>
Reference-contexts: A database system that exploits the con---guration such that any computer can access any of the data within a single transaction is called a "data sharing" system. Two commercial systems offering data sharing are Digital's Rdb/VMS (and DBMS) <ref> [10] </ref> and IBM's IMS/VS Data Sharing product [14]. A data sharing system does what is called "data shipping". That is, a data block, as it comes from the disk, is sent to the requesting computer. <p> We call such a system a "shared data/private log" system. A private log avoids both the sending of remote logging messages and synchronization for use of a common log. Digital's Rdb/VMS system <ref> [10] </ref> already has a private log for transaction recovery. And for transaction recovery, no synchronization is needed for logging. However it is an UNDO log and Rdb/VMS recovery requires forcing all updated pages back to disk at transaction commit. <p> Such cache management keeps track of which CACHEs contain which BLOCKs. This permits a NODE to request a BLOCK and for the most recent version of the BLOCK to be shipped to it. Cache management also keeps multiple NODEs from simultaneously updating a BLOCK. In Rdb/VMS <ref> [10] </ref>, the VMS lock manager is used both for database locks and to support global cache management. Because of the need for global cache management, we augment the list of operations that we describe to include not only the standard ones, but also the global cache management operations. <p> Such "locks" can be given up on request, of course, much as a borrower might return a book should he receive a call from the librarian that another patron has requested the book. This feature is already present in Rdb/VMS <ref> [10] </ref>. 19 6.3 Coping with Workstation Failures Perhaps the largest uncertainty involved with caching data at a remote workstation is how to make the cached data available again should the workstation crash or the network partition, and should this result in a very long delay.
Reference: 11. <author> Shoens, K. </author> <title> Data sharing vs. partitioning for capacity and availability. </title> <journal> IEEE Database Engineering 9,1 (Jan. </journal> <year> 1986) </year> <month> 10-16. </month>
Reference: 12. <author> Shoens, K., Narang, I., Obermark, R. Palmer, J., Silen, S., Traiger, I., and Treiber, K. </author> <title> The Amoeba project. </title> <booktitle> Proc. of IEEE Spring Compcon (1985), </booktitle> <pages> 102-105. </pages>
Reference: 13. <author> Stonebraker, M. </author> <title> The case for shared nothing. </title> <booktitle> IEEE Database Engineering 9,1 (Jan 1986) 4-9. </booktitle>
Reference: 14. <author> Strickland, J., Uhrowczik, P., and Watts, V. IMS/VS: </author> <title> an evolving system. </title> <journal> IBM Systems J. </journal> <month> 21,4 </month> <year> (1982) </year> <month> 490-510. </month>
Reference-contexts: A database system that exploits the con---guration such that any computer can access any of the data within a single transaction is called a "data sharing" system. Two commercial systems offering data sharing are Digital's Rdb/VMS (and DBMS) [10] and IBM's IMS/VS Data Sharing product <ref> [14] </ref>. A data sharing system does what is called "data shipping". That is, a data block, as it comes from the disk, is sent to the requesting computer.
Reference: 15. <author> Tandem Database Group. </author> <title> NonStop SQL, A Distributed, High-Performance, High-Availability Implementation of SQL. </title> <type> Tandem Technical Report 87.4, </type> <institution> Cupertino, </institution> <address> CA (April 1987) 23 </address>
Reference-contexts: In the partitioned system, messages are needed to fetch the account record when it is remote, then additional rounds of messages are needed to coordinate the commit. 1.2.2 Dynamically Scalable Performance and Load Balancing Partitioning a database has been used successfully to achieve scalable performance <ref> [15] </ref>. However, partitioning requires advanced planning and is dif---cult to change in a dynamic way in response to changes in load. With data sharing, since any processor in the cluster can access the shared data, if the load becomes unbalanced, a processor can readily be switched over to help out.
References-found: 15

