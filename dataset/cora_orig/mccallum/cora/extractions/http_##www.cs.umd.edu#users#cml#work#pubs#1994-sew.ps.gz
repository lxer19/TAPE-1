URL: http://www.cs.umd.edu/users/cml/work/pubs/1994-sew.ps.gz
Refering-URL: http://www.cs.umd.edu/users/cml/work/pubs/
Root-URL: 
Title: Does Software Design Complexity Affect Maintenance Effort?  
Author: Andreas Epping Christopher M. Lott 
Date: Dec 1994  
Note: Appeared in NASA/Goddard 19th Annual Software Engineering Workshop, 30 Nov1  At the time this study was performed, Epping was a student in the  
Address: 22297 Hamburg, Germany 67653 Kaiserslautern, Germany  
Affiliation: Coopers Lybrand Software Technology Transfer Initiative Consulting GmbH Department of Computer Science New-York-Ring 13 University of Kaiserslautern  Department of Computer Science, University of Kaiser-slautern.  
Abstract: The design complexity of a software system may be characterized within a refinement level (e.g., data flow among modules), or between refinement levels (e.g., traceability between the specification and the design). We analyzed an existing set of data from NASA's Software Engineering Laboratory to test whether changing software modules with high design complexity requires more personnel effort than changing modules with low design complexity. By analyzing variables singly, we identified strong correlations between software design complexity and change effort for error corrections performed during the maintenance phase. By analyzing variables in combination, we found patterns which identify modules in which error corrections were costly to perform during the acceptance test phase. 
Abstract-found: 1
Intro-found: 1
Reference: [BBH93] <author> Lionel C. Briand, Victor R. Basili, and Christopher J. Hetmanski. </author> <title> Developing interpretable models with optimized set reduction for identifying high-risk software components. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 19(11):10281044, </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: Appeared in Proc. 19th Software Engineering Workshop, December 1994 4 Based on the notion that a combination of independent variables might better explain high change effort than only a single variable, we planned to analyze multiple variables in combination using a machine-learning technique called Optimized Set Reduction (OSR) <ref> [BTH93, BBH93] </ref>. OSR finds patterns in the independent (explanatory) variables which reliably predict values of a single dependent variable. The OSR approach is insensitive to the scale of the data, but requires a large data set, ideally several hundred points. <p> Appeared in Proc. 19th Software Engineering Workshop, December 1994 12 Results of multivariate analyses. Because we had data for several hundred changes in the acceptance test phase, we were able to apply the OSR technique <ref> [BTH93, BBH93] </ref>. Based on the results achieved when working with the maintenance data, we restricted the data set to the error corrections.
Reference: [BR88] <author> Victor R. Basili and H. Dieter Rom-bach. </author> <title> The TAME Project: Towards improvementoriented software environments. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-14(6):758773, </volume> <month> June </month> <year> 1988. </year>
Reference-contexts: Hypothesis 2: Changing modules that are tightly coupled to each other via data and control-flow relationships requires more effort than changing modules that are loosely coupled to each other. 2.1 Design The case study to test our hypotheses was designed using the Goal/Question/Metric Paradigm <ref> [BW84, BR88] </ref>. Our G/Q/M goal was to analyze two FOR TRAN systems for the purpose of characterizing them with respect to the influence of design complexity on the maintainability of modules, from the point of view of the researchers within the context of the SEL.
Reference: [BTH93] <author> Lionel C. Briand, William M. Thomas, and Christopher J. Hetmanski. </author> <title> Modeling and managing risk early in software development. </title> <booktitle> In Proceedings of the Fifteenth International Conference on Software Engineering, </booktitle> <pages> pages 5565. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1993. </year>
Reference-contexts: Appeared in Proc. 19th Software Engineering Workshop, December 1994 4 Based on the notion that a combination of independent variables might better explain high change effort than only a single variable, we planned to analyze multiple variables in combination using a machine-learning technique called Optimized Set Reduction (OSR) <ref> [BTH93, BBH93] </ref>. OSR finds patterns in the independent (explanatory) variables which reliably predict values of a single dependent variable. The OSR approach is insensitive to the scale of the data, but requires a large data set, ideally several hundred points. <p> Appeared in Proc. 19th Software Engineering Workshop, December 1994 12 Results of multivariate analyses. Because we had data for several hundred changes in the acceptance test phase, we were able to apply the OSR technique <ref> [BTH93, BBH93] </ref>. Based on the results achieved when working with the maintenance data, we restricted the data set to the error corrections. <p> The technique found reliable patterns when using isolation effort as the dependent variable, but found no reliable results when using implementation effort or locality as the dependent variable. All results are expressed as OSR patterns. Patterns provide interpretable models where the impact of each predicate can be easily evaluated <ref> [BTH93] </ref>. An OSR pattern is a set of one or more predicates, where predicates have the form (EV i 2 EVclass ij ), meaning that a particular explanatory (independent) variable EV i belongs to part of its value domain, i.e., EVclass ij .
Reference: [BW84] <author> Victor R. Basili and David M. Weiss. </author> <title> A methodology for collecting valid software engineering data. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-10(6):728738, </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: Hypothesis 2: Changing modules that are tightly coupled to each other via data and control-flow relationships requires more effort than changing modules that are loosely coupled to each other. 2.1 Design The case study to test our hypotheses was designed using the Goal/Question/Metric Paradigm <ref> [BW84, BR88] </ref>. Our G/Q/M goal was to analyze two FOR TRAN systems for the purpose of characterizing them with respect to the influence of design complexity on the maintainability of modules, from the point of view of the researchers within the context of the SEL.
Reference: [CA88] <author> David N. Card and William W. </author> <title> Agresti. Measuring software design complexity. </title> <journal> Journal of Systems and Software, </journal> <pages> pages 185197, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: These changes, as well as many others, are performed during a period of time called the maintenance phase. Some authors see software design complexity as a highly important factor affecting the costs of software development and maintenance <ref> [Rom87, CA88] </ref>. We performed a study to test the hypothesis that changes to modules with high software design complexity require more personnel effort than changes to modules with low complexity. <p> He found a correlation of both isolation effort and locality with external complexity, but no correlation of implementation effort with external complexity. Our results support his with respect to isolation and implementation effort, but not locality. Card and Agresti <ref> [CA88] </ref> performed a case study on SEL FORTRAN systems to test for a relationship between a combined complexity measure and either productivity (lines of code delivered per unit of time) or fault rate in the context of development.
Reference: [CCA86] <author> David N. Card, Victor E. Church, and William W. </author> <title> Agresti. An empirical study of software design practices. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-12(2):264271, </volume> <month> February </month> <year> 1986. </year>
Reference-contexts: The primary dependent variable was the time required to implement the enhancement. They found no significant differences attributable to the use of global variables versus formal parameters. Card et al. <ref> [CCA86] </ref> performed a case study on five SEL FORTRAN systems to examine the impact of various design practices on the dependent variables fault rate and cost in the context of development.
Reference: [Cur80] <author> Bill Curtis. </author> <booktitle> Measurement and experimentation in software engineering. Proceedings of the IEEE, </booktitle> <address> 68(9):11441157, </address> <month> September </month> <year> 1980. </year>
Reference-contexts: One significant threat to external validity is the specialization of the software-system design used by the SEL. These results may not be applicable to other FORTRAN systems. 3 Complexity and Maintainability Curtis refines the concept of software complexity into algorithmic and psychological complexity <ref> [Cur80] </ref>. Algorithmic (or computational) complexity characterizes the run-time performance of an algorithm. Psychological complexity affects the performance of programmers trying to understand or modify a code module.
Reference: [Epp94] <author> Andreas Epping. </author> <title> An empirical investigation of the impact of the structure of two software systems on their maintainability (in German). </title> <type> Master's thesis, </type> <institution> Department of Computer Science, University of Kaiserslautern, </institution> <address> 67653 Kaiserslautern, Germany, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: The independent variables of the design complexity included a mapping to the specification, global data bindings, and control flow relationships. The dependent variables on maintainability were gathered by the SEL and include the necessary effort for isolating and implementing changes. This paper extends work first presented in <ref> [Epp94] </ref>. Section 2 gives the design of the case study, Section 3 discusses our complexity and effort metrics, and Section 4 explains the context of the study. Section 5 states the results for the maintenance and acceptance test data, and sketches related work.
Reference: [HB85] <author> David H. Hutchens and Victor R. Basili. </author> <title> System structure analysis: clustering with data bindings. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(8):749757, </volume> <month> August </month> <year> 1985. </year>
Reference-contexts: variables that were also used in the code. * Ratio of used global variables to visible global variables. * For modules p and q, and a variable x within the static scope of both p and q, a potential data binding is defined as an ordered triple (p; q; x) <ref> [HB85] </ref>. * Again using p, q, and x, a used data binding is a potential data binding where p and q either read a value from or assign a value to x [HB85]. * The fan-in measure of a module is the number of other modules which call the module. * <p> of both p and q, a potential data binding is defined as an ordered triple (p; q; x) <ref> [HB85] </ref>. * Again using p, q, and x, a used data binding is a potential data binding where p and q either read a value from or assign a value to x [HB85]. * The fan-in measure of a module is the number of other modules which call the module. * The fan-out measure of a module is the number of other modules which the module calls. 3.3 Maintainability Maintainability is an abstract concept that cannot be assessed directly but may be defined
Reference: [LZ84] <author> John B. Lohse and Stuart H. </author> <title> Zweben. Experimental evaluation of software design principles: An investigation into the effect of module coupling on system modifiability. </title> <journal> Journal of Systems and Software, </journal> <volume> 4(4):301308, </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: The counts of used globals and actual data bindings showed the most significant correlation of all measures; in an absolute sense the correlation is weak (approximately 0.60). These results support the idea that global variables make a program difficult to understand, although this conjecture was not supported by <ref> [LZ84] </ref> (see also Section 5.4). We found no significant correlation between complexity measures and implementation effort, nor between complexity measures and the number of modules changed. The measures of control-flow complexity were not helpful. <p> Note that comparisons with related work are dangerous owing to different definitions of both independent and dependent variables. Lohse and Zweben <ref> [LZ84] </ref> ran a controlled experiment to examine the effects of data coupling (data flow among modules) via global variables versus formal parameters, in the context of performing maintenance changes (enhancements) to two software systems. The primary dependent variable was the time required to implement the enhancement.
Reference: [Nat90] <author> National Aeronautics and Space Administration. </author> <title> Software Engineering Laboratory (SEL) Database Organization and User's Guide, Revision 1. </title> <type> Technical Report SEl-89-101, </type> <institution> NASA Goddard Space Flight Center, Greenbelt MD 20771, </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: This information told us which modules were part of a particular subsystem. While collecting these data, we found that not all of the modules changed are executable mod ules, and therefore are not in the call tree. Measures of change effort were obtained by querying the SEL database <ref> [Nat90] </ref> and by examining the data-collection forms completed by the maintainers after making the changes. Results from univariate analyses. For Project 1, 19 modules that were changed were found in the call tree. Of those 19 executable modules, only 3 supported multiple subsystems; i.e., helped implement more than one specification. <p> Data collection process. The measures M2 to M9 were computed from the source code as of the end of the maintenance phase. Again, module complexity values were averaged on a per change basis as explained in Section 5.1. Measures of change effort were obtained by querying the SEL database <ref> [Nat90] </ref>. Results of univariate analyses. Figure 4 uses data about error corrections from the acceptance test phase to plot the isolation effort against the average number of common blocks in the modules affected by each change.
Reference: [Nat91a] <author> National Aeronautics and Space Administration. </author> <title> Manager's handbook for software development. </title> <type> Technical Report SEL-84-101, </type> <institution> NASA Goddard Space Flight Center, Greenbelt MD 20771, </institution> <year> 1991. </year>
Reference-contexts: the application domain (ground-support software for satellites), which were similar for both systems, and the solution domain (FORTRAN), which was identical for both systems. 4.2 Activities in the acceptance test phase During the acceptance test phase, the original developers exercise the system to detect failures and repair faults as needed <ref> [Nat91a] </ref>. Enhancements and adaptations may also be made to the software during this phase owing to new requirements. 4.3 Activities in the maintenance phase During the maintenance phase, a team of software engineers who were not the original developers tests the software using simulators and modifies the systems as needed [Nat91a]. <p> <ref> [Nat91a] </ref>. Enhancements and adaptations may also be made to the software during this phase owing to new requirements. 4.3 Activities in the maintenance phase During the maintenance phase, a team of software engineers who were not the original developers tests the software using simulators and modifies the systems as needed [Nat91a]. These engineers are experts in their application domain, but not necessarily highly familiar with the software systems.
Reference: [Nat91b] <institution> National Aeronautics and Space Administration. </institution> <note> Software engineering laboratory Appeared in Proc. 19th Software Engineering Workshop, December 1994 16 (SEL) relationships, </note> <editor> models, </editor> <title> and management rules. </title> <type> Technical Report SEL-91-001, </type> <institution> NASA Goddard Space Flight Center, Greenbelt MD 20771, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: Step two involves isolating the modules to be changed. In step three, they plan and implement the change. Finally, in step four they test the changed code. The change effort data that was available to us were limited to the following, routinely collected items <ref> [Nat91b] </ref>: * Isolation effort: the effort to determine which modules must be changed (step two). * Implementation effort: the effort to plan, implement, and test the change (steps three and four) * Locality: the number of components affected by a change.
Reference: [PS93] <author> Rose Pajerski and Donald Smith. </author> <title> Recent SEL experiments and studies. </title> <booktitle> In Proceedings of the Eighteenth Annual Software Engineering Workshop, </booktitle> <pages> pages 81 94. </pages> <institution> NASA Goddard Space Flight Center, Greenbelt MD 20771, </institution> <year> 1993. </year>
Reference-contexts: Of those 28 months, the acceptance test phase lasted approximately 5 months. 1 A single-mission system is expected to cost 2% of development costs per year in maintenance until it is taken out of service, while a multi-mission system is expected to cost 10% <ref> [PS93] </ref>. Appeared in Proc. 19th Software Engineering Workshop, December 1994 7 Changes in maintenance. The single maintainer processed 15 change requests during maintenance. Of those, 5 were corrections, 9 were enhancements and 1 was an adaptation.
Reference: [Rom87] <author> H. Dieter Rombach. </author> <title> A controlled experiment on the impact of software structure on maintainability. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(3):344354, </volume> <month> March </month> <year> 1987. </year>
Reference-contexts: These changes, as well as many others, are performed during a period of time called the maintenance phase. Some authors see software design complexity as a highly important factor affecting the costs of software development and maintenance <ref> [Rom87, CA88] </ref>. We performed a study to test the hypothesis that changes to modules with high software design complexity require more personnel effort than changes to modules with low complexity. <p> Section 5 states the results for the maintenance and acceptance test data, and sketches related work. Finally, Section 6 summarizes lessons for the SEL, the researchers, and the software-engineering community. 2 Designing the Study This study, which was motivated in part by <ref> [Rom87] </ref>, began by refining the original hypothesis into two, closely related hypotheses: Hypothesis 1: Changing modules that implement many specifications requires more effort than changing modules that implement few specifications. <p> They found no correlation with the percentage of referenced variables in COMMON blocks but a positive correlation with the number of descendants (fan-out). The percentage of unreferenced variables from COMMON blocks correlated with faults, but not with cost. Rombach <ref> [Rom87] </ref> ran a controlled experiment to examine the effects of various programming-language constructs on isolation effort, implementation effort, and locality in the context of performing maintenance changes (enhancements) to two software systems. Complexity was measured in terms of information flow, which includes both data bindings and control flow between modules. <p> Future work might include replicating our study by analyzing the designs of other SEL software systems or systems from other software development organizations. Our data might also be used as a basis for planning and running a controlled experiment such as the one discussed in <ref> [Rom87] </ref> to test our hypotheses more rigorously. In a controlled experiment, programmers (subjects) might implement changes of similar sizes in modules that have low, medium, and high software design complexities.
Reference: [RUV92] <author> H. Dieter Rombach, Bradford T. Ulery, and Jon Valett. </author> <title> Toward full life cycle control: Adding maintenance measurement to the SEL. </title> <journal> Journal of Systems and Software, </journal> <volume> 18(2):125138, </volume> <month> May </month> <year> 1992. </year>
Reference-contexts: The SEL was founded and began collecting data about the FDD's development activities in 1976. Data collection from maintenance activities began in 1988 <ref> [RUV92] </ref>. 4.1 FDD Staff The staff who performed the changes were familiar with both the application domain (ground-support software for satellites), which were similar for both systems, and the solution domain (FORTRAN), which was identical for both systems. 4.2 Activities in the acceptance test phase During the acceptance test phase, the
References-found: 16

