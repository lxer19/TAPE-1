URL: http://www.cs.iastate.edu/tech-reports/TR91-09a.ps
Refering-URL: http://www.cs.iastate.edu/tech-reports/catalog.html
Root-URL: 
Title: Kolmogorov Complexity, Complexity Cores, and the Distribution of Hardness TR91-09a  
Author: David W. Juedes Jack H. Lutz 
Date: April, 1991  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. W. Allender, </author> <title> "Some consequences of the existence of pseudorandom generators." </title> <journal> Journal of Computer and System Sciences 39 (1989), </journal> <pages> pp. 101-124. </pages>
Reference: [2] <author> E. W. Allender, and R. Rubinstein, </author> <title> "P-printable sets." </title> <journal> SIAM Journal on Computing 17 (1988), </journal> <pages> pp. 1193-1202. </pages>
Reference: [3] <author> E. W. Allender and O. Watanabe, </author> <title> "Kolmogorov complexity and degrees of tally sets." </title> <booktitle> Information and Computation 86 (1990), </booktitle> <pages> pp. 160-178. </pages>
Reference: [4] <author> K. Ambos-Spies, </author> <title> "Randomness, relativizations, and polynomial re-ducibilities." </title> <booktitle> Proceedings of the First Annual Structure in Complexity Theory Conference. </booktitle> <year> (1986), </year> <pages> pp. 23-34. </pages>
Reference: [5] <author> J. L. Balcazar and R. </author> <title> Book, "Sets with small generalized Kolmogorov complexity." </title> <journal> Acta Informatica 23 (1986), </journal> <pages> pp. 679-688. </pages>
Reference: [6] <author> J. L. Balcazar, J. Daz, and J. Gabarro, </author> <title> Structural Complexity I, </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: a language A f0; 1g fl is the function CS A : N ! N defined as follows: For each n 2 N, CS A (n) is the minimum size (number of gates) required for an n-input, 1-output Boolean (acyclic, combinational) circuit to decide the set A =n . (See <ref> [34, 6, 49] </ref> for details of the circuit model, which can be varied in minor ways without affecting this discussion.) Circuit-size complexity has been investigated extensively for over forty years.
Reference: [7] <author> J. L. Balcazar, J. Daz, and J. Gabarro, </author> <title> Structural Complexity II, </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Note also that, if t (n) = O (s (n)), then every DSPACE (s (n))-complexity core of A is a DSPACE (t (n))-complexity core of A. Remark 10. Definition 9 quantifies over all machines consistent with A, while the standard definition of complexity cores (cf. <ref> [7] </ref>) quantifies only over machines that decide A. This difference renders Definition 9 stronger than the standard definition when A is not recursive. For example, consider tally languages (i.e., languages A f0g fl ).
Reference: [8] <author> J. L. Balcazar and U. Schoning, </author> <title> "Bi-immune sets for complexity classes." </title> <booktitle> Mathematical Systems Theory 18 (1985), </booktitle> <pages> pp. 1-10. </pages>
Reference-contexts: This enables one to eliminate the extraneous hypothesis that A is recursive from several results. In some cases (e.g., the fact that A is P-bi-immune if and only if f0; 1g fl is a P-complexity core for A <ref> [8] </ref>), this improvement is of little interest. However in x6 below, we show that every P m -hard language H for ESPACE has unusually small complexity cores, hence unusually low space-bounded Kolmogorov complexity. This upper bound holds regardless of whether H is recursive.
Reference: [9] <author> L. Berman, </author> <title> "On the structure of complete sets: almost everywhere complexity and infinitely often speed-up." </title> <booktitle> Proceedings of the 17th. IEEE Symp. of the Foundations of Computer Science, (1976) pp. </booktitle> <pages> 76-80. </pages>
Reference-contexts: Mayordomo's proof exploits the Berman <ref> [9] </ref> result that every P m -complete problem for E has an infinite subset in P.) 2 Preliminaries Most of our notation and terminology is standard. We deal with strings, languages, functions, and classes.
Reference: [10] <author> L. Berman and J. Hartmanis, </author> <title> "On isomorphism and density of NP and other complete sets." </title> <journal> SIAM Journal on Computing 6 (1977), </journal> <pages> pp. 305-322. </pages> <note> 32 Kolmogorov Complexity, Cores, and Hardness </note>
Reference: [11] <author> R. Book and D.-Z. Du, </author> <title> "The existence and density of generalized complexity cores." </title> <journal> Journal of the Association for Computing Machinery 34 (1987), </journal> <pages> pp. 718-730. </pages>
Reference-contexts: All the measure-theoretic results in x5-6 are proven by appeal to this almost everywhere lower bound on space-bounded Kolmogorov complexity. In x5 , we review the fundamental notion of a complexity core, introduced by Lynch [36] and investigated by many others <ref> [14, 16, 42, 43, 11, 21, 44, 12, 15, 51, etc.] </ref>. Intuitively, a complexity core for a language A is a fixed set K of inputs such that every machine whose decisions are consistent with A fails to decide efficiently on almost all elements of K.
Reference: [12] <author> R. Book, R., D.-Z Du, and D. Russo. </author> <title> "On polynomial and generalized complexity cores." </title> <booktitle> Proceedings of the Third Structure in Complexity Theory Conference (1988), </booktitle> <pages> pp. 236-250. </pages>
Reference-contexts: All the measure-theoretic results in x5-6 are proven by appeal to this almost everywhere lower bound on space-bounded Kolmogorov complexity. In x5 , we review the fundamental notion of a complexity core, introduced by Lynch [36] and investigated by many others <ref> [14, 16, 42, 43, 11, 21, 44, 12, 15, 51, etc.] </ref>. Intuitively, a complexity core for a language A is a fixed set K of inputs such that every machine whose decisions are consistent with A fails to decide efficiently on almost all elements of K.
Reference: [13] <author> G. J. Chaitin, </author> <title> "On the length of programs for computing finite binary sequences." </title> <journal> Journal of the Association for Computing Machinery 13 (1966), </journal> <pages> pp. 547-569. </pages>
Reference-contexts: In x3 below we summarize those aspects of resource-bounded measure that are used in this paper. Kolmogorov complexity, discussed in several papers in this volume, was introduced by Solomonoff [47], Kolmogorov [28], and Chaitin <ref> [13] </ref>. Resource-bounded Kolmogorov complexity has been investigated extensively [28, 18, 46, 29, 30, 5, 20, 26, 2, 1, 3, 32, 34, etc.]. In this paper we work with the space-bounded Kolmogorov complexity of languages.
Reference: [14] <author> D.-Z. Du, </author> <title> "Generalized complexity cores and levelability of intractable sets." </title> <type> Ph.D. dissertation, </type> <institution> University of California, Santa Barbara, </institution> <address> CA. </address> <year> 1985. </year>
Reference-contexts: All the measure-theoretic results in x5-6 are proven by appeal to this almost everywhere lower bound on space-bounded Kolmogorov complexity. In x5 , we review the fundamental notion of a complexity core, introduced by Lynch [36] and investigated by many others <ref> [14, 16, 42, 43, 11, 21, 44, 12, 15, 51, etc.] </ref>. Intuitively, a complexity core for a language A is a fixed set K of inputs such that every machine whose decisions are consistent with A fails to decide efficiently on almost all elements of K.
Reference: [15] <author> D.-Z. Du and R. </author> <title> Book, "On inefficient special cases of NP-complete problems." </title> <booktitle> Theoretical Computer Science 63 (1989): </booktitle> <pages> 239-252. </pages>
Reference-contexts: All the measure-theoretic results in x5-6 are proven by appeal to this almost everywhere lower bound on space-bounded Kolmogorov complexity. In x5 , we review the fundamental notion of a complexity core, introduced by Lynch [36] and investigated by many others <ref> [14, 16, 42, 43, 11, 21, 44, 12, 15, 51, etc.] </ref>. Intuitively, a complexity core for a language A is a fixed set K of inputs such that every machine whose decisions are consistent with A fails to decide efficiently on almost all elements of K.
Reference: [16] <author> S. Even, A. Selman, and Y. Yacobi, </author> <title> "Hard core theorems for complexity classes." </title> <journal> Journal of the Association for Computing Machinery 35 (1985), </journal> <pages> pp. 205-217. </pages>
Reference-contexts: All the measure-theoretic results in x5-6 are proven by appeal to this almost everywhere lower bound on space-bounded Kolmogorov complexity. In x5 , we review the fundamental notion of a complexity core, introduced by Lynch [36] and investigated by many others <ref> [14, 16, 42, 43, 11, 21, 44, 12, 15, 51, etc.] </ref>. Intuitively, a complexity core for a language A is a fixed set K of inputs such that every machine whose decisions are consistent with A fails to decide efficiently on almost all elements of K.
Reference: [17] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability: A Guide to the Theory of NP-completeness, W.H. </title> <publisher> Freeman and Company, </publisher> <year> 1979. </year>
Reference-contexts: A language C is P m complete for C if C 2 C and C is P m -hard for C. If C = NP, this is the usual notion of NP-completeness <ref> [17] </ref>.
Reference: [18] <author> J. Hartmanis, </author> <title> "Generalized Kolmogorov complexity and the structure of feasible computations." </title> <booktitle> Proceedings of the 24th IEEE Symposium on the Foundations of Computer Science (1983), </booktitle> <pages> pp. 439-445. </pages>
Reference: [19] <author> J. Hartmanis and Y. Yesha, </author> <title> "Computation times of NP sets of different densities." </title> <booktitle> Theoretical Computer Science 34 (1984), </booktitle> <pages> pp. 17-32. </pages> <editor> D. W. Juedes and J. H. </editor> <volume> Lutz 33 </volume>
Reference-contexts: In particular, much is known about the distribution of Kolmogorov complexities in ESPACE [34, x4 below], while very little is known at lower complexity levels. Second, the structure of ESPACE is closely related to the structure of polynomial complexity classes. For example, Hartmanis and Yesha <ref> [19] </ref> have shown that E $ ESPACE () P $ P=Poly " PSPACE: This, together with the first reason, suggests that the separation of P from PSPACE might best be achieved by separating E from ESPACE. We thus seek a detailed, quantitative account of the structure of ESPACE. <p> n By Theorem 2 (a), this implies that almost every language A 2 ESPACE is "very hard to approximate with circuits." This fact, together with the result of Nisan and Wigderson, immediately yields an upward measure separation theorem, stating that P 6= BPP ) (EjESPACE) = 0: (Hartmanis and Yesha <ref> [19] </ref> had previously shown that P 6= BPP ) E$ ESPACE.) 16 Kolmogorov Complexity, Cores, and Hardness In each of the above examples, space-bounded Kolmogorov complexity is used to prove that some set Z of languages has measure 1 in ESPACE.
Reference: [20] <author> D. T. Huynh, </author> <title> "Resource-bounded Kolmogorov complexity of hard languages." </title> <booktitle> Proceedings of the First Annual Structure in Complexity Theory Conference (1986), </booktitle> <pages> pp. 184-195. </pages>
Reference-contexts: Such problems are correctly regarded as exceedingly complex. They are provably intractable in terms of computational time and space. They have exponential circuit-size complexity [23], weakly exponential space-bounded Kolmogorov complexity <ref> [20] </ref>, and dense complexity cores [43, 21]. Problems that are P m -hard for ESPACE have all these properties and need not even be recursive. <p> In x6, we apply these results to our main topic, which is the complexity and distribution of P m -hard problems for ESPACE. It is well-known that such problems are not feasibly decidable and must obey certain lower bounds on their complexities. As noted above, Huynh <ref> [20] </ref> has proven that every P m -hard for ESPACE has weakly exponential (i.e., &gt; 2 n * for some * &gt; 0) space-bounded Kolmogorov complexity; and Orponen and Schoning [43] have (essentially) proven that every P m -hard language for ESPACE has a dense DSPACE (2 cn )-complexity core. <p> Proof. By Theorem 16, H " fA f0; 1g fl jKS 2 2n (A =n ) &gt; 2 n n a.e.g = ;, so this follows from Corollary 8. 2 7 Conclusion Very roughly speaking, our results (together with earlier work of <ref> [43, 20] </ref>) admit the following simple summary. We use KS (A =n ) and jK =n j as measures of the complexity of a language A, where K is a "largest" complexity core for A.
Reference: [21] <author> D. T. Huynh, </author> <title> "On solving hard problems by polynomial-size circuits." </title> <booktitle> Information Processing Letters 24 (1987), </booktitle> <pages> pp. 171-176. </pages>
Reference-contexts: Such problems are correctly regarded as exceedingly complex. They are provably intractable in terms of computational time and space. They have exponential circuit-size complexity [23], weakly exponential space-bounded Kolmogorov complexity [20], and dense complexity cores <ref> [43, 21] </ref>. Problems that are P m -hard for ESPACE have all these properties and need not even be recursive. Notwithstanding these lower bounds on the complexity of P m -hard problems for ESPACE, we will prove in x6 below that such problems are unusually simple in two respects. <p> All the measure-theoretic results in x5-6 are proven by appeal to this almost everywhere lower bound on space-bounded Kolmogorov complexity. In x5 , we review the fundamental notion of a complexity core, introduced by Lynch [36] and investigated by many others <ref> [14, 16, 42, 43, 11, 21, 44, 12, 15, 51, etc.] </ref>. Intuitively, a complexity core for a language A is a fixed set K of inputs such that every machine whose decisions are consistent with A fails to decide efficiently on almost all elements of K.
Reference: [22] <author> D. W. Juedes and J. H. Lutz, </author> <title> "The complexity and distribution of hard problems," </title> <note> in preparation. </note>
Reference-contexts: In order to simplify the exposition of the main ideas and to highlight the role played by Kolmogorov complexity, we do not state our results in the strongest possible form in this volume. The interested reader may wish to consult the technical paper <ref> [22] </ref> for a more thorough treatment of these issues. For example, it is shown in [22] that P m -hard problems for E have unusually small complexity cores, whence the P m -complete problems for E form a measure 0 subset of E. (Note added in proof: Recently, Mayordomo [38] has <p> The interested reader may wish to consult the technical paper <ref> [22] </ref> for a more thorough treatment of these issues. For example, it is shown in [22] that P m -hard problems for E have unusually small complexity cores, whence the P m -complete problems for E form a measure 0 subset of E. (Note added in proof: Recently, Mayordomo [38] has independently proven that the P m -complete problems for E form a measure 0 subset <p> In both measures, almost every language in ESPACE has complexity 2 n for almost every n. In both measures, every hard language for ESPACE has complexity between 2 n * and 2 n 2 n * for infinitely many n. In fact <ref> [22] </ref>, these bounds are tight. Acknowledgment We thank Osamu Watanabe and two anonymous reviewers for suggestions that have improved the exposition of this paper. D. W. Juedes and J. H. Lutz 31
Reference: [23] <author> R. Kannan, </author> <title> "Circuit-size lower bounds and non-reducibility to sparse sets." </title> <booktitle> Information and Control 55 (1982), </booktitle> <pages> pp. 40-56. </pages>
Reference-contexts: Problems that are P m -complete for ESPACE have been exhibited by Meyer and Stockmeyer [40], Stockmeyer and Chandra [48], and others. Such problems are correctly regarded as exceedingly complex. They are provably intractable in terms of computational time and space. They have exponential circuit-size complexity <ref> [23] </ref>, weakly exponential space-bounded Kolmogorov complexity [20], and dense complexity cores [43, 21]. Problems that are P m -hard for ESPACE have all these properties and need not even be recursive.
Reference: [24] <author> R. Karp, </author> <title> "Reducibility among combinatorial problems." In Complexity of Computer Computations. </title> <editor> Ed. R. E. Miller, and J. W. </editor> <booktitle> Thatcher, </booktitle> <pages> 85-104. </pages> <address> New York: </address> <publisher> Plenum Press, </publisher> <year> 1972. </year>
Reference-contexts: We thus seek a detailed, quantitative account of the structure of ESPACE. For simplicity of exposition, we work with polynomial time, many-one reducibility (" P m -reducibility"), introduced by Karp <ref> [24] </ref>. Problems that are P m -complete for ESPACE have been exhibited by Meyer and Stockmeyer [40], Stockmeyer and Chandra [48], and others. Such problems are correctly regarded as exceedingly complex. They are provably intractable in terms of computational time and space.
Reference: [25] <author> R. M. Karp and R. J. Lipton, </author> <title> "Some connections between nonuniform and uniform complexity classes." </title> <booktitle> Proceedings of the 12th ACM Symposium on Theory of Computing (1980), </booktitle> <pages> pp. 302-309. </pages> <note> Also published as "Turing machines that take advice", </note> <month> L'Enseignement Mathematique 28 </month> <year> (1982), </year> <pages> pp. 191-209. </pages>
Reference-contexts: It is well-known <ref> [25] </ref> that P/Poly consists exactly of those languages that are computed by polynomial-size Boolean circuits.
Reference: [26] <author> K. I. Ko, </author> <title> "On the notion of infinite pseudorandom sequences." </title> <booktitle> Theoretical Computer Science 48 (1986), </booktitle> <pages> pp. 9-33. </pages>
Reference: [27] <author> K. I. Ko, and D. Moore, </author> <title> "Completeness, approximation, and density." </title> <journal> SIAM Journal on Computing 10 (1981), </journal> <pages> pp. 787-796. </pages>
Reference: [28] <author> A. N. </author> <title> Kolmogorov, "Three approaches to the quantitative definition of `information'." </title> <booktitle> Problems of Information Transmission 1 (1965), </booktitle> <pages> pp. 1-7. </pages>
Reference-contexts: In x3 below we summarize those aspects of resource-bounded measure that are used in this paper. Kolmogorov complexity, discussed in several papers in this volume, was introduced by Solomonoff [47], Kolmogorov <ref> [28] </ref>, and Chaitin [13]. Resource-bounded Kolmogorov complexity has been investigated extensively [28, 18, 46, 29, 30, 5, 20, 26, 2, 1, 3, 32, 34, etc.]. In this paper we work with the space-bounded Kolmogorov complexity of languages.
Reference: [29] <author> L. A. Levin, </author> <title> "Randomness conservation inequalities; information and independence in mathematical theories." </title> <booktitle> Information and Control 61 (1984), </booktitle> <pages> pp. 15-37. </pages> <note> 34 Kolmogorov Complexity, Cores, and Hardness </note>
Reference: [30] <author> L. Longpre, </author> <title> "Resource bounded Kolmogorov complexity, a link between computational complexity and information theory." </title> <type> Ph.D. thesis, </type> <institution> Cor-nell University, </institution> <year> 1986. </year> <note> Technical Report TR-86-776. </note>
Reference: [31] <author> O. B. Lupanov, </author> <title> "On the synthesis of contact networks." </title> <journal> Dokl. Akad. Nauk SSSR 19 (1958), </journal> <pages> pp. 23-26. </pages>
Reference-contexts: That is, if we choose the language A f0; 1g fl probabilistically, according to a random experiment in which an independent toss of a fair coin is used to decide membership of each string x 2 f0; 1g fl in A, then Pr A [CS A (n) &gt; n Lupanov <ref> [31] </ref> proved that every language A f0; 1g fl has circuit-size com plexity CS A (n) &lt; n 1 n Since the lower bound (4.1) and the upper bound (4.3) have asympotic ratio 1, these results say that almost every language A has essentially maximum D. W. Juedes and J.
Reference: [32] <author> J. H. Lutz, </author> <title> "Category and measure in complexity classes." </title> <journal> SIAM Journal on Computing 19 (1990), </journal> <pages> pp. 1100-1131. </pages>
Reference-contexts: bound pspace = ff : f0; 1g fl ! f0; 1g fl j f is computable in polynomial spaceg: (The length jf (x)j of the output is included as part of the space used in computing f .) Resource-bounded measure was originally developed in terms of "modulated covering by cylinders" <ref> [32] </ref>. Though the main results of this paper are true, the underlying development was technically flawed. This situation is remedied in [34, 35], where resource-bounded measure is reformulated in terms of density functions. We review relevant aspects of the latter formulation here.
Reference: [33] <author> J. H. Lutz, </author> <title> "An upward measure separation theorem." </title> <booktitle> Theoretical Computer Science, 81 (1991), </booktitle> <pages> pp. 127-135. </pages>
Reference-contexts: Subsequent to this, Lutz <ref> [33] </ref> proved that there is a constant c 2 N such that every language A that is not "very hard to approximate with circuits" has space-bounded Kolmogorov complexity KS 2 cn n By Theorem 2 (a), this implies that almost every language A 2 ESPACE is "very hard to approximate with
Reference: [34] <author> J. H. Lutz, </author> <title> "Almost everywhere high nonuniform complexity." </title> <journal> Journal of Computer and System Sciences, </journal> <note> to appear. </note>
Reference-contexts: There are two related reasons for this choice. First, ESPACE has a rich, well-behaved structure that is well enough understood that we can prove absolute results, unblemished by oracles or unproven hypotheses. In particular, much is known about the distribution of Kolmogorov complexities in ESPACE <ref> [34, x4 below] </ref>, while very little is known at lower complexity levels. Second, the structure of ESPACE is closely related to the structure of polynomial complexity classes. <p> We thus turn to resource-bounded measure, a complexity-theoretic generalization of Lebesque mea 4 Kolmogorov Complexity, Cores, and Hardness sure developed by Lutz <ref> [34, 35] </ref>. Suppose we are given a resource bound, e.g., the set pspace, consisting of all functions computable in polynomial space. Then resource-bounded measure theory defines the pspace-measure pspace (X) of a set X of languages (provided that X is pspace-measurable). In all cases, 0 pspace (X) 1. <p> In all cases, 0 pspace (X) 1. If pspace (X) = 0 or pspace (X) = 1, then Pr (X) = 0 or Pr (X) = 1, respectively, but the pspace-measure conditions are much stronger than this: It is shown in <ref> [34, 35] </ref> that, if pspace (X) = 0, then X " ESPACE is a negligibly small subset of ESPACE. In fact, pspace-measure induces a natural, internal, measure structure on ESPACE. <p> W. Juedes and J. H. Lutz 5 complexity KS 2 cn (This improves the 2 n 2 *n lower bound of <ref> [34] </ref>.) It should be noted that the proof of this result is the only direct use of resource-bounded measure in this paper. All the measure-theoretic results in x5-6 are proven by appeal to this almost everywhere lower bound on space-bounded Kolmogorov complexity. <p> P m -hard or P m -complete for ESPACE. 3 Resource-Bounded Measure In this section we very briefly give some fundamentals of resource-bounded measure, where the resource bound is polynomial space. (This is the resource bound that endows ESPACE with measure structure.) For more details, examples, motivation, and proofs, see <ref> [34, 35] </ref>. <p> Though the main results of this paper are true, the underlying development was technically flawed. This situation is remedied in <ref> [34, 35] </ref>, where resource-bounded measure is reformulated in terms of density functions. We review relevant aspects of the latter formulation here. A density function is a function d : f0; 1g fl ! [0; 1) satisfying d (x) 2 for all x 2 f0; 1g fl . <p> In this case, we say that X contains almost every language in ESPACE. 12 Kolmogorov Complexity, Cores, and Hardness It is shown in <ref> [34, 35] </ref> that these definitions endow ESPACE with internal measure-theoretic structure. <p> More importantly, it is shown that the ideal I ESPACE is a proper ideal, i.e., that ESPACE does not have measure 0 in ESPACE. Our proof of Theorem 7 below does not proceed directly from the above definitions. Instead we use a sufficient condition, proved in <ref> [34] </ref>, for a set to have pspace-measure 0. To state this condition we need a polynomial notion of convergence for infinite series. All our series here consist of nonnegative terms. <p> A series is p-convergent if it has a modulus that is a polynomial. The following sufficient condition for a set to have pspace-measure 0 is a special case (for pspace) of a resource-bounded generalization of the classical first Borel-Cantelli lemma. Lemma 1. (Lutz <ref> [34] </ref>). <p> We now recall the following almost-everywhere lower bound result. Theorem 2. (Lutz <ref> [34] </ref>). <p> a language A f0; 1g fl is the function CS A : N ! N defined as follows: For each n 2 N, CS A (n) is the minimum size (number of gates) required for an n-input, 1-output Boolean (acyclic, combinational) circuit to decide the set A =n . (See <ref> [34, 6, 49] </ref> for details of the circuit model, which can be varied in minor ways without affecting this discussion.) Circuit-size complexity has been investigated extensively for over forty years. <p> W. Juedes and J. H. Lutz 15 circuit-size complexity almost everywhere. Lupanov named this phenomenon the Shannon effect. Lutz <ref> [34] </ref> used Theorem 2 to investigate the Shannon effect in ESPACE. The upper bound (4.3) applies a fortiori to languages in ESPACE, but the lower bound (4.2) does not directly say anything about ESPACE because Pr A [A 62 ESPACE] = 1 in the same random experiment. <p> However, it is not difficult to see that an upper bound on CS A (n) implies an upper bound on KS (A =n ). In fact, Lutz <ref> [34] </ref> showed that the quantitave details of this relation, combined with Theorem 2 (a), imply that, for every real ff &lt; 1, almost every language A 2 ESPACE (and, as a corollary, almost every language A f0; 1g fl ) has circuit-size complexity CS A (n) &gt; n ff log n <p> W. Juedes and J. H. Lutz 17 (ii) KS 2 c 2 n (Part (i) of Theorem 5 is well known and obvious. Part (ii), proven in <ref> [34] </ref>, extends a result of Martin-Lof [37].) Since the bound of Theorem 2 (b) is considerably lower than that of (4.4), one might expect to improve Theorem 2 (b).
Reference: [35] <author> J. H. Lutz, </author> <title> "Resource-bounded measure." </title> <note> in preparation. </note>
Reference-contexts: We thus turn to resource-bounded measure, a complexity-theoretic generalization of Lebesque mea 4 Kolmogorov Complexity, Cores, and Hardness sure developed by Lutz <ref> [34, 35] </ref>. Suppose we are given a resource bound, e.g., the set pspace, consisting of all functions computable in polynomial space. Then resource-bounded measure theory defines the pspace-measure pspace (X) of a set X of languages (provided that X is pspace-measurable). In all cases, 0 pspace (X) 1. <p> In all cases, 0 pspace (X) 1. If pspace (X) = 0 or pspace (X) = 1, then Pr (X) = 0 or Pr (X) = 1, respectively, but the pspace-measure conditions are much stronger than this: It is shown in <ref> [34, 35] </ref> that, if pspace (X) = 0, then X " ESPACE is a negligibly small subset of ESPACE. In fact, pspace-measure induces a natural, internal, measure structure on ESPACE. <p> P m -hard or P m -complete for ESPACE. 3 Resource-Bounded Measure In this section we very briefly give some fundamentals of resource-bounded measure, where the resource bound is polynomial space. (This is the resource bound that endows ESPACE with measure structure.) For more details, examples, motivation, and proofs, see <ref> [34, 35] </ref>. <p> Though the main results of this paper are true, the underlying development was technically flawed. This situation is remedied in <ref> [34, 35] </ref>, where resource-bounded measure is reformulated in terms of density functions. We review relevant aspects of the latter formulation here. A density function is a function d : f0; 1g fl ! [0; 1) satisfying d (x) 2 for all x 2 f0; 1g fl . <p> A null cover of a set X of languages is a 1-DS d such that, for all k 2 N, d k covers X with global value d k () 2 k . It is easy to show <ref> [35] </ref> that a set X of languages has classical Lebesgue measure 0 (i.e., probability 0 in the coin-tossing random experiment) if and only if there exists a null cover of X. In this paper we are interested in the situation where the null cover d is pspace-computable. Definitions. <p> In this case, we say that X contains almost every language in ESPACE. 12 Kolmogorov Complexity, Cores, and Hardness It is shown in <ref> [34, 35] </ref> that these definitions endow ESPACE with internal measure-theoretic structure.
Reference: [36] <author> N. Lynch, </author> <title> "On reducibility to complex or sparse sets." </title> <journal> Journal of the Association for Computing Machinery 22 (1975), </journal> <pages> pp. 341-345. </pages>
Reference-contexts: All the measure-theoretic results in x5-6 are proven by appeal to this almost everywhere lower bound on space-bounded Kolmogorov complexity. In x5 , we review the fundamental notion of a complexity core, introduced by Lynch <ref> [36] </ref> and investigated by many others [14, 16, 42, 43, 11, 21, 44, 12, 15, 51, etc.]. <p> This upper bound holds regardless of whether H is recursive. It should also be noted that standard existence theorems on complexity cores (e.g., every language A 62 P has an infinite P-complexity core <ref> [36] </ref>; every P m -hard language for E has a dense P-complexity core [43]) remain true under Definition 9. Thus no harm is done by quantifying over all machines consistent with A. Intuitively, a language is complex if it has very large complexity cores.
Reference: [37] <author> P. Martin-Lof, </author> <title> "Complexity oscillations in infinite binary sequences." </title> <journal> Zeitschrift fur Wahrscheinlichkeitstheory und Verwandte Gebiete 19 (1971), </journal> <pages> pp. 225-230. </pages>
Reference-contexts: We are thus led to ask how tight the lower bounds of Theorem 2 are. We first consider Theorem 2 (b). Martin-Lof <ref> [37] </ref> has shown that, for every real a &gt; 1, almost every language A f0; 1g fl has space-bounded Kolmogorov complexity KS 2 cn (In fact, Martin-Lof showed that this holds even in the absence of a space bound.) The following known bounds show that the lower bound (4.4) is tight. <p> W. Juedes and J. H. Lutz 17 (ii) KS 2 c 2 n (Part (i) of Theorem 5 is well known and obvious. Part (ii), proven in [34], extends a result of Martin-Lof <ref> [37] </ref>.) Since the bound of Theorem 2 (b) is considerably lower than that of (4.4), one might expect to improve Theorem 2 (b).
Reference: [38] <author> E. Mayordomo, </author> <title> "Almost every set in exponential time is P-bi-immune." </title> <type> Technical Report 91-19, </type> <institution> Department of Computer Science, Iowa State University, </institution> <year> 1991. </year>
Reference-contexts: For example, it is shown in [22] that P m -hard problems for E have unusually small complexity cores, whence the P m -complete problems for E form a measure 0 subset of E. (Note added in proof: Recently, Mayordomo <ref> [38] </ref> has independently proven that the P m -complete problems for E form a measure 0 subset of E. Mayordomo's proof exploits the Berman [9] result that every P m -complete problem for E has an infinite subset in P.) 2 Preliminaries Most of our notation and terminology is standard.
Reference: [39] <author> A. R. Meyer, </author> <note> reported in [10]. </note>
Reference-contexts: Theorem 14. For every P m -hard language H for ESPACE, there exist B; D 2 DSPACE (2 n ) such that D is dense and B = H " D. Proof. By a construction of Meyer <ref> [39] </ref>, there is a language A 2 DSPACE (2 n ) that is incompressible by P m -reductions. For the sake of completeness, we review the construction of A at the end of this proof. First, however, we use A to prove Theorem 14.
Reference: [40] <author> A. R. Meyer and L. Stockmeyer, </author> <title> "The equivalence problem for regular expressions with squaring requires exponential space." </title> <booktitle> Proceedings of the 13 th IEEE Symposium on Switching and Automata Theory (1972), </booktitle> <pages> pp. 125-129. </pages> <editor> D. W. Juedes and J. H. </editor> <volume> Lutz 35 </volume>
Reference-contexts: We thus seek a detailed, quantitative account of the structure of ESPACE. For simplicity of exposition, we work with polynomial time, many-one reducibility (" P m -reducibility"), introduced by Karp [24]. Problems that are P m -complete for ESPACE have been exhibited by Meyer and Stockmeyer <ref> [40] </ref>, Stockmeyer and Chandra [48], and others. Such problems are correctly regarded as exceedingly complex. They are provably intractable in terms of computational time and space. They have exponential circuit-size complexity [23], weakly exponential space-bounded Kolmogorov complexity [20], and dense complexity cores [43, 21].
Reference: [41] <author> N. Nisan and A. Wigderson, </author> <title> "Hardness vs. randomness." </title> <booktitle> Proceedings of the 29th IEEE Symposium on Foundations of Computer Science, </booktitle> <year> 1988, </year> <pages> pp. 2-11. </pages>
Reference-contexts: Thus the Shannon effect holds with full force in ESPACE. Example 4. Nisan and Wigderson <ref> [41] </ref> proved that, if E contains a language A that is, in a certain technical sense, "very hard to approximate with circuits," then this language A can be used to construct a pseudorandom generator that is fast enough and secure enough to establish the condition P = BPP.
Reference: [42] <author> P. Orponen, </author> <title> "A classification of complexity core lattices." </title> <booktitle> Theoretical Computer Science 70 (1986), </booktitle> <pages> pp. 121-130. </pages>
Reference-contexts: All the measure-theoretic results in x5-6 are proven by appeal to this almost everywhere lower bound on space-bounded Kolmogorov complexity. In x5 , we review the fundamental notion of a complexity core, introduced by Lynch [36] and investigated by many others <ref> [14, 16, 42, 43, 11, 21, 44, 12, 15, 51, etc.] </ref>. Intuitively, a complexity core for a language A is a fixed set K of inputs such that every machine whose decisions are consistent with A fails to decide efficiently on almost all elements of K.
Reference: [43] <author> P. Orponen and U. Schoning, </author> <title> "The density and complexity of polynomial cores for intractable sets." </title> <booktitle> Information and Control 70 (1986), </booktitle> <pages> pp. 54-68. </pages>
Reference-contexts: Such problems are correctly regarded as exceedingly complex. They are provably intractable in terms of computational time and space. They have exponential circuit-size complexity [23], weakly exponential space-bounded Kolmogorov complexity [20], and dense complexity cores <ref> [43, 21] </ref>. Problems that are P m -hard for ESPACE have all these properties and need not even be recursive. Notwithstanding these lower bounds on the complexity of P m -hard problems for ESPACE, we will prove in x6 below that such problems are unusually simple in two respects. <p> All the measure-theoretic results in x5-6 are proven by appeal to this almost everywhere lower bound on space-bounded Kolmogorov complexity. In x5 , we review the fundamental notion of a complexity core, introduced by Lynch [36] and investigated by many others <ref> [14, 16, 42, 43, 11, 21, 44, 12, 15, 51, etc.] </ref>. Intuitively, a complexity core for a language A is a fixed set K of inputs such that every machine whose decisions are consistent with A fails to decide efficiently on almost all elements of K. <p> As noted above, Huynh [20] has proven that every P m -hard for ESPACE has weakly exponential (i.e., &gt; 2 n * for some * &gt; 0) space-bounded Kolmogorov complexity; and Orponen and Schoning <ref> [43] </ref> have (essentially) proven that every P m -hard language for ESPACE has a dense DSPACE (2 cn )-complexity core. Intuitively, such results are not surprising, as we do not expect hard problems to be simple. <p> This upper bound holds regardless of whether H is recursive. It should also be noted that standard existence theorems on complexity cores (e.g., every language A 62 P has an infinite P-complexity core [36]; every P m -hard language for E has a dense P-complexity core <ref> [43] </ref>) remain true under Definition 9. Thus no harm is done by quantifying over all machines consistent with A. Intuitively, a language is complex if it has very large complexity cores. <p> Proof. By Theorem 16, H " fA f0; 1g fl jKS 2 2n (A =n ) &gt; 2 n n a.e.g = ;, so this follows from Corollary 8. 2 7 Conclusion Very roughly speaking, our results (together with earlier work of <ref> [43, 20] </ref>) admit the following simple summary. We use KS (A =n ) and jK =n j as measures of the complexity of a language A, where K is a "largest" complexity core for A.
Reference: [44] <author> D. A. Russo and P. </author> <booktitle> Orponen "On P-subset structures." Mathematical Systems Theory 20 (1987), </booktitle> <pages> pp. 129-136 </pages>
Reference-contexts: All the measure-theoretic results in x5-6 are proven by appeal to this almost everywhere lower bound on space-bounded Kolmogorov complexity. In x5 , we review the fundamental notion of a complexity core, introduced by Lynch [36] and investigated by many others <ref> [14, 16, 42, 43, 11, 21, 44, 12, 15, 51, etc.] </ref>. Intuitively, a complexity core for a language A is a fixed set K of inputs such that every machine whose decisions are consistent with A fails to decide efficiently on almost all elements of K.
Reference: [45] <author> C. E. Shannon, </author> <title> "The synthesis of two-terminal switching circuits." </title> <journal> Bell System Technical Journal 28 (1949), </journal> <pages> pp. 59-98. </pages>
Reference-contexts: Shannon <ref> [45] </ref> proved that almost every language A f0; 1g fl has circuit-size complexity CS A (n) &gt; n That is, if we choose the language A f0; 1g fl probabilistically, according to a random experiment in which an independent toss of a fair coin is used to decide membership of each
Reference: [46] <author> M. Sipser, </author> <title> "A complexity-theoretic approach to randomness." </title> <booktitle> Proceedings of the 15th ACM Symposium of the Theory of Computing (1983), </booktitle> <pages> pp. 330-335. </pages>
Reference: [47] <author> R. J. Solomonoff, </author> <title> "A formal theory of inductive inference." </title> <booktitle> Information and Control 7 (1964), </booktitle> <pages> pp. 1-22, 224-254. </pages>
Reference-contexts: Finally, we say that almost every language in ESPACE is in some set X of languages if (XjESPACE) = 1. In x3 below we summarize those aspects of resource-bounded measure that are used in this paper. Kolmogorov complexity, discussed in several papers in this volume, was introduced by Solomonoff <ref> [47] </ref>, Kolmogorov [28], and Chaitin [13]. Resource-bounded Kolmogorov complexity has been investigated extensively [28, 18, 46, 29, 30, 5, 20, 26, 2, 1, 3, 32, 34, etc.]. In this paper we work with the space-bounded Kolmogorov complexity of languages.
Reference: [48] <author> L. Stockmeyer and A. K. Chandra, </author> <title> "Provably difficult combinatorial games." </title> <journal> SIAM Journal on Computing 8 (1979), </journal> <pages> pp. 151-174. </pages>
Reference-contexts: For simplicity of exposition, we work with polynomial time, many-one reducibility (" P m -reducibility"), introduced by Karp [24]. Problems that are P m -complete for ESPACE have been exhibited by Meyer and Stockmeyer [40], Stockmeyer and Chandra <ref> [48] </ref>, and others. Such problems are correctly regarded as exceedingly complex. They are provably intractable in terms of computational time and space. They have exponential circuit-size complexity [23], weakly exponential space-bounded Kolmogorov complexity [20], and dense complexity cores [43, 21].
Reference: [49] <author> I. Wegener, </author> <title> The Complexity of Boolean Functions. </title> <booktitle> (Wiley-Teubner series in computer science), </booktitle> <address> Stuttgart: Wiley-Teubner, </address> <year> 1987. </year>
Reference-contexts: a language A f0; 1g fl is the function CS A : N ! N defined as follows: For each n 2 N, CS A (n) is the minimum size (number of gates) required for an n-input, 1-output Boolean (acyclic, combinational) circuit to decide the set A =n . (See <ref> [34, 6, 49] </ref> for details of the circuit model, which can be varied in minor ways without affecting this discussion.) Circuit-size complexity has been investigated extensively for over forty years.
Reference: [50] <author> C. B. Wilson, </author> <title> "Relativized Circuit Complexity." </title> <journal> Journal of Computer and System Sciences 31 (1985), </journal> <pages> pp. 169-181. </pages>
Reference: [51] <author> H. Ye, </author> <title> "Complexity cores for P/poly," </title> <note> submitted. </note>
Reference-contexts: All the measure-theoretic results in x5-6 are proven by appeal to this almost everywhere lower bound on space-bounded Kolmogorov complexity. In x5 , we review the fundamental notion of a complexity core, introduced by Lynch [36] and investigated by many others <ref> [14, 16, 42, 43, 11, 21, 44, 12, 15, 51, etc.] </ref>. Intuitively, a complexity core for a language A is a fixed set K of inputs such that every machine whose decisions are consistent with A fails to decide efficiently on almost all elements of K.
References-found: 51

