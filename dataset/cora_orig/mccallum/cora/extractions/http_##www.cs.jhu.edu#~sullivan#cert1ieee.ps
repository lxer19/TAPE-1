URL: http://www.cs.jhu.edu/~sullivan/cert1ieee.ps
Refering-URL: http://www.cs.jhu.edu/~sullivan/ftdownload.html
Root-URL: http://www.cs.jhu.edu
Author: Gregory F. Sullivan Dwight S. Wilson Gerald M. Masson 
Keyword: Software fault tolerance, error monitoring, design diversity, data structures.  
Address: Baltimore, MD 21218  
Affiliation: Dept. of Computer Science, Johns Hopkins Univ.,  
Abstract: Certification of Computational Results Abstract We describe a conceptually novel and powerful technique to achieve fault detection and fault tolerance in hardware and software systems. When used for software fault detection, this new technique uses time and software redundancy and can be outlined as follows. In the initial phase, a program is run to solve a problem and store the result. In addition, this program leaves behind a trail of data which we call a certification trail. In the second phase, another program is run which solves the original problem again. This program, however, has access to the certification trail left by the first program. Because of the availability of the certification trail, the second phase can be performed by a less complex program and can execute more quickly. In the final phase, the two results are compared and if they agree the results are accepted as correct; otherwise an error is indicated. An essential aspect of this approach is that the second program must always generate either an error indication or a correct output even when the certification trail it receives from the first program is incorrect. We formalize the certification trail approach to fault tolerance and illustrate realizations of it by considering algorithms for the following problems: convex hull, sorting, and shortest path. We compare the certification trail approach to other approaches to fault tolerance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adel'son-Vel'skii, G. M., and Landis, E. M., </author> <title> "An algorithm for the organization of information", </title> <journal> Soviet Math. Dokl., </journal> <pages> pp. 1259-1262, 3, </pages> <year> 1962. </year>
Reference-contexts: First execution: In this execution the SHORTEST-PATH code is used and the abstract data type is implemented with a balanced search tree such as an AVL tree <ref> [1] </ref>, a red-black tree [15], or a b-tree [5]. In 14 15 addition, an array indexed from 1 to n is used. Each element of this array contains two fields, InSet, a boolean, and Value, storing the same type as the value used in the ordered pairs.
Reference: [2] <author> Anderson, T., and Lee, P., </author> <title> Fault tolerance: principles and practices, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: The advantages of utilizing this additional information are shown in the body of this paper. In effect, N-version programming can be thought of relative to the certification trail approach as the employment of a null trail. Another valuable technique, known as the recovery block approach <ref> [2, 19, 22] </ref>, was proposed by Randell. It uses acceptance tests and alternative procedures to produce what is to be regarded as a correct output from a program. <p> The rigor, completeness, and nature of the acceptance test is left to the program designer, and many of the acceptance tests that have been proposed for use tend to be somewhat straightforward <ref> [2] </ref>. When using certification trails it is clearly possible to combine the second execution and the comparison test to yield a program which certifies the correctness of the output of the first execution. Unlike an acceptance test this certifier must satisfy strict formal properties of correctness.
Reference: [3] <author> Avizienis, A., </author> <title> "The N-version approach to fault tolerant software," </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> vol. 11, </volume> <pages> pp. 1491-1501, </pages> <month> Dec., </month> <year> 1985. </year>
Reference-contexts: A variation of the above method uses two separate algorithms, one for each execution, which have been written independently based on the problem specification. This technique, called N-version programming <ref> [10, 3] </ref> (in this case N=2), allows for the detection of errors caused by some faults in the software in addition to those caused by transient hardware faults and utilizes both time and software redundancy. <p> These distinctions are primarily related to the generation and character of the certification trail and the manner in which the secondary algorithm uses the certification trail. First consider the important and useful technique called N-version programming <ref> [10, 3] </ref>. When using this technique N different implementations of an algorithm are independently executed with subsequent comparison of the resulting N outputs.
Reference: [4] <author> Babai, L., Fortnow, L., Levin, L., and Szegedy, M., </author> <title> "Checking computations in polylogarithmic time, </title> " <booktitle> Proceedings of the 23rd ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 21-31, </pages> <year> 1991. </year>
Reference-contexts: Recently, Blum and Kannan [6] have defined what they call a program checker. This valuable work has been followed by a burst of activity in this general area <ref> [13, 7, 28, 8, 4] </ref>. Each of these papers, however, describes work which differs significantly from the work we present. A program checker is an algorithm which checks the output of another algorithm for correctness. <p> Also, we are able to achieve a speed up in the checking process and they are not. 25 Babai, Fortnow, Levin and Szegedy <ref> [4] </ref> present methods which appear to allow remarkably fast checking, i.e., in polylogarithmic time. Their approach has some similarities to the methods we propose. Both methods modify original algorithms to yield new algorithms which output additional information. <p> In our case we are interested in modified algorithms which have the same asymptotic time complexity as the original algorithm. Indeed, in our implementations the modified algorithms run almost as quickly as the original ones. In <ref> [4] </ref> the modified algorithm is slowed down by more than any fixed multiplicative factor. Specifically, if the original algorithm has a time complexity of O (T ) then the modified algorithm has a time complexity of O (T 1+* ).
Reference: [5] <author> Bayer, R., and McCreight, E., </author> <title> "Organization of large ordered indexes", </title> <journal> Acta Inform., </journal> <pages> pp 173-189, 1, </pages> <year> 1972. </year>
Reference-contexts: First execution: In this execution the SHORTEST-PATH code is used and the abstract data type is implemented with a balanced search tree such as an AVL tree [1], a red-black tree [15], or a b-tree <ref> [5] </ref>. In 14 15 addition, an array indexed from 1 to n is used. Each element of this array contains two fields, InSet, a boolean, and Value, storing the same type as the value used in the ordered pairs. Initially, InSet is false for all array elements.
Reference: [6] <author> Blum, M., and Kannan, S., </author> <title> "Designing programs that check their work", </title> <booktitle> Proceedings of the 1989 ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 86-97, </pages> <publisher> ACM Press, </publisher> <year> 1989. </year>
Reference-contexts: As seen in Section 6, the production of this trail does not add significant overhead. 24 Moreover, any combination of computational errors occurring in a single phase of the certification trail solution can be handled. Recently, Blum and Kannan <ref> [6] </ref> have defined what they call a program checker. This valuable work has been followed by a burst of activity in this general area [13, 7, 28, 8, 4]. Each of these papers, however, describes work which differs significantly from the work we present.
Reference: [7] <author> Blum, M., Luby, M., and Rubinfeld, R., </author> <title> "Self-testing/correcting with applications to numerical problems," </title> <booktitle> Proceedings of the 22nd ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 73-83, </pages> <year> 1990. </year>
Reference-contexts: Recently, Blum and Kannan [6] have defined what they call a program checker. This valuable work has been followed by a burst of activity in this general area <ref> [13, 7, 28, 8, 4] </ref>. Each of these papers, however, describes work which differs significantly from the work we present. A program checker is an algorithm which checks the output of another algorithm for correctness. <p> However, it is clearly possible to define them in the more general probabilistic context. Other work has been done to extend the ideas of Blum-Kannan to give methods which allow the conversion of some programs into new programs which are self-testing and self-correcting <ref> [13, 7] </ref>. However, these methods are also based on treating programs as black boxes and thus have constraints similar to Blum-Kannan program checkers. A recent paper by Blum et al. [8] concerns checking the correctness of memories and data structures.
Reference: [8] <author> Blum, M., Evans, W., Gemmell P., Kannan, S., and Naor, M., </author> <title> "Checking the correctness of memories," </title> <booktitle> Proceedings of the 32nd IEEE Symposium on Foundations of Computer Science pp. </booktitle> <pages> 90-99, </pages> <year> 1991. </year>
Reference-contexts: Recently, Blum and Kannan [6] have defined what they call a program checker. This valuable work has been followed by a burst of activity in this general area <ref> [13, 7, 28, 8, 4] </ref>. Each of these papers, however, describes work which differs significantly from the work we present. A program checker is an algorithm which checks the output of another algorithm for correctness. <p> However, these methods are also based on treating programs as black boxes and thus have constraints similar to Blum-Kannan program checkers. A recent paper by Blum et al. <ref> [8] </ref> concerns checking the correctness of memories and data structures. The results described in that paper differ from our work using abstract data types in one central way. The checkers that they design are tightly constrained in memory usage.
Reference: [9] <author> Bright, J., Sullivan, G., Masson, G. </author> <title> "A formally verifiable sorting certifier," </title> <institution> Department of Computer Science Technical Report TR 94/95-1, Johns Hopkins University, Baltimore, Maryland, </institution> <year> 1994. </year>
Reference-contexts: Furthermore, the second execution algorithm is typically simpler and easier to reason about than the original problem. In fact, the second algorithm may be simple enough so that formal verification techniques may be applied to it <ref> [9] </ref>. We also note that an important class of errors are those arising in problem specification rather than design and implementation. While the certification trail technique does not directly address this, the insight gained from developing a 26 certification trail solution may help uncover specification errors.
Reference: [10] <author> Chen, L., and Avizienis A., </author> <title> "N-version programming: a fault tolerant approach to reliability of software operation," </title> <booktitle> Digest of the 1978 Fault Tolerant Computing Symposium, </booktitle> <pages> pp. 3-9, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1978. </year>
Reference-contexts: A variation of the above method uses two separate algorithms, one for each execution, which have been written independently based on the problem specification. This technique, called N-version programming <ref> [10, 3] </ref> (in this case N=2), allows for the detection of errors caused by some faults in the software in addition to those caused by transient hardware faults and utilizes both time and software redundancy. <p> These distinctions are primarily related to the generation and character of the certification trail and the manner in which the secondary algorithm uses the certification trail. First consider the important and useful technique called N-version programming <ref> [10, 3] </ref>. When using this technique N different implementations of an algorithm are independently executed with subsequent comparison of the resulting N outputs.
Reference: [11] <author> Cormen, T. H., and Leiserson, C. E., and Rivest, R. L., </author> <title> Introduction to Algorithms McGraw-Hill, </title> <address> New York, NY, </address> <year> 1990. </year>
Reference-contexts: Our approach is applied to a variant of the Dijkstra algorithm [12] as explicated in <ref> [11] </ref>. First we require some preliminary definitions. Definition 5.1 A graph G = (V; E) consists of a vertex set V and an edge set E. <p> There are at most O (m) such operations and O (m) additional time overhead where jEj = m. Thus, the first execution can be performed in O (m log (n)). Detailed proofs of this result may be found in many algorithms texts, including <ref> [11] </ref>. In the second execution each data structure operation can be performed in O (1) time. There are still at most O (m) such operations and O (m) additional time overhead. Hence, the second execution can be performed in O (m) time, i.e., linear time.
Reference: [12] <author> Dijkstra, E. W., </author> <title> "A note on two problems in connexion with graphs," </title> <journal> Numer. Math. </journal> <volume> 1, </volume> <pages> pp. 269-271, </pages> <month> Sept., </month> <year> 1959. </year>
Reference-contexts: Our approach is applied to a variant of the Dijkstra algorithm <ref> [12] </ref> as explicated in [11]. First we require some preliminary definitions. Definition 5.1 A graph G = (V; E) consists of a vertex set V and an edge set E.
Reference: [13] <author> Gemmell, R., Lipton, R., Rubinfeld, R., Sudan, M., and Wigderson, A., </author> <title> "Self-testing/correcting for polynomials and for approximate functions," </title> <booktitle> Proceedings of the 23rd ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 32-42, </pages> <year> 1991. </year> <month> 29 </month>
Reference-contexts: Recently, Blum and Kannan [6] have defined what they call a program checker. This valuable work has been followed by a burst of activity in this general area <ref> [13, 7, 28, 8, 4] </ref>. Each of these papers, however, describes work which differs significantly from the work we present. A program checker is an algorithm which checks the output of another algorithm for correctness. <p> However, it is clearly possible to define them in the more general probabilistic context. Other work has been done to extend the ideas of Blum-Kannan to give methods which allow the conversion of some programs into new programs which are self-testing and self-correcting <ref> [13, 7] </ref>. However, these methods are also based on treating programs as black boxes and thus have constraints similar to Blum-Kannan program checkers. A recent paper by Blum et al. [8] concerns checking the correctness of memories and data structures.
Reference: [14] <author> Graham, R. L., </author> <title> "An efficient algorithm for determining the convex hull of a planar set", </title> <journal> Information Processing Letters, </journal> <pages> pp. 132-133, 1, </pages> <year> 1972. </year>
Reference-contexts: Rather, these algorithms have been selected for illustrative purposes. 3 Certification Trails for Convex Hulls 3.1 Description of the Algorithm and Certification Trail The convex hull problem is a fundamental one in computational geometry. Our certification trail solution is based on a solution due to Graham <ref> [14] </ref> called Graham's Scan. For basic definitions in computational geometry see the text of Preparata and Shamos [21]. This text also illustrates some statistical applications of convex hull computations.
Reference: [15] <author> Guibas, L. J., and Sedgewick, R., </author> <title> "A dichromatic framework for balanced trees", </title> <booktitle> Proceedings of the Nineteenth Annual Symposium on Foundations of Computing, </booktitle> <pages> pp. 8-21, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1978. </year>
Reference-contexts: First execution: In this execution the SHORTEST-PATH code is used and the abstract data type is implemented with a balanced search tree such as an AVL tree [1], a red-black tree <ref> [15] </ref>, or a b-tree [5]. In 14 15 addition, an array indexed from 1 to n is used. Each element of this array contains two fields, InSet, a boolean, and Value, storing the same type as the value used in the ordered pairs.
Reference: [16] <author> Huang, K.-H., and Abraham, J., </author> <title> "Algorithm-based fault tolerance for matrix operations," </title> <journal> IEEE Trans. on Computers, pp. </journal> <volume> 518-529, vol. C-33, </volume> <month> June, </month> <year> 1984. </year>
Reference-contexts: Also note that the certification trail technique emphasizes the capability of generating additional data to ease the certifying process and does not rely solely on data which would normally be computed. It should be possible to fruitfully combine the ideas of recovery blocks and certification trails. Algorithm-based fault tolerance <ref> [16, 18, 20] </ref> uses error detecting and correcting codes for performing reliable computations with specific algorithms. This technique encodes data at a high level and algorithms are specifically designed or modified to operate on encoded data and produce encoded output data.
Reference: [17] <author> Johnson, B., </author> <title> Design and analysis of fault tolerant digital systems Addison-Wesley, </title> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: This method requires additional time, so called time 1 Research partially supported by NSF Grants CCR-8910569, CCR-8908092 and CCR-9319945. 2 Research partially supported by NSF Grant CDA-9015667. 3 Research partially supported by NASA Grant NSG 1442 and NSF Grant CCR-9319945. 1 redundancy <ref> [17, 24] </ref>; however, it requires no additional software. It is particularly valuable for detecting errors caused by transient fault phenomena. If such faults cause an error during only one of the executions then either the error will be detected or the output will be correct.
Reference: [18] <author> Jou, J.-Y. and Abraham, J. </author> <title> "Fault tolerant FFT networks," </title> <booktitle> Dig. of the 1985 Fault Tolerant Computing Symposium, </booktitle> <pages> pp. 338-343, </pages> <publisher> IEEE Computer Society Press, </publisher> <month> June, </month> <year> 1985. </year>
Reference-contexts: Also note that the certification trail technique emphasizes the capability of generating additional data to ease the certifying process and does not rely solely on data which would normally be computed. It should be possible to fruitfully combine the ideas of recovery blocks and certification trails. Algorithm-based fault tolerance <ref> [16, 18, 20] </ref> uses error detecting and correcting codes for performing reliable computations with specific algorithms. This technique encodes data at a high level and algorithms are specifically designed or modified to operate on encoded data and produce encoded output data.
Reference: [19] <author> Lee, Y.H. and Shin, K.G., </author> <title> "Design and evaluation of a fault-tolerant multiprocessor using hardware recovery blocks," </title> <journal> IEEE Trans. Comput., </journal> <volume> vol. C-33, </volume> <pages> pp. 113-124, </pages> <month> Feb. </month> <year> 1984. </year>
Reference-contexts: The advantages of utilizing this additional information are shown in the body of this paper. In effect, N-version programming can be thought of relative to the certification trail approach as the employment of a null trail. Another valuable technique, known as the recovery block approach <ref> [2, 19, 22] </ref>, was proposed by Randell. It uses acceptance tests and alternative procedures to produce what is to be regarded as a correct output from a program.
Reference: [20] <author> Nair, V., and Abraham, J., </author> <title> "General linear codes for fault-tolerant matrix operations on processor arrays," </title> <booktitle> Dig. of the 1988 Fault Tolerant Computing Symposium, </booktitle> <pages> pp. 180-185, </pages> <month> June, </month> <year> 1988. </year>
Reference-contexts: Also note that the certification trail technique emphasizes the capability of generating additional data to ease the certifying process and does not rely solely on data which would normally be computed. It should be possible to fruitfully combine the ideas of recovery blocks and certification trails. Algorithm-based fault tolerance <ref> [16, 18, 20] </ref> uses error detecting and correcting codes for performing reliable computations with specific algorithms. This technique encodes data at a high level and algorithms are specifically designed or modified to operate on encoded data and produce encoded output data.
Reference: [21] <author> Preparata F. P., and Shamos M. I., </author> <title> Computational geometry: an introduction, </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1985. </year>
Reference-contexts: Our certification trail solution is based on a solution due to Graham [14] called Graham's Scan. For basic definitions in computational geometry see the text of Preparata and Shamos <ref> [21] </ref>. This text also illustrates some statistical applications of convex hull computations. For simplicity in the following discussion we will assume the points are in so called general position, i.e., no three points are co-linear. It is not difficult to remove this restriction. <p> Time complexity: In the first execution the sorting of the input points takes O (n log (n)) time where n is the number of input points. One can show that this cost dominates and the overall complexity is O (n log (n)). See <ref> [21] </ref> for a proof of this result. It is possible to implement the second execution so that all five checks are done in O (n) time. <p> Note that generation of the certification trail does not affect the output of the Graham Scan algorithm. Thus the condition on F 1 (d) is satisfied by the correctness of the Graham Scan algorithm, the proof of which is well known <ref> [21] </ref>. To show that F 2 (d; t) = s, note that a copy of s is contained on the trail t. Our description of F 2 (d; t) states that s is output unless one of the five checks above fails. <p> Thus we may determine containing triangles for the non-hull points in O (nlogh) time. Under several distributions the number of hull points is much smaller than the number of input points <ref> [21] </ref> so this overhead will often be quite small. 4 Sorting 4.1 Description of the Algorithm and Certification Trail Sorting is one of the most important basic problems in computer science.
Reference: [22] <author> Randell, B., </author> <title> "System structure for software fault tolerance," </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> vol. 1, </volume> <pages> pp. 220-232, </pages> <month> June, </month> <year> 1975. </year>
Reference-contexts: The advantages of utilizing this additional information are shown in the body of this paper. In effect, N-version programming can be thought of relative to the certification trail approach as the employment of a null trail. Another valuable technique, known as the recovery block approach <ref> [2, 19, 22] </ref>, was proposed by Randell. It uses acceptance tests and alternative procedures to produce what is to be regarded as a correct output from a program.
Reference: [23] <author> Rubinfeld, R., </author> <title> "A Mathematical Theory of Self-Checking, Self-Testing, and Self-Correcting Programs," </title> <type> PhD Thesis, </type> <institution> University of California at Berkeley, </institution> <year> 1990. </year>
Reference-contexts: Therefore, both conditions of Definition 2.2 are satisfied, so F 1 and F 2 constitute a certification trail solution to sorting. The certification trail solution is similar to the program checker for sorting discovered independently which appears in <ref> [23] </ref>. In their model, however, the sorting program is treated as a black box that cannot be modified, so the tags must be encoded as part of the input.
Reference: [24] <author> Siewiorek, D., and Swarz, R., </author> <title> The theory and practice of reliable design, </title> <publisher> Digital Press, </publisher> <address> Bedford, MA, </address> <year> 1982. </year>
Reference-contexts: This method requires additional time, so called time 1 Research partially supported by NSF Grants CCR-8910569, CCR-8908092 and CCR-9319945. 2 Research partially supported by NSF Grant CDA-9015667. 3 Research partially supported by NASA Grant NSG 1442 and NSF Grant CCR-9319945. 1 redundancy <ref> [17, 24] </ref>; however, it requires no additional software. It is particularly valuable for detecting errors caused by transient fault phenomena. If such faults cause an error during only one of the executions then either the error will be detected or the output will be correct.
Reference: [25] <author> Sullivan, </author> <title> G.F., and Masson, G.M., "Certification trails for data structures," </title> <booktitle> Digest of the 1991 Fault Tolerant Computing Symposium, </booktitle> <pages> pp. 240-247, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Finally, we note that the technique of certifying a set of data operations, as used in the shortest path example, provides a general widely applicable method for generating certification trail solutions <ref> [25] </ref> of problems. When using N-version programming it is important to consider the risk of common design and implementation faults.
Reference: [26] <author> Tarjan, R. E., </author> <title> "Applications of path compression on balanced trees", </title> <journal> J. ACM, </journal> <pages> pp. 690-715, </pages> <month> Oct., </month> <year> 1979. </year>
Reference-contexts: Each of these papers, however, describes work which differs significantly from the work we present. A program checker is an algorithm which checks the output of another algorithm for correctness. An early example of a program checker is the algorithm developed by Tarjan <ref> [26] </ref> which takes as input a graph and a supposed minimum spanning tree and indicates whether or not the tree actually is a minimum spanning tree. The Blum-Kannan program checking method differs from the certification trail method in two important ways.
Reference: [27] <author> Wallich, P., </author> <title> "Crunching Epsilon," </title> <publisher> Scientific American, </publisher> <pages> pp. 22-24, </pages> <month> Jan., </month> <year> 1993. </year>
Reference-contexts: It is also necessary to decode the output after the check. Lastly, although this is a promising direction, we note that Fortnow has stated that their result is currently not practical <ref> [27] </ref>. A possible objection to the use of certification trails is that considerable effort may be required to design a certification trail solution.
Reference: [28] <author> Yao, Andrew Chi-Chih, </author> <title> "Coherent Functions and Program Checkers," </title> <booktitle> Proc. 22 ACM Symp. of Theory of Computing, </booktitle> <pages> pp. 84-94. 30 </pages>
Reference-contexts: Recently, Blum and Kannan [6] have defined what they call a program checker. This valuable work has been followed by a burst of activity in this general area <ref> [13, 7, 28, 8, 4] </ref>. Each of these papers, however, describes work which differs significantly from the work we present. A program checker is an algorithm which checks the output of another algorithm for correctness.
References-found: 28

