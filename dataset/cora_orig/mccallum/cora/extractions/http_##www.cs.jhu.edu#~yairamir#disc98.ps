URL: http://www.cs.jhu.edu/~yairamir/disc98.ps
Refering-URL: http://www.cs.jhu.edu/~yairamir/
Root-URL: http://www.cs.jhu.edu
Email: dshaw-@cs.jhu.edu  
Title: Seamlessly Selecting the Best Copy from Internet-Wide Replicated Web Servers  
Author: Yair Amir, Alec Peterson, and David Shaw 
Address: -yairamir, chuckie,  
Affiliation: Department of Computer Science Johns Hopkins University  
Abstract: The explosion of the web has led to a situation where a majority of the traffic on the Internet is web related. Today, practically all of the popular web sites are served from single locations. This necessitates frequent long distance network transfers of data (potentially repeatedly) which results in a high response time for users, and is wasteful of the available network bandwidth. Moreover, it commonly creates a single point of failure between the web site and its Internet provider. This paper presents a new approach to web replication, where each of the replicas resides in a different part of the network, and the browser is automatically and transparently directed to the best server. Implementing this architecture for popular web sites will result in a better response-time and a higher availability of these sites. Equally important, this architecture will potentially cut down a significant fraction of the traffic on the Internet, freeing bandwidth for other uses. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <institution> NSFNET Backbone Traffic Distribution by Service report. ftp://ftp.merit.edu/nsfnet/statistics/1995/nsf-9503.ports.gz </institution>
Reference-contexts: 1. Introduction The explosion of the web has led to a situation where a majority of the traffic on the Internet is web related. In fact, in the beginning of 1995, web traffic became the single largest load on the Internet <ref> [1] </ref>. Today, practically all of the popular web sites are served from single locations. This necessitates frequent long distance network transfers of data (potentially repeatedly) which results in a high response time for users, and is wasteful of the available network bandwidth.
Reference: 2. <author> Trigdell, A., Mackerras, P.: </author> <title> The Rsync Algorithm. </title> <type> Technical Report TR-CS-96-05, </type> <institution> The Australian National University. </institution> <note> Available at ftp://samba.anu.edu.au/pub/rsync/ </note>
Reference-contexts: The next section describes our experience with the different methods. We then analyze and compare the methods, and draw our conclusions. Due to lack of space, we will not detail a replication protocol to keep the consistency of the web server replicas. Several primary-backup <ref> [2] </ref>, active replication [3], or lazy replication [4] techniques may be adequate. 2. Current web caching and replication methods The two most common methods used to alleviate slow web response problems today are caching and replication (sometimes referred to as mirroring). In caching, we still have one main web server.
Reference: 3. <author> Amir, Y.: </author> <title> Replication Using Group Communication over a Partitioned Network, </title> <type> Ph.D. Thesis, </type> <institution> Institute of Computer Science, The Hebrew University of Jerusalem, Israel (1995). </institution> <note> Available at http://www.cs.jhu.edu/~yairamir/ </note>
Reference-contexts: The next section describes our experience with the different methods. We then analyze and compare the methods, and draw our conclusions. Due to lack of space, we will not detail a replication protocol to keep the consistency of the web server replicas. Several primary-backup [2], active replication <ref> [3] </ref>, or lazy replication [4] techniques may be adequate. 2. Current web caching and replication methods The two most common methods used to alleviate slow web response problems today are caching and replication (sometimes referred to as mirroring). In caching, we still have one main web server.
Reference: 4. <author> Ladin, R., Liskov, B., Shrira, L., Ghemawat, S.: </author> <title> Providing Availability Using Lazy Replication ACM Transactions on Computer Systems, </title> <booktitle> 10(4), </booktitle> <pages> pages 360-391. </pages>
Reference-contexts: We then analyze and compare the methods, and draw our conclusions. Due to lack of space, we will not detail a replication protocol to keep the consistency of the web server replicas. Several primary-backup [2], active replication [3], or lazy replication <ref> [4] </ref> techniques may be adequate. 2. Current web caching and replication methods The two most common methods used to alleviate slow web response problems today are caching and replication (sometimes referred to as mirroring). In caching, we still have one main web server.
Reference: 5. <institution> Squid Internet Object Cache. </institution> <note> http://squid.nlanr.net/Squid/ </note>
Reference-contexts: Many browsers (including Netscapes and Microsofts) support this capability internally and perform client side caching on a peruser basis. This peruser caching is independent of any other caching done at the server or client side. Some caches used on a per-site basis include the Squid <ref> [5] </ref>, Harvest [6], and Apache [7] caches. In serverside caching, there are one or more caching servers that are front ends to the main server.
Reference: 6. <author> Chankhunthod, A., Danzig, P. B., Neerdaels, C., Schwartz, M. F., Worrell, K. J.: </author> <title> A Hierarchical Internet Object Cache. </title> <type> Technical Report 95-611, </type> <institution> Computer Science Department, University of Southern California, </institution> <address> Los Angeles, California, </address> <year> (1995). </year>
Reference-contexts: Many browsers (including Netscapes and Microsofts) support this capability internally and perform client side caching on a peruser basis. This peruser caching is independent of any other caching done at the server or client side. Some caches used on a per-site basis include the Squid [5], Harvest <ref> [6] </ref>, and Apache [7] caches. In serverside caching, there are one or more caching servers that are front ends to the main server.
Reference: 7. <institution> The Apache HTTP Server Project. </institution> <note> http://www.apache.org/ </note>
Reference-contexts: This peruser caching is independent of any other caching done at the server or client side. Some caches used on a per-site basis include the Squid [5], Harvest [6], and Apache <ref> [7] </ref> caches. In serverside caching, there are one or more caching servers that are front ends to the main server.
Reference: 8. <author> Berners-Lee, T., Fielding, R., Frystyk, H.: RFC-1945: </author> <title> Hypertext Transfer Protocol HTTP/1.0. (1996). </title>
Reference-contexts: No cache can guarantee complete freshness of data without the immediate expiration of cache data and the subsequent refresh from the master server. This clearly defeats the purpose of a cache, as every request will need to be served (however indirectly) by the master server. In HTTP 1.0 <ref> [8] </ref> the cache management abilities were poor. This has been rectified in HTTP 1.1 [9], and authors now have the ability to exercise considerable control over the caching (or non-caching) of their documents, but this is not universally supported yet. <p> Once the best server is determined, there are several possible ways to get the client there. The HTTP redirect method This is clearly the simplest method, but is the most limited as well. The HTTP protocol since version 1.0 <ref> [8] </ref> has supported an HTTP redirect, where the client seeking a particular resource is told to go elsewhere for it. In combination with an intelligent server, this capability can be used to send the client to the proper place.
Reference: 9. <author> Fielding, R., Gettys, J., Mogul, J., Frystyk, H., Berners-Lee, T.: RFC-2068: </author> <title> Hypertext Transfer Protocol HTTP/1.1. (1997). 10.On Interpreting Access Statistics. </title> <note> http://www.cranfield.ac.uk/docs/stats/ 11.Mockapetris, </note> <author> P.: RFC-1034: </author> <title> Domain Names Concepts and Facilities. (1987). </title> <editor> 12.Mockapetris, P.: RFC-1035: </editor> <title> Domain Names Implementation and Specification. (1987). </title> <editor> 13.Vixie, P. (ed), Thompson, S., Rekhter, Y., Bound, J.: RFC-2136: </editor> <title> Dynamic Updates in the Domain Name System (DNS UPDATE). </title> <year> (1997). </year>
Reference-contexts: This clearly defeats the purpose of a cache, as every request will need to be served (however indirectly) by the master server. In HTTP 1.0 [8] the cache management abilities were poor. This has been rectified in HTTP 1.1 <ref> [9] </ref>, and authors now have the ability to exercise considerable control over the caching (or non-caching) of their documents, but this is not universally supported yet. Web replication duplicates the web server, including its data and any ancillary abilities.
References-found: 9

