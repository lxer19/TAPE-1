URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR92204-S.ps
Refering-URL: http://www.cs.rice.edu/~ken/kennedy-vita.html
Root-URL: 
Title: Interprocedural Static Performance Estimation  
Author: Ken Kennedy Nathaniel McIntosh Kathryn S. M c Kinley 
Address: P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: Rice University Department of Computer Science  
Abstract: Static performance estimation seeks to determine at compile time how long a given program construct, such as a loop or a subroutine call, will take to execute. Perfor- mance estimation provides useful information to parallelizing compilers, particularly compilers which aggressively use code transformations to improve parallelism. In an interactive parallel programming tool, performance estimation can direct users to the most important and computation-intensive portions of their programs. This paper describes the design and implementation of a performance estimator developed to assist in the parallelization and optimization of scientific Fortran programs for shared-memory multiprocessors in the ParaScope programming environment. 
Abstract-found: 1
Intro-found: 1
Reference: [AG89] <author> D. Atapattu and D. Gannon. </author> <title> Building analytical models into an in-teractive performance prediction tool. </title> <booktitle> In Proceedings of the 1989 ACM International Conference on Supercomputing, </booktitle> <address> Crete, Greece, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Our approach is modeled on this one, but offers an efficient and practical solution for programs that contain procedure calls. Atapattu and Gannon describe a performance predictor as part of a general multiprocessor programming environment <ref> [AG89] </ref>. In their system, a user can interactively request a performance estimate for a particular loop or procedure. Their estimator works by disassembling the object code for the procedure or loop in question, and then generating an expression which represents its estimated execution time.
Reference: [AK87] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: Because such programs spend most of their time executing loops [Knu71], much research has been devoted to techniques for determining which loops in the program may be performed in parallel and which loop transformations may be carried out safely <ref> [AK87, Ban88, BC86, Wol89] </ref>. After analysis identifies all of the potentially parallel loops and legal transformations, however, a parallelizing compiler must still discover fl Corresponding author. Email: mcintosh@cs.rice.edu. Phone: 713-527-6077. <p> The compiler uses dependence information to determine if a loop's iterations can safely execute in parallel. A dependence is loop-carried if its endpoints lie in different iterations of a loop <ref> [All83, AK87] </ref>. Loop-carried dependences inhibit safe parallelization of the loop. Control Dependence Intuitively, a control dependence, S 1 ffi c S 2 , indicates that the execution of S 1 directly determines whether S 2 will be executed and is precisely what performance estimation requires.
Reference: [All83] <author> J. R. Allen. </author> <title> Dependence Analysis for Subscripted Variables and Its Application to Program Transformations. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1983. </year>
Reference-contexts: The compiler uses dependence information to determine if a loop's iterations can safely execute in parallel. A dependence is loop-carried if its endpoints lie in different iterations of a loop <ref> [All83, AK87] </ref>. Loop-carried dependences inhibit safe parallelization of the loop. Control Dependence Intuitively, a control dependence, S 1 ffi c S 2 , indicates that the execution of S 1 directly determines whether S 2 will be executed and is precisely what performance estimation requires.
Reference: [Ban88] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, MA, </address> <year> 1988. </year>
Reference-contexts: Because such programs spend most of their time executing loops [Knu71], much research has been devoted to techniques for determining which loops in the program may be performed in parallel and which loop transformations may be carried out safely <ref> [AK87, Ban88, BC86, Wol89] </ref>. After analysis identifies all of the potentially parallel loops and legal transformations, however, a parallelizing compiler must still discover fl Corresponding author. Email: mcintosh@cs.rice.edu. Phone: 713-527-6077.
Reference: [BC86] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and parallelization. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year> <month> 26 </month>
Reference-contexts: Because such programs spend most of their time executing loops [Knu71], much research has been devoted to techniques for determining which loops in the program may be performed in parallel and which loop transformations may be carried out safely <ref> [AK87, Ban88, BC86, Wol89] </ref>. After analysis identifies all of the potentially parallel loops and legal transformations, however, a parallelizing compiler must still discover fl Corresponding author. Email: mcintosh@cs.rice.edu. Phone: 713-527-6077.
Reference: [Ber66] <author> A. J. Bernstein. </author> <title> Analysis of programs for parallel processing. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 15(5) </volume> <pages> 757-763, </pages> <month> October </month> <year> 1966. </year>
Reference-contexts: Data Dependence A data dependence between statements S 1 and S 2 , written S 1 ffiS 2 , indicates that S 1 and S 2 read or write a common memory location in a way that requires their execution order to be preserved <ref> [Ber66] </ref>. The compiler uses dependence information to determine if a loop's iterations can safely execute in parallel. A dependence is loop-carried if its endpoints lie in different iterations of a loop [All83, AK87]. Loop-carried dependences inhibit safe parallelization of the loop.
Reference: [BFKK90] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <address> Charleston, SC, </address> <month> April </month> <year> 1990. </year>
Reference-contexts: 0 seismic 4 1 trfd 0 0 23 We believe interprocedural constant propagation and the interprocedural structure of our estimator are key to the quality of our implementation. 5 Related Work Balasundaram et al. introduce a static performance estimator for distributed memory machines which pioneered the use of training sets <ref> [BFKK90, BFKK91] </ref>. They use training sets to create a cost model for an architecture by summarizing empirically obtained data, as opposed to using a theoretical machine model. Our approach is modeled on this one, but offers an efficient and practical solution for programs that contain procedure calls.
Reference: [BFKK91] <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static per-formance estimator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Estimates for statements and basic blocks are then generated by summing the estimates of their components. The table of performance data is collected using a training set <ref> [BFKK91] </ref>. A training set contains a benchmark code for each operation designed to measure its average execution time. When the training set is run on a target machine, it generates a table of data that includes execution times for most computation-related language constructs. <p> 0 seismic 4 1 trfd 0 0 23 We believe interprocedural constant propagation and the interprocedural structure of our estimator are key to the quality of our implementation. 5 Related Work Balasundaram et al. introduce a static performance estimator for distributed memory machines which pioneered the use of training sets <ref> [BFKK90, BFKK91] </ref>. They use training sets to create a cost model for an architecture by summarizing empirically obtained data, as opposed to using a theoretical machine model. Our approach is modeled on this one, but offers an efficient and practical solution for programs that contain procedure calls.
Reference: [CCH + 88] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. </author> <title> ParaScope: A parallel programming environment. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-99, </pages> <month> Winter </month> <year> 1988. </year>
Reference-contexts: In Section 3 we describe the design of the performance estimator. Section 4 contains experimental results. 5 2 Background 2.1 ParaScope The ParaScope programming environment is an integrated tool set designed to assist users in developing parallel programs for shared-memory multiprocessors <ref> [CCH + 88, KMT91a] </ref>. ParaScope gathers and computes the information necessary for program parallelization. For example, it performs interprocedural constant propagation and dependence analysis [GKT91]. This analysis determines if parallelization is safe, i.e., if it preserves the meaning of the program. <p> However, by using advanced analysis such as local and interprocedural constant propagation, we believe that the estimates will be accurate enough for use in a compiler. To make performance estimation practical for use in a compiler, we use the inter- procedural framework in ParaScope <ref> [CCH + 88] </ref> which divides interprocedural problems into two phases, a local phase and an interprocedural phase [CKT86, Hal91]. 1 ParaScope runs the local phase automatically immediately following an editing session. It determines the immediate interprocedural effects of each edited procedure and stores the results in a database.
Reference: [CCKT86] <author> D. Callahan, K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural constant propagation. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, </booktitle> <address> Palo Alto, CA, </address> <month> June </month> <year> 1986. </year>
Reference-contexts: Interprocedural constant propagation determines the values of scalars on entry to each procedure and as a result of executing each procedure <ref> [CCKT86] </ref>. The local constant propagation phase determines the values of scalars at particular references.
Reference: [CFS90] <author> R. Cytron, J. Ferrante, and V. Sarkar. </author> <title> Experiences using control depen-dence in PTRAN. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The following formal definitions of control dependence and the postdominance relation on G f , the control flow graph, are taken from the literature <ref> [FOW87, CFS90] </ref>: Definition 1 A statement x is postdominated by a statement y in the control flow graph if every path from x to the exit node of G f contains y.
Reference: [CKP91] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Even in situations where it is impossible to eliminate memory latency, it is often feasible to hide it by initiating a read or load and then 2 performing some other computation while the operation completes. Examples of optimizations based on this idea are software prefetching <ref> [MLG92, CKP91] </ref> and message hoisting/vectorization [HKT92, Ger90].
Reference: [CKPK90] <author> G. Cybenko, L. Kipp, L. Pointer, and D. Kuck. </author> <title> Supercomputer per-formance evaluation and the Perfect benchmarks. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <address> Amsterdam, The Netherlands, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Coherency is maintained with a snoopy bus scheme. 3 Nodes are visited in depth-first search order. 17 4.2 Test Programs We have tested the performance estimator on the four programs in Figure 4; we plan to also use a number of larger programs, including several more Perfect benchmarks <ref> [CKPK90] </ref>. Figure 4 gives the size of each program in lines (excluding comments), number of procedures, and sequential execution time in seconds on the Sequent. Erlebacher is a 3-D tri-diagonal solver for the calculation of variable derivatives written by Thomas Eidson at ICASE, NASA-Langley.
Reference: [CKT86] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the IR n programming environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: To make performance estimation practical for use in a compiler, we use the inter- procedural framework in ParaScope [CCH + 88] which divides interprocedural problems into two phases, a local phase and an interprocedural phase <ref> [CKT86, Hal91] </ref>. 1 ParaScope runs the local phase automatically immediately following an editing session. It determines the immediate interprocedural effects of each edited procedure and stores the results in a database. This summary information includes a local performance estimate.
Reference: [FBZ92] <author> T. Fahringer, R. Blasko, and H. Zima. </author> <title> Automatic performance predic-tion to support parallelization of Fortran programs for massively parallel systems. </title> <booktitle> In Proceedings of the 1992 ACM International Conference on Supercomputing, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1992. </year> <month> 27 </month>
Reference-contexts: Fahringer, Blasko, and Zima use performance prediction as part of the Vienna Fortran Compilation System to assist in automatic support for data distribution <ref> [FBZ92] </ref>. Their approach provides good precision, but at the cost of requiring an initial profiling run to generate values for unknown symbolics. 6 Future work Much work remains to be done on the ParaScope performance estimator implementation.
Reference: [FOW87] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The following formal definitions of control dependence and the postdominance relation on G f , the control flow graph, are taken from the literature <ref> [FOW87, CFS90] </ref>: Definition 1 A statement x is postdominated by a statement y in the control flow graph if every path from x to the exit node of G f contains y.
Reference: [GAY91] <author> E. Gabber, A. Averbuch, and A. Yehudai. </author> <title> Experience with a portable parallelizing Pascal compiler. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: These approaches are similar to Atapattu and Gannon's in that they are very architecture specific. Gabber, Averbach and Yehudai generate estimates of execution times of loops in their compiler as a means of deciding when to make loops parallel <ref> [GAY91] </ref>. Their estimates are calculated in a way that is similar to the method we are proposing, however their compiler does not perform interprocedural analysis. Additionally, they 24 have a fixed policy for guessing unknowns.
Reference: [Ger90] <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency: Practice & Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Examples of optimizations based on this idea are software prefetching [MLG92, CKP91] and message hoisting/vectorization <ref> [HKT92, Ger90] </ref>. Consider the following example: do i = 1, n statement 1 statement 2 statement 3 do j = 1, 100 enddo enddo Suppose that we are compiling this loop nest for a computer with a deep memory hierarchy and hardware support for compiler-directed prefetching.
Reference: [GH91] <author> A. Goldberg and J. Hennessy. </author> <title> Mtool: A method for isolating memory bottlenecks in shared memory multiprocessor programs. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: They do address mitigating the effects of memory contention once it is detected. They find that even a single hot spot can severely degrade performance. Goldberg and Hennessy also address these issues <ref> [GH91] </ref>. 11 in turn is control dependent on the do i loop. We first compute the cost expression for S 1 , then use it to compute the cost expression for the do j loop, which becomes a component used in the estimate of the do i loop.
Reference: [GKT91] <author> G. Goff, K. Kennedy, and C. Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Program Language Design and Implementation, </booktitle> <address> Toronto, Canada, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: ParaScope gathers and computes the information necessary for program parallelization. For example, it performs interprocedural constant propagation and dependence analysis <ref> [GKT91] </ref>. This analysis determines if parallelization is safe, i.e., if it preserves the meaning of the program. The program compiler uses the analysis to determine program parallelization and optimization. Performance estimation is used in concert with this analysis in the compiler to determine if parallelization is profitable [McK92].
Reference: [Hal91] <author> M. W. Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: To make performance estimation practical for use in a compiler, we use the inter- procedural framework in ParaScope [CCH + 88] which divides interprocedural problems into two phases, a local phase and an interprocedural phase <ref> [CKT86, Hal91] </ref>. 1 ParaScope runs the local phase automatically immediately following an editing session. It determines the immediate interprocedural effects of each edited procedure and stores the results in a database. This summary information includes a local performance estimate.
Reference: [Har77] <author> W. H. Harrison. </author> <title> Compiler analysis of the value ranges for variables. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-3(3):243-250, </volume> <month> May </month> <year> 1977. </year>
Reference-contexts: This approach could be considered an interprocedural version of range propagation and analysis <ref> [Har77] </ref>. Range information could then be used to derive better guesses for loop upper bounds. 4 Experimental Results We have completed an implementation of the performance estimator within Para- Scope. This implementation consists of the local phase, the interprocedural phase, and a training set for the Sequent Symmetry.
Reference: [HHK + 93] <author> M. W. Hall, T. Harvey, K. Kennedy, N. McIntosh, K. S. M c Kinley, J. D. Oldham, M. Paleczny, and G. Roth. </author> <title> Experiences using the ParaScope Editor: an interactive parallel programming tool. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: By ranking the subroutines and loops which account for the largest fraction of the computation performed in a program, performance estimation provides a mechanism for focusing either the compiler or a user on the most computational intensive portions of a program <ref> [HHK + 93] </ref>. Performance estimation for a complete application requires interprocedural analysis. A variant of static performance estimation can also be used to determine execution time lower and upper bounds for real-time systems [Sha89, Par92]. <p> The program compiler uses the analysis to determine program parallelization and optimization. Performance estimation is used in concert with this analysis in the compiler to determine if parallelization is profitable [McK92]. The ParaScope Editor, Ped, is an interactive parallel programming tool that assists users in parallelizing programs <ref> [KMT91a, KMT91b, HHK + 93] </ref>. It provides the analysis and transformation capabilities of a parallelizing compiler in a powerful editor. In a recent study, researchers using Ped often desired more assistance in navigating their way through their programs [HHK + 93]. <p> It provides the analysis and transformation capabilities of a parallelizing compiler in a powerful editor. In a recent study, researchers using Ped often desired more assistance in navigating their way through their programs <ref> [HHK + 93] </ref>. In particular, they wanted Ped to guide them to the most time consuming portions of their applications in a methodical fashion. This guidance would enable them to concentrate their efforts on the program parts most likely to yield a high payoff. <p> In recent evaluations of the ParaScope Editor, users requested performance estimation or profiling be integrated into the tool in order to guide them to the computation-intensive portions of their program <ref> [HHK + 93] </ref>. As we demonstrated in the introduction, if user edits or transformations are performed, profiling information is insufficient. The additional flexibility offered by performance estimation and its relative accuracy serve this purpose well.
Reference: [HKM91] <author> M. W. Hall, K. Kennedy, and K. S. M c Kinley. </author> <title> Interprocedural transfor-mations for parallel code generation. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <address> Albuquerque, NM, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: values are constants, dependence analysis and performance estimation often produce much better results. 2.2.3 Augmented Call Graph The program representation for our work on whole program optimization and par- allelization requires an augmented call graph, G ac , to describe the calling relationships among procedures and to specify loop nests <ref> [HKM91] </ref>. For this purpose, the program's call graph, which contains the usual procedure nodes and call edges, is augmented to include special loop nodes and nesting edges. The loop nodes contain loop header information.
Reference: [HKT92] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Examples of optimizations based on this idea are software prefetching [MLG92, CKP91] and message hoisting/vectorization <ref> [HKT92, Ger90] </ref>. Consider the following example: do i = 1, n statement 1 statement 2 statement 3 do j = 1, 100 enddo enddo Suppose that we are compiling this loop nest for a computer with a deep memory hierarchy and hardware support for compiler-directed prefetching.
Reference: [KMT91a] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Analysis and transformation in the ParaScope Editor. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year> <month> 28 </month>
Reference-contexts: In Section 3 we describe the design of the performance estimator. Section 4 contains experimental results. 5 2 Background 2.1 ParaScope The ParaScope programming environment is an integrated tool set designed to assist users in developing parallel programs for shared-memory multiprocessors <ref> [CCH + 88, KMT91a] </ref>. ParaScope gathers and computes the information necessary for program parallelization. For example, it performs interprocedural constant propagation and dependence analysis [GKT91]. This analysis determines if parallelization is safe, i.e., if it preserves the meaning of the program. <p> The program compiler uses the analysis to determine program parallelization and optimization. Performance estimation is used in concert with this analysis in the compiler to determine if parallelization is profitable [McK92]. The ParaScope Editor, Ped, is an interactive parallel programming tool that assists users in parallelizing programs <ref> [KMT91a, KMT91b, HHK + 93] </ref>. It provides the analysis and transformation capabilities of a parallelizing compiler in a powerful editor. In a recent study, researchers using Ped often desired more assistance in navigating their way through their programs [HHK + 93].
Reference: [KMT91b] <author> K. Kennedy, K. S. M c Kinley, and C. Tseng. </author> <title> Interactive parallel pro--gramming using the ParaScope Editor. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 329-341, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The program compiler uses the analysis to determine program parallelization and optimization. Performance estimation is used in concert with this analysis in the compiler to determine if parallelization is profitable [McK92]. The ParaScope Editor, Ped, is an interactive parallel programming tool that assists users in parallelizing programs <ref> [KMT91a, KMT91b, HHK + 93] </ref>. It provides the analysis and transformation capabilities of a parallelizing compiler in a powerful editor. In a recent study, researchers using Ped often desired more assistance in navigating their way through their programs [HHK + 93].
Reference: [Knu71] <author> D. Knuth. </author> <title> An empirical study of FORTRAN programs. </title> <journal> Software| Practice and Experience, </journal> <volume> 1 </volume> <pages> 105-133, </pages> <year> 1971. </year>
Reference-contexts: 1 Introduction When compiling scientific Fortran programs for high-performance computer architectures, a compiler must effectively exploit the parallelism in the program and make effective use of the target machine's memory hierarchy. Because such programs spend most of their time executing loops <ref> [Knu71] </ref>, much research has been devoted to techniques for determining which loops in the program may be performed in parallel and which loop transformations may be carried out safely [AK87, Ban88, BC86, Wol89].
Reference: [McK92] <author> K. S. McKinley. </author> <title> Automatic and Interactive Parallelization. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: This analysis determines if parallelization is safe, i.e., if it preserves the meaning of the program. The program compiler uses the analysis to determine program parallelization and optimization. Performance estimation is used in concert with this analysis in the compiler to determine if parallelization is profitable <ref> [McK92] </ref>. The ParaScope Editor, Ped, is an interactive parallel programming tool that assists users in parallelizing programs [KMT91a, KMT91b, HHK + 93]. It provides the analysis and transformation capabilities of a parallelizing compiler in a powerful editor.
Reference: [MLG92] <author> Todd C. Mowry, Monica S. Lam, and Anoop Gupta. </author> <title> Design and eval-uation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <address> Boston, MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Even in situations where it is impossible to eliminate memory latency, it is often feasible to hide it by initiating a read or load and then 2 performing some other computation while the operation completes. Examples of optimizations based on this idea are software prefetching <ref> [MLG92, CKP91] </ref> and message hoisting/vectorization [HKT92, Ger90].
Reference: [Par92] <author> Change Yun Park. </author> <title> Predicting deterministic execution times of real-time programs. </title> <type> PhD thesis, </type> <institution> University of Washington, Dept. of Computer Science and Engineering, </institution> <year> 1992. </year>
Reference-contexts: Performance estimation for a complete application requires interprocedural analysis. A variant of static performance estimation can also be used to determine execution time lower and upper bounds for real-time systems <ref> [Sha89, Par92] </ref>.
Reference: [PN85] <author> G. Pfister and V. A. Norton. </author> <title> Hot spot contention and combining in multistage interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(10):943-948, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: Consider Figure 1. Statement S 1 is control dependent on the do j loop, which 2 Pfister and Norton discuss detecting memory effects in detail, but do not address the issue of predicting when memory contention or "hot spots" will occur <ref> [PN85] </ref>. They do address mitigating the effects of memory contention once it is detected. They find that even a single hot spot can severely degrade performance. Goldberg and Hennessy also address these issues [GH91]. 11 in turn is control dependent on the do i loop.
Reference: [Pol86] <author> C. Polychronopoulos. </author> <title> On Program Restructuring, Scheduling, and Communication for Parallel Processor Systems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: By looking at the actual assembly language output of the compiler, they can generate fairly accurate estimates, but at the price of making their estimator very architecture specific. Both Polychronopoulos and Sarkar have also used machine models in their research which estimate the amount of computation in a loop <ref> [Pol86, Sar89] </ref>. These approaches are similar to Atapattu and Gannon's in that they are very architecture specific. Gabber, Averbach and Yehudai generate estimates of execution times of loops in their compiler as a means of deciding when to make loops parallel [GAY91].
Reference: [Sar89] <author> V. Sarkar. </author> <title> Partition and Scheduling Parallel Programs for Multiprocessors. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: By looking at the actual assembly language output of the compiler, they can generate fairly accurate estimates, but at the price of making their estimator very architecture specific. Both Polychronopoulos and Sarkar have also used machine models in their research which estimate the amount of computation in a loop <ref> [Pol86, Sar89] </ref>. These approaches are similar to Atapattu and Gannon's in that they are very architecture specific. Gabber, Averbach and Yehudai generate estimates of execution times of loops in their compiler as a means of deciding when to make loops parallel [GAY91].
Reference: [Sha89] <author> Alan C. Shaw. </author> <title> Reasoning about time in higher-level language soft-ware. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-15(7):875-889, </volume> <month> July </month> <year> 1989. </year>
Reference-contexts: Performance estimation for a complete application requires interprocedural analysis. A variant of static performance estimation can also be used to determine execution time lower and upper bounds for real-time systems <ref> [Sha89, Par92] </ref>.
Reference: [Wol89] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Because such programs spend most of their time executing loops [Knu71], much research has been devoted to techniques for determining which loops in the program may be performed in parallel and which loop transformations may be carried out safely <ref> [AK87, Ban88, BC86, Wol89] </ref>. After analysis identifies all of the potentially parallel loops and legal transformations, however, a parallelizing compiler must still discover fl Corresponding author. Email: mcintosh@cs.rice.edu. Phone: 713-527-6077.
Reference: [Wri91] <author> S. J. Wright. </author> <title> Partitioned dynamic programming for optimal control. </title> <journal> SIAM Journal of Optimization, </journal> <volume> 1(4) </volume> <pages> 620-642, </pages> <month> November </month> <year> 1991. </year> <month> 29 </month>
Reference-contexts: Erlebacher is a 3-D tri-diagonal solver for the calculation of variable derivatives written by Thomas Eidson at ICASE, NASA-Langley. Control computes solutions for linear-quadratic optimal control problems <ref> [Wri91] </ref>. Seis- mic checks the adjointness of two routines and was written by Michael Lewis at Rice University.
References-found: 37

