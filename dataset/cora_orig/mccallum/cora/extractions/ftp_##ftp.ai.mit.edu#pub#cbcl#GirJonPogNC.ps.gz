URL: ftp://ftp.ai.mit.edu/pub/cbcl/GirJonPogNC.ps.gz
Refering-URL: http://www.ai.mit.edu/people/girosi/home-page/work.html
Root-URL: 
Title: Regularization Theory and Neural Networks Architectures  
Author: Federico Girosi, Michael Jones and Tomaso Poggio 
Address: Cambridge, MA 02139 USA  
Affiliation: Center for Biological and Computational Learning and Artificial Intelligence Laboratory Massachusetts Institute of Technology,  
Abstract: We had previously shown that regularization principles lead to approximation schemes which are equivalent to networks with one layer of hidden units, called Regularization Networks. In particular, standard smoothness functionals lead to a subclass of regularization networks, the well known Radial Basis Functions approximation schemes. This paper shows that regularization networks encompass a much broader range of approximation schemes, including many of the popular general additive models and some of the neural networks. In particular, we introduce new classes of smoothness functionals that lead to different classes of basis functions. Additive splines as well as some tensor product splines can be obtained from appropriate classes of smoothness functionals. Furthermore, the same generalization that extends Radial Basis Functions (RBF) to Hyper Basis Functions (HBF) also leads from additive models to ridge approximation models, containing as special cases Breiman's hinge functions, some forms of Projection Pursuit Regression and several types of neural networks. We propose to use the term Generalized Regularization Networks for this broad class of approximation schemes that follow from an extension of regularization. In the probabilistic interpretation of regularization, the different classes of basis functions correspond to different classes of prior probabilities on the approximating function spaces, and therefore to different types of smoothness assumptions. In summary, different multilayer networks with one hidden layer, which we collectively call Generalized Regularization Networks, correspond to different classes of priors and associated smoothness functionals in a classical regularization principle. Three broad classes are a) Radial Basis Functions that can be generalized to Hyper Basis Functions, b) some tensor product splines, and c) additive splines that can be generalized to schemes of the type of ridge approximation, hinge functions and several perceptron-like neural networks with one-hidden layer. 1 This paper will appear on Neural Computation, vol. 7, pages 219-269, 1995. An earlier version of 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F.A. Aidu and V.N. Vapnik. </author> <title> Estimation of probability density on the basis of the method of stochastic regularization. </title> <journal> Avtomatika i Telemekhanika, </journal> (4):84-97, April 1989. 
Reference-contexts: Let us consider some simple experiments that show the shape of the equivalent kernels in specific situations. We first considered a data set composed of 36 equally spaced points on the domain <ref> [0; 1] </ref> fi [0; 1], at the nodes of a regular grid with spacing equal to 0.2. <p> Let us consider some simple experiments that show the shape of the equivalent kernels in specific situations. We first considered a data set composed of 36 equally spaced points on the domain <ref> [0; 1] </ref> fi [0; 1], at the nodes of a regular grid with spacing equal to 0.2. <p> Consider now a one dimensional example with a multiquadric basis function: G (x) = 2 + x 2 : The data set was chosen to be a non uniform sampling of the interval <ref> [0; 1] </ref>, that is the set In figure 1c, 1d and 1e we have drawn, respectively, the equivalent kernels b 3 , b 5 and b 6 , under the same definitions. Notice that all of them are bell-shaped, although the original basis function is an increasing, cup-shaped function. <p> To do this we used the model f (x) = P n ff c ff G (w ff x t ff ) to approximate the function h (x) = sin (2x) on <ref> [0; 1] </ref>, where G (x) is one of the basis functions of figure 2. Fifty training points and 10,000 test points were chosen uniformly on [0; 1]. <p> f (x) = P n ff c ff G (w ff x t ff ) to approximate the function h (x) = sin (2x) on <ref> [0; 1] </ref>, where G (x) is one of the basis functions of figure 2. Fifty training points and 10,000 test points were chosen uniformly on [0; 1]. The parameters were learned using the iterative backfitting algorithm (Friedman and Stuezle, 1981; Hastie and Tibshirani, 1990; 24 Breiman, 1993) that will be described in section 7. We looked at the function learned after fitting 1, 2, 4, 8 and 16 basis functions. <p> add (x; y) = sin (2x) + 4 (y 0:5) 2 and the two-dimensional Gabor function: g Gabor (x; y) = e kxk 2 cos (:75 (x + y)): The training data for the functions h add and g Gabor consisted of 20 points picked from a uniform distribution on <ref> [0; 1] </ref> fi [0; 1] and [1; 1] fi [1; 1] respectively. Another 10,000 points were randomly chosen to serve as test data. The results are summarized in table 2 (see Girosi, Jones and Poggio, 1993 for a more extensive description of the results). <p> = sin (2x) + 4 (y 0:5) 2 and the two-dimensional Gabor function: g Gabor (x; y) = e kxk 2 cos (:75 (x + y)): The training data for the functions h add and g Gabor consisted of 20 points picked from a uniform distribution on <ref> [0; 1] </ref> fi [0; 1] and [1; 1] fi [1; 1] respectively. Another 10,000 points were randomly chosen to serve as test data. The results are summarized in table 2 (see Girosi, Jones and Poggio, 1993 for a more extensive description of the results). <p> + 4 (y 0:5) 2 and the two-dimensional Gabor function: g Gabor (x; y) = e kxk 2 cos (:75 (x + y)): The training data for the functions h add and g Gabor consisted of 20 points picked from a uniform distribution on [0; 1] fi [0; 1] and <ref> [1; 1] </ref> fi [1; 1] respectively. Another 10,000 points were randomly chosen to serve as test data. The results are summarized in table 2 (see Girosi, Jones and Poggio, 1993 for a more extensive description of the results). <p> 0:5) 2 and the two-dimensional Gabor function: g Gabor (x; y) = e kxk 2 cos (:75 (x + y)): The training data for the functions h add and g Gabor consisted of 20 points picked from a uniform distribution on [0; 1] fi [0; 1] and <ref> [1; 1] </ref> fi [1; 1] respectively. Another 10,000 points were randomly chosen to serve as test data. The results are summarized in table 2 (see Girosi, Jones and Poggio, 1993 for a more extensive description of the results).
Reference: [2] <author> A. Albert. </author> <title> Regression and the Moore-Penrose Pseudoinverse. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference: [3] <author> D. Allen. </author> <title> The relationship between variable selection and data augmentation and a method for prediction. </title> <journal> Technometrics, </journal> <volume> 16 </volume> <pages> 125-127, </pages> <year> 1974. </year>
Reference: [4] <author> N. Aronszajn. </author> <title> Theory of reproducing kernels. </title> <journal> Trans. Amer. Math. Soc., </journal> <volume> 686 </volume> <pages> 337-404, </pages> <year> 1950. </year>
Reference: [5] <author> D. H. Ballard. </author> <title> Cortical connections and parallel processing: structure and function. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 9 </volume> <pages> 67-120, </pages> <year> 1986. </year>
Reference: [6] <author> A. R. Barron and Barron R. L. </author> <title> Statistical learning networks: a unifying view. </title> <booktitle> In Symposium on the Interface: Statistics and Computing Science, </booktitle> <address> Reston, Virginia, </address> <month> April </month> <year> 1988. </year>
Reference: [7] <author> A.R. Barron. </author> <title> Approximation and estimation bounds for artificial neural networks. </title> <type> Technical Report 59, </type> <institution> Department of Statistics, University of Illinois at Urbana-Champaign, Champaign, IL, </institution> <month> March </month> <year> 1991. </year>
Reference: [8] <author> A.R. Barron. </author> <title> Universal approximation bounds for superpositions of a sigmoidal function. </title> <journal> IEEE Transaction on Information Theory, </journal> <volume> 39(3) </volume> <pages> 930-945, </pages> <month> May </month> <year> 1993. </year>
Reference: [9] <author> A.R. Barron. </author> <title> Approximation and estimation bounds for artificial neural networks. </title> <journal> Machine Learning, </journal> <volume> 14 </volume> <pages> 115-133, </pages> <year> 1994. </year>
Reference: [10] <author> E. B. Baum. </author> <title> On the capabilities of multilayer perceptrons. </title> <journal> J. Complexity, </journal> <volume> 4 </volume> <pages> 193-215, </pages> <year> 1988. </year>
Reference: [11] <author> E.B. Baum and D. Haussler. </author> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1 </booktitle> <pages> 151-160, </pages> <year> 1989. </year>
Reference: [12] <author> R.E. Bellman. </author> <title> Adaptive Control Processes. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1961. </year>
Reference: [13] <author> M. Bertero. </author> <title> Regularization methods for linear inverse problems. </title> <editor> In C. G. Talenti, editor, </editor> <title> Inverse Problems. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1986. </year> <month> 47 </month>
Reference: [14] <author> M. Bertero, T. Poggio, and V. Torre. </author> <title> Ill-posed problems in early vision. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 76 </volume> <pages> 869-889, </pages> <year> 1988. </year>
Reference: [15] <author> L. Bottou and V. Vapnik. </author> <title> Local learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 4(6) </volume> <pages> 888-900, </pages> <month> November </month> <year> 1992. </year>
Reference: [16] <author> L. Breiman. </author> <title> Hinging hyperplanes for regression, classification, and function approximation. </title> <journal> IEEE Transaction on Information Theory, </journal> <volume> 39(3) </volume> <pages> 999-1013, </pages> <month> May </month> <year> 1993. </year>
Reference: [17] <author> D.S. Broomhead and D. Lowe. </author> <title> Multivariable functional interpolation and adaptive networks. </title> <journal> Complex Systems, </journal> <volume> 2 </volume> <pages> 321-355, </pages> <year> 1988. </year>
Reference: [18] <author> M.D. Buhmann. </author> <title> Multivariate cardinal interpolation with radial basis functions. Constructive Approximation, </title> <booktitle> 6 </booktitle> <pages> 225-255, </pages> <year> 1990. </year>
Reference: [19] <author> M.D. Buhmann. </author> <title> On quasi-interpolation with Radial Basis Functions. Numerical Analysis Reports DAMPT 1991/NA3, </title> <institution> Department of Applied Mathematics and Theoretical Physics, </institution> <address> Cambridge, England, </address> <month> March </month> <year> 1991. </year>
Reference: [20] <author> A. Buja, T. Hastie, and R. Tibshirani. </author> <title> Linear smoothers and additive models. </title> <journal> The Annals of Statistics, </journal> <volume> 17 </volume> <pages> 453-555, </pages> <year> 1989. </year>
Reference: [21] <author> B. Caprile and F. Girosi. </author> <title> A nondeterministic minimization algorithm. A.I. </title> <type> Memo 1254, </type> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <month> September </month> <year> 1990. </year>
Reference: [22] <author> D.D. Cox. </author> <title> Multivariate smoothing spline functions. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 21 </volume> <pages> 789-813, </pages> <year> 1984. </year>
Reference: [23] <author> P. Craven and G. Wahba. </author> <title> Smoothing noisy data with spline functions: estimating the correct degree of smoothing by the method of generalized cross validation. </title> <journal> Numer. Math, </journal> <volume> 31 </volume> <pages> 377-403, </pages> <year> 1979. </year>
Reference: [24] <author> G. Cybenko. </author> <title> Approximation by superposition of a sigmoidal function. </title> <journal> Math. Control Systems Signals, </journal> <volume> 2(4) </volume> <pages> 303-314, </pages> <year> 1989. </year>
Reference: [25] <author> C. de Boor. </author> <title> A Practical Guide to Splines. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1978. </year>
Reference: [26] <author> C. de Boor. </author> <title> Quasi-interpolants and approximation power of multivariate splines. </title> <editor> In M. Gasca and C.A. Micchelli, editors, </editor> <booktitle> Computation of Curves and Surfaces, </booktitle> <pages> pages 313-345. </pages> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, Netherlands, </address> <year> 1990. </year>
Reference: [27] <author> R. DeVore, R. Howard, and C. Micchelli. </author> <title> Optimal nonlinear approximation. </title> <journal> Manuskripta Mathematika, </journal> <year> 1989. </year> <month> 48 </month>
Reference: [28] <author> R.A. DeVore. </author> <title> Degree of nonlinear approximation. </title> <editor> In C.K. Chui, L.L. Schumaker, and D.J. Ward, editors, </editor> <booktitle> Approximation Theory, VI, </booktitle> <pages> pages 175-201. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1991. </year>
Reference: [29] <author> R.A. DeVore and X.M. Yu. </author> <title> Nonlinear n-widths in Besov spaces. </title> <editor> In C.K. Chui, L.L. Schumaker, and D.J. Ward, editors, </editor> <booktitle> Approximation Theory, VI, </booktitle> <pages> pages 203-206. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1991. </year>
Reference: [30] <author> L.P. Devroye and T.J. Wagner. </author> <title> Distribution-free consistency results in nonpara-metric discrimination and regression function estimation. </title> <journal> The Annals of Statistics, </journal> <volume> 8 </volume> <pages> 231-239, </pages> <year> 1980. </year>
Reference: [31] <author> P. Diaconis and D. Freedman. </author> <title> Asymptotics of graphical projection pursuit. </title> <journal> The Annals of Statistics, </journal> <volume> 12(3) </volume> <pages> 793-815, </pages> <year> 1984. </year>
Reference: [32] <author> D.L. Donoho and I.M. Johnstone. </author> <title> Projection-based approximation and a duality with kernel methods. </title> <journal> The Annals of Statistics, </journal> <volume> 17(1) </volume> <pages> 58-106, </pages> <year> 1989. </year>
Reference: [33] <author> J. Duchon. </author> <title> Spline minimizing rotation-invariant semi-norms in Sobolev spaces. </title> <editor> In W. Schempp and K. Zeller, editors, </editor> <title> Constructive theory of functions os several variables, </title> <booktitle> Lecture Notes in Mathematics, </booktitle> <volume> 571. </volume> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1977. </year>
Reference: [34] <author> N. Dyn. </author> <title> Interpolation of scattered data by radial functions. </title> <editor> In C.K. Chui, L.L. Schumaker, and F.I. Utreras, editors, </editor> <title> Topics in multivariate approximation. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference: [35] <author> N. Dyn. </author> <title> Interpolation and approximation by radial and related functions. </title> <editor> In C.K. Chui, L.L. Schumaker, and D.J. Ward, editors, </editor> <booktitle> Approximation Theory, VI, </booktitle> <pages> pages 211-234. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1991. </year>
Reference: [36] <author> N. Dyn, I.R.H. Jackson, D. Levin, and A. Ron. </author> <title> On multivariate approximation by integer translates of a basis function. </title> <type> Computer Sciences Technical Report 886, </type> <institution> University of Wisconsin-Madison, </institution> <month> November </month> <year> 1989. </year>
Reference: [37] <author> N. Dyn, D. Levin, and S. Rippa. </author> <title> Numerical procedures for surface fitting of scattered data by radial functions. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7(2) </volume> <pages> 639-659, </pages> <month> April </month> <year> 1986. </year>
Reference: [38] <author> R.L. Eubank. </author> <title> Spline Smoothing and Nonparametric Regression, </title> <booktitle> volume 90 of Statistics, textbooks and monographs. </booktitle> <publisher> Marcel Dekker, </publisher> <address> Basel, </address> <year> 1988. </year>
Reference: [39] <author> R. Franke. </author> <title> Scattered data interpolation: tests of some method. </title> <journal> Math. Comp., </journal> <volume> 38(5) </volume> <pages> 181-200, </pages> <year> 1982. </year>
Reference: [40] <author> R. Franke. </author> <title> Recent advances in the approximation of surfaces from scattered data. </title> <editor> In C.K. Chui, L.L. Schumaker, and F.I. Utreras, editors, </editor> <title> Topics in multivariate approximation. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1987. </year>
Reference: [41] <author> J.H. Friedman and W. Stuetzle. </author> <title> Projection pursuit regression. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 76(376) </volume> <pages> 817-823, </pages> <year> 1981. </year>
Reference: [42] <author> K. Funahashi. </author> <title> On the approximate realization of continuous mappings by neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 183-192, </pages> <year> 1989. </year>
Reference: [43] <author> Th. Gasser and H.G. Muller. </author> <title> Estimating regression functions and their derivatives by the kernel method. </title> <journal> Scand. Journ. Statist., </journal> <volume> 11 </volume> <pages> 171-185, </pages> <year> 1985. </year>
Reference: [44] <author> I.M. Gelfand and G.E. Shilov. </author> <title> Generalized functions. Vol. 1: Properties and Operations. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1964. </year>
Reference: [45] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-58, </pages> <year> 1992. </year>
Reference: [46] <author> F. Girosi. </author> <title> Models of noise and robust estimates. A.I. </title> <type> Memo 1287, </type> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1991. </year>
Reference: [47] <author> F. Girosi. </author> <title> On some extensions of radial basis functions and their applications in artificial intelligence. </title> <journal> Computers Math. Applic., </journal> <volume> 24(12) </volume> <pages> 61-80, </pages> <year> 1992. </year>
Reference: [48] <author> F. Girosi. </author> <title> Regularization theory, radial basis functions and networks. </title> <editor> In V. Cherkassky, J.H. Friedman, and H. Wechsler, editors, </editor> <title> From Statistics to Neural Networks. Theory and Pattern Recognition Applications. </title> <publisher> Springer-Verlag, </publisher> <address> Subseries F, </address> <institution> Computer and Systems Sciences, </institution> <year> 1993. </year>
Reference: [49] <author> F. Girosi and G. Anzellotti. </author> <title> Rates of convergence of approximation by translates. A.I. </title> <type> Memo 1288, </type> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1992. </year>
Reference: [50] <author> F. Girosi and G. Anzellotti. </author> <title> Rates of convergence for radial basis functions and neural networks. </title> <editor> In R.J. Mammone, editor, </editor> <booktitle> Artificial Neural Networks for Speech and Vision, </booktitle> <pages> pages 97-113, </pages> <address> London, 1993. </address> <publisher> Chapman & Hall. </publisher>
Reference: [51] <author> F. Girosi, M. Jones, and T. Poggio. </author> <title> Priors, stabilizers and basis functions: From regularization to radial, tensor and additive splines. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1430, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1993. </year>
Reference: [52] <author> F. Girosi and T. Poggio. </author> <title> Networks and the best approximation property. </title> <journal> Biological Cybernetics, </journal> <volume> 63 </volume> <pages> 169-176, </pages> <year> 1990. </year> <month> 50 </month>
Reference: [53] <author> F. Girosi, T. Poggio, and B. Caprile. </author> <title> Extensions of a theory of networks for approximation and learning: outliers and negative examples. </title> <editor> In R. Lippmann, J. Moody, and D. Touretzky, editors, </editor> <booktitle> Advances in Neural information processings systems 3, </booktitle> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: [54] <author> G. Golub, M. Heath, and G. Wahba. </author> <title> Generalized cross validation as a method for choosing a good ridge parameter. </title> <journal> Technometrics, </journal> <volume> 21 </volume> <pages> 215-224, </pages> <year> 1979. </year>
Reference: [55] <author> W. E. L. </author> <title> Grimson. A computational theory of visual surface interpolation. </title> <journal> Proceedings of the Royal Society of London B, </journal> <volume> 298 </volume> <pages> 395-427, </pages> <year> 1982. </year>
Reference: [56] <author> R.L. </author> <title> Harder and R.M. Desmarais. Interpolation using surface splines. </title> <journal> J. Aircraft, </journal> <volume> 9 </volume> <pages> 189-191, </pages> <year> 1972. </year>
Reference: [57] <author> W. Hardle. </author> <title> Applied nonparametric regression, </title> <booktitle> volume 19 of Econometric Society Monographs. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1990. </year>
Reference: [58] <author> R.L. Hardy. </author> <title> Multiquadric equations of topography and other irregular surfaces. </title> <journal> J. Geophys. Res, </journal> <volume> 76 </volume> <pages> 1905-1915, </pages> <year> 1971. </year>
Reference: [59] <author> R.L. Hardy. </author> <title> Theory and applications of the multiquadric-biharmonic method. </title> <journal> Computers Math. Applic., </journal> 19(8/9):163-208, 1990. 
Reference: [60] <author> T. Hastie and R. Tibshirani. </author> <title> Generalized additive models. </title> <journal> Statistical Science, </journal> <volume> 1 </volume> <pages> 297-318, </pages> <year> 1986. </year>
Reference: [61] <author> T. Hastie and R. Tibshirani. </author> <title> Generalized additive models: some applications. </title> <journal> J. Amer. Statistical Assoc., </journal> <volume> 82 </volume> <pages> 371-386, </pages> <year> 1987. </year>
Reference: [62] <author> T. Hastie and R. Tibshirani. </author> <title> Generalized Additive Models, </title> <booktitle> volume 43 of Monographs on Statistics and Applied Probability. </booktitle> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1990. </year>
Reference: [63] <author> D. Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <type> Technical Report UCSC-CRL-91-02, </type> <institution> University of California, Santa Cruz, </institution> <year> 1989. </year>
Reference: [64] <author> J.A. Hertz, A. Krogh, and R. Palmer. </author> <title> Introduction to the theory of neural computation. </title> <publisher> Addison-Wesley, </publisher> <address> Redwood City, CA, </address> <year> 1991. </year>
Reference: [65] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Multilayer feedforward networks are universal approximators. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 359-366, </pages> <year> 1989. </year>
Reference: [66] <author> P.J. Huber. </author> <title> Projection pursuit. </title> <journal> The Annals of Statistics, </journal> <volume> 13(2) </volume> <pages> 435-475, </pages> <year> 1985. </year> <month> 51 </month>
Reference: [67] <author> A. Hurlbert and T. Poggio. </author> <title> Synthetizing a color algorithm from examples. </title> <journal> Science, </journal> <volume> 239 </volume> <pages> 482-485, </pages> <year> 1988. </year>
Reference: [68] <author> B. Irie and S. Miyake. </author> <title> Capabilities of three-layered Perceptrons. </title> <booktitle> IEEE International Conference on Neural Networks, </booktitle> <volume> 1 </volume> <pages> 641-648, </pages> <year> 1988. </year>
Reference: [69] <author> I.R.H. Jackson. </author> <title> Radial Basis Functions methods for multivariate approximation. </title> <type> Ph.d. thesis, </type> <institution> University of Cambridge, U.K., </institution> <year> 1988. </year>
Reference: [70] <author> L.K. Jones. </author> <title> A simple lemma on greedy approximation in Hilbert space and convergence rates for Projection Pursuit Regression and neural network training. </title> <journal> The Annals of Statistics, </journal> <volume> 20(1) </volume> <pages> 608-613, </pages> <month> March </month> <year> 1992. </year>
Reference: [71] <author> E.J. Kansa. </author> <title> Multiquadrics a scattered data approximation scheme with applications to computational fluid dynamics I. </title> <journal> Computers Math. Applic., </journal> 19(8/9):127-145, 1990a. 
Reference: [72] <author> E.J. Kansa. </author> <title> Multiquadrics a scattered data approximation scheme with applications to computational fluid dynamics - ii. </title> <journal> Computers Math. Applic., </journal> 19(8/9):147-161, 1990b. 
Reference: [73] <author> G.S. Kimeldorf and G. Wahba. </author> <title> A correspondence between Bayesan estimation on stochastic processes and smoothing by splines. </title> <journal> Ann. Math. Statist., </journal> <volume> 2 </volume> <pages> 495-502, </pages> <year> 1971. </year>
Reference: [74] <author> T. Kohonen. </author> <title> The self-organizing map. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(9) </volume> <pages> 1464-1480, </pages> <year> 1990. </year>
Reference: [75] <author> S.Y. Kung. </author> <title> Digital Neural Networks. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1993. </year>
Reference: [76] <author> P. Lancaster and K. Salkauskas. </author> <title> Curve and Surface Fitting. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1986. </year>
Reference: [77] <author> A. Lapedes and R. Farber. </author> <title> How neural nets work. </title> <editor> In Dana Z. Anderson, editor, </editor> <booktitle> Neural Information Processing Systems, </booktitle> <pages> pages 442-456. </pages> <address> Am. </address> <publisher> Inst. Physics, </publisher> <address> NY, </address> <year> 1988. </year> <booktitle> Proceedings of the Denver, 1987 Conference. </booktitle>
Reference: [78] <author> R. P. Lippmann. </author> <title> Review of neural networks for speech recognition. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 1-38, </pages> <year> 1989. </year>
Reference: [79] <author> R. P. Lippmann and Y. Lee. </author> <title> A critical overview of neural network pattern classifiers. </title> <booktitle> Presented at Neural Networks for Computing Conference, </booktitle> <address> Snowbird, UT, </address> <year> 1991. </year>
Reference: [80] <author> G. G. Lorentz. </author> <title> Metric entropy, widths, and superposition of functions. </title> <journal> Amer. Math. Monthly, </journal> <volume> 69 </volume> <pages> 469-485, </pages> <year> 1962. </year>
Reference: [81] <author> G. G. Lorentz. </author> <title> Approximation of Functions. </title> <publisher> Chelsea Publishing Co., </publisher> <address> New York, </address> <year> 1986. </year>
Reference: [82] <author> W.R. Madych and S.A. Nelson. </author> <title> Multivariate interpolation and conditionally positive definite functions. II. </title> <journal> Mathematics of Computation, </journal> <volume> 54(189) </volume> <pages> 211-230, </pages> <month> January </month> <year> 1990. </year>
Reference: [83] <author> W.R. Madych and S.A. Nelson. </author> <title> Polyharmonic cardinal splines: a minimization property. </title> <journal> Journal of Approximation Theory, </journal> <volume> 63 </volume> <pages> 303-320, </pages> <year> 1990a. </year>
Reference: [84] <author> J. L. Marroquin, S. Mitter, and T. Poggio. </author> <title> Probabilistic solution of ill-posed problems in computational vision. </title> <journal> J. Amer. Stat. Assoc., </journal> <volume> 82 </volume> <pages> 76-89, </pages> <year> 1987. </year>
Reference: [85] <author> M. Maruyama, F. Girosi, and T. Poggio. </author> <title> A connection between HBF and MLP. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1291, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1992. </year>
Reference: [86] <author> J. Meinguet. </author> <title> Multivariate interpolation at arbitrary points made simple. </title> <journal> J. Appl. Math. Phys., </journal> <volume> 30 </volume> <pages> 292-304, </pages> <year> 1979. </year>
Reference: [87] <author> B. W. Mel. MURPHY: </author> <title> a robot that learns by doing. </title> <editor> In D. Z. Anderson, editor, </editor> <booktitle> Neural Information Processing Systems. </booktitle> <institution> American Institute of Physics, University of Colorado, </institution> <address> Denver, </address> <year> 1988. </year>
Reference: [88] <author> B.W. </author> <title> Mel. The Sigma-Pi column: A model of associative learning in cerebral neocortex. </title> <type> Technical Report 6, </type> <institution> California Institute of Technology, </institution> <year> 1990. </year>
Reference: [89] <author> B.W. </author> <title> Mel. NMDA-based pattern-discrimination in a modeled cortical neuron. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 502-517, </pages> <year> 1992. </year>
Reference: [90] <author> H.N. Mhaskar. </author> <title> Approximation properties of a multilayered feedforward artificial neural network. </title> <booktitle> Advances in Computational Mathematics, </booktitle> <volume> 1 </volume> <pages> 61-80, </pages> <year> 1993. </year>
Reference: [91] <author> H.N. Mhaskar. </author> <title> Neural networks for localized approximation of real functions. In C.A. </title> <editor> Kamm et al., editor, </editor> <booktitle> Neural networks for signal processing III, Proceedings of the 1993 IEEE-SP Workshop, </booktitle> <pages> pages 190-196, </pages> <address> New York, </address> <year> 1993a. </year> <booktitle> IEEE Signal Processing Society. </booktitle>
Reference: [92] <author> H.N. Mhaskar and C.A. Micchelli. </author> <title> Approximation by superposition of a sigmoidal function. </title> <booktitle> Advances in Applied Mathematics, </booktitle> <volume> 13 </volume> <pages> 350-373, </pages> <year> 1992. </year> <month> 53 </month>
Reference: [93] <author> H.N. Mhaskar and C.A. Micchelli. </author> <title> How to choose an activation function. </title> <editor> In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference: [94] <author> C. A. Micchelli. </author> <title> Interpolation of scattered data: distance matrices and conditionally positive definite functions. Constructive Approximation, </title> <booktitle> 2 </booktitle> <pages> 11-22, </pages> <year> 1986. </year>
Reference: [95] <author> J. Moody. </author> <title> Note on generalization, regularization, and architecture selection in nonlinear learning systems. </title> <booktitle> In Proceedings of the First IEEE-SP Workshop on Neural Networks for Signal Processing, </booktitle> <pages> pages 1-10, </pages> <address> Los Alamitos, CA, 1991a. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: [96] <author> J. Moody. </author> <title> The effective number of parameters: an analysis of generalization and regularization in nonlinear learning systems. </title> <editor> In J. Moody, S. Hanson, and R. Lipp-mann, editors, </editor> <booktitle> Advances in Neural information processings systems 4, </booktitle> <pages> pages 847-854, </pages> <address> Palo Alto, CA, 1991b. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: [97] <author> J. Moody and C. Darken. </author> <title> Learning with localized receptive fields. </title> <editor> In G. Hinton, T. Sejnowski, and D. Touretzsky, editors, </editor> <booktitle> Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 133-143, </pages> <address> Palo Alto, </address> <year> 1988. </year>
Reference: [98] <author> J. Moody and C. Darken. </author> <title> Fast learning in networks of locally-tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 281-294, </pages> <year> 1989. </year>
Reference: [99] <author> J. Moody and N. Yarvin. </author> <title> Networks with learned unit response functions. </title> <editor> In J. Moody, S. Hanson, and R. Lippmann, editors, </editor> <booktitle> Advances in Neural information processings systems 4, </booktitle> <pages> pages 1048-1055, </pages> <address> Palo Alto, CA, 1991. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: [100] <author> V.A. Morozov. </author> <title> Methods for solving incorrectly posed problems. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference: [101] <author> E.A. Nadaraya. </author> <title> On estimating regression. </title> <journal> Theor. Prob. Appl., </journal> <volume> 9 </volume> <pages> 141-142, </pages> <year> 1964. </year>
Reference: [102] <author> P. Niyogi and F. Girosi. </author> <title> On the relationship between generalization error, hypothesis complexity, and sample complexity for radial basis functions. A.I. </title> <type> Memo 1467, </type> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1994. </year>
Reference: [103] <author> S. Omohundro. </author> <title> Efficient algorithms with neural network behaviour. </title> <journal> Complex Systems, </journal> <volume> 1:273, </volume> <year> 1987. </year>
Reference: [104] <author> G. Parisi. </author> <title> Statistical Field Theory. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusets, </address> <year> 1988. </year> <month> 54 </month>
Reference: [105] <author> E. Parzen. </author> <title> On estimation of a probability density function and mode. </title> <journal> Ann. Math. Statis., </journal> <volume> 33 </volume> <pages> 1065-1076, </pages> <year> 1962. </year>
Reference: [106] <author> R. Penrose. </author> <title> A generalized inverse for matrices. </title> <journal> Proc. Cambridge Philos. Soc., </journal> <volume> 51 </volume> <pages> 406-413, </pages> <year> 1955. </year>
Reference: [107] <author> A. Pinkus. </author> <title> N-widths in Approximation Theory. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1986. </year>
Reference: [108] <author> T. Poggio. </author> <title> On optimal nonlinear associative recall. </title> <journal> Biological Cybernetics, </journal> <volume> 19 </volume> <pages> 201-209, </pages> <year> 1975. </year>
Reference: [109] <author> T. Poggio. </author> <title> A theory of how the brain might work. </title> <booktitle> In Cold Spring Harbor Symposia on Quantitative Biology, </booktitle> <pages> pages 899-910. </pages> <note> Cold Spring Harbor Laboratory Press, </note> <year> 1990. </year>
Reference: [110] <author> T. Poggio and F. Girosi. </author> <title> A theory of networks for approximation and learning. </title> <journal> A.I. </journal> <volume> Memo No. </volume> <pages> 1140, </pages> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1989. </year>
Reference: [111] <author> T. Poggio and F. Girosi. </author> <title> Networks for approximation and learning. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(9), </volume> <month> September </month> <year> 1990. </year>
Reference: [112] <author> T. Poggio and F. Girosi. </author> <title> Extension of a theory of networks for approximation and learning: dimensionality reduction and clustering. </title> <booktitle> In Proceedings Image Understanding Workshop, </booktitle> <pages> pages 597-603, </pages> <address> Pittsburgh, Pennsylvania, September 11-13 1990a. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [113] <author> T. Poggio and F. Girosi. </author> <title> Regularization algorithms for learning that are equivalent to multilayer networks. </title> <journal> Science, </journal> <volume> 247 </volume> <pages> 978-982, </pages> <year> 1990b. </year>
Reference: [114] <author> T. Poggio and A. Hurlbert. </author> <title> Observation on cortical mechanisms for object recognition and learning. </title> <editor> In C. Koch and J. Davis, editors, </editor> <title> Large-scale Neuronal Theories of the Brain. </title> <publisher> In press. </publisher>
Reference: [115] <author> T. Poggio, V. Torre, and C. Koch. </author> <title> Computational vision and regularization theory. </title> <journal> Nature, </journal> <volume> 317 </volume> <pages> 314-319, </pages> <year> 1985. </year>
Reference: [116] <author> T. Poggio, H. Voorhees, and A. Yuille. </author> <title> A regularized solution to edge detection. </title> <journal> Journal of Complexity, </journal> <volume> 4 </volume> <pages> 106-123, </pages> <year> 1988. </year>
Reference: [117] <author> D. Pollard. </author> <title> Convergence of stochastic processes. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference: [118] <author> M. J. D. Powell. </author> <title> Radial basis functions for multivariable interpolation: a review. </title> <editor> In J. C. Mason and M. G. Cox, editors, </editor> <title> Algorithms for Approximation. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1987. </year> <month> 55 </month>
Reference: [119] <author> M.J.D. Powell. </author> <title> The theory of radial basis functions approximation in 1990. In W.A. Light, editor, Advances in Numerical Analysis Volume II: Wavelets, Subdivision Algorithms and Radial Basis Functions, </title> <address> pages 105-210. </address> <publisher> Oxford University Press, </publisher> <year> 1992. </year>
Reference: [120] <author> M.B. </author> <title> Priestley and M.T. Chao. Non-parametric function fitting. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 34 </volume> <pages> 385-392, </pages> <year> 1972. </year>
Reference: [121] <author> C. Rabut. </author> <title> How to build quasi-interpolants. applications to polyharmonic B-splines. </title> <editor> In P.-J. Laurent, A. Le Mehaute, and L.L. Schumaker, editors, </editor> <booktitle> Curves and Surfaces, </booktitle> <pages> pages 391-402. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1991. </year>
Reference: [122] <author> C. Rabut. </author> <title> An introduction to Schoenberg's approximation. </title> <journal> Computers Math. Applic., </journal> <volume> 24(12) </volume> <pages> 149-175, </pages> <year> 1992. </year>
Reference: [123] <author> B.D. Ripley. </author> <title> Neural networks and related methods for classification. </title> <journal> Proc. Royal Soc. London, </journal> <note> in press, </note> <year> 1994. </year>
Reference: [124] <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference: [125] <author> M. Rosenblatt. </author> <title> Curve estimates. </title> <journal> Ann. Math. Statist., </journal> <volume> 64 </volume> <pages> 1815-1842, </pages> <year> 1971. </year>
Reference: [126] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning representations by back-propagating errors. </title> <journal> Nature, </journal> <volume> 323(9) </volume> <pages> 533-536, </pages> <month> October </month> <year> 1986. </year>
Reference: [127] <author> I.J. </author> <title> Schoenberg. Contributions to the problem of approximation of equidistant data by analytic functions, part a: On the problem of smoothing of graduation, a first class of analytic approximation formulae. </title> <journal> Quart. Appl. Math., </journal> <volume> 4 </volume> <pages> 45-99, </pages> <year> 1946a. </year>
Reference: [128] <author> I.J. </author> <title> Schoenberg. Cardinal interpolation and spline functions. </title> <journal> Journal of Approximation theory, </journal> <volume> 2 </volume> <pages> 167-206, </pages> <year> 1969. </year>
Reference: [129] <author> L.L. Schumaker. </author> <title> Spline functions: basic theory. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1981. </year>
Reference: [130] <author> T. J. Sejnowski and C. R. Rosenberg. </author> <title> Parallel networks that learn to pronounce english text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168, </pages> <year> 1987. </year>
Reference: [131] <author> B.W. Silverman. </author> <title> Spline smoothing: the equivalent variable kernel method. </title> <journal> The Annals of Statistics, </journal> <volume> 12 </volume> <pages> 898-916, </pages> <year> 1984. </year>
Reference: [132] <author> N. Sivakumar and J.D. Ward. </author> <title> On the best least square fit by radial functions to multidimensional scattered data. </title> <type> Technical Report 251, </type> <institution> Center for Approximation Theory, Texas A & M University, </institution> <month> June </month> <year> 1991. </year> <month> 56 </month>
Reference: [133] <author> R.J. Solomonoff. </author> <title> Complexity-based induction systems: comparison and conver-gence theorems. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 24, </volume> <year> 1978. </year>
Reference: [134] <author> D.F. Specht. </author> <title> A general regression neural network. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(6) </volume> <pages> 568-576, </pages> <year> 1991. </year>
Reference: [135] <author> E.M. Stein. </author> <title> Singular integrals and differentiability properties of functions. </title> <address> Prince-ton, N.J., </address> <publisher> Princeton University Press, </publisher> <year> 1970. </year>
Reference: [136] <author> J. Stewart. </author> <title> Positive definite functions and generalizations, an historical survey. </title> <journal> Rocky Mountain J. Math., </journal> <volume> 6 </volume> <pages> 409-434, </pages> <year> 1976. </year>
Reference: [137] <author> C. J. Stone. </author> <title> Additive regression and other nonparametric models. </title> <journal> The Annals of Statistics, </journal> <volume> 13 </volume> <pages> 689-705, </pages> <year> 1985. </year>
Reference: [138] <author> A. N. </author> <title> Tikhonov. Solution of incorrectly formulated problems and the regularization method. </title> <journal> Soviet Math. Dokl., </journal> <volume> 4 </volume> <pages> 1035-1038, </pages> <year> 1963. </year>
Reference: [139] <author> A. N. Tikhonov and V. Y. Arsenin. </author> <title> Solutions of Ill-posed Problems. </title> <editor> W. H. Winston, </editor> <address> Washington, D.C., </address> <year> 1977. </year>
Reference: [140] <author> A.F. Timan. </author> <title> Theory of approximation of functions of a real variable. </title> <publisher> Macmillan, </publisher> <address> New York, </address> <year> 1963. </year>
Reference: [141] <author> V. Tresp, J. Hollatz, and S. Ahmad. </author> <title> Network structuring and training using rule-based knowledge. </title> <editor> In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference: [142] <author> F. Utreras. </author> <title> Cross-validation techniques for smoothing spline functions in one or two dimensions. </title> <editor> In T. Gasser and M. Rosenblatt, editors, </editor> <title> Smoothing techniques for Curve Estimation, </title> <address> pages 196-231. </address> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, </address> <year> 1979. </year>
Reference: [143] <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1982. </year>
Reference: [144] <author> V. N. Vapnik and A. Y. Chervonenkis. </author> <title> On the uniform convergence of relative fre-quences of events to their probabilities. </title> <journal> Th. Prob. and its Applications, </journal> <volume> 17(2) </volume> <pages> 264-280, </pages> <year> 1971. </year>
Reference: [145] <author> V.N. Vapnik and A. Ya. Chervonenkis. </author> <title> The necessary and sufficient conditions for the uniform convergence of averages to their expected values. </title> <journal> Teoriya Veroyatnos-tei i Ee Primeneniya, </journal> <volume> 26(3) </volume> <pages> 543-564, </pages> <year> 1981. </year> <month> 57 </month>
Reference: [146] <author> V.N. Vapnik and A. Ya. Chervonenkis. </author> <title> The necessary and sufficient conditions for consistency in the empirical risk minimization method. </title> <journal> Pattern Recognition and Image Analysis, </journal> <volume> 1(3) </volume> <pages> 283-305, </pages> <year> 1991. </year>
Reference: [147] <author> V.N. Vapnik and A.R. Stefanyuk. </author> <title> Nonparametric methods for restoring probability densities. </title> <journal> Avtomatika i Telemekhanika, </journal> (8):38-52, 1978. 
Reference: [148] <author> G. Wahba. </author> <title> Smoothing noisy data by spline functions. </title> <journal> Numer. Math, </journal> <volume> 24 </volume> <pages> 383-393, </pages> <year> 1975. </year>
Reference: [149] <author> G. Wahba. </author> <title> Smoothing and ill-posed problems. </title> <editor> In M. Golberg, editor, </editor> <title> Solutions methods for integral equations and applications, </title> <address> pages 183-194. </address> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1979. </year>
Reference: [150] <author> G. Wahba. </author> <title> Spline bases, regularization, and generalized cross-validation for solving approximation problems with large quantities of noisy data. </title> <editor> In J. Ward and E. Cheney, editors, </editor> <booktitle> Proceedings of the International Conference on Approximation theory in honour of George Lorenz, </booktitle> <address> Austin, TX, January 8-10 1980. </address> <publisher> Academic Press. </publisher>
Reference: [151] <author> G. Wahba. </author> <title> A comparison of GCV and GML for choosing the smoothing parameter in the generalized splines smoothing problem. </title> <journal> The Annals of Statistics, </journal> <volume> 13 </volume> <pages> 1378-1402, </pages> <year> 1985. </year>
Reference: [152] <author> G. Wahba. </author> <title> Splines Models for Observational Data. </title> <booktitle> Series in Applied Mathematics, </booktitle> <volume> Vol. 59, </volume> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1990. </year>
Reference: [153] <author> G. Wahba and S. </author> <title> Wold. A completely automatic french curve. </title> <journal> Commun. Statist., </journal> <volume> 4 </volume> <pages> 1-17, </pages> <year> 1975. </year>
Reference: [154] <author> G.S. Watson. </author> <title> Smooth regression analysis. </title> <journal> Sankhya A, </journal> <volume> 26 </volume> <pages> 359-372, </pages> <year> 1964. </year>
Reference: [155] <author> H. White. </author> <title> Learning in artificial neural networks: a statistical perspective. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 425-464, </pages> <year> 1989. </year>
Reference: [156] <author> H. White. </author> <title> Connectionist nonparametric regression: Multilayer perceptrons can learn arbitrary mappings. </title> <booktitle> Neural Networks, </booktitle> <pages> 3(535-549), </pages> <year> 1990. </year>
Reference: [157] <author> A. Yuille and N. Grzywacz. </author> <title> The motion coherence theory. </title> <booktitle> In Proceedings of the International Conference on Computer Vision, </booktitle> <pages> pages 344-354, </pages> <address> Washington, D.C., December 1988. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: [158] <author> W.P. Ziemer. </author> <title> Weakly differentiable functions : Sobolev spaces and functions of bounded variation. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1989. </year> <month> 58 </month>
References-found: 158

