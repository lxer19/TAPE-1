URL: http://www.cs.colorado.edu/home/gc/cs/barrier/barrier-paper.ps
Refering-URL: http://www.cs.colorado.edu/home/gc/cs/barrier/barrier.html
Root-URL: http://www.cs.colorado.edu
Email: (Email:fgrunwald,suvasg@cs.colorado.edu)  
Phone: 430,  
Title: Efficient Barriers for Distributed Shared Memory Computers  
Author: Dirk Grunwald Suvas Vajracharya 
Address: Campus Box  Boulder, CO 80309-0430  
Affiliation: Department of Computer Science,  University of Colorado,  
Abstract: Barrier algorithms are central to the performance of numerous algorithms on scalable, high-performance architectures. Numerous barrier algorithms have been suggested and studied for Non-Uniform Memory Access (NUMA) architectures, but less work has been done for Cache One Memory Access (COMA) architectures such as the KSR-1. Modern barrier algorithms rely on tree-structured distributed synchronization with individual processors spinning on memory local to the processor. In this paper, we present a barrier algorithm, f -way Tournament, that offers the best performance we have recorded on the KSR-1 distributed cache multiprocessor. We discuss the trade-offs and the performance of five algorithm on two architectures. The new barrier algorithm adapts well to a hierarchical caching memory model with non-uniform communication costs, has a short critical path for notification ( log f P , where 2 f 9 and f is a function of number processors participating in the barrier), and takes advantage of parallel communication offered by most multiprocessor interconnection networks. Performance results are shown for a 64-processor KSR-1 and a 20-processor Sequent Symmetry. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T.E. Anderson. </author> <title> The performance of spinlock alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: The hotspot problem can be somewhat alleviated in cache coherent machines because a copy of the counter is copied on to local memory, reducing the amount of network communication, and various centralized barrier and lock designs make effective use of local caches <ref> [1] </ref>. However, the writers to the barrier contend between themselves, and notification still require O (N) time. 2.2 Software Combining Trees To address the hot spot problem, a combining tree barrier was proposed by Yew et al. [10]. A software combining tree spreads the congestion over a tree of variables.
Reference: [2] <author> Allan Gottlieb, Ralph Grishman, Clyde P. Kruskal, Kevin P. McAuliffe, Larray Rudolph, and Marc Snir. </author> <title> The nyu ultracomputer: Designing a mimd, shared-memory parallel machine. </title> <booktitle> Proceedings of 9th Annual International Symposium on Computer, </booktitle> <volume> 10(3) </volume> <pages> 27-42, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Once the rendezvous has been achieved, the counter is reset to zero for the next rendezvous. There are two disadvantages to this approach. First, the counter must be updated atomically, either via explicit locking or hardware operations such as fetch and <ref> [2] </ref>. Second, all processes must contend with each other to read and write a single memory location. As mentioned, this causes hot-spots, or high traffic congestion. Consequently, this barrier is not scalable since each read and a write involves a network communication.
Reference: [3] <author> D. Hensgen, R. Finkel, and U. Manber. </author> <title> Two algorithms for barrier synchronization. </title> <journal> Intl. Journal of Parallel Programming, </journal> <volume> 17(1), </volume> <year> 1988. </year> <title> 2 We're trying to locate a larger Sequent Symmetry for further testing. 12 Uses a Broadcast Wakeup Rather Than Tree Wakeup 13 </title>
Reference-contexts: To alleviate these problems, Brooks described the butterfly barrier [5], which is similar to the exchange-swap prefix and synchronization algorithm used in message-passing computers with a hyper-cube interconnection network. Hensgen et al <ref> [3] </ref> improved the butterfly-network for situations where the number of processors accessing the barrier are not a power of two. Their dissemination barrier uses the communication structure show in Figure 1. In the dissemination barrier, each barrier rendezvous is divided into log 2 P rounds for P processors.
Reference: [4] <author> W. Daniel Hillis and G.L. Steele. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29, No.12:1170-1183, </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: 1 Introduction Barriers are a synchronization tool for parallel computers, including shared and distributed (message-passing) address-space architectures. No processor may pass the barrier until all processes have arrived at the barrier; this synchronization tool is used in many algorithms, and is central to the data-parallel programming model <ref> [4] </ref>. There are numerous barrier algorithms for message-passing and shared-address space computers, and some architectures, such as the Thinking Machines CM-5, provide special hardware support for barrier synchronization. On architectures lacking such hardware support, scalable barriers must be implemented in software, using the underlying communication network.
Reference: [5] <author> Edward D. Brooks III. </author> <title> The butterfly barrier. </title> <journal> Intl. Journal of Parallel Programming, </journal> <volume> 15(4) </volume> <pages> 295-307, </pages> <year> 1986. </year>
Reference-contexts: Also, both methods require spinning at a remote memory location, leading to unnecessary contention for the interconnections bandwidth on machines that are not broadcast-based and lack cache-coherency. To alleviate these problems, Brooks described the butterfly barrier <ref> [5] </ref>, which is similar to the exchange-swap prefix and synchronization algorithm used in message-passing computers with a hyper-cube interconnection network. Hensgen et al [3] improved the butterfly-network for situations where the number of processors accessing the barrier are not a power of two.
Reference: [6] <author> Boris D. Lubachecsky. </author> <title> Synchronization barrier and relation tools for shared memory parallel programs. </title> <booktitle> In Proc. of the 1989 Int. Conf. on Parallel Processing, pages II-175-II-179. </booktitle> <institution> Penn State, </institution> <year> 1989. </year>
Reference-contexts: The communication structure of the barrier algorithm must match the physical interconnection network for best performance, 2.4 Tournament Algorithm 3 4 To counter the additional communication in the dissemination algorithm, Hensgen et al also developed the tournament algorithm, apparently at the suggestion of Lubachecsky <ref> [6] </ref>. As with the dissemination algorithm, the tournament algorithm avoids special hardware by using a pre-determined communication structure; however, the tournament algorithm incurs only O (log 2 P ) total network transactions, as illustrated by the communication structure shown in Figure 1.
Reference: [7] <author> John Mellor-Crummey and Michael Scott. </author> <title> Algorithms for scalable synchronization on shared memory multiprocessors. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: We show that conclusions fl This work was funded in part by NSF grant No. ASC-9217394. 1 drawn in previous studies of barriers <ref> [7] </ref> are not necessarily true on COMA architectures, highlighting the need for a formal model to predict the performance of barrier algorithms. 2 Previous Barrier Algorithms As computer architectures have evolved, numerous barrier algorithms have been proposed. A central barrier suffices for small-scale cache-coherent multiprocessors with 2-10 processors. <p> Reinitialization involves resetting the counter at each node in the tree. A central busy-flag is desired if the machine is broadcast cache-coherent, such as KSR or Sequent Symmetry, since the hardware can quickly do the wakeup <ref> [7] </ref>, while the tree wake-up should be used for architectures where non-local operations must always use the network, such as BBN Butterfly. 2 This presents a design choice exercised in different ways in the various barrier algorithms decreasing hot spot contention vs. increasing the critical path for barrier notification and wakeup. <p> No wakeup notification is needed in this algorithm, since each processor has been notified by every other processor after log 2 P rounds. The dissemination algorithm achieved the best performance of a variety of barrier algorithms in the previous study of Mellor-Crummey et al <ref> [7] </ref> on non-uniform memory shared-address machines without cache-coherency and broadcasting, such as BBN Butterfly. However the performance of the dissemination algorithm is poor on cache-coherent architectures, because these machines do not benefit from the local spinning performed by that algorithm. <p> Reinitialization is done by sense reversing the flags. Finally, wakeup notification is done using either a global wakeup flag for broadcast-based machines or a tree-wakeup algorithm that involves climbing back down the tree for machines without the broadcast. 2.5 MCS-Tree Algorithm Mellor-Crummey et al <ref> [7] </ref> proposed a variation on the tournament algorithm called the MCS-barrier. Figure 2 depicts the communication structure for the MCS-barrier algorithm, and Figure 3 presents an alternate view for a larger number of processors. In this algorithm readers (parents) and writers (children) are statically pre-determined. <p> As can be seen in Table 1, the f -way tournament algorithms improves upon the previous algorithms. We implemented each algorithm on the KSR-1 and Sequent Symmetry. The timing measurements shown in used this experimental method to facilitate comparison with the results of Mellor-Crummey <ref> [7] </ref>, who used the same method. Our empirical results 1 on a 64-processor KSR-1, shown in Figure 8, confirm the expected 1 For the full paper, we'll include results for a 192-processor KSR-1, however we were unable to collect the full spectrum of results for this abstract. <p> Note that the dissemination barrier suffers a sharp increase in the time to rendezvous between 32 and 33 processors; this is when inter-cluster communication occurs in the KSR-1 interconnection network. Previous studies <ref> [7] </ref> had shown that the MCS algorithm out-performed other algorithms, such as the tournament algorithm. However, there is a greater degree of parallelism in the tournament algorithm, and that can be efficiently exploited on the KSR-1. This was also recently noted by Ramachandran et al [9].
Reference: [8] <author> G. Pfister and V. Norton. </author> <title> Hot spot contention and combining in multistage interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-34(10):943-948, </volume> <month> October </month> <year> 1985. </year>
Reference-contexts: A central barrier suffices for small-scale cache-coherent multiprocessors with 2-10 processors. As the promise of larger-scale systems including tens and hundreds of processors was realized, it was noted that centralized barriers limited system performance. The concentration communication to a single memory location induced a hot spot <ref> [8] </ref> in the network. Both hardware solutions, including combining networks, and software solutions [10] were proposed. Several barrier algorithms have been proposed that distribute the communication, either over different cache locations, network connections or processor clusters. Most of these algorithms use some tree-based structure to distribute the communication.
Reference: [9] <author> Umakishore Ramachandran, Gautam Shah, S. Ravikumar, and Jeyakumar Muthukumarasamy. </author> <title> Scalability study of the ksr-1. </title> <type> GIT-CC 93/03, </type> <institution> Georgia Inst. of Technology, </institution> <year> 1993. </year>
Reference-contexts: Previous studies [7] had shown that the MCS algorithm out-performed other algorithms, such as the tournament algorithm. However, there is a greater degree of parallelism in the tournament algorithm, and that can be efficiently exploited on the KSR-1. This was also recently noted by Ramachandran et al <ref> [9] </ref>. Furthermore, the tournament algorithm involves less inter-cluster communication, which is very important on the KSR-1 - intra-cluster memory references take 150 machine cycles, while inter-cluster references take 600 cycles. As shown in Figure 3, each child i communicates with its parent processor b i1 4 c.
Reference: [10] <author> Pen Yew, N. Tzeng, and Ducan Lawrie. </author> <title> Distributing host-spot addressing in large-scale multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 388-395, </pages> <month> April </month> <year> 1987. </year> <month> 14 </month>
Reference-contexts: As the promise of larger-scale systems including tens and hundreds of processors was realized, it was noted that centralized barriers limited system performance. The concentration communication to a single memory location induced a hot spot [8] in the network. Both hardware solutions, including combining networks, and software solutions <ref> [10] </ref> were proposed. Several barrier algorithms have been proposed that distribute the communication, either over different cache locations, network connections or processor clusters. Most of these algorithms use some tree-based structure to distribute the communication. <p> However, the writers to the barrier contend between themselves, and notification still require O (N) time. 2.2 Software Combining Trees To address the hot spot problem, a combining tree barrier was proposed by Yew et al. <ref> [10] </ref>. A software combining tree spreads the congestion over a tree of variables. Processors at the leaves are divided into groups of size k. The last arriving processor becomes the group representative and advances to next level in the tree.
References-found: 10

