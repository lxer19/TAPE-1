URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3663/3663.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: BFGS WITH UPDATE SKIPPING AND VARYING MEMORY  
Author: TAMARA GIBSON DIANNE P. O'LEARY AND LARRY NAZARETH 
Keyword: Key words. minimization, quasi-Newton, BFGS, limited-memory, update skipping, Broyden family  
Date: July 9, 1996  
Abstract: We give conditions under which limited-memory quasi-Newton methods with exact line searches will terminate in n steps when minimizing n-dimensional quadratic functions. We show that although all Broyden family methods terminate in n steps in their full-memory versions, only BFGS does so with limited-memory. Additionally, we show that full-memory Broyden family methods with exact line searches terminate in at most n + p steps when p matrix updates are skipped. We introduce new limited-memory BFGS variants and test them on nonquadratic minimization problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. Bongartz, A. R. Conn, N. Gould, and P. L. Toint, ftp://thales.math.fundp.ac.be/ pub/cute. </author> <title> [2] , http://www.rl.ac.uk/departments/ccd/numerical/cute/cute.html. [3] , CUTE: constrained and unconstrained testing environment, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 21 (1995), </volume> <pages> pp. 123-160. </pages>
Reference-contexts: Test Problems. For our test problems, we used the Constrained and Unconstrained Testing Environment (CUTE) by Bongartz, Conn, Gould and Toint. The package is documented in [3] and can be obtained via the world wide web [2] or via ftp <ref> [1] </ref>. The package contains a large collection of test problems as well as the interfaces necessary for using the problems. The test problems are stored as "SIF" files. We chose a collection of 22 unconstrained problems.
Reference: [4] <author> A. Buckley, </author> <title> Test functions for unconstrained minimization, </title> <type> Tech. Report TR 1989CS-3, </type> <institution> Mathematics, statistics and computing centre, Dalhousie University, Halifax (CDN), </institution> <year> 1989. </year> <note> Cited in [1, 2, 3]. </note>
Reference-contexts: the Broyden tridiagonal system with a band away from diagonal [29]. 13 PENALTY1 1000 First penalty problem [17, Problem 23]. 14 POWER 1000 Power problem by Oren [25]. 15 MSQRTALS 1024 The dense matrix square root problem by No-cedal and Liu (case 0) seen as a nonlinear equa tion problem <ref> [4, Problem 204] </ref>. 16 MSQRTBLS 1025 The dense matrix square root problem by No-cedal and Liu (case 1) seen as a nonlinear equa tion problem [4, Problem 201]. 17 CRAGGLVY 5000 Extended Cragg & Levy problem [30, Prob lem 32]. 18 NONDQUAR 10000 Nondiagonal quartic test problem [5, Prob lem 57]. <p> by Oren [25]. 15 MSQRTALS 1024 The dense matrix square root problem by No-cedal and Liu (case 0) seen as a nonlinear equa tion problem [4, Problem 204]. 16 MSQRTBLS 1025 The dense matrix square root problem by No-cedal and Liu (case 1) seen as a nonlinear equa tion problem <ref> [4, Problem 201] </ref>. 17 CRAGGLVY 5000 Extended Cragg & Levy problem [30, Prob lem 32]. 18 NONDQUAR 10000 Nondiagonal quartic test problem [5, Prob lem 57]. 19 POWELLSG 10000 Extended Powell singular function [17, Prob lem 13]. 20 SINQUAD 10000 Another function with nontrivial groups and rep etitious elements [12]. 21 <p> [30, Prob lem 32]. 18 NONDQUAR 10000 Nondiagonal quartic test problem [5, Prob lem 57]. 19 POWELLSG 10000 Extended Powell singular function [17, Prob lem 13]. 20 SINQUAD 10000 Another function with nontrivial groups and rep etitious elements [12]. 21 SPMSRTLS 10000 Liu and Nocedal tridiagonal matrix square root problem <ref> [4, Problem 151] </ref>. 22 TRIDIA 10000 Shanno's TRIDIA quadratic tridiagonal problem [30, Problem 8]. Table 4.1 Test problem collection. Each problems was chosen from the CUTE package. and David Thuente from a 1983 version of MINPACK.
Reference: [5] <author> A. Conn, N. Gould, M. Lescrenier, and P. Toint, </author> <title> Performance of a multifrontal scheme for partially separable optimization, </title> <type> Tech. Report 88/4, </type> <institution> Department of Mathematics, FUNDP, </institution> <address> Namur, Belgium, </address> <year> 1988. </year> <note> Cited in [1, 2]. </note>
Reference-contexts: FLETCHBV 100 Fletcher's boundary value problem [8, Prob lem 1]. 8 FLETCHCR 100 Fletcher's chained Rosenbrock function [8, Prob lem 2]. 9 PENALTY2 100 Second penalty problem [17, Problem 24]. 10 GENROSE 500 Generalized Rosenbrock function [18, Problem 5]. 11 BDQRTIC 1000 Quartic with a banded Hessian with band width=9 <ref> [5, Problem 61] </ref>. 12 BROYDN7D 1000 Seven diagonal variant of the Broyden tridiagonal system with a band away from diagonal [29]. 13 PENALTY1 1000 First penalty problem [17, Problem 23]. 14 POWER 1000 Power problem by Oren [25]. 15 MSQRTALS 1024 The dense matrix square root problem by No-cedal and Liu <p> equa tion problem [4, Problem 204]. 16 MSQRTBLS 1025 The dense matrix square root problem by No-cedal and Liu (case 1) seen as a nonlinear equa tion problem [4, Problem 201]. 17 CRAGGLVY 5000 Extended Cragg & Levy problem [30, Prob lem 32]. 18 NONDQUAR 10000 Nondiagonal quartic test problem <ref> [5, Prob lem 57] </ref>. 19 POWELLSG 10000 Extended Powell singular function [17, Prob lem 13]. 20 SINQUAD 10000 Another function with nontrivial groups and rep etitious elements [12]. 21 SPMSRTLS 10000 Liu and Nocedal tridiagonal matrix square root problem [4, Problem 151]. 22 TRIDIA 10000 Shanno's TRIDIA quadratic tridiagonal problem [30,
Reference: [6] <author> R. S. Dembo and T. Steihaug, </author> <title> Truncated-Newton algorithms for large-scale unconstrained optimization, </title> <journal> Mathematical Programming, </journal> <volume> 26 (1983), </volume> <pages> pp. 190-212. </pages>
Reference-contexts: Limited-memory quasi-Newton methods fall in between these extremes in terms of performance and storage. There are other methods that fall into the middle ground; for example, conjugate gradient methods such as those proposed by Shanno [27] and Nazareth [20], the truncated-Newton method <ref> [24, 6] </ref> and the partitioned quasi-Newton method [13]. We have characterized which limited-memory quasi-Newton methods fitting a general form (2.1) have the property of producing conjugate search directions on convex quadratics.
Reference: [7] <author> J. Dennis and R. B. Schnabel, </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations, </title> <booktitle> Series in Computational Mathematics, </booktitle> <publisher> Prentice Hall, </publisher> <year> 1983. </year>
Reference-contexts: In general, an approximation to the second derivative matrix is built by accumulating the results of earlier steps. Descriptions of many quasi-Newton algorithms can be found in books by Luenberger [16], Dennis and Schnabel <ref> [7] </ref>, and Golub and Van Loan [11]. Although there are an infinite number of quasi-Newton methods, one method surpasses the others in popularity: the BFGS algorithm of Broyden, Fletcher, Goldfarb, and Shanno; see, e.g., Dennis and Schnabel [7]. This method exhibits more robust behavior than its relatives. <p> quasi-Newton algorithms can be found in books by Luenberger [16], Dennis and Schnabel <ref> [7] </ref>, and Golub and Van Loan [11]. Although there are an infinite number of quasi-Newton methods, one method surpasses the others in popularity: the BFGS algorithm of Broyden, Fletcher, Goldfarb, and Shanno; see, e.g., Dennis and Schnabel [7]. This method exhibits more robust behavior than its relatives. Many attempts have been made to explain this robustness, but a complete understanding is yet to be obtained [23].
Reference: [8] <author> R. Fletcher, </author> <title> An optimal positive definite update for sparse Hessian matrices, Numerical Analysis NA/145, </title> <note> University of Dundee, 1992. Cited in [1, 2]. </note>
Reference-contexts: Problem 10]. 2 WATSONS 31 Watson problem [17, Problem 20]. 3 TOINTGOR 50 Toint's operations research problem [29]. 4 TOINTPSP 50 Toint's PSP operations research problem [29]. 5 CHNROSNB 50 Chained Rosenbrock function [29]. 6 ERRINROS 50 Nonlinear problem similar to CHNROSNB [28]. 7 FLETCHBV 100 Fletcher's boundary value problem <ref> [8, Prob lem 1] </ref>. 8 FLETCHCR 100 Fletcher's chained Rosenbrock function [8, Prob lem 2]. 9 PENALTY2 100 Second penalty problem [17, Problem 24]. 10 GENROSE 500 Generalized Rosenbrock function [18, Problem 5]. 11 BDQRTIC 1000 Quartic with a banded Hessian with band width=9 [5, Problem 61]. 12 BROYDN7D 1000 Seven <p> TOINTGOR 50 Toint's operations research problem [29]. 4 TOINTPSP 50 Toint's PSP operations research problem [29]. 5 CHNROSNB 50 Chained Rosenbrock function [29]. 6 ERRINROS 50 Nonlinear problem similar to CHNROSNB [28]. 7 FLETCHBV 100 Fletcher's boundary value problem [8, Prob lem 1]. 8 FLETCHCR 100 Fletcher's chained Rosenbrock function <ref> [8, Prob lem 2] </ref>. 9 PENALTY2 100 Second penalty problem [17, Problem 24]. 10 GENROSE 500 Generalized Rosenbrock function [18, Problem 5]. 11 BDQRTIC 1000 Quartic with a banded Hessian with band width=9 [5, Problem 61]. 12 BROYDN7D 1000 Seven diagonal variant of the Broyden tridiagonal system with a band away
Reference: [9] <author> T. Gibson, D. O'Leary, and L. Nazareth, </author> <note> http://www.cs.umd.edu/users/oleary/LBFGS/ index.html, </note> <year> 1996. </year>
Reference-contexts: The MAPLE notebook file used to compute this example is available on the World Wide Web <ref> [9] </ref>. Remark. Using the above example, we can easily see that no limited-memory Broyden class method except limited-memory BFGS terminates within the first n iterations. 3. Update-Skipping Variations for Broyden Class Quasi-Newton Algorithms. The previous section discussed limited-memory methods that behave like conjugate gradients on n-dimensional strictly convex quadratic functions. <p> Using exact arithmetic in MAPLE, we observe that the process does not terminate even after 100 iterations <ref> [9] </ref>. 4. Experimental Results. The results of x2 and x3 lead to a number of ideas for new methods for unconstrained optimization. In this section, we motivate, develop, and test these ideas. We describe the collection of test problems in x4.2. The test environment is described in x4.3. <p> Section 4.4.1 outlines the implementation of the L-BFGS method (our base for all comparisons) and xx4.4.2-4.4.7 describe the variations. Pseudo-code for L-BFGS and its variations is given in Appendix B. Complete numerical results, many graphs of the numerical results, and the original FORTRAN code are available <ref> [9] </ref>. 4.1. Motivation. So far we have only given results for convex quadratic functions. While termination on quadratics is beautiful in theory, it does not necessarily yield insight into how these methods will do in practice. <p> We used four values of m: 5,10,15 and 50, for each algorithm. The results are summarized in Tables 4.4 - 4.8. More extensive results can be obtained <ref> [9] </ref>. Table 4.4 shows that these algorithms had roughly the same number of failures as L-BFGS. Table 4.5 compares each algorithm to L-BFGS in terms of function evaluations. <p> Table 4.8 gives the mean of the ratios of time to solve for each value of m in each algorithm. Note that most of the ratios are far below one in this case. These variations did particularly well on problem 7. See <ref> [9] </ref> for more information. 4.4.3. Disposing of old information: Algorithm 5. We may decide that we are storing too much old information and that we should stop using it. <p> However, Table 4.7 shows that the differences were minor. In terms of time, we observe that the algorithm generally was faster than L-BFGS (Table 4.6), but again, considering the mean ratios of time (Table 4.8), the differences were minor. The method also does particularly well on problem 7 <ref> [9] </ref>. 4.4.4. Backing Up in the Update to H: Algorithms 6-11. As discussed in x2.2, if we always use the most recent s and y in the update, we preserve quadratic termination regardless of which older values of s and y we use. 24 T. Gibson, D. P. O'Leary, L.
Reference: [10] <author> P. E. Gill and W. Murray, </author> <title> Conjugate-gradient methods for large-scale nonlinear optimization, </title> <type> Tech. Report SOL 79-15, </type> <institution> Systems Optimization Laboratory, Department of Operations Research, Stanford University, Stanford, California, </institution> <month> 94305, </month> <year> 1979. </year>
Reference-contexts: Let fl k in (2.1) be the scaling constant, and choose the other vectors and matrices as in L-BFGS (2.6). Combinations of variants are left to the reader. Remark. Part 3 of the previous corollary shows that the "accumulated step" method of Gill and Murray <ref> [10] </ref> terminates on quadratics. Remark. Part 4 of the previous corollary shows that scaling does not affect termination in L-BFGS. In fact, for any method that fits the general form, it is easy to see that scaling will not affect termination on quadratics. 2.3.
Reference: [11] <author> G. H. Golub and C. F. Van Loan, </author> <title> Matrix Computations, </title> <publisher> The Johns Hopkins University Press, Baltimore, </publisher> <editor> 2nd ed., </editor> <year> 1989. </year>
Reference-contexts: In general, an approximation to the second derivative matrix is built by accumulating the results of earlier steps. Descriptions of many quasi-Newton algorithms can be found in books by Luenberger [16], Dennis and Schnabel [7], and Golub and Van Loan <ref> [11] </ref>. Although there are an infinite number of quasi-Newton methods, one method surpasses the others in popularity: the BFGS algorithm of Broyden, Fletcher, Goldfarb, and Shanno; see, e.g., Dennis and Schnabel [7]. This method exhibits more robust behavior than its relatives.
Reference: [12] <author> N. Gould. </author> <title> Private communication to authors of [3]. </title> <booktitle> Cited in [1, </booktitle> <pages> 2]. </pages>
Reference-contexts: problem [4, Problem 201]. 17 CRAGGLVY 5000 Extended Cragg & Levy problem [30, Prob lem 32]. 18 NONDQUAR 10000 Nondiagonal quartic test problem [5, Prob lem 57]. 19 POWELLSG 10000 Extended Powell singular function [17, Prob lem 13]. 20 SINQUAD 10000 Another function with nontrivial groups and rep etitious elements <ref> [12] </ref>. 21 SPMSRTLS 10000 Liu and Nocedal tridiagonal matrix square root problem [4, Problem 151]. 22 TRIDIA 10000 Shanno's TRIDIA quadratic tridiagonal problem [30, Problem 8]. Table 4.1 Test problem collection. Each problems was chosen from the CUTE package. and David Thuente from a 1983 version of MINPACK.
Reference: [13] <author> A. Griewank and P. L. Toint, </author> <title> Partitioned variable metric updates for large structured optimization problems, </title> <journal> Numer. Math., </journal> <volume> 39 (1982), </volume> <pages> pp. 119-137. </pages>
Reference-contexts: Limited-memory quasi-Newton methods fall in between these extremes in terms of performance and storage. There are other methods that fall into the middle ground; for example, conjugate gradient methods such as those proposed by Shanno [27] and Nazareth [20], the truncated-Newton method [24, 6] and the partitioned quasi-Newton method <ref> [13] </ref>. We have characterized which limited-memory quasi-Newton methods fitting a general form (2.1) have the property of producing conjugate search directions on convex quadratics. We have shown that limited-memory BFGS is the only Broyden family member that has a limited-memory analog with this property.
Reference: [14] <author> H. Khalfan, R. Byrd, and R. Schnabel, </author> <title> A theoretical and experimental study of the symmetric rank one update, </title> <type> Tech. Report CU-CS-489-90, </type> <institution> Department of Computer Science, University of Colorado at Boulder, </institution> <year> 1990. </year>
Reference-contexts: The parameter is usually restricted to values that are guaranteed to produce a positive definite update, although recent work with SR1, a Broyden Class method, by Khalfan, Byrd and Schnabel <ref> [14] </ref> may change this practice. No restriction on is necessary for the development of our theory.
Reference: [15] <author> D. C. Liu and J. Nocedal, </author> <title> On the limited memory BFGS method for large scale optimization, </title> <journal> Mathematical Programming, </journal> <volume> 45 (1989), </volume> <pages> pp. 503-528. </pages>
Reference-contexts: We will not present any new results relating to convergence of these algorithms on general functions; however, many of these can be shown to converge using the convergence analysis presented in x7 of <ref> [15] </ref>. In [15], Liu and Nocedal show that a limited-memory BFGS method implemented with a line search that satisfies the strong Wolfe conditions (see x4.3 for a definition) is R-linearly convergent on a convex function that satisfies a few modest conditions. 4.2. Test Problems. <p> We will not present any new results relating to convergence of these algorithms on general functions; however, many of these can be shown to converge using the convergence analysis presented in x7 of <ref> [15] </ref>. In [15], Liu and Nocedal show that a limited-memory BFGS method implemented with a line search that satisfies the strong Wolfe conditions (see x4.3 for a definition) is R-linearly convergent on a convex function that satisfies a few modest conditions. 4.2. Test Problems. <p> We are using L-BFGS as our basis for comparison. For information on the performance of L-BFGS see Liu and Nocedal <ref> [15] </ref> and Nash and Nocedal [19]. 4.4.2. Varying m iteratively: Algorithms 1-4. In typical implementations of L-BFGS, m is fixed throughout the iterations: once m updates have accumulated, m updates are always used. We considered the possibility of varying m iteratively, preserving finite termination on convex quadratics. <p> Varying m iteratively: Algorithms 1-4. In typical implementations of L-BFGS, m is fixed throughout the iterations: once m updates have accumulated, m updates are always used. We considered the possibility of varying m iteratively, preserving finite termination on convex quadratics. Using an argument similar to that presented in <ref> [15] </ref>, we can also prove that this algorithm has a linear rate of convergence on a convex function that satisfies a few modest conditions. We tried four different variations on this theme. All were based on the following linear formula that scales m in relation to the size of kgk.
Reference: [16] <author> D. G. Luenberger, </author> <title> Linear and Nonlinear Programming, </title> <publisher> Addison Wesley, </publisher> <editor> 2nd ed., </editor> <year> 1984. </year> <note> 30 T. </note> <author> Gibson, D. P. O'Leary, L. </author> <note> Nazareth </note>
Reference-contexts: In general, an approximation to the second derivative matrix is built by accumulating the results of earlier steps. Descriptions of many quasi-Newton algorithms can be found in books by Luenberger <ref> [16] </ref>, Dennis and Schnabel [7], and Golub and Van Loan [11]. Although there are an infinite number of quasi-Newton methods, one method surpasses the others in popularity: the BFGS algorithm of Broyden, Fletcher, Goldfarb, and Shanno; see, e.g., Dennis and Schnabel [7]. <p> Limited-memory BFGS (L-BFGS) was shown by Nocedal [22] to terminate in n steps. The preconditioned conjugate gradient method, which can be cast as a limited-memory quasi-Newton method, is also known to terminate in n iterations; see, e.g., Luenberger <ref> [16] </ref>. Little else is known about termination of limited-memory methods. Let f (x) denote the strictly convex quadratic function to be minimized, and let g (x) denote the gradient of f. We define g k g (x k ), where x k is the kth iterate. <p> Note that if the update is not positive definite, we may produce a d k such that d T k s k &gt; 0 in which case we choose ff k over all negative ff rather than all positive ff. Example. The method of steepest descent <ref> [16] </ref> fits the general form (2.1). For each k we define fl k = 1; m k = 0; and P k = Q k = H 0 = I:(2.3) Note that neither w nor z vectors are specified since m k = 0. Example. <p> Example. The Broyden Class or Broyden Family is the class of quasi-Newton methods whose matrices are linear combinations of the DFP and BFGS matrices: H k+1 = H BF GS k ; 2 R; see, e.g., Luenberger <ref> [16, Chap. 9] </ref>. The parameter is usually restricted to values that are guaranteed to produce a positive definite update, although recent work with SR1, a Broyden Class method, by Khalfan, Byrd and Schnabel [14] may change this practice. No restriction on is necessary for the development of our theory. <p> L-BFGS Variations 15 Example. Steepest descent, see (2.3), does not satisfy condition (2.15) of Theorem 2.2 and thus does not produce conjugate search directions. This fact is well known; see, e.g., Luenberger <ref> [16] </ref>. Example. Limited-memory DFP, see (2.8), with m &lt; n does not satisfy the condition on P k (2.15) for all k, and so the method will not produce conjugate directions.
Reference: [17] <author> J. J. Mor e, B. S. Garbow, and K. E. Hillstrom, </author> <title> Testing unconstrained optimization software, </title> <journal> ACM Trans. on Math. Software, </journal> <volume> 7 (1981), </volume> <pages> pp. 17-41. </pages>
Reference-contexts: For the line search, we use the routines cvsrch and cstep written by Jorge J. More 18 T. Gibson, D. P. O'Leary, L. Nazareth No. SIF Name n Description & Reference 1 EXTROSNB 10 Extended Rosenbrock function (nonseparable version) [30, Problem 10]. 2 WATSONS 31 Watson problem <ref> [17, Problem 20] </ref>. 3 TOINTGOR 50 Toint's operations research problem [29]. 4 TOINTPSP 50 Toint's PSP operations research problem [29]. 5 CHNROSNB 50 Chained Rosenbrock function [29]. 6 ERRINROS 50 Nonlinear problem similar to CHNROSNB [28]. 7 FLETCHBV 100 Fletcher's boundary value problem [8, Prob lem 1]. 8 FLETCHCR 100 Fletcher's <p> Toint's PSP operations research problem [29]. 5 CHNROSNB 50 Chained Rosenbrock function [29]. 6 ERRINROS 50 Nonlinear problem similar to CHNROSNB [28]. 7 FLETCHBV 100 Fletcher's boundary value problem [8, Prob lem 1]. 8 FLETCHCR 100 Fletcher's chained Rosenbrock function [8, Prob lem 2]. 9 PENALTY2 100 Second penalty problem <ref> [17, Problem 24] </ref>. 10 GENROSE 500 Generalized Rosenbrock function [18, Problem 5]. 11 BDQRTIC 1000 Quartic with a banded Hessian with band width=9 [5, Problem 61]. 12 BROYDN7D 1000 Seven diagonal variant of the Broyden tridiagonal system with a band away from diagonal [29]. 13 PENALTY1 1000 First penalty problem [17, <p> [17, Problem 24]. 10 GENROSE 500 Generalized Rosenbrock function [18, Problem 5]. 11 BDQRTIC 1000 Quartic with a banded Hessian with band width=9 [5, Problem 61]. 12 BROYDN7D 1000 Seven diagonal variant of the Broyden tridiagonal system with a band away from diagonal [29]. 13 PENALTY1 1000 First penalty problem <ref> [17, Problem 23] </ref>. 14 POWER 1000 Power problem by Oren [25]. 15 MSQRTALS 1024 The dense matrix square root problem by No-cedal and Liu (case 0) seen as a nonlinear equa tion problem [4, Problem 204]. 16 MSQRTBLS 1025 The dense matrix square root problem by No-cedal and Liu (case 1) <p> matrix square root problem by No-cedal and Liu (case 1) seen as a nonlinear equa tion problem [4, Problem 201]. 17 CRAGGLVY 5000 Extended Cragg & Levy problem [30, Prob lem 32]. 18 NONDQUAR 10000 Nondiagonal quartic test problem [5, Prob lem 57]. 19 POWELLSG 10000 Extended Powell singular function <ref> [17, Prob lem 13] </ref>. 20 SINQUAD 10000 Another function with nontrivial groups and rep etitious elements [12]. 21 SPMSRTLS 10000 Liu and Nocedal tridiagonal matrix square root problem [4, Problem 151]. 22 TRIDIA 10000 Shanno's TRIDIA quadratic tridiagonal problem [30, Problem 8]. Table 4.1 Test problem collection.
Reference: [18] <author> S. Nash, </author> <title> Newton-type minimization via the Lanczos process, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 21 (1984), </volume> <pages> pp. 770-788. </pages>
Reference-contexts: Chained Rosenbrock function [29]. 6 ERRINROS 50 Nonlinear problem similar to CHNROSNB [28]. 7 FLETCHBV 100 Fletcher's boundary value problem [8, Prob lem 1]. 8 FLETCHCR 100 Fletcher's chained Rosenbrock function [8, Prob lem 2]. 9 PENALTY2 100 Second penalty problem [17, Problem 24]. 10 GENROSE 500 Generalized Rosenbrock function <ref> [18, Problem 5] </ref>. 11 BDQRTIC 1000 Quartic with a banded Hessian with band width=9 [5, Problem 61]. 12 BROYDN7D 1000 Seven diagonal variant of the Broyden tridiagonal system with a band away from diagonal [29]. 13 PENALTY1 1000 First penalty problem [17, Problem 23]. 14 POWER 1000 Power problem by Oren
Reference: [19] <author> S. G. Nash and J. Nocedal, </author> <title> A numerical study of the limited memory BFGS method and the truncated-Newton method for large scale optimization, </title> <journal> SIAM J. Optimization, </journal> <volume> 1 (1991), </volume> <pages> pp. 358-372. </pages>
Reference-contexts: We are using L-BFGS as our basis for comparison. For information on the performance of L-BFGS see Liu and Nocedal [15] and Nash and Nocedal <ref> [19] </ref>. 4.4.2. Varying m iteratively: Algorithms 1-4. In typical implementations of L-BFGS, m is fixed throughout the iterations: once m updates have accumulated, m updates are always used. We considered the possibility of varying m iteratively, preserving finite termination on convex quadratics.
Reference: [20] <author> L. Nazareth, </author> <title> A relationship between BFGS and conjugate gradient algorithms and its implications for new algorithms, </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 16 (1979), </volume> <pages> pp. 794-800. </pages> <note> [21] , On the BFGS method. </note> <institution> Univ. of California, Berkeley, </institution> <year> 1981. </year>
Reference-contexts: Limited-memory quasi-Newton methods fall in between these extremes in terms of performance and storage. There are other methods that fall into the middle ground; for example, conjugate gradient methods such as those proposed by Shanno [27] and Nazareth <ref> [20] </ref>, the truncated-Newton method [24, 6] and the partitioned quasi-Newton method [13]. We have characterized which limited-memory quasi-Newton methods fitting a general form (2.1) have the property of producing conjugate search directions on convex quadratics.
Reference: [22] <author> J. Nocedal, </author> <title> Updating quasi-Newton matrices with limited storage, </title> <journal> Mathematics of Computation, </journal> <volume> 35 (1980), </volume> <pages> pp. </pages> <month> 773-782. </month> <title> [23] , Theory of algorithms for unconstrained optimization, </title> <booktitle> in Acta Numerica (1991), </booktitle> <publisher> Cam-bridge Univ. Press, </publisher> <year> 1992, </year> <pages> pp. 199-242. </pages>
Reference-contexts: In this section we characterize full-memory and limited-memory methods that terminate in n iterations on n-dimensional strictly convex quadratic minimization problems using exact line searches. Most full-memory versions of the methods we will discuss are known to terminate in n iterations. Limited-memory BFGS (L-BFGS) was shown by Nocedal <ref> [22] </ref> to terminate in n steps. The preconditioned conjugate gradient method, which can be cast as a limited-memory quasi-Newton method, is also known to terminate in n iterations; see, e.g., Luenberger [16]. Little else is known about termination of limited-memory methods. <p> Example. The (k + 1)st update for the conjugate gradient method with precon ditioner H 0 fits the general form (2.1) with fl k = 1; m k = 0; P k = I k k y k Example. The L-BFGS update, see Nocedal <ref> [22] </ref>, with limited-memory constant m can be written as H k+1 = V T k X V T s i s T s T V i+1;k ;(2.5) where m k = minfk + 1; mg and V ik = j=i I i i y i : L-BFGS fits the general form <p> In practice, however, one would never do that because it would take more memory than storing the BFGS matrix. Example. We will define limited-memory DFP (L-DFP). Our definition is consis tent with the definition of limited-memory BFGS given in Nocedal <ref> [22] </ref>. Let m 1 and let m k = minfk + 1; mg. <p> L-BFGS and its variations. We tried a number of variations to the standard L-BFGS algorithm. L-BFGS and these variations are described in this subsection and summarized in Table 4.2. 4.4.1. L-BFGS: Algorithm 0. The limited-memory BFGS update is given in (2.5) and described fully by Byrd, Nocedal and Schnabel <ref> [22] </ref>. Our implementation and the following description come essentially from [22]. Let H 0 be symmetric and positive definite and assume that the m k pairs fs i ; y i g k1 each satisfy s T i y i &gt; 0. <p> L-BFGS and these variations are described in this subsection and summarized in Table 4.2. 4.4.1. L-BFGS: Algorithm 0. The limited-memory BFGS update is given in (2.5) and described fully by Byrd, Nocedal and Schnabel <ref> [22] </ref>. Our implementation and the following description come essentially from [22]. Let H 0 be symmetric and positive definite and assume that the m k pairs fs i ; y i g k1 each satisfy s T i y i &gt; 0.
Reference: [24] <author> D. P. O'Leary, </author> <title> A discrete Newton algorithm for minimizing a function of many variables, </title> <journal> Mathematical Programming, </journal> <volume> 23 (1982), </volume> <pages> pp. 20-33. </pages>
Reference-contexts: Limited-memory quasi-Newton methods fall in between these extremes in terms of performance and storage. There are other methods that fall into the middle ground; for example, conjugate gradient methods such as those proposed by Shanno [27] and Nazareth [20], the truncated-Newton method <ref> [24, 6] </ref> and the partitioned quasi-Newton method [13]. We have characterized which limited-memory quasi-Newton methods fitting a general form (2.1) have the property of producing conjugate search directions on convex quadratics.
Reference: [25] <author> S. Oren, </author> <title> Self-scaling variable metric algorithms, Part II: implementation and experiments, </title> <booktitle> Management Science, 20 (1974), </booktitle> <pages> pp. 863-874. </pages> <note> Cited in [1, 2]. </note>
Reference-contexts: Problem 5]. 11 BDQRTIC 1000 Quartic with a banded Hessian with band width=9 [5, Problem 61]. 12 BROYDN7D 1000 Seven diagonal variant of the Broyden tridiagonal system with a band away from diagonal [29]. 13 PENALTY1 1000 First penalty problem [17, Problem 23]. 14 POWER 1000 Power problem by Oren <ref> [25] </ref>. 15 MSQRTALS 1024 The dense matrix square root problem by No-cedal and Liu (case 0) seen as a nonlinear equa tion problem [4, Problem 204]. 16 MSQRTBLS 1025 The dense matrix square root problem by No-cedal and Liu (case 1) seen as a nonlinear equa tion problem [4, Problem 201].
Reference: [26] <author> M. J. D. Powell, </author> <title> Quadratic termination properties of minimization algorithms I. Statement and discussion of results., </title> <journal> J. Inst. Maths Applics, </journal> <volume> 10 (1972), </volume> <pages> pp. 333-342. </pages>
Reference-contexts: In this section, we are concerned with methods that skip some updates in order to reduce the memory demands. We establish conditions under which finite termination is preserved but delayed for the Broyden Class. 3.1. Termination when Updates are Skipped. It was shown by Powell <ref> [26] </ref> that if we skip every other update and take direct prediction steps (i.e. steps of length one) in a Broyden class method, then the procedure will terminate in no more than 2n +1 iterations on an n-dimensional strictly convex quadratic function.
Reference: [27] <author> D. F. Shanno, </author> <title> Conjugate gradient methods with inexact line searches, </title> <journal> Math. of Oper. Res., </journal> <volume> 3 (1978), </volume> <pages> pp. 244-256. </pages>
Reference-contexts: Limited-memory quasi-Newton methods fall in between these extremes in terms of performance and storage. There are other methods that fall into the middle ground; for example, conjugate gradient methods such as those proposed by Shanno <ref> [27] </ref> and Nazareth [20], the truncated-Newton method [24, 6] and the partitioned quasi-Newton method [13]. We have characterized which limited-memory quasi-Newton methods fitting a general form (2.1) have the property of producing conjugate search directions on convex quadratics.
Reference: [28] <author> P. Toint, </author> <title> An error in specifying problem CHNROSNB. Cited in [1, 2]. [29] , Some numerical results using a sparse matrix updating formula in unconstrained optimization, </title> <journal> Mathematics of Computation, </journal> <volume> 32 (1978), </volume> <pages> pp. </pages> <month> 839-852. </month> <title> [30] , Test problems for partially separable optimization and results for the routine PSPMIN, </title> <type> Tech. Report 83/4, </type> <institution> Department of Mathematics, FUNDP, </institution> <address> Namur, Belgium, </address> <year> 1983. </year> <note> Cited in [1, 2, 3]. </note>
Reference-contexts: EXTROSNB 10 Extended Rosenbrock function (nonseparable version) [30, Problem 10]. 2 WATSONS 31 Watson problem [17, Problem 20]. 3 TOINTGOR 50 Toint's operations research problem [29]. 4 TOINTPSP 50 Toint's PSP operations research problem [29]. 5 CHNROSNB 50 Chained Rosenbrock function [29]. 6 ERRINROS 50 Nonlinear problem similar to CHNROSNB <ref> [28] </ref>. 7 FLETCHBV 100 Fletcher's boundary value problem [8, Prob lem 1]. 8 FLETCHCR 100 Fletcher's chained Rosenbrock function [8, Prob lem 2]. 9 PENALTY2 100 Second penalty problem [17, Problem 24]. 10 GENROSE 500 Generalized Rosenbrock function [18, Problem 5]. 11 BDQRTIC 1000 Quartic with a banded Hessian with band
References-found: 24

