URL: http://www.cs.nyu.edu/phd_students/archi/nips.ps
Refering-URL: http://www.cs.nyu.edu/phd_students/archi/research.html
Root-URL: http://www.cs.nyu.edu
Email: geiger@cs.nyu.edu  archi@cs.nyu.edu  ltm@cns.nyu.edu  
Title: Features as Sufficient Statistics  
Author: D. Geiger A. Rudra L. Maloney 
Address: New York University  New York University  New York University  
Affiliation: Department of Computer Science Courant Institute and Center for Neural Science  Department of Computer Science Courant Institute  Departments of Psychology and Neural Science  
Date: 1997  
Note: Adv. in Neural Information Processing Sys. 10, eds. M. Jordan, M. Kearns, S. Solla,  
Abstract: An image is often represented by a set of detected features. We get an enormous compression by representing images in this way. Furthermore, we get a representation which is little affected by small amounts of noise in the image. However, features are typically chosen in an ad hoc manner. We show how a good set of features can be obtained using sufficient statistics. The idea of sparse data representation naturally arises. We treat the 1-dimensional and 2-dimensional signal reconstruction problem to make our ideas concrete.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Berger and S. Della Pietra and V. </author> <title> Della Pietra "A Maximum Entropy Approach to Natural Language Processing" Computational Linguistics, </title> <booktitle> Vol.22 (1), </booktitle> <pages> pp 39-71, </pages> <year> 1996. </year>
Reference: [2] <author> T. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley Interscience, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Let (I) denote a feature vector derived from the the image I. Initially, we assume that (I) is a deterministic function of I. For any choice of random variables, X, Y , define <ref> [2] </ref> the mutual information of X and Y to be M (X; Y ) = X;Y P (X; Y )log P (X)P (Y ) . <p> The information about the environmental parameters contained in the image is then M (f ; I), while the information about the environmental parameters contained in the feature vector (I) is then M (f ; (I)). As a consequence of the data processing inequality <ref> [2] </ref>, M (f ; (I)) M (f ; I). A vector (I), of features is defined to be sufficient if the inequality above is an equality. We will use the terms feature and statistic interchangeably. <p> A vector (I), of features is defined to be sufficient if the inequality above is an equality. We will use the terms feature and statistic interchangeably. The definition of a sufficient feature vector above is then just the usual definition of a set of jointly sufficient statistics <ref> [2] </ref>. To summarize, a feature vector (I) captures all the information about the envi ronmental state parameters f precisely when it is sufficent. 1 Graded Sufficiency: A feature vector either is or is not sufficient. <p> Let us now consider both methods, the continuation one and the direct one to estimate the features. Continuation Method: Here we apply @s j P s [i; j] by computing H s [i; j], given by (5), straight forwardly. We use the Baum-Welch method <ref> [2] </ref> for Markov chains to exactly compute E P s [(f i I i ) 2 (f j I j ) 2 ], E P s [(f i I i ) 2 ], and E P s [(f j I j ) 2 ].
Reference: [3] <author> D. Geiger and J. E. Kogler. </author> <title> Scaling Images and Image Feature via the Renormaliza-tion Group. </title> <booktitle> In Proc. IEEE Conf. on Computer Vision & Pattern Recognition , New York, </booktitle> <address> NY, </address> <year> 1993. </year>
Reference-contexts: We can then approx imately compute (e.g., see <ref> [3] </ref>) P (f ij jI) Z 2 2 (f ij I ij ) 2 ij (f ij N where, analogously to the 1D case, we have 2K ij + i;jK ij i;jK K h;K K + iK;j ij iK;j i+K;j i+K;j i+K;j where K i;j = K h;K v;K h;K v;K
Reference: [4] <author> G. Hinton and Z. Ghahramani. </author> <title> Generative Models for Discovering Sparse Distributed Representations To Appear Phil. </title> <journal> Trans. of the Royal Society B, </journal> <year> 1997. </year>
Reference-contexts: We are interested in estimating the parameters f of the environmental model given the image (compare <ref> [4] </ref>). We assume in the sequel that f , the environmental parameters, are themselves a random vector with known prior distribution. Let (I) denote a feature vector derived from the the image I. Initially, we assume that (I) is a deterministic function of I.
Reference: [5] <author> R. Linsker. </author> <title> Self-Organization in a Perceptual Network. </title> <booktitle> Computer, </booktitle> <month> March </month> <year> 1988, </year> <pages> 105-117. </pages>
Reference-contexts: Let us 1 An information-theoretic framework has been adopted in neural networks by others; e.g., <ref> [5] </ref> [9][6] [1][8]. However, the connection between features and sufficiency is new. 2 We won't prove the result here.
Reference: [6] <author> J. Principe, U. </author> <title> of Florida at Gainesville Personal Communication </title>
Reference: [7] <author> T. Sejnowski. </author> <title> Computational Models and the Development of Topographic Projections Trends Neurosci, </title> <booktitle> 10, </booktitle> <pages> 304-305. </pages>
Reference: [8] <author> S.C. Zhu, Y.N. Wu, D. Mumford. </author> <title> Minimax entropy principle and its application to texture modeling Neural Computation 1996 B. </title>
Reference: [9] <author> P. Viola and W.M. Wells III. </author> <title> "Alignment by Maximization of Mutual Information". </title> <booktitle> In Proceedings of the International Conference on Computer Vision. </booktitle> <address> Boston. </address> <year> 1995. </year>
References-found: 9

