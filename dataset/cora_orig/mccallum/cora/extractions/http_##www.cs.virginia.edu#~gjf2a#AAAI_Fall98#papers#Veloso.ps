URL: http://www.cs.virginia.edu/~gjf2a/AAAI_Fall98/papers/Veloso.ps
Refering-URL: http://www.cs.virginia.edu/~gjf2a/AAAI_Fall98/sched.html
Root-URL: http://www.cs.virginia.edu
Email: ex04@iau.dtu.dk  mmv@cs.cmu.edu  
Title: Interleaving Deliberative and Reactive Planning in Dynamic Multi-Agent Domains  
Author: Rune M. Jensen Manuela M. Veloso 
Address: 2800 Lyngby, Denmark  Pittsburgh, PA 15213-3891, U.S.A.  
Affiliation: Department of Automation Technical University of Denmark  Computer Science Department Carnegie Mellon University  
Abstract: Reactive planning, consisting of pre-defined sensor-action rules, is well suited to effectively respond to dynamic changes in real-time environments. However, it is in general challenging to strategically reason about long or short-term objectives using reactive planning. Therefore, ideally, deliberative and reactive planning should be integrated. In this paper, we introduce an adaptive interleaving of deliberative and reactive planning as our approach for dealing with real-time dynamic environments. Two main aspects are responsible for the success of the approach. First, the deliberative planner uses depth-bounded forward chaining guided by goal-based heuristics. Second, the real-time state space is discretized as a function of the average time that the deliberative planner needs to generate a plan. This ensures that the state, as seen by the deliberative planner, does not change in average while the plan is being generated. When a plan fails or a new plan is needed, the reactive planner takes over. We extend our approach to multi-agent real-time domains, where the need for collaborative deliberative planning is particularly needed. We implemented and demonstrate our integration using the Prodigy deliberative planner and the RoboCup soc cer simulator server.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. F. Bersano-Begey, P. G. Kenny, and E. H. Durfee. </author> <title> Multi-agent teamwork, adaptive learning, and adversarial planning in robocup using PRS architecture. </title> <note> submitted to the Robocup 97 Workshop, </note> <year> 1997. </year>
Reference-contexts: If some important new fact or request becomes known, PRS will reassess its goals, and perhaps choose to skip the current active KA and work on another one. Recently PRS has been applied to the RoboSoc-cer simulator domain <ref> [1] </ref>. Here KAs takes the form of simple plays among several agents (e.g. restart plays). In this way agents are capable of behaving deliberatively and perform actions not reactively optimal but efficient in the overall game (e.g., making a backward pass to confuse an opponent). <p> To raise the probability that the generated plan is applicable the new state is based on the assumption that the old plan succeeds. As noted in <ref> [1] </ref> Plan received Situation demand _ Execution failure :Situation demand [Send state] Reactive Control Deliberative Control planning for single-agent domains. an environmental demand can be both positive and negative. <p> We introduce a planner-dependent approach for constructing a state abstrac tion used for the deliberative planning. Further, we describe adaptive interleaving of reactive and deliberative planning, that in contrast to other previous approaches (e.g. <ref> [6, 1, 10] </ref>) builds on different representations of the domain. The paper contributes to the current discussion on how to generate plans in dynamic domains by proposing frequent depth bounded re-planning as a means for integrating dynamic changes.
Reference: [2] <author> R. A. Brooks. </author> <title> A robust layered control system for a mobile robot. </title> <type> Technical Report 864, </type> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <year> 1985. </year>
Reference-contexts: Activity of agents during plan generation in these domains is crucial. Moreover, the agents must be able to respond instantly to changes in the environment during plan execution. Architectures for addressing these problems have 2 been proposed by Lansky's Procedural Reasoning Sys--tem (PRS) [6] and Brooks' subsumption architecture <ref> [2] </ref>. The subsumption architecture decomposes the behavior problem into task-achieving units realizing distinct behaviors of the agent separately. The units are ordered hierarchically with more complex behavior inhibiting simpler default behavior according to environmental demands. The subsumption architecture has proved to be efficient for implementing highly reactive behavior.
Reference: [3] <author> J. G. Carbonell, J. Blythe, O. Etzioni, Y. Gil, R. Joseph, D. Kahn, C. Knoblock, S. Minton, A. Perez, S. Reilly, M. M. Veloso, and X. Wang. PRODIGY4.0: </author> <title> The manual and tutorial. </title> <type> Technical Report CMU-CS-92-150, </type> <institution> Department of 9 Computer Science, Carnegie Mellon University, </institution> <year> 1992. </year>
Reference-contexts: communicates this to the other agents, which then try to enter a deliberative phase (see Figure 4). 5 Real-time Dynamic Multi agent Domain: Robotic Soc cer Simulator Our example implementation uses an artificial environment provided by the RoboCup soccer server [11] and multi-agent plans generated by the nonlinear planner PRODIGY4.0 <ref> [3] </ref>. Each agent represents a player and is implemented as a separate program, that communicates with the soccer server and PRODIGY. The agent program generates an action command whenever it receives visual information send by the soccer server every 100 ms.
Reference: [4] <author> E. H. Durfee and V. R. Lesser. </author> <title> Partial global planning: A coordination framework for distributed hypothesis formation. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 21(5) </volume> <pages> 1167-1183, </pages> <month> September </month> <year> 1991. </year> <title> Special Issue on Distributed Sensor Networks. </title>
Reference-contexts: Current research in this field mainly deals with how to represent and share multi-agent plans in order to make distributed plan generation and negotiate which plan to follow <ref> [13, 4, 17] </ref>. We use one central planning system, which all agents connect to. The resulting architecture is shown in Figure 3. The multi-agent plan generation consists of three steps: 1.
Reference: [5] <author> R. E. Fikes and N. J. Nilsson. </author> <title> STRIPS: A new approach to the application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 189-208, </pages> <year> 1971. </year>
Reference-contexts: Because this gives a one (current location) to many (several possible goal positions) search, we use forward chaining planning. The operators used by the planner are classical STRIPS operators <ref> [5] </ref> and we use the Prodigy planner [15]. As the planning progresses we reason about the uncertainty of the plan being generated. The dynamic changes in the state cause regions of uncertainty to grow in the state space, that in turn inhibits further consistent planning.
Reference: [6] <author> M. P. Georgeff and A. L. Lansky. </author> <title> Rective reasoning and planning. </title> <booktitle> In Proceedings of AAAI, </booktitle> <pages> pages 677-682, </pages> <year> 1987. </year>
Reference-contexts: Activity of agents during plan generation in these domains is crucial. Moreover, the agents must be able to respond instantly to changes in the environment during plan execution. Architectures for addressing these problems have 2 been proposed by Lansky's Procedural Reasoning Sys--tem (PRS) <ref> [6] </ref> and Brooks' subsumption architecture [2]. The subsumption architecture decomposes the behavior problem into task-achieving units realizing distinct behaviors of the agent separately. The units are ordered hierarchically with more complex behavior inhibiting simpler default behavior according to environmental demands. <p> We introduce a planner-dependent approach for constructing a state abstrac tion used for the deliberative planning. Further, we describe adaptive interleaving of reactive and deliberative planning, that in contrast to other previous approaches (e.g. <ref> [6, 1, 10] </ref>) builds on different representations of the domain. The paper contributes to the current discussion on how to generate plans in dynamic domains by proposing frequent depth bounded re-planning as a means for integrating dynamic changes.
Reference: [7] <author> K. Z. Haigh. </author> <title> Situation-Dependent Learning for Interleaved Planning and Robot Execution. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> February </month> <year> 1998. </year> <month> CMU-CS-98-108. </month>
Reference-contexts: Most existing architectures for integrating deliberative planning in dynamic environments consist of a plan constructor and a plan executer interleaving passive planning phases and active plan execution phases. Recent systems in this category is the ROGUE architecture <ref> [7, 8] </ref> that integrates and learns from dynamic changes in the environment, and a system developed by Nourbakhsh [12] that uses conditional planning for dealing with environmental uncertainty. As noted in the introduction, these approaches faces problems in domains with a high frequency of dynamic changes.
Reference: [8] <author> K. Z. Haigh and M. M. Veloso. </author> <title> Planning, execution and learning in a robotic agent. </title> <editor> In R. Sim-mons, M. Veloso, and S. Smith, editors, </editor> <booktitle> Proceedings Fourth International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> pages 120-127. </pages> <address> AIPS'98, </address> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1998. </year>
Reference-contexts: Most existing architectures for integrating deliberative planning in dynamic environments consist of a plan constructor and a plan executer interleaving passive planning phases and active plan execution phases. Recent systems in this category is the ROGUE architecture <ref> [7, 8] </ref> that integrates and learns from dynamic changes in the environment, and a system developed by Nourbakhsh [12] that uses conditional planning for dealing with environmental uncertainty. As noted in the introduction, these approaches faces problems in domains with a high frequency of dynamic changes.
Reference: [9] <author> S. Kurihara, S. Aoyago, and R. Onai. </author> <title> Adaptive selection of reactive/deliberate planning for the dynamic environment. </title> <editor> In M. Boman and W. Velde, editors, </editor> <booktitle> Multi-Agent Rationality, volume 1237 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 112-127. </pages> <booktitle> 8th European Workshop on Modelling Autonomous Agents in a Multi-Agent World, </booktitle> <address> MAAMAW'97, </address> <year> 1997. </year>
Reference-contexts: In this paper we present such an approach allowing the agent to execute plans when the environmental demand for response is low, and degenerate to reactive behavior when the environmental demand is high or a failure occurs. Our approach has been inspired by MRR-planning introduced in <ref> [9] </ref>. In MRR-planning reactive and classical planners compete in each cycle for choosing the next action of the agent. If the response demand is low the action selection mechanism waits until all planners have finished processing making the agent behavior deliberative.
Reference: [10] <author> V. Matellan and D. </author> <title> Borrajo. Combining classical and reactive planning: the ABC 2 model. </title> <editor> In R. Bergmann and A. Kott, editors, </editor> <booktitle> AIPS'98 Workshop: Integrating Planning, Scheduling and Execution in Dynamic and Uncertain Environments, </booktitle> <pages> pages 121-126. </pages> <address> AIPS'98, </address> <publisher> The AAAI Press, </publisher> <month> June </month> <year> 1998. </year>
Reference-contexts: Agents should be able to develop new plays exploring the current situation using the available actions. In our approach the purpose of the depth bounded forward chaining planning is just to do that. The Agenda Based Control for Agents Behaviors Coordination model (ABC 2 ) <ref> [10] </ref> also addresses the problem of combining classical and reactive planning. This model resembles PRS by having a central control process selecting acts from an agenda containing a set of acts currently under consideration. <p> We introduce a planner-dependent approach for constructing a state abstrac tion used for the deliberative planning. Further, we describe adaptive interleaving of reactive and deliberative planning, that in contrast to other previous approaches (e.g. <ref> [6, 1, 10] </ref>) builds on different representations of the domain. The paper contributes to the current discussion on how to generate plans in dynamic domains by proposing frequent depth bounded re-planning as a means for integrating dynamic changes.
Reference: [11] <author> I. Noda. </author> <title> Soccer server: a simulator of robocup. </title> <booktitle> In Proceedings of AI symposium '95, </booktitle> <pages> pages 29-34. </pages> <booktitle> Japanese Society for Artificial Intelligence, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: In contrast to MRR-planning we further generalize our approach to multi-agent domains making collaboration between the agents an important goal of the deliberative planning. As an example domain, the RoboCup robotic soccer simulator <ref> [11] </ref> is used as a very challenging real-time dynamic environment. We further use the Prodigy deliberative planner [15]. The paper is organized as follows: section 2 discusses previous reasoning systems dealing with dynamic environments. <p> hand, when an agent finds the environment low demanding it communicates this to the other agents, which then try to enter a deliberative phase (see Figure 4). 5 Real-time Dynamic Multi agent Domain: Robotic Soc cer Simulator Our example implementation uses an artificial environment provided by the RoboCup soccer server <ref> [11] </ref> and multi-agent plans generated by the nonlinear planner PRODIGY4.0 [3]. Each agent represents a player and is implemented as a separate program, that communicates with the soccer server and PRODIGY.
Reference: [12] <author> I. Nourbakhsh. </author> <title> Interleaving Planning and Execution for Autonomous Robots. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stanford University, Stanford, </institution> <address> CA, </address> <year> 1997. </year> <month> STAN-CS-TR-97-1593. </month>
Reference-contexts: Recent systems in this category is the ROGUE architecture [7, 8] that integrates and learns from dynamic changes in the environment, and a system developed by Nourbakhsh <ref> [12] </ref> that uses conditional planning for dealing with environmental uncertainty. As noted in the introduction, these approaches faces problems in domains with a high frequency of dynamic changes. Consider for example a flight control or a battle field scenario. Activity of agents during plan generation in these domains is crucial.
Reference: [13] <author> A. E. F. Seghrouchni and S. Haddad. </author> <title> A recursive model for distributed planning. </title> <editor> In M. Tokoro, editor, </editor> <booktitle> Proceedings Second International Conference on Multi-Agent Systems, </booktitle> <pages> pages 307-314. </pages> <address> ICMAS-96, </address> <publisher> AAAI Press, </publisher> <year> 1996. </year>
Reference-contexts: Current research in this field mainly deals with how to represent and share multi-agent plans in order to make distributed plan generation and negotiate which plan to follow <ref> [13, 4, 17] </ref>. We use one central planning system, which all agents connect to. The resulting architecture is shown in Figure 3. The multi-agent plan generation consists of three steps: 1.
Reference: [14] <author> P. Stone and M. M. Veloso. </author> <title> Task decomposition and dynamic role assignment for real-time strategic teamwork. </title> <booktitle> Third International Conference on Multi-Agent Systems (ICMAS'98), </booktitle> <year> 1998. </year>
Reference-contexts: Unfortunately ABC 2 like PRS does not reason about future states. When applied to the RoboSoccer simulator domain this approach thus has much in common with elaborate reactive approaches, where collaboration is handled by role based formations <ref> [14] </ref>. <p> The reactive planning used in the multi-agent extension of the architecture is similar to the 5 reactive planning used in the single-agent case. As argued by <ref> [14] </ref> roles are assigned to agents to enable efficient collaboration. The last modification of the multi-agent extension concerns the interleaving of the deliberative and reactive planning. Due to synchronization of the agents during a deliberative phase the agents must enter this phase at the same time.
Reference: [15] <author> M. M. Veloso, J. Carbonell, M. A. Perez, D. Bor-rajo, E. Fink, and J. Blythe. </author> <title> Integrating planning and learning: The PRODIGY architecture. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 7(1) </volume> <pages> 81-120, </pages> <year> 1995. </year>
Reference-contexts: As an example domain, the RoboCup robotic soccer simulator [11] is used as a very challenging real-time dynamic environment. We further use the Prodigy deliberative planner <ref> [15] </ref>. The paper is organized as follows: section 2 discusses previous reasoning systems dealing with dynamic environments. In section 3 we introduce interleaving of reactive and deliberative planning for single agents in dynamic environments. <p> Because this gives a one (current location) to many (several possible goal positions) search, we use forward chaining planning. The operators used by the planner are classical STRIPS operators [5] and we use the Prodigy planner <ref> [15] </ref>. As the planning progresses we reason about the uncertainty of the plan being generated. The dynamic changes in the state cause regions of uncertainty to grow in the state space, that in turn inhibits further consistent planning.
Reference: [16] <author> M. M. Veloso, M. E. Pollack, and M. T. Cox. </author> <title> Rationale-based monitoring for planning in dynamic domains. </title> <editor> In R. Simmons, M. Veloso, and S. Smith, editors, </editor> <booktitle> Proceedings Fourth International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> pages 171-179. </pages> <address> AIPS'98, </address> <publisher> AAAI Press, </publisher> <month> June </month> <year> 1998. </year>
Reference-contexts: We find this approach suitable for highly dynamic real-time domains like the soccer domain or a flight control domain. For domains with a low frequency of dynamic changes the price of re-planning may be too high. In these domains plan monitoring and elaboration may be more feasible <ref> [16] </ref>. Acknowledgments This work was carried out while the first author was visiting Carnegie Mellon University. This research is sponsored in part by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number F30602-97-2-0250.
Reference: [17] <author> M. Wooldridge and N. R. Jennings. </author> <title> Towards a theory of cooperative problem solving. </title> <editor> In J. W. Perram and J. Muller, editors, </editor> <booktitle> Distributed Software Agents and Applications, volume 1069 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 40-53. </pages> <booktitle> 6th European Workshop on Modelling Autonomous Agents in a Multi-Agent World, </booktitle> <address> MAAMAW'94, </address> <publisher> Springer-Verlag, </publisher> <year> 1994. </year> <month> 10 </month>
Reference-contexts: Current research in this field mainly deals with how to represent and share multi-agent plans in order to make distributed plan generation and negotiate which plan to follow <ref> [13, 4, 17] </ref>. We use one central planning system, which all agents connect to. The resulting architecture is shown in Figure 3. The multi-agent plan generation consists of three steps: 1.
References-found: 17

