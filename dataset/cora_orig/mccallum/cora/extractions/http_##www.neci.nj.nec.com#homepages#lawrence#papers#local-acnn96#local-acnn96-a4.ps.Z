URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/local-acnn96/local-acnn96-a4.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/lawrence/papers/local-acnn96/
Root-URL: http://www.neci.nj.nec.com
Email: flawrence,act,backg@elec.uq.edu.au  
Title: Function Approximation with Neural Networks and Local Methods: Bias, Variance and Smoothness  
Author: Peter Bartlett, Anthony Burkitt, and Robert Williamson, Steve Lawrence Ah Chung Tsoi, Andrew D. Back 
Address: St. Lucia 4072 Australia  
Affiliation: Department of Electrical and Computer Engineering University of Queensland,  
Note: Appears in Australian Conference on Neural Networks, ACNN 96, Edited by  Australian National University, pp. 1621, 1996.  
Abstract: We review the use of global and local methods for estimating a function mapping R m ) R n from samples of the function containing noise. The relationship between the methods is examined and an empirical comparison is performed using the multi-layer perceptron (MLP) global neural network model, the single nearest-neighbour model, a linear local approximation (LA) model, and the following commonly used datasets: the Mackey-Glass chaotic time series, the Sunspot time series, British English Vowel data, TIMIT speech phonemes, building energy prediction data, and the sonar dataset. We find that the simple local approximation models often outperform the MLP. No criterion such as classification/prediction, size of the training set, dimensionality of the training set, etc. can be used to distinguish whether the MLP or the local approximation method will be superior. However, we find that if we consider histograms of the k-NN density estimates for the training datasets then we can choose the best performing method a priori by selecting local approximation when the spread of the density histogram is large and choosing the MLP otherwise. This result correlates with the hypothesis that the global MLP model is less appropriate when the characteristics of the function to be approximated varies throughout the input space. We discuss the results, the smoothness assumption often made in function approximation, and the bias/variance dilemma. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. </author> <title> Arya and D.M. Mount. Algorithms for fast vector quantization. </title> <editor> In J. A. Storer and M. Cohn, editors, </editor> <booktitle> Proceedings of DCC 93: Data Compression Conference, </booktitle> <pages> pages 381390. </pages> <publisher> IEEE Press, </publisher> <year> 1993. </year>
Reference-contexts: Bentley gives an algorithm for finding m nearest neighbours in k-dimensional space requiring log 2 n nodes to be visited and approximately m2 k distance calculations [12]. The K-D tree is known to scale poorly in high dimensions significant improvements can be found with approximate nearest neighbour techniques <ref> [1] </ref>. * Memory requirements. This problem can be partially ad dressed by removing unneccesary training data from regions with little uncertainty. Determining the optimal number of neighbours to use is difficult because the answer usually depends on the location in the input space.
Reference: [2] <author> J.L. Bentley. </author> <title> Multidimensional binary search trees used for associative searching. </title> <journal> Communications of the ACM, </journal> <volume> 18(9):509517, </volume> <year> 1975. </year>
Reference-contexts: The most straightforward approach to finding nearest-neighbours is to compute the distance to each point which is an O (N ) solution. This can be reduced to O (logN ) by using a decision tree. The K-D tree is a popular decision tree introduced by Bentley <ref> [2] </ref>, which is a gener-alisation of a binary tree to the case of k keys.
Reference: [3] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: Increasing the number of neighbours can compromise the local validity of a model (eg. approximating a curved manifold with a linear plane) and increase the bias of results. This is the classic bias/variance dilemma [25] which a designer often faces. Algorithms such as: classification and regression trees (CART) <ref> [3] </ref>, multivariate adaptive regression splines (MARS) [11], ID3 [23], and the hierarchical mixtures of experts (HME) algorithm of Jordan and Jacobs [18], are local approximation models where the input space is divided, at training time, into a hierarchy of regions where simple surfaces are fit to the local data. 3.1 Interpolation
Reference: [4] <author> M. Casdagli. </author> <title> Chaos and deterministic versus stochastic non-linear mod-elling. J.R. </title> <journal> Statistical Society B, </journal> <volume> 54(2):302328, </volume> <year> 1991. </year>
Reference-contexts: Linear models the group of nearest-neighbours is used to create a local linear model. This model is then used to find the desired value for a new point. One example, which we use later, is given by Casdagli <ref> [4] </ref>: y = 1 x 1 : : : x k 2 6 4 ff 1 ff k 7 7 (1) where ff i , i = 0; 1; : : : are constants, k is the number of neighbours, and n is the dimension of the input vector x i
Reference: [5] <author> G. Cybenko. </author> <title> Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals, and Systems, </title> <address> 2:303314, </address> <year> 1989. </year>
Reference-contexts: 1: Global and local function approximation methods. 2 Neural Networks It has been shown that an MLP neural network, with a single hidden layer, can approximate any given continuous function on any compact subset to any degree of accuracy, providing that a sufficient number of hidden layer neurons is used <ref> [5, 17] </ref>. However, in practice, the number of hidden layer neurons required may be impractically large. In addition, the training algorithms are plagued by the possible existence of many local minima or flat spots on the error surface.
Reference: [6] <author> C. Darken and J.E. Moody. </author> <title> Note on learning rate schedules for stochastic optimization. </title> <editor> In R.P. Lippmann, J.E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 832838. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: For the multilayer perceptron we used the tanh activation function, and a search then converge learning rate schedule <ref> [6] </ref>: j = j 0 N=2 + c 1 (1c 2 )N where j = learning rate, j 0 = initial learning rate = 0.1, N = total training epochs, n = current training epoch, c 1 = 50, c 2 = 0:65. Target outputs were scaled by 0.8.
Reference: [7] <author> S.A. Dudani. </author> <title> The distance-weighted k-nearest-neighbor rule. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, SMC-6, </journal> <volume> 4:325327, </volume> <year> 1976. </year>
Reference-contexts: No interpolation the test output is equal to the closest training point output. 2. Weighted average the output for the test point is equal to the average of the output of the k nearest-neighbours. This average is usually weighted inversely by the distance from the test point <ref> [7] </ref>. Weighted averages have been used extensively (eg. [26]) and analysed extensively (eg. [15]). 3. Linear models the group of nearest-neighbours is used to create a local linear model. This model is then used to find the desired value for a new point.
Reference: [8] <author> D. Farmer and J. Sidorowich. </author> <title> Exploiting chaos to predict the future and reduce noise. </title> <editor> In W.C. Lee, editor, </editor> <title> Evolution, Learning, </title> <journal> and Cognition, </journal> <pages> pages 277330. </pages> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1988. </year>
Reference-contexts: The smoothness assumption is required because the problem of function approximation (especially from sparse data) is ill-posed and must be constrained. Function approximation methods fall into two broad categories: global and local. Global approximations can be made with many different function representations, eg. polynomials, rational approximation, and multi-layer perceptrons <ref> [8] </ref>. Often a single global model is inappropriate because it does not apply to the entire state space. To approximate a function f , a model must be able to represent its many possible variations. <p> If f is complicated, there is no guarantee that any given representation will approximate f well. The dependence on representation can be reduced using local approximation where the domain of f is broken into local neighbourhoods and a separate model is used for each neighbourhood <ref> [8] </ref>. Different function representations can be used in both local and global models as shown in table 1. Global models Local models Linear None Polynomial Weighted average Splines Linear Neural networks Polynomial . . . Splines Neural networks . . . <p> The parameters ff i , i = 0; 1; : : : ; k are found by using ordinary least squares. Weighted regressions to fit local polynomial models have been used by many people for classification (eg. [20]). Recently, they have also become popular for approximation. Farmer and Sidorowich <ref> [8] </ref> have used local polynomial models for forecasting but have only had success using low order models. 4. Non-linear models. One example is splines. Mhaskar [21] has used tensor product b-splines for local approximation. Standard results in spline approximation theory can be used.
Reference: [9] <author> E. Fix and J.L. Hodges. </author> <title> Discriminatory analysis nonparametric discrimination: Consistency properties. </title> <type> Technical Report Project 21-49-004, Report No. 4, 261-279, </type> <institution> USAF School of Aviation Medicine, Randolph Field, Texas, </institution> <year> 1951. </year>
Reference-contexts: The networks suffer from the curse of dimensionality. 3 Local Approximation Local approximation is based on nearest-neighbour techniques. An early use of nearest-neighbours was in the field of pattern classification. Fix and Hodges <ref> [9] </ref> classified new patterns by searching for a similar pattern in a stored set and using the classification of the retrieved pattern as the classification of the new one. Many papers thereafter suggested new rules for the classification of a point based on its nearest-neighbours (weighted averages, etc.).
Reference: [10] <author> Peter V. Foukal. </author> <title> The variable sun. </title> <publisher> Scientific American, </publisher> <address> 262:34, </address> <year> 1990. </year>
Reference-contexts: There are 528 training patterns and 462 test patterns. 3. Sunspot data. Sunspots are dark blotches on the sun and yearly averages have been recorded since 1700 <ref> [10] </ref>. In this example, 220 samples are used for training, and the remaining 60 for testing. 4. Sonar data.
Reference: [11] <author> J.H. Friedman. </author> <title> Multivariate Adaptive Regression Splines. </title> <journal> Annals of Statistics, </journal> <volume> 19(1):1141, </volume> <year> 1991. </year>
Reference-contexts: This is the classic bias/variance dilemma [25] which a designer often faces. Algorithms such as: classification and regression trees (CART) [3], multivariate adaptive regression splines (MARS) <ref> [11] </ref>, ID3 [23], and the hierarchical mixtures of experts (HME) algorithm of Jordan and Jacobs [18], are local approximation models where the input space is divided, at training time, into a hierarchy of regions where simple surfaces are fit to the local data. 3.1 Interpolation The type of local model used
Reference: [12] <author> J.H. Friedman, J.L. Bentley, and R.A. Finkel. </author> <title> An algorithm for finding best matches in logarithmic time. </title> <type> Technical Report 75-482, </type> <institution> Stanford CS, </institution> <year> 1975. </year>
Reference-contexts: Bentley gives an algorithm for finding m nearest neighbours in k-dimensional space requiring log 2 n nodes to be visited and approximately m2 k distance calculations <ref> [12] </ref>. The K-D tree is known to scale poorly in high dimensions significant improvements can be found with approximate nearest neighbour techniques [1]. * Memory requirements. This problem can be partially ad dressed by removing unneccesary training data from regions with little uncertainty.
Reference: [13] <author> K. Fukunaga. </author> <title> Introduction to Statistical Pattern Recognition, Second Edition. </title> <publisher> Academic Press, </publisher> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: Furthermore, it is harder for a global MLP model to adapt to differing density throughout the input space than it is for an LA model. Figure 2 shows k-NN density estimates 4 <ref> [13] </ref> of the training datasets for k = 3. The estimates have been normalised so the median (a statistic insensitive to outliers) estimate is 1.
Reference: [14] <author> S. Geman, E. Bienenstock, and R. Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <booktitle> Neural Computation, </booktitle> <address> 4(1):158, </address> <year> 1992. </year>
Reference-contexts: We also note that the size of the local neighorhood and the number of hidden nodes in the best models is correlated with the size of the training data sets. 6 Discussion and Open Issues The bias/variance dilemma, which has been well covered in the literature (eg. <ref> [14] </ref>), can help explain the varying results of the different methods.
Reference: [15] <author> W. J. Gordon and J.A. Wixom. </author> <title> Shepards method of metric interpolation to bivariate and multivariate interpolation. </title> <journal> Mathematics of Computation, </journal> <volume> 32 (141):253264, </volume> <year> 1978. </year>
Reference-contexts: Weighted average the output for the test point is equal to the average of the output of the k nearest-neighbours. This average is usually weighted inversely by the distance from the test point [7]. Weighted averages have been used extensively (eg. [26]) and analysed extensively (eg. <ref> [15] </ref>). 3. Linear models the group of nearest-neighbours is used to create a local linear model. This model is then used to find the desired value for a new point.
Reference: [16] <author> R.P. Gorman and T.J. Sejnowski. </author> <title> Analysis of hidden units in a layered network trained to classify sonar targets. Neural Networks, </title> <address> 1:7589, </address> <year> 1988. </year>
Reference-contexts: In this example, 220 samples are used for training, and the remaining 60 for testing. 4. Sonar data. Discrimination between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock, as used in <ref> [16] </ref>. 103 patterns have been used in both the training and test sets. 5. Speech data. This dataset consists of the phoneme aa extracted from the TIMIT database and arranged as a number of sequences: prior phoneme (s), current phoneme, next phoneme (s).
Reference: [17] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Multilayer feedforward networks are universal approximators. Neural Networks, </title> <address> 2:359366, </address> <year> 1989. </year>
Reference-contexts: 1: Global and local function approximation methods. 2 Neural Networks It has been shown that an MLP neural network, with a single hidden layer, can approximate any given continuous function on any compact subset to any degree of accuracy, providing that a sufficient number of hidden layer neurons is used <ref> [5, 17] </ref>. However, in practice, the number of hidden layer neurons required may be impractically large. In addition, the training algorithms are plagued by the possible existence of many local minima or flat spots on the error surface.
Reference: [18] <author> M.I. Jordan and R.A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <booktitle> Neural Computation, </booktitle> <address> 6:181214, </address> <year> 1994. </year>
Reference-contexts: This is the classic bias/variance dilemma [25] which a designer often faces. Algorithms such as: classification and regression trees (CART) [3], multivariate adaptive regression splines (MARS) [11], ID3 [23], and the hierarchical mixtures of experts (HME) algorithm of Jordan and Jacobs <ref> [18] </ref>, are local approximation models where the input space is divided, at training time, into a hierarchy of regions where simple surfaces are fit to the local data. 3.1 Interpolation The type of local model used controls the method of interpolation between the neighbours. <p> Each MLP result is followed by the standard deviation. input space are the most important points for decreasing the variance). Local approximation algorithms generally tend to be variance increasing algorithms which is particularly problematic in high-dimensional spaces where data becomes exceedingly sparse <ref> [18] </ref>. The bias/variance tradeoff can be controlled in the multi-layer perceptron using various methods including controlling the number of hidden nodes (and hence the representational power of the network), and weight elimination.
Reference: [19] <author> M.C. Mackey and L. Glass. </author> <title> Oscillation and chaos in physiological control systems. </title> <booktitle> Science, </booktitle> <address> 197:287, </address> <year> 1977. </year>
Reference-contexts: The Mackay-Glass equation. The Mackey-Glass equation is a time delay differential equation first proposed as a model of white blood cell production <ref> [19] </ref>: dx dt = [1+x c (to )] bx (t) where the constants are commonly chosen as a = 0:2, b = 0:1 and c = 10. The delay parameter o determines the behaviour of the system. For o &gt; 16:8 the system produces a chaotic attractor.
Reference: [20] <author> D.H. McLain. </author> <title> Drawing contours from arbitrary data points. </title> <journal> The Computer Journal, </journal> <volume> 17(4):318324, </volume> <year> 1974. </year>
Reference-contexts: The parameters ff i , i = 0; 1; : : : ; k are found by using ordinary least squares. Weighted regressions to fit local polynomial models have been used by many people for classification (eg. <ref> [20] </ref>). Recently, they have also become popular for approximation. Farmer and Sidorowich [8] have used local polynomial models for forecasting but have only had success using low order models. 4. Non-linear models. One example is splines. Mhaskar [21] has used tensor product b-splines for local approximation.
Reference: [21] <author> H.N. Mhaskar. </author> <title> Neural networks for localized approximation of real functions. In C.A. </title> <editor> Kamm et al., editor, </editor> <booktitle> Neural Networks for Signal Processing III: Proceedings of the 1993 IEEE Workshop. IEEE, </booktitle> <year> 1993. </year>
Reference-contexts: Recently, they have also become popular for approximation. Farmer and Sidorowich [8] have used local polynomial models for forecasting but have only had success using low order models. 4. Non-linear models. One example is splines. Mhaskar <ref> [21] </ref> has used tensor product b-splines for local approximation. Standard results in spline approximation theory can be used.
Reference: [22] <author> T. Poggio, F. Girosi, and M. Jones. </author> <title> From regularization to radial, tensor and additive splines. In C.A. </title> <editor> Kamm et al., editor, </editor> <booktitle> Neural Networks for Signal Processing III: Proceedings of the 1993 IEEE Workshop, </booktitle> <pages> pages 310. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction The problem of learning by example can be considered equivalent to a multivariate function approximation problem in many cases <ref> [22] </ref>, ie. find a mapping R m ) R n given a set of example points.
Reference: [23] <author> Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: This is the classic bias/variance dilemma [25] which a designer often faces. Algorithms such as: classification and regression trees (CART) [3], multivariate adaptive regression splines (MARS) [11], ID3 <ref> [23] </ref>, and the hierarchical mixtures of experts (HME) algorithm of Jordan and Jacobs [18], are local approximation models where the input space is divided, at training time, into a hierarchy of regions where simple surfaces are fit to the local data. 3.1 Interpolation The type of local model used controls the
Reference: [24] <author> A.J. Robinson. </author> <title> Dynamic Error Propagation Networks. </title> <type> PhD thesis, </type> <institution> Cam-bridge University Engineering Department, </institution> <address> Cambridge, UK, </address> <month> February </month> <year> 1989. </year>
Reference-contexts: We have used o = 30. Our dataset consisted of 3000 training patterns and 500 test patterns. 2. Vowel data. Speaker independent recognition of the eleven steady state vowels of British English using a specified training set of 10 LPC derived log area ratios <ref> [24] </ref>. There are 528 training patterns and 462 test patterns. 3. Sunspot data. Sunspots are dark blotches on the sun and yearly averages have been recorded since 1700 [10]. In this example, 220 samples are used for training, and the remaining 60 for testing. 4. Sonar data.
Reference: [25] <author> T. Sauer. </author> <title> Time series prediction using delay coordinate embedding. In A.S. Weigend and N.A. Gershenfeld, editors, Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: Increasing the number of neighbours can compromise the local validity of a model (eg. approximating a curved manifold with a linear plane) and increase the bias of results. This is the classic bias/variance dilemma <ref> [25] </ref> which a designer often faces.
Reference: [26] <author> D. Shepard. </author> <title> A two-dimensional function for irregularly spaced data. </title> <booktitle> In Proceedings of the 23rd ACM National Conference, </booktitle> <pages> pages 517524, </pages> <year> 1968. </year>
Reference-contexts: Weighted average the output for the test point is equal to the average of the output of the k nearest-neighbours. This average is usually weighted inversely by the distance from the test point [7]. Weighted averages have been used extensively (eg. <ref> [26] </ref>) and analysed extensively (eg. [15]). 3. Linear models the group of nearest-neighbours is used to create a local linear model. This model is then used to find the desired value for a new point.
Reference: [27] <author> H. Tong and K.S. Lim. </author> <title> Threshold autoregression, limit cycles and cyclical data. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 42(3):245292, </volume> <year> 1980. </year>
Reference-contexts: Many papers thereafter suggested new rules for the classification of a point based on its nearest-neighbours (weighted averages, etc.). For function approximation, the threshold au-toregressive model of <ref> [27] </ref> is of some interest. The model effectively splits the state space in half and uses a separate linear model for each half.
Reference: [28] <author> D. Waltz. </author> <title> Memory-based reasoning. </title> <editor> In M. Arbib and J. Robinson, editors, </editor> <booktitle> Natural and Artificial Parallel Computation, </booktitle> <pages> pages 251276. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: However, it is known that MLPs do not respond well to isolated data points <ref> [28] </ref> (meaning points in the input space where the density of the training data is very low compared to other areas), hence we may expect the global MLP methods to be less appropriate if the training data has a lot of points where the density of the data is low.
Reference: [29] <author> D.F. Watson. Contouring: </author> <title> A Guide to the Analysis and Display of Spatial Data. </title> <publisher> Pergamon Press, </publisher> <year> 1992. </year> <month> 6 </month>
Reference-contexts: However, we generally do not have a differentiable function because this is what we are trying to estimate. Smoothness criteria for a dataset exist - eg. the roughness and outlier indexes of Watson <ref> [29] </ref>. However, these require the use of the natural neighbour order of the data. Natural neighbour order is based on calculation of the Voronoi tessellation which is impractical for dimensions greater than about 10.
References-found: 29

