URL: http://www.eecis.udel.edu:80/~samuel/work/papers/ltbl/samuel-1998b.ps
Refering-URL: http://www.eecis.udel.edu:80/~samuel/work/papers/ltbl/index.html
Root-URL: http://www.cis.udel.edu
Email: samuel@cis.udel.edu  
Title: Lazy Transformation-Based Learning  
Author: Ken Samuel 
Web: http://www.eecis.udel.edu/~samuel/  
Address: Newark, Delaware 19716 USA  
Affiliation: Department of Computer and Information Sciences University of Delaware  
Abstract: We introduce a significant improvement for a relatively new machine learning method called Transformation-Based Learning. By applying a Monte Carlo strategy to randomly sample from the space of rules, rather than exhaustively analyzing all possible rules, we drastically reduce the memory and time costs of the algorithm, without compromising accuracy on unseen data. This enables Transformation-Based Learning to apply to a wider range of domains, as it can effectively consider a larger number of different features and feature interactions in the data. In addition, the Monte Carlo improvement decreases the labor demands on the human developer, who no longer needs to develop a minimal set of rule templates to maintain tractability. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Brill, E., and Mooney, R. J. </author> <year> 1997. </year> <title> An overview of empirical natural language processing. </title> <journal> AI Magazine 18(4) </journal> <pages> 13-24. </pages>
Reference-contexts: So, TBL's output offers insights into a theory to explain the data. This is a reason to prefer TBL over probabilistic machine learning methods, since TBL's rules could "allow developers to more easily understand, manipulate, and debug the resulting system." <ref> (Brill & Mooney 1997) </ref> TBL is capable of discarding irrelevant rules, so it is not necessary that all of the given rule templates be useful. If an irrelevant rule is generated, its effect on the training corpus is essentially random, resulting in a low score, on average.
Reference: <author> Brill, E. </author> <year> 1995. </year> <title> Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. </title> <booktitle> Computational Linguistics 21(4) </booktitle> <pages> 543-566. </pages>
Reference-contexts: Introduction Transformation-Based Learning (TBL) <ref> (Brill 1995) </ref> is a promising new machine learning algorithm, which has a number of advantages over alternative approaches. However, one major limitation of TBL is that it requires detailed information specifying the set of feature patterns that are relevant to a particular problem. <p> SUGGEST A 3 Okay. ACCEPT Transformation-Based Learning TBL is a relatively new symbolic machine learning algorithm. When tested on the Part-of-Speech Tagging problem, 1 TBL was as effective as or better than the alternative approaches, producing the correct tag for 97.2% of the words in unseen data <ref> (Brill 1995) </ref>. In comparison with other machine learning methods, TBL has a number of advantages, which we will present in a later section. <p> Naturally, some restrictions must be imposed on the way in which the system may compute rules for 3 Typically, the stopping criterion is to terminate training when no rule can be found that improves the tagging accuracy on the training corpus by more than some predetermined threshold <ref> (Brill 1995) </ref>. 4 The score measures the amount of improvement in the tagging accuracy of the training corpus that would result from including a given rule in the final model (Brill 1995). step 2ai, as there are an infinite number of rules that can fix the tag of a given instance, <p> no rule can be found that improves the tagging accuracy on the training corpus by more than some predetermined threshold <ref> (Brill 1995) </ref>. 4 The score measures the amount of improvement in the tagging accuracy of the training corpus that would result from including a given rule in the final model (Brill 1995). step 2ai, as there are an infinite number of rules that can fix the tag of a given instance, most of which are completely unrelated to the task at hand. 5 For this reason, the human developer must provide the system with a set of rule templates, to restrict <p> For some tasks, it might not even be theoretically possible to capture all of the necessary information, while still maintaining tractability. Brill circumvented this problem by hand-selecting fewer than 30 templates, each consisting of only one or two conditions <ref> (Brill 1995) </ref>. Unfortunately, it is often very difficult to construct such a limited set of templates without omitting any relevant patterns.
Reference: <author> Ramshaw, L. A., and Marcus, M. P. </author> <year> 1994. </year> <title> Exploring the statistical derivation of transformation rule sequences for part-of-speech tagging. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the ACL, </booktitle> <pages> 86-95. </pages> <address> Las Cruces, New Mexico: </address> <booktitle> Association for Computational Linguistics. Balancing Act Workshop. </booktitle>
Reference: <author> Samuel, K.; Carberry, S.; and Vijay-Shanker, K. </author> <year> 1998. </year> <title> Computing dialogue acts from features with transformation-based learning. </title> <booktitle> In Proceedings of the AAAI 1998 Spring Symposium on Applying Machine Learning to Discourse Processing. </booktitle>
Reference-contexts: All of the examples and experimental results in this paper are drawn from our work on a language understanding problem called Dialogue Act Tagging, where the goal is to label each utterance in a conversational dialogue with the correct dialogue act, which is an abstraction of the speaker's intention <ref> (Samuel, Carberry, & Vijay-Shanker 1998) </ref>. Examples of dialogue acts are illustrated by the dialogue in Figure 1. Speaker Utterance Dialogue Act A 1 I have some problems INFORM with the homework. A 2 Can I ask you a couple REQUEST of questions? B 1 I can't help you now.
Reference: <author> Satta, G., and Henderson, J. C. </author> <year> 1997. </year> <title> String transformation learning. </title> <booktitle> In Proceedings of the 35th Annual Meeting of the ACL, </booktitle> <pages> 444-451. </pages> <institution> Madrid, Spain: Association for Computational Linguistics. </institution>
Reference-contexts: B 2 in Figure 1: IF the third letter in the second word of the utterance is "s", THEN change the utterance's tag to SUGGEST. 6 In a later section, we argue that TBL is capable of discarding irrelevant rules, so this approach should be effective, in theory. and Henderson <ref> (Satta & Henderson 1997) </ref> suggested an alternative solution: They introduced a data structure that can efficiently keep track of all possible transformations simultaneously, which allows TBL to consider a large number of rule templates.
References-found: 5

