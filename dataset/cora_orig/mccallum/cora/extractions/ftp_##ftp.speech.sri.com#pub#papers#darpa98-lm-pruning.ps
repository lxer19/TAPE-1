URL: ftp://ftp.speech.sri.com/pub/papers/darpa98-lm-pruning.ps
Refering-URL: http://www.speech.sri.com/people/stolcke/publications.html
Root-URL: 
Title: Entropy-based Pruning of Backoff Language Models  
Author: Andreas Stolcke 
Address: Menlo Park, California  
Affiliation: Speech Technology And Research Laboratory SRI International  
Abstract: A criterion for pruning parameters from N-gram backoff language models is developed, based on the relative entropy between the original and the pruned model. It is shown that the relative entropy resulting from pruning a single N-gram can be computed exactly and efficiently for backoff models. The relative entropy measure can be expressed as a relative change in training set perplexity. This leads to a simple pruning criterion whereby all N-grams that change perplexity by less than a threshold are removed from the model. Experiments show that a production-quality Hub4 LM can be reduced to 26% its original size without increasing recognition error. We also compare the approach to a heuristic pruning criterion by Seymore and Rosenfeld [9], and show that their approach can be interpreted as an approximation to the relative entropy criterion. Experimentally, both approaches select similar sets of N-grams (about 85% overlap), with the exact relative entropy criterion giving marginally better performance. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> T. C. Bell, J. G. Cleary, and I. H. Witten. </author> <title> Text Compression. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1990. </year>
Reference-contexts: Relation to Other Work Our choice of relative entropy as an optimization criterion is by no means new. Relative entropy minimization (sometimes in the guise of likelihood maximization) is the basis of many model optimization techniques proposed in the past, e.g., for text compression <ref> [1] </ref>, Markov model induction [10, 7]. Kneser [6] first suggested applying it to backoff N-gram models, although, as shown in Section 5, the heuristic pruning algorithm of Seymore and Rosenfeld [9] amounts to an approximate relative entropy minimization.
Reference: 2. <author> T. M. Cover and J. A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley and Sons, Inc., </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: A standard measure of divergence between distributions is relative entropy or Kullback-Leibler distance (see, e.g., <ref> [2] </ref>). Although not strictly a distance metric, it is a non-negative, continuous function that is zero if and only if the two distributions are identical. Let p (j) denote the conditional probabilities assigned by the original model, and p 0 (j) the probabilities in the pruned model.
Reference: 3. <author> I. J. </author> <title> Good. The population frequencies of species and the estimation of population parameters. </title> <journal> Biometrika, </journal> <volume> 40:237264, </volume> <year> 1953. </year>
Reference-contexts: As noted in Section 2, the pruning algorithm is applicable irrespective of the particular N-gram estimator used. We used Good-Turing smoothing <ref> [3] </ref> throughout and did not investigate possible interactions between smoothing methods and pruning. Table 1 shows model size, perplexity and word error results as determined on the development test set, for various pruning thresholds.
Reference: 4. <author> F. Jelinek. </author> <title> Up from trigrams! The struggle for improved language models. </title> <booktitle> In Proc. EUROSPEECH, </booktitle> <pages> pp. 10371040, </pages> <address> Genova, Italy, </address> <year> 1991. </year>
Reference-contexts: 1. Introduction N-gram backoff models [5], despite their shortcomings, still dominate as the technology of choice for state-of-the-art speech recognizers <ref> [4] </ref>. Two sources of performance improvements are the use of higher-order models (several DARPA-Hub4 sites now use 4-gram or 5-gram models) and the inclusion of more training data from more sources (Hub4 models typically include Broadcast News, NABN and WSJ data).
Reference: 5. <author> S. M. Katz. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE ASSP, </journal> <volume> 35(3):400401, </volume> <year> 1987. </year>
Reference-contexts: 1. Introduction N-gram backoff models <ref> [5] </ref>, despite their shortcomings, still dominate as the technology of choice for state-of-the-art speech recognizers [4].
Reference: 6. <author> R. Kneser. </author> <title> Statistical language modeling using a variable context length. </title> <booktitle> In Proc. EUROSPEECH, </booktitle> <volume> vol. 1, </volume> <pages> pp. 494497, </pages> <address> Rhodes, Greece, </address> <year> 1997. </year>
Reference-contexts: In the case of N-gram models, the goal of parameter selection is to chose which N-grams should have explicit conditional probability estimates assigned by the model, so as to maximize performance (i.e., minimize perplexity and/or recognition error) while minimizing model size. As pointed out in <ref> [6] </ref>, pruning (selecting parameters from) a full N-gram model of higher order amounts to building a variable-length N-gram model, i.e., one in which training set contexts are not uniformly represented by N-grams of the same length. <p> Relative entropy minimization (sometimes in the guise of likelihood maximization) is the basis of many model optimization techniques proposed in the past, e.g., for text compression [1], Markov model induction [10, 7]. Kneser <ref> [6] </ref> first suggested applying it to backoff N-gram models, although, as shown in Section 5, the heuristic pruning algorithm of Seymore and Rosenfeld [9] amounts to an approximate relative entropy minimization. <p> The algorithm described in the next section is novel in that it removes some of the approximations employed in previous approaches. Specifically, the algorithm of <ref> [6] </ref> assumes that backoff weights are unchanged by the pruning, and [9] does not consider the effect that a changed backoff weight has on N-gram probabilities other than the pruned one (this effect is discussed in more detail in Section 5). <p> Acknowledgments This work was sponsored by DARPA through the Naval Command and Control Ocean Surveillance Center under contract N66001-94-C-6048. I thank Roni Rosenfeld and Kristie Sey-more for clarifications and discussions regarding their paper [9]. Thanks also to Hermann Ney and Dietrich Klakow for pointing out similarities to <ref> [6] </ref>.
Reference: 7. <author> D. Ron, Y. Singer, and N. Tishby. </author> <title> The power of amnesia. </title> <editor> In J. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> NIPS-5, </booktitle> <pages> pp. 176183. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1994. </year>
Reference-contexts: Relation to Other Work Our choice of relative entropy as an optimization criterion is by no means new. Relative entropy minimization (sometimes in the guise of likelihood maximization) is the basis of many model optimization techniques proposed in the past, e.g., for text compression [1], Markov model induction <ref> [10, 7] </ref>. Kneser [6] first suggested applying it to backoff N-gram models, although, as shown in Section 5, the heuristic pruning algorithm of Seymore and Rosenfeld [9] amounts to an approximate relative entropy minimization.
Reference: 8. <author> A. Sankar, L. Heck, and A. Stolcke. </author> <title> Acoustic modeling for the SRI Hub4 partitioned evaluation continuous speech recognition system. </title> <booktitle> In ProceedingsDARPA Speech Recognition Workshop, </booktitle> <pages> pp. 127132, </pages> <address> Chantilly, VA, </address> <year> 1997. </year>
Reference-contexts: Experiments We evaluated relative entropy-based language model pruning in the Broadcast News domain, using SRI's 1996 Hub4 evaluation system <ref> [8] </ref>.
Reference: 9. <author> K. Seymore and R. Rosenfeld. </author> <title> Scalable backoff language models. </title> <booktitle> In Proc. ICSLP, </booktitle> <volume> vol. 1, </volume> <pages> pp. 232235, </pages> <address> Philadelphia, </address> <year> 1996. </year>
Reference-contexts: As pointed out in [6], pruning (selecting parameters from) a full N-gram model of higher order amounts to building a variable-length N-gram model, i.e., one in which training set contexts are not uniformly represented by N-grams of the same length. Seymore and Rosenfeld <ref> [9] </ref> showed that selecting N-grams based on their conditional probability estimates and frequency of use is more effective than the traditional absolute frequency thresholding. <p> Kneser [6] first suggested applying it to backoff N-gram models, although, as shown in Section 5, the heuristic pruning algorithm of Seymore and Rosenfeld <ref> [9] </ref> amounts to an approximate relative entropy minimization. The algorithm described in the next section is novel in that it removes some of the approximations employed in previous approaches. Specifically, the algorithm of [6] assumes that backoff weights are unchanged by the pruning, and [9] does not consider the effect that <p> pruning algorithm of Seymore and Rosenfeld <ref> [9] </ref> amounts to an approximate relative entropy minimization. The algorithm described in the next section is novel in that it removes some of the approximations employed in previous approaches. Specifically, the algorithm of [6] assumes that backoff weights are unchanged by the pruning, and [9] does not consider the effect that a changed backoff weight has on N-gram probabilities other than the pruned one (this effect is discussed in more detail in Section 5). <p> The pruned ( = 10 8 ) four-gram has the same perplexity and lower word error (p &lt; 0:07) than the full trigram. 5. Comparison to Seymore and Rosenfeld's Approach In <ref> [9] </ref>, Seymore and Rosenfeld proposed a different pruning scheme for backoff models (henceforth called the SR crite rion, as opposed to the relative entropy, or RE criterion). <p> Acknowledgments This work was sponsored by DARPA through the Naval Command and Control Ocean Surveillance Center under contract N66001-94-C-6048. I thank Roni Rosenfeld and Kristie Sey-more for clarifications and discussions regarding their paper <ref> [9] </ref>. Thanks also to Hermann Ney and Dietrich Klakow for pointing out similarities to [6].
Reference: 10. <author> A. Stolcke and S. Omohundro. </author> <title> Hidden Markov model induction by Bayesian model merging. </title> <editor> In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, </editor> <booktitle> NIPS-5, </booktitle> <pages> pp. 1118. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Relation to Other Work Our choice of relative entropy as an optimization criterion is by no means new. Relative entropy minimization (sometimes in the guise of likelihood maximization) is the basis of many model optimization techniques proposed in the past, e.g., for text compression [1], Markov model induction <ref> [10, 7] </ref>. Kneser [6] first suggested applying it to backoff N-gram models, although, as shown in Section 5, the heuristic pruning algorithm of Seymore and Rosenfeld [9] amounts to an approximate relative entropy minimization.
References-found: 10

