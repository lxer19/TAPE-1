URL: http://www.sgi.com/Technology/mlc/util/util.ps
Refering-URL: http://www.sgi.com/Technology/mlc/docs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: mlc@postofc.corp.sgi.com  
Title: Mac hine Learning library in SGIMLC Utilities 2.0  
Author: By: Ronny Kohavi and Dan Sommerfield 
Note: The URL for MLC is http://www.sgi.com/Technology/mlc  
Date: October 7, 1996  
Abstract-found: 0
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1992), </year> <title> `Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms', </title> <journal> International Journal of Man-Machine Studies 36(1), </journal> <pages> 267-287. </pages>
Reference-contexts: Consider raising the LOGLEVEL to 2 to see the progress. You can use the project utility on the final node in order to study the selected attributes in isolation. 4.5 Instance Based Algorithms IB is an instance-based inducer <ref> (Aha 1992, Wettschereck 1994) </ref>. A good, robust algorithm, but still slow when there are many attributes. NUM NEIGHBORS determines the number of neighbors to use. <p> Thus a higher number means less attributes will be used. See Kohavi (1995c) for more information. 13 4.10 Aha Instance-based series (IBL) Aha-ib is an external inducer that interfaces the IB1-4 series from 3/9/94 <ref> (Aha 1992) </ref>. IB CLASS should be set to one of the following values: ib1, ib2, ib3, or ib4. The seed and specific flags can be set in the options IBL SEED and IBL FLAGS. The executable "ibl" must be in the current path. <p> Value i is mapped into the binary representation of i + 1, and the binary zero is allocated for unknown values. Option name Domain Default Explanation CONVERSION local, binary, none, aha local See above. The "Aha" format converts to David Aha's IBL programs format <ref> (Aha 1992) </ref>. ATTR DELIM space, comma, period, semicolon, colon comma the attribute delimiter. 25 Option name Domain Default Explanation LAST ATTR DELIM ditto comma The last delimiter before the label. END OF LINE DELIM ditto period End of line marker.
Reference: <author> Auer, P., Holte, R. & Maass, W. </author> <year> (1995), </year> <title> Theory and applications of agnostic PAC-learning with small decision trees, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> `Machine Learning: Proceedings of the Twelfth International Conference', </booktitle> <publisher> Morgan Kaufmann Publishers, Inc. 32 Breiman, </publisher> <editor> L. </editor> <year> (1994), </year> <institution> Bagging predictors, Technical Report Statistics Department, University of California at Berkeley. </institution>
Reference-contexts: If we look at the error rate, then C4.5 has an error of 14.07% and OneR therefore makes 40% more errors than C4.5. 4.8 T2 T2 is a two-level decision tree that minimizes the number of errors and discretizes continuous attributes <ref> (Auer, Holte & Maass 1995) </ref>. It requires large amounts of memory if you have many clases. 4.9 HOODG/List-HOODG: Oblivious Decision Graphs An inducer for building oblivious decision graphs bottom-up (Kohavi 1994a, Kohavi 1994b). Does not handle unknown values.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A. & Stone, C. J. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: If you use the OC1 software in the context of any of your publications, please reference Murthy et al. (1994). The following options are Supported: OC1 SEED to set the seed. OC1 AXIS PARALLEL ONLY to force axis parallel splits. OC1 CART LINEAR COMBINATION MODE to force CART-like splits <ref> (Breiman, Friedman, Ol shen & Stone 1984) </ref>. OC1 PRUNING RATE to set the pruning rate. The executable "mktree" must be in the current path. 4.13 PEBLS PEBLS is an external inducer that interfaces the Parallel Exemplar-Based Learning System version 2.0 by Cost & Salzberg (1993).
Reference: <author> Clark, P. & Boswell, R. </author> <year> (1991), </year> <title> Rule induction with CN2: Some recent improvements, </title> <editor> in Y. Kodratoff, ed., </editor> <booktitle> `Proceedings of the fifth European conference (EWSL-91)', </booktitle> <publisher> Springer Verlag, </publisher> <pages> pp. 151-163. </pages> <note> *http://www.cs.utexas.edu/users/pclark/papers/newcn.ps Clark, </note> <author> P. & Niblett, T. </author> <year> (1989), </year> <title> `The CN2 induction algorithm', </title> <booktitle> Machine Learning 3(4), </booktitle> <pages> 261-283. </pages>
Reference: <author> Cost, S. & Salzberg, S. </author> <year> (1993), </year> <title> `A weighted nearest neighbor algorithm for learning with symbolic features', </title> <booktitle> Machine Learning 10(1), </booktitle> <pages> 57-78. </pages>
Reference: <author> Devijver, P. A. & Kittler, J. </author> <year> (1982), </year> <title> Pattern Recognition: A Statistical Approach, </title> <booktitle> Prentice-Hall International. </booktitle>
Reference: <author> Dougherty, J., Kohavi, R. & Sahami, M. </author> <year> (1995), </year> <title> Supervised and unsupervised discretization of continuous features, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> `Machine Learning: Proceedings of the Twelfth International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 194-202. </pages>
Reference-contexts: Unknown values in the test instance are skipped (equivalent to marginalizing over them). Better results are commonly achieved by discretizing the continuous attributes. The disc-naive-bayes inducer provides this preprocessing step by chaining disc-filter-inducer to naive-bayes inducer <ref> (Dougherty, Kohavi & Sahami 1995) </ref>. <p> Options for the wrapped inducer will be prefixed by the "DISCF " prefix. The most important option is the discretization type: entropy, 1r, bin, c4.5-disc, t2-disc. The entropy discretization seems to be the best discretization method from the allowed options for most practical datasets <ref> (Dougherty et al. 1995) </ref>. Methods which require specifying the number of intervals need the option DISC NUM INTR, which determines the number of intervals. Possible options are: Algo-heuristic (algorithm dependent heuristic), Fixed-value (you specify the number), and MDL (based on Fayyad & Irani (1993)).
Reference: <author> Duda, R. & Hart, P. </author> <year> (1973), </year> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley. </publisher>
Reference: <author> Efron, B. & Tibshirani, R. </author> <year> (1993), </year> <title> An Introduction to the Bootstrap, </title> <publisher> Chapman & Hall. </publisher>
Reference-contexts: An accuracy range in parentheses is a 95% percentile interval <ref> (Efron & Tibshirani 1993) </ref>; the percentile bound is pessimistic in the sense that it includes a wider range due to the integral number of samples. <p> Given all that, it works pretty well in practice. Stratified cross-validation Same as cross-validation, except that the folds are stratified so that they contain approximately the same proportions of labels as the original dataset. Bootstrap The .632 Bootstrap <ref> (Efron & Tibshirani 1993) </ref> estimates the accuracy as follows. Given a dataset of size n, a bootstrap sample is created by sampling n instances uniformly from the data (with replacement).
Reference: <author> Fayyad, U. M. & Irani, K. B. </author> <year> (1993), </year> <title> Multi-interval discretization of continuous-valued attributes for classification learning, </title> <booktitle> in `Proceedings of the 13th International Joint Conference on Artificial Intelligence', </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 1022-1027. </pages>
Reference: <author> Fisher, R. A. </author> <year> (1936), </year> <title> `The use of multiple measurements in taxonomic problems', </title> <journal> Annals of Eugenics 7(1), </journal> <pages> 179-188. </pages>
Reference-contexts: Each dataset should include a names file describing how to parse the data, a data file containing the data, and an optional test file for estimating accuracy. Example 1 (Running ID3) Fisher's iris dataset <ref> (Fisher 1936) </ref> contains four attributes of iris plants: sepal length, sepal width, petal length, and petal width. The task is to categorize each instance into one of the three classes: Iris Setosa, Iris Versicolour, and Iris Virginica.
Reference: <author> Friedman, J., Kohavi, R. & Yun, Y. </author> <year> (1996), </year> <title> Lazy decision trees, </title> <booktitle> in `Proceedings of the Thirteenth National Conference on Artificial Intelligence', </booktitle> <publisher> AAAI Press and the MIT Press, </publisher> <pages> pp. 717-724. </pages>
Reference: <author> Geman, S., Bienenstock, E. & Doursat, R. </author> <year> (1992), </year> <title> `Neural networks and the bias/variance dilemma', </title> <booktitle> Neural Computation 4, </booktitle> <pages> 1-48. </pages>
Reference-contexts: The wrapped inducer must be a regular inducer (not a base inducer). Bagging seems to work best on unstable inducers, that is, inducers that suffer from high variance because of small perturbations in the data <ref> (Geman, Bienenstock & Doursat 1992) </ref>. Unstable inducers include decision trees (e.g., ID3) and perceptrons; an example of a very stable inducer is nearest neighbor, which has a high bias in high-dimensional spaces, but very little variance.
Reference: <author> Good, I. J. </author> <year> (1965), </year> <title> The Estimation of Probabilities: An Essay on Modern Bayesian Methods, </title> <publisher> M.I.T. Press. </publisher>
Reference: <author> Hertz, J., Krogh, A. & Palmer, R. G. </author> <year> (1991), </year> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison Wesley. </publisher>
Reference: <author> Holte, R. C. </author> <year> (1993), </year> <title> `Very simple classification rules perform well on most commonly used datasets', </title> <booktitle> Machine Learning 11, </booktitle> <pages> 63-90. </pages>
Reference-contexts: It was crucial to make sure that even if the interface files were removed, a smooth recovery would occur by regenerating the files. 12 4.7 Holte's OneR OneR is a simple classifier that makes a one-rule, i.e., a rule based on the value of a single attribute <ref> (Holte 1993) </ref>. MIN INST is the minimum number of instances for a discretization interval. Holte recommends the value six for most datasets. OneR is currently implemented only as a base inducer. OneR shows that it is easy to get reasonable accuracy on many tasks by simply looking at one attribute. <p> Contrary to common claims and misinterpretations regarding Holte's results, the inducer is significantly inferior to C4.5. The average accuracy of OneR for the datasets tested by Holte is 5.7% lower than that of C4.5 <ref> (Holte 1993, page 67) </ref>. If we look at the error rate, then C4.5 has an error of 14.07% and OneR therefore makes 40% more errors than C4.5. 4.8 T2 T2 is a two-level decision tree that minimizes the number of errors and discretizes continuous attributes (Auer, Holte & Maass 1995). <p> The entropy method requires MIN SPLIT, the minimum number of instances in an interval, and the algorithm heuristic defaults to MDL. The 1r method is Holte's method of discretization used in the OneR rule <ref> (Holte 1993) </ref>. It requires MIN INST, the minimum number of instances per bin (0 will be changed to 6, the default suggested by Holte).
Reference: <author> John, G., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title> <booktitle> in `Machine Learning: Proceedings of the Eleventh International Conference', </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 121-129. </pages>
Reference: <author> Kohavi, R. </author> <year> (1994a), </year> <title> Bottom-up induction of oblivious, read-once decision graphs, </title> <editor> in F. Bergadano & L. D. Raedt, eds, </editor> <booktitle> `Proceedings of the European Conference on Machine Learning', </booktitle> <pages> pp. 154-169. </pages>
Reference-contexts: It requires large amounts of memory if you have many clases. 4.9 HOODG/List-HOODG: Oblivious Decision Graphs An inducer for building oblivious decision graphs bottom-up <ref> (Kohavi 1994a, Kohavi 1994b) </ref>. Does not handle unknown values. HOODG suffers from irrelevant or weakly relevant features, which is why you should use feature subset selection. HOODG also requires discretized data, so disc-filter must be used.
Reference: <author> Kohavi, R. </author> <year> (1994b), </year> <title> Bottom-up induction of oblivious, read-once decision graphs : strengths and limitations, </title> <booktitle> in `Twelfth National Conference on Artificial Intelligence', </booktitle> <pages> pp. 613-618. </pages>
Reference: <author> Kohavi, R. </author> <year> (1994c), </year> <title> Feature subset selection as search with probabilistic estimates, </title> <booktitle> in `AAAI Fall Symposium on Relevance', </booktitle> <pages> pp. 122-126. </pages>
Reference: <author> Kohavi, R. </author> <year> (1995a), </year> <title> The power of decision tables, </title> <editor> in N. Lavrac & S. Wrobel, eds, </editor> <booktitle> `Proceedings of the European Conference on Machine Learning', Lecture Notes in Artificial Intelligence 914, </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <pages> pp. 174-189. </pages> <note> 33 Kohavi, </note> <author> R. </author> <year> (1995b), </year> <title> A study of cross-validation and bootstrap for accuracy estimation and model selection, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> `Proceedings of the 14th International Joint Conference on Artificial Intelligence', </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 1137-1143. </pages>
Reference-contexts: Stores a table of all instances, predicts according to the table. If an instance is not found, table-majority predicts the majority class of the table and table-no-majority returns "unknown" (always wrong against test-set). When coupled with feature subset selection it provides a powerful inducer for discrete data <ref> (Kohavi 1995a) </ref>. If discretization is done, it is also powerful for data with continuous attributes. For example, to run discretization and feature subset selection, one can define the following options: setenv INDUCER disc-filter setenv DISCF_INDUCER FSS setenv DISCF_FSS_INDUCER table-majority setenv DATAFILE cleve and run the Inducer utility. <p> Example 6 (Feature Subset Selection) Feature subset selection on Table-majority inducer found that out of 180 bits used in the DNA splice-junction dataset used in the StatLog project (Taylor et al. 1994), a small subset of 11 bits were most useful for Table-majority <ref> (Kohavi 1995a) </ref>. To generate this subset, one can project on the features numbered 81, 83, 84, 89, 92, 93, 94, 95, 96, 101, 104.
Reference: <author> Kohavi, R. </author> <year> (1995c), </year> <title> Wrappers for Performance Enhancement and Oblivious Decision Graphs, </title> <type> PhD thesis, </type> <institution> Stanford University, Computer Science department. STAN-CS-TR-95-1560, ftp://starry.stanford.edu/pub/ronnyk/teza.ps. </institution>
Reference: <author> Kohavi, R. </author> <year> (1996), </year> <title> Scaling up the accuracy of naive-bayes classifiers: a decision-tree hybrid, </title> <booktitle> in `Proceedings of the Second International Conference on Knowledge Discovery and Data Mining', </booktitle> <address> p. </address> <note> to appear. </note>
Reference: <author> Kohavi, R. & John, G. </author> <year> (1995), </year> <title> Automatic parameter selection by minimizing estimated error, </title> <editor> in A. Prieditis & S. Russell, eds, </editor> <booktitle> `Machine Learning: Proceedings of the Twelfth International Conference', </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 304-312. </pages>
Reference-contexts: Unknown values in the test instance are skipped (equivalent to marginalizing over them). Better results are commonly achieved by discretizing the continuous attributes. The disc-naive-bayes inducer provides this preprocessing step by chaining disc-filter-inducer to naive-bayes inducer <ref> (Dougherty, Kohavi & Sahami 1995) </ref>. <p> The "info" utility is probably a better way of getting basic statistics about a data file. EODG is an inducer for building oblivious decision graphs top-down <ref> (Kohavi & Li 1995) </ref>. Cannot handle unknown values. LazyDT is a lazy decision tree algorithm, described in Friedman, Kohavi & Yun (1996). Order-fss searches for an attribute ordering. Very researchy. 15 Disc-search is a wrapper discretizer that searches for the best number of intervals for each attribute. Very slow. <p> Due to high variance in the estimation, this option does not seem to work well in practice. 16 5.3 Feature Subset Selection The feature subset selection is a wrapper inducer that selects a good subset of features for improved accuracy performance <ref> (Kohavi & Sommerfield 1995, Kohavi 1994c, John, Kohavi & Pfleger 1994) </ref>. All options in accuracy estimation (Section 3) can be used with the extra options listed below. Option name Domain Default Explanation FSS INDUCER Inducer | Inducer to wrap around. <p> This determines one termination condition. FSS EPSILON Real 0 0.001 Consider a node non-improving if estimated accuracy was better than the best by this number. FSS USE COMPOUND yes/no yes Generate nodes that combine the features of the best generated children <ref> (Kohavi & Sommerfield 1995) </ref>. FSS CMPLX PENALTY Real 0.001 How much to penalize the estimate for each feature.
Reference: <author> Kohavi, R., John, G., Long, R., Manley, D. & Pfleger, K. </author> <year> (1994), </year> <title> MLC++: A machine learning library in C++, </title> <booktitle> in `Tools with Artificial Intelligence', </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 740-743. </pages> <address> http://www.sgi.com/Technology/mlc. </address>
Reference-contexts: Better results are commonly achieved by discretizing the continuous attributes. The disc-naive-bayes inducer provides this preprocessing step by chaining disc-filter-inducer to naive-bayes inducer (Dougherty, Kohavi & Sahami 1995). Further improvements can usually be achieved by running feature subset selection <ref> (Langley & Sage 1994, Kohavi & Sommerfield 1995) </ref> as shown below: setenv INDUCER disc-filter setenv DISCF_INDUCER FSS setenv DISCF_FSS_INDUCER naive setenv DISCF_FSS_CMPLX_PENALTY 0.001 setenv DISCF_FSS_CV_TIMES 0 setenv DISCF_FSS_ACC_ESTIMATOR cv setenv DISCF_FSS_CV_FOLDS 5 setenv DISCF_FSS_DIRECTION backward 4.3 ID3, MC4 ID3 is a very basic decision tree algorithm with no pruning.
Reference: <author> Kohavi, R. & Li, C.-H. </author> <year> (1995), </year> <title> Oblivious decision trees, graphs, and top-down pruning, </title> <editor> in C. S. Mellish, ed., </editor> <booktitle> `Proceedings of the 14th International Joint Conference on Artificial Intelligence', </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 1071-1077. </pages>
Reference-contexts: Unknown values in the test instance are skipped (equivalent to marginalizing over them). Better results are commonly achieved by discretizing the continuous attributes. The disc-naive-bayes inducer provides this preprocessing step by chaining disc-filter-inducer to naive-bayes inducer <ref> (Dougherty, Kohavi & Sahami 1995) </ref>. <p> The "info" utility is probably a better way of getting basic statistics about a data file. EODG is an inducer for building oblivious decision graphs top-down <ref> (Kohavi & Li 1995) </ref>. Cannot handle unknown values. LazyDT is a lazy decision tree algorithm, described in Friedman, Kohavi & Yun (1996). Order-fss searches for an attribute ordering. Very researchy. 15 Disc-search is a wrapper discretizer that searches for the best number of intervals for each attribute. Very slow. <p> Due to high variance in the estimation, this option does not seem to work well in practice. 16 5.3 Feature Subset Selection The feature subset selection is a wrapper inducer that selects a good subset of features for improved accuracy performance <ref> (Kohavi & Sommerfield 1995, Kohavi 1994c, John, Kohavi & Pfleger 1994) </ref>. All options in accuracy estimation (Section 3) can be used with the extra options listed below. Option name Domain Default Explanation FSS INDUCER Inducer | Inducer to wrap around. <p> This determines one termination condition. FSS EPSILON Real 0 0.001 Consider a node non-improving if estimated accuracy was better than the best by this number. FSS USE COMPOUND yes/no yes Generate nodes that combine the features of the best generated children <ref> (Kohavi & Sommerfield 1995) </ref>. FSS CMPLX PENALTY Real 0.001 How much to penalize the estimate for each feature.
Reference: <author> Kohavi, R. & Sommerfield, D. </author> <year> (1995), </year> <title> Feature subset selection using the wrapper model: Overfitting and dynamic search space topology, </title> <booktitle> in `The First International Conference on Knowledge Discovery and Data Mining', </booktitle> <pages> pp. 192-197. </pages>
Reference-contexts: Unknown values in the test instance are skipped (equivalent to marginalizing over them). Better results are commonly achieved by discretizing the continuous attributes. The disc-naive-bayes inducer provides this preprocessing step by chaining disc-filter-inducer to naive-bayes inducer <ref> (Dougherty, Kohavi & Sahami 1995) </ref>. <p> The "info" utility is probably a better way of getting basic statistics about a data file. EODG is an inducer for building oblivious decision graphs top-down <ref> (Kohavi & Li 1995) </ref>. Cannot handle unknown values. LazyDT is a lazy decision tree algorithm, described in Friedman, Kohavi & Yun (1996). Order-fss searches for an attribute ordering. Very researchy. 15 Disc-search is a wrapper discretizer that searches for the best number of intervals for each attribute. Very slow. <p> Due to high variance in the estimation, this option does not seem to work well in practice. 16 5.3 Feature Subset Selection The feature subset selection is a wrapper inducer that selects a good subset of features for improved accuracy performance <ref> (Kohavi & Sommerfield 1995, Kohavi 1994c, John, Kohavi & Pfleger 1994) </ref>. All options in accuracy estimation (Section 3) can be used with the extra options listed below. Option name Domain Default Explanation FSS INDUCER Inducer | Inducer to wrap around. <p> This determines one termination condition. FSS EPSILON Real 0 0.001 Consider a node non-improving if estimated accuracy was better than the best by this number. FSS USE COMPOUND yes/no yes Generate nodes that combine the features of the best generated children <ref> (Kohavi & Sommerfield 1995) </ref>. FSS CMPLX PENALTY Real 0.001 How much to penalize the estimate for each feature.
Reference: <author> Kohavi, R., Sommerfield, D. & Dougherty, J. </author> <year> (1996), </year> <title> Data mining using MLC++: A machine learning library in C++, </title> <booktitle> in `Tools with Artificial Intelligence', </booktitle> <publisher> IEEE Computer Society Press, </publisher> <address> p. </address> <note> To Appear. http://www.sgi.com/Technology/mlc. </note>
Reference: <author> Kohavi, R. & Wolpert, D. H. </author> <year> (1996), </year> <title> Bias plus variance decomposition for zero-one loss functions, </title> <editor> in L. Saitta, ed., </editor> <booktitle> `Machine Learning: Proceedings of the Thirteenth International Conference', </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> Inc. </address> <note> Available at http://robotics.stanford.edu/users/ronnyk. </note>
Reference: <author> Koutsofios, E. & North, S. C. </author> <year> (1994), </year> <title> Drawing graphs with dot. </title> <note> Available by anonymous ftp from research.att.com:dist/drawdag/dotdoc.ps.Z. </note>
Reference: <author> Krogh, A. & Vedelsby, J. </author> <year> (1995), </year> <title> Neural network ensembles, cross validation, and active learning, </title> <booktitle> in `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. 7, </volume> <publisher> MIT Press. </publisher>
Reference-contexts: BAG PROPORTION determines the proportion of the training set that will be passed to each copy of the inducer. The higher the proportion, the larger the internal training set; however, if the data is not perturbed enough, the classifiers won't be different and bagging won't work well <ref> (Krogh & Vedelsby 1995) </ref>. BAG UNIF WEIGHTS is a Boolean option that determines whether the votes are equal or estimated.
Reference: <author> Langley, P., Iba, W. & Thompson, K. </author> <year> (1992), </year> <title> An analysis of bayesian classifiers, </title> <booktitle> in `Proceedings of the tenth national conference on artificial intelligence', </booktitle> <publisher> AAAI Press and MIT Press, </publisher> <pages> pp. 223-228. </pages>
Reference-contexts: We now describe the available inducers and their options. 4.1 Const Const predicts a constant class|the majority class in the training set. The accuracy of the const inducer is commonly referred to as the baseline accuracy. 4.2 Naive Bayes The Naive-Bayes inducer <ref> (Langley, Iba & Thompson 1992, Duda & Hart 1973, Good 1965) </ref> computes conditional probabilities of the classes given the instance and picks the class with the highest posterior.
Reference: <author> Langley, P. & Sage, S. </author> <year> (1994), </year> <title> Induction of selective bayesian classifiers, </title> <booktitle> in `Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence', </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Seattle, WA, </address> <pages> pp. 399-406. </pages>
Reference-contexts: Better results are commonly achieved by discretizing the continuous attributes. The disc-naive-bayes inducer provides this preprocessing step by chaining disc-filter-inducer to naive-bayes inducer (Dougherty, Kohavi & Sahami 1995). Further improvements can usually be achieved by running feature subset selection <ref> (Langley & Sage 1994, Kohavi & Sommerfield 1995) </ref> as shown below: setenv INDUCER disc-filter setenv DISCF_INDUCER FSS setenv DISCF_FSS_INDUCER naive setenv DISCF_FSS_CMPLX_PENALTY 0.001 setenv DISCF_FSS_CV_TIMES 0 setenv DISCF_FSS_ACC_ESTIMATOR cv setenv DISCF_FSS_CV_FOLDS 5 setenv DISCF_FSS_DIRECTION backward 4.3 ID3, MC4 ID3 is a very basic decision tree algorithm with no pruning.
Reference: <author> LeBlank, J., Ward, M. & Wittels, N. </author> <year> (1990), </year> <title> Exploring n-dimensional databases, </title> <booktitle> in `Proceedings of Visualization', </booktitle> <pages> pp. 230-237. </pages>
Reference-contexts: They were used in Thrun et al. (1991) and in Wnek & Michalski (1994) to compare algorithms. GLDs have a long history and have been rediscovered many times. They are sometimes called Dimensional Stacking <ref> (LeBlank, Ward & Wittels 1990) </ref>. GLDs will only work with inducers, not base inducers. Each possible instance in the space defines exactly one box in the GLD. The GLD utility has the following display options (GLD SET): Test Show the test set instances with their classes.
Reference: <author> Littlestone, N. </author> <year> (1988), </year> <title> `Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm', </title> <booktitle> Machine Learning 2, </booktitle> <pages> 285-318. </pages>
Reference: <author> Maass, W. </author> <year> (1994), </year> <title> Efficient agnostic PAC-learning with simple hypotheses, </title> <booktitle> in `Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory', </booktitle> <pages> pp. 67-75. </pages> <note> 34 Michalski, </note> <author> R. S. </author> <year> (1978), </year> <title> A planar geometric model for representing multidimensional discrete spaces and multiple-valued logic functions, </title> <type> Technical Report UIUCDCS-R-78-897, </type> <institution> University of Illinois at Urbaba-Champaign. </institution>
Reference-contexts: The bin method uses uniform binning (equal intervals) and the algorithm heuristic for the number of bins is to use twice the log (base 2) of the number of distinct values, a heuristic used in Splus (Spector 1994) and compared in Dougherty et al. (1995). The T2 algorithm <ref> (Maass 1994, Auer et al. 1995) </ref> heuristic is to form number of classes plus one bins. 5.2 Bagging Bagging is a wrapper inducer that runs the wrapped inducer, specified in the BAG INDUCER option, multiple times on subsets of the training set.
Reference: <author> Murthy, S. K., Kasif, S. & Salzberg, S. </author> <year> (1994), </year> <title> `A system for the induction of oblique decision trees', </title> <journal> Journal of Artificial Intelligence Research 2, </journal> <pages> 1-33. </pages>
Reference-contexts: For different normalization types you can use cont-filter inducer as a preprocessor or run the "conv" utility. The reason for this normalization is that winnow overflows really fast when it raises numbers to powers. 4.12 OC1 OC1 is an external inducer that interfaces OC1 version 3 <ref> (Murthy, Kasif & Salzberg 1994) </ref>. The source can be obtained from the Johns Hopkins University, Department of Computer Science.
Reference: <author> Quinlan, J. R. </author> <year> (1986), </year> <title> `Induction of decision trees', </title> <booktitle> Machine Learning 1, </booktitle> <pages> 81-106. </pages> <note> Reprinted in Shavlik and Dietterich (eds.) Readings in Machine Learning. </note>
Reference-contexts: The task is to categorize each instance into one of the three classes: Iris Setosa, Iris Versicolour, and Iris Virginica. To run the ID3 induction algorithm <ref> (Quinlan 1986) </ref> on the iris dataset, consisting of iris.names, iris.data, and iris.test, one can type: setenv DATAFILE iris # The dataset stem setenv INDUCER ID3 # pick ID3 setenv ID3_UNKNOWN_EDGES no # Don't bother with unknown edges setenv DISP_CONFUSION_MAT yes # Show confusion matrix setenv DISPLAY_STRUCT dotty # Show the tree
Reference: <author> Quinlan, J. R. </author> <year> (1993), </year> <title> C4.5: Programs for Machine Learning, </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, California. </address>
Reference-contexts: Text starting with the pound sign (#) is a comment. By default, required options that are not set will be prompted for. Datasets are assumed to be in the MLC ++ format, which is very similar to the C4.5 <ref> (Quinlan 1993) </ref> format. 2 . Each dataset should include a names file describing how to parse the data, a data file containing the data, and an optional test file for estimating accuracy. <p> MC4 includes pruning similar to C4.5 <ref> (Quinlan 1993) </ref>. Except for unknown handling, which is different, MC4 should give you similar results to those of C4.5. Underneath, both are the same algorithm with different default parameter settings.
Reference: <author> Rice, J. A. </author> <year> (1988), </year> <title> Mathematical Statistics and Data Analysis, </title> <publisher> Wadsworth & Brooks/Cole. </publisher>
Reference: <author> Spector, P. </author> <year> (1994), </year> <title> An Introduction to S and S-PLUS, </title> <publisher> Duxbury Press. </publisher>
Reference-contexts: The bin method uses uniform binning (equal intervals) and the algorithm heuristic for the number of bins is to use twice the log (base 2) of the number of distinct values, a heuristic used in Splus <ref> (Spector 1994) </ref> and compared in Dougherty et al. (1995).
Reference: <author> Taylor, C., Michie, D. & Spiegalhalter, D. </author> <year> (1994), </year> <title> Machine Learning, Neural and Statistical Classification, </title> <publisher> Paramount Publishing International. </publisher>
Reference-contexts: This may help pinpoint inappropriate declarations of attributes or even continuous attributes which simply have very few values. Converting attributes with only two values to nominal is generally suggested to gain speedup. For example, the running time for C4.5 (excluding MLC ++ overhead) on the StatLog DNA dataset <ref> (Taylor, Michie & Spiegalhalter 1994) </ref> is 14 seconds on an SGI Indy if the attributes are declared continuous and 4.7 seconds if they are declared nominal. Minor accuracy differences may result due to slightly different ways of handling such attributes. <p> The names file, data file, and test file are all converted to the projected space. Example 6 (Feature Subset Selection) Feature subset selection on Table-majority inducer found that out of 180 bits used in the DNA splice-junction dataset used in the StatLog project <ref> (Taylor et al. 1994) </ref>, a small subset of 11 bits were most useful for Table-majority (Kohavi 1995a). To generate this subset, one can project on the features numbered 81, 83, 84, 89, 92, 93, 94, 95, 96, 101, 104.
Reference: <author> Thrun et al. </author> <year> (1991), </year> <title> The Monk's problems: A performance comparison of different learning algorithms, </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University. </institution>
Reference: <author> Weiss, S. M. & Kulikowski, C. A. </author> <year> (1991), </year> <title> Computer Systems that Learn, </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA. </address>
Reference: <author> Wettschereck, D. </author> <year> (1994), </year> <title> A Study of Distance-Based Machine Learning Algorithms, </title> <type> PhD thesis, </type> <institution> Oregon State University. </institution>
Reference: <author> Wnek, J. & Michalski, R. S. </author> <year> (1994), </year> <title> `Hypothesis-driven constructive induction in AQ17-HCI : A method and experiments', </title> <booktitle> Machine Learning 14(2), </booktitle> <pages> 139-168. </pages>
Reference: <author> Wnek, J., Sarma, J., Wahab, A. A. & Michalski, R. S. </author> <year> (1990), </year> <title> Comparing learning paradigms via diagrammatic visualization, </title> <booktitle> in `Methodologies for Intelligent Systems, 5. Proceedings of the Fifth International Symposium', </booktitle> <pages> pp. 428-437. </pages> <note> Also technical report MLI90-2, </note> <institution> University of Illinois at Urbaba-Champaign. </institution>
Reference: <author> Wolpert, D. H. </author> <year> (1992), </year> <title> `Stacked generalization', </title> <booktitle> Neural Networks 5, </booktitle> <pages> 241-259. 35 </pages>
References-found: 48

