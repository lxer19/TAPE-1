URL: http://www.cs.washington.edu/homes/eric/thesis/thesis_arabic.ps.gz
Refering-URL: http://www.cs.washington.edu/homes/eric/thesis/thesis.html
Root-URL: http://www.cs.washington.edu
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> G. Ahlers and R. P. Behringer. </author> <title> Evolution of turbulence from the Rayleigh-Benard instability. </title> <journal> Phys. Rev. Lett, </journal> <volume> 40 </volume> <pages> 712-716, </pages> <year> 1978. </year>
Reference-contexts: The one-dimensional minimization can be divided into two methods, finding the exact minimum and finding an approximate one. On the assumption of a single minimum, the exact minimum can be readily calculated (to a specified tolerance in ) by the method of golden mean bisection of the interval <ref> [0; 1] </ref>. This golden mean bisection method starts with four specific points, a = 0; b = (3 5)=2; c = ( 5 1)=2; d = 1: These four points form two triplets, fa; b; cg and fb; c; dg. <p> If that point is also not adequate, the three function values calculated so far, together with the derivative at zero, form a cubic. It can be shown that that cubic must have a minimum in <ref> [0; 1 ] </ref>. This process can be repeated as necessary, fitting a cubic to the values given by f (0); f 0 (0) and the two most recently calculated values of , until sufficient decrease. The proposed value of will decrease at each step.
Reference: [2] <author> Mieczyslaw Altman. </author> <title> Iterative methods of contractor directions. Nonlinear Analysis: </title> <journal> Theory, Methods and Applications, </journal> <volume> 4 </volume> <pages> 761-772, </pages> <year> 1980. </year>
Reference-contexts: The Goldstein-Armijo conditions date from 1966 and 1967, and important results were rigorously proved by Wolfe in 1969 and 1971. See [13, 15]. Other work has applied line search or "damped" inexact Newton methods directly to specific classes of nonlinear equations. See <ref> [2, 3] </ref>. In [3], quadratic convergence of a Newton-like method was proved for a specific sequence of step length factors. These factors provided convergence slowly enough to guarantee a condition similar to the alpha condition of Armijo, yet rapidly enough to provide quadratic convergence close to the solution.
Reference: [3] <author> R. E. Bank and D. J. Rose. </author> <title> Global approximate Newton methods. </title> <journal> Numer. Math., </journal> <volume> 37 </volume> <pages> 279-295, </pages> <year> 1981. </year>
Reference-contexts: The Goldstein-Armijo conditions date from 1966 and 1967, and important results were rigorously proved by Wolfe in 1969 and 1971. See [13, 15]. Other work has applied line search or "damped" inexact Newton methods directly to specific classes of nonlinear equations. See <ref> [2, 3] </ref>. In [3], quadratic convergence of a Newton-like method was proved for a specific sequence of step length factors. These factors provided convergence slowly enough to guarantee a condition similar to the alpha condition of Armijo, yet rapidly enough to provide quadratic convergence close to the solution. <p> The Goldstein-Armijo conditions date from 1966 and 1967, and important results were rigorously proved by Wolfe in 1969 and 1971. See [13, 15]. Other work has applied line search or "damped" inexact Newton methods directly to specific classes of nonlinear equations. See [2, 3]. In <ref> [3] </ref>, quadratic convergence of a Newton-like method was proved for a specific sequence of step length factors. These factors provided convergence slowly enough to guarantee a condition similar to the alpha condition of Armijo, yet rapidly enough to provide quadratic convergence close to the solution. <p> The algorithm was apparently designed to limit the number of necessary calculations of the function F , even at the cost of increasing the number of Newton steps. The algorithm in <ref> [3] </ref> is shown in [15] to be a special case of a more general formulation. 6 A comprehensive example 6.1 Introduction This section describes a detailed, comprehensive example used to illustrate the operation in practice of the algorithm described in section 3.
Reference: [4] <author> P. </author> <title> Bjorstad. The direct solution of a generalized biharmonic equation on a disk. </title> <editor> In W. Hackbusch, editor, </editor> <booktitle> Notes on Numerical Fluid Mechanics, </booktitle> <volume> volume 10, </volume> <pages> pages 1-10, </pages> <address> Braunschweig, 1984. </address> <publisher> Vieweg. </publisher>
Reference: [5] <author> P. E. </author> <title> Bjorstad. Fast numerical solution of the biharmonic Dirichlet problem on rectangles. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20 </volume> <pages> 59-71, 193. </pages>
Reference-contexts: The result (see Appendix A or <ref> [5, 6] </ref> for a derivation) is a thirteen-point stencil, illustrated in the case of h x = h y = h by 2 = h 4 6 6 6 1 1 8 20 8 1 1 7 7 7 (13) h 2 ( xxxxxx + xxxxyy + xxyyyy + yyyyyy ) <p> In particular, the availability of the "fast direct constant-coefficient biharmonic solver" of Bjorstad <ref> [5] </ref> suggests a splitting J = L + N where L is a constant-coefficient biharmonic operator based on B. In [20] the researchers found that an operator L = B + C for an appropriately chosen constant (diagonal) C led to a reasonably effective method for the time-dependent equation. <p> And further, there is no need to explicitly form the "square root" of W , since it appears only in this symmetrized form. This feature is important in our case in considering preconditioning by the "fast direct solver" of Bjorstad <ref> [5] </ref>. The "fast direct solver" applies to a (generalized) biharmonic operator B with constant values along the diagonal. <p> We have not investigated the possibility of using the "fast direct solver" of Bjorstad <ref> [5] </ref> as the basis for a preconditioner for the linear solver.
Reference: [6] <author> P. E. </author> <title> Bjorstad. Numerical solution of the biharmonic equation. </title> <type> PhD thesis, </type> <institution> Stanford University, Department of Computer Science, </institution> <year> 1981. </year>
Reference-contexts: The y-axis scale is expanded as compared to Figure 2. The Lyapunov functional decreases monotonically in the later stages of the trajectory as it approaches the stable final state. 17 equation, Equation (1), in time and space. (Some technical details are left to Appendix A; see also <ref> [6] </ref>.) Once the discretization is established, I can describe the stability limitation on the use of the explicit "forward Euler" method. For the "backward Euler" method, I derive, and make some qualitative observations about, the Newton equations for the time-dependent problem (Equation (1)) and the time-independent problem (Equation (3)). <p> The result (see Appendix A or <ref> [5, 6] </ref> for a derivation) is a thirteen-point stencil, illustrated in the case of h x = h y = h by 2 = h 4 6 6 6 1 1 8 20 8 1 1 7 7 7 (13) h 2 ( xxxxxx + xxxxyy + xxyyyy + yyyyyy ) <p> The approximation is lower order than the basic stencil. For a higher order stencil at the boundary, see <ref> [6] </ref>. 21 for each grid point at which the field value is to be computed. Our first step is to indicate theoretically, and show numerically, the limitation imposed by the stability limit on the forward Euler method.
Reference: [7] <author> Richard L. Bowers and James R. Wilson. </author> <title> Numerical Modeling in Applied Physics and Astrophysics. </title> <publisher> Jones and Bartlett Publishers, </publisher> <address> Boston, </address> <year> 1991. </year>
Reference-contexts: Since this splitting method is used inside a Newton iteration, this difficulty effectively placed a timestep limitation even on fully implicit methods such as backward Euler. This limitation was a significant restriction on these researchers' ability to carry the time integration to long times. Indeed, other authors <ref> [7] </ref> argue that a splitting method will generally be less and less effective as a system approaches a steady state. Following [7], consider a simple one-dimensional ordinary differential equation, where the time evolution of a system F is given by dF = A (F )F + B (F )F; where A <p> This limitation was a significant restriction on these researchers' ability to carry the time integration to long times. Indeed, other authors <ref> [7] </ref> argue that a splitting method will generally be less and less effective as a system approaches a steady state. Following [7], consider a simple one-dimensional ordinary differential equation, where the time evolution of a system F is given by dF = A (F )F + B (F )F; where A (F ) and B (F ) are operators.
Reference: [8] <author> Peter N. Brown and Youcef Saad. </author> <title> Hybrid Krylov methods for nonlinear systems of equations. </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <volume> 11(3) </volume> <pages> 450-481, </pages> <year> 1990. </year>
Reference-contexts: (As for the fact of convergence, in many cases including where J is symmetric any subspace dimension of at least two does not impede theoretical convergence of the overall Newton-Krylov method, see [9], Section 5.) In the examples, we have chosen a dimension of twenty as a typical value (compare <ref> [8] </ref>, ten or fifteen, with [22], twenty). Additional research is needed to pin down the relationship between the amount of storage for the Krylov subspace and the convergence behavior of GMRES in practice in this context. 7.3.6 Biconjugate gradient. <p> , some authors (<ref> [8, 13] </ref>) recommend limiting the decrease in the proposed to a factor of (say) ten, and simultaneously ensuring a minimum decrease in of at least ten percent, that is, imposing a reduction factor of between 0.1 and 0.9. This "approximate minimum" method is discussed and advocated in [8, 13]. The limiter brings the method within the convergence proof of Section 5 for backtracking methods. As a technical matter, it should be noted that the "sufficient decrease" condition is implemented only approximately in the code.
Reference: [9] <author> Peter N. Brown and Youcef Saad. </author> <title> Convergence theory of nonlinear Newton-Krylov algorithms. </title> <journal> SIAM J. Opt., </journal> <volume> 4 </volume> <pages> 297-330, </pages> <year> 1994. </year>
Reference-contexts: that the approximate solution at step k is ffi k = ffi 0 + p k ; p k 2 K k : The conjugate gradient method consists of choosing p k such that the k'th residual r k = F J ffi k is orthogonal to K k . <ref> [9, 21] </ref>. <p> In the process, the convergence guarantee is generally lost, but the method works well in practice. In some cases the convergence of the outer Newton's method is preserved, see Section 7.3.5 and <ref> [9] </ref>, section 5. 4.3 The conjugate gradient algorithm. (Hereafter we consider the general matrix problem Ax = b, which we apply equally to Js = F or related equations for the residual, Jffi = r.) The basic conjugate gradient algorithm (adapted here and in the code from [18], section 10.2) for <p> This has the geometric interpretation that the angle of p k with the gradient of f is bounded away from 90 o . See <ref> [9] </ref>, equations 20, 24. 5.6.1 The global theory. <p> See <ref> [9] </ref>, section 3, for details. Thus, assuming appropriate properties of J and F , compliance with the Goldstein-Armijo conditions is 43 sufficient for global convergence of the inexact Newton method for nonlinear equations. The Armijo rule ensures sufficient decrease in the value of f . <p> This guarantees convergence. In the case of the inexact Newton's method applied to nonlinear equations, there is a minimum ^ &gt; 0, not depending on x k , for which ^ will always satisfy the alpha condition. See <ref> [9] </ref>, Lemma 3.8 and Theorem 3.10. This specific lower bound allows us to dispense with the beta condition, and to give a fully constructive proof of global convergence based on backtracking, even without being able to calculate ^ specifically. <p> So long as the method tries = 1 at least occasionally (and used if it meets the alpha condition), thereafter the method becomes the locally quadratic inexact Newton's method described above. See <ref> [9] </ref>, Section 3.2. In that case the "global" method will be locally quadratic as well. 5.6.2 Related work. As described above, the "global Newton" theory was originally developed for function minimization. <p> steps. 56 This pre-determined dimension of the Krylov subspace, one expects, may well affect the rate of convergence. (As for the fact of convergence, in many cases including where J is symmetric any subspace dimension of at least two does not impede theoretical convergence of the overall Newton-Krylov method, see <ref> [9] </ref>, Section 5.) In the examples, we have chosen a dimension of twenty as a typical value (compare [8], ten or fifteen, with [22], twenty). <p> A simple step length method is the one illustrated above: simply divide the step length in half until sufficient decrease is achieved, successively testing steps of length 1, 0.5, 0.25, and so on. (See for example <ref> [9] </ref>, section 3.1.) Although simplistic, the method will not likely be far wrong. Another approach to step length is to treat the one-dimensional problem as an optimization problem.
Reference: [10] <author> M. C. </author> <title> Cross. Ingredients of a theory of convective textures close to onset. </title> <journal> Phys. Rev. A, </journal> <volume> 25 </volume> <pages> 1065-1076, </pages> <year> 1982. </year>
Reference: [11] <author> M. C. Cross and P. C. Hohenberg. </author> <title> Pattern formation outside of equilibrium. </title> <journal> Rev. Mod. Phys., </journal> <volume> 65(3) </volume> <pages> 851-1112, </pages> <year> 1993. </year>
Reference-contexts: See [20] for a discussion of alternative initial conditions. The value of * was held constant and uniform at 0.1, resulting in a field whose peak amplitude should according to theory (see <ref> [11] </ref>) be approximately 0.4 at the steady state. The initial timestep was 0.0001. The timestep was adaptively adjusted, generally doubling until Newton's method failed (which failure could include failure of the sparse matrix solver).
Reference: [12] <author> Ron S. Dembo, Stanley C. Eisenstat, and Trond Steihaug. </author> <title> Inexact Newton methods. </title> <journal> SIAM J. Numer. Anal, </journal> <volume> 19(2) </volume> <pages> 400-408, </pages> <year> 1982. </year>
Reference-contexts: I first review the definition of an "inexact Newton method", and its application to the class of problems typified by solutions to the time-dependent or time-independent Swift-Hohenberg equations. Then I review the local convergence theory of Newton methods and inexact Newton methods as discussed in <ref> [12, 13, 28] </ref>. Then I discuss "global" Newton methods, where a full Newton step does not lead to an acceptable decrease in the norm of the residual. This theory holds that if an appropriate fractional Newton step is taken, the method will still converge. <p> See, e.g., [13]. An "inexact Newton method" is defined in <ref> [12] </ref> as a sequence fx n g for which the Newton equation J s = F (26) is satisfied only approximately, i.e, where at each step an approximate so-lution to the Newton equation is found in an unspecified manner, but in practice for us by an iterative matrix method that satisfies <p> Next note that j n = 0 corresponds to an exact solution, i.e., unmodified Newton's method. Finally, the characterization by Dennis and More above (see [13]) corresponds in this formulation to the proposition that convergence is superlinear iff j n ! 0. (See <ref> [12] </ref>.) 5.4 Local convergence theory. Overview. The two principal results in [12] important here are that (1) j n &lt; j &lt; 1 is a sufficient condition for local convergence, and (2) j n = O (jjF (x n )jj) is sufficient for quadratic convergence. <p> Finally, the characterization by Dennis and More above (see [13]) corresponds in this formulation to the proposition that convergence is superlinear iff j n ! 0. (See <ref> [12] </ref>.) 5.4 Local convergence theory. Overview. The two principal results in [12] important here are that (1) j n &lt; j &lt; 1 is a sufficient condition for local convergence, and (2) j n = O (jjF (x n )jj) is sufficient for quadratic convergence. <p> The important observation is that only in a small neighborhood of the solution (parameterized by the norm of J 1 and the Lipschitz constant for J ) will Newton's method provably reduce the distance to the solution. Then, citing <ref> [12] </ref>, I note that the result applies equally well to "inexact Newton methods". The argument follows [28] and [12]. <p> Then, citing <ref> [12] </ref>, I note that the result applies equally well to "inexact Newton methods". The argument follows [28] and [12]. <p> Practical implications. The "inexact Newton equation" is a simple equation to implement in practice: one merely solves J s = F 2 In the original text, <ref> [12] </ref> Theorem 3.3, this is stated as "Holder continuous with exponent p", which for p = 1 means Lipschitz; the statement below about quadratic conver gence is stated as convergence "with order at least two". 40 to a relative tolerance jjJ s + F jj minfj; jjF jjg fl jjF jj,
Reference: [13] <author> John E. Dennis and Robert B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1983. </year>
Reference-contexts: I first review the definition of an "inexact Newton method", and its application to the class of problems typified by solutions to the time-dependent or time-independent Swift-Hohenberg equations. Then I review the local convergence theory of Newton methods and inexact Newton methods as discussed in <ref> [12, 13, 28] </ref>. Then I discuss "global" Newton methods, where a full Newton step does not lead to an acceptable decrease in the norm of the residual. This theory holds that if an appropriate fractional Newton step is taken, the method will still converge. <p> See, e.g., <ref> [13] </ref>. <p> Next note that j n = 0 corresponds to an exact solution, i.e., unmodified Newton's method. Finally, the characterization by Dennis and More above (see <ref> [13] </ref>) corresponds in this formulation to the proposition that convergence is superlinear iff j n ! 0. (See [12].) 5.4 Local convergence theory. Overview. <p> p k satisfies the original Goldstein condition (the "beta condition") if f (x k+1 ) f (x k ) + firf (x k ) T p k Another expression, used by Dennis and Schnabel, requires rf (x k+1 ) T p k firf (x k ) T p k See <ref> [13] </ref>, Theorem 6.3.2, p. 120. The important mathematical fact is that the stepsize can simultaneously be made large enough to satisfy the beta condition and small enough to satisfy the alpha condition. This guarantees convergence. <p> In that case the "global" method will be locally quadratic as well. 5.6.2 Related work. As described above, the "global Newton" theory was originally developed for function minimization. The Goldstein-Armijo conditions date from 1966 and 1967, and important results were rigorously proved by Wolfe in 1969 and 1971. See <ref> [13, 15] </ref>. Other work has applied line search or "damped" inexact Newton methods directly to specific classes of nonlinear equations. See [2, 3]. In [3], quadratic convergence of a Newton-like method was proved for a specific sequence of step length factors. <p> , some authors (<ref> [8, 13] </ref>) recommend limiting the decrease in the proposed to a factor of (say) ten, and simultaneously ensuring a minimum decrease in of at least ten percent, that is, imposing a reduction factor of between 0.1 and 0.9. This "approximate minimum" method is discussed and advocated in [8, 13]. The limiter brings the method within the convergence proof of Section 5 for backtracking methods. As a technical matter, it should be noted that the "sufficient decrease" condition is implemented only approximately in the code.
Reference: [14] <author> Stanley C. Eisenstat, Howard C. Elman, and Martin H. Schultz. </author> <title> Variational iterative methods for nonsymmetric systems of linear equations. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 20 </volume> <pages> 345-57, </pages> <year> 1983. </year>
Reference-contexts: The "orthomin" scheme attempts to salvage partial Aconjugacy by choosing a search direction p k that is Aconjugate to the previous s directions, minimizing the Euclidean norm of the residual along that direction. For a discussion of the convergence behavior of "orthomin", see <ref> [14] </ref>. 7.3.8 CG for the normal equations. Even if a real-valued matrix A is not symmetric positive definite, A T A will be.
Reference: [15] <author> Stanley C. Eisenstat and Homer F. Walker. </author> <title> Globally convergent inexact New-ton methods. </title> <journal> SIAM J. Opt., </journal> <volume> 4 </volume> <pages> 393-422, </pages> <year> 1994. </year> <month> 91 </month>
Reference-contexts: Extensions. Eisenstat and Walker <ref> [15] </ref> have also shown convergence in the case of certain sequences fj k g for which j ! 1. This extension has limited practical importance for our case, since it is easy enough to ensure j n &lt; j &lt; 1 from our linear solver. <p> This extension has limited practical importance for our case, since it is easy enough to ensure j n &lt; j &lt; 1 from our linear solver. They also provide results (see, e.g., <ref> [15] </ref>, Theorem 3.4) under which the convergence of jjF jj to zero is sufficient to ensure convergence of the iterates fx k g to a root x fl . Practical implications. <p> In that case the "global" method will be locally quadratic as well. 5.6.2 Related work. As described above, the "global Newton" theory was originally developed for function minimization. The Goldstein-Armijo conditions date from 1966 and 1967, and important results were rigorously proved by Wolfe in 1969 and 1971. See <ref> [13, 15] </ref>. Other work has applied line search or "damped" inexact Newton methods directly to specific classes of nonlinear equations. See [2, 3]. In [3], quadratic convergence of a Newton-like method was proved for a specific sequence of step length factors. <p> The algorithm was apparently designed to limit the number of necessary calculations of the function F , even at the cost of increasing the number of Newton steps. The algorithm in [3] is shown in <ref> [15] </ref> to be a special case of a more general formulation. 6 A comprehensive example 6.1 Introduction This section describes a detailed, comprehensive example used to illustrate the operation in practice of the algorithm described in section 3. First I 45 describe the quantitative parameters used to define the example.
Reference: [16] <author> Alexandre Ern, Vincent Giovangigli, David E. Keyes, and Mitchell D. Smooke. </author> <title> Towards polyalgorithmic linear system solvers for nonlinear elliptic problems. </title> <journal> SIAM J. Sci. Computing, </journal> <volume> 15(3) </volume> <pages> 681-702, </pages> <year> 1994. </year>
Reference-contexts: In the code we choose j = 0:1. It would seem difficult to predict the practical effect of different choices of values for j, because the convergence behavior of Krylov methods can be erratic (see for example <ref> [16] </ref>, Figure 5). If so, it may not be likely that empirical data about the relationship of the value of j to the behavior of Newton's method would be generally useful in other contexts. 5.5 Step length control and "global" convergence. 5.5.1 Introduction. <p> The value is zero at a steady state. The value declines as the steady state is reached. 52 trix problems. See for example <ref> [16, 25] </ref>. Instead, the efficiency of particular matrix solvers is primarily a practical question. The experiments here were motivated in large part by a recent similar study undertaken by numerical scientists in the field of flame propagation [16]. <p> See for example [16, 25]. Instead, the efficiency of particular matrix solvers is primarily a practical question. The experiments here were motivated in large part by a recent similar study undertaken by numerical scientists in the field of flame propagation <ref> [16] </ref>. These investigators discovered that the choice of preconditioner, and more generally of linear method, could not be definitively made at the outset, since the relative efficiencies can change as the character of the solution changes over the course of the time iteration.
Reference: [17] <author> J. P. Gollub, A. R. McCarriar, and J. F. Steinman. </author> <title> Convective pattern evolution and secondary instabilities. </title> <journal> J. Fluid Mech., </journal> <volume> 125 </volume> <pages> 259-281, </pages> <year> 1982. </year>
Reference: [18] <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, </address> <year> 1989. </year>
Reference-contexts: This formalism does not yet tell us how effective the iteration described by Equation (21) will be. The classical theory of iterative methods (see <ref> [18, 21] </ref>) suggests that for the splitting J = L + N we examine the spectral radius ae of the matrix M j L 1 N Setting s = J 1 (F ) to be the solution of Equation (20), and using algebra to rewrite Equation (22) in terms of the <p> Section 7.3.5 and [9], section 5. 4.3 The conjugate gradient algorithm. (Hereafter we consider the general matrix problem Ax = b, which we apply equally to Js = F or related equations for the residual, Jffi = r.) The basic conjugate gradient algorithm (adapted here and in the code from <ref> [18] </ref>, section 10.2) for solving Ax = b (or J s = F ) is as follows: k := 0; x 0 := 0; r 0 := F ; while (r k large) k := k + 1; p 1 = r 0 ; else fi k = r T k2 <p> The following properties of the conjugate gradient algorithm are proved in many places, e.g. <ref> [18, 21] </ref>. * p T k Ap j = 0 for j &lt; k, that is, the search directions p k are A-conjugate. * r T i r j = 0 for i 6= j, that is, the residuals are mutually orthogonal. 32 * p k is a linear function of <p> In particular, the spectrum of the time-independent Jacobian might be used as an indicator for the transition to the time-independent solution. This approach is particularly promising because the Krylov subspace can be used as the starting point for estimating the eigenvalues. See <ref> [18] </ref>. Numerical methods. This work focuses on the applicability of finite difference schemes, and on diagonally preconditioned Krylov methods, which are easily programmed and easily parallelized.
Reference: [19] <author> Donald D. Gray and Aldo Giorgini. </author> <title> The validity of the Boussinesq approximation for liquids and gases. </title> <journal> Int. J. Heat Mass Transfer, </journal> <volume> 19 </volume> <pages> 545-551, </pages> <year> 1976. </year>
Reference: [20] <author> H. S. Greenside and W. M. Coughran, Jr. </author> <title> Nonlinear pattern formation near the onset of Rayleigh-Benard convection. </title> <journal> Phys. Rev. A, </journal> <volume> 30 </volume> <pages> 398-428, </pages> <year> 1984. </year>
Reference-contexts: In the context of linear solvers (Section 4), I drop the superscript k, and focus on the purely linear equation for s, for example i j 3.4.3 Existing approaches: operator splitting. In <ref> [20] </ref>, Equation (20) was tackled using an "operator splitting" approach. This approach was effective for developing time-dependent solutions (where the timestep dt was kept of modest size for accuracy) on a serial workstation. <p> In particular, the availability of the "fast direct constant-coefficient biharmonic solver" of Bjorstad [5] suggests a splitting J = L + N where L is a constant-coefficient biharmonic operator based on B. In <ref> [20] </ref> the researchers found that an operator L = B + C for an appropriately chosen constant (diagonal) C led to a reasonably effective method for the time-dependent equation. This formalism does not yet tell us how effective the iteration described by Equation (21) will be. <p> resolution of eight gridpoints per roll. (Earlier work suggests that this resolution is sufficient to illustrate the features of interest.<ref> [20] </ref>) The initial conditions were chosen randomly at each gridpoint with amplitude 0.02 (that is, uniformly on the interval [0:02; 0:02]) corresponding roughly to small noise around a zero state. See [20] for a discussion of alternative initial conditions. The value of * was held constant and uniform at 0.1, resulting in a field whose peak amplitude should according to theory (see [11]) be approximately 0.4 at the steady state. The initial timestep was 0.0001. <p> This one-dimensional problem may add additional overhead. The effectiveness of the Bjorstad solver in this context may also be related to the question of the effectiveness of the "splitting method" used by previous researchers to solve the Swift-Hohenberg equation in the form Equation (20); see <ref> [20] </ref>. In that case, the researchers found that a constant coefficient biharmonic solver was not an effective basis for a splitting method at large timesteps. Aside from the question of computational efficiency, the incorporation of the Bjorstad "fast direct solver"[5] would have other implications on the algorithm.
Reference: [21] <author> Wolfgang Hackbusch. </author> <title> Iterative Solution of Large Sparse Systems of Equations. </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: This formalism does not yet tell us how effective the iteration described by Equation (21) will be. The classical theory of iterative methods (see <ref> [18, 21] </ref>) suggests that for the splitting J = L + N we examine the spectral radius ae of the matrix M j L 1 N Setting s = J 1 (F ) to be the solution of Equation (20), and using algebra to rewrite Equation (22) in terms of the <p> that the approximate solution at step k is ffi k = ffi 0 + p k ; p k 2 K k : The conjugate gradient method consists of choosing p k such that the k'th residual r k = F J ffi k is orthogonal to K k . <ref> [9, 21] </ref>. <p> The following properties of the conjugate gradient algorithm are proved in many places, e.g. <ref> [18, 21] </ref>. * p T k Ap j = 0 for j &lt; k, that is, the search directions p k are A-conjugate. * r T i r j = 0 for i 6= j, that is, the residuals are mutually orthogonal. 32 * p k is a linear function of
Reference: [22] <author> Mikdat Kadioglu and Stephen Mudrick. </author> <title> On the implementation of the GM-RES(m) method to elliptic equations in meteorology. </title> <journal> J. Comp. Phys., </journal> <volume> 102 </volume> <pages> 348-359, </pages> <year> 1992. </year>
Reference-contexts: convergence, in many cases including where J is symmetric any subspace dimension of at least two does not impede theoretical convergence of the overall Newton-Krylov method, see [9], Section 5.) In the examples, we have chosen a dimension of twenty as a typical value (compare [8], ten or fifteen, with <ref> [22] </ref>, twenty). Additional research is needed to pin down the relationship between the amount of storage for the Krylov subspace and the convergence behavior of GMRES in practice in this context. 7.3.6 Biconjugate gradient.
Reference: [23] <author> John G. Lewis and Robert van de Geijn. </author> <title> Distributed memory matrix-vector multiplication and conjugate gradient algorithms. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 484-492, </pages> <year> 1993. </year>
Reference: [24] <author> Stephen W. Morris, Eberhard Bodenschatz, David S. Cannell, and Guenter Ahlers. </author> <title> Spiral defect chaos in large-aspect-ratio Rayleigh-Benard convection. </title> <journal> Phys. Rev. Lett., </journal> <volume> 71(13) </volume> <pages> 2026-2029, </pages> <month> September </month> <year> 1993. </year>
Reference: [25] <author> N. M. Nachtigal, S.C.Reddy, and L.N. Trefethen. </author> <title> How fast are nonsymmetric matrix iterations? J. </title> <journal> Mat. Anal. Appl., </journal> <volume> 13 </volume> <pages> 778-795, </pages> <year> 1992. </year>
Reference-contexts: The value is zero at a steady state. The value declines as the steady state is reached. 52 trix problems. See for example <ref> [16, 25] </ref>. Instead, the efficiency of particular matrix solvers is primarily a practical question. The experiments here were motivated in large part by a recent similar study undertaken by numerical scientists in the field of flame propagation [16].
Reference: [26] <author> Youcef Saad and M. H. Schultz. </author> <title> Gmres: A generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <volume> 7 </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference: [27] <author> Peter Sonneveld. </author> <title> Cgs, a fast Lanczos-type solver for nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <volume> 10 </volume> <pages> 36-52, </pages> <year> 1989. </year>
Reference-contexts: any method that relies on a three-term recurrence and minimizes the residual vector with respect to some vector norm, if the matrix A is real but not symmetric positive definite ("SPD"), then the algorithm in effect involves the solution of the normal equations with the coefficient matrix A T A. <ref> [27] </ref>. For this section, the testbed is based on a discretization on a rectangular grid. As a consequence, the matrices considered here are all 55 symmetric. <p> As a consequence, the matrices considered here are all 55 symmetric. It is an important open question to extend these experimental findings to nonsymmetric matrices such as those generated by annular and disk domains. 7.3.4 Conjugate gradient squared (CGS). The conjugate gradient squared ("CGS") scheme of <ref> [27] </ref> is a three-term recurrence designed to give good performance in practice, though (in the case of matrices that are not SPD) without guaranteed theoretical convergence properties. <p> The recurrence requires two matrix-vector products per iteration (compared to one for basic CG), but in some sense applies the "contracting effect" of OE (A) twice per step. Also, the transpose need not explicitly be formed in the course of the iteration. See <ref> [27] </ref>. 7.3.5 Generalized minimum residual (GMRES). GMRES is an alternative method for choosing a vector from the Krylov subspace. <p> The method converges for matrices that are not necessarily SPD. Furthermore, where A is symmetric (so that A = A T ) the method is identical to CG (but with an additional, redundant, matrix-vector product). The computational work of Bi-CG is twice the work of the CG algorithm. <ref> [27] </ref>. 7.3.7 Orthomin. The conjugate gradient scheme has the property that (for SPD matrices A) the new search direction p k is Aconjugate to all previous search directions. That property breaks down for nonsymmetric A.
Reference: [28] <author> Josef Stoer and Roland Bulirsch. </author> <title> Introduction to Numerical Analysis, volume 12 of Texts in Applied Mathematics. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: I first review the definition of an "inexact Newton method", and its application to the class of problems typified by solutions to the time-dependent or time-independent Swift-Hohenberg equations. Then I review the local convergence theory of Newton methods and inexact Newton methods as discussed in <ref> [12, 13, 28] </ref>. Then I discuss "global" Newton methods, where a full Newton step does not lead to an acceptable decrease in the norm of the residual. This theory holds that if an appropriate fractional Newton step is taken, the method will still converge. <p> Thus it makes sense that in multiple dimensions the effectiveness of Newton's method would be related to the Lipschitz constant. 37 We have the following Lemma, stated here without proof (see <ref> [28] </ref>, Lemma 5.3.1): If F is Lipschitz with constant fl in a convex set N , then for all x; y 2 N jjF (y) F (x) J (x)(y x)jj 2 A neighborhood N * (x) of a point x 2 R n , defined as N * (x) = fyjjjyxjj <p> Then, citing [12], I note that the result applies equally well to "inexact Newton methods". The argument follows <ref> [28] </ref> and [12]. <p> The remainder of the proof is just algebra, given in many textbooks (see, e.g., <ref> [28] </ref>, Theorem 5.3.2).
Reference: [29] <author> Gilbert Strang. </author> <title> Introduction to Applied Mathematics. </title> <publisher> Wellesley-Cambridge Press, </publisher> <address> Wellesley, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: See [30].) To illustrate the stability limitation on a timestep, consider a general linear biharmonic partial differential equation, @ t OE = (c 0 + c 1 + c 2 2 )OE (15) subject to some appropriate boundary conditions. A standard expansion (see, for example, <ref> [29] </ref>) leads to the conclusion that for some wavelengths the stencil of Equation (14) applied to OE leads to a growth factor G that contributes to instability unless dt 1 : (16) where dt is the timestep and h the mesh spacing.
Reference: [30] <author> John C. Strikwerda. </author> <title> Finite Difference Schemes and Partial Differential Equations. </title> <publisher> Wadsworth & Brookes/Cole, </publisher> <address> Pacific Grove, California, </address> <year> 1989. </year>
Reference-contexts: This stability limitation restricts the timestep to a constant times some power of the spatial mesh size. (Not all explicit methods necessarily have a stability limitation, although the timestep may still be restricted by consistency requirements. See <ref> [30] </ref>.) To illustrate the stability limitation on a timestep, consider a general linear biharmonic partial differential equation, @ t OE = (c 0 + c 1 + c 2 2 )OE (15) subject to some appropriate boundary conditions.
Reference: [31] <author> Andrew M. Stuart and A. R. Humphries. </author> <title> The essential stability of local error control for dynamical systems. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 32 </volume> <pages> 1940-1971, </pages> <year> 1995. </year> <month> 92 </month>
Reference: [32] <author> H. A. van der Vorst. </author> <title> BI-CGSTAB: A fast and smoothly converging variant of BI-CG for the solution of nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Stat. Comp., </journal> <volume> 13 </volume> <pages> 631-644, </pages> <year> 1992. </year>
Reference-contexts: Additional research is needed to pin down the relationship between the amount of storage for the Krylov subspace and the convergence behavior of GMRES in practice in this context. 7.3.6 Biconjugate gradient. The "biconjugate gradient" scheme is discussed in <ref> [32] </ref>. (The "stabilized" modification developed there, which seems to be a combination of biconju-gate gradient and a minimal residual method, is not analyzed in this thesis.) In each iteration, biconjugate gradient incorporates a three-term recurrence for a residual based on A and another for a residual based on A T .
Reference: [33] <author> S. Zaleski, Y. Pomeau, and A. Pumir. </author> <title> Optimal merging of rolls near a plane boundary. </title> <journal> Phys. Rev. A, </journal> <volume> 29 </volume> <pages> 366-370, </pages> <year> 1984. </year> <month> 93 </month>
References-found: 33

