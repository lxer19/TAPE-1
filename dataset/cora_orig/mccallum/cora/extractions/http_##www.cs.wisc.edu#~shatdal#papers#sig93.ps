URL: http://www.cs.wisc.edu/~shatdal/papers/sig93.ps
Refering-URL: http://www.cs.wisc.edu/~shatdal/shatdal.html
Root-URL: 
Email: fshatdal,naughtong@cs.wisc.edu  
Title: Using Shared Virtual Memory for Parallel Join Processing  
Author: Ambuj Shatdal and Jeffrey F. Naughton 
Affiliation: Computer Sciences Department University of Wisconsin-Madison  
Abstract: In this paper, we show that shared virtual memory, in a shared-nothing multiprocessor, facilitates the design and implementation of parallel join processing algorithms that perform significantly better in the presence of skew than previously proposed parallel join processing algorithms. We propose two variants of an algorithm for parallel join processing using shared virtual memory, and perform a detailed simulation to investigate their performance. The algorithm is unique in that it employs both the shared virtual memory paradigm and the message-passing paradigm used by current shared-nothing parallel database systems. The implementation of the algorithm requires few modifications to existing shared-nothing parallel database systems. 
Abstract-found: 1
Intro-found: 1
Reference: [BBDW83] <author> D. Bitton, H. Boral, et al. </author> <title> Parallel algorithms for the execution of relational database operations. </title> <journal> ACM Trans. on Database Systems, </journal> <volume> 8(3), </volume> <month> September </month> <year> 1983. </year>
Reference: [CBZ91] <author> J. B. Carter, J. K. Bennet, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proc. of the 1991 Symp. on Operating System Principles, </booktitle> <year> 1991. </year>
Reference-contexts: To our knowledge, no work has appeared on query processing in systems with SVM. 3 Brief Overview of SVM Shared virtual memory <ref> [LH89, CBZ91] </ref> provides a single virtual address space shared by all the processors in a shared-nothing architecture. <p> This is mainly because page faults depend principally on program properties and only indirectly on the coherence mechanism. In recent years, research into SVM have produced significant performance enhancements by exploiting specific patterns of sharing in the system <ref> [CBZ91] </ref>. This results in a reduction in the number and size of messages. <p> A hashed partitioning strategy is used, where a randomizing function is applied to the key attribute of each tuple to select a particular disk drive. The performance of SVM is modeled explicitly for the shared hash tables under assumptions similar to those made in <ref> [LH89, CBZ91] </ref>. The status variables are ignored because they are updated very infrequently.
Reference: [DG85] <author> D. J. DeWitt and R. Gerber. </author> <title> Multiprocessor hash-based join algorithms. </title> <booktitle> In Proc. of the 12th VLDB Conf., </booktitle> <year> 1985. </year>
Reference-contexts: Our parallel join processing algorithm uses both message passing and SVM. Briefly, stream-oriented processing is handled by message passing, while access to shared data structures is provided in SVM. Both of our parallel join processing algorithms are based upon the parallel hybrid hash join <ref> [DG85, SD89] </ref>. In the absence of skew, this algorithm has been shown to have the best performance. However, in the presence of skew, the performance of hybrid hash join degrades since the response time of the parallel join is limited by that of the slowest processor in the join. <p> The algorithm of Wolf et al. [WDYT90] analyzes the base relations by doing an initial scan on them. This information is used in the actual repartitioning of the relations. Using an analytical model to compare the scheduling hash-join algorithm of [WDYT90] and the hybrid hash-join algorithm of Gamma <ref> [DG85, SD89, DGS + 90] </ref>, Walton et al. conclude that scheduling hash effectively handles redistribution skew while hybrid hash degrades and eventually becomes worse than scheduling hash as redistribution skew increases. <p> Any result tuples from the queries are "sent back to the terminals," an operation that consumes some small amount of processor cycles for network protocol overheads 4 . The default join algorithm used in the simulator is the hybrid hash join algorithm <ref> [DG85, SD89] </ref>. The database itself is modeled as a set of relations. All relations are declustered [RE78, LKB87] (horizontally partitioned) across all the disks in the configuration.
Reference: [DGS + 90] <author> D. DeWitt, S. Ghandeharizadeh, D. Schneider et al. </author> <title> The Gamma database machine project. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 2(1), </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: The algorithm of Wolf et al. [WDYT90] analyzes the base relations by doing an initial scan on them. This information is used in the actual repartitioning of the relations. Using an analytical model to compare the scheduling hash-join algorithm of [WDYT90] and the hybrid hash-join algorithm of Gamma <ref> [DG85, SD89, DGS + 90] </ref>, Walton et al. conclude that scheduling hash effectively handles redistribution skew while hybrid hash degrades and eventually becomes worse than scheduling hash as redistribution skew increases. <p> This realization led us to develop the dual-paradigm algorithm described in the next subsection. 4.2 A Dual-Paradigm Algorithm Our dual-paradigm algorithm begins exactly like the basic parallel hybrid hash algorithm <ref> [DGS + 90] </ref>. Suppose the join is of relations R and S, say with the join condition R:A = S:B, and that R has been chosen as the building relation. At a high level, the join proceeds in two stages: 1. Build. <p> If the skew in building relation is extreme, then SVMRPHHJ does best. This is because it mitigates redistribution skew by doing range partitioning. 5.1 Simulation Methodology The simulator is based upon an earlier, event-driven simulation model of the Gamma parallel database machine <ref> [DGS + 90] </ref>. The earlier model was a useful starting point since it had been validated against the actual Gamma implementation; it had also been used extensively in previous work on parallel database machines [GD90, SD90, HD91].
Reference: [DNSS92] <author> D. J. DeWitt, J. F. Naughton, D. A. Schneider, and S. Seshadri. </author> <title> Practical skew handling in parallel joins. </title> <booktitle> In Proc. of the 19th VLDB Conf., </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: While they did not implement their algorithm nor simulate its perfor Page 2 mance, their analytical model shows that it is effective in handling attribute value skew. DeWitt et al. <ref> [DNSS92] </ref> investigate the use of sampling coupled with range partitioning (instead of hash partitioning) to balance the work among processors by mitigating redistribution skew. The algorithm was very successful in handling redistribution skew, but much less successful in dealing with join product skew. <p> Processor p 1 then proceeds to probe the local bucket hash table built out of bucket zero of S 2 ; that is, it probes the hash table on p 2 . Note 1 Set valued split tables were also used in <ref> [DNSS92] </ref>. Page 4 that this is trivially possible since the hash table was built in SVM. Initially, this probe is likely to cause a SVM fault. However, eventually there will be no more SVM faults since the hash table pages will have been replicated to p 1 . <p> To see this, note that there is no provision for moving hash table pages from one processor to another during the building phase. To handle this problem, we can use the technique proposed in <ref> [DNSS92] </ref>. The idea is that instead of using hashing to partition the relation among the processors of the system, we use range partitioning. <p> HHJ Basic Parallel Hybrid Hash Join. RPHHJ Sampling based Range Partitioning Hybrid Hash Join. This is the algorithm from <ref> [DNSS92] </ref>. SVMHHJ Basic SVM Hybrid Hash Join. This is the dual-paradigm algorithm discussed in Subsec tion 4.2. 2 Every update will result in a message to all the nodes. The nodes must have a separate thread waiting for such message and upon receiving the message take appropriate action.
Reference: [ELZ86] <author> D. L. Eager, E. D. Lazowska, and J. Zahorjan. </author> <title> Adaptive load sharing in homogeneous distributed systems. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-12(5), </volume> <month> May </month> <year> 1986. </year>
Reference-contexts: Thus, instead of precomputing the load at each node and balancing it, as is done in many previous proposals for skew handling join algorithms, each node follows the policy of "don't be idle if there is work left" using a simple heuristic for selecting a processor. Eager et al. <ref> [ELZ86] </ref> shows and we verify that a simple heuristic, like random, for selecting a busy processor works almost as well as more complex kinds (e.g. those involving es timated work left). 4.3 A Sampling Variant The algorithm as described in the previous subsection deals elegantly with join product skew, but it
Reference: [ESW78] <author> R. Epstein, M. Stonebraker, and E. Wong. </author> <title> Distributed query processing in a relational database system. </title> <booktitle> In Proc. of the ACM-SIGMOD Conf., </booktitle> <year> 1978. </year>
Reference-contexts: Effectively, the local bucket hash table at p 2 has been replicated to p 1 in a lazy, "copy-on-reference" fashion. Note that this accomplishes a dynamic subset-replicate <ref> [ESW78] </ref> join of R 2 and S 2 , with R 2 being replicated (in the hash tables) and S 2 being subsetted (by the random selection of p 1 or p 2 .) In this way, once p 1 has updated the forwarding table entry for p 2 , p
Reference: [GD90] <author> S. Ghandeharizadeh and D. J. DeWitt. </author> <title> Hybrid-range partitioning strategy: A new declustering strategy for multiprocessor database machines. </title> <booktitle> In Proc. of the 16th VLDB Conf., </booktitle> <month> September </month> <year> 1990. </year>
Reference-contexts: The earlier model was a useful starting point since it had been validated against the actual Gamma implementation; it had also been used extensively in previous work on parallel database machines <ref> [GD90, SD90, HD91] </ref>. The new simulator, which is much more modular, is written in the CSIM/C++ process-oriented simulation language [Sch90]. The simulator accurately captures the algorithms and techniques used in Gamma.
Reference: [HD91] <author> H. I. Hsiao and D. J. DeWitt. </author> <title> A performance study of three high-availability data replication strategies. </title> <booktitle> In Proc. of the 1st Int'l Conf. on Parallel and Distributed Systems, </booktitle> <month> De-cember </month> <year> 1991. </year>
Reference-contexts: The earlier model was a useful starting point since it had been validated against the actual Gamma implementation; it had also been used extensively in previous work on parallel database machines <ref> [GD90, SD90, HD91] </ref>. The new simulator, which is much more modular, is written in the CSIM/C++ process-oriented simulation language [Sch90]. The simulator accurately captures the algorithms and techniques used in Gamma.
Reference: [HL91] <author> K. A. Hua and C. Lee. </author> <title> Handling data skew in multiprocessor database computers using partition tuning. </title> <booktitle> In Proc. of the 17th VLDB Conf., </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: This result is confirmed in our work. The bucket-spreading parallel hash join [KO90] and its variant tuple interleaving hash join <ref> [HL91] </ref> balance the redistribution skew by ensuring that processors get approximately same number of tuples for the final join phase. This involves sending the tuples twice over the network. Adaptive load balancing hash join [HL91] algorithm attempts to balance the load statically by relocating buckets after initial partition. <p> The bucket-spreading parallel hash join [KO90] and its variant tuple interleaving hash join <ref> [HL91] </ref> balance the redistribution skew by ensuring that processors get approximately same number of tuples for the final join phase. This involves sending the tuples twice over the network. Adaptive load balancing hash join [HL91] algorithm attempts to balance the load statically by relocating buckets after initial partition. Its extended version [HL91] is similar to tuple interleaving hash join but avoids the extra network cost by storing tuples locally. <p> This involves sending the tuples twice over the network. Adaptive load balancing hash join <ref> [HL91] </ref> algorithm attempts to balance the load statically by relocating buckets after initial partition. Its extended version [HL91] is similar to tuple interleaving hash join but avoids the extra network cost by storing tuples locally.
Reference: [HT88] <author> M. Hsu and V.-O. Tam. </author> <title> Managing databases in distributed virtual memory. </title> <type> Technical Report TR-07-88, </type> <institution> Aiken Computation Lab., Harvard Univ., </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: Shared virtual memory has received very little attention in previous database literature|the only work of which we are aware is some early work by the Hsu et al. on transaction processing in an SVM system (see <ref> [HT89, HT88] </ref> for examples of this work). To our knowledge, no work has appeared on query processing in systems with SVM. 3 Brief Overview of SVM Shared virtual memory [LH89, CBZ91] provides a single virtual address space shared by all the processors in a shared-nothing architecture.
Reference: [HT89] <author> M. Hsu and V.-O. Tam. </author> <title> Transaction synchronization in distributed shared virtual memory systems. </title> <type> Technical Report TR-05-89, </type> <institution> Center for Research in Computing Technology, Har-vard Univ., </institution> <year> 1989. </year>
Reference-contexts: Shared virtual memory has received very little attention in previous database literature|the only work of which we are aware is some early work by the Hsu et al. on transaction processing in an SVM system (see <ref> [HT89, HT88] </ref> for examples of this work). To our knowledge, no work has appeared on query processing in systems with SVM. 3 Brief Overview of SVM Shared virtual memory [LH89, CBZ91] provides a single virtual address space shared by all the processors in a shared-nothing architecture.
Reference: [KO90] <author> M. Kitsuregawa and Y. Ogawa. </author> <title> Bucket spreading parallel hash: A new, robust, parallel hash join method for data skew in the Super Database Computer (SDC). </title> <booktitle> In Proc. of the 16th VLDB Conf., </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: This result is confirmed in our work. The bucket-spreading parallel hash join <ref> [KO90] </ref> and its variant tuple interleaving hash join [HL91] balance the redistribution skew by ensuring that processors get approximately same number of tuples for the final join phase. This involves sending the tuples twice over the network. <p> Omiecinski [Omi91] proposed a load balancing hash-join algorithm for systems running on shared physical memory multiprocessors. The algorithm is based on the bucket-spreading algorithm of Kitsuregawa and Ogawa <ref> [KO90] </ref>. Analytical and limited experimental results from a 10 processor Sequent machine show that the algorithm is effective in limiting the effects of attribute value skew for double-skew joins, but again the author did not compare the performance of the algorithm with basic parallel join algorithms.
Reference: [LH89] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4), </volume> <month> November </month> <year> 1989. </year>
Reference-contexts: To our knowledge, no work has appeared on query processing in systems with SVM. 3 Brief Overview of SVM Shared virtual memory <ref> [LH89, CBZ91] </ref> provides a single virtual address space shared by all the processors in a shared-nothing architecture. <p> This protocol allows multiple reader processes to share a page by replication but a writer process obtains an exclusive copy by invalidating the other copies. There are several alternatives to implement the mapping managers using this idea. These are discussed at length in <ref> [LH89] </ref>. When a process wants to access a page which is not in its physical memory, it suffers a page fault. The missing page is then brought in either from the memory of some other processor or from the disk. <p> A hashed partitioning strategy is used, where a randomizing function is applied to the key attribute of each tuple to select a particular disk drive. The performance of SVM is modeled explicitly for the shared hash tables under assumptions similar to those made in <ref> [LH89, CBZ91] </ref>. The status variables are ignored because they are updated very infrequently.
Reference: [LKB87] <author> M. Livny, S. Khoshafian, and H. Boral. </author> <title> Multi-disk management algorithms. </title> <booktitle> In Proc. of the 1987 ACM-SIGMETRICS Conf., </booktitle> <month> May </month> <year> 1987. </year>
Reference-contexts: The default join algorithm used in the simulator is the hybrid hash join algorithm [DG85, SD89]. The database itself is modeled as a set of relations. All relations are declustered <ref> [RE78, LKB87] </ref> (horizontally partitioned) across all the disks in the configuration. A hashed partitioning strategy is used, where a randomizing function is applied to the key attribute of each tuple to select a particular disk drive.
Reference: [LT91] <author> H. Lu and K.-L.Tan. </author> <title> A dynamic and load-balanced task-oriented approach to parallel query processing. </title> <type> DISC Technical Report TRC7/91, </type> <institution> National University of Singapore, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Analytical and limited experimental results from a 10 processor Sequent machine show that the algorithm is effective in limiting the effects of attribute value skew for double-skew joins, but again the author did not compare the performance of the algorithm with basic parallel join algorithms. Lu and Tan <ref> [LT91] </ref> present a dynamic task-oriented algorithm for load balancing in a shared disk and hybrid environment having both private and shared memory.
Reference: [LTS90] <author> H. Lu, K.-L. Tan, and M.-C. Shan. </author> <title> Hash-based join algorithms for multiprocessor computers with shared memory. </title> <booktitle> In Proc. of the 16th VLDB Conf., </booktitle> <month> September </month> <year> 1990. </year>
Reference-contexts: Unless the memory of each processor was large enough to hold the entire global hash table, many of these hash table pages would thrash to and from the local disks of the processors. Other hash based algorithms <ref> [LTS90, Omi91] </ref> suffer the same fate for the same reasons, lack of processor locality and swamping due to replication of hash tables. When we began our work on this problem we developed a series of algorithms that attempted to use the SVM carefully to avoid these two problems.
Reference: [LY90] <author> M. S. Lakshmi and P. S. Yu. </author> <title> Effectiveness of parallel joins. </title> <journal> IEEE Trans. on Knowledge and Data Engineering, </journal> <volume> 2(4), </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: In practice, this implied that given any unbiased partitioning strategy, each processor was likely to have the same amount of work to do. With time this assumption has been challenged (see, e.g. <ref> [LY90, SD89] </ref>) by the claim that many real data sets are not uniform but suffer from data skew. In the presence of such skew, an unbiased partitioning strategy like hashing will result in unequal load on participating processors.
Reference: [Omi91] <author> E. Omiecinski. </author> <title> Performance analysis of a load balancing hash-join algorithm for a shared memory multiprocessor. </title> <booktitle> In Proc. of the 17th VLDB Conf., </booktitle> <month> September </month> <year> 1991. </year>
Reference-contexts: All these algorithms suffer from two major problems: (1) they can not exploit memory the way hybrid hash join algorithm does thus not performing optimally (this difference is analogous to the difference between hybrid hash and GRACE algorithm); and (2) they do not handle join product skew. Omiecinski <ref> [Omi91] </ref> proposed a load balancing hash-join algorithm for systems running on shared physical memory multiprocessors. The algorithm is based on the bucket-spreading algorithm of Kitsuregawa and Ogawa [KO90]. <p> Unless the memory of each processor was large enough to hold the entire global hash table, many of these hash table pages would thrash to and from the local disks of the processors. Other hash based algorithms <ref> [LTS90, Omi91] </ref> suffer the same fate for the same reasons, lack of processor locality and swamping due to replication of hash tables. When we began our work on this problem we developed a series of algorithms that attempted to use the SVM carefully to avoid these two problems.
Reference: [RE78] <author> D. Ries and R. Epstein. </author> <title> Evaluation of distribution criteria for distributed database systems. </title> <type> UCB/ERL Technical Report M78/22, </type> <institution> University of California, Berkeley, </institution> <month> May </month> <year> 1978. </year>
Reference-contexts: The default join algorithm used in the simulator is the hybrid hash join algorithm [DG85, SD89]. The database itself is modeled as a set of relations. All relations are declustered <ref> [RE78, LKB87] </ref> (horizontally partitioned) across all the disks in the configuration. A hashed partitioning strategy is used, where a randomizing function is applied to the key attribute of each tuple to select a particular disk drive.
Reference: [Sch90] <author> H. Schwetman. </author> <title> CSIM users' guide. </title> <type> MCC Tech Report ACT-126-90, </type> <institution> Microelectronics and Computer Technology Corp., </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: The new simulator, which is much more modular, is written in the CSIM/C++ process-oriented simulation language <ref> [Sch90] </ref>. The simulator accurately captures the algorithms and techniques used in Gamma. The remainder of this section provides a more detailed description of the relevant portions of the current simulation model, and concludes with a table of the simulation parameter settings used for this study.
Reference: [SD89] <author> D. A. Schneider and D. J. DeWitt. </author> <title> A performance evaluation of four parallel join algorithms in a shared-nothing multiprocessor environment. </title> <booktitle> In Proc. of the ACM-SIGMOD Conf., </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Our parallel join processing algorithm uses both message passing and SVM. Briefly, stream-oriented processing is handled by message passing, while access to shared data structures is provided in SVM. Both of our parallel join processing algorithms are based upon the parallel hybrid hash join <ref> [DG85, SD89] </ref>. In the absence of skew, this algorithm has been shown to have the best performance. However, in the presence of skew, the performance of hybrid hash join degrades since the response time of the parallel join is limited by that of the slowest processor in the join. <p> In practice, this implied that given any unbiased partitioning strategy, each processor was likely to have the same amount of work to do. With time this assumption has been challenged (see, e.g. <ref> [LY90, SD89] </ref>) by the claim that many real data sets are not uniform but suffer from data skew. In the presence of such skew, an unbiased partitioning strategy like hashing will result in unequal load on participating processors. <p> The algorithm of Wolf et al. [WDYT90] analyzes the base relations by doing an initial scan on them. This information is used in the actual repartitioning of the relations. Using an analytical model to compare the scheduling hash-join algorithm of [WDYT90] and the hybrid hash-join algorithm of Gamma <ref> [DG85, SD89, DGS + 90] </ref>, Walton et al. conclude that scheduling hash effectively handles redistribution skew while hybrid hash degrades and eventually becomes worse than scheduling hash as redistribution skew increases. <p> However, unless the join is significantly skewed, the absolute performance of hybrid hash is significantly better than that of scheduling hash due to the absence of the initial scan of relations. Schneider and Dewitt <ref> [SD89] </ref> conclude that the parallel hash-based join algorithms (Hybrid, Grace, and Simple) are sensitive to redistribution skew resulting from attribute value skew in the "building" relation (due to hash table overflow) but are relatively insensitive to redistribution skew in the "probing" relation. This result is confirmed in our work. <p> Any result tuples from the queries are "sent back to the terminals," an operation that consumes some small amount of processor cycles for network protocol overheads 4 . The default join algorithm used in the simulator is the hybrid hash join algorithm <ref> [DG85, SD89] </ref>. The database itself is modeled as a set of relations. All relations are declustered [RE78, LKB87] (horizontally partitioned) across all the disks in the configuration.
Reference: [SD90] <author> D. Schneider and D. J. DeWitt. </author> <title> Tradeoffs in processing complex join queries via hashing in multiprocessor database machines. </title> <booktitle> In Proc. of the 16th VLDB Conf., </booktitle> <month> September </month> <year> 1990. </year>
Reference-contexts: The earlier model was a useful starting point since it had been validated against the actual Gamma implementation; it had also been used extensively in previous work on parallel database machines <ref> [GD90, SD90, HD91] </ref>. The new simulator, which is much more modular, is written in the CSIM/C++ process-oriented simulation language [Sch90]. The simulator accurately captures the algorithms and techniques used in Gamma.
Reference: [WDJ91] <author> C. B. Walton, A. G. Dale, and R. M. Jenevein. </author> <title> A taxonomy and performance model of data skew effects in parallel joins. </title> <booktitle> In Proc. of the 17th VLDB Conf., </booktitle> <month> September </month> <year> 1991. </year>
Reference-contexts: Also, these algorithms have no obvious extension to handling mul-tiway joins without storing the intermediate relations. We discuss the prominent ones in brief. Walton et al. <ref> [WDJ91] </ref> present a taxonomy of skew in parallel databases. They made the distinction between attribute value skew, which is skew inherent in the dataset, and partition skew, which occurs in parallel machines when the load is not balanced between the nodes.
Reference: [WDYT90] <author> J. L. Wolf, D. M. Dias, et al. </author> <title> An effective algorithm for parallelizing hash joins in the presence of data skew. </title> <institution> IBM T. J. Watson Research Center Tech Report RC 15510, </institution> <year> 1990. </year> <pages> Page 10 </pages>
Reference-contexts: Different kinds of partition skew can be classified as initial tuple placement skew, selectivity skew, redistribution skew, and join product skew. The algorithm of Wolf et al. <ref> [WDYT90] </ref> analyzes the base relations by doing an initial scan on them. This information is used in the actual repartitioning of the relations. Using an analytical model to compare the scheduling hash-join algorithm of [WDYT90] and the hybrid hash-join algorithm of Gamma [DG85, SD89, DGS + 90], Walton et al. conclude <p> The algorithm of Wolf et al. <ref> [WDYT90] </ref> analyzes the base relations by doing an initial scan on them. This information is used in the actual repartitioning of the relations. Using an analytical model to compare the scheduling hash-join algorithm of [WDYT90] and the hybrid hash-join algorithm of Gamma [DG85, SD89, DGS + 90], Walton et al. conclude that scheduling hash effectively handles redistribution skew while hybrid hash degrades and eventually becomes worse than scheduling hash as redistribution skew increases.
References-found: 25

