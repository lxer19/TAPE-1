URL: http://www.cs.colostate.edu/~pyeatt/kbse96.ps
Refering-URL: http://www.cs.colostate.edu/~pyeatt/
Root-URL: 
Email: email: fhowe,pyeattg@cs.colostate.edu  
Title: Constructing Transition Models of AI Planner Behavior  
Author: Adele E. Howe Larry D. Pyeatt 
Web: URL: http://www.cs.colostate.edu/fhowe,pyeattg  
Address: Fort Collins, CO 80523  
Affiliation: Computer Science Department Colorado State University  
Abstract: Evaluation and debugging of AI systems require coherent views of program performance and behavior. We have developed a family of methods, called Dependency Detection, for analyzing execution traces for small patterns. Unfortunately, these methods provide only a local view of program behavior. The approach described in this paper integrates two methods, dependency detection [10] and CHAID-based analysis [3], to produce an abstract model of system behavior: a transition diagram of merged states. We present the algorithm and demonstrate it on synthetic examples and data from two AI planning and control systems. The models produced by the algorithm summarize sequences and cycles evident in the synthesized models and highlight some key aspects of behavior in the two systems. We conclude by identifying some of the inadequacies of the current algorithm and suggesting enhancements. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Abrams, A. Batongbacal, R. Ribler, and D. Vazirani. CHITRA94: </author> <title> A tool to dynamically characterize ensembles of traces for input data modeling and output analysis. </title> <institution> Department of Computer Science 94-21, Virginia Polytechnical Institute and State University, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: The Chitra family of systems (Chitra 92-96) support the modeling and visualization of performance data from parallel and distributed systems <ref> [1, 2] </ref>. At an abstract level, an AI planner operating in its environment can be characterized as two partially predictable interacting processes. Our method combines two approaches: one from statistical evaluation of AI systems and one from performance analysis of distributed systems.
Reference: [2] <author> M. Abrams and C. </author> <title> Group. Chitra project page. </title> <address> http://info.cs.vt.edu/chitra/. </address>
Reference-contexts: The Chitra family of systems (Chitra 92-96) support the modeling and visualization of performance data from parallel and distributed systems <ref> [1, 2] </ref>. At an abstract level, an AI planner operating in its environment can be characterized as two partially predictable interacting processes. Our method combines two approaches: one from statistical evaluation of AI systems and one from performance analysis of distributed systems.
Reference: [3] <author> H. T. Cadiz. </author> <title> The development of a CHAID-based model for CHITRA93. </title> <institution> Computer science dept., Virginia Polytechnic Institute, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: Extensions to the basic model have been used to find temporally disparate dependencies as well as to compare similar dependencies to determine which exerts the strongest effect. The CHAID-based analysis constructs transition matrices for describing event traces from distributed systems <ref> [3] </ref>. The transition matrices include transition probabilities from one event to another in the traces and can be used as the basis for Semi-Markov models. <p> Thus, the models were limited to representing dependencies between predecessors and their immediate successors. As an enhancement, the CHAID-based model developed for Chitra93 analyzes execution sequences to construct an n-step transition probability matrix <ref> [3] </ref>. This tool allows a developer to find relationships between pairs of events separated by a set number of other events. Given a set of target events and a temporal window, the CHAID-based analysis searches for the most predictive position within the window for each of the targets.
Reference: [4] <author> S. A. Chien. </author> <title> Static and completion analysis for planning knowledge base development and verification. </title> <booktitle> In Proceedings of the Third International Conference on Artificial Intelligence Planning Systems (AIPS96), </booktitle> <pages> pages 53-61, </pages> <address> Edin-burgh, Scotland, </address> <year> 1996. </year>
Reference-contexts: Debugging of AI systems has been largely ignored, but two notable exceptions are Chien's research in static and dynamic analysis of planner knowledge bases <ref> [4] </ref> and Fickas et al.'s work on predicting failure [7]. The difficulty of evaluating and debugging in distributed systems has led to many tools and techniques for visualizing and assessing their performance.
Reference: [5] <author> P. R. Cohen. </author> <title> Empirical Methods for Artificial Intelligence. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: Within AI, evaluation research focuses on metrics for performance and methods for designing and analyzing experiments (see <ref> [5] </ref> for an overview of this field). Researchers in validation and verification of knowledge based systems have developed methods for measuring and assessing performance in predominantly rule-based systems (see [8] for the most recent proceedings from a continuing workshop series on the topic).
Reference: [6] <author> P. R. Cohen, M. Greenberg, D. Hart, and A. Howe. </author> <title> Trial by fire: Understanding the design requirements for agents in complex environments. </title> <journal> AI Magazine, </journal> <month> Fall </month> <year> 1989. </year>
Reference-contexts: The Phoenix system encompasses a simulation of forest fire fighting in Yellowstone National Park and the agents that operate within it <ref> [6] </ref>. Typically, we configured the simulation to include about 10 agents working together over 60-100 hours of simulated time. Decisions about what actions to take often had long term effects that were difficult for programmers to predict.
Reference: [7] <author> S. Fickas, R. Helm, and M. Feather. </author> <title> When things go wrong: Predicting failure in multi-agent systems. </title> <institution> Department of Computer and Information Science CIS-TR-91-15, University of Oregon, Eugene, </institution> <address> OR, </address> <year> 1991. </year>
Reference-contexts: Debugging of AI systems has been largely ignored, but two notable exceptions are Chien's research in static and dynamic analysis of planner knowledge bases [4] and Fickas et al.'s work on predicting failure <ref> [7] </ref>. The difficulty of evaluating and debugging in distributed systems has led to many tools and techniques for visualizing and assessing their performance.
Reference: [8] <editor> R. Gamble and C. Landuaer, editors. </editor> <booktitle> Working Notes of the IJCAI-95 Workshop on Verification and Validation of Knowledge-Based Systems, </booktitle> <address> Montreal, CA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Within AI, evaluation research focuses on metrics for performance and methods for designing and analyzing experiments (see [5] for an overview of this field). Researchers in validation and verification of knowledge based systems have developed methods for measuring and assessing performance in predominantly rule-based systems (see <ref> [8] </ref> for the most recent proceedings from a continuing workshop series on the topic). Debugging of AI systems has been largely ignored, but two notable exceptions are Chien's research in static and dynamic analysis of planner knowledge bases [4] and Fickas et al.'s work on predicting failure [7].
Reference: [9] <author> A. E. Howe. </author> <title> Detecting imperfect patterns in event streams using local search. </title> <editor> In D. Fisher and H. Lenz, editors, </editor> <booktitle> Learning from Data: Artificial Intelligence and Statistics V. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: By introducing heuristics and user guidance into the search, DD has been extended to longer, more complex precursors <ref> [9] </ref> (e.g., precursors separated by up to eight events, precursors that include wild cards for don't care positions, and relative order precursors with an absolute length that can vary over a window) and multiple streams (i.e., events that can be vectors of values) [14]. 2.2 CHAID-based Model for Chitra93 The CHAID-based
Reference: [10] <author> A. E. Howe and P. R. Cohen. </author> <title> Detecting and explaining dependencies in execution traces. </title> <editor> In P. Cheeseman and R. Oldford, editors, </editor> <booktitle> Selecting Models from Data; Artificial Intelligence and Statistics IV, </booktitle> <pages> pages 71-78. </pages> <publisher> Springer-Verlag, </publisher> <address> NY,NY, </address> <year> 1994. </year>
Reference-contexts: This method integrates our basic Dependency Detection method with a Chi Automatic Interaction Detection (CHAID) based method of constructing n-step transition matrices. DD models the relationships between substrings of discrete events in execution traces of a planner <ref> [10] </ref>. DD tests whether the occurrence of one event depends on the occurrence of other events prior to it (i.e., whether one set of events appears to cause another event).
Reference: [11] <author> A. E. Howe and P. R. Cohen. </author> <title> Understanding planner behavior. </title> <journal> Artificial Intelligence, </journal> <volume> 76(1-2):125-166, </volume> <year> 1995. </year>
Reference-contexts: For a more practical demonstration of our method, we constructed behavioral models for two systems for planning and control: Phoenix and RARS. 5.1 The Phoenix Planner Dependency Detection was originally motivated by the difficulties encountered in debugging the Phoenix planning system <ref> [11] </ref>. The Phoenix system encompasses a simulation of forest fire fighting in Yellowstone National Park and the agents that operate within it [6]. Typically, we configured the simulation to include about 10 agents working together over 60-100 hours of simulated time. <p> By searching for static and dynamic connections between the plan actions involved, we identified bugs (e.g., variables calculated incorrectly and improper plan synchronization) using a methodology called Failure Recovery Analysis <ref> [11] </ref>. However, these bugs still involved fairly simple interactions local to just a few events. Some of the more pernicious failures were included in many dependencies, making it difficult to determine what was causing the problem.
Reference: [12] <author> A. E. Howe and D. Peterson. </author> <title> Validation and verification in the planner life cycle. </title> <booktitle> In Working Notes of the IJCAI'95 Workshop on Validation & Verification of Knowledge-Based Systems, </booktitle> <address> Montreal, CA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: As practical applications, we will be incorporating the model generation into two other projects: a debugging assistant and a two layer control system. We have been developing a tool for planner debugging, the Planner Debugging Assistant <ref> [12] </ref> (PDA). Currently, PDA relies exclusively on dependency detection. With transition models as a supplement, programmers should also be able to determine more complicated sources of bugs (involving more interactions) and possible consequences of changing the code.
Reference: [13] <author> T. J. Lee. Chitra93: </author> <title> A tool to analyze system behavior by visualizing and modeling ensembles of traces. </title> <institution> Computer science dept., Virginia Polytechnic Institute, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: While a review of those efforts is well beyond the scope of this paper, one tool that was obviously applicable to the problem of characterizing AI planner behavior is Chitra <ref> [13] </ref>. The Chitra family of systems (Chitra 92-96) support the modeling and visualization of performance data from parallel and distributed systems [1, 2]. At an abstract level, an AI planner operating in its environment can be characterized as two partially predictable interacting processes. <p> positions, and relative order precursors with an absolute length that can vary over a window) and multiple streams (i.e., events that can be vectors of values) [14]. 2.2 CHAID-based Model for Chitra93 The CHAID-based analysis technique was developed for the Chitra93 system for modeling visualizing execution traces from distributed systems <ref> [13] </ref>. With direction from a user, Chitra93 could develop a continuous time, semi-Markov (CTSM) model, which was based on the Markov-dependence assumption. Thus, the models were limited to representing dependencies between predecessors and their immediate successors.
Reference: [14] <author> T. Oates, D. Gregory, and P. R. Cohen. </author> <title> Detecting complex dependencies in categorical data. </title> <booktitle> In Preliminary Papers of the Fifthe International Workshop on Artificial Intelligence and Statistics, </booktitle> <month> January </month> <year> 1995. </year>
Reference-contexts: been extended to longer, more complex precursors [9] (e.g., precursors separated by up to eight events, precursors that include wild cards for don't care positions, and relative order precursors with an absolute length that can vary over a window) and multiple streams (i.e., events that can be vectors of values) <ref> [14] </ref>. 2.2 CHAID-based Model for Chitra93 The CHAID-based analysis technique was developed for the Chitra93 system for modeling visualizing execution traces from distributed systems [13]. With direction from a user, Chitra93 could develop a continuous time, semi-Markov (CTSM) model, which was based on the Markov-dependence assumption.
Reference: [15] <editor> R. R. Sokal and F. J. Rohlf. Biometry: </editor> <booktitle> The Principles and Practice of Statistics in Biological Research. W.H. </booktitle> <publisher> Freeman and Co., </publisher> <address> New York, </address> <note> second edition, </note> <year> 1981. </year>
Reference-contexts: A statistical test, the G-test, indicates whether the two events are likely to be dependent <ref> [15] </ref>; in this case, G = 5:174; p &lt; :023, which means that the two events are unlikely to be independent and conclude that T t depends on P a .
Reference: [16] <author> M. E. Timin. </author> <title> Robot Auto Racing Simulator. </title> <note> Anonymous ftp from ftp.ijs.com:/rars, 1995. 9 </note>
Reference-contexts: To take that step will require additional state information captured in the execution traces. 5.2 The RARS Control System We are also interested in detecting the causes of agent failures in the Robot Automobile Racing Simulator (RARS) <ref> [16] </ref>. This simulator is designed to allow different agents to compete in simulated automobile races. RARS runs through the simulation at a fixed lockstep. At each time step, RARS updates the position and velocity of each car based on the environment model and control signals.
References-found: 16

