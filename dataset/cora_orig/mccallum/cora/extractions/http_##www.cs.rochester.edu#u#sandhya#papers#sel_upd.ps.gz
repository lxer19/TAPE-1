URL: http://www.cs.rochester.edu/u/sandhya/papers/sel_upd.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/sandhya/papers/
Root-URL: 
Email: e-mail: sandhya@cs.rice.edu  
Title: Compiler-Directed Selective Update Mechanisms for Software Distributed Shared Memory  
Author: Sandhya Dwarkadas, Alan L. Cox, Honghui Lu, and Willy Zwaenepoel 
Address: P.O. Box 1892 Houston, TX 77251  
Affiliation: Rice University Department of Computer Science  
Pubnum: Technical Report TR95-253  
Abstract: We estimate the performance gains of a compiler-directed selective update protocol for software distributed shared memory systems. Our base system uses an invalidate-based lazy release consistent protocol. The goal is to reduce the cost of communication, by aggregating messages, by reducing the effects of false sharing, and by eliminating the communication of unnecessary data. We achieve our goal through the use of compiler analysis for prefetching and for further relaxing the consistency model. Well-known compiler techniques, such as regular section analysis, can be adapted to determine data access patterns between synchronization points and supply the necessary information to the runtime system. We propose three selective update techniques - cross-synchronization prefetching, partial update, and consistency elimination, and describe the compiler and runtime support necessary. Cross-synchronization prefetching updates requested data at the time of synchronization without changing the consistency model. Partial update relaxes the consistency model in addition to prefetching the requested data. Data not requested is not kept consistent, reducing both consistency maintenance overhead and the effects of false sharing. Consistency elimination relaxes the consistency model by ensuring that data is not updated when accessed. This technique deals with write-first data access patterns, where data is written before it is read, between synchronization operations. We augmented the base runtime system to support the selective update protocol and hand-modified programs to take advantage of it. Our experimental platform is a 100Mbps ATM network connecting 8 DECStation-5000/240s running Ultrix. The compiler-directed selective update protocol improves performance for all the programs examined when compared to either an invalidate or a runtime hybrid protocol, with a 16% improvement for Integer Sort, 11% for Water, 35% for FFT, and a 10% improvement for SOR when compared to the invalidate protocol. Performance is equivalent to the hand-tuned message passing versions of the program in some cases. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Adve and M. Hill. </author> <title> Weak ordering: A new definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Figure 2 provides an example of how the protocol works. At each acquire, invalidations are piggybacked on the reply message for all pages modified "preceding" the acquire (The term "preceding" is to be interpreted here as preceding in the happens-before-1 partial order <ref> [1] </ref>).
Reference: [2] <author> D. Bailey, J. Barton, T. Lasinski, and H. Simon. </author> <title> The NAS parallel benchmarks. </title> <type> Technical Report TR RNR-91-002, </type> <institution> NASA Ames, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: The TreadMarks DSM on which our optimizations are prototyped, is implemented as a user-level library that is linked in with the application program. No kernel modifications are necessary. 4.2 Results We present results for four applications Integer Sort and 3-D FFT from the NAS benchmark suite <ref> [2] </ref>, M-Water from the SPLASH benchmark suite [24], and SOR, a locally developed program. Integer Sort uses cross-synchronization prefetching and consistency elimination. M-Water uses partial updates to avoid false sharing. <p> Messages and KBytes represent the total number of messages and amount of data communicated during each timed execution. The performance of each application is described in detail in the following sections. 4.3 Discussion 4.3.1 Integer Sort Integer Sort (IS) is a program from the NAS benchmark suite <ref> [2] </ref>. IS requires ranking an unsorted sequence of N keys. The rank of a key in a sequence is the index value i that the key would have if the sequence of keys were sorted.
Reference: [3] <author> V. Balasundaram and K. Kennedy. </author> <title> A technique for summarizing data access and its use in parallelism enhancing transformations. </title> <booktitle> In Proceedings of the SIGPLAN `89 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Despite the recent improvements in memory consistency algorithms [15, 27], hand-tuned message-passing programs frequently outperform comparable shared memory programs by a significant margin. We believe that compilation techniques developed for data parallel languages, such as regular section analysis <ref> [4, 3] </ref>, can be used to reduce this gap for explicitly parallel programs. In this paper, we first describe how a compiler can help the DSM system to improve performance. <p> Our approach allows the association of different data items with a lock at each instantiation. Compiler Support. Regular section analysis is used in compilers to determine array accesses in a loop nest <ref> [3, 4] </ref>. Regular section descriptors [4] provide compact representations of data accesses made, and can be used to efficiently supply the required access information to the runtime system. <p> Regular section descriptors [4] provide compact representations of data accesses made, and can be used to efficiently supply the required access information to the runtime system. In the case of large prefetch requests, data access descriptors <ref> [3] </ref>, which describe the order in which the accesses are made, may be used to determine the order in which the update messages are sent out.
Reference: [4] <author> D. Callahan. </author> <title> A Global Approach to Detection of Parallelism. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> March </month> <year> 1987. </year>
Reference-contexts: Despite the recent improvements in memory consistency algorithms [15, 27], hand-tuned message-passing programs frequently outperform comparable shared memory programs by a significant margin. We believe that compilation techniques developed for data parallel languages, such as regular section analysis <ref> [4, 3] </ref>, can be used to reduce this gap for explicitly parallel programs. In this paper, we first describe how a compiler can help the DSM system to improve performance. <p> Our approach allows the association of different data items with a lock at each instantiation. Compiler Support. Regular section analysis is used in compilers to determine array accesses in a loop nest <ref> [3, 4] </ref>. Regular section descriptors [4] provide compact representations of data accesses made, and can be used to efficiently supply the required access information to the runtime system. <p> Our approach allows the association of different data items with a lock at each instantiation. Compiler Support. Regular section analysis is used in compilers to determine array accesses in a loop nest [3, 4]. Regular section descriptors <ref> [4] </ref> provide compact representations of data accesses made, and can be used to efficiently supply the required access information to the runtime system.
Reference: [5] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Mowry et al. [22] discuss the design and evaluation of a compiler algorithm for prefetching. Their algorithm concentrates on improving the performance of cache-based systems and issues prefetch requests for data that are likely to incur a cache miss. Porterfield et al. <ref> [5] </ref> present an algorithm for 18 inserting prefetches one loop iteration ahead. Gornish, Granston, and Veidenbaum [11] present an algorithm for determining the earliest time when it is safe to prefetch shared data. Our work differs in the granularity of information required, and takes advantage of the software-based consistency maintenance.
Reference: [6] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: A major obstacle to continued growth is the 1 difficulty of programming in the message-passing paradigm that is native to the hardware. As one way to overcome this obstacle, researchers have developed software distributed shared memory (DSM) systems that enable processes on different workstations to share memory <ref> [20, 9, 7, 6, 15, 27, 16] </ref>. Despite the recent improvements in memory consistency algorithms [15, 27], hand-tuned message-passing programs frequently outperform comparable shared memory programs by a significant margin. <p> For example, the release consistency model [10] permits several implementation techniques for reducing the amount of communication without significantly changing the programming model from sequential consistency. Two such techniques are lazy release consistency [14] and the multiple-writer protocol <ref> [6] </ref>. Lazy release consistency (LRC) delays the communication of invalidations or updates to shared memory until a synchronization point in the program. In contrast to earlier techniques, such as eager release consistency [6], LRC doesn't require a broadcast, only the synchronizing processors communicate. The multiple-writer protocol [6] allows several processes to <p> Two such techniques are lazy release consistency [14] and the multiple-writer protocol <ref> [6] </ref>. Lazy release consistency (LRC) delays the communication of invalidations or updates to shared memory until a synchronization point in the program. In contrast to earlier techniques, such as eager release consistency [6], LRC doesn't require a broadcast, only the synchronizing processors communicate. The multiple-writer protocol [6] allows several processes to simultaneously modify different parts of a page without having the page "ping-pong" between the processors. Despite these techniques, certain shared-memory programs don't perform as well as their message-passing counterparts. <p> and the multiple-writer protocol <ref> [6] </ref>. Lazy release consistency (LRC) delays the communication of invalidations or updates to shared memory until a synchronization point in the program. In contrast to earlier techniques, such as eager release consistency [6], LRC doesn't require a broadcast, only the synchronizing processors communicate. The multiple-writer protocol [6] allows several processes to simultaneously modify different parts of a page without having the page "ping-pong" between the processors. Despite these techniques, certain shared-memory programs don't perform as well as their message-passing counterparts. We have found several reasons for this.
Reference: [7] <author> P. Dasgupta, R.C. Chen, S. Menon, M. Pearson, R. Ananthanarayanan, U. Ramachandran, M. Ahamad, R. LeBlanc Jr., W. Applebe, J.M. Bernabeu-Auban, P.W. Hutto, M.Y.A. Khalidi, and C.J. Wileknloh. </author> <title> The design and implementation of the Clouds distributed operating system. </title> <journal> Computing Systems Journal, </journal> <volume> 3, </volume> <month> Winter </month> <year> 1990. </year>
Reference-contexts: A major obstacle to continued growth is the 1 difficulty of programming in the message-passing paradigm that is native to the hardware. As one way to overcome this obstacle, researchers have developed software distributed shared memory (DSM) systems that enable processes on different workstations to share memory <ref> [20, 9, 7, 6, 15, 27, 16] </ref>. Despite the recent improvements in memory consistency algorithms [15, 27], hand-tuned message-passing programs frequently outperform comparable shared memory programs by a significant margin. <p> In addition, computation and communication are overlapped. Midway [27] attempts to provide a similar functionality to cross-synchronization prefetching at the expense of additional user complexity. Clouds <ref> [7] </ref> is a distributed system that uses objects to achieve a similar purpose. The disadvantage of these approaches is that data must be explicitly associated by the program with the synchronization variable or object in question, and additional communication is required to change this association.
Reference: [8] <author> S. Dwarkadas, P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 244-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The invalidate-based protocol has the advantage that it communicates data only when necessary. However, it has the disadvantage that it suffers from the most access misses. The run-time hybrid invalidate/update-based protocol <ref> [8] </ref> is intended to reduce communication by aggregating data communication and combining it with synchronization. In this protocol, data is piggy-backed on the message releasing a synchronization object when the releaser believes that the acquiring processor caches that data. <p> page is later accessed, a page fault is generated and the modifications to the page are retrieved in the form of diffs. 5 2.3 An Alternative Runtime Hybrid Protocol A runtime approach to address the problem of access miss latency is the hybrid invalidate/update protocol described in Dwarkadas et al. <ref> [8] </ref>. At a synchronization operation, updates are sent for pages for which the acquirer is in the releaser's approximate copyset and for which the releaser has all the necessary diffs present. Invalidations are sent for the remaining modifications.
Reference: [9] <author> B. Fleisch and G. Popek. </author> <title> Mirage: A coherent distributed shared memory design. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 211-223, </pages> <month> December </month> <year> 1989. </year> <month> 19 </month>
Reference-contexts: A major obstacle to continued growth is the 1 difficulty of programming in the message-passing paradigm that is native to the hardware. As one way to overcome this obstacle, researchers have developed software distributed shared memory (DSM) systems that enable processes on different workstations to share memory <ref> [20, 9, 7, 6, 15, 27, 16] </ref>. Despite the recent improvements in memory consistency algorithms [15, 27], hand-tuned message-passing programs frequently outperform comparable shared memory programs by a significant margin.
Reference: [10] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Unfortunately, sequential consistency requires a total ordering of all shared data accesses, sometimes resulting in communication at every write to shared memory. To reduce the amount of communication, state-of-the-art software DSM systems now use relaxed memory models [15, 27]. For example, the release consistency model <ref> [10] </ref> permits several implementation techniques for reducing the amount of communication without significantly changing the programming model from sequential consistency. Two such techniques are lazy release consistency [14] and the multiple-writer protocol [6]. <p> The IVY system [20] implemented this model. The disadvantage of this model is that it can result in an excessively large amount of communication since modifications to shared data must be sent out immediately in the form of invalidates or updates. Release consistency (RC) <ref> [10] </ref> is a relaxed memory consistency model that addresses the abovementioned efficiency problems of sequential consistency. In RC, synchronization operations are made explicit and categorized into acquires (corresponding to getting access to data) and releases (corresponding to providing access to data).
Reference: [11] <author> E. Gornish, E. Granston, and A. Veidenbaum. </author> <title> Compiler-directed data prefetching in multiprocessors with memory hierarchies. </title> <booktitle> In Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <year> 1990. </year>
Reference-contexts: Their algorithm concentrates on improving the performance of cache-based systems and issues prefetch requests for data that are likely to incur a cache miss. Porterfield et al. [5] present an algorithm for 18 inserting prefetches one loop iteration ahead. Gornish, Granston, and Veidenbaum <ref> [11] </ref> present an algorithm for determining the earliest time when it is safe to prefetch shared data. Our work differs in the granularity of information required, and takes advantage of the software-based consistency maintenance.
Reference: [12] <author> F. Granston and H. Wijshoff. </author> <title> Managing pages in shared virtual memory systems: Getting the compiler into the game. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: Our techniques concentrate on developing methods to reduce the amount of data transferred and the number of messages communicated by a program. Granston and Wijshoff <ref> [12] </ref> present techniques to perform data layout optimizations for distributed shared memory in order to reduce false sharing. These techniques work in conjunction with those described here for false sharing within a data structure.
Reference: [13] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification. </title> <booktitle> Scientific Programming, </booktitle> <address> 2(1-2):1-170, </address> <year> 1993. </year>
Reference-contexts: Implicit parallelism, as in HPF <ref> [13] </ref>, achieves the same goal by relying on user-provided data distributions which are then used by the compiler to generate message passing code. This approach is suitable for data-parallel programs, such as SOR. Programs exhibiting dynamic parallelism, such as TSP, are not easily expressed in the HPF framework.
Reference: [14] <author> P. Keleher, A. L. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: For example, the release consistency model [10] permits several implementation techniques for reducing the amount of communication without significantly changing the programming model from sequential consistency. Two such techniques are lazy release consistency <ref> [14] </ref> and the multiple-writer protocol [6]. Lazy release consistency (LRC) delays the communication of invalidations or updates to shared memory until a synchronization point in the program. In contrast to earlier techniques, such as eager release consistency [6], LRC doesn't require a broadcast, only the synchronizing processors communicate. <p> RC, however, requires that the non-synchronizing shared memory accesses be visible at all processors before the corresponding release can perform at any processor. This potentially implies communication with every processor at a synchronization point. TreadMarks uses lazy release consistency (LRC) <ref> [14] </ref>, which is a lazy implementation of the release consistency model. The LRC algorithm delays the propagation of data modifications to a processor until that processor executes an acquire.
Reference: [15] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: A major obstacle to continued growth is the 1 difficulty of programming in the message-passing paradigm that is native to the hardware. As one way to overcome this obstacle, researchers have developed software distributed shared memory (DSM) systems that enable processes on different workstations to share memory <ref> [20, 9, 7, 6, 15, 27, 16] </ref>. Despite the recent improvements in memory consistency algorithms [15, 27], hand-tuned message-passing programs frequently outperform comparable shared memory programs by a significant margin. <p> As one way to overcome this obstacle, researchers have developed software distributed shared memory (DSM) systems that enable processes on different workstations to share memory [20, 9, 7, 6, 15, 27, 16]. Despite the recent improvements in memory consistency algorithms <ref> [15, 27] </ref>, hand-tuned message-passing programs frequently outperform comparable shared memory programs by a significant margin. We believe that compilation techniques developed for data parallel languages, such as regular section analysis [4, 3], can be used to reduce this gap for explicitly parallel programs. <p> Unfortunately, sequential consistency requires a total ordering of all shared data accesses, sometimes resulting in communication at every write to shared memory. To reduce the amount of communication, state-of-the-art software DSM systems now use relaxed memory models <ref> [15, 27] </ref>. For example, the release consistency model [10] permits several implementation techniques for reducing the amount of communication without significantly changing the programming model from sequential consistency. Two such techniques are lazy release consistency [14] and the multiple-writer protocol [6].
Reference: [16] <author> P.T. Koch, R.J. Fowler, and E. </author> <month> Jul. </month> <title> Message-driven relaxed consistency in a software distributed shared memory. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 75-86, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: A major obstacle to continued growth is the 1 difficulty of programming in the message-passing paradigm that is native to the hardware. As one way to overcome this obstacle, researchers have developed software distributed shared memory (DSM) systems that enable processes on different workstations to share memory <ref> [20, 9, 7, 6, 15, 27, 16] </ref>. Despite the recent improvements in memory consistency algorithms [15, 27], hand-tuned message-passing programs frequently outperform comparable shared memory programs by a significant margin. <p> Two approaches to integrating message-passing and shared-memory in a hardware coherent environment are Alewife [17] and FLASH [18]. Both try to exploit the advantages of bulk data transfer afforded by message passing, and the techniques described here could be used on these machines as well. Carlos <ref> [16] </ref> is a distributed shared memory system that integrates support for message passing with and without coherence. This paper deals with methods of using the underlying message passing to implement various types of updates with the help of compiler assistance.
Reference: [17] <author> D. Kranz, K. Johnson, A. Agarwal, J. Kubiatowicz, and B. Lim. </author> <title> Integrating message-passing and shared-memory: Early experience. </title> <booktitle> In Proceedings of the 1993 Conference on the Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: This approach is suitable for data-parallel programs, such as SOR. Programs exhibiting dynamic parallelism, such as TSP, are not easily expressed in the HPF framework. Two approaches to integrating message-passing and shared-memory in a hardware coherent environment are Alewife <ref> [17] </ref> and FLASH [18]. Both try to exploit the advantages of bulk data transfer afforded by message passing, and the techniques described here could be used on these machines as well. Carlos [16] is a distributed shared memory system that integrates support for message passing with and without coherence.
Reference: [18] <author> J. Kuskin and D. Ofelt et al. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: This approach is suitable for data-parallel programs, such as SOR. Programs exhibiting dynamic parallelism, such as TSP, are not easily expressed in the HPF framework. Two approaches to integrating message-passing and shared-memory in a hardware coherent environment are Alewife [17] and FLASH <ref> [18] </ref>. Both try to exploit the advantages of bulk data transfer afforded by message passing, and the techniques described here could be used on these machines as well. Carlos [16] is a distributed shared memory system that integrates support for message passing with and without coherence.
Reference: [19] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: The consistency model seen by an application determines when modifications to data may be expected to be seen at a given processor. Traditionally, sequential consistency <ref> [19] </ref> has been the model provided to users of shared memory 4 machines. Sequential consistency, in essence, requires that writes to shared memory are visible "immediately" at all processors. The IVY system [20] implemented this model.
Reference: [20] <author> K. Li and P. Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: A major obstacle to continued growth is the 1 difficulty of programming in the message-passing paradigm that is native to the hardware. As one way to overcome this obstacle, researchers have developed software distributed shared memory (DSM) systems that enable processes on different workstations to share memory <ref> [20, 9, 7, 6, 15, 27, 16] </ref>. Despite the recent improvements in memory consistency algorithms [15, 27], hand-tuned message-passing programs frequently outperform comparable shared memory programs by a significant margin. <p> In particular, we describe how compile-time analysis can drive a selective update protocol that is designed to reduce the amount of communication. We then measure the performance of several hand-compiled programs against two run-time protocols and message-passing versions of the same programs. The first software DSM system, Ivy <ref> [20] </ref>, provided a sequentially consistent memory, the same as hardware shared-memory multiprocessors of that period. Unfortunately, sequential consistency requires a total ordering of all shared data accesses, sometimes resulting in communication at every write to shared memory. <p> Traditionally, sequential consistency [19] has been the model provided to users of shared memory 4 machines. Sequential consistency, in essence, requires that writes to shared memory are visible "immediately" at all processors. The IVY system <ref> [20] </ref> implemented this model. The disadvantage of this model is that it can result in an excessively large amount of communication since modifications to shared data must be sent out immediately in the form of invalidates or updates. <p> All data structures are updated at every phase regardless of whether they are used in that phase, resulting in more data being sent than in the invalidate protocol, and hence poorer performance. 5 Related Work Distributed shared memory systems <ref> [20] </ref> have gained popularity because they hide the underlying details of message passing and data communication from the user. Implicit parallelism, as in HPF [13], achieves the same goal by relying on user-provided data distributions which are then used by the compiler to generate message passing code.
Reference: [21] <author> R. Mirchandaney, S. Hiranandani, and A. Sethi. </author> <title> Improving the performance of software dsm systems via compiler involvement. </title> <booktitle> In Proceedings Supercomputing '94, </booktitle> <year> 1994. </year>
Reference-contexts: Granston and Wijshoff [12] present techniques to perform data layout optimizations for distributed shared memory in order to reduce false sharing. These techniques work in conjunction with those described here for false sharing within a data structure. Similarly, the work in <ref> [21] </ref> describes techniques by which DSMs may be improved as a compilation target. Our work differs in that we deal with explicitly parallel programs and identify situations in which consistency overhead and false sharing can be eliminated, and communication can be aggregated.
Reference: [22] <author> T.C. Mowry, M.S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the 5th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-75, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Similarly, the work in [21] describes techniques by which DSMs may be improved as a compilation target. Our work differs in that we deal with explicitly parallel programs and identify situations in which consistency overhead and false sharing can be eliminated, and communication can be aggregated. Mowry et al. <ref> [22] </ref> discuss the design and evaluation of a compiler algorithm for prefetching. Their algorithm concentrates on improving the performance of cache-based systems and issues prefetch requests for data that are likely to incur a cache miss.
Reference: [23] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <month> April </month> <year> 1994. </year>
Reference-contexts: Carlos [16] is a distributed shared memory system that integrates support for message passing with and without coherence. This paper deals with methods of using the underlying message passing to implement various types of updates with the help of compiler assistance. Tempest <ref> [23] </ref> is an interface that permits programmers and compilers to use hardware communication facilities and to modify the semantics and performance of shared-memory operations. Our techniques concentrate on developing methods to reduce the amount of data transferred and the number of messages communicated by a program.
Reference: [24] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: No kernel modifications are necessary. 4.2 Results We present results for four applications Integer Sort and 3-D FFT from the NAS benchmark suite [2], M-Water from the SPLASH benchmark suite <ref> [24] </ref>, and SOR, a locally developed program. Integer Sort uses cross-synchronization prefetching and consistency elimination. M-Water uses partial updates to avoid false sharing. SOR uses the enhanced version of the partial update technique since both the producer and the consumer of the data are known. <p> The selective update protocol is able to continue to avoid this, since the modifications are sent out from the most recent updater of the page after the barrier has been reached. 4.3.2 M-Water M-Water, a modified version of the program from the SPLASH suite <ref> [24] </ref>, is a molecular dynamics simulation. We present results for M-Water using 1000 molecules and run for 5 time steps. The original Water program obtains a lock on the record representing a molecule each time it updates the force vectors in the record's contents.
Reference: [25] <author> J.P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford parallel applications for shared-memory. </title> <journal> Computer Aarchitecture News, </journal> <volume> 20(1) </volume> <pages> 2-12, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: We modified M-Water such that each processor instead uses a local variable to accumulate its updates to a molecule's record during this phase of each iteration. A similar modification was used in the second release of the SPLASH benchmark suitei <ref> [25] </ref>. At the end of the phase, it then acquires a per processor lock and applies the accumulated updates to the force vectors of the molecules owned by each processor at once. The next phase consists of using these force vectors to determine the new displacements of each molecule.
Reference: [26] <author> C-W. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: The data access information is then supplied at the barrier. Control flow analysis can be used in conjunction with regular section analysis to determine data access intersection between any two adjacent regions. If this intersection can be determined on a per processor basis <ref> [26] </ref>, the producers and consumers of each piece of data can be identified, allowing further optimization. The shared accesses in the loop are to the DISP field of the molecule type data structure. No other field is accessed, and no other shared data is accessed.
Reference: [27] <author> M.J. Zekauskas, </author> <title> W.A. Sawdon, and B.N. Bershad. Software write detection for distributed shared memory. </title> <booktitle> In Proceedings of the First USENIX Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 87-100, </pages> <month> November </month> <year> 1994. </year> <month> 20 </month>
Reference-contexts: A major obstacle to continued growth is the 1 difficulty of programming in the message-passing paradigm that is native to the hardware. As one way to overcome this obstacle, researchers have developed software distributed shared memory (DSM) systems that enable processes on different workstations to share memory <ref> [20, 9, 7, 6, 15, 27, 16] </ref>. Despite the recent improvements in memory consistency algorithms [15, 27], hand-tuned message-passing programs frequently outperform comparable shared memory programs by a significant margin. <p> As one way to overcome this obstacle, researchers have developed software distributed shared memory (DSM) systems that enable processes on different workstations to share memory [20, 9, 7, 6, 15, 27, 16]. Despite the recent improvements in memory consistency algorithms <ref> [15, 27] </ref>, hand-tuned message-passing programs frequently outperform comparable shared memory programs by a significant margin. We believe that compilation techniques developed for data parallel languages, such as regular section analysis [4, 3], can be used to reduce this gap for explicitly parallel programs. <p> Unfortunately, sequential consistency requires a total ordering of all shared data accesses, sometimes resulting in communication at every write to shared memory. To reduce the amount of communication, state-of-the-art software DSM systems now use relaxed memory models <ref> [15, 27] </ref>. For example, the release consistency model [10] permits several implementation techniques for reducing the amount of communication without significantly changing the programming model from sequential consistency. Two such techniques are lazy release consistency [14] and the multiple-writer protocol [6]. <p> This asynchronous prefetching 7 reduces the potential problem of larger acquire latencies due to data shipping at the time of an acquire, while still taking advantage of the benefits of aggregating data communication, especially on a network of workstations. In addition, computation and communication are overlapped. Midway <ref> [27] </ref> attempts to provide a similar functionality to cross-synchronization prefetching at the expense of additional user complexity. Clouds [7] is a distributed system that uses objects to achieve a similar purpose.
References-found: 27

