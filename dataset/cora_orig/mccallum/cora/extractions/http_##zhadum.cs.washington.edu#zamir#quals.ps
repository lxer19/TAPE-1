URL: http://zhadum.cs.washington.edu/zamir/quals.ps
Refering-URL: http://zhadum.cs.washington.edu/zamir/about.html
Root-URL: http://www.cs.washington.edu
Title: Fast and Intuitive Clustering of Web Documents  
Author: Oren Zamir 
Date: April 1997  
Abstract: Quals Report This report is adapted from an paper by Oren Zamir, Oren Etzioni, Omid Madani and Richard M. Karp Abstract Conventional document retrieval systems (e.g., Alta Vista) return long lists of ranked documents in response to user queries. Recently, document clustering has been put forth as an alternative method of organizing the results of a retrieval [6]. A person browsing the clusters can discover patterns that would be overlooked in the traditional ranked-list presentation. In this context, a document clustering algorithm has two key requirements. First, the algorithm ought to produce clusters that are easy-to-browse a user needs to determine at a glance whether the contents of a cluster are of interest. Second, the algorithm has to be fast even when applied to thousands of documents with no preprocessing. This paper describes several novel clustering methods, which intersect the documents in a cluster to determine the set of words (or phrases) shared by all the documents in the cluster. We report on experiments that evaluate these intersection-based clustering methods on collections of snippets returned from Web search engines. First, we show that word-intersection clustering produces superior clusters and does so faster than standard techniques. Second, we show that our O(n log n) time phrase-intersection clustering method produces comparable clusters and does so more than two orders of magnitude faster than word-intersection. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Arne Anderson, N. Jesper Larsson, and Kurt Swanson. </author> <title> Suffix trees on words. In Combinatorial Pattern Matching, </title> <year> 1996. </year>
Reference-contexts: The suffix tree data structure has found many applications in string (e.g. in computational biology) and text processing, see for example [3]. Space efficient algorithms for the construction of suffix trees on words are presented in <ref> [1] </ref>; however, due to the special requirements and characteristics of our domain, we do not use their algorithms. [13] mention the use suffix trees to find common substrings in computer virus programs in order to construct computer virus phylogenies. 8 Conclusion We have attempted to address the hard problem of enabling
Reference: [2] <author> J. R. Anderson and M. Matessa. </author> <title> An iterative bayesian algorithm for categorization. </title> <editor> In D. Fisher and M. Pazzani, editors, </editor> <title> Concept formation: Knowledge and experience in unsupervised learning. </title> <publisher> Morgan Kaufman, </publisher> <year> 1991. </year>
Reference-contexts: Measuring the global quality of a partition of the data in a Bayesian context appeared in COBWEB [11], in AUTOCLASS [5], as well as in other systems <ref> [2] </ref>. These clustering techniques are typically slow and do not perform well in domains with sparse attribute vectors.
Reference: [3] <author> A. Apostolico. </author> <title> The myriad virtues of subword trees. In Combinatorial Algorithms on Words, </title> <booktitle> NATO ISI Series, </booktitle> <pages> pages 85-96. </pages> <publisher> Springer Verlag, </publisher> <year> 1985. </year>
Reference-contexts: The suffix tree data structure has found many applications in string (e.g. in computational biology) and text processing, see for example <ref> [3] </ref>.
Reference: [4] <author> F. </author> <title> Can. Concept and effectiveness of the cover-coefficient-based clustering methodology for text databases. </title> <journal> ACM Trans. On database systems, </journal> <volume> 15 </volume> <pages> 483-517, </pages> <year> 1990. </year>
Reference-contexts: Various other methods have been investigated in the document clustering domain. The non-hierarchical ones include single-pass methods such as assign-to-nearest, as well as multi-pass methods such as iterative-seed-selection that is used in CLUSTER [21] and in the Cover-Coefficient method <ref> [4] </ref>. These methods are fast, but have shown poor results for document clustering. MetaCrawler and Excite [10] allow the user to view the documents returned from queries either as a ranked list or sorted according to site. This can be viewed as a very basic form of clustering.
Reference: [5] <author> P. Cheeseman, J. Kelly, M. Self, J. Stutz, W. Taylor, and D. Freeman. </author> <title> Autoclass: A bayesian classification system. </title> <booktitle> In Proceedings of the Fifth International Machine Learning Conference, </booktitle> <pages> pages 54-64, </pages> <year> 1988. </year>
Reference-contexts: Measuring the global quality of a partition of the data in a Bayesian context appeared in COBWEB [11], in AUTOCLASS <ref> [5] </ref>, as well as in other systems [2]. These clustering techniques are typically slow and do not perform well in domains with sparse attribute vectors.
Reference: [6] <author> D. R. Cutting, D. R. Karger, J. O. Pedersen, and J. W. Tukey. Scatter/gather: </author> <title> a cluster-based approach to browsing large document collections. </title> <booktitle> In 15th International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 318-29, </pages> <year> 1992. </year>
Reference-contexts: Finally, this problem gets worse as the Web continues to grow. Instead of attempting to reduce the number of documents returned (e.g., by filtering methods [28] or by advanced pruning options [27]) we attempt to make search engine results easy to browse. Following the approach outlined in <ref> [6] </ref>, we investigate document clustering as a method that enables users to efficiently navigate through a large collection of search engine results. In addition, clustering enables the user to discover patterns and structure in the document set that could be overlooked in the traditional ranked-list presentation. <p> We conclude with a discussion of related and future work. 2 Document Clustering Document clustering has been traditionally investigated mainly as a means of improving document search and retrieval. Recently, a technique named Scatter/Gather <ref> [6, 15] </ref> intro 1 duced document clustering as a document browsing method. Our work follows the same paradigm. Document clustering algorithms fall into one of two classes: hierarchical and non-hierarchical (i.e., "flat") methods [23]. <p> After constructing such a multi-word index, we will apply a selection process that chooses which of the entries in the index to display to the user as clusters (and whether to merge several entries together). 7 Related Work Dynamic document clustering of online search outputs was introduced in <ref> [6] </ref>, and investigated in [19] as well, as a means to identify near-duplicate documents. [15] describes user studies on the Scatter/Gather system that demonstrate the systems usefulness.
Reference: [7] <institution> Digital Equipment Corporation. Altavista home page. </institution> <note> See http://www.altavista.com. </note>
Reference-contexts: 1 Introduction Conventional document retrieval systems return long lists of ranked documents that users are forced to sift through to find documents relevant to their queries. On the Web, this problem is exacerbated by the high recall and low precision of search engines such as Alta Vista <ref> [7] </ref>, Hotbot [17], etc. Moreover, the typical user has trouble formulating highly specific queries and does not take advantage of advanced search options. Finally, this problem gets worse as the Web continues to grow. <p> These methods are fast, but have shown poor results for document clustering. MetaCrawler and Excite [10] allow the user to view the documents returned from queries either as a ranked list or sorted according to site. This can be viewed as a very basic form of clustering. Alta Vista <ref> [7] </ref> has introduced LiveTopics, a query refinement tool that performs word frequency analysis. It does not perform document clustering but addresses the problem of helping the user handle the large number of documents typically returned by the search engine in a different way.
Reference: [8] <author> E. W. Dijkstra. </author> <title> The problem of the shortest subspanning tree. </title> <booktitle> In A Discipline of Programming, </booktitle> <pages> pages 154-60. </pages> <publisher> Prentice Hall, </publisher> <year> 1976. </year>
Reference-contexts: This sort of "noise" causes the HAC algorithm to be particularly sensitive to the thresholds of the halting criteria. HAC algorithms are typically slow when applied to large document collections. Single-Link <ref> [29, 25, 8] </ref> and Group-Average [33] methods typically take O (n 2 ) time 2 , while Complete 1 A clustering is the set of clusters produced by a clustering algorithm. 2 Throughout this paper n denotes the number of documents to be clustered.
Reference: [9] <author> A. El-Hamdouchi and P. Willet. </author> <title> Techniques for the measurement of clustering tendency in document retrieval systems. </title> <journal> J. Information Science, </journal> <volume> 13 </volume> <pages> 361-65, </pages> <year> 1989. </year>
Reference-contexts: The number of words per document is assumed to be bounded by a constant. 2 Link methods typically take O (n 3 ) time <ref> [9] </ref>. In terms of quality, on the other hand, Complete--Link algorithms have been shown to perform well in comparative studies of document retrieval [32], as they tend to produce tightly bound clusters, i.e., clusters in which all the documents strongly relate to one another.
Reference: [10] <institution> Excite, Inc. Excite home page. </institution> <note> See http://www.excite.com. </note>
Reference-contexts: The non-hierarchical ones include single-pass methods such as assign-to-nearest, as well as multi-pass methods such as iterative-seed-selection that is used in CLUSTER [21] and in the Cover-Coefficient method [4]. These methods are fast, but have shown poor results for document clustering. MetaCrawler and Excite <ref> [10] </ref> allow the user to view the documents returned from queries either as a ranked list or sorted according to site. This can be viewed as a very basic form of clustering. Alta Vista [7] has introduced LiveTopics, a query refinement tool that performs word frequency analysis.
Reference: [11] <author> D. H. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 139-192, </pages> <year> 1987. </year> <month> 12 </month>
Reference-contexts: Measuring the global quality of a partition of the data in a Bayesian context appeared in COBWEB <ref> [11] </ref>, in AUTOCLASS [5], as well as in other systems [2]. These clustering techniques are typically slow and do not perform well in domains with sparse attribute vectors.
Reference: [12] <author> E. Fredkin. </author> <title> Trie memory. </title> <journal> Communications of the ACM, </journal> <volume> 3 </volume> <pages> 490-499, </pages> <year> 1960. </year>
Reference-contexts: Phrase-IC using suffix trees is an O (n log n) expected time algorithm that results in a large speedup without much degradation in quality. The suffix tree [34, 14] of a set of strings is a compact trie <ref> [12] </ref> containing all the suffixes of all the strings. In our application, we build a suffix tree of all the documents (each treated as a string).
Reference: [13] <author> Leslie Ann Goldberg, Paul W. Goldberg, Cynthia A. Phillips, and Gregory B. Sorkin. </author> <title> Constructing computer virus phylogenies. In Combinatorial Pattern Matching, </title> <year> 1996. </year>
Reference-contexts: Space efficient algorithms for the construction of suffix trees on words are presented in [1]; however, due to the special requirements and characteristics of our domain, we do not use their algorithms. <ref> [13] </ref> mention the use suffix trees to find common substrings in computer virus programs in order to construct computer virus phylogenies. 8 Conclusion We have attempted to address the hard problem of enabling users to quickly navigate through the results of Web search engines.
Reference: [14] <author> Dan Gusfield. </author> <title> Algorithms on Strings, Trees and Sequences: </title> <booktitle> Computer Science and Computational Biology, chapter 6. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1997. </year>
Reference-contexts: Phrase-IC using suffix trees is an O (n log n) expected time algorithm that results in a large speedup without much degradation in quality. The suffix tree <ref> [34, 14] </ref> of a set of strings is a compact trie [12] containing all the suffixes of all the strings. In our application, we build a suffix tree of all the documents (each treated as a string). <p> The space requirement of the suffix tree is O (n). As the branching factor of a document suffix tree varies considerably between the different levels, we use different data structures at the different levels of the tree. The construction time of a suffix tree can be O (n) <ref> [31, 14] </ref>. We have adapted a different suffix tree construction algorithm: when a document is received all its suffixes are inserted into the tree one by one. This method allows us to also score and sort the potential clusters 6 incrementally, as the documents arrive.
Reference: [15] <author> M. A. Hearst and J. O. Pedersen. </author> <title> Reexamining the cluster hypothesis: </title> <booktitle> Scatter/gather on retrieval results. In 19th International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 76-84, </pages> <year> 1996. </year>
Reference-contexts: We conclude with a discussion of related and future work. 2 Document Clustering Document clustering has been traditionally investigated mainly as a means of improving document search and retrieval. Recently, a technique named Scatter/Gather <ref> [6, 15] </ref> intro 1 duced document clustering as a document browsing method. Our work follows the same paradigm. Document clustering algorithms fall into one of two classes: hierarchical and non-hierarchical (i.e., "flat") methods [23]. <p> that chooses which of the entries in the index to display to the user as clusters (and whether to merge several entries together). 7 Related Work Dynamic document clustering of online search outputs was introduced in [6], and investigated in [19] as well, as a means to identify near-duplicate documents. <ref> [15] </ref> describes user studies on the Scatter/Gather system that demonstrate the systems usefulness. The Scatter/Gather system uses a linear time partition algorithm that relies on clusters created by applying the COS-GAVG algorithm to a sample (of size p n) of the collection.
Reference: [16] <author> L. C. K. Hui. </author> <title> Color set size problem with applications to string matching. </title> <booktitle> In Combinatorial Pattern Matching Third Annual Symposium, </booktitle> <year> 1992. </year>
Reference-contexts: For example, for a query on the word "Clinton" a sample of the phrases found common to many documents includes: "progress on the aids pandemic", "democratic party", and "Hillary Rodham Clinton". Another advantage of Phrase-IC is that there exist efficient algorithms (for example <ref> [16] </ref>) for discovering long substrings common to many documents. Thus, for certain definitions of cluster cohesion based on common phrases we can leverage off this potential computational advantage. 4.1 Phrase-Intersection Clustering using GQF A small variation in the definition of the GQF allows us to perform Phrase-IC.
Reference: [17] <institution> Inktomi, Inc. Hotbot home page. </institution> <note> See http://www.hotbot.com. </note>
Reference-contexts: 1 Introduction Conventional document retrieval systems return long lists of ranked documents that users are forced to sift through to find documents relevant to their queries. On the Web, this problem is exacerbated by the high recall and low precision of search engines such as Alta Vista [7], Hotbot <ref> [17] </ref>, etc. Moreover, the typical user has trouble formulating highly specific queries and does not take advantage of advanced search options. Finally, this problem gets worse as the Web continues to grow.
Reference: [18] <author> M. Iwayama and T. Tokunaga. </author> <title> Cluster-based text categorization: a comparison of category search techniques. </title> <booktitle> In 18th International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <year> 1995. </year>
Reference-contexts: These clustering techniques are typically slow and do not perform well in domains with sparse attribute vectors. Only recently has an attempt been made to apply these Bayesian quality measures to guide a HAC algorithm in the document clustering domain <ref> [18] </ref>, but more comprehensive comparative studies are required in order to determine the benefit of this method. Various other methods have been investigated in the document clustering domain.
Reference: [19] <author> J.W. Kirriemuir and P. Willet. </author> <title> Identification of duplicate and near-duplicate full-text records in database search-outputs using hierarchic cluster analysis. </title> <booktitle> In 18th International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 241-256, </pages> <year> 1995. </year>
Reference-contexts: such a multi-word index, we will apply a selection process that chooses which of the entries in the index to display to the user as clusters (and whether to merge several entries together). 7 Related Work Dynamic document clustering of online search outputs was introduced in [6], and investigated in <ref> [19] </ref> as well, as a means to identify near-duplicate documents. [15] describes user studies on the Scatter/Gather system that demonstrate the systems usefulness.
Reference: [20] <author> Michael Mauldin. </author> <note> Lycos home page. See http://lycos.cs.cmu.edu. </note>
Reference: [21] <author> R. S. Michalski and R. E. Stepp. </author> <title> Automated construction of classifications: conceptual clustering versus numerical taxonomy. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence., PAMI-5, </journal> <volume> no.4:396-410, </volume> <year> 1983. </year>
Reference-contexts: Various other methods have been investigated in the document clustering domain. The non-hierarchical ones include single-pass methods such as assign-to-nearest, as well as multi-pass methods such as iterative-seed-selection that is used in CLUSTER <ref> [21] </ref> and in the Cover-Coefficient method [4]. These methods are fast, but have shown poor results for document clustering. MetaCrawler and Excite [10] allow the user to view the documents returned from queries either as a ranked list or sorted according to site.
Reference: [22] <author> G. W. Milligan and M. C. Cooper. </author> <title> An examination of procedures for detecting the number of clusters in a data set. </title> <journal> Psychometrika, </journal> <volume> 50 </volume> <pages> 159-79, </pages> <year> 1985. </year>
Reference-contexts: The commonly used distance functions between clusters are Single-Link (which defines the distance as the minimum document distance between the two clusters), Complete-Link (the maximum distance) and Group-Average (the average distance). Several halting criteria for HAC methods have been suggested <ref> [22] </ref>, but typically they fail to produce the desired result of having a balanced trade-off between the number of clusters, their size and their cohesion. This is mainly because they are typically arbitrary, i.e., they do not relate to the collection of documents being clustered.
Reference: [23] <author> E. Rasmussen. </author> <title> Clustering algorithms. </title> <editor> In W.B. Frakes and R. Baeza-Yates, editors, </editor> <booktitle> Information Retrieval, </booktitle> <pages> pages 419-442. </pages> <publisher> Prentice Hall, </publisher> <address> Eaglewood Cliffs, N.J., </address> <year> 1992. </year>
Reference-contexts: Recently, a technique named Scatter/Gather [6, 15] intro 1 duced document clustering as a document browsing method. Our work follows the same paradigm. Document clustering algorithms fall into one of two classes: hierarchical and non-hierarchical (i.e., "flat") methods <ref> [23] </ref>. Hierarchical clustering methods [35] create a binary-tree like organization (a dendrogram) and have the advantage of allowing the clusters to be viewed at different levels of resolution.
Reference: [24] <editor> C.J. Van Rijsbergen. </editor> <booktitle> Information Retrieval. </booktitle> <address> London, </address> <publisher> Butterworths, </publisher> <year> 1979. </year>
Reference-contexts: As we are dealing with a cluster as a set of documents, and are looking at the intersection of that set, the result is a monothetic classification: all the documents in a given cluster must contain certain terms if they are to belong to it <ref> [24] </ref>. These clusters are not elongated, as all the documents in a cluster are similar to all other members by at least the cohesion of the cluster.
Reference: [25] <author> C.J. Van Rijsbergen. </author> <title> An algorithm for information structuring and retrieval. </title> <journal> Computer Journal, </journal> <volume> 14 </volume> <pages> 407-412, 71. </pages>
Reference-contexts: This sort of "noise" causes the HAC algorithm to be particularly sensitive to the thresholds of the halting criteria. HAC algorithms are typically slow when applied to large document collections. Single-Link <ref> [29, 25, 8] </ref> and Group-Average [33] methods typically take O (n 2 ) time 2 , while Complete 1 A clustering is the set of clusters produced by a clustering algorithm. 2 Throughout this paper n denotes the number of documents to be clustered.
Reference: [26] <author> G. Salton. </author> <title> Automatic text processing. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: The Dice, Jaccard, and Cosine similarity measures all look at the dot product between two attribute vectors, and differ by their normalization methods <ref> [26] </ref>. Given a definition for the distance between two documents, there are many ways to define the distance between two clusters (i.e., sets of documents).
Reference: [27] <author> Erik Selberg and Oren Etzioni. </author> <title> Multi-service search and comparison using the metac-rawler. </title> <booktitle> In Proc. 4th World Wide Web Conf., </booktitle> <pages> pages 195-208, </pages> <address> Boston, MA USA, </address> <year> 1995. </year> <note> See http://www.cs.washington.edu/research/metacrawler. 13 </note>
Reference-contexts: Finally, this problem gets worse as the Web continues to grow. Instead of attempting to reduce the number of documents returned (e.g., by filtering methods [28] or by advanced pruning options <ref> [27] </ref>) we attempt to make search engine results easy to browse. Following the approach outlined in [6], we investigate document clustering as a method that enables users to efficiently navigate through a large collection of search engine results. <p> We chose to apply the algorithms to snippet collections created by merging several distinct base collections. We then scored the resulting clusterings 7 by comparing them to the original partition of the snippets into base collections. We created 88 base collections from the snippets returned by MetaCrawler <ref> [27] </ref> in response to 88 different queries. MetaCrawler is a parallel search engine it routes queries to various different search engines and collates the results thus assuring us of a wide and heterogeneous sample of Web documents.
Reference: [28] <author> J. Shakes, M. Langheinrich, and O. Etzioni. </author> <title> Ahoy! the home page finder. </title> <booktitle> In Proc. 6th World Wide Web Conf., </booktitle> <address> Santa Clara, CA USA, </address> <year> 1997. </year> <note> See http://www.cs.washington.edu/research/ahoy. </note>
Reference-contexts: Moreover, the typical user has trouble formulating highly specific queries and does not take advantage of advanced search options. Finally, this problem gets worse as the Web continues to grow. Instead of attempting to reduce the number of documents returned (e.g., by filtering methods <ref> [28] </ref> or by advanced pruning options [27]) we attempt to make search engine results easy to browse. Following the approach outlined in [6], we investigate document clustering as a method that enables users to efficiently navigate through a large collection of search engine results.
Reference: [29] <author> R. Sibson. Slink: </author> <title> an optimally efficient algorithm for the single link cluster method. </title> <journal> Computer Journal, </journal> <volume> 16 </volume> <pages> 30-34, </pages> <year> 1973. </year>
Reference-contexts: This sort of "noise" causes the HAC algorithm to be particularly sensitive to the thresholds of the halting criteria. HAC algorithms are typically slow when applied to large document collections. Single-Link <ref> [29, 25, 8] </ref> and Group-Average [33] methods typically take O (n 2 ) time 2 , while Complete 1 A clustering is the set of clusters produced by a clustering algorithm. 2 Throughout this paper n denotes the number of documents to be clustered.
Reference: [30] <author> R. E. Stepp and R. S. Michalski. </author> <title> Conceptual clustering. </title> <editor> In R. Michalski, J. Car-bonell, and T. Mitchell, editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> volume II, </volume> <pages> pages 371-392. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: Conceptual clustering is a clustering method meant to produce clusters that are easy to understand by restricting the clustering algorithm to only consider clusters that can be characterized through logical combinations of predicates in a particular predicate language <ref> [30] </ref>. Word-IC and Phrase-IC can be viewed as following a similar approach in that the clusters created can be defined in a clear and simple manner, but they are faster and do not require the structured background knowledge needed for conceptual clustering.
Reference: [31] <author> Esko Ukkonen. </author> <title> On-line construction of suffix-trees. </title> <journal> Algorithmica, </journal> <volume> 14 </volume> <pages> 249-260, </pages> <year> 1995. </year>
Reference-contexts: The space requirement of the suffix tree is O (n). As the branching factor of a document suffix tree varies considerably between the different levels, we use different data structures at the different levels of the tree. The construction time of a suffix tree can be O (n) <ref> [31, 14] </ref>. We have adapted a different suffix tree construction algorithm: when a document is received all its suffixes are inserted into the tree one by one. This method allows us to also score and sort the potential clusters 6 incrementally, as the documents arrive.
Reference: [32] <author> E.M. Voorhees. </author> <title> The effectiveness and efficiency of agglomerative hierarchic clustering in document retrieval. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <year> 1986. </year>
Reference-contexts: The number of words per document is assumed to be bounded by a constant. 2 Link methods typically take O (n 3 ) time [9]. In terms of quality, on the other hand, Complete--Link algorithms have been shown to perform well in comparative studies of document retrieval <ref> [32] </ref>, as they tend to produce tightly bound clusters, i.e., clusters in which all the documents strongly relate to one another. Single-Link, and to a lesser degree Group-Average methods, exhibit a tendency toward creating elongated clusters.
Reference: [33] <author> E.M. Voorhees. </author> <title> Implementing agglomerative hierarchical clustering algorithms for use in document retrieval. </title> <booktitle> Information Processing & Management, </booktitle> <volume> 22 </volume> <pages> 465-476, </pages> <year> 1986. </year>
Reference-contexts: The characteristics of the resulting clustering 1 and the time complexity of the HAC algorithm are both greatly determined by the definition of this cluster distance <ref> [33] </ref>. The commonly used distance functions between clusters are Single-Link (which defines the distance as the minimum document distance between the two clusters), Complete-Link (the maximum distance) and Group-Average (the average distance). <p> This sort of "noise" causes the HAC algorithm to be particularly sensitive to the thresholds of the halting criteria. HAC algorithms are typically slow when applied to large document collections. Single-Link [29, 25, 8] and Group-Average <ref> [33] </ref> methods typically take O (n 2 ) time 2 , while Complete 1 A clustering is the set of clusters produced by a clustering algorithm. 2 Throughout this paper n denotes the number of documents to be clustered.
Reference: [34] <author> P. Weiner. </author> <title> Linear pattern matching algorithms. </title> <booktitle> In 14th Annual Symposium on Foundations of Computer Science (FOCS), </booktitle> <pages> pages 1-11, </pages> <year> 1973. </year>
Reference-contexts: Phrase-IC using suffix trees is an O (n log n) expected time algorithm that results in a large speedup without much degradation in quality. The suffix tree <ref> [34, 14] </ref> of a set of strings is a compact trie [12] containing all the suffixes of all the strings. In our application, we build a suffix tree of all the documents (each treated as a string).
Reference: [35] <author> P. Willet. </author> <title> Recent trends in hierarchical document clustering: a critical review. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24 </volume> <pages> 577-97, </pages> <year> 1988. </year> <month> 14 </month>
Reference-contexts: Recently, a technique named Scatter/Gather [6, 15] intro 1 duced document clustering as a document browsing method. Our work follows the same paradigm. Document clustering algorithms fall into one of two classes: hierarchical and non-hierarchical (i.e., "flat") methods [23]. Hierarchical clustering methods <ref> [35] </ref> create a binary-tree like organization (a dendrogram) and have the advantage of allowing the clusters to be viewed at different levels of resolution.
References-found: 35

