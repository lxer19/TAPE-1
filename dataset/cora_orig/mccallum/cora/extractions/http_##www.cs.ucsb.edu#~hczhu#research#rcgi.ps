URL: http://www.cs.ucsb.edu/~hczhu/research/rcgi.ps
Refering-URL: http://www.cs.ucsb.edu/~hczhu/researchlink.html
Root-URL: http://www.cs.ucsb.edu
Title: A Scheduling Framework for Web Server Clusters with Intensive Dynamic Content Processing  
Abstract: Clustering support for large-scale Web servers is important to improve the system scalability in processing a large number of concurrent requests from Internet, especially when dynamic content generation using CGI or other protocols becomes increasingly popular. This paper studies a two-level scheduling framework with a master/slave architecture for clustering Web servers. Such an architecture has advantages in dynamic resource recruitment, fail-over management and it can also improve server throughput by separating static and dynamic content requests. This framework uses a reservation-guided scheme and efficient remote dynamic content generation to achieve load balancing with an insignificant amount of rescheduling overhead. This paper provides a comparison of several scheduling approaches and presents experimental and analytic results to demonstrate the performance gain of the proposed techniques.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Acharya, G. Edjlali, and J. Saltz. </author> <title> The utility of exploiting idle workstations for parallel computation. </title> <booktitle> ACM SIG-METRICS'97, </booktitle> <pages> pages 225-236, </pages> <year> 1997. </year>
Reference-contexts: It is widely recognized that idle computers can be found easily in a corporation or institutional environment <ref> [1, 2] </ref> and can be recruited to respond to bursty situations. * The capability of load balancing provided in a switch is still limited because a switch must forward packets as fast possible. <p> Replication is common in systems requiring high availability [32]. We also assume that a non-dedicated node can be used only when it becomes idle. This policy is used in <ref> [1, 24] </ref>. If one examines popular Web sites such as Microsoft, Inktomi and Altavista, clustering with a certain degree of data replication is fairly common. In the Inktomi design [17], a few front-end hosts handle user HTTP requests. <p> The number of non-dedicated nodes varies from 8 to 32. We assume that those nodes are available 50% of time during the test. The X axis marks the average number of idle nodes recruited. This assumption is consistent with the previous studies on idle machine availability <ref> [2, 1] </ref>. The result shows that resource recruiting becomes more effective when CGI load is significant (r = 1=160; 1=80 for UCB. r = 1=40; 1=80; 1=160 for KSU and ADL).
Reference: [2] <author> T. E. Anderson, D. Culler, D. Patterson, </author> <title> and the NOW Team. A case for NOW(networks of workstations). </title> <booktitle> IEEE Micro, </booktitle> <month> Feb. </month> <year> 1995. </year> <month> 19 </month>
Reference-contexts: It is widely recognized that idle computers can be found easily in a corporation or institutional environment <ref> [1, 2] </ref> and can be recruited to respond to bursty situations. * The capability of load balancing provided in a switch is still limited because a switch must forward packets as fast possible. <p> Equipped with this result, we can further derive the following theorem: Theorem 1 Let = h h , 2 = m r (pm) m (a+1+a=ra)p a+r then 0 1 2 1, and 8 2 <ref> [ 1 ; 2 ] </ref>, S M S F . <p> The number of non-dedicated nodes varies from 8 to 32. We assume that those nodes are available 50% of time during the test. The X axis marks the average number of idle nodes recruited. This assumption is consistent with the previous studies on idle machine availability <ref> [2, 1] </ref>. The result shows that resource recruiting becomes more effective when CGI load is significant (r = 1=160; 1=80 for UCB. r = 1=40; 1=80; 1=160 for KSU and ADL). <p> The feasibility and importance of recruiting idle resource in a clustered environment has been studied in parallel computing <ref> [2] </ref> and distributed job execution [24]. Our research is focused on clustering for Web servers and takes into consideration Web access characteristics. A layered architecture for network service is proposed in [13], which generalizes the structure of the Inktomi's searching server. Their work demonstrates its usefulness in the distillation proxy.
Reference: [3] <author> D. Andresen, L. Carver, R. Dolin, C. Fischer, J. Frew, M. Goodchild, O. Ibarra, R. Kothuri, M. Larsgaard, B. Manjunath, D. Nebert, J. Simpson, T. Smith, T. Yang, and Q. Zheng. </author> <title> The WWW Prototype of the Alexandria Digital Library. </title> <booktitle> In Proceedings of ISDL'95: International Symposium on Digital Libraries, </booktitle> <month> Aug. </month> <year> 1995. </year> <note> Revised version appeared in IEEE Computer, 1996, No.5. </note>
Reference-contexts: Examples of such systems can be found in IBM's Atlanta Olympics Web server and the Alexandria Digital Library system <ref> [3, 16, 18] </ref>. Clustering is the most commonly used approach to increase throughput of a Web site. With a server cluster, multiple servers behave as a single host, from clients' perspective. NCSA [20] first proposed a clustering technique that uses DNS rotation to balance load among cluster nodes. <p> The UCB log [15] was gathered from the Home IP service offered by UC Berkeley to its modem pool users. The KSU log was recorded by the Web server for online library service at Kansas State University. The ADL log is from the testbed of Alexandria Digital Library <ref> [3] </ref>, a digital library for spatially referenced data. For these logs we can distinguish dynamic and static requests, and can extract the completion time of each request and the response size. We also have a DEC trace [21] available.
Reference: [4] <author> D. Andresen, T. Yang, V. Holmedahl, and O. Ibarra. Sweb: </author> <title> Towards a scalable WWW server on multicomputers. </title> <booktitle> Proc. of Intl. Symp. on Parallel Processing, IEEE, </booktitle> <pages> pages 850-856, </pages> <month> Apr. </month> <year> 1996. </year>
Reference-contexts: NCSA [20] first proposed a clustering technique that uses DNS rotation to balance load among cluster nodes. Research has demonstrated that DNS round-robin rotation does not evenly distribute the load among servers, due to non-uniform resource demands of requests and DNS entry caching. A number of projects <ref> [4, 8] </ref> have proposed methods for more fairly distributing load among a group of servers based on HTTP redirection or intelligent DNS rotation. The main weakness of a DNS-based server cluster is that the IP addresses of all nodes in a cluster are exposed to the Internet. <p> We 1 illustrate its effectiveness with the CGI protocol and call our scheme remote CGI (RCGI). We show that RCGI can effectively achieve load re-balancing and its overhead is negligible and even smaller than standard local CGI execution. We do not use HTTP redirection <ref> [4] </ref> for request re-scheduling because it adds client round-trip latency for every rescheduled request and also exposes IP addresses of server nodes. Instances of M/S architecture can be found in current industry Web sites such as Web searching sites at Inktomi [17] and AltaVista [10]. <p> A client will pick one address from the set and then send its request to the selected server node. If the DNS server cyclically rotates possible IP addresses in response to IP address queries, then client requests may be distributed <ref> [4, 20] </ref>. Since IP addresses of a Web site are cached at client sites, clients at one site may actually access the same server node until a cached address expires. Thus load imbalance may be caused by address caching. <p> Our study extends this work by providing a generalized solution for Web servers and proposes necessary optimization support in order to make an layered architecture efficient. In addition to the NCSA solution for Web clustering [20], the SWEB project <ref> [4] </ref> addresses load balancing on a cluster of nodes and uses the cost prediction to assign requests to nodes using multiple resource load indices. Previous work on general distributed load balancing [33] normally considers one load index. The SWEB experiment identifies the importance of using multiple indices for Web clustering. <p> Our scheme uses a sampling technique and a relative cost function, which does not require the estimation of direct processing cost. Such a scheme is more general and practical. Our work is also an extension of the research for digital library server clustering [38]. Also in <ref> [4, 38] </ref> URL redirection is used to achieve request assignment, which adds the round-trip overhead of client-server communication and exposes more node IP address. Our scheme uses remote dynamic content processing with low overhead to achieve rescheduling.
Reference: [5] <author> Apache. </author> <title> Apache HTTP Server Project. </title> <note> http://www.apache.org, 1995. </note>
Reference-contexts: A permanent connection is established between the server and the FastCGI application. Individual requests are forwarded from the server to the application, removing the need for process creation for each request. Currently only Apache <ref> [5] </ref> and Zeus [37] implement this interface. This paper demonstrates the proposed techniques for CGI due to its popularity, but our techniques could be applied to other dynamic content generation protocols. cluster is presented to the Internet as a single logical server. <p> Our current implementation of the M/S architecture is based on the freely available Apache Web server version 1.3 source code and the Swala server with cooperative content caching support <ref> [16, 5] </ref>. To prevent malicious requests from being sent to the RCGI servers, access control and identity verification may be done by each RCGI server for each CGI request. This security check is not currently implemented in our testbed.
Reference: [6] <author> H. Casanova and J. Dongarra. Netsolve: </author> <title> a network server for solving computational science problems. </title> <booktitle> Proceedings of Supercomputing'96, </booktitle> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: Recently more Web sites generate dynamic content because it enables many new services such as electronic commerce, database searching, personalized information presentation, and scientific/engineering computing <ref> [6] </ref>. Since dynamic content generation places greater I/O and CPU demands on the server, the server bottleneck becomes more critical compared to the network bottleneck and it limits the scalability of such servers in processing large numbers of simultaneous client requests.
Reference: [7] <author> CISCO. </author> <title> Local Director. </title> <note> http://www.cisco.com/warp/public/751/lodir/index.shtml, 1997. </note>
Reference-contexts: To address this issue, one solution is to maintain a hot-standby machine for each host [32]. Another technique for clustering Web servers with load balancing and fault tolerance support is load balancing switching products from Cisco, Foundry Networks and F5Labs <ref> [7, 12, 11] </ref>. These switching products assign a single address to a group of servers and distribute incoming requests among the live nodes, therefore effectively preventing users from accessing dead nodes. Switches use simple load balancing schemes which may not be sufficient for intensive dynamic content.
Reference: [8] <author> M. Colajanni, P. S. Yu, and D. M. Dias. </author> <title> Analysis of task assignment policies in scalable distributed web-server systems. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 585-598, </pages> <month> June </month> <year> 1998. </year>
Reference-contexts: NCSA [20] first proposed a clustering technique that uses DNS rotation to balance load among cluster nodes. Research has demonstrated that DNS round-robin rotation does not evenly distribute the load among servers, due to non-uniform resource demands of requests and DNS entry caching. A number of projects <ref> [4, 8] </ref> have proposed methods for more fairly distributing load among a group of servers based on HTTP redirection or intelligent DNS rotation. The main weakness of a DNS-based server cluster is that the IP addresses of all nodes in a cluster are exposed to the Internet. <p> User requests are evenly distributed to nodes. This reflects a switch-based flat architecture with round-robin scheduling. It also can be a DNS-based solution in a perfect situation, i.e. clients are uniformly distributed geographically and IP address caching does not create a problem. The study in <ref> [8] </ref> shows that 3 Currently Yahoo receives 115 million page views per day in June 1998 [36] which is around 1331 hits per second. <p> We expect the hit ratios will increase substantially over next few years. 12 its performance is near-optimal for a DNS setting, better than the other methods proposed in <ref> [8] </ref> with this uniform-distribution assumption. * Flat-C. Requests are scheduled to a node with the least number of outstanding connections. This is achieved in our simulation by selecting a node with the minimum queue length.
Reference: [9] <author> M. E. Crovella and M. Harchol-Balter. </author> <title> Task assignment in a distributed system: Improving performance by unbalancing load. </title> <booktitle> ACM SIGMETRICS, </booktitle> <month> July </month> <year> 1998. </year>
Reference-contexts: In a system with highly variable task sizes, users may be willing to wait longer for large tasks to complete, but expect that small tasks should complete quickly. Reducing average response time does not by itself insure that this will be the case <ref> [9] </ref>. Furthermore, stretch factor reveals the load of a system. A system with a high stretch factor is obviously overloaded, but one with high response time may not be so because the high value may be due to actual long task service demand. <p> Request scheduling for static content processing based on file sizes is studied in <ref> [9] </ref> where short jobs for accessing small file sizes are assigned to the lighted loaded nodes to avoid mixing with heavy jobs which access large files.
Reference: [10] <institution> Digital Equipment Corporation. About Alta Vista. </institution> <note> http://www.altavista.com/av/content/about.htm, 1995. </note>
Reference-contexts: Instances of M/S architecture can be found in current industry Web sites such as Web searching sites at Inktomi [17] and AltaVista <ref> [10] </ref>. A generalization for network services including proxy servers is made in [13]. Our contribution is to develop a generalized scheme for clustering Web servers with necessary optimization which considers characteristics of Web workloads.
Reference: [11] <author> F5Labs. BigIP. </author> <note> http://www.f5.com/, 1997. </note>
Reference-contexts: To address this issue, one solution is to maintain a hot-standby machine for each host [32]. Another technique for clustering Web servers with load balancing and fault tolerance support is load balancing switching products from Cisco, Foundry Networks and F5Labs <ref> [7, 12, 11] </ref>. These switching products assign a single address to a group of servers and distribute incoming requests among the live nodes, therefore effectively preventing users from accessing dead nodes. Switches use simple load balancing schemes which may not be sufficient for intensive dynamic content.
Reference: [12] <author> Foundry Networks. </author> <title> ServerIron Server Load Balancing Switch. </title> <note> http://www.foundrynet.com, 1998. </note>
Reference-contexts: To address this issue, one solution is to maintain a hot-standby machine for each host [32]. Another technique for clustering Web servers with load balancing and fault tolerance support is load balancing switching products from Cisco, Foundry Networks and F5Labs <ref> [7, 12, 11] </ref>. These switching products assign a single address to a group of servers and distribute incoming requests among the live nodes, therefore effectively preventing users from accessing dead nodes. Switches use simple load balancing schemes which may not be sufficient for intensive dynamic content.
Reference: [13] <author> A. Fox, S. Gribble, Y. Chawathe, E.A.Brewer, and P. Gauthier. </author> <title> Cluster-based scalable network services. </title> <booktitle> In Proceedings of the Sixteenth ACM Symposium on Operating System Principles, </booktitle> <month> Oct. </month> <year> 1997. </year>
Reference-contexts: Instances of M/S architecture can be found in current industry Web sites such as Web searching sites at Inktomi [17] and AltaVista [10]. A generalization for network services including proxy servers is made in <ref> [13] </ref>. Our contribution is to develop a generalized scheme for clustering Web servers with necessary optimization which considers characteristics of Web workloads. Another contribution is that we provide a detailed comparison and evaluation of different solutions, which benefits other Web application domains. <p> If these 3 requests are for Internet searches, requests are forwarded and processed by a cluster of SUN Ultra workstations. A similar setup is used at Altavista where DEC SMPs are employed as cluster nodes. A generalization from the Inktomi server to a layered structure is studied in <ref> [13] </ref> for general network services including proxy distillation. Their work is focused on general principles in organizing a cluster. There is no detailed study and performance evaluation on issues specific to Web servers. <p> Fault tolerance for CGI scripts is also cheap to implement. If a slave server dies during CGI execution, the master can mask this failure by restarting the request on a different slave node. This advantage is discussed in <ref> [13] </ref> for their layered network service. If DNS is used to direct requests to masters, the M/S setting reduces the number of server IP addresses exposed to clients compared to a flat architecture, thereby reducing the number of nodes that need to be monitored by hot-standby backup nodes. <p> The feasibility and importance of recruiting idle resource in a clustered environment has been studied in parallel computing [2] and distributed job execution [24]. Our research is focused on clustering for Web servers and takes into consideration Web access characteristics. A layered architecture for network service is proposed in <ref> [13] </ref>, which generalizes the structure of the Inktomi's searching server. Their work demonstrates its usefulness in the distillation proxy.
Reference: [14] <author> A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM: Parallel Virtual Machine A Users Guide and Tutorial for Network Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Local CGI RCGI Ultra-1 (Solaris 2.5.1) 32.2 41.9 46.0 37.6 Ultra-30 (Solaris 2.6) 20.0 27.8 27.9 20.3 Table 4: Response times in milliseconds for remote and local null-CGI execution and overhead of forking. 7 Related Work The concept of a master/slave structure can be found in earlier parallel programming research <ref> [14] </ref> where a host and multiple slaves are used to exploit the fork-join type of parallelism. The feasibility and importance of recruiting idle resource in a clustered environment has been studied in parallel computing [2] and distributed job execution [24].
Reference: [15] <author> S. Gribble. </author> <title> UC Berkeley Home IP HTTP Traces. </title> <note> http://www.acm.org/sigcomm/ITA/, 1997. </note>
Reference-contexts: We discuss the workload in our trace-driven experiments, the method to replay the traces, and present experimental results. 6.1 Workload description and evaluation methodology Table 1 summarizes the main characteristics of the traces we have obtained. The UCB log <ref> [15] </ref> was gathered from the Home IP service offered by UC Berkeley to its modem pool users. The KSU log was recorded by the Web server for online library service at Kansas State University.
Reference: [16] <author> V. Holmedahl, B. Smith, and T. Yang. </author> <title> Coperative caching of dynamic content on a distributed web server. </title> <booktitle> Procecedings of the Seventh High Performance Distributed Computing, </booktitle> <month> July </month> <year> 1998. </year>
Reference-contexts: Examples of such systems can be found in IBM's Atlanta Olympics Web server and the Alexandria Digital Library system <ref> [3, 16, 18] </ref>. Clustering is the most commonly used approach to increase throughput of a Web site. With a server cluster, multiple servers behave as a single host, from clients' perspective. NCSA [20] first proposed a clustering technique that uses DNS rotation to balance load among cluster nodes. <p> Our current implementation of the M/S architecture is based on the freely available Apache Web server version 1.3 source code and the Swala server with cooperative content caching support <ref> [16, 5] </ref>. To prevent malicious requests from being sent to the RCGI servers, access control and identity verification may be done by each RCGI server for each CGI request. This security check is not currently implemented in our testbed. <p> Otherwise, the load would be too light or too heavy. The average ratio of CGI processing rate to static request rate, r, is chosen to be: 1=20; 1=40; 1=80; 1=160 to represent a wide range of CGI resource demands. This choice is based on the previous studies <ref> [19, 16, 38] </ref>. a Arrival rates r UCB 0.12 1K, 2K for 32 nodes. 1 20 ; 1 80 ; 1 4K, 8K for 128 nodes. KSU 0.42 0.5K,1K for 32 nodes. 1 20 ; 1 80 ; 1 2K, 4K for 128 nodes. <p> Web caching for static contents at a server cluster is considered in [31] and dynamic content processing is not their focus. Web caching for dynamic content is possible if content is not changed frequently and this issue is studied in the Swala Web server <ref> [16] </ref>. Our work in this paper does not consider CGI caching.
Reference: [17] <institution> Inktomi Corporation. The Inktomi Technology Behind HotBot, </institution> <note> a White Paper. http://www.inktomi.com, 1996. </note>
Reference-contexts: We do not use HTTP redirection [4] for request re-scheduling because it adds client round-trip latency for every rescheduled request and also exposes IP addresses of server nodes. Instances of M/S architecture can be found in current industry Web sites such as Web searching sites at Inktomi <ref> [17] </ref> and AltaVista [10]. A generalization for network services including proxy servers is made in [13]. Our contribution is to develop a generalized scheme for clustering Web servers with necessary optimization which considers characteristics of Web workloads. <p> We also assume that a non-dedicated node can be used only when it becomes idle. This policy is used in [1, 24]. If one examines popular Web sites such as Microsoft, Inktomi and Altavista, clustering with a certain degree of data replication is fairly common. In the Inktomi design <ref> [17] </ref>, a few front-end hosts handle user HTTP requests. If these 3 requests are for Internet searches, requests are forwarded and processed by a cluster of SUN Ultra workstations. A similar setup is used at Altavista where DEC SMPs are employed as cluster nodes.
Reference: [18] <author> A. Iyengar and J. Challenger. </author> <title> Improving web server performance by caching dynamic data. </title> <booktitle> USENIX, </booktitle> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: Examples of such systems can be found in IBM's Atlanta Olympics Web server and the Alexandria Digital Library system <ref> [3, 16, 18] </ref>. Clustering is the most commonly used approach to increase throughput of a Web site. With a server cluster, multiple servers behave as a single host, from clients' perspective. NCSA [20] first proposed a clustering technique that uses DNS rotation to balance load among cluster nodes. <p> Finally, our log analysis of four Web sites in Section 6 shows that their average CGI result sizes are small, varying from 2KB to 7.5KB. Also notice that the average processing rates of CGI requests are one or two orders of magnitude slower than static requests <ref> [18] </ref>. This suggests that the bandwidth demands of CGI request should be fairly small, and would not cause contention in the cluster network. 3.2 Why M/S can be better The previous subsection shows that M/S does not inherently suffer any disadvantages over a standard flat architecture. <p> Also in [4, 38] URL redirection is used to achieve request assignment, which adds the round-trip overhead of client-server communication and exposes more node IP address. Our scheme uses remote dynamic content processing with low overhead to achieve rescheduling. The IBM group <ref> [18] </ref> proposes several methods to improve the DNS-based solution, by having the DNS server monitor the load of cluster nodes. They found that scheduling optimization is needed because IP address caching creates load imbalance. They also found that round-robin performs well if clients are uniformly distributed.
Reference: [19] <author> A. Iyengar, E. MacNair, and T. Nguyen. </author> <title> An analysis of web server performance. </title> <booktitle> Proceedings of IEEE GLOBECOM, </booktitle> <pages> pages 1943-1947, </pages> <month> Nov. </month> <year> 1997. </year>
Reference-contexts: Otherwise, the load would be too light or too heavy. The average ratio of CGI processing rate to static request rate, r, is chosen to be: 1=20; 1=40; 1=80; 1=160 to represent a wide range of CGI resource demands. This choice is based on the previous studies <ref> [19, 16, 38] </ref>. a Arrival rates r UCB 0.12 1K, 2K for 32 nodes. 1 20 ; 1 80 ; 1 4K, 8K for 128 nodes. KSU 0.42 0.5K,1K for 32 nodes. 1 20 ; 1 80 ; 1 2K, 4K for 128 nodes.
Reference: [20] <author> E. Katz, M. Butler, and R. McGrath. </author> <title> A scalable HTTP server: the NCSA prototype. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 27 </volume> <pages> 155-164, </pages> <year> 1994. </year>
Reference-contexts: Clustering is the most commonly used approach to increase throughput of a Web site. With a server cluster, multiple servers behave as a single host, from clients' perspective. NCSA <ref> [20] </ref> first proposed a clustering technique that uses DNS rotation to balance load among cluster nodes. Research has demonstrated that DNS round-robin rotation does not evenly distribute the load among servers, due to non-uniform resource demands of requests and DNS entry caching. <p> A client will pick one address from the set and then send its request to the selected server node. If the DNS server cyclically rotates possible IP addresses in response to IP address queries, then client requests may be distributed <ref> [4, 20] </ref>. Since IP addresses of a Web site are cached at client sites, clients at one site may actually access the same server node until a cached address expires. Thus load imbalance may be caused by address caching. <p> Our study extends this work by providing a generalized solution for Web servers and proposes necessary optimization support in order to make an layered architecture efficient. In addition to the NCSA solution for Web clustering <ref> [20] </ref>, the SWEB project [4] addresses load balancing on a cluster of nodes and uses the cost prediction to assign requests to nodes using multiple resource load indices. Previous work on general distributed load balancing [33] normally considers one load index.
Reference: [21] <author> T. Kroeger, J. Mogul, and C. Maltzahn. </author> <title> Digital's Web Proxy Traces. </title> <address> ftp://ftp.digital.com/pub/DEC/traces/proxy/webtraces.html, </address> <year> 1997. </year>
Reference-contexts: The ADL log is from the testbed of Alexandria Digital Library [3], a digital library for spatially referenced data. For these logs we can distinguish dynamic and static requests, and can extract the completion time of each request and the response size. We also have a DEC trace <ref> [21] </ref> available. For both UCB and DEC traces we cannot recognize URLs and their parameters because they are scrambled for privacy concern. Since the CGI percentage of the DEC trace is similar to the UCB log, we decided not to use that trace.
Reference: [22] <author> E. D. Lazowska, J. Zahorjan, G. S. Graham, and K. C. Sevcik. </author> <title> Quantitative System Performance: Computer System Analysis Using Queueing Network Models. </title> <publisher> Prentice Hall, </publisher> <year> 1984. </year>
Reference-contexts: To analyze these two models, we do not require specific request arrival patterns or request processing time distributions. For ease of analysis, we assume that the request queue length seen on average by an arriving customer must be equal to the time averaged queue length <ref> [22] </ref>. This assumption is satisfied by requests following Poisson distributions for arrival rates and for service times with exponential distribution. Requests can be processed in the First Come First Serve (FCFS) manner or time sharing manner. The results of this analysis are used in our system design (see next section).
Reference: [23] <author> S. J. Le*er, M. K. McKusick, M. J. Karels, and J. S. Quarterman. </author> <title> The Design and Impelmentation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison Wesley, </publisher> <year> 1990. </year>
Reference-contexts: This static request could wait nq units of time before being processed, compared to nt if all those requests were static. Typically, t is in an order of one millisecond since most static requests access small files, q is 0.01 or 0.1 second <ref> [23, 29] </ref>, and g is in an order of 0.1 or few seconds. If n=100, a static request could wait 100q seconds, which is an order of seconds before delivery. In today's Web sites, pages are increasingly complicated, containing a number of small files (e.g. buttons, icons, images) per page. <p> Each Web request will be provided with service time, I/O and CPU demand distribution. Each request job will be modeled as a sequence of CPU bursts and I/O bursts, submitted to the CPU queue and I/O queue. CPU scheduling is based on the UNIX BSD 4.3 strategy <ref> [23] </ref>. The process ready queue is a multilevel feedback queue divided into multiple lists according to process priority. Processes are scheduled based on priority and may be preempted following quantum expiration. The I/O queue also maintains a set of I/O processes and is scheduled using round-robin.
Reference: [24] <author> M. Litzkow, M. Livny, and W. </author> <title> Mutka. Condor ahunter of idle workstations. </title> <booktitle> Proceedings of the 8th International Conference of Distributed Computing Systems, </booktitle> <pages> pages 104-111, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Replication is common in systems requiring high availability [32]. We also assume that a non-dedicated node can be used only when it becomes idle. This policy is used in <ref> [1, 24] </ref>. If one examines popular Web sites such as Microsoft, Inktomi and Altavista, clustering with a certain degree of data replication is fairly common. In the Inktomi design [17], a few front-end hosts handle user HTTP requests. <p> The feasibility and importance of recruiting idle resource in a clustered environment has been studied in parallel computing [2] and distributed job execution <ref> [24] </ref>. Our research is focused on clustering for Web servers and takes into consideration Web access characteristics. A layered architecture for network service is proposed in [13], which generalizes the structure of the Inktomi's searching server. Their work demonstrates its usefulness in the distillation proxy.
Reference: [25] <author> U. Manber, M. Smith, and B. Gopal. </author> <title> Webglimpse combining browsing and searching. </title> <booktitle> Proceedings of the Usenix Technical Conference, </booktitle> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: As a result, these CGI requests are CPU intensive. For the KSU library-searching requests, we created a database with approximately 10000 items using the WebGlimpse software <ref> [25] </ref> and replaced the CGI library requests with WebGlimpse commands. These requests have mixed CPU and I/O demands, but on average 90% of service time is spent searching index information in memory.
Reference: [26] <author> L. McVoy and C. Staelin. lmbench: </author> <title> Portable tools for performance analysis. </title> <booktitle> Proceedings of the Usenix Technical Conference, </booktitle> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Other operating systems (for example, Linux) may use copy-on-write for fork, and will not see this performance difference. Informal experiments on Linux support this conclusion. 9 approximately 1.5ms 2 to the execution time for a remote CGI invocation. <ref> [26] </ref> This is still faster than running the CGI locally. <p> Those rates may be higher than what average workstation servers can deliver; however, these numbers are reasonable for a commercial Web site or for future machines, considering advances in CPU, disk, memory and networking technology. The system overhead charged in the simulation is based on current high-end server performance <ref> [26] </ref>. The CPU quantum is 10milliseconds. The priority update period is 100 milliseconds. The context switch overhead is 50 microseconds. The fork overhead is 3 milliseconds. The remote CGI latency (excluding fork) is 1 millisecond, representing TCP connection time. The page size is 8KB. <p> Simulated and actual stretch factors. Figure 9 compares the average stretch factors from experiments and from the simulation. The parameters we choose for the simulator are based on experimental specifications and performance indices of the Sun Ultra I <ref> [26] </ref>. The result shows that the difference between simulated and experimental results is less than 12%, with an average of 9%. This is very small, considering the simulator does not do detailed OS emulation. Simulated results vs. actual improvement ratios.
Reference: [27] <institution> National Coordination Office. Next Generation Internet Initiative. </institution> <note> http://www.ccic.gov/ngi/, 1996. </note>
Reference-contexts: Notice that the NGI initiative <ref> [27] </ref> (Next Generation Internet) addresses end-to-end connectivity at speeds from 100 Mbps up to 1 Gbps.
Reference: [28] <author> NCSA. </author> <title> Common Gateway Interface. </title> <note> http://booboo.ncsa.uiuc.edu/cgi/, 1995. </note>
Reference-contexts: To create dynamic content in response to an HTTP request, most servers implement the Common Gateway Interface (CGI), created by NCSA <ref> [28] </ref>. The CGI interface requires that the Web server initialize an environment containing the script parameters, and then fork and execute the script. As a result, every CGI request requires the creation of a new process.
Reference: [29] <author> J. Nieh and M. S. Lam. </author> <title> SMART unix svr4 support for multimedia applications. </title> <booktitle> Proceedings of IEEE International Conference on Multimedia Computing and Systems, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: This static request could wait nq units of time before being processed, compared to nt if all those requests were static. Typically, t is in an order of one millisecond since most static requests access small files, q is 0.01 or 0.1 second <ref> [23, 29] </ref>, and g is in an order of 0.1 or few seconds. If n=100, a static request could wait 100q seconds, which is an order of seconds before delivery. In today's Web sites, pages are increasingly complicated, containing a number of small files (e.g. buttons, icons, images) per page.
Reference: [30] <author> OpenMarket. </author> <title> Fast CGI. </title> <address> http://www.fastcgi.com/. </address>
Reference-contexts: Portability is also a concern, because a script written for one vendor's Web server will not work with another vendor's Web server. FastCGI, an extension of CGI developed by OpenMarket, attempts to circumvent some of the limitations of both proprietary interfaces and CGI <ref> [30] </ref>. A permanent connection is established between the server and the FastCGI application. Individual requests are forwarded from the server to the application, removing the need for process creation for each request. Currently only Apache [5] and Zeus [37] implement this interface.
Reference: [31] <author> V. S. Pai, M. Aron, G. Banga, M. Svendsen, P. Druschel, W. Zwaenepoel, and E. </author> <title> Nahum. Locality-Aware Request Distribution in Cluster-based Network Service. </title> <booktitle> In Proceedings of ASPLOS-VIII, </booktitle> <month> Oct. </month> <year> 1998. </year>
Reference-contexts: While this work does not address dynamic contention generation, our work is reminiscent in the sense that short file access jobs are assigned to masters and long CGI jobs are mainly assigned slaves. Web caching for static contents at a server cluster is considered in <ref> [31] </ref> and dynamic content processing is not their focus. Web caching for dynamic content is possible if content is not changed frequently and this issue is studied in the Swala Web server [16]. Our work in this paper does not consider CGI caching.
Reference: [32] <author> G. F. Pfister. </author> <title> In Search of Clusters. </title> <publisher> Prentice Hall, </publisher> <year> 1998. </year>
Reference-contexts: Hiding server failures is critical, especially when global customers depend on a Web site to get vital updated news and information. To address this issue, one solution is to maintain a hot-standby machine for each host <ref> [32] </ref>. Another technique for clustering Web servers with load balancing and fault tolerance support is load balancing switching products from Cisco, Foundry Networks and F5Labs [7, 12, 11]. <p> Moreover, a client site cannot be aware if a machine is no longer in service and may be denied service if that client tries to use a cached IP address to access a dead node. A solution for masking server failures is called "failover" using hot-standby techniques <ref> [32] </ref>. This solution dictates that one computer be used to monitor another one and it is activated to take over if the monitored computer crashes. Industry has sold such solutions for many years. <p> This can be realized if each server node has its local disk storage system and Web data is replicated to all nodes, or if they use a shared file system. Replication is common in systems requiring high availability <ref> [32] </ref>. We also assume that a non-dedicated node can be used only when it becomes idle. This policy is used in [1, 24]. If one examines popular Web sites such as Microsoft, Inktomi and Altavista, clustering with a certain degree of data replication is fairly common.
Reference: [33] <author> B. A. Shirazi, A. R. Hurson, and K. M. Kavi, </author> <title> editors. Scheduling and Load Balancing in Parallel and Distributed Systems. </title> <publisher> IEEE CS Press, </publisher> <year> 1995. </year>
Reference-contexts: In addition to the NCSA solution for Web clustering [20], the SWEB project [4] addresses load balancing on a cluster of nodes and uses the cost prediction to assign requests to nodes using multiple resource load indices. Previous work on general distributed load balancing <ref> [33] </ref> normally considers one load index. The SWEB experiment identifies the importance of using multiple indices for Web clustering. Its scheme requires an accurate cost prediction of each request, which is possible for file accesses and certain operations, but not for most applications.
Reference: [34] <institution> SPEC. </institution> <note> SPECWeb96 Benchmark. http://www.spec.org/osg/web96/, 1996. </note>
Reference-contexts: According to data submitted to SPEC in the third quarter of 1998, high performance Web servers can process file fetch requests at a rate of up to 13000 requests per second based on the SPECWeb96 benchmark <ref> [34] </ref>. Because Web traffic for static content is dominated by small file access, the average response size is 15Kbytes in SPECWeb96. This indicates that the delivery rate of such a node approaches 1.6Gbits/second.
Reference: [35] <author> G. Trend and M. Sake. WebSTONE: </author> <title> The first generation in HTTP server benchmarking. Silicon Graphics, </title> <publisher> Inc. </publisher> <address> whitepa-per: http://www.sgi.com/, Feb. </address> <year> 1995. </year>
Reference-contexts: We will need to replace those CGI requests with real operations in order to run experiments. In doing such a replacement, we generate synthetic loads which represent CPU and I/O intensive or mixed situations. For the UCB trace, we use a CGI script from the WebSTONE benchmark <ref> [35] </ref>. This script receives a file size as a parameter and dynamically generates a file of that size and returns it to the client. We modified this script so that we can control the running time of the script in order to generate different loads by CPU busy-spinning.
Reference: [36] <institution> Yahoo! Inc. Yahoo! Investor Relations Center. </institution> <note> http://www.yahoo.com/info/investor/, 1998. </note>
Reference-contexts: It also can be a DNS-based solution in a perfect situation, i.e. clients are uniformly distributed geographically and IP address caching does not create a problem. The study in [8] shows that 3 Currently Yahoo receives 115 million page views per day in June 1998 <ref> [36] </ref> which is around 1331 hits per second. We expect the hit ratios will increase substantially over next few years. 12 its performance is near-optimal for a DNS setting, better than the other methods proposed in [8] with this uniform-distribution assumption. * Flat-C.
Reference: [37] <institution> Zeus Technology. </institution> <note> Zeus Web Server v3. http://www.zeustech.net/, 1998. </note>
Reference-contexts: A permanent connection is established between the server and the FastCGI application. Individual requests are forwarded from the server to the application, removing the need for process creation for each request. Currently only Apache [5] and Zeus <ref> [37] </ref> implement this interface. This paper demonstrates the proposed techniques for CGI due to its popularity, but our techniques could be applied to other dynamic content generation protocols. cluster is presented to the Internet as a single logical server. Each server node has a unique Internet Protocol (IP) address.
Reference: [38] <author> H. Zhu, T. Yang, Q. Zheng, D. Watson, O. H. Ibarra, and T. Smith. </author> <title> Adaptive load sharing for clustered digital library servers. </title> <booktitle> Procecedings of the Seventh High Performance Distributed Computing, </booktitle> <month> July </month> <year> 1998. </year> <month> 20 </month>
Reference-contexts: The study in <ref> [38] </ref> shows that scheduling based on the I/O and CPU demands of CGI requests can result in substantial performance gains. * The class of CGI requests normally require much more computing and I/O resources, compared to static file retrieval requests. <p> Otherwise, the load would be too light or too heavy. The average ratio of CGI processing rate to static request rate, r, is chosen to be: 1=20; 1=40; 1=80; 1=160 to represent a wide range of CGI resource demands. This choice is based on the previous studies <ref> [19, 16, 38] </ref>. a Arrival rates r UCB 0.12 1K, 2K for 32 nodes. 1 20 ; 1 80 ; 1 4K, 8K for 128 nodes. KSU 0.42 0.5K,1K for 32 nodes. 1 20 ; 1 80 ; 1 2K, 4K for 128 nodes. <p> Our scheme uses a sampling technique and a relative cost function, which does not require the estimation of direct processing cost. Such a scheme is more general and practical. Our work is also an extension of the research for digital library server clustering <ref> [38] </ref>. Also in [4, 38] URL redirection is used to achieve request assignment, which adds the round-trip overhead of client-server communication and exposes more node IP address. Our scheme uses remote dynamic content processing with low overhead to achieve rescheduling. <p> Our scheme uses a sampling technique and a relative cost function, which does not require the estimation of direct processing cost. Such a scheme is more general and practical. Our work is also an extension of the research for digital library server clustering [38]. Also in <ref> [4, 38] </ref> URL redirection is used to achieve request assignment, which adds the round-trip overhead of client-server communication and exposes more node IP address. Our scheme uses remote dynamic content processing with low overhead to achieve rescheduling.
References-found: 38

