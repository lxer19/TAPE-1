URL: http://www.neci.nj.nec.com/homepages/eric/ml.ps
Refering-URL: http://www.neci.nj.nec.com/homepages/eric/
Root-URL: 
Email: eric@research.nj.nec.com  
Title: Toward a Model of Mind as a Laissez-Faire Economy of Idiots Extended Abstract  
Author: Eric B. Baum 
Address: 4 Independence Way Princeton, NJ 08540  
Affiliation: NEC Research Institute  
Abstract: I argue that the mind should be viewed as an economy, and describe an algorithm that autonomously apportions complex tasks to multiple cooperating agents in such a way that the incentive of each agent is exactly to maximize my reward, as owner of the system. A specific model, called "The Hayek Machine" is proposed and tested on a simulated Blocks World (BW) planning problem. Hayek learns to solve far more complex BW problems than any previous learning algorithm. If given intermediate reward and simple features, it learns to efficiently solve arbitrary BW problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bacchus, F., Kabanza, F. </author> <title> (1995) Using temporal logic to control search in planning, </title> <note> unpublished document available from http://logos.uwaterloo.ca/tlplan/tlplan.html. </note>
Reference-contexts: The main thrust of ongoing work is how to improve Hayek to expand its representational capability autonomously. (see x8). In the meantime, I have added some useful variables to Hayek's representation by hand. I introduced three terms: top <ref> [1] </ref>, top [2], top [3]. top [i] is the height at which the next block will go, if placed on stack i. When creating a new random condition, wherever a term of "type" height might appear, top [i] is placed randomly with probability (1=3)P top . <p> Hayek solutions (even without incremental feedback and top [i] variables) are better than our simple programming efforts, using a similar impoverished representation, in which it is impossible to specify a general program. Bacchus and Kabanza <ref> [1] </ref> tried some sophisticated planning algorithms such as SNLP [15],[20], Prodigy 4.0 [7], and their own TLPlan on Blocks World Problems related to (but different from) ours.
Reference: [2] <author> Barto, A. G., Bradtke, S. J., Singh, S. P. </author> <title> (1995) learning to act using real-time dynamic programming, </title> <journal> AI Journal, </journal> <note> to appear. </note>
Reference-contexts: Thus each agent has a fixed bid and a fixed action. Roughly speaking, this is analagous to a nearest neighbor approach to learning the functions mapping state to a bid, action pair. In the standard Q-learning [25] or RTDP <ref> [2] </ref> framework, an evaluation function maps states of the world to numerical estimates of expected value. If there are many states, one must regularize somehow to avoid the curse of dimensionality. Hayek maps condition action pairs to expected value. <p> The main thrust of ongoing work is how to improve Hayek to expand its representational capability autonomously. (see x8). In the meantime, I have added some useful variables to Hayek's representation by hand. I introduced three terms: top [1], top <ref> [2] </ref>, top [3]. top [i] is the height at which the next block will go, if placed on stack i. When creating a new random condition, wherever a term of "type" height might appear, top [i] is placed randomly with probability (1=3)P top .
Reference: [3] <author> Baum, E. B., </author> <title> (1996) Toward a Model of Mind as Laissez-Faire Economy of Idiots, </title> <note> full paper, http://www.neci.nj.nec.com:80/homepages/eric/eric.html </note>
Reference-contexts: The main thrust of ongoing work is how to improve Hayek to expand its representational capability autonomously. (see x8). In the meantime, I have added some useful variables to Hayek's representation by hand. I introduced three terms: top [1], top [2], top <ref> [3] </ref>. top [i] is the height at which the next block will go, if placed on stack i. When creating a new random condition, wherever a term of "type" height might appear, top [i] is placed randomly with probability (1=3)P top .
Reference: [4] <author> Baum, E. B., </author> <title> (1996) Models of Both Mind and Economy, to appear in The Economy as an Evolving Complex System II, </title> <editor> eds W. Brian Arthur, Steven Durlauf, and David A. Lane, </editor> <publisher> Santa Fe Institute series Addison Wesley, </publisher> <address> Reading MA. </address>
Reference-contexts: Assuming we understand how optimally to design a human-like intelligent systems, how much raw computing power will we need to solve interesting problems? Finally, numerous questions are suggested within the domain of economics <ref> [4] </ref>. Acknowledgements Charles Garrett wrote all the code and ran all the experiments in the first year, from high level (English language) specifications. I thank him for his efficient and expert assistance which was integral to making this paper a reality.
Reference: [5] <author> Baum, E. B., Boneh, D. Garrett, C. </author> <booktitle> (1995) On Genetic Algorithms, in Proceedings of the Eighth Annual Conference on Computational Learning Theory, </booktitle> <pages> pp 230-239. </pages>
Reference-contexts: Practical considerations have generally forced use of small systems. These are effectively small finite state machines. Are they a particularly efficient or learnable encoding of finite state machines? Genetic algorithms (GA's) may possibly be highly inefficient at training Classifier systems. While recent results <ref> [5] </ref> show that some GA's can beat hill climbing dramatically in some contexts, and even provide some direct motivation for their use in training classifier-like systems, GA's often often scale less efficiently, leading to huge performance divergences on large problems. Hayek is (currently) trained by hill climbing.
Reference: [6] <author> Birk, A., Paul, W. J., </author> <title> (1995) Schemas and Genetic Programming, </title> <note> document to be published. </note>
Reference-contexts: Whitehead and Ballard [24] applied Reinforcement Learning techniques to learn to pick up a green block, with at most three other blocks on top of it. Birk and Paul <ref> [6] </ref> attempted to learn to move a single block around an otherwise empty table.
Reference: [7] <author> Carbonell, J. G., J. Blythe, O. Etzioni, Y. Gill, R. Joseph, D. Khan, C. Knoblock, S. Minton, A. Perez, S. Reilly, M. Veloso, and X. </author> <title> Wang (1992) Prodigy 4.0: The manual and Tutorial. </title> <type> Technical Report CMU-CS-92-150, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: Hayek solutions (even without incremental feedback and top [i] variables) are better than our simple programming efforts, using a similar impoverished representation, in which it is impossible to specify a general program. Bacchus and Kabanza [1] tried some sophisticated planning algorithms such as SNLP [15],[20], Prodigy 4.0 <ref> [7] </ref>, and their own TLPlan on Blocks World Problems related to (but different from) ours. Even when run on a Blocks World with an unbounded table, SNLP, Prodigy, and TLPlan all exceeded resource bounds on problems of about six blocks.
Reference: [8] <author> Cosimidies, L. and J. </author> <title> Tooby (1992) Cognitive adaptations for Social Exchange, </title> <editor> in Barkow, J. H., L. Cosimidies, and J. </editor> <title> Tooby (1992) The adapted mind, </title> <publisher> Oxford University Press. NY., </publisher> <pages> pp 163-228. </pages>
Reference-contexts: Half the rules are novel and half are mutations of previous rules. A new rule has i clauses in 3 This encoding implicitly encodes topology, departing from tabula rasa. Human infants are apparently born with implicit knowledge of topology and object permanence <ref> [8] </ref>. Of course, by the time he had any chance of solving these BW problems, a child would have learned much about the world.
Reference: [9] <editor> Drescher, </editor> <publisher> G.L.(1991)Made-Up Minds,MIT Press. </publisher>
Reference: [10] <author> Forrest, S. </author> <title> (1985) Implementing semantic network structures using the classifier system. </title> <booktitle> In Proc. First International Conference on Genetic Algorithms, </booktitle> <pages> pp 188-196. </pages> <address> Hillsdale NJ: </address> <publisher> Lawrence Erl-baum Associates. </publisher>
Reference: [11] <author> Hardin, G. </author> <title> "The Tragedy of the Commons". </title> <journal> Sci ence, 1968, </journal> <volume> 162, </volume> <pages> 1243-1248. </pages>
Reference: [12] <author> Holland, J. H. </author> <title> (1986) Escaping brittleness: the possibilities of general purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, (eds.) </editor> <booktitle> Machine Learning II pp 593-623, </booktitle> <publisher> Los Altos CA Morgan Kauffman. </publisher>
Reference-contexts: See e.g. [17] for a discussion of parasitic behavior in Eurisko. In my view, specifying the interaction of dynamic systems in the Society of Mind [18] would be rather more challenging than centrally managing the Soviet economy. Holland's seminal Classifier systems <ref> [12] </ref> were (to my knowledge) the first explicit proposal of an economic analog of mind. Holland also first suggested condition, action, bid agents. Holland classifiers have, however, been viewed as empirically disappointing [26]. I think that Holland classifiers again suffer from distorted incentives. <p> This method did not work well because it was subject to inflation. The constant entry of new agents injects noise. Holland's classifier systems <ref> [12] </ref> set bids by b = ffsW where ff is a small constant, s is the specificity of a classifier, and W its wealth.
Reference: [13] <editor> Koza, J.R., </editor> <booktitle> (1992) Genetic Programming, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <pages> pp 459-470. </pages>
Reference-contexts: Whitehead and Ballard [24] applied Reinforcement Learning techniques to learn to pick up a green block, with at most three other blocks on top of it. Birk and Paul [6] attempted to learn to move a single block around an otherwise empty table. Koza <ref> [13] </ref> addressed a simplified problem having an unbounded table, the concrete goal of building the same fixed stack every instance, fixed number of blocks and at most two obstructing solution, and built in pre-supplied sensors giving all pertinent information, including, e.g., the Next Needed Block, and both the top block and
Reference: [14] <author> Lenat, D. B., </author> <title> "The role of heuristics in learn ing by discovery: three case studies", </title> <editor> in Michal-ski,R.,S., Carbonell, J.G., and Mitchell, T., eds, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <publisher> (Tioga Pub. Co., </publisher> <address> Palo Alto CA 1983 pp 243-306. </address>
Reference-contexts: Thus Hayek should metalearn as creation agents are trained to look in likely directions. Within Hayek creation agents should be rewarded precisely as investors in the created agents. The economic importance of intellectual property rights in the created agent will be studied. Previous efforts at metalearning (e.g. <ref> [14] </ref>) did not view the system as an economy, and so, in my opinion, did not correctly compensate their meta-agents, resulting in distorted incentives. Biological evolution learned starting from tabula rasa, driven strictly by reinforcement. The struggle for survival is real time. Quick reaction is critical.
Reference: [15] <author> McAllester, D., and D. </author> <booktitle> Rosenblitt (1991) "Sys tematic nonlinear planning" in Proceedings of the AAAI National Conference, </booktitle> <pages> pp 634-639. </pages>
Reference: [16] <author> Miller, M. S., and K. E. Drexler, </author> <title> "Markets and computation: Agoric open systems", </title> <editor> in B. A. Huberman, ed, </editor> <booktitle> The Ecology of Computation, Studies in Computer Science and Artificial Intelligence 2, </booktitle> <publisher> North Holland, </publisher> <address> New York, pp133-176, </address> <year> (1988). </year>
Reference-contexts: Hayek is (currently) trained by hill climbing. Miller and Drexler have proposed "Agoric computation", a model of computing based on an economic analog <ref> [16] </ref>. These authors remark that computational markets may avoid any imperfections of real ones. Hayek incorporates a perfectly accurate credit check which advances credit to exactly those agents who will be able to meet their obligations. Several authors have studied learning to plan in a Blocks world context.
Reference: [17] <author> Miller, M. S., and K. E. </author> <title> Drexler, </title> <editor> "Compara tive ecology", in B. A. Huberman, ed, </editor> <booktitle> The Ecology of Computation, Studies in Computer Science and Artificial Intelligence 2, </booktitle> <publisher> North Holland, </publisher> <address> New York, pp51-76, </address> <year> (1988). </year>
Reference-contexts: This motivation and application will be discussed elsewhere [4],[3]. 1.1 Previous Related Work Many authors have considered how mental capabilities may arise from the interaction of smaller units, e.g. [18][9] [23][22][14][19]. These models are not economic and in many cases arguably have suffered seriously from distorted incentives. See e.g. <ref> [17] </ref> for a discussion of parasitic behavior in Eurisko. In my view, specifying the interaction of dynamic systems in the Society of Mind [18] would be rather more challenging than centrally managing the Soviet economy.
Reference: [18] <author> Minsky, M., </author> <title> (1986)The Society of Mind Simon and Schuster, </title> <address> NY. </address>
Reference-contexts: Any such understanding must model how large computational tasks can be broken down into smaller components, how such components can be coordinated, how the system can gain knowledge, how computations performed can be tractable, and must not appeal to a homunculus <ref> [18] </ref>. What these smaller components, or agents, calculate, how they calculate it, and when they contribute, can't be specified by a superior intelligence either. But somehow these things must be determined. This can only happen if they are somehow given rewards so that they learn what and when to contribute. <p> from the interaction of smaller units, e.g. <ref> [18] </ref>[9] [23][22][14][19]. These models are not economic and in many cases arguably have suffered seriously from distorted incentives. See e.g. [17] for a discussion of parasitic behavior in Eurisko. In my view, specifying the interaction of dynamic systems in the Society of Mind [18] would be rather more challenging than centrally managing the Soviet economy. Holland's seminal Classifier systems [12] were (to my knowledge) the first explicit proposal of an economic analog of mind. Holland also first suggested condition, action, bid agents. Holland classifiers have, however, been viewed as empirically disappointing [26].
Reference: [19] <author> Newell, A. </author> <year> (1990), </year> <title> Universal Theories of Cogni tion, </title> <publisher> Harvard University Press, </publisher> <address> Cambridge MA. </address>
Reference: [20] <author> Soderlan, S. , Barrett, T., and Weld, D. </author> <title> (1990) The SNLP planner implementation, contact bug-snlp@cs.washington.edu. </title>
Reference: [21] <author> Sutton, R. S. </author> <title> (1988) Learning to predict by the methods of temporal differences, </title> <booktitle> Machine Learning 3: </booktitle> <pages> 9-44. </pages>
Reference-contexts: By resetting, Hayek effectively runs a perfect credit market, in which agents extending credit appeal to an oracle that foretells whether the applicant will meet his obligations. We experimented with other means of bid selection. To compare with Real Time Dynamic Programming (RTDP) or Temporal Difference type (TD) learning <ref> [21] </ref>, we updated the bid of active agents by b ! b (1 *) + (b 0 + payoff)(*) where b 0 is the bid of the following agent, payoff is any reward on that particular step, and * is a small parameter.
Reference: [22] <author> Valiant, L. </author> <title> (1995) Rationality, </title> <booktitle> in Proceedings of the Eighth Annual Conference on Computational Learning Theory, </booktitle> <pages> 3-14. </pages>
Reference: [23] <author> Valiant, L. </author> <title> (1994) Circuits of the Mind, </title> <publisher> Oxford University Press. </publisher>
Reference: [24] <author> Whitehead, S. D. and D. H. Ballard. </author> <title> (1991) "Learning to Perceive and Act." </title> <journal> Machine Learning 7, </journal> <volume> 1, </volume> <pages> 45-83. </pages>
Reference-contexts: However the versions of Blocks World they have studied are far simpler than that studied here. All previous learning work I'm aware of involved an unrestricted table size, and more importantly did not involve learning an abstract goal. Whitehead and Ballard <ref> [24] </ref> applied Reinforcement Learning techniques to learn to pick up a green block, with at most three other blocks on top of it. Birk and Paul [6] attempted to learn to move a single block around an otherwise empty table.
Reference: [25] <author> Watkins, C. J. C. H. </author> <title> (1989) Learning from delayed rewards, </title> <type> Doctoral thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge England. </address>
Reference-contexts: Thus each agent has a fixed bid and a fixed action. Roughly speaking, this is analagous to a nearest neighbor approach to learning the functions mapping state to a bid, action pair. In the standard Q-learning <ref> [25] </ref> or RTDP [2] framework, an evaluation function maps states of the world to numerical estimates of expected value. If there are many states, one must regularize somehow to avoid the curse of dimensionality. Hayek maps condition action pairs to expected value.
Reference: [26] <author> Wilson, S. W., Goldberg, D. E. </author> <title> (1989) A critical review of classifier systems. </title> <editor> In J. D. Schaffer, ed. </editor> <booktitle> Proc. Third International Conf. </booktitle> <address> on Genetic Algorithms San Mateo CA, </address> <publisher> Morgan Kauffman. </publisher>
Reference-contexts: Holland's seminal Classifier systems [12] were (to my knowledge) the first explicit proposal of an economic analog of mind. Holland also first suggested condition, action, bid agents. Holland classifiers have, however, been viewed as empirically disappointing <ref> [26] </ref>. I think that Holland classifiers again suffer from distorted incentives. Initial wealth values for agents are a critical problem.
References-found: 26

