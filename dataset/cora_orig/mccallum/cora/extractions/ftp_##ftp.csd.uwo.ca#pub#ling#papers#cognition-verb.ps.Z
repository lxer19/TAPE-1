URL: ftp://ftp.csd.uwo.ca/pub/ling/papers/cognition-verb.ps.Z
Refering-URL: http://www.csd.uwo.ca/faculty/ling/sub-pub.html
Root-URL: 
Email: Email: ling@csd.uwo.ca.  
Title: Answering the Connectionist Challenge: A Symbolic Model Of Learning the Past Tenses Of English Verbs  
Author: Charles X. Ling Marin Marinov 
Address: Ontario, London, Ontario, Canada N6A 5B7.  Bul. Tzar Osvoboditel #15, Sofia 1000, Bulgaria.  
Affiliation: Department of Computer Science, The University of Western  Department of Philosophy, Sofia University,  
Date: 49(3), 235-290, December, 1993.  
Note: Preprint. Appeared in Cognition,  Corresponding author.  
Abstract: Supporters of eliminative connectionism have argued for a pattern association based explanation of language learning and language processing. They deny that explicit rules and symbolic representations play any role in language processing and cognition in general. Their argument is based to a large extent on two artificial neural network (ANN) models that are claimed to be able to learn the past tenses of English verbs. (Rumelhart and McClelland, 1986; MacWhinney and Leinbach, 1991). In this article we critically review Rumelhart and McClelland's as well as MacWhinney and Leinbach's ANN-models and conclude that they do not succeed in the assigned task of learning the past tenses of English verbs. In order to answer their challenge to the symbolic processing approach, we present our Symbolic Pattern Associator (SPA) a general purpose pattern associator that can learn to associate arbitrary discrete patterns. We carried out several experiments with the SPA using the same set of verbs that was used in MacWhinney and Leinbach's simulation with more realistic training and testing procedures. The SPA outperformed the connectionist models by a wide margin in the accuracy of learning, and successful inductive generalizations to unseen verbs. Our SPA has very natural and psychologically realistic explanations to many psychological effects such as U-shaped learning curve, and is much closer to human subjects in predicting past tenses of the pseudo-verbs. In contrast to ANNs whose internal representations are entirely opaque, the SPA can represent the acquired knowledge in the form of production rules that allow for further higher level processing and integration, resulting in linguistically realistic associative templates for irregular verbs and production rules for regular verbs. In the light of these findings, we conclude that eliminative connectionists' vision of cognition as simple pattern association and pattern recognition without symbolic representation is inadequate. Pattern association as such does not imply rule-less or cue-based models of language acquisition or of human learning in general. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aronoff, M. </author> <year> (1976). </year> <title> Word Formation in Generative grammar. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Atlas, L, Cole, R., Connor, J., El-Sharkawi, M., Marks, R., Muthasamy, Y., & Barnard, E. </author> <year> (1990). </year> <title> Performance comparisons between backpropagation networks and classification trees on three real-world applications. </title> <editor> In D. Touretzky, (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems (pp. </booktitle> <volume> 622- 29), Vol. </volume> <pages> 2, </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Inc. </publisher>
Reference: <author> Bybee, J. & Slobin, D. </author> <year> (1982). </year> <title> Why small children cannot change language on their own: Suggestions from the English past tense. </title> <editor> In A. Ahlqvist (Ed.), </editor> <booktitle> Papers from the 5th International Conference on Historical Linguistics. (Current Issues in Linguistic Theory, </booktitle> <volume> Vol. 21) Philadelphia/Amsterdam: </volume> <pages> Benjamins. </pages>
Reference: <author> Churchland, P.M. </author> <year> (1988). </year> <title> Matter and Consciousness. Revised edition. </title> <publisher> Cambridge, MA : MIT Press. </publisher>
Reference: <author> Churchland, P.M. </author> <year> (1989). </year> <title> A Neurocomputational Perspective. </title> <publisher> Cambridge, MA : MIT Press. </publisher>
Reference: <author> Egedi, D. and Sproat, R. </author> <year> (1991). </year> <title> Connectionist networks and natural language morphology. </title> <type> Unpublished manuscript. </type> <institution> Linguistics Research Department, AT&T Bell Lab., </institution> <address> Murray Hill, NJ. </address>
Reference: <author> Feng, C., King, R., Sutherland, A., & Henery, R. </author> <year> (1992). </year> <title> Comparison of symbolic, statistical and neural network classifiers. </title> <type> Manuscript. </type> <institution> Department of Computer Science, University of Ottawa. </institution>
Reference-contexts: Shavlik, Mooney and Towell (1991) carried out a thorough experimental comparison between ID3, ANNs using BP and the perceptron on five real-world data sets including the famous connectionist text-to-speech conversion model NETtalk (Sejnowski and Rosenberg, 1987). In the StatLog project <ref> (Feng et al., 1992) </ref>, 18 different symbolic, statistical and neural network learning algorithms were critically (and more or less fairly) compared over 6 large data sets from industrial applications.
Reference: <author> Fisher, D. & McKusick, K. </author> <year> (1989). </year> <title> An empirical comparison of ID3 and back-propagation. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 788-793). </pages> <address> Detroit, MI. </address>
Reference: <author> Fodor, J.A. & McLaughlin, B.P. </author> <year> (1990). </year> <title> Connectionism and the problem of systematicity. </title> <journal> Cognition, </journal> <volume> 35, </volume> <pages> 183-204. </pages>
Reference: <author> Fodor, J. A. & Pylyshyn, Z. </author> <year> (1988). </year> <title> Connectionism and Cognitive Architecture: A Critical Analysis. </title> <editor> In S. Pinker & J. Mehler, (Eds.), </editor> <title> Connections and Symbols (pp. </title> <address> 3-71). Cambridge, </address> <publisher> MA : MIT Press. </publisher>
Reference-contexts: Such capacities are essential if artificial 1 neural network (ANN)-based cognitive architectures can hope to explain certain pervasive cognitive phenomena like systematicity <ref> (cf. Fodor and Pylyshyn, 1988) </ref>.
Reference: <author> Gorman, R.P. & Sejnowski, T.J. </author> <year> (1988). </year> <title> Analysis of hidden units in a layered network trained to classify sonar targets. </title> <journal> Neural Network, </journal> <volume> 1 (Part </volume> 1):75-89. 
Reference: <author> Hunt, E., Marin, J., & Stone, P. </author> <year> (1966). </year> <title> Experiments in Induction. </title> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference-contexts: One of the most widely studied of these systems is ID3 (Quinlan, 1986a). The ID3 learning system has been designed by Quinlan (1986a). C4.5 is a latter improved implementation of ID3 (Quinlan, 1986b, 1989). ID3 is a descendant from Hunt, Marin and Stone's CLS <ref> (Hunt, Marin, and Stone, 1966) </ref> and falls into the class of TDIDT (top-down induction of decision trees) learning systems.
Reference: <author> Kim, J., Pinker, S., Prince, A., and Prasada, S. </author> <year> (1991). </year> <title> Why no mere mortal has ever flown out to center field. </title> <journal> Cognitive Science, </journal> <volume> 15, </volume> <pages> 173-218. </pages> <note> 50 Kiparsky, </note> <author> P. </author> <year> (1982). </year> <title> Lexical phonology and morphology. </title> <editor> In I.S.Yang (Ed.), </editor> <booktitle> Linguistics in the Morning Calm, </booktitle> <pages> (3-91). </pages> <editor> Seul: Hansin Lachter, J. & Bever, T. </editor> <year> (1988). </year> <title> The relation between linguistic structure and associative theories of language learning A constructive critique of some connectionist learning models. </title>
Reference: <editor> In S. Pinker & J. Mehler, (Eds.), </editor> <title> Connections and Symbols (pp. </title> <address> 195-247). Cambridge, </address> <publisher> MA : MIT Press. </publisher>
Reference: <author> Lapointe, S., Ling, C.X., & Matwin, M. </author> <year> (1993). </year> <title> Constructive Inductive Logic Programming. </title> <booktitle> In Proceedings of IJCAI-93 (Thirteenth International Conference on Artificial Intelligence). </booktitle> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: <author> Ling, C.X. </author> <year> (1994). </year> <title> Learning the past tense of English verbs: the Symbolic Pattern Associator vs. connectionist models. </title> <journal> Journal of Artificial Intelligence Research. </journal> <note> To appear. </note>
Reference: <author> Marcus, G., Pinker, S., Ullman, M., Hollander, M., Rosen, T., and Xu, F. </author> <year> (1993). </year> <title> Overregular-ization in language acquisition. </title> <booktitle> Monographs of the Society for Research in Child Development, </booktitle> <volume> 57(4, Serial No. </volume> <pages> 228). </pages>
Reference-contexts: For example, if m is increased only from 2 to 3, we observe less than 10% drop in the accuracy of irregular verb token due to overregularization. This low rate of overregularization is in accordance with the latest experimental results. <ref> (Cf. Marcus et al, 1993) </ref>. We can also explain very elegantly why less frequent irregular verbs are more likely to be overregularized than more frequent irregular verbs.
Reference: <author> McClelland, J. & Rumelhart, D. </author> <year> (1988). </year> <title> Explorations in Parallel Distributed Processing. </title> <publisher> The MIT Press. </publisher>
Reference: <author> McClelland, J., Rumelhart, D., & Hinton, G. </author> <year> (1986). </year> <title> The appeal of parallel distributed processing. </title> <editor> In Rumelhart, D., McClelland, J. </editor> & <booktitle> the PDP Research Group (Eds.), Parallel Distributed Processing, </booktitle> <volume> Vol. </volume> <pages> 1 (pp. 3-44). </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Eliminative connectionism has received a great deal of attention largely due to a famous PDP model developed by Rumelhart and McClelland that was claimed to be capable of learning the past tenses of English verbs without the use of explicit rules and symbolic representations <ref> (Rumelhart and McClelland, 1986, pp. 216-271) </ref>. After several critical reviews of this model by Pinker and Prince (1988), Lachter and Bever (1988) and a detailed background analysis of the model by Plunkett and Marchman (1991), a new revised PDP model was proposed by MacWhinney and Leinbach (1991) (M&L henceforth). <p> The child need not decide whether a verb is regular or irregular. There is no question as to whether the inflected form should be stored directly in the lexicon or derived from more general principles <ref> (Rumelhart and McClelland, 1986, p. 267) </ref>. 2.3 Criticism of Rumelhart and McClelland's Model Rumelhart and McClelland's model has received extensive critical attention in the literature. Two of the best known critical reviews were written by Pinker and Prince (1988), and Lachter and Bever (1988).
Reference: <author> MacWhinney, B. </author> <year> (1990). </year> <title> The CHILDES Project: Tools for Analyzing Talk. </title> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: as bEt b__E_t____________ _E__t template: CCCVVCCCVVCCCVVCCC VVCCC (left-justified) (right-justified) In this way all English verbs that have more than three syllables, more than three consonantal phonemes in a row, or more than two vocalic phonemes in a row were left out of this 3 MacWhinney and Leinbach use the UNIBET <ref> (MacWhinney, 1990) </ref> phoneme coding scheme which allows the assignment of a single-letter ASCII code to each phoneme. 8 experiment because they did not fit the chosen template. Altogether 2062 regular and irregular English verbs were selected.
Reference: <author> MacWhinney, B. & Leinbach, J. </author> <year> (1991). </year> <title> Implementations are not conceptualizations: Revising the verb model. </title> <journal> Cognition, </journal> <volume> 40, </volume> <pages> 121-157. </pages>
Reference: <author> Marinov, M. </author> <year> (1993). </year> <title> On the spuriousness of the symbolic/subsymbolic distinction. Minds and Machines (forthcoming). </title>
Reference-contexts: The SPA is based on the well-known decision tree learning algorithm ID3 4 . 4 Several modifications in the C4.5 program are made. See <ref> (Ling & Marinov, 1993) </ref>. 12 3.1 The ID3 Symbolic Learning System Many connectionists believe that neural networks possess some unique advantage over symbolic systems in that they are capable of learning things that symbolic systems are inherently incapable of.
Reference: <author> Muggleton, S. & Buntine, W. </author> <year> (1988). </year> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Pinker, S. </author> <year> (1984). </year> <title> Language Learnability and Language Development, </title> <address> Cambridge, MA: </address> <publisher> Har-vard University Press Pinker, </publisher> <editor> S. </editor> <year> (1991). </year> <title> Rules of Language. </title> <journal> Science, </journal> <volume> 253, </volume> <pages> 530-535. </pages>
Reference-contexts: Our modelling of the U-shaped learning effect is based to a large degree on the Blocking and Retrieval Failure hypothesis about the mechanism of overregularization advanced by Marcus et al. (1993). This hypothesis is in turn based on the Blocking hypothesis <ref> (Pinker, 1984) </ref> which we discuss in detail in Section 5.
Reference: <author> Pinker, S. & Prince, A. </author> <year> (1988). </year> <title> On language and connectionism: Analysis of a parallel distributed processing model of language acquisition. </title> <editor> In S. Pinker & J. Mehler, (Eds.), </editor> <title> Connections 51 and Symbols (pp. </title> <address> 73-193). Cambridge, </address> <publisher> MA : MIT Press. </publisher>
Reference-contexts: 1 Introduction The current controversy between the supporters of the symbolic and the connectionist approaches in cognitive science has been focused on two main areas of disagreement, roughly following a split inside the connectionist camp between implementational connectionism and eliminative connectionism <ref> (Pinker and Prince, 1988, pp. 76-77) </ref>. <p> In contrast, eliminative connectionism is a much more radical doctrine. This is the view that "once PDP models are fully developed, they will replace symbol-processing models as explanations of cognitive processes" <ref> (Pinker and Prince, 1988, p. 77) </ref>. <p> Rumelhart and McClelland assume that the acquisition process establishes a direct mapping from the phonetic representation of the stem to the phonetic representation of the past tense form. <ref> (Pinker and Prince, 1988, pp. 87-88) </ref> At least two crucial steps are still required to go from the phonetic strings to lexical items and then to morphological categories like "present tense" and "past tense". <p> it: [one of the inherent deficits of the model is that] there is no such thing as a variable for any stem, regardless of its phonetic composition, and hence no way for the model to attain the knowledge that you could add /d/ to a "stem" to get its past <ref> (Pinker and Prince, 1988, p. 124) </ref>. Representational Format. The representational format used by Rumelhart and McClelland in their model was also subjected to severe criticism. <p> Another criticism is that this format cannot encode all possible words unambiguously. For example, the model would be unable to distinguish between the words algal and algalgal in the Australian language Oykangand. This means that with this format we cannot represent strings of arbitrary lengths <ref> (Pinker and Prince, 1988, p. 97) </ref>. Generalization Power. On purely conceptual grounds we are not convinced that Rumelhart and McClelland have provided an adequate model of the learning of the past tenses of English verbs because of the inductive limitations built in their model. <p> As Pinker and Prince pointed out "This suggests that the reason for the model's muteness is that it failed to learn the relevant transformations; i.e. to generalize appropriately about the regular past." <ref> (Pinker and Prince, 1988, p. 124) </ref> Even more difficult to explain is a number of "strange" mistakes: e.g. squat 7! squakt, mail 7! membled, tour 7! toureder, mate 7! maded, brown 7! brawned, shape 7! shipt, and sip 7! sept.
Reference: <author> Pinker, S. and Prince, A. </author> <year> (1991). </year> <title> Regular and irregular morphology and the psychological status of rules of grammar. </title> <editor> In L. Sutton (Ed.) </editor> <booktitle> Proceedings of the 1991 Meeting of the Berkeley Linguistic Society. </booktitle> <institution> Berkeley, California: Berkeley Linguistics Society. </institution>
Reference-contexts: Such errors occur with non-zero probability in young children and some English dialects actually use this past tense form. <ref> (Cf. Pinker and Prince, 1991) </ref>. But when a child learns that the correct transformation is bring 7! brought, he/she has to accommodate this exception among the existing templates. The SPA can very easily model this process. <p> There is a growing body of psychological and neurophysiological evidence that supports the existence of such functionally distinct (sub)modules for the processing of regular and irregular forms. <ref> (Cf. Pinker and Prince, 1991) </ref>. The existence of precedence-ordered (sub)modules of propositional-level templates and first-order rules can provide an even more plausible mechanism for implementing the Blocking Principle. (Cf. Kiparsky, 1982; Aronoff, 1976; Pinker and Prince, 1991, Marcus et al., 1993). <p> One problem with our model might be in accounting for the real-time implementation of blocking <ref> (Cf. Pinker and Prince, 1991) </ref>.
Reference: <author> Plotkin, G. </author> <year> (1970). </year> <title> A note on inductive generalizations. </title> <editor> In B. Meltzer & D. Michie, (Eds.), </editor> <booktitle> Machine Intelligence, </booktitle> <volume> Vol. </volume> <pages> 5, </pages> <address> (pp.153-63). New York: </address> <publisher> North Holland. </publisher>
Reference-contexts: We do not claim that exactly the same inductive steps 39 are taken by human learners, however, the rules resulting from the generalizations carried out algorithmically using the following two strategies seem to be psychologically realistic. The "Horizontal" Compression Strategy Applying the LGG algorithm (least general generalization) <ref> (Cf. Plotkin, 1970) </ref> on the indexes of the attributes produces one rule that is the least general generalization of two adjacent regular rules for one and the same final phoneme.
Reference: <author> Plunkett, K. & Marchman, V. </author> <year> (1991). </year> <title> U-shaped learning and frequency effects in a multilayered perceptron: Implications for child language acquisition. </title> <journal> Cognition, </journal> <volume> 38, </volume> <pages> 43-102. </pages>
Reference-contexts: The idea that children learn first high frequency verbs and then medium and lower frequency verbs is not unreasonable. However, this training procedure is not entirely psychologically realistic. Children never experience such sharp discontinuities in vocabulary size and content <ref> (cf. Plunkett and Marchman, 1991, p.47) </ref>.
Reference: <author> Pollack, J. </author> <year> (1991). </year> <title> Recursive distributed representations. In G.E. </title> <editor> Hinton, (Ed.), </editor> <booktitle> Connectionist Symbol Processing (pp. </booktitle> <pages> 77-106). </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Prasada, S. and Pinker, S. </author> <year> (1993). </year> <title> Generalization of regular and irregular morphological patterns. </title> <booktitle> Language and Cognitive Processes. </booktitle> <volume> 8(1), </volume> <pages> 1-56. </pages>
Reference-contexts: One possible solution to this "genuine novel verbs problem" has been offered by Prasada and Pinker who designed a special pseudo-verb testing paradigm <ref> (Prasada and Pinker, 1993) </ref>. Prasada and Pinker created 60 English-like pseudo-verbs. The verbs are divided into two main groups - irregular-like and regular-like pseudo-verbs. Each of these groups are subdivided into three classes Prototypical, Intermediate, and Distant.
Reference: <author> Quinlan, R. </author> <year> (1986a). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> Vol. 1, </volume> <pages> pp. 81-106. </pages>
Reference-contexts: One of the most widely studied of these systems is ID3 <ref> (Quinlan, 1986a) </ref>. The ID3 learning system has been designed by Quinlan (1986a). C4.5 is a latter improved implementation of ID3 (Quinlan, 1986b, 1989).
Reference: <author> Quinlan, R. </author> <year> (1986b). </year> <title> Probabilistic decision trees. </title> <editor> In Y. Kodratoff, & R. Michalski, (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. 3, </volume> <pages> (pp. 140-152). </pages> <address> San Mateo, CA, USA: </address> <publisher> Morgan Kaufmann, Inc. </publisher>
Reference-contexts: One of the most widely studied of these systems is ID3 (Quinlan, 1986a). The ID3 learning system has been designed by Quinlan (1986a). C4.5 is a latter improved implementation of ID3 <ref> (Quinlan, 1986b, 1989) </ref>. ID3 is a descendant from Hunt, Marin and Stone's CLS (Hunt, Marin, and Stone, 1966) and falls into the class of TDIDT (top-down induction of decision trees) learning systems.
Reference: <author> Quinlan, R. </author> <year> (1989). </year> <title> Unknown attribute values in induction. </title> <editor> In B. Spatz, (Ed.), </editor> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> San Mateo, CA, USA: </address> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <pages> pp. 164-68. </pages>
Reference: <author> Ripley, B.D. </author> <year> (1992). </year> <title> Statistical aspects of neural networks. In invited talk at SemSat, </title> <address> Sandjerg, Denmark, April 1992. </address> <publisher> Chapman and Hall. </publisher>
Reference: <author> Rumelhart, D. & McClelland, J. </author> <year> (1986). </year> <title> On learning the past tenses of English verbs. </title> <editor> In Rumel-hart, D., McClelland, J. </editor> & <booktitle> the PDP Research Group (Eds.), Parallel Distributed Processing, </booktitle> <volume> Vol. </volume> <pages> 2 (pp. 216-271). </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Eliminative connectionism has received a great deal of attention largely due to a famous PDP model developed by Rumelhart and McClelland that was claimed to be capable of learning the past tenses of English verbs without the use of explicit rules and symbolic representations <ref> (Rumelhart and McClelland, 1986, pp. 216-271) </ref>. After several critical reviews of this model by Pinker and Prince (1988), Lachter and Bever (1988) and a detailed background analysis of the model by Plunkett and Marchman (1991), a new revised PDP model was proposed by MacWhinney and Leinbach (1991) (M&L henceforth). <p> The child need not decide whether a verb is regular or irregular. There is no question as to whether the inflected form should be stored directly in the lexicon or derived from more general principles <ref> (Rumelhart and McClelland, 1986, p. 267) </ref>. 2.3 Criticism of Rumelhart and McClelland's Model Rumelhart and McClelland's model has received extensive critical attention in the literature. Two of the best known critical reviews were written by Pinker and Prince (1988), and Lachter and Bever (1988). <p> match even the modest results achieved by Rumelhart and McClelland's connectionist model, eliminative connectionists could dismiss many of the important theoretical objections levied against them by typically claiming that "PDP models may provide more accurate accounts of human performance than models based on a set of rules representing human competence." <ref> (McClel-land, Rumelhart, and Hinton, 1986, pp. 24-25) </ref> In this paper we show that with respect to the learning of the past tenses of English verbs and in many other learning tasks, this need no longer be the case. 2.4 MacWhinney and Leinbach's Model In a paper recently published in this journal
Reference: <author> Rumelhart, D. & McClelland, J. </author> <year> (1987). </year> <title> Learning the past tenses of English verbs: Implicit rules or parallel distributed processing? In B. MacWhinney (Ed.), Mechanisms of Language Acquisition. </title> <address> Hillsdale, NJ: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: of such networks may be describable (at least approximately) as conforming to some system of rules, we suggest that an account of the fine structure of the phenomena of language use and language acquisition can best be formulated in models that make reference to the characteristics of the underlying networks. <ref> (Rumelhart and McClelland, 1987, p. 196) </ref> We will not describe in any great detail Rumelhart and McClelland model since this model is one of the best known connectionist simulations. We will only note that at the heart of their model is a simple perceptron based pattern associator. <p> The child would therefore have to generate the appropriate present tense form internally with the aid of the entire sentence and context, and this, we suppose, requires that the child already know the present tense of the word. <ref> (Rumelhart and McClelland, 1987, p. 222) </ref>. (Note: by present tense verbs R&M actually mean verb stems).
Reference: <author> Sejnowski, T. & Rosenberg, C. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 145-168. </pages>
Reference-contexts: Shavlik, Mooney and Towell (1991) carried out a thorough experimental comparison between ID3, ANNs using BP and the perceptron on five real-world data sets including the famous connectionist text-to-speech conversion model NETtalk <ref> (Sejnowski and Rosenberg, 1987) </ref>. In the StatLog project (Feng et al., 1992), 18 different symbolic, statistical and neural network learning algorithms were critically (and more or less fairly) compared over 6 large data sets from industrial applications.
Reference: <author> Shavlik, J., Mooney, R., & Towell, G. </author> <year> (1991). </year> <title> Symbolic and neural learning algorithms: An experimental comparison. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 111-143. </pages>
Reference: <author> Slobin, D.I. </author> <year> (1971). </year> <title> On the learning of morphological rules: A reply to Palermo and Eber-hart. </title> <editor> In D.I. Slobin (Ed.), </editor> <booktitle> The ontogenesis of grammar: A theoretical symposium. </booktitle> <address> New York: </address> <publisher> 52 Academic Press. </publisher>
Reference-contexts: (1993) for a state of the art discussion.) Previously, it has been argued that the phenomenon of overregular-ization lends direct support to the symbolic processing approach to cognition in that cognitive processes must be explained in terms of rules and symbolic representations rather than in terms of rote and reinforcement <ref> (Slobin, 1971) </ref>. However, Rumelhart and McClelland claimed that they can model this U-shaped developmental curve with their connectionist pattern associator without the need to postulate the existence of rules and symbolic representations.
Reference: <author> Smolensky, P. </author> <year> (1988). </year> <title> On the proper treatment of connectionism. </title> <journal> Behavioral and Brain Sciences, </journal> <volume> 11, </volume> <pages> 1-74. </pages>
Reference-contexts: At the same time, the supporters of this approach believe that the resources of connectionist archi 1 We use the qualifier "artificial" following the majority of connectionist researchers who do not insist on the neurophysiological adequacy of their models <ref> (Cf. Smolensky, 1988) </ref>. 1 tectures will be sufficient to explain in principle all cognitive phenomena without the need to postulate the existence of rules and symbolic representations. <p> It is also quite common to read that symbolic systems are hard and brittle while connectionists systems are soft and flexible and can account for a wider range of cognitive phenomena <ref> (Smolensky, 1988) </ref>.
Reference: <author> Smolensky, P. </author> <year> (1991a). </year> <title> Connectionism, </title> <booktitle> constituency, and the language of thought. </booktitle> <editor> In B. Loewer & G. Ray, (Eds.), Meaning and Mind: Fodor and His Critics (pp.201-228). </editor> <address> Cambridge, MA: </address> <publisher> Basil Blackwell. </publisher>
Reference: <author> Smolensky, P. </author> <year> (1991b). </year> <title> Tensor product variable binding and the representation of symbolic structures in connectionist systems. In G.E. </title> <editor> Hinton (Ed.), </editor> <booktitle> Connectionist Symbolic Processing (pp. </booktitle> <pages> 159-216). </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <institution> Thrun, S.B. at el. </institution> <year> (1991). </year> <title> The MONK's problems a performance comparison of different learning algorithms. </title> <type> Technical Report, </type> <institution> Computer Science Department, Carnegie Mellon University. </institution>
Reference-contexts: comparisons involving a few learning algorithms and/or a few selected data sets have been done (Tsaptsinos et al., 1990; Gorman and Sejnowski, 1988; Thrun et al., 1991; Atlas et al. 1990), but the results were inconclusive since the data sets were too small and selective, or little analysis was done <ref> (Thrun et al., 1991) </ref>. Several more extensive studies have been published, such as (Weiss and Kulikowski, 1991; Fisher and McKusick, 1989; Shavlik, Mooney, and Towell, 1991; Feng et al., 1992; Ripley, 1992).
Reference: <author> Touretzky, D. </author> <year> (1991). </year> <title> BoltzCONS: Dynamic symbol structures in a connectionist network. In G.E. </title> <editor> Hinton (Ed.), </editor> <booktitle> Connectionist Symbolic Processing (pp. </booktitle> <pages> 5-46). </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Tsaptsinos, D., Mirzai, A., & Jervis, B. </author> <year> (1990). </year> <title> Comparison of machine learning paradigms in a classification task. </title> <editor> In G. Rzevsk, editor, </editor> <booktitle> Applications of artificial intelligence in engineering V: Proceedings of the fifth international conference. </booktitle> <publisher> Springer-Verlag. </publisher>
Reference: <author> Weiss, </author> <title> S.M. & Kulikowski, </title> <address> C.A. </address> <year> (1991). </year> <title> Computer Systems that Learn: classification and prediction methods from statistics, neural networks, machine learning, and expert systems. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <month> 53 </month>
References-found: 46

