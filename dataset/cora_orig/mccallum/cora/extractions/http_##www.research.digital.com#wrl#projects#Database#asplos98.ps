URL: http://www.research.digital.com/wrl/projects/Database/asplos98.ps
Refering-URL: http://www.research.digital.com/wrl/projects/Database/index.html
Root-URL: http://www.research.digital.com
Email: fparthas,saritag@rice.edu fbarroso,kouroshg@pa.dec.com  
Title: Performance of Database Workloads on Shared-Memory Systems with Out-of-Order Processors  
Author: Parthasarathy Ranganathan Kourosh Gharachorloo Sarita V. Adve and Luiz Andr e Barroso 
Affiliation: Electrical and Computer Engineering Western Research Laboratory Rice University Compaq Computer Corporation  
Date: October 1998.  
Note: To Appear in the Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS),  
Abstract: This paper examines the behavior of database workloads on shared-memory multiprocessors with aggressive out-of-order processors, and considers simple optimizations that can provide further performance improvements. Our study is based on detailed simulations of the Oracle commercial database engine. The results show that the combination of out-of-order execution and multiple instruction issue is indeed effective in improving performance of database workloads, providing gains of 1.5 and 2.6 times over an in-order single-issue processor for OLTP and DSS, respectively. In addition, speculative techniques enable optimized implementations of memory consistency models that significantly improve the performance of stricter consistency models, bringing the performance to within 10-15% of the performance of more relaxed models. The second part of our study focuses on the more challenging OLTP workload. We show that an instruction stream buffer is effective in reducing the remaining instruction stalls in OLTP, providing a 17% reduction in execution time (approaching a perfect instruction cache to within 15%). Furthermore, our characterization shows that a large fraction of the data communication misses in OLTP exhibit migratory behavior; our preliminary results show that software prefetch and writeback/flush hints can be used for this data to further reduce execution time by 12%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Abdel-Shafi, J. Hall, S. V. Adve, and V. S. Adve. </author> <title> An Evaluation of Fine-Grain Producer-Initiated Communication in Cache-Coherent Multiprocessors. </title> <booktitle> In Proceedings of the 3rd International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 204215, </pages> <month> Febru-ary </month> <year> 1997. </year>
Reference-contexts: Support for such software-directed prefetch instructions already exists in most current processors. Second, a solution that identifies the end of a sequence of accesses to migratory data can schedule a flush or WriteThrough <ref> [1] </ref> instruction to update the memory with the latest values. Subsequent read requests can then be serviced at memory, avoiding the extra latency associated with a cache-to-cache transfer (for our system configuration, this can reduce the latency by almost 40%). <p> Primitives similar to the flush primitive that we use in Section 4 have been proposed and studied by a number of other groups (e.g., [9, 22]). Our flush primitive is modeled after the WriteThrough primitive used by Abdel-Shafi et al. <ref> [1] </ref>. That study also showed that the combination of prefetching and WriteThrough could be used to achieve better performance improvements than using either of them alone (in the context of scientific applications). Our study is based on systems with conventional cache hier-archies.
Reference: [2] <author> L. A. Barroso, K. Gharachorloo, and E. D. Bugnion. </author> <title> Memory System Characterization of Commercial Workloads. </title> <booktitle> In Proceedings of the 25th International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: While the behavior of DSS workloads has been shown to be somewhat reminiscent of scientific/engineering applications <ref> [2, 28] </ref>, a number of recent studies have underscored the radically different behavior of OLTP work-loads [2, 4, 5, 11, 14, 20, 21]. <p> While the behavior of DSS workloads has been shown to be somewhat reminiscent of scientific/engineering applications [2, 28], a number of recent studies have underscored the radically different behavior of OLTP work-loads <ref> [2, 4, 5, 11, 14, 20, 21] </ref>. In general, OLTP workloads lead to inefficient executions with a large memory stall component and present a more challenging set of requirements for processor and memory system design. <p> In general, OLTP workloads lead to inefficient executions with a large memory stall component and present a more challenging set of requirements for processor and memory system design. This behavior arises from large instruction and data footprints and high communication miss rates that are characteristic for such workloads <ref> [2] </ref>. The dramatic change in the target market for shared-memory servers has yet to be fully reflected in the design of these systems. <p> In contrast, most previous studies of aggressive out-of-order processors in shared-memory systems have focused on scientific and engineering applications. Similarly, architectural studies of database workloads have been mostly based on simple in-order processor models <ref> [2, 5, 28] </ref>. To investigate the behavior of databases, we have instrumented and studied the Oracle commercial database engine (version 7.3.2) running on Alpha processors under Digital Unix. We use traces of OLTP and DSS workloads running on Oracle to drive a highly detailed trace-driven multiprocessor simulator. <p> First, TPC-B has much simpler setup requirements than TPC-C, and therefore lends itself better for experimentation through simulation. Second, our performance monitoring experiments with TPC-B and TPC-C show similar processor and memory system behavior, with TPC-B exhibiting somewhat worse memory system behavior than TPC-C <ref> [2] </ref>. As a result, we expect that changes in processor and memory system features to affect both benchmarks in similar ways. <p> Query 6 scans the largest table in the database to assess the increase in revenue that would have resulted if some discounts were eliminated. The behavior of this query is representative of other TPC-D queries <ref> [2] </ref>. For DSS, we used Oracle with the Parallel Query Optimization option, which allows the database engine to decompose the query into multiple sub-tasks and assign each one to an Oracle server process. <p> The total number of instructions simulated was approximately 200 million for both OLTP and DSS. Warmup transients were ignored in the statistics collection for both the workloads. 2.3 Scaling and Validation We followed the recommendations of Barroso et al. <ref> [2] </ref> in scaling our workloads to enable tracing and simulation. Specifically, we carefully scaled down our database and block buffer sizes while continuing to use the same number of processes per processor as a full-sized database. <p> Since our methodology uses user-level traces, we do not take into account the non-negligible operating system overheads of OLTP. However, as reported in Barroso et al. <ref> [2] </ref>, the execution behavior of Digital Unix running this OLTP workload is very similar to the user-level behavior of the application, including CPI, cache miss ratios, and contributions of different types of misses. <p> We also verified our statistics with those reported in <ref> [2] </ref> and [13] for similar configurations. 2.4 Simulated Architecture We use RSIM to simulate a hardware cache-coherent non-uniform memory access (CC-NUMA) shared-memory multiprocessor system using an invalidation-based, four-state MESI directory coherence protocol. Due to constraints of simulation time, we only model a system with four nodes. <p> The local miss rates for DSS are 0.0% and 0.9% for the first-level instruction and data caches and 23.1% for the second level cache. These observations are consistent with those reported in previous studies <ref> [2, 13] </ref>. The results further indicate that support for multiple issue, out-of-order execution, and multiple outstanding loads provide significant benefits for OLTP and DSS, even though the benefits for OLTP are smaller in comparison. <p> There are a number of studies based on the performance of out-of-order processors for non-database workloads (e.g., [8, 16, 18]). Most previous studies of databases are based on in-order processors <ref> [2, 4, 5, 6, 14, 20, 27, 28] </ref>, and therefore do not address the benefits of more aggressive processor architectures. A number of the studies are limited to uniprocessor systems [4, 6, 13, 14]. <p> Another important distinction among the database studies is whether they are based on monitoring existing systems [4, 5, 11, 27] (typically through performance counters) or based on simulations [6, 13, 14, 20, 21, 28]; one study uses a combination of both techniques <ref> [2] </ref>. Monitoring studies have the advantage of using larger scale versions of the workloads and allowing for a larger number of experiments to be run.
Reference: [3] <author> A. L. Cox and R. J. Fowler. </author> <title> Adaptive cache coherency for detecting migratory shared data. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 98108, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The above observations suggest two possible solutions for reducing the performance loss due to migratory dirty read misses. First, a software solution that identifies accesses to migratory data struc 2 We use the following heuristic to identify migratory data <ref> [3, 25] </ref>. A cache line is marked as migratory when the directory receives a request for exclusive ownership to a line, the number of cached copies of the line is 2, and the last writer to the line is not the requester.
Reference: [4] <author> Z. Cventanovic and D. Bhandarkar. </author> <title> Performance characterization of the Alpha 21164 microprocessor using TP and SPECworkloads. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 6070, </pages> <month> Apr </month> <year> 1994. </year>
Reference-contexts: While the behavior of DSS workloads has been shown to be somewhat reminiscent of scientific/engineering applications [2, 28], a number of recent studies have underscored the radically different behavior of OLTP work-loads <ref> [2, 4, 5, 11, 14, 20, 21] </ref>. In general, OLTP workloads lead to inefficient executions with a large memory stall component and present a more challenging set of requirements for processor and memory system design. <p> There are a number of studies based on the performance of out-of-order processors for non-database workloads (e.g., [8, 16, 18]). Most previous studies of databases are based on in-order processors <ref> [2, 4, 5, 6, 14, 20, 27, 28] </ref>, and therefore do not address the benefits of more aggressive processor architectures. A number of the studies are limited to uniprocessor systems [4, 6, 13, 14]. <p> Most previous studies of databases are based on in-order processors [2, 4, 5, 6, 14, 20, 27, 28], and therefore do not address the benefits of more aggressive processor architectures. A number of the studies are limited to uniprocessor systems <ref> [4, 6, 13, 14] </ref>. <p> Another important distinction among the database studies is whether they are based on monitoring existing systems <ref> [4, 5, 11, 27] </ref> (typically through performance counters) or based on simulations [6, 13, 14, 20, 21, 28]; one study uses a combination of both techniques [2].
Reference: [5] <author> Z. Cvetanovic and D. D. Donaldson. </author> <title> AlphaServer 4100 performance characterization. </title> <journal> Digital Technical Journal, </journal> <volume> 8(4):320, </volume> <year> 1996. </year>
Reference-contexts: While the behavior of DSS workloads has been shown to be somewhat reminiscent of scientific/engineering applications [2, 28], a number of recent studies have underscored the radically different behavior of OLTP work-loads <ref> [2, 4, 5, 11, 14, 20, 21] </ref>. In general, OLTP workloads lead to inefficient executions with a large memory stall component and present a more challenging set of requirements for processor and memory system design. <p> In contrast, most previous studies of aggressive out-of-order processors in shared-memory systems have focused on scientific and engineering applications. Similarly, architectural studies of database workloads have been mostly based on simple in-order processor models <ref> [2, 5, 28] </ref>. To investigate the behavior of databases, we have instrumented and studied the Oracle commercial database engine (version 7.3.2) running on Alpha processors under Digital Unix. We use traces of OLTP and DSS workloads running on Oracle to drive a highly detailed trace-driven multiprocessor simulator. <p> There are a number of studies based on the performance of out-of-order processors for non-database workloads (e.g., [8, 16, 18]). Most previous studies of databases are based on in-order processors <ref> [2, 4, 5, 6, 14, 20, 27, 28] </ref>, and therefore do not address the benefits of more aggressive processor architectures. A number of the studies are limited to uniprocessor systems [4, 6, 13, 14]. <p> Another important distinction among the database studies is whether they are based on monitoring existing systems <ref> [4, 5, 11, 27] </ref> (typically through performance counters) or based on simulations [6, 13, 14, 20, 21, 28]; one study uses a combination of both techniques [2].
Reference: [6] <author> R. J. Eickemeyer, R. E. Johnson, S. R. Kunkel, M. S. Squillante, and S. Liu. </author> <title> Evaluation of multithreaded uniprocessors for commercial application environments. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 203212, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: There are a number of studies based on the performance of out-of-order processors for non-database workloads (e.g., [8, 16, 18]). Most previous studies of databases are based on in-order processors <ref> [2, 4, 5, 6, 14, 20, 27, 28] </ref>, and therefore do not address the benefits of more aggressive processor architectures. A number of the studies are limited to uniprocessor systems [4, 6, 13, 14]. <p> Most previous studies of databases are based on in-order processors [2, 4, 5, 6, 14, 20, 27, 28], and therefore do not address the benefits of more aggressive processor architectures. A number of the studies are limited to uniprocessor systems <ref> [4, 6, 13, 14] </ref>. <p> Another important distinction among the database studies is whether they are based on monitoring existing systems [4, 5, 11, 27] (typically through performance counters) or based on simulations <ref> [6, 13, 14, 20, 21, 28] </ref>; one study uses a combination of both techniques [2]. Monitoring studies have the advantage of using larger scale versions of the workloads and allowing for a larger number of experiments to be run. <p> Finally, our study evaluates the benefits of exploiting intra-thread parallelism in database workloads. Two previous studies have focused on exploiting inter-thread parallelism through the use of multithreaded processors <ref> [6, 13] </ref>. This approach depends on the fact that database workloads are already inherently parallel (either in the form of threads or processes) for hiding I/O latency.
Reference: [7] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Two techniques to enhance the performance of memory consistency models. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages I:355364, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Given the range of memory consistency models supported by commercial multiprocessors (sequential consistency for SGI, processor consistency-like for Intel and Sun, and Alpha or PowerPC consistency for Digital and IBM), we were also interested in evaluating the effectiveness of speculative techniques that can be used in conjunction with out-of-order processors <ref> [7] </ref>. Our results show that these techniques can reduce the execution time of OLTP and DSS in sequentially consistent systems by 26-37%, bringing the performance to within 10-15% of systems with more relaxed models (e.g., Alpha consistency). <p> These optimized implementations exploit the observation that the system must only appear to execute memory operations according to the specified constraints of a model. The technique of hardware prefetching from the instruction window <ref> [7] </ref> issues non-binding prefetches for memory operations whose addresses are known, and yet are blocked due to consistency constraints. Speculative load execution [7] increases the benefits of prefetching by allowing the return value of the load to be consumed early, regardless of consistency constraints. <p> The technique of hardware prefetching from the instruction window <ref> [7] </ref> issues non-binding prefetches for memory operations whose addresses are known, and yet are blocked due to consistency constraints. Speculative load execution [7] increases the benefits of prefetching by allowing the return value of the load to be consumed early, regardless of consistency constraints. The latter technique requires hardware support for detecting violations of ordering requirements due to early consumption of values and for recovering from such violations.
Reference: [8] <author> K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> Hiding memory latency using dynamic scheduling in shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2233, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Our work is also the first to study the performance of memory consistency models in the context of database workloads. There are a number of studies based on the performance of out-of-order processors for non-database workloads (e.g., <ref> [8, 16, 18] </ref>). Most previous studies of databases are based on in-order processors [2, 4, 5, 6, 14, 20, 27, 28], and therefore do not address the benefits of more aggressive processor architectures. A number of the studies are limited to uniprocessor systems [4, 6, 13, 14].
Reference: [9] <author> M. D. Hill, J. R. Larus, S. K. Reinhardt, and D. A. Wood. </author> <title> Cooperative shared memory: Software and hardware support for scalable multiprocessors. </title> <journal> TOCS, </journal> <volume> 11(4):300318, </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: Primitives similar to the flush primitive that we use in Section 4 have been proposed and studied by a number of other groups (e.g., <ref> [9, 22] </ref>). Our flush primitive is modeled after the WriteThrough primitive used by Abdel-Shafi et al. [1]. That study also showed that the combination of prefetching and WriteThrough could be used to achieve better performance improvements than using either of them alone (in the context of scientific applications).
Reference: [10] <author> N. P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 364373, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: These characteristics suggest potential benefits from adding a simple instruction stream buffer between the first and second level caches. A stream buffer is a simple hardware-based prefetching mechanism that automatically initiates prefetches to successive cache lines following a miss <ref> [10] </ref>. To avoid cache pollution due to the prefetched lines, the hardware stores the prefetched requests in the stream buffer until the cache uses the prefetched data. <p> Even though the SMT functionality is added on top of an aggressive out-of-order processor, the study primarily focuses on the effects of SMT as opposed to the underlying mechanisms in the base processor. We are not aware of any work other than the original stream buffer work by Jouppi <ref> [10] </ref> that has studied the effect of stream buffers to alleviate the instruction miss bottleneck. This may partly be due to the fact that the standard SPEC, SPLASH, and STREAM benchmark suites do not stress the instruction cache significantly. Our evaluation differs from Jouppi's [10] in two key respects. <p> original stream buffer work by Jouppi <ref> [10] </ref> that has studied the effect of stream buffers to alleviate the instruction miss bottleneck. This may partly be due to the fact that the standard SPEC, SPLASH, and STREAM benchmark suites do not stress the instruction cache significantly. Our evaluation differs from Jouppi's [10] in two key respects. First, we evaluate the impact of the stream buffer optimization on the execution time of a complex commercially-used database engine.
Reference: [11] <author> K. Keeton, D. A. Patterson, Y. Q. He, R. C. Raphael, and W. E. Baker. </author> <title> Performance Characterization of the Quad Pentium Pro SMP Using OLTP Workloads. </title> <booktitle> In Proceedings of the 25th International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: While the behavior of DSS workloads has been shown to be somewhat reminiscent of scientific/engineering applications [2, 28], a number of recent studies have underscored the radically different behavior of OLTP work-loads <ref> [2, 4, 5, 11, 14, 20, 21] </ref>. In general, OLTP workloads lead to inefficient executions with a large memory stall component and present a more challenging set of requirements for processor and memory system design. <p> Another important distinction among the database studies is whether they are based on monitoring existing systems <ref> [4, 5, 11, 27] </ref> (typically through performance counters) or based on simulations [6, 13, 14, 20, 21, 28]; one study uses a combination of both techniques [2]. <p> Nevertheless, we found monitoring extremely important for tuning, scaling, and tracing our workloads and for verifying the results of our simulations. The following describes the database studies based on out-of-order processors in more detail. Keeton et al. <ref> [11] </ref> present a monitoring study of the behavior of an OLTP workload (modeled after TPC-C) on top of Informix, using the performance counters on a quad Pentium Pro system. Similar to our study, this paper shows that out-of-order execution can achieve performance benefits on OLTP.
Reference: [12] <author> D. Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In Proceedings of the 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 8185, </pages> <year> 1981. </year>
Reference-contexts: The L1 data cache is dual-ported, and uses a write-allocate, write-back policy. The unified L2 cache is a fully pipelined, write-allocate write-back cache. In addition, all caches are non-blocking and allow up to 8 outstanding requests to separate cache lines. At each cache, miss status holding registers (MSHRs) <ref> [12] </ref> store information about the misses and coalesce multiple requests to the same cache line. All caches are physically addressed and physically tagged. The virtual memory system uses a bin-hopping page mapping policy with 8K page sizes, and includes separate 128-element fully associative data and instruction TLBs.
Reference: [13] <author> J. L. Lo, L. A. Barroso, S. J. Eggers, K. Gharachorloo, H. M. Levy, and S. S. Parekh. </author> <title> An Analysis of Database Workload Performance on Simultaneous Multithreaded Processors. </title> <booktitle> In Proceedings of the 25th International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: This trace-driven simulation methodology is similar to that used by Lo et al. <ref> [13] </ref>. The traces were derived with a custom tool built using ATOM [23]. Only the Oracle server processes were traced since the many daemon processes have negligible CPU requirements. However, the behavior of the daemons with respect to synchroniz-ation and I/O operations was preserved in the traces. <p> The DSS experiments use an in-memory 500MB database. The number of processes per CPU was eight for OLTP and four for DSS. Similar configurations were used by Lo et al. <ref> [13] </ref>. In the past, transaction processing applications were reported to be mainly I/O bound and to have a dominant component of their execution time in the operating system. <p> We also verified our statistics with those reported in [2] and <ref> [13] </ref> for similar configurations. 2.4 Simulated Architecture We use RSIM to simulate a hardware cache-coherent non-uniform memory access (CC-NUMA) shared-memory multiprocessor system using an invalidation-based, four-state MESI directory coherence protocol. Due to constraints of simulation time, we only model a system with four nodes. <p> The local miss rates for DSS are 0.0% and 0.9% for the first-level instruction and data caches and 23.1% for the second level cache. These observations are consistent with those reported in previous studies <ref> [2, 13] </ref>. The results further indicate that support for multiple issue, out-of-order execution, and multiple outstanding loads provide significant benefits for OLTP and DSS, even though the benefits for OLTP are smaller in comparison. <p> Most previous studies of databases are based on in-order processors [2, 4, 5, 6, 14, 20, 27, 28], and therefore do not address the benefits of more aggressive processor architectures. A number of the studies are limited to uniprocessor systems <ref> [4, 6, 13, 14] </ref>. <p> Another important distinction among the database studies is whether they are based on monitoring existing systems [4, 5, 11, 27] (typically through performance counters) or based on simulations <ref> [6, 13, 14, 20, 21, 28] </ref>; one study uses a combination of both techniques [2]. Monitoring studies have the advantage of using larger scale versions of the workloads and allowing for a larger number of experiments to be run. <p> The paper does however make the observation that the decoupling of the fetch and execute units allows for some overlap of the instruction stall time in the uniprocessor system. Lo et al. <ref> [13] </ref> examine the performance of OLTP and DSS on a simultaneous multithreaded (SMT) uniprocessor system with support for multiple hardware contexts using the same simulation methodology as our study. <p> Finally, our study evaluates the benefits of exploiting intra-thread parallelism in database workloads. Two previous studies have focused on exploiting inter-thread parallelism through the use of multithreaded processors <ref> [6, 13] </ref>. This approach depends on the fact that database workloads are already inherently parallel (either in the form of threads or processes) for hiding I/O latency. <p> This approach depends on the fact that database workloads are already inherently parallel (either in the form of threads or processes) for hiding I/O latency. These studies show that multithreading can provide substantial gains, with simultaneous multithreading (SMT) <ref> [13] </ref> providing higher gains (as high as three times improvement for OLTP). Our study shows that DSS can benefit significantly from intra-thread parallelism (2.6 times improvement). The incremental gains from the addition of SMT are less significant in comparison [13]. <p> show that multithreading can provide substantial gains, with simultaneous multithreading (SMT) <ref> [13] </ref> providing higher gains (as high as three times improvement for OLTP). Our study shows that DSS can benefit significantly from intra-thread parallelism (2.6 times improvement). The incremental gains from the addition of SMT are less significant in comparison [13]. Intra-thread parallelism is not as beneficial for OLTP (1.5 times improvement) due to the data dependent nature of the workload.
Reference: [14] <author> A. M. G. Maynard, C. M. Donnelly, and B. R. Olszewski. </author> <title> Contrasting characteristics and cache performance of technical and multi-user commercial workloads. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 145156, </pages> <month> Oct </month> <year> 1994. </year>
Reference-contexts: While the behavior of DSS workloads has been shown to be somewhat reminiscent of scientific/engineering applications [2, 28], a number of recent studies have underscored the radically different behavior of OLTP work-loads <ref> [2, 4, 5, 11, 14, 20, 21] </ref>. In general, OLTP workloads lead to inefficient executions with a large memory stall component and present a more challenging set of requirements for processor and memory system design. <p> There are a number of studies based on the performance of out-of-order processors for non-database workloads (e.g., [8, 16, 18]). Most previous studies of databases are based on in-order processors <ref> [2, 4, 5, 6, 14, 20, 27, 28] </ref>, and therefore do not address the benefits of more aggressive processor architectures. A number of the studies are limited to uniprocessor systems [4, 6, 13, 14]. <p> Most previous studies of databases are based on in-order processors [2, 4, 5, 6, 14, 20, 27, 28], and therefore do not address the benefits of more aggressive processor architectures. A number of the studies are limited to uniprocessor systems <ref> [4, 6, 13, 14] </ref>. <p> Another important distinction among the database studies is whether they are based on monitoring existing systems [4, 5, 11, 27] (typically through performance counters) or based on simulations <ref> [6, 13, 14, 20, 21, 28] </ref>; one study uses a combination of both techniques [2]. Monitoring studies have the advantage of using larger scale versions of the workloads and allowing for a larger number of experiments to be run.
Reference: [15] <author> J. D. McCalpin. </author> <title> Memory bandwidth and machine balance in current high performance computers. </title> <booktitle> In IEEE Technical Committee on Computer Architecture Newsletter, </booktitle> <month> Dec </month> <year> 1995. </year>
Reference-contexts: Current processors have been primarily optimized to perform well on the SPEC benchmark suite [24], and system designs are focused on scientific and engineering benchmarks such as STREAMS <ref> [15] </ref> and SPLASH-2 [31]. One important outcome of this trend has been the emergence of aggressive out-of-order processors that exploit instruction-level parallelism (ILP) with ever-increasing design complexity.
Reference: [16] <author> B. A. Nayfeh, L. Hammond, and K. Olukotun. </author> <title> Evaluation of Design Alternatives for a Multiprocessor Micro processor. </title> <booktitle> In Proceedings of the 23rd International Symp. on Computer Architec ture, </booktitle> <pages> pages 6777, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Our work is also the first to study the performance of memory consistency models in the context of database workloads. There are a number of studies based on the performance of out-of-order processors for non-database workloads (e.g., <ref> [8, 16, 18] </ref>). Most previous studies of databases are based on in-order processors [2, 4, 5, 6, 14, 20, 27, 28], and therefore do not address the benefits of more aggressive processor architectures. A number of the studies are limited to uniprocessor systems [4, 6, 13, 14].
Reference: [17] <author> V. S. Pai, P. Ranganathan, and S. V. Adve. </author> <title> RSIM Reference Manual version 1.0. </title> <type> Technical Report 9705, </type> <institution> Department of Electrical and Computer e University, </institution> <month> August </month> <year> 1997. </year>
Reference-contexts: The queries were parallelized to generate four server processes per processor (16 processes in a 4-processor system). 2.2 Simulation Methodology We use the RSIM simulation infrastructure <ref> [17] </ref> to model multiprocessor systems with processors that exploit ILP techniques. Due to the difficulty of running a commercial-grade database engine on a user-level simulator (such as RSIM), our strategy was to use traces of the applications running on a four-processor AlphaServer4100, and drive the simulator with those traces.
Reference: [18] <author> V. S. Pai, P. Ranganathan, and S. V. Adve. </author> <title> The Impact of Instruction Level Parallelism on Multiprocessor Performance and Simulation Methodology. </title> <booktitle> In Proceedings of the 3rd International Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 7283, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: While these stalls are included for all memory categories, their impact is particularly visible in the L1+misc component because the base L1 latency is only one cycle. Similar conventions have been adopted by many previous studies (e.g., <ref> [18, 21] </ref>). Overall, our results show that the OLTP workload is characterized by a significant L2 component due to its large instruction and data footprint. In addition, there is a significant memory component arising from frequent data communication misses. <p> Our work is also the first to study the performance of memory consistency models in the context of database workloads. There are a number of studies based on the performance of out-of-order processors for non-database workloads (e.g., <ref> [8, 16, 18] </ref>). Most previous studies of databases are based on in-order processors [2, 4, 5, 6, 14, 20, 27, 28], and therefore do not address the benefits of more aggressive processor architectures. A number of the studies are limited to uniprocessor systems [4, 6, 13, 14].
Reference: [19] <author> V. S. Pai, P. Ranganathan, S. V. Adve, and T. Harton. </author> <title> An Evaluation of Memory Consistency Models for Shared-Memory Systems with ILP Processors. </title> <booktitle> In Proceedings of the 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 1223, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: In contrast, previous studies based on the same optimizations have shown a significant performance gap (greater than 15%) between SC and RC for scientific workloads <ref> [19] </ref>. 3.5 Summary of ILP Benefits Techniques such as multiple issue, out-of-order execution, and multiple outstanding loads provide significant benefits for both OLTP and DSS. The gains for DSS are more substantial as compared with OLTP.
Reference: [20] <author> S. E. Perl and R. L. </author> <title> Sites. Studies of windows NT performance using dynamic execution traces. </title> <booktitle> In Proceedings of the Second Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 169 184, </pages> <month> Oct. </month> <year> 1996. </year>
Reference-contexts: While the behavior of DSS workloads has been shown to be somewhat reminiscent of scientific/engineering applications [2, 28], a number of recent studies have underscored the radically different behavior of OLTP work-loads <ref> [2, 4, 5, 11, 14, 20, 21] </ref>. In general, OLTP workloads lead to inefficient executions with a large memory stall component and present a more challenging set of requirements for processor and memory system design. <p> There are a number of studies based on the performance of out-of-order processors for non-database workloads (e.g., [8, 16, 18]). Most previous studies of databases are based on in-order processors <ref> [2, 4, 5, 6, 14, 20, 27, 28] </ref>, and therefore do not address the benefits of more aggressive processor architectures. A number of the studies are limited to uniprocessor systems [4, 6, 13, 14]. <p> Another important distinction among the database studies is whether they are based on monitoring existing systems [4, 5, 11, 27] (typically through performance counters) or based on simulations <ref> [6, 13, 14, 20, 21, 28] </ref>; one study uses a combination of both techniques [2]. Monitoring studies have the advantage of using larger scale versions of the workloads and allowing for a larger number of experiments to be run.
Reference: [21] <author> M. Rosenblum, E. Bugnion, S. A. Herrod, E. Witchel, and A. Gupta. </author> <title> The impact of architectural trends on operating system performance. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 285298, </pages> <year> 1995. </year>
Reference-contexts: While the behavior of DSS workloads has been shown to be somewhat reminiscent of scientific/engineering applications [2, 28], a number of recent studies have underscored the radically different behavior of OLTP work-loads <ref> [2, 4, 5, 11, 14, 20, 21] </ref>. In general, OLTP workloads lead to inefficient executions with a large memory stall component and present a more challenging set of requirements for processor and memory system design. <p> While these stalls are included for all memory categories, their impact is particularly visible in the L1+misc component because the base L1 latency is only one cycle. Similar conventions have been adopted by many previous studies (e.g., <ref> [18, 21] </ref>). Overall, our results show that the OLTP workload is characterized by a significant L2 component due to its large instruction and data footprint. In addition, there is a significant memory component arising from frequent data communication misses. <p> Another important distinction among the database studies is whether they are based on monitoring existing systems [4, 5, 11, 27] (typically through performance counters) or based on simulations <ref> [6, 13, 14, 20, 21, 28] </ref>; one study uses a combination of both techniques [2]. Monitoring studies have the advantage of using larger scale versions of the workloads and allowing for a larger number of experiments to be run. <p> This enables us to quantitatively isolate the performance benefits from various ILP techniques as well as evaluate solutions to the various performance bottlenecks. We also evaluate the performance of DSS in addition to OLTP. Rosenblum et al. <ref> [21] </ref> present a simulation study that focuses on the impact of architectural trends on operating system performance using three workloads, one of which is TPC-B on Sybase. The study considers both uniprocessor and multiprocessor systems, with the multiprocessor study limited to in-order processors.
Reference: [22] <author> J. Skeppstedt and P. Stenstrom. </author> <title> A Compiler Algorithm that Reduces Read Latency in Ownership-Based Cache Coherence Protocols. </title> <booktitle> In International Conference on Parallel Architectures and Compilation Techniques, </booktitle> <year> 1995. </year>
Reference-contexts: Primitives similar to the flush primitive that we use in Section 4 have been proposed and studied by a number of other groups (e.g., <ref> [9, 22] </ref>). Our flush primitive is modeled after the WriteThrough primitive used by Abdel-Shafi et al. [1]. That study also showed that the combination of prefetching and WriteThrough could be used to achieve better performance improvements than using either of them alone (in the context of scientific applications).
Reference: [23] <author> A. Srivastava and A. Eustace. </author> <title> ATOM: A System for Building Customized Program Analys is Tools. </title> <booktitle> Proceedings of the ACM SIGPLAN `94 Conference on Programming Languages, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: This trace-driven simulation methodology is similar to that used by Lo et al. [13]. The traces were derived with a custom tool built using ATOM <ref> [23] </ref>. Only the Oracle server processes were traced since the many daemon processes have negligible CPU requirements. However, the behavior of the daemons with respect to synchroniz-ation and I/O operations was preserved in the traces.
Reference: [24] <author> Standard Performance Council. </author> <title> The SPEC95 CPU Benchmark Suite. </title> <note> http://www.specbench.org, 1995. </note>
Reference-contexts: The dramatic change in the target market for shared-memory servers has yet to be fully reflected in the design of these systems. Current processors have been primarily optimized to perform well on the SPEC benchmark suite <ref> [24] </ref>, and system designs are focused on scientific and engineering benchmarks such as STREAMS [15] and SPLASH-2 [31]. One important outcome of this trend has been the emergence of aggressive out-of-order processors that exploit instruction-level parallelism (ILP) with ever-increasing design complexity.
Reference: [25] <author> P. Stenstrom, M. Brorsson, and L. Sandberg. </author> <title> An adaptive cache coherence protocol optimized for migratory sharing. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 109118, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The above observations suggest two possible solutions for reducing the performance loss due to migratory dirty read misses. First, a software solution that identifies accesses to migratory data struc 2 We use the following heuristic to identify migratory data <ref> [3, 25] </ref>. A cache line is marked as migratory when the directory receives a request for exclusive ownership to a line, the number of cached copies of the line is 2, and the last writer to the line is not the requester. <p> Because our base system uses a relaxed memory consistency model, optimizations for dealing with migratory data such as those suggested by Stenstrom et al. <ref> [25] </ref> will not provide any gains since the write latency is already hidden. tures can schedule prefetches to the data, enabling the latency to be overlapped with other useful work. Support for such software-directed prefetch instructions already exists in most current processors.
Reference: [26] <author> T.-Y.Yeh and Y.N.Patt. </author> <title> Alternative Implementations of Two-level Adaptive Branch Prediction. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: Our base system models an out-of-order processor with support for multiple issue, out-of-order instruction execution, non-blocking loads, and speculative execution. We use an aggressive branch prediction scheme that consists of a hybrid pa/g branch predictor for the conditional branches <ref> [26] </ref>, a branch target buffer for the jump target branches, and a return address stack for the call-return branches.
Reference: [27] <author> S. S. Thakkar and M. Sweiger. </author> <title> Performance of an OLTP application on Symmetry multiprocessor system. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 228 238, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: There are a number of studies based on the performance of out-of-order processors for non-database workloads (e.g., [8, 16, 18]). Most previous studies of databases are based on in-order processors <ref> [2, 4, 5, 6, 14, 20, 27, 28] </ref>, and therefore do not address the benefits of more aggressive processor architectures. A number of the studies are limited to uniprocessor systems [4, 6, 13, 14]. <p> Another important distinction among the database studies is whether they are based on monitoring existing systems <ref> [4, 5, 11, 27] </ref> (typically through performance counters) or based on simulations [6, 13, 14, 20, 21, 28]; one study uses a combination of both techniques [2].
Reference: [28] <author> P. Trancoso, J.-L. Larriba-Pey, Z. Zhang, and J. Torrellas. </author> <title> The memory performance of DSS commercial workloads in shared-memory multiprocessors. </title> <booktitle> In Third International Symposium on High-Performance Computer Architecture, </booktitle> <month> Jan </month> <year> 1997. </year>
Reference-contexts: While the behavior of DSS workloads has been shown to be somewhat reminiscent of scientific/engineering applications <ref> [2, 28] </ref>, a number of recent studies have underscored the radically different behavior of OLTP work-loads [2, 4, 5, 11, 14, 20, 21]. <p> In contrast, most previous studies of aggressive out-of-order processors in shared-memory systems have focused on scientific and engineering applications. Similarly, architectural studies of database workloads have been mostly based on simple in-order processor models <ref> [2, 5, 28] </ref>. To investigate the behavior of databases, we have instrumented and studied the Oracle commercial database engine (version 7.3.2) running on Alpha processors under Digital Unix. We use traces of OLTP and DSS workloads running on Oracle to drive a highly detailed trace-driven multiprocessor simulator. <p> There are a number of studies based on the performance of out-of-order processors for non-database workloads (e.g., [8, 16, 18]). Most previous studies of databases are based on in-order processors <ref> [2, 4, 5, 6, 14, 20, 27, 28] </ref>, and therefore do not address the benefits of more aggressive processor architectures. A number of the studies are limited to uniprocessor systems [4, 6, 13, 14]. <p> Another important distinction among the database studies is whether they are based on monitoring existing systems [4, 5, 11, 27] (typically through performance counters) or based on simulations <ref> [6, 13, 14, 20, 21, 28] </ref>; one study uses a combination of both techniques [2]. Monitoring studies have the advantage of using larger scale versions of the workloads and allowing for a larger number of experiments to be run.
Reference: [29] <author> Transaction Processing Performance Council. </author> <title> TPC Benchmark B (Online Transaction Processing) Standard Specification, </title> <year> 1990. </year>
Reference-contexts: The metadata area is used to keep directory information for the block buffer, as well as for communication and synchronization between the various Oracle processes. 2.1.1 OLTP Workload Our OLTP application is modeled after the TPC-B benchmark from the Transaction Processing Performance Council (TPC) <ref> [29] </ref>. TPC-B models a banking database system that keeps track of customers' account balances, as well as balances per branch and teller.
Reference: [30] <author> Transaction Processing Performance Council. </author> <title> TPC Benchmark D (Decision Support) Standard Specification, </title> <month> Dec </month> <year> 1995. </year>
Reference-contexts: Finally, it is widely acknowledged that actual customer database applications will typically show poorer performance than TPC-C itself. 2.1.2 DSS Workload The DSS application is modeled after Query 6 of the TPC-D benchmark <ref> [30] </ref>. The TPC-D benchmark represents the activities of a business that sells a large number of products on a worldwide scale. It consists of several inter-related tables that keep information such as parts and customer orders.
Reference: [31] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22nd International Symposium on Computer Architecture, </booktitle> <pages> pages 2436, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Current processors have been primarily optimized to perform well on the SPEC benchmark suite [24], and system designs are focused on scientific and engineering benchmarks such as STREAMS [15] and SPLASH-2 <ref> [31] </ref>. One important outcome of this trend has been the emergence of aggressive out-of-order processors that exploit instruction-level parallelism (ILP) with ever-increasing design complexity.
References-found: 31

