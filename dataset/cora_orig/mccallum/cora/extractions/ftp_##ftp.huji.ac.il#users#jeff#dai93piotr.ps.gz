URL: ftp://ftp.huji.ac.il/users/jeff/dai93piotr.ps.gz
Refering-URL: http://www.cs.huji.ac.il/labs/dai/papers.html
Root-URL: 
Email: piotr@cs.huji.ac.il, jeff@cs.huji.ac.il  
Title: The Utility of Embedded Knowledge-Oriented Actions  
Author: Piotr Gmytrasiewicz Jeffrey S. Rosenschein 
Date: February 19, 1995  
Address: Givat Ram, Jerusalem, Israel  
Affiliation: Computer Science Department Hebrew University  
Abstract: We extend the decision-theoretic principle of maximization of expected utility, previously used to compute the utility of purely communicative acts, to include all actions that are aimed at changing the state of knowledge of the other agents involved. These actions are called knowledge- oriented actions (KOA's), and, in general, may contain nonverbal as well as verbal components. To compute the value of candidate KOA's we use the the transformation of knowledge resulting from the information being transmitted due to the knowledge-oriented action. Further, we account for the fact that, frequently, these acts are embedded in sequences of acts of similar kind. In our calculation of the expected utility of a KOA that could give rise to a further exchange of KOA's, called an embedded KOA, we consider the notion of a dialogue tree, which represents the possible sequences of the future KOA's that the particular KOA under consideration could lead to. The expected utility of any knowledge-oriented action can then be defined as a difference of the probabilistic mixture of utilities of the states at the leaves of the dialogue tree and the state before the KOA was performed.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ken Binmore. </author> <booktitle> Essays on Foundations of Game Theory. </booktitle> <publisher> Pitman, </publisher> <year> 1982. </year>
Reference-contexts: Primarily, equilibrium points signify stable solutions without motivating how those solutions are to be reached. See <ref> [1] </ref> for a discussion of the weaknesses of equilibrium as the solution concept in games, and [3] for a recent attempt at relating and reconciling the two solution concepts.
Reference: [2] <author> P. R. Cohen and H. J. Levesque. </author> <title> Rational interaction as the basis for communication. </title> <editor> In P. R. Cohen, J. Morgan, and M. E. Pollack, editors, </editor> <title> Intentions in Communication. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In this research, we are interested in rational agents that autonomously operate in a multiagent world without the benefit of predetermined protocols (see also <ref> [2, 5, 4] </ref>). 1 Our basic framework for evaluating when and what KOA should be performed is as follows: rational agents are to use knowledge-oriented actions that are most valuable to them in the particular situation, or technically speaking, ones that have the highest expected utility.
Reference: [3] <author> Edmund H. Durfee, Jaeho Lee, and Piotr Gmytrasieiwcz. </author> <title> Overeager rationality and mixed strategy equilibria. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <month> July </month> <year> 1993. </year> <title> 7 Quite similar to, say, the theory of structural mechanics not being in everyday use by construction engineers working on a particular single family house. However, one would hope that the engineers are aware of the limitations imposed by the theory! 14 </title>
Reference-contexts: Primarily, equilibrium points signify stable solutions without motivating how those solutions are to be reached. See [1] for a discussion of the weaknesses of equilibrium as the solution concept in games, and <ref> [3] </ref> for a recent attempt at relating and reconciling the two solution concepts. Finally we would like to stress that agents engaged in an interaction do not really need to have common knowledge to perform effective decision making and communication.
Reference: [4] <author> E. Ephrati and J. S. Rosenschein. </author> <title> Constrained intelligent action: Planning under the influence of a master agent. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 263-268, </pages> <address> San Jose, California, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: In this research, we are interested in rational agents that autonomously operate in a multiagent world without the benefit of predetermined protocols (see also <ref> [2, 5, 4] </ref>). 1 Our basic framework for evaluating when and what KOA should be performed is as follows: rational agents are to use knowledge-oriented actions that are most valuable to them in the particular situation, or technically speaking, ones that have the highest expected utility.
Reference: [5] <author> Michael R. Genesereth, Matthew L. Ginsberg, and Jeffrey S. Rosenschein. </author> <title> Cooperation without communication. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 5157, </pages> <address> Philadelphia, Pennsylvania, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: In this research, we are interested in rational agents that autonomously operate in a multiagent world without the benefit of predetermined protocols (see also <ref> [2, 5, 4] </ref>). 1 Our basic framework for evaluating when and what KOA should be performed is as follows: rational agents are to use knowledge-oriented actions that are most valuable to them in the particular situation, or technically speaking, ones that have the highest expected utility.
Reference: [6] <author> Piotr J. Gmytrasiewicz and Edmund H. Durfee. </author> <title> Decision-theoretic recursive modeling and the coordinated attack problem. </title> <booktitle> In Proceedings of the First International Conference on Artificial Intelligence Planning Systems | AIPS92, </booktitle> <pages> pages 88-95, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Finally we would like to stress that agents engaged in an interaction do not really need to have common knowledge to perform effective decision making and communication. As some of our experiments (and thought experiments) show <ref> [6] </ref>, in all of the practical decision making situations 3 a knowledge about certain facts nested down into finite levels is sufficient.
Reference: [7] <author> Piotr J. Gmytrasiewicz and Edmund H. Durfee. </author> <title> Toward a theory of honesty and trust among communicating autonomous agents. Group Decision and Negotiation, </title> <note> 1993, to appear. </note>
Reference-contexts: The effect of relaxing the assumptions about the messages being truthful and always believed adds even more overhead (see <ref> [7] </ref>). The extension we presented in this paper makes considerable computational demands as well.
Reference: [8] <author> Piotr J. Gmytrasiewicz, Edmund H. Durfee, and David K. Wehe. </author> <title> A decision-theoretic approach to coordinating multiagent interactions. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 62-68, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: The resulting hierarchy has one branch of alternating models representing the decision making situations of the sides via their payoff matrices. As we have argued in other applications of the RMM algorithm <ref> [9, 8] </ref>, the hierarchy in Figure 1a is finite and has to end, although at a possibly deep and unknown level. That is due to the fact that the two sides do not have any practical means for arriving at knowledge that would be nested down to infinity. <p> The threatening agent must wait for the action of the threatened agent, and only if this action is the action B, can the threat be enforced. Thus, unlike in the previous case above (and the other cases considered in <ref> [8, 9] </ref>), the actions of the players cannot be considered as simultaneous. 4.1 Verbal Threat We will again use the scenario of the defending and the invading agents presented above to illustrate our approach. <p> Any resulting state of knowledge S ij::: can be represented as a hierarchy of payoff matrices, as postulated in <ref> [8] </ref> and solved yielding the intentions of the other agents in this state. We have previously called such a distribution an intentional probability distribution, and we will label it here with the path used to derive it|p ij::: . <p> Issues and Future Work The methods we have presented above, for the computation of the expected utility of knowledge- oriented actions and the KOAs that could be part of longer sequences of responses or dialogues is the latest of extensions of the basic idea of decision-theoretic recursive modeling presented in <ref> [8] </ref>, which we called the Recursive Modeling Method (RMM). RMM is intended to represent all of the information an agent could have about the other agents relevant to the decision making problem in a multiagent situation.
Reference: [9] <author> Piotr J. Gmytrasiewicz, Edmund H. Durfee, and David K. Wehe. </author> <title> The utility of communication in coordinating intelligent agents. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 166-172, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Thus, we use the classic concept of rationality as defined in decision theory, which considers rational behavior to be that which maximizes utility. In this paper, we extend the method of calculating the expected utilities of certain kinds of purely communicative acts presented in <ref> [9] </ref>, by including other knowledge-oriented actions. We show how such actions may improve the position of one side in an encounter, and thus can serve as an important strategic choice in an automated encounter among self-interested agents. In [9], a method called the Recursive Modeling Method (RMM) was used to represent <p> the expected utilities of certain kinds of purely communicative acts presented in <ref> [9] </ref>, by including other knowledge-oriented actions. We show how such actions may improve the position of one side in an encounter, and thus can serve as an important strategic choice in an automated encounter among self-interested agents. In [9], a method called the Recursive Modeling Method (RMM) was used to represent and evaluate all of the knowledge relevant to the problem of choosing the best action, in the presence of other agents. <p> The resulting hierarchy has one branch of alternating models representing the decision making situations of the sides via their payoff matrices. As we have argued in other applications of the RMM algorithm <ref> [9, 8] </ref>, the hierarchy in Figure 1a is finite and has to end, although at a possibly deep and unknown level. That is due to the fact that the two sides do not have any practical means for arriving at knowledge that would be nested down to infinity. <p> Assuming for now that the message would be both delivered and believed by the other side, the message transforms the RMM hierarchy as depicted in Figure 1b (we call this transformation the pragmatic meaning of M1; see <ref> [9] </ref> for transformations induced by other types of messages). As we would expect, in the resulting hierarchy the defenders model the invading army as having only two options (Attack and SP), as opposed to three options before. <p> The threatening agent must wait for the action of the threatened agent, and only if this action is the action B, can the threat be enforced. Thus, unlike in the previous case above (and the other cases considered in <ref> [8, 9] </ref>), the actions of the players cannot be considered as simultaneous. 4.1 Verbal Threat We will again use the scenario of the defending and the invading agents presented above to illustrate our approach. <p> Our subsequent extensions of RMM to the issue of rational communicative behavior further add to the computational burden. The computation of the expected utility of isolated messages 12 KOA1 1 presented in <ref> [9] </ref> requires finding the transformations that the messages induce on the state of the agent's knowledge state and the solution of multiple recursive hierarchies resulting from these transformations. The effect of relaxing the assumptions about the messages being truthful and always believed adds even more overhead (see [7]).
Reference: [10] <author> J. Y. Halpern and R. Fagin. </author> <title> A formal model of knowledge, action and communication in dis-tributed systems: preliminary report. </title> <booktitle> In Proceedings of the 4th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 224-236, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: This class of actions include, of course, purely communicative acts (like speech), but also encompass any physical actions that result in a change of knowledge of the agents involved. While in the distributed systems' literature <ref> [10] </ref> this concept is used to analyze properties of an abstract class of protocols, called knowledge-based protocols, our approach is quite different.
Reference: [11] <author> Joseph Y. Halpern and Yoram Moses. </author> <title> Knowledge and common knowledge in a distributed environment. </title> <booktitle> In Third ACM Conference on Principles of Distributed Computing, </booktitle> <year> 1984. </year>
Reference-contexts: That is due to the fact that the two sides do not have any practical means for arriving at knowledge that would be nested down to infinity. A very elegant proof of this has been given by Halpern and Moses in <ref> [11] </ref>. There, the authors show that in situations in which agents use communication channels that have a non-zero probability of losing messages, infinitely nested knowledge, called common knowledge, in not achievable if the agents are not willing to "jump to conclusions" and simply assume common knowledge.
Reference: [12] <author> David E. Heckerman, John S. Breese, and Eric J. Horvitz. </author> <title> The compilation of decision models. </title> <booktitle> In Proceedings of the 1989 Workshop on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 162-173, </pages> <year> 1989. </year>
Reference-contexts: A very attractive method of dealing with the complexities of an all-encompassing formalism is compilation. It has been advocated before in <ref> [12, 15, 13] </ref> that decision theory can be made to justify heuristic rules of behavior under uncertainty and be compiled in a number of different ways into condition-action rules, action-utility rules, and so on. These suggestions are quite applicable to the formalisms of multiagent interaction and communication.
Reference: [13] <author> C. P. Langlotz, E. Shortlife, and L. M. Fagan. </author> <title> Using decision theory to justify heuristics. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 215-219, </pages> <address> Philadelphia, Pennsylvania, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: A very attractive method of dealing with the complexities of an all-encompassing formalism is compilation. It has been advocated before in <ref> [12, 15, 13] </ref> that decision theory can be made to justify heuristic rules of behavior under uncertainty and be compiled in a number of different ways into condition-action rules, action-utility rules, and so on. These suggestions are quite applicable to the formalisms of multiagent interaction and communication.
Reference: [14] <author> Y. Moses. </author> <title> Knowledge and communication (a tutorial). </title> <booktitle> In Proceedings of the Fourth Conference on Theoretical Aspects of reasoning about Knowledge, </booktitle> <pages> pages 1-14, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: If the environment is also populated by other intelligent agents, however, there arises a possibility of behaviors which can be aimed not only at changing the physical environment, but also at changing the state of knowledge of other agents. Following terminology introduced in the theory of distributed systems <ref> [14] </ref>, we call such acts knowledge-oriented actions, or KOA's. This class of actions include, of course, purely communicative acts (like speech), but also encompass any physical actions that result in a change of knowledge of the agents involved.
Reference: [15] <author> S. J. Russell. </author> <title> Execution architectures and compilation. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 15-20, </pages> <address> Detroit, Michigan, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: A very attractive method of dealing with the complexities of an all-encompassing formalism is compilation. It has been advocated before in <ref> [12, 15, 13] </ref> that decision theory can be made to justify heuristic rules of behavior under uncertainty and be compiled in a number of different ways into condition-action rules, action-utility rules, and so on. These suggestions are quite applicable to the formalisms of multiagent interaction and communication.
Reference: [16] <author> Thomas C. Schelling. </author> <title> The Strategy of Conflict. </title> <publisher> Harvard University Press, </publisher> <year> 1960. </year> <month> 15 </month>
Reference-contexts: Threats will be assumed to have the form "If you do A, then I will do B," where A is an option of an opponent and B is an option available to the threatening agent. One of the subtleties of threats, as discussed for example in <ref> [16] </ref>, is that they seem to involve a degree of irrationality on the part of the threatening side. <p> The threat is then believable in spite of the fact that following it through would be disadvantageous to the threatening side (see <ref> [16] </ref> for an illustrative discussion). Thus, responding with an attack to an attack may be irrational for the invaders, but setting up the mechanism for an automatic response is highly rational: it improves their utility, by presumably forcing the defenders to act in a certain way.
References-found: 16

