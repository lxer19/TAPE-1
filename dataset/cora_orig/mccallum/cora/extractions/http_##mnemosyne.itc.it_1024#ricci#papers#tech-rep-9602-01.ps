URL: http://mnemosyne.itc.it:1024/ricci/papers/tech-rep-9602-01.ps
Refering-URL: http://mnemosyne.itc.it:1024/ricci/tech-reports-list.html
Root-URL: 
Email: email: ricci@irst.itc.it  
Title: Nearest Neighbor Classification with a Local Asymmetrically Weighted Metric  
Author: Francesco Ricci and Paolo Avesani 
Address: 38050 Povo (TN) Italy  
Affiliation: Istituto per la Ricerca Scientifica e Tecnologica  
Abstract: This paper introduces a new local asymmetric weighting scheme for the nearest neighbor classification algorithm. It is shown both with theoretical arguments and computer experiments that good compression rates can be achieved outperforming the accuracy of the standard nearest neighbor classification algorithm and obtaining almost the same accuracy as the k-NN algorithm with k optimised in each data set. The improvement in time performance is proportional to the compression rate and in general it depends on the data set. The comparison of the classification accuracy of the proposed algorithm with a local symmetrically weighted metric and with a global metric strongly shows that the proposed scheme is to be preferred. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. W. Aha. </author> <title> Incremental, instance-based learning of independent and graded concept description. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> Ithaca, NY, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For an ordered feature, a distinction is made between "left" and "right" direction, introducing two weights for each feature. The motivation for this is related to the algorithm used for computing local weights. It is based on a feedback method [29] similar to the delta rule <ref> [21, 1, 2] </ref> or that used in competitive learning [31] or learning vector quantization [14]. <p> Some cases are "generalised", which means, in this case, that the point representation is changed for a two point representation. Two points x; y in <ref> [0; 1] </ref> N 2 Stanfill and Waltz in fact compute different weights for different tasks, i.e., the weight depends on the goal feature that has to be predicted. 4 define an hyper rectangle, that is the subset H xy [0; 1] N , H xy = fz 2 [0; 1] N <p> Two points x; y in <ref> [0; 1] </ref> N 2 Stanfill and Waltz in fact compute different weights for different tasks, i.e., the weight depends on the goal feature that has to be predicted. 4 define an hyper rectangle, that is the subset H xy [0; 1] N , H xy = fz 2 [0; 1] N j z i 2 [min (x i ; y i ); max (x i ; y i )] 8 i = 1; : : : ; N g. <p> y in <ref> [0; 1] </ref> N 2 Stanfill and Waltz in fact compute different weights for different tasks, i.e., the weight depends on the goal feature that has to be predicted. 4 define an hyper rectangle, that is the subset H xy [0; 1] N , H xy = fz 2 [0; 1] N j z i 2 [min (x i ; y i ); max (x i ; y i )] 8 i = 1; : : : ; N g. <p> Let Q d j=1 F j be the input space and x 2 Q d j=1 F j be a generic example. Assume that F j is the closed unit interval <ref> [0; 1] </ref> (real feature) or a generic set of symbols (categorical feature). <p> F j is a set of symbols and x j 6= y j 0:5 if x j or y j is unknown Given an example x 2 Q d j=1 F j a set of asymmetric weights w (x) for x is a 2 fi d matrix with values in <ref> [0; 1] </ref>. Let w k j (x), k = 0; 1 and j = 1; : : : ; d, be a generic element of w (x). Assume that w 0 j (x) = w 1 j (x) if F j is a set of symbols. <p> Assume that w 0 j (x) = w 1 j (x) if F j is a set of symbols. Let y be another point in the input space, if F j = <ref> [0; 1] </ref> then the following notation will be adopted: w j (x) ffi (x j ; y j ) p = w 0 w 1 j (x)ffi (x j ; y j ) p otherwise (1) for all p = 1; : : : ; 1. <p> Let CB = fx 1 ; : : : ; x m g be a subset of Q d j=1 F j (case base). W = <ref> [0; 1] </ref> 2djCBj is the space of all possible local weights for CB. <p> f (x i ) 6= f (y) then: P 0 ij ; x i ; y j ) = w 0 2 (1 j2w 0 w 0 P 1 ij ; x i ; y j ) = w 1 w 1 2 (1 j2w 1 if F j = <ref> [0; 1] </ref>. fi (1 j2w ij 1j)g (ffi j (x ij ; y j )) if F j is a set of symbols. ff 2 [0; 1] and fi 2 [0; 1] are called the reinforcement and punishment rate respectively. The function g : [0; 1] ! [0; 1] is used <p> w 0 P 1 ij ; x i ; y j ) = w 1 w 1 2 (1 j2w 1 if F j = <ref> [0; 1] </ref>. fi (1 j2w ij 1j)g (ffi j (x ij ; y j )) if F j is a set of symbols. ff 2 [0; 1] and fi 2 [0; 1] are called the reinforcement and punishment rate respectively. The function g : [0; 1] ! [0; 1] is used to take into account feature distances. <p> ; x i ; y j ) = w 1 w 1 2 (1 j2w 1 if F j = <ref> [0; 1] </ref>. fi (1 j2w ij 1j)g (ffi j (x ij ; y j )) if F j is a set of symbols. ff 2 [0; 1] and fi 2 [0; 1] are called the reinforcement and punishment rate respectively. The function g : [0; 1] ! [0; 1] is used to take into account feature distances. <p> j2w 1 if F j = <ref> [0; 1] </ref>. fi (1 j2w ij 1j)g (ffi j (x ij ; y j )) if F j is a set of symbols. ff 2 [0; 1] and fi 2 [0; 1] are called the reinforcement and punishment rate respectively. The function g : [0; 1] ! [0; 1] is used to take into account feature distances. <p> F j = <ref> [0; 1] </ref>. fi (1 j2w ij 1j)g (ffi j (x ij ; y j )) if F j is a set of symbols. ff 2 [0; 1] and fi 2 [0; 1] are called the reinforcement and punishment rate respectively. The function g : [0; 1] ! [0; 1] is used to take into account feature distances. Section 5 illustrates qualitatively the effects of different choices of those parameters on a simple classification problem, whereas in Section 7 the accuracy given by alternative choices of ff and fi is experimentally tested on ten data sets. <p> Note that each learning step updates at most d parameters and maintains the weights in <ref> [0; 1] </ref>. In fact if f (x i ) = f (y), then ffw 0 ij jx ij y j j w 0 ij , because ff 1 and jx ij y j j 1. Therefore w 0 ij ffw 0 ij jx ij y j j 0. <p> The function to be learned is f : <ref> [0; 1] </ref> fi [0; 1] ! f0; 1g, f (x; y) = 0 if x :2 1 otherwise 6 If Generate-Probe is the random generator it stops the loop after K fl jS n CBj iterations 9 are supposed to belong to CB. <p> The function to be learned is f : <ref> [0; 1] </ref> fi [0; 1] ! f0; 1g, f (x; y) = 0 if x :2 1 otherwise 6 If Generate-Probe is the random generator it stops the loop after K fl jS n CBj iterations 9 are supposed to belong to CB. <p> The circular curves C (P i ; r) around the point P i are the loci of points of equal distance from P i , C (P i ; r) = fP 2 <ref> [0; 1] </ref> 2 : ffi (P i ; P ) = rg. Initially these curves are circles because all the weights are equal and therefore the local metrics are Euclidean up to a scaling factor. <p> In this case the accuracy becomes optimal but the rate of convergence is quite slow as a quite exact class separation is achieved after two thousand steps. The reason for this behaviour is quite simple, if the sample is uniformly distributed on <ref> [0; 1] </ref> 2 the probability to get a sample that is misclassified, which is the only situation that causes an adaptation of the metric, is equal to the error. So as the accuracy function increases the rate of this accuracy decreases. That yields an exponential decreasing behaviour (e kt +1). <p> In practice, both previous choices do not give the best results. In fact, a compromise can be obtained by changing the g function. If g (z) = z, ff = 0:1 and fi = 1:0 we get rather 7 The samples are supposed to be uniformly distributed in <ref> [0; 1] </ref> 2 , and are randomly extracted by Generate-Probe (S n CB). 10 fi = 0:1 and g (z) = 1=(1 + z)) fi = 0:1 and g (z) = 1=(1 + z)) fast and accurate learning (see Figure 6). <p> Summing the previous equations we get n = 2N + 4 (1 H ), and using Theorem 1 we get the thesis of the lemma 2 Lemma 2 If P is rectilinear polygon in <ref> [0; 1] </ref> fi [0; 1] with H holes then the complement of P in [0; 1] fi [0; 1] can be optimally partitioned in n=2 H + 2 L 0 rectangles, where n is the number of faces of P and L 0 is the maximum number of non-intersecting chords that <p> Summing the previous equations we get n = 2N + 4 (1 H ), and using Theorem 1 we get the thesis of the lemma 2 Lemma 2 If P is rectilinear polygon in <ref> [0; 1] </ref> fi [0; 1] with H holes then the complement of P in [0; 1] fi [0; 1] can be optimally partitioned in n=2 H + 2 L 0 rectangles, where n is the number of faces of P and L 0 is the maximum number of non-intersecting chords that can be drawn <p> Summing the previous equations we get n = 2N + 4 (1 H ), and using Theorem 1 we get the thesis of the lemma 2 Lemma 2 If P is rectilinear polygon in <ref> [0; 1] </ref> fi [0; 1] with H holes then the complement of P in [0; 1] fi [0; 1] can be optimally partitioned in n=2 H + 2 L 0 rectangles, where n is the number of faces of P and L 0 is the maximum number of non-intersecting chords that can be drawn either vertically or horizontally between two notches of the complement of <p> the previous equations we get n = 2N + 4 (1 H ), and using Theorem 1 we get the thesis of the lemma 2 Lemma 2 If P is rectilinear polygon in <ref> [0; 1] </ref> fi [0; 1] with H holes then the complement of P in [0; 1] fi [0; 1] can be optimally partitioned in n=2 H + 2 L 0 rectangles, where n is the number of faces of P and L 0 is the maximum number of non-intersecting chords that can be drawn either vertically or horizontally between two notches of the complement of P . <p> Let us now suppose that p = 1, that is: ffi (x; y) = max fw j (x)jx j y j jg: In L 1 the locus of points equidistant from a given point are rectangular. Given a point x 2 <ref> [0; 1] </ref> d and a system of asymmetric weights w for x, the ball of radius r, B 1 (x; r) is defined as follows: B 1 (x; r) = fy 2 [0; 1] d : ffi (x; y) rg d Y [x j r=w 0 j ] We can now <p> Given a point x 2 <ref> [0; 1] </ref> d and a system of asymmetric weights w for x, the ball of radius r, B 1 (x; r) is defined as follows: B 1 (x; r) = fy 2 [0; 1] d : ffi (x; y) rg d Y [x j r=w 0 j ] We can now prove the following: Theorem 2 n + 1 L L 0 examples are enough for exact learning a rectilinear polygon P with n faces in two dimensions, where L and L 0 <p> In [22] the symmetric difference between sets is proposed as a measure of the distance between two concepts. Let C and C 0 be two subsets of <ref> [0; 1] </ref> d then C is said to be *-similar to C 0 if (C n C 0 [ C 0 n C) *, where () is the usual (Lebesgue) measure on [0; 1] d . <p> Let C and C 0 be two subsets of <ref> [0; 1] </ref> d then C is said to be *-similar to C 0 if (C n C 0 [ C 0 n C) *, where () is the usual (Lebesgue) measure on [0; 1] d . The following Theorem holds: Theorem 3 Let C be a polygonal tessellation with n edges. <p> From a practical point of view, the user should choose an initial value for fi in the range <ref> [0:6; 1] </ref> and then optimise ff taking into account that the larger ff gets, the less the system fits the training set and the faster convergence of the training phase becomes (see example in Section 5).
Reference: [2] <author> D. W. Aha. </author> <title> A study of instance-based algorithms for supervised learning tasks: Mathematical, empirical and psycological evaluations. </title> <type> Technical Report TR-90-42, </type> <institution> University of California, Irvine, </institution> <year> 1990. </year>
Reference-contexts: For an ordered feature, a distinction is made between "left" and "right" direction, introducing two weights for each feature. The motivation for this is related to the algorithm used for computing local weights. It is based on a feedback method [29] similar to the delta rule <ref> [21, 1, 2] </ref> or that used in competitive learning [31] or learning vector quantization [14].
Reference: [3] <author> D. W. Aha and R. L. Goldstone. </author> <title> Learning attribute relevance in context in instance-based learning algorithms. </title> <booktitle> In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 141-148, </pages> <address> Cambridge, MA, 1990. </address> <publisher> Lawrence Earlbaum. </publisher>
Reference-contexts: introduction of a new local metric [19] and a procedure that progressively transforms the Euclidean metric in a set of locally defined metrics attached to a reduced set of examples in the training set. The idea of using local metrics is not new, many approaches have been considered <ref> [24, 25, 27, 8, 3, 4, 12, 11] </ref> and some of these will be reviewed in the next Section. The novelty of our proposal is related to the weighting method and the particular process that learns the local metric. <p> In this way, in relation to the size of the Hyper rectangle the metric behaves differently in the input space. In [7] Cost and Salzberg exploit a modified version of VDM, by a weighting scheme that weights examples in memory according to their performance history. Aha and Goldstone <ref> [3, 4] </ref> claim that an attribute's importance in human classification depends on its context and they provide a computational model that is able to generalise with results similar to those shown by experiments conducted with humans. <p> In fact all of the previously introduced metrics on real features are functions of the jx yj vector, whereas ffi is a true function of (x y). This distinction obviously could not be made on categorical feature spaces. Moreover ffi, as well as other local metrics <ref> [27, 3, 4] </ref>, is not symmetric, i.e. ffi (x; y) 6= ffi (y; x), when x; y 2 CB, because the weights stored for x could be different from those of y. <p> If the two values are equal then the prediction is correct, and the distance between the nearest neighbor and the sample is decreased, whereas if the two values are not equal the distance between the nearest neighbor and the sample is increased (see also <ref> [14, 21, 3, 31] </ref> for other applications of this well known technique). Let CB = fx 1 ; : : : ; x m g be a subset of Q d j=1 F j (case base).
Reference: [4] <author> D. W. Aha and R. L. Goldstone. </author> <title> Concept learning and flexible weighting. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 534-539, </pages> <address> Bloomington, IN, 1992. </address> <publisher> Lawrence Earlbaum. </publisher>
Reference-contexts: introduction of a new local metric [19] and a procedure that progressively transforms the Euclidean metric in a set of locally defined metrics attached to a reduced set of examples in the training set. The idea of using local metrics is not new, many approaches have been considered <ref> [24, 25, 27, 8, 3, 4, 12, 11] </ref> and some of these will be reviewed in the next Section. The novelty of our proposal is related to the weighting method and the particular process that learns the local metric. <p> In this way, in relation to the size of the Hyper rectangle the metric behaves differently in the input space. In [7] Cost and Salzberg exploit a modified version of VDM, by a weighting scheme that weights examples in memory according to their performance history. Aha and Goldstone <ref> [3, 4] </ref> claim that an attribute's importance in human classification depends on its context and they provide a computational model that is able to generalise with results similar to those shown by experiments conducted with humans. <p> They show that GCM-ISW provides a better fit to the subject data than other methods with no local weights. Therefore, they provide a cognitive foundation to local metric and to local weights update rules (see the reference in <ref> [4] </ref> for other models of human concept formation with local metric). 3 Weights and Metrics This section introduces the definitions of system of asymmetric weights and local asymmetrically weighted metric. <p> In fact all of the previously introduced metrics on real features are functions of the jx yj vector, whereas ffi is a true function of (x y). This distinction obviously could not be made on categorical feature spaces. Moreover ffi, as well as other local metrics <ref> [27, 3, 4] </ref>, is not symmetric, i.e. ffi (x; y) 6= ffi (y; x), when x; y 2 CB, because the weights stored for x could be different from those of y.
Reference: [5] <author> D. W. Aha, D. Kibler, and M. K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: With respect to the techniques used, many editing experiments [28, 20] try to select those examples that are closer to the classes' boundary or by deleting those examples that reduce accuracy <ref> [30, 5] </ref>. Conversely, using a local asymmetrically weighted metric, it seems more reasonable to select those examples that lie within the training clusters [6] or even better to choose one example for each set in a rectangular decomposition of the target concept (see Section 6).
Reference: [6] <author> C.-L. Chang. </author> <title> Finding prototypes for nearest neighbour classifier. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-23(11):1179-1184, </volume> <year> 1974. </year>
Reference-contexts: Conversely, using a local asymmetrically weighted metric, it seems more reasonable to select those examples that lie within the training clusters <ref> [6] </ref> or even better to choose one example for each set in a rectangular decomposition of the target concept (see Section 6).
Reference: [7] <author> S. Cost and S. Salzberg. </author> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 57-78, </pages> <year> 1993. </year> <month> 21 </month>
Reference-contexts: In this way, in relation to the size of the Hyper rectangle the metric behaves differently in the input space. In <ref> [7] </ref> Cost and Salzberg exploit a modified version of VDM, by a weighting scheme that weights examples in memory according to their performance history.
Reference: [8] <author> R. H. Creecy, B. M. Masand, S. J. Smith, and D. L. Waltz. </author> <title> Trading MIPS and memory for knowledge engineering. </title> <journal> Communication of ACM, </journal> <volume> 35 </volume> <pages> 48-64, </pages> <year> 1992. </year>
Reference-contexts: introduction of a new local metric [19] and a procedure that progressively transforms the Euclidean metric in a set of locally defined metrics attached to a reduced set of examples in the training set. The idea of using local metrics is not new, many approaches have been considered <ref> [24, 25, 27, 8, 3, 4, 12, 11] </ref> and some of these will be reviewed in the next Section. The novelty of our proposal is related to the weighting method and the particular process that learns the local metric. <p> For categorical features, statistics on the distri-bution of values and the correlation between values and categories are exploited. Weights for real features have been computed both with conditional probabilities (see [29] for a discussion) and with feed-back based learning techniques. Creecy et al. <ref> [8] </ref> consider boolean features and compute two types of weights: the per category feature importance (PCF), which is the conditional probability that an example belongs to a class given that a feature is true; and the cross category feature importance (CCF) that averages the PCF on all the categories. <p> The metric is local but a feature weight, for a given example, depends only on its feature value and on the class of the example. In <ref> [8] </ref> the authors show that the per category weighting method gives better results on a particular classification problem using an error minimising metric that combines the weighted local metric and the maximisation of P [cjx i = 1]. <p> Stanfill and Waltz [27] introduced the notion of context-sensitive similarity metric early in their Memory-Based Reasoning approach. The need for different weightings was motivated mostly by coupling features' relevance to the task to be solved. The cross category feature importance CCF discussed in <ref> [8] </ref> was in fact introduced early by Stanfill and Waltz in the task of pronouncing novel words, using a database of English words and their pronunciation. In this case feature values are discrete but not restricted to being boolean.
Reference: [9] <author> B. V. Dasarathy, </author> <title> editor. Nearest beighbour (NN) norms: NN pattern classification techniques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1991. </year>
Reference-contexts: Our study is related to the line of research called editing experiments, that is those techniques aimed at selecting a subset of prototypes from a given set of training examples both for computational efficiency and for making the classification more reliable (see <ref> [9] </ref> Chapter 6). Even if the final result is similar, an edited training set, the focus of this paper is quite different. Rather than search for a subset of the original training set, we simply choose a subset at random and we adapt the metric to that choice. <p> In the classical Nearest Neighbor algorithm the whole sample S is stored, but the idea of using a subset of S to reduce the amount of storage required arose early and sped up query time. Different approaches, which are called edited Nearest Neighbor, have been proposed (see <ref> [9] </ref> Chapter 6 for a collection of papers on this topic).
Reference: [10] <author> L. Ferrari, P. V. Sankar, and J. Sklansky. </author> <title> Minimal rectilinear partitions of digitized blocks. </title> <booktitle> Computer Vision Graphics and Image Processing, </booktitle> <volume> 28 </volume> <pages> 58-71, </pages> <year> 1984. </year>
Reference-contexts: The reader can refer to [13] for a survey paper on the decomposition problem. Decomposing a rectilinear polygon with holes into the minimum number of rectangles has been studied by Ferrari, Sancar and Sklansky <ref> [10] </ref>. In the case of non-degenerate holes, they give a O (n 5=2 ) procedure for solving this task. The degenerate case has been long considered NP-hard but recently Soltan and Gorpinevich have shown [26] that this problem is solvable in O (n 3=2 log n) time. <p> Before giving a bound on the number of rectangles needed to decompose a rectangular polygon (connected) a definition is needed. If the interior angle at a vertex is reflex (&gt; 180 deg.) the vertex is called a notch. In <ref> [10] </ref> the following theorem is proven (see also [26]): Theorem 1 A rectilinear connected polygon with H non-degenerate holes can be partitioned into N L + 1 H rectangles, where N is the number of notches, L is the maximum number of non-intersecting chords that can be drawn either vertically or
Reference: [11] <author> J. H. Friedman. </author> <title> Flexible metric nearest neighbour classification. </title> <note> Unpublished manuscript available by anonymous FTP from playfair.stanford.edu. </note>
Reference-contexts: introduction of a new local metric [19] and a procedure that progressively transforms the Euclidean metric in a set of locally defined metrics attached to a reduced set of examples in the training set. The idea of using local metrics is not new, many approaches have been considered <ref> [24, 25, 27, 8, 3, 4, 12, 11] </ref> and some of these will be reviewed in the next Section. The novelty of our proposal is related to the weighting method and the particular process that learns the local metric.
Reference: [12] <author> T. Hastie and R. Tibshirani. </author> <title> Discriminant adaptive nearest neighbour classification. </title> <editor> In U.M.Fayad and R.Uthurusamy, editors, </editor> <booktitle> KDD-95: Proceedings First International Conference on Knowledge Discovery and Data Mining, </booktitle> <year> 1994. </year>
Reference-contexts: introduction of a new local metric [19] and a procedure that progressively transforms the Euclidean metric in a set of locally defined metrics attached to a reduced set of examples in the training set. The idea of using local metrics is not new, many approaches have been considered <ref> [24, 25, 27, 8, 3, 4, 12, 11] </ref> and some of these will be reviewed in the next Section. The novelty of our proposal is related to the weighting method and the particular process that learns the local metric.
Reference: [13] <author> J. M. Keil and J. Sack. </author> <title> Minimum decomposition of polygonal objects. </title> <editor> In G. Toussaint, editor, </editor> <booktitle> Computational Geometry. </booktitle> <publisher> North Holland, </publisher> <year> 1985. </year>
Reference-contexts: Let us suppose that d = 2. A well studied problem in computational geometry consists of finding a procedure for optimal decomposition of a polygonal object into simpler components. The reader can refer to <ref> [13] </ref> for a survey paper on the decomposition problem. Decomposing a rectilinear polygon with holes into the minimum number of rectangles has been studied by Ferrari, Sancar and Sklansky [10]. In the case of non-degenerate holes, they give a O (n 5=2 ) procedure for solving this task.
Reference: [14] <author> T. Kohonen. </author> <title> The self-organizing map. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(9) </volume> <pages> 1464-1480, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: The motivation for this is related to the algorithm used for computing local weights. It is based on a feedback method [29] similar to the delta rule [21, 1, 2] or that used in competitive learning [31] or learning vector quantization <ref> [14] </ref>. But, while in the majority of these approaches the examples stored ("weight vectors" representing cluster centres or "codebooks") are moved in the input space 1 , we instead change the metrics attached to the stored examples, and therefore directional information has to be taken into account. <p> If the two values are equal then the prediction is correct, and the distance between the nearest neighbor and the sample is decreased, whereas if the two values are not equal the distance between the nearest neighbor and the sample is increased (see also <ref> [14, 21, 3, 31] </ref> for other applications of this well known technique). Let CB = fx 1 ; : : : ; x m g be a subset of Q d j=1 F j (case base).
Reference: [15] <author> P. M. Murphy and D. W. Aha. </author> <title> UCI Repository of Machine Learning Databases. </title> <institution> University of California, Department of Information and Computer Science, </institution> <address> Irvine, CA, </address> <year> 1994. </year>
Reference-contexts: no Iris 150 3 4 4C no Liver Disorders 345 2 6 6C no Thyroid 215 3 5 5C no Wine 178 3 13 13C no 7 Experimental Results In this Section we shall present the results of some experiments conducted on ten data sets taken from the UCI Repository <ref> [15] </ref>. Some general information on the chosen data sets is shown in Table 1. We have chosen those data sets that contain many real features, as only on this type of feature is an asymmetrically weighted local metric different from a symmetrically weighted one.
Reference: [16] <author> J. P. Myles and D. J. </author> <title> Hand. The multi-class metric problem in nearest neighbour discrimination rules. </title> <journal> Pattern Recognition, </journal> <volume> 23(11) </volume> <pages> 1291-1297, </pages> <year> 1990. </year>
Reference-contexts: They show that the metric d (x; y) = jP [1jx] P [1jy]j achieves that goal (P [1jx] is the probability of x being classified 1). They also give an estimate of such a metric: d (x; y) = @ @t P [1jt]j t=x jx yj. Myles and Hand <ref> [16] </ref> generalise that framework to the multi-class case, showing various possible extensions. Salzberg [23] uses a global metric on real features with an additional weight for each stored case that measures how frequently the case has been used to make a correct clas sification (prediction).
Reference: [17] <author> F. P. Preparata and M. I. Shamos. </author> <title> Computational Geometry. </title> <publisher> Springer, </publisher> <year> 1985. </year>
Reference-contexts: Voronoi diagram of a set S of n points in the plane is the partition of the plane composed of n loci of points: for each point p in S the Voronoi diagram contains the locus of points that are closer to p than to any other point in S <ref> [17, 32] </ref>.
Reference: [18] <author> F. Ricci. </author> <title> Constraint reasoning with learning automata. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 9(12) </volume> <pages> 1059-1082, </pages> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: R, the reinforcement step is chosen if the value of f on y is equal to the value of f on x. If this is not the case P , the punishment step is used (see also <ref> [18] </ref> for more details on learning steps and learning procedures based on reinforcement). A learning procedure iterates that step adjusting an initially Euclidean system of weights for CB, aiming to optimise classification accuracy.
Reference: [19] <author> F. Ricci and P. Avesani. </author> <title> Learning a local similarity metric for case-based reasoning. </title> <booktitle> In International Conference on Case-Based Reasoning (ICCBR-95), </booktitle> <address> Sesimbra, Portugal, </address> <month> Oct. </month> <pages> 23-26, </pages> <year> 1995, </year> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: introduction of a new local metric <ref> [19] </ref> and a procedure that progressively transforms the Euclidean metric in a set of locally defined metrics attached to a reduced set of examples in the training set. <p> Asymmetrically weighted local metrics more flexibly adapt to the data, as will be shown in the example in Section 5; they enable a freer choice of examples to store in the case base (see <ref> [19, page 306] </ref>) and make possible high compression rates (see Section 6). Figure 1 shows a set of level curves for different values of p in a two dimensional space.
Reference: [20] <author> G. L. Ritter, H. B. Woodruff, S. R. Lowry, and T. L. Isenhour. </author> <title> An algorithm for selective nearest neighbor decision rule. </title> <journal> IEEE Transaction on Information Theory, </journal> <volume> IT-21(6):665-669, </volume> <year> 1975. </year>
Reference-contexts: With respect to the techniques used, many editing experiments <ref> [28, 20] </ref> try to select those examples that are closer to the classes' boundary or by deleting those examples that reduce accuracy [30, 5].
Reference: [21] <editor> D. E. Rumelhart and J. L. McClelland, editors. </editor> <booktitle> Parallel Distributed Processing: Exploration in the Miscrostructure of Cognition. </booktitle> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: For an ordered feature, a distinction is made between "left" and "right" direction, introducing two weights for each feature. The motivation for this is related to the algorithm used for computing local weights. It is based on a feedback method [29] similar to the delta rule <ref> [21, 1, 2] </ref> or that used in competitive learning [31] or learning vector quantization [14]. <p> When the distance between an input point x from a point in the training set y is to be computed an interpolation of the local (attached to y) and global weights is used. Weights are updated with a method similar to the delta rule <ref> [21] </ref>. They show that GCM-ISW provides a better fit to the subject data than other methods with no local weights. <p> If the two values are equal then the prediction is correct, and the distance between the nearest neighbor and the sample is decreased, whereas if the two values are not equal the distance between the nearest neighbor and the sample is increased (see also <ref> [14, 21, 3, 31] </ref> for other applications of this well known technique). Let CB = fx 1 ; : : : ; x m g be a subset of Q d j=1 F j (case base).
Reference: [22] <author> S. Salzberg, A. Delcher, D. Heath, and S. Kasif. </author> <title> Best-case results for nearest neighbor learning. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 17(6) </volume> <pages> 599-610, </pages> <year> 1995. </year>
Reference-contexts: For bidimensional concepts represented by a rectilinear polygon it is shown that a number of points equal to the number of faces of the polygon is enough to learn exactly the concept. This compares favourably with a quadratic bound proved by Salzberg et al. <ref> [22] </ref> for the Euclidean metric. Similarly, given a concept represented by a polygonal tessellation it is shown that an *-similar concept can be learned exactly with fewer points than those required by NN if the Euclidean metric is used. <p> This point is addressed by showing that a local metric such as that defined in equation 2 can learn exactly a class of concepts, i.e. classify correctly all the examples, storing fewer examples than a Euclidean metric. In <ref> [22] </ref> the authors use geometrical tools to prove upper and lower bounds on the minimum number of examples required by NN to learn a concept exactly. The upper (lower) bound is the number of examples that are at most (least) necessary to learn exactly a given concept. <p> The upper (lower) bound is the number of examples that are at most (least) necessary to learn exactly a given concept. Because LASM could turn into an NN classifier setting all the weights equal to one, all the upper bounds shown in <ref> [22] </ref> apply to LASM. In some cases, using the flexibility introduced with the local weighting system, the upper bound can be lowered as will be shown next. Let us consider first an example. <p> Let us first consider the case of rectilinear polytopes, that is, objects in a d dimensional space 12 rectangles. in which all the faces are parallel to the axes. Figure 8 shows an example of a rectilinear polygon in a two dimensional space. In <ref> [22] </ref> it is proven that (2n=d) d examples are enough to learn exactly a rectilinear polytope, where n is the number of faces of the polytope and the Euclidean metric is used. Let us suppose that d = 2. <p> Finally we observe that the bound on the number of points needed to exact learning a rectilinear polygon provided by Theorem 2, i.e. n + 1 compares favourably with the quadratic bound obtained in <ref> [22] </ref> for the Euclidean Metric. polygon Let us suppose now that the concept to be learned is not a rectilinear polygon. <p> If the target concept can be approximated in some sense by a rectilinear polygon then the original concept can be learned with a bounded number of examples and a given error. In <ref> [22] </ref> the symmetric difference between sets is proposed as a measure of the distance between two concepts.
Reference: [23] <author> S. L. Salzberg. </author> <title> A nearest hyperrectangle learning method. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 251-276, </pages> <year> 1991. </year>
Reference-contexts: They also give an estimate of such a metric: d (x; y) = @ @t P [1jt]j t=x jx yj. Myles and Hand [16] generalise that framework to the multi-class case, showing various possible extensions. Salzberg <ref> [23] </ref> uses a global metric on real features with an additional weight for each stored case that measures how frequently the case has been used to make a correct clas sification (prediction).
Reference: [24] <author> R. D. Short and K. Fukunaga. </author> <title> A new nearest neighbour distance measure. </title> <booktitle> In Pro--ceeding of the 5th IEEE International Conference on Patter Recognition, </booktitle> <pages> pages 81-86, </pages> <address> Miami beach, FL, </address> <year> 1980. </year>
Reference-contexts: introduction of a new local metric [19] and a procedure that progressively transforms the Euclidean metric in a set of locally defined metrics attached to a reduced set of examples in the training set. The idea of using local metrics is not new, many approaches have been considered <ref> [24, 25, 27, 8, 3, 4, 12, 11] </ref> and some of these will be reviewed in the next Section. The novelty of our proposal is related to the weighting method and the particular process that learns the local metric. <p> Both previous approaches apply only to categorical features. For real features, Short and Fukunaga <ref> [24, 25] </ref> have proposed, for the two classes classification problem, a local metric that minimises E [!(x; y)], where !(x; y) is the quadratic difference of the probability of misclassifying x given that the NN of x is y and the probability of misclassifying x given an hypothetically infinite set.
Reference: [25] <author> R. D. Short and K. Fukunaga. </author> <title> Optimal distance measure for nearest neighbour classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 27 </volume> <pages> 622-627, </pages> <year> 1981. </year>
Reference-contexts: introduction of a new local metric [19] and a procedure that progressively transforms the Euclidean metric in a set of locally defined metrics attached to a reduced set of examples in the training set. The idea of using local metrics is not new, many approaches have been considered <ref> [24, 25, 27, 8, 3, 4, 12, 11] </ref> and some of these will be reviewed in the next Section. The novelty of our proposal is related to the weighting method and the particular process that learns the local metric. <p> Both previous approaches apply only to categorical features. For real features, Short and Fukunaga <ref> [24, 25] </ref> have proposed, for the two classes classification problem, a local metric that minimises E [!(x; y)], where !(x; y) is the quadratic difference of the probability of misclassifying x given that the NN of x is y and the probability of misclassifying x given an hypothetically infinite set.
Reference: [26] <author> V. Soltan and A. Gorpinevich. </author> <title> Minimum dissection of a rectilinear polygon with arbitrary holes into rectangles. </title> <journal> Discrete and Computational Geometry, </journal> <volume> 9 </volume> <pages> 57-79, </pages> <year> 1993. </year>
Reference-contexts: In the case of non-degenerate holes, they give a O (n 5=2 ) procedure for solving this task. The degenerate case has been long considered NP-hard but recently Soltan and Gorpinevich have shown <ref> [26] </ref> that this problem is solvable in O (n 3=2 log n) time. Before giving a bound on the number of rectangles needed to decompose a rectangular polygon (connected) a definition is needed. <p> Before giving a bound on the number of rectangles needed to decompose a rectangular polygon (connected) a definition is needed. If the interior angle at a vertex is reflex (&gt; 180 deg.) the vertex is called a notch. In [10] the following theorem is proven (see also <ref> [26] </ref>): Theorem 1 A rectilinear connected polygon with H non-degenerate holes can be partitioned into N L + 1 H rectangles, where N is the number of notches, L is the maximum number of non-intersecting chords that can be drawn either vertically or horizontally between two notches.
Reference: [27] <author> C. Stanfill and D. Waltz. </author> <title> Toward memory-based reasoning. </title> <journal> Communication of ACM, </journal> <volume> 29 </volume> <pages> 1213-1229, </pages> <year> 1986. </year>
Reference-contexts: introduction of a new local metric [19] and a procedure that progressively transforms the Euclidean metric in a set of locally defined metrics attached to a reduced set of examples in the training set. The idea of using local metrics is not new, many approaches have been considered <ref> [24, 25, 27, 8, 3, 4, 12, 11] </ref> and some of these will be reviewed in the next Section. The novelty of our proposal is related to the weighting method and the particular process that learns the local metric. <p> Interestingl y, in a very similar problem they got better results with cross-category weighting and k nearest neighbors. Stanfill and Waltz <ref> [27] </ref> introduced the notion of context-sensitive similarity metric early in their Memory-Based Reasoning approach. The need for different weightings was motivated mostly by coupling features' relevance to the task to be solved. <p> In fact all of the previously introduced metrics on real features are functions of the jx yj vector, whereas ffi is a true function of (x y). This distinction obviously could not be made on categorical feature spaces. Moreover ffi, as well as other local metrics <ref> [27, 3, 4] </ref>, is not symmetric, i.e. ffi (x; y) 6= ffi (y; x), when x; y 2 CB, because the weights stored for x could be different from those of y.
Reference: [28] <author> C. W. Swonger. </author> <title> Sample set condensation for a condensed nearest neighbor decision rule for pattern recognition. </title> <editor> In S. Watanabe, editor, </editor> <booktitle> Frontiers of Pattern Recognition, </booktitle> <pages> pages 511-519. </pages> <publisher> Academic Press, </publisher> <year> 1972. </year>
Reference-contexts: With respect to the techniques used, many editing experiments <ref> [28, 20] </ref> try to select those examples that are closer to the classes' boundary or by deleting those examples that reduce accuracy [30, 5].
Reference: [29] <author> D. Wettschereck and D. Aha. </author> <title> Weighting features. </title> <editor> In M.Veloso and A. Aamodt, editors, </editor> <booktitle> Case-Based Reasoning, Research and Development, </booktitle> <pages> pages 347-358. </pages> <publisher> Springer, </publisher> <year> 1995. </year>
Reference-contexts: For an ordered feature, a distinction is made between "left" and "right" direction, introducing two weights for each feature. The motivation for this is related to the algorithm used for computing local weights. It is based on a feedback method <ref> [29] </ref> similar to the delta rule [21, 1, 2] or that used in competitive learning [31] or learning vector quantization [14]. <p> For categorical features, statistics on the distri-bution of values and the correlation between values and categories are exploited. Weights for real features have been computed both with conditional probabilities (see <ref> [29] </ref> for a discussion) and with feed-back based learning techniques.
Reference: [30] <author> D. L. Wilson. </author> <title> Asymptotic properties of nearest neighbor rule using edited data. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-2(3):408-421, </volume> <year> 1972. </year>
Reference-contexts: With respect to the techniques used, many editing experiments [28, 20] try to select those examples that are closer to the classes' boundary or by deleting those examples that reduce accuracy <ref> [30, 5] </ref>. Conversely, using a local asymmetrically weighted metric, it seems more reasonable to select those examples that lie within the training clusters [6] or even better to choose one example for each set in a rectangular decomposition of the target concept (see Section 6).
Reference: [31] <author> L. Xu, A. Krzyzak, and E. Oja. </author> <title> Rival penalized competitive learning for cluster analysis, RBF net, and curve detection. </title> <journal> IEEE Transaction on Neural Networks, </journal> <volume> 4(4) </volume> <pages> 636-649, </pages> <year> 1993. </year>
Reference-contexts: The motivation for this is related to the algorithm used for computing local weights. It is based on a feedback method [29] similar to the delta rule [21, 1, 2] or that used in competitive learning <ref> [31] </ref> or learning vector quantization [14]. <p> If the two values are equal then the prediction is correct, and the distance between the nearest neighbor and the sample is decreased, whereas if the two values are not equal the distance between the nearest neighbor and the sample is increased (see also <ref> [14, 21, 3, 31] </ref> for other applications of this well known technique). Let CB = fx 1 ; : : : ; x m g be a subset of Q d j=1 F j (case base).
Reference: [32] <author> F. F. Yao. </author> <title> Computational geometry. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <pages> pages 343-389. </pages> <publisher> Elsevier, </publisher> <year> 1990. </year> <month> 23 </month>
Reference-contexts: Voronoi diagram of a set S of n points in the plane is the partition of the plane composed of n loci of points: for each point p in S the Voronoi diagram contains the locus of points that are closer to p than to any other point in S <ref> [17, 32] </ref>.
References-found: 32

