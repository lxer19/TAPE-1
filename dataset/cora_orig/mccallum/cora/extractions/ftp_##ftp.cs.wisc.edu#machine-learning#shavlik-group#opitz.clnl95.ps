URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/opitz.clnl95.ps
Refering-URL: http://www.lehigh.edu/~ob00/integrated/references-new.html
Root-URL: 
Email: opitz@cs.wisc.edu  
Phone: (608) 262-6613 1210  
Title: Appears in Computational Learning Theory and Natural Learning Systems, Vol. III, Using Heuristic Search to
Author: T. Petsche, S. Hanson and J. Shavlik, David W. Opitz and Jude W. Shavlik 
Affiliation: Computer Sciences Department University of Wisconsin Madison  
Address: W. Dayton St.  Madison, WI 53706  
Note: editors, (pp. 3-19), MIT Press, 1995.  
Abstract: Knowledge-based neural networks are networks whose topology is determined by mapping the dependencies of a domain-specific rulebase into a neural network. However, existing network training methods lack the ability to add new rules to the (reformulated) rulebases. Thus, on domain theories that are lacking rules, generalization is poor, and training can corrupt the original rules, even those that were initially correct. We present TopGen, an extension to the Kbann algorithm, that heuristically searches for possible expansions of a knowledge-based neural network, guided by the domain theory, the network, and the training data. TopGen does this by dynamically adding hidden nodes to the neural representation of the domain theory, in a manner analogous to adding rules and conjuncts to the symbolic rule base. Experiments indicate that our method is able to heuristically find effective places to add nodes to the knowledge bases of four real-world problems, as well as an artificial chess domain. The experiments also verify that new nodes must be added in an intelligent manner. Our algorithm showed statistically-significant improvements over Kbann in all five domains. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Fahlman, S. E. & Lebiere, C. </author> <year> (1989). </year> <title> The cascade-correlation learning architecture. </title> <editor> In Touretzky, D., editor, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 2), </booktitle> <pages> (pp. 524-532), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Frean, M. </author> <year> (1990). </year> <title> The upstart algorithm: A method for constructing and training feedfor-ward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 198-209. </pages>
Reference: <author> Fu, L. M. </author> <year> (1991). </year> <title> Rule learning by searching on adapted nets. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 590-595), </pages> <address> Anaheim, CA. </address>
Reference: <author> Ginsberg, A. </author> <year> (1990). </year> <title> Theory reduction, theory revision, </title> <booktitle> and retranslation. In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 777-782), </pages> <address> Boston, MA. </address>
Reference-contexts: In summary, DAID, tries to locate low-level links with errors, while TopGen searches for nodes with errors. Additional related work includes theory-refinement systems. Systems such as EITHER (Ourston & Mooney, 1990) and RTLS <ref> (Ginsberg, 1990) </ref> are propositional in nature. These systems differ from TopGen, in that their approaches are purely symbolic.
Reference: <author> Hinton, G. E. </author> <year> (1986). </year> <title> Learning distributed representations of concepts. </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> (pp. 1-12), </pages> <address> Amherst, MA. </address>
Reference-contexts: We also do not want to change the domain theory unless there is considerable evidence that it is incorrect. That is, there is a trade-off between changing the domain theory and disregarding the misclassified training examples as noise. To help address this, TopGen uses a variant of weight decay <ref> (Hinton, 1986) </ref>. Weights that are part of the original domain theory, decay toward their initial value, while other weights decay toward zero.
Reference: <author> Mezard, M. & Nadal, J.-P. </author> <year> (1989). </year> <title> Learning in feedforward layered networks: The tiling algorithm. </title> <journal> Journal of Physics A, </journal> <volume> 22 </volume> <pages> 2191-2204. </pages>
Reference: <author> Ourston, D. & Mooney, R. J. </author> <year> (1990). </year> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 815-820), </pages> <address> Boston, MA. </address>
Reference-contexts: In summary, DAID, tries to locate low-level links with errors, while TopGen searches for nodes with errors. Additional related work includes theory-refinement systems. Systems such as EITHER <ref> (Ourston & Mooney, 1990) </ref> and RTLS (Ginsberg, 1990) are propositional in nature. These systems differ from TopGen, in that their approaches are purely symbolic.
Reference: <author> Pazzani, M. J., Brunk, C. A., & Silverstein, B. </author> <year> (1991). </year> <title> A knowledge-intensive approach to relational concept learning. </title> <booktitle> In Proceedings of the Eighth International Machine Learning Workshop, </booktitle> <pages> (pp. 432-436), </pages> <address> Evanston, IL. </address>
Reference-contexts: While Towell (1992) showed that Kbann was superior to EITHER on a promoter problem, TopGen outperformed Kbann. Systems such as FOCL <ref> (Pazzani et al., 1991) </ref> and FORTE (Richards & Mooney, 1991) revise first-order theories. One drawback to these types of systems is that, due to their computational demands, the problems currently used by these systems are quite simple. Another drawback is that many such systems are unable to create new predicates.
Reference: <author> Richards, B. L. & Mooney, R. J. </author> <year> (1991). </year> <title> First-order theory revision. </title> <booktitle> In Proceedings of the Eighth International Machine Learning Workshop, </booktitle> <pages> (pp. 447-451), </pages> <address> Evanston, IL. </address>
Reference-contexts: While Towell (1992) showed that Kbann was superior to EITHER on a promoter problem, TopGen outperformed Kbann. Systems such as FOCL (Pazzani et al., 1991) and FORTE <ref> (Richards & Mooney, 1991) </ref> revise first-order theories. One drawback to these types of systems is that, due to their computational demands, the problems currently used by these systems are quite simple. Another drawback is that many such systems are unable to create new predicates.
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. & McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the microstructure of cognition. Volume 1: Foundations. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: TopGen uses Kbann's rule-to-network translation algorithm to define an initial guess for the network's topology. This network is trained using backpropagation <ref> (Rumelhart et al., 1986) </ref> and is placed on an OPEN list. In each cycle, TopGen takes the best network from the OPEN list (as measured by validation-set-2), decides possible ways to add new nodes, trains these new networks, and places them on the OPEN list.
Reference: <author> Sestito, S. & Dillon, T. </author> <year> (1990). </year> <title> Using multi-layered neural networks for learning symbolic knowledge. </title> <booktitle> In Proceedings of the 1990 Australian Artificial Intelligence Conference, </booktitle> <address> Perth, Australia. </address>
Reference: <author> Towell, G. & Shavlik, J. </author> <year> (1992). </year> <title> Using symbolic learning to improve knowledge-based neural networks. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 177-182), </pages> <address> San Jose, CA. </address>
Reference-contexts: A learning system should make repairs that minimize the changes to the initial domain theory, while making it consistent with the data. We present a connectionist approach to theory refinement, particularly focusing on the task of expanding impoverished domain theories. Kbann <ref> (Towell, 1992) </ref> is a connectionist theory-refinement system that translates a set of approximately-correct, domain-specific inference rules (called a domain theory) into a neural network, thereby determining the network's topology. It then applies backpropagation to refine these reformulated rules. <p> In a symbolic rule base, we can decrease false positives by either adding antecedents to existing rules or removing rules from the rule base. While Kbann can effectively remove rules <ref> (Towell, 1992) </ref>, it is less effective at adding antecedents to rules and is unable to invent (constructively induce) new terms as antecedents. Figures 2b,d show the ways (analogous to Figures 2a,c explained above) of adding constructively-induced antecedents. <p> Other possible approaches include: adding them to only a portion of the inputs, adding them to nodes that have a high correlation with the error, or adding them to the next "layer" of nodes. 7 Related Work The most obvious related work is the Kbann system <ref> (Towell, 1992) </ref>, described in detail earlier in this paper. The DAID algorithm (Towell & Shavlik, 1992), an extension to Kbann, uses the domain knowledge to help train the Kbann network. <p> The DAID algorithm <ref> (Towell & Shavlik, 1992) </ref>, an extension to Kbann, uses the domain knowledge to help train the Kbann network. Because Kbann is more effective at dropping antecedents than adding them, DAID tries to find potentially-useful inputs features not mentioned in the domain theory.
Reference: <author> Towell, G. & Shavlik, J. </author> <year> (1993). </year> <title> Extracting refined rules from knowledge-based neural networks. </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 71-101. </pages>
Reference-contexts: Also, large changes to the domain theory greatly complicated rule extraction following training <ref> (Towell & Shavlik, 1993) </ref>. <p> Trained Kbann networks are interpretable because (a) the meaning of its nodes does not significantly shift during training and (b) almost all the nodes are either fully active or inactive <ref> (Towell & Shavlik, 1993) </ref>. Not only does TopGen add nodes in a symbolic fashion, it adds them in a fashion that does not violate these two assumptions. Other future work includes extensively testing other approaches for localizing error.
Reference: <author> Towell, G. & Shavlik, J. </author> <title> (in press). </title> <booktitle> Knowledge-based artificial neural networks. Artificial Intelligence. </booktitle>
Reference: <author> Towell, G. G. </author> <year> (1992). </year> <title> Symbolic Knowledge and Neural Networks: Insertion, Refinement, and Extraction. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, WI. </institution>
Reference-contexts: A learning system should make repairs that minimize the changes to the initial domain theory, while making it consistent with the data. We present a connectionist approach to theory refinement, particularly focusing on the task of expanding impoverished domain theories. Kbann <ref> (Towell, 1992) </ref> is a connectionist theory-refinement system that translates a set of approximately-correct, domain-specific inference rules (called a domain theory) into a neural network, thereby determining the network's topology. It then applies backpropagation to refine these reformulated rules. <p> In a symbolic rule base, we can decrease false positives by either adding antecedents to existing rules or removing rules from the rule base. While Kbann can effectively remove rules <ref> (Towell, 1992) </ref>, it is less effective at adding antecedents to rules and is unable to invent (constructively induce) new terms as antecedents. Figures 2b,d show the ways (analogous to Figures 2a,c explained above) of adding constructively-induced antecedents. <p> Other possible approaches include: adding them to only a portion of the inputs, adding them to nodes that have a high correlation with the error, or adding them to the next "layer" of nodes. 7 Related Work The most obvious related work is the Kbann system <ref> (Towell, 1992) </ref>, described in detail earlier in this paper. The DAID algorithm (Towell & Shavlik, 1992), an extension to Kbann, uses the domain knowledge to help train the Kbann network. <p> The DAID algorithm <ref> (Towell & Shavlik, 1992) </ref>, an extension to Kbann, uses the domain knowledge to help train the Kbann network. Because Kbann is more effective at dropping antecedents than adding them, DAID tries to find potentially-useful inputs features not mentioned in the domain theory.
Reference: <author> Towell, G. G., Shavlik, J. W., & Noordewier, M. O. </author> <year> (1990). </year> <title> Refinement of approximately correct domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 861-866), </pages> <address> Boston, MA. </address>
Reference: <author> Weigand, A. S., Rumelhart, D. E., & Huberman, B. A. </author> <year> (1990). </year> <title> Generalization by weight-elimination with application to forecasting. </title> <editor> In Lippmann, R., Moody, J., & Touretzky, D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems (volume 3), </booktitle> <pages> (pp. 875-882), </pages> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Our weight decay term, then, decays weights as a function of their distance from their initial value and is a slight variant of the term proposed by Rumelhart in 1987 <ref> (Weigand et al., 1990) </ref>.
References-found: 17

