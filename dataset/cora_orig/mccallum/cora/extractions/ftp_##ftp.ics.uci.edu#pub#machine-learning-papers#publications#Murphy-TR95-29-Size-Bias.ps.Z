URL: ftp://ftp.ics.uci.edu/pub/machine-learning-papers/publications/Murphy-TR95-29-Size-Bias.ps.Z
Refering-URL: http://www.ics.uci.edu/AI/ML/MLAbstracts.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: An Empirical Analysis of the Benefit of Decision Tree Size Biases as a Function of
Author: Patrick M. Murphy 
Keyword: Induction, Decision Trees, Bias  
Address: Irvine, CA 92717  
Affiliation: Department of Information Computer Science University of California,  
Email: pmurphy@ics.uci.edu  
Phone: (714) 824-4035  
Web: http://www.ics.uci.edu/~pmurphy  
Abstract: The results reported here empirically show the benefit of decision tree size biases as a function of concept distribution. First, it is shown how concept distribution complexity (the number of internal nodes in the smallest decision tree consistent with the example space) affects the benefit of minimum size and maximum size decision tree biases. Second, a policy is described that defines what a learner should do given knowledge of the complexity of the distribution of concepts. Third, explanations for why the distribution of concepts seen in practice is amenable to the minimum size decision tree bias are given and evaluated empirically. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., Friedman, J., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Pacific Grove, CA: </address> <publisher> Wadsworth & Brooks. </publisher>
Reference-contexts: 1 Introduction Top down induction of decision trees has been significantly studied by a number of researchers, e.g. <ref> (Breiman, Friedman, Olshen, & Stone, 1984) </ref> and (Quinlan, 1986). The majority of the algorithms that construct decision trees from examples use splitting heuristics that aim to minimize the size of the induced decision trees.
Reference: <author> Buntine, W. </author> <year> (1992). </year> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 8 (1), </volume> <pages> 75-85. </pages> <note> 11 Murphy, </note> <author> P., & Pazzani, M. </author> <year> (1994). </year> <title> Exploring the decision forest: An empir-ical investigation of occam's razor in decision tree induction. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <pages> 257-275. </pages>
Reference-contexts: The majority of the algorithms that construct decision trees from examples use splitting heuristics that aim to minimize the size of the induced decision trees. Empirical evidence <ref> (Buntine, 1992) </ref> has suggested that, for real world problems, the bias for small trees tends to be useful for increasing predictive accuracy. Unfortunately, there has been little explanation for why the distribution of concepts, seen in practice, is amenable to the bias of choosing the minimum sized decision tree.
Reference: <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 (1), </volume> <pages> 81-106. </pages>
Reference-contexts: 1 Introduction Top down induction of decision trees has been significantly studied by a number of researchers, e.g. (Breiman, Friedman, Olshen, & Stone, 1984) and <ref> (Quinlan, 1986) </ref>. The majority of the algorithms that construct decision trees from examples use splitting heuristics that aim to minimize the size of the induced decision trees.
Reference: <author> Schaffer, C. </author> <year> (1994). </year> <title> A conservation law for generalization performance. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh International Conference, </booktitle> <pages> pp. 259-265. 12 </pages>
Reference-contexts: For example, * When the complexity of the distribution is small use the minimum size decision tree bias. * When the complexity of the distribution is large, use the maximum size decision tree bias. However, the results presented in Figure 1 are somewhat misleading. In <ref> (Schaffer, 1994) </ref>, the benefit of the bias used by a learner (generalization performance) is defined as the difference between the mean accuracy of the learner and the mean accuracy of a random guesser (0.5 for two class problems).
References-found: 4

