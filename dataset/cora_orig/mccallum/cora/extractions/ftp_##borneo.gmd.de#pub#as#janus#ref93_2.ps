URL: ftp://borneo.gmd.de/pub/as/janus/ref93_2.ps
Refering-URL: http://borneo.gmd.de/AS/janus/publi/publi.html
Root-URL: 
Title: Rejection of Incorrect Answers from a Neural Net Classifier  
Note: In proceedings of IWANN93  
Abstract: Frank Smieja Report number: 1993/2 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> U. Beyer and F. J. Smieja. </author> <title> Quantitative aspects of data-driven information processing. </title> <type> Technical Report 732, </type> <institution> Gesellschaft fur Mathematik und Datenverarbeitung, </institution> <address> St Au-gustin, Germany, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: Reflection can appear in many forms, all of which involve a degree of self-assessement and interpretability from the appro ximator. This subject is too extensive to be discussed in full here (see <ref> [9, 1, 2, 10] </ref>). It suffices to mention that one aspect of reflection is the ability to reject an x because the y produced by f is considered (by the approximator) to be unsatisfactory. <p> Rejection can proceed either through the positive affirmation of outputs (and rejection of all others), or through the positive rejection of outputs (and acceptance of all others). In fact, rejection is a discrete form of a more general "confidence" measure, estimating the degree of believability of y <ref> [9, 1] </ref>. Rejection rate, R, is the (absolute) percentage of patterns rejected by the classifier. Approximation trade-off t , is defined: t := rejected ^ correct rejected ^ wrong where rejection is measured relative to the entire test set. t measures the cost of rejection. <p> Furthermore, they are in general a form of classification problem, i.e. the number of possible different output vectors (classes) N is highly constrained. OCR is a typical real-world problem, which possesses another property we believe also to be characteristic of large real-world problems <ref> [1] </ref>. This property is that the classes are approximately (linearly) separable up to a limit (generally around a 70% success rate [4]). <p> This alone will reduce E considerably for the whole system, providing the functions f are sufficiently different. Current work indicates this indeed to be the case, and at least 5% improvement has been observed, even with quite similar f 's <ref> [1] </ref>. Additionally, when none of the approxi-mators wants to accept the pattern, a consensus can be taken over their y's. If a consensus exists, this answer should be accepted. It is necessary that the approximators can produce a reasonably good performance when working alone.
Reference: [2] <author> S. Hubrig-Schaumburg. </author> <title> Handwritten character recognition using a reflective modular neural network system, </title> <year> 1992. </year>
Reference-contexts: Reflection can appear in many forms, all of which involve a degree of self-assessement and interpretability from the appro ximator. This subject is too extensive to be discussed in full here (see <ref> [9, 1, 2, 10] </ref>). It suffices to mention that one aspect of reflection is the ability to reject an x because the y produced by f is considered (by the approximator) to be unsatisfactory.
Reference: [3] <author> R. P. Lippmann. </author> <title> An introduction to computing with neural nets. </title> <journal> IEEE ASSP Magazine, </journal> <month> April </month> <year> 1987. </year>
Reference-contexts: Hyperplane class separation method The nodes of the hidden layer can be viewed as hyperplanes <ref> [8, 7, 3] </ref>, performing the separations between classes in the input space. Each pair of points in the input space required to be separa ted must be separated in this space with a hy-perplane. Patterns that are liable to cause error are most likely to be border patterns.
Reference: [4] <author> H. Muhlenbein. </author> <title> Editorial. </title> <journal> Parallel Computing, </journal> <volume> 14(3) </volume> <pages> 247-248, </pages> <month> August </month> <year> 1990. </year> <title> special edition on neural networks. </title>
Reference-contexts: OCR is a typical real-world problem, which possesses another property we believe also to be characteristic of large real-world problems [1]. This property is that the classes are approximately (linearly) separable up to a limit (generally around a 70% success rate <ref> [4] </ref>). After that the classes overlap and envelop one another so thickly that the decision planes that have to be drawn become ever more numerous and particular in order to increase success rate further. This all leads to exponentially increasing costs of improvement during the adaptation phase.
Reference: [5] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <journal> Nature, </journal> <volume> 323(533), </volume> <year> 1986. </year>
Reference-contexts: For the training of the network we selected a set of 4616 patterns, and for testing a separate set of 4363 patterns. The network used was trained with back-propagation <ref> [5] </ref>, had 100 hidden nodes, and the classes were coded as one-class-one-position in the output vector. Rejection based on the output vector (scheme 1) Here the cleanness of the output vector only was taken into account.
Reference: [6] <author> O. G. Selfridge. Pandemonium: </author> <title> a paradigm for learning. </title> <booktitle> In The Mechanisation of Thought Processes: Proceedings of a Symposium Held at the National Physical Laboratory, </booktitle> <month> November </month> <year> 1958, </year> <pages> pages 511-527, </pages> <address> Lon-don: HMSO, </address> <year> 1958. </year>
Reference-contexts: After this point it we suggest that approximation can only be improved at a meta-level of a modular system, whereby a number of different approximators (i.e. that generate significantly different approximation functions f ) are used in a Pandemonium-style parallel system <ref> [6, 9] </ref>. We further hold that most real-world, large-scale problems are of the OCR type, and that steps can most usefully be made in the direction of meta-levels of approximation and reflective systems, rather than that of the details of the ap-proximator itself.
Reference: [7] <author> F. J. Smieja. </author> <title> Neural network constructive algorithms: Trading generalization for learning efficiency? Circuits, </title> <journal> Systems and Signal Processing, </journal> <volume> 12(2) </volume> <pages> 331-374, </pages> <year> 1993. </year>
Reference-contexts: Hyperplane class separation method The nodes of the hidden layer can be viewed as hyperplanes <ref> [8, 7, 3] </ref>, performing the separations between classes in the input space. Each pair of points in the input space required to be separa ted must be separated in this space with a hy-perplane. Patterns that are liable to cause error are most likely to be border patterns.
Reference: [8] <author> F. J. Smieja and H. Muhlenbein. </author> <title> The geometry of multilayer perceptron solutions. </title> <journal> Parallel Computing, </journal> <volume> 14 </volume> <pages> 261-275, </pages> <year> 1990. </year> <note> 6 Smieja </note>
Reference-contexts: Hyperplane class separation method The nodes of the hidden layer can be viewed as hyperplanes <ref> [8, 7, 3] </ref>, performing the separations between classes in the input space. Each pair of points in the input space required to be separa ted must be separated in this space with a hy-perplane. Patterns that are liable to cause error are most likely to be border patterns.
Reference: [9] <author> F. J. Smieja and H. Muhlenbein. </author> <title> Reflective modular neural network systems. </title> <type> Technical Report 633, </type> <institution> Gesellschaft fur Mathe-matik und Datenverarbeitung, </institution> <address> St Augustin, Germany, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: Reflection can appear in many forms, all of which involve a degree of self-assessement and interpretability from the appro ximator. This subject is too extensive to be discussed in full here (see <ref> [9, 1, 2, 10] </ref>). It suffices to mention that one aspect of reflection is the ability to reject an x because the y produced by f is considered (by the approximator) to be unsatisfactory. <p> Rejection can proceed either through the positive affirmation of outputs (and rejection of all others), or through the positive rejection of outputs (and acceptance of all others). In fact, rejection is a discrete form of a more general "confidence" measure, estimating the degree of believability of y <ref> [9, 1] </ref>. Rejection rate, R, is the (absolute) percentage of patterns rejected by the classifier. Approximation trade-off t , is defined: t := rejected ^ correct rejected ^ wrong where rejection is measured relative to the entire test set. t measures the cost of rejection. <p> After this point it we suggest that approximation can only be improved at a meta-level of a modular system, whereby a number of different approximators (i.e. that generate significantly different approximation functions f ) are used in a Pandemonium-style parallel system <ref> [6, 9] </ref>. We further hold that most real-world, large-scale problems are of the OCR type, and that steps can most usefully be made in the direction of meta-levels of approximation and reflective systems, rather than that of the details of the ap-proximator itself.
Reference: [10] <author> F. Weber. </author> <title> Self-reflective exploration of the kinematics of a two-joint robot arm. </title> <institution> Di-plomarbeit, University of Bonn, Germany, </institution> <year> 1992. </year> <note> in German. </note>
Reference-contexts: Reflection can appear in many forms, all of which involve a degree of self-assessement and interpretability from the appro ximator. This subject is too extensive to be discussed in full here (see <ref> [9, 1, 2, 10] </ref>). It suffices to mention that one aspect of reflection is the ability to reject an x because the y produced by f is considered (by the approximator) to be unsatisfactory.
References-found: 10

