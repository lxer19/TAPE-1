URL: file://ftp.cs.utexas.edu/pub/mooney/papers/rapture-isiknh-94.ps.Z
Refering-URL: http://www.lehigh.edu/~ob00/integrated/references-new.html
Root-URL: 
Email: mahoney@cs.utexas.edu,  mooney@cs.utexas.edu,  
Phone: (512) 471-9589  (512) 471-9558  
Title: Modifying Network Architectures for Certainty-Factor Rule-Base Revision  
Author: J. Jeffrey Mahoney and Raymond J. Mooney 
Date: March 11, 1994  
Address: Austin, TX 78712  
Affiliation: Dept. of Computer Sciences University of Texas  
Note: Appears in The Intl Symposium on Integrating Knowledge and Neural Heuristics(ISIKNH-94)  
Abstract: This paper describes Rapture | a system for revising probabilistic rule bases that converts symbolic rules into a connectionist network, which is then trained via connectionist techniques. It uses a modified version of backpropagation to refine the certainty factors of the rule base, and uses ID3's information-gain heuristic (Quinlan, 1986) to add new rules. Work is currently under way for finding improved techniques for modifying network architectures that include adding hidden units using the UPSTART algorithm (Frean, 1990). A case is made via comparison with fully connected connectionist techniques for keeping the rule base as close to the original as possible, adding new input units only as needed.
Abstract-found: 1
Intro-found: 1
Reference: <author> Buchanan, G., and E.H. Shortliffe, e., </author> <title> editors (1984). Rule-Based Expert Systems:The MYCIN Experiments of the Stanford Heuristic Programming Project. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley Publishing Co. 9 Cohen, W. </publisher> <year> (1992). </year> <title> Compiling prior knowledge into an explicit bias. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> 102-110. </pages> <address> Aberdeen, Scotland. </address>
Reference-contexts: Probabilistic sum allows two pieces of evidence (x and y) to combine to give resulting evidence 2 x + y x fl y. Further details on the combining functions can be found in <ref> (Buchanan and E.H. Shortliffe, 1984) </ref>. 2.1 Building the Network Given these probabilistic rules, a Rapture network is built from them via a one-to-one mapping. Each rule in the rule base corresponds to one link in the network.
Reference: <author> Frean, M. </author> <year> (1990). </year> <title> The upstart algorithm: A method for constructing and training feedfor-ward neural networks. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 198-209. </pages>
Reference-contexts: Currently, rules are added through the use of ID3's information-gain metric. Rules can also be deleted through weight decay. Work in progress includes attempting to add hidden units to the network using a modified version of the UPSTART <ref> (Frean, 1990) </ref> algorithm. In the next section, we describe the Rapture system. Results making comparisons to fully connected techniques are presented, as well as preliminary UPSTART results. This is followed by related and future work, and conclusions. 2 The Rapture Algorithm The Rapture algorithm breaks down into three separate modules. <p> It is for this reason that more steamlined methods for adding in new units and modifying network architecture are desired. 4 Creating Hidden Units One method for creating hidden units that we are currently exploring uses the UPSTART algorithm <ref> (Frean, 1990) </ref>. This is a connectionist technique for adding new hidden units directly below an output unit. For each output unit that has mistaken examples, all of the false negative and false positive examples are collected. Two new hidden nodes are then created, and placed directly below this output unit.
Reference: <author> Fu, L. M. </author> <year> (1993). </year> <title> Knowledge-based connectionism for revising domain theories. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 23(1) </volume> <pages> 173-182. </pages>
Reference: <author> Lacher, R. </author> <year> (1992). </year> <title> Node error assignment in expert networks. </title> <editor> In Kandel, A., and Langholz, G., editors, </editor> <booktitle> Hybrid Architectures for Intelligent Systems, </booktitle> <pages> 29-48. </pages> <address> Boca Raton, FL: </address> <publisher> CRC Press, Inc. </publisher>
Reference-contexts: Opitz (Opitz and Shavlik, 1993) recently created a means for adding new hidden nodes that link into the nodes in the Kbann network with the highest error rates. These nodes are selected heuristically by measuring their performance on sets of tuning examples. Lacher <ref> (Lacher, 1992) </ref> has independently developed a very similar backpropagation technique (ENBP) for use with EMYCIN expert networks, though there apparently has been no work on network architecture modification.
Reference: <author> Mahoney, J. J., and Mooney, R. J. </author> <year> (1993). </year> <title> Combining connectionist and symbolic learning to refine certainty-factor rule-bases. Connection Science, </title> <publisher> 5(3-4):339-364. </publisher>
Reference-contexts: CFBP is simply gradient descent learning where the standard backpropagation formulae are modified to work with certainty-factor combining functions. Further details, along with a derivation of these formulae can be found in <ref> (Mahoney and Mooney, 1993) </ref>. 4 Loop until 100% training accuracy is achieved. 1. Perform CFBP on the network. Use given training examples, and as many epochs as necessary until mean-squared error decreases by &lt; *. 2.
Reference: <author> Michalski, R. S., and Chilausky, S. </author> <year> (1980). </year> <title> Learning by being told and learning from examples: An experimental comparison of the two methods of knowledge acquisition in the context of developing an expert system for soybean disease diagnosis. </title> <journal> Journal of Policy Analysis and Information Systems, </journal> <volume> 4(2) </volume> <pages> 126-161. </pages>
Reference: <author> O'Neill, M., and Chiafari, F. </author> <year> (1989). </year> <title> Escherichia coli promoters. </title> <journal> Journal of Biological Chemistry, </journal> <volume> 264 </volume> <pages> 5531-5534. </pages>
Reference: <author> Opitz, D. W., and Shavlik, J. W. </author> <year> (1993). </year> <title> Heuristically expanding knowledge-based neural networks. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial intelligence, </booktitle> <pages> 512-517. </pages> <address> Chamberry, France. </address>
Reference-contexts: Towell's original work contained a suggestion for adding new hidden units into a Kbann-net that would seem to work only in very specialized domains, such as Promoter Recognition. This was achieved through adding "cone" units that would connect contiguous features. Opitz <ref> (Opitz and Shavlik, 1993) </ref> recently created a means for adding new hidden nodes that link into the nodes in the Kbann network with the highest error rates. These nodes are selected heuristically by measuring their performance on sets of tuning examples.
Reference: <author> Ourston, D., and Mooney, R. </author> <year> (1990). </year> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> 815-820. </pages> <address> Detroit, MI. </address>
Reference: <author> Pazzani, M., and Kibler, D. </author> <year> (1992). </year> <title> The utility of background knowledge in inductive learning. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 57-94. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106. </pages>
Reference-contexts: This is due to the fact that no values are propagating forward through this link, and its removal can have no effect on the network. Similarly, Rapture contains a simple mechanism for adding links into the network. This is achieved through utilizing ID3's <ref> (Quinlan, 1986) </ref> information-gain metric. For each output category with misclassified examples, a search is made for the best feature-value combination (e.g., COLOR-RED) that could be used to help correctly classify these examples, using information-gain to discriminate examples.
Reference: <author> Shavlik, J. W., and Towell, G. G. </author> <year> (1989). </year> <title> Combining explanation-based and neural learning: An algorithm and empirical results. </title> <journal> Connection Science, </journal> <volume> 1(3) </volume> <pages> 325-339. </pages>
Reference: <author> Towell, G. G. </author> <year> (1991). </year> <title> Symbolic Knowledge and Neural Networks: Insertion, Refinement, and Extraction. </title> <type> PhD thesis, </type> <institution> University of Wisconsin, Madison, WI. </institution> <month> 10 </month>
Reference-contexts: When compared to pure 1 inductive learning, these knowledge rich methods show superior rates of learning, usually requiring fewer examples. One of the weaknesses of the connectionist approaches to date is the inability to modify their network architectures. The approach of Kbann <ref> (Towell, 1991) </ref> is to include every possible feature from the domain in the network, fully connecting, and letting those links that don't significantly contribute drop out, while training the network via backpropagation. In extremely complex domains, this can result in excessively large networks that can become unmanageable. <p> An overview of the Rapture algorithm is depicted in Figure 2. 3 Comparisons with Fully Connected Systems promoter is a short DNA sequence that precedes the beginnings of genes, and are locations where the protein RNA polymerase binds to the DNA structure <ref> (Towell, 1991) </ref>. A theory designed to recognize such strings composed of DNA-nucleotides was given to Rapture for revision. The data set used for these experiments is one of 106 examples, for which there are 5 53 positive examples (i.e. promoters), and 53 negative examples. <p> This graph is a plot of average performance in accuracy at classifying novel test examples over 25 independent trials. It is significant that Rapture is performing better than the fully connected systems. Kbann <ref> (Towell, 1991) </ref> is a neural network that has been initialized with expert knowledge. CF-Kbann is a hybrid system that is an implementation of Kbann that uses certainty-factor combining functions and CFBP.
References-found: 13

