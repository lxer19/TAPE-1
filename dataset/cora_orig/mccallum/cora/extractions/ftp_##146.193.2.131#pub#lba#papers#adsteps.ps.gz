URL: ftp://146.193.2.131/pub/lba/papers/adsteps.ps.gz
Refering-URL: http://www.cnl.salk.edu/~schraudo/pubs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Parameter Adaptation in Stochastic Optimization  
Author: Lus B. Almeida, Thibault Langlois, Jose D. Amaral and Alexander Plakhov INESC, R. Alves Redol, , Lisboa, Portugal 
Abstract: Optimization is an important operation in many domains of science and technology. Local optimization techniques typically employ some form of iterative procedure, based on derivatives of the function to be optimized (objective function). These techniques normally involve parameters that must be set by the user, often by trial and error. Those parameters can have a strong influence on the convergence speed of the optimization. In several cases, a significant speed advantage could be gained if one could vary these parameters during the optimization, to reflect the local characteristics of the function being optimized. Some parameter adaptation methods have been proposed for this purpose, for deterministic optimization situations. For stochastic (also called on-line) optimization situations, there appears to be no simple and effective parameter adaptation method. This paper proposes a new method for parameter adaptation in stochastic optimization. The method is applicable to a wide range of objective functions, as well as to a large set of local optimization techniques. We present the derivation of the method, details of its application to gradient descent and to some of its variants, and examples of its use in the gradient optimization of several functions, as well as in the training of a multilayer perceptron by on-line backpropagation.
Abstract-found: 1
Intro-found: 1
Reference: <author> Almeida, </author> <title> L.B. (1996) `Multilayer Perceptrons', section C1.2 of Handbook of Neural Computation, </title> <editor> Fiesler, E. (ed.), </editor> <address> New York, NY: Oxford University Press. </address> <note> Parameter Adaptation in Stochastic Optimization 23 Almeida, L.B., </note> <author> Langlois, T., Amaral, J.D. </author> <title> (1997) `On-Line Step Size Adaptation', </title> <type> technical report RT07/97, </type> <address> INESC, Lisbon, Portugal. </address>
Reference-contexts: Section 5 presents experimental results and Section 6 concludes. 2 Deterministic step size adptation Following ideas of Kesten (1958) and Jacobs (1988) a step size adaptation method for deterministic gradient optimization was proposed in (Silva and Almeida 1990a and 1990b), see also <ref> (Almeida 1996) </ref>. A similar method was independently proposed in (Tollenaere, 1990). The central idea behind these methods is that if successive updates of x i (a component of x), are made in the same direction, then the movement along that component should be made faster. <p> Normally one uses u 1:1 and d 0:9. This step size adaptation method has shown to be very effective in increasing the learning speed of multilayer perceptrons (MLPs) trained by backpropagation. Used together with momentum and an error control scheme <ref> (Almeida 1996) </ref>, it yields a very fast and robust training method for multilayer percep trons. The method, as defined above, is not directly applicable to stochastic gradient optimization, since the partial derivatives that appear in (2.3) are not available in such a case.
Reference: <author> Battiti, R. </author> <title> (1992) `First- and second-order methods for learning: Between steepest descent and Newton's method', </title> <journal> Neural Comput. </journal> <volume> 4, </volume> <pages> 141-166. </pages>
Reference-contexts: Gradient descent probably is the best known local mimization method, and forms the basis of several learning algorithms, such as backpropagation, used in the field of neural networks. Other local techniques include second order methods such as the Newton method and its variants <ref> (Battiti 1992) </ref>, and the various conjugate gradient techniques (Press et al. 1986), (Moller 1990). Many of these methods have adjustable parameters, such as p in gradient descent. There is often no good way to choose these parameters a priori.
Reference: <author> Becker, S., Le Cun, Y. </author> <title> (1989) `Improving the convergence of back-propagation learning with second order methods', </title> <booktitle> Proc. 1988 Connectionist Models Summer School, </booktitle> <editor> Touretzky, D., Hinton, G., Sejnowski, T. (eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 29-37. </pages>
Reference-contexts: This has led several authors to propose techniques for adapting p (Chan et al. 1987), (Werbos 1989 and 1992) or for adapting multiple p's, one for each coordinate (Silva and Almeida 1990a and 1990b), (Tollenaere 1990). Other acceleration techniques have also been proposed <ref> (Becker and Le Cun 1989) </ref>, (Fahlman 1989), (Silva and Almeida 1991). Most of these techniques were devised only for deterministic optimization (also called batch-mode learning, in the neural networks field). <p> A few methods to accelerate the convergence in stochastic optimization have been proposed <ref> (Becker and Le Cun 1989) </ref>, (Sutton 1992), (Murata et al. 1997), (Orr and Leen 1997). However, none of them seems to have gained widespread application. In this paper we derive a simple method for adapting the parameters of local stochastic optimization algorithms in order to improve their speed.
Reference: <author> Chan, L.W., Fallside, F. </author> <title> (1987) `An adaptive training algorithm for back propagation networks', </title> <booktitle> Computer Speech & Language 2, </booktitle> <pages> 205-218. </pages>
Reference-contexts: This has led several authors to propose techniques for adapting p <ref> (Chan et al. 1987) </ref>, (Werbos 1989 and 1992) or for adapting multiple p's, one for each coordinate (Silva and Almeida 1990a and 1990b), (Tollenaere 1990). Other acceleration techniques have also been proposed (Becker and Le Cun 1989), (Fahlman 1989), (Silva and Almeida 1991).
Reference: <author> Darken, C., Moody, </author> <title> J.E. (1992) `Towards faster stochastic gradient search', </title> <booktitle> in Advances in Neural Information Processing Systems 4, </booktitle> <editor> Moody, J.E., Hanson, S.J., Lipmann, R.P. (eds), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fahlman, </author> <title> S.E. (1989) `Fast-learning variations on back-propagation: An empirical study', </title> <booktitle> Proc. 1988 Connectionist Models Summer School, </booktitle> <editor> Touretzky, D., Hinton, G., Sejnowski, T. (eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <month> 38-51. </month> <title> Jacobs, R (1988) `Increased rates of convergence trough learning rate adaptation', </title> <booktitle> Neural Networks 1:4. </booktitle>
Reference-contexts: This has led several authors to propose techniques for adapting p (Chan et al. 1987), (Werbos 1989 and 1992) or for adapting multiple p's, one for each coordinate (Silva and Almeida 1990a and 1990b), (Tollenaere 1990). Other acceleration techniques have also been proposed (Becker and Le Cun 1989), <ref> (Fahlman 1989) </ref>, (Silva and Almeida 1991). Most of these techniques were devised only for deterministic optimization (also called batch-mode learning, in the neural networks field).
Reference: <author> Kesten, H. </author> <title> (1958) `Accelerated stochastic approximation', </title> <journal> Annals of Mathematical Statistics 29, </journal> <pages> 41-59. </pages>
Reference: <author> Moller, M.F. </author> <title> (1990) `A scaled conjugated gradient algorithm for fast supervised learning', </title> <type> Preprint PB-339, </type> <institution> Computer Science Department, University of Aarhus, Aarhus, Denmark. </institution>
Reference-contexts: Other local techniques include second order methods such as the Newton method and its variants (Battiti 1992), and the various conjugate gradient techniques (Press et al. 1986), <ref> (Moller 1990) </ref>. Many of these methods have adjustable parameters, such as p in gradient descent. There is often no good way to choose these parameters a priori.
Reference: <author> Murata, N., Mueller, K., Ziehe, A., Amari, S. </author> <title> (1997) `Adaptive on-line learning in changing environments', </title> <booktitle> in Advances in Neural Information Processing Systems 9, </booktitle> <editor> Mozer, M.C., Jordan, M.I., Petsche, T. (eds.), </editor> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: A few methods to accelerate the convergence in stochastic optimization have been proposed (Becker and Le Cun 1989), (Sutton 1992), <ref> (Murata et al. 1997) </ref>, (Orr and Leen 1997). However, none of them seems to have gained widespread application. In this paper we derive a simple method for adapting the parameters of local stochastic optimization algorithms in order to improve their speed.
Reference: <author> Orr, G., Leen, T. </author> <title> (1997) `Using curvature information for fast stochastic search', </title> <booktitle> in Advances in Neural Information Processing Systems 9, </booktitle> <editor> Mozer, M.C., Jordan, M.I., Petsche, T. (eds.), </editor> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: A few methods to accelerate the convergence in stochastic optimization have been proposed (Becker and Le Cun 1989), (Sutton 1992), (Murata et al. 1997), <ref> (Orr and Leen 1997) </ref>. However, none of them seems to have gained widespread application. In this paper we derive a simple method for adapting the parameters of local stochastic optimization algorithms in order to improve their speed.
Reference: <author> Plakhov, A., Almeida, </author> <note> L.B. (1998) in preparation. </note>
Reference-contexts: We don't see any special advantage of the exponential form in practical use, although it may be more amenable to theoretical analysis <ref> (Plakhov and Almeida, 1998) </ref>.
Reference: <author> Press, </author> <title> W.H., Flannery, B.P., Teukolsky, S.A., Vetterling, W.T. (1986) Numerical Recipes, </title> <address> Cambridge, UK: </address> <publisher> Cambridge University Press. </publisher>
Reference-contexts: Other local techniques include second order methods such as the Newton method and its variants (Battiti 1992), and the various conjugate gradient techniques <ref> (Press et al. 1986) </ref>, (Moller 1990). Many of these methods have adjustable parameters, such as p in gradient descent. There is often no good way to choose these parameters a priori.
Reference: <author> Silva, </author> <title> F.M., Almeida, L.B. (1990a) `Acceleration techniques for the backpropagation algorithm', in Neural Networks, Almeida, L.B., Wellekens, </title> <editor> C.J. (eds.), </editor> <publisher> Berlin: Springer, </publisher> <pages> 110-119. </pages>
Reference-contexts: This has led several authors to propose techniques for adapting p (Chan et al. 1987), (Werbos 1989 and 1992) or for adapting multiple p's, one for each coordinate <ref> (Silva and Almeida 1990a and 1990b) </ref>, (Tollenaere 1990). Other acceleration techniques have also been proposed (Becker and Le Cun 1989), (Fahlman 1989), (Silva and Almeida 1991). Most of these techniques were devised only for deterministic optimization (also called batch-mode learning, in the neural networks field). <p> Section 5 presents experimental results and Section 6 concludes. 2 Deterministic step size adptation Following ideas of Kesten (1958) and Jacobs (1988) a step size adaptation method for deterministic gradient optimization was proposed in <ref> (Silva and Almeida 1990a and 1990b) </ref>, see also (Almeida 1996). A similar method was independently proposed in (Tollenaere, 1990). <p> This may lead to oscillation or even divergence of the minimization process. 10 Almeida, Langlois, Amaral and Plakhov 4.3 Full matrix P Use of independent step size parameters for the various components of x is appropriate to deal with ravines that are parallel to the coordinate axes, as discussed in <ref> (Silva and Almeida 1990a and 1990b) </ref>. When ravines are not parallel to the axes, this form of the gradient algorithm loses some of its efficiency. For that case, the use of the full matrix P (or of momentum, which we discuss in the next section) is more appropriate.
Reference: <author> Silva, </author> <title> F.M., Almeida, L.B. (1990b) `Speeding up backpropagation', in Advanced Neural Computers, </title> <editor> Eckmiller, R. (ed.), </editor> <publisher> Amsterdam: Elsevier, </publisher> <pages> 151-160. </pages>
Reference: <author> Silva, </author> <title> F.M., Almeida, L.B. (1991) `Speeding-up backpropagation by data orthonor-malization', </title> <booktitle> in Artificial Neural Networks vol. </booktitle> <volume> 2, </volume> <editor> Kohonen, T., Makisara, K., Simula, O., Kangas, J. (eds.), </editor> <publisher> Amsterdam: Elsevier, </publisher> <pages> 149-156 24 Almeida, </pages> <editor> Langlois, Amaral and Plakhov Sutton, </editor> <title> R.S. (1992) `Adapting bias by gradient descent: An incremental version of delta-bar-delta', </title> <booktitle> Proc. Tenth Nat. Conf. Artif. Intell., </booktitle> <address> Cambridge, MA: </address> <publisher> MIT press, </publisher> <pages> 171-176. </pages>
Reference-contexts: Other acceleration techniques have also been proposed (Becker and Le Cun 1989), (Fahlman 1989), <ref> (Silva and Almeida 1991) </ref>. Most of these techniques were devised only for deterministic optimization (also called batch-mode learning, in the neural networks field).
Reference: <author> Tollenaere, T. </author> <year> (1990) </year> <month> `SuperSAB: </month> <title> Fast adaptive back propagation with good scaling properties', </title> <booktitle> Neural Networks 3, </booktitle> <pages> 561-574. </pages>
Reference-contexts: This has led several authors to propose techniques for adapting p (Chan et al. 1987), (Werbos 1989 and 1992) or for adapting multiple p's, one for each coordinate (Silva and Almeida 1990a and 1990b), <ref> (Tollenaere 1990) </ref>. Other acceleration techniques have also been proposed (Becker and Le Cun 1989), (Fahlman 1989), (Silva and Almeida 1991). Most of these techniques were devised only for deterministic optimization (also called batch-mode learning, in the neural networks field). <p> A similar method was independently proposed in <ref> (Tollenaere, 1990) </ref>. The central idea behind these methods is that if successive updates of x i (a component of x), are made in the same direction, then the movement along that component should be made faster.
Reference: <author> Werbos. P.J. </author> <title> (1989) `Maximizing long-term gas industry profits in two minutes in Lotus using neural network methods', </title> <journal> IEEE Trans. Sys. Man Cybernet. </journal> <volume> 19:2, </volume> <pages> 315-333. </pages>
Reference-contexts: This has led several authors to propose techniques for adapting p (Chan et al. 1987), <ref> (Werbos 1989 and 1992) </ref> or for adapting multiple p's, one for each coordinate (Silva and Almeida 1990a and 1990b), (Tollenaere 1990). Other acceleration techniques have also been proposed (Becker and Le Cun 1989), (Fahlman 1989), (Silva and Almeida 1991).
Reference: <author> Werbos, P.J. </author> <title> (1992)`Neurocontrol and Supervised learning: An overview and evaluation', in Handbook of Intelligent Control, White, D.A., </title> <editor> Sofge, D.A. (eds.), </editor> <address> New York: </address> <publisher> Van Nostrand Reinhold, </publisher> <pages> 65-89. </pages>
References-found: 18

