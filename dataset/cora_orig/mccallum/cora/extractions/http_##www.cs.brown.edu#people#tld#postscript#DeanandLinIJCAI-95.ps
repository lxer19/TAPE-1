URL: http://www.cs.brown.edu/people/tld/postscript/DeanandLinIJCAI-95.ps
Refering-URL: http://www.cs.brown.edu/people/tld/
Root-URL: 
Email: Email: ftld,shlg@cs.brown.edu  
Title: Decomposition Techniques for Planning in Stochastic Domains  
Author: Thomas Dean Shieu-Hong Lin 
Date: 1910,  
Address: Box  Providence, RI 02906, USA  
Affiliation: Department of Computer Science  Brown University  
Abstract: This paper is concerned with modeling planning problems involving uncertainty as discrete-time, finite-state stochastic automata. Solving planning problems is reduced to computing policies for Markov decision processes. Classical methods for solving Markov decision processes cannot cope with the size of the state spaces for typical problems encountered in practice. As an alternative, we investigate methods that decompose global planning problems into a number of local problems, solve the local problems separately, and then combine the local solutions to generate a global solution. We present algorithms that decompose planning problems into smaller problems given an arbitrary partition of the state space. The local problems are interpreted as Markov decision processes and solutions to the local problems are interpreted as policies restricted to the subsets of the state space defined by the partition. One algorithm relies on constructing and solving an abstract version of the original decision problem. A second algorithm iteratively approximates parameters of the local problems to converge to an optimal solution. We show how properties of a specified partition affect the time and storage required for these algorithms.
Abstract-found: 1
Intro-found: 1
Reference: [ Bellman, 1961 ] <author> Richard Bellman. </author> <title> Adaptive Control Processes. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1961. </year>
Reference-contexts: We mention two standard methods for solving Markov decision processes. Bellman's value iteration method <ref> [ Bellman, 1961 ] </ref> iterates by computing the optimal expected cumulative cost function accounting for n steps of lookahead using the optimal expected cumulative cost function accounting for n1 steps of lookahead. <p> We are currently testing these algorithms on a set of benchmark problems. Since the discussion is somewhat lengthy and requires some understanding of both Howard's policy iteration [ Howard, 1960 ] and Bellman's value iteration <ref> [ Bellman, 1961 ] </ref> for solving Markov decision processes, we refer the interested reader to the longer version of the paper [ Dean and Lin, 1995 ] . 6 Related Work The related work on abstraction and decomposition is extensive.
Reference: [ Boutilier et al., 1995 ] <author> Craig Boutilier, Richard Dear-den, and Moises Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the 1995 International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: Similarly, policies for large factored state spaces can often be efficiently encoded using decision trees that branch on state variables <ref> [ Boutilier et al., 1995 ] </ref> .
Reference: [ Caines and Wang, 1990 ] <author> Peter E. Caines and S. Wang. COCOLOG: </author> <title> A conditional observer and controller logic for finite machines. </title> <booktitle> In Proceedings of the 29th IEEE Conference on Decision and Control, Hawaii, </booktitle> <year> 1990. </year>
Reference-contexts: Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [ Zhong and Wonham, 1990 ] <ref> [ Caines and Wang, 1990 ] </ref> . In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces [ Moore and Atkeson, 1995 ] and stochastic models and discrete state spaces [ Kaelbling, 1993 ] .
Reference: [ Chvatal, 1980 ] <author> Vasek Chvatal. </author> <title> Linear Programming. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1980. </year>
Reference-contexts: The details of the material presented in this section depend on some understanding of linear programming <ref> [ Chvatal, 1980 ] </ref> and methods for decomposing and solving large systems [ Lasdon, 1970 ] .
Reference: [ Dantzig and Wolfe, 1960 ] <author> George Dantzig and Philip Wolfe. </author> <title> Decomposition principle for dynamic programs. </title> <journal> Operations Research, </journal> <volume> 8(1) </volume> <pages> 101-111, </pages> <year> 1960. </year>
Reference-contexts: This iterative method is based on a reduction to the methods of Kush-ner and Chen [ Kushner and Chen, 1974 ] that demonstrate how to solve Markov decision processes as linear programs using Dantzig-Wolfe decomposition <ref> [ Dantzig and Wolfe, 1960 ] </ref> . The details of the material presented in this section depend on some understanding of linear programming [ Chvatal, 1980 ] and methods for decomposing and solving large systems [ Lasdon, 1970 ] . <p> in this paper and found in the longer version of the paper borrows heavily from the work in operations research and combinatorial optimization for representing Markov decision processes as linear programs [ D'Epenoux, 1963 ] [ Derman, 1970 ] [ Kush-ner and Kleinman, 1971 ] and decomposing large systems generally <ref> [ Dantzig and Wolfe, 1960 ] </ref> [ Lasdon, 1970 ] and Markov decision processes specifically [ Kushner and Chen, 1974 ] .
Reference: [ Dean and Kanazawa, 1989 ] <author> Thomas Dean and Keiji Kanazawa. </author> <title> A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: A factored state-space representation uses state variables to represent different aspects of the overall state of the system. 1 Compact encodings for stochastic processes can be achieved for many applications using factored state-space representations, where the size of the model is usually logarithmic in the size of the state space <ref> [ Dean and Kanazawa, 1989 ] </ref> . Similarly, policies for large factored state spaces can often be efficiently encoded using decision trees that branch on state variables [ Boutilier et al., 1995 ] .
Reference: [ Dean and Lin, 1995 ] <author> Thomas Dean and Shieu-Hong Lin. </author> <title> Decomposition techniques for planning in stochastic domains. </title> <type> Technical Report CS-95-10, </type> <institution> Brown University Department of Computer Science, </institution> <year> 1995. </year>
Reference-contexts: Section 5 illustrates the method of the iterative approximation and briefly addresses issues concerning convergence, optimality, and complexity. Details are available in a longer version of the paper <ref> [ Dean and Lin, 1995 ] </ref> . 2 Markov Decision Processes Let M = ( X ; A ; p; c) be a Markov decision process with finite state space X , actions A , state transition matrix p, and cost matrix c. <p> The details of the material presented in this section depend on some understanding of linear programming [ Chvatal, 1980 ] and methods for decomposing and solving large systems [ Lasdon, 1970 ] . Rather than assume this understanding, we refer the reader to the longer version of the paper <ref> [ Dean and Lin, 1995 ] </ref> for the details and just sketch the method in the following. 1. <p> Proposition 1 The iterative method described above improves the solution quality on each iteration, and converges to an optimal solution in a finite number of steps. For a proof of this proposition and a more detailed description of the algorithm see the longer version of this paper <ref> [ Dean and Lin, 1995 ] </ref> . <p> Since the discussion is somewhat lengthy and requires some understanding of both Howard's policy iteration [ Howard, 1960 ] and Bellman's value iteration [ Bellman, 1961 ] for solving Markov decision processes, we refer the interested reader to the longer version of the paper <ref> [ Dean and Lin, 1995 ] </ref> . 6 Related Work The related work on abstraction and decomposition is extensive.
Reference: [ Dean et al., 1993 ] <author> Thomas Dean, Leslie Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proceedings AAAI-93, </booktitle> <pages> pages 574-579. </pages> <publisher> AAAI, </publisher> <year> 1993. </year>
Reference-contexts: The approach described in <ref> [ Dean et al., 1993 ] </ref> [ Dean et al., 1995 ] represents a special case of the framework presented here, in which the partition consists of singleton sets for all of the states in the envelope and a set for all the states in the complement of the envelope. 7
Reference: [ Dean et al., 1995 ] <author> Thomas Dean, Leslie Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning under time constraints in stochastic domains. </title> <note> To appear in Artificial Intelligence, </note> <year> 1995. </year>
Reference-contexts: The approach described in [ Dean et al., 1993 ] <ref> [ Dean et al., 1995 ] </ref> represents a special case of the framework presented here, in which the partition consists of singleton sets for all of the states in the envelope and a set for all the states in the complement of the envelope. 7 Conclusion The benefit of decomposition techniques
Reference: [ D'Epenoux, 1963 ] <author> F. D'Epenoux. </author> <title> Sur un probleme de production et de stockage dans l'aleatoire. </title> <journal> Management Science, </journal> <volume> 10 </volume> <pages> 98-108, </pages> <year> 1963. </year>
Reference-contexts: The analysis hinted at in this paper and found in the longer version of the paper borrows heavily from the work in operations research and combinatorial optimization for representing Markov decision processes as linear programs <ref> [ D'Epenoux, 1963 ] </ref> [ Derman, 1970 ] [ Kush-ner and Kleinman, 1971 ] and decomposing large systems generally [ Dantzig and Wolfe, 1960 ] [ Lasdon, 1970 ] and Markov decision processes specifically [ Kushner and Chen, 1974 ] .
Reference: [ Derman, 1970 ] <author> Cyrus Derman. </author> <title> Finite State Markovian Decision Processes. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1970. </year>
Reference-contexts: The abstract policy has the interpretation of providing a global perspective and indicating for each region the best local policy to use. Generally, it is best to set fl very close to one or use an alternative performance criterion such as average expected cost per step <ref> [ Derman, 1970 ] </ref> . The following is an algorithm to construct a global policy using the abstract decision process (P; F ; ; p 0 ; c 0 ). 1. Set and compute R!S for R;S 2 P . 2. <p> The analysis hinted at in this paper and found in the longer version of the paper borrows heavily from the work in operations research and combinatorial optimization for representing Markov decision processes as linear programs [ D'Epenoux, 1963 ] <ref> [ Derman, 1970 ] </ref> [ Kush-ner and Kleinman, 1971 ] and decomposing large systems generally [ Dantzig and Wolfe, 1960 ] [ Lasdon, 1970 ] and Markov decision processes specifically [ Kushner and Chen, 1974 ] .
Reference: [ Fikes and Nilsson, 1971 ] <author> Richard Fikes and Nils J. Nils-son. </author> <title> Strips: A new approach to the application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 189-208, </pages> <year> 1971. </year>
Reference-contexts: Assuming that both the problem (a stochastic process) and the solution (a policy) can be encoded in a compact form, we would like to generate solutions in time bounded by some small factor of the problem and solution size. 1 Propositions representing fluents in STRIPS operators <ref> [ Fikes and Nilsson, 1971 ] </ref> correspond to state variables in a factored state-space representation. three-dimensional space is represented as the union of two-dimensional abstract subspaces shaded dark gray. A factored state-space representation with n boolean state variables represents an n-dimensional state space with O (2 n ) states.
Reference: [ Howard, 1960 ] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1960. </year>
Reference-contexts: Value iteration is guaranteed to converge in the limit to the optimal expected cumulative cost function accounting for an infinite lookahead. Howard's policy iteration <ref> [ Howard, 1960 ] </ref> iterates by first computing the expected cumulative cost function for the current policy and then improving the policy by using this cost function. Policy iteration is guaranteed to converge to the optimal policy in time polynomial in N . <p> We are currently testing these algorithms on a set of benchmark problems. Since the discussion is somewhat lengthy and requires some understanding of both Howard's policy iteration <ref> [ Howard, 1960 ] </ref> and Bellman's value iteration [ Bellman, 1961 ] for solving Markov decision processes, we refer the interested reader to the longer version of the paper [ Dean and Lin, 1995 ] . 6 Related Work The related work on abstraction and decomposition is extensive.
Reference: [ Kaelbling, 1993 ] <author> Leslie Pack Kaelbling. </author> <title> Hierarchical learning in stochastic domains: A preliminary report. </title> <booktitle> In Proceedings Tenth International Conference on Machine Learning, </booktitle> <year> 1993. </year>
Reference-contexts: In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces [ Moore and Atkeson, 1995 ] and stochastic models and discrete state spaces <ref> [ Kaelbling, 1993 ] </ref> . The hierarchical policy construction method described in Section 4 provides an alternative formulation of Kaelbling's hierarchical learning algorithm [ Kaelbling, 1993 ] and suggests how Moore and Atkeson's parti-game algorithm might be extended to handle discrete state spaces. <p> of reinforcement learning, there is work on deterministic action models and continuous state spaces [ Moore and Atkeson, 1995 ] and stochastic models and discrete state spaces <ref> [ Kaelbling, 1993 ] </ref> . The hierarchical policy construction method described in Section 4 provides an alternative formulation of Kaelbling's hierarchical learning algorithm [ Kaelbling, 1993 ] and suggests how Moore and Atkeson's parti-game algorithm might be extended to handle discrete state spaces.
Reference: [ Knoblock, 1991 ] <author> Craig A. Knoblock. </author> <title> Search reduction in hierarchical problem solving. </title> <booktitle> In Proceedings AAAI-91, </booktitle> <pages> pages 686-691. </pages> <publisher> AAAI, </publisher> <year> 1991. </year>
Reference-contexts: In the area planning and search assuming deterministic action models, there is the work on macro operators [ Korf, 1985 ] and hierarchies of state-space operators [ Sacerdoti, 1974 ] <ref> [ Knoblock, 1991 ] </ref> . Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [ Zhong and Wonham, 1990 ] [ Caines and Wang, 1990 ] .
Reference: [ Korf, 1985 ] <author> Richard Korf. Macro-operators: </author> <title> A weak method for learning. </title> <journal> Artificial Intelligence, </journal> <volume> 26 </volume> <pages> 35-77, </pages> <year> 1985. </year>
Reference-contexts: In the area planning and search assuming deterministic action models, there is the work on macro operators <ref> [ Korf, 1985 ] </ref> and hierarchies of state-space operators [ Sacerdoti, 1974 ] [ Knoblock, 1991 ] . Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [ Zhong and Wonham, 1990 ] [ Caines and Wang, 1990 ] .
Reference: [ Kushner and Chen, 1974 ] <author> Harold J. Kushner and Ching-Hui Chen. </author> <title> Decomposition of systems governed by Markov chains. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> AC-19(5):501-507, </volume> <year> 1974. </year>
Reference-contexts: In this section, we focus on a particular iterative method that resolves these three issues. This iterative method is based on a reduction to the methods of Kush-ner and Chen <ref> [ Kushner and Chen, 1974 ] </ref> that demonstrate how to solve Markov decision processes as linear programs using Dantzig-Wolfe decomposition [ Dantzig and Wolfe, 1960 ] . <p> Q is defined by T 0 = U and T i = Kernel (R i ) for 1 i m. The resulting structure induces a star topology that is critical in applying the techniques in <ref> [ Kushner and Chen, 1974 ] </ref> . T 0 = U is called the coupling region; removing the states in T 0 separates the state space into isolated regions, each of which corresponds to a T i , i &gt; 0. <p> Our approach to analysis involves (i) reformulating this iterative method in terms of solving large linear programs, and (ii) applying a reduction to the methods of Kushner and Chen <ref> [ Kushner and Chen, 1974 ] </ref> that solve these large linear programs for Markov decision processes using Dantzig-Wolfe decomposition. <p> the work in operations research and combinatorial optimization for representing Markov decision processes as linear programs [ D'Epenoux, 1963 ] [ Derman, 1970 ] [ Kush-ner and Kleinman, 1971 ] and decomposing large systems generally [ Dantzig and Wolfe, 1960 ] [ Lasdon, 1970 ] and Markov decision processes specifically <ref> [ Kushner and Chen, 1974 ] </ref> .
Reference: [ Kushner and Kleinman, 1971 ] <author> H. J. Kushner and A. J. Kleinman. </author> <title> Mathematical programming and the control of Markov chains. </title> <journal> International Journal on Control, </journal> <volume> 13(5) </volume> <pages> 801-820, </pages> <year> 1971. </year>
Reference: [ Lasdon, 1970 ] <author> Leon S. Lasdon. </author> <title> Optimization Theory for Large Systems. </title> <publisher> Macmillan Company, </publisher> <year> 1970. </year>
Reference-contexts: The details of the material presented in this section depend on some understanding of linear programming [ Chvatal, 1980 ] and methods for decomposing and solving large systems <ref> [ Lasdon, 1970 ] </ref> . Rather than assume this understanding, we refer the reader to the longer version of the paper [ Dean and Lin, 1995 ] for the details and just sketch the method in the following. 1. <p> the longer version of the paper borrows heavily from the work in operations research and combinatorial optimization for representing Markov decision processes as linear programs [ D'Epenoux, 1963 ] [ Derman, 1970 ] [ Kush-ner and Kleinman, 1971 ] and decomposing large systems generally [ Dantzig and Wolfe, 1960 ] <ref> [ Lasdon, 1970 ] </ref> and Markov decision processes specifically [ Kushner and Chen, 1974 ] .
Reference: [ Lin and Dean, 1994 ] <author> Shieu-Hong Lin and Thomas Dean. </author> <title> Exploiting locality in temporal reasoning. </title> <editor> In E. Sandewall and C. Backstrom, editors, </editor> <booktitle> Current Trends in AI Planning, </booktitle> <address> Amsterdam, 1994. </address> <publisher> IOS Press. </publisher>
Reference-contexts: The size of the union of these abstract subspaces is no more than m2 r . The problem of automatically constructing such a partition is not addressed in this paper, but see <ref> [ Lin and Dean, 1994 ] </ref> for some relevant techniques. Figure 2 illustrates how a three-dimensional state space might be represented as the union of two-dimensional abstract subspaces.
Reference: [ Moore and Atkeson, 1995 ] <author> Andrew W. Moore and Christopher G. Atkeson. </author> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state spaces. </title> <note> To appear in Machine Learning, </note> <year> 1995. </year>
Reference-contexts: Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [ Zhong and Wonham, 1990 ] [ Caines and Wang, 1990 ] . In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces <ref> [ Moore and Atkeson, 1995 ] </ref> and stochastic models and discrete state spaces [ Kaelbling, 1993 ] .
Reference: [ M.S. Bazaraa, 1990 ] <author> H. D. Sherali M.S. Bazaraa, J. J. Jarvis. </author> <title> Linear Programming and Network Flows. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1990. </year>
Reference: [ Papadimitriou and Tsitsiklis, 1987 ] <author> Christos H. Pa-padimitriou and John N. Tsitsiklis. </author> <title> The complexity of Markov chain decision processes. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12(3) </volume> <pages> 441-450, </pages> <year> 1987. </year>
Reference-contexts: Figure 2 illustrates how a three-dimensional state space might be represented as the union of two-dimensional abstract subspaces. There exist methods for computing policies that are polynomial in the size of the state and action spaces <ref> [ Papadimitriou and Tsitsiklis, 1987 ] </ref> [ Puterman, 1994 ] , but these methods are impractical for large state spaces (e.g., &gt; 10 6 states, given 20 state variables).
Reference: [ Puterman, 1994 ] <author> Martin L. Puterman. </author> <title> Markov Decision Processes. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Figure 2 illustrates how a three-dimensional state space might be represented as the union of two-dimensional abstract subspaces. There exist methods for computing policies that are polynomial in the size of the state and action spaces [ Papadimitriou and Tsitsiklis, 1987 ] <ref> [ Puterman, 1994 ] </ref> , but these methods are impractical for large state spaces (e.g., &gt; 10 6 states, given 20 state variables). <p> Howard's policy iteration [ Howard, 1960 ] iterates by first computing the expected cumulative cost function for the current policy and then improving the policy by using this cost function. Policy iteration is guaranteed to converge to the optimal policy in time polynomial in N . Puterman <ref> [ Puterman, 1994 ] </ref> provides an up-to-date overview of algorithms for solving Markov decision processes. In iterative methods, it is often useful to be able to compute a bound on the difference between the value of the current solution and the optimum.
Reference: [ Sacerdoti, 1974 ] <author> Earl Sacerdoti. </author> <title> Planning in a hierarchy of abstraction spaces. </title> <journal> Artificial Intelligence, </journal> <volume> 7 </volume> <pages> 231-272, </pages> <year> 1974. </year>
Reference-contexts: In the area planning and search assuming deterministic action models, there is the work on macro operators [ Korf, 1985 ] and hierarchies of state-space operators <ref> [ Sacerdoti, 1974 ] </ref> [ Knoblock, 1991 ] . Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines [ Zhong and Wonham, 1990 ] [ Caines and Wang, 1990 ] .
Reference: [ Zhong and Wonham, 1990 ] <author> H. Zhong and W. M. Won-ham. </author> <title> On the consistency of hierarchical supervision in discrete-event systems. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 35(10) </volume> <pages> 1125-1134, </pages> <year> 1990. </year>
Reference-contexts: Closely related is the work on decomposing discrete event systems modeled as (deterministic) finite state machines <ref> [ Zhong and Wonham, 1990 ] </ref> [ Caines and Wang, 1990 ] . In the area of reinforcement learning, there is work on deterministic action models and continuous state spaces [ Moore and Atkeson, 1995 ] and stochastic models and discrete state spaces [ Kaelbling, 1993 ] .
References-found: 26

