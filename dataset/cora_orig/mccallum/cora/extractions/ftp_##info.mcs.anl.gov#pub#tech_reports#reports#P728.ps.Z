URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P728.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts98.htm
Root-URL: http://www.mcs.anl.gov
Title: High-Performance Computational Chemistry: Hartree-Fock Electronic Structure Calculations on Massively Parallel Processors  
Author: Jeffrey L. Tilson, Mike Minkoff, Albert F. Wagner, Ron Shepard, and Paul Sutton Robert J. Harrison, Ricky A. Kendall and Adrian T. Wong 
Date: October 7, 1998  
Address: Argonne, IL 60439  Richland,WA 99352  
Affiliation: Argonne National Laboratory  Environmental and Molecular Sciences Laboratory Pacific Northwest National Laboratory  
Abstract: The parallel performance of the NWChem version 1.2ff parallel direct-SCF code has been characterized on five massively parallel supercomputers (IBM SP, Kendall Square KSR-2, Cray T3D and T3E, and Intel Touchstone Delta) using single-point energy calculations on seven molecules of varying size (up to 389 atoms) and composition (first-row atoms, halogens, and transition metals). We compare the performance using both replicated-data and distributed-data algorithms and the original McMurchie-Davidson and recently incorporated TEXAS integrals packages. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Almlof, J.; Faegri, K.; Korsell, K., </author> <title> Principles for a Direct SCF Approach to LCAO-MO Ab-Initio Calculations. </title> <journal> J. Comp. Chem. 1982, </journal> <volume> 3, </volume> <pages> 385-399. </pages>
Reference-contexts: 1 Introduction In recent years, there has been considerable effort in adapting computational chemistry applications to parallel computer architectures, including many parallel implementations of the direct self-consistent-field <ref> [1] </ref> (direct-SCF) electronic structure method as described in a recent review [2]. Initial work in this area greatly reduced the direct-SCF time to solution by utilizing efficient parallelism in the integral generation step; one common limitation, however, was the use of replicated-data structures.
Reference: [2] <author> Harrison, R. J.; Shepard, R. </author> <title> Ab Initio Molecular Electronic Structure on Parallel Computers. Annu. </title> <journal> Rev. Chem. 1994, </journal> <volume> 45, </volume> <pages> 623-658. </pages>
Reference-contexts: 1 Introduction In recent years, there has been considerable effort in adapting computational chemistry applications to parallel computer architectures, including many parallel implementations of the direct self-consistent-field [1] (direct-SCF) electronic structure method as described in a recent review <ref> [2] </ref>. Initial work in this area greatly reduced the direct-SCF time to solution by utilizing efficient parallelism in the integral generation step; one common limitation, however, was the use of replicated-data structures. On modestly parallel computers, this limitation is of little practical consequence.
Reference: [3] <author> Colvin, M. E.; Janssen, C. L.; Whiteside, R. A.; Tong, C. H. </author> <title> Parallel Direct SCF for Large-Scale Calculations. </title> <journal> Theor. Chim. Acta. 1993, </journal> <volume> 84, </volume> <pages> 301-314. </pages>
Reference-contexts: This restriction is very severe given that the memory of a single node may be several orders of magnitude smaller than the aggregate memory. More recent efforts at constructing parallel direct-SCF codes, distribute all sizable data structures. Colvin et al. <ref> [3] </ref> were the first to demonstrate such a code. Based on a systolic loop, their algorithm is memory efficient but requires computing integrals up to three separate times each iteration and can suffer from load imbalance.
Reference: [4] <author> Furlani, T. R.; King, H. F. </author> <title> Implementation of a Parallel Direct SCF Algorithm on Distributed Memory Computers. </title> <journal> J. Comp. Chem. 1995, </journal> <volume> 16, </volume> <pages> 91-104. </pages>
Reference-contexts: Colvin et al. [3] were the first to demonstrate such a code. Based on a systolic loop, their algorithm is memory efficient but requires computing integrals up to three separate times each iteration and can suffer from load imbalance. Furlani and King <ref> [4] </ref> used dynamic blocking and distributed-data techniques to create a parallel direct-SCF code that efficiently computes the integrals. They used dynamic load balancing and obtained performance results on up to 16 processors of the Intel IPSC/2. We have published several algorithms for fully distributed direct-SCF calculations [5]. <p> Improvements in NWChem (version 1.2ff) permitted far better performance on a wider variety of molecules as compared with NWChem (version 1.0ff). These improvements include * dynamic blocking in the distributed-data Fock matrix build similar to that of Furlani and King <ref> [4] </ref>, * reduced linear algebra overhead, which improves scalability for small systems, and * more robust convergence resulting from tightening of tolerances, an improved ap proximation to the matrix exponential, and miscellaneous code optimizations. We have also included a significantly faster integral generation scheme.
Reference: [5] <author> Foster, I. T.; Tilson, J. L.; Shepard, R.; Wagner, A. F.; Harrison, R. J.; Kendall, R. A.; Littlefield, R. L. </author> <title> Toward High-Performance Computational Chemistry: I. Scalable Fock Matrix Construction Algorithms. </title> <journal> J. Comp. Chem., 1995, </journal> <volume> 17, </volume> <pages> 109-123. </pages>
Reference-contexts: Furlani and King [4] used dynamic blocking and distributed-data techniques to create a parallel direct-SCF code that efficiently computes the integrals. They used dynamic load balancing and obtained performance results on up to 16 processors of the Intel IPSC/2. We have published several algorithms for fully distributed direct-SCF calculations <ref> [5] </ref>. The simplest, an atom-blocked algorithm, was implemented [6] in an early version of NWChem (version 1.0ff) using a simple dynamic task-scheduling technique and a distributed-data toolkit called Global Arrays (GA) [7, 8]. Generation of integral blocks and all significant linear algebra operations are performed in parallel.
Reference: [6] <author> Harrison, R. J.; Guest, M. F.; Kendall, R. A.; Bernholdt, D. E.; Wong, A. T.; Stave, M.; Anchell, J.; Hess, A. C.; Littlefield, R. L.; Fann, G. L.; Nieplocha, J.; Thomas, G. S.; Elwood, D.; Tilson, J. L. Shepard, R.; Wagner, A. F.; Foster, I. T.; Lusk, E.; Stevens, R. </author> <title> Toward High Performance Computational Chemistry: II. A Scalable SCF Program. </title> <journal> J. Comp. Chem., 1995, </journal> <volume> 17, </volume> <pages> 124-132. </pages>
Reference-contexts: They used dynamic load balancing and obtained performance results on up to 16 processors of the Intel IPSC/2. We have published several algorithms for fully distributed direct-SCF calculations [5]. The simplest, an atom-blocked algorithm, was implemented <ref> [6] </ref> in an early version of NWChem (version 1.0ff) using a simple dynamic task-scheduling technique and a distributed-data toolkit called Global Arrays (GA) [7, 8]. Generation of integral blocks and all significant linear algebra operations are performed in parallel. <p> Section 2 describes the algorithm; Section 3 compares the specific MPP machines; Section 4 describes the benchmark molecules and shows the results of the calculations; and Section 5 summarizes results. 2 2 NWChem SCF Algorithm The distributed-data Fock matrix construction algorithm within NWChem (version 1.2ff) is essentially as described in <ref> [6] </ref> with several additional algorithmic improvements and an improved version of the quadratically convergent algorithm described in reference [9]. A replicated-data model is also provided within the NWChem package and is useful for small to medium-sized calculations. <p> In Figure 5, T (P) is displayed for only two molecules, Co-cat and biphenyl. These calculations are for NWChem (version 1.0ff), an earlier version first discussed in <ref> [6] </ref> that was the only version available to run on the Intel DELTA and the CRAY T3D. Results for these two machines are shown in the figure along with results for the SP1. All calculations in the figure were done with the McMD integrals package available in version 1.0ff. <p> The results demonstrate that the general scalability of NWChem and its distributed nature have not come at the price of rapid time to solution. 10 5 Conclusions The performance of a fully distributed, parallel direct-SCF algorithm <ref> [6] </ref> has been characterized on five MPP computers using single-point energy calculations on seven molecules of widely varying size and composition.
Reference: [7] <author> Harrison, R. </author> <title> Moving beyond Message Passing. Experiments with a Distributed-Data Model. Theo. </title> <journal> Chim. Acta. 1993, </journal> <volume> 84, </volume> <pages> 363-375. </pages>
Reference-contexts: We have published several algorithms for fully distributed direct-SCF calculations [5]. The simplest, an atom-blocked algorithm, was implemented [6] in an early version of NWChem (version 1.0ff) using a simple dynamic task-scheduling technique and a distributed-data toolkit called Global Arrays (GA) <ref> [7, 8] </ref>. Generation of integral blocks and all significant linear algebra operations are performed in parallel. Data communications within the GA library use the most efficient mechanism available for each target machine.
Reference: [8] <author> Nieplocha, J.; Harrison, R. J.; Littlefield, R. J. </author> <title> Global Arrays: A Portable "Shared-Memory" Programming Model for Distributed Memory Computers. </title> <institution> Institute of Electrical and Electronics Engineers and Association for Computing Machinery IEEE Computer Society, </institution> <address> Los Alamitos, Supercomputing'94, </address> <year> 1994 </year>
Reference-contexts: We have published several algorithms for fully distributed direct-SCF calculations [5]. The simplest, an atom-blocked algorithm, was implemented [6] in an early version of NWChem (version 1.0ff) using a simple dynamic task-scheduling technique and a distributed-data toolkit called Global Arrays (GA) <ref> [7, 8] </ref>. Generation of integral blocks and all significant linear algebra operations are performed in parallel. Data communications within the GA library use the most efficient mechanism available for each target machine.
Reference: [9] <author> Wong, A. T.; Harrison, R. J. </author> <title> Approaches to Large-scale Parallel Self-Consistent Field Calculations. </title> <journal> J. Comp. Chem. 1995, </journal> <volume> 16, </volume> <pages> 1291-1300. </pages>
Reference-contexts: and shows the results of the calculations; and Section 5 summarizes results. 2 2 NWChem SCF Algorithm The distributed-data Fock matrix construction algorithm within NWChem (version 1.2ff) is essentially as described in [6] with several additional algorithmic improvements and an improved version of the quadratically convergent algorithm described in reference <ref> [9] </ref>. A replicated-data model is also provided within the NWChem package and is useful for small to medium-sized calculations. Improvements in NWChem (version 1.2ff) permitted far better performance on a wider variety of molecules as compared with NWChem (version 1.0ff). <p> To update the molecular orbital coefficients, NWChem avoids the use of diagonalization, which historically has been inefficient on massively parallel computers [14], by using instead a preconditioned conjugate-gradient (PCG) algorithm <ref> [9] </ref> that is a refinement of an approach suggested in this context by one of us [15] and related to the SCF algorithm of Bacskay [16].
Reference: [10] <author> Tilson, J. L. </author> <title> Massively Parallel Self-Consistent-Field Calculations. </title> <journal> I&EC RESEARCH, 1995, </journal> <volume> 34, </volume> <pages> 4161-4165. </pages>
Reference-contexts: However, the time to solution (see below) for our ZSM-5 zeolite fragment test case (389 atoms, 2053 function, STO-3G basis, no symmetry) was reduced by nearly a factor of seven <ref> [10] </ref>. This exceptional reduction resulted because the minimal basis set and high latency of the IBM SPn were inconsistent with obtaining optimal performance with the old algorithm.
Reference: [11] <author> McMurchie, L. E.; Davidson, E. R. </author> <title> One- and Two-Electron Integrals over Cartesian Gaussian Functions. </title> <journal> J. Comp. Phys., 1978, </journal> <volume> 26, </volume> <pages> 218-231. </pages>
Reference-contexts: The calculation of integrals within NWChem may be performed in either a segmented or generalized contracted basis using a McMurchie-Davidson (McMD) algorithm <ref> [11] </ref>, unless only s, p, or L shells are involved, in which case the rotated-axes algorithm of Pople et al. [18] is used.
Reference: [12] <author> Dongarra, J. J. </author> <title> Linpack Benchmark, </title> <note> http://performance.netlib.org/performance/html/linpack-peak.data.col0.html, 1998. </note>
Reference-contexts: Five characteristics of these specific computers are relevant to this study. 1. the number of nodes, 2. the computational power per node in billions (10 9 ) of floating point operations per second (GFLOPS=s) estimated from the LINPACK report <ref> [12] </ref>, 3. the aggregate memory of the computer in gigabytes (GB), 4. the latency (measured in sec) of remote memory reference, and 5. the bandwidth (measured in megabytes per second, MB/s) for remote memory ref erence.
Reference: [13] <author> Dowd, K. </author> <title> In High Performance Computing., </title> <publisher> O'Reilly and Associates. Inc. </publisher> <year> 1995. </year>
Reference-contexts: Degradation from peak performance in serial computers is generally a complicated relationship between the number and type of arithmetic operations per clock cycle and the numbers and types of memory references (i.e., local cache, local RAM, disk access, etc.) <ref> [13] </ref>. The parallel environment can be understood in much the same way by modeling the additional accesses to nonlocal memory and the amount of load imbalance during the calculation.
Reference: [14] <author> Littlefield, R.; Maschhoff, K. </author> <title> Investigating The Performance of Parallel Eigensolvers for Large Processor Counts. </title> <journal> Theor. Chim. Acta 1993, </journal> <volume> 84, </volume> <pages> 457-473. 12 </pages>
Reference-contexts: To update the molecular orbital coefficients, NWChem avoids the use of diagonalization, which historically has been inefficient on massively parallel computers <ref> [14] </ref>, by using instead a preconditioned conjugate-gradient (PCG) algorithm [9] that is a refinement of an approach suggested in this context by one of us [15] and related to the SCF algorithm of Bacskay [16].
Reference: [15] <author> Shepard, R. </author> <title> Elimination of the Diagonalization Bottleneck in Parallel Direct-SCF Methods. </title> <journal> Theor. Chim. Acta 1993, </journal> <volume> 84, </volume> <pages> 343-351. </pages>
Reference-contexts: To update the molecular orbital coefficients, NWChem avoids the use of diagonalization, which historically has been inefficient on massively parallel computers [14], by using instead a preconditioned conjugate-gradient (PCG) algorithm [9] that is a refinement of an approach suggested in this context by one of us <ref> [15] </ref> and related to the SCF algorithm of Bacskay [16].
Reference: [16] <author> Bacskay, G. B. </author> <title> A Quadratically Convergent Hartree-Fock (QC-SCF) Method. Application to Closed Shell Systems. </title> <journal> Chem. Phys. 1981, </journal> <volume> 61, </volume> <pages> 385-404. </pages>
Reference-contexts: coefficients, NWChem avoids the use of diagonalization, which historically has been inefficient on massively parallel computers [14], by using instead a preconditioned conjugate-gradient (PCG) algorithm [9] that is a refinement of an approach suggested in this context by one of us [15] and related to the SCF algorithm of Bacskay <ref> [16] </ref>. In the early iterations, preconditioning is performed with a one-electron approximation to the orbital Hessian (which is related to an approximate diagonaliztion scheme), but once the maximum element of the orbital gradient (or off-diagonal Fock-matrix element) falls below some threshold, preconditioning is performed with the exact orbital Hessian.
Reference: [17] <author> Fann, G. I.; Littlefield, R. J.; Elwood, D. M. </author> <title> Performance of a Fully Parallel Dense Real Symmetric Eigensolver in Quantum Chemistry Applications., In High Performance Computing 1995, Grand Challenges in Computer Simulation. </title> <editor> Tentner, A., ed.; </editor> <booktitle> The Society for Computer Simulation, </booktitle> <year> 1995, </year> <pages> pp. 329-336. </pages>
Reference-contexts: The total number of explicit diag-onalizations is typically three. The diagonalization software used in NWChem, called PeIGS <ref> [17] </ref>, executes in parallel and was developed, in part, for this project. The performance of parallel diagonalization has improved greatly in the timeframe of our research project, and elimination of the diagonalization step is no longer a sufficient justification 5 for the additional complexity of the PCG algorithm.
Reference: [18] <author> Pople, J. A.; Hehre, W. J. </author> <title> Computation of Electron Repulsion Integrals Involving Contracted Gaussian Basis Functions. </title> <journal> J. Comp. Phys. 1978, </journal> <volume> 27, </volume> <pages> 161-168. </pages>
Reference-contexts: The calculation of integrals within NWChem may be performed in either a segmented or generalized contracted basis using a McMurchie-Davidson (McMD) algorithm [11], unless only s, p, or L shells are involved, in which case the rotated-axes algorithm of Pople et al. <ref> [18] </ref> is used. Recently the TEXAS integral package [19] has been incorporated into NWChem, resulting in a marked decrease in the overall time to solution, and permitting analysis of the parallel performance under conditions of much greater relative communications cost. Several starting guess options are available for NWChem.
Reference: [19] <author> Wolinski, K.; Pulay, P.;Hamilton, T. </author> <title> Integral package from TEXAS 95 extended for use within NWChem including analytic gradients by Harrison, </title> <editor> R. J.; Kendall, R. A.; and Wolinski, K. </editor> <year> 1997. </year>
Reference-contexts: Recently the TEXAS integral package <ref> [19] </ref> has been incorporated into NWChem, resulting in a marked decrease in the overall time to solution, and permitting analysis of the parallel performance under conditions of much greater relative communications cost. Several starting guess options are available for NWChem.
Reference: [20] <author> Pulay, P. </author> <title> Convergence Acceleration of Iterative Sequences. The Case of SCF Iteration. </title> <journal> Chem. Phys. Lett. 1980, </journal> <volume> 15, </volume> <month> 393-398. </month> <title> 13 14 15 with McMD and TEXAS integrals packages on the SP1. TEXAS results are indicated with dashed lines. 16 with the McMD integrals package on the SP1 and T3E. T3E results are indicated with dashed lines. 17 McMD integrals package on the SP1, T3D, and DELTA. </title> <type> 18 </type>
References-found: 20

