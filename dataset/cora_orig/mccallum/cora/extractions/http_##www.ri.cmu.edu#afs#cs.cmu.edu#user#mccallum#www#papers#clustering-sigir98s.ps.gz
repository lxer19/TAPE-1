URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/mccallum/www/papers/clustering-sigir98s.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/mccallum/www/index.html
Root-URL: 
Title: Distributional Clustering of Words for Text Classification  
Author: L. Douglas Baker yz Andrew Kachites McCallum yz 
Address: Pittsburgh, PA 15213  4616 Henry Street Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  Justsystem Pittsburgh Research Center  
Abstract: This paper applies Distributional Clustering (Pereira et al. 1993) to document classification. The approach clusters words into groups based on the distribution of class labels associated with each word. Thus, unlike some other unsupervised dimensionality-reduction techniques, such as Latent Semantic Indexing, we are able to compress the feature space much more aggressively, while still maintaining high document classification accuracy. Experimental results obtained on three real-world data sets show that we can reduce the feature dimensionality by three orders of magnitude and lose only 2% accuracy|significantly better than Latent Semantic Indexing (Deerwester et al. 1990), class-based clustering (Brown et al. 1992), feature selection by mutual information (Yang and Pederson 1997), or Markov-blanket-based feature selection (Koller and Sahami 1996). We also show that less aggressive clustering sometimes results in improved classification accuracy over classification without clustering. 
Abstract-found: 1
Intro-found: 1
Reference: <author> P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della Pietra, and J. C. Lai. </author> <title> Class-based n-gram models of natural language. </title> <journal> Computational Linguistics, </journal> <volume> 18(4) </volume> <pages> 467-479, </pages> <year> 1992. </year>
Reference-contexts: We present experimental results on three real-world text corpora, including newswire stories, UseNet articles and Web pages. Results show that Distributional Clustering can reduce the feature dimensionality by three orders of magnitude, and lose only 2% accuracy. This performance is significantly better than class-based clustering using mutual information <ref> (Brown et al. 1992) </ref>, clustering by Latent Semantic Indexing (Deerwester et al. 1990), feature selection by information gain (Yang and Pederson 1997) and feature selection by Markov-blanket (Koller and Sahami 1996). On one of the data sets we show that clustering increases classification accuracy. <p> Chi2 (Liu and Setiono 1995) is an extension of ChiMerge for use as a feature selector of numeric attributes. Liu and Setiono observe that if all the values of any attribute are clustered together, then that value is irrelevant to the classification task and can be removed. Class-based clustering <ref> (Brown et al. 1992) </ref> uses an agglomerative, hard clustering algorithm where the clustering criterion is designed to maximize the overall average mutual information between clusters and the class variable. <p> Experimental Results This section provides empirical evidence that Distributional Clustering is able to aggressively reduce the number of features while maintaining high classification accuracy. At equal feature dimensionality, it achieves significantly higher accuracy than four other feature clustering and feature selection algorithms: Latent Semantic Indexing (Dumais 1995), class-based clustering <ref> (Brown et al. 1992) </ref>, feature selection by mutual information with the class variable (Yang and Ped-erson 1997) and feature selection by a Markov-blanket method (Koller and Sahami 1996). The 20 Newsgroups data set, collected by Ken Lang, contains about 20,000 articles evenly divided among 20 UseNet discussion groups (Joachims 1997).
Reference: <author> Thomas Cover and Peter Hart. </author> <title> Nearest neighbor pattern classification. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 13(1) </volume> <pages> 21-27, </pages> <year> 1967. </year>
Reference: <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley, </publisher> <year> 1991. </year>
Reference-contexts: Create a new cluster consisting of the next word from the sorted list. Table 2: The Algorithm P (W jd i ) and the distribution of words in the class P (W jc j ) <ref> (Cover and Thomas 1991) </ref>, where W is a random variable over words.
Reference: <author> M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. Mitchell, K. Nigam, and S. Slattery. </author> <title> Learning to extract symbolic knowledge from the World Wide Web. </title> <type> Technical report, </type> <institution> School of Computer Science, </institution> <address> CMU, </address> <year> 1998. </year>
Reference-contexts: As the amount of documents and number of users rise, automatic document categorization becomes an increasingly important tool for helping people organize this vast amount of data. Statistical document classification algorithms have been applied to categorizing newsfeeds (Joachims 1997), classifying Web pages <ref> (Craven et al. 1998) </ref>, sorting electronic mail (Lewis and Knowles 1997) and learning the interests of users (Lang 1995). In this paper we cluster words into groups specifically for the benefit of document classification.
Reference: <author> Ido Dagan, Fernando Pereira, and Lillian Lee. </author> <title> Similarity-based estimation of word cooccurrence probabilities. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <year> 1994. </year>
Reference: <author> S. C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(6) </volume> <pages> 391-407, </pages> <year> 1990. </year>
Reference-contexts: Results show that Distributional Clustering can reduce the feature dimensionality by three orders of magnitude, and lose only 2% accuracy. This performance is significantly better than class-based clustering using mutual information (Brown et al. 1992), clustering by Latent Semantic Indexing <ref> (Deerwester et al. 1990) </ref>, feature selection by information gain (Yang and Pederson 1997) and feature selection by Markov-blanket (Koller and Sahami 1996). On one of the data sets we show that clustering increases classification accuracy. We hypothesize why this did not happen in more cases, and discuss possible future improvements. <p> Koller and Sahami present a Markov-blanket-based feature selection algorithm that aims to address exactly this (Koller and Sahami 1996). Their technique is based on the same principles as Distributional Clustering|it examines P (Cjw t ), and tries to preserve the proper C distribution. Latent Semantic Indexing <ref> (Deerwester et al. 1990) </ref> is an unsupervised dimensionality reduction technique for information retrieval that explicitly accounts for the dependencies between words. In brief, it applies Principle Component Analysis to documents represented as word vectors. Dumais applies it to text classification (Dumais 1995).
Reference: <author> P. Domingos and M. Pazzani. </author> <title> Beyond independence: Conditions for the optimality of the simple bayesian classifier. </title> <journal> Machine Learning, </journal> <volume> 29 </volume> <pages> 103-130, </pages> <year> 1997. </year>
Reference-contexts: Domingos and Pazzani discuss why the violation of the word independence assumption does little damage to classification accuracy <ref> (Domingos and Paz-zani 1997) </ref>. Measuring Word Similarity for Distributional Clustering Now we address the question of how to cluster words in the context of our generative model and naive Bayes.
Reference: <author> Susan T. Dumais. </author> <title> Using lsi for information filtering: Trec-3 experiments. </title> <type> Technical Report 500-225, </type> <institution> National Institute of Standards and Technology, </institution> <year> 1995. </year> <title> Thorsten Joachims. A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization. </title> <booktitle> In International Conference on Machine Learning (ICML), </booktitle> <year> 1997. </year>
Reference: <author> R. Kerber. Chimerge: </author> <title> Discretization of numeric attributes. </title> <booktitle> In Proceedings of AAAI-92, </booktitle> <year> 1992. </year>
Reference-contexts: We have used larger data sets with more prevalent sparseness and fewer class labels. ChiMerge <ref> (Kerber 1992) </ref> uses a form of Distributional Clustering to discretize numeric attributes for subsequent classification. It is an agglomerative, hard clustering algorithm that uses the 2 statistic as the the similarity metric. We have also tried 2 in our experiments and found that the KL divergence average yields better performance.
Reference: <author> D. Koller and M. Sahami. </author> <title> Toward optimal feature selection. </title> <booktitle> In Proceedings of ICML-96, </booktitle> <year> 1996. </year>
Reference-contexts: This performance is significantly better than class-based clustering using mutual information (Brown et al. 1992), clustering by Latent Semantic Indexing (Deerwester et al. 1990), feature selection by information gain (Yang and Pederson 1997) and feature selection by Markov-blanket <ref> (Koller and Sahami 1996) </ref>. On one of the data sets we show that clustering increases classification accuracy. We hypothesize why this did not happen in more cases, and discuss possible future improvements. <p> However, mutual information between words and classes does not capture dependencies between words. Koller and Sahami present a Markov-blanket-based feature selection algorithm that aims to address exactly this <ref> (Koller and Sahami 1996) </ref>. Their technique is based on the same principles as Distributional Clustering|it examines P (Cjw t ), and tries to preserve the proper C distribution. <p> At equal feature dimensionality, it achieves significantly higher accuracy than four other feature clustering and feature selection algorithms: Latent Semantic Indexing (Dumais 1995), class-based clustering (Brown et al. 1992), feature selection by mutual information with the class variable (Yang and Ped-erson 1997) and feature selection by a Markov-blanket method <ref> (Koller and Sahami 1996) </ref>. The 20 Newsgroups data set, collected by Ken Lang, contains about 20,000 articles evenly divided among 20 UseNet discussion groups (Joachims 1997). Several of the topic classes are quite confusable: four of them are about computers; three discuss religion.
Reference: <author> Ken Lang. Newsweeder: </author> <title> Learning to filter netnews. </title> <booktitle> In International Conference on Machine Learning (ICML), </booktitle> <pages> pages 331-339, </pages> <year> 1995. </year>
Reference: <author> Lillian Lee. </author> <title> Similarity-Based Approaches to Natural Language Processing. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1997. </year> <note> (also Technical Report TR-11-97). </note>
Reference: <author> David D. Lewis and Kimberly A. Knowles. </author> <title> Threading electronic mail: A preliminary study. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 33(2) </volume> <pages> 209-217, </pages> <year> 1997. </year>
Reference: <author> David Lewis and Marc Ringuette. </author> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Third Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 81-93, </pages> <year> 1994. </year>
Reference: <author> H. Liu and R. Setiono. Chi2: </author> <title> Feature selection and discretization of numeric attributes. </title> <booktitle> In Proceedings of 7th IEEE Int'l Conference on Tools with Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference-contexts: It is an agglomerative, hard clustering algorithm that uses the 2 statistic as the the similarity metric. We have also tried 2 in our experiments and found that the KL divergence average yields better performance. Chi2 <ref> (Liu and Setiono 1995) </ref> is an extension of ChiMerge for use as a feature selector of numeric attributes. Liu and Setiono observe that if all the values of any attribute are clustered together, then that value is irrelevant to the classification task and can be removed.
Reference: <author> Fernando Pereira, Naftali Tishby, and Lillian Lee. </author> <title> Distributional clustering of english words. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 183-90, </pages> <year> 1993. </year>
Reference: <author> V. Vapnik. </author> <title> Estimations of dependences based on statistical data. </title> <publisher> Springer Publisher, </publisher> <year> 1982. </year>
Reference-contexts: We can now calculate Bayes optimal estimates of all these parameters from the training data. The estimates consist of straightforward counting of events, supplemented by simple `smoothing' that gives uniform priors by priming each estimate with a count of one <ref> (Vapnik 1982) </ref>.
Reference: <author> Yiming Yang and Jan Pederson. </author> <title> Feature selection in statistical learning of text categorization. </title> <booktitle> In ICML-97, </booktitle> <pages> pages 412-420, </pages> <year> 1997. </year>
References-found: 18

