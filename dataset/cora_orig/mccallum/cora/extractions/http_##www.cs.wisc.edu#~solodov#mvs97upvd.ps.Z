URL: http://www.cs.wisc.edu/~solodov/mvs97upvd.ps.Z
Refering-URL: http://www.cs.wisc.edu/~solodov/solodov.html
Root-URL: 
Title: New Inexact Parallel Variable Distribution Algorithms  Editor:  
Author: MICHAEL V. SOLODOV Estrada Dona Castorina , Jardim Botanico, 
Keyword: Parallel optimization, load balancing, unconstrained minimization, linear convergence  
Address: RJ, CEP 22460-320, Brazil  
Affiliation: Instituto de Matematica Pura e Aplicada,  Rio de Janeiro,  
Note: Computational Optimization and Applications,  c 1997 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Email: solodov@impa.br  
Date: 7, 165-182 (1997)  Received June 26, 1995; Revised October 24, 1995  
Abstract: We consider the recently proposed parallel variable distribution (PVD) algorithm of Ferris and Mangasarian [4] for solving optimization problems in which the variables are distributed among p processors. Each processor has the primary responsibility for updating its block of variables while allowing the remaining "secondary" variables to change in a restricted fashion along some easily computable directions. We propose useful generalizations that consist, for the general unconstrained case, of replacing exact global solution of the subproblems by a certain natural sufficient descent condition, and, for the convex case, of inexact subproblem solution in the PVD algorithm. These modifications are the key features of the algorithm that has not been analyzed before. The proposed modified algorithms are more practical and make it easier to achieve good load balancing among the parallel processors. We present a general framework for the analysis of this class of algorithms and derive some new and improved linear convergence results for problems with weak sharp minima of order 2 and strongly convex problems. We also show that nonmonotone synchronization schemes are admissible, which further improves flexibility of PVD approach. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D.P. Bertsekas and J.N. Tsitsiklis. </author> <title> Parallel and Distributed Computation. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year> <month> 181 </month>
Reference-contexts: The presence of this term allows for a change in "secondary" variables. This makes PVD fundamentally different from the block Jacobi <ref> [1] </ref>, coordinate descent [20] and parallel gradient distribution algorithms [10]. The directions D i l are typically easily computable steepest descent or quasi-Newton directions in the space of the corresponding variables. The "forget-me-not" approach improves robustness and accelerates convergence of the algorithm and is the key to its success. <p> The modified algorithms present a flexible framework and make it easier to achieve good load balancing among the parallel processors. New and improved linear convergence results were derived for strongly convex problems and problems with weak sharp minima of order 2. A study of partially asynchronous distributed algorithms <ref> [1] </ref> that make use of PVD approach can be an interesting subject of future research.
Reference: 2. <author> J.V. Burke and M.C. Ferris. </author> <title> Weak sharp minima in mathematical programming. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 31(5) </volume> <pages> 1340-1359, </pages> <year> 1993. </year>
Reference-contexts: We also derive some new convergence results for weakly sharp problems of order 2 (see Definition below). This class of problems can be viewed as a generalization of strongly convex problems and a certain unconstrained smooth analogue of weak sharp minima <ref> [2] </ref>. We begin by imposing a natural sufficient descent condition on an algorithm (Algorithm A below) used to solve the subproblems (2) generated by the PVD Algorithm 1. Algorithm A. <p> The class of problems with weak sharp minima of order 2 can be thought of as a certain unconstrained smooth analogue of weak sharp minima (of order 1) [16], <ref> [2] </ref>. Note that it subsumes strongly convex programs. Let f () be strongly convex with modulus 2. Then its unique optimal point x is globally (with * = 1) weakly sharp of order 2. This can be easily verified as follows.
Reference: 3. <editor> R.W. Cottle, F. Giannessi, and J.-L. Lions (editors). </editor> <title> Variational Inequalities and Complementarity Problems : Theory and Applications. </title> <address> Whiley, New York, </address> <year> 1980. </year>
Reference-contexts: Remark. A practically important example of weak sharp minima of order 2 is provided by the implicit Lagrangian reformulation [12] of the nonlinear complementarity problem. Consider the following nonlinear complementarity problem <ref> [3] </ref>, [15] (NCP) of finding an x 2 &lt; n such that 180 where F : &lt; n ! &lt; n is a continuously differentiable mapping.
Reference: 4. <author> M.C. Ferris and O.L. Mangasarian. </author> <title> Parallel variable distribution. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4(4) </volume> <pages> 815-832, </pages> <year> 1994. </year>
Reference-contexts: 1. Introduction We consider the general unconstrained optimization problem min f (x); (1) where f : &lt; n ! &lt;. We first state the original PVD algorithm <ref> [4] </ref>. Let x 2 &lt; n be partitioned into p blocks x 1 ; : : : ; x p , such that x l 2 &lt; n l , P p l=1 n l = n. These blocks of variables are then distributed among p parallel processors. <p> In <ref> [4] </ref> it was shown that every accumulation point of the PVD iterates is a stationary point of f () if an exact global solution to subproblems (2) is computed at every iteration. <p> We note that the original requirement of exact subproblem solution is also undesirable. In Section 2 we describe an algorithm with inexact subproblem solution in the convex case and derive a sharper linear convergence result than the one given in <ref> [4] </ref>. We emphasize that the sufficient descent and inexact subproblem solution approaches provide a flexible framework that allows for effective load balancing among the parallel processors. In Section 3 we also exhibit that synchronization step can be combined with nonmonotone stabilization schemes, if needed. <p> One of the keys to our analysis is imposing certain reasonable conditions on the choice of directions for the change in secondary variables. The choice of those directions is very important for the success of the PVD approach. This fact was empirically observed in <ref> [4] </ref>. It can also be vividly illustarted by theoretical considerations for the constrained optimization problems [19]. We briefly describe our notation now. The usual inner product of two vectors x 2 &lt; n , y 2 &lt; n is denoted by hx; yi. <p> By making an explicit use of the "forget-me-not" terms in the subproblems, we also improve on the linear convergence result given in <ref> [4] </ref>. In [4] it is established that, for the strongly convex case, the following estimate is valid kx i xk c 1 1 p 2 where x is the (unique) solution of the problem, p is the number of parallel processors, and c 1 ; c 2 are positive constants. <p> By making an explicit use of the "forget-me-not" terms in the subproblems, we also improve on the linear convergence result given in <ref> [4] </ref>. In [4] it is established that, for the strongly convex case, the following estimate is valid kx i xk c 1 1 p 2 where x is the (unique) solution of the problem, p is the number of parallel processors, and c 1 ; c 2 are positive constants. <p> This result is not quite satisfactory because the presense of p in the denominator suggests that the convergence speed goes down as the number of processors used increases. We point out that the proof given in <ref> [4] </ref> fails to make use of the "forget-me-not" terms which are the key to the algorithm. By refining the proof, we obtain a better convergence speed estimate kx i xk c 1 (1 c 3 ) 2 ; where c 3 &gt; 0 does not depend on p. <p> In particular, we show that there is no need to find an exact global solution for the subproblems. Any point that satisfies a natural sufficient descent condition can be accepted for the next iteration. We note, in the passing, that the proof given in <ref> [4] </ref> makes use of exact global solutions in an essential way and breaks down if, for example, only stationary points in the subproblems are available.
Reference: 5. <author> L. Grippo, F. Lampariello, and S. Lucidi. </author> <title> A nonmonotone line search technique for Newton's method. </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> 23 </volume> <pages> 707-716, </pages> <year> 1986. </year>
Reference-contexts: Note that our synchronization step may increase rather than decrease the objective function when compared to the values obtained by the parallel processors. This provides the algorithm with more flexibility and is known to be sometimes useful in nonlinear nonconvex optimization <ref> [5] </ref>, [6]. Of course, only computational experiments can give an insight into the usefulness of nonmonotone synchronization schemes for PVD algorithms. We next introduce a notion of weak sharp minima of order 2 which allows us to strengthen some of the traditional convergence results. Definition.
Reference: 6. <author> L. Grippo, F. Lampariello, and S. Lucidi. </author> <title> A class of nonmonotone stabilization methods in unconstrained optimization. </title> <journal> Numerische Mathematik, </journal> <volume> 59 </volume> <pages> 779-805, </pages> <year> 1991. </year>
Reference-contexts: Synchronization can be performed at any time provided every processor has achieved the sufficient descent condition. Furthermore, we show that synchronization step need not be monotone and can be combined with nonmonotone stabilization schemes similar to <ref> [6] </ref>. We also derive some new convergence results for weakly sharp problems of order 2 (see Definition below). This class of problems can be viewed as a generalization of strongly convex problems and a certain unconstrained smooth analogue of weak sharp minima [2]. <p> Note that our synchronization step may increase rather than decrease the objective function when compared to the values obtained by the parallel processors. This provides the algorithm with more flexibility and is known to be sometimes useful in nonlinear nonconvex optimization [5], <ref> [6] </ref>. Of course, only computational experiments can give an insight into the usefulness of nonmonotone synchronization schemes for PVD algorithms. We next introduce a notion of weak sharp minima of order 2 which allows us to strengthen some of the traditional convergence results. Definition.
Reference: 7. <author> X.-D. Luo and P. Tseng. </author> <title> On global projection-type error bound for the linear complementarity problem. Linear Algebra and Its Applications. </title> <note> To appear. </note>
Reference-contexts: This error bound is known to hold when F () is affine (see [9], [17]) or F () has certain strong monotonicity structure (see [21], Theorem 2). Moreover, under additional assumptions on F (), this condition holds globally with * = 1 (see <ref> [7] </ref>, [8], [11], [14]). Therefore our analysis shows that certain unconstrained minimization techniques applied to minimizing the implicit Lagrangian attain linear rate of convergence (under certain conditions).
Reference: 8. <author> Z.-Q. Luo, O.L. Mangasarian, J. Ren, </author> <title> and M.V. Solodov. New error bounds for the linear complementarity problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 19 </volume> <pages> 880-892, </pages> <year> 1994. </year>
Reference-contexts: In particular, the implicit Lagrangian is nonnegative everywhere in &lt; n and assumes the value of zero precisely at the solutions of the NCP. In <ref> [8] </ref> it was established that 2 (ff 1)kr (x)k 2 M (x; ff) 2ff (ff 1)kr (x)k 2 ; 8x 2 &lt; n ; where r (x) := x [x F (x)] + . <p> This error bound is known to hold when F () is affine (see [9], [17]) or F () has certain strong monotonicity structure (see [21], Theorem 2). Moreover, under additional assumptions on F (), this condition holds globally with * = 1 (see [7], <ref> [8] </ref>, [11], [14]). Therefore our analysis shows that certain unconstrained minimization techniques applied to minimizing the implicit Lagrangian attain linear rate of convergence (under certain conditions).
Reference: 9. <author> Z.-Q. Luo and P. Tseng. </author> <title> Error bound and convergence analysis of matrix splitting algorithms for the affine variational inequality problem. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2 </volume> <pages> 43-54, </pages> <year> 1992. </year>
Reference-contexts: This error bound is known to hold when F () is affine (see <ref> [9] </ref>, [17]) or F () has certain strong monotonicity structure (see [21], Theorem 2). Moreover, under additional assumptions on F (), this condition holds globally with * = 1 (see [7], [8], [11], [14]).
Reference: 10. <author> O.L. Mangasarian. </author> <title> Parallel gradient distribution in unconstrained optimization. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 33 </volume> <pages> 1916-1925, </pages> <year> 1995. </year>
Reference-contexts: The presence of this term allows for a change in "secondary" variables. This makes PVD fundamentally different from the block Jacobi [1], coordinate descent [20] and parallel gradient distribution algorithms <ref> [10] </ref>. The directions D i l are typically easily computable steepest descent or quasi-Newton directions in the space of the corresponding variables. The "forget-me-not" approach improves robustness and accelerates convergence of the algorithm and is the key to its success. <p> Note that the above condition is satisfied by a single iteration of any reasonable descent algorithm [16], <ref> [10] </ref> applied to the problem of minimizing '() with t 0 as a starting point. Hence it is also satisfied for a minimum or a stationary point computed by some descent algorithm provided it uses t 0 as a starting point. We now state our new PVD algorithm.
Reference: 11. <author> O.L. Mangasarian and J. Ren. </author> <title> New improved error bounds for the linear complementarity problem. </title> <journal> Mathematical Programming, </journal> <volume> 66 </volume> <pages> 241-255, </pages> <year> 1994. </year>
Reference-contexts: This error bound is known to hold when F () is affine (see [9], [17]) or F () has certain strong monotonicity structure (see [21], Theorem 2). Moreover, under additional assumptions on F (), this condition holds globally with * = 1 (see [7], [8], <ref> [11] </ref>, [14]). Therefore our analysis shows that certain unconstrained minimization techniques applied to minimizing the implicit Lagrangian attain linear rate of convergence (under certain conditions). This is an interesting result given that the implicit Lagrangian is not known to be strongly convex in any neighborhood of its zero minima. 4.
Reference: 12. <editor> O.L. Mangasarian and M.V. Solodov. </editor> <title> Nonlinear complementarity as unconstrained and constrained minimization. </title> <journal> Mathematical Programming, </journal> <volume> 62 </volume> <pages> 277-297, </pages> <year> 1993. </year>
Reference-contexts: This observation is however of significance because it implies that we are allowed a lot of flexibility in devising PVD algorithms and, in particular, in defining the points of synchronization. Remark. A practically important example of weak sharp minima of order 2 is provided by the implicit Lagrangian reformulation <ref> [12] </ref> of the nonlinear complementarity problem. Consider the following nonlinear complementarity problem [3], [15] (NCP) of finding an x 2 &lt; n such that 180 where F : &lt; n ! &lt; n is a continuously differentiable mapping. In [12] it was established that the NCP can be solved via (smooth) <p> minima of order 2 is provided by the implicit Lagrangian reformulation <ref> [12] </ref> of the nonlinear complementarity problem. Consider the following nonlinear complementarity problem [3], [15] (NCP) of finding an x 2 &lt; n such that 180 where F : &lt; n ! &lt; n is a continuously differentiable mapping. In [12] it was established that the NCP can be solved via (smooth) unconstrained minimization of the following implicit Lagrangian function : M (x; ff) := 2ffhx; F (x)i + k [x ffF (x)] + k 2 kxk 2 + k [F (x) ffx] + k 2 kF (x)k 2 ; where
Reference: 13. <author> J.M. Ortega and W.C. Rheinboldt. </author> <title> Iterative Solution of Nonlinear Equations in Several Variables. </title> <publisher> Academic Press, </publisher> <year> 1970. </year>
Reference-contexts: By R-linear convergence and Q-linear convergence, we mean linear convergence in the root sense and in the quotient sense, respectively, as defined in <ref> [13] </ref>. We now state a classical lemma ([16],p.6), as well as another lemma (a slight modification of [16],p.44) that will be used later.
Reference: 14. <author> J.-S. Pang. </author> <title> A posteriori error bounds for the linearly-constrained variational inequality problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12 </volume> <pages> 474-484, </pages> <year> 1987. </year>
Reference-contexts: This error bound is known to hold when F () is affine (see [9], [17]) or F () has certain strong monotonicity structure (see [21], Theorem 2). Moreover, under additional assumptions on F (), this condition holds globally with * = 1 (see [7], [8], [11], <ref> [14] </ref>). Therefore our analysis shows that certain unconstrained minimization techniques applied to minimizing the implicit Lagrangian attain linear rate of convergence (under certain conditions). This is an interesting result given that the implicit Lagrangian is not known to be strongly convex in any neighborhood of its zero minima. 4.
Reference: 15. <author> J.-S. Pang. </author> <title> Complementarity problems. </title> <editor> In R. Horst and P. Pardalos, editors, </editor> <title> Handbook of Global Optimization. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, Massachusetts, </address> <year> 1994. </year>
Reference-contexts: Remark. A practically important example of weak sharp minima of order 2 is provided by the implicit Lagrangian reformulation [12] of the nonlinear complementarity problem. Consider the following nonlinear complementarity problem [3], <ref> [15] </ref> (NCP) of finding an x 2 &lt; n such that 180 where F : &lt; n ! &lt; n is a continuously differentiable mapping.
Reference: 16. <author> B.T. Polyak. </author> <title> Introduction to Optimization. Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Note that the above condition is satisfied by a single iteration of any reasonable descent algorithm <ref> [16] </ref>, [10] applied to the problem of minimizing '() with t 0 as a starting point. Hence it is also satisfied for a minimum or a stationary point computed by some descent algorithm provided it uses t 0 as a starting point. We now state our new PVD algorithm. <p> The class of problems with weak sharp minima of order 2 can be thought of as a certain unconstrained smooth analogue of weak sharp minima (of order 1) <ref> [16] </ref>, [2]. Note that it subsumes strongly convex programs. Let f () be strongly convex with modulus 2. Then its unique optimal point x is globally (with * = 1) weakly sharp of order 2. This can be easily verified as follows.
Reference: 17. <author> S.M. Robinson. </author> <title> Some continuity properties of polyhedral multifunctions. </title> <journal> Mathematical Programming Study, </journal> <volume> 14 </volume> <pages> 206-214, </pages> <year> 1981. </year>
Reference-contexts: This error bound is known to hold when F () is affine (see [9], <ref> [17] </ref>) or F () has certain strong monotonicity structure (see [21], Theorem 2). Moreover, under additional assumptions on F (), this condition holds globally with * = 1 (see [7], [8], [11], [14]).
Reference: 18. <author> M. V. Solodov and S. K. Zavriev. </author> <title> Error-stabilty properties of generalized gradient-type algorithms. </title> <type> Mathematical Programming Technical Report 94-05, </type> <institution> Computer Science Department, University of Wisconsin, </institution> <address> 1210 West Dayton Street, Madison, Wisconsin 53706, U.S.A., </address> <month> June </month> <year> 1994 </year> <month> (revised July </month> <year> 1995). </year>
Reference-contexts: i l + D i 169 (*) Synchronization : Compute x i+1 such that f (x i+1 ) min i l ; i To make the parallelization step precise, we say that the current approximation to the solution of a subproblem is admissible if it belongs to an "-stationary set <ref> [18] </ref> of this subproblem. The parallelization subproblems are therefore equivalent to computing a point (y i l ) 2 X l;i l (x l ; l )k " i;l g: (5) We first establish some preliminary results. <p> Depending on the particular forcing functions, some arguments in the subsequent analysis may need to be changed. We finally state a useful lemma which is the basis for devising algorithms with inexact subproblem solution in the convex case. This result is a simplification of <ref> [18] </ref>, Lemma 2.4 for smooth unconstrained case. We include the simplified proof for completeness. Lemma 5 Let '() be convex and differentiable. Let x fl 2 X s := arg min x2&lt; n '(x) and x 2 X s (") := fx 2 &lt; n j kr'(x)k "g, " 0.
Reference: 19. <author> M.V. Solodov. </author> <title> On the convergence of constrained parallel variable distribution algorithms. </title> <type> Technical Report B-094, </type> <institution> Instituto de Matematica Pura e Aplicada, Estrada Dona Castorina 110, Jardim Botanico, Rio de Janeiro, </institution> <address> RJ, CEP 22460, Brazil, </address> <month> October </month> <year> 1995. </year> <note> SIAM Journal on Optimization, accepted for publication. </note>
Reference-contexts: The choice of those directions is very important for the success of the PVD approach. This fact was empirically observed in [4]. It can also be vividly illustarted by theoretical considerations for the constrained optimization problems <ref> [19] </ref>. We briefly describe our notation now. The usual inner product of two vectors x 2 &lt; n , y 2 &lt; n is denoted by hx; yi. The Euclidean 2-norm of x 2 &lt; n is given by kxk 2 = hx; xi.
Reference: 20. <author> P. Tseng. </author> <title> Dual coordinate ascent methods for non-strictly convex minimization. </title> <journal> Mathematical Programming, </journal> <volume> 59 </volume> <pages> 231-248, </pages> <year> 1993. </year>
Reference-contexts: The presence of this term allows for a change in "secondary" variables. This makes PVD fundamentally different from the block Jacobi [1], coordinate descent <ref> [20] </ref> and parallel gradient distribution algorithms [10]. The directions D i l are typically easily computable steepest descent or quasi-Newton directions in the space of the corresponding variables. The "forget-me-not" approach improves robustness and accelerates convergence of the algorithm and is the key to its success.
Reference: 21. <author> P. Tseng. </author> <title> On linear convergence of iterative methods for the variational inequality problem. </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 60 </volume> <pages> 237-252, </pages> <year> 1995. </year>
Reference-contexts: This error bound is known to hold when F () is affine (see [9], [17]) or F () has certain strong monotonicity structure (see <ref> [21] </ref>, Theorem 2). Moreover, under additional assumptions on F (), this condition holds globally with * = 1 (see [7], [8], [11], [14]). Therefore our analysis shows that certain unconstrained minimization techniques applied to minimizing the implicit Lagrangian attain linear rate of convergence (under certain conditions).
References-found: 21

