URL: http://www.almaden.ibm.com/cs/quest/papers/kdd97_class.ps
Refering-URL: http://www.almaden.ibm.com/cs/quest/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fali,stefanosg@almaden.ibm.com  srikant@almaden.ibm.com  
Title: Partial Classification using Association Rules  
Author: Kamal Ali and Stefanos Manganaris Ramakrishnan Srikant 
Address: 650 Harry Rd, San Jose, CA 95120  650 Harry Rd, San Jose, CA 95120  
Affiliation: IBM Global Business Intelligence Solutions  IBM Almaden Research Center  
Abstract: Many real-life problems require a partial classification of the data. We use the term "partial classification" to describe the discovery of models that show characteristics of the data classes, but may not cover all classes and all examples of any given class. Complete classification may be infeasible or undesirable when there are a very large number of class attributes, most attributes values are missing, or the class distribution is highly skewed and the user is interested in understanding the low-frequency class. We show how association rules can be used for partial classification in such domains, and present two case studies: reducing telecommunications order failures and detecting redundant medical tests. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agrawal, R.; Mannila, H.; Srikant, R.; Toivonen, H.; and Verkamo, A. I. </author> <year> 1996. </year> <title> Fast Discovery of Association Rules. </title> <editor> In Fayyad, U. M.; Piatetsky-Shapiro, G.; Smyth, P.; and Uthurusamy, R., eds., </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI/MIT Press. </publisher> <pages> chapter 12, 307-328. </pages>
Reference-contexts: Each of the attributes must be modeled based on the other attributes. 5. The number of training examples is very large. There has been some recent work on decision-tree classifiers that can scale to large training examples <ref> (Mehta, Agrawal, & Rissanen 1996) </ref> and set-valued features (Cohen 1996). However characteristics 2, 3 and 4 still make using decision-trees problematic. Characteristics 2 and 5 precluded the use of neural networks (Rumelhart & McClelland 1986). Characteristic 5 made the use of existing inductive logic programming (ILP) techniques (Muggleton 1992) cumbersome. <p> The first step is responsible for most of the computation time, and has been the focus of considerable work on developing fast algorithms, e.g. <ref> (Agrawal et al. 1996) </ref> (Brin et al. 1997). Telecommunications Case Study This case study focuses on one of the first components of a process re-engineering project. A typical service order for telecommunications related products undergoes a series of processing steps from entry to completion.
Reference: <author> Agrawal, R.; Imielinski, T.; and Swami, A. </author> <year> 1993. </year> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proc. of the ACM SIGMOD Conference on Management of Data, </booktitle> <pages> 207-216. </pages>
Reference-contexts: Further the examples covered by different rules may overlap. Hence the model discovered by partial classification can provide valuable insights into the data, but cannot be directly used for prediction. The problem of mining association rules was introduced in <ref> (Agrawal, Imielinski, & Swami 1993) </ref>. The 1 Copyright c fl1997, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. input consists of a set of transactions, where each transaction is a set of literals (called items).
Reference: <author> Ali, K. </author> <year> 1995. </year> <title> A comparison of methods for learning and combining evidence from multiple models. </title> <type> Technical Report UCI TR #95-47, </type> <institution> University of Califor-nia, Irvine, Dept. of Information and Computer Sciences. </institution>
Reference-contexts: Future Work The successful use of associations for partial classification poses an intriguing question: can associations be used for complete classification? One approach to building a classifier on top of associations would be to add machinery for "conflict resolution" <ref> (Ali 1995) </ref>. Let R j be the set of rules for class j. The goal would be to combine evidence among the rules of R j that fire for some test example x to come up with a posterior probability for class j.
Reference: <author> Brin, S.; Motwani, R.; Ullman, J. D.; and Tsur, S. </author> <year> 1997. </year> <title> Dynamic itemset counting and implication rules for market basket data. </title> <booktitle> In Proc. of the ACM SIG-MOD Conference on Management of Data. </booktitle>
Reference-contexts: The first step is responsible for most of the computation time, and has been the focus of considerable work on developing fast algorithms, e.g. (Agrawal et al. 1996) <ref> (Brin et al. 1997) </ref>. Telecommunications Case Study This case study focuses on one of the first components of a process re-engineering project. A typical service order for telecommunications related products undergoes a series of processing steps from entry to completion.
Reference: <author> Cohen, W. W. </author> <year> 1996. </year> <title> Learning trees and rules with set-valued features. </title> <booktitle> In Proc. of the 13th National Conference on Artificial Intelligence (AAAI). </booktitle>
Reference-contexts: Each of the attributes must be modeled based on the other attributes. 5. The number of training examples is very large. There has been some recent work on decision-tree classifiers that can scale to large training examples (Mehta, Agrawal, & Rissanen 1996) and set-valued features <ref> (Cohen 1996) </ref>. However characteristics 2, 3 and 4 still make using decision-trees problematic. Characteristics 2 and 5 precluded the use of neural networks (Rumelhart & McClelland 1986). Characteristic 5 made the use of existing inductive logic programming (ILP) techniques (Muggleton 1992) cumbersome.
Reference: <author> Mehta, M.; Agrawal, R.; and Rissanen, J. </author> <year> 1996. </year> <title> SLIQ: A fast scalable classifier for data mining. </title> <booktitle> In Proc. of the Fifth Int'l Conference on Extending Database Technology (EDBT). </booktitle>
Reference-contexts: Each of the attributes must be modeled based on the other attributes. 5. The number of training examples is very large. There has been some recent work on decision-tree classifiers that can scale to large training examples <ref> (Mehta, Agrawal, & Rissanen 1996) </ref> and set-valued features (Cohen 1996). However characteristics 2, 3 and 4 still make using decision-trees problematic. Characteristics 2 and 5 precluded the use of neural networks (Rumelhart & McClelland 1986). Characteristic 5 made the use of existing inductive logic programming (ILP) techniques (Muggleton 1992) cumbersome.
Reference: <author> Michie, D.; Spiegelhalter, D. J.; and Taylor, C. C. </author> <year> 1994. </year> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood. </publisher>
Reference-contexts: Introduction Classification, or supervised learning, has been widely studied in the machine learning community, e.g. <ref> (Michie, Spiegelhalter, & Taylor 1994) </ref>. The input data for classification, also called the training set, consists of multiple examples (records), each having multiple attributes or features. Additionally, each example is tagged with a special class label.
Reference: <author> Muggleton, S., ed. </author> <year> 1992. </year> <title> Inductive Logic Programming. </title> <publisher> Academic Press. </publisher>
Reference-contexts: However characteristics 2, 3 and 4 still make using decision-trees problematic. Characteristics 2 and 5 precluded the use of neural networks (Rumelhart & McClelland 1986). Characteristic 5 made the use of existing inductive logic programming (ILP) techniques <ref> (Muggleton 1992) </ref> cumbersome. First order logic concept descriptions are very expressive, allowing variables, higher arity predicates, and negation of predicates. However, due to their flexibility and larger search space, current ILP algorithms are much slower than algorithms for finding association rules.
Reference: <author> Nearhos, J.; Rothman, M.; and Viveros, M. </author> <year> 1996. </year> <title> Applying data mining techniques to a health insurance information system. </title> <booktitle> In Proc. of the 22nd Int'l Conference on Very Large Databases. </booktitle>
Reference-contexts: The problem is to find all association rules that satisfy user-specified minimum support and minimum confidence constraints. Applications include discovering affinities for market basket analysis and cross-marketing, catalog design, loss-leader analysis and fraud detection. See <ref> (Nearhos, Rothman, & Viveros 1996) </ref> for a case study of a successful application in health insurance. In this paper, we show that association rules can often be used to solve partial classification problems.
Reference: <author> Rumelhart, D., and McClelland, J. </author> <year> 1986. </year> <title> Parallel Distributed Processing: Exploration in the Mi-crostructure of Cognition. </title> <publisher> MIT Press. </publisher>
Reference-contexts: There has been some recent work on decision-tree classifiers that can scale to large training examples (Mehta, Agrawal, & Rissanen 1996) and set-valued features (Cohen 1996). However characteristics 2, 3 and 4 still make using decision-trees problematic. Characteristics 2 and 5 precluded the use of neural networks <ref> (Rumelhart & McClelland 1986) </ref>. Characteristic 5 made the use of existing inductive logic programming (ILP) techniques (Muggleton 1992) cumbersome. First order logic concept descriptions are very expressive, allowing variables, higher arity predicates, and negation of predicates.
Reference: <author> Srikant, R.; Vu, Q.; and Agrawal, R. </author> <year> 1997. </year> <title> Mining Association Rules with Item Constraints. </title> <booktitle> In Proc. of the 3rd Int'l Conference on Knowledge Discovery in Databases and Data Mining. </booktitle>
Reference-contexts: Thus, it becomes important to integrate in the discovery algorithm a constraint that we are only interested in rules that contain an RMA. Without such integrated constraints, the huge search space makes selection via post-filtering impractical. Recent work integrating item constraints into the association discovery algorithm <ref> (Srikant, Vu, & Agrawal 1997) </ref>, enabled us to test this approach and verify that we got similar results. Medical Diagnosis Case Study In this proof-of-concept project, the Information Services department of a hospital cleaned the data and prepared two flat files for mining.
References-found: 11

