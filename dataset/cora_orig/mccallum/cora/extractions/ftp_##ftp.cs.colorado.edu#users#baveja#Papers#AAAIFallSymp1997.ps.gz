URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/AAAIFallSymp1997.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Title: Planning with Closed-Loop Macro Actions  
Author: Doina Precup Richard S. Sutton Satinder Singh 
Web: http://www.cs.umass.edu/~dprecup  http://www.cs.umass.edu/~rich  http://www.cs.colorado.edu/~baveja  
Address: Amherst, MA 01003-4610  Amherst, MA 01003-4610  Boulder, CO 80309-0430  
Affiliation: University of Massachusetts  University of Massachusetts  University of Colorado  
Abstract: Planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence. In this paper we summarize an approach to this problem based on the mathematical framework of Markov decision processes and reinforcement learning. Conventional model-based reinforcement learning uses primitive actions that last one time step and that can be modeled independently of the learning agent. These can be generalized to macro actions, multi-step actions specified by an arbitrary policy and a way of completing. Macro actions generalize the classical notion of a macro operator in that they are closed loop, uncertain, and of variable duration. Macro actions are needed to represent common-sense higher-level actions such as going to lunch, grasping an object, or traveling to a distant city. This paper generalizes prior work on temporally abstract models (Sutton 1995) and extends it from the prediction setting to include actions, control, and planning. We define a semantics of models of macro actions that guarantees the validity of planning using such models. This paper present new results in the theory of planning with macro actions and illustrates its potential advantages in a gridworld task. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bertsekas, D. P. </author> <year> 1987. </year> <title> Dynamic Programming: Deterministic and Stochastic Models. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice Hall. </publisher>
Reference-contexts: Proof: It is straightforward to show that for any macro action a, the operator T a (v) = g a + p a v is a contraction. The result follows from the contraction mapping theorem <ref> (Bertsekas 1987) </ref>. Theorem 6 (Bellman Optimality Equation) For any set of actions A, v fl a2A s A : (11) The proof is analogous to the case in which only primitive actions are used (see, for instance, Ross, 1983), and is omitted here due to space constraints. <p> The two previous theorems can be proven by show ing that for any set of actions A and any state s, the operator T A (v) = max a2A s g a + p a v is a contraction. The results follow from the contraction mapping theorem <ref> (Bertsekas 1987) </ref>.
Reference: <author> Dayan, P., and Hinton, G. E. </author> <year> 1993. </year> <title> Feudal reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <pages> 271-278. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dayan, P. </author> <year> 1993. </year> <title> Improving generalization for temporal difference learning: </title> <booktitle> The successor representation. Neural Computation 5 </booktitle> <pages> 613-624. </pages>
Reference: <author> Dietterich, T. G. </author> <year> 1997. </year> <note> Personnal comunication. </note>
Reference: <author> Huber, M., and Grupen, R. A. </author> <year> 1997. </year> <title> Learning to coordinate controllers reinforcement learning on a control basis. </title> <booktitle> In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, IJCAI-97. </booktitle> <address> San Francisco, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kaelbling, L. P. </author> <year> 1993. </year> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning ICML'93, </booktitle> <pages> 167-173. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Korf, R. E. </author> <year> 1985. </year> <title> Learning to Solve Problems by Searching for Macro-Operators. </title> <publisher> London: Pitman Publishing Ltd. </publisher>
Reference: <author> Laird, J. E.; Rosenbloom, P. S.; and Newell, A. </author> <year> 1986. </year> <title> Chunking in SOAR: The anatomy of a general learning mechanism. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 11-46. </pages>
Reference: <author> Mahadevan, S., and Connell, J. </author> <year> 1992. </year> <title> Automatic programming of behavior-based robots using reinforcement learning. </title> <journal> Artificial Intelligence 55(2-3):311-365. </journal>
Reference: <author> McGovern, E. A.; Sutton, R. S.; and Fagg, A. H. </author> <year> 1997. </year> <title> Roles of macro-actions in accelerating reinforcement learning. </title> <booktitle> In Grace Hopper Celebration of Women in Computing. </booktitle>
Reference: <author> Moore, A. W., and Atkeson, C. G. </author> <year> 1993. </year> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <booktitle> Machine Learning 13 </booktitle> <pages> 103-130. </pages>
Reference: <author> Parr, R., and Russell, S. </author> <year> 1997. </year> <note> Personnal comunica-tion. </note>
Reference: <author> Peng, J., and Williams, J. </author> <year> 1993. </year> <title> Efficient learning and planning within the Dyna framework. </title> <booktitle> Adaptive Behavior 4 </booktitle> <pages> 323-334. </pages>
Reference: <author> Ross, S. </author> <year> 1983. </year> <title> Introduction to Stochastic Dynamic Programming. </title> <address> New York, NY: </address> <publisher> Academic Press. </publisher>
Reference-contexts: The result follows from the contraction mapping theorem (Bertsekas 1987). Theorem 6 (Bellman Optimality Equation) For any set of actions A, v fl a2A s A : (11) The proof is analogous to the case in which only primitive actions are used <ref> (see, for instance, Ross, 1983) </ref>, and is omitted here due to space constraints.
Reference: <author> Sacerdoti, E. D. </author> <year> 1977. </year> <title> A Structure for Plans and Behavior. </title> <publisher> North-Holland, NY: Elsevier. </publisher>
Reference: <author> Singh, S. P. </author> <year> 1992. </year> <title> Scaling reinforcement learning by learning variable temporal resolution models. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning ICML'92, </booktitle> <pages> 202-207. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. S., and Barto, A. G. </author> <year> 1998. </year> <title> Reinforcement Learning. An Introduction. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Sutton, R. S., and Pinette, B. </author> <year> 1985. </year> <title> The learning of world models by connectionist networks. </title> <booktitle> In Proceedings of the Seventh Annual Conference of the Cognitive Science Society, </booktitle> <pages> 54-64. </pages>
Reference: <author> Sutton, R. S. </author> <year> 1990. </year> <title> Integrating architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning ICML'90, </booktitle> <pages> 216-224. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. S. </author> <year> 1995. </year> <title> TD models: Modeling the world as a mixture of time scales. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning ICML'95, </booktitle> <pages> 531-539. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: We used Q-learning (Watkins 1989) to learn the optimal state-action value function for reaching each target hallway. The greedy policy with respect to this value function is the policy associated with the macro action. At the same time, we used the fi-model learning algorithm <ref> (Sutton 1995) </ref> to compute the models for each action. The learning algorithm is completely online and incremental, and its complexity is comparable to that of regular 1-step TD-learning. Acknowledgements The authors thank Amy McGovern, Andy Fagg, Man-fred Huber, and Ron Parr for helpful discussions and comments contributing to this paper.
Reference: <author> Watkins, C. J. C. H. </author> <year> 1989. </year> <title> Learning with Delayed Rewards. </title> <type> Ph.D. Dissertation, </type> <institution> Cambridge University. </institution>
Reference-contexts: In order to learn the policies and the models corresponding to each action, we gave each target hallway a hypothetical value of 1, while the failure outcome state (stumbling onto the wrong hallway) had a hypothetical value of 0. We used Q-learning <ref> (Watkins 1989) </ref> to learn the optimal state-action value function for reaching each target hallway. The greedy policy with respect to this value function is the policy associated with the macro action. At the same time, we used the fi-model learning algorithm (Sutton 1995) to compute the models for each action.
References-found: 21

