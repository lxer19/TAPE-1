URL: http://www.research.att.com/~schapire/papers/AuerCeFrSc98.ps.Z
Refering-URL: http://www.research.att.com/~schapire/publist.html
Root-URL: 
Email: pauer@igi.tu-graz.ac.at  cesabian@dsi.unimi.it  fyoav, schapireg@research.att.com  
Title: Gambling in a rigged casino: The adversarial multi-armed bandit problem  
Author: Peter Auer Nicolo Cesa-Bianchi Yoav Freund Robert E. Schapire 
Date: June 8, 1998  
Address: A-8010 Graz (Austria)  I-20135 Milano (Italy)  180 Park Avenue Florham Park, NJ 07932-0971  
Affiliation: Institute for Theoretical Computer Science University of Technology Graz  Department of Computer Science Universita di Milano  AT&T Labs  
Abstract: In the multi-armed bandit problem, a gambler must decide which arm of K non-identical slot machines to play in a sequence of trials so as to maximize his reward. This classical problem has received much attention because of the simple model it provides of the trade-off between exploration (trying out each arm to find the best one) and exploitation (playing the arm believed to give the best payoff). Past solutions for the bandit problem have almost always relied on assumptions about the statistics of the slot machines. In this work, we make no statistical assumptions whatsoever about the nature of the process generating the payoffs of the slot machines. We give a solution to the bandit problem in which an adversary, rather than a well-behaved stochastic process, has complete control over the payoffs. In a sequence of T plays, we prove that the expected per-round payoff of our algorithm approaches that of the best arm at the rate O(T 1=2 ), and we give an improved rate of convergence when the best arm has fairly low payoff. We also prove a general matching lower bound on the best possible performance of any algorithm in our setting. In addition, we consider a setting in which the player has a team of experts advising him on which arm to play; here, we give a strategy that will guarantee expected payoff close to that of the best expert. Finally, we apply our result to the problem of learning to play an unknown repeated matrix game against an all-powerful adversary.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfredo Banos. </author> <title> On pseudo-games. </title> <journal> The Annals of Mathematical Statistics, </journal> <volume> 39(6) </volume> <pages> 1932-1945, </pages> <year> 1968. </year>
Reference-contexts: If the matrix is known, then a randomized strategy that achieves the value of the game can be computed (say, using a linear-programming algorithm) and employed by the player. The case where the matrix is entirely unknown was previously considered by Ba nos <ref> [1] </ref> and Megiddo [14], who proposed two different strategies whose per-round payoff converges to the game value. Both of these algorithms are extremely inefficient. <p> The game is parameterized by the number K of possible actions, where each action is denoted by an integer i, 1 i K. We will assume that all the rewards belong to the unit interval <ref> [0; 1] </ref>. The generalization to rewards in [a; b] for arbitrary a &lt; b is straightforward. The game is played in a sequence of trials t = 1; 2; : : :; T . <p> On each trial t of the full information game: 3 1. The adversary selects a vector x (t) 2 <ref> [0; 1] </ref> K of current rewards. The ith component x i (t) is interpreted as the reward associated with action i at trial t. 2. <p> Here and throughout this paper, we make use of the function F M (x) which is defined for M 6= 0 to be F M (x) = M 2 : 2 These modifications enable Hedge to handle gains (rewards in [0; M]) rather than losses (rewards in <ref> [1; 0] </ref>). Note that we also allow rewards larger than 1. <p> Corollary 3.2 For &gt; 0, and for any sequence of reward vectors x (1); : : : ; x (T ) with x i (t) 2 <ref> [0; 1] </ref>, the probability vectors p (t) computed by Hedge satisfy T X p (t) x (t) P T e 1 Note that p (t) x (t) = E i t [x i t (t) j i 1 ; : : : ; i t1 ], so this corollary immediately implies <p> Repeat for t = 1; 2; : : : until game ends 1. Get the distribution p (t) from Hedge. 2. Select action i t to be j with probability p j (t) = (1 fl)p j (t) + fl . 3. Receive reward x i t (t) 2 <ref> [0; 1] </ref>. 4. Feed the simulated reward vector x (t) back to Hedge, where x j (t) = 8 : p i t (t) 0 otherwise. <p> In Section 6, we give a technique that does not require prior knowledge of such an upper bound. If the rewards x i (t) are in the range [a; b], a &lt; b, then Exp3 can be used after the rewards have been translated and rescaled to the range <ref> [0; 1] </ref>. Applying Corollary 4.2 with g = T gives the bound 8 (b a)2 e 1 T K ln K) on the regret. For instance, this is applicable to a standard loss model where the rewards fall in the range [1; 0]. Proof of Theorem 4.1. <p> Applying Corollary 4.2 with g = T gives the bound 8 (b a)2 e 1 T K ln K) on the regret. For instance, this is applicable to a standard loss model where the rewards fall in the range <ref> [1; 0] </ref>. Proof of Theorem 4.1. By the definition of the algorithm, we have that x i (t) 1= p i (t) K=fl. <p> Formally, at each trial t, we assume that the player, prior to choosing an action, is provided with a set of N probability vectors ~ j (t) 2 <ref> [0; 1] </ref> K , j = 1; : : : ; N , P K j i (t) = 1. <p> recommended probability of playing action i. (As a special case, the distribution can be concentrated on a single action, which represents a deterministic recommendation.) If the adversary chooses payoff vector x (t), then the expected reward for expert j (with 15 Algorithm Exp4 Parameters: Reals &gt; 0 and fl 2 <ref> [0; 1] </ref> Initialization: Initialize Hedge (with K replaced by N ) Repeat for t = 1; 2; : : : until game ends 1. Get the distribution q (t) 2 [0; 1] N from Hedge. 2. Get advice vectors ~ j (t) 2 [0; 1] K , and let p (t) <p> x (t), then the expected reward for expert j (with 15 Algorithm Exp4 Parameters: Reals &gt; 0 and fl 2 <ref> [0; 1] </ref> Initialization: Initialize Hedge (with K replaced by N ) Repeat for t = 1; 2; : : : until game ends 1. Get the distribution q (t) 2 [0; 1] N from Hedge. 2. Get advice vectors ~ j (t) 2 [0; 1] K , and let p (t) := P N 3. Select action i t to be j with probability p j (t) = (1 fl)p j (t) + fl=K. 4. <p> Parameters: Reals &gt; 0 and fl 2 <ref> [0; 1] </ref> Initialization: Initialize Hedge (with K replaced by N ) Repeat for t = 1; 2; : : : until game ends 1. Get the distribution q (t) 2 [0; 1] N from Hedge. 2. Get advice vectors ~ j (t) 2 [0; 1] K , and let p (t) := P N 3. Select action i t to be j with probability p j (t) = (1 fl)p j (t) + fl=K. 4. Receive reward x i t (t) 2 [0; 1]. 5. <p> Get advice vectors ~ j (t) 2 <ref> [0; 1] </ref> K , and let p (t) := P N 3. Select action i t to be j with probability p j (t) = (1 fl)p j (t) + fl=K. 4. Receive reward x i t (t) 2 [0; 1]. 5. Compute the simulated reward vector x (t) as x j (t) = 8 : p i t (t) 0 otherwise. 6. <p> We define the vector x (t) 2 [0; K=fl] K as before, and we finally feed the vector ^y (t) 2 [0; K=fl] N to Hedge where 16 y j (t) = ~ j (t) x (t). Let us also define y (t) 2 <ref> [0; 1] </ref> N to be the vector with components corresponding to the gains of the experts: y j (t) : The simplest possible expert is one which always assigns uniform weight to all actions so that ~ i (t) = 1=K on each round t. <p> In repeated play, the player's goal is to maximize its expected total payoff over a sequence of plays. Suppose in some trial the player chooses its next move i randomly according to a probability distribution on rows represented by a (column) vector p 2 <ref> [0; 1] </ref> n , and the adversary similarly chooses according to a probability vector q 2 [0; 1] m . Then the expected payoff is p T Mq. <p> Suppose in some trial the player chooses its next move i randomly according to a probability distribution on rows represented by a (column) vector p 2 <ref> [0; 1] </ref> n , and the adversary similarly chooses according to a probability vector q 2 [0; 1] m . Then the expected payoff is p T Mq. Von Neumann's celebrated minimax theorem states that max min p T Mq = min max p T Mq ; where maximum and minimum are taken over the (compact) set of all distribution vectors p and q. <p> The problem of learning to play when the player gets to see only the single element of the matrix associated with his choice and the choice of the adversary corresponds to the partial information game which is our emphasis here. This problem was previously considered by Ba nos <ref> [1] </ref> and Megiddo [14]. However, these previously proposed strategies are extremely inefficient. Not only is our strategy simpler and much more efficient, but we also are able to prove much faster rates of convergence. In fact, the application of our earlier algorithms to this problem is entirely straightforward. <p> Then the player's expected payoff per trial is at least v 2 (b a) (e 1)n ln n : 5 If T is not known in advance, the methods developed in Section 6 can be applied. 18 Proof. We assume that [a; b] = <ref> [0; 1] </ref>; the extension to the general case is straightforward.
Reference: [2] <author> David Blackwell. </author> <title> Controlled random walks. </title> <type> invited address, </type> <institution> Institute of Mathematical Statistics Meeting, </institution> <address> Seattle, Washington, </address> <year> 1956. </year> <month> 19 </month>
Reference-contexts: The problem of learning to play a repeated game when the player gets to see the whole column of rewards associated with the choice of the adversary corresponds to our full-information game. This problem was studied by Hannan [10], Blackwell <ref> [2] </ref> and more recently by Foster and Vohra [5], Fudenberg and Levin [8] and Freund and Schapire [7].
Reference: [3] <author> Nicolo Cesa-Bianchi, Yoav Freund, David Haussler, David P. Helmbold, Robert E. Schapire, and Manfred K. Warmuth. </author> <title> How to use expert advice. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 44(3) </volume> <pages> 427-485, </pages> <month> May </month> <year> 1997. </year>
Reference-contexts: There, the distribution over the rewards is fixed as T ! 1. For the full information game, matching upper and lower bounds of the form Q p were already known <ref> [3, 6] </ref>. Our lower bound shows that for the partial information game the dependence on the number of actions increases considerably. Specifically, our lower bound implies that no upper bound is possible of the form O (T ff (log K) fi ) where 0 ff &lt; 1, fi &gt; 0. <p> These strategies might select different actions at different iterations. The strategies can be computations performed by the player or they can be external advice given to the player by experts. We will use the more general term expert (borrowed from Cesa-Bianchi et al. <ref> [3] </ref>) because we place no restrictions on the generation of the advice. The player's goal in this case is to combine the advice of the experts in such a way that its total reward is close to that of the best expert (rather than the best single action).
Reference: [4] <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference: [5] <author> Dean P. Foster and Rakesh V. Vohra. </author> <title> A randomization rule for selecting forecasts. </title> <journal> Operations Research, </journal> <volume> 41(4) </volume> <pages> 704-709, </pages> <month> July-August </month> <year> 1993. </year>
Reference-contexts: The problem of learning to play a repeated game when the player gets to see the whole column of rewards associated with the choice of the adversary corresponds to our full-information game. This problem was studied by Hannan [10], Blackwell [2] and more recently by Foster and Vohra <ref> [5] </ref>, Fudenberg and Levin [8] and Freund and Schapire [7]. The problem of learning to play when the player gets to see only the single element of the matrix associated with his choice and the choice of the adversary corresponds to the partial information game which is our emphasis here.
Reference: [6] <author> Yoav Freund and Robert E. Schapire. </author> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 55(1) </volume> <pages> 119-139, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: Our algorithm is based in part on an algorithm presented by Freund and Schapire <ref> [6, 7] </ref>, which in turn is a variant of Littlestone and Warmuth's [13] weighted majority algorithm, and Vovk's [17] aggregating strategies. <p> We distinguish two variants: the partial information game, which captures the adversarial multi-armed bandit problem; and the full information game, which is essentially equivalent to the framework studied by Freund and Schapire <ref> [6] </ref>. On each trial t of the full information game: 3 1. The adversary selects a vector x (t) 2 [0; 1] K of current rewards. The ith component x i (t) is interpreted as the reward associated with action i at trial t. 2. <p> The version of Hedge presented here is a variant 2 of the algorithm introduced by Freund and Schapire <ref> [6] </ref> which itself is a direct generalization of Littlestone and Warmuth's Weighted Majority [13] algorithm. Hedge is described in Figure 1. <p> There, the distribution over the rewards is fixed as T ! 1. For the full information game, matching upper and lower bounds of the form Q p were already known <ref> [3, 6] </ref>. Our lower bound shows that for the partial information game the dependence on the number of actions increases considerably. Specifically, our lower bound implies that no upper bound is possible of the form O (T ff (log K) fi ) where 0 ff &lt; 1, fi &gt; 0.
Reference: [7] <author> Yoav Freund and Robert E. Schapire. </author> <title> Adaptive game playing using multiplicative weights. Games and Economic Behavior, </title> <note> (to appear). </note>
Reference-contexts: Our algorithm is based in part on an algorithm presented by Freund and Schapire <ref> [6, 7] </ref>, which in turn is a variant of Littlestone and Warmuth's [13] weighted majority algorithm, and Vovk's [17] aggregating strategies. <p> This problem was studied by Hannan [10], Blackwell [2] and more recently by Foster and Vohra [5], Fudenberg and Levin [8] and Freund and Schapire <ref> [7] </ref>. The problem of learning to play when the player gets to see only the single element of the matrix associated with his choice and the choice of the adversary corresponds to the partial information game which is our emphasis here.
Reference: [8] <author> Drew Fudenberg and David K. Levine. </author> <title> Consistency and cautious fictitious play. </title> <journal> Journal of Economic Dynamics and Control, </journal> <volume> 19 </volume> <pages> 1065-1089, </pages> <year> 1995. </year>
Reference-contexts: This problem was studied by Hannan [10], Blackwell [2] and more recently by Foster and Vohra [5], Fudenberg and Levin <ref> [8] </ref> and Freund and Schapire [7]. The problem of learning to play when the player gets to see only the single element of the matrix associated with his choice and the choice of the adversary corresponds to the partial information game which is our emphasis here.
Reference: [9] <author> J. C. Gittins. </author> <title> Multi-armed Bandit Allocation Indices. </title> <publisher> John Wiley & Sons, </publisher> <year> 1989. </year>
Reference-contexts: This bound is weaker than the bound on the expected regret. It is not clear whether or not this bound can be improved to have a dependence of O ( p T ) on the number of trials. 2 A non-stochastic bandit problem was also considered by Gittins <ref> [9] </ref> and Ishikida and Varaiya [11]. However, their version of the bandit problem is very different from ours: they assume that the player can compute ahead of time exactly what payoffs will be received from each arm, and their problem is thus one of optimization, rather than exploration and exploitation.
Reference: [10] <author> James Hannan. </author> <title> Approximation to Bayes risk in repeated play. </title> <editor> In M. Dresher, A. W. Tucker, and P. Wolfe, editors, </editor> <title> Contributions to the Theory of Games, </title> <booktitle> volume III, </booktitle> <pages> pages 97-139. </pages> <publisher> Princeton University Press, </publisher> <year> 1957. </year>
Reference-contexts: The problem of learning to play a repeated game when the player gets to see the whole column of rewards associated with the choice of the adversary corresponds to our full-information game. This problem was studied by Hannan <ref> [10] </ref>, Blackwell [2] and more recently by Foster and Vohra [5], Fudenberg and Levin [8] and Freund and Schapire [7].
Reference: [11] <author> T. Ishikida and P. Varaiya. </author> <title> Multi-armed bandit problem revisited. </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 83(1) </volume> <pages> 113-154, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: It is not clear whether or not this bound can be improved to have a dependence of O ( p T ) on the number of trials. 2 A non-stochastic bandit problem was also considered by Gittins [9] and Ishikida and Varaiya <ref> [11] </ref>. However, their version of the bandit problem is very different from ours: they assume that the player can compute ahead of time exactly what payoffs will be received from each arm, and their problem is thus one of optimization, rather than exploration and exploitation.
Reference: [12] <author> T. L. Lai and Herbert Robbins. </author> <title> Asymptotically efficient adaptive allocation rules. </title> <booktitle> Advances in Applied Mathematics, </booktitle> <volume> 6 </volume> <pages> 4-22, </pages> <year> 1985. </year>
Reference-contexts: Our worst-case bounds may appear weaker than the bounds proved using statistical assumptions, such as those shown by Lai and Robbins <ref> [12] </ref> of the form O (log T ). However, when comparing our results to those in the statistics literature, it is important to point out an important difference in the asymptotic quantification. <p> This dependence of the distribution on T is the reason that our lower bound does not contradict the upper bounds of the form O (log T ) which appear in the statistics literature <ref> [12] </ref>. There, the distribution over the rewards is fixed as T ! 1. For the full information game, matching upper and lower bounds of the form Q p were already known [3, 6].
Reference: [13] <author> Nick Littlestone and Manfred K. Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108 </volume> <pages> 212-261, </pages> <year> 1994. </year>
Reference-contexts: Our algorithm is based in part on an algorithm presented by Freund and Schapire [6, 7], which in turn is a variant of Littlestone and Warmuth's <ref> [13] </ref> weighted majority algorithm, and Vovk's [17] aggregating strategies. <p> The version of Hedge presented here is a variant 2 of the algorithm introduced by Freund and Schapire [6] which itself is a direct generalization of Littlestone and Warmuth's Weighted Majority <ref> [13] </ref> algorithm. Hedge is described in Figure 1.
Reference: [14] <author> N. Megiddo. </author> <title> On repeated games with incomplete information played by non-Bayesian players. </title> <journal> International Journal of Game Theory, </journal> <volume> 9(3) </volume> <pages> 157-167, </pages> <year> 1980. </year>
Reference-contexts: If the matrix is known, then a randomized strategy that achieves the value of the game can be computed (say, using a linear-programming algorithm) and employed by the player. The case where the matrix is entirely unknown was previously considered by Ba nos [1] and Megiddo <ref> [14] </ref>, who proposed two different strategies whose per-round payoff converges to the game value. Both of these algorithms are extremely inefficient. <p> This problem was previously considered by Ba nos [1] and Megiddo <ref> [14] </ref>. However, these previously proposed strategies are extremely inefficient. Not only is our strategy simpler and much more efficient, but we also are able to prove much faster rates of convergence. In fact, the application of our earlier algorithms to this problem is entirely straightforward. <p> The generality of the theorem also allows us to handle games in which the outcome for given plays i and j is a random variable (rather than a constant M ij ). Finally, as pointed out by Megiddo <ref> [14] </ref>, such a result is valid for non-cooperative, multi-person games; the average per-trial payoff of any player using this strategy will converge rapidly to the maximin payoff of the one-shot game.
Reference: [15] <editor> J. Neveu. Discrete-Parameter Martingales. </editor> <publisher> North Holland, </publisher> <year> 1975. </year>
Reference: [16] <author> H. Robbins. </author> <title> Some aspects of the sequential design of experiments. </title> <journal> Bulletin American Mathematical Society, </journal> <volume> 55 </volume> <pages> 527-535, </pages> <year> 1952. </year>
Reference-contexts: 1 Introduction In the well studied multi-armed bandit problem, originally proposed by Robbins <ref> [16] </ref>, a gambler must choose which of K slot machines to play. At each time step, he pulls the arm of one of the machines and receives a reward or payoff (possibly zero or negative). The gambler's purpose is to maximize his total reward over a sequence of trials.
Reference: [17] <author> Volodimir G. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383, </pages> <year> 1990. </year>
Reference-contexts: Our algorithm is based in part on an algorithm presented by Freund and Schapire [6, 7], which in turn is a variant of Littlestone and Warmuth's [13] weighted majority algorithm, and Vovk's <ref> [17] </ref> aggregating strategies. In the setting analyzed by Freund and Schapire (which we call here the full information game), the player on each trial scores the reward of the chosen arm, but gains access to the rewards associated with all of the arms (not just the one that was chosen).
References-found: 17

