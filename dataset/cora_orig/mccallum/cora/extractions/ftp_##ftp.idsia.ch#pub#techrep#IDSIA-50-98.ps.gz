URL: ftp://ftp.idsia.ch/pub/techrep/IDSIA-50-98.ps.gz
Refering-URL: http://www.idsia.ch/techrep.html
Root-URL: http://www.idsia.ch/techrep.html
Email: juergen@idsia.ch jieyu@idsia.ch  
Title: DIRECT POLICY SEARCH AND UNCERTAIN POLICY EVALUATION  
Author: Jurgen Schmidhuber and Jieyu Zhao 
Date: August 1998  
Web: www.idsia.ch  
Address: Corso Elvezia 36, 6900 Lugano, Switzerland  
Affiliation: IDSIA,  
Pubnum: Technical Report IDSIA-50-98  
Abstract: Reinforcement learning based on direct search in policy space requires few assumptions about the environment. Hence it is applicable in certain situations where most traditional reinforcement learning algorithms are not, especially in partially observable, deterministic worlds. In realistic settings, however, reliable policy evaluations are complicated by numerous sources of uncertainty, such as stochasticity in policy and environment. Given a limited life-time, how much time should a direct policy searcher spend on policy evaluations to obtain reliable statistics? Our efficient approach based on the success-story algorithm (SSA) is radical in the sense that it never stops evaluating any previous policy modification except those it undoes for lack of empirical evidence that they have contributed to lifelong reward accelerations. While previous experimental research has already demonstrated SSA's applicability to large-scale partially observable environments, a study of why it performs well has been lacking. Here we identify for the first time SSA's fundamental advantages over traditional direct policy search (such as stochastic hill-climbing) on problems involving several sources of stochasticity and uncertainty. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. P. Bertsekas and J. N. Tsitsiklis. </author> <title> Neuro-dynamic Programming. </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA, </address> <year> 1996. </year>
Reference-contexts: VF-based algorithms learn a mapping from input-action pairs to expected discounted future reward and use online variants of dynamic programming for constructing rewarding policies, e.g., <ref> [6, 9, 10, 1] </ref>. Their convergence theorems (e.g. [10]) typically require full observability of the environment. Although general VF-based methods for partially observable environments (POEs) do exist, most are feasible only for small problems [5].
Reference: [2] <author> J. C. Gittins. </author> <title> Multi-armed Bandit Allocation Indices. Wiley-Interscience series in systems and optimization. </title> <publisher> Wiley, </publisher> <address> Chichester, NY, </address> <year> 1989. </year>
Reference: [3] <author> A. Juels and M. Wattenberg. </author> <title> Stochastic hillclimbing as a baseline method for evaluating genetic algorithms. </title> <editor> In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 8, </volume> <pages> pages 430-436. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: Stochastic Hill-Climbing (SHC). SHC is one of the simplest elitist algorithms using direct search in policy space. It should be mentioned, however, that despite its simplicity SHC often outperforms more complex elitist methods such as GAs <ref> [3] </ref>. We implement SHC as follows: 1. Initialize policy p to 0.5, and real-valued variables BestP olicy and BestResult to p and 0, respectively. 2. If there have been more than 30000 T rialLength pulls then exit (T rialLength is an integer constant).
Reference: [4] <author> L. A. Levin. </author> <title> Universal sequential search problems. </title> <journal> Problems of Information Transmission, </journal> <volume> 9(3) </volume> <pages> 265-266, </pages> <year> 1973. </year>
Reference-contexts: Their policy space consists of complete (possibly probabilistic) algorithms defining agent behaviors and they search policy space directly (during successive trials) without making strong (e.g. Markovian) assumptions about the environment. Members of this class are Levin Search <ref> [4] </ref> and algorithms that build new policy candidates from some of the policies ("elitists") with highest evaluations observed so far, e.g., stochastic hill-climbing (SHC), genetic algorithms (GAs), and adaptive Levin Search [8]. We will refer to such methods as elitist algorithms.
Reference: [5] <author> M.L. Littman. </author> <title> Algorithms for Sequential Decision Making. </title> <type> PhD thesis, </type> <institution> Brown University, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: Their convergence theorems (e.g. [10]) typically require full observability of the environment. Although general VF-based methods for partially observable environments (POEs) do exist, most are feasible only for small problems <ref> [5] </ref>. Direct methods are simpler and less limited in the sense that they do not depend on VFs at all. Their policy space consists of complete (possibly probabilistic) algorithms defining agent behaviors and they search policy space directly (during successive trials) without making strong (e.g. Markovian) assumptions about the environment.
Reference: [6] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal on Research and Development, </journal> <volume> 3 </volume> <pages> 210-229, </pages> <year> 1959. </year>
Reference-contexts: VF-based algorithms learn a mapping from input-action pairs to expected discounted future reward and use online variants of dynamic programming for constructing rewarding policies, e.g., <ref> [6, 9, 10, 1] </ref>. Their convergence theorems (e.g. [10]) typically require full observability of the environment. Although general VF-based methods for partially observable environments (POEs) do exist, most are feasible only for small problems [5].
Reference: [7] <author> J. Schmidhuber, J. Zhao, and N. Schraudolph. </author> <title> Reinforcement learning with self-modifying policies. </title> <editor> In S. Thrun and L. Pratt, editors, </editor> <booktitle> Learning to learn, </booktitle> <pages> pages 293-309. </pages> <publisher> Kluwer, </publisher> <year> 1997. </year>
Reference-contexts: in case of noisy performance evaluations | here SHC will represent all so-called elitist algorithms that do not use information about long-term effects of policy changes and need a priori knowledge about "good" trial lengths. 2 Success-Story Algorithm (SSA) Here we will briefly review basic principles presented in previous work <ref> [7, 8] </ref>. An agent lives in environment E from time 0 to unknown time T . Life is one-way: even if it is decomposable into numerous consecutive "learning trials", time will never be reset. <p> Learning algorithms (LAs). Methods that modify POL are called learning algorithms (LAs). Previous work <ref> [7, 8] </ref> included LAs in the action set itself, but here we will focus on a simpler case where all LAs are externally triggered procedures. Checkpoints. The entire lifetime of the agent can be partitioned into time intervals separated by special times called checkpoints. <p> Checkpoints. The entire lifetime of the agent can be partitioned into time intervals separated by special times called checkpoints. In general, checkpoints are computed dynamically during the agent's life by certain actions in A executed according to POL itself <ref> [7] </ref>. In this paper, however, all checkpoints will be set in advance. The agent's k-th checkpoint is denoted c k . <p> The older some surviving SPM, the more time will have passed to collect statistics concerning its long-term consequences, and the more stable it will be if it is indeed useful and not just there by chance <ref> [7, 8] </ref>. Implementing SSA. <p> The older some surviving SPM, the more time will have passed to collect statistics concerning its long-term consequences, and the more stable it will be if it is indeed useful and not just there by chance [7, 8]. Implementing SSA. Using efficient stack-based backtracking methods one can guarantee <ref> [8, 7] </ref> that SSC will be satisfied after each checkpoint, even in only partially observable, stochastic environments. 3 Problems of Direct Search With Elitist Algorithms Elitist algorithms such as stochastic hill-climbing (SHC) and genetic algorithms (GAs) test policy candidates during time-limited trials, then build new policy candidates from some of the <p> In contrast, an SSA trial of any previous policy modification never ends unless its reward/time ratio drops below that of the most recent previously started (still unfinished) effective trial. Here we will go beyond previous work <ref> [7, 8] </ref> by clearly demonstrating SSA's benefits under noisy performance evaluations. Task. To illustrate the drawbacks of standard elitist algorithms we apply the simplest one (SHC) to one of the simplest tasks sufficient for this purpose: a two-armed bandit problem. There 3 are two arms A and B. <p> Many elitist methods, however, can be augmented by SSA. Finally, since SSA automatically collects statistics about long-term effects of earlier policy changes on later ones, it is of interest for improving the credit assignment method itself <ref> [7, 8] </ref>. Although the present paper's SSA application is much less complex than previous ones [7, 8] it is the first to provide insight into SSA's fundamental advantages over alternative direct search methods in case of uncertain policy evaluations. <p> Finally, since SSA automatically collects statistics about long-term effects of earlier policy changes on later ones, it is of interest for improving the credit assignment method itself <ref> [7, 8] </ref>. Although the present paper's SSA application is much less complex than previous ones [7, 8] it is the first to provide insight into SSA's fundamental advantages over alternative direct search methods in case of uncertain policy evaluations.
Reference: [8] <author> J. Schmidhuber, J. Zhao, and M. Wiering. </author> <title> Shifting inductive bias with success-story algorithm, adaptive Levin search, and incremental self-improvement. </title> <journal> Machine Learning, </journal> <volume> 28 </volume> <pages> 105-130, </pages> <year> 1997. </year>
Reference-contexts: Markovian) assumptions about the environment. Members of this class are Levin Search [4] and algorithms that build new policy candidates from some of the policies ("elitists") with highest evaluations observed so far, e.g., stochastic hill-climbing (SHC), genetic algorithms (GAs), and adaptive Levin Search <ref> [8] </ref>. We will refer to such methods as elitist algorithms. They are of interest where DP-based RL algorithms cannot be expected to yield good results because the environmental inputs do not satisfy their assumptions. <p> in case of noisy performance evaluations | here SHC will represent all so-called elitist algorithms that do not use information about long-term effects of policy changes and need a priori knowledge about "good" trial lengths. 2 Success-Story Algorithm (SSA) Here we will briefly review basic principles presented in previous work <ref> [7, 8] </ref>. An agent lives in environment E from time 0 to unknown time T . Life is one-way: even if it is decomposable into numerous consecutive "learning trials", time will never be reset. <p> Learning algorithms (LAs). Methods that modify POL are called learning algorithms (LAs). Previous work <ref> [7, 8] </ref> included LAs in the action set itself, but here we will focus on a simpler case where all LAs are externally triggered procedures. Checkpoints. The entire lifetime of the agent can be partitioned into time intervals separated by special times called checkpoints. <p> The older some surviving SPM, the more time will have passed to collect statistics concerning its long-term consequences, and the more stable it will be if it is indeed useful and not just there by chance <ref> [7, 8] </ref>. Implementing SSA. <p> The older some surviving SPM, the more time will have passed to collect statistics concerning its long-term consequences, and the more stable it will be if it is indeed useful and not just there by chance [7, 8]. Implementing SSA. Using efficient stack-based backtracking methods one can guarantee <ref> [8, 7] </ref> that SSC will be satisfied after each checkpoint, even in only partially observable, stochastic environments. 3 Problems of Direct Search With Elitist Algorithms Elitist algorithms such as stochastic hill-climbing (SHC) and genetic algorithms (GAs) test policy candidates during time-limited trials, then build new policy candidates from some of the <p> In contrast, an SSA trial of any previous policy modification never ends unless its reward/time ratio drops below that of the most recent previously started (still unfinished) effective trial. Here we will go beyond previous work <ref> [7, 8] </ref> by clearly demonstrating SSA's benefits under noisy performance evaluations. Task. To illustrate the drawbacks of standard elitist algorithms we apply the simplest one (SHC) to one of the simplest tasks sufficient for this purpose: a two-armed bandit problem. There 3 are two arms A and B. <p> Many elitist methods, however, can be augmented by SSA. Finally, since SSA automatically collects statistics about long-term effects of earlier policy changes on later ones, it is of interest for improving the credit assignment method itself <ref> [7, 8] </ref>. Although the present paper's SSA application is much less complex than previous ones [7, 8] it is the first to provide insight into SSA's fundamental advantages over alternative direct search methods in case of uncertain policy evaluations. <p> Finally, since SSA automatically collects statistics about long-term effects of earlier policy changes on later ones, it is of interest for improving the credit assignment method itself <ref> [7, 8] </ref>. Although the present paper's SSA application is much less complex than previous ones [7, 8] it is the first to provide insight into SSA's fundamental advantages over alternative direct search methods in case of uncertain policy evaluations.
Reference: [9] <author> R. S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: VF-based algorithms learn a mapping from input-action pairs to expected discounted future reward and use online variants of dynamic programming for constructing rewarding policies, e.g., <ref> [6, 9, 10, 1] </ref>. Their convergence theorems (e.g. [10]) typically require full observability of the environment. Although general VF-based methods for partially observable environments (POEs) do exist, most are feasible only for small problems [5].
Reference: [10] <author> C. J. C. H. Watkins and P. </author> <title> Dayan. </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8 </volume> <pages> 279-292, </pages> <year> 1992. </year> <month> 6 </month>
Reference-contexts: VF-based algorithms learn a mapping from input-action pairs to expected discounted future reward and use online variants of dynamic programming for constructing rewarding policies, e.g., <ref> [6, 9, 10, 1] </ref>. Their convergence theorems (e.g. [10]) typically require full observability of the environment. Although general VF-based methods for partially observable environments (POEs) do exist, most are feasible only for small problems [5]. <p> VF-based algorithms learn a mapping from input-action pairs to expected discounted future reward and use online variants of dynamic programming for constructing rewarding policies, e.g., [6, 9, 10, 1]. Their convergence theorems (e.g. <ref> [10] </ref>) typically require full observability of the environment. Although general VF-based methods for partially observable environments (POEs) do exist, most are feasible only for small problems [5]. Direct methods are simpler and less limited in the sense that they do not depend on VFs at all.
References-found: 10

