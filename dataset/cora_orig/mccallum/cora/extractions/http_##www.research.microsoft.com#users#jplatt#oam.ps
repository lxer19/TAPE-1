URL: http://www.research.microsoft.com/users/jplatt/oam.ps
Refering-URL: http://www.research.microsoft.com/users/jplatt/papers.html
Root-URL: http://www.research.microsoft.com
Email: platt@synaptics.com, nada@synaptics.com  
Title: A Constructive RBF Network for Writer Adaptation  
Author: John C. Platt and Nada P. Matic 
Note: To appear: Advances in Neural Information Processing Systems, 9  
Date: January 2, 1997  
Address: 2698 Orchard Parkway San Jose, CA 95134  
Affiliation: Synaptics, Inc.  
Abstract: This paper discusses a fairly general adaptation algorithm which augments a standard neural network to increase its recognition accuracy for a specific user. The basis for the algorithm is that the output of a neural network is characteristic of the input, even when the output is incorrect. We exploit this characteristic output by using an Output Adaptation Module (OAM) which maps this output into the correct user-dependent confidence vector. The OAM is a simplified Resource Allocating Network which constructs radial basis functions on-line. We applied the OAM to construct a writer-adaptive character recognition system for on-line hand-printed characters. The OAM decreases the word error rate on a test set by an average of 45%, while creating only 3 to 25 basis functions for each writer in the test set. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. Guyon, D. Henderson, P. Albrecht, Y. Le Cun, and J. Denker. </author> <title> Writer independent and writer adaptive neural network for on-line character recognition. </title> <editor> In S. Impedovo, editor, </editor> <title> From Pixels to Features III, </title> <booktitle> Ams-terdam, 1992. </booktitle> <publisher> Elsevier. </publisher>
Reference-contexts: A previous example of user adaptation of a neural handwriting recog-nizer employed a Time Delay Neural Network (TDNN), where the last layer of a TDNN was replaced with a tunable classifier that is more appropriate for adaptation <ref> [1] </ref>[3]. In Guyon, et al. [1], the last layer of a TDNN was replaced by a k-nearest neighbor classifier. This work was further extended in Matic, et al. [3], where the last layer of the TDNN was replaced with an optimal hyperplane classifier which is retrained for adaptation purposes. <p> This neural network was a carefully tuned multilayer feed-forward network, trained with the back-propagation algorithm. The network has 510 inputs, 200 hidden units, and 72 outputs. The input to the OAM was a vector of 72 confidences, one per class. These confidences were in the range <ref> [0; 1] </ref>. There is one input for every upper case character, lower case character, and digit. There is also one input for each member of a subset of punctuation characters ( !$&',-:;=? ). The OAM was tested interactively.
Reference: [2] <author> V. Kadirkamanathan and M. Niranjan. </author> <title> A function estimation approach to sequential learning with neural networks. </title> <journal> Neural Computation, </journal> <volume> 5(6) </volume> <pages> 954-976, </pages> <year> 1993. </year>
Reference: [3] <author> N. Matic, I. Guyon, J. Denker, and V. Vapnik. </author> <title> Writer adaptation for on-line handwritten character recognition. </title> <booktitle> In ICDAR93, </booktitle> <address> Tokyo, 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Neural Network (TDNN), where the last layer of a TDNN was replaced with a tunable classifier that is more appropriate for adaptation [1]<ref> [3] </ref>. In Guyon, et al. [1], the last layer of a TDNN was replaced by a k-nearest neighbor classifier. This work was further extended in Matic, et al. [3], where the last layer of the TDNN was replaced with an optimal hyperplane classifier which is retrained for adaptation purposes. The optimal hyperplane classifier retained the same accuracy as the k-nearest neighbor classifier, while reducing the amount of computation and memory required for adaptation.
Reference: [4] <author> J. Platt. </author> <title> A resource-allocating network for function interpolation. </title> <journal> Neural Computation, </journal> <volume> 3(2) </volume> <pages> 213-225, </pages> <year> 1991. </year>
Reference: [5] <author> M. Powell. </author> <title> Radial basis functions for multivariate interpolation: A review. </title> <editor> In J. C. Mason and M. G. Cox, editors, </editor> <title> Algorithms for Approximation, </title> <publisher> Oxford, </publisher> <address> 1987. </address> <publisher> Clarendon Press. </publisher>
Reference-contexts: The OAM learns to recognize these con-sistent incorrect output vectors, and produces a more correct output vector (see figure 1). The units of the OAM are radial basis functions (RBF) <ref> [5] </ref>. Adaptation of these RBF units is performed using a simplified version of the Resource Allocating Network (RAN) algorithm of Platt [4][2].
Reference: [6] <author> D. Wolpert. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5(2) </volume> <pages> 241-260, </pages> <year> 1992. </year>
Reference-contexts: Third, our adaptation experiments are performed on a neural network which recognizes a full character set. These previous papers only experimented with neural networks that recognized character subsets, which is a less difficult adaptation problem. The OAM is related to stacking <ref> [6] </ref>. In stacking, outputs of multiple recognizers are combined via training on partitions of the training set. With the OAM, the multiple outputs of a recognizer are combined using memory-based learning.
References-found: 6

