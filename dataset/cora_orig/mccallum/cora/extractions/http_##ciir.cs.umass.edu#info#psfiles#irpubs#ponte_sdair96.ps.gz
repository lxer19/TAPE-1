URL: http://ciir.cs.umass.edu/info/psfiles/irpubs/ponte_sdair96.ps.gz
Refering-URL: http://ciir.cs.umass.edu/info/psfiles/irpubs/irnew.html
Root-URL: 
Title: USeg: A Retargetable Word Segmentation Procedure for Information Retrieval  
Author: Jay M. Ponte and W. Bruce Croft 
Address: Amherst, MA 01003-4610, USA  
Affiliation: Computer Science Department  
Abstract: Many languages, such as Chinese, are written without interword delimiters. For these languages, a segmenter is required as a pre-processing step for information retrieval systems. We describe USeg, a platform for word segmentation designed to fulfill the requirments imposed by the information retrieval task. USeg is based on an underlying probabalistic automaton which serves as a simple language model. A description of the proposed model(s), implementation issues for these models and experimental results are presented. The experiments show that a fairly simple underlying model can produce reasonable segmentation results, can do so quickly enough to be useful for indexing in an information retrieval system and can be re-targeted to new languages without a great deal of human effort. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rabiner, </author> <title> L.R. A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. </title> <booktitle> Proceedings of the IEEE vol. </booktitle> <volume> 77, no. 2, </volume> <month> Feb. </month> <year> 1989. </year>
Reference-contexts: Two different types of probabilistic automata were used for the underlying model. The first is a simple word based model suggested by Barnett [9] and the second is a word bigram model which can be thought of as a Markov model <ref> [1] </ref>. The resources required to build a segmenter for a new language are a lexicon and some unsegmented text for training. Some experiments that did not require a lexicon were also performed. 2 Previous Work For an overview of Chinese segmentation for information retrieval see [4]. <p> For ease of discussion, they are presented in reverse order, segmenting first followed by training. Given the automaton representing word bigrams, segmenting some free text means finding the best path through the model. The Viterbi algorithm accomplishes this <ref> [1] </ref>. In order to uncover the maximum likelihood state sequence for a given output sequence, all possible paths through the model must be considered along with their probabilities. Clearly, the number of paths is exponential in the length of the sequence. <p> The model is trained using the Baum-Welch algorithm, a special case of Expectation Maximization <ref> [1] </ref>. Like the Viterbi algorithm, it uses dynamic programming. In this case, two tables are filled in, one table of probabilities propagated forward in time and a second set propagated backward. Instead of using the maximum likelihood path, the sum of all paths is computed at each step. <p> The edge probabilities are updated incrementally based on the probability of traversal until the model converges to a local maximum <ref> [1] </ref>. 6 Implementation Issues 6.1 Very Sparsely Populated Matri ces The transition matrices are very sparsely populated since the model graphs are connected in a very constrained fashion. In addition, the outputs for each state are limited, so at each time step, there will be very few potential transitions. <p> Since there are generally very few edges (under ten), a simple in line shell sort is used to avoid the overhead of a more complicated sort. 6.4 Scaling As mentioned in <ref> [1] </ref>, there is a potential problem in the probability calculations. Since at each step, numbers significantly less than one are being multiplied together, the values will approach zero at an exponential rate, quickly exceeding the precision of the machine. There are two methods of getting around this problem.
Reference: [2] <author> King, </author> <title> S.F. Syntactic Pattern Recognition and Applications Prentice Hall, </title> <year> 1982 </year>
Reference: [3] <author> Broglio, J., Callan, J. P., and Croft, W. B. </author> <title> INQUERY System Overview Proceedings of the TIPSTER Text Program (Phase I) San Francisco, </title> <address> CA. </address> <publisher> Morgan Kaufman, </publisher> <pages> 47-67. </pages>
Reference-contexts: The three characters individually mean "lower", "middle" and "country". The segmentation on the left, meaning "lower-middle country" is probably not as good as the one on the right, which cooresponds to "lower China" . USeg was designed to work in the context of the INQUERY information retrieval system <ref> [3] </ref> . In order for it to be a useful tool it needs to satisfy the following requirements: * Retargetability The INQUERY system is designed to be retargetable to different languages. <p> The results are interesting because they show the effects of a large amount of segmented training data. For English, only the word based model was used. The word probabilities were estimated from 1 Gigabyte of data from the Tipster collection <ref> [3] </ref>. The lexicon consisted of 558,238 words obtained by "dumping" INQUERY's inverted index file. This produces a list of all of the words in the collection along with frequency information. The list was fil tered by throwing away any word that occurred less than one hundred times.
Reference: [4] <author> Wu, Z., Tseng, G. </author> <title> Chinese Text Segmentation for Text Retrieval Achievements and Problems. </title> <address> JASIS, </address> <month> Oct, </month> <year> 1993. </year>
Reference-contexts: The resources required to build a segmenter for a new language are a lexicon and some unsegmented text for training. Some experiments that did not require a lexicon were also performed. 2 Previous Work For an overview of Chinese segmentation for information retrieval see <ref> [4] </ref>. Sproat et al [8] describe a Chinese word segmentation algorithm based on probabilistic automata. Their approach includes special recognizers for Chinese names and transliterated foreign names and a component for morphologically derived words. Our approach is to develop the above components as post processors to the segmenter.
Reference: [5] <author> Matsumoto, Y., Kurokashi, S., Myoki, Y., </author> <title> User's Guide for the JUMAN System </title> - 
Reference-contexts: The current Japanese version of INQUERY uses the JUMAN morphological analyzer for segmentation. JUMAN employs sophisticated morphological analysis to handle the inflectional endings <ref> [5] </ref>. In principle, the underlying model of USeg is general enough to incorporate morphological rules, but it would require considerable human effort. Instead, our approach was to use the lexical acquisition procedure from the Chinese experiments.
References-found: 5

