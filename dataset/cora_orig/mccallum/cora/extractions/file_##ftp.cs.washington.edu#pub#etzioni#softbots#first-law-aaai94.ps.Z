URL: file://ftp.cs.washington.edu/pub/etzioni/softbots/first-law-aaai94.ps.Z
Refering-URL: http://www.cs.washington.edu/research/projects/softbots/www/bib.html
Root-URL: 
Email: fweld, etzionig@cs.washington.edu  
Title: The First Law of Robotics (a call to arms)  
Author: Daniel Weld Oren Etzioni 
Address: Seattle, WA 98195  
Affiliation: Department of Computer Science and Engineering University of Washington  
Note: Appears in AAAI-94  The Three Laws of Robotics:  
Abstract: Even before the advent of Artificial Intelligence, science fiction writer Isaac Asimov recognized that an agent must place the protection of humans from harm at a higher priority than obeying human orders. Inspired by Asimov, we pose the following fundamental questions: (1) How should one formalize the rich, but informal, notion of "harm"? (2) How can an agent avoid performing harmful actions, and do so in a com-putationally tractable manner? (3) How should an agent resolve conflict between its goals and the need to avoid harm? (4) When should an agent prevent a human from harming herself? While we address some of these questions in technical detail, the primary goal of this paper is to focus attention on Asimov's concern: society will reject autonomous agents unless we have some credible means of making them safe! 
Abstract-found: 1
Intro-found: 1
Reference: <author> Allen, J. </author> <year> 1991. </year> <title> Planning as temporal reasoning. </title> <booktitle> In Proceedings of the Second International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> 3-14. </pages>
Reference-contexts: Before executing an action, we could ask an agent to prove that the action is not harmful. While elegant, this approach is computationally intractable as well. Another alternative would be to use a planner such as ilp <ref> (Allen 1991) </ref> or zeno (Penberthy & Weld 1994) which supports temporally quantified goals. Unfortunately, at present these planners seem too inefficient for our needs. 2 Situated action researchers might suggest that non-deliberative, reactive agents could be made "safe" by carefully engineering their interactions with the environment.
Reference: <author> Asimov, I. </author> <year> 1942. </year> <note> Runaround. Astounding Science Fiction. </note>
Reference-contexts: A robot must obey orders given it by human beings except where such orders would conflict with the First Law. 3. A robot must protect its own existence as long as such protection does not conflict with the First or Second Law. Isaac Asimov <ref> (Asimov 1942) </ref>: Motivation In 1940, Isaac Asimov stated the First Law of Robotics, capturing an essential insight: an intelligent agent 1 fl We thank Steve Hanks, Nick Kushmerick, Neal Lesh, Kevin Sullivan, and Mike Williamson for helpful discussions.
Reference: <author> Barrett, A., and Weld, D. </author> <year> 1993. </year> <title> Characterizing subgoal interactions for planning. </title> <booktitle> In Proc. 13th Int. Joint Conf. on A.I., </booktitle> <pages> 1388-1393. </pages>
Reference-contexts: Many environments have been stabilized (Hammond, Converse, & Grass 1992) (e.g., by implementing reversible commands or adding dishwashers) in a way that makes them easy to keep tidy. * We conjecture that, for a partial-order planner, most cleanup goals are trivially serializable <ref> (Barrett & Weld 1993) </ref> with respect to each other. 9 When these properties are true of restore constraints in a domain, our tidiness algorithm does sat isfy constraint 2. Trivial serializability ensures that backtracking over phase one decisions (or previously achieved cleanup goals) is unnecessary. Tractability is another issue. <p> Trivial serializability means that every subgoal ordering allows monotonic progress <ref> (Barrett & Weld 1993) </ref>. While goal ordering is often important among the top level goals, we observe that cleanup goals are usually trivially serial-izable once the block of top level goals has been solved.
Reference: <author> Chapman, D. </author> <year> 1987. </year> <title> Planning for conjunctive goals. </title> <booktitle> Artificial Intelligence 32(3) </booktitle> <pages> 333-377. </pages>
Reference: <author> Davis, E. </author> <year> 1990. </year> <title> Representations of Commonsense Knowledge. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: The expressive power of our constraint language is weaker than that of utility functions, but our constraints are easier to incorporate into standard planning algorithms. By using a general, temporal logic such as that of (Shoham 1988) or <ref> (Davis 1990, Ch. 5) </ref> we could specify constraints that would ensure the agent would not cause harm. Before executing an action, we could ask an agent to prove that the action is not harmful. While elegant, this approach is computationally intractable as well.
Reference: <author> Dean, T., Firby, J., and Miller, D. </author> <year> 1988. </year> <title> Hierarchical planning involving deadlines, travel times, and resources. </title> <booktitle> Computational Intelligence 4(4) </booktitle> <pages> 381-398. </pages>
Reference: <author> Drummond, M. </author> <year> 1989. </year> <title> Situated control rules. </title> <booktitle> In Proceedings of the First International Conference on Knowledge Representation and Reasoning. </booktitle>
Reference-contexts: We say that A 1 ; : : : ; A n satisfies the constraint dont-disturb (C) if for all j 2 [1; n], for all sentences C, and for all substitutions , if w 0 j= C then w j j= C (1) Unlike the behavioral constraints of <ref> (Drummond 1989) </ref> and others, dont-disturb does not require the agent to make C true over a particular time interval; rather, the agent must avoid creating any additional violations of C.
Reference: <author> Etzioni, O., Lesh, N., and Segal, R. </author> <year> 1993. </year> <title> Building softbots for UNIX (preliminary report). </title> <type> Technical Report 93-09-01, </type> <institution> University of Washington. </institution> <note> Available via anonymous FTP from pub/etzioni/softbots/ at cs.washington.edu. </note>
Reference: <author> Etzioni, O. </author> <year> 1991. </year> <title> Embedding decision-analytic control in a learning architecture. </title> <journal> Artificial Intelligence 49(1-3):129-160. </journal>
Reference: <author> Etzioni, O. </author> <year> 1993. </year> <title> Intelligence without robots (a reply to brooks). </title> <journal> AI Magazine 14(4). </journal> <note> Available via anonymous FTP from pub/etzioni/softbots/ at cs.washington.edu. </note>
Reference: <author> Fox, M., and Smith, S. </author> <year> 1984. </year> <title> ISIS | a knowldges-based system for factory scheduling. </title> <booktitle> Expert Systems 1(1) </booktitle> <pages> 25-49. </pages>
Reference: <author> Haddawy, P., and Hanks, S. </author> <year> 1992. </year> <title> Representations for Decision-Theoretic Planning: Utility Functions for Dealine Goals. </title> <booktitle> In Proc. 3rd Int. Conf. on Principles of Knowledge Representation and Reasoning. </booktitle>
Reference: <author> Hammond, K., Converse, T., and Grass, J. </author> <year> 1992. </year> <title> The stabilization of environments. </title> <journal> Artificial Intelligence. </journal> <note> To appear. </note>
Reference-contexts: There are two reasons why tidi-ness is often easy to achieve (e.g., in software domains and kitchens): * Most actions are reversible. The compress action has uncompress as an inverse. Similarly, a short sequence of actions will clean up most messes in a kitchen. Many environments have been stabilized <ref> (Hammond, Converse, & Grass 1992) </ref> (e.g., by implementing reversible commands or adding dishwashers) in a way that makes them easy to keep tidy. * We conjecture that, for a partial-order planner, most cleanup goals are trivially serializable (Barrett & Weld 1993) with respect to each other. 9 When these properties are <p> A precursor of dont-disturb is discussed in the work of Wilensky and more extensively by Luria (Luria 1988) under the heading of "goal conflict." Similarly, a precursor of restore is mentioned briefly in Hammond et. al's analysis of "stabilization" under the heading of "clean up plans" <ref> (Hammond, Converse, & Grass 1992) </ref>. Our advances include precise and unified semantics for the notions, a mechanism for incorporating dont-disturb and restore into standard planning algorithms, and an analysis of the computational complexity of enforcing safety and tidiness. Even so, our work raises more questions than it an swers.
Reference: <author> Korf, R. </author> <year> 1987. </year> <title> Planning as search: A quantitative approach. </title> <booktitle> Artificial Intelligence 33(1) </booktitle> <pages> 65-88. </pages>
Reference-contexts: To make an omelet, you have to break some eggs. The question is, "How many?" Since squandering resources clearly constitutes harm, we could tag a valuable resources with a min-consume constraint and demand that the agent be thrifty | i.e., that it use as little as 9 Formally, serializability <ref> (Korf 1987) </ref> means that there exists a ordering among the subgoals which allows each to be solved in turn without backtracking over past progress. Trivial serializability means that every subgoal ordering allows monotonic progress (Barrett & Weld 1993).
Reference: <author> Leveson, N. G. </author> <year> 1986. </year> <title> Software safety: Why, what, and how. </title> <journal> ACM Computing Surveys 18(2) </journal> <pages> 125-163. </pages>
Reference-contexts: Even so, our work raises more questions than it an swers. Are constraints like dont-disturb and restore the "right" way to represent harm to an agent? How does agent safety relate to the more general software safety <ref> (Leveson 1986) </ref>? Can we handle tradeoffs short of using expensive decision theoretic techniques? What guarantees can one provide on resource usage? Most importantly, how do we weaken the assumptions of a static world and complete information?
Reference: <author> Levesque, H., and Brachman, R. </author> <year> 1985. </year> <title> A fundamental tradeoff in knowledge representation. </title> <editor> In Brachman, R., and Levesque, H., eds., </editor> <booktitle> Readings in Knowledge Representation. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 42-70. </pages>
Reference-contexts: In general, a sequence of actions satisfies dont-disturb (C) if none of the actions make C false. Formally, we say that a plan satisfies an dont-disturb constraint when every consistent, totally-ordered, sequence of plan actions satisfies the constraint as defined below. ference tractable by formulating restricted representation languages <ref> (Levesque & Brachman 1985) </ref>. 4 Although unpalatable, this is standard in the planning literature. For example, a strips operator that moves block A from B to C must delete on (A,B) and also add clear (B) even though clear (x) could be defined as 8y :on (y; x).
Reference: <author> Luria, M. </author> <year> 1988. </year> <title> Knowledge Intensive Planning. </title> <type> Ph.D. Dissertation, </type> <institution> UC Berkeley. </institution> <note> Available as technical report UCB/CSD 88/433. </note>
Reference-contexts: We showed that the well-understood, and computational tractable, mechanism of threat detection can be extended to avoid harm. Other researchers have considered related questions. A precursor of dont-disturb is discussed in the work of Wilensky and more extensively by Luria <ref> (Luria 1988) </ref> under the heading of "goal conflict." Similarly, a precursor of restore is mentioned briefly in Hammond et. al's analysis of "stabilization" under the heading of "clean up plans" (Hammond, Converse, & Grass 1992).
Reference: <author> McAllester, D., and Rosenblitt, D. </author> <year> 1991. </year> <title> Systematic nonlinear planning. </title> <booktitle> In Proc. 9th Nat. Conf. on A.I., </booktitle> <pages> 634-639. </pages>
Reference-contexts: If violation ever returns something other than False, 5 If E contains "lifted variables" <ref> (McAllester & Rosen blitt 1991) </ref> (as opposed to universally quantified variables which pose no problem) then violation may return an overly conservative R. Soundness and safety are maintained, but completeness could be lost.
Reference: <author> Pednault, E. </author> <year> 1988. </year> <title> Synthesizing plans that contain actions with context-dependent effects. </title> <booktitle> Computational Intelligence 4(4) </booktitle> <pages> 356-372. </pages>
Reference-contexts: Evade: Alternatively, by definition of violation it is legal to execute A p as long as R violation (E; C) will not be true after execution. The planner can achieve this via goal regression, i.e. by computing the causation preconditions <ref> (Pednault 1988) </ref> for :R and A p , to be made true at the time when A p is executed. 7 4. Refuse: Otherwise, the planner must refuse to add A p and backtrack to find another way to to support G for A c . <p> Since violation returns :written.to.tape (paper.tex), the rm action threatens safety. To disarm the threat, the planner must perform one of the options above. Unfortunately, disavowal (option one) isn't viable since paper.tex exists 6 Note that :S is strictly weaker than Pednault's preservation preconditions <ref> (Pednault 1988) </ref> for A p and C; it is more akin to preservation preconditions to a single effect of the action. 7 While confrontation and evasion are similar in the sense that they negate a disjunct (S and R, respectively), they differ in two ways.
Reference: <author> Pednault, E. </author> <year> 1989. </year> <title> ADL: Exploring the middle ground between STRIPS and the situation calculus. </title> <booktitle> In Proc. 1st Int. Conf. on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> 324-332. </pages>
Reference-contexts: This sidesteps the ramification problem, since domain axioms are banned. Instead, we demand that individual action descriptions explicitly enumerate changes to every predicate that is affected. 4 Note, however, that we are not assuming the strips representation; Instead we adopt an action language (based on adl <ref> (Pednault 1989) </ref>) which includes universally quantified and disjunctive preconditions as well as conditional effects (Penberthy & Weld 1992). Given the above assumptions, the next two sections define the primitives dont-disturb and restore, and explain how they should be treated by a generative planning algorithm.
Reference: <author> Penberthy, J., and Weld, D. </author> <year> 1992. </year> <title> UCPOP: A sound, complete, partial order planner for ADL. </title> <booktitle> In Proc. 3rd Int. Conf. on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> 103-114. </pages> <note> Available via FTP from pub/ai/ at cs.washington.edu. </note>
Reference-contexts: Instead, we demand that individual action descriptions explicitly enumerate changes to every predicate that is affected. 4 Note, however, that we are not assuming the strips representation; Instead we adopt an action language (based on adl (Pednault 1989)) which includes universally quantified and disjunctive preconditions as well as conditional effects <ref> (Penberthy & Weld 1992) </ref>. Given the above assumptions, the next two sections define the primitives dont-disturb and restore, and explain how they should be treated by a generative planning algorithm. <p> If A x violates a restore constraint, A y does not, and no other actions can cleanup the mess, then phase two will fail to achieve tidiness. One 8 Case 3 is similar to the expansion of a universally quantified goal into the universal base <ref> (Penberthy & Weld 1992) </ref>, but case 3 removes ground literals that aren't true in the initial state. could fix this problem by making phase two failures spawn backtracking over phase one decisions, but this could engender exhaustive search over all possible ways of satisfying top level goals.
Reference: <author> Penberthy, J., and Weld, D. </author> <year> 1994. </year> <title> Temporal planning with continuous change. </title> <booktitle> In Proc. 12th Nat. Conf. on A.I. </booktitle>
Reference-contexts: Before executing an action, we could ask an agent to prove that the action is not harmful. While elegant, this approach is computationally intractable as well. Another alternative would be to use a planner such as ilp (Allen 1991) or zeno <ref> (Penberthy & Weld 1994) </ref> which supports temporally quantified goals. Unfortunately, at present these planners seem too inefficient for our needs. 2 Situated action researchers might suggest that non-deliberative, reactive agents could be made "safe" by carefully engineering their interactions with the environment.
Reference: <author> Pollack, M. </author> <year> 1992. </year> <title> The uses of plans. </title> <booktitle> Artificial Intelligence 57(1). </booktitle>
Reference: <author> Russell, S., and Wefald, E. </author> <year> 1991. </year> <title> Do the Right Thing. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Shoham, Y. </author> <year> 1988. </year> <title> Reasoning about Change: </title> <booktitle> Time and Causation from the Standpoint of Artificial Intelligence. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The expressive power of our constraint language is weaker than that of utility functions, but our constraints are easier to incorporate into standard planning algorithms. By using a general, temporal logic such as that of <ref> (Shoham 1988) </ref> or (Davis 1990, Ch. 5) we could specify constraints that would ensure the agent would not cause harm. Before executing an action, we could ask an agent to prove that the action is not harmful. While elegant, this approach is computationally intractable as well.
Reference: <author> Tate, A. </author> <year> 1977. </year> <title> Generating project networks. </title> <booktitle> In Proc. 5th Int. Joint Conf. on A.I., </booktitle> <pages> 888-893. </pages>
Reference: <author> Wellman, M., and Doyle, J. </author> <year> 1992. </year> <title> Modular utility representation for decision theoretic planning. </title> <booktitle> In Proc. 1st Int. Conf. on A.I. Planning Systems, </booktitle> <pages> 236-242. </pages>
Reference: <author> Wilkins, D. E. </author> <year> 1988. </year> <title> Practical Planning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Williamson, M., and Hanks, S. </author> <year> 1994. </year> <title> Optimal planning with a goal-directed utility model. </title> <booktitle> In Proc. 2nd Int. Conf. on A.I. Planning Systems. </booktitle>
References-found: 29

