URL: http://www.is.cs.cmu.edu/papers/speech/EUROSPEECH97/EUROSPEECH97-klaus.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.speech.publications.html
Root-URL: 
Email: kries@ira.uka.de ries+@cs.cmu.edu  
Title: A CLASS BASED APPROACH TO DOMAIN ADAPTATION AND CONSTRAINT INTEGRATION FOR EMPIRICAL M-GRAM MODELS  
Author: Klaus Ries 
Address: Pittsburgh, PA, USA  
Affiliation: Interactive Systems Laboratories University of Karlsruhe, Karlsruhe, Germany Carnegie Mellon University,  
Abstract: The first class based adaptation approaches [FGH + 97, Ueb97] take the use of classes in the construction of statistical m-gram models one significant step further than just using them as a smoothing technique: The m-gram of classes is trained on the large background corpus while the word likelihoods given the class are estimated on the small target corpus. To make full use of this technique a specialized clusteralgorithm has been developed [FGH + 97, Ueb97]. In this paper we extend class adaptation to make use of the m-gram distribution of the target domain. As a second independent contribution this paper introduces an efficient morphing algorithm, that tries to achieve adaptation by using a stochastic mapping of words between the vocabularies of the respective domains. As a result we can show, that for small adaptation steps class based adaptation is a very useful technique. For larger adaptation steps the perplexity of the modified model is greatly improved, yet no improvement over the unadapted model was observed when used in linear interpolation. Whether this is due to the fact that we use class based adaptation or that we do just modify the unigram distribution is still unresolved, although the new stochastic mapping technique might help to give an answer to this question in the future. 
Abstract-found: 1
Intro-found: 1
Reference: [BCP + 90] <author> Peter F. Brown, John Cocke, Stephen A. Della Pietra, Vincent J. Della Pietra, Fredrick Jellinek, John D. Lafferty, Robert L. Mercer, and Paul S. Roossin. </author> <title> A statistical approach to machine translation. </title> <journal> Computational Linguistics, </journal> <volume> 16(2) </volume> <pages> 79-85, </pages> <year> 1990. </year>
Reference-contexts: Without the modeling assumption of classes one can try to define a stochastic mapping function from one corpus to the other and estimate this mapping function directly without resorting to classes (section 4). Since, in contrast to the classical approach in statistical machine translation as exemplified as <ref> [BCP + 90] </ref>, no alignment between the two corpora exists, the model is trained to estimate statistical coocurrence properties of the target domain using cooccurence statistics from the source domain.
Reference: [FGH + 97] <author> Michael Finke, Petra Geutner, Hermann Hild, Thomas Kemp, Klaus Ries, and Martin West-phal. </author> <title> The Karlsruhe-Verbmobil Speech Recog-niton Engine. </title> <booktitle> In ICASSP, </booktitle> <year> 1997. </year>
Reference-contexts: These ideas did not have success on the Switchboard-domain, where [RMR95] shows in a contrastive experiment that a standard linear interpolation scheme works just as good or better. In [FZM + 96] class-based mgrams are used in a linear interpolation setting to make maximal use of all knowledge sources. <ref> [FGH + 97, Ueb97] </ref> estimated the probability ^p (wjc) of word w given the class c is known based on the small corpus, whereas the mgram-model on the classes is estimated on a larger corpus. <p> We will restrict ourselves to word classifications that partition the vocabulary and to specific choices of h and ^ h such that the model can be stated as below. Please note that for ^m = 1 we arrive at the model of the original work <ref> [FGH + 97, Ueb97] </ref>. p (c t jc t1 ; : : : ; c tm+1 ) ^p (w t jc t ; w t1 ; : : : ; w t ^m+1 ) The partitioning of the words into classes as well as the size of the m-gram model of <p> The grainsize of the classes is therefore indicative of the adaptation strength of this model. If the classes are getting too coarse the predictive power of the model might get too low in the case of the original work <ref> [FGH + 97, Ueb97] </ref>. By using higher order distributions of the target domain directly one can try to overcome the natural problem, that for coarse classes very little contextual information from the background corpus is used. <p> The log-likelihood of the standard class based model on testdata can be written as L = w 1 ;w 2 ^ N (w 1 ;w 2 ) In the original <ref> [FGH + 97, Ueb97] </ref> adapted scheme p (w 1 jc 1 ) from [KN93] was replaced by ^p (w 1 jc 1 ), the new scheme proposed here replaces this by ^p (w 1 jc 1 ; w 2 ) and the classes are found such that the likelihood is optimized <p> adapted scheme p (w 1 jc 1 ) from [KN93] was replaced by ^p (w 1 jc 1 ), the new scheme proposed here replaces this by ^p (w 1 jc 1 ; w 2 ) and the classes are found such that the likelihood is optimized on the testset. <ref> [KN93, FGH + 97, Ueb97] </ref> use leaving one out estimates to accout for the fact, that the target corpus is used to estimate ^p from ^ N (w 1 ; w 2 ). <p> In principle it would not be necessary to make a leaving one out estimate for p, yet we decide to do so since in some situations we also mix the target and the background corpus [Ueb97]. The description of the algorithm will be done only for the <ref> [FGH + 97, Ueb97] </ref> criterion. The new criterion using ^p (w 1 jc 1 ; w 2 ) can be constructed fairly similar, though the presentation is slightly more complicated. <p> We are also calculating p (w 1 ; w 2 ) incrementally since the implicit hidden constant of that calculation is fairly large otherwise. 5. EXPERIMENTS In the original experiments <ref> [FGH + 97] </ref> on the VerbMo-bil domain the only adaptation that needed to be done was a small change in the register of the language. It was demonstrated, that significant improvements over the straightforward and well know linear interpolation and over standard class based schemes are possible. <p> For the class based adaptation we used three differ ent partitions of the vocabulary in appr. 500 classes, one was derived with the standard word classification procedures (stand), one was derived using the adaptive word classification algorithm (adapt) <ref> [FGH + 97, Ueb97] </ref>. For the class based adaptation we used both just ^p being a unigram-distribution (uni) as well as ^p being a trigram distribution (tri-adapt), p is always a trigram. <p> CONCLUSION AND FUTURE RESEARCH The adaptation of the monogram is an important step in the adaptation process of mgram models to small changes in a given domain, which was already demonstrated earlier <ref> [FGH + 97] </ref>. The question addressed by this paper was, whether the change in the unigram distribution is also an important matter when adapting from one domain to another.
Reference: [Fun95] <author> Pascale Fung. </author> <title> Compiling bilingual lexicon entries from a non-parallel english-chinese corpus. </title> <booktitle> In The 3rd Annual Workshop on Very Large Corpora, </booktitle> <pages> pages 173-183, </pages> <address> Boston, Mas-sachusettes, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Some attempts to identify translation properties without alignments just based on coocurrence properties in bilingual corpora take a similar approach <ref> [Rei95, Fun95] </ref>. In our case the statistical property used is the bigram distribution, specifically we will define the stochastic mapping such that the image of p (w 1 ; w 2 ) models the bigram distribution of the target corpus optimally.
Reference: [FZM + 96] <author> Michael Finke, Torsten Zeppenfeld, Martin Maier, Laura Mayfield, Klaus Ries, Pum-ing Zhan, John Lafferty, and Alex Waibel. </author> <title> Switchboard evaluation report. </title> <booktitle> In Proceedings of LVCSR Hub 5 Workshop, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: These ideas did not have success on the Switchboard-domain, where [RMR95] shows in a contrastive experiment that a standard linear interpolation scheme works just as good or better. In <ref> [FZM + 96] </ref> class-based mgrams are used in a linear interpolation setting to make maximal use of all knowledge sources. [FGH + 97, Ueb97] estimated the probability ^p (wjc) of word w given the class c is known based on the small corpus, whereas the mgram-model on the classes is estimated
Reference: [IO96] <author> Rukmini Iyer and Mari Ostendorf. </author> <title> Modeling long distance dependence in language, topic mixtures vs. dynamic cache models. </title> <booktitle> In IC-SLP, </booktitle> <year> 1996. </year>
Reference-contexts: 1. INTRODUCTION The adaptation of statistical mgrams of language to new domains is one of the big challenges in speech recognition. In the past a number of approaches like topic dependent (maximum entropy) models [PPMR92, LS95, RMR95] and linear interpolation have been proposed. Additionally topic mixture models <ref> [IO96] </ref> and backoff models have been suggested. The research question brought up here is whether there is a way to parameterize m-grams such that a large mgram can be modified with a relatively small number of parameters.
Reference: [KN93] <author> Reinhard Kneser and Herman Ney. </author> <title> Improved clustering techniques for class-based statistical language modeling. </title> <booktitle> In Eurospeech, </booktitle> <address> Berlin, Germany, </address> <year> 1993. </year>
Reference-contexts: The log-likelihood of the standard class based model on testdata can be written as L = w 1 ;w 2 ^ N (w 1 ;w 2 ) In the original [FGH + 97, Ueb97] adapted scheme p (w 1 jc 1 ) from <ref> [KN93] </ref> was replaced by ^p (w 1 jc 1 ), the new scheme proposed here replaces this by ^p (w 1 jc 1 ; w 2 ) and the classes are found such that the likelihood is optimized on the testset. [KN93, FGH + 97, Ueb97] use leaving one out estimates <p> adapted scheme p (w 1 jc 1 ) from [KN93] was replaced by ^p (w 1 jc 1 ), the new scheme proposed here replaces this by ^p (w 1 jc 1 ; w 2 ) and the classes are found such that the likelihood is optimized on the testset. <ref> [KN93, FGH + 97, Ueb97] </ref> use leaving one out estimates to accout for the fact, that the target corpus is used to estimate ^p from ^ N (w 1 ; w 2 ). <p> Rear ranging terms we get the criterion: F ML = c 1 ;c 2 c Applying the leaving-one-out criterion the final criterion is received similar to <ref> [KN93] </ref>: F ML = c 1 ;c 2 ;N (c 1 ;c 2 )&gt;1 +^n 1 log n 0 + 1 X ^ N (c) (log (N (c) 1) + log ( ^ N (c) 1)) where b is an absolute discounting factor, n 1 is the number of bigrams occuring <p> This criterion corresponds exactly to <ref> [KN93] </ref> if the original corpus and the corpus that is being adapted on are identical. This criterion was integrated straightforward in the current fast implementation of [KN93], the criterion based on ^p (w 1 jc 1 ; w 2 ) needed some more significant changes to the code. 4. <p> This criterion corresponds exactly to <ref> [KN93] </ref> if the original corpus and the corpus that is being adapted on are identical. This criterion was integrated straightforward in the current fast implementation of [KN93], the criterion based on ^p (w 1 jc 1 ; w 2 ) needed some more significant changes to the code. 4.
Reference: [LS95] <author> John Lafferty and Bernhard Suhm. </author> <title> Efficient iterative scaling of a class of maximum entropy models. In XV Workshop on Maximum Entropy and Bayesian Methods, </title> <address> Los Alamos, </address> <year> 1995. </year>
Reference-contexts: 1. INTRODUCTION The adaptation of statistical mgrams of language to new domains is one of the big challenges in speech recognition. In the past a number of approaches like topic dependent (maximum entropy) models <ref> [PPMR92, LS95, RMR95] </ref> and linear interpolation have been proposed. Additionally topic mixture models [IO96] and backoff models have been suggested. The research question brought up here is whether there is a way to parameterize m-grams such that a large mgram can be modified with a relatively small number of parameters. <p> Following this idea maximum entropy models have been proposed, which adapt the monogram distribution of higher order mgram models <ref> [PPMR92, LS95, RMR95] </ref>. These ideas did not have success on the Switchboard-domain, where [RMR95] shows in a contrastive experiment that a standard linear interpolation scheme works just as good or better.
Reference: [PPMR92] <author> S. Della Pietra, D. Della Pietra, R. L. Mercer, and S. Roukos. </author> <title> Adaptive language modeling using minimum discrimination information. </title> <booktitle> In ICASSP, </booktitle> <volume> volume 1, </volume> <pages> pages 633-637, </pages> <year> 1992. </year>
Reference-contexts: 1. INTRODUCTION The adaptation of statistical mgrams of language to new domains is one of the big challenges in speech recognition. In the past a number of approaches like topic dependent (maximum entropy) models <ref> [PPMR92, LS95, RMR95] </ref> and linear interpolation have been proposed. Additionally topic mixture models [IO96] and backoff models have been suggested. The research question brought up here is whether there is a way to parameterize m-grams such that a large mgram can be modified with a relatively small number of parameters. <p> Following this idea maximum entropy models have been proposed, which adapt the monogram distribution of higher order mgram models <ref> [PPMR92, LS95, RMR95] </ref>. These ideas did not have success on the Switchboard-domain, where [RMR95] shows in a contrastive experiment that a standard linear interpolation scheme works just as good or better.
Reference: [Rei95] <author> Rapp Reinhard. </author> <title> Identifying word translations in nonparallel texts. </title> <booktitle> In ACL, </booktitle> <year> 1995. </year>
Reference-contexts: Some attempts to identify translation properties without alignments just based on coocurrence properties in bilingual corpora take a similar approach <ref> [Rei95, Fun95] </ref>. In our case the statistical property used is the bigram distribution, specifically we will define the stochastic mapping such that the image of p (w 1 ; w 2 ) models the bigram distribution of the target corpus optimally.
Reference: [RMR95] <author> P. Srinivasa Rao, Michael D. Momkowski, and Salim Rouskos. </author> <title> Language modeling adaption via minimum discrimination information. </title> <booktitle> In ICASSP, </booktitle> <year> 1995. </year>
Reference-contexts: 1. INTRODUCTION The adaptation of statistical mgrams of language to new domains is one of the big challenges in speech recognition. In the past a number of approaches like topic dependent (maximum entropy) models <ref> [PPMR92, LS95, RMR95] </ref> and linear interpolation have been proposed. Additionally topic mixture models [IO96] and backoff models have been suggested. The research question brought up here is whether there is a way to parameterize m-grams such that a large mgram can be modified with a relatively small number of parameters. <p> Following this idea maximum entropy models have been proposed, which adapt the monogram distribution of higher order mgram models <ref> [PPMR92, LS95, RMR95] </ref>. These ideas did not have success on the Switchboard-domain, where [RMR95] shows in a contrastive experiment that a standard linear interpolation scheme works just as good or better. <p> Following this idea maximum entropy models have been proposed, which adapt the monogram distribution of higher order mgram models [PPMR92, LS95, RMR95]. These ideas did not have success on the Switchboard-domain, where <ref> [RMR95] </ref> shows in a contrastive experiment that a standard linear interpolation scheme works just as good or better.
Reference: [Ueb97] <author> Joerg P Ueberla. </author> <title> Domain adaptation with clustered language models. </title> <booktitle> In ICASSP, </booktitle> <year> 1997. </year>
Reference-contexts: These ideas did not have success on the Switchboard-domain, where [RMR95] shows in a contrastive experiment that a standard linear interpolation scheme works just as good or better. In [FZM + 96] class-based mgrams are used in a linear interpolation setting to make maximal use of all knowledge sources. <ref> [FGH + 97, Ueb97] </ref> estimated the probability ^p (wjc) of word w given the class c is known based on the small corpus, whereas the mgram-model on the classes is estimated on a larger corpus. <p> We will restrict ourselves to word classifications that partition the vocabulary and to specific choices of h and ^ h such that the model can be stated as below. Please note that for ^m = 1 we arrive at the model of the original work <ref> [FGH + 97, Ueb97] </ref>. p (c t jc t1 ; : : : ; c tm+1 ) ^p (w t jc t ; w t1 ; : : : ; w t ^m+1 ) The partitioning of the words into classes as well as the size of the m-gram model of <p> The grainsize of the classes is therefore indicative of the adaptation strength of this model. If the classes are getting too coarse the predictive power of the model might get too low in the case of the original work <ref> [FGH + 97, Ueb97] </ref>. By using higher order distributions of the target domain directly one can try to overcome the natural problem, that for coarse classes very little contextual information from the background corpus is used. <p> The log-likelihood of the standard class based model on testdata can be written as L = w 1 ;w 2 ^ N (w 1 ;w 2 ) In the original <ref> [FGH + 97, Ueb97] </ref> adapted scheme p (w 1 jc 1 ) from [KN93] was replaced by ^p (w 1 jc 1 ), the new scheme proposed here replaces this by ^p (w 1 jc 1 ; w 2 ) and the classes are found such that the likelihood is optimized <p> adapted scheme p (w 1 jc 1 ) from [KN93] was replaced by ^p (w 1 jc 1 ), the new scheme proposed here replaces this by ^p (w 1 jc 1 ; w 2 ) and the classes are found such that the likelihood is optimized on the testset. <ref> [KN93, FGH + 97, Ueb97] </ref> use leaving one out estimates to accout for the fact, that the target corpus is used to estimate ^p from ^ N (w 1 ; w 2 ). <p> In principle it would not be necessary to make a leaving one out estimate for p, yet we decide to do so since in some situations we also mix the target and the background corpus <ref> [Ueb97] </ref>. The description of the algorithm will be done only for the [FGH + 97, Ueb97] criterion. The new criterion using ^p (w 1 jc 1 ; w 2 ) can be constructed fairly similar, though the presentation is slightly more complicated. <p> In principle it would not be necessary to make a leaving one out estimate for p, yet we decide to do so since in some situations we also mix the target and the background corpus [Ueb97]. The description of the algorithm will be done only for the <ref> [FGH + 97, Ueb97] </ref> criterion. The new criterion using ^p (w 1 jc 1 ; w 2 ) can be constructed fairly similar, though the presentation is slightly more complicated. <p> For the class based adaptation we used three differ ent partitions of the vocabulary in appr. 500 classes, one was derived with the standard word classification procedures (stand), one was derived using the adaptive word classification algorithm (adapt) <ref> [FGH + 97, Ueb97] </ref>. For the class based adaptation we used both just ^p being a unigram-distribution (uni) as well as ^p being a trigram distribution (tri-adapt), p is always a trigram. <p> If more than one model was used a context independent linear interpolation of the resulting models was used, where the weights were optimized on held out data of SWB. The perplexity is evaluated on a separate testset from SWB. Additional to that we used the proposal by <ref> [Ueb97] </ref> to use linear interpolation in the class-finding process.
References-found: 11

