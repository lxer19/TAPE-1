URL: ftp://ftp.cs.wisc.edu/math-prog/tech-reports/97-03.ps
Refering-URL: http://www.cs.wisc.edu/~paulb/papers.html
Root-URL: 
Title: Parsimonious Least Norm Approximation  
Author: P. S. Bradley O. L. Mangasarian J. B. Rosen 
Keyword: Minimal cardinality, least norm approximation  
Abstract: A theoretically justifiable fast finite successive linear approximation algorithm is proposed for obtaining a parsimonious solution to a corrupted linear system Ax = b + p, where the corruption p is due to noise or error in measurement. The proposed linear-programming-based algorithm finds a solution x by parametrically minimizing the number of nonzero elements in x and the error k Ax b p k 1 . Numerical tests on a signal-processing-based example indicate that the proposed method is comparable to a method that parametrically minimizes the 1-norm of the solution x and the error k Ax b p k 1 , and that both methods are superior, by orders of magnitude, to solutions obtained by least squares as well by combinatorially choosing an optimal solution with a specific number of nonzero elements. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. J. Abatzoglou, J. M. Mendel, and G. A. Harada. </author> <title> The constrained total least squares technique and its application to harmonic superposition. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 39 </volume> <pages> 1070-1087, </pages> <year> 1991. </year>
Reference-contexts: In fact problem (2) can be viewed as a multiobjective optimization problem [8] with the the two objectives of parsimony in the number of nonzero components of x and smallness of the error kAx bk 1 . By letting range over the interval <ref> [0; 1] </ref> the cardinality of the nonzero elements of the solution x varies from a maximum of n to 0, while the error kAx bk 1 will be nondecreasing monotonically. Depending on the problem, one of those x's will be the most desirable. <p> One can relate this to a machine learning framework by treating the first system as a training set, and the second system as a testing set [12]. The linear systems used are based upon ideas related to signal processing [10, 28] and more specifically to an example in <ref> [1, Equation (8)] </ref>. <p> The linear systems used are based upon ideas related to signal processing [10, 28] and more specifically to an example in [1, Equation (8)]. We consider the following true signal g (t) : <ref> [0; 1] </ref> ! R: g (t) = j=1 We assume that the true signal g (t) cannot be sampled precisely, but that the following observed signal can be sampled: ~g (t) = (g (t) + error); sampled at times : t i = i 4 t; 4t = 0:04; i = <p> i = i 4 t; 4t = 0:04; i = 0; 1; : : : ; 25: (19) We further assume that we do not know the true signal g (t) (18), and we attempt to model it as: 10 X x j " a j t ; t 2 <ref> [0; 1] </ref>; a = [0 4 7 0:1 2 3 3:9 4:1 6:9 7:1] 0 : (20) The problem now is to compute the coefficients x j ; j = 1; : : :; 10; of ^g (t) (20) so that we can adequately recover g (t), given only the noisy
Reference: [2] <author> E. Amaldi and V. Kann. </author> <title> On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems. </title> <type> Technical Report 96-15, </type> <institution> Cornell University, </institution> <address> Ithaca, NY, </address> <year> 1976. </year> <note> To Appear in Theoretical Computer Science. Available at http://www.cs.cornell.edu/Info/People/amaldi/amaldi.html. </note>
Reference-contexts: This is clearly a combinatorial problem which is closely related to the NP-hard problem considered by Amaldi and Kann <ref> [2] </ref> and consisting of solving a consistent system of linear inequalities or equalities with rational entries such that the solution x has a minimal number of nonzeros.
Reference: [3] <author> K. P. Bennett and J. A. </author> <title> Blue. A support vector machine approach to decision trees. </title> <institution> Department of Mathematical Sciences Math Report No. 97-100, Rensselaer Polytechnic Institute, </institution> <address> Troy, NY 12180, </address> <year> 1997. </year> <note> Available at http://www.math.rpi.edu/ bennek/. </note>
Reference-contexts: We then prescribe a linear-programming-based successive linearization algorithm SLA 3.1 for the solution of the smooth problem and establish its finite termination in Theorem 3.2. For comparative purposes we shall also employ Vapnik's support vector machine approach <ref> [29, 3] </ref> of minimizing the size of the solution vector x as well as the error kAx bk 1 , thereby decreasing the VC dimension [29, p 76] (a capacity measure) and improving generalization.
Reference: [4] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Occam's razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: Diego during his sabbatical leave January-May 1997. x Computer Science & Engineering, University of California San Diego, La Jolla, CA 92093 jbrosen@ucsd.edu 1 The idea behind using as few columns of A as possible to span b is motivated by the parsimony principle of machine learning, known as Occam's Razor <ref> [26, 4] </ref>, which says in essence: "simplest is best".
Reference: [5] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Clustering via concave minimization. </title> <editor> In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, </editor> <booktitle> Advances in Neural Information Processing Systems -9-, </booktitle> <pages> pages 368-374, </pages> <address> Cambridge, MA, 1997. </address> <publisher> MIT Press. ftp://ftp.cs.wisc.edu/math-prog/tech-reports/96-03.ps.Z. </publisher>
Reference-contexts: We shall solve our problem by a novel method, based on minimizing a concave function on a polyhedral set, that has been successfully used in such machine learning problems as misclassification minimization [17], and feature selection [6] and in data mining <ref> [5, 19] </ref>. fl Mathematical Programming Technical Report 97-03, March 1997 Revised September & November 1997.
Reference: [6] <author> P. S. Bradley, O. L. Mangasarian, and W. N. </author> <title> Street. Feature selection via mathematical programming. </title> <journal> INFORMS Journal on Computing, </journal> <note> 1998. To appear. Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-21.ps.Z. </note>
Reference-contexts: We shall solve our problem by a novel method, based on minimizing a concave function on a polyhedral set, that has been successfully used in such machine learning problems as misclassification minimization [17], and feature selection <ref> [6] </ref> and in data mining [5, 19]. fl Mathematical Programming Technical Report 97-03, March 1997 Revised September & November 1997. <p> Depending on the problem, one of those x's will be the most desirable. In many of the machine learning applications small values of such as 0:05 often gave parsimonious results that improved tenfold cross-validation <ref> [6] </ref>. We shall call problem (2), with a possibly noise-corrupted b, the parsimonious least norm approximation problem (PLNA). Our approach here for solving (2) will be to convert it to a concave minimization problem on a polyhedral set (problem (12) below). <p> We first show that this problem always has a solution (Theorem 2.1 below). We then replace the discontinuous step function in the objective function of (12) below by an exponential smooth function in problem (14) below, just as was done in <ref> [18, 6] </ref>, and relate the two problems. Our novel theorem (Theorem 2.1 below) shows that the continuous problem yields an exact solution of the discontinuous problem once a repeating optimal vertex is identified for increasing but finite values of the smoothing parameter ff.
Reference: [7] <author> CPLEX Optimization Inc., </author> <title> Incline Village, Nevada. Using the CPLEX(TM) Linear Optimizer and CPLEX(TM) Mixed Integer Optimizer (Version 2.0), </title> <booktitle> 1992. </booktitle> <pages> 17 </pages>
Reference-contexts: The time needed by each approach to compute a solution was determined by performing a single run on a Sun SparcStation 20 with 96 megabytes of memory running MATLAB 5.1, using the commands "tic" and "toc" [21]. All linear programs were solved with CPLEX <ref> [7] </ref> interfaced with MATLAB. Solving the PLNA problem with = 0:5 with initial point (24) and ff = 5 took 0.4603 seconds. Solving the LLNA problem with = 0:5 took 0.1978 seconds. Determining the least squares solution by Algorithm 4.1 with t = 0:0001 took 0.0224 seconds.
Reference: [8] <author> Luc Dinh. </author> <title> Theory of Vector Optimization. </title> <booktitle> Lecture Notes in Economics and Mathematical Systems 319. </booktitle> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: We are interested in solutions to problem (2) with 2 [0; 1) that make e 0 jxj fl k for some desired k &lt; n and such that kAx bk 1 is acceptably small. In fact problem (2) can be viewed as a multiobjective optimization problem <ref> [8] </ref> with the the two objectives of parsimony in the number of nonzero components of x and smallness of the error kAx bk 1 .
Reference: [9] <author> M. Frank and P. Wolfe. </author> <title> An algorithm for quadratic programming. </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 3 </volume> <pages> 95-110, </pages> <year> 1956. </year>
Reference-contexts: our attention now to solving (14) by a finitely terminating successive linearization algorithm. 4 3 The Concave Minimization Algorithm The finite method that we shall propose is the successive linear approximation (SLA) method of minimizing a concave function on a polyhedral set which is a finitely terminating stepless Frank-Wolfe algorithm <ref> [9] </ref>. In [18] finite termination of the SLA was established for a differentiable concave function, and in [20] for a nondifferentiable concave function using its supergradient. We state now the SLA for problem (14) which has a differentiable concave objective function.
Reference: [10] <author> Arthur A. Giordano. </author> <title> Least Square Estimation With Applications to Digital Signal Processing. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: One can relate this to a machine learning framework by treating the first system as a training set, and the second system as a testing set [12]. The linear systems used are based upon ideas related to signal processing <ref> [10, 28] </ref> and more specifically to an example in [1, Equation (8)].
Reference: [11] <author> G. H. Golub and C. F. Van Loan. </author> <title> An analysis of the total least squares problem. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 17 </volume> <pages> 883-893, </pages> <year> 1980. </year>
Reference-contexts: Methods for solving such problems include least squares [15], total least squares <ref> [11, 14] </ref> and structured total least norm [24, 13].
Reference: [12] <author> M. H. Hassoun. </author> <title> Fundamentals of Artificial Neural Networks. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1995. </year>
Reference-contexts: One can relate this to a machine learning framework by treating the first system as a training set, and the second system as a testing set <ref> [12] </ref>. The linear systems used are based upon ideas related to signal processing [10, 28] and more specifically to an example in [1, Equation (8)].
Reference: [13] <author> S. Van Huffel, H. Park, and J. B. Rosen. </author> <title> Formulation and solution of structured total least norm problems for parameter estimation. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 44 </volume> <pages> 2464-2474, </pages> <year> 1996. </year>
Reference-contexts: Methods for solving such problems include least squares [15], total least squares [11, 14] and structured total least norm <ref> [24, 13] </ref>. In this paper we consider the closely related problem of minimizing the 1-norm of the residual vector Ax b, where b is subject to error and with the additional condition that only a specified number k &lt; n of columns of A are used.
Reference: [14] <author> S. Van Huffel and J. Vandewalle. </author> <title> The Total Least Squares Problem, Computational Aspects and Analysis. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1991. </year>
Reference-contexts: Methods for solving such problems include least squares [15], total least squares <ref> [11, 14] </ref> and structured total least norm [24, 13].
Reference: [15] <author> C. L. Lawson and R. J. Hanson. </author> <title> Solving Least Squares Problems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1974. </year>
Reference-contexts: Methods for solving such problems include least squares <ref> [15] </ref>, total least squares [11, 14] and structured total least norm [24, 13].
Reference: [16] <author> Y. le Cun, J. S. Denker, and S. A. Solla. </author> <title> Optimal brain damage. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems II (Denver 1989), </booktitle> <pages> pages 598-605, </pages> <address> San Mateo, California, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This principle is highly effective for generalization purposes <ref> [16, 25, 30] </ref> where, for example, one wishes to use the "solution" x of (1) on new data not represented by the rows of [A b] as would be the case if either A or b is corrupted by noise.
Reference: [17] <author> O. L. Mangasarian. </author> <title> Misclassification minimization. </title> <journal> Journal of Global Optimization, </journal> <volume> 5 </volume> <pages> 309-323, </pages> <year> 1994. </year>
Reference-contexts: We shall solve our problem by a novel method, based on minimizing a concave function on a polyhedral set, that has been successfully used in such machine learning problems as misclassification minimization <ref> [17] </ref>, and feature selection [6] and in data mining [5, 19]. fl Mathematical Programming Technical Report 97-03, March 1997 Revised September & November 1997.
Reference: [18] <author> O. L. Mangasarian. </author> <title> Machine learning via polyhedral concave minimization. </title> <editor> In H. Fischer, B. Riedmueller, and S. Schae*er, editors, </editor> <booktitle> Applied Mathematics and Parallel Computing - Festschrift for Klaus Ritter, </booktitle> <pages> pages 175-188. </pages> <publisher> Physica-Verlag A Springer-Verlag Company, </publisher> <address> Hei-delberg, </address> <year> 1996. </year> <note> Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-20.ps.Z. </note>
Reference-contexts: We first show that this problem always has a solution (Theorem 2.1 below). We then replace the discontinuous step function in the objective function of (12) below by an exponential smooth function in problem (14) below, just as was done in <ref> [18, 6] </ref>, and relate the two problems. Our novel theorem (Theorem 2.1 below) shows that the continuous problem yields an exact solution of the discontinuous problem once a repeating optimal vertex is identified for increasing but finite values of the smoothing parameter ff. <p> In <ref> [18] </ref> finite termination of the SLA was established for a differentiable concave function, and in [20] for a nondifferentiable concave function using its supergradient. We state now the SLA for problem (14) which has a differentiable concave objective function. <p> i+1 ; y i+1 ; z i+1 ) 2 arg min (1 )e 0 y + ff (" ffz i Stop when (x i ; y i ; z i ) 2 T and (1 )e 0 y i + ff (" ffz i ) 0 z i+1 (16) By <ref> [18, Theorem 4.2] </ref> we have the following finite termination result for the SLA algorithm.
Reference: [19] <author> O. L. Mangasarian. </author> <title> Mathematical programming in data mining. Data Mining and Knowledge Discovery, </title> <booktitle> 1(2) </booktitle> <pages> 183-201, </pages> <year> 1997. </year> <note> Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/96-05.ps.Z. </note>
Reference-contexts: We shall solve our problem by a novel method, based on minimizing a concave function on a polyhedral set, that has been successfully used in such machine learning problems as misclassification minimization [17], and feature selection [6] and in data mining <ref> [5, 19] </ref>. fl Mathematical Programming Technical Report 97-03, March 1997 Revised September & November 1997.
Reference: [20] <author> O. L. Mangasarian. </author> <title> Solution of general linear complementarity problems via nondifferen-tiable concave minimization. </title> <journal> Acta Mathematica Vietnamica, </journal> <volume> 22(1) </volume> <pages> 199-205, </pages> <year> 1997. </year> <note> Available at ftp://ftp.cs.wisc.edu/math-prog/tech-reports/96-10.ps.Z. </note>
Reference-contexts: In [18] finite termination of the SLA was established for a differentiable concave function, and in <ref> [20] </ref> for a nondifferentiable concave function using its supergradient. We state now the SLA for problem (14) which has a differentiable concave objective function.
Reference: [21] <author> MATLAB. </author> <title> User's Guide. The MathWorks, </title> <publisher> Inc., </publisher> <year> 1992. </year>
Reference-contexts: Thus we resort to a singular value decomposition approach for solving (27). We determine an approximate solution x (ls) to (27) by the following method which utilizes the singular value decomposition [27]. Ordinary MATLAB <ref> [21] </ref> commands such as x = An (b + p) for our perturbed system Ax = b + p give an x with an error kx x fl k 2 = 2:1379e + 08 compared to kx x fl k 2 = 2:6675e + 03 given by the method described below, <p> Algorithm 4.1 Least Squares via Singular Value Decomposition. Let A 2 R mfin with m n. Let t be a small positive tolerance. 1. Determine the economy singular value decomposition of A <ref> [21, svd (A,0)] </ref>, U 2 R mfin ; S 2 R nfin ; V 2 R nfin : A = U SV 0 ; (28) where U 0 U = V 0 V = I n (the n fi n identity matrix), and S = diag ( 1 ; : : <p> The time needed by each approach to compute a solution was determined by performing a single run on a Sun SparcStation 20 with 96 megabytes of memory running MATLAB 5.1, using the commands "tic" and "toc" <ref> [21] </ref>. All linear programs were solved with CPLEX [7] interfaced with MATLAB. Solving the PLNA problem with = 0:5 with initial point (24) and ff = 5 took 0.4603 seconds. Solving the LLNA problem with = 0:5 took 0.1978 seconds.
Reference: [22] <author> B. T. Polyak. </author> <title> Introduction to Optimization. Optimization Software, </title> <publisher> Inc., Publications Division, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: The set D (f (x)) of supergradients of f at the point x is nonempty, convex, compact and reduces to the ordinary gradient rf (x), when f is differentiable at x <ref> [22, 23] </ref>.
Reference: [23] <author> R. T. Rockafellar. </author> <title> Convex Analysis. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1970. </year> <month> 18 </month>
Reference-contexts: The set D (f (x)) of supergradients of f at the point x is nonempty, convex, compact and reduces to the ordinary gradient rf (x), when f is differentiable at x <ref> [22, 23] </ref>. <p> Note that if the objective function of (6) is concave (which it is not in general because of the nonconcavity of h 0 jsj fl ) then by <ref> [23, Corollary 32.3.3] </ref> problem (6) has a solution and by [23, Corollary 32.3.4] it has a vertex solution since S contains no straight lines that go to infinity in both directions. <p> Note that if the objective function of (6) is concave (which it is not in general because of the nonconcavity of h 0 jsj fl ) then by [23, Corollary 32.3.3] problem (6) has a solution and by <ref> [23, Corollary 32.3.4] </ref> it has a vertex solution since S contains no straight lines that go to infinity in both directions. <p> + h 0 (e " ffz ); where (s; z) 2 T := f (s; z) j j j s 2 S; z s zg: (9) Since the objective function of this problem is concave in (s; z) on R 2k and is bounded below on T, it follows by <ref> [23, Corollaries 32.3.3 and 32.3.4] </ref> that it has a vertex (s (ff); z (ff)) of T as a solution for each ff &gt; 0.
Reference: [24] <author> J. Ben Rosen, Haesun Park, and John Glick. </author> <title> Total least norm formulation and solution for structured problems. </title> <journal> SIAM Journal on Matrix Analysis, </journal> <volume> 17(1) </volume> <pages> 110-128, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Methods for solving such problems include least squares [15], total least squares [11, 14] and structured total least norm <ref> [24, 13] </ref>. In this paper we consider the closely related problem of minimizing the 1-norm of the residual vector Ax b, where b is subject to error and with the additional condition that only a specified number k &lt; n of columns of A are used.
Reference: [25] <author> C. Schaffer. </author> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10 </volume> <pages> 153-178, </pages> <year> 1993. </year>
Reference-contexts: This principle is highly effective for generalization purposes <ref> [16, 25, 30] </ref> where, for example, one wishes to use the "solution" x of (1) on new data not represented by the rows of [A b] as would be the case if either A or b is corrupted by noise.
Reference: [26] <author> J. W. Shavlik and T. G. Dietterich (editors). </author> <booktitle> Readings in Machine Learning. </booktitle> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, California, </address> <year> 1990. </year>
Reference-contexts: Diego during his sabbatical leave January-May 1997. x Computer Science & Engineering, University of California San Diego, La Jolla, CA 92093 jbrosen@ucsd.edu 1 The idea behind using as few columns of A as possible to span b is motivated by the parsimony principle of machine learning, known as Occam's Razor <ref> [26, 4] </ref>, which says in essence: "simplest is best".
Reference: [27] <author> G. Strang. </author> <title> Introduction to Linear Algebra. </title> <publisher> Wellesley-Cambridge Press, </publisher> <address> Wellesley, MA, </address> <year> 1993. </year>
Reference-contexts: Thus we resort to a singular value decomposition approach for solving (27). We determine an approximate solution x (ls) to (27) by the following method which utilizes the singular value decomposition <ref> [27] </ref>.
Reference: [28] <author> Charles W. Therrien. </author> <title> Discrete Random Signals and Statistical Signal Processing. </title> <publisher> Prentice-Hall, Inc, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: One can relate this to a machine learning framework by treating the first system as a training set, and the second system as a testing set [12]. The linear systems used are based upon ideas related to signal processing <ref> [10, 28] </ref> and more specifically to an example in [1, Equation (8)].
Reference: [29] <author> V. N. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: We then prescribe a linear-programming-based successive linearization algorithm SLA 3.1 for the solution of the smooth problem and establish its finite termination in Theorem 3.2. For comparative purposes we shall also employ Vapnik's support vector machine approach <ref> [29, 3] </ref> of minimizing the size of the solution vector x as well as the error kAx bk 1 , thereby decreasing the VC dimension [29, p 76] (a capacity measure) and improving generalization. <p> For comparative purposes we shall also employ Vapnik's support vector machine approach [29, 3] of minimizing the size of the solution vector x as well as the error kAx bk 1 , thereby decreasing the VC dimension <ref> [29, p 76] </ref> (a capacity measure) and improving generalization.
Reference: [30] <author> D. H. Wolpert, </author> <title> editor. The Mathematics of Generalization, </title> <address> Reading, MA, 1995. </address> <publisher> Addison-Wesley. </publisher> <pages> 19 </pages>
Reference-contexts: This principle is highly effective for generalization purposes <ref> [16, 25, 30] </ref> where, for example, one wishes to use the "solution" x of (1) on new data not represented by the rows of [A b] as would be the case if either A or b is corrupted by noise.
References-found: 30

