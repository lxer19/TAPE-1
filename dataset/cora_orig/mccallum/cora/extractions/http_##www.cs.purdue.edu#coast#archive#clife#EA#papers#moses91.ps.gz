URL: http://www.cs.purdue.edu/coast/archive/clife/EA/papers/moses91.ps.gz
Refering-URL: http://www.cs.purdue.edu/coast/archive/clife/EA/papers/
Root-URL: http://www.cs.purdue.edu
Email: baeck@unido.uucp  frank@unido.uucp  
Title: Adaptive Search by Evolutionary Algorithms  
Author: Thomas Back Frank Hoffmeister 
Date: December 1990  
Abstract: Evolution Strategies (ESs) and Genetic Algorithms (GAs) are compared in a formal way. It is shown, that both algorithms are identical with respect to their major working scheme, but nevertheless they exhibit significant differences with respect to the details of the selection scheme, the amount of the genetic representation and, especially, the self-adaptation of strategy parameters.
Abstract-found: 1
Intro-found: 1
Reference: [Bak85] <author> James Edward Baker. </author> <title> Adaptive selection methods for genetic algorithms. </title> <editor> In J. J. Grefenstette, editor, </editor> <booktitle> Proceedings of the First International Conference on Genetic Algorithms and Their Applications, </booktitle> <pages> pages 101-111, </pages> <address> Hillsdale, New Jersey, 1985. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: This can also be formulated as a contradiction between exploration of the search space and the amount of exploitation. Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values [Jon75, Gol89, GB89], crowding [Jon75] and generation gap [Gre86], sharing [DG89, GR87], ranking <ref> [Bak85] </ref>, and a multiple-point crossover [CES89, ECS89, SCED89]. 4 Comparison of the Algorithms Many differences between ESs and GAs directly or indirectly stem from a substantial difference in the amount of `genetic' information and its representation.
Reference: [CES89] <editor> Richard A. Caruna, Larry J. Eshelman, and J. David Schaffer. </editor> <title> Representation and hidden bias II: Eliminating defining length bias in genetic search via shu*e crossover. </title> <editor> In N. S. Sridharan, editor, </editor> <booktitle> Eleventh international joint conference on artificial intelligence, </booktitle> <pages> pages 750-755. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> August </month> <year> 1989. </year>
Reference-contexts: Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values [Jon75, Gol89, GB89], crowding [Jon75] and generation gap [Gre86], sharing [DG89, GR87], ranking [Bak85], and a multiple-point crossover <ref> [CES89, ECS89, SCED89] </ref>. 4 Comparison of the Algorithms Many differences between ESs and GAs directly or indirectly stem from a substantial difference in the amount of `genetic' information and its representation.
Reference: [DG89] <editor> Kalyanmoy Deb and David E. Goldberg. </editor> <title> An investigation of niche and species formation in genetic function optimization. </title> <booktitle> In Schaffer [Sch89], </booktitle> <pages> pages 42-50. </pages>
Reference-contexts: This can also be formulated as a contradiction between exploration of the search space and the amount of exploitation. Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values [Jon75, Gol89, GB89], crowding [Jon75] and generation gap [Gre86], sharing <ref> [DG89, GR87] </ref>, ranking [Bak85], and a multiple-point crossover [CES89, ECS89, SCED89]. 4 Comparison of the Algorithms Many differences between ESs and GAs directly or indirectly stem from a substantial difference in the amount of `genetic' information and its representation.
Reference: [ECS89] <editor> Larry L. Eshelman, Richard A. Caruna, and J. David Schaffer. </editor> <title> Biases in the crossover landscape. </title> <booktitle> In Schaffer [Sch89], </booktitle> <pages> pages 10-19. </pages>
Reference-contexts: Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values [Jon75, Gol89, GB89], crowding [Jon75] and generation gap [Gre86], sharing [DG89, GR87], ranking [Bak85], and a multiple-point crossover <ref> [CES89, ECS89, SCED89] </ref>. 4 Comparison of the Algorithms Many differences between ESs and GAs directly or indirectly stem from a substantial difference in the amount of `genetic' information and its representation.
Reference: [GB89] <author> John J. Grefenstette and James E. Baker. </author> <title> How genetic algorithms work: A critical look at implicit parallelism. </title> <booktitle> In Schaffer [Sch89], </booktitle> <pages> pages 20-27. </pages>
Reference-contexts: This can also be formulated as a contradiction between exploration of the search space and the amount of exploitation. Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values <ref> [Jon75, Gol89, GB89] </ref>, crowding [Jon75] and generation gap [Gre86], sharing [DG89, GR87], ranking [Bak85], and a multiple-point crossover [CES89, ECS89, SCED89]. 4 Comparison of the Algorithms Many differences between ESs and GAs directly or indirectly stem from a substantial difference in the amount of `genetic' information and its representation.
Reference: [Gol89] <author> David E. Goldberg. </author> <title> Sizing populations for serial and parallel genetic algorithms. </title> <booktitle> In Schaffer [Sch89], </booktitle> <pages> pages 70-79. </pages>
Reference-contexts: genetic operator set f : I ! IR fitness function t : I ! f0; 1g termination criterion The GA is working on a genotype-level of binary encoded individuals, a choice, which is founded by the argument of maximizing the number of schemata available with respect to a given code <ref> [Hol75, Gol89] </ref>. P 0 is the randomly generated initial population, and the parameters and l describe the number of individuals representing one generation and the length of the `genetic' representation of each individual, respectively. <p> This can also be formulated as a contradiction between exploration of the search space and the amount of exploitation. Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values <ref> [Jon75, Gol89, GB89] </ref>, crowding [Jon75] and generation gap [Gre86], sharing [DG89, GR87], ranking [Bak85], and a multiple-point crossover [CES89, ECS89, SCED89]. 4 Comparison of the Algorithms Many differences between ESs and GAs directly or indirectly stem from a substantial difference in the amount of `genetic' information and its representation.
Reference: [GR87] <author> David E. Goldberg and J. Richardson. </author> <title> Genetic algorithms with sharing for multimodal function optimization. </title> <editor> In J. J. Grefenstette, editor, </editor> <booktitle> Genetic Algorithms and their Applications, </booktitle> <pages> pages 41-49, </pages> <address> Hillsdale, New Jersey, 1987. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: This can also be formulated as a contradiction between exploration of the search space and the amount of exploitation. Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values [Jon75, Gol89, GB89], crowding [Jon75] and generation gap [Gre86], sharing <ref> [DG89, GR87] </ref>, ranking [Bak85], and a multiple-point crossover [CES89, ECS89, SCED89]. 4 Comparison of the Algorithms Many differences between ESs and GAs directly or indirectly stem from a substantial difference in the amount of `genetic' information and its representation.
Reference: [Gre86] <author> John J. Grefenstette. </author> <title> Optimization of control parameters for genetic algorithms. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> SMC-16(1):122-128, </volume> <year> 1986. </year>
Reference-contexts: This can also be formulated as a contradiction between exploration of the search space and the amount of exploitation. Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values [Jon75, Gol89, GB89], crowding [Jon75] and generation gap <ref> [Gre86] </ref>, sharing [DG89, GR87], ranking [Bak85], and a multiple-point crossover [CES89, ECS89, SCED89]. 4 Comparison of the Algorithms Many differences between ESs and GAs directly or indirectly stem from a substantial difference in the amount of `genetic' information and its representation.
Reference: [HB90] <author> Frank Hoffmeister and Thomas Back. </author> <title> Genetic algorithms and evolution strategies: Similarities and differences. </title> <type> Technical Report "Grune Reihe" No. 365, </type> <institution> Department of Computer Science, University of Dortmund, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: The aim of this paper is to give a formal description of both algorithms and to show their similarities as well as differences. A more detailed comparison (also in an experimental way) can be found in <ref> [HB90] </ref>. 2 Evolution Strategies Neglecting the earlier steps in the development of ESs ((1+1)-ES [Rec73]) it is concentrated here on the (+)-ES and the (,)-ES as developed by Schwefel [Sch77, Sch81a]. They provide the capability of self-adaptation of the strategy parameters.
Reference: [Hol75] <author> John H. Holland. </author> <title> Adaptation in natural and artificial systems. </title> <publisher> The University of Michigan Press, </publisher> <address> Ann Arbor, </address> <year> 1975. </year>
Reference-contexts: The left illustration in figure 1 shows some individuals with their corresponding ellipsoids if the step sizes are not correlated (simple mutations), while the right illustration shows the same individuals with correlated mutations. 3 Genetic Algorithms Abstracting from the work of Holland <ref> [Hol75] </ref>, a GA can be formulated as an 8-tuple: GA = (P 0 ; ; l; s; ; ; f; t) (8) where P 0 = (a 0 ) 2 I I = f0; 1g l initial population 2 N population size l 2 N length of individuals' representation s : <p> genetic operator set f : I ! IR fitness function t : I ! f0; 1g termination criterion The GA is working on a genotype-level of binary encoded individuals, a choice, which is founded by the argument of maximizing the number of schemata available with respect to a given code <ref> [Hol75, Gol89] </ref>. P 0 is the randomly generated initial population, and the parameters and l describe the number of individuals representing one generation and the length of the `genetic' representation of each individual, respectively. <p> This selection scheme is called proportional selection <ref> [Hol75] </ref>. It leads to the expectation of individual a t i to occur t i = p s (a t i ) times in generation t + 1 (generally t i is called the expected value of a t i ). <p> The genetic operator set f! : I fi I ! P ! Ig includes genetic operators like crossover and mutation <ref> [Hol75] </ref>. The stochastic elements of these operators (application probabilities, e.g. p m 0:001 and p c 0:6, selection of loci) are included in a somehow abstract manner in the probability distribution p 2 P.
Reference: [Jon75] <author> Kenneth De Jong. </author> <title> An analysis of the behaviour of a class of genetic adaptive systems. </title> <type> PhD thesis, </type> <institution> University of Michigan, </institution> <year> 1975. </year> <note> Diss. Abstr. Int. 36(10), 5140B, University Microfilms No. 76-9381. </note>
Reference-contexts: This can also be formulated as a contradiction between exploration of the search space and the amount of exploitation. Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values <ref> [Jon75, Gol89, GB89] </ref>, crowding [Jon75] and generation gap [Gre86], sharing [DG89, GR87], ranking [Bak85], and a multiple-point crossover [CES89, ECS89, SCED89]. 4 Comparison of the Algorithms Many differences between ESs and GAs directly or indirectly stem from a substantial difference in the amount of `genetic' information and its representation. <p> This can also be formulated as a contradiction between exploration of the search space and the amount of exploitation. Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values [Jon75, Gol89, GB89], crowding <ref> [Jon75] </ref> and generation gap [Gre86], sharing [DG89, GR87], ranking [Bak85], and a multiple-point crossover [CES89, ECS89, SCED89]. 4 Comparison of the Algorithms Many differences between ESs and GAs directly or indirectly stem from a substantial difference in the amount of `genetic' information and its representation.
Reference: [Rec73] <author> Ingo Rechenberg. </author> <title> Evolutionsstrategie: Optimierung technischer Systeme nach Prinzipien der biologis-chen Evolution. </title> <publisher> Frommann-Holzboog Verlag, Stuttgart, </publisher> <year> 1973. </year>
Reference-contexts: A more detailed comparison (also in an experimental way) can be found in [HB90]. 2 Evolution Strategies Neglecting the earlier steps in the development of ESs ((1+1)-ES <ref> [Rec73] </ref>) it is concentrated here on the (+)-ES and the (,)-ES as developed by Schwefel [Sch77, Sch81a]. They provide the capability of self-adaptation of the strategy parameters. <p> For the two model functions of a linear corridor and the sphere model, a maximum rate of convergence is achieved for the values 1 6:0 and 2 4:7. These values are pretty close to Rechenberg's 1/5 success rule for his deterministic control of the step size <ref> [Rec73] </ref>. Thus, ESs with = 1=6 are often tuned for a maximum rate of convergence. With a growing ratio of = the bias towards local search is shifted towards global search, which in the limit of a (,)-ES is nothing but random walk.
Reference: [SCED89] <author> J. David Schaffer, Richard A. Caruna, Larry J. Eshelman, and Rajarshi Das. </author> <title> A study of control parameters affecting online performance of genetic algorithms for function optimization. </title> <booktitle> In Schaffer [Sch89], </booktitle> <pages> pages 51-60. </pages>
Reference-contexts: Some remarkable countermeasures, which try to solve these problems, are e.g. scaling of fitness values [Jon75, Gol89, GB89], crowding [Jon75] and generation gap [Gre86], sharing [DG89, GR87], ranking [Bak85], and a multiple-point crossover <ref> [CES89, ECS89, SCED89] </ref>. 4 Comparison of the Algorithms Many differences between ESs and GAs directly or indirectly stem from a substantial difference in the amount of `genetic' information and its representation.
Reference: [Sch77] <editor> Hans-Paul Schwefel. </editor> <title> Numerische Optimierung von Computer-Modellen mittels der Evolutionsstrategie, </title> <booktitle> volume 26 of Interdisciplinary systems research. </booktitle> <publisher> Birkhauser, </publisher> <address> Basel, </address> <year> 1977. </year>
Reference-contexts: A more detailed comparison (also in an experimental way) can be found in [HB90]. 2 Evolution Strategies Neglecting the earlier steps in the development of ESs ((1+1)-ES [Rec73]) it is concentrated here on the (+)-ES and the (,)-ES as developed by Schwefel <ref> [Sch77, Sch81a] </ref>. They provide the capability of self-adaptation of the strategy parameters. <p> Like in a (1+1)-ES the maximum rate of convergence of a (+)-ES and (,)-ES is inversely proportional to n, the number of object variables <ref> [Sch77, Sch81a] </ref>. Due to this fact ESs have the tendency to reduce the search space by adjusting some step sizes towards zero in order to achieve a higher rate of convergence.
Reference: [Sch81a] <editor> Hans-Paul Schwefel. </editor> <title> Numerical Optimization of Computer Models. </title> <publisher> Wiley, </publisher> <address> Chichester, </address> <year> 1981. </year>
Reference-contexts: A more detailed comparison (also in an experimental way) can be found in [HB90]. 2 Evolution Strategies Neglecting the earlier steps in the development of ESs ((1+1)-ES [Rec73]) it is concentrated here on the (+)-ES and the (,)-ES as developed by Schwefel <ref> [Sch77, Sch81a] </ref>. They provide the capability of self-adaptation of the strategy parameters. <p> Like in a (1+1)-ES the maximum rate of convergence of a (+)-ES and (,)-ES is inversely proportional to n, the number of object variables <ref> [Sch77, Sch81a] </ref>. Due to this fact ESs have the tendency to reduce the search space by adjusting some step sizes towards zero in order to achieve a higher rate of convergence.
Reference: [Sch81b] <editor> Hans-Paul Schwefel. </editor> <title> Optimum seeking methods: Subroutines for the minimization of non-linear functions of several variables by means of direct (derivative-free) methods. </title> <institution> Interner Bericht KFA-STE-IB-7/81, Kernforschungsanlage Julich GmbH, Julich, Germany, </institution> <month> October </month> <year> 1981. </year>
Reference-contexts: The individuals of P 0t are obtained by applying recombination and mutation, where prior to its mutation an offspring is produced by recombining two parents a = (x a ; a ; a ) and b = (x b ; b ; b ) 2 I <ref> [Sch81b] </ref>: r (P t ) = a 0 = (x 0 ; 0 ; 0 ) 2 I v 0 8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; : v a;i (A) no recombination v a;i or v b;i (B) discrete 1 2 (v a;i + v b;i ) (C) intermediate
Reference: [Sch87] <editor> Hans-Paul Schwefel. </editor> <booktitle> Collective phenomena in evolutionary systems. In Preprints of the 31st Annual Meeting of the International Society for General System Research, Budapest, </booktitle> <volume> volume 2, </volume> <pages> pages 1025-1033, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: This might result in a restricted search along some coordinate axes, only, eventually yielding a poor rate of convergence, since flipping some step sizes is a rare event. With the introduction of correlated mutations <ref> [Sch87] </ref> these effects are reduced if not avoided: m (a 0 ) = a 00 = (x 00 ; 00 ; 00 ) 2 I I = IR n fi IR n fi IR w 00 = 0 + N 0 () (6) where N 0 denotes a vector of independent
Reference: [Sch89] <editor> J. David Schaffer, editor. </editor> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms and Their Applications, </booktitle> <address> San Mateo, California, June 1989. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference: [Sys89] <author> Gilbert Syswerda. </author> <title> Uniform crossover in genetic algorithms. </title> <booktitle> In Schaffer [Sch89], </booktitle> <pages> pages 2-9. </pages>
Reference-contexts: While the standard 1-point crossover in GAs shows no resemblance to recombination in ESs, uniform crossover <ref> [Sys89] </ref> is identical to discrete recombination in ESs if the cardinality of the alphabets is ignored.
References-found: 19

