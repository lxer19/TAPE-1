URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR351.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Title: RULE-BASED PROGRAM RESTRUCTURING FOR HIGH PERFORMANCE PARALLEL PROCESSOR SYSTEMS  
Author: by Lawrence J. Tenny 
Degree: Submitted to the faculty of the Graduate School in partial fulfillment of the requirements for the degree Doctor of Philosophy in the  
Date: April 3, 1992  
Affiliation: Department of Computer Science Indiana University  
Abstract-found: 0
Intro-found: 0
Reference: [1] <author> Abu-Sufah, W., Kuck, D., and Lawrie, D. </author> <title> Automatic Program Transformations for Virtual Memory Computers. </title> <booktitle> In Proc. National Computer Conference (June 1979), </booktitle> <pages> pp. 969-974. </pages>
Reference-contexts: Interchanging loops also interchanges the loop bounds on the innermost loop. This can change a loop's stride through memory, often an important consideration for optimizing memory access. Together with strip mining, loop permutation can be used to optimize cache usage by increasing the locality of data reference <ref> [36, 1] </ref>. Interchanging loops may increase the size of the vector, hence reduce the startup overhead. For example, do i=1,100 a (i,j) = a (i,j)*b (i)+c (i,j) enddo enddo can be vectorized, but the resulting code executes 100 small vectors. <p> By blocking a loop into sections small enough to fit into the cache, the locality of reference of a loop can be strengthened, thus increasing the number of cache hits and improving performance. This strategy has also been used to improve performance of virtual memory systems <ref> [1] </ref>.
Reference: [2] <author> Aho, A. V., Sethi, R., and Ullman, J. D. </author> <booktitle> Compilers Principles, Techniques, and Tools. </booktitle> <publisher> Addison Wesley, </publisher> <year> 1985. </year>
Reference-contexts: Thus the semantics of the programming language demand a particular order of execution. This order might seem to pose a rather serious problem for compilers that attempt to optimize serial code, since many of these optimizations result in a reordering of statements <ref> [2, 4] </ref>. Compilers that attempt to automatically vectorize or parallelize statements are not exempt from this problem. Many of the transformation operators used by these compilers change the execution order of statements. Under certain conditions, statement ordering can be relaxed. <p> In this section we describe a framework for analyzing the data dependencies in a program. Along the way we will assume that information about control is readily obtainable from a traditional control dependence analysis. For an indepth treatment of control analysis see <ref> [2] </ref> and [75]. We will take the liberty of referring to data dependence analysis as simply dependence analysis unless the reference is unclear. We begin our discussion with a definition. Definition 2.1 Let P be a program in our model.
Reference: [3] <author> Allen, F., Burke, M., Charles, P., Cytron, R., and Ferrante, J. </author> <title> An Overview of the PTRAN Analysis System for Multiprocessing. </title> <journal> Journal of Parallel and Distributed Computing 5 (March 1988), </journal> <pages> 617-640. </pages>
Reference-contexts: Chapter 1. Preliminaries 18 Tiny is a loop restructuring research tool developed at the Oregon Graduate Institute of Science and Technology by Wolfe [74]. Other lesser known restructuring compilers include, PAT (Parallelizing Assistant Tool) [61], ParaScope [9], PTRAN (Parallel TRANslator) <ref> [3] </ref>, the Texas Instruments ASC compiler [24, 70], the Cray-1 Fortran compiler [40], and the Massachusetts Computer Associates Vectorizer [52, 55]. Finally, some work has focused on the rule-based approach to program restructuring.
Reference: [4] <author> Allen, F. E., and Cocke, J. </author> <title> A Catalogue of Optimizing Transformations. In Design an optimization of compilers (March 1971), </title> <editor> R. Rustin, Ed., </editor> <publisher> Prentice-Hall, </publisher> <pages> pp. 1-30. </pages>
Reference-contexts: Thus the semantics of the programming language demand a particular order of execution. This order might seem to pose a rather serious problem for compilers that attempt to optimize serial code, since many of these optimizations result in a reordering of statements <ref> [2, 4] </ref>. Compilers that attempt to automatically vectorize or parallelize statements are not exempt from this problem. Many of the transformation operators used by these compilers change the execution order of statements. Under certain conditions, statement ordering can be relaxed.
Reference: [5] <author> Allen, J., and Kennedy, K. </author> <title> Conversion of Control Dependence to Data Dependence. </title> <booktitle> In 10th ACM Symposium on Principles of Programming Languages (1983), ACM, </booktitle> <pages> pp. 177-189. </pages>
Reference-contexts: Allen and Kennedy's PFC (Parallel Fortran Converter) at Rice University [8] was originally derived from the Illinois Parafrase compiler. PFC converts Fortran 77 programs to a vector similar to Fortran 90. 5 PFC was innovative in that it implemented if-conversion <ref> [5] </ref> so that control and data dependencies could be treated uniformly. PTOOL [7], another restructurer from Rice, incorporates many of the ideas of PFC. SUPERB (SUprenum ParallelizER Bonn) is an interactive Fortran parallelizer developed at the University of Bonn for the SUPRENUM project [76, 44, 37]. <p> But it also generates a dependence from the mask array to the new vector statement. 3 The where clause is part of the Fortran 90 standard. Chapter 4. Transformation Operators 83 This technique of converting the control dependence to a data dependence is related to if-conversion <ref> [5] </ref> used in PFC [8]. The following rule adds dependence created by the if-conversion.
Reference: [6] <author> Allen, J. R., and Kennedy, K. </author> <title> Automatic Loop Interchange. </title> <booktitle> In SIGPLAN Notices: Proc. ACM SIGPLAN 84 Sym. on Compiler Construction (June 1984), </booktitle> <volume> vol. </volume> <pages> 19. </pages>
Reference-contexts: Loop interchange can be used to move a recurrence to an outer loop, while scalar expansion can be used to break a recurrence. In either case, the change can allow some statements to be vectorized. Loop Interchange Wolfe [72], Allen and Kennedy <ref> [6] </ref>, and Banerjee [11] have studied the loop interchange problem extensively. Loop interchange is widely used in restructuring compilers primarily because it represents a powerful method with which one can exploit statement level parallelism, reduce memory bank conflicts, or increase register utilization.
Reference: [7] <author> Allen, R., Baumgartner, D., Kennedy, K., and Porterfield, A. </author> <title> PTOOL: A Semi-automatic Parallel Programming Assistant. </title> <type> Tech. rep., </type> <institution> Department of Computer Science, Rice University, </institution> <month> January </month> <year> 1987. </year> <note> 135 BIBLIOGRAPHY 136 </note>
Reference-contexts: PFC converts Fortran 77 programs to a vector similar to Fortran 90. 5 PFC was innovative in that it implemented if-conversion [5] so that control and data dependencies could be treated uniformly. PTOOL <ref> [7] </ref>, another restructurer from Rice, incorporates many of the ideas of PFC. SUPERB (SUprenum ParallelizER Bonn) is an interactive Fortran parallelizer developed at the University of Bonn for the SUPRENUM project [76, 44, 37]. The target language is SUPRENUM Fortran similar to Fortran 90 with additional MIMD extensions.
Reference: [8] <author> Allen, R., and Kennedy, K. </author> <title> Automatic Translation of fortran Programs to Vector Form. </title> <journal> ACM Trans. on Programming Languages and Systems 9, </journal> <month> 4 (October </month> <year> 1987), </year> <pages> 491-542. </pages>
Reference-contexts: Building data dependence graphs, finding cycles in the graphs, and constructing what would later be called the -block graph, were all innovations of the Parafrase project. Allen and Kennedy's PFC (Parallel Fortran Converter) at Rice University <ref> [8] </ref> was originally derived from the Illinois Parafrase compiler. PFC converts Fortran 77 programs to a vector similar to Fortran 90. 5 PFC was innovative in that it implemented if-conversion [5] so that control and data dependencies could be treated uniformly. <p> Chapter 4. Transformation Operators 83 This technique of converting the control dependence to a data dependence is related to if-conversion [5] used in PFC <ref> [8] </ref>. The following rule adds dependence created by the if-conversion.
Reference: [9] <author> Balasundarum, V., Kennedy, K., Kremer, U., and McKinley, K. </author> <title> The ParaScope Editor: An Interactive Parallel Programming Tool. </title> <booktitle> In Proc. of the Supercomputing 89 (November 1989), </booktitle> <pages> pp. 540-550. </pages>
Reference-contexts: Chapter 1. Preliminaries 18 Tiny is a loop restructuring research tool developed at the Oregon Graduate Institute of Science and Technology by Wolfe [74]. Other lesser known restructuring compilers include, PAT (Parallelizing Assistant Tool) [61], ParaScope <ref> [9] </ref>, PTRAN (Parallel TRANslator) [3], the Texas Instruments ASC compiler [24, 70], the Cray-1 Fortran compiler [40], and the Massachusetts Computer Associates Vectorizer [52, 55]. Finally, some work has focused on the rule-based approach to program restructuring.
Reference: [10] <author> Banerjee, U. </author> <title> Data Dependence in Ordinary Programs. </title> <type> Master's thesis, </type> <institution> University of Illinios at Urbana-Champaign, </institution> <year> 1976. </year>
Reference-contexts: The Separability Test can be applied only if the dependence equation has exactly one induction variable. However, the test yields both a necessary and sufficient condition for dependence. Finally, the Banerjee Test <ref> [10] </ref> can be used to determine of a solution exists within the iteration space but can not be used to determine if the solution is integer or real.
Reference: [11] <author> Banerjee, U. </author> <title> A Theory of Loop Permutations. </title> <booktitle> In Languages and Compilers for Parallel Computing (1989), </booktitle> <publisher> MIT Press, </publisher> <pages> pp. 54-74. </pages>
Reference-contexts: Loop interchange can be used to move a recurrence to an outer loop, while scalar expansion can be used to break a recurrence. In either case, the change can allow some statements to be vectorized. Loop Interchange Wolfe [72], Allen and Kennedy [6], and Banerjee <ref> [11] </ref> have studied the loop interchange problem extensively. Loop interchange is widely used in restructuring compilers primarily because it represents a powerful method with which one can exploit statement level parallelism, reduce memory bank conflicts, or increase register utilization. <p> If the permutation changes the direction of a dependence, then the interchange is invalid. Lemma 4.1 If a loop interchange changes leftmost loop-carried direction of any direction vector from a &lt; direction to a &gt; direction, then the loop interchange is invalid. Proof See <ref> [11] </ref>. 2 Chapter 4. Transformation Operators 88 For example, a pair of loops with direction vectors, f (=,&lt;),(<,&gt;)g, could not be interchanged because the direction vector would change from (&lt;,&gt;) to (&gt;,&lt;). Here the leftmost loop-carried direction is changed from &lt; to &gt;, hence the interchange is invalid.
Reference: [12] <author> Banerjee, U., Chen, S.-C., Kuck, D., and Towel, R. </author> <title> Time and Parallel Processor Bonds for fortran-like Loops. </title> <journal> IEEE Trans. Comput. </journal> <volume> C-28, </volume> <month> 9 </month> <year> (1979). </year>
Reference-contexts: Clearly this grain size requires a MIMD organization, but memory could be arranged either in the dance hall or the boudoir configurations, or somewhere in between. Within a procedure, loops offer considerable opportunity for parallelism <ref> [12, 47, 49] </ref>. We will see in the discussion below that in some cases different iterations of Chapter 1. Preliminaries 7 a loop can be distributed to different processors. <p> Here we shift the focus to dependence analysis in the presence of arrays, especially in the context of loops. Arrays, often called subscripted variables or vectors, are used extensively in loops. Loops offer a great potential source for speedup in parallel systems <ref> [47, 12, 49] </ref>. However, vectors used in the context of loops complicate the analysis considerably [71, 72, 75]. Most of the restructuring transformations discussed in the following chapters are applied to loops and thus rely extensively on the information obtained from the dependence analysis of arrays in loops.
Reference: [13] <author> Bernstein, A. </author> <title> Analysis of Programs for Parallel Processing. </title> <journal> IEEE Trans. Electronic Computers 15 (1966), </journal> <pages> 757-62. </pages>
Reference-contexts: In fact, it has been shown that the more general problem is undecidable <ref> [13] </ref>. The clauses in (1) present three different conditions that cause S 1 and S 2 to fail the sufficient condition for commutativity. If any of the clauses is not ;, then the Chapter 2. Data Dependence Analysis 29 statements would not meet the sufficient condition.
Reference: [14] <author> Bodin, F., Windheiser, D., Jalby, W., Atapattu, D., Lee, M., and Gannon, D. </author> <title> Performance Evaluation and Prediction for Parallel Algorithms on the BBN GP1000. </title> <type> Tech. Rep. 304, </type> <institution> Computer Science Department, Indiana University, </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: There is a considerable spectrum of machines between these two extremes. Some machines with the boudoir configuration, like the BBN Butterfly, have hardware and software to support seamless shared memory access, but at considerable expense due to network latency <ref> [14] </ref>. Machines of the dance hall configuration, like the NYU Ultracomputer [38], attempt to battle the cost of network latency by associating a Chapter 1. <p> Matrices are often quite large. The communication topology of the Butterfly and the non-uniform access, shared global memory, impose a considerable penalty for non-local memory access <ref> [14] </ref>. Ideally, we would have local copies of i th row of arrays a and x on each processor. The Butterfly has a primitive block transfer function btransfer 3 which efficiently moves blocks of 3 We assume that we are using the GP1000 model.
Reference: [15] <author> Bose, P. </author> <title> Heuristic Rule-based Transformations for Enhanced Vectorization. </title> <booktitle> In 1988 International Conference on Parallel Processing (1988), </booktitle> <pages> pp. 63-66. </pages>
Reference-contexts: Although the focus leans toward applying general techniques associated with artificial intelligence and not specifically a rule-based approach, the advantages of modularity, flexibility, and retargetability are clearly evident. At IBM Yorktown, Bose <ref> [15, 16] </ref> has implemented a rule-based advisor called EAVE. EAVE advises programmers on the best way to write loops so that IBM's VS FORTRAN compiler will generate the most favorable code. Although EAVE does not restructure code, it does reason with similar knowledge about similar issues.
Reference: [16] <author> Bose, P. </author> <title> Interactive Program Improvement via EAVE: An Expert Advisor for Vectorization. </title> <booktitle> In Proc. 1988 ACM Int'l. Conf. on Supercomuting (January 1988). </booktitle> <address> BIBLIOGRAPHY 137 </address>
Reference-contexts: Although the focus leans toward applying general techniques associated with artificial intelligence and not specifically a rule-based approach, the advantages of modularity, flexibility, and retargetability are clearly evident. At IBM Yorktown, Bose <ref> [15, 16] </ref> has implemented a rule-based advisor called EAVE. EAVE advises programmers on the best way to write loops so that IBM's VS FORTRAN compiler will generate the most favorable code. Although EAVE does not restructure code, it does reason with similar knowledge about similar issues.
Reference: [17] <author> Brainerd, W. S., Goldberg, C. H., and Adams, J. C. </author> <title> Programmer's Guide to Fortran 90. </title> <publisher> McGraw Hill, </publisher> <year> 1990. </year>
Reference-contexts: On a vector processor, the operator is applied sequentially, but through the use of pipelined hardware, the sequential application can be much faster than with the iterative process used on a conventional scalar machine. In languages that support vector semantics, such as Fortran 90 <ref> [17] </ref>, vector operands are specified as sections of arrays.
Reference: [18] <author> Brandes, T. </author> <title> Automatic Vectorisation for High Level Languages Based on an Expert System. </title> <booktitle> In Lecture Notes in Computer Science no. 237: CONPAR86 (1986), </booktitle> <pages> pp. 303-310. </pages>
Reference-contexts: Although EAVE does not restructure code, it does reason with similar knowledge about similar issues. The only restructuring project that uses rule-based approach is the SAVER project under development at the University of Marburg in Germany <ref> [18] </ref>. SAVER attempts to apply both parallelization and vectorization transformations. It is not clear from the literature if SAVER will be an entirely rule-based restructurer. <p> For parallel processors, the minimum and optimal grain sizes are important. These are just a few of the many useful machine properties, others can be found in the literature <ref> [68, 43, 28, 18, 65, 22, 37, 30] </ref>. In order to simplify the discussion, we consider only a small set of properties (see Table 2). Some of these properties, like the best vector length, are only rough first approximations.
Reference: [19] <author> Brandes, T. </author> <title> Determination of Dependencies in a Knowledge-Based Paralleliza-tion Tool. </title> <booktitle> Parallel Computing 8 (1988), </booktitle> <pages> 111-119. </pages>
Reference-contexts: SAVER attempts to apply both parallelization and vectorization transformations. It is not clear from the literature if SAVER will be an entirely rule-based restructurer. The preliminary reports indicate that dependence analysis is rule-based <ref> [19] </ref> and that vec-torization and parallelization will be done by an "expert system." 1.5 Thesis Overview It is important to define the scope of this thesis. The aim of our research is not to develop new transformation operators nor to discuss in depth issues in dependence Chapter 1. <p> Using information obtained by a parse of the program along with simple flow analysis, the specialists construct individual abstract program representations. These representations drive the restructuring process. Although rule-based methods for subscript analysis have been proposed <ref> [19] </ref>, for simplicity we assumed that subscript analysis is performed by a separate GCD Test-based solver. Finally, an interactive interface is useful. <p> Variable v 0 is said to be an alias for v. Aliases can come from the use of equivalence statements or common blocks in Fortran. For many other languages, pointers provide another mechanism for variable aliasing. Whatever the source, variable aliases present obvious problems for dependence analysis. Brandes <ref> [19] </ref> has investigated rule-based methods of dependence analysis capable of reasoning about execution paths. Combining these methods with the results from Chapter 2, might be fruitful in addressing the variable aliasing problem. The dependence analysis of Chapter 2 was limited to a single region of the program.
Reference: [20] <author> Brownston, L., Farrel, R., Kant, E., and Martin, N. </author> <title> Programming Expert Systems in OPS5. </title> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: Hopefully, this approach serves the reader better than Chapter 1. Preliminaries 20 a more complete introduction. When more information is needed, the formal syntax of Rex-UPSL can be found in Appendix A. Other sources of interest are the user's manual [64] and a book on a closely related language <ref> [20] </ref>. 1.5.2 Overview In Chapter 2, Data Dependence Analysis, we discuss the theory of dependence analysis and derive one well known test, the GCD Test, for performing subscript analysis. <p> In this section, we discuss a method of enabling some subset of rules by the use of context elements <ref> [20] </ref>. The idea behind context elements is simple. Suppose in some phase of the reasoning process, we need to focus the inference process on some sub-collection of rules.
Reference: [21] <author> Byler, M., Davies, J. R. B., Huson, C., Leasure, B., and Wolfe, M. </author> <title> Multiple Version Loops. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing (August 1987), </booktitle> <pages> pp. 312-318. </pages>
Reference-contexts: Hardcoding heuristics also makes understanding and explaining why a particular operator was chosen very difficult because the heuristics that guided the choice are far removed from a central organization. The KAP [28, 43] series of retargetable restructurers, use decision tables <ref> [73, 21] </ref> to organize heuristics. While they do allow some degree of retargetability, decision tables offer only limited centralization of heuristics. Further, decision tables provide only a flat view of the heuristic space. This view makes the use of sophisticated reasoning methods much more difficult.
Reference: [22] <author> Callahan, D., Cocke, J., and Kennedy, K. </author> <title> Estimating Interlock and Improving Balance for Pipelined Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 (1988), </volume> <pages> 334-358. </pages>
Reference-contexts: For parallel processors, the minimum and optimal grain sizes are important. These are just a few of the many useful machine properties, others can be found in the literature <ref> [68, 43, 28, 18, 65, 22, 37, 30] </ref>. In order to simplify the discussion, we consider only a small set of properties (see Table 2). Some of these properties, like the best vector length, are only rough first approximations.
Reference: [23] <author> Chow, F., Himelstein, M., Killian, E., and Weber, L. </author> <title> Engineering a RISC Compiler System. </title> <booktitle> In Proceedings of COMPCON Spring 86 (March 1986), </booktitle> <pages> pp. 132-137. </pages>
Reference-contexts: Additional condition levels may also be required in the planning model. The results of this thesis could be extended to work in areas outside of program restructurering. For example, the current generation of reduced instruction set computers (RISC) [57] need very sophisticated compilers <ref> [25, 41, 23] </ref>. The compiler for a RISC architecture must determine strategies for handling branch prediction, register allocation, pipeline scheduling, and many other issues. These problems could be addressed through the use of a rule-based reasoning system.
Reference: [24] <author> Cohagen, W. L. </author> <title> Vector Optimization for the ASC. </title> <booktitle> In Proceedings of the Seventh Annual Princeton Conference on Information Sciences and Systems (Princeton, </booktitle> <address> N.J., </address> <year> 1973), </year> <institution> Dept. of Electrical Engineering, </institution> <note> pp. 169-174. </note>
Reference-contexts: Kuck's research focused on programming the 4 See [42] for an entertaining account of the turbulent history of the ILLIAC-IV. Chapter 1. Preliminaries 17 ILLIAC-IV [48] and on uncovering parallelism in Fortran [47]. Others <ref> [58, 50, 24] </ref> were also at work uncovering parallelism and proposing program transformations. Today, restructuring compilers for Fortran is indeed a popular research topic. The following is a brief outline of restructurers that are most visible in the literature and have influenced this research considerably. <p> Chapter 1. Preliminaries 18 Tiny is a loop restructuring research tool developed at the Oregon Graduate Institute of Science and Technology by Wolfe [74]. Other lesser known restructuring compilers include, PAT (Parallelizing Assistant Tool) [61], ParaScope [9], PTRAN (Parallel TRANslator) [3], the Texas Instruments ASC compiler <ref> [24, 70] </ref>, the Cray-1 Fortran compiler [40], and the Massachusetts Computer Associates Vectorizer [52, 55]. Finally, some work has focused on the rule-based approach to program restructuring. In [69, 68], Wang and Gannon propose an organizational structure for transformation heuristics and a set of transformation operators embodied in rules.
Reference: [25] <author> Coutant, D., Hammond, C., and Kelly, J. </author> <title> Compiler for the New Generation of Hewlett-Packard Computers. </title> <booktitle> In Proceedings of the COMPCON Spring 86 (March 1986), </booktitle> <pages> pp. 48-61. BIBLIOGRAPHY 138 </pages>
Reference-contexts: Additional condition levels may also be required in the planning model. The results of this thesis could be extended to work in areas outside of program restructurering. For example, the current generation of reduced instruction set computers (RISC) [57] need very sophisticated compilers <ref> [25, 41, 23] </ref>. The compiler for a RISC architecture must determine strategies for handling branch prediction, register allocation, pipeline scheduling, and many other issues. These problems could be addressed through the use of a rule-based reasoning system.
Reference: [26] <author> Crowther, W., Goodhue, J., Starr, E., Thomas, R., Milliken, W., and Blackadar, T. </author> <title> Performance Measurements on a 128-node Butterfly Parallel Processor. </title> <booktitle> In Proc. International Conference on Parallel Processing (August 1985). </booktitle>
Reference-contexts: Each processor acts on a disjoint data set. Vector and array processors are typical examples of this architecture. The multiple instruction stream, multiple data stream (MIMD) architecture is characterized by multiple processors, each executing a disjoint set of instructions on disjoint data sets. Multiprocessors like the BBN Butterfly <ref> [26] </ref> are prime examples of this type of architecture. The last combination, multiple instruction streams acting on a single data stream (MISD), is not widely identified. However, it is reasonable to classify pipelined machines as MISD, so we label it as such in Table 1. Chapter 1.
Reference: [27] <author> Date, C. </author> <title> An Introduction to Database Systems. </title> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: The language, called Rex-UPSL [64], supports both a traditional Scheme [59] environment as well as a tuple space similar to that of a relational database <ref> [27] </ref>. The tuple space is called memory and is denoted by M. The individual tuples in M, called memory elements, are denoted by M e . <p> If M were a strictly relational database, each M e could contain only atomic values and M would then be a collection of normalized relations <ref> [27] </ref>. Although normalized relations lead to simpler rules, using lists in M e has advantages for the imperative code. For our representation of recurrences we use a normalized form for the benefit of the rules, and an unnormalized form for the benefit of the right hand side expressions.
Reference: [28] <author> Davies, J., Huson, C., Macke, T., leasure, B., and Wolfe, M. </author> <title> The KAP/S-1: An Advanced Source-to-Source Vectorizer for the S-1 Mark IIa Supercomputer. </title> <booktitle> In Proc. International Conference on Parallel Processing (1986), </booktitle> <pages> pp. 833-835. </pages>
Reference-contexts: This can make modifying the heuristics difficult and retargeting the restructurer nearly impossible. Hardcoding heuristics also makes understanding and explaining why a particular operator was chosen very difficult because the heuristics that guided the choice are far removed from a central organization. The KAP <ref> [28, 43] </ref> series of retargetable restructurers, use decision tables [73, 21] to organize heuristics. While they do allow some degree of retargetability, decision tables offer only limited centralization of heuristics. Further, decision tables provide only a flat view of the heuristic space. <p> For parallel processors, the minimum and optimal grain sizes are important. These are just a few of the many useful machine properties, others can be found in the literature <ref> [68, 43, 28, 18, 65, 22, 37, 30] </ref>. In order to simplify the discussion, we consider only a small set of properties (see Table 2). Some of these properties, like the best vector length, are only rough first approximations.
Reference: [29] <author> Eigenmann, R., Hoeflinger, J., Jaxon, G., Li, Z., and Padua, D. </author> <title> Restructuring fortran Programs for Cedar. </title> <booktitle> In Proc. of the International Conf. on Parallel Processing (1991), </booktitle> <pages> pp. 57-66. </pages>
Reference-contexts: The better match results in better performance. 1.2.4 Loop Level Parallel Execution Another way to achieve faster execution is to arrange for each iteration of the loop to be executed simultaneously on different processors. Some languages, like Cedar Fortran <ref> [29] </ref>, provide semantics for this type of execution. The previous loop would be written as, Chapter 1. Preliminaries 9 doall 10 i=1,n c (i) = a (i) + b (i) 10 continue Here, each iteration of the loop is scheduled for execution on the available processors. <p> KAP/205 is a commercial Fortran restructurer for the Cyber-205. It is a member of the family of KAP retargetable restructurers produced by a Kuck & Associates [43]. The Cedar Fortran restructuring compiler built for the Ceder project at Illinois <ref> [30, 29] </ref> was also developed by Kuck & Associates. 5 Although Fortran 90, then Fortran 8x, was not firmly specified at the time PFC was developed. Chapter 1. Preliminaries 18 Tiny is a loop restructuring research tool developed at the Oregon Graduate Institute of Science and Technology by Wolfe [74]. <p> In this section, we focus on the parallel execution of loop iterations. This is a common form of parallelism and is supported by most parallel dialects of the Fortran language. To simplify the discussion, only the doall <ref> [29] </ref> style constructs that impose no explicit synchronization between loop iterations are considered. Other constructs, such as doaccross [29], that allow synchronization are not discussed. For semantic invariance the parallelize transformation operator requires, among other things, that no loop-carried dependencies exist. The reason for this should be clear. <p> This is a common form of parallelism and is supported by most parallel dialects of the Fortran language. To simplify the discussion, only the doall <ref> [29] </ref> style constructs that impose no explicit synchronization between loop iterations are considered. Other constructs, such as doaccross [29], that allow synchronization are not discussed. For semantic invariance the parallelize transformation operator requires, among other things, that no loop-carried dependencies exist. The reason for this should be clear.
Reference: [30] <author> Eigenmann, R., Hoeflinger, J., Jaxon, G., and Padua, D. </author> <title> Cedar fortran and Its Compiler. </title> <booktitle> In Lecture Notes in Computer Science, </booktitle> <volume> No. 457: </volume> <booktitle> CONPAR90-VAPP IV (1990), </booktitle> <pages> pp. 288-299. </pages>
Reference-contexts: KAP/205 is a commercial Fortran restructurer for the Cyber-205. It is a member of the family of KAP retargetable restructurers produced by a Kuck & Associates [43]. The Cedar Fortran restructuring compiler built for the Ceder project at Illinois <ref> [30, 29] </ref> was also developed by Kuck & Associates. 5 Although Fortran 90, then Fortran 8x, was not firmly specified at the time PFC was developed. Chapter 1. Preliminaries 18 Tiny is a loop restructuring research tool developed at the Oregon Graduate Institute of Science and Technology by Wolfe [74]. <p> For parallel processors, the minimum and optimal grain sizes are important. These are just a few of the many useful machine properties, others can be found in the literature <ref> [68, 43, 28, 18, 65, 22, 37, 30] </ref>. In order to simplify the discussion, we consider only a small set of properties (see Table 2). Some of these properties, like the best vector length, are only rough first approximations.
Reference: [31] <author> Flynn, M. </author> <title> Very High Speed Computing Systems. </title> <booktitle> Proc. IEEE 54 (1966), </booktitle> <pages> 1901-1909. </pages>
Reference-contexts: This means that the programmer must carefully consider how parallelism is to be used to best solve the problem. Characterizations Flynn [32] introduced a simple characterization of computer architectures based on the notion of streams <ref> [31] </ref>. A stream is a sequence of items acted on by a processor. The items can be either data or instructions. The four combinations of single and multiple, data and instruction streams gives rise to the four architectural characterizations of computers in Table 1.
Reference: [32] <author> Flynn, M. </author> <title> Some Computer Organizations and Their Effectiveness. </title> <journal> IEEE Transactions on Computers C-21, </journal> <month> 9 (September </month> <year> 1972). </year>
Reference-contexts: This means that the programmer must carefully consider how parallelism is to be used to best solve the problem. Characterizations Flynn <ref> [32] </ref> introduced a simple characterization of computer architectures based on the notion of streams [31]. A stream is a sequence of items acted on by a processor. The items can be either data or instructions.
Reference: [33] <author> Forgy, C. </author> <title> OPS5 User's Manual. </title> <type> Tech. Rep. </type> <institution> CMU-CS-81-135, Department of Computer Science, Carnegie-Mellon University, </institution> <month> july </month> <year> 1981. </year>
Reference-contexts: Appendix A Rex-UPSL Syntax Rex-UPSL is a programming language primarily designed for building large systems of program restructuring rules, for historical reasons, called production systems. The left hand side patterns for rules in Rex-UPSL are written using a syntax very similar to OPS5 <ref> [33] </ref>. The right hand side actions of rules are written in Scheme [59]. In this appendix we present an extended Backus-Naur Form (BNF) syntax for Rex-UPSL. The syntax for Scheme expressions is adapted from the offical Scheme report [59] and is integrated into the sytactic description of Rex-UPSL where appropriate.
Reference: [34] <author> Forgy, C. </author> <title> RETE: A Fast Algorithm for the Many Pattern/Many Object Pattern Match Problem. </title> <booktitle> Artificial Intelligence 19 (1982), </booktitle> <pages> 17-37. BIBLIOGRAPHY 139 </pages>
Reference-contexts: Rules do not examine M when encountered by the Rex-UPSL interpreter. Rather, the set of satisfied rules is determined after each change to M. This set is constructed using an efficient, state saving, many pattern, many object matching algorithm <ref> [34] </ref>. The syntax and semantics of our rule-based language will be illustrated here by way of example and discussion. A formal syntactic description is presented in Appendix A. <p> The inference process of a rule-based reasoning system consists of cycle of three phases: 1) rule satisfaction, 2) conflict resolution, and 3) rule application. In the rule satisfaction phase, a state saving discrimination network, called a RETE Tree <ref> [34] </ref>, is used to determine the set of instantiations. 1 These instantiations are entered into the conflict set. Conflict resolution is the process of selecting from the conflict set, a single instantiation for the rule application phase. After the rule application phase, the process continues with the rule satisfaction phase.
Reference: [35] <author> Gannon, D., Guarna, V. A., and Lee, J. K. </author> <title> Static Analysis and Runtime Support for Parallel Execution of C. </title> <booktitle> In Languages and Compilers for Parallel Computing (1989), </booktitle> <editor> D. Gelernter, A. Nicolau, and D. Padua, Eds., </editor> <publisher> MIT Press, </publisher> <pages> pp. 254-275. </pages>
Reference-contexts: Some languages, like Ada and Pl/i, provide semantics for procedure level parallelism. Other languages, like Fortran 90, provide semantics for statement level parallelism. Concomitant with parallel semantics, each language must also provide the necessary synchronization mechanisms. Some languages, like Vpc++ <ref> [35] </ref>, also provide semantics for explicit domain decomposition. Whatever the extent of the semantics, the algorithm must be realized in the tools provided by the language. Chapter 1.
Reference: [36] <author> Gannon, D., Jalby, W., and Gallivan, K. </author> <title> Strategies for Cache and Local Memory Management by Global Program Transformation. </title> <journal> Journal of parallel and distributed computing 5 (1988), </journal> <pages> 587-616. </pages>
Reference-contexts: But, finding sources of parallelism is not the whole story. The match can also be improved, and thus the performance improved, by increasing data localization on non-uniform memory access machines [51], optimizing cache performance <ref> [36] </ref>, re-using registers, 72 Chapter 4. Transformation Operators 73 and other optimizations. In a series of transformation operators, each operator either improves the program-machine match or it converts the program into a form suitable for the application of another operator. <p> Interchanging loops also interchanges the loop bounds on the innermost loop. This can change a loop's stride through memory, often an important consideration for optimizing memory access. Together with strip mining, loop permutation can be used to optimize cache usage by increasing the locality of data reference <ref> [36, 1] </ref>. Interchanging loops may increase the size of the vector, hence reduce the startup overhead. For example, do i=1,100 a (i,j) = a (i,j)*b (i)+c (i,j) enddo enddo can be vectorized, but the resulting code executes 100 small vectors. <p> The Cyber-205, a memory-to-memory architecture, is an example of a machine with the rather large maximum vector length of 64k. Strip mining has also been shown to be useful in reducing the demand on memory caches <ref> [36] </ref>. By blocking a loop into sections small enough to fit into the cache, the locality of reference of a loop can be strengthened, thus increasing the number of cache hits and improving performance. This strategy has also been used to improve performance of virtual memory systems [1].
Reference: [37] <author> Gerndt, H. M., and Zima, H. P. </author> <title> Optimizing Communication in Superb. </title> <booktitle> In Lecture Notes in Computer Science No. 457: CONPAR90-VAPP IV (1990), </booktitle> <pages> pp. 300-311. </pages>
Reference-contexts: PTOOL [7], another restructurer from Rice, incorporates many of the ideas of PFC. SUPERB (SUprenum ParallelizER Bonn) is an interactive Fortran parallelizer developed at the University of Bonn for the SUPRENUM project <ref> [76, 44, 37] </ref>. The target language is SUPRENUM Fortran similar to Fortran 90 with additional MIMD extensions. KAP/205 is a commercial Fortran restructurer for the Cyber-205. It is a member of the family of KAP retargetable restructurers produced by a Kuck & Associates [43]. <p> For parallel processors, the minimum and optimal grain sizes are important. These are just a few of the many useful machine properties, others can be found in the literature <ref> [68, 43, 28, 18, 65, 22, 37, 30] </ref>. In order to simplify the discussion, we consider only a small set of properties (see Table 2). Some of these properties, like the best vector length, are only rough first approximations.
Reference: [38] <author> Gottlieb, A., Grishman, R., Kruskal, C. P., McAuliffe, K. P., Rudolph, L., and Snir, M. </author> <title> The NYU Ultracomputer - Desiging a MIMD Shared Memory Parallel Computer. </title> <journal> IEEE Trans. on Computers (February 1983), </journal> <pages> 175-189. </pages>
Reference-contexts: Some machines with the boudoir configuration, like the BBN Butterfly, have hardware and software to support seamless shared memory access, but at considerable expense due to network latency [14]. Machines of the dance hall configuration, like the NYU Ultracomputer <ref> [38] </ref>, attempt to battle the cost of network latency by associating a Chapter 1. Preliminaries 6 small amount of local memory with each processor. 1.2.2 Levels of Parallelism Often referred to as the granularity of parallelism, there are many different possibilities for the size of parallel subtasks.
Reference: [39] <author> Haghighat, M. R. </author> <title> Symbolic Dependence Analysis for High Performance Par-allelizing Compilers. </title> <type> Tech. Rep. 995, </type> <institution> CSRD, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: Much about data dependence analysis is not covered in this chapter. In particular, we do not address interprocedural analysis [67], semantic analysis [56], nor symbolic subscript analysis <ref> [39] </ref>.
Reference: [40] <author> Higbee, L. </author> <title> Vectorization and Conversion of fortran Programs for the Cray-1 CFT Compiler. </title> <type> Tech. Rep. 2240207, </type> <institution> Cray Research Inc., Mendota Heights, Minn., </institution> <month> June </month> <year> 1979. </year>
Reference-contexts: Other lesser known restructuring compilers include, PAT (Parallelizing Assistant Tool) [61], ParaScope [9], PTRAN (Parallel TRANslator) [3], the Texas Instruments ASC compiler [24, 70], the Cray-1 Fortran compiler <ref> [40] </ref>, and the Massachusetts Computer Associates Vectorizer [52, 55]. Finally, some work has focused on the rule-based approach to program restructuring. In [69, 68], Wang and Gannon propose an organizational structure for transformation heuristics and a set of transformation operators embodied in rules.
Reference: [41] <author> Hopkins, M. </author> <title> Compiling for the RT PC ROMP. </title> <type> Tech. Rep. </type> <institution> SA23-1057, IBM, </institution> <year> 1986. </year>
Reference-contexts: Additional condition levels may also be required in the planning model. The results of this thesis could be extended to work in areas outside of program restructurering. For example, the current generation of reduced instruction set computers (RISC) [57] need very sophisticated compilers <ref> [25, 41, 23] </ref>. The compiler for a RISC architecture must determine strategies for handling branch prediction, register allocation, pipeline scheduling, and many other issues. These problems could be addressed through the use of a rule-based reasoning system.
Reference: [42] <author> Hord, R. M. </author> <title> Parallel Supercomputing in SIMD Architectures. </title> <publisher> CRC Press, </publisher> <year> 1990. </year> <note> BIBLIOGRAPHY 140 </note>
Reference-contexts: Kuck's research focused on programming the 4 See <ref> [42] </ref> for an entertaining account of the turbulent history of the ILLIAC-IV. Chapter 1. Preliminaries 17 ILLIAC-IV [48] and on uncovering parallelism in Fortran [47]. Others [58, 50, 24] were also at work uncovering parallelism and proposing program transformations. Today, restructuring compilers for Fortran is indeed a popular research topic.
Reference: [43] <author> Huson, C., Macke, T., Davies, J. R., Wolfe, M., and Leasure, B. </author> <title> The KAP/205: An Advanced Source-to-Source Vectorizer for the Cyber 205 Supercomputer. </title> <booktitle> In Proceedings of the 1986 International Conf. on Parallel Processing (1986), </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp. 827-832. </pages>
Reference-contexts: This can make modifying the heuristics difficult and retargeting the restructurer nearly impossible. Hardcoding heuristics also makes understanding and explaining why a particular operator was chosen very difficult because the heuristics that guided the choice are far removed from a central organization. The KAP <ref> [28, 43] </ref> series of retargetable restructurers, use decision tables [73, 21] to organize heuristics. While they do allow some degree of retargetability, decision tables offer only limited centralization of heuristics. Further, decision tables provide only a flat view of the heuristic space. <p> The target language is SUPRENUM Fortran similar to Fortran 90 with additional MIMD extensions. KAP/205 is a commercial Fortran restructurer for the Cyber-205. It is a member of the family of KAP retargetable restructurers produced by a Kuck & Associates <ref> [43] </ref>. The Cedar Fortran restructuring compiler built for the Ceder project at Illinois [30, 29] was also developed by Kuck & Associates. 5 Although Fortran 90, then Fortran 8x, was not firmly specified at the time PFC was developed. Chapter 1. <p> For parallel processors, the minimum and optimal grain sizes are important. These are just a few of the many useful machine properties, others can be found in the literature <ref> [68, 43, 28, 18, 65, 22, 37, 30] </ref>. In order to simplify the discussion, we consider only a small set of properties (see Table 2). Some of these properties, like the best vector length, are only rough first approximations. <p> Transformation Operators 77 In this rule, the stride attribute is marked ok if the loop is a non-unit stride, but such strides are allowed by the hardware. For some machines, such as the Cyber 205, the use of non-unit strides require expensive scatter/gather operations <ref> [43] </ref>. Often, the cost of these operations may dominate the performance enhancement gained by vectorization. For machines like the Cyber, this rule might be modified to allow non-unit strides if the length of the vector operation is above the some break even threshold.
Reference: [44] <author> Kremer, U., Bast, H.-J., Gerndt, M., and Zima, H. </author> <title> Advanced Tools and Techniques for Automatic Parallelization. </title> <booktitle> Parallel Computing, </booktitle> <month> 7 (July </month> <year> 1988), </year> <pages> 387-393. </pages>
Reference-contexts: PTOOL [7], another restructurer from Rice, incorporates many of the ideas of PFC. SUPERB (SUprenum ParallelizER Bonn) is an interactive Fortran parallelizer developed at the University of Bonn for the SUPRENUM project <ref> [76, 44, 37] </ref>. The target language is SUPRENUM Fortran similar to Fortran 90 with additional MIMD extensions. KAP/205 is a commercial Fortran restructurer for the Cyber-205. It is a member of the family of KAP retargetable restructurers produced by a Kuck & Associates [43].
Reference: [45] <author> Kuck, D. </author> <title> The Structure of Computers and Computations. </title> <publisher> John Wiley, </publisher> <year> 1978. </year>
Reference-contexts: Chapter 1. Preliminaries 5 Single Data Stream Multiple Data Stream Single Instruction Stream SISD SIMD von Neumann data parallel Multiple Instruction Stream MISD MIMD pipeline function parallel Table 1: Classification of computer architectures. There are other characterizations of architectures. Most notable are those by Kuck <ref> [45] </ref> and Treleaven [66]. These are in many ways more detailed, but are much less prevalent in the literature. Memory and Processors In addition to the SIMD and MIMD distinctions given above, parallel machines are also characterized by memory organization.
Reference: [46] <author> Kuck, D., Kuhn, R., Padua, D., Leasure, B., and Wolfe, M. </author> <title> Dependence Graphs and Compiler Optimizations. </title> <booktitle> In Conference Record of the 8th Annual ACM Symposium on Principles of Programming Languages (1981), </booktitle> <pages> pp. 207-218. </pages>
Reference-contexts: Since we require that the solutions be integer, the Banerjee Test can only be used to disprove a dependence. 2.4 The Data-Dependence Graph A data dependence graph (DDG) <ref> [46] </ref> is a digraph G = (V; E) where V = fS i jS i 2 P g and E = f (S i ; S j ) j S i ffi S j g. Edge direction in G corresponds to the implied direction of the dependence. <p> Some recurrences might be subcycles of other recurrences. While some recurrences are guaranteed to be maximal recurrences. The set of maximal recurrences are called -blocks <ref> [46, 49] </ref>. Given the set of -blocks for a dependence graph G, we can construct a new graph G 0 whose nodes are the -blocks of G and whose edges are the dependencies between statements in different -blocks. G 0 represents the strongly connected components of G.
Reference: [47] <author> Kuck, D., Muraoka, Y., and Chen, S.-C. </author> <title> On the Number of Operations Simultaneously Executable in fortran-like Programs and Their Resulting Speedup. </title> <journal> IEEE Trans. Comput. </journal> <volume> C-21 (1972), </volume> <pages> 1293-1310. </pages>
Reference-contexts: Clearly this grain size requires a MIMD organization, but memory could be arranged either in the dance hall or the boudoir configurations, or somewhere in between. Within a procedure, loops offer considerable opportunity for parallelism <ref> [12, 47, 49] </ref>. We will see in the discussion below that in some cases different iterations of Chapter 1. Preliminaries 7 a loop can be distributed to different processors. <p> Kuck's research focused on programming the 4 See [42] for an entertaining account of the turbulent history of the ILLIAC-IV. Chapter 1. Preliminaries 17 ILLIAC-IV [48] and on uncovering parallelism in Fortran <ref> [47] </ref>. Others [58, 50, 24] were also at work uncovering parallelism and proposing program transformations. Today, restructuring compilers for Fortran is indeed a popular research topic. The following is a brief outline of restructurers that are most visible in the literature and have influenced this research considerably. <p> Here we shift the focus to dependence analysis in the presence of arrays, especially in the context of loops. Arrays, often called subscripted variables or vectors, are used extensively in loops. Loops offer a great potential source for speedup in parallel systems <ref> [47, 12, 49] </ref>. However, vectors used in the context of loops complicate the analysis considerably [71, 72, 75]. Most of the restructuring transformations discussed in the following chapters are applied to loops and thus rely extensively on the information obtained from the dependence analysis of arrays in loops.
Reference: [48] <author> Kuck, D. J. </author> <title> ILLIAC IV Software and Application Programming. </title> <journal> IEEE Trans. on Computers C-17, </journal> <month> 8 (August </month> <year> 1968), </year> <pages> 758-770. </pages>
Reference-contexts: Kuck's research focused on programming the 4 See [42] for an entertaining account of the turbulent history of the ILLIAC-IV. Chapter 1. Preliminaries 17 ILLIAC-IV <ref> [48] </ref> and on uncovering parallelism in Fortran [47]. Others [58, 50, 24] were also at work uncovering parallelism and proposing program transformations. Today, restructuring compilers for Fortran is indeed a popular research topic.
Reference: [49] <author> Kuck, D. J., Sameb, A. H., Cytron, R., Veidenbaum, A. V., Poly-chronopoulos, C. D., Lee, G., McDaniel, T., Leasure, B. R., Beck-man, C., Davies, J. R. B., and Kruskal, C. P. </author> <title> The Effects of Program Restructuring, Algorithm Change, and Architecture Choice on Program Performance. </title> <booktitle> In 1984 International Conference on Parallel Processing (1984), </booktitle> <editor> R. M. Keller, </editor> <publisher> Ed., </publisher> <pages> pp. 129-138. BIBLIOGRAPHY 141 </pages>
Reference-contexts: Clearly this grain size requires a MIMD organization, but memory could be arranged either in the dance hall or the boudoir configurations, or somewhere in between. Within a procedure, loops offer considerable opportunity for parallelism <ref> [12, 47, 49] </ref>. We will see in the discussion below that in some cases different iterations of Chapter 1. Preliminaries 7 a loop can be distributed to different processors. <p> Others [58, 50, 24] were also at work uncovering parallelism and proposing program transformations. Today, restructuring compilers for Fortran is indeed a popular research topic. The following is a brief outline of restructurers that are most visible in the literature and have influenced this research considerably. The Parafrase project <ref> [49] </ref> at the University of Illinois is the grandfather of all restructurers. It was the first to incorporate and spawn much of the research in dependence analysis and vectorization. <p> Here we shift the focus to dependence analysis in the presence of arrays, especially in the context of loops. Arrays, often called subscripted variables or vectors, are used extensively in loops. Loops offer a great potential source for speedup in parallel systems <ref> [47, 12, 49] </ref>. However, vectors used in the context of loops complicate the analysis considerably [71, 72, 75]. Most of the restructuring transformations discussed in the following chapters are applied to loops and thus rely extensively on the information obtained from the dependence analysis of arrays in loops. <p> Some recurrences might be subcycles of other recurrences. While some recurrences are guaranteed to be maximal recurrences. The set of maximal recurrences are called -blocks <ref> [46, 49] </ref>. Given the set of -blocks for a dependence graph G, we can construct a new graph G 0 whose nodes are the -blocks of G and whose edges are the dependencies between statements in different -blocks. G 0 represents the strongly connected components of G. <p> Chapter 4. Transformation Operators 90 Scalar Expansion Some recurrences cannot be moved to outer loops. Perhaps there is no outer loop to move the recurrence to or the conditions of Rule 4.9 are not satisfied. In these circumstances, scalar expansion <ref> [49] </ref> can sometimes be applied to eliminate a recurrence. The idea behind scalar expansion is simple. Recall from Chapter 2 that output and anti dependencies arise from the re-use of variables. With the scalar expansion operator, we provide distinct variables for every use by expanding scalars into arrays.
Reference: [50] <author> Lamport, L. </author> <title> The Parallel Execution of DO Loops. </title> <booktitle> Comm. of ACM 17 (1974), </booktitle> <pages> 83-93. </pages>
Reference-contexts: Kuck's research focused on programming the 4 See [42] for an entertaining account of the turbulent history of the ILLIAC-IV. Chapter 1. Preliminaries 17 ILLIAC-IV [48] and on uncovering parallelism in Fortran [47]. Others <ref> [58, 50, 24] </ref> were also at work uncovering parallelism and proposing program transformations. Today, restructuring compilers for Fortran is indeed a popular research topic. The following is a brief outline of restructurers that are most visible in the literature and have influenced this research considerably.
Reference: [51] <author> Lee, M.-H. </author> <title> Data Localization in Parallel Computer Systems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Indiana University, </institution> <month> February </month> <year> 1991. </year> <note> Techinal Report No. 325. </note>
Reference-contexts: The process is, in part, one of finding and exploiting parallelism already present in the original serial program. But, finding sources of parallelism is not the whole story. The match can also be improved, and thus the performance improved, by increasing data localization on non-uniform memory access machines <ref> [51] </ref>, optimizing cache performance [36], re-using registers, 72 Chapter 4. Transformation Operators 73 and other optimizations. In a series of transformation operators, each operator either improves the program-machine match or it converts the program into a form suitable for the application of another operator.
Reference: [52] <author> Levesque, J. M. </author> <title> Applications of the Vectorizer for Effective Use of High-Speed Computers. In High Speed Computer and Algorithm Organization, </title> <editor> D. Kuck, D. Lawrie, and A. H. Sameh, Eds. </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1977, </year> <pages> pp. 447-449. </pages>
Reference-contexts: Other lesser known restructuring compilers include, PAT (Parallelizing Assistant Tool) [61], ParaScope [9], PTRAN (Parallel TRANslator) [3], the Texas Instruments ASC compiler [24, 70], the Cray-1 Fortran compiler [40], and the Massachusetts Computer Associates Vectorizer <ref> [52, 55] </ref>. Finally, some work has focused on the rule-based approach to program restructuring. In [69, 68], Wang and Gannon propose an organizational structure for transformation heuristics and a set of transformation operators embodied in rules.
Reference: [53] <author> Loveman, D. B. </author> <title> Program Improvement by Source-to-Source Transformation. </title> <journal> JACM 24, </journal> <volume> 1 (1977), </volume> <pages> 121-145. </pages>
Reference-contexts: Strip Mining Strip mining <ref> [53] </ref>, sometimes called loop sectioning or loop blocking, divides the iteration space of a loop into blocks of fixed size. Each block can either be vectorized or parallelized. The size of a block corresponds to either the optimum vector size or number of processors of the target machine.
Reference: [54] <author> McDermott, J., and Forgy, C. </author> <title> Production System Conflict Resolution Strategies. In Pattern-Directed Inference Systems, </title> <editor> D. Waterman and F. Haynes-Roth, Eds. </editor> <publisher> Academic Press, Inc, </publisher> <year> 1978. </year>
Reference-contexts: Because conflict resolution plays a critical part in defining the semantics of a rule-based reasoning system, it is discussed in some detail here. Many conflict resolution strategies have been proposed and used in rule-based reasoning systems <ref> [54] </ref>. Rather than review other methods, in this section we describe the conflict resolution strategy assumed for the rules presented in this thesis. The inference process of a rule-based reasoning system consists of cycle of three phases: 1) rule satisfaction, 2) conflict resolution, and 3) rule application.
Reference: [55] <author> Myszewski, M. </author> <title> The Vectorizer Systems: Current and Proposed Capabilites. </title> <type> Tech. Rep. </type> <institution> CA-17809-1511, Massachusetts Computer Associates, Inc., </institution> <address> Wakefield, MA, </address> <month> September </month> <year> 1978. </year>
Reference-contexts: Other lesser known restructuring compilers include, PAT (Parallelizing Assistant Tool) [61], ParaScope [9], PTRAN (Parallel TRANslator) [3], the Texas Instruments ASC compiler [24, 70], the Cray-1 Fortran compiler [40], and the Massachusetts Computer Associates Vectorizer <ref> [52, 55] </ref>. Finally, some work has focused on the rule-based approach to program restructuring. In [69, 68], Wang and Gannon propose an organizational structure for transformation heuristics and a set of transformation operators embodied in rules.
Reference: [56] <author> Padua, D. A., and Wolfe, M. J. </author> <title> Advanced Compiler Optimizations for Supercomputers. </title> <journal> Communications of the ACM 29, </journal> <month> 12 (December </month> <year> 1986). </year>
Reference-contexts: Much about data dependence analysis is not covered in this chapter. In particular, we do not address interprocedural analysis [67], semantic analysis <ref> [56] </ref>, nor symbolic subscript analysis [39].
Reference: [57] <author> Patterson, D. A. </author> <title> Reduced Instruction Set Computers. </title> <journal> Communications of the ACM 28, </journal> <month> 1 (January </month> <year> 1985), </year> <pages> 8-21. </pages>
Reference-contexts: Additional condition levels may also be required in the planning model. The results of this thesis could be extended to work in areas outside of program restructurering. For example, the current generation of reduced instruction set computers (RISC) <ref> [57] </ref> need very sophisticated compilers [25, 41, 23]. The compiler for a RISC architecture must determine strategies for handling branch prediction, register allocation, pipeline scheduling, and many other issues. These problems could be addressed through the use of a rule-based reasoning system.
Reference: [58] <author> Ramamoorthy, C., and Gonzalez, M. </author> <title> A Survey of Techniques for Recognizing Parallel Processable Streams in Computer Programs. </title> <booktitle> In 1969 Joint Comput. Conf., AFIPS Conf. Proc. (1969), </booktitle> <volume> vol. 35, </volume> <publisher> AFIPS Press, </publisher> <pages> pp. 1-15. BIBLIOGRAPHY 142 </pages>
Reference-contexts: Kuck's research focused on programming the 4 See [42] for an entertaining account of the turbulent history of the ILLIAC-IV. Chapter 1. Preliminaries 17 ILLIAC-IV [48] and on uncovering parallelism in Fortran [47]. Others <ref> [58, 50, 24] </ref> were also at work uncovering parallelism and proposing program transformations. Today, restructuring compilers for Fortran is indeed a popular research topic. The following is a brief outline of restructurers that are most visible in the literature and have influenced this research considerably.
Reference: [59] <editor> Rees, J., and Clinger, W. </editor> <title> Revised 3 Report on the Algorithmic Language Scheme. </title> <type> Tech. Rep. 174, </type> <institution> Department of Computer Science, Indiana University, </institution> <month> December </month> <year> 1986. </year>
Reference-contexts: The language, called Rex-UPSL [64], supports both a traditional Scheme <ref> [59] </ref> environment as well as a tuple space similar to that of a relational database [27]. The tuple space is called memory and is denoted by M. The individual tuples in M, called memory elements, are denoted by M e . <p> The left hand side patterns for rules in Rex-UPSL are written using a syntax very similar to OPS5 [33]. The right hand side actions of rules are written in Scheme <ref> [59] </ref>. In this appendix we present an extended Backus-Naur Form (BNF) syntax for Rex-UPSL. The syntax for Scheme expressions is adapted from the offical Scheme report [59] and is integrated into the sytactic description of Rex-UPSL where appropriate. <p> The right hand side actions of rules are written in Scheme <ref> [59] </ref>. In this appendix we present an extended Backus-Naur Form (BNF) syntax for Rex-UPSL. The syntax for Scheme expressions is adapted from the offical Scheme report [59] and is integrated into the sytactic description of Rex-UPSL where appropriate. An in depth discussion of the language and its uses can be found in [64].
Reference: [60] <author> Sacerdoti, E. </author> <title> Planning in a Hierarchy of Abstraction Spaces. </title> <journal> Artificial Intelligence 5, </journal> <volume> 2, </volume> <pages> 115-135. </pages>
Reference-contexts: In the field of artificial intelligence, planning methods have received a considerable amount of attention. Hierarchical strategies have proven useful in domains where operator interaction is significant <ref> [60] </ref>. Hierarchical planning involves developing plans of successively higher resolution at successively higher levels. The final plan, derived from the highest planning level, has the desired resolution.
Reference: [61] <author> Smith, K. </author> <title> PAT An Interative fortran Parallelizing Assistant Tool. </title> <booktitle> In 1988 International Conference on Parallel Processing (1988), </booktitle> <pages> pp. 58-62. </pages>
Reference-contexts: Chapter 1. Preliminaries 18 Tiny is a loop restructuring research tool developed at the Oregon Graduate Institute of Science and Technology by Wolfe [74]. Other lesser known restructuring compilers include, PAT (Parallelizing Assistant Tool) <ref> [61] </ref>, ParaScope [9], PTRAN (Parallel TRANslator) [3], the Texas Instruments ASC compiler [24, 70], the Cray-1 Fortran compiler [40], and the Massachusetts Computer Associates Vectorizer [52, 55]. Finally, some work has focused on the rule-based approach to program restructuring.
Reference: [62] <author> Tanenbaum, A. S. </author> <title> Structured Computer Organization. </title> <publisher> Prentice-Hall, </publisher> <year> 1984. </year>
Reference-contexts: For each of these there is an associated virtual machine with its own language and computational model <ref> [62] </ref>. For the virtual machine at the algorithm level, there is an abstract parallel computational model. This model usually conforms to either the SIMD or MIMD organization, but can contain elements of both.
Reference: [63] <author> Tarjan, R. </author> <title> Depth First Search and Linear Graph Algorithms. </title> <journal> SIAM J. Com-put. </journal> <volume> 1, 2 (1972), </volume> <pages> 146-160. </pages>
Reference-contexts: To build the set of -blocks, we need to find the strongly connected components of the dependence graph. In an imperative environment, Tarjan's algorithm <ref> [63] </ref> would be best. For our purposes, a more computationally expensive, yet simpler approach will do. The following lemma serves as the basis for our approach. Lemma 3.1 If two recurrences have a statement in common, then the union of statements in the recurrences form a larger recurrence.
Reference: [64] <author> Tenny, L. </author> <title> UPSL User's Manual. </title> <type> Tech. Rep. 257, </type> <institution> Department of Computer Science, Indiana University, Bloomington, Indiana, </institution> <year> 1988. </year>
Reference-contexts: Hopefully, this approach serves the reader better than Chapter 1. Preliminaries 20 a more complete introduction. When more information is needed, the formal syntax of Rex-UPSL can be found in Appendix A. Other sources of interest are the user's manual <ref> [64] </ref> and a book on a closely related language [20]. 1.5.2 Overview In Chapter 2, Data Dependence Analysis, we discuss the theory of dependence analysis and derive one well known test, the GCD Test, for performing subscript analysis. <p> The language, called Rex-UPSL <ref> [64] </ref>, supports both a traditional Scheme [59] environment as well as a tuple space similar to that of a relational database [27]. The tuple space is called memory and is denoted by M. The individual tuples in M, called memory elements, are denoted by M e . <p> This set is constructed using an efficient, state saving, many pattern, many object matching algorithm [34]. The syntax and semantics of our rule-based language will be illustrated here by way of example and discussion. A formal syntactic description is presented in Appendix A. See <ref> [64] </ref> for a full discussion of the language. 2.4.2 Variables, Data Flow, and Statements We begin our discussion of the DDG derivation rules by developing a representation in M for variables, data flow, and statements in a region R of program P. We use the following definitions. <p> The syntax for Scheme expressions is adapted from the offical Scheme report [59] and is integrated into the sytactic description of Rex-UPSL where appropriate. An in depth discussion of the language and its uses can be found in <ref> [64] </ref>. The BNF presented here is extended with the following notation. * Terminals are written in boldface type * Descriptive terminals are written in italics. * Items followed by + occur one or more times. * Items followed by * occur zero or more times.
Reference: [65] <author> Torres, J., Ayguade, E., Labarta, J., Llaberia, J., and Valero, M. </author> <title> On Automatic Loop Data-mapping for Distributed-memory Multiprocessors. </title> <booktitle> In Lecture Notes in Computer Science no. 487 (1991), </booktitle> <pages> pp. 173-182. </pages>
Reference-contexts: For parallel processors, the minimum and optimal grain sizes are important. These are just a few of the many useful machine properties, others can be found in the literature <ref> [68, 43, 28, 18, 65, 22, 37, 30] </ref>. In order to simplify the discussion, we consider only a small set of properties (see Table 2). Some of these properties, like the best vector length, are only rough first approximations.
Reference: [66] <author> Treleaven, P. </author> <title> Control-driven Data-driven, and Demand-driven Computer Architecture. </title> <booktitle> Parallel Computing 2 (1985). </booktitle>
Reference-contexts: Chapter 1. Preliminaries 5 Single Data Stream Multiple Data Stream Single Instruction Stream SISD SIMD von Neumann data parallel Multiple Instruction Stream MISD MIMD pipeline function parallel Table 1: Classification of computer architectures. There are other characterizations of architectures. Most notable are those by Kuck [45] and Treleaven <ref> [66] </ref>. These are in many ways more detailed, but are much less prevalent in the literature. Memory and Processors In addition to the SIMD and MIMD distinctions given above, parallel machines are also characterized by memory organization.
Reference: [67] <author> Triolet, R. </author> <title> Interprocedural Analysis for Program Restructuring with Parafrase. </title> <type> Tech. Rep. 538, </type> <institution> Center for Supercomputer Research and Development, Univ. of Illinios, Urbana, </institution> <year> 1985. </year>
Reference-contexts: Much about data dependence analysis is not covered in this chapter. In particular, we do not address interprocedural analysis <ref> [67] </ref>, semantic analysis [56], nor symbolic subscript analysis [39]. <p> Procedures invoked in this region where implicitly assumed to not modify any of the dependence relations. In most circumstances, however, shared, global variables or modified procedure parameters could result in a different set of dependencies. Interprocedural dependence analysis <ref> [67] </ref> attempts to find these other sources of dependencies by examining the entire program, including all of the procedures. This analysis is more complex than the analysis presented in Chapter 2, but the methods of Chapter 2 could be extended to include an interprocedural analysis component.
Reference: [68] <author> Wang, K.-Y. </author> <title> Intelligent Program Optimization and Parallelization for Parallel Computers. </title> <type> PhD thesis, </type> <institution> Purdue University, </institution> <month> May </month> <year> 1991. </year> <note> BIBLIOGRAPHY 143 </note>
Reference-contexts: Second, the example illustrates the importance of knowning the features of the underlying hardware. In many cases, the transformation process is guided by the features of the target machine. This has lead some to term the process feature driven <ref> [68] </ref>. Chapter 1. Preliminaries 14 Our next example illustrates the importance of this last point. <p> Finally, some work has focused on the rule-based approach to program restructuring. In <ref> [69, 68] </ref>, Wang and Gannon propose an organizational structure for transformation heuristics and a set of transformation operators embodied in rules. Although the focus leans toward applying general techniques associated with artificial intelligence and not specifically a rule-based approach, the advantages of modularity, flexibility, and retargetability are clearly evident. <p> For parallel processors, the minimum and optimal grain sizes are important. These are just a few of the many useful machine properties, others can be found in the literature <ref> [68, 43, 28, 18, 65, 22, 37, 30] </ref>. In order to simplify the discussion, we consider only a small set of properties (see Table 2). Some of these properties, like the best vector length, are only rough first approximations. <p> Our planning model requires a good deal of control. Control is complicated by the interactions between rules. In fact, it has been suggested that a rule-based system could not be used in a program restructurer because of the complexity of rule interactions <ref> [68] </ref>. 6.2.1 Lexicographic Conflict Resolution Rules become active based on the contents of M. However, it is not the case that only one rule is satisfied by M at any particular time. In fact, under most circumstances, a number of rules are satisfied simultaneously.
Reference: [69] <author> Wang, K.-Y., and Gannon, D. </author> <title> Applying AI Techniques to Program Optimization for Parallel Computers. </title> <booktitle> In Parallel Processing for Supercomputers and Artificial Intelligence (1989), </booktitle> <editor> K. Hwang and D. DeGroot, Eds., </editor> <publisher> McGraw-Hill, </publisher> <pages> pp. 441-485. </pages>
Reference-contexts: Finally, some work has focused on the rule-based approach to program restructuring. In <ref> [69, 68] </ref>, Wang and Gannon propose an organizational structure for transformation heuristics and a set of transformation operators embodied in rules. Although the focus leans toward applying general techniques associated with artificial intelligence and not specifically a rule-based approach, the advantages of modularity, flexibility, and retargetability are clearly evident.
Reference: [70] <author> Wedel, D. </author> <title> Fortran for the Texas Instruments ASC System. </title> <journal> ACM SIGPLAN Notices 3, </journal> <month> 10 (March </month> <year> 1975). </year>
Reference-contexts: Chapter 1. Preliminaries 18 Tiny is a loop restructuring research tool developed at the Oregon Graduate Institute of Science and Technology by Wolfe [74]. Other lesser known restructuring compilers include, PAT (Parallelizing Assistant Tool) [61], ParaScope [9], PTRAN (Parallel TRANslator) [3], the Texas Instruments ASC compiler <ref> [24, 70] </ref>, the Cray-1 Fortran compiler [40], and the Massachusetts Computer Associates Vectorizer [52, 55]. Finally, some work has focused on the rule-based approach to program restructuring. In [69, 68], Wang and Gannon propose an organizational structure for transformation heuristics and a set of transformation operators embodied in rules.
Reference: [71] <author> Wolfe, M. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1982. </year>
Reference-contexts: Arrays, often called subscripted variables or vectors, are used extensively in loops. Loops offer a great potential source for speedup in parallel systems [47, 12, 49]. However, vectors used in the context of loops complicate the analysis considerably <ref> [71, 72, 75] </ref>. Most of the restructuring transformations discussed in the following chapters are applied to loops and thus rely extensively on the information obtained from the dependence analysis of arrays in loops. In this section we introduce some additional dependence notation along with a few new concepts. <p> Chapter 2. Data Dependence Analysis 42 do i=1,n S 2 : c (i+3) = a (i) enddo Another dependence test, the Separability Test <ref> [71] </ref>, is based on the explicit representation of the solution space of the dependence equation. The Separability Test can be applied only if the dependence equation has exactly one induction variable. However, the test yields both a necessary and sufficient condition for dependence.
Reference: [72] <author> Wolfe, M. </author> <title> Advanced Loop Interchanging. </title> <booktitle> In Proc. of the 1986 International Conference on Parallel Processing (1986), </booktitle> <publisher> IEEE Computer Science Press, </publisher> <pages> pp. 536-543. </pages>
Reference-contexts: Arrays, often called subscripted variables or vectors, are used extensively in loops. Loops offer a great potential source for speedup in parallel systems [47, 12, 49]. However, vectors used in the context of loops complicate the analysis considerably <ref> [71, 72, 75] </ref>. Most of the restructuring transformations discussed in the following chapters are applied to loops and thus rely extensively on the information obtained from the dependence analysis of arrays in loops. In this section we introduce some additional dependence notation along with a few new concepts. <p> Loop interchange can be used to move a recurrence to an outer loop, while scalar expansion can be used to break a recurrence. In either case, the change can allow some statements to be vectorized. Loop Interchange Wolfe <ref> [72] </ref>, Allen and Kennedy [6], and Banerjee [11] have studied the loop interchange problem extensively. Loop interchange is widely used in restructuring compilers primarily because it represents a powerful method with which one can exploit statement level parallelism, reduce memory bank conflicts, or increase register utilization.
Reference: [73] <author> Wolfe, M. </author> <title> Vector Optimization vs Vectorization. </title> <journal> Journal of Parallel and Distributed Computing 5 (1988), </journal> <pages> 551-567. </pages>
Reference-contexts: Hardcoding heuristics also makes understanding and explaining why a particular operator was chosen very difficult because the heuristics that guided the choice are far removed from a central organization. The KAP [28, 43] series of retargetable restructurers, use decision tables <ref> [73, 21] </ref> to organize heuristics. While they do allow some degree of retargetability, decision tables offer only limited centralization of heuristics. Further, decision tables provide only a flat view of the heuristic space. This view makes the use of sophisticated reasoning methods much more difficult.
Reference: [74] <author> Wolfe, M. </author> <title> The Tiny Loop Restructuring Research Tool. </title> <booktitle> In Proc. of the 1991 International Conference on Parallel Processing (1991), </booktitle> <pages> pp. 46-53. </pages>
Reference-contexts: Chapter 1. Preliminaries 18 Tiny is a loop restructuring research tool developed at the Oregon Graduate Institute of Science and Technology by Wolfe <ref> [74] </ref>. Other lesser known restructuring compilers include, PAT (Parallelizing Assistant Tool) [61], ParaScope [9], PTRAN (Parallel TRANslator) [3], the Texas Instruments ASC compiler [24, 70], the Cray-1 Fortran compiler [40], and the Massachusetts Computer Associates Vectorizer [52, 55]. Finally, some work has focused on the rule-based approach to program restructuring.
Reference: [75] <author> Zima, H. </author> <title> Supercompilers for Parallel and Vector Computers. Frontier Series. </title> <publisher> ACM Press, </publisher> <year> 1990. </year>
Reference-contexts: In this section we describe a framework for analyzing the data dependencies in a program. Along the way we will assume that information about control is readily obtainable from a traditional control dependence analysis. For an indepth treatment of control analysis see [2] and <ref> [75] </ref>. We will take the liberty of referring to data dependence analysis as simply dependence analysis unless the reference is unclear. We begin our discussion with a definition. Definition 2.1 Let P be a program in our model. <p> Arrays, often called subscripted variables or vectors, are used extensively in loops. Loops offer a great potential source for speedup in parallel systems [47, 12, 49]. However, vectors used in the context of loops complicate the analysis considerably <ref> [71, 72, 75] </ref>. Most of the restructuring transformations discussed in the following chapters are applied to loops and thus rely extensively on the information obtained from the dependence analysis of arrays in loops. In this section we introduce some additional dependence notation along with a few new concepts.
Reference: [76] <author> Zima, H., Bast, H.-J., and Gerndt, M. </author> <title> SUPERB: A Tool for Semiautomatic MIMD/SIMD Parallelization. </title> <booktitle> Parallel Computing 6 (June 1988), </booktitle> <pages> 1-18. </pages>
Reference-contexts: PTOOL [7], another restructurer from Rice, incorporates many of the ideas of PFC. SUPERB (SUprenum ParallelizER Bonn) is an interactive Fortran parallelizer developed at the University of Bonn for the SUPRENUM project <ref> [76, 44, 37] </ref>. The target language is SUPRENUM Fortran similar to Fortran 90 with additional MIMD extensions. KAP/205 is a commercial Fortran restructurer for the Cyber-205. It is a member of the family of KAP retargetable restructurers produced by a Kuck & Associates [43].
References-found: 76

