URL: ftp://ftp.cnl.salk.edu/pub/marni/nips95.ps.Z
Refering-URL: http://www.cnl.salk.edu/~marni/
Root-URL: 
Email: @salk.edu  jlarsen@fys.ku.dk  jchager@ibm.net  ekmansf@itsa.ucsf.edu  
Title: Classifying Facial Action  
Author: Marian Stewart Bartlett, Paul A. Viola, Terrence J. Sejnowski, Beatrice A. Golomb marni, viola, terry, beatrice Jan Larsen Joseph C. Hager Paul Ekman 
Address: La Jolla, CA 92037  Denmark  Salt Lake City, Utah  San Francisco San Francisco, CA 94143  
Affiliation: Howard Hughes Medical Institute The Salk Institute,  The Niels Bohr Institute 2100 Copenhagen  Network Information Research Corp  University of California  
Note: In press: Advances in Neural Information Processing Systems 8, D. Touretzky, M. Mozer, and M. Hasselmo (Eds.), MIT Press, Cambridge, MA, 1996.  
Abstract: The Facial Action Coding System, (FACS), devised by Ekman and Friesen (1978), provides an objective means for measuring the facial muscle contractions involved in a facial expression. In this paper, we approach automated facial expression analysis by detecting and classifying facial actions. We generated a database of over 1100 image sequences of 24 subjects performing over 150 distinct facial actions or action combinations. We compare three different approaches to classifying the facial actions in these images: Holistic spatial analysis based on principal components of graylevel images; explicit measurement of local image features such as wrinkles; and template matching with motion flow fields. On a dataset containing six individual actions and 20 subjects, these methods had 89%, 57%, and 85% performances respectively for generalization to novel subjects. When combined, performance improved to 92%.
Abstract-found: 1
Intro-found: 1
Reference: <author> Cottrell, G.,& Metcalfe, J. </author> <year> (1991): </year> <title> Face, gender and emotion recognition using holons. </title> <booktitle> In Advances in Neural Information Processing Systems 3, </booktitle> <editor> D. Touretzky, (Ed.) </editor> <address> San Mateo: </address> <publisher> Morgan & Kaufman. </publisher> <pages> 564 - 571. </pages>
Reference-contexts: data set by reflecting each image about the vertical axis, giving a total of 800 images. 2: Outer brow raiser. 4: Brow lower. 5: Upper lid raiser (widening the eyes). 6: Cheek raiser. 7: Lid tightener (partial squint). 3 HOLISTIC SPATIAL ANALYSIS The Eigenface (Turk & Pentland, 1991) and Holon <ref> (Cottrell & Metcalfe, 1991) </ref> representations are holistic representations based on principal components, which can be extracted by feed forward networks trained by back propagation.
Reference: <author> Ekman, P., & Friesen, W. </author> <year> (1978): </year> <title> Facial Action Coding System: A Technique for the Measurement of Facial Movement. </title> <address> Palo Alto, CA: </address> <publisher> Consulting Psychologists Press. </publisher>
Reference-contexts: The Facial Action Coding System (FACS) is a method for measuring facial expressions in terms of activity in the underlying facial muscles <ref> (Ekman & Friesen, 1978) </ref>. We are exploring ways to automate FACS. Rather than classifying images into emotion categories such as happy, sad, or sur-prised, the goal of this work is instead to detect the muscular actions that comprise a facial expression.
Reference: <author> Ekman, P., Huang, T., Sejnowski, T., & Hager, J. </author> <year> (1992): </year> <title> Final Report to NSF of the Planning Workshop on Facial Expression Understanding. Available from HIL-0984, </title> <address> UCSF, San Francisco, CA 94143. </address>
Reference-contexts: 1 INTRODUCTION Measurement of facial expressions is important for research and assessment psychiatry, neurology, and experimental psychology <ref> (Ekman, Huang, Sejnowski, & Hager, 1992) </ref>, and has technological applications in consumer-friendly user interfaces, interactive video and entertainment rating. The Facial Action Coding System (FACS) is a method for measuring facial expressions in terms of activity in the underlying facial muscles (Ekman & Friesen, 1978).
Reference: <author> Essa, I., & Pentland, A. </author> <year> (1995). </year> <title> Facial expression recognition using visually extracted facial action parameters. </title> <booktitle> Proceedings of the International Workshop on Automatic Face- and Gesture-Recognition. </booktitle> <institution> University of Zurich, Multimedia Laboratory. </institution>
Reference: <author> Golomb, B., Lawrence, D., & Sejnowski, T. </author> <year> (1991). </year> <title> SEXnet: A neural network identifies sex from human faces. </title> <booktitle> In Advances in Neural Information Processing Systems 3, </booktitle> <address> D. </address>
Reference: <editor> Touretzky, (Ed.) </editor> <address> San Mateo: </address> <publisher> Morgan & Kaufman: </publisher> <pages> 572 - 577. </pages>
Reference: <author> Hager, J., & Ekman, P., </author> <year> (1995). </year> <title> The essential behavioral science of the face and gesture that computer scientists need to know. </title> <booktitle> Proceedings of the International Workshop on Automatic Face- and Gesture-Recognition. </booktitle> <institution> University of Zurich, Multimedia Laboratory. </institution>
Reference-contexts: These differences can signify not only which emotion is occurring, but whether two or more emotions have blended together, the intensity of the emotion (s), and if an attempt is being made to control the expression of emotion <ref> (Hager & Ekman, 1995) </ref>. An alternative to training a system explicitly on a large number of expression categories is to detect the facial actions that comprise the expressions. Thousands of facial expressions can be defined in terms of this smaller set of structural components. <p> FACS also provides a means for obtaining reliable training data. Other approaches to automating facial measurement have mistakenly relied upon voluntary expressions, which tend to contain exaggerated and redundant cues, while omitting some muscular actions altogether <ref> (Hager & Ekman, 1995) </ref>. 2 IMAGE DATABASE We have collected a database of image sequences of subjects performing specified facial actions. The full database contains over 1100 sequences containing over 150 distinct actions, or action combinations, and 24 different subjects.
Reference: <author> Mase, K. </author> <year> (1991): </year> <title> Recognition of facial expression from optical flow. </title> <journal> IEICE Transactions E 74(10): </journal> <pages> 3474-3483. </pages>
Reference: <author> Padgett, C., Cottrell, G., </author> <year> (1995). </year> <title> Emotion in static face images. </title> <booktitle> Proceedings of the Institute for Neural Computation Annual Research Symposium, Vol 5. </booktitle> <address> La Jolla, CA. </address>
Reference: <author> Turk, M., & Pentland, A. </author> <year> (1991): </year> <title> Eigenfaces for Recognition. </title> <journal> Journal of Cognitive Neu roscience 3(1): </journal> <volume> 71 - 86. </volume>
Reference-contexts: we doubled the size of our data set by reflecting each image about the vertical axis, giving a total of 800 images. 2: Outer brow raiser. 4: Brow lower. 5: Upper lid raiser (widening the eyes). 6: Cheek raiser. 7: Lid tightener (partial squint). 3 HOLISTIC SPATIAL ANALYSIS The Eigenface <ref> (Turk & Pentland, 1991) </ref> and Holon (Cottrell & Metcalfe, 1991) representations are holistic representations based on principal components, which can be extracted by feed forward networks trained by back propagation.
Reference: <author> Yacoob, Y., & Davis, L. </author> <year> (1994): </year> <title> Recognizing human facial expression. </title> <institution> University of Maryland Center for Automation Research Technical Report No. </institution> <month> 706. </month>
References-found: 11

