URL: http://bugle.cs.uiuc.edu/Papers/IOhpdc96.ps.Z
Refering-URL: http://bugle.cs.uiuc.edu/Projects/IO/sioDir/papers.html
Root-URL: http://www.cs.uiuc.edu
Email: email:fesmirni,aydt,achien,reedg@cs.uiuc.edu  
Title: I/O Requirements of Scientific Applications: An Evolutionary View  
Author: Evgenia Smirni, Ruth A. Aydt, Andrew A. Chien, Daniel A. Reed 
Address: Urbana, Illinois 61801  
Affiliation: Department of Computer Science University of Illinois  
Abstract: The modest I/O configurations and file system limitations of many current high-performance systems preclude solution of problems with large I/O needs. I/O hardware and file system parallelism is the key to achieve high performance. We analyze the I/O behavior of several versions of two scientific applicationson the Intel Paragon XP/S. The versions involve incremental application code enhancements across multiple releases of the operating system. Studying the evolution of I/O access patterns underscores the interplay between application access patterns and file system features. Our results show that both small and large request sizes are common, that at present application developers must manually aggregate small requests to obtain high disk transfer rates, that concurrent file accesses are frequent, and that appropriate matching of the application access pattern and the file system access mode can significantly increase application I/O performance. Based on these results, we describe a set of file system design principles. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Chen, P. M., Lee, E. K., Gibson, G. A., Katz, R. H., and Patterson, D. A., </author> <title> RAID: High-performance, Reliable Secondary Storage, </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 26(2), </volume> <pages> pp. 145-185, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: During the past decade, a wide variety of parallel I/O systems have been proposed and built [2, 8, 11]. All these systems exploit parallel I/O devices (partitioning data across disk arrays <ref> [1] </ref> for parallelism) and data management techniques (prefetching and write-behind) in an attempt to de fl This work was supported in part by the Advanced Research Projects Agency under ARPA contracts DAVT63-91-C-0029 and DABT63-93-C-0040, by the National Science Foundation under grant NSF ASC 92-12369, and by the Aeronautics and Space Administration
Reference: [2] <author> Corbett, P., Feitelson, D. G., Prost, J.-P., and Baylor, S. J., </author> <title> Parallel Access to Files in the Vesta File System, </title> <booktitle> in Proc. Supercomputing '93, </booktitle> <pages> pp. 472-481, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: This increasing disparity between the hardware performance of individual processors and storage devices dictates use of hardware and software I/O parallelism. During the past decade, a wide variety of parallel I/O systems have been proposed and built <ref> [2, 8, 11] </ref>.
Reference: [3] <author> Crandall, P. E., Aydt, R. A., Chien A. A., and Reed, D. A., </author> <title> Input/Output Characterization of Scalable Parallel Applications, </title> <booktitle> in Proc. Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: By focusing on both physical resource usage and functional application composition, Pasquale and Polyzos concluded that supercomputer I/O behavior was recurrent and predictable, based on the iterative nature of many scientific applications. In contrast to the results on vector systems, our more recent analysis on the Intel Paragon XP/S <ref> [3] </ref> indicated that there are large variations in the temporal and spatial access patterns of scientific applications. Some cyclic behavior was noted, but the applications' I/O patterns were more irregular, with both extremely small and extremely large requests.
Reference: [4] <author> Henderson, R. D., and Karniadakis G. E., </author> <title> Unstructured Spectral Element Methods for Simulation of Turbulent Flows", </title> <journal> J. Comput. Phys., </journal> <volume> Vol. 122(2), </volume> <pages> pp. 191-217, </pages> <year> 1995. </year>
Reference-contexts: In this paper, we report an analysis of the evolving I/O behavior and performance of two SIO codes, an electron scattering application [18] and a computational fluid dynamics application <ref> [4] </ref>. Over eighteen months, we studied the evolving resource demands of both applications on the Intel Paragon XP/S and captured the interplay between application resource demands and system responses. <p> PRISM is a parallel implementation of a 3-D numerical simulation of the Navier-Stokes equations written in C <ref> [4] </ref>. The parallelization is achieved by apportioning slides of the periodic domain to the nodes, with a combination of spectral elements and Fourier modes used to investigate the dynamics and transport properties of turbulent flow.
Reference: [5] <author> Henderson, R. D., </author> <title> Private Communication, </title> <month> January </month> <year> 1996. </year>
Reference-contexts: Moreover, only standard UNIX I/O calls were used. Conversations with the application developers revealed that they opted for the easiest and most natural implementation for their I/O <ref> [19, 5] </ref>. Both codes relied on a single node to coordinate parallel read and write operations by all nodes. This partitioning of I/O tasks across nodes is partially an artifact of the codes' previous platforms and partially a consequence of a restricted set of I/O modes. <p> Likewise, both codes would have benefited from concurrent, synchronized writes by all nodes, but in the first release of PFS no such I/O mode was available, and both developers opted to to perform all writes through node zero <ref> [19, 5] </ref>. 6.2. Optimized Access Patterns Because there were clear research benefits, both code developers worked assiduously to optimize their application I/O behavior and to exploit Intel PFS features. Most changes optimized the read request sizes. For ESCAT, the number of small requests, those less than 1K bytes, dropped substantially.
Reference: [6] <author> Huber, J., Elford, C. L., Reed, D. A., Chien, A., and Blu-menthal, D. S., </author> <title> PPFS: A High Performance Portable Parallel File System, </title> <booktitle> in Proc. 9 th ACM International Conference on Supercomputing, </booktitle> <pages> pp. 385-394, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Robust I/O operations that employ caching or prefetching are an attractive and less confusing alternative to manual request aggregation. A file system that dynamically tunes its policy to match the requirements of the application access patterns and disk performance characteristics is a promising alternative <ref> [6] </ref>. 6. Application Comparisons Although the ESCAT and PRISM codes differ dramatically in their algorithmic approaches, they share many common I/O characteristics and problems. First, small code changes (e.g. a few I/O calls) can produce large changes in I/O performance. This is both heartening and dismaying.
Reference: [7] <author> Kotz, D. and Nieuwejaar, N., </author> <title> Dynamic File-Access Characteristics of a Production Parallel Scientific Workload, </title> <booktitle> in Proc. Supercomputing '94, </booktitle> <pages> pp. 640-649, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Some cyclic behavior was noted, but the applications' I/O patterns were more irregular, with both extremely small and extremely large requests. Our work complements this earlier study by isolating the reasons that application developers opt to use parallel file systems in particular ways. Kotz and Nieuwejaar <ref> [7] </ref> examined application file access characteristics on the Intel iPSC/860's Concurrent File System at NASA's Ames Research Center. In a related study, Parakayastha et al [15] investigated file access patterns on a CM-5 under the Scalable File System (SFS) at the National Center of Supercomputing Applications.
Reference: [8] <author> LoVerso, S. J., Isman, M., Nanopoulos, A., Nesheim, W., Milne, E. D., and Wheeler, R., sfs: </author> <title> A Parallel File System for the CM-5", </title> <booktitle> in Proc. 1993 Summer USENIX Conference, </booktitle> <pages> pp. 291-305, </pages> <year> 1993. </year>
Reference-contexts: This increasing disparity between the hardware performance of individual processors and storage devices dictates use of hardware and software I/O parallelism. During the past decade, a wide variety of parallel I/O systems have been proposed and built <ref> [2, 8, 11] </ref>.
Reference: [9] <author> Miller, E. L., and Katz, R. H., </author> <booktitle> Input/Output Behavior of Supercomputing Applications, in Proc. Supercomputing '91, </booktitle> <pages> pp. 388-397, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: In x4-x5, we describe the evolution of two I/O intensive codes, followed in x6 by a comparison of their I/O behaviors. Finally, x7 summarizes our findings and outlines future work. 2. Related Work Several I/O characterization efforts for scientific applications have concentrated on the I/O behavior of vector supercomputers <ref> [9, 12, 13] </ref>. Miller and Katz [9] measured a workload of mostly computational fluid dynamics applications on a Cray Y-MP and characterized the I/O behavior as highly regular, cyclical, and bursty. They first proposed a high-level I/O classification based on compulsory, checkpoint, and data staging I/O operations. <p> Finally, x7 summarizes our findings and outlines future work. 2. Related Work Several I/O characterization efforts for scientific applications have concentrated on the I/O behavior of vector supercomputers [9, 12, 13]. Miller and Katz <ref> [9] </ref> measured a workload of mostly computational fluid dynamics applications on a Cray Y-MP and characterized the I/O behavior as highly regular, cyclical, and bursty. They first proposed a high-level I/O classification based on compulsory, checkpoint, and data staging I/O operations.
Reference: [10] <author> Nieuwejaar, N., Kotz, D., Purakayastha, A., Ellis, C. S., and Best, M., </author> <title> File-Access Characteristics of Parallel Scientific Workloads", </title> <institution> PCS-TR95-263, Dept. of Computer Science, Dartmouth College, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: The results of both studies showed that the majority of the file accesses were small, even though the vast majority of data was transferred via a few large requests. Subsequent comparison of the file access characteristics on both systems <ref> [10] </ref> indicated that users tune their applications to the underlying parallel system's idiosyncrasies in an effort to maximize performance. Our work builds on these earlier studies by examining the interplay between file system features and application developer decisions. 3.
Reference: [11] <author> Nitzberg, B., </author> <title> Performance of the iPSC/860 Concurrent File System, </title> <institution> RND-92-020, NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: This increasing disparity between the hardware performance of individual processors and storage devices dictates use of hardware and software I/O parallelism. During the past decade, a wide variety of parallel I/O systems have been proposed and built <ref> [2, 8, 11] </ref>.
Reference: [12] <author> Pasquale, B. K., amd Polyzos, G., </author> <title> A Static Analysis of I/O Characteristics of Scientific Applications in a Production Workload, </title> <booktitle> in Proc. Supercomputing '93, </booktitle> <pages> pp. 388-397, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: In x4-x5, we describe the evolution of two I/O intensive codes, followed in x6 by a comparison of their I/O behaviors. Finally, x7 summarizes our findings and outlines future work. 2. Related Work Several I/O characterization efforts for scientific applications have concentrated on the I/O behavior of vector supercomputers <ref> [9, 12, 13] </ref>. Miller and Katz [9] measured a workload of mostly computational fluid dynamics applications on a Cray Y-MP and characterized the I/O behavior as highly regular, cyclical, and bursty. They first proposed a high-level I/O classification based on compulsory, checkpoint, and data staging I/O operations. <p> Miller and Katz [9] measured a workload of mostly computational fluid dynamics applications on a Cray Y-MP and characterized the I/O behavior as highly regular, cyclical, and bursty. They first proposed a high-level I/O classification based on compulsory, checkpoint, and data staging I/O operations. Pasquale and Polyzos <ref> [12] </ref> used clustering and regression analysis to examine I/O demands and their relationship to the elapsed computation time of the Cray Y-MP workload at the San Diego Supercomputing Center (SDSC). They also isolated and analyzed two I/O intensive applications on a Cray 90 at SDSC [13].
Reference: [13] <author> Pasquale, B. K., amd Polyzos, G., </author> <title> Dynamic I/O Characterization of I/O Intensive Scientific Applications, </title> <booktitle> in Proc. Supercomputing '94, </booktitle> <pages> pp. 660-669, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: In x4-x5, we describe the evolution of two I/O intensive codes, followed in x6 by a comparison of their I/O behaviors. Finally, x7 summarizes our findings and outlines future work. 2. Related Work Several I/O characterization efforts for scientific applications have concentrated on the I/O behavior of vector supercomputers <ref> [9, 12, 13] </ref>. Miller and Katz [9] measured a workload of mostly computational fluid dynamics applications on a Cray Y-MP and characterized the I/O behavior as highly regular, cyclical, and bursty. They first proposed a high-level I/O classification based on compulsory, checkpoint, and data staging I/O operations. <p> Pasquale and Polyzos [12] used clustering and regression analysis to examine I/O demands and their relationship to the elapsed computation time of the Cray Y-MP workload at the San Diego Supercomputing Center (SDSC). They also isolated and analyzed two I/O intensive applications on a Cray 90 at SDSC <ref> [13] </ref>. By focusing on both physical resource usage and functional application composition, Pasquale and Polyzos concluded that supercomputer I/O behavior was recurrent and predictable, based on the iterative nature of many scientific applications.
Reference: [14] <author> Poole, J. T., </author> <title> Preliminary Survey of I/O Intensive Applications", </title> <institution> California Institute of Technology, </institution> <note> http://www.ccsd.caltech.edu/SIO/SIO.html, 1994. </note>
Reference-contexts: Clearly, understanding the interactions between application I/O request patterns and the hardware and software of parallel I/O systems is a precursor to designing more effective I/O management policies. As part of the Scalable I/O Initiative (SIO) <ref> [14] </ref>, our twin goals are to collect detailed performance data on the I/O characteristics and access patterns of a variety of scalable parallel scientific applications and to use this information to design and evaluate parallel I/O file system management policies.
Reference: [15] <author> Purakayastha, A., Ellis, C. S., Kotz, D., Nieuwejaar, N., and Best, M., </author> <title> Characterizing Parallel File-Access Patterns on a Large-Scale Multiprocessor, </title> <booktitle> in Proc. 9 th International Parallel Processing Symposium, </booktitle> <pages> pp. 165-172, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Kotz and Nieuwejaar [7] examined application file access characteristics on the Intel iPSC/860's Concurrent File System at NASA's Ames Research Center. In a related study, Parakayastha et al <ref> [15] </ref> investigated file access patterns on a CM-5 under the Scalable File System (SFS) at the National Center of Supercomputing Applications. The results of both studies showed that the majority of the file accesses were small, even though the vast majority of data was transferred via a few large requests.
Reference: [16] <author> Reed, D. A., Aydt, R. A., Noe, R. J., Roth, P. C., Shields, K. A., Schwartz, B. W., and Tavera, L. F., </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment, </title> <booktitle> in Proc. Scalable Parallel Libraries Conference, </booktitle> <editor> A. Skjellum (ed.), </editor> <publisher> IEEE Computer Society, </publisher> <pages> pp. 104-113, </pages> <year> 1993. </year>
Reference-contexts: Below, we describe the I/O analysis mechanisms and the salient characteristics of the Intel parallel file system. 3.1. Pablo Software Infrastructure Building on the lessons of several previous performance analysis toolkits, Pablo <ref> [16, 17] </ref> is a portable performance environment that supports both performance data capture and analysis. The Pablo instrumentation software captures dynamic performance data via instrumented source code that is linked with a data capture library.
Reference: [17] <author> Reed, D. A., Elford C.L., Madhyastha T., Scullin W.H., Aydt, R. A., and Smirni E., </author> <title> I/O, Performance Analysis, and Performance Data Immersion, </title> <booktitle> in Proc. MASCOTS'96, </booktitle> <pages> pp. 5-16, </pages> <year> 1996. </year>
Reference-contexts: To capture traces of application I/O requests and parallel file system responses, we used an extended version of the Pablo performance analysis environment <ref> [17] </ref>. All data was captured on the 512-node Intel Paragon XP/S at the Caltech Center of Advanced Computing Research. Below, we describe the I/O analysis mechanisms and the salient characteristics of the Intel parallel file system. 3.1. <p> Below, we describe the I/O analysis mechanisms and the salient characteristics of the Intel parallel file system. 3.1. Pablo Software Infrastructure Building on the lessons of several previous performance analysis toolkits, Pablo <ref> [16, 17] </ref> is a portable performance environment that supports both performance data capture and analysis. The Pablo instrumentation software captures dynamic performance data via instrumented source code that is linked with a data capture library.
Reference: [18] <author> Winstead, C., Pritchard, H., and McKoy, V., </author> <title> Parallel Computation of Electron-Molecule Collisions, </title> <journal> IEEE Computational Science & Engineering, </journal> <pages> pp. 34-42, </pages> <year> 1995. </year>
Reference-contexts: In this paper, we report an analysis of the evolving I/O behavior and performance of two SIO codes, an electron scattering application <ref> [18] </ref> and a computational fluid dynamics application [4]. Over eighteen months, we studied the evolving resource demands of both applications on the Intel Paragon XP/S and captured the interplay between application resource demands and system responses. <p> The Schwinger Multichannel (SMC) method is an adaptation of Schwinger's variational principle for the scattering amplitude that makes it suitable for calculating low-energy electron-molecule collisions <ref> [18] </ref>. The scattering probabilities are obtained by solving linear systems whose terms must be evaluated by numerical quadrature. Generation of the quadrature data is compu-tationally intensive, and the total quadrature data volume is highly dependent on the nature of the problem. <p> Because the quadrature data is too voluminous to fit in the individual processor memory, an out of core solution is required. ESCAT is a parallel implementation of the Schwinger Multichannel method written in C, FORTRAN, and assembly language <ref> [18] </ref>. From an I/O perspective, the code has four distinct execution phases: * Phase One: Initialization data is read from three input files (compulsory I/O). During this phase, the problem definition and some initial matrices are loaded. * Phase Two: Quadrature data is written to disk (data staging).
Reference: [19] <author> Winstead, C., </author> <title> Private Communication, </title> <month> January </month> <year> 1996. </year>
Reference-contexts: Moreover, only standard UNIX I/O calls were used. Conversations with the application developers revealed that they opted for the easiest and most natural implementation for their I/O <ref> [19, 5] </ref>. Both codes relied on a single node to coordinate parallel read and write operations by all nodes. This partitioning of I/O tasks across nodes is partially an artifact of the codes' previous platforms and partially a consequence of a restricted set of I/O modes. <p> Because there was no equivalent to the M GLOBAL mode on the Intel Delta, where ESCAT was developed, or the Intel iPSC/860, where PRISM was developed, concurrent reads using the M UNIX were the easiest and most natural choice <ref> [19] </ref>. Likewise, both codes would have benefited from concurrent, synchronized writes by all nodes, but in the first release of PFS no such I/O mode was available, and both developers opted to to perform all writes through node zero [19, 5]. 6.2. <p> Likewise, both codes would have benefited from concurrent, synchronized writes by all nodes, but in the first release of PFS no such I/O mode was available, and both developers opted to to perform all writes through node zero <ref> [19, 5] </ref>. 6.2. Optimized Access Patterns Because there were clear research benefits, both code developers worked assiduously to optimize their application I/O behavior and to exploit Intel PFS features. Most changes optimized the read request sizes. For ESCAT, the number of small requests, those less than 1K bytes, dropped substantially.
References-found: 19

