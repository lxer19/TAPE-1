URL: http://theory.lcs.mit.edu/~rivest/GoldmanRivest-MakingMaximumEntropyComputationsEasierByAddingExtraConstraints.ps
Refering-URL: http://theory.lcs.mit.edu/~rivest/publications.html
Root-URL: 
Title: Making Maximum Entropy Computations Easier By Adding Extra Constraints (Extended Abstract)  
Author: Sally A. Goldman Ronald L. Rivest 
Date: March 12, 1997  
Address: Cambridge, Massachusetts 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: This paper presents a new way to compute the probability distribution with maximum entropy satisfying a set of constraints. Unlike previous approaches, our method is integrated with the planning of data collection and tabulation. We show how adding constraints and performing the associated additional tabulations can substantially speed up computation by replacing the usual iterative techniques with a straight-forward computation. These extra constraints are shown to correspond to the intermediate tables used in Cheeseman's method. We also show that the class of constraint graphs that our method handles is a proper generalization of Pearl's singly-connected networks. An open problem is to determine a minimal set of constraints necessary to make a hypergraph acyclic. We conjecture that this problem is NP-complete, and discuss heuristics to approximate the optimal solution. 
Abstract-found: 1
Intro-found: 1
Reference: [BFMMUY81] <author> Berri, C., R. Fagin, D. Maier, A. Mendelzon, J.D. Ullman and M. Yannakakis, </author> <title> "Properties of Acyclic Database Schemas," </title> <booktitle> in Proc. 13 th Annual ACM STOC (1981), </booktitle> <pages> 355-362. </pages>
Reference-contexts: We begin by describing how to model a set of attributes and associated constraints as a hypergraph. (A sim 5 ilar model was given by [EK83].) It is interesting to note that the work on the desirability of acyclic schemas first appeared in the database literature <ref> [BFMMUY81, BFMY83, TY82] </ref>. The attributes of the database replace the attributes in our problem, and the relations replace the constraint sets.
Reference: [BFMY83] <author> Beeri, C., R. Fagin, D. Maier and M. Yannakakis, </author> <title> "On the Desirability of Acyclic Database Schemas,"J. </title> <booktitle> ACM, 30,3 (1983), </booktitle> <pages> 355-362. </pages>
Reference-contexts: We begin by describing how to model a set of attributes and associated constraints as a hypergraph. (A sim 5 ilar model was given by [EK83].) It is interesting to note that the work on the desirability of acyclic schemas first appeared in the database literature <ref> [BFMMUY81, BFMY83, TY82] </ref>. The attributes of the database replace the attributes in our problem, and the relations replace the constraint sets.
Reference: [Br59] <author> Brown, </author> <title> D.T., "A Note on Approximations to Discrete Probability Distributions," </title> <journal> Information and Control, </journal> <volume> 2 (1959), </volume> <pages> 386-392. </pages>
Reference-contexts: Thus one must iterate repeatedly through the constraints until the desired accuracy is reached. (We note that the implicit constraint | that the probabilities sum to one | must usually be explicitly considered here.) Examples of this type of algorithm are given in <ref> [Br59, Ch83, Cs75, Fi70, IK68, KK69] </ref>. Representing the probability distribution explicitly as a table of 2 jV j values is usually impractical.
Reference: [Ch83] <author> Cheeseman, </author> <title> P.C., "A Method For Computing Generalized Bayesian Probability Values For Expert Systems," </title> <booktitle> in Proc. Eighth International Conference on Artificial Intelligence (August 1983), </booktitle> <pages> 198-202. </pages>
Reference-contexts: Thus one must iterate repeatedly through the constraints until the desired accuracy is reached. (We note that the implicit constraint | that the probabilities sum to one | must usually be explicitly considered here.) Examples of this type of algorithm are given in <ref> [Br59, Ch83, Cs75, Fi70, IK68, KK69] </ref>. Representing the probability distribution explicitly as a table of 2 jV j values is usually impractical. <p> Thus in originally calculating the ff's and later in evaluating queries it is necessary to evaluate a sum of terms, where each term is a product of ff's. This sum is difficult to compute since it may involve an exponential number of terms. Cheeseman <ref> [Ch83] </ref> proposes a clever technique for rewriting such sums in order to evaluate them more efficiently. For example ff A:::F is rewritten as follows. First, P A:::F is broken into six sums, each over one variable.
Reference: [Ch84] <author> Cheeseman, </author> <title> P.C., "Learning of Expert Systems From Data," </title> <booktitle> in Proc. IEEE Workshop on Principles of Knowledge Based Systems (1984), </booktitle> <pages> 115-122. </pages>
Reference-contexts: Here is a summary of how our method works: 8 1. We begin with a set of variables (attributes) and a set of constraint groups deemed to be of interest. (Cheeseman <ref> [Ch84] </ref> discusses a learning program which uses the raw data to find a set of significant constraints.
Reference: [CL68] <author> Chow, C.K. and C.N. Liu, </author> <title> "Approximating Discrete Probability Distributions With Dependence Trees,"IEEE Trans. </title> <journal> on Info. Theory, </journal> <month> IT-14,3 (May </month> <year> 1968), </year> <pages> 462-467. </pages>
Reference-contexts: These approaches usually restrict the kinds of constraints that might be supplied, and assume conditional independence explicitly as needed to force a unique result. This approach is taken by Chow and Liu <ref> [CL68] </ref> and Pearl [Pe85]. These methods construct a dependency tree where nodes represent variables and links represent direct dependencies; all direct influences on a node come from its parent.
Reference: [Cs75] <author> Csiszar, I., </author> <title> "I-Divergence geometry of probability distributions and minimization problems," </title> <journal> Annals of Probability, </journal> <month> 3,1 </month> <year> (1975), </year> <pages> 146-158. 16 </pages>
Reference-contexts: Thus one must iterate repeatedly through the constraints until the desired accuracy is reached. (We note that the implicit constraint | that the probabilities sum to one | must usually be explicitly considered here.) Examples of this type of algorithm are given in <ref> [Br59, Ch83, Cs75, Fi70, IK68, KK69] </ref>. Representing the probability distribution explicitly as a table of 2 jV j values is usually impractical.
Reference: [EK83] <author> Edwards, D., and S. Kreiner, </author> <title> "Analysis of contingency tables by graph-ical models," </title> <journal> Biometrika 70,3 (1983), </journal> <pages> 553-565. </pages>
Reference-contexts: We begin by describing how to model a set of attributes and associated constraints as a hypergraph. (A sim 5 ilar model was given by <ref> [EK83] </ref>.) It is interesting to note that the work on the desirability of acyclic schemas first appeared in the database literature [BFMMUY81, BFMY83, TY82]. The attributes of the database replace the attributes in our problem, and the relations replace the constraint sets. <p> We begin with a set of variables (attributes) and a set of constraint groups deemed to be of interest. (Cheeseman [Ch84] discusses a learning program which uses the raw data to find a set of significant constraints. Edwards and Kreiner <ref> [EK83] </ref> also discuss how to choose a good set of constraints.) Here a "constraint group" is a set of variables; the intent is that during data-gathering there will be one table created for each constraint group, and the observed events will be tabulated once in each table according to the values
Reference: [Fi70] <author> Fienberg, </author> <title> S.E., "An Iterative Procedure For Estimation In Contingency Tables," </title> <journal> The Annals of Mathematical Statistics, </journal> <month> 41,3 </month> <year> (1970), </year> <pages> 907-917. </pages>
Reference-contexts: Thus one must iterate repeatedly through the constraints until the desired accuracy is reached. (We note that the implicit constraint | that the probabilities sum to one | must usually be explicitly considered here.) Examples of this type of algorithm are given in <ref> [Br59, Ch83, Cs75, Fi70, IK68, KK69] </ref>. Representing the probability distribution explicitly as a table of 2 jV j values is usually impractical.
Reference: [Ge86] <author> Geman, S., </author> <title> "Stochastic Relaxation Methods For Image Restoration and Expert Systems," </title> <editor> In Cooper, D.B., R.L. Launer, and E. McClure, editors, </editor> <title> Automated Image Analysis: Theory and Experiments, </title> <address> New York: </address> <note> Academic Press, (to appear). </note>
Reference-contexts: Since the minimum fill-in problem has been proven to be NP-Complete [Ya81], we conjecture that this problem is as well. Some alternative approaches to the iterative scheme have been proposed. One of the more interesting proposals is due to Geman <ref> [Ge86, Li86] </ref>. This method uses stochastic relaxation to simultaneously adjust the ff's to meet all of the constraints, instead of satisfying one constraint at a time.
Reference: [Gr79] <author> Graham, M.H., </author> <title> "On the Universal Relation," </title> <institution> University of Toronto Technical Report (1979). </institution>
Reference-contexts: Delete any vertices which belong to only one hyperedge. 2. Delete any hyperedges which are subsumed by another hyperedge. Graham's algorithm is the procedure of applying reduction steps 1 and 2 until either the empty set is reached, or neither can be applied <ref> [Gr79] </ref>. Before proceeding, we define some necessary notation regarding the above reduction procedure. Let E (0) = fE (0) m g, where E (0) i is the i th hyperedge of G.
Reference: [IK68] <author> Ireland, C.T., and S. Kullback, </author> <title> "Contingency tables with given marginals," </title> <journal> Biometrika 55,1 (1968), </journal> <pages> 179-188. </pages>
Reference-contexts: Thus one must iterate repeatedly through the constraints until the desired accuracy is reached. (We note that the implicit constraint | that the probabilities sum to one | must usually be explicitly considered here.) Examples of this type of algorithm are given in <ref> [Br59, Ch83, Cs75, Fi70, IK68, KK69] </ref>. Representing the probability distribution explicitly as a table of 2 jV j values is usually impractical.
Reference: [Ja79] <author> Jaynes, E.T., </author> <title> "Where Do We Stand On Maximum Entropy," </title> <editor> In Levine and Tribune, editors, </editor> <title> The Maximum Entropy Formalism, </title> <publisher> M.I.T. Press, </publisher> <year> (1979). </year>
Reference-contexts: In general, there may be many probability distributions P satisfying the given constraints; in this case we are interested in that unique distribution P fl which maximizes the entropy H (P ) = V Motivation for this choice can be found in <ref> [Ja79, Ja82, JS83, Le59, SJ80, TTL84] </ref>. The maximum entropy distribution P fl is known to have a simple representation. For each ! in E i , there are 2 jE i j non-negative real variables ff E i (!) (i.e., one variable per constraint), that determine P fl as follows.
Reference: [Ja82] <author> Jaynes, E.T., </author> <title> "On the Rationale of Maximum-Entropy Methods," </title> <booktitle> Proceedings of the IEEE, </booktitle> <month> 70,9 (September </month> <year> 1982), </year> <pages> 939-952. </pages>
Reference-contexts: In general, there may be many probability distributions P satisfying the given constraints; in this case we are interested in that unique distribution P fl which maximizes the entropy H (P ) = V Motivation for this choice can be found in <ref> [Ja79, Ja82, JS83, Le59, SJ80, TTL84] </ref>. The maximum entropy distribution P fl is known to have a simple representation. For each ! in E i , there are 2 jE i j non-negative real variables ff E i (!) (i.e., one variable per constraint), that determine P fl as follows.
Reference: [JS83] <author> Johnson, R.W., and J.E. Shore, </author> <title> "Comments and corrections to `Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy'," </title> <journal> IEEE Trans. Inform. Theory IT-29, </journal> <volume> 6 (Nov. </volume> <year> 1983), </year> <pages> 942-943. </pages>
Reference-contexts: In general, there may be many probability distributions P satisfying the given constraints; in this case we are interested in that unique distribution P fl which maximizes the entropy H (P ) = V Motivation for this choice can be found in <ref> [Ja79, Ja82, JS83, Le59, SJ80, TTL84] </ref>. The maximum entropy distribution P fl is known to have a simple representation. For each ! in E i , there are 2 jE i j non-negative real variables ff E i (!) (i.e., one variable per constraint), that determine P fl as follows.
Reference: [KK69] <author> Ku, H.H. and S. Kullback, </author> <title> "Approximating Discrete Probability Distributions," </title> <journal> IEEE Trans. on Info. Theory, </journal> <month> IT-15,4 (July </month> <year> 1969), </year> <pages> 444-447. </pages>
Reference-contexts: Thus one must iterate repeatedly through the constraints until the desired accuracy is reached. (We note that the implicit constraint | that the probabilities sum to one | must usually be explicitly considered here.) Examples of this type of algorithm are given in <ref> [Br59, Ch83, Cs75, Fi70, IK68, KK69] </ref>. Representing the probability distribution explicitly as a table of 2 jV j values is usually impractical.
Reference: [Le59] <author> Lewis, </author> <title> P.M., "Approximating Probability Distributions to Reduce Storage Requirements," </title> <journal> Information and Control, </journal> <volume> 2 (1959), </volume> <pages> 214-225. </pages>
Reference-contexts: In general, there may be many probability distributions P satisfying the given constraints; in this case we are interested in that unique distribution P fl which maximizes the entropy H (P ) = V Motivation for this choice can be found in <ref> [Ja79, Ja82, JS83, Le59, SJ80, TTL84] </ref>. The maximum entropy distribution P fl is known to have a simple representation. For each ! in E i , there are 2 jE i j non-negative real variables ff E i (!) (i.e., one variable per constraint), that determine P fl as follows.
Reference: [Li86] <author> Lippman, </author> <title> A.F., "A Maximum Entropy Method for Expert System Construction," </title> <type> PhD thesis, </type> <institution> Brown University, Division of Applied Mathematics, </institution> <month> (May </month> <year> 1986). </year>
Reference-contexts: Since the minimum fill-in problem has been proven to be NP-Complete [Ya81], we conjecture that this problem is as well. Some alternative approaches to the iterative scheme have been proposed. One of the more interesting proposals is due to Geman <ref> [Ge86, Li86] </ref>. This method uses stochastic relaxation to simultaneously adjust the ff's to meet all of the constraints, instead of satisfying one constraint at a time.
Reference: [Ma85] <author> Malvestuto, </author> <title> F.M., "Approximating Discrete Probability Distributions: Easy and Difficult Cases," </title> <type> Unpublished Manuscript, </type> <month> (December </month> <year> 1985). </year> <month> 17 </month>
Reference-contexts: With the enlarged set of constraints, the computation of the maximum entropy distribution has a simple form which generalizes the equation suggested by Pearl. 4 Using Acyclic Hypergraphs Our approach is based on the work of Malvestuto <ref> [Ma85] </ref>, who derived sufficient conditions for writing marginals of the maximum entropy formula as a product of easily calculated probabilities. <p> If G is acyclic then there exists an l such that E (l+1) = ;. When the hypergraph is acyclic, the maximum entropy distribution, P fl (V ), is given by: <ref> [Ma85] </ref> P fl (V ) = k=0 i P (E i ) i P (Y i ) Y P (E i ) (5) Note that no ff's are needed; the formula depends only on probabilities in the original input data (constraints). <p> This formula is an immediate extension of the following theorem. Theorem 1 <ref> [Ma85] </ref> Given a decomposition, E = fE 1 ; : : : ; E m g, the maximum entropy distribution is given by the following.
Reference: [Pe85] <author> Pearl, J., </author> <title> "Fusion, Propagation and Structuring in Bayesian Net--works," </title> <institution> University Of California, Los Angeles Dept. of Computer Science Technical Report CSD-850022 R-42, </institution> <month> (April </month> <year> 1985). </year>
Reference-contexts: These approaches usually restrict the kinds of constraints that might be supplied, and assume conditional independence explicitly as needed to force a unique result. This approach is taken by Chow and Liu [CL68] and Pearl <ref> [Pe85] </ref>. These methods construct a dependency tree where nodes represent variables and links represent direct dependencies; all direct influences on a node come from its parent. <p> Here the set of all conditional probabilities of the form, P (childjparent), together with the probability distribution of the variable at the root, suffice to define a unique probability distribution. Pearl <ref> [Pe85] </ref> generalizes the tree condition to a network which has at most one undirected path between any pair of nodes (a singly-connected network).
Reference: [RT78] <author> Rose, D.J. and R.E. Tarjan, </author> <title> "Algorithmic Aspects of Vertex Elimination in Directed Graphs," </title> <journal> SIAM Journal Applied Math, </journal> <volume> 24 (1978), </volume> <pages> 176-197. </pages>
Reference-contexts: The choice of variable ordering which minimizes the cost of evaluating a sum can be viewed as a vertex ordering problem in a graph. This problem is very similar to the minimum fill-in problem encountered when performing Gaussian elimination on sparse symmetric matrices <ref> [RT78, RTL76] </ref>. Since the minimum fill-in problem has been proven to be NP-Complete [Ya81], we conjecture that this problem is as well. Some alternative approaches to the iterative scheme have been proposed. One of the more interesting proposals is due to Geman [Ge86, Li86].
Reference: [RTL76] <author> Rose, D.J., R.E. Tarjan, and G.S. Lueker, </author> <title> "Algorithmic Aspects of Vertex Elimination on Graphs," </title> <journal> SIAM Journal Comput., </journal> <month> 5,2 (June </month> <year> 1976), </year> <pages> 266-283. </pages>
Reference-contexts: The choice of variable ordering which minimizes the cost of evaluating a sum can be viewed as a vertex ordering problem in a graph. This problem is very similar to the minimum fill-in problem encountered when performing Gaussian elimination on sparse symmetric matrices <ref> [RT78, RTL76] </ref>. Since the minimum fill-in problem has been proven to be NP-Complete [Ya81], we conjecture that this problem is as well. Some alternative approaches to the iterative scheme have been proposed. One of the more interesting proposals is due to Geman [Ge86, Li86].
Reference: [SJ80] <author> Shore, J.E., and R.W. Johnson, </author> <title> "Axiomatic Derivation of the Principle of Maximum Entropy and the Principle of Minimum Cross-Entropy," </title> <journal> IEEE Trans. Inform. Theory, </journal> <month> IT-26,1 (Jan. </month> <year> 1980), </year> <pages> 26-37. </pages>
Reference-contexts: In general, there may be many probability distributions P satisfying the given constraints; in this case we are interested in that unique distribution P fl which maximizes the entropy H (P ) = V Motivation for this choice can be found in <ref> [Ja79, Ja82, JS83, Le59, SJ80, TTL84] </ref>. The maximum entropy distribution P fl is known to have a simple representation. For each ! in E i , there are 2 jE i j non-negative real variables ff E i (!) (i.e., one variable per constraint), that determine P fl as follows.
Reference: [TTL84] <author> Tikochinsky, Y., N.Z. Tishby, and R.D. Levine, </author> <title> "Consistent inference of probabilities for reproducible experiments," </title> <journal> Physical Rev. Letters 52, </journal> <month> 16 (16 April </month> <year> 1984), </year> <pages> 1357-1360. </pages>
Reference-contexts: In general, there may be many probability distributions P satisfying the given constraints; in this case we are interested in that unique distribution P fl which maximizes the entropy H (P ) = V Motivation for this choice can be found in <ref> [Ja79, Ja82, JS83, Le59, SJ80, TTL84] </ref>. The maximum entropy distribution P fl is known to have a simple representation. For each ! in E i , there are 2 jE i j non-negative real variables ff E i (!) (i.e., one variable per constraint), that determine P fl as follows.
Reference: [TY82] <author> Tarjan, R.E. and M. Yannakakis, </author> <title> "Simple Linear-Time Algorithms to Test Chordality of Graphs, Test Acyclicity of Hypergraphs, and Selectively Reduce Acyclic Hypergraphs," </title> <journal> SIAM J. Comp., </journal> <month> 13,3 (August </month> <year> 1984), </year> <pages> 566-579. </pages>
Reference-contexts: We begin by describing how to model a set of attributes and associated constraints as a hypergraph. (A sim 5 ilar model was given by [EK83].) It is interesting to note that the work on the desirability of acyclic schemas first appeared in the database literature <ref> [BFMMUY81, BFMY83, TY82] </ref>. The attributes of the database replace the attributes in our problem, and the relations replace the constraint sets.
Reference: [Ya81] <author> Yannakakis, M., </author> <title> "Computing the Minimum Fill-in is NP-Complete," </title> <journal> SIAM Journal Alg. Disc. Meth., </journal> <month> 2,1 (March </month> <year> 1981), </year> <pages> 77-79. 18 </pages>
Reference-contexts: This problem is very similar to the minimum fill-in problem encountered when performing Gaussian elimination on sparse symmetric matrices [RT78, RTL76]. Since the minimum fill-in problem has been proven to be NP-Complete <ref> [Ya81] </ref>, we conjecture that this problem is as well. Some alternative approaches to the iterative scheme have been proposed. One of the more interesting proposals is due to Geman [Ge86, Li86].
References-found: 26

