URL: http://www.umiacs.umd.edu/research/yaser/cvpr94.proc.ps.Z
Refering-URL: http://www.umiacs.umd.edu/users/lsd/pubs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Computing Spatio-Temporal Representations of Human Faces  
Author: Yaser Yacoob Larry Davis 
Address: College Park, MD 20742  
Affiliation: Computer Vision Laboratory University of Maryland  
Abstract: An approach for analysis and representation of facial dynamics for recognition of facial expressions from image sequences is proposed. The algorithms we develop utilize optical flow computation to identify the direction of rigid and non-rigid motions that are caused by human facial expressions. A mid-level symbolic representation that is motivated by linguistic and psychological considerations is developed. Recognition of six facial expressions, as well as eye blinking, on a large set of image sequences is reported. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Abdel-Mottaleb, R. Chellappa, and A. Rosen-feld, </author> <title> "Binocular motion stereo using MAP estimation", </title> <journal> IEEE CVPR, </journal> <year> 1993, </year> <pages> 321-327. </pages>
Reference-contexts: A temporal consistency procedure is applied to the mid-level representation to filter out errors due to noise or illumination changes (see [14]). 3.4 Computing basic action cues The approach we use for optical flow computation is one recently proposed by Abdel-Mottaleb et al. <ref> [1] </ref>. The flow magnitudes are first thresholded to reduce the effect of small motions probably due to noise. The motion vectors are then re-quantized into eight principle directions. The optical flow vectors are filtered using both spatial and temporal procedures that improve their coherence and continuity, respectively.
Reference: [2] <author> J.N. Bassili, </author> <title> "Emotion recognition: The role of facial movement and the relative importance of upper and lower areas of the face," </title> <journal> Journal of Personality and Social Psychology, </journal> <volume> Vol. 37, </volume> <year> 1979, </year> <pages> 2049-2059. </pages>
Reference-contexts: These pictures allow one to detect the presence of static cues (e.g., wrinkles) as well as the position and shape of the facial features. Few studies have directly investigated the influence of the motion and deformation of facial features on the interpretation of facial expressions. Bassili <ref> [2] </ref> suggested that motion in the image of a face would allow emotions to be identified even with minimal information about the spatial arrangement of features. <p> It is based on the descriptions of the epic of facial expressions from static pictures as suggested by Ekman and Friesen in [6], and the descriptions of motion patterns of the face as proposed by Bassili in <ref> [2] </ref>. We chose not to model or analyze facial muscle actions, setting our work apart from [9,10,12] as well as not to use models for muscle actions [7]. Instead, we focus on the motions associated with the edges of the mouth, eyes, and eyebrows (the relevant considerations appear in [14]). <p> The dictionary borrows from the facial cues of universal expression descriptions proposed in [6], and from the motion patterns of expression proposed in <ref> [2] </ref>. As a result, we arrive at a dictionary that is a motion-based feature description of facial actions. The dictionary we propose is divided into: components, basic actions of these components, and motion cues. <p> As a result, we acquired a variety of presumably similar facial expressions; some were consistent with <ref> [2] </ref> and [6] while others varied.
Reference: [3] <author> V. Bruce, </author> <title> Recognizing Faces, </title> <publisher> Lawrence Erlbaum Assoc., </publisher> <address> London, </address> <year> 1988. </year>
Reference: [4] <author> G. Chow and X. Li, </author> <title> "Towards a system for automatic facial feature detection," </title> <journal> Pattern Recognition, </journal> <volume> Vol. 26, No. 12, </volume> <year> 1993, </year> <pages> 1739-1755. </pages>
Reference: [5] <author> P. Ekman, </author> <title> (Edited) Darwin and Facial Expression, </title> <publisher> Academic Press, Inc. </publisher> <year> 1973. </year>
Reference: [6] <author> P. Ekman and W. Friesen, </author> <title> Unmasking the Face, </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1975. </year>
Reference-contexts: We provide an expression classifier that employs a representation of facial feature actions. It is based on the descriptions of the epic of facial expressions from static pictures as suggested by Ekman and Friesen in <ref> [6] </ref>, and the descriptions of motion patterns of the face as proposed by Bassili in [2]. We chose not to model or analyze facial muscle actions, setting our work apart from [9,10,12] as well as not to use models for muscle actions [7]. <p> motion directions within a rectangle are used to verify translation of rectangles upward and downward (by measuring significant sim ilar optical flow) and verify scaling of the rectangles (by measuring motions that imply scaling). 3 Computing motion representations 3.1 Psychological basis A summary of the results of Ekman and Friesen <ref> [6] </ref> on the universal cues for recognizing the six principle emotions appears in [14]. These cues describe the peak of each expression and thus they provide a human interpretation of the static appearance of the facial feature. <p> The dictionary borrows from the facial cues of universal expression descriptions proposed in <ref> [6] </ref>, and from the motion patterns of expression proposed in [2]. As a result, we arrive at a dictionary that is a motion-based feature description of facial actions. The dictionary we propose is divided into: components, basic actions of these components, and motion cues. <p> As a result, we acquired a variety of presumably similar facial expressions; some were consistent with [2] and <ref> [6] </ref> while others varied. The variance can be attributed to the real variance in dynamics and intensities of expressions of individuals as well as to the artificial environment in which the subjects had to express emotions they were not feeling at the time (fear and sadness were hard to induce).
Reference: [7] <author> P. Ekman and W. Friesen, </author> <title> The Facial Action Coding System, </title> <publisher> Consulting Psychologists Press, Inc., </publisher> <address> San Francisco, CA, </address> <year> 1978. </year>
Reference-contexts: We chose not to model or analyze facial muscle actions, setting our work apart from [9,10,12] as well as not to use models for muscle actions <ref> [7] </ref>. Instead, we focus on the motions associated with the edges of the mouth, eyes, and eyebrows (the relevant considerations appear in [14]). Before proceeding, we introduce some terminology needed in the paper.
Reference: [9] <author> H. Li, P. Roivainen, and R. Forcheimer, </author> <title> "3-D motion estimation in model-based facial image coding," </title> <journal> IEEE PAMI, </journal> <volume> Vol. 15, No. 6, </volume> <year> 1993, </year> <pages> 545-555. </pages>
Reference: [10] <author> K. Mase, </author> <title> "Recognition of facial expression from optical flow," </title> <journal> IEICE Transactions, </journal> <volume> Vol. E 74, No. 10, </volume> <year> 1991, </year> <pages> 3474-3483. </pages>
Reference: [11] <author> K.R. Scherer and P. </author> <title> Ekman (Edited), Approaches to Emotion, </title> <publisher> Lawrence Erlbaum Associates, Inc., </publisher> <year> 1984. </year>
Reference: [12] <author> D. Terzopoulos, and K. Waters, </author> <title> "Analysis and synthesis of facial image sequences using physical and anatomical models," </title> <journal> IEEE PAMI, </journal> <volume> Vol. 15, No. 6, </volume> <year> 1993, </year> <pages> 569-579. </pages>
Reference-contexts: sequence is densely sampled in time. * We consider only the six primary emotions - happiness, sadness, anger, fear, disgust and surprise- and eye blinking. of our facial expression system. 2 Tracking face regions Approaches for localization of facial features were proposed in [4,13,16], and for facial feature tracking in <ref> [12] </ref>. Our approach to tracking the face regions is based on computing two sets of parameters at the points with high gradient values within the rectangle that encloses each feature.
Reference: [13] <author> Y. Yacoob, and L.S. Davis, </author> <title> "Labeling of human face components from range data," </title> <journal> IEEE CVPR, </journal> <year> 1993, </year> <pages> 592-593. </pages>
Reference: [14] <author> Y. Yacoob, and L.S. Davis, </author> <title> Recognizing Facial Expressions, </title> <note> in preparation. </note>
Reference-contexts: We chose not to model or analyze facial muscle actions, setting our work apart from [9,10,12] as well as not to use models for muscle actions [7]. Instead, we focus on the motions associated with the edges of the mouth, eyes, and eyebrows (the relevant considerations appear in <ref> [14] </ref>). Before proceeding, we introduce some terminology needed in the paper. Face region motion refers to the changes in images of facial features caused by facial actions corresponding to physical feature deformations on the 3-D surface of the face. <p> and downward (by measuring significant sim ilar optical flow) and verify scaling of the rectangles (by measuring motions that imply scaling). 3 Computing motion representations 3.1 Psychological basis A summary of the results of Ekman and Friesen [6] on the universal cues for recognizing the six principle emotions appears in <ref> [14] </ref>. These cues describe the peak of each expression and thus they provide a human interpretation of the static appearance of the facial feature. <p> For example, the raising of the right and left eyebrows produce a "raising brows" coordinated action. A temporal consistency procedure is applied to the mid-level representation to filter out errors due to noise or illumination changes (see <ref> [14] </ref>). 3.4 Computing basic action cues The approach we use for optical flow computation is one recently proposed by Abdel-Mottaleb et al. [1]. The flow magnitudes are first thresholded to reduce the effect of small motions probably due to noise. The motion vectors are then re-quantized into eight principle directions. <p> Single vertical partitions capture mouth horizontal expansions and contractions when the mouth is not completely horizontal. The two vertical partitions are designed to capture the motion of the corners of the mouth. The statistical measurements of these partitions (for details see <ref> [14] </ref>) are used to construct the mid-level representation of a region motion. The highest ranking partition in each type is used as a pointer into the dictionary of motions (see Table 1), to determine the action that may have occurred at the feature.
Reference: [15] <author> A.W. Young and H.D. Ellis (Edited), </author> <title> Handbook of Research on Face Processing, </title> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1989. </year>
Reference-contexts: The six principle emotions are: happiness, sadness, surprise, fear, anger, and disgust (Figure 1). Most psychology research on facial expression has been conducted on "mug-shot" pictures that capture the subject's expression at its peak <ref> [15] </ref>. These pictures allow one to detect the presence of static cues (e.g., wrinkles) as well as the position and shape of the facial features. Few studies have directly investigated the influence of the motion and deformation of facial features on the interpretation of facial expressions.
Reference: [16] <author> A.L. Yuille, D.S. Cohen, and P.W. Hallinan, </author> <title> "Feature extraction from faces using deformable templates," </title> <journal> IEEE CVPR, </journal> <year> 1989, </year> <pages> 104-109. </pages>
References-found: 15

