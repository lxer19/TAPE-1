URL: http://www.first.gmd.de/promoter/papers/PDPTA97.ps.gz
Refering-URL: http://www.first.gmd.de/promoter/papers/index.html
Root-URL: 
Title: Object-oriented Data Parallel Programming in C++  
Author: Hua Bi 
Keyword: data parallel programming, object orientation, SPMD paradigm, distributed memory parallel architecture  
Address: Rudower Chaussee 5, 12489 Berlin, Germany  
Affiliation: RWCP Laboratory at GMD-FIRST GMD Institute for Computer Architecture and Software Technology  
Note: In the Proceedings of International Conference on Parallel and Distributed Processing Techniques and Applications (PDPTA'97), pp. 1044 1053,  Las Vegas, USA. Copyright  
Email: email: bi@first.gmd.de  
Date: June 30 July 3, 1997,  1997 CSREA  
Abstract: A lot of applications executed on distributed memory parallel computers can be classified as data parallel applications, in which parallel operations on data elements are performed in a loosely synchronous manner. However implementation of such applications is still difficult. This paper presents an object-oriented approach for an easy and efficient data parallel programming in C++. More exactly, an object model for distributed data is defined for modularity, polymorphism, and object sharing in programs, and data distribution and message passing details are encapsulated to reduce programming complexity and make reuse of code easier. Experiments show that this approach is efficient for a large class of data parallel applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Besch, H. Bi, P. Enskonatus, G. Heber, M. Wilhelmi. </author> <title> High-level Data Parallel Programming in PROMOTER, </title> <booktitle> In Proc. of 2nd Int. Workshop on High-level Parallel Programming Models and Supportive Environments, </booktitle> <pages> pp. 47-54, </pages> <publisher> IEEE CS Press, </publisher> <address> April, 1997, Geneva, Switzerland. </address>
Reference-contexts: A Triangular class can be defined as follows: class Triangular f int n; public: Triangular (const int n) f n=n;g ~ Triangular () fg int valid (const Point p) const f if ((p [0]&gt;=0) && (p [0]&lt; n) && (p [0]&gt;=p <ref> [1] </ref>)) return 1; else return 0;g static int dim () const f return 2;g g; given. For a Triangular (n) , we can define many different distributions. The following code just gives a block distribution on the first dimension. <p> len; public: TriMat (const Triangular& tp, const int n) : topo (tp) f blk len=n/domain max; g ~TriMat () f g const Triangular& topo () const f return topo; g Domain domain (const Point p) const f return p [0]=blk len;g int index (const Point p) const f int idx=p <ref> [1] </ref>; int row=domain (p)flblk len; for (int i=p [0]1; i&gt;= row; i) f return idx;g int length (const Domain d) const f return dflblk lenflblk len+blk lenfl (blk len+1)=2; g g; After defining a distribution, we can declare distributed objects which may share the same distribution as in the following code: <p> 3, the logical communication relation then can be represented by the following class: class Near : CommRel&lt;2, 2&gt; f PointList&lt;2&gt; pl (8); publlic: PointList&lt;2&gt;& connect (const Point p) f pl.rewind (); return pl &lt;< p [0] &lt;< p <ref> [1] </ref>+1 &lt;< p [0] &lt;< p [1]-1 &lt;< p [0]-1 &lt;< p [1]; &lt;< p [0]-1 &lt;< p [1]+1; &lt;< p [0]+1 &lt;< p [1]-1; g Having a logical communication relation, the following template function can be used for communication scheduling: template&lt;class DT1, class RS1, class DT2, class RS2&gt; void CommSch (CP*, DT1&, RS1&, DT2&, RS2&, CR&); CommSch generates a communication pattern of <p> It can be used at three levels: C++-level, library-level, and compiler-level. This paper just describes the approach used in C++-level without library support or compiler support. In the project Promoter <ref> [5, 1, 2] </ref>, this approach is exploited at all three levels. At a library-level, libraries for different Topology and Distribution classes can be provided, and runtime support to compute Local Iter and Comm Pattern with respect to defined Topology and Distribution classes can be used in writing data parallel programs.
Reference: [2] <author> H. Bi. </author> <title> Towards Abstraction of Message Passing Programming . In Proc. </title> <booktitle> of Int. Conference on Advances on Parallel and 1 MANNA is a scalable parallel machine with distributed memory. See http://www.first.gmd.de. Distributed Computing, </booktitle> <pages> pp. 100-107, </pages> <publisher> IEEE CS Press, </publisher> <address> March, 1997, Shanghai, China. </address>
Reference-contexts: It can be used at three levels: C++-level, library-level, and compiler-level. This paper just describes the approach used in C++-level without library support or compiler support. In the project Promoter <ref> [5, 1, 2] </ref>, this approach is exploited at all three levels. At a library-level, libraries for different Topology and Distribution classes can be provided, and runtime support to compute Local Iter and Comm Pattern with respect to defined Topology and Distribution classes can be used in writing data parallel programs.
Reference: [3] <author> C. Chang, J. Saltz, and A. Sussman. </author> <title> Chaos++: A runtime library for supporting distributed dynamic data structure. </title> <type> Technical Report CRPC-TR95624, </type> <institution> Center for Res. on Parallel Computation, Rice University, </institution> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: For example, a Hpf compiler should generate Local Iter and Comm Pattern for a forall loop with distributed arrays with the (cycle) block distribution. There are a lot of work in parallel C++ Efforts: Icc++ [4], C** [7], Pc++ [8], Chaos++ <ref> [3] </ref>, Hpc++[6], and others. Roughly speaking these efforts exploit two types of parallelism: data and task parallelism. In our approach, only data parallelism is exploited and no task parallel efforts such as ob ject concurrency model have been made.
Reference: [4] <author> A. A. Chien and J. Dolby. </author> <title> The illinois concert system: A problem-solving environment for irregular applications. </title> <booktitle> In Proc. of DAGS'94, The Sym. on Parallel Computation and Problem Solving Environments, </booktitle> <year> 1994. </year>
Reference-contexts: For example, a Hpf compiler should generate Local Iter and Comm Pattern for a forall loop with distributed arrays with the (cycle) block distribution. There are a lot of work in parallel C++ Efforts: Icc++ <ref> [4] </ref>, C** [7], Pc++ [8], Chaos++ [3], Hpc++[6], and others. Roughly speaking these efforts exploit two types of parallelism: data and task parallelism. In our approach, only data parallelism is exploited and no task parallel efforts such as ob ject concurrency model have been made.
Reference: [5] <author> W. K. Giloi, M. Kessler, and A. Schramm. </author> <title> Promoter : A high level object-parallel programming language. </title> <booktitle> In Proc. of Inter-nat. Conf. on High Performance Computing, </booktitle> <address> New Delhi, India, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: It can be used at three levels: C++-level, library-level, and compiler-level. This paper just describes the approach used in C++-level without library support or compiler support. In the project Promoter <ref> [5, 1, 2] </ref>, this approach is exploited at all three levels. At a library-level, libraries for different Topology and Distribution classes can be provided, and runtime support to compute Local Iter and Comm Pattern with respect to defined Topology and Distribution classes can be used in writing data parallel programs.
Reference: [6] <author> G. </author> <title> HPC. </title> <type> Hpc++ white paper. Technical Report CRPC-TR95633, </type> <institution> Center for Res. on Parallel Computation, Rice University, </institution> <year> 1995. </year>
Reference: [7] <author> L. R. Larus. </author> <title> A large-grain, object-oriented data-parallel programming language. </title> <editor> In U. Banerjee, A. N. D. Gelernter, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing (5th International Workshop), </booktitle> <pages> pages 326-341. </pages> <publisher> Springer-Verlag, </publisher> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: For example, a Hpf compiler should generate Local Iter and Comm Pattern for a forall loop with distributed arrays with the (cycle) block distribution. There are a lot of work in parallel C++ Efforts: Icc++ [4], C** <ref> [7] </ref>, Pc++ [8], Chaos++ [3], Hpc++[6], and others. Roughly speaking these efforts exploit two types of parallelism: data and task parallelism. In our approach, only data parallelism is exploited and no task parallel efforts such as ob ject concurrency model have been made.
Reference: [8] <author> A. Malony, B. Mohr, D. Beckman, D. Gan-non, S. Yang, F. Bodin, and S. Kesavan. </author> <title> A parallel c++ runtime system for scalable parallel systems. </title> <booktitle> In Proc. of Supercomputing'93, </booktitle> <pages> pages 140-152. </pages> <publisher> IEEE CS. Press, </publisher> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: For example, a Hpf compiler should generate Local Iter and Comm Pattern for a forall loop with distributed arrays with the (cycle) block distribution. There are a lot of work in parallel C++ Efforts: Icc++ [4], C** [7], Pc++ <ref> [8] </ref>, Chaos++ [3], Hpc++[6], and others. Roughly speaking these efforts exploit two types of parallelism: data and task parallelism. In our approach, only data parallelism is exploited and no task parallel efforts such as ob ject concurrency model have been made.
Reference: [9] <author> A. Schramm. </author> <title> Irregular applications in promoter. </title> <editor> In W. K. Giloi, S. Jaenichen, and B. Shriver, editors, </editor> <booktitle> Proc. of Internat. MPPM Conference, </booktitle> <address> Berlin, Germany, Oct. 1995. </address> <publisher> IEEE CS. Press. </publisher>
Reference-contexts: Table 1 shows the times (seconds) needed for matrix multiplication (mm), relaxation (rl), and conjugate gradients (cg). One of our future work is support of Dynamic Topology <ref> [9] </ref>, that is, a distributed object can change its shape at runtime. It provides a conceptual equivalence to dynamic creation or expansion in pointer-based data structures.
References-found: 9

