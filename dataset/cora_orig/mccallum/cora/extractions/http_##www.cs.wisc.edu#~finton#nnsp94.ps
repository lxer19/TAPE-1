URL: http://www.cs.wisc.edu/~finton/nnsp94.ps
Refering-URL: http://www.cs.wisc.edu/~finton/djfpubs.html
Root-URL: 
Title: AN APPLICATION OF IMPORTANCE-BASED FEATURE EXTRACTION IN REINFORCEMENT LEARNING  
Author: David J. Finton Yu Hen Hu 
Address: Madison, WI 53706  Madison, WI 53706  
Affiliation: Computer Sciences Department University of Wisconsin-Madison  Department of Electrical and Computer Engineering University of Wisconsin-Madison  
Abstract: The sparse feedback in reinforcement learning problems makes feature extraction difficult. We present importance-based feature extraction, which guides a bottom-up self-organization of feature detectors according to top-down information as to the importance of the features; we define importance in terms of the reinforcement values expected as a result of taking different actions when a feature is recognized. We illustrate these ideas in terms of the pole-balancing task and a learning system which combines bottom-up tuning with a distributed version of Q-learning; adding importance-based feature extraction to the detector tuning resulted in faster learning. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. W. Anderson, </author> <title> "Strategy learning with multilayer connectionist representations," </title> <booktitle> in Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <year> 1987, </year> <pages> pp. 103-114. </pages>
Reference-contexts: We define importance in terms of the weights w i;1 and w i;2 , which are the weights from detector i to the effector nodes for "push left" and "push right," respectively. Thus imp i = 0:5 j w i;1 w i;2 j which is in the range <ref> [0, 1] </ref>, since w i;j is in [-1,1]. We define closeness in terms of the Euclidean distance and the importance: close i = e dist i (1 + imp i ); with the importance factor 0. <p> w ij from detectors to the winning effector j are updated as follows: w ij (t+1) = w ij (t) + fidet i (t)(r (t + 1) goodness (t)) if r 6= 0 w ij (t) + fidet i (t)(fl goodness (t + 1) goodness (t)) otherwise where fl 2 <ref> [0; 1] </ref> is a discount factor. The weights to the other effectors are not updated. Note that this algorithm may be regarded as a fuzzy version of Q-learning. <p> Indeed, if the input representation is a partition (i.e., exactly one detector is active for each input state), the predicted values, eff k , are exactly those of Q-learning. POLE BALANCING The pole balancing task has been studied by Barto, Sutton and Anderson [3], Anderson <ref> [1] </ref> and others. The task involves a wheeled cart on a track, with a pole hinged to the top of the cart. <p> Our approach differs from theirs by separating feature learning from action learning, since we feel that feature extraction is a separate problem, which requires additional information. Anderson <ref> [1] </ref> reports results for a similar system which used back propagation to learn features in the pole-balancing task. This system required around 5000 failed trials before the feature detectors emerged, with learning proceeding quickly after that point.
Reference: [2] <author> C. W. Anderson and W. T. Miller, III, </author> <title> "A challenging set of control problems," </title> <editor> in W. T. Miller, III, R. S. Sutton and P. S. Werbos, editors, </editor> <title> Neural Networks for Control, </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1990, </year> <pages> Appendix. </pages>
Reference-contexts: The output of the controller is a binary value indicating a push on the cart either to the left or to the right. RESULTS We implemented the pole and cart dynamics according to the equations given in <ref> [2] </ref>. The criterion for a successful run was learning to keep the pole balanced for a trial of 100000 steps, which represents slightly more than half an hour of balancing in real-time. For each experiment, we made 15 runs with different random initialization seeds.
Reference: [3] <author> A. G. Barto, R. S. Sutton, and C. W. Anderson, </author> <title> "Neuronlike adaptive elements that can solve difficult learning control problems," </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> vol. SMC-13, no. 5, </volume> <pages> pp. 834-846, </pages> <month> Septem-ber </month> <year> 1983. </year>
Reference-contexts: Indeed, if the input representation is a partition (i.e., exactly one detector is active for each input state), the predicted values, eff k , are exactly those of Q-learning. POLE BALANCING The pole balancing task has been studied by Barto, Sutton and Anderson <ref> [3] </ref>, Anderson [1] and others. The task involves a wheeled cart on a track, with a pole hinged to the top of the cart. <p> Importance-based feature extraction combines elements of both approaches. We propose that feature extraction be done by a bottom-up competitive interaction among feature detectors, but that it be guided by top-down information as to the importance of the features. Barto, Sutton and Anderson <ref> [3] </ref>, Sutton [7] and others have constructed learning systems composed of a critic module and an action module; the critic module learns to predict the reinforcement feedback, and the action module learns to perform the task based on feedback from the critic module.
Reference: [4] <author> R. M. Holdaway, </author> <title> "Enhancing supervised learning algorithms via self-organization," </title> <booktitle> in Proceedings of the International Joint Conference on Neural Networks, </booktitle> <year> 1989, </year> <pages> pp. 523-529. </pages>
Reference-contexts: Whereas An-derson's back-propagation system tuned features according to the top-down feedback, our system merely uses top-down information to guide a bottom-up competitive learning which tunes the detectors. Our position is that detector tuning requires optimization of some kind of importance measure, which is absent in typical gradient-descent tuning. Holdaway <ref> [4] </ref> used competitive learning for feature extraction, but for a supervised learning task. His feature extraction module was trained off-line and used Kohonen's SOM [5], which produces a feature set based on the frequency of the input data points.
Reference: [5] <author> T. Kohonen, </author> <title> "Self-organized formation of topologically correct feature maps," </title> <journal> Biological Cybernetics, </journal> <volume> vol. 43, </volume> <pages> pp. 59-69, </pages> <year> 1982. </year>
Reference-contexts: Our position is that detector tuning requires optimization of some kind of importance measure, which is absent in typical gradient-descent tuning. Holdaway [4] used competitive learning for feature extraction, but for a supervised learning task. His feature extraction module was trained off-line and used Kohonen's SOM <ref> [5] </ref>, which produces a feature set based on the frequency of the input data points.
Reference: [6] <author> D. E. Rumelhart and D. Zipser, </author> <title> "Feature discovery by competitive learning," </title> <editor> in D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> vol. </volume> <pages> 1, </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1986, </year> <journal> ch. </journal> <volume> 5, </volume> <pages> pp. 151-193. </pages>
Reference: [7] <author> R. S. Sutton, </author> <title> Temporal Credit Assignment in Reinforcement Learning, </title> <type> Ph.D. thesis, </type> <institution> University of Massachusetts, Amherst, Massachusetts, </institution> <year> 1984. </year>
Reference-contexts: Importance-based feature extraction combines elements of both approaches. We propose that feature extraction be done by a bottom-up competitive interaction among feature detectors, but that it be guided by top-down information as to the importance of the features. Barto, Sutton and Anderson [3], Sutton <ref> [7] </ref> and others have constructed learning systems composed of a critic module and an action module; the critic module learns to predict the reinforcement feedback, and the action module learns to perform the task based on feedback from the critic module.
Reference: [8] <author> L. Z. Wang and J. V. Hanson, </author> <title> "Competitive learning and winning-weighted competition for optimal vector quantizer design," </title> <editor> in C. A. Kamm, G. M. Kuhn, B. Yoon, R. Chellappa and S. Y. Kung, editors, </editor> <booktitle> Neural Networks for Signal Processing III: Proceedings of the 1993 IEEE Workshop, </booktitle> <publisher> IEEE Press, </publisher> <year> 1993, </year> <pages> pp. 50-59. </pages>
Reference-contexts: Wang and Hanson's tuning rule <ref> [8] </ref> is similar to ours, but uses a "win-rate" parameter instead of our importance parameter. Their aim is to make the detector set correspond more closely to the probability density function of the inputs by equalizing the winning rates of the detectors.
Reference: [9] <author> C. J. C. H. Watkins, </author> <title> Learning From Delayed Rewards, </title> <type> Ph.D. thesis, </type> <institution> Cam-bridge University, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: The weights between the detectors and effectors are then updated by a fuzzy version of Q-learning <ref> [9] </ref>. In our system, the output value of an effector represents a Q-value: the predicted reinforcement for taking that action from the current system state. Therefore, an important detector is one which predicts very different reinforcement values according to which effector node activates.
References-found: 9

