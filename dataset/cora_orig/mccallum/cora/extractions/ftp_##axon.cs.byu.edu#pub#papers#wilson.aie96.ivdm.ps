URL: ftp://axon.cs.byu.edu/pub/papers/wilson.aie96.ivdm.ps
Refering-URL: http://synapse.cs.byu.edu/~randy/misc/pubs.html
Root-URL: 
Email: e-mail: randy@axon.cs.byu.edu, martinez@cs.byu.edu  
Title: Value Difference Metrics for Continuously Valued Attributes  
Author: D. Randall Wilson, Tony R. Martinez 
Keyword: Key words: instance-based learning, generalization, distance metrics, nearest neighbor  
Address: Provo, UT 84602, U.S.A.  
Affiliation: Computer Science Department, Brigham Young University,  
Note: Proceedings of the International Conference on Artificial Intelligence, Expert Systems and Neural Networks (AIE96), pp. 11-14, 1996.  
Abstract: Nearest neighbor and instance-based learning techniques typically handle continuous and linear input values well, but often do not handle symbolic input attributes appropriately. The Value Difference Metric (VDM) was designed to find reasonable distance values between symbolic attribute values, but it largely ignores continuous attributes, using discretization to map continuous values into symbolic values. This paper presents two heterogeneous distance metrics, called the Interpolated VDM (IVDM) and Windowed VDM (WVDM), that extend the Value Difference Metric to handle continuous attributes more appropriately. In experiments on 21 data sets the new distance metrics achieves higher classification accuracy in most cases involving continuous attributes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cover, T. M., and P. E. Hart, </author> <year> (1967). </year> <title> Nearest Neighbor Pattern Classification, </title> <journal> Institute of Electrical and Electronics Engineers Transactions on Information Theory, </journal> <volume> vol. 13, no. 1, </volume> <month> January </month> <year> 1967, </year> <pages> pp. 21-27. </pages>
Reference: [2] <author> Hart, P. E., </author> <year> (1968). </year> <title> The Condensed Nearest Neighbor Rule, </title> <journal> Institute of Electrical and Electronics Engineers Transactions on Information Theory, </journal> <volume> vol. 14, </volume> <pages> pp. 515-516. </pages>
Reference: [3] <author> Dasarathy, Belur V., </author> <year> (1991). </year> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques, </title> <publisher> Los Alamitos, </publisher> <address> CA: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: [4] <author> Aha, David W., Dennis Kibler, Marc K. Albert, </author> <year> (1991). </year> <title> Instance-Based Learning Algorithms, </title> <journal> Machine Learning, </journal> <volume> vol. 6, </volume> <pages> pp. 37-66. </pages>
Reference: [5] <author> Aha, David W., </author> <year> (1992). </year> <title> Tolerating noisy, irrelevant and novel attributes in instance-based learning algorithms, </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> vol. 36, </volume> <pages> pp. 267-287. </pages>
Reference: [6] <author> Wilson, D. Randall, </author> <year> (1994). </year> <title> Prototype Styles of Generalization, </title> <type> Masters Thesis, </type> <institution> Brigham Young University. </institution>
Reference: [7] <author> Wettschereck, Dietrich, David W. Aha, and Takao Mohri, </author> <year> (1995). </year> <title> A Review and Comparative Evaluation of Feature Weighting Methods for Lazy Learning Algorithms, </title> <type> Technical Report AIC-95-012, </type> <institution> Washington, D.C.: Naval Research Laboratory, </institution> <note> Navy Center for Applied Research in Artificial Intelligence. </note>
Reference: [8] <author> Stanfill, C., and D. Waltz, </author> <year> (1986). </year> <title> Toward memory-based reasoning, </title> <journal> Communications of the ACM , vol. </journal> <volume> 29, </volume> <month> December </month> <year> 1986, </year> <pages> pp. 1213-1228. </pages>
Reference-contexts: 1. Introduction Nearest neighbor (NN) techniques [1][2][3], Instance-Based Learning (IBL) algorithms [4][5][6][7], and Memory-Based Reasoning methods <ref> [8] </ref> have had much success on a wide variety of applications. Such algorithms typically store some or all available training examples during learning. <p> These metrics work well for numerical attributes, but they do not appropriately handle symbolic (i.e., nonlinear, and perhaps unordered) attributes. The Value Difference Metric (VDM) was introduced by Stanfill & Waltz <ref> [8] </ref> in order to provide an appropriate distance function for symbolic attributes. The Modified Value Difference Metric (MVDM) was introduced by Cost & Salzberg [9] and used in the PEBLS system [10], and modifies the weighting scheme used by the VDM. <p> Using a linear distance measurement such as (1) or (2) on such values makes little sense in this case, so the above distance functions would be inappropriate for symbolic attributes. Value Difference Metric. The Value Difference Metric (VDM) <ref> [8] </ref> was introduced to provide an appropriate distance function for symbolic attributes.
Reference: [9] <author> Cost, Scott, and Steven Salzberg, </author> <year> (1993). </year> <title> A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features, </title> <journal> Machine Learning, </journal> <volume> vol. 10, </volume> <pages> pp. 57-78. </pages>
Reference-contexts: The Value Difference Metric (VDM) was introduced by Stanfill & Waltz [8] in order to provide an appropriate distance function for symbolic attributes. The Modified Value Difference Metric (MVDM) was introduced by Cost & Salzberg <ref> [9] </ref> and used in the PEBLS system [10], and modifies the weighting scheme used by the VDM. These distance metrics work well in many symbolic domains, but they do not handle continuous attributes directly. Instead, they rely upon discretization, which often degrades generalization accuracy [11]. <p> Any of the various available weighting schemes can be used in conjunction with the new distance functions presented here. Subsequent Extensions. There have been several extensions made to the original distance function presented by Stanfill & Waltz. Cost & Salzberg <ref> [9] </ref> used Equation (3) with q=1, which they found to be approximately as accurate and computationally less expensive than q=2 as used by Stanfill & Waltz. They also modified the instance weighting scheme, and implemented their algorithm in a system called PEBLS [10].
Reference: [10] <author> Rachlin, John, Simon Kasif, Steven Salzberg, David W. Aha, </author> <year> (1994). </year> <title> Towards a Better Understanding of Memory-Based and Bayesian Classifiers, </title> <booktitle> in Proceedings of the Eleventh International Machine Learning Conference, </booktitle> <address> New Brunswick, NJ: </address> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 242-250. </pages>
Reference-contexts: The Value Difference Metric (VDM) was introduced by Stanfill & Waltz [8] in order to provide an appropriate distance function for symbolic attributes. The Modified Value Difference Metric (MVDM) was introduced by Cost & Salzberg [9] and used in the PEBLS system <ref> [10] </ref>, and modifies the weighting scheme used by the VDM. These distance metrics work well in many symbolic domains, but they do not handle continuous attributes directly. Instead, they rely upon discretization, which often degrades generalization accuracy [11]. <p> Cost & Salzberg [9] used Equation (3) with q=1, which they found to be approximately as accurate and computationally less expensive than q=2 as used by Stanfill & Waltz. They also modified the instance weighting scheme, and implemented their algorithm in a system called PEBLS <ref> [10] </ref>. Domingos [12] presented a system which combines instance-based learning with inductive rules. His system used Equation (3) with q=1 for symbolic attributes and a normalized linear difference for linear attributes.
Reference: [11] <author> Ventura, Dan, </author> <year> (1995). </year> <title> On Discretization as a Preprocessing Step for Supervised Learning Models , Masters Thesis, </title> <institution> Brigham Young University, </institution> <year> 1995. </year>
Reference-contexts: These distance metrics work well in many symbolic domains, but they do not handle continuous attributes directly. Instead, they rely upon discretization, which often degrades generalization accuracy <ref> [11] </ref>. This paper presents two extensions of the Value Difference Metric which allow for more appropriate use of continuous attributes. Section 2 provides more background on the original VDM and subsequent extensions to it. <p> This method has the advantage of generating a large enough statistical sample for each symbolic value that the P values have some significance. However, discretization can throw away much of the important information available in the continuous values, and thus can reduce generalization accuracy <ref> [11] </ref>. In this paper, we propose a new alternative, which is to use discretization in order to collect statistics and determine good values of P a,x,c for continuous values occurring in the training set instances, but then retain the continuous values for later use. <p> For the purposes of this paper, we use a simple heuristic to determine s automatically: let s be 5 or C, whichever is greatest, where C is the number of output classes in the problem domain. Current research is examining more sophisticated techniques for determining good values of s <ref> [11] </ref>. The width w a of a discretized interval for attribute a is: w a = s where max a and min a are the maximum and minimum value for attribute a, respectively, occurring in the training set.
Reference: [12] <author> Domingos, Pedro, </author> <year> (1995). </year> <title> Rule Induction and Instance-Based Learning: A Unified Approach, </title> <booktitle> to appear in The 1995 International Joint Conference on Artificial Intelligence (IJCAI-95). </booktitle>
Reference-contexts: Cost & Salzberg [9] used Equation (3) with q=1, which they found to be approximately as accurate and computationally less expensive than q=2 as used by Stanfill & Waltz. They also modified the instance weighting scheme, and implemented their algorithm in a system called PEBLS [10]. Domingos <ref> [12] </ref> presented a system which combines instance-based learning with inductive rules. His system used Equation (3) with q=1 for symbolic attributes and a normalized linear difference for linear attributes. <p> Because of these problems, it is quite inappropriate to use the VDM directly on continuous attributes. One solution to this problem, as explored by Domingos <ref> [12] </ref> and Wilson & Martinez [13], is to use a heterogeneous distance function. A heterogeneous distance function can use a different distance metric for each attribute, based on what kind of attribute it is [14]. Another approach to this problem is discretization [11][15]. <p> We repeat it here for convenience: vdm a (x, y) = P a,x,c Pa,y,c c=1 Unknown input values [17] are treated as simply another discrete value, as was done in <ref> [12] </ref>. As an example, consider two training instances A and B as shown in Figure 3, and a new input vector y to be classified.
Reference: [13] <author> Wilson, D. Randall, and Tony R. Martinez, </author> <year> (1995). </year> <title> Heterogeneous Radial Basis Functions, </title> <booktitle> to appear in Proceedings of the 1996 International Conference on Neural Networks (ICNN96). </booktitle>
Reference-contexts: Domingos [12] presented a system which combines instance-based learning with inductive rules. His system used Equation (3) with q=1 for symbolic attributes and a normalized linear difference for linear attributes. Wilson & Martinez <ref> [13] </ref> introduced a Radial Basis Function (RBF) neural network that used Equation (3) with q=2 for symbolic attributes and normalized Euclidean distance for linear attributes. The attribute distances were weighted in order to bring the distance for each attribute into an approximately equal range. <p> Because of these problems, it is quite inappropriate to use the VDM directly on continuous attributes. One solution to this problem, as explored by Domingos [12] and Wilson & Martinez <ref> [13] </ref>, is to use a heterogeneous distance function. A heterogeneous distance function can use a different distance metric for each attribute, based on what kind of attribute it is [14]. Another approach to this problem is discretization [11][15].
Reference: [14] <author> Giraud-Carrier, Christophe, and Tony Martinez, </author> <year> (1995). </year> <title> An Efficient Metric for Heterogeneous Inductive Learning Applications in the Attribute-Value Language, </title> <booktitle> Intelligent Systems, </booktitle> <pages> pp. 341-350. </pages>
Reference-contexts: One solution to this problem, as explored by Domingos [12] and Wilson & Martinez [13], is to use a heterogeneous distance function. A heterogeneous distance function can use a different distance metric for each attribute, based on what kind of attribute it is <ref> [14] </ref>. Another approach to this problem is discretization [11][15]. Some models that have used the VDM or extensions of it (notably PEBLS [9][10]) have discretized continuous attributes into a somewhat arbitrary number of discrete ranges, and then treated these values as symbolic (discrete unordered) values.
Reference: [15] <author> Lebowitz, Michael, </author> <year> (1985). </year> <title> Categorizing Numeric Information for Generalization, </title> <journal> Cognitive Science, </journal> <volume> vol. 9, </volume> <pages> pp. 285-308. </pages>
Reference: [16] <author> Murphy, P. M., and D. W. Aha, </author> <year> (1993). </year> <title> UCI Repository of Machine Learning Databases . Irvine, </title> <institution> CA: University of California Irvine, Department of Information and Computer Science. Internet: ftp://ftp.ics.uci.edu/pub/machine-learning-databases. </institution>
Reference-contexts: As an example, consider the Iris database, available as part of the Machine Learning Databases at the University of California Irvine (UCI) <ref> [16] </ref>. The Iris database has four continuous input attributes, the first of which is sepal length. Let T be a training set consisting of 90% of the 150 available training instances, and S be a test set consisting of the remaining 10%. <p> p 1,2 (v) p 1,3 (v) ivdm 1 ( v,y) vdm 1 ( v,y) A 5.0 .687 .268 .046 .005 .273 y 5.1 .634 .317 .050 The IVDM and DVDM algorithms were implemented and tested on 21 databases from the Machine Learning Database Repository at the University of California Irvine <ref> [16] </ref>. These data sets were selected because they contain at least some continuous attributes. Recall that on domains with all symbolic attributes, IVDM and DVDM are equivalent.

References-found: 16

