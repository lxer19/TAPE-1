URL: http://pc19.rtna.daimlerbenz.com/~rogers/update.ps
Refering-URL: http://ai.eecs.umich.edu/people/laird/airesearch.html
Root-URL: 
Email: E-mail: srogers@eecs.umich.edu  
Title: Continuous Domains 1  
Author: Seth O. Rogers 
Date: January 8, 1996  
Address: Ann Arbor, MI 48109-2110  
Affiliation: Artificial Intelligence Laboratory The University of Michigan  
Abstract: New Results on Learning from Experience in Abstract Although a basic prototype agent existed at the time of my thesis proposal [5], I needed to make major corrections and refinements to produce a robust agent capable of improving its performance through experience with its environment. To speed development, I switched to a simpler artificial environment from the original simulated flight domain. I made a number of changes and enhancements to the agent affecting virtually all areas of the agent's execution. I designed a strawman agent incapable of generalizing goals for comparison purposes. The results of experiments show that the agent on average outperforms the strawman after it gains enough experience with the environment. Now that the implementation of the agent in a simplified domain is finished, I plan to move to more complex domains and further enhance the agent to deal with the challenges presented by these domains.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Kenneth D. Forbus. </author> <title> Qualitative physics: Past, present, and future. </title> <editor> In Howard Shrobe, editor, </editor> <booktitle> Exploring Artificial Intelligence, </booktitle> <pages> pages 239-296. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: goal windows, we replaced the clock with a simple decision-cycle counter to simplify development. 4 Elimination of Special Treatment of Zero Since the domain model is a qualitative approximation of the domain laws, I designed the qualitative variables to take on the traditional three qualitative values: positive, negative, and zero <ref> [1] </ref>. Therefore, I designed the entire system with the assumption that this would be the default initial quantity space until the agent learned more specific regions. However, I encountered significant implementational problems when using zero as a default region.
Reference: [2] <author> John E. Laird, Allen Newell, and Paul S. Rosenbloom. </author> <title> Soar: An architecture for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33(1) </volume> <pages> 1-64, </pages> <year> 1987. </year>
Reference-contexts: This document will describe each of the major changes, present the results of the experiments, and conclude with an analysis of accomplishments and a program of future work. I will go into some detail on the Soar <ref> [2] </ref> implementation, so readers uninterested in this level of detail can skip to the results and the conclusion. 1 Update on dissertation proposal.
Reference: [3] <author> Craig M. Miller. </author> <title> A model of concept acquisition in the context of a unified theory of cognition. </title> <type> PhD thesis, </type> <institution> The University of Michigan, Dept. of Computer Science and Electrical Engineering, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: I solved this problem by associating a parameter value with a list of regions. Each region would point to the next more general region it belonged to, up to the maximally general positive or negative. The SCA <ref> [3] </ref> action net would abstract one region of one parameter at a time until it matched a stored situation. Although this provided the necessary functionality, the order in which this ab 5 straction occurred became critical.
Reference: [4] <author> Seth O. Rogers. </author> <title> Increasing learning rate via active goal selection. </title> <booktitle> In Working Notes of the 1995 AAAI Fall Symposium on Active Learning, </booktitle> <address> pages ?-?, Cambridge, MA, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: I also performed a number of experiments with the finished agent for inclusion in the Working Notes of the 1995 AAAI Fall Symposium on Active Learning <ref> [4] </ref>. This document will describe each of the major changes, present the results of the experiments, and conclude with an analysis of accomplishments and a program of future work.
Reference: [5] <author> Seth O. Rogers. </author> <title> Learning from experience in continuous domains. </title> <type> Thesis Proposal, </type> <institution> Artificial Intelligence Laboratory, University of Michigan., </institution> <month> Febru-ary </month> <year> 1995. </year> <month> 14 </month>
Reference-contexts: 1 Introduction This document is intended to describe the changes to the theory and implementation of a model for performance and learning in continuous domains as described in my thesis proposal <ref> [5] </ref>. These changes took place over the course of Summer 1995, and the effort to implement a more capable agent than the prototype available at the time of the proposal primarily motivated the changes.
References-found: 5

