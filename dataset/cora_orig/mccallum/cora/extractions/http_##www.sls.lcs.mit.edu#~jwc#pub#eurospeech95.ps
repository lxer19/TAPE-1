URL: http://www.sls.lcs.mit.edu/~jwc/pub/eurospeech95.ps
Refering-URL: http://www.sls.lcs.mit.edu/~jwc/pub/pub.html
Root-URL: 
Title: A STUDY OF SPEECH RECOGNITION SYSTEM ROBUSTNESS TO MICROPHONE VARIATIONS 1  
Author: Jane Chang and Victor Zue 
Address: Cambridge, Massachusetts 02139 USA  
Affiliation: Spoken Language Systems Group Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: This study seeks to improve our understanding of the effects of microphone variations on speech recognition systems. The timit corpus provides data recorded on close talking and far field microphones and over telephone lines. The summit system is configured for phonetic classification and recognition. At the last icslp, we presented an analysis of the data and experiments in phonetic classification using a baseline system and various preprocessing techniques. In this paper, we present experiments in phonetic recognition using an improved baseline system and compensation techniques that require varying amounts of microphone specific data. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Chang, </author> <title> Speech recognition system robustness to microphone variations, </title> <type> Sm Thesis, </type> <institution> Mit, </institution> <year> 1995. </year>
Reference-contexts: As described, the recognition system is consistent with the classification system used in our previous experiments [2]. Original experiments using this system show that recognition errors are larger in number but similar in kind to the classification errors we had studied in detail <ref> [1] </ref>. <p> First, the system uses Cepstral Mean Normalization (cmn). Studies <ref> [1, 7] </ref> show that cmn significantly reduces degradation under mismatched training and testing conditions without degrading the matched condition. Second, the system handles wideband and narrowband speech. Studies [1, 5] show that downsampling significantly reduces degradation under mismatched training and testing conditions that also differ in bandwidth. <p> First, the system uses Cepstral Mean Normalization (cmn). Studies [1, 7] show that cmn significantly reduces degradation under mismatched training and testing conditions without degrading the matched condition. Second, the system handles wideband and narrowband speech. Studies <ref> [1, 5] </ref> show that downsampling significantly reduces degradation under mismatched training and testing conditions that also differ in bandwidth. Our system produces both wideband and nar-rowband models in training and uses the model that matches the bandwidth of the input signal in testing. <p> As the Sennheiser and B&K are relatively similar, testing on the B&K rather than the Sennheiser incurs a relatively small 5% error increase. Furthermore, differences between the microphones at low frequencies can explain many of the additional errors involving voicing, formants and weak events <ref> [1] </ref>. As the Sennheiser and Telephone are relatively different, testing on the Telephone rather than the Sennheiser incurs a relatively large 41% error increase. Other than bandlimiting, the Telephone introduces higher levels of distortion and noise [5], and without high frequency information, recognition is more sensitive to these effects. <p> Preprocessing techniques can compensate for input variations before recognition without the use of microphone specific data. The baseline (base) uses cmn, which subtracts an estimate of the convolutional distortion in the input signal. Among the other preprocessing techniques with which we experimented <ref> [1] </ref>, we present results for log spectral subtraction (sub) [9], which subtracts an estimate of additive noise, and Codebook Dependent Cepstral Normalization (cdcn) [7], which subtracts an estimate of both convolutional and additive effects. <p> The simple technique of cmn is at least as effective as log spectral subtraction and most of the other preprocessing techniques with which we experimented <ref> [1] </ref>. One exception is the significantly more complex technique of cdcn, which reduces the degradation from (s, s) to (s, t) by 28%, achieving a 64.5% error when testing on the Telephone.
Reference: [2] <author> J. Chang and V. Zue, </author> <title> "A study of speech recognition system robustness to microphone variations: experiments in phonetic classification", </title> <booktitle> Proc. Icslp, </booktitle> <pages> 995-998, </pages> <year> 1994. </year>
Reference-contexts: The models consist of context independent diagonal Gaussian mixtures and a bigram language model. Performance is evaluated on 56 classes, with all closures collapsed into one class. As described, the recognition system is consistent with the classification system used in our previous experiments <ref> [2] </ref>. Original experiments using this system show that recognition errors are larger in number but similar in kind to the classification errors we had studied in detail [1]. <p> Regardless of the microphone, most of the additional errors under mismatched conditions are deletions, especially of weak events such as stop closures. This suggests a lack of robustness in the segmenta-tion, classification and search components. In our previous study of classification <ref> [2] </ref>, we forced the system to use the segment boundaries derived from the time aligned phonetic transcription and bypassed the search component.
Reference: [3] <author> W. Fisher, G. Doddington and K. Goudie-Marshall, </author> <title> "The darpa speech recognition research database: specifications and status", </title> <booktitle> Proc. Darpa Speech Recognition Workshop, </booktitle> <pages> 93-99, </pages> <year> 1986. </year>
Reference-contexts: The following sections discuss the data, baseline system and compensation techniques used in this study. DATA The timit <ref> [3] </ref> acoustic phonetic corpus provides continuous speech data with time aligned phonetic transcriptions. Timit is particularly useful for studies of microphone variations because it provides three different recordings of the same data. The original release of timit was recorded using a Sennheiser HMD-414. <p> As a result, the B&K is more sensitive to non-oral resonances emitted from the nose and throat, as well as any environmental noise present in the recording booth <ref> [3] </ref>. The Telephone is characterized by the combination of the Sennheiser with a telephone handset and channel that introduce noise and bandlimiting effects [4]. Figure 1 shows general spectral characteristics of the data.
Reference: [4] <author> C. Jankowski, A. Kalyanswamy, S. Basson and J. Spitz, "N-timit: </author> <title> a phonetically balanced, continuous speech, telephone bandwidth speech database", </title> <booktitle> Proc.Icassp, </booktitle> <pages> 109-112, </pages> <year> 1990. </year>
Reference-contexts: Timit is particularly useful for studies of microphone variations because it provides three different recordings of the same data. The original release of timit was recorded using a Sennheiser HMD-414. Subsequently, the Sennheiser data was recorded over telephone lines and released as ntimit <ref> [4] </ref>. The third set of data, less known to the research community, was recorded in stereo with the Sennheiser using a Bruel and Kjaer (B&K) 4165. With help from nist, we recovered 97% of the B&K utterances. For consistency, we only use utterances that are common to all three microphones. <p> The Telephone is characterized by the combination of the Sennheiser with a telephone handset and channel that introduce noise and bandlimiting effects <ref> [4] </ref>. Figure 1 shows general spectral characteristics of the data.
Reference: [5] <author> P. Moreno and R. Stern, </author> <title> "Sources of degradation of speech recognition in the telephone network", </title> <booktitle> Proc. Icassp, </booktitle> <address> I:109-112, </address> <year> 1994. </year>
Reference-contexts: First, the system uses Cepstral Mean Normalization (cmn). Studies [1, 7] show that cmn significantly reduces degradation under mismatched training and testing conditions without degrading the matched condition. Second, the system handles wideband and narrowband speech. Studies <ref> [1, 5] </ref> show that downsampling significantly reduces degradation under mismatched training and testing conditions that also differ in bandwidth. Our system produces both wideband and nar-rowband models in training and uses the model that matches the bandwidth of the input signal in testing. <p> As the Sennheiser and Telephone are relatively different, testing on the Telephone rather than the Sennheiser incurs a relatively large 41% error increase. Other than bandlimiting, the Telephone introduces higher levels of distortion and noise <ref> [5] </ref>, and without high frequency information, recognition is more sensitive to these effects. Regardless of the microphone, most of the additional errors under mismatched conditions are deletions, especially of weak events such as stop closures. This suggests a lack of robustness in the segmenta-tion, classification and search components.
Reference: [6] <author> R. Lippman, E. Martin and D. Paul, </author> <title> "Multi-style training for robust isolated-word speech recognition", </title> <booktitle> Proc. Icassp, </booktitle> <pages> 705-708, </pages> <year> 1987. </year>
Reference-contexts: Training techniques use a relatively large amount of data to train microphone specific models. For consistency, we present results using the same training parameters for all techniques. For example, we maintain the same total amount of training data and maximum number of mixtures per model. Multi-style training (multi) <ref> [6] </ref> pools one third of each microphone training set into a single model. Microphone selection (select) uses each microphone training set to produce a separate model and selects the highest scoring model for each test utterance. in percent.
Reference: [7] <author> F. Liu, R. Stern, A. Acero and P. Moreno, </author> <title> "Environment normalization for robust speech recognition using direct cepstral comparisons", </title> <booktitle> Proc. Icassp, </booktitle> <address> II:61-64, </address> <year> 1994. </year>
Reference-contexts: First, the system uses Cepstral Mean Normalization (cmn). Studies <ref> [1, 7] </ref> show that cmn significantly reduces degradation under mismatched training and testing conditions without degrading the matched condition. Second, the system handles wideband and narrowband speech. Studies [1, 5] show that downsampling significantly reduces degradation under mismatched training and testing conditions that also differ in bandwidth. <p> The baseline (base) uses cmn, which subtracts an estimate of the convolutional distortion in the input signal. Among the other preprocessing techniques with which we experimented [1], we present results for log spectral subtraction (sub) [9], which subtracts an estimate of additive noise, and Codebook Dependent Cepstral Normalization (cdcn) <ref> [7] </ref>, which subtracts an estimate of both convolutional and additive effects. Other techniques use a relatively small amount of microphone specific data to compensate for input variations during recognition. As suggested, search parameters that control the tradeoff between deletions and insertions can compensate for large numbers of deletions.
Reference: [8] <author> M. Phillips and J. Glass, </author> <title> "Phonetic transition modeling for continuous speech recognition", </title> <journal> J. Acoust. Soc. Am., </journal> <volume> 95:2877, </volume> <year> 1994. </year>
Reference-contexts: In this paper, we present experiments using an improved system 2 The lowest phonetic recognition error rate achieved by summit using context dependent models is approximately 30% <ref> [8] </ref>. each testing microphone on the x-axis, the line indicates the matched error when training and testing on that microphone, while the bar indicates the mismatched error when training on the Sennheiser and testing on that microphone. that incorporates two known findings. First, the system uses Cepstral Mean Normalization (cmn).
Reference: [9] <author> D. Van Compernolle, </author> <title> "Increased noise immunity in large vocabulary speech recognition with the aid of spectral subtraction", </title> <booktitle> Proc. Icassp, </booktitle> <pages> 1143-1146, </pages> <year> 1987. </year>
Reference-contexts: The baseline (base) uses cmn, which subtracts an estimate of the convolutional distortion in the input signal. Among the other preprocessing techniques with which we experimented [1], we present results for log spectral subtraction (sub) <ref> [9] </ref>, which subtracts an estimate of additive noise, and Codebook Dependent Cepstral Normalization (cdcn) [7], which subtracts an estimate of both convolutional and additive effects. Other techniques use a relatively small amount of microphone specific data to compensate for input variations during recognition.
Reference: [10] <author> V. Zue, J. Glass, D. Goodine, H. Leung, M. Phillips, J. Polifroni and S. Senneff, </author> <title> "Recent progress on the summit system", </title> <booktitle> Proc. Darpa Speech and Natural Language Workshop, </booktitle> <pages> 380-385, </pages> <year> 1990. </year>
Reference-contexts: The Telephone shows the effects of normalization and noise at lower frequencies and bandlimiting at higher frequencies. BASELINE SYSTEM The summit <ref> [10] </ref> system is a segment based speech recognition system that explicitly detects phonetic segment boundaries in order to extract features in relation to specific acoustic events. For our comparative experiments, we are interested in relative rather than absolute performance. <p> Therefore, in configuring summit for phonetic recognition, we use rather simple components to maintain consistency, facilitate training and otherwise reduce confounding effects, at the expense of achieving optimal performance 2 . The system uses a Mel Frequency Cepstral Coefficient (mfcc) representation. The segmentation algorithm <ref> [10] </ref> is based solely on spectral change with no probabilistic modeling. The features extracted for each segment consist of 3 averages over each third of the segment, 2 derivatives with neighboring segments, and duration. The models consist of context independent diagonal Gaussian mixtures and a bigram language model.
References-found: 10

