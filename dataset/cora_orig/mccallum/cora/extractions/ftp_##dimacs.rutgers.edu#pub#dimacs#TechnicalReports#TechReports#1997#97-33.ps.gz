URL: ftp://dimacs.rutgers.edu/pub/dimacs/TechnicalReports/TechReports/1997/97-33.ps.gz
Refering-URL: http://dimacs.rutgers.edu/TechnicalReports/1997.html
Root-URL: http://www.cs.rutgers.edu
Title: On Local Register Allocation  
Author: by Martin Farach ; Vincenzo Liberatore ; 
Address: New Brunswick, NJ 08903  700 Mountain Avenue, Murray Hill, NJ 07974.  New Brunswick, NJ 08903.  
Affiliation: Rutgers University and Bell Labs  Rutgers University  Department of Computer Science, Hill Center, Rutgers University,  and Information Sciences Research Center, Bell Labs,  Department of Computer Science, Hill Center, Rutgers University,  
Note: July  Research supported by NSF Career Development Award CCR-9501942, NATO Grant CRG 960215, NSF/NIH Grant BIR 94-12594-03-CONF and an Alfred P. Sloan Research Fellowship. 3 Affiliated Graduate Student  Research supported by NSF Career Development Award CCR-9501942, NATO Grant CRG 960215, NSF/NIH Grant BIR 94-12594-03-CONF and an Alfred P. Sloan Research Fellowship. DIMACS is a partnership of Rutgers University, Princeton University, AT&T Labs, Bellcore, and Bell Labs. DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives support from the New Jersey Commission on Science and Technology.  
Email: E-mail: farach@cs.rutgers.edu.  E-mail: liberato@cs.rutgers.edu.  
Phone: 2  4  
Web: URL: http://www.cs.rutgers.edu/~farach/.  Memeber  URL: http://www.cs.rutgers.edu/~liberato/.  
Date: 1997  
Abstract: DIMACS Technical Report 97-33 
Abstract-found: 1
Intro-found: 1
Reference: [AMO93] <author> Ravindra K. Ahuja, Thomas L. Magnanti, and James B. Orlin. </author> <title> Network Flows. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliff, NJ, </address> <year> 1993. </year>
Reference-contexts: Since ff = 1, the register pressure never increases by more than one. It follows that the maximum supply is one. Consequently, weighted off-line weighted caching can be solved in O (r 2 log log r) time with the successive shortest path algorithm <ref> [AMO93, Tho96] </ref>. No explicit algorithm had been previously given. However, the construction in [You94] relies on algorithms for the assignment problem and implicitly yields algorithms that run in O (r 2 p r log (rC)) time and in strongly polynomial time in O (r 3 ) [AMO93]. <p> No explicit algorithm had been previously given. However, the construction in [You94] relies on algorithms for the assignment problem and implicitly yields algorithms that run in O (r 2 p r log (rC)) time and in strongly polynomial time in O (r 3 ) <ref> [AMO93] </ref>. The 2-Approximation Algorithm We will now extend the algorithm for weighted off-line caching to a polynomial time 2-approximation algorithm for the general local register allocation problem. <p> Consequently, the total supply is now bounded by s. The running time of the algorithm is e O (s 2 ) <ref> [AMO93] </ref>. 4.2 Fast Approximation In the case of off-line paging, if we consider both compulsory and capacity costs, then furthest-first is a 2-approximation algorithm: for any load, we can at most charge an extra store.
Reference: [ASU86] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction Register allocation is a crucial task of optimizing compilers. It adds the largest single performance improvement to compiled programs [HP96]. Register allocation has received widespread attention in academic and industrial research in the past few decades <ref> [ASU86, FL88, TS85] </ref>. Register allocation is the problem of deciding which variables occupy the register file at each step of a program. The objective of the register allocation phase is to minimize the total memory traffic between the CPU and the memory system. <p> The main difficulty of local register allocation stems from the trade-off between the cost of loads and the cost of stores. In the compiler context, local register allocation should take priority over global register allocation <ref> [ASU86, PF96] </ref>; for example, inner loops should have priority over outer loops [ASU86]. Once a local allocation has been determined, it can be extended to a global register allocation [Bea74, CK91, HGAM93, Ken72, K + 86, Mor91, PF96]. <p> The main difficulty of local register allocation stems from the trade-off between the cost of loads and the cost of stores. In the compiler context, local register allocation should take priority over global register allocation [ASU86, PF96]; for example, inner loops should have priority over outer loops <ref> [ASU86] </ref>. Once a local allocation has been determined, it can be extended to a global register allocation [Bea74, CK91, HGAM93, Ken72, K + 86, Mor91, PF96]. The objective of the local register allocation phase is to minimize the total traffic between CPU and memory. <p> However, the algorithm fails to terminate in more recent implementations due to excessive memory space demands [HFG89]. Some heuristics have been proposed to produce quickly a local register allocation <ref> [ASU86, FL88, HFG89, TS85] </ref>. For example, the furthest-first algorithm evicts the program variable that will be referenced furthest in the future [Bel66, FH92, HGAM93, Nak67]. <p> For example, the furthest-first algorithm evicts the program variable that will be referenced furthest in the future [Bel66, FH92, HGAM93, Nak67]. Related heuristics have been proposed in the context of spilling , for whose definition and algorithms we refer the reader to the textbooks <ref> [ASU86, FL88, TS85] </ref>. Previous to this paper, no optimum algorithm was known to take less than exponential time and space in the worst case, nor to work "reasonably" well on actual benchmarks. On the other hand, no lower bound was known. <p> A program variable is said to be alive on exit if its value has to be maintained across basic block boundaries. The set of variables alive on exit is denoted by L. The variables alive on exit are determined by a live range analysis procedure <ref> [ASU86, FL88, TS85] </ref>. We will denote with 0 (i) the first step where program variable i is referenced: 0 (i) = minfj : 9s 2 T:(i; s) 2 j g.
Reference: [BBB + 57] <author> J. W. Backus, R. J. Beeber, S. Best, R. Goldberg, L. M. Haibt, H. L. Herrick, R. A. Nelson, D. Sayre, P. B. Sheridan, H. Stern, I. Ziller, R. A. Hughes, and R. Nutt. </author> <title> The Fortran automatic coding system. </title> <booktitle> In Western Joint Computer Conference, </booktitle> <pages> pages 188-198, </pages> <year> 1957. </year>
Reference-contexts: The problem of local register allocation is left as an open question in the "Dragon Book" ([ASU86], section 9.6, function getreg, step 3) and in other standard textbooks on compilers [FL88, TS85]. Local register allocation has been performed since the first FORTRAN compiler <ref> [BBB + 57] </ref>, and formally studied since the mid-sixties [HKMW66]. Currently, the best algorithm for local register allocation takes exponential time and space in the worst case, but no lower bound is known. New Results In this paper, we present the first NP-hardness proof for the local register allocation problem.
Reference: [BCT94] <author> Preston Briggs, Keith D. Cooper, and Linda Torczon. </author> <title> Improvements to graph coloring register allocation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(3) </volume> <pages> 428-455, </pages> <month> May </month> <year> 1994. </year> <title> 2 The proof is found in the appendix. 3 Detailed experimental results are reported in the appendix. </title> <type> 9 </type>
Reference-contexts: The second difference is the following. In the paging problem, only one page reference is generated in one step. In local register allocation, multiple references can be generated in one step. For example, consider the ILOC <ref> [BCT94] </ref> instruction iADD t0 t1 =&gt; t2 which adds the variables t0 and t1 and stores the results in t2. The variables t0 and t1 have to be read at the same time and fed simultaneously to the ALU unit. Multiple simultaneous references arise also in VLIW architectures [Fis83, HP96]. <p> Then, t0 can be loaded in a register by a load immediate operation, which does not involve a memory access. Loading t0 is therefore cheaper than fetching a variable from memory. A load immediate is considered to cost half of a memory access by some authors <ref> [BCT94] </ref>. 2.2 Local Register Allocation We now formulate the problem of local register allocation. Let V = f1; 2; : : :; M g be a set of program variables. <p> Assume without loss of generality that the costs are normalized so that minfS i : 1 i M g = 1, and define C = maxfS i : 1 i M g. In write-back paging, C = 1. In compiler applications, C ' 2 typically <ref> [BCT94] </ref>.
Reference: [Bea74] <author> J. C. Beatty. </author> <title> Register assignment algorithm for generation of highly optimized code. </title> <journal> IBM J. Res. Develop., </journal> <volume> 18 </volume> <pages> 20-39, </pages> <month> January </month> <year> 1974. </year>
Reference-contexts: In the compiler context, local register allocation should take priority over global register allocation [ASU86, PF96]; for example, inner loops should have priority over outer loops [ASU86]. Once a local allocation has been determined, it can be extended to a global register allocation <ref> [Bea74, CK91, HGAM93, Ken72, K + 86, Mor91, PF96] </ref>. The objective of the local register allocation phase is to minimize the total traffic between CPU and memory.
Reference: [Bel66] <author> L. A. Belady. </author> <title> A study of replacement algorithms for a virtual storage computer. </title> <journal> IBM Systems Journal, </journal> <volume> 5(2) </volume> <pages> 78-101, </pages> <year> 1966. </year>
Reference-contexts: Register allocation is tremendously effective because of the huge speed gap between processors and memories. In addition to the attention register allocation in its own right, it has both Off-Line Paging with Write-Backs <ref> [Bel66, Car84] </ref> and Off-Line Weighted Caching [MMS90, You94] as particular cases. Local Register Allocation In this paper, we will focus on local register allocation and we will provide strong negative and positive results. Local register allocation assigns registers to variables in basic blocks, which are maximal branch-free sequences of instructions. <p> Some heuristics have been proposed to produce quickly a local register allocation [ASU86, FL88, HFG89, TS85]. For example, the furthest-first algorithm evicts the program variable that will be referenced furthest in the future <ref> [Bel66, FH92, HGAM93, Nak67] </ref>. Related heuristics have been proposed in the context of spilling , for whose definition and algorithms we refer the reader to the textbooks [ASU86, FL88, TS85]. <p> Write-back paging is much harder than paging without write-backs. Indeed, the latter can be optimally solved by the furthest-first rule: in response to a page fault, evict the page that will be referenced furthest in the future <ref> [Bel66, HKMW66] </ref> (see also [MS91] for a shorter proof). Consequently, the minimum number of page faults can be found in O (r log log N ) by an implementation of furthest-first with Thorup's priority queues [Tho96]. By contrast, write-back paging is proved to be NP-hard.
Reference: [Car84] <author> Richard William Carr. </author> <title> Virtual Memory Management, </title> <booktitle> volume 20 of Computer Science: Systems Programming. </booktitle> <publisher> UMI Research Press, </publisher> <address> Ann Arbor, MI, </address> <year> 1984. </year>
Reference-contexts: Register allocation is tremendously effective because of the huge speed gap between processors and memories. In addition to the attention register allocation in its own right, it has both Off-Line Paging with Write-Backs <ref> [Bel66, Car84] </ref> and Off-Line Weighted Caching [MMS90, You94] as particular cases. Local Register Allocation In this paper, we will focus on local register allocation and we will provide strong negative and positive results. Local register allocation assigns registers to variables in basic blocks, which are maximal branch-free sequences of instructions. <p> A paging strategy aims at minimizing the total number of page faults and write-backs. Paging with write-backs is a more realistic problem than traditional paging with only page faults <ref> [Car84] </ref> because it captures the total traffic between fast and slow memory. Write-back paging is similar to the problem of local register allocation.
Reference: [CK91] <author> David Callahan and Brian Koblenz. </author> <title> Register allocation via hierarchical graph coloring. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Languages Design and Implementation, </booktitle> <pages> pages 192-203, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: In the compiler context, local register allocation should take priority over global register allocation [ASU86, PF96]; for example, inner loops should have priority over outer loops [ASU86]. Once a local allocation has been determined, it can be extended to a global register allocation <ref> [Bea74, CK91, HGAM93, Ken72, K + 86, Mor91, PF96] </ref>. The objective of the local register allocation phase is to minimize the total traffic between CPU and memory.
Reference: [FH92] <author> Christopher W. Fraser and David R. Hanson. </author> <title> Simple register spilling in a retargetable compiler. </title> <journal> Software | Practice and Experience, </journal> <volume> 22(1) </volume> <pages> 85-99, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: We also provide a 2-approximation algorithm W for the local register allocation problem. Our algorithm is the first polynomial-time approximation algorithm for this problem with a performance guarantee. We consider the furthest-first algorithm algorithm that is used in existing compilers <ref> [FH92, HGAM93] </ref>. We propose a simple modification of the furthest-first algorithm which we call conservative-furthest-first (CFF). We analyze the behavior of CFF and prove a performance guarantee. Our analysis is tight. We study weighted off-line caching, which was known to be polynomially solvable [You94]. <p> Some heuristics have been proposed to produce quickly a local register allocation [ASU86, FL88, HFG89, TS85]. For example, the furthest-first algorithm evicts the program variable that will be referenced furthest in the future <ref> [Bel66, FH92, HGAM93, Nak67] </ref>. Related heuristics have been proposed in the context of spilling , for whose definition and algorithms we refer the reader to the textbooks [ASU86, FL88, TS85].
Reference: [Fis83] <author> J. A. Fisher. </author> <title> Very long instruction word architectures and ELI-512. </title> <booktitle> In Proc. Tenth Symposium on Computer Architecture, </booktitle> <pages> pages 140-150, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: The variables t0 and t1 have to be read at the same time and fed simultaneously to the ALU unit. Multiple simultaneous references arise also in VLIW architectures <ref> [Fis83, HP96] </ref>. Finally, in the paging problem, a fault always accounts for one unit of memory traffic. In local register allocation, different variables are fetched at different costs. Indeed, many variables are loaded from memory, but others are not.
Reference: [FL88] <author> Charles N. Fischer and Richard J. LeBlanc, Jr. </author> <title> Crafting a Compiler. </title> <address> Benjamin/Cummings, Menlo Park, CA, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction Register allocation is a crucial task of optimizing compilers. It adds the largest single performance improvement to compiled programs [HP96]. Register allocation has received widespread attention in academic and industrial research in the past few decades <ref> [ASU86, FL88, TS85] </ref>. Register allocation is the problem of deciding which variables occupy the register file at each step of a program. The objective of the register allocation phase is to minimize the total memory traffic between the CPU and the memory system. <p> The advantages of local register allocation over graph coloring are summarized for example in [HFG89]. The problem of local register allocation is left as an open question in the "Dragon Book" ([ASU86], section 9.6, function getreg, step 3) and in other standard textbooks on compilers <ref> [FL88, TS85] </ref>. Local register allocation has been performed since the first FORTRAN compiler [BBB + 57], and formally studied since the mid-sixties [HKMW66]. Currently, the best algorithm for local register allocation takes exponential time and space in the worst case, but no lower bound is known. <p> However, the algorithm fails to terminate in more recent implementations due to excessive memory space demands [HFG89]. Some heuristics have been proposed to produce quickly a local register allocation <ref> [ASU86, FL88, HFG89, TS85] </ref>. For example, the furthest-first algorithm evicts the program variable that will be referenced furthest in the future [Bel66, FH92, HGAM93, Nak67]. <p> For example, the furthest-first algorithm evicts the program variable that will be referenced furthest in the future [Bel66, FH92, HGAM93, Nak67]. Related heuristics have been proposed in the context of spilling , for whose definition and algorithms we refer the reader to the textbooks <ref> [ASU86, FL88, TS85] </ref>. Previous to this paper, no optimum algorithm was known to take less than exponential time and space in the worst case, nor to work "reasonably" well on actual benchmarks. On the other hand, no lower bound was known. <p> A program variable is said to be alive on exit if its value has to be maintained across basic block boundaries. The set of variables alive on exit is denoted by L. The variables alive on exit are determined by a live range analysis procedure <ref> [ASU86, FL88, TS85] </ref>. We will denote with 0 (i) the first step where program variable i is referenced: 0 (i) = minfj : 9s 2 T:(i; s) 2 j g.
Reference: [FR97] <author> Amos Fiat and Ziv Rosen. </author> <title> Experimental studies of access graph based heuristics: </title> <booktitle> Beating the LRU standard? In Proceedings of the Eight Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 63-73, </pages> <year> 1997. </year>
Reference-contexts: In the on-line version, future page references are unknown. In the off-line version, the whole sequence of page references is known in advance. Off-line paging algorithms serve as a theoretical [IKP96, MS91, ST85] and experimental <ref> [FR97, Tan92] </ref> point of comparison for on-line algorithms, and stimulate researchers to find on-line 2 approximations [SP83]. Local register allocation is inherently an off-line problem because the whole sequence of references is completely found in the compiled code. The second difference is the following.
Reference: [HFG89] <author> Wei-Chung Hsu, Charles N. Fischer, and James R. Goodman. </author> <title> On the minimization of load/stores in local register allocation. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(10) </volume> <pages> 1252-1260, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Consequently, it is more accurate and effective than global heuristics based on graph coloring, in which the goal is to minimize the number of registers needed to satisfy a sequence of requests. The advantages of local register allocation over graph coloring are summarized for example in <ref> [HFG89] </ref>. The problem of local register allocation is left as an open question in the "Dragon Book" ([ASU86], section 9.6, function getreg, step 3) and in other standard textbooks on compilers [FL88, TS85]. <p> The branch-and-bound procedure takes polynomial space and never ran for more than nine minutes even on long basic blocks and with 256 registers. The previous optimum algorithm failed to terminate on much shorter basic blocks and with only 4 registers due to insufficient memory space <ref> [HFG89] </ref>. So, the branch-and-bound procedure is an optimum algorithm that is more general and practical than the previous dynamic programming algorithm [HKMW66, HFG89]. Previous Work To the best of our knowledge, local register allocation was first considered formally in a 1966 paper by Horwitz, Karp, Miller, and Winograd [HKMW66]. <p> The previous optimum algorithm failed to terminate on much shorter basic blocks and with only 4 registers due to insufficient memory space [HFG89]. So, the branch-and-bound procedure is an optimum algorithm that is more general and practical than the previous dynamic programming algorithm <ref> [HKMW66, HFG89] </ref>. Previous Work To the best of our knowledge, local register allocation was first considered formally in a 1966 paper by Horwitz, Karp, Miller, and Winograd [HKMW66]. In that paper, an algorithm was presented to produce an optimal allocation through dynamic programming. <p> The algorithm accurately captures the index register architecture used at the time. Unfortunately, after more than thirty years, the index register model does not reflect the costs of a modern architecture with general purpose registers. Nonetheless, the algorithm was implemented as recently as 1989 <ref> [HFG89] </ref>, followed by heuristics to fix the points were the allocation was infeasible for new architectures. As such, the dynamic programming algorithm no longer guarantees an optimal solution on modern architectures. <p> The space and time requirements were not a problem in early applications, as only 2 registers and short basic block were considered [Ken72]. However, the algorithm fails to terminate in more recent implementations due to excessive memory space demands <ref> [HFG89] </ref>. Some heuristics have been proposed to produce quickly a local register allocation [ASU86, FL88, HFG89, TS85]. For example, the furthest-first algorithm evicts the program variable that will be referenced furthest in the future [Bel66, FH92, HGAM93, Nak67]. <p> However, the algorithm fails to terminate in more recent implementations due to excessive memory space demands [HFG89]. Some heuristics have been proposed to produce quickly a local register allocation <ref> [ASU86, FL88, HFG89, TS85] </ref>. For example, the furthest-first algorithm evicts the program variable that will be referenced furthest in the future [Bel66, FH92, HGAM93, Nak67]. <p> Moreover, we feel that the formulation is of independent practical interest. Indeed, we used the formulation in a branch-and-bound algorithm to solve exactly the local register allocation problem. The resulting procedure is very fast on real programs and, in contrast with the dynamic programming algorithm <ref> [HFG89] </ref>, it takes a polynomial amount of space 1 . As such, the formulation and the related branch-and-bound algorithm constitutes a substantial practical improvement over previous implementations of the dynamic programming algorithm.
Reference: [HGAM93] <author> Laurie J. Hendren, Guang R. Gao, Erik R. Altman, and Chandrika Mukerji. </author> <title> A register allocation framework based on hierarchical cyclic interval graphs. </title> <type> ACAPS Technical Memo 33, </type> <institution> McGill University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: In the compiler context, local register allocation should take priority over global register allocation [ASU86, PF96]; for example, inner loops should have priority over outer loops [ASU86]. Once a local allocation has been determined, it can be extended to a global register allocation <ref> [Bea74, CK91, HGAM93, Ken72, K + 86, Mor91, PF96] </ref>. The objective of the local register allocation phase is to minimize the total traffic between CPU and memory. <p> We also provide a 2-approximation algorithm W for the local register allocation problem. Our algorithm is the first polynomial-time approximation algorithm for this problem with a performance guarantee. We consider the furthest-first algorithm algorithm that is used in existing compilers <ref> [FH92, HGAM93] </ref>. We propose a simple modification of the furthest-first algorithm which we call conservative-furthest-first (CFF). We analyze the behavior of CFF and prove a performance guarantee. Our analysis is tight. We study weighted off-line caching, which was known to be polynomially solvable [You94]. <p> Some heuristics have been proposed to produce quickly a local register allocation [ASU86, FL88, HFG89, TS85]. For example, the furthest-first algorithm evicts the program variable that will be referenced furthest in the future <ref> [Bel66, FH92, HGAM93, Nak67] </ref>. Related heuristics have been proposed in the context of spilling , for whose definition and algorithms we refer the reader to the textbooks [ASU86, FL88, TS85].
Reference: [HKMW66] <author> L. P. Horwitz, R. M. Karp, R. E. Miller, and S. Winograd. </author> <title> Index register allocation. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 13(1) </volume> <pages> 43-61, </pages> <month> January </month> <year> 1966. </year>
Reference-contexts: Local register allocation has been performed since the first FORTRAN compiler [BBB + 57], and formally studied since the mid-sixties <ref> [HKMW66] </ref>. Currently, the best algorithm for local register allocation takes exponential time and space in the worst case, but no lower bound is known. New Results In this paper, we present the first NP-hardness proof for the local register allocation problem. <p> The previous optimum algorithm failed to terminate on much shorter basic blocks and with only 4 registers due to insufficient memory space [HFG89]. So, the branch-and-bound procedure is an optimum algorithm that is more general and practical than the previous dynamic programming algorithm <ref> [HKMW66, HFG89] </ref>. Previous Work To the best of our knowledge, local register allocation was first considered formally in a 1966 paper by Horwitz, Karp, Miller, and Winograd [HKMW66]. In that paper, an algorithm was presented to produce an optimal allocation through dynamic programming. <p> So, the branch-and-bound procedure is an optimum algorithm that is more general and practical than the previous dynamic programming algorithm [HKMW66, HFG89]. Previous Work To the best of our knowledge, local register allocation was first considered formally in a 1966 paper by Horwitz, Karp, Miller, and Winograd <ref> [HKMW66] </ref>. In that paper, an algorithm was presented to produce an optimal allocation through dynamic programming. The algorithm accurately captures the index register architecture used at the time. <p> Write-back paging is much harder than paging without write-backs. Indeed, the latter can be optimally solved by the furthest-first rule: in response to a page fault, evict the page that will be referenced furthest in the future <ref> [Bel66, HKMW66] </ref> (see also [MS91] for a shorter proof). Consequently, the minimum number of page faults can be found in O (r log log N ) by an implementation of furthest-first with Thorup's priority queues [Tho96]. By contrast, write-back paging is proved to be NP-hard.
Reference: [HP96] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <note> second edition, </note> <year> 1996. </year>
Reference-contexts: 1 Introduction Register allocation is a crucial task of optimizing compilers. It adds the largest single performance improvement to compiled programs <ref> [HP96] </ref>. Register allocation has received widespread attention in academic and industrial research in the past few decades [ASU86, FL88, TS85]. Register allocation is the problem of deciding which variables occupy the register file at each step of a program. <p> The variables t0 and t1 have to be read at the same time and fed simultaneously to the ALU unit. Multiple simultaneous references arise also in VLIW architectures <ref> [Fis83, HP96] </ref>. Finally, in the paging problem, a fault always accounts for one unit of memory traffic. In local register allocation, different variables are fetched at different costs. Indeed, many variables are loaded from memory, but others are not. <p> Then, i has to be loaded at least once by any allocation strategy. The first load of a program variable has been variously called a compulsory miss, a cold start miss, or a first reference miss <ref> [HP96] </ref>. Compulsory misses cause a fixed cost that any allocation has to pay. Therefore, compulsory misses can be disregarded while seeking an optimum allocation. Moreover, if compulsory misses are disregarded, the cost of an allocation does not depend on the initial register configuration Q 0 . <p> Moreover, if compulsory misses are disregarded, the cost of an allocation does not depend on the initial register configuration Q 0 . Henceforth, compulsory misses will be ignored, and we will consider only non-compulsory or, in the terminology of <ref> [HP96] </ref>, capacity misses. Assume without loss of generality that the costs are normalized so that minfS i : 1 i M g = 1, and define C = maxfS i : 1 i M g. In write-back paging, C = 1. In compiler applications, C ' 2 typically [BCT94].
Reference: [IKP96] <author> Sandy Irani, Anna R. Karlin, and Steven Phillips. </author> <title> Strongly competitive algorithms for paging with locality of reference. </title> <journal> SIAM J. Comput., </journal> <volume> 25(3) </volume> <pages> 477-497, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: First, write-back paging has been studied both as an on-line and as an off-line problem. In the on-line version, future page references are unknown. In the off-line version, the whole sequence of page references is known in advance. Off-line paging algorithms serve as a theoretical <ref> [IKP96, MS91, ST85] </ref> and experimental [FR97, Tan92] point of comparison for on-line algorithms, and stimulate researchers to find on-line 2 approximations [SP83]. Local register allocation is inherently an off-line problem because the whole sequence of references is completely found in the compiled code. The second difference is the following.
Reference: [K + 86] <author> D. Kranz et al. </author> <title> ORBIT: An optimizing compiler for scheme. </title> <booktitle> In Proceedings of the ACM SIGPLAN '86 Conference on Programming Languages Design and Implementation, </booktitle> <year> 1986. </year>
Reference-contexts: In the compiler context, local register allocation should take priority over global register allocation [ASU86, PF96]; for example, inner loops should have priority over outer loops [ASU86]. Once a local allocation has been determined, it can be extended to a global register allocation <ref> [Bea74, CK91, HGAM93, Ken72, K + 86, Mor91, PF96] </ref>. The objective of the local register allocation phase is to minimize the total traffic between CPU and memory.
Reference: [Ken72] <author> Ken Kennedy. </author> <title> Index register allocation in straight line code and simple loops. </title> <editor> In Randall Rustin, editor, </editor> <booktitle> Design and Optimization of Compilers, </booktitle> <pages> pages 51-63. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1972. </year>
Reference-contexts: In the compiler context, local register allocation should take priority over global register allocation [ASU86, PF96]; for example, inner loops should have priority over outer loops [ASU86]. Once a local allocation has been determined, it can be extended to a global register allocation <ref> [Bea74, CK91, HGAM93, Ken72, K + 86, Mor91, PF96] </ref>. The objective of the local register allocation phase is to minimize the total traffic between CPU and memory. <p> The space and time requirements were not a problem in early applications, as only 2 registers and short basic block were considered <ref> [Ken72] </ref>. However, the algorithm fails to terminate in more recent implementations due to excessive memory space demands [HFG89]. Some heuristics have been proposed to produce quickly a local register allocation [ASU86, FL88, HFG89, TS85].
Reference: [MMS90] <author> M. S. Manasse, L. A. McGeoch, and D. D. Sleator. </author> <title> Competitive algorithms for server problems. </title> <journal> Journal of Algorithms, </journal> <volume> 11 </volume> <pages> 208-230, </pages> <year> 1990. </year>
Reference-contexts: Register allocation is tremendously effective because of the huge speed gap between processors and memories. In addition to the attention register allocation in its own right, it has both Off-Line Paging with Write-Backs [Bel66, Car84] and Off-Line Weighted Caching <ref> [MMS90, You94] </ref> as particular cases. Local Register Allocation In this paper, we will focus on local register allocation and we will provide strong negative and positive results. Local register allocation assigns registers to variables in basic blocks, which are maximal branch-free sequences of instructions. <p> Weighted caching is similar to paging without write-backs except that the value of S i is not necessarily equal for all pages. The on-line version of this problem was introduced in <ref> [MMS90] </ref>. Intuitively, weighted off-line caching is much simpler than write-back paging because the costs do not depend on the page status. The subnetwork N 1 in figure 1 gives an example of minimum cost network flow problems arising from weighted off-line caching.
Reference: [Mor91] <author> W. G. Morris. CCG: </author> <title> A prototype coagulating code generator. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Languages Design and Implementation, </booktitle> <pages> pages 45-58, </pages> <month> June </month> <year> 1991. </year> <month> 10 </month>
Reference-contexts: In the compiler context, local register allocation should take priority over global register allocation [ASU86, PF96]; for example, inner loops should have priority over outer loops [ASU86]. Once a local allocation has been determined, it can be extended to a global register allocation <ref> [Bea74, CK91, HGAM93, Ken72, K + 86, Mor91, PF96] </ref>. The objective of the local register allocation phase is to minimize the total traffic between CPU and memory.
Reference: [MS91] <author> Lyle A. McGeoch and Daniel D. Sleator. </author> <title> A strongly competitive randomized paging algorithm. </title> <journal> Algorithmica, </journal> <volume> 6 </volume> <pages> 816-825, </pages> <year> 1991. </year>
Reference-contexts: First, write-back paging has been studied both as an on-line and as an off-line problem. In the on-line version, future page references are unknown. In the off-line version, the whole sequence of page references is known in advance. Off-line paging algorithms serve as a theoretical <ref> [IKP96, MS91, ST85] </ref> and experimental [FR97, Tan92] point of comparison for on-line algorithms, and stimulate researchers to find on-line 2 approximations [SP83]. Local register allocation is inherently an off-line problem because the whole sequence of references is completely found in the compiled code. The second difference is the following. <p> Write-back paging is much harder than paging without write-backs. Indeed, the latter can be optimally solved by the furthest-first rule: in response to a page fault, evict the page that will be referenced furthest in the future [Bel66, HKMW66] (see also <ref> [MS91] </ref> for a shorter proof). Consequently, the minimum number of page faults can be found in O (r log log N ) by an implementation of furthest-first with Thorup's priority queues [Tho96]. By contrast, write-back paging is proved to be NP-hard.
Reference: [Nak67] <author> Ikuo Nakata. </author> <title> On compiling algorithms for arithmetic expressions. </title> <journal> Communications of the ACM, </journal> <volume> 10(8) </volume> <pages> 492-494, </pages> <month> August </month> <year> 1967. </year>
Reference-contexts: Some heuristics have been proposed to produce quickly a local register allocation [ASU86, FL88, HFG89, TS85]. For example, the furthest-first algorithm evicts the program variable that will be referenced furthest in the future <ref> [Bel66, FH92, HGAM93, Nak67] </ref>. Related heuristics have been proposed in the context of spilling , for whose definition and algorithms we refer the reader to the textbooks [ASU86, FL88, TS85].
Reference: [NW88] <author> George L. Nemhauser and Laurence A. Wolsey. </author> <title> Integer and Combinatorial Optimization. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1988. </year>
Reference: [PF96] <author> Todd A. Proebsting and Charles N. Fischer. </author> <title> Demand-driven register allocation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 18(6) </volume> <pages> 683-710, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: The main difficulty of local register allocation stems from the trade-off between the cost of loads and the cost of stores. In the compiler context, local register allocation should take priority over global register allocation <ref> [ASU86, PF96] </ref>; for example, inner loops should have priority over outer loops [ASU86]. Once a local allocation has been determined, it can be extended to a global register allocation [Bea74, CK91, HGAM93, Ken72, K + 86, Mor91, PF96]. <p> In the compiler context, local register allocation should take priority over global register allocation [ASU86, PF96]; for example, inner loops should have priority over outer loops [ASU86]. Once a local allocation has been determined, it can be extended to a global register allocation <ref> [Bea74, CK91, HGAM93, Ken72, K + 86, Mor91, PF96] </ref>. The objective of the local register allocation phase is to minimize the total traffic between CPU and memory.
Reference: [SP83] <author> Abraham Silberschatz and James L. Peterson. </author> <title> Operating Systems Concepts. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1983. </year>
Reference-contexts: In the off-line version, the whole sequence of page references is known in advance. Off-line paging algorithms serve as a theoretical [IKP96, MS91, ST85] and experimental [FR97, Tan92] point of comparison for on-line algorithms, and stimulate researchers to find on-line 2 approximations <ref> [SP83] </ref>. Local register allocation is inherently an off-line problem because the whole sequence of references is completely found in the compiled code. The second difference is the following. In the paging problem, only one page reference is generated in one step.
Reference: [ST85] <author> D. D. Sleator and R. E. Tarjan. </author> <title> Amortized efficiency of list update and paging rules. </title> <journal> Communications of the ACM, </journal> <volume> 28 </volume> <pages> 202-208, </pages> <year> 1985. </year>
Reference-contexts: First, write-back paging has been studied both as an on-line and as an off-line problem. In the on-line version, future page references are unknown. In the off-line version, the whole sequence of page references is known in advance. Off-line paging algorithms serve as a theoretical <ref> [IKP96, MS91, ST85] </ref> and experimental [FR97, Tan92] point of comparison for on-line algorithms, and stimulate researchers to find on-line 2 approximations [SP83]. Local register allocation is inherently an off-line problem because the whole sequence of references is completely found in the compiled code. The second difference is the following.
Reference: [Tan92] <author> Andrew S. Tanenbaum. </author> <title> Modern Operating Systems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: In the on-line version, future page references are unknown. In the off-line version, the whole sequence of page references is known in advance. Off-line paging algorithms serve as a theoretical [IKP96, MS91, ST85] and experimental <ref> [FR97, Tan92] </ref> point of comparison for on-line algorithms, and stimulate researchers to find on-line 2 approximations [SP83]. Local register allocation is inherently an off-line problem because the whole sequence of references is completely found in the compiled code. The second difference is the following.
Reference: [Tho96] <author> Mikkel Thorup. </author> <title> On RAM priority queues. </title> <booktitle> In Proc. Seventh ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 59-67, </pages> <year> 1996. </year>
Reference-contexts: Consequently, the minimum number of page faults can be found in O (r log log N ) by an implementation of furthest-first with Thorup's priority queues <ref> [Tho96] </ref>. By contrast, write-back paging is proved to be NP-hard. Since the optimum value of write-back paging is a non-negative integer value less than or equal to 2r, write-back paging does not admit a fully polynomial time approximation scheme unless P = NP. <p> Since ff = 1, the register pressure never increases by more than one. It follows that the maximum supply is one. Consequently, weighted off-line weighted caching can be solved in O (r 2 log log r) time with the successive shortest path algorithm <ref> [AMO93, Tho96] </ref>. No explicit algorithm had been previously given. However, the construction in [You94] relies on algorithms for the assignment problem and implicitly yields algorithms that run in O (r 2 p r log (rC)) time and in strongly polynomial time in O (r 3 ) [AMO93].
Reference: [TS85] <author> Jean-Paul Tremblay and Paul G. Sorenson. </author> <title> The Theory and Practice of Compiler Writing. </title> <publisher> McGraw Hill, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: 1 Introduction Register allocation is a crucial task of optimizing compilers. It adds the largest single performance improvement to compiled programs [HP96]. Register allocation has received widespread attention in academic and industrial research in the past few decades <ref> [ASU86, FL88, TS85] </ref>. Register allocation is the problem of deciding which variables occupy the register file at each step of a program. The objective of the register allocation phase is to minimize the total memory traffic between the CPU and the memory system. <p> The advantages of local register allocation over graph coloring are summarized for example in [HFG89]. The problem of local register allocation is left as an open question in the "Dragon Book" ([ASU86], section 9.6, function getreg, step 3) and in other standard textbooks on compilers <ref> [FL88, TS85] </ref>. Local register allocation has been performed since the first FORTRAN compiler [BBB + 57], and formally studied since the mid-sixties [HKMW66]. Currently, the best algorithm for local register allocation takes exponential time and space in the worst case, but no lower bound is known. <p> However, the algorithm fails to terminate in more recent implementations due to excessive memory space demands [HFG89]. Some heuristics have been proposed to produce quickly a local register allocation <ref> [ASU86, FL88, HFG89, TS85] </ref>. For example, the furthest-first algorithm evicts the program variable that will be referenced furthest in the future [Bel66, FH92, HGAM93, Nak67]. <p> For example, the furthest-first algorithm evicts the program variable that will be referenced furthest in the future [Bel66, FH92, HGAM93, Nak67]. Related heuristics have been proposed in the context of spilling , for whose definition and algorithms we refer the reader to the textbooks <ref> [ASU86, FL88, TS85] </ref>. Previous to this paper, no optimum algorithm was known to take less than exponential time and space in the worst case, nor to work "reasonably" well on actual benchmarks. On the other hand, no lower bound was known. <p> A program variable is said to be alive on exit if its value has to be maintained across basic block boundaries. The set of variables alive on exit is denoted by L. The variables alive on exit are determined by a live range analysis procedure <ref> [ASU86, FL88, TS85] </ref>. We will denote with 0 (i) the first step where program variable i is referenced: 0 (i) = minfj : 9s 2 T:(i; s) 2 j g.
Reference: [You94] <author> Neal Young. </author> <title> The K-server dual and loose competitiveness for paging. </title> <journal> Algorithmica, </journal> <volume> 11(6) </volume> <pages> 525-541, </pages> <year> 1994. </year>
Reference-contexts: Register allocation is tremendously effective because of the huge speed gap between processors and memories. In addition to the attention register allocation in its own right, it has both Off-Line Paging with Write-Backs [Bel66, Car84] and Off-Line Weighted Caching <ref> [MMS90, You94] </ref> as particular cases. Local Register Allocation In this paper, we will focus on local register allocation and we will provide strong negative and positive results. Local register allocation assigns registers to variables in basic blocks, which are maximal branch-free sequences of instructions. <p> We propose a simple modification of the furthest-first algorithm which we call conservative-furthest-first (CFF). We analyze the behavior of CFF and prove a performance guarantee. Our analysis is tight. We study weighted off-line caching, which was known to be polynomially solvable <ref> [You94] </ref>. We present the first algorithm that runs in (almost) quadratic time. We conduct experiments with our algorithms on real benchmarks. We compare a branch-and-bound algorithm based on linear programming relaxation, the approximation algorithm W, and CFF. <p> It follows that the maximum supply is one. Consequently, weighted off-line weighted caching can be solved in O (r 2 log log r) time with the successive shortest path algorithm [AMO93, Tho96]. No explicit algorithm had been previously given. However, the construction in <ref> [You94] </ref> relies on algorithms for the assignment problem and implicitly yields algorithms that run in O (r 2 p r log (rC)) time and in strongly polynomial time in O (r 3 ) [AMO93]. <p> The worst-case allocation of W is typically better than the worst-case allocation of CFF. On the other hand, W is much slower. Acknowledgements Uli Kremer has been an invaluable sources of information on the issues arising in existing optimizing compilers. Rakesh Barve pointed out to us reference <ref> [You94] </ref>. We thank Sri Divakaran, Mike Grigoriadis, Sampath Kannan, Leonid Khachyian, Sanjeev Khanna, Lorant Porkolab, Barbara Ryder, Jorge Villavicencio, and Shiyu Zhou for helpful discussions.
References-found: 31

