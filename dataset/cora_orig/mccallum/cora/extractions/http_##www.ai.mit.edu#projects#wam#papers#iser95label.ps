URL: http://www.ai.mit.edu/projects/wam/papers/iser95label.ps
Refering-URL: http://www.ai.mit.edu/projects/handarm-haptics/abstracts/iser95-abst.html
Root-URL: 
Email: jesse@ai.mit.edu  jjs@mit.edu  
Title: Experiments in Hand-Eye Coordination Using Active Vision  
Author: Won Hong Jean-Jacques E. Slotine 
Address: Cambridge, MA, USA  Cambridge, MA, USA  
Affiliation: Massachusetts Institute of Technology  Massachusetts Institute of Technology  
Abstract: Robot hand-eye coordination has recently enjoyed much attention. Previous research at MIT has examined combining vision and manipulation applied to the task of tracking and catching tossed balls in controlled environments. Building upon the foundations of this past research, this paper presents work which incorporates a new active vision system which requires a minimally controlled environment, and implements methods for object tracking, robot/camera calibration, and new catching algorithms. Experimental results for real time catching of free-flying spherical balls are presented. The system was tested on under-hand tosses from random locations approximately 1.5-2.5 meters distant from the base of the arm. The best performance results were found to be 70-80% success for similar tosses.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. K. Salisbury, </author> <title> Whole Arm Manipulation, </title> <booktitle> Proc. 4th Int. Symposium on Robotics Research, </booktitle> <address> Santa Cruz, CA, </address> <month> August, </month> <year> 1987. </year>
Reference-contexts: 1. Introduction This paper presents work on the application of a manipulator arm (the Whole Arm Manipulator (WAM) <ref> [1] </ref> [2] [3]) and an active vision system (the Fast Eye Gimbals (FEGs) [4]) to the task of real time catching of free flying objects.
Reference: [2] <author> J. K. Salisbury, W. T. Townsend, B. S. Eberman, D. M. DiPietro, </author> <title> Preliminary Design of a Whole-Arm Manipulation System (WAM), </title> <booktitle> Proc. 1988 IEEE Int. Conf. on Robotics and Automation, </booktitle> <address> Philadelphia, PA, </address> <month> April </month> <year> 1988. </year>
Reference-contexts: 1. Introduction This paper presents work on the application of a manipulator arm (the Whole Arm Manipulator (WAM) [1] <ref> [2] </ref> [3]) and an active vision system (the Fast Eye Gimbals (FEGs) [4]) to the task of real time catching of free flying objects.
Reference: [3] <author> W. T. Townsend, </author> <title> The Effect of Transmission Design on Force-Controlled Manipulator Performance, </title> <type> PhD Thesis, </type> <institution> Department of Mechanical Engineering, MIT, </institution> <month> April </month> <year> 1988. </year> <note> (See also MIT AI Lab Technical Report 1054). </note>
Reference-contexts: 1. Introduction This paper presents work on the application of a manipulator arm (the Whole Arm Manipulator (WAM) [1] [2] <ref> [3] </ref>) and an active vision system (the Fast Eye Gimbals (FEGs) [4]) to the task of real time catching of free flying objects.
Reference: [4] <author> N. Swarup, </author> <title> Design and Control of a Two-Axis Gimbal System for Use in Active Vision, </title> <type> S.B. Thesis, </type> <institution> Department of Mechanical Engineering, MIT, </institution> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: 1. Introduction This paper presents work on the application of a manipulator arm (the Whole Arm Manipulator (WAM) [1] [2] [3]) and an active vision system (the Fast Eye Gimbals (FEGs) <ref> [4] </ref>) to the task of real time catching of free flying objects. The purpose of this research is to investigate the challenges involved in achieving successful catching and also to lay the foundations for further study into hand-eye coordination and robot learning with this system.
Reference: [5] <author> J. Billingsley, </author> <title> Robot ping pong, </title> <booktitle> Practical Computing, </booktitle> <month> May </month> <year> 1983. </year>
Reference-contexts: Although there is little current research in the area of robotic catching, there is a wealth of research in the general area of applying manipulators and vision systems to accomplishing dynamic tasks. Good examples of hand-eye coordination systems are the robot ping pong players <ref> [5] </ref> [6] [7]. These have exceptional vision systems fl To be published in Experimental Robotics IV, Springer-Verlag, Proceedings of the Fourth International Symposium on Experimental Robotics, ISER'95, Stanford, California, June 30-July 2, 1995. combined with fast manipulators, but are constrained to work in heavily controlled environments.
Reference: [6] <author> H. Fassler, H. A. Beyer, and J. Wen, </author> <title> A robot ping pong player: optimized mechanics, high performance 3D vision, and intelligent sensor control, Robotersysteme 6, </title> <publisher> Springer-Verlag, </publisher> <pages> pp. 161-170, </pages> <year> 1990. </year>
Reference-contexts: Although there is little current research in the area of robotic catching, there is a wealth of research in the general area of applying manipulators and vision systems to accomplishing dynamic tasks. Good examples of hand-eye coordination systems are the robot ping pong players [5] <ref> [6] </ref> [7]. These have exceptional vision systems fl To be published in Experimental Robotics IV, Springer-Verlag, Proceedings of the Fourth International Symposium on Experimental Robotics, ISER'95, Stanford, California, June 30-July 2, 1995. combined with fast manipulators, but are constrained to work in heavily controlled environments.
Reference: [7] <author> R. L. Andersson, </author> <title> A robot ping-pong player: Experiment in real time control, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1987. </year>
Reference-contexts: Although there is little current research in the area of robotic catching, there is a wealth of research in the general area of applying manipulators and vision systems to accomplishing dynamic tasks. Good examples of hand-eye coordination systems are the robot ping pong players [5] [6] <ref> [7] </ref>. These have exceptional vision systems fl To be published in Experimental Robotics IV, Springer-Verlag, Proceedings of the Fourth International Symposium on Experimental Robotics, ISER'95, Stanford, California, June 30-July 2, 1995. combined with fast manipulators, but are constrained to work in heavily controlled environments.
Reference: [8] <author> E. Aboaf, S. Drucker, and C. Atkeson, </author> <title> Task-level robot learning: juggling a tennis ball more accurately, </title> <booktitle> Proc. IEEE Int. Conf. on Robotics and Automation, </booktitle> <pages> pp. 1290-1295, </pages> <address> Scottsdale, AZ, </address> <month> May </month> <year> 1989. </year>
Reference-contexts: Previous work has also studied robot juggling <ref> [8] </ref> [9]. As for active vision, a good survey and definition of future research directions has come from the NSF Active Vision Workshop, 1991 [10]. More recently, there have been some portable vision systems designed to study topics in active vision [11] [12] [13].
Reference: [9] <author> A. A. Rizzi and D. E. Koditschek, </author> <title> Further progress in robot juggling: Solvable mirror laws, </title> <booktitle> Int. Conf. on Robotics and Automation, </booktitle> <pages> pp. 2935-2940, </pages> <year> 1994. </year>
Reference-contexts: Previous work has also studied robot juggling [8] <ref> [9] </ref>. As for active vision, a good survey and definition of future research directions has come from the NSF Active Vision Workshop, 1991 [10]. More recently, there have been some portable vision systems designed to study topics in active vision [11] [12] [13].
Reference: [10] <editor> Promising Directions in Active Vision, </editor> <title> NSF Active Vision Workshop, </title> <institution> University of Chicago, </institution> <month> August 5-7, </month> <year> 1991, </year> <title> in Int. </title> <journal> Journal of Computer Vision, </journal> <volume> Vol. 11:2, </volume> <pages> pp. 109-126, </pages> <year> 1993. </year>
Reference-contexts: Previous work has also studied robot juggling [8] [9]. As for active vision, a good survey and definition of future research directions has come from the NSF Active Vision Workshop, 1991 <ref> [10] </ref>. More recently, there have been some portable vision systems designed to study topics in active vision [11] [12] [13]. In contrast with these systems, our system uses two independent two degree of freedom actuators in combination to achieve stereo vision.
Reference: [11] <author> J. C. Fiala, R. Lumia, K. J. Roberts, and A. J. </author> <month> Wavering, </month> <institution> National Institute of Standards and Technology, </institution> <month> TRICLOPs: </month> <title> A Tool for Studying Active Vision, </title> <journal> Int. Journal of Computer Vision, </journal> <volume> Vol. 12:2/3, </volume> <pages> pp. 231-250, </pages> <year> 1994. </year>
Reference-contexts: Previous work has also studied robot juggling [8] [9]. As for active vision, a good survey and definition of future research directions has come from the NSF Active Vision Workshop, 1991 [10]. More recently, there have been some portable vision systems designed to study topics in active vision <ref> [11] </ref> [12] [13]. In contrast with these systems, our system uses two independent two degree of freedom actuators in combination to achieve stereo vision.
Reference: [12] <author> A. J. Wavering and R. Lumia, </author> <title> Predictive Visual Tracking, </title> <booktitle> SPIE Volume 2056, Intelligent Robots and Computer Vision XII, </booktitle> <year> 1993. </year>
Reference-contexts: Previous work has also studied robot juggling [8] [9]. As for active vision, a good survey and definition of future research directions has come from the NSF Active Vision Workshop, 1991 [10]. More recently, there have been some portable vision systems designed to study topics in active vision [11] <ref> [12] </ref> [13]. In contrast with these systems, our system uses two independent two degree of freedom actuators in combination to achieve stereo vision.
Reference: [13] <author> H. K. Nishihara, H. J. Thomas, </author> <title> Real-Time Tracking of People Using Stereo and Motion, </title> <booktitle> IS&T/SPIE Symposium on Electronic Imaging: Science & Technology, </booktitle> <address> San Jose, California, </address> <month> February 6-10, </month> <year> 1994. </year>
Reference-contexts: As for active vision, a good survey and definition of future research directions has come from the NSF Active Vision Workshop, 1991 [10]. More recently, there have been some portable vision systems designed to study topics in active vision [11] [12] <ref> [13] </ref>. In contrast with these systems, our system uses two independent two degree of freedom actuators in combination to achieve stereo vision.
Reference: [14] <author> B. M. Hove and J.-J. E. Slotine, </author> <title> Experiments in Robotic Catching, </title> <booktitle> Proc. of the 1991 American Control Conf. </booktitle> <volume> Vol. </volume> <pages> 1, </pages> <address> Boston, MA, </address> <pages> pp. 380-385, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Past research at our site has examined robotic catching. Successful catching results were first presented using the WAM combined with two stationary black and white cameras <ref> [14] </ref>. Adaptive visual tracking algorithms were later added which provided cleaner vision information and improved catching reliability [15] [16]. Our current system uses the WAM once again, combined with new vision hardware. The new vision system is comprised of two color CCD cameras, each mounted on two DOF gimbals.
Reference: [15] <author> H. Kimura, N. Mukai, and J.-J. E. Slotine, </author> <title> Adaptive Visual Tracking and Gaussian Network Algorithms for Robotic Catching, DSC-Vol. 43, Advances in Robust and Nonlinear Control Systems, </title> <booktitle> Winter Annual Meeting of the ASME, </booktitle> <address> Anaheim, CA, </address> <pages> pp. 67-74, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Past research at our site has examined robotic catching. Successful catching results were first presented using the WAM combined with two stationary black and white cameras [14]. Adaptive visual tracking algorithms were later added which provided cleaner vision information and improved catching reliability <ref> [15] </ref> [16]. Our current system uses the WAM once again, combined with new vision hardware. The new vision system is comprised of two color CCD cameras, each mounted on two DOF gimbals. Vision processing is accomplished through fast color keyed blob detection. <p> WAM Path Generation Once a satisfactory catch point is determined, the WAM attempts to intercept and match position and velocity with the object. Third order polynomials in x, y, and z are used to generate an intercepting path for the WAM <ref> [15] </ref>. p (t) = 6 1 a p t 2 + v p t + p p (4) The constants in the polynomial path are determined each servo loop from knowledge of the current position and velocity of the WAM (p (t 1 ) and v (t 1 )) and the
Reference: [16] <author> H. Kimura, </author> <title> Adaptive Visual Tracking Algorithms for 3-D Robotic Catching, M.S. </title> <type> Thesis, </type> <institution> Department of Mechanical Engineering, MIT, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: Past research at our site has examined robotic catching. Successful catching results were first presented using the WAM combined with two stationary black and white cameras [14]. Adaptive visual tracking algorithms were later added which provided cleaner vision information and improved catching reliability [15] <ref> [16] </ref>. Our current system uses the WAM once again, combined with new vision hardware. The new vision system is comprised of two color CCD cameras, each mounted on two DOF gimbals. Vision processing is accomplished through fast color keyed blob detection. <p> This attempts to compensate for differences in back-swings among various people. Once the toss has been triggered, data storage, fitting and prediction begins. The future path of the tossed object is predicted using recursive least squares techniques <ref> [16] </ref> assuming a parabolic model for the trajectory. The algorithm is recursive, therefore the computation required for each new data point is independent of the number of data points already collected. Similarly, each data point is weighted equally, the last having as much effect as the first.
Reference: [17] <author> W. Hong, </author> <title> Robotic Catching and Manipulation Using Active Vision, M.S. </title> <type> Thesis, </type> <institution> Department of Mechanical Engineering, MIT, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: These estimates are then used to integrate over the time delay assuming constant acceleration to obtain predictions for the current position and velocity of the object. These values are then fed to the FEG adaptive/feedforward controller <ref> [17] </ref>. It should be noted that there is a trade-off between control and vision performance. The large focal length of the lenses yields higher resolution, lower distortion, and focuses attention better.
Reference: [18] <author> J.-J. E. Slotine and W. Li, </author> <title> Applied Nonlinear Control, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1991. </year>
Reference-contexts: The camera lenses have approximately a 5 degree field of view for higher resolution, better focusing of attention, and less to a ceiling rafter. put from the blob detector boards. distortion. The FEGs are controlled using PD, PID, and adaptive/feedforward <ref> [18] </ref> controllers depending upon the nature of the trajectory they are commanded. The camera output from the FEGs are directed to simple blob detector vision boards [19]. The vision boards initially convert the color camera image into a binary image through comparison with a color histogram (which is re-trainable).
Reference: [19] <author> A. Wright, </author> <title> A high speed low-latency portable vision sensing system, </title> <booktitle> SPIE, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: The FEGs are controlled using PD, PID, and adaptive/feedforward [18] controllers depending upon the nature of the trajectory they are commanded. The camera output from the FEGs are directed to simple blob detector vision boards <ref> [19] </ref>. The vision boards initially convert the color camera image into a binary image through comparison with a color histogram (which is re-trainable).
Reference: [20] <author> B. K. P. Horn, </author> <title> Robot Vision, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: The translation is found by determining the vector which overlaps the centroids of the two sets of data. The angle of rotation is found by using a least squares approach <ref> [20] </ref>.
Reference: [21] <author> G. Niemeyer and J.-J. E. Slotine, </author> <title> Performance in Adaptive Manipulator Control, </title> <month> December </month> <year> 1988, </year> <title> Int. </title> <journal> Journal of Robotics Research, </journal> <volume> 10(2). </volume>
Reference-contexts: If the path exceeds the outer limit, ff is reduced, thereby scaling down the velocity to be matched, which reduces the "out-swing" of the generated path. This resulting polynomial path is then fed to the WAM controller which utilizes adaptive control to achieve high performance tracking <ref> [21] </ref>. After the WAM has reached the catch point, the WAM matches trajectories with the object for a short duration of time and then begins deceleration along the previous path of the object.
Reference: [22] <author> M. Cannon and J.-J.E. Slotine, </author> <title> Space Frequency Localized Basis Function Networks for Nonlinear System Estimation and Control, </title> <journal> Neurocomputing, </journal> <volume> 9(3), </volume> <year> 1995. </year>
Reference-contexts: Objects of interest are light-weight foam balls, paper airplanes, and other additional items with non-parabolic trajectories. The current model based least squares prediction methods for the tossed object are replaced by wavelet network based prediction methods <ref> [22] </ref> [23]. Interfacing this with the catching algorithms used here have yielded good pre and a paper airplane (left to right). liminary results. Figure 12 shows 3D plots of successful catches using the network based path prediction method.
Reference: [23] <author> I. Watanabe and J.-J.E. Slotine, </author> <title> Stable real-time prediction of the trajectories of light objects in air using wavelet networks, </title> <address> MIT-NSL 100195, </address> <year> 1995. </year>
Reference-contexts: Objects of interest are light-weight foam balls, paper airplanes, and other additional items with non-parabolic trajectories. The current model based least squares prediction methods for the tossed object are replaced by wavelet network based prediction methods [22] <ref> [23] </ref>. Interfacing this with the catching algorithms used here have yielded good pre and a paper airplane (left to right). liminary results. Figure 12 shows 3D plots of successful catches using the network based path prediction method.
References-found: 23

