URL: ftp://ife.ethz.ch/pub/music/paper/music_paper.ps
Refering-URL: http://www.ife.ee.ethz.ch/music/music.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail: toni@nimbus.ethz.ch  
Title: Achieving Super Computer Performance with a DSP Array Processor  
Author: A. Gunzinger, U. A. Muller, W. Scott*, B. Baumle, P. Kohler, H.R. vonder Muhll, F. Muller-Plathe*, W. F. van Gunsteren*, W. Guggenbuhl 
Address: CH-8092 Zurich, Switzerland  
Affiliation: Electronics Laboratory, *Laboratory of Physical Chemistry Swiss Federal Institute of Technology  
Abstract: The MUSIC system (MUlti Signal processor system with Intelligent Communication) is a parallel distributed memory architecture based on digital signal processors (DSP). A system with 60 processor elements is operational. It has a peak performance of 3.8 GFlops, an electrical power consumption of less than 800 W (including forced air cooling) and fits into a 19" rack. Two applications (the back-propagation algorithm for neural net learning and molecular dynamics simulations) run about 6 times faster than on a CRAY Y-MP and 2 times faster than on a NEC SX-3. A sustained performance of more than 1 GFlops is reached. The selling price of such a system would be in the range of about 300'000 US$. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Annaratone, E. Arnould, T. Gross, H. T. Kung, M. Lam, O. Menzilicioglu, J. A. Webb. </author> <title> The WARP Computer: Architecture, Implementation and Per-formence. </title> <journal> IEEE Trans. on Computer, </journal> <volume> Vol. C-36, No. 12, </volume> <month> December </month> <year> 1987, </year> <month> pp.1523-1538 </month>
Reference-contexts: 1 Introduction Digital signal processing and scientific computing often demand high computing power. Because many tasks in these applications have a great potential for parallel processing, developers have proffered a number of parallel computer architectures. The WARP <ref> [1] </ref> multiprocessor system has a ring communication network with a maximum of 10 processor elements. The peak performance is 100 MFlops. A ring architecture is also found in the RAP (Ring Array Processor) architecture [2] which uses TMS320C30 digital signal processors.
Reference: [2] <author> Nelson Morgan, James Beck, Phil Kohn, Jeff Bilmes, Eric Allman, and Joachim Beer. </author> <title> The ring array processor: A multiprocessing peripheral for connectionist applications. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14(3) </volume> <pages> 248-259, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The WARP [1] multiprocessor system has a ring communication network with a maximum of 10 processor elements. The peak performance is 100 MFlops. A ring architecture is also found in the RAP (Ring Array Processor) architecture <ref> [2] </ref> which uses TMS320C30 digital signal processors. Another parallel system based on digital signal processors is the DSP3 [4] which consists of processing elements realized with multi-chip modules and a relatively complex communication network topology.
Reference: [3] <author> Michael Witbrock and Marco Zagha. </author> <title> An implementation of backpropagation learning on GF11, a large SIMD parallel computer. </title> <journal> Parallel Computing, </journal> <volume> 14(3) </volume> <pages> 329-346, </pages> <year> 1990. </year>
Reference-contexts: Another parallel system based on digital signal processors is the DSP3 [4] which consists of processing elements realized with multi-chip modules and a relatively complex communication network topology. The GF11 <ref> [3] </ref> is a large SIMD (single instruction on multiple data) machine with a maximum of 566 pro cessors and a peak performance of 11 GFlops. The Connection Machine CM-2 [14] finally is a massively parallel computer with up to 65'000 simple processor elements arranged in a hypercube topology. <p> That means that it allows almost any modification on the neural network structure and learning algo rithm. Other implementations are much more restricted in this point. The IBM GF11 implementation (900 MCUPS) for instance <ref> [3] </ref> parallelizes over the training set. This method only allows batch-learning (no immediate weight update) which has the effect that the learning convergence is in many cases much slower. 6 Molecular Dynamics The program MD-Atom is used for time-discrete simulations of the dynamics of atomic fluids. <p> (80486, 33 MHz)* 1 0.16 | Yes Sun (Sparcstation 2)* 1 0.51 48.0 Yes Transputer T800 [12] 64 9.9 | | Warp [13] 10 17.0 | No CM-2 [14] 64K 40.0 | No NEX SX-3** 1 130.0 9.6 Yes MUSIC-10* 30 150 34 Yes MUSIC-20* 60 246 28 Yes GF11 <ref> [3] </ref> 356 901 54 No *Based on our own measurements. **Presented by N. Koike of NEC at the 1992 Second ETH-NEC Joint Workshop on Supercomputing (no published reference available).
Reference: [4] <author> R. R. Shively and L. J. Wu. </author> <title> Application and Packaging of the AT&T DSP3 Parallel Signal Processor. </title> <editor> In V. Cappellini and A. G. Constantinides, editors, </editor> <booktitle> Digital Signal Processing-91. </booktitle> <publisher> Elsevier, </publisher> <year> 1991. </year>
Reference-contexts: The peak performance is 100 MFlops. A ring architecture is also found in the RAP (Ring Array Processor) architecture [2] which uses TMS320C30 digital signal processors. Another parallel system based on digital signal processors is the DSP3 <ref> [4] </ref> which consists of processing elements realized with multi-chip modules and a relatively complex communication network topology. The GF11 [3] is a large SIMD (single instruction on multiple data) machine with a maximum of 566 pro cessors and a peak performance of 11 GFlops.
Reference: [5] <author> Anton Gunzinger, Urs Muller, and Hansruedi Von-der Muhll. </author> <title> Architecture and Realization of a Multi Signalprocessor System. </title> <editor> In Amnon Aliphas, editor, </editor> <address> Berlin '91, </address> <pages> pages 242-249. </pages> <publisher> DSP Associates, </publisher> <year> 1991. </year>
Reference-contexts: The speedup factor denotes how many times faster a certain algorithm is carried out on a system containing n processor elements compared to a single processor system. A simple model allows the prediction of the speedup factor s of the MUSIC system <ref> [5] </ref>: s (n) = t comp (1) t comp (n) + t comm = n + c The so called complexity factor c is the ratio between the computation time t comp (1) and the communication time t comm of a single processor element for a given data block and algorithm.
Reference: [6] <author> W. F. van Gunsteren and H. J. C. Berend-sen. </author> <title> Molecular Dynamics Computer Simulations: Methodology, </title> <booktitle> Applications and Perspectives in Chemistry.Angewandte Chemie Int. </booktitle> <editor> Ed. Engl. </editor> <volume> 29(1990), </volume> <pages> pages 992-1023. </pages>
Reference: [7] <author> M. P. Allen and D. J. Tildesley. </author> <title> Computer Simulations of Liquids, </title> <publisher> Oxford University Press 1987 </publisher>
Reference: [8] <author> W. F. van Gunsteren, H. J. C. Berendsen, F. Colonna, D. Perahia, J.P. Hollenberg and D. </author> <title> Lel-louch On Searching Neighbours in Computer Simulations of Macromolecular Systems J. </title> <journal> Comp. Chem. </journal> <volume> Vol. 5 No. </volume> <month> 3 272 -279 </month> <year> (1984) </year>
Reference-contexts: To compare the performance of MD-Atom on MUSIC with other super computers, an implementation on the NEC SX-3, one on the CRAY YMP, and one on the Sun-4 (IPX) was done by the Laboratory of Physical Chemistry, Swiss Federal Institute of Technology <ref> [8] </ref> [9]. These implementations were written in FORTRAN and have been optimized for the respective hardware. As far as we know, these are the fastest implementations of MD Atom on super computers. The measured results of this comparison are given in the following table.
Reference: [9] <institution> Florian Muller-Plathe Parallelising a Molecular Dynamics Algorithm on a Multi-Processor Workstation Computer Physics Communications 61 285-293 (1990) </institution>
Reference-contexts: To compare the performance of MD-Atom on MUSIC with other super computers, an implementation on the NEC SX-3, one on the CRAY YMP, and one on the Sun-4 (IPX) was done by the Laboratory of Physical Chemistry, Swiss Federal Institute of Technology [8] <ref> [9] </ref>. These implementations were written in FORTRAN and have been optimized for the respective hardware. As far as we know, these are the fastest implementations of MD Atom on super computers. The measured results of this comparison are given in the following table.
Reference: [10] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representation by error propagation. </title> <editor> In David E. Rumelhart and James L. McClelland, editors, </editor> <booktitle> Parallel Distributet Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> Bradford Books, </publisher> <address> Cambridge MA, </address> <year> 1986. </year>
Reference-contexts: This problem can be overcome by using parallel architectures like the implementation on MUSIC. Back-propagation is a very popular algorithm for the learning of layered feed forward neural networks (multilayer perceptrons) <ref> [10] </ref>. Although many sug gestions for improvements exist, the standard backpropagation algorithm is still a basic method and therefore it is implemented on many computers. This allows a comparison between the performance of MUSIC and other systems. <p> To train the neural net, examples are needed which consist of selected input vectors and the desired corresponding output (target) vectors of the net. The most time consuming part of the algorithm <ref> [10] </ref> is given by (2) for the forward and (3,4) for the backward path (learning): o j = f ( i w new ji + ffi j o i with (3) 1 PE 1 Board 10 Boards 20 Boards MUSIC-1 MUSIC-10 MUSIC-20 PE's 1 3 30 60 Peak Performance [MFLOPS] 60
Reference: [11] <author> Wei-Ming Lin, Viktor K. Prasanna, and K. Wo-jtek Przytula. </author> <title> Algorithmic mapping of neural network models onto parallel simd machines. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(12) </volume> <pages> 1390-1401, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: To avoid the communication of the updated weights, which would lead to communication saturation very easily, two different weight subsets for the forward and the backward propagation are stored and updated individually on every processor element <ref> [11] </ref>. A single connection update demands a multiplication plus an addition in the forward (2) as well as in the backward path (4). The actual weight update (3) needs an additional multiplication and addition.
Reference: [12] <author> Heinz Muhlbein and Klaus Wolf. </author> <title> Neural network simulation on parallel computers. </title> <editor> In David J. Evans, Gerhard R. Joubert, and Frans J. Peters, editors, </editor> <booktitle> Parallel Computing-89, </booktitle> <pages> pages 365-374, </pages> <address> Amsterdam, 1990. </address> <publisher> North Holland. </publisher>
Reference-contexts: of kinetic and potential energies, virial and pressure, cubic 3D periodic boundary conditions, leap-frog scheme for integration of Newton's equation of motion Continuous System No. of Backprop Peak (%) weight PEs [MCUPS] update PC (80486, 33 MHz)* 1 0.16 | Yes Sun (Sparcstation 2)* 1 0.51 48.0 Yes Transputer T800 <ref> [12] </ref> 64 9.9 | | Warp [13] 10 17.0 | No CM-2 [14] 64K 40.0 | No NEX SX-3** 1 130.0 9.6 Yes MUSIC-10* 30 150 34 Yes MUSIC-20* 60 246 28 Yes GF11 [3] 356 901 54 No *Based on our own measurements. **Presented by N.
Reference: [13] <author> Dean A. Pomerleau, George L. Gusciora, David S. Touretzky, and H. T. Kung. </author> <title> Neural network simulation at warp speed: How we got 17 million connections per second. </title> <booktitle> In IEEE International Conference on Neural Networks, pages II.143-150, </booktitle> <address> July 24-27, San Diego, California 1988. </address>
Reference-contexts: and pressure, cubic 3D periodic boundary conditions, leap-frog scheme for integration of Newton's equation of motion Continuous System No. of Backprop Peak (%) weight PEs [MCUPS] update PC (80486, 33 MHz)* 1 0.16 | Yes Sun (Sparcstation 2)* 1 0.51 48.0 Yes Transputer T800 [12] 64 9.9 | | Warp <ref> [13] </ref> 10 17.0 | No CM-2 [14] 64K 40.0 | No NEX SX-3** 1 130.0 9.6 Yes MUSIC-10* 30 150 34 Yes MUSIC-20* 60 246 28 Yes GF11 [3] 356 901 54 No *Based on our own measurements. **Presented by N.
Reference: [14] <author> Xiru Zhang, Michael Mckenna, Jill P. Mesirov, and David L. Waltz. </author> <title> An efficient implementation of the back-propagation algorithm on the connection machine cm-2. </title> <editor> In David S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems (NIPS-89), </booktitle> <pages> pages 801-809, </pages> <address> 2929 Campus Drive, Suite 260, San Mateo, CA 94403, 1990. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: The GF11 [3] is a large SIMD (single instruction on multiple data) machine with a maximum of 566 pro cessors and a peak performance of 11 GFlops. The Connection Machine CM-2 <ref> [14] </ref> finally is a massively parallel computer with up to 65'000 simple processor elements arranged in a hypercube topology. Its successor CM-5 is now available on the market. Such systems can outperform even conventional supercomputers. <p> conditions, leap-frog scheme for integration of Newton's equation of motion Continuous System No. of Backprop Peak (%) weight PEs [MCUPS] update PC (80486, 33 MHz)* 1 0.16 | Yes Sun (Sparcstation 2)* 1 0.51 48.0 Yes Transputer T800 [12] 64 9.9 | | Warp [13] 10 17.0 | No CM-2 <ref> [14] </ref> 64K 40.0 | No NEX SX-3** 1 130.0 9.6 Yes MUSIC-10* 30 150 34 Yes MUSIC-20* 60 246 28 Yes GF11 [3] 356 901 54 No *Based on our own measurements. **Presented by N.
References-found: 14

