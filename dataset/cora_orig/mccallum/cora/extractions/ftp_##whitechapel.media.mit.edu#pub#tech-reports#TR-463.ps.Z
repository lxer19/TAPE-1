URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-463.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: fjebara,bernt,nuria,sandyg@media.mit.edu  
Title: Dynamic Personal Enhanced Reality System  
Author: Tony Jebara Bernt Schiele Nuria Oliver Alex Pentland 
Web: http://www.media.mit.edu/vismod/demos/dypers  
Address: Cambridge, MA 02139  
Affiliation: Media Laboratory, Massachusetts Institute of Technology  
Note: DyPERS:  
Abstract: MIT Media Laboratory, Perceptual Computing Technical Report #463 Abstract DyPERS, 'Dynamic Personal Enhanced Reality System', is a wearable system which uses augmented reality and computer vision to autonomously retrieve 'media memories' based on associations with real objects the user encounters. These are evoked as audio and video clips taken by the user and over-layed on top of real objects the user looks at. The user's visual and auditory scene is stored in real-time by the system (upon request) and is then associated (by user input) with a snap shot of a visual object. The object acts as a key which is detected by a real-time vision system when it is in view, triggering DyPERS to play back the appropriate audio-visual sequence. The vision system is a probabilistic algorithm which is capable of discriminating between hundreds of everyday objects under varying viewing conditions (lighting, pose, etc.). The record-and-associate paradigm of the system has many potential applications. Results on the use of the system in a museum tour scenario are described. 1 
Abstract-found: 1
Intro-found: 1
Reference: [Bederson, 1995] <author> B.B. Bederson. </author> <title> Audio augmented reality: A prototype automated tour guide. </title> <booktitle> In ACM SIGCHI, </booktitle> <year> 1995. </year>
Reference-contexts: Since the system features audio-visual memory and significant automatic computer vision processing, test conditions involved these aspects in particular. DyPERS was evaluated in a museum-gallery scenario. Audio-only augmented reality in a museum situation was previously investigated by <ref> [Bederson, 1995] </ref>. The museum constitutes a rich visual environment (paintings, sculptures, etc.) which is accompanied by many relevant facts and details (from a guide or text). Thus, it is an audio-visual educational experience and well-suited for verifying the system's usefulness as an educational tool.
Reference: [Feiner et al., 1992] <author> S. Feiner, B. MacIntyre, and D. Seligmann. </author> <title> Annotating the real world with knowledge-based graphics on see-through head-mounted display. </title> <booktitle> In Proc. of Graphics Interface, </booktitle> <year> 1992. </year>
Reference-contexts: When a cue is recognized at some later time, DyPERS automatically overlays the appropriate audio-video clip on the user's world through a heads-up-display (HUD) <ref> [Feiner et al., 1992] </ref>, as a reminder of the content.
Reference: [Jacob et al., 1993] <author> R.J.K. Jacob, J.J. Leggett, B.A. Myers, and R. Pausch. </author> <title> Interaction styles and input/output devices. </title> <booktitle> Behaviour and Information Technology, </booktitle> <year> 1993. </year>
Reference-contexts: Perceptual Interfaces: Most human-computer interaction is still limited to keyboards and point ing devices. The usability bottleneck that plagues interactive systems lies not in performing the processing task itself but rather in communicating requests and results between the system and the user <ref> [Jacob et al., 1993] </ref>. Faster, more natural and convenient means for users to exchange information with computers are needed. This communication bottleneck has spurred increased research in providing perceptual capabilities (speech, vision, haptics) to interfaces.
Reference: [Jebara et al., 1997] <author> T. Jebara, C. Eyster, J. Weaver, T. Starner, and A. Pentland. Stochasticks: </author> <title> Augmenting the billiards experience with probabilistic vision and wearable computers. </title> <booktitle> In Intl. Symp. on Wearable Computers, </booktitle> <year> 1997. </year>
Reference-contexts: Other applications include a navigation system, Walk-Navi [Nagao and Rekimoto, 1996]. Audio Aura [Mynatt et al., 1997] is an active badge distributed system that augments the physical world with auditory cues. Users passively trigger the transmission of auditory cues as they move through their workplace. Finally, Jebara <ref> [Jebara et al., 1997] </ref> proposes a vision-based wearable enhanced reality system called Stochasticks for augmenting a billiards game with computer generated shot planning. Perceptual Interfaces: Most human-computer interaction is still limited to keyboards and point ing devices.
Reference: [Kakez et al., 1997] <author> S. Kakez, C. Vania, and P. Bis-son. </author> <title> Virtually documented environment. </title> <booktitle> In Intl. Symp. on Wearable Computers, </booktitle> <year> 1997. </year> <editor> [Lamming and Flynn, 1993] M. Lamming and M. Flynn. Forget-me-not: </editor> <booktitle> intimate computing in support of human memory. In FRIEND21 Intl. Symp. on Next Generation Human Interface, </booktitle> <year> 1993. </year>
Reference-contexts: Both systems collect and organize data that is relevant to the user for subsequent retrieval. Augmented Reality: Augmented reality systems form a more natural interface between user and machine which is a critical feature for a system like DyPERS. In <ref> [Kakez et al., 1997] </ref> a virtually documented environment system is described which assists the user in some performance task. It registers synthetic multimedia data acquired using a head-mounted video camera. However, information is retrieved explicitly by the user via speech commands.
Reference: [Levine, 1997] <author> J. Levine. </author> <title> Real-time target and pose recognition for 3-d graphical overlay. </title> <type> Master's thesis, </type> <institution> EECS Dept., MIT, </institution> <year> 1997. </year>
Reference-contexts: It registers synthetic multimedia data acquired using a head-mounted video camera. However, information is retrieved explicitly by the user via speech commands. On the other hand, the retrieval process is automated in <ref> [Levine, 1997] </ref>, a predecessor of DyPERS. This system used machine vision to locate `visual cues,' and then overlaid a stabilized image, messages or clips on the user's view of the cue object (via HUD).
Reference: [Mann, 1996] <author> S. Mann. Wearable, </author> <title> tetherless computer-mediated reality. </title> <type> Technical Report 361, </type> <institution> M.I.T. Media Lab, </institution> <year> 1996. </year>
Reference-contexts: The clips are the rendered as an overlay via the user's GlassTron. Two A/V wireless channels are used at all times for a bidirectional real-time connection (user to SGI and SGI to user) <ref> [Mann, 1996] </ref>. 4 Scenarios This section briefly describes some applications of DyPERS using the record-and-associate paradigm: * Daily scheduling and to-do list can be stored and associated with the user's watch or other personal trigger object. * An important conversation can be recorded and associated with the individual's business card. *
Reference: [Mann, 1997] <author> S. Mann. </author> <title> Wearable computing: A first step toward personal imaging. </title> <journal> IEEE Computer, </journal> <volume> 30(2), </volume> <month> February </month> <year> 1997. </year>
Reference-contexts: However, wearable computers attempt to augment the user directly and provide a mobile platform while ubiquitous computing augments the surrounding physical environment with a network of machines and sensors. Weiser [Weiser, 1991] discusses the merits of ubiquitous computing while Mann <ref> [Mann, 1997] </ref> argues in favor of mobile, personal audio-visual augmentation in his wearable platform. Memory Augmentation: Memory augmentation has evolved from simple pencil and paper paradigms to sophisticated personal digital assistants (PDAs) and beyond.
Reference: [Mynatt et al., 1997] <author> E.D. Mynatt, M. Back, R. Want, and R. Frederik. </author> <title> Audio aura: Light weight audio augmented reality. </title> <booktitle> In UIST, </booktitle> <year> 1997. </year>
Reference-contexts: Users view the real-world together with context sensitive information generated by the computer. NaviCam is extended in the Ubiquitous Talker [Rekimoto and Nagao, 1995] to include a speech dialogue interface. Other applications include a navigation system, Walk-Navi [Nagao and Rekimoto, 1996]. Audio Aura <ref> [Mynatt et al., 1997] </ref> is an active badge distributed system that augments the physical world with auditory cues. Users passively trigger the transmission of auditory cues as they move through their workplace.
Reference: [Nagao and Rekimoto, 1996] <author> K. Nagao and J. Reki-moto. </author> <title> Agent augmented reality: a software agent meets the real world. </title> <booktitle> In Proc. of Intl. Conf. on Multiagent Sys., </booktitle> <year> 1996. </year>
Reference-contexts: Users view the real-world together with context sensitive information generated by the computer. NaviCam is extended in the Ubiquitous Talker [Rekimoto and Nagao, 1995] to include a speech dialogue interface. Other applications include a navigation system, Walk-Navi <ref> [Nagao and Rekimoto, 1996] </ref>. Audio Aura [Mynatt et al., 1997] is an active badge distributed system that augments the physical world with auditory cues. Users passively trigger the transmission of auditory cues as they move through their workplace.
Reference: [Rekimoto and Nagao, 1995] <author> J. Rekimoto and K. Nagao. </author> <title> The world through the computer: computer augmented interaction with real world environments. </title> <booktitle> UIST, </booktitle> <year> 1995. </year>
Reference-contexts: In addition, the vision algorithm used was limited to 2D objects viewed head-on and at appropriate distances. An earlier version [Starner et al., 1997] further simplified the machine vision by using colored bar code tags as cues. In <ref> [Rekimoto and Nagao, 1995] </ref> the NaviCam system is described as a portable computer with video camera which detects pre-tagged objects. Users view the real-world together with context sensitive information generated by the computer. NaviCam is extended in the Ubiquitous Talker [Rekimoto and Nagao, 1995] to include a speech dialogue interface. <p> In <ref> [Rekimoto and Nagao, 1995] </ref> the NaviCam system is described as a portable computer with video camera which detects pre-tagged objects. Users view the real-world together with context sensitive information generated by the computer. NaviCam is extended in the Ubiquitous Talker [Rekimoto and Nagao, 1995] to include a speech dialogue interface. Other applications include a navigation system, Walk-Navi [Nagao and Rekimoto, 1996]. Audio Aura [Mynatt et al., 1997] is an active badge distributed system that augments the physical world with auditory cues.
Reference: [Rhodes and Starner, 1996] <author> B. Rhodes and T. Star-ner. </author> <title> Remembrance agent: a continuously running automated information retrieval system. </title> <booktitle> In Intl. Conf. on the Practical Application of Intelligent Agents and Multi Agent Technology, </booktitle> <year> 1996. </year>
Reference-contexts: Some closely related memory augmentation systems include the "Forget-me not" system [Lamming and Flynn, 1993], which is a personal information manager inspired by Weiser's ubiquitous computing paradigm, and the Remembrance Agent <ref> [Rhodes and Starner, 1996] </ref>, which is a text-based context-driven wearable augmented reality memory system. Both systems collect and organize data that is relevant to the user for subsequent retrieval.
Reference: [Schiele and Crowley, 1996] <author> B. Schiele and J.L. Crowley. </author> <title> Probabilistic object recognition using multidimensional receptive field histograms. </title> <booktitle> In 13th Intl. Conf. on Pattern Recognition, </booktitle> <volume> Volume B, </volume> <pages> pages 50-54, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: The recognition results are then sent to the audio-visual associative memory system which plays the appropriate clip. The generic object recognition system used by DyPERS has been recently proposed by Schiele and Crowley <ref> [Schiele and Crowley, 1996] </ref>. A major result of their work is that a statistical model based on local object descriptors provides a reliable representation and recognition of object appearances. Objects are represented by multidimensional histograms of vector responses from local neighborhood operators. <p> Simple matching of such histograms (using 2 -statistics or intersection [Schiele and Crowley, 1997]) can be used to determine the most probable object, independently of position, scale and image-plane rotation. Furthermore the approach is considerably robust to view point changes. This technique has been extended to probabilistic object recognition <ref> [Schiele and Crowley, 1996] </ref>, in order to determine the probability of each object given various degrees of occlusion. Experiments showed that only a small portion of the image (between 15% and 30%) is needed in order to recognize 100 objects correctly. <p> With 33.6% visibility the recognition rate is still above 99% (10 errors in total). Using 13.5% of the object the recognition rate is still above 90%. More remarkably, the recognition rate is 76% with only 6.8% visibility of the object. See <ref> [Schiele and Crowley, 1996, Schiele and Crowley, 1997] </ref> for further details. 3.3 Hardware Currently, the system is fully tetherless with wireless radio connections allowing the user to roam around a significant amount of space (i.e. a few office rooms).
Reference: [Schiele and Crowley, 1997] <author> B. Schiele and J.L. Crowley. </author> <title> Recognition without correspondence using multidimensional receptive field histograms. </title> <type> Technical Report 453, </type> <institution> MIT, Media Lab, </institution> <year> 1997. </year>
Reference-contexts: A major result of their work is that a statistical model based on local object descriptors provides a reliable representation and recognition of object appearances. Objects are represented by multidimensional histograms of vector responses from local neighborhood operators. Simple matching of such histograms (using 2 -statistics or intersection <ref> [Schiele and Crowley, 1997] </ref>) can be used to determine the most probable object, independently of position, scale and image-plane rotation. Furthermore the approach is considerably robust to view point changes. <p> With 33.6% visibility the recognition rate is still above 99% (10 errors in total). Using 13.5% of the object the recognition rate is still above 90%. More remarkably, the recognition rate is 76% with only 6.8% visibility of the object. See <ref> [Schiele and Crowley, 1996, Schiele and Crowley, 1997] </ref> for further details. 3.3 Hardware Currently, the system is fully tetherless with wireless radio connections allowing the user to roam around a significant amount of space (i.e. a few office rooms).
Reference: [Starner et al., 1997] <author> T. Starner, S. Mann, B. Rhodes, J. Levine, J. Healey, D. Kirsch, R.W. </author> <title> Picard, and A.P. Pentland. Augmented reality through wearable computing. Presence, Special Issue on Augmented Reality, </title> <year> 1997. </year>
Reference-contexts: The visual cues and the images/messages had to be prepared o*ine and the collection process was not automated. In addition, the vision algorithm used was limited to 2D objects viewed head-on and at appropriate distances. An earlier version <ref> [Starner et al., 1997] </ref> further simplified the machine vision by using colored bar code tags as cues. In [Rekimoto and Nagao, 1995] the NaviCam system is described as a portable computer with video camera which detects pre-tagged objects. <p> The user dons a Sony GlassTron heads-up display with a semi-transparent visor and headphones. Attached to the visor is an ELMO video camera (with wide angle lens) which is aligned as closely as possible with the user's line of sight <ref> [Starner et al., 1997] </ref>. Thus the vision system is directed by the user's head motions to interesting objects. In addition, a nearby microphone is incorporated. The A/V data captured by the camera and microphone is continuously broadcast using a wireless radio transmitter.
Reference: [Turk, 1997] <editor> M. Turk, editor. </editor> <booktitle> Perceptual User Interfaces Workshop Proceedings, </booktitle> <year> 1997. </year>
Reference-contexts: Faster, more natural and convenient means for users to exchange information with computers are needed. This communication bottleneck has spurred increased research in providing perceptual capabilities (speech, vision, haptics) to interfaces. These perceptual interfaces are likely to be a major model for future human-computer interaction <ref> [Turk, 1997] </ref>. 3 System Overview The system's building blocks are depicted in Figure 1. The following describes the audio-visual association module, the object recognition algorithm used and gives a short overview of the hardware. 3.1 Audio-Visual Associative Memory System The audio-visual associative memory operates on a record-and-associate paradigm.
Reference: [Weiser, 1991] <author> M. Weiser. </author> <booktitle> The computer of the twenty-first century. </booktitle> <publisher> Scientific American, </publisher> <year> 1991. </year>
Reference-contexts: Ubiquitous vs. Wearable Computing: Both wearable/personal computing and ubiquitous computing present interesting routes to augmenting human capabilities with computers. However, wearable computers attempt to augment the user directly and provide a mobile platform while ubiquitous computing augments the surrounding physical environment with a network of machines and sensors. Weiser <ref> [Weiser, 1991] </ref> discusses the merits of ubiquitous computing while Mann [Mann, 1997] argues in favor of mobile, personal audio-visual augmentation in his wearable platform. Memory Augmentation: Memory augmentation has evolved from simple pencil and paper paradigms to sophisticated personal digital assistants (PDAs) and beyond.
References-found: 17

