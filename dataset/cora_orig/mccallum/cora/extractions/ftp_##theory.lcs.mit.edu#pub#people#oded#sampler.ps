URL: ftp://theory.lcs.mit.edu/pub/people/oded/sampler.ps
Refering-URL: http://theory.lcs.mit.edu/~oded/complexity.html
Root-URL: 
Email: Email: oded@wisdom.weizmann.ac.il  
Title: A Sample of Samplers: A Computational Perspective on Sampling  
Author: Oded Goldreich 
Keyword: Sampling, randomness complexity, saving randomness, pairwise independent random variables, Expander graphs, random walks on graphs, lower bounds.  
Date: May 26, 1997  
Address: Rehovot, Israel.  
Affiliation: Department of Computer Science and Applied Mathematics Weizmann Institute of Science,  
Abstract: We consider the problem of estimating the average of a huge set of values. That is, given oracle access to an arbitrary function f : f0; 1g n 7! [0; 1], we need to estimate 2 n P x2f0;1g n f(x) upto an additive error of *. We are allowed to employ a randomized algorithm which may err with probability at most ffi. We survey known algorithms for this problem and focus on the ideas underlying their construction. In particular, we present an algorithm which makes O(* 2 log(1=ffi)) queries and uses n+O(log(1=*))+O(log(1=ffi)) coin tosses, both complexities being very close to the corresponding lower bounds. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Ajtai, J. Komlos, E. Szemeredi, </author> <title> "Deterministic Simulation in LogSpace", </title> <booktitle> Proc. 19th STOC, </booktitle> <year> 1987, </year> <pages> pp. 132-140. </pages>
Reference-contexts: 1 Introduction In many settings repeated sampling is used to estimate the average value of a huge set of values. Namely, there is a value function defined over a huge space, say - : f0; 1g n 7! <ref> [0; 1] </ref>, and one wishes to approximate - def 2 n x2f0;1g n -(x) without having to inspect the value of on the entire domain. We comment that it is essential to have the range of be bounded (or else no reasonable approximation may be possible). <p> <ref> [0; 1] </ref>, and one wishes to approximate - def 2 n x2f0;1g n -(x) without having to inspect the value of on the entire domain. We comment that it is essential to have the range of be bounded (or else no reasonable approximation may be possible). Our convention of having [0; 1] be the range of is adopted for simplicity, and the problem for other (predetermined) ranges can be treated analogously. 1.1 Formal Setting Our notion of approximation depends on two parameters: accuracy (denoted *) and error probability (denoted ffi). <p> This leads to the following definition. Definition 1.1 (sampler): A sampler is a randomized algorithm that on input parameters n (length), * (accuracy) and ffi (error), and oracle access to any function - : f0; 1g n 7! <ref> [0; 1] </ref>, outputs, with prob ability at least 1 ffi, a value that is at most * away from - def 2 n x2f0;1g n -(x). Namely, Pr (jsampler - where the probability is taken over the internal coin tosses of the sampler. <p> Actually, the randomness bound can be improved to n + log 2 (1=ffi) log 2 log 2 (1=ffi) while using a constant factor larger sample complexity and more sophisticated techniques [27]. More generally, 3 Theorem 2.3 [27]: For every function s : <ref> [0; 1] </ref> 2 7! R such that s (*; ffi) 2 log 2 (1=ffi) * 2 , there exists a (non-efficient) sampler with sample complexity s (*; ffi) and randomness complexity n + log 2 (1=ffi) + 2 log 2 (4=*) log 2 s (*; ffi) This gets us very close <p> * 2 , there exists a (non-efficient) sampler with sample complexity s (*; ffi) and randomness complexity n + log 2 (1=ffi) + 2 log 2 (4=*) log 2 s (*; ffi) This gets us very close to the following lower bound Theorem 2.4 [10]: Let s : N fi <ref> [0; 1] </ref> 2 7! R. <p> Similarly, one should not be surprised at the effect of 1 2* on the bound: For example, when * = 0:5, a sample may merely output ~- = 1 2 as its estimate and be within * of the average of any function - : f0; 1g n 7! <ref> [0; 1] </ref>. <p> The Pairwise-Independent Sampler To motivate the Pairwise-Independent Sampler, let us confront two well-known central limit theorems: Chernoff Bound which refers to totally independent random variables and Chebishev's In equality which refers to pairwise-independent random variables Chernoff Bound: Let 1 ; :::; m be totally independent random variables, each ranging in <ref> [0; 1] </ref> and having expected value . Then, Pr fi fi m i=1 fi fi fi ! Chebishev's Inequality: Let 1 ; :::; m be pairwise-independent random variables, each ranging in [0; 1] and having expected value . <p> to pairwise-independent random variables Chernoff Bound: Let 1 ; :::; m be totally independent random variables, each ranging in <ref> [0; 1] </ref> and having expected value . Then, Pr fi fi m i=1 fi fi fi ! Chebishev's Inequality: Let 1 ; :::; m be pairwise-independent random variables, each ranging in [0; 1] and having expected value . Then, Pr fi fi m i=1 fi fi fi ! 1 4 Our conclusion is that these two bounds essentially agree when m = O (1=* 2 ). <p> Then, there exists an efficient sampler with sample complexity s (n + log 2 (1=*); *=2; ffi) and randomness complexity r (n + log 2 (1=*); *=2; ffi). Proof: As a mental experiment, given an arbitrary function - : f0; 1g n 7! <ref> [0; 1] </ref>, we define a Boolean function : f0; 1g n+` 7! [0; 1], where ` def = log 2 (1=*), as follows: For i = 1; :::; * 1 , (x; i) def only if -(x) &lt; (i 0:5) *. <p> Proof: As a mental experiment, given an arbitrary function - : f0; 1g n 7! <ref> [0; 1] </ref>, we define a Boolean function : f0; 1g n+` 7! [0; 1], where ` def = log 2 (1=*), as follows: For i = 1; :::; * 1 , (x; i) def only if -(x) &lt; (i 0:5) *. Then, j-(x) * P 1=* i=1 (x; i)j &lt; *=2. <p> That is, given access to a function - : f0; 1g n 7! <ref> [0; 1] </ref>, and determining a sequence r of coins for the first sampler, we consider the function -r : [m] 7! [0; 1] defined by letting -r (i) = -(q r;i ) where q r;i is the i th query made by the first sampler on coins r. <p> That is, given access to a function - : f0; 1g n 7! <ref> [0; 1] </ref>, and determining a sequence r of coins for the first sampler, we consider the function -r : [m] 7! [0; 1] defined by letting -r (i) = -(q r;i ) where q r;i is the i th query made by the first sampler on coins r. We run the second sampler providing it virtual access to the function -r in the obvious manner, and output its output. <p> Boolean Samplers vs general ones: Another fact presented in Figure 2 is that we can currently do better if we are guaranteed that the oracle function is Boolean (rather than mapping to the interval <ref> [0; 1] </ref>). We stress that the lower bound holds also with respect to samplers which need only to work for Boolean functions. Adaptive vs non-adaptive: All known samplers are non-adaptive; that it, they determine the sample points (queries) as a function of their coin tosses. <p> generally, Theorem 6.2 [6]: For every fl &gt; 0 and ffi, there exists a polynomial p and a deterministic algorithm which for any n; * and any (p (n=*); p (n=*) fl )-source X, given input (n; *; X) and access to any oracle - : f0; 1g n 7! <ref> [0; 1] </ref>, runs in time poly (n=*) and outputs a value ~- so that Pr (j~- -j &gt; *) &lt; 2 n ffi Acknowledgments I would like to thank Noga Alon, Nabil Kahale, and Luca Trevisan for useful discussions.
Reference: [2] <author> N. Alon, </author> <title> "Eigenvalues, Geometric Expanders, Sorting in Rounds and Ramsey Theory", </title> <journal> Combinatorica, </journal> <volume> 6 (1986), </volume> <pages> pp. 231-243. </pages>
Reference: [3] <author> N. Alon, J. Bruck, J. Naor, M. Naor and R. Roth, </author> <title> "Construction of Asymptotically Good, Low-Rate Error-Correcting Codes through Pseudo-Random Graphs", </title> <journal> IEEE Transactions on Information Theory 38 (1992), </journal> <pages> pp. 509-516. </pages>
Reference: [4] <author> N. Alon and V.D. Milman, </author> <title> 1 , Isoperimetric Inequalities for Graphs and Superconcentrators, </title> <journal> J. Combinatorial Theory, Ser. </journal> <volume> B 38 (1985), </volume> <pages> pp. 73-88. </pages>
Reference: [5] <author> N. Alon and J.H. Spencer, </author> <title> The Probabilistic Method, </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1992. </year>
Reference: [6] <author> A.E. Andreev, A.E.F. Clementi, J.D.P. Rolin and L. Trevisan, </author> <title> "Weak Random Sources, Hitting Sets, and BPP Simulation", </title> <type> manuscript, </type> <month> February </month> <year> 1997. </year>
Reference-contexts: That is, m is a lower bound on the min-entropy of X defined as min ff2f0;1g ` f log 2 (P rob (X = ff))g. A recent result <ref> [6] </ref> states that any BPP-algorithm can be converted to work with very weak sources. Specifically, for any fl &gt; 0, there exists a robust BPP-algorithm working with any (`; ` fl )-source. More generally, Theorem 6.2 [6]: For every fl &gt; 0 and ffi, there exists a polynomial p and a <p> A recent result <ref> [6] </ref> states that any BPP-algorithm can be converted to work with very weak sources. Specifically, for any fl &gt; 0, there exists a robust BPP-algorithm working with any (`; ` fl )-source. More generally, Theorem 6.2 [6]: For every fl &gt; 0 and ffi, there exists a polynomial p and a deterministic algorithm which for any n; * and any (p (n=*); p (n=*) fl )-source X, given input (n; *; X) and access to any oracle - : f0; 1g n 7! [0; 1], runs in
Reference: [7] <author> M. Bellare, O. Goldreich, and S. </author> <title> Goldwasser "Randomness in Interactive Proofs", </title> <journal> Computational Complexity, </journal> <volume> Vol. 4, No. 4 (1993), </volume> <pages> pp. 319-354. </pages> <note> Extended abstract in 31st FOCS, </note> <year> 1990, </year> <pages> pp. 318-326. </pages>
Reference-contexts: Specifically, we use random walks on expander graphs (cf., Appendix A) to generate a sequence of ` "seeds" each of length O (n). Each seed is used to generate a sequence of m pairwise independent elements in f0; 1g n , as above. Theorem 4.1 (general median-composition <ref> [7] </ref>): Suppose we are given an efficient sampler of sample complexity s (n; *; ffi) and randomness complexity r (n; *; ffi). <p> Item 2 follows. Combining the Pairwise-Independent Sampler with Theorem 4.1, we get Corollary 4.2 (The Median-of-Averages Sampler <ref> [7] </ref>): There exists an efficient sampler with * Sample Complexity: O ( log (1=ffi) * 2 ). * Randomness Complexity: O (n + log (1=ffi)). Furthermore, we can obtain randomness complexity 2n + (4 + o (1)) log 2 (1=ffi)). <p> In the next section, we further reduce the randomness complexity of samplers to n + O (log (1=*) + log (1=ffi)), while maintaining the sample complexity (up-to a multiplicative constant). 5 The Expander Sampler and two Generic Techniques The main result of this section is Theorem 5.1 <ref> [7, 17] </ref>: There exists an efficient sampler which has * Sample Complexity: O ( log (1=ffi) * 2 ). * Randomness Complexity: n + log 2 (1=*) + O (log (1=ffi)).
Reference: [8] <author> M. Bellare, O. Goldreich, and S. Goldwasser. </author> <note> Addendum to [7], available from http://theory.lcs.mit.edu/~oded/papers.html, May 1997. </note>
Reference: [9] <author> M. Bellare, and J. Rompel, </author> <title> "Randomness-efficient oblivious sampling", </title> <booktitle> 35th FOCS, </booktitle> <year> 1994. </year>
Reference-contexts: Yet, it will be nice to have a direct and more tight proof of the above intuition. Averaging (or Oblivious) Samplers: A special type of non-adaptive samplers are ones which output the average value of the function over their sample points. Such samplers were first defined in <ref> [9] </ref> and called "oblivious". We prefer the term averaging. Averaging samplers have some applications not offered by arbitrary non-adaptive samplers (cf., [9] and [25]). More importantly, averaging samplers are very appealing since averaging over a sample seem the natural thing to do. <p> Such samplers were first defined in <ref> [9] </ref> and called "oblivious". We prefer the term averaging. Averaging samplers have some applications not offered by arbitrary non-adaptive samplers (cf., [9] and [25]). More importantly, averaging samplers are very appealing since averaging over a sample seem the natural thing to do. Furthermore, as pointed out in [27], averaging samplers are related to dispersers and to randomness extractors.
Reference: [10] <author> R. Canetti, G. Even and O. Goldreich, </author> <title> "Lower Bounds for Sampling Algorithms for Estimating the Average", </title> <journal> IPL, </journal> <volume> Vol. 53, </volume> <pages> pp. 17-25, </pages> <year> 1995. </year>
Reference-contexts: The following theorem is analogous to many results known in statistics, though we are not aware of a prior reference where it can be found. Theorem 2.1 <ref> [10] </ref>: Any sampler has sample complexity bounded below by min 2 (n4)=2 ; 4* 2 provided * 1 8 and ffi 1 Note that a (constant factor) gap remains between the lower bound asserted here and the upper bound established by the Naive Sampler. <p> First evidence towards our claim is provided by a non-explicit (and so inefficient) sampler: Theorem 2.2 <ref> [10] </ref>: There exists a (non-efficient) sampler with sample complexity 1+ln (1=ffi) 2* 2 and randomness complexity n + 2 log 2 (2=ffi) + log 2 log 2 (1=*). <p> (*; ffi) 2 log 2 (1=ffi) * 2 , there exists a (non-efficient) sampler with sample complexity s (*; ffi) and randomness complexity n + log 2 (1=ffi) + 2 log 2 (4=*) log 2 s (*; ffi) This gets us very close to the following lower bound Theorem 2.4 <ref> [10] </ref>: Let s : N fi [0; 1] 2 7! R. <p> Using Theorem 2.4, we obtain a lower bound on the randomness complexity of any sample-optimal sampler: Corollary 2.5 <ref> [10] </ref>: Any sampler which has sample complexity O ( log (1=ffi) * 2 ), has randomness complexity bounded below by n + (1 o (1)) log 2 (1=ffi) 2 log 2 (1=*) provided *; ffi &lt; 0:4 and log (1=ffi) The exact bound is n + log 2 (1=ffi) 2 log
Reference: [11] <author> L. Carter and M. Wegman, </author> <title> "Universal Classes of Hash Functions", </title> <journal> J. Computer and System Sciences, </journal> <volume> Vol. 18, </volume> <pages> pp. </pages> <month> 143-154 </month> <year> (1979). </year>
Reference: [12] <author> B. Chor and O. Goldreich, </author> <title> "Unbiased Bits from Sources of Weak Randomness and Probabilistic Communication Complexity", </title> <journal> SIAM J. Comput., </journal> <volume> Vol. 17, No. 2, </volume> <month> April </month> <year> 1988, </year> <pages> pp. 230-261. </pages>
Reference-contexts: A question which has received a lot of attention in the last decade is whether algorithms can be transformed into robust counterparts which may work given also "weak random sources" (cf., e.g., [26]). Following <ref> [12] </ref>, we call a random variable X a (`; m)-source if its support is a subset of f0; 1g ` and no string in its support is assigned probability mass greater than 2 m .
Reference: [13] <author> B. Chor and O. Goldreich, </author> <title> "On the Power of Two-Point Based Sampling," </title> <journal> Jour. of Complexity, </journal> <volume> Vol 5, </volume> <year> 1989, </year> <pages> pp. 96-106. </pages>
Reference-contexts: Furthermore, in the first case we may be save a lot in terms of randomness. The Pairwise-Independent Sampler <ref> [13] </ref>: On input parameters n, * and ffi, set m def 4* 2 ffi and generate a sequence of m pairwise-independently and uniformly distributed strings in f0; 1g n , denoted s 1 ; :::; s m .
Reference: [14] <author> A. Cohen and A. Wigderson, "Dispensers, </author> <title> Deterministic Amplification, and Weak Random Sources", </title> <booktitle> 30th FOCS, </booktitle> <year> 1989, </year> <pages> pp. 14-19. </pages>
Reference: [15] <author> O. Gaber and Z. Galil, </author> <title> "Explicit Constructions of Linear Size Superconcentrators", </title> <journal> JCSS, </journal> <volume> 22 (1981), </volume> <pages> pp. 407-420. </pages>
Reference: [16] <author> O. Goldreich, R. Impagliazzo, L.A. Levin, R. Venkatesan, and D. Zuckerman, </author> <title> "Security Preserving Amplification of Hardness", </title> <booktitle> 31st FOCS, </booktitle> <pages> pp. 318-326, </pages> <year> 1990. </year>
Reference: [17] <author> O. Goldreich and A. Wigderson, </author> <title> "Tiny Families of Functions with Random Properties: </title>
Reference-contexts: In the next section, we further reduce the randomness complexity of samplers to n + O (log (1=*) + log (1=ffi)), while maintaining the sample complexity (up-to a multiplicative constant). 5 The Expander Sampler and two Generic Techniques The main result of this section is Theorem 5.1 <ref> [7, 17] </ref>: There exists an efficient sampler which has * Sample Complexity: O ( log (1=ffi) * 2 ). * Randomness Complexity: n + log 2 (1=*) + O (log (1=ffi)). <p> The algorithm has * Sample Complexity: O ( 1 ffi* 2 ). * Randomness Complexity: n. * Computational Complexity: polynomial in n, * 1 and ffi 1 . Lemma 5.3 <ref> [17] </ref>: The above algorithm constitutes an efficient Boolean sampler. Proof: We denote by B the set of bad choices for the algorithm; namely, the set of vertices that once selected by the algorithm yield a wrong estimate. <p> Using a similar argument, we can show that (B n B 0 ) ffi (1 (A)). Thus, (B) ffi and the claim follows. Comment 5.4 <ref> [17] </ref>: Observe that if we were to use an arbitrary d-regular graph with second eigen value then the above proof would hold provided that ffi* 2 (6) This would have yield an efficient Boolean sampler with sample complexity d and randomness complexity n. 9 5.2 From Boolean Samplers to General Samplers
References-found: 17

