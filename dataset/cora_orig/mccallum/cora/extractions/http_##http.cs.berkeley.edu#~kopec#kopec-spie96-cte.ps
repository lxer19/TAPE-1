URL: http://http.cs.berkeley.edu/~kopec/kopec-spie96-cte.ps
Refering-URL: http://elib.cs.berkeley.edu:8080/admin/quarterly_reports/report.95.html
Root-URL: 
Title: Document-Specific Character Template Estimation century newspapers and connected text in a script-like font. Three applications
Author: Gary E. Kopec Mauricio Lomelin 
Note: degraded images of 19 th  
Affiliation: Xerox PARC  Microsoft Corp.  
Abstract: An approach to supervised training of document-specific character templates from sample page images and unaligned transcriptions is presented. The template estimation problem is formulated as one of constrained maximum likelihood parameter estimation within the document image decoding (DID) framework. This leads to a two-phase iterative training algorithm consisting of transcription alignment and aligned template estimation (ATE) steps. The ATE step is the heart of the algorithm and involves assigning template pixel colors to maximize likelihood while satisifying a template disjointness constraint. The training algorithm is demonstrated on a variety of English documents, including newspaper columns, 15 th century books,
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adobe Systems Inc, </author> <title> PostScript Language Tutorial and Cookbook, </title> <address> Reading: </address> <publisher> Addison-Wesley, </publisher> <year> 1985. </year>
Reference-contexts: Given a set of bitmap templates and set width estimates (e.g. computed from glyph positions, as described previously), it is relatively straightforward to construct a font suitable for use by a printer. For example, the bitmaps can be used directly in a PostScript bitmap font program, as described in <ref> [1] </ref>. Alternatively, an outline font can be created by fitting curves to the template foreground boundaries. Fig. 12 (a) shows an example in which fonts estimated from a decoded document are used to construct a drycleaned version of the document.
Reference: [2] <author> H. Baird and G. Nagy, </author> <title> A self-correcting 100-font classifier, in Document Recognition, </title> <editor> L. Vincent and T. Pavlidis, editors, </editor> <booktitle> Proc. SPIE vol. </booktitle> <volume> 2181, </volume> <pages> pp. 106-115, </pages> <year> 1994. </year>
Reference-contexts: The use of document-specific models is motivated, in part, by the practical observation that the error rate of a font-specific recognizer can be significantly less than that of a multi-font system <ref> [2] </ref>. Thus, in many cases the cost of constructing specialized character models will be more than offset by a reduced need for post-recognition proofreading and error correction.
Reference: [3] <author> California Dept. </author> <title> of Water Resources, General Comparison of Water District Acts, </title> <journal> Bulletin 155-94, </journal> <month> March, </month> <year> 1994. </year>
Reference-contexts: We will illustrate the accuracy improvement possible by summarizing our experience creating a high-accuracy transcription for one of the documents in the UC Berkeley Environmental Digital Library [8, 14]. Bulletin 155 (B155) <ref> [3] </ref> is a report by the California Department of Water Resources (DWR) that summarizes the provisions of 157 state water district acts. The information about the acts is presented in 375 pages of two-column tables (543,779 glyphs).
Reference: [4] <author> F. Chen, D. Bloomberg and L. Wilcox, </author> <title> Spotting phrases in lines of imaged text, Document Recognition II, </title> <editor> L. Vincent and H. Baird, editors, </editor> <booktitle> Proc. SPIE vol. </booktitle> <volume> 2422, </volume> <pages> pp. 256-269, </pages> <year> 1995. </year>
Reference-contexts: The success of automatic speech recognition systems based on hidden Markov models (HMMs) has motivated recent attempts to develop HMM-based methods for printed text recognition <ref> [10, 4] </ref>. One of the main advantages of the HMM approach is that maximum likelihood estimation of character model parameters can be carried out using whole word or text line images with unaligned transcriptions. This eliminates the need to segment the training data into individual glyph images prior to training.
Reference: [5] <author> P. Chou and G. Kopec, </author> <title> Stochastic attribute grammar model of document production and its use in document image decoding, in Document Recognition II, </title> <editor> L. Vincent and H. Baird, editors, </editor> <booktitle> Proc. SPIE vol. </booktitle> <volume> 2422, </volume> <pages> pp. 66-73, </pages> <year> 1995. </year>
Reference-contexts: The use of bitmap character templates in DID is also natural from a theoretical perspective because DID is based on explicitly modeling the processes of document image creation and degradation. DID document models take the form of attributed grammars that can be used to synthesize two-dimensional document images <ref> [5] </ref>. Bitmap templates are the terminal image elements in these grammars just as bitmap font characters are the primitive image elements in many digital typography systems. The traditional approach to creating a character template is to average and threshold a set of aligned glyph images for the character.
Reference: [6] <author> T. Fruchterman, DAFS: </author> <title> a standard for document and image understanding, </title> <booktitle> Proc. 1995 Symposium on Document Image Understanding Technology, </booktitle> <address> Bowie, MD, </address> <month> Oct. </month> <pages> 24-25, </pages> <year> 1995, </year> <pages> pp. 94-100. </pages>
Reference-contexts: Manual segmentation and labeling is a simple and reliable approach that is applicable to a wide variety of document types. However, it can be very expensive, even with powerful interactive tools such as those described in <ref> [6] </ref>. Thus, automated methods of character template estimation are required if the use of document-specific templates is to become cost-effective over a wide spectrum of applications. <p> A simple way to obtain initial templates is to excise a single sample of each required character from the training images using an interactive tool <ref> [6] </ref>. Alternatively, initial templates can be created from an existing bitmap or outline font. Fig. 8 shows a set of Helvetica templates synthesized using a PostScript font program. The newspaper templates shown in fig. 6 were obtained from these templates after five iterations, using manually constructed transcriptions.
Reference: [7] <author> G. Kopec and P. Chou, </author> <title> Document image decoding using Markov source models, </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 16, no. 6, </volume> <month> June, </month> <year> 1994, </year> <pages> pp. 602-617. </pages>
Reference-contexts: 1. INTRODUCTION Document image decoding (DID) is an approach to document recognition that is based on a communication theory view of the processes of document creation, transmission and recognition <ref> [7] </ref>. A distinctive feature of the DID methodology is its focus on document-specific bitmap templates as character models, rather than the omni-font feature-based models used by other approaches. <p> A valid fi is required to satisfy the template disjointness constraint, which can be informally stated as the requirement that the shifted templates Q t i [~x i ] on the right hand side of (1) have disjoint support <ref> [7] </ref>. The disjointness constraint is motivated by the observation that typefaces are normally designed so that adjacent characters in a line of printed text do not overlap. <p> For the case of Markov image sources and the asymmetric bit-flip channel model commonly used in DID <ref> [7] </ref>, it is not difficult to show that (2) is well approximated by ^ fi = arg max max [L (Z j Q ;fi ) + log Pr fg] (3) where L (Z j Q ;fi ) = fl kQ ;fi ^ Zk + fi kQ ;fi k (4) *** Kopec
Reference: [8] <author> G. Kopec, </author> <title> Document Image Decoding in the Berkeley Digital Library Project, </title> <booktitle> IS&T/SPIE 1996 Intl. Symposium on Electronic Imaging: Science & Technology, </booktitle> <address> San Jose, CA, Jan. 27-Feb. 2, </address> <year> 1996. </year>
Reference-contexts: We will illustrate the accuracy improvement possible by summarizing our experience creating a high-accuracy transcription for one of the documents in the UC Berkeley Environmental Digital Library <ref> [8, 14] </ref>. Bulletin 155 (B155) [3] is a report by the California Department of Water Resources (DWR) that summarizes the provisions of 157 state water district acts. The information about the acts is presented in 375 pages of two-column tables (543,779 glyphs).
Reference: [9] <author> G. Kopec and M. Lomelin, </author> <title> Document image decoding approach to character template estimation, </title> <journal> submitted to IEEE. Trans. Pattern Analysis and Machine Intelligence, </journal> <month> Nov., </month> <year> 1995. </year>
Reference-contexts: This paper presents a brief overview of a recently proposed approach to supervised maximum likelihood template estimation that is applicable to the general sidebearing model of character shape and positioning used in DID <ref> [9, 11] </ref>. The inputs to the training procedure are images of whole pages or text lines plus the corresponding unaligned transcriptions. The method does not assume that the training data can be segmented into individual glyphs by rectangular subdivision. <p> A complete path or a set of labeled glyph positions defines a composite ideal image Q 1 This paper focuses on bilevel templates. A more general class of multilevel templates is discussed in <ref> [9] </ref>. *** Kopec and Lomelin, SPIE '96 *** 3 template coordinate system. by P 1 [ Q t i [~x i ] (1) where Q t i [~x i ] denotes Q t i shifted so that the origin of its local coordinate system is located at ~x i and S <p> The general case of maximizing (5) subject to the template disjointness constraint is an NP-complete problem. Indeed, the problem remains NP-complete even if the disjointness constraint is removed. Thus, a practical (polynomial time) ATE 2 The template in the upper left corner is a write-black space template, discussed in <ref> [9, 11] </ref>. *** Kopec and Lomelin, SPIE '96 *** 6 The templates in each row (column) are aligned with their origins on a horizontal (vertical) line. procedure (B; C; Z) do begin q t (~x) := 0 8t 2 B; ~x 2 C while Q 6= ; do begin (s; ~w)
Reference: [10] <author> S. Kuo and O. Agazzi, </author> <title> Keyword spotting in poorly printed documents using pseudo 2-d hidden Markov models, </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 16, no. 8, </volume> <month> Aug., </month> <year> 1994, </year> <pages> pp. 842-848. </pages>
Reference-contexts: The success of automatic speech recognition systems based on hidden Markov models (HMMs) has motivated recent attempts to develop HMM-based methods for printed text recognition <ref> [10, 4] </ref>. One of the main advantages of the HMM approach is that maximum likelihood estimation of character model parameters can be carried out using whole word or text line images with unaligned transcriptions. This eliminates the need to segment the training data into individual glyph images prior to training.
Reference: [11] <author> M. Lomelin, </author> <title> Character Template Estimation from Document Images and Their Transcriptions, </title> <type> S.M. thesis, </type> <institution> M.I.T., </institution> <month> June, </month> <year> 1995. </year>
Reference-contexts: This paper presents a brief overview of a recently proposed approach to supervised maximum likelihood template estimation that is applicable to the general sidebearing model of character shape and positioning used in DID <ref> [9, 11] </ref>. The inputs to the training procedure are images of whole pages or text lines plus the corresponding unaligned transcriptions. The method does not assume that the training data can be segmented into individual glyphs by rectangular subdivision. <p> The general case of maximizing (5) subject to the template disjointness constraint is an NP-complete problem. Indeed, the problem remains NP-complete even if the disjointness constraint is removed. Thus, a practical (polynomial time) ATE 2 The template in the upper left corner is a write-black space template, discussed in <ref> [9, 11] </ref>. *** Kopec and Lomelin, SPIE '96 *** 6 The templates in each row (column) are aligned with their origins on a horizontal (vertical) line. procedure (B; C; Z) do begin q t (~x) := 0 8t 2 B; ~x 2 C while Q 6= ; do begin (s; ~w)
Reference: [12] <institution> San Francisco Chronicle, </institution> <month> Oct. 6, </month> <year> 1993. </year>
Reference-contexts: Fig. 3 (b) shows a collection of glyph image regions for the character a drawn from a set of scanned newspaper columns <ref> [12] </ref>; a portion of one column is shown in fig. 3 (a). As the figure illustrates, a glyph image region typically contains glyphs and/or parts of glyphs in addition to the glyph located at the canvas origin.
Reference: [13] <author> H. Stabler, </author> <title> Experiences with high-volume, high-accuracy document capture, in Document Analysis Systems, </title> <editor> L. Spitz and A. Dengel, eds., </editor> <address> 1995, Singapore: </address> <publisher> World Scientific Publishing. </publisher>
Reference-contexts: Transcription Error Visualization. Many document capture applications have accuracy requirements that are well beyond the capabilities of current OCR technology, including document-specific methods. In such situations it is necessary to perform extensive manual proof-checking, a task than can represent 70% of the total labor cost of a conversion effort <ref> [13] </ref>. Thus, improved tools for transcription error detection and correction are important. Error visualization is a means of detecting errors in a transcription by comparing an image of the transcription with the original document image.
Reference: [14] <author> R. Wilensky, </author> <title> Toward work-centered digital information services, </title> <note> to appear in IEEE Computer, special issue on Building Large-scale Digital Libraries, </note> <month> May, </month> <year> 1996. </year>
Reference-contexts: We will illustrate the accuracy improvement possible by summarizing our experience creating a high-accuracy transcription for one of the documents in the UC Berkeley Environmental Digital Library <ref> [8, 14] </ref>. Bulletin 155 (B155) [3] is a report by the California Department of Water Resources (DWR) that summarizes the provisions of 157 state water district acts. The information about the acts is presented in 375 pages of two-column tables (543,779 glyphs).
References-found: 14

