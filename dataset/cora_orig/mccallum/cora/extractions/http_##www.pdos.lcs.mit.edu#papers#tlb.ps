URL: http://www.pdos.lcs.mit.edu/papers/tlb.ps
Refering-URL: http://www.pdos.lcs.mit.edu/PDOS-papers.html
Root-URL: 
Title: Software Prefetching and Caching for Translation Lookaside Buffers  
Author: Kavita Bala M. Frans Kaashoek William E. Weihl 
Address: Cambridge, MA 02139, USA  
Affiliation: MIT Laboratory for Computer Science  
Date: November 1994  
Note: Appears in Proceedings of the First Symposium on Operating System Design and Implementation,  
Abstract: A number of interacting trends in operating system structure, processor architecture, and memory systems are increasing both the rate of translation lookaside buffer (TLB) misses and the cost of servicing a miss. This paper presents two novel software schemes, implemented under Mach 3.0, to decrease both the number and the cost of kernel TLB misses (i.e., misses on kernel data structures, including user page tables). The first scheme is a new use of prefetching for TLB entries on the IPC path, and the second scheme is a new use of software caching of TLB entries for hierarchical page table organizations. For a range of applications, prefetching decreases the number of kernel TLB misses by 40% to 50%, and caching decreases TLB penalties by providing a fast path for over 90% of the misses. Our caching scheme also decreases the number of nested TLB traps due to the page table hierarchy, reducing the number of kernel TLB miss traps for applications by 20% to 40%. Prefetching and caching, when used alone, each improve application performance by up to 3.5%; when used together, they improve application performance by up to 3%. On synthetic benchmarks that involve frequent communication among several different address spaces (and thus put more pressure on the TLB), prefetching improves overall performance by about 6%, caching improves overall performance by about 10%, and the two used together improve overall performance by about 12%. Our techniques are very effective in reducing kernel TLB penalties, which currently range from 1% to 5% of application runtime for the benchmarks studied. Since processor speeds continue to increase relative to memory speeds, our schemes should be even more effective in improving application performance in future architectures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Anderson, H. Levy, B. Bershad, and E. Lazowska. </author> <title> The interaction of architecture and operating system design. </title> <booktitle> In Proceedings of the 4th Conference on Architectural Support for Programming Languages and Systems, </booktitle> <pages> pages 108-119, </pages> <address> Santa Clara, CA, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: In addition, recent commercial standards, such as OLE [9] and OpenDoc [21], place an increasing emphasis on inter- application communication. The net effect of all these factors is that the impact of TLB misses on overall system performance is increasing <ref> [1, 7] </ref>. While TLB penalties have been recognized as a problem, proposed solutions typically require expensive hardware [17, 16]. We present two schemes to address this problem that rely only on software mechanisms; no additional hardware is required. Our techniques reduce both the number and cost of TLB misses.
Reference: [2] <author> B. Bershad, C. Chambers, S. Eggers, C. Maeda D., McNamee, P. Pardyak, S. Savage, and E. Sire. </author> <title> SPIN an ex-tensible microkernel for application-specific operating sys-tem services. </title> <type> Technical Report TR94-03-03, </type> <institution> University of Washington, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: Our experiments assume a microkernel-based operating system. However, our techniques can be applied to other operating system organizations, such as large single address space systems [6] and systems with software segmentation [24]. Furthermore, with the current emphasis on application-controlled resource management <ref> [2, 10] </ref>, our prefetching techniques could become even more effective, since the prefetching strategy can be tailored for individual applications. Prefetching can also be integrated with other VM functions such as prefetching cache entries. Section 2 presents some background material on page table organizations and VM management.
Reference: [3] <author> B.N. Bershad. </author> <title> The increasing irrelevance of IPC perfor-mance for microkernel-based operating systems. </title> <booktitle> In USENIX Workshop on Microkernels and Other Kernel Architectures, </booktitle> <pages> pages 205-211, </pages> <address> Seattle, WA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: This penalty becomes even more significant as CPU speeds increase relative to memory speeds, since the cost of accessing page table entries is growing [18]. As researchers continue to improve inter-process communication (IPC) performance [4, 15], TLB penalties will become an increasing fraction of the IPC cost <ref> [3, 11] </ref>. In addition, recent commercial standards, such as OLE [9] and OpenDoc [21], place an increasing emphasis on inter- application communication. The net effect of all these factors is that the impact of TLB misses on overall system performance is increasing [1, 7].
Reference: [4] <author> B.N. Bershad, T.E. Anderson, E.D. Lazowska, and H.M. Levy. </author> <title> Lightweight remote procedure call. </title> <booktitle> In Proceedings of the 12th Symposium on Operating Systems Principles, </booktitle> <pages> pages 102-113, </pages> <address> Litchfield Park, AZ, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: However, the penalty for a TLB miss increases. This penalty becomes even more significant as CPU speeds increase relative to memory speeds, since the cost of accessing page table entries is growing [18]. As researchers continue to improve inter-process communication (IPC) performance <ref> [4, 15] </ref>, TLB penalties will become an increasing fraction of the IPC cost [3, 11]. In addition, recent commercial standards, such as OLE [9] and OpenDoc [21], place an increasing emphasis on inter- application communication.
Reference: [5] <author> J. Boykin, D. Kirschen, A. Langerman, and S. LoVerso. </author> <title> Programming under Mach. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1993. </year>
Reference-contexts: We decided to dynamically prefetch these entries on the IPC path between processes. TLB misses on L2 entries for the data segment of processes were also frequent. Mach supports sparse memory allocation, and therefore permits allocation of memory in non-contiguous locations of the virtual address space of a process <ref> [5] </ref>. While this is a useful feature of the VM system, it is not easy to dynamically determine the location of the data segment of a process.
Reference: [6] <author> J.S. Chase, H.M. Levy, E.D. Lazowska, and M. BakerHarvey. </author> <title> Lightweight shared objects in a 64-bit operating sys-tem. </title> <booktitle> In Proceedings of the Conference on Object-Oriented Programming Systems, Languages and Applications, </booktitle> <pages> pages 397-413, </pages> <address> Vancouver, Canada, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Our experiments assume a microkernel-based operating system. However, our techniques can be applied to other operating system organizations, such as large single address space systems <ref> [6] </ref> and systems with software segmentation [24]. Furthermore, with the current emphasis on application-controlled resource management [2, 10], our prefetching techniques could become even more effective, since the prefetching strategy can be tailored for individual applications. Prefetching can also be integrated with other VM functions such as prefetching cache entries.
Reference: [7] <author> B. Chen and B. Bershad. </author> <title> The impact of operating system structure on memory system performance. </title> <booktitle> In Proceedingsof the 14th Symposium on Operating Systems Principles, </booktitle> <pages> pages 120-133, </pages> <address> Asheville, NC, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: In addition, recent commercial standards, such as OLE [9] and OpenDoc [21], place an increasing emphasis on inter- application communication. The net effect of all these factors is that the impact of TLB misses on overall system performance is increasing <ref> [1, 7] </ref>. While TLB penalties have been recognized as a problem, proposed solutions typically require expensive hardware [17, 16]. We present two schemes to address this problem that rely only on software mechanisms; no additional hardware is required. Our techniques reduce both the number and cost of TLB misses. <p> Table 2 lists some relevant statistics for each benchmark. For a full description of the benchmarks refer to [16]. Network traffic, Mach's random page replacement policy <ref> [7] </ref>, and the aging of the kernel [22], result in variability in application runtimes. The machine was taken off the network to eliminate variability due to network traffic. As the system stays up for a long time, the number of L1K misses increases.
Reference: [8] <author> Digital Equipment Corporation. </author> <title> DECstation and DECsystem 5000 Model 240 Technical Overview. </title> <year> 1991. </year>
Reference-contexts: Therefore we have, Trap time = i + m fi t. The time to service a cache miss on the DECstation 5000/240 is t = 24 cycles <ref> [8] </ref>. On a machine with a clock speed of 200 MHz and the same memory system, the time to service a cache miss will be (200/40) fi 24 = 120 cycles.
Reference: [9] <author> Microsoft Corporation. </author> <title> Microsoft OLE programmer's refer-ence. </title> <publisher> Microsoft Press, </publisher> <year> 1993. </year>
Reference-contexts: As researchers continue to improve inter-process communication (IPC) performance [4, 15], TLB penalties will become an increasing fraction of the IPC cost [3, 11]. In addition, recent commercial standards, such as OLE <ref> [9] </ref> and OpenDoc [21], place an increasing emphasis on inter- application communication. The net effect of all these factors is that the impact of TLB misses on overall system performance is increasing [1, 7].
Reference: [10] <author> D. Engler, M. F. Kaashoek, and J. O'Toole. </author> <title> The operating system kernel as a secure programmable machine. </title> <booktitle> In Proceedings of the 6th European SIGOPS Workshop, </booktitle> <address> Germany, </address> <month> September </month> <year> 1994. </year> <month> 10 </month>
Reference-contexts: Our experiments assume a microkernel-based operating system. However, our techniques can be applied to other operating system organizations, such as large single address space systems [6] and systems with software segmentation [24]. Furthermore, with the current emphasis on application-controlled resource management <ref> [2, 10] </ref>, our prefetching techniques could become even more effective, since the prefetching strategy can be tailored for individual applications. Prefetching can also be integrated with other VM functions such as prefetching cache entries. Section 2 presents some background material on page table organizations and VM management.
Reference: [11] <author> W. Hsieh, M. F. Kaashoek, and W. E. Weihl. </author> <title> The persistent relevance of IPC performance: New techniques for reducing the IPC penalty. </title> <booktitle> In Proceedings of the 4th Workshop on Workstation Operating Systems, </booktitle> <pages> pages 186-190, </pages> <address> Napa, CA, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: This penalty becomes even more significant as CPU speeds increase relative to memory speeds, since the cost of accessing page table entries is growing [18]. As researchers continue to improve inter-process communication (IPC) performance [4, 15], TLB penalties will become an increasing fraction of the IPC cost <ref> [3, 11] </ref>. In addition, recent commercial standards, such as OLE [9] and OpenDoc [21], place an increasing emphasis on inter- application communication. The net effect of all these factors is that the impact of TLB misses on overall system performance is increasing [1, 7].
Reference: [12] <author> J. Huck and J. Hays. </author> <title> Architectural support for translation table management in large address space machines. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 39-50, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: To the best of our knowledge, this paper is the first to present a technique to reduce nested TLB traps in hierarchical page table organizations. A scheme similar to the STLB was simulated with a different organization of the hardware TLB and the page tables <ref> [12] </ref>, and is discussed with related work in Section 7. We have implemented our prefetching and caching techniques in the Mach 3.0 kernel running on a MIPS R3000- based DECstation 5000/240. <p> This improves overall application performance by up to 3.5%. The penalties of kernel TLB misses for these applications range from 1% to 5% [16]; benchmarks in other studies have higher TLB penalties <ref> [12] </ref>. Thus, prefetching eliminates a significant fraction of kernel TLB penalties. The software TLB achieves high hit rates for kernel TLB misses that range from 90% to nearly 100%. It also decreases the number of kernel TLB misses by eliminating nested TLB traps. <p> For example, different operating systems can implement different page table organizations for the same architecture. Some common page table organizations are inverted page tables and forward-mapped page tables. For example, HP-UX implements an inverted page table for the PA-RISC <ref> [12] </ref>, while Mach 3.0 implements a 3-level page table hierarchy for the 2 Type of miss Penalty (cycles) L1U 10 or 30-40 L2 555 TLB-invalid 338 Table 1: Average TLB Miss Penalties. Average penalties, measured in CPU cycles, for TLB misses under Mach 3.0 on a 40 MHz R3000-based DECstation. <p> Most of the previous work has focussed on hardware caches. Huck and Hays <ref> [12] </ref> have explored an idea similar to the STLB in the context of the HP PA-RISC. Their Hashed Page Table (HPT) is a page table organization suited for large address spaces.
Reference: [13] <author> Motorola Inc. </author> <title> PowerPC 601: RISC Microprocessor User's Manual. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1993. </year>
Reference-contexts: Another approach to decreasing TLB miss handling costs is to provide variable-sized pages that use a single TLB entry to map large contiguous regions of memory. This scheme is used by the PowerPC architecture <ref> [13] </ref>. Prefetch- ing of TLB entries is still useful for dynamically allocated data structures. As operating systems move to modular organizations supporting finer grained processes interacting with each other, the number of L2 TLB misses will increase.
Reference: [14] <author> N. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <address> Seattle, WA, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: The STLB provides a flat space of TLB entries in unmapped physical memory, and thus prevents such cascaded TLB misses. As with any cache, the STLB can be organized in many different ways - direct-mapped, direct-mapped with a victim cache <ref> [14] </ref>, n-way associative, or fully associative [18]. The organization and the size of the STLB affect its hit rate. 4.1 Implementation Like the PTLB, the STLB resides in unmapped, cached physical memory, and therefore does not occupy page table entries in the hardware TLB.
Reference: [15] <author> J. Liedtke. </author> <title> Improving IPC by kernel design. </title> <booktitle> In Proceedings of the 14th Symposium on Operating Systems Principles, </booktitle> <pages> pages 175-188, </pages> <address> Asheville, NC, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: However, the penalty for a TLB miss increases. This penalty becomes even more significant as CPU speeds increase relative to memory speeds, since the cost of accessing page table entries is growing [18]. As researchers continue to improve inter-process communication (IPC) performance <ref> [4, 15] </ref>, TLB penalties will become an increasing fraction of the IPC cost [3, 11]. In addition, recent commercial standards, such as OLE [9] and OpenDoc [21], place an increasing emphasis on inter- application communication.
Reference: [16] <author> D. Nagle, R. Uhlig, T. Mudge, and S. Sechrest. </author> <title> Optimal allocation of on-chip memory for multiple-API operating systems. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 358-369, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: The net effect of all these factors is that the impact of TLB misses on overall system performance is increasing [1, 7]. While TLB penalties have been recognized as a problem, proposed solutions typically require expensive hardware <ref> [17, 16] </ref>. We present two schemes to address this problem that rely only on software mechanisms; no additional hardware is required. Our techniques reduce both the number and cost of TLB misses. The first technique involves prefetching TLB entries during IPC. <p> This improves overall application performance by up to 3.5%. The penalties of kernel TLB misses for these applications range from 1% to 5% <ref> [16] </ref>; benchmarks in other studies have higher TLB penalties [12]. Thus, prefetching eliminates a significant fraction of kernel TLB penalties. The software TLB achieves high hit rates for kernel TLB misses that range from 90% to nearly 100%. <p> The benchmarks studied were mpeg play, jpeg play, video play, which are X applications, and ousterhout, IOzone, and mab (modified Andrew benchmark), which are file-system-oriented applications. Table 2 lists some relevant statistics for each benchmark. For a full description of the benchmarks refer to <ref> [16] </ref>. Network traffic, Mach's random page replacement policy [7], and the aging of the kernel [22], result in variability in application runtimes. The machine was taken off the network to eliminate variability due to network traffic.
Reference: [17] <author> D. Nagle, R. Uhlig, T. Stanley, S. Sechrest, T. Mudge, and R. Brown. </author> <title> Design tradeoffs for software managed TLBs. </title> <booktitle> In Proceedings of the 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 27-38, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Microkernel-based operating systems achieve modularity and flexibility by providing OS functionality in user-level server processes. Frequent communication between client processes, the kernel, and server processes results in more TLB misses than in monolithic-kernel systems <ref> [17] </ref>. Fur- thermore, many microkernel-based systems use virtual memory rather than physical memory for most OS data structures. This increases the number of pages in the active working set that require TLB entries. <p> The net effect of all these factors is that the impact of TLB misses on overall system performance is increasing [1, 7]. While TLB penalties have been recognized as a problem, proposed solutions typically require expensive hardware <ref> [17, 16] </ref>. We present two schemes to address this problem that rely only on software mechanisms; no additional hardware is required. Our techniques reduce both the number and cost of TLB misses. The first technique involves prefetching TLB entries during IPC. <p> L1U PTEs map the pages of user address spaces. L2 PTEs map the user page tables. L1K PTEs map kernel data structures. L3 PTEs map the pages containing L2 and L1K PTEs. A page pinned down in physical memory at the root of the hierarchy holds L3 PTEs <ref> [17] </ref>. There are five types of TLB misses: L1U, L1K, L2, and L3 misses, as well as TLB-invalid misses. A TLB-invalid miss takes place on user addresses that are marked invalid by the virtual memory system. <p> Our prefetching and caching schemes decrease the number of L1K, L2, and L3 misses, from now on referred to as the kernel TLB misses. Kernel TLB misses typically account for greater than 50% of TLB penalties <ref> [17] </ref>. Since L1U miss handling is extremely fast, and TLB-invalid misses require the intervention of the VM system, our schemes do not service these types of TLB misses. 3 Prefetching TLB Entries One software approach to decrease TLB overheads is to prefetch page table entries. <p> Prefetching results in speedups of up to 3.5% of overall application run time. Kernel TLB misses typically constitute greater than 50% of the overall TLB penalties in an application 1 . Ker- nel TLB miss penalties constitute 1% to 5% of the overall application time for the benchmarks studied <ref> [17] </ref>. Thus, prefetching decreases a significant fraction of the kernel TLB penalties. 1 The remaining penalties are due to L1U and TLB-invalid misses.
Reference: [18] <author> D. Patterson and J. Hennessy. </author> <title> Computer architecture: a quantitative approach. </title> <publisher> Morgan Kaufman Publishers, </publisher> <year> 1989. </year>
Reference-contexts: This simplifies the hardware considerably, and provides greater flexibility to the operating system. However, the penalty for a TLB miss increases. This penalty becomes even more significant as CPU speeds increase relative to memory speeds, since the cost of accessing page table entries is growing <ref> [18] </ref>. As researchers continue to improve inter-process communication (IPC) performance [4, 15], TLB penalties will become an increasing fraction of the IPC cost [3, 11]. In addition, recent commercial standards, such as OLE [9] and OpenDoc [21], place an increasing emphasis on inter- application communication. <p> The STLB provides a flat space of TLB entries in unmapped physical memory, and thus prevents such cascaded TLB misses. As with any cache, the STLB can be organized in many different ways - direct-mapped, direct-mapped with a victim cache [14], n-way associative, or fully associative <ref> [18] </ref>. The organization and the size of the STLB affect its hit rate. 4.1 Implementation Like the PTLB, the STLB resides in unmapped, cached physical memory, and therefore does not occupy page table entries in the hardware TLB.
Reference: [19] <author> D. Patterson and J. Hennessy. </author> <title> Computer organization and design: the hardware/software interface. </title> <publisher> Morgan Kaufman Publishers, </publisher> <year> 1993. </year>
Reference-contexts: Thus, prefetching and the STLB should decrease TLB miss handling penalties significantly on faster, newer architectures, and should have an even greater impact on overall application performance. 7 Related Work Prefetching and caching are two well-known techniques that have been widely applied in computer systems <ref> [19, 20] </ref>, but have not been applied to software management of TLBs. Most of the previous work has focussed on hardware caches. Huck and Hays [12] have explored an idea similar to the STLB in the context of the HP PA-RISC.
Reference: [20] <author> A. S. Tanenbaum. </author> <title> Modern Operating Systems. </title> <publisher> PrenticeHall, Inc., </publisher> <year> 1992. </year>
Reference-contexts: Thus, prefetching and the STLB should decrease TLB miss handling penalties significantly on faster, newer architectures, and should have an even greater impact on overall application performance. 7 Related Work Prefetching and caching are two well-known techniques that have been widely applied in computer systems <ref> [19, 20] </ref>, but have not been applied to software management of TLBs. Most of the previous work has focussed on hardware caches. Huck and Hays [12] have explored an idea similar to the STLB in the context of the HP PA-RISC.
Reference: [21] <author> OpenDoc Design Team. </author> <title> OpenDoc technical summary. In Apple's World Wide Developers Conference Technologies CD, </title> <address> San Jose, CA, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: As researchers continue to improve inter-process communication (IPC) performance [4, 15], TLB penalties will become an increasing fraction of the IPC cost [3, 11]. In addition, recent commercial standards, such as OLE [9] and OpenDoc <ref> [21] </ref>, place an increasing emphasis on inter- application communication. The net effect of all these factors is that the impact of TLB misses on overall system performance is increasing [1, 7]. While TLB penalties have been recognized as a problem, proposed solutions typically require expensive hardware [17, 16].
Reference: [22] <author> R. Uhlig. </author> <title> Private communication, </title> <year> 1993. </year>
Reference-contexts: Table 2 lists some relevant statistics for each benchmark. For a full description of the benchmarks refer to [16]. Network traffic, Mach's random page replacement policy [7], and the aging of the kernel <ref> [22] </ref>, result in variability in application runtimes. The machine was taken off the network to eliminate variability due to network traffic. As the system stays up for a long time, the number of L1K misses increases. This happens because the kernel starts allocating kernel data structures from mapped virtual memory.
Reference: [23] <author> R. Uhlig, D. Nagle, T. Mudge, and S. Sechrest. </author> <title> Software TLB management in OSF/1 and Mach 3.0. </title> <type> Technical report, </type> <institution> University of Michigan, </institution> <year> 1993. </year>
Reference-contexts: Average penalties, measured in CPU cycles, for TLB misses under Mach 3.0 on a 40 MHz R3000-based DECstation. These averages were measured using the IOASIC counter. The cycles counts for the misses exhibited fairly high variability. MIPS R2000/3000 <ref> [23] </ref>. by Mach 3.0 for an R3000-based machine. There are four types of page table entries (PTEs): L1U, L2, L1K, and L3. L1U PTEs map the pages of user address spaces. L2 PTEs map the user page tables. L1K PTEs map kernel data structures.
Reference: [24] <author> R. Wahbe, S. Lucco, T. Anderson, and S. Graham. </author> <title> Efficient software-based fault isolation. </title> <booktitle> In Proceedings of the 14th Symposium on Operating Systems Principles, pages 203216, </booktitle> <address> Asheville, NC, </address> <month> December </month> <year> 1993. </year> <month> 11 </month>
Reference-contexts: Our experiments assume a microkernel-based operating system. However, our techniques can be applied to other operating system organizations, such as large single address space systems [6] and systems with software segmentation <ref> [24] </ref>. Furthermore, with the current emphasis on application-controlled resource management [2, 10], our prefetching techniques could become even more effective, since the prefetching strategy can be tailored for individual applications. Prefetching can also be integrated with other VM functions such as prefetching cache entries.
References-found: 24

