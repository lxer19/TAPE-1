URL: http://polaris.cs.uiuc.edu/reports/1179.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: Performance Evaluation of Wire-limited Hierarchical Networks  
Author: William Tsun-yuk Hsu and Pen-chung Yew 
Address: Street Urbana, Illinois 61801  
Affiliation: University of Illinois at Urbana-Champaign 104 S. Wright  
Date: August 3, 1992  
Note: Center for Supercomputing Research and Development  
Abstract: CSRD report no. 1179 
Abstract-found: 1
Intro-found: 1
Reference: [Aboe91] <author> M. Aboelaze, </author> <title> ``Multi-level hypercube network,'' </title> <booktitle> 1991 Int. Parallel Processing Symposium, </booktitle> <address> Anaheim CA, </address> <pages> 4/30-5/2. </pages>
Reference-contexts: A variety of hierarchical configurations have been studied. For example, clusters of processors using shared buses [WuLi81], clusters of processors with crossbar switches [AgMa85], local meshes of processors, with a global mesh connecting the clusters [Carl85], two-level systems based on hypercubes and other network topologies [DaEa90] [DaEa91] <ref> [Aboe91] </ref>, combinations of Omega networks and composite cube networks [Padm91a], and systems with buses within a cluster and a global hypercube network [ChHo91] [HsYe91]. However, most of these studies are only limited to the discussions of traffic patterns with a high locality. Very few of them addressed physical packaging constraints. <p> A possible expensive variation is the extended hypercube of [KuPa92], which gives each processor its own connection to the shared global switch, in addition to a local network. (II) Shared global channels with an interface processor. <ref> [Aboe91] </ref>, [Carl85], [DaEa90], and [DaEa91] suggest a variant of the shared global channels approach, in which only a single interface processor can access the global switch in each cluster. All other processors are connected to the interface processor via a local network as shown in Figure 1 (B).
Reference: [AbPa89] <author> S. Abraham and K. Padmanabhan, </author> <title> ``Performance of the Direct Binary n-Cube Network for Multiprocessors,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. 38, No. 7, </volume> <month> July </month> <year> 1989. </year>
Reference-contexts: Routing strategy does not affect latency when there is no network contention. Neither does it affect maximum network capacity. However, at low to medium traffic, hypercubes and meshes that -14 use LR routing have lower queueing delays than the same systems that use random routing <ref> [AbPa89] </ref>. We assume random routing in the clustered systems for convenience of analysis, but LR routing in flat systems. Hence, our performance numbers for clustered systems are conservative estimates.
Reference: [AbPa90] <author> S. Abraham and K. Padmanabhan, </author> <title> ``Constraint Based Evaluation of Multicomputer Networks,'' </title> <booktitle> 1990 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: However, most of these studies are only limited to the discussions of traffic patterns with a high locality. Very few of them addressed physical packaging constraints. On the other hand, most recent work on packaging issues in multiprocessor interconnects, such as [Dall90], <ref> [AbPa90] </ref>, and [Agar91], address only non-hierarchical systems. -4 In this paper, we study and compare the design of interconnects for hierarchical multiprocessors with packaging constraints. We will limit our study to two-level hierarchical systems. <p> Bisection width [Thom79] is the minimum number of wires crossing a bisection, over all possible bisections; it has often been used as an estimate of wiring on a two-dimensional area within the boundary of a package. In [Dall90], k-ary n-cubes with constant bisection widths were compared. In <ref> [AbPa90] </ref>, both switch pinout and bisection width were used as cost factors in analytical studies of k-ary n-cubes. In both studies, different network topologies were constrained to have the same wiring costs or pinouts by varying the widths of their channels. <p> Partial cut-through is similar to wormhole routing in [Dall90]. It allows a message with enough routing information to ``cut-through'' to the next switch once the outgoing channel is free, even if part of the message has not arrived <ref> [AbPa90] </ref>. We will focus on partial cut-through because of its apparent advantages over message switching. In hypercubes and meshes, there are two possible routing strategies. In random routing, a message at a switch takes at random any one of the possible output channels that are on its minimum-distance routes. <p> Hence, our performance numbers for clustered systems are conservative estimates. We expect network operation to be in regions of moderate channel utilization (&lt;80%), where the message delay curve is relatively flat and few buffers are necessary. Realistic finite-buffer analysis is very computationally intensive. Infinite-buffer analysis <ref> [AbPa90] </ref> [Agar91] is a reasonable compromise. It can be considered as an upper bound on performance. Simulations show that the relative performance will not change much if only a small number of buffers are available per switch [Dall90]. <p> Assume each processor in a flat system is attached to a BxB switch. Under the uniform traffic model, all channels have the same traffic rate. This is true for all the networks under consideration, regardless of routing strategy. Generalizing from the analysis in <ref> [AbPa90] </ref>, we have m= BP t m g t m hhhhh . This equation was originally derived for networks with random routing, but it is also holds for LR routing (see [Hsu92]). Saturation occurs when m=1, so m g,sat = t m BP t hhhh . <p> In these situations, k needs to be even higher depending on the intensity of the local traffic, since the system now saturates at approximately m g,sat /(1-a). A crossbar switch may be needed to handle local traffic. 6. Queueing analysis and performance Flat systems have been analyzed extensively <ref> [AbPa90] </ref> [HsYe92] [HsYZ87] [Padm91b]. We will only summarize their results, and compare them with the analysis of the clustered systems in this section. 6.1. Analysis of clustered systems Consider a cluster split into k subclusters, each with its own bus to and from the global switch. <p> For partial cut through, we estimate the savings to be P t t m hhh for global messages since cluster-local messages do not route through the global network, and there is no cut-through mechanism on the local buses. While this is a simpler approximation for cut-through than <ref> [AbPa90] </ref>, it has been widely used (see, for example, [Agar91]) with reasonable results.
Reference: [Agar91] <author> A. Agarwal, </author> <title> ``Limits on interconnection network performance,'' </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> Vol. 2, No. 4, </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: However, most of these studies are only limited to the discussions of traffic patterns with a high locality. Very few of them addressed physical packaging constraints. On the other hand, most recent work on packaging issues in multiprocessor interconnects, such as [Dall90], [AbPa90], and <ref> [Agar91] </ref>, address only non-hierarchical systems. -4 In this paper, we study and compare the design of interconnects for hierarchical multiprocessors with packaging constraints. We will limit our study to two-level hierarchical systems. <p> For large systems, where signals need to run across the boundary of packages, pinout is a more severe constraint. However, in this paper, we will use both pinout and bisection width as the determinants of the system cost. Other constraints have also been studied in the literature. For example, <ref> [Agar91] </ref> examined fixed channel width, bisection width and node pinout, and took into account both switch delay and wire delay. In our analytical models, we ignore the effect of wire length and time-of-flight delays. <p> Hence, our performance numbers for clustered systems are conservative estimates. We expect network operation to be in regions of moderate channel utilization (&lt;80%), where the message delay curve is relatively flat and few buffers are necessary. Realistic finite-buffer analysis is very computationally intensive. Infinite-buffer analysis [AbPa90] <ref> [Agar91] </ref> is a reasonable compromise. It can be considered as an upper bound on performance. Simulations show that the relative performance will not change much if only a small number of buffers are available per switch [Dall90]. In the region close to maximum network capacity, message delays becomes extremely unstable. <p> While this is a simpler approximation for cut-through than [AbPa90], it has been widely used (see, for example, <ref> [Agar91] </ref>) with reasonable results. A cluster-to-global bus is modeled as a single queue with N 0 /k inputs (from N 0 /k processors), a global-to-cluster bus as a single queue with B+k inputs (B from the global switch and k from the local buses).
Reference: [AgMa85] <author> D. Agrawal and I. Mahgoub, </author> <title> ``Performance analysis of cluster-based supersystems,'' </title> <booktitle> 1st Int. Conf. on Supercomputer Systems, IEEE Comp. </booktitle> <publisher> Soc. Press, </publisher> <year> 1985. </year>
Reference-contexts: Hence, more cost-effective interconnect schemes, such as hypercubes, meshes, or shuffle-exchange networks have to be used. A variety of hierarchical configurations have been studied. For example, clusters of processors using shared buses [WuLi81], clusters of processors with crossbar switches <ref> [AgMa85] </ref>, local meshes of processors, with a global mesh connecting the clusters [Carl85], two-level systems based on hypercubes and other network topologies [DaEa90] [DaEa91] [Aboe91], combinations of Omega networks and composite cube networks [Padm91a], and systems with buses within a cluster and a global hypercube network [ChHo91] [HsYe91].
Reference: [AsJM89] <author> A. Asthana, H. Jagadish, and B. Mathews, </author> <title> ``Impact of advanced VLSI packaging on the design of a large parallel computer,'' </title> <booktitle> 1989 Int. Conf. on Parallel Processing, </booktitle> <month> August </month> <year> 1989. </year>
Reference-contexts: These packaging technologies impose an inherent hierarchical structure on the system; this hierarchy often influences the design of the machine organization (see, for example, <ref> [AsJM89] </ref>). For example, several processors may be laid out on a single chip, or placed on a multi-chip module to form a cluster; several of such clusters may reside on a multi-layer board, and several of such boards in a chassis.
Reference: [Carl85] <author> D. Carlson, </author> <title> ``The mesh with a global mesh: a flexible, high-speed organization for parallel computation,'' </title> <booktitle> 1st Int. Conf. on Supercomputer Systems, IEEE Comp. </booktitle> <publisher> Soc. Press, </publisher> <year> 1985. </year>
Reference-contexts: A variety of hierarchical configurations have been studied. For example, clusters of processors using shared buses [WuLi81], clusters of processors with crossbar switches [AgMa85], local meshes of processors, with a global mesh connecting the clusters <ref> [Carl85] </ref>, two-level systems based on hypercubes and other network topologies [DaEa90] [DaEa91] [Aboe91], combinations of Omega networks and composite cube networks [Padm91a], and systems with buses within a cluster and a global hypercube network [ChHo91] [HsYe91]. <p> A possible expensive variation is the extended hypercube of [KuPa92], which gives each processor its own connection to the shared global switch, in addition to a local network. (II) Shared global channels with an interface processor. [Aboe91], <ref> [Carl85] </ref>, [DaEa90], and [DaEa91] suggest a variant of the shared global channels approach, in which only a single interface processor can access the global switch in each cluster. All other processors are connected to the interface processor via a local network as shown in Figure 1 (B).
Reference: [ChHo91] <author> S. Chowdhury and M. Holliday, </author> <title> ``Stability and performance of alternative two-level interconnection networks,'' </title> <booktitle> 1991 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: processors with crossbar switches [AgMa85], local meshes of processors, with a global mesh connecting the clusters [Carl85], two-level systems based on hypercubes and other network topologies [DaEa90] [DaEa91] [Aboe91], combinations of Omega networks and composite cube networks [Padm91a], and systems with buses within a cluster and a global hypercube network <ref> [ChHo91] </ref> [HsYe91]. However, most of these studies are only limited to the discussions of traffic patterns with a high locality. Very few of them addressed physical packaging constraints. <p> Such extra bandwidth in the local buses, beyond what is provided by the k buses, will not yield substantial improvement in network delay, except for applications with heavy and sustained local traffic [HsYe91]. See Section 5.3 for further discussions on local bandwidth requirements. <ref> [ChHo91] </ref> assumes an approach that is similar to shared global channels with equitable access.
Reference: [Dall90] <author> W. Dally, </author> <title> ``Performance Analysis of k-ary n-cube Interconnection Networks,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. C-39, No. 6, </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: However, most of these studies are only limited to the discussions of traffic patterns with a high locality. Very few of them addressed physical packaging constraints. On the other hand, most recent work on packaging issues in multiprocessor interconnects, such as <ref> [Dall90] </ref>, [AbPa90], and [Agar91], address only non-hierarchical systems. -4 In this paper, we study and compare the design of interconnects for hierarchical multiprocessors with packaging constraints. We will limit our study to two-level hierarchical systems. <p> Bisection width [Thom79] is the minimum number of wires crossing a bisection, over all possible bisections; it has often been used as an estimate of wiring on a two-dimensional area within the boundary of a package. In <ref> [Dall90] </ref>, k-ary n-cubes with constant bisection widths were compared. In [AbPa90], both switch pinout and bisection width were used as cost factors in analytical studies of k-ary n-cubes. <p> For example, for the CM, B clus =4. For an L-bit message (including header), the number of nibbles in the message is t m = R J C H J Now consider the bisection widths of k-ary n-cubes. We can adapt the terms for bisection width from <ref> [Dall90] </ref>, making appropriate adjustments for bidirectional channels. For a hypercube with N 1 processors and a channel width C, the bisection width is CN 1 . For bidirectional meshes, it is 4C dd N 1 . <p> Saturation performance estimates We can use either message switching or partial cut-through to route messages through networks. In message switching for multiple-nibble messages, an entire message has to arrive at the output port of a switch before it can be forwarded. Partial cut-through is similar to wormhole routing in <ref> [Dall90] </ref>. It allows a message with enough routing information to ``cut-through'' to the next switch once the outgoing channel is free, even if part of the message has not arrived [AbPa90]. We will focus on partial cut-through because of its apparent advantages over message switching. <p> Realistic finite-buffer analysis is very computationally intensive. Infinite-buffer analysis [AbPa90] [Agar91] is a reasonable compromise. It can be considered as an upper bound on performance. Simulations show that the relative performance will not change much if only a small number of buffers are available per switch <ref> [Dall90] </ref>. In the region close to maximum network capacity, message delays becomes extremely unstable. Small increases in system load can cause huge increases in average delays. Hence, it should be avoided. <p> Hence, in a carefully designed system where all these parameters are taken into account, channel efficiency may not be a factor. 5.2. Saturation of networks with constant bisection widths As in <ref> [Dall90] </ref>, we can use bisection width as a measure of wiring area to compare flat and clustered systems. To obtain systems with a constant bisection width, we normalize all the networks by varying their channel widths so that they have the same bisection width as an FH.
Reference: [DaEa90] <author> S. Dandamudi and D. Eager, </author> <title> ``Hierarchical Interconnection Networks for Multicomputer Systems,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. C-39, No. 6, </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: A variety of hierarchical configurations have been studied. For example, clusters of processors using shared buses [WuLi81], clusters of processors with crossbar switches [AgMa85], local meshes of processors, with a global mesh connecting the clusters [Carl85], two-level systems based on hypercubes and other network topologies <ref> [DaEa90] </ref> [DaEa91] [Aboe91], combinations of Omega networks and composite cube networks [Padm91a], and systems with buses within a cluster and a global hypercube network [ChHo91] [HsYe91]. However, most of these studies are only limited to the discussions of traffic patterns with a high locality. <p> A possible expensive variation is the extended hypercube of [KuPa92], which gives each processor its own connection to the shared global switch, in addition to a local network. (II) Shared global channels with an interface processor. [Aboe91], [Carl85], <ref> [DaEa90] </ref>, and [DaEa91] suggest a variant of the shared global channels approach, in which only a single interface processor can access the global switch in each cluster. All other processors are connected to the interface processor via a local network as shown in Figure 1 (B). <p> All other processors are connected to the interface processor via a local network as shown in Figure 1 (B). This approach requires the interface processor to have a larger fanout than the other processors in the cluster. A major problem with this approach is observed in <ref> [DaEa90] </ref> for traffic that does not have an extremely high degree of locality. The local channels -8 attached to the interface processor are carrying all of the global traffic to and from the cluster.
Reference: [DaEa91] <author> S. Dandamudi and D. Eager, </author> <title> ``On hypercube-based hierarchical interconnection network design,'' </title> <booktitle> Jour. of Parallel and Distributed Computing 12, </booktitle> <month> July </month> <year> 1991. </year>
Reference-contexts: A variety of hierarchical configurations have been studied. For example, clusters of processors using shared buses [WuLi81], clusters of processors with crossbar switches [AgMa85], local meshes of processors, with a global mesh connecting the clusters [Carl85], two-level systems based on hypercubes and other network topologies [DaEa90] <ref> [DaEa91] </ref> [Aboe91], combinations of Omega networks and composite cube networks [Padm91a], and systems with buses within a cluster and a global hypercube network [ChHo91] [HsYe91]. However, most of these studies are only limited to the discussions of traffic patterns with a high locality. <p> A possible expensive variation is the extended hypercube of [KuPa92], which gives each processor its own connection to the shared global switch, in addition to a local network. (II) Shared global channels with an interface processor. [Aboe91], [Carl85], [DaEa90], and <ref> [DaEa91] </ref> suggest a variant of the shared global channels approach, in which only a single interface processor can access the global switch in each cluster. All other processors are connected to the interface processor via a local network as shown in Figure 1 (B).
Reference: [GJMW89] <author> K. Gallivan, W. Jalby, A. Malony, and H. Wijshoff, </author> <title> ``Performance prediction of loop constructs on multiprocessor hierarchical-memory systems,'' </title> <booktitle> 1989 Int. Conf. on Supercomputing, </booktitle> <address> Crete, Greece, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: Software technology probably will limit the level of system hierarchy to a very small number, most likely at two levels, in the foreseeable future. Several machines which have been build with a hierarchical structure to investigate parallel applications include Cedar <ref> [GJMW89] </ref>, Dash [LLGW92], and Cenju [NTKM91]. -3 Another important issue in designing such systems is scalability. To allow program compatibility and portability, the major characteristics of a system architecture should remain unchanged as we scale the system up, from a few processors to hundreds or thousands of processors.
Reference: [GiMo91] <author> W. Giloi and S. Montenegro, </author> <title> ``Choosing the interconnect of distributed memory systems by cost and blocking behavior,'' </title> <booktitle> 1991 Int. Parallel Processing Symposium, </booktitle> <address> Anaheim CA, </address> <pages> 4/30-5/2. </pages>
Reference: [HaMS86] <author> J.P. Hayes, T.N. Mudge, and Q.F. Stout, </author> <title> ``Architecture of a Hypercube Supercomputer", </title> <booktitle> 1986 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1986. </year>
Reference-contexts: Because of the problems in approaches (II) and (III), we will focus on the shared global channels, equitable access approach. For example, consider an NCUBE-like system <ref> [HaMS86] </ref>: a 1024-processor FH has a 64-processor subcube and 512 pins per board. Each processor has a 10x10 switch. Four of its inputs and four of its outputs go off-board. Hence, there are 64*4*2=512 channels. Each channel can only be one bit wide.
Reference: [Hsu92] <author> W. Hsu, </author> <title> Multiprocessor communications: design and technology, </title> <type> PhD dissertation, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <note> in preparation. -29 -30 </note>
Reference-contexts: The number of channels going outside a board is 2B. The channel width is C= J 2B J P . The bisection width of an N 1 -node GSE with BxB switches and C-bit channels is 5CN 1 B/16 <ref> [Hsu92] </ref>. -13 simple observations about the effect of varying the fanout B. A high fanout would imply short message distance. The channel width, however, would be smaller, resulting in more nibbles in a message. We will examine these ramifications in more detail in section 5.1. <p> This is true for all the networks under consideration, regardless of routing strategy. Generalizing from the analysis in [AbPa90], we have m= BP t m g t m hhhhh . This equation was originally derived for networks with random routing, but it is also holds for LR routing (see <ref> [Hsu92] </ref>). Saturation occurs when m=1, so m g,sat = t m BP t hhhh . For a clustered system, let B be the number of global channels going out of the cluster. <p> Let T cluster-to-global be the average delay encountered by a message on a cluster-to-global bus, and T global-to-cluster the average delay encountered on a global-to cluster bus (derived in detail in <ref> [Hsu92] </ref>). The total message delay is [Hsu92] -22 mP t b d hhhh )t m +(1+(1-a) P t 1 hhh )t h +T cluster-to-global +T global-to-cluster -(1-a)t m /P t . We simulated some of these networks to verify our analyses. <p> Let T cluster-to-global be the average delay encountered by a message on a cluster-to-global bus, and T global-to-cluster the average delay encountered on a global-to cluster bus (derived in detail in <ref> [Hsu92] </ref>). The total message delay is [Hsu92] -22 mP t b d hhhh )t m +(1+(1-a) P t 1 hhh )t h +T cluster-to-global +T global-to-cluster -(1-a)t m /P t . We simulated some of these networks to verify our analyses. <p> Messages are forwarded through buffers in simulated switches. The delays predicted by our analysis match the numbers obtained through simulations (with small errors), up to channel utilizations of at least 80% <ref> [Hsu92] </ref>. The correction factor for partial cut-through is fairly accurate, and the inaccuracies in the total delay under cut-through mostly come from the message switching analysis, with the well-known problems of assuming independent arrivals etc. 6.2.
Reference: [HsYe91] <author> W. Hsu and P. Yew, </author> <title> ``The performance of hierarchical systems with wiring constraints,'' </title> <booktitle> 1991 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: with crossbar switches [AgMa85], local meshes of processors, with a global mesh connecting the clusters [Carl85], two-level systems based on hypercubes and other network topologies [DaEa90] [DaEa91] [Aboe91], combinations of Omega networks and composite cube networks [Padm91a], and systems with buses within a cluster and a global hypercube network [ChHo91] <ref> [HsYe91] </ref>. However, most of these studies are only limited to the discussions of traffic patterns with a high locality. Very few of them addressed physical packaging constraints. <p> But it will come with an increased cost. Such extra bandwidth in the local buses, beyond what is provided by the k buses, will not yield substantial improvement in network delay, except for applications with heavy and sustained local traffic <ref> [HsYe91] </ref>. See Section 5.3 for further discussions on local bandwidth requirements. [ChHo91] assumes an approach that is similar to shared global channels with equitable access. <p> These guidelines ignore channel efficiency effects. If messages are short and transmissions are not efficient, the number of local buses prescribed above may be insufficient to handle saturation traffic. As observed in <ref> [HsYe91] </ref>, if local traffic is heavy, processors will be able to issue requests past the rate of m g,sat without saturating the global channels. <p> The system saturates when the utilization of the former reaches 100%, which is identical to the saturation load at uniform traffic. However, CHs saturate at higher loads, especially if more cluster buses are used. The saturation loads of FMs and CMs also improve with locality of traffic <ref> [HsYe91] </ref>. Under the constant pinout constraint, the CGSE is the best network because of its superior saturation load. Under the constant bisection width constraint, the contenders are the CH and FM, with the CH winning for longer messages, and the FM for shorter messages.
Reference: [HsYe92] <author> W. Hsu and P. Yew, </author> <title> ``The impact of wiring constraints on hierarchical network performance,'' </title> <booktitle> 1992 Int. Parallel Processing Symp., </booktitle> <month> March </month> <year> 1992. </year>
Reference-contexts: We implied constant board pinout, but note that an FH and a CH (and any flat k-ary n-cube and its clustered analogue) have the same board pinout and bisection width. This is because only the organization of the channels changes, and not the network topology. <ref> [HsYe92] </ref> applied this approach to meshes with torus-like wraparound channels. Consider a 1024-processor bidirectional flat mesh (FM) with a 64-processor submesh and 512 pins per board. Only the processors on the perimeter of the submesh have off-board channels. There are 64 off-board channels, and each is 8-bit wide. <p> In these situations, k needs to be even higher depending on the intensity of the local traffic, since the system now saturates at approximately m g,sat /(1-a). A crossbar switch may be needed to handle local traffic. 6. Queueing analysis and performance Flat systems have been analyzed extensively [AbPa90] <ref> [HsYe92] </ref> [HsYZ87] [Padm91b]. We will only summarize their results, and compare them with the analysis of the clustered systems in this section. 6.1. Analysis of clustered systems Consider a cluster split into k subclusters, each with its own bus to and from the global switch. <p> For a k-ary n-cube, the probability of i arrivals from global channels at a global channel is <ref> [HsYe92] </ref> d i = J B-1 J I L B-1 m (1-P t ) hhhhhhhh M O J 1 m (1-P t ) hhhhhhhh M O for iB-1, and 0 otherwise.
Reference: [HsYZ87] <author> W. Hsu, P. Yew, and C. Zhu, </author> <title> ``An enhancement scheme for hypercube interconnection networks,'' </title> <booktitle> 1987 Int. Conf. on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1987. </year>
Reference-contexts: The traffic patterns and switch designs of multistage networks are thus different from a mesh or a hypercube, where each processor is directly connected to its adjacent processors, or memory modules. To allow more equitable comparisons among these topologies, we use a generalized shuffle-exchange network (GSE) <ref> [HsYZ87] </ref> instead. A GSE network is constructed by attaching a processor node to each switching element (as in a mesh, or a hypercube). All switching -6 elments are then connected with a shuffle connection. To illustrate our discussions, assume we have a system with 1024 processors. <p> B can be chosen by a system architect based on the desired performance. For the purposes of this paper, we have restricted B to powers of 2. [Padm91b] presented analysis for flat GSEs with non-power-of-2 fanouts. A simple routing algorithm for the GSE was proposed in <ref> [HsYZ87] </ref>. Consider the case where logN 1 is divisible by logB. At each hop through the network, a message is forwarded to the node whose address corresponds to the appropriate logB bits of the destination address of the message. <p> Each hop through the network rotates the address bits and changes 2 bits of the intermediate node address. A slight modification suffices for the case where logN 1 is not divisible by logB <ref> [HsYZ87] </ref>. In such cases, multiple paths exist. We will not consider fast-finishing routing algorithms in this paper, because they complicate analysis; for details see [YePL82]. A flat GSE (FGSE) with fanout B is simply a GSE partitioned across N boards, with N 0 processors per board. <p> A crossbar switch may be needed to handle local traffic. 6. Queueing analysis and performance Flat systems have been analyzed extensively [AbPa90] [HsYe92] <ref> [HsYZ87] </ref> [Padm91b]. We will only summarize their results, and compare them with the analysis of the clustered systems in this section. 6.1. Analysis of clustered systems Consider a cluster split into k subclusters, each with its own bus to and from the global switch. <p> For the CGSE, the d i term is slightly different, because messages arriving on an input can be routed to any of B outputs <ref> [HsYZ87] </ref>: d i = J B J I L B m (1-P t ) hhhhhhhh M O J 1 m (1-P t ) hhhhhhhh M O for iB-1, and 0 otherwise.
Reference: [KuPa92] <author> J. Kumar and L. Patnaik, </author> <title> ``Hierarchical network of hypercubes with folded connections,'' </title> <booktitle> Proceedings of the Parallel Systems Fair, 1992 Int. Parallel Processing Symp., </booktitle> <month> March </month> <year> 1992. </year>
Reference-contexts: See Section 5.3 for further discussions on local bandwidth requirements. [ChHo91] assumes an approach that is similar to shared global channels with equitable access. A possible expensive variation is the extended hypercube of <ref> [KuPa92] </ref>, which gives each processor its own connection to the shared global switch, in addition to a local network. (II) Shared global channels with an interface processor. [Aboe91], [Carl85], [DaEa90], and [DaEa91] suggest a variant of the shared global channels approach, in which only a single interface processor can access the
Reference: [Lawr75] <author> D. Lawrie, </author> <title> ``Access and Alignment of Data in an Array Processor", </title> <journal> IEEE Trans. on Computers, </journal> <month> December </month> <year> 1975. </year>
Reference-contexts: There have been quite a few interconnect topologies proposed in the past. Here, we only consider major topologies which have been used most frequently in existing parallel machines, namely k-ary n-cubes such as meshes and hypercubes, and shuffle-exchange networks. However, shuffle-exchange networks such as Omega networks <ref> [Lawr75] </ref> are primarily multiple-stage networks. A processor has to go through a series of dedicated intermediate switching elements, arranged in stages, to communicate with another processor or a memory module.
Reference: [LLGW92] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam, </author> <title> ``The Stanford Dash multiprocessor,'' </title> <journal> IEEE Computer, </journal> <volume> Vol. 25, No. 3, </volume> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: Software technology probably will limit the level of system hierarchy to a very small number, most likely at two levels, in the foreseeable future. Several machines which have been build with a hierarchical structure to investigate parallel applications include Cedar [GJMW89], Dash <ref> [LLGW92] </ref>, and Cenju [NTKM91]. -3 Another important issue in designing such systems is scalability. To allow program compatibility and portability, the major characteristics of a system architecture should remain unchanged as we scale the system up, from a few processors to hundreds or thousands of processors.
Reference: [MaBa92] <author> Q. Malluhi and M. Bayoumi, </author> <title> ``Properties and performance of the hierarchical hypercube,'' </title> <booktitle> 1992 Int. Parallel Processing Symp., </booktitle> <month> March </month> <year> 1992. </year>
Reference-contexts: This can create extra congestion for local communications. Also, processors in a cluster do not have an equitable access to global channels. The cube-connected-cycles (CCC) network [PrVu81] is a classical example of the partitioned global channels approach. The hierarchical hypercube of <ref> [MaBa92] </ref> is another example of the partitioned global channels. Figure 1 (C) shows a cycle of 8 processors in a CCC network, with 4 processors having direct access to the 4 global channels.
Reference: [NTKM91] <author> T. Nakata, N. Tanabe, N. Kajihara, S. Matsushita, H. Onozuka, Y. Asano, and N. Koike, ``Cenju: </author> <title> a multiprocessor system with a distributed shared memory scheme for modular circuit simulation,'' </title> <booktitle> Int. Symp. on Shared Memory Multiprocessing, </booktitle> <address> Tokyu, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: Software technology probably will limit the level of system hierarchy to a very small number, most likely at two levels, in the foreseeable future. Several machines which have been build with a hierarchical structure to investigate parallel applications include Cedar [GJMW89], Dash [LLGW92], and Cenju <ref> [NTKM91] </ref>. -3 Another important issue in designing such systems is scalability. To allow program compatibility and portability, the major characteristics of a system architecture should remain unchanged as we scale the system up, from a few processors to hundreds or thousands of processors.
Reference: [Padm91a] <author> K. Padmanabhan, </author> <title> ``Effective architectures for data access in a shared memory hierarchy,'' </title> <booktitle> Jour. of Parallel and Distributed Computing 11, </booktitle> <year> 1991. </year>
Reference-contexts: For example, clusters of processors using shared buses [WuLi81], clusters of processors with crossbar switches [AgMa85], local meshes of processors, with a global mesh connecting the clusters [Carl85], two-level systems based on hypercubes and other network topologies [DaEa90] [DaEa91] [Aboe91], combinations of Omega networks and composite cube networks <ref> [Padm91a] </ref>, and systems with buses within a cluster and a global hypercube network [ChHo91] [HsYe91]. However, most of these studies are only limited to the discussions of traffic patterns with a high locality. Very few of them addressed physical packaging constraints.
Reference: [Padm91b] <author> K. Padmanabhan, </author> <title> ``On the tradeoff between node degree and communication channel width in shuffle-exchange networks,'' </title> <booktitle> 3rd IEEE Symposium on Parallel and Distributed Processing, </booktitle> <address> Dallas TX, </address> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: B can be chosen by a system architect based on the desired performance. For the purposes of this paper, we have restricted B to powers of 2. <ref> [Padm91b] </ref> presented analysis for flat GSEs with non-power-of-2 fanouts. A simple routing algorithm for the GSE was proposed in [HsYZ87]. Consider the case where logN 1 is divisible by logB. <p> A crossbar switch may be needed to handle local traffic. 6. Queueing analysis and performance Flat systems have been analyzed extensively [AbPa90] [HsYe92] [HsYZ87] <ref> [Padm91b] </ref>. We will only summarize their results, and compare them with the analysis of the clustered systems in this section. 6.1. Analysis of clustered systems Consider a cluster split into k subclusters, each with its own bus to and from the global switch.
Reference: [PrVu81] <author> F. Preparata and J. Vuillemin, </author> <title> ``The cube-connected cycles: a versatile communication network for parallel computation,'' </title> <journal> Communications of the ACM, </journal> <volume> Vol. 24, No. 5, </volume> <year> 1981. </year>
Reference-contexts: While this alleviates the expense of a large global switch, other problems arise. Intercluster traffic has to be routed through local channels. This can create extra congestion for local communications. Also, processors in a cluster do not have an equitable access to global channels. The cube-connected-cycles (CCC) network <ref> [PrVu81] </ref> is a classical example of the partitioned global channels approach. The hierarchical hypercube of [MaBa92] is another example of the partitioned global channels. Figure 1 (C) shows a cycle of 8 processors in a CCC network, with 4 processors having direct access to the 4 global channels.
Reference: [Thom79] <author> C.D. Thompson, </author> <title> ``Area-Time Complexity for VLSI,'' Ann. </title> <booktitle> Symp. on Theory of Computing, </booktitle> <month> May </month> <year> 1979. </year>
Reference-contexts: Channel degree is the number of channels at a node. The bisection of a graph is a partition of its nodes into two equal halves. Bisection width <ref> [Thom79] </ref> is the minimum number of wires crossing a bisection, over all possible bisections; it has often been used as an estimate of wiring on a two-dimensional area within the boundary of a package. In [Dall90], k-ary n-cubes with constant bisection widths were compared.
Reference: [WuLi81] <author> S. Wu and M. Liu, </author> <title> ``A cluster structure as an interconnection network for large multiprocessor systems,'' </title> <journal> IEEE Trans. on Computers, </journal> <volume> Vol. C-30, No. 4, </volume> <month> Apr. </month> <year> 1981. </year>
Reference-contexts: Hence, more cost-effective interconnect schemes, such as hypercubes, meshes, or shuffle-exchange networks have to be used. A variety of hierarchical configurations have been studied. For example, clusters of processors using shared buses <ref> [WuLi81] </ref>, clusters of processors with crossbar switches [AgMa85], local meshes of processors, with a global mesh connecting the clusters [Carl85], two-level systems based on hypercubes and other network topologies [DaEa90] [DaEa91] [Aboe91], combinations of Omega networks and composite cube networks [Padm91a], and systems with buses within a cluster and a global

References-found: 28

