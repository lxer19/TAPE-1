URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/thrun.nips7.neuro-chess.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/thrun/papers/thrun.nips7.neuro-chess.html
Root-URL: http://www.cs.cmu.edu
Email: E-mail: thrun@carbon.informatik.uni-bonn.de  
Title: Learning To Play the Game of Chess  
Author: Sebastian Thrun G. Tesauro, D. Touretzky, and T. Leen, 
Date: 1995  
Note: to appear in: Advances in Neural Information Processing Systems 7  eds.,  
Address: Romerstr. 164, D-53117 Bonn, Germany  
Affiliation: University of Bonn Department of Computer Science III  
Abstract: This paper presents NeuroChess, a program which learns to play chess from the final outcome of games. NeuroChess learns chess board evaluation functions, represented by artificial neural networks. It integrates inductive neural network learning, temporal differencing, and a variant of explanation-based learning. Performance results illustrate some of the strengths and weaknesses of this approach.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Thomas S. Anantharaman. </author> <title> A Statistical Study of Selective Min-Max Search in Computer Chess. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, </address> <year> 1990. </year> <note> Technical Report CMU-CS-90-173. </note>
Reference-contexts: Most of today's chess programs rely on intensive search to generate moves. To evaluate boards, fast evaluation functions are employed which are usually carefully designed by hand, sometimes augmented by automatic parameter tuning methods <ref> [1] </ref>. Building a chess machine that learns to play solely from the final outcome of games (win/loss/draw) is a challenging open problem in AI. In this paper, we are interested in learning to play chess from the final outcome of games.
Reference: [2] <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1957. </year>
Reference-contexts: In practice, however, random disturbations of the evaluation function can seriously hurt learning, for reasons given in [4, 17]. Empirically we found that learning failed completely when no discount factor was used. Currently, NeuroChess uses fl = 0:98. Learning rate. TD approaches minimize a Bellman equation <ref> [2] </ref>. In the NeuroChess domain, a close-to-optimal approximation of the Bellman equation is the constant function V (s) 0. This function violates the Bellman equation only at the end of games (Eq. (1)), which is rare if complete games are considered.
Reference: [3] <author> Hans J. Berliner, Gordon Goetsch, Murray S. Campbell, and Carl Ebeling. </author> <title> Measuring the performance potential of chess programs. </title> <journal> Artificial Intelligence, </journal> <volume> 43 </volume> <pages> 7-20, </pages> <year> 1990. </year>
Reference-contexts: It has been well recognized that the ultimate cost in chess is determined by the time it takes to generate a move. Chess programs can generally invest their time in search, or in the evaluation of chess boards (search-knowledge trade-off) <ref> [3] </ref>. Currently, NeuroChess does a poor job, because it spends most of its time computing board evaluations. Computing a large neural network function takes two orders of magnitude longer than evaluating an optimized linear evaluation function (like that of GNU-Chess).
Reference: [4] <author> Justin A. Boyan. </author> <title> Generalization in reinforcement learning: Safely approximating the value function. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <address> San Mateo, CA, </address> <year> 1995. </year> <note> Morgan Kaufmann. (to appear). </note>
Reference-contexts: Indeed, without discounting the evaluation function predicts the probability for winningin the ideal case. In practice, however, random disturbations of the evaluation function can seriously hurt learning, for reasons given in <ref> [4, 17] </ref>. Empirically we found that learning failed completely when no discount factor was used. Currently, NeuroChess uses fl = 0:98. Learning rate. TD approaches minimize a Bellman equation [2]. In the NeuroChess domain, a close-to-optimal approximation of the Bellman equation is the constant function V (s) 0.
Reference: [5] <author> Gerald DeJong and Raymond Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: Gherrity [6] presented a similar system which he applied to entire chess games. Both approaches learn purely inductively from the final outcome of games. Tadepalli [15] applied a lazy version of explanation-based learning <ref> [5, 7] </ref> to endgames in chess. His approach learns from the final outcome, too, but unlike the inductive neural network approaches listed above it learns analytically, by analyzing and generalizing experiences in terms of chess-specific knowledge. <p> Hence, quite a few versions of a knight fork have to be experienced in order to generalize accurately. In a domain as complex as chess, such an approach might require unreasonably large amounts of training data. Explanation-based methods (EBL) <ref> [5, 7, 15] </ref> generalize more accurately from less training data. They rely instead on the availability of domain knowledge, which they use for explaining and generalizing training examples.
Reference: [6] <author> Michael Gherrity. </author> <title> A Game-Learning Machine. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> San Diego, </address> <year> 1993. </year>
Reference-contexts: While his TD-Gammon approach plays grandmaster-level backgammon, recent attempts to reproduce these results in the context of Go [12] and chess have been less successful. For example, Schafer [11] reports a system just like Tesauro's TD-Gammon, applied to learning to play certain chess endgames. Gherrity <ref> [6] </ref> presented a similar system which he applied to entire chess games. Both approaches learn purely inductively from the final outcome of games. Tadepalli [15] applied a lazy version of explanation-based learning [5, 7] to endgames in chess. <p> It decays V exponentially in time and hence favors early over late success. Notice that in NeuroChess V is represented by an artificial neural network, which is trained to fit the target values V target obtained via Eqs. (1) and (2) (cf. <ref> [6, 11, 12, 16] </ref>). 3 Explanation-Based Neural Network Learning In a domain as complex as chess, pure inductive learning techniques, such as neural network Back-Propagation, suffer from enormous training times.
Reference: [7] <author> Tom M. Mitchell, Rich Keller, and Smadar Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: Gherrity [6] presented a similar system which he applied to entire chess games. Both approaches learn purely inductively from the final outcome of games. Tadepalli [15] applied a lazy version of explanation-based learning <ref> [5, 7] </ref> to endgames in chess. His approach learns from the final outcome, too, but unlike the inductive neural network approaches listed above it learns analytically, by analyzing and generalizing experiences in terms of chess-specific knowledge. <p> Hence, quite a few versions of a knight fork have to be experienced in order to generalize accurately. In a domain as complex as chess, such an approach might require unreasonably large amounts of training data. Explanation-based methods (EBL) <ref> [5, 7, 15] </ref> generalize more accurately from less training data. They rely instead on the availability of domain knowledge, which they use for explaining and generalizing training examples.
Reference: [8] <author> Tom M. Mitchell and Sebastian Thrun. </author> <title> Explanation based learning: A comparison of symbolic and neural network approaches. </title> <editor> In Paul E. Utgoff, editor, </editor> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 197-204, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This illustrates the hardness of the problem of learning to play chess from the final outcome of games. This paper presents NeuroChess, a program that learns to play chess from the final outcome of games. The central learning mechanisms is the explanation-based neural network (EBNN) algorithm <ref> [9, 8] </ref>. Like Tesauro's TD-Gammon approach, NeuroChess constructs a neural network evaluation function for chess boards using TD. In addition, a neural network version of explanation-based learning is employed, which analyzes games in terms of a previously learned neural network chess model. <p> They can be interpreted as biasing the network V based on chess-specific domain knowledge, embodied in M . For the relation of EBNN and EBL and the accommodation of inaccurate slopes in EBNN see <ref> [8] </ref>. 4 Training Issues In this section we will briefly discuss some training issues that are essential for learning good evaluation functions in the domain of chess. This list of points has mainly been produced through practical experience with the NeuroChess and related TD approaches.
Reference: [9] <author> Tom M. Mitchell and Sebastian Thrun. </author> <title> Explanation-based neural network learning for robot control. </title> <editor> In S. J. Hanson, J. Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 287-294, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This illustrates the hardness of the problem of learning to play chess from the final outcome of games. This paper presents NeuroChess, a program that learns to play chess from the final outcome of games. The central learning mechanisms is the explanation-based neural network (EBNN) algorithm <ref> [9, 8] </ref>. Like Tesauro's TD-Gammon approach, NeuroChess constructs a neural network evaluation function for chess boards using TD. In addition, a neural network version of explanation-based learning is employed, which analyzes games in terms of a previously learned neural network chess model. <p> Most current approaches to EBL require that the domain knowledge be represented by a set of symbolic rules. Since NeuroChess relies on neural network representations, it employs a neural network version of EBL, called explanation-based neural network learning (EBNN) <ref> [9] </ref>. In the context of chess, EBNN works in the following way: The domain-specific knowledge is represented by a separate neural network, called the chess model M . M maps arbitrary chess boards s t to the corresponding expected board s t+2 two half-moves later.
Reference: [10] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal on research and development, </journal> <volume> 3 </volume> <pages> 210-229, </pages> <year> 1959. </year>
Reference-contexts: In this paper, we are interested in learning to play chess from the final outcome of games. One of the earliest approaches, which learned solely by playing itself, is Samuel's famous checker player program <ref> [10] </ref>. His approach employed temporal difference learning (in short: TD) [14], which is a technique for recursively learning an evaluation function. Recently, Tesauro reported the successful application of TD to the game of Backgammon, using artificial neural network representations [16].
Reference: [11] <author> Johannes Schafer. </author> <title> Erfolgsorientiertes Lernen mit Tiefensuche in Bauernendspielen. </title> <type> Technical report, </type> <institution> Universitat Karlsruhe, </institution> <year> 1993. </year> <note> (in German). </note>
Reference-contexts: Recently, Tesauro reported the successful application of TD to the game of Backgammon, using artificial neural network representations [16]. While his TD-Gammon approach plays grandmaster-level backgammon, recent attempts to reproduce these results in the context of Go [12] and chess have been less successful. For example, Schafer <ref> [11] </ref> reports a system just like Tesauro's TD-Gammon, applied to learning to play certain chess endgames. Gherrity [6] presented a similar system which he applied to entire chess games. Both approaches learn purely inductively from the final outcome of games. <p> It decays V exponentially in time and hence favors early over late success. Notice that in NeuroChess V is represented by an artificial neural network, which is trained to fit the target values V target obtained via Eqs. (1) and (2) (cf. <ref> [6, 11, 12, 16] </ref>). 3 Explanation-Based Neural Network Learning In a domain as complex as chess, pure inductive learning techniques, such as neural network Back-Propagation, suffer from enormous training times.
Reference: [12] <author> Nikolaus Schraudolph, Pater Dayan, and Terrence J. Sejnowski. </author> <title> Using the TD(lambda) algorithm to learn an evaluation function for the game of go. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Recently, Tesauro reported the successful application of TD to the game of Backgammon, using artificial neural network representations [16]. While his TD-Gammon approach plays grandmaster-level backgammon, recent attempts to reproduce these results in the context of Go <ref> [12] </ref> and chess have been less successful. For example, Schafer [11] reports a system just like Tesauro's TD-Gammon, applied to learning to play certain chess endgames. Gherrity [6] presented a similar system which he applied to entire chess games. Both approaches learn purely inductively from the final outcome of games. <p> It decays V exponentially in time and hence favors early over late success. Notice that in NeuroChess V is represented by an artificial neural network, which is trained to fit the target values V target obtained via Eqs. (1) and (2) (cf. <ref> [6, 11, 12, 16] </ref>). 3 Explanation-Based Neural Network Learning In a domain as complex as chess, pure inductive learning techniques, such as neural network Back-Propagation, suffer from enormous training times.
Reference: [13] <author> Patrice Simard, Bernard Victorri, Yann LeCun, and John Denker. </author> <title> Tangent prop a formalism for specifying selected invariances in an adaptive network. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 895-903, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Hence, instead of just fitting the target values V target (s t ), NeuroChess also fits these target slopes. This is done using the Tangent-Prop algorithm <ref> [13] </ref>. The complete NeuroChess learning architecture is depicted in Fig. 2. The target slopes provide a first-order approximation to the relevance of each chess board feature in the goodness of a board position.
Reference: [14] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <year> 1988. </year>
Reference-contexts: In this paper, we are interested in learning to play chess from the final outcome of games. One of the earliest approaches, which learned solely by playing itself, is Samuel's famous checker player program [10]. His approach employed temporal difference learning (in short: TD) <ref> [14] </ref>, which is a technique for recursively learning an evaluation function. Recently, Tesauro reported the successful application of TD to the game of Backgammon, using artificial neural network representations [16]. <p> This paper describes the NeuroChess approach, discusses several training issues in the domain of chess, and presents results which elucidate some of its strengths and weaknesses. 2 Temporal Difference Learning in the Domain of Chess Temporal difference learning (TD) <ref> [14] </ref> comprises a family of approaches to prediction in cases where the event to be predicted may be delayed by an unknown number of time steps. In the context of game playing, TD methods have frequently been applied to learn functions which predict the final outcome of games.
Reference: [15] <author> Prasad Tadepalli. </author> <title> Planning in games using approximately learned macros. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> pages 221-223, </pages> <address> Ithaca, NY, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: For example, Schafer [11] reports a system just like Tesauro's TD-Gammon, applied to learning to play certain chess endgames. Gherrity [6] presented a similar system which he applied to entire chess games. Both approaches learn purely inductively from the final outcome of games. Tadepalli <ref> [15] </ref> applied a lazy version of explanation-based learning [5, 7] to endgames in chess. His approach learns from the final outcome, too, but unlike the inductive neural network approaches listed above it learns analytically, by analyzing and generalizing experiences in terms of chess-specific knowledge. <p> Hence, quite a few versions of a knight fork have to be experienced in order to generalize accurately. In a domain as complex as chess, such an approach might require unreasonably large amounts of training data. Explanation-based methods (EBL) <ref> [5, 7, 15] </ref> generalize more accurately from less training data. They rely instead on the availability of domain knowledge, which they use for explaining and generalizing training examples.
Reference: [16] <author> Gerald J. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <year> 1992. </year>
Reference-contexts: His approach employed temporal difference learning (in short: TD) [14], which is a technique for recursively learning an evaluation function. Recently, Tesauro reported the successful application of TD to the game of Backgammon, using artificial neural network representations <ref> [16] </ref>. While his TD-Gammon approach plays grandmaster-level backgammon, recent attempts to reproduce these results in the context of Go [12] and chess have been less successful. For example, Schafer [11] reports a system just like Tesauro's TD-Gammon, applied to learning to play certain chess endgames. <p> It decays V exponentially in time and hence favors early over late success. Notice that in NeuroChess V is represented by an artificial neural network, which is trained to fit the target values V target obtained via Eqs. (1) and (2) (cf. <ref> [6, 11, 12, 16] </ref>). 3 Explanation-Based Neural Network Learning In a domain as complex as chess, pure inductive learning techniques, such as neural network Back-Propagation, suffer from enormous training times.
Reference: [17] <author> Sebastian Thrun and Anton Schwartz. </author> <title> Issues in using function approximation for reinforcement learning. </title> <editor> In M. Mozer, P. Smolensky, D. Touretzky, J. Elman, and A. Weigend, editors, </editor> <booktitle> Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <address> Hillsdale, NJ, 1993. </address> <publisher> Erlbaum Associates. </publisher>
Reference-contexts: Indeed, without discounting the evaluation function predicts the probability for winningin the ideal case. In practice, however, random disturbations of the evaluation function can seriously hurt learning, for reasons given in <ref> [4, 17] </ref>. Empirically we found that learning failed completely when no discount factor was used. Currently, NeuroChess uses fl = 0:98. Learning rate. TD approaches minimize a Bellman equation [2]. In the NeuroChess domain, a close-to-optimal approximation of the Bellman equation is the constant function V (s) 0.
References-found: 17

