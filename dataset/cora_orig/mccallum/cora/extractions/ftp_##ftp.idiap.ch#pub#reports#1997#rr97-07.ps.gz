URL: ftp://ftp.idiap.ch/pub/reports/1997/rr97-07.ps.gz
Refering-URL: http://www.idiap.ch/~perry/allpubs.html
Root-URL: http://www.idiap.ch/~perry/allpubs.html
Title: E  IDIAP Martigny Valais Suisse Mixtures of Experts Estimate A Posteriori Probabilities  
Author: E R H E R R P Perry Moerland 
Note: internet  published in Proceedings of The International Conference on Artificial Neural Networks (ICANN'97), Lausanne, Switzerland, October 1997,  
Address: I  P.O.Box 592 Martigny Valais Switzerland  
Affiliation: I  for Perceptive Artificial Intelligence  
Pubnum: IDIAP-RR 97-07  
Email: e-mail secretariat@idiap.ch  e-mail: Perry.Moerland@idiap.ch  
Phone: phone +41 27 721 77 11 fax +41 27 721 77 12  499-504  
Date: September 97  
Web: http://www.idiap.ch  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Christopher M. Bishop. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1995. </year>
Reference-contexts: IDIAP-RR 97-07 2 x x x Gating network x g g 2 y Expert 1 Expert 3Expert 2 A probabilistic interpretation of a ME can be given in the context of mixture models for conditional probability distributions (see section 6.4 in <ref> [1] </ref>): p (tjx) = j=1 where the OE j represent the conditional densities of target vector t for expert j. <p> the Expectation-Maximization (EM) algorithm [8]. 3 Estimating Posterior Probabilities A standard way to motivate error functions is from the principle of maximum likelihood of the (independently distributed) training data with input vectors x n and target vectors t n : fx n ; t n g (see section 6.1 in <ref> [1] </ref>): Y p (x n ; t n ) = n where dependence of p (x n ; t n ) and p (t n jx n ) on the network parameters has been left implicit. <p> The solution of these equations will then result in expressions for g j (x) and y j (x) at the minimum of E (along the lines of section 6.1.3 of <ref> [1] </ref> for the sum-of-squares error function). Defining: E 0 = ln j=1 we are then interested in the following two functional derivatives set to zero. For the gating network: ffiE ffiz j = @E 0 and for the expert network: ffiE ffiy jc = @E 0 In section 6.4 of [1], <p> <ref> [1] </ref> for the sum-of-squares error function). Defining: E 0 = ln j=1 we are then interested in the following two functional derivatives set to zero. For the gating network: ffiE ffiz j = @E 0 and for the expert network: ffiE ffiy jc = @E 0 In section 6.4 of [1], the partial derivative for the gating network occurring in (6) have been calculated in the context of a gradient descent algorithm for the mixture model (3). <p> The functional derivative set to zero with respect to the expert network outputs is (substituting (16) in (13)): ffiE ffia jc = ( j (x; t)y jc j (x; t)t c ) p (tjx)p (x) dt = 0: (17) 3.2 Gaussian Conditional Density In section 6.4 of <ref> [1] </ref> mixture models are considered with multi-dimensional Gaussian conditional dens ities (where the covariance matrix is the identity matrix) as mixture components: OE j (t n jx n ) = (2) (d=2) exp 2 ; (18) IDIAP-RR 97-07 5 where d is the dimensionality of t. <p> This is exactly the same as for the outputs of a network trained by minimizing the sum-of-squares or cross-entropy error functions <ref> [1] </ref>[10]. It is a well-known result that for a classification problem with 1-of-c coding the conditional average of the target data is (see, for example, section 6.6 in [1]) : y c (x) = ht c jxi = P (C c jx); so that the outputs of a ME do indeed estimate the a posteriori probability that x belongs to class C c .
Reference: [2] <author> Herve Bourlard and Nelson Morgan. </author> <title> Connectionist Speech Recognition: A Hybrid Approach. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction It is well-known that for artificial neural networks trained by minimizing sum-of-squares or cross-entropy error functions for a classification problem, the optimal network outputs approximate the a posteriori probabilities of class membership <ref> [2] </ref> [10]. This property is a very useful one, especially when the network outputs are to be used in a further decision-making stage (e.g. rejection thresholds) or integrated in other statistical pattern recognition methods (as in hybrid NN-HMMs). <p> It is well-known that at the global minimum of these error functions when trained for classification problems, the optimal network outputs approximate a posteriori probabilities (independent of the network topology) <ref> [2] </ref> [6]. In this paper, it is shown that the minimization IDIAP-RR 97-07 3 of the ME error function based on a mixture of conditional densities (3) also leads to network outputs that estimate a posteriori probabilities.
Reference: [3] <author> J. S. Bridle. </author> <title> Probabilistic interpretation of feedforward classification network outputs with relationships to statistical pattern recognition. </title> <editor> In F. Fogelman Soulie and J. Herault, editors, Neurocomputing: </editor> <booktitle> Algorithms, Architectures, and Applications, </booktitle> <pages> pages 227-236. </pages> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: In order to ensure this probabilistic interpretation, the activation function for the outputs of the gating network is chosen to be the soft-max function <ref> [3] </ref>: g j = P m ; (2) where the z i are the gating network outputs before thresholding. This soft-max function makes that the gating network outputs sum to unity and are non-negative; thus implementing the (soft) competition between the experts.
Reference: [4] <author> Jurgen Fritsch, Michael Finke, and Alex Waibel. </author> <title> Context-dependent hybrid HME/HMM speech recognition using polyphone clustering decision trees. </title> <booktitle> In Proceedings of ICASSP-97, </booktitle> <year> 1997. </year>
Reference: [5] <author> Mariano Giaquinta and Stefan Hildebrandt. </author> <title> Calculus of Variations. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1996. </year>
Reference-contexts: an integral: E = ln @ j=1 1 A p (t; x) dtdx; factoring the joint distribution: E = ln @ j=1 1 A p (tjx)p (x) dtdx: The interpretation of the ME outputs when this error function is minimized, can be obtained by setting to zero the functional derivatives <ref> [5] </ref> of E with respect to the gating network outputs z j (x) and the expert network outputs y jc (x).
Reference: [6] <author> J. B. Hampshire and B. Pearlmutter. </author> <title> Equivalence proofs for multilayer perceptron classifiers and the Bayesian discriminant function. </title> <editor> In D. S. Touretzky, J. L. Elman, T. J. Sejnowski, and G. E. Hinton, editors, </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School, </booktitle> <pages> pages 159-172, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is well-known that at the global minimum of these error functions when trained for classification problems, the optimal network outputs approximate a posteriori probabilities (independent of the network topology) [2] <ref> [6] </ref>. In this paper, it is shown that the minimization IDIAP-RR 97-07 3 of the ME error function based on a mixture of conditional densities (3) also leads to network outputs that estimate a posteriori probabilities.
Reference: [7] <author> Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1) </volume> <pages> 79-87, </pages> <year> 1991. </year>
Reference: [8] <author> Michael I. Jordan and Robert A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6(2) </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: The standard choices for gating and expert networks are generalized linear models <ref> [8] </ref> and multilayer perceptrons [12]. The output vector of a ME is the weighted (by the gating network outputs) mean of the expert outputs: y (x) = j=1 The gating network outputs g j (x) can be regarded as the probability that input x is attributed to expert j. <p> soft-max function in the gating network and the fact that the OE j are densities guarantee that the distribution is normalized: R As outlined in the next section this distribution forms the basis for the ME error function which can be optimized using gradient descent or the Expectation-Maximization (EM) algorithm <ref> [8] </ref>. 3 Estimating Posterior Probabilities A standard way to motivate error functions is from the principle of maximum likelihood of the (independently distributed) training data with input vectors x n and target vectors t n : fx n ; t n g (see section 6.1 in [1]): Y p (x n
Reference: [9] <author> Geoffrey J. McLachlan and Kaye E. Basford. </author> <title> Mixture Models: Inference and Applications to Clustering. </title> <publisher> Marcel Dekker, Inc., </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The divide-and-conquer approach has shown particularly useful in attributing experts to different regimes in piece-wise stationary time series [12], modeling discontinuities in the input-output mapping, and classification problems [4][11]. The ME error function is based on the interpretation of MEs as a mixture model <ref> [9] </ref> with conditional densities as mixture components (for the experts) and gating network outputs as mixing coefficients. This error function is in fact a generalization of the sum-of-squares and cross-entropy error functions which arise in the special case of a ME with only one expert network.
Reference: [10] <author> M. D. Richard and R. P. Lippmann. </author> <title> Neural network classifiers estimate Bayesian a posteriori probabilities. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 461-483, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction It is well-known that for artificial neural networks trained by minimizing sum-of-squares or cross-entropy error functions for a classification problem, the optimal network outputs approximate the a posteriori probabilities of class membership [2] <ref> [10] </ref>. This property is a very useful one, especially when the network outputs are to be used in a further decision-making stage (e.g. rejection thresholds) or integrated in other statistical pattern recognition methods (as in hybrid NN-HMMs).
Reference: [11] <author> S. R. Waterhouse and A. J. Robinson. </author> <title> Classification using hierarchical mixtures of experts. </title> <booktitle> In Proceedings 1994 IEEE Workshop on Neural Networks for Signal Processing, </booktitle> <pages> pages 177-186, </pages> <address> Long Beach CA, 1994. </address> <publisher> IEEE Press. </publisher>
Reference: [12] <author> Andreas S. Weigend, Morgan Mangeas, and Ashok N. Srivastava. </author> <title> Nonlinear gated experts for time series: Discovering regimes and avoiding overfitting. </title> <journal> International Journal of Neural Systems, </journal> <volume> 6 </volume> <pages> 373-399, </pages> <year> 1995. </year>
Reference-contexts: In particular, the gating network of a ME learns to partition the input space (in a soft way, so overlaps are possible) and attributes expert networks to these different regions. The divide-and-conquer approach has shown particularly useful in attributing experts to different regimes in piece-wise stationary time series <ref> [12] </ref>, modeling discontinuities in the input-output mapping, and classification problems [4][11]. The ME error function is based on the interpretation of MEs as a mixture model [9] with conditional densities as mixture components (for the experts) and gating network outputs as mixing coefficients. <p> The standard choices for gating and expert networks are generalized linear models [8] and multilayer perceptrons <ref> [12] </ref>. The output vector of a ME is the weighted (by the gating network outputs) mean of the expert outputs: y (x) = j=1 The gating network outputs g j (x) can be regarded as the probability that input x is attributed to expert j.
References-found: 12

