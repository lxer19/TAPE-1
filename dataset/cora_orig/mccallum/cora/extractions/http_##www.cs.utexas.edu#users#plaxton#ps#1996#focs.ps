URL: http://www.cs.utexas.edu/users/plaxton/ps/1996/focs.ps
Refering-URL: http://www.cs.utexas.edu/users/plaxton/html/abc.html
Root-URL: 
Title: Fast Fault-Tolerant Concurrent Access to Shared Objects  
Author: C. Greg Plaxton Rajmohan Rajaraman 
Abstract: We consider a synchronous model of distributed computation in which n nodes communicate via point-to-point messages, subject to the following constraints: (i) in a single "step", a node can only send or receive O(log n) words, and (ii) communication is unreliable in that a constant fraction of all messages are lost at each step due to node and/or link failures. We design and analyze a simple local protocol for providing fast concurrent access to shared objects in this faulty network environment. In our protocol, clients use a hashing-based method to access shared objects. When a large number of clients attempt to read a given object at the same time, the object is rapidly replicated to an appropriate number of servers. Once the necessary level of replication has been achieved, each remaining request for the object is serviced within O(1) expected steps. Our protocol has practical potential for supporting high levels of concurrency in distributed file systems over wide area networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon and J. H. Spencer. </author> <title> The Probabilistic Method. </title> <publisher> Wiley, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: The experiment of Definition 6.1 can be analyzed using standard probabilistic techniques such as Chernoff bounds [6] and Azuma's inequality <ref> [1] </ref>.
Reference: [2] <author> T. E. Anderson, M. D. Dahlin, J. N. Neefe, D. A. Patterson, D. S. Rosselli, and R. Y. Wang. </author> <title> Serverless network file systems. </title> <booktitle> In Proceedings of the 15th Symposium on Operating Systems Principles, </booktitle> <pages> pages 109-126, </pages> <year> 1995. </year>
Reference-contexts: See Section 4 for a formal statement of our main results. We remark that existing implementations for handling concurrent access to shared objects (e.g., <ref> [2, 5, 9] </ref>) do not provide fast concurrent access in the sense considered in this paper. While these schemes incorporate replication of objects, the only way for a client to determine where the copies of a given object are stored is to consult the "manager" of the object.
Reference: [3] <author> Y. Aumann, Z. Kedem, K. V. Palem, and M. O. Ra-bin. </author> <title> Highly efficient asynchronous execution of large-grained parallel programs. </title> <booktitle> In Proceedings of the 34th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 271-280, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Thus, IDA can be used to obtain fault-tolerance with only a constant factor space penalty by setting m to fi (log n) and k to fi (m), e.g., k = 2m. This powerful technique is used by Aumann et al. <ref> [3] </ref> as part of an efficient scheme for emulating large-grained PRAM programs on an asynchronous parallel machine. In this paper, we use the same technique to store the "primary" copy of each object.
Reference: [4] <author> E. Berlekamp and L. Welch. </author> <title> Error correction of algebraic block codes. </title> <type> U.S. Patent Number 4,633,470. </type>
Reference-contexts: Unless the noisy fragments can be easily identified as such, the client cannot efficiently reconstruct the object using IDA. In such a noisy setting, it would be worthwhile to consider variants of our protocol based on the Berlekamp-Welch decoder <ref> [4] </ref> (see also [15, Appendix A]), which tolerates noise in a constant fraction of the fragments. We would like to extend our protocols to other interesting models of distributed computation that incorporate asynchrony or locality information.
Reference: [5] <author> M. A. </author> <title> Blaze. Caching in large-scale distributed file systems. </title> <type> Technical Report TR-397-92, </type> <institution> Department of Computer Science, Princeton University, </institution> <month> January </month> <year> 1993. </year> <type> PhD Thesis. </type>
Reference-contexts: See Section 4 for a formal statement of our main results. We remark that existing implementations for handling concurrent access to shared objects (e.g., <ref> [2, 5, 9] </ref>) do not provide fast concurrent access in the sense considered in this paper. While these schemes incorporate replication of objects, the only way for a client to determine where the copies of a given object are stored is to consult the "manager" of the object.
Reference: [6] <author> H. Chernoff. </author> <title> A measure of the asymptotic efficiency for tests of a hypothesis based on the sum of observations. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 23 </volume> <pages> 493-509, </pages> <year> 1952. </year>
Reference-contexts: By a straightforward Chernoff-type argument <ref> [6] </ref>, we can show that a constant fraction of the client requests for A are satisfied at the current step, establishing Invariant 2. Cache management. <p> The experiment of Definition 6.1 can be analyzed using standard probabilistic techniques such as Chernoff bounds <ref> [6] </ref> and Azuma's inequality [1].
Reference: [7] <author> V. Chvatal. </author> <title> The tail of the hypergeometric distribution. </title> <journal> Discrete Mathematics, </journal> <volume> 25 </volume> <pages> 285-287, </pages> <year> 1979. </year>
Reference-contexts: It follows from bounds on the tail of the hypergeometric distribution <ref> [7] </ref> that a constant fraction of the messages in N (A j ; i; 2t+ 1) are attempted by nodes in U . Therefore, jN 0 (A j ; i; 2t + 1)j is (jN (A j ; i; 2t + 1)j) whp. <p> Since the mapping of servers in B i (A j ) is independent of M (t 0 ) n M (A j ; i; t 0 ), it follows from bounds on the tail of the hypergeomet-ric distribution <ref> [7] </ref> that jW y j is at least c 4 jB i (A j )j whp, where c 4 can be set arbitrarily close to 1 for appropriate values of c 0 , 0 , 1 , and 2 .
Reference: [8] <author> S. Deering and D. Cheriton. </author> <title> Multicast routing in datagram internetworks and extended LANs. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 85-111, </pages> <year> 1990. </year>
Reference-contexts: Most of the details of our protocol are concerned with ensuring fast access to popular objects. A variety of other well-known methods have been used for solving this problem, including broadcast, combining [14], and multicast <ref> [8] </ref>. However, the class of architectures that support the efficient implementation of these methods is restricted. For example, a single-bus network can efficiently support broadcast, which enables an arbitrary subset of the processors to obtain copies of a single object at the same time.
Reference: [9] <author> J. S. Gwertzman and M. Seltzer. </author> <title> The case for geographical push-caching. </title> <booktitle> In Proceedings of the 5th Workshop on Hot Topics in Operating Systems, </booktitle> <pages> pages 51-57, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: See Section 4 for a formal statement of our main results. We remark that existing implementations for handling concurrent access to shared objects (e.g., <ref> [2, 5, 9] </ref>) do not provide fast concurrent access in the sense considered in this paper. While these schemes incorporate replication of objects, the only way for a client to determine where the copies of a given object are stored is to consult the "manager" of the object.
Reference: [10] <author> R. Karp, M. Luby, and F. Meyer auf der Heide. </author> <title> Efficient PRAM simulation on a distributed memory machine. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 318-326, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: length), we assume that a worst-case adversary determines which subset of the messages of total size c 0 log n are successfully received by a given node if the c 0 log n limit on total size would otherwise be exceeded. (The related c-arbitrary DMM model of Karp et al. <ref> [10] </ref> does not take into account contention among clients trying to access the same object and hence is not well-suited for our study.) Message types. Our protocol makes use of a constant number of different types of messages.
Reference: [11] <author> P. D. MacKenzie, C. G. Plaxton, and R. Rajaraman. </author> <title> On contention resolution protocols and associated probabilistic phenomena. </title> <booktitle> In Proceedings of the 26th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 153-162, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: As in the c-arbitrary crossbar model of MacKenzie et al. <ref> [11] </ref> (where all messages have unit length), we assume that a worst-case adversary determines which subset of the messages of total size c 0 log n are successfully received by a given node if the c 0 log n limit on total size would otherwise be exceeded. (The related c-arbitrary DMM
Reference: [12] <author> C. G. Plaxton and R. Rajaraman. </author> <title> Fast fault-tolerant concurrent access to shared objects. </title> <type> Technical Report TR-96-21, </type> <institution> Department of Computer Science, University of Texas at Austin, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: In the second part (Section 6.2), we restrict our attention to the fixed probability distribution model and prove Therorem 1. Due to space constraints, we omit most of the proofs in this abstract. We refer the reader to the full version of the paper <ref> [12] </ref> for complete proofs of the results stated in Section 4. 6.1 Protocol Properties Definition 6.1 Let X denote a set of balls numbered 0 through jXj 1. Let U denote a set of bins. <p> To simplify the presentation, we assume in this section that there is no contention among messages of distinct types. The message priorities can easily be used to remove this assumption <ref> [12] </ref>. Let size (ff) denote the number of words in a message of type ff. <p> In this section, we describe our algorithm for handling write operations. We consider two different approaches: the write-and-update and the invalidate-and-write protocols. We briefly describe both protocols here. We refer the reader to the full version of the paper <ref> [12] </ref> for details. At a given time step, any number of clients may attempt to simultaneously initiate a write operation on some object A. The first part of each protocol consists of a simple randomized leader election procedure to select one of these clients to actually write the object A.
Reference: [13] <author> M. O Rabin. </author> <title> Efficient dispersal of information for security, load balancing and fault tolerance. </title> <journal> JACM, </journal> <volume> 36 </volume> <pages> 335-348, </pages> <year> 1989. </year>
Reference-contexts: Unfortunately, this results in an (log n)-fold increase in the space needed to store each object. However, the theory of erasure codes provides a convenient method for achieving fault-tolerance while paying only a constant factor space penalty. For example, using Rabin's Information Dispersal Algorithm <ref> [13] </ref> (IDA), for any k &gt; m, a given b-bit string can be encoded as a set of k (b=m)-bit strings of length m, with the property that any m of the (b=m)-bit strings suffice to reconstruct the original b-bit string. <p> In our scheme, we choose to replicate whole copies of popular objects, as opposed to fragments, so that the encode-decode overhead associated with IDA can be avoided on retrieval of popular objects. (This may be viewed as a minor optimization since the overhead of IDA is actually quite small <ref> [13] </ref>.) At a high level, our protocol achieves fast concurrent access by enforcing the following two invariants.
Reference: [14] <author> A. G. Ranade. </author> <title> How to emulate shared memory. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 42 </volume> <pages> 307-326, </pages> <year> 1991. </year>
Reference-contexts: Most of the details of our protocol are concerned with ensuring fast access to popular objects. A variety of other well-known methods have been used for solving this problem, including broadcast, combining <ref> [14] </ref>, and multicast [8]. However, the class of architectures that support the efficient implementation of these methods is restricted. For example, a single-bus network can efficiently support broadcast, which enables an arbitrary subset of the processors to obtain copies of a single object at the same time.
Reference: [15] <author> M. Sudan. </author> <title> Efficient Checking of Polynomials and Proofs and the Hardness of Approximation Problems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of California at Berkeley, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: Unless the noisy fragments can be easily identified as such, the client cannot efficiently reconstruct the object using IDA. In such a noisy setting, it would be worthwhile to consider variants of our protocol based on the Berlekamp-Welch decoder [4] (see also <ref> [15, Appendix A] </ref>), which tolerates noise in a constant fraction of the fragments. We would like to extend our protocols to other interesting models of distributed computation that incorporate asynchrony or locality information.
Reference: [16] <author> L. Valiant. </author> <title> A combining mechanism for parallel computers. </title> <type> Technical Report TR-24-92, </type> <institution> Center for Research in Computing Technology, Harvard University, </institution> <month> January </month> <year> 1992. </year>
Reference-contexts: does not increase dramatically during subsequent steps, each of the outstanding requests is serviced in O (1) expected steps, and in O (log n) steps whp 1 .) The methods used to establish these invariants, discussed in Section 3, are loosely related to Valiant's hashing-based combining mechanism for parallel computers <ref> [16] </ref>. However, the focus of our algorithm design and analysis is different, since Valiant is primarily concerned with the problem of CRCW PRAM emulation, while we have optimized our protocol to obtain fast performance (e.g., expected O (1) time) on a more restricted class of access patterns.
Reference: [17] <author> L. G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33 </volume> <pages> 103-111, </pages> <year> 1990. </year>
Reference-contexts: For this reason, we have chosen to implement and evaluate the performance of our protocol under a model of computation that may be loosely viewed as a variant of Valiant's bulk-synchronous parallel (BSP) model <ref> [17] </ref>. As in the BSP model, we assume the existence of a simple point-to-point router with no built-in combining or multicast capability. (See [17] for a detailed justification of this assumption.) In order to demonstrate the fault-tolerance of our protocol, our model of computation incorporates both static and dynamic node faults <p> implement and evaluate the performance of our protocol under a model of computation that may be loosely viewed as a variant of Valiant's bulk-synchronous parallel (BSP) model <ref> [17] </ref>. As in the BSP model, we assume the existence of a simple point-to-point router with no built-in combining or multicast capability. (See [17] for a detailed justification of this assumption.) In order to demonstrate the fault-tolerance of our protocol, our model of computation incorporates both static and dynamic node faults as well as a notion of faulty communication. <p> Receiving messages. In a BSP-like model <ref> [17] </ref>, where communication is assumed to take the form of an h-relation, we might now tend to add a requirement that the total number of words in all messages destined to a single node in one step must be O (log n).
References-found: 17

