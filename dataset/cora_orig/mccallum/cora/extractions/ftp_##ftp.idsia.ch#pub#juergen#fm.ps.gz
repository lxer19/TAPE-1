URL: ftp://ftp.idsia.ch/pub/juergen/fm.ps.gz
Refering-URL: http://www.idsia.ch/~juergen/locoface/newlocoface.html
Root-URL: 
Email: hochreit@informatik.tu-muenchen.de  juergen@idsia.ch  
Title: FLAT MINIMA Neural Computation 9(1):1-42 (1997)  
Author: Sepp Hochreiter Jurgen Schmidhuber 
Date: March 1996  
Web: http://www7.informatik.tu-muenchen.de/~hochreit  http://www.idsia.ch/~juergen  
Address: 80290 Munchen, Germany  Corso Elvezia 36 6900 Lugano, Switzerland  
Affiliation: Fakultat fur Informatik Technische Universitat Munchen  IDSIA  
Abstract: We present a new algorithm for finding low complexity neural networks with high generalization capability. The algorithm searches for a "flat" minimum of the error function. A flat minimum is a large connected region in weight-space where the error remains approximately constant. An MDL-based, Bayesian argument suggests that flat minima correspond to "simple" networks and low expected overfitting. The argument is based on a Gibbs algorithm variant and a novel way of splitting generalization error into underfitting and overfitting error. Unlike many previous approaches, ours does not require Gaussian assumptions and does not depend on a "good" weight prior instead we have a prior over input/output functions, thus taking into account net architecture and training set. Although our algorithm requires the computation of second order derivatives, it has backprop's order of complexity. Automatically, it effectively prunes units, weights, and input lines. Various experiments with feedforward and recurrent nets are described. In an application to stock market prediction, flat minimum search outperforms (1) conventional backprop, (2) weight decay, (3) "optimal brain surgeon" / "optimal brain damage". We also provide pseudo code of the algorithm (omitted from the NC-version). 
Abstract-found: 1
Intro-found: 1
Reference: <author> Akaike, H. </author> <year> (1970). </year> <title> Statistical predictor identification. </title> <journal> Ann. Inst. Statist. Math., </journal> <volume> 22 </volume> <pages> 203-217. </pages>
Reference-contexts: Such assumptions are implicit in methods based on validation sets (Mosteller & Tukey, 1968; Stone, 1974; Eubank, 1988; Hastie & Tibshirani, 1993), e.g., "generalized cross validation" (Craven & Wahba, 1979; Golub et al., 1979), "final prediction error" <ref> (Akaike, 1970) </ref>, "generalized prediction error" (Moody & Utans, 1994; Moody, 1992). See also Holden (1994), Wang et al. (1994), Amari and Murata (1993), and Vapnik's "structural risk minimization" (Guyon et al., 1992; Vapnik, 1992). Constructive algorithms / pruning algorithms.
Reference: <author> Amari, S. and Murata, N. </author> <year> (1993). </year> <title> Statistical theory of learning curves under entropic loss criterion. </title> <journal> Neural Computation, </journal> <volume> 5(1) </volume> <pages> 140-153. </pages>
Reference: <author> Ash, T. </author> <year> (1989). </year> <title> Dynamic node creation in backpropagation neural networks. </title> <journal> Connection Science, </journal> <volume> 1(4) </volume> <pages> 365-375. </pages>
Reference: <author> Bishop, C. M. </author> <year> (1993). </year> <title> Curvature-driven smoothing: A learning algorithm for feed-forward networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(5) </volume> <pages> 882-884. </pages>
Reference: <author> Buntine, W. L. and Weigend, A. S. </author> <year> (1991). </year> <title> Bayesian back-propagation. </title> <journal> Complex Systems, </journal> <volume> 5 </volume> <pages> 603-643. </pages>
Reference: <author> Carter, M. J., Rudolph, F. J., and Nucci, A. J. </author> <year> (1990). </year> <title> Operational fault tolerance of CMAC networks. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 340-347. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Craven, P. and Wahba, G. </author> <year> (1979). </year> <title> Smoothing noisy data with spline functions: Estimating the correct degree of smoothing by the method of generalized cross-validation. </title> <journal> Numer. Math., </journal> <volume> 31 </volume> <pages> 377-403. </pages>
Reference: <author> Eubank, R. L. </author> <year> (1988). </year> <title> Spline smoothing and nonparametric regression. </title> <editor> In Farlow, S., editor, </editor> <title> Self-Organizing Methods in Modeling. </title> <publisher> Marcel Dekker, </publisher> <address> New York. </address>
Reference: <author> Fahlman, S. E. and Lebiere, C. </author> <year> (1990). </year> <title> The cascade-correlation learning algorithm. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 525-532. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Golub, G., Heath, H., and Wahba, G. </author> <year> (1979). </year> <title> Generalized cross-validation as a method for choosing a good ridge parameter. </title> <journal> Technometrics, </journal> <volume> 21 </volume> <pages> 215-224. </pages>
Reference: <author> Guyon, I., Vapnik, V., Boser, B., Bottou, L., and Solla, S. A. </author> <year> (1992). </year> <title> Structural risk minimization for character recognition. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 471-479. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hanson, S. J. and Pratt, L. Y. </author> <year> (1989). </year> <title> Comparing biases for minimal network construction with backpropagation. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <pages> pages 177-185. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hassibi, B. and Stork, D. G. </author> <year> (1993). </year> <title> Second order derivatives for network pruning: Optimal brain surgeon. </title>
Reference-contexts: Hence, OBS is not fully theoretically justified. Still, we used OBS to delete high weights, assuming that higher order derivatives are small if second order derivatives are. To obtain reasonable performance, we modified the original OBS procedure <ref> (notation following Hassibi and Stork, 1993) </ref>: * To detect the weight that deserves deletion, we use both L q = w 2 [H 1 ] qq (the original value used by Hassibi and Stork) and T q := @E @w q w q + 1 @ 2 E w 2 q <p> Examples are "sequential network construction" (Fahlman & Lebiere, 1990; Ash, 1989; Moody, 1989), input pruning (Moody, 1992; Refenes et al., 1994), unit pruning (White, 1989; Mozer & Smolensky, 1989; Levin et al., 1994), weight pruning, e.g. "optimal brain damage" (LeCun et al., 1990), "optimal brain surgeon" <ref> (Hassibi & Stork, 1993) </ref>. Hinton and van Camp (1993). They minimize the sum of two terms: the first is conventional error plus variance, the other is the distance R p (wjD 0 ) p (w) dw between posterior p (w j D 0 ) and weight prior p (w).
Reference: <editor> In S. J. Hanson, J. D. C. and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 164-171. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hastie, T. J. and Tibshirani, R. J. </author> <year> (1990). </year> <title> Generalized additive models. </title> <journal> Monographs on Statisics and Applied Probability, </journal> <volume> 43. </volume>
Reference: <author> Hinton, G. E. and van Camp, D. </author> <year> (1993). </year> <title> Keeping neural networks simple. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, Amsterdam, </booktitle> <pages> pages 11-18. </pages> <publisher> Springer. </publisher>
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1994). </year> <title> Flat minimum search finds simple nets. </title> <type> Technical Report FKI-200-94, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen. </institution>
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1995). </year> <title> Simplifying nets by discovering flat minima. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 529-536. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference-contexts: 1 BASIC IDEAS / OUTLINE Our algorithm tries to find a large region in weight space with the property that each weight vector from that region leads to similar small error. Such a region is called a "flat minimum" <ref> (Hochreiter & Schmidhuber, 1995) </ref>. To get an intuitive feeling for why a flat minimum is interesting, consider this: a "sharp" minimum (see figure 2) corresponds to weights which have to be specified with high precision.
Reference: <author> Holden, S. B. </author> <year> (1994). </year> <title> On the Theory of Generalization and Self-Structuring in Linearly Weighted Connectionist Networks. </title> <type> PhD thesis, </type> <institution> Cambridge University, Engineering Department. </institution>
Reference: <author> Kerlirzin, P. and Vallet, F. </author> <year> (1993). </year> <title> Robustness in multilayer perceptrons. </title> <journal> Neural Computation, </journal> <volume> 5(1) </volume> <pages> 473-482. </pages>
Reference: <author> Krogh, A. and Hertz, J. A. </author> <year> (1992). </year> <title> A simple weight decay can improve generalization. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 950-957. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kullback, S. </author> <year> (1959). </year> <title> Statistics and Information Theory. </title> <editor> J. </editor> <publisher> Wiley and Sons, </publisher> <address> New York. </address>
Reference-contexts: We will see that overfitting and underfitting error correspond to the two different error terms in our algorithm: decreasing one term is equivalent to decreasing E o , decreasing the other is equivalent to decreasing E u . Using the Kullback-Leibler distance <ref> (Kullback, 1959) </ref>, we measure the information conveyed by p (: j D 0 ), but not by p D 0 (: j D) (see figure 4).
Reference: <author> LeCun, Y., Denker, J. S., and Solla, S. A. </author> <year> (1990). </year> <title> Optimal brain damage. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 598-605. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Here H denotes the Hessian and H 1 its approximate inverse. We delete the weight causing minimal training set error (after tentative deletion). * Like with OBD <ref> (LeCun et al., 1990) </ref>, to prevent numerical errors due to small eigenvalues 11 of H, we do: if L q &lt; 0:00001 or T q &lt; 0:00001 or k I H 1 H k&gt; 10:0 (bad approximation of H 1 ), we only delete the weight detected in the previous step <p> Examples are "sequential network construction" (Fahlman & Lebiere, 1990; Ash, 1989; Moody, 1989), input pruning (Moody, 1992; Refenes et al., 1994), unit pruning (White, 1989; Mozer & Smolensky, 1989; Levin et al., 1994), weight pruning, e.g. "optimal brain damage" <ref> (LeCun et al., 1990) </ref>, "optimal brain surgeon" (Hassibi & Stork, 1993). Hinton and van Camp (1993).
Reference: <author> Levin, A. U., Leen, T. K., and Moody, J. E. </author> <year> (1994). </year> <title> Fast pruning using principal components. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 35-42. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Levin, E., Tishby, N., and Solla, S. </author> <year> (1990). </year> <title> A statistical approach to learning and generalization in layered neural networks. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(10) </volume> <pages> 1568-1574. </pages>
Reference: <author> MacKay, D. J. C. </author> <year> (1992a). </year> <title> Bayesian interpolation. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 415-447. </pages>
Reference-contexts: Unlike our approach, theirs explicitly prefers weights near zero. In addition, their approach appears to require much more computation time (due to second order derivatives in the error term). 7 LIMITATIONS / FINAL REMARKS / FUTURE RE SEARCH How to adjust ? Given recent trends in neural computing <ref> (see, e.g., MacKay, 1992a, 1992b) </ref>, it may seem like a step backwards that is adapted using an ad-hoc heuristic from Weigend et al., 1991. However, for determining in MacKay's style, one would have to compute the Hessian of the cost function. <p> This is impracticable. Also, to optimize the regularizing parameter (see MacKay, 1992b), we need to compute the function R d L w exp (B (w; X 0 )), but it is not obvious how: the "quick and dirty version" <ref> (MacKay, 1992a) </ref> cannot deal with the unknown constant * in B (w; X 0 ). Future work will investigate how to adjust without too much computational effort.
Reference: <author> MacKay, D. J. C. </author> <year> (1992b). </year> <title> A practical Bayesian framework for backprop networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 448-472. </pages>
Reference-contexts: However, for determining in MacKay's style, one would have to compute the Hessian of the cost function. Since our term B (w; X 0 ) includes first order derivatives, adjusting would require the computation of third order derivatives. This is impracticable. Also, to optimize the regularizing parameter <ref> (see MacKay, 1992b) </ref>, we need to compute the function R d L w exp (B (w; X 0 )), but it is not obvious how: the "quick and dirty version" (MacKay, 1992a) cannot deal with the unknown constant * in B (w; X 0 ).
Reference: <author> Matsuoka, K. </author> <year> (1992). </year> <title> Noise injection into inputs in back-propagation learning. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 22(3) </volume> <pages> 436-440. </pages>
Reference: <author> Minai, A. A. and Williams, R. D. </author> <year> (1994). </year> <title> Perturbation response in feedforward networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(5) </volume> <pages> 783-796. </pages>
Reference: <author> Mtller, M. F. </author> <year> (1993). </year> <title> Exact calculation of the product of the Hessian matrix of feed-forward network error functions and a vector in O(N) time. </title> <type> Technical Report PB-432, </type> <institution> Computer Science Department, Aarhus University, Denmark. </institution>
Reference: <author> Moody, J. E. </author> <year> (1989). </year> <title> Fast learning in multi-resolution hierarchies. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 1. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Moody, J. E. </author> <year> (1992). </year> <title> The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 847-854. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Moody, J. E. and Utans, J. </author> <year> (1994). </year> <title> Architecture selection strategies for neural networks: Application to corporate bond rating prediction. </title> <editor> In Refenes, A. N., editor, </editor> <title> Neural Networks in the Capital Markets. </title> <publisher> John Wiley & Sons. </publisher>
Reference: <author> Mosteller, F. and Tukey, J. W. </author> <year> (1968). </year> <title> Data analysis, including statistics. </title> <editor> In Lindzey, G. and Aronson, E., editors, </editor> <booktitle> Handbook of Social Psychology, </booktitle> <volume> Vol. 2. </volume> <publisher> Addison-Wesley. </publisher>
Reference: <author> Mozer, M. C. and Smolensky, P. </author> <year> (1989). </year> <title> Skeletonization: A technique for trimming the fat from a network via relevance assessment. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 1, </booktitle> <pages> pages 107-115. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Murray, A. F. and Edwards, P. J. </author> <year> (1993). </year> <title> Synaptic weight noise during MLP learning enhances fault-tolerance, generalisation and learning trajectory. </title> <editor> In S. J. Hanson, J. D. C. and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 491-498. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Neti, C., Schneider, M. H., and Young, E. D. </author> <year> (1992). </year> <title> Maximally fault tolerant neural networks. </title> <journal> In IEEE Transactions on Neural Networks, </journal> <volume> volume 3, </volume> <pages> pages 14-23. </pages>
Reference: <author> Nowlan, S. J. and Hinton, G. E. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight sharing. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 173-193. </pages>
Reference: <author> Opper, M. and Haussler, D. </author> <year> (1991). </year> <title> Calculation of the learning curve of Bayes optimal classification algorithm for learning a perceptron with noise. </title> <booktitle> In Computational Learning Theory: Proceedings of the Forth Annual Workshop. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: E (D n D 0 ; h) := p=n+1 k y p h (x p ) k 2 . E (D; h) = E (D 0 ; h) + E (D n D 0 ; h) holds. Learning. We use a variant of the Gibbs formalism <ref> (see Opper & Haussler, 1991, or Levin et al., 1990) </ref>. Consider a stochastic learning algorithm (random weight initialization, random learning rate).
Reference: <author> Pearlmutter, B. A. </author> <year> (1994). </year> <title> Fast exact multiplication by the Hessian. </title> <journal> Neural Computation, </journal> <volume> 6(1) </volume> <pages> 147-160. </pages>
Reference-contexts: Using the technique in <ref> (Pearlmutter, 1994) </ref>, recurrent networks can be dealt with as well. 27 A.4. PSEUDO CODE OF THE ALGORITHM Below the algorithm in pseudo code (using fast multiplication as in appendix A.3.2). Comments are marked by "**". Note: the pseudo code was omitted from the version for Neural Computation.
Reference: <author> Pearlmutter, B. A. and Rosenfeld, R. </author> <year> (1991). </year> <title> Chaitin-Kolmogorov complexity and generalization in neural networks. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 925-931. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Refenes, A. N., Francis, G., and Zapranis, A. D. </author> <year> (1994). </year> <title> Stock performance modeling using neural networks: A comparative study with regression models. Neural Networks. 37 Rehkugler, </title> <editor> H. and Poddig, T. </editor> <year> (1990). </year> <title> Statistische Methoden versus Kunstliche Neuronale Netzwerke zur Aktienkursprognose. </title> <type> Technical Report 73, </type> <institution> University Bamberg, Fakultat Sozial- und Wirtschaftswis-senschaften. </institution>
Reference: <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471. </pages>
Reference: <author> Schmidhuber, J. </author> <year> (1996). </year> <title> A general method for multi-agent learning and incremental self-improvement in unrestricted environments. </title> <editor> In Yao, X., editor, </editor> <booktitle> Evolutionary Computation: Theory and Applications. </booktitle> <publisher> Scientific Publ. Co., Singapore. </publisher>
Reference: <author> Schmidhuber, J. H. </author> <year> (1994a). </year> <title> Discovering problem solutions with low Kolmogorov complexity and high generalization capability. </title> <type> Technical Report FKI-194-94, </type> <institution> Fakultat fur Informatik, Technische Univer-sitat Munchen. </institution> <note> Short version in A. </note> <editor> Prieditis and S. Russell, eds., </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 488-496, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1995. </year>
Reference: <author> Schmidhuber, J. H. </author> <year> (1994b). </year> <title> On learning how to learn learning strategies. </title> <type> Technical Report FKI-198-94, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen. </institution>
Reference: <author> Shannon, C. E. </author> <year> (1948). </year> <title> A mathematical theory of communication (parts I and II). </title> <journal> Bell System Technical Journal, XXVII:379-423. </journal>
Reference-contexts: ED (w; w b w) is described by a Gaussian distribution with mean zero. Hence, the description length of ED (w; w b w) is ED (w; w b w) <ref> (Shannon, 1948) </ref>. w b , the center of the "bit-box", cannot be known before training. However, we do know the expected description length of the net function, which is ED mean + ~ B (w; X 0 )+c (c is a constant independent of w).
Reference: <author> Stone, M. </author> <year> (1974). </year> <title> Cross-validatory choice and assessment of statistical predictions. </title> <journal> Roy. Stat. Soc., </journal> <volume> 36 </volume> <pages> 111-147. </pages>
Reference: <author> Vapnik, V. </author> <year> (1992). </year> <title> Principles of risk minimization for learning theory. </title> <editor> In Moody, J. E., Hanson, S. J., and Lippman, R. P., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 831-838. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Wallace, C. S. and Boulton, D. M. </author> <year> (1968). </year> <title> An information theoretic measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11(2) </volume> <pages> 185-194. </pages>
Reference: <author> Wang, C., Venkatesh, S. S., and Judd, J. S. </author> <year> (1994). </year> <title> Optimal stopping and effective machine complexity in learning. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 303-310. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Weigend, A. S., Rumelhart, D. E., and Huberman, B. A. </author> <year> (1991). </year> <title> Generalization by weight-elimination with application to forecasting. </title> <editor> In Lippmann, R. P., Moody, J. E., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 875-882. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: FMS details. To control B (w; D 0 )'s influence during learning, its gradient is normalized and multiplied by the length of E (net (w); D 0 )'s gradient (same for weight decay, see below). is computed like in <ref> (Weigend et al., 1991) </ref> and initialized with 0. Absolute values of first order derivatives are replaced by 10 20 if below this value. We ought to judge a weight w ij as being pruned if ffiw ij (see equation (5) in section 4) exceeds the length of the weight range.
Reference: <author> White, H. </author> <year> (1989). </year> <title> Learning in artificial neural networks: A statistical perspective. </title> <journal> Neural Computation, </journal> <volume> 1(4) </volume> <pages> 425-464. </pages>
Reference: <author> Williams, P. M. </author> <year> (1994). </year> <title> Bayesian regularisation and pruning using a Laplace prior. </title> <type> Technical report, </type> <institution> School of Cognitive and Computing Sciences, University of Sussex, </institution> <address> Falmer, Brighton. </address>
Reference: <author> Wolpert, D. H. </author> <year> (1994a). </year> <title> Bayesian backpropagation over i-o functions rather than weights. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 200-207. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Choice of prior belief. Now we select some p (f ), our prior belief in target f . We introduce a formalism similar to Wolpert's <ref> (Wolpert, 1994a ) </ref>. p (f ) is defined as the probability of obtaining f = net (w) by choosing a w randomly according to p (w). Let us first have a look at Wolpert's formalism: p (f ) = R dwp (w)ffi (net (w) f ).
Reference: <author> Wolpert, D. H. </author> <year> (1994b). </year> <title> The relationship between PAC, the statistical physics framework, the Bayesian framework, and the VC framework. </title> <type> Technical Report SFI-TR-03-123, </type> <institution> Santa Fe Institute, </institution> <address> NM 87501. </address> <month> 38 </month>
Reference-contexts: This is left for future work. Notes on generalization error. If the prior distribution of targets p (f ) (see appendix A.1) is uniform (or if the distribution of prior distributions is uniform), no algorithm can obtain a lower expected generalization error than training error reducing algorithms <ref> (see, e.g., Wolpert, 1994b) </ref>. Typical target distributions in the real world are not uniform, however the real world appears to favor problem solutions with low algorithmic complexity. See, e.g., Schmidhuber (1994a). MacKay (1992a) suggests to search for alternative priors if the generalization error indicates a "poor regulariser".
References-found: 56

