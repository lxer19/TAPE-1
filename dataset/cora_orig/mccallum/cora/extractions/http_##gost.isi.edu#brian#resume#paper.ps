URL: http://gost.isi.edu/brian/resume/paper.ps
Refering-URL: http://gost.isi.edu/brian/resume/
Root-URL: http://www.isi.edu
Title: Using Finite State Automata to Produce Self-Optimization and Self-Control  
Author: Brian Tung Leonard Kleinrock 
Address: Los Angeles, CA 90024-1596 USA  
Affiliation: UCLA Computer Science Department,  
Abstract: A simple game provides a framework within which agents can spontaneously self-organize. In this paper, we present this game, and develop basic theory underlying a robust method for distributed coordination based on this game. This method makes use of finite state automata, one associated with each agent, which guide the agents. We give a new, general method of analysis of these systems, which previously had been studied only in limited cases. We also provide a physical example, which should hint at the type of problems resolvable using this method. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> V. A. Borovikov and V. I. Bryzgalov. </author> <title> A simple symmetric game between many automata. </title> <journal> Avtomat. Telemekh., </journal> <volume> Vol. 26(No. 4), </volume> <year> 1965. </year>
Reference-contexts: If the output is A 0 , the reward probability is r = r 0 ; if it is A 1 , the reward probability is r = r 1 . (We assume we are given r 0 ; r 1 2 <ref> [0; 1] </ref>.) Then, with probability r, the automaton is rewarded; with probability 1 r, it is penalized. This cycle is repeated: the automaton chooses either A 0 or A 1 , the corresponding reward probability is determined, and the automaton is rewarded or penalized. <p> Also, let a (t) represent the number of automata with output A 1 and f (t) the fraction of automata with output A 1 at time t. We assume the existence of a reward function r (f), defined by r: <ref> [0; 1] </ref> ! [0; 1]. For each moment t, we compute a reward probability r = r (f), whose value depends solely on the fraction f = f (t) (f = 0; 1=N; 2=N; . . . ; 1). <p> Also, let a (t) represent the number of automata with output A 1 and f (t) the fraction of automata with output A 1 at time t. We assume the existence of a reward function r (f), defined by r: <ref> [0; 1] </ref> ! [0; 1]. For each moment t, we compute a reward probability r = r (f), whose value depends solely on the fraction f = f (t) (f = 0; 1=N; 2=N; . . . ; 1). <p> Borovikov and Bryzgalov <ref> [1] </ref> show that when n = 1|that is, when the automata have two states|the behavior is not optimal; in fact, with probability one, f (t) approaches 1=2 in the limit as N; t ! 1.
Reference: [2] <editor> Douglas R. Hofstadter. Metamagical themas. </editor> <publisher> Scientific American, </publisher> <month> May </month> <year> 1983. </year>
Reference-contexts: Moreover, each player plays solely in a greedy fashion, each time voting the way that seems to give that player the best payoff. This is somewhat unexpected. Typically, greed leads to significantly suboptimal outcomes; an example of this is the prisoner's dilemma <ref> [2] </ref>. However, we will see that the method used here does not have this property, because each player effectively evaluates the success of an action jointly with the actions of the other players, rather than conditioning it on those 2 actions.
Reference: [3] <author> Leonard Kleinrock. </author> <title> Queueing Systems, Volume 1: Theory. </title> <publisher> John Wiley and Sons, </publisher> <year> 1975. </year>
Reference-contexts: This gives us the recurrence equation t i;i1 (r) = 1 + r [t i+1;i (r) + t i;i1 (r)]; 1 i &lt; n This recurrence equation can be solved by the usual z-transform techniques <ref> [3] </ref> to yield t i;i1 (r) = 2r 1 r ni+1 # for 1 &lt; i n, and n 1 1 r 1 (11) Immediately after a trigger transition, at least one of the automata|in particular, the one that made the trigger transition|must be in either state 1 or 1.
Reference: [4] <author> Leonard Kleinrock. </author> <title> On distributed systems performance. </title> <booktitle> In ITC Specialist Seminar: Computer Networks and ISDN Systems, </booktitle> <pages> pages 209-216, </pages> <year> 1990. </year>
Reference-contexts: Therefore, it is necessary for the machines to organize themselves without explicitly communicating control information. (Notice that we do not prohibit the machines from making deductions about each other's state, based on the state of the channel.) The method outlined here allows them to do that. (In <ref> [4] </ref>, we find mentioned the inherent cost in having the agents physically separated, due to the lack of global knowledge. In our approach, that cost is not being sidestepped; it is merely being distributed among the agents.) The remainder of this paper is organized as follows.
Reference: [5] <author> P. J. Ramadge and W. Wonham. </author> <title> Supervisory control of a class of discrete event processes. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> Vol. 25(No. 1), </volume> <year> 1987. </year>
Reference: [6] <author> M. L. Tsetlin. </author> <title> Finite Automata and Modeling the Simplest Forms of Behavior. </title> <type> PhD thesis, </type> <institution> V. A. Steklov Mathematical Institute, </institution> <year> 1964. </year>
Reference-contexts: Ramadge and Wonham [5][9] give a similar treatment by means of discrete event decision systems (DEDS), but the conditions differ, as will be detailed below. We introduce this scheme with a simple game, called the Gur Game by Tsetlin <ref> [6] </ref>. Imagine that we have N players, none of whom are aware of the others, and a referee. <p> We wish to devise an automaton that performs "well" (that is, it receives a greater proportion of rewards), where performance is measured relative to that of a "null" automaton, which simply chooses A 0 or A 1 randomly with probability 1/2. Tsetlin <ref> [6] </ref> gives the following design, which he calls L 2;2 . Let the automaton have two states, 1 and 1. If the current state is 1, the automaton chooses A 0 ; if it is 1, it chooses A 1 . <p> To improve performance, we can give the automaton more than two states. Suppose that it has 2n states, S n = fi; i j 1 i ng. This automaton is then said to have a memory size of n. (Tsetlin <ref> [6] </ref> calls this automaton L 2n;2 .) If the current state is negative, the automaton chooses A 0 ; if it is positive, it chooses A 1 . 4 If the automaton receives a reward, it stays in states n or n if it is in either of those states; otherwise, <p> Even though f can only take on one of N + 1 values, the function r (f ) is defined as accepting a continuous domain of values, both for historical reasons <ref> [6] </ref>, and because it allows for changes in the population. (We do not analyze the case of a changing population in this paper, however.) Each automaton then independently receives a stimulus x m (t), which is a binary valued random variable. <p> We then ask: is it possible to design (finite state) automata in such a way that (k fl ) is arbitrarily close to 1? (If k fl is not unique, then we simply sum over all optimal k.) The answer is yes, as noted by Tsetlin <ref> [6] </ref>, although he only describes the construction and behavior of the automata, and does not develop a general method of analysis. The automaton we examine is the one defined in the previous section; the state diagram is displayed in Figure 2. <p> We map states to outputs in a straightforward way. If s m (t) &lt; 0, then u m (t) = A 0 ; otherwise, u m (t) = A 1 . The automaton is said to be linear <ref> [6] </ref>; that is, state transitions occur only between adjacent states, except for the self transitions at states n and n.
Reference: [7] <author> Brian Tung. </author> <title> Distributed Control Using Finite State Automata. </title> <type> PhD thesis, </type> <institution> UCLA, </institution> <year> 1994. </year> <month> 24 </month>
Reference-contexts: to allow more than two outputs, by adding more "arms" to the automaton design. (Consider that our current model has two|a positively numbered arm, and a negatively numbered arm|and that the sign has no semantics other than to distinguish the two arms.) For complete details, the reader is referred to <ref> [7] </ref>. 4 Example: Cooperative Robotics The Commotion Laboratory 1 at UCLA conducts research into all aspects of mobile robots (or "mobots"). One of these is the prospect of some means of automated cooperation.
Reference: [8] <author> V. A. Volkonskiy. </author> <title> Asymptotic properties of the behavior of simple automata in a game. Probl. </title> <journal> Peredachi Inform., </journal> <volume> Vol. 1(No. 2), </volume> <year> 1965. </year>
Reference-contexts: Therefore, this behavior can be decomposed into 1. the probability distribution of a (t) during a plateau 2. the average length of a plateau The quantity (k) is simply a normalized product of these two factors. (Volkonskiy <ref> [8] </ref> makes use of this general method for the simple case where the reward function is of the form r (f) = r 0 for f f c , r (f ) = r 1 &lt; r 0 for f &gt; f c , where f c &lt; 1=2 is some
Reference: [9] <author> W. Wonham and P. J. Ramadge. </author> <title> Modular supervisory control of discrete-event systems. Mathematics of Control, Signals, </title> <journal> and Systems, </journal> <volume> Vol. 1(No. 1):13-30, 1988. 25 </volume>
References-found: 9

