URL: http://www.cs.brown.edu/people/tld/postscript/GivanLeachandDeanECP-97.ps
Refering-URL: http://www.cs.brown.edu/research/ai/publications/
Root-URL: http://www.cs.brown.edu
Email: Email: frlg,sml,tldg@cs.brown.edu  
Phone: Phone: (401) 863-7600 Fax: (401) 863-7657  
Title: Bounded Parameter Markov Decision Processes  
Author: Robert Givan and Sonia Leach and Thomas Dean 
Web: http://www.cs.brown.edu/people/frlg,sml,tldg  
Address: 115 Waterman Street, Providence, RI 02912, USA  
Affiliation: Department of Computer Science, Brown University  
Abstract: In this paper, we introduce the notion of an bounded parameter Markov decision process (BMDP) as a generalization of the familiar exact MDP. A bounded parameter MDP is a set of exact MDPs specified by giving upper and lower bounds on transition probabilities and rewards (all the MDPs in the set share the same state and action space). BMDPs form an efficiently solvable special case of the already known class of MDPs with imprecise parameters (MDPIPs). Bounded parameter MDPs can be used to represent variation or uncertainty concerning the parameters of sequential decision problems in cases where no prior probabilities on the parameter values are available. Bounded parameter MDPs can also be used in aggregation schemes to represent the variation in the transition probabilities for different base states aggregated together in the same aggregate state. We introduce interval value functions as a natural extension of traditional value functions. An interval value function assigns a closed real interval to each state, representing the assertion that the value of that state falls within that interval. An interval value function can be used to bound the performance of a policy over the set of exact MDPs associated with a given bounded parameter MDP. We describe an iterative dynamic programming algorithm called interval policy evaluation which computes an interval value function for a given BMDP and specified policy. Interval policy evaluation on a policy computes the most restrictive interval value function that is sound, i.e., that bounds the value function for in every exact MDP in the set defined by the bounded parameter MDP. We define optimistic and pessimistic notions of optimal policy, and provide a variant of value iteration [ Bellman, 1957 ] that we call interval value iteration which computes a policies for a BMDP that are optimal in these senses.
Abstract-found: 1
Intro-found: 1
Reference: [ Bellman, 1957 ] <author> Bellman, Richard 1957. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press. </publisher>
Reference: [ Bertsekas and Casta~non, 1989 ] <author> Bertsekas, D. P. and Casta~non, D. A. </author> <year> 1989. </year> <title> Adaptive aggregation for infinite horizon dynamic programming. </title> <journal> IEEE Transactions on Automatic Control 34(6) </journal> <pages> 589-598. </pages>
Reference: [ Boutilier and Dearden, 1994 ] <author> Boutilier, Craig and Dearden, Richard 1994. </author> <title> Using abstractions for decision theoretic planning with time constraints. </title> <booktitle> In Proceedings AAAI-94. AAAI. </booktitle> <pages> 1016-1022. </pages>
Reference: [ Boutilier et al., 1995a ] <author> Boutilier, Craig; Dean, Thomas; and Hanks, </author> <title> Steve 1995a. Planning under uncertainty: Structural assumptions and computational leverage. </title> <booktitle> In Proceedings of the Third European Workshop on Planning. </booktitle>
Reference-contexts: 1 Introduction The theory of Markov decision processes (MDPs) provides the semantic foundations for a wide range of problems involving planning under uncertainty <ref> [ Boutilier et al., 1995a, Littman, 1997 ] </ref> . In this paper, we introduce a generalization of Markov decision processes called bounded parameter Markov decision processes (BMDPs) that allows us to model uncertainty in the parameters that comprise an MDP.
Reference: [ Boutilier et al., 1995b ] <author> Boutilier, Craig; Dearden, Richard; and Goldszmidt, </author> <month> Moises </month> <year> 1995b. </year> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings IJCAI 14. IJCAII. </booktitle> <pages> 1104-1111. </pages>
Reference: [ Dean and Givan, 1997 ] <author> Dean, Thomas and Givan, Robert 1997. </author> <title> Model minimization in Markov decision processes. </title> <booktitle> In Proceedings AAAI-97. </booktitle> <publisher> AAAI. </publisher>
Reference: [ Dean et al., 1997 ] <author> Dean, Thomas; Givan, Robert; and Leach, </author> <title> Sonia 1997. Model reduction techniques for computing approximately optimal solutions for Markov decision processes. </title> <booktitle> In Thirteenth Conference on Uncertainty in Artificial Intelligence. </booktitle>
Reference-contexts: In a related paper, we have shown how BMDPs can be used as part of a strategy for efficiently approximating the solution of MDPs with very large state spaces and dynamics compactly encoded in a factored (or implicit) representation <ref> [ Dean et al., 1997 ] </ref> . In this paper, we focus exclusively on BMDPs, on the BMDP analog of value functions, called interval value functions, and on policy selection for a BMDP.
Reference: [ Littman et al., 1995 ] <author> Littman, Michael; Dean, Thomas; and Kaelbling, </author> <title> Leslie 1995. On the complexity of solving Markov decision problems. </title> <booktitle> In Eleventh Conference on Uncertainty in Artificial Intelligence. </booktitle> <pages> 394-402. </pages>
Reference-contexts: Theorem 9 guarantees the convergence of value iteration to the optimal value function. Similarly, we can specify an algorithm called policy evaluation which finds V by repeatedly apply V I starting with an initial v 0 2 V. The following theorem from <ref> [ Littman et al., 1995 ] </ref> states a convergence rate of value iteration and policy evaluation which can be derived using bounds on the precision needed to represent solutions to a linear program of limited precision (each algorithm can be viewed as solving a linear program). Theorem 10.
Reference: [ Littman, 1997 ] <author> Littman, Michael L. </author> <year> 1997. </year> <title> Probabilistic propositional planning: Representations and complexity. </title> <booktitle> In Proceedings AAAI-97. </booktitle> <publisher> AAAI. </publisher>
Reference-contexts: 1 Introduction The theory of Markov decision processes (MDPs) provides the semantic foundations for a wide range of problems involving planning under uncertainty <ref> [ Boutilier et al., 1995a, Littman, 1997 ] </ref> . In this paper, we introduce a generalization of Markov decision processes called bounded parameter Markov decision processes (BMDPs) that allows us to model uncertainty in the parameters that comprise an MDP.
Reference: [ Lovejoy, 1991 ] <author> Lovejoy, William S. </author> <year> 1991. </year> <title> A survey of algorithmic methods for partially observed Markov decision processes. </title> <journal> Annals of Operations Research 28 </journal> <pages> 47-66. </pages>
Reference: [ Puterman, 1994 ] <author> Puterman, Martin L. </author> <year> 1994. </year> <title> Markov Decision Processes. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: where in the case of ^ F we require 0 l u 1. 3 To ensure that ^ F admits well-formed transition functions, we require that for 2 In this paper, we focus on expected discounted cumulative reward as a performance criterion, but other criteria, e.g., total or average reward <ref> [ Puterman, 1994 ] </ref> , are also applicable to bounded parameter MDPs. 3 To simplify the remainder of the paper, we assume that the reward bounds are always tight, i.e., that for all q 2 Q, for some real l, ^ R (q) = [l; l], and we refer to l
Reference: [ Satia and Lave, 1973 ] <author> Satia, J. K. and Lave, R. E. </author> <year> 1973. </year> <title> Markovian decision processes with uncertain transition probabilities. </title> <journal> Operations Research 21 </journal> <pages> 728-740. </pages>
Reference-contexts: ] allows an arbitrary linear program to define the bounds on the transition probabilities (and allows no imprecision in the reward parameters)| as a result, the solution technique presented appeals to linear programming at each iteration of the solution algorithm rather than exploit the specific structure available in a BMDP. <ref> [ Satia and Lave, 1973 ] </ref> mention the restriction to BMDPs but give no special algorithms to exploit this restriction. <p> Our work, while originally developed independently of the MDPIP literature, follows similar lines to <ref> [ Satia and Lave, 1973 ] </ref> in defining optimistic and pessimistic optimal policies. Bertsekas and Casta~non [ 1989 ] use the notion of aggregated Markov chains and consider grouping together states with approximately the same residuals.
Reference: [ White and Eldeib, 1986 ] <author> White, C. C. and Eldeib, H. K. </author> <year> 1986. </year> <title> Parameter imprecision in finite state, finite action dynamic programs. </title> <journal> Operations Research 34 </journal> <pages> 120-129. </pages>
Reference: [ White and Eldeib, 1994 ] <author> White, C. C. and Eldeib, H. K. </author> <year> 1994. </year> <title> Markov decision processes with imprecise transition probabilities. Operations Research 43 739-749. This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: The more general MDPIPs described in these papers require more general and expensive algorithms for solution. For example, <ref> [ White and Eldeib, 1994 ] </ref> allows an arbitrary linear program to define the bounds on the transition probabilities (and allows no imprecision in the reward parameters)| as a result, the solution technique presented appeals to linear programming at each iteration of the solution algorithm rather than exploit the specific structure
References-found: 14

