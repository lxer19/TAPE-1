URL: http://www.cs.gmu.edu/~swlee/Paper/aaai97.ps
Refering-URL: http://www.cs.gmu.edu/~swlee/publication~.html
Root-URL: 
Email: -swlee, sfischt, jwnek-@aic.gmu.edu  
Title: A Multistrategy Learning Approach to Flexible Knowledge Organization and Discovery Abstract Properly organizing knowledge so
Author: Seok Won Lee, Scott Fischthal and Janusz Wnek 
Address: M.S. 4A5  4400 University Drive Fairfax, VA, 22030-4444  Gaithersburg, MD.  
Affiliation: Laboratory,  George Mason University  Lockheed Martin Federal Systems,  
Note: Machine Learning and Inference  1 Also with  2 Also with Science Applications International Corp., Tysons Corner, VA.  These taxonomies can also be represented  
Abstract-found: 0
Intro-found: 1
Reference: <author> Arciszewski, T., Michalski, R. S., and Wnek, J. </author> <year> 1995. </year> <title> Constructive Induction: the Key to Design Creativity . Reports of the Machine Learning and Inference Laboratory, </title> <type> MLI 95-10, </type> <institution> George Mason University, Fairfax, VA. </institution>
Reference-contexts: CI searches for patterns in data, learned hypotheses, and knowledge from experts , using them to create a new knowledge representation space (Wnek & Michalski 1994). The illustration of this problem is given by <ref> (Arciszewski et al. 1995) </ref>. Let us suppose that the problem is to construct a description that separates points marked by + from points marked by - (Figure 1A).
Reference: <author> Bloedorn, E., and Michalski, R. S. </author> <year> 1991. </year> <title> Constructive Induction from Data in AQ17-DCI: Further Experiments. Reports of the Machine Learning and Inference Laboratory, </title> <type> MLI 91-12, </type> <institution> George Mason University, Fairfax, VA. </institution>
Reference-contexts: In addition to the distance-based clustering methods described above, other subsymbolic systems such as SOFMs (Kohonen 1995) and k-NN methods can be employed as the unsupervised classification engine. Multiple engines may also be used to perform additional constructive induction prior to unsupervised classification. For example, AQ17-DCI <ref> (Bloedorn & Michalski 1991) </ref> and AQ17-HCI (Wnek & Michalski 1994) construct new features based on interrelationships among existing ones. Varying the rule learner based on the application may also prove productive.
Reference: <author> Cheeseman, P., and Stutz, J. </author> <year> 1996. </year> <title> Bayesian Classification (AutoClass): Theory and Results. </title> <booktitle> In Advances in Knowledge Discovery and Data Mining. </booktitle>
Reference-contexts: These new concepts expand the knowledge representation space for the supervised inductive learning system. The system employs constructive induction to create and enhance the knowledge representation space with the aid of the unsupervised Bayesian classifier, AutoClass <ref> (Cheeseman et al. 1996) </ref>. AutoClass provides a maximum posterior probability grouping objects into classes. The constructed classes define abstract concepts, with descriptions learned from class members using the inductive learning system, Aq15c ( Wnek et al. 1995). <p> AutoClass: An Unsupervised Bayesian Classifier AutoClass is an unsupervised Bayesian classification system that looks for a maximum posterior probability classification <ref> (Cheeseman et al. 1996) </ref>. The system infers classes based on Bayesian statistics, deriving a belief network via probability theory. The idea of accumulating and applying auxiliary evidence here can be mapped into the constructive induction mechanism that employs a new attribute which summarizes the data patterns.
Reference: <editor> Fayyad, U. M., Piatetsky-Shapiro, G., Smyth, P., and Uthurusamy, R, eds.: </editor> <publisher> AAAI Press, </publisher> <address> Menlo Park. </address>
Reference: <author> Falkenhainer, B. C. , and Michalski, R. S. </author> <year> 1990. </year> <title> Integration Quantitative and Qualitative Discovery in the ABACUS System. </title> <booktitle> In Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. </volume> <editor> III. Kodratoff, Y., and Michalski, R. S., eds.: </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: For example, AQ17-DCI (Bloedorn & Michalski 1991) and AQ17-HCI (Wnek & Michalski 1994) construct new features based on interrelationships among existing ones. Varying the rule learner based on the application may also prove productive. If mathematical formulae are desired instead of conjunctive rules, a system such as ABACUS <ref> ( Falkenhainer & Michalski 1990) </ref> could be employed in place of AQ15c. There are still many challenging real world applications to which this multistrategy approach could be applied for new knowledge discovery. Acknowledgments The authors thank Dr. Ryszard Michalski for his helpful comments and criticism.
Reference: <editor> Fayyad, U. M., Piatetsky-Shapiro, G., Smyth, P., and Uthurusamy, R. eds. </editor> <year> 1996. </year> <title> Advances in Knowledge Discovery and Data Mining. </title> : <publisher> The AAAI Press, </publisher> <address> Menlo Park. </address>
Reference-contexts: Required as part of the technical aspect of knowledge management solutions is a new approach for intelligent and automated knowledge discovery <ref> (Fayyad et al. 1996) </ref> and reorganization. By discovering and reorganizing this knowledge, it becomes easier to apply it to an organizations decision making. We present AqBC (Lee 1996), a multistrategy knowledge discovery approach to concept learning.
Reference: <author> Hanson, R., Stutz, J., and Cheeseman, P. </author> <year> 1991. </year> <title> Bayesian Classification Theory, </title> <type> Technical Report FIA-90-12-7-01, </type> <institution> NASA Ames Research Center, Artificial Intelligence Branch. </institution>
Reference-contexts: The new attributes degree of belief is very high because it is generated from the best model of Bayesian classification. Therefore, this new attribute can potentially reorganize and improve the knowledge representation space. The theory of how Bayesian learning is applied in AutoClass, summarized from <ref> (Hanson et al. 1991) </ref>, is described below. Let E denote a set of evidence and H a set of possible hypotheses that can conceptualize a set of combinations in E. Assume that the sets of possible evidence E and possible hypotheses H are mutually exclusive power sets.
Reference: <author> Heckerman, D. </author> <year> 1990. </year> <title> Probabilistic interpretations for Mycins certainty factors. </title> <booktitle> In Readings in Uncertain Reasoning. </booktitle> <pages> 298-312 . Shafer, </pages> <editor> G., and Pearl, J. e ds.: </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference: <author> Kohonen, T. </author> <year> 1995. </year> <title> Self Organizing Maps .: Springer-Verlag, </title> <editor> Heidelberg Lee, S. W. </editor> <year> 1996. </year> <title> Multistrategy Learning: An Empirical Study with AQ + Bayesian Approach, Reports of the Machine Learning and Inference Laboratory, </title> <type> MLI 96-10, </type> <institution> George Mason University, Fairfax, VA. </institution>
Reference-contexts: Not only Bayesian classifiers benefit from the use of AQ15c to learn the descriptive representations of the generated classes. In addition to the distance-based clustering methods described above, other subsymbolic systems such as SOFMs <ref> (Kohonen 1995) </ref> and k-NN methods can be employed as the unsupervised classification engine. Multiple engines may also be used to perform additional constructive induction prior to unsupervised classification. For example, AQ17-DCI (Bloedorn & Michalski 1991) and AQ17-HCI (Wnek & Michalski 1994) construct new features based on interrelationships among existing ones.
Reference: <author> Michalski, R. S. </author> <year> 1969. </year> <title> On the Quasi-Minimal Solution of the General Covering Problem. </title> <booktitle> In Proceedings of the International Symposium on Information Processing (FCIP 69), Vol. A3 (Switching Circuits), </booktitle> <pages> 125-128, </pages> <address> Yugoslavia, Bled, </address> <month> October 8-11. </month>
Reference-contexts: AQ-family of inductive learning programs implements the representation space: A) the initial representation space; B) the improved representation space due to the AqBC approach STAR method of inductive learning ( Michalski 1983a).It is based on the AQ algorithm for solvin g the general covering problem <ref> (Michalski 1969) </ref> . AQ15c learns decision rules for a given set of decision classes from examples.AQ15c can generate decision rules that represent either characteristic or discriminant concept descriptions, or an intermediate form, depending on the settings of its parameters.
Reference: <author> Michalski, R. S. </author> <year> 1973. </year> <booktitle> AQVAL/1-Computer Implementation of a Variable-Valued Logic System VL1 and Examples of its Application to Pattern Recognition . In Proceedings of the First International Joint Conference on Pattern Recognition, </booktitle> <address> Washington D. C. </address>
Reference-contexts: Considering attributes as dimensions spanning a multidimensional space, concept instances map to points in this space. An example of a representational formalism is predicate calculus with its set of logical operators. AQ15c is a program that learns concept descriptions from examples. It uses the Variable-Valued Logic system VL1 <ref> (Michalski 1973) </ref>, a form of propositional logic, as its representational formalism that defines representation spaces in terms of attribute sets , where attributes may have multiple, discrete values. The representation space and the set of concepts for learning must be supplied by an oracle. AQ15c Implementation .
Reference: <author> Michalski, R. S. </author> <year> 1978. </year> <title> Pattern Recognition as Knowledge-Guided Computer Induction, </title> <type> Technical Report No. 927, </type> <institution> Department of Computer Science, University of Illinois, Urbana-Champaign. </institution>
Reference-contexts: In this paper, we show that the newly created knowledge facilitates classification and, in turn, problem solving that employs classification or pattern recognition in large databases. The Significance of Improving the Knowledge Representation Space Constructive Induction (CI) is a concept proposed in the field of inductive concept learning <ref> (Michalski 1978) </ref> to solve learning problems in which the original representation space is inadequate for the problem at hand and needs to be improved in order to correctly formulate the knowledge to be learned. In other words, constructive induction hypothesizes new knowledge using a search process.
Reference: <author> Michalski, R. S. </author> <year> 1980. </year> <title> Knowledge acquisition through conceptual clustering: A theoretical framework and algorithm for partitioning data into conjunctive concepts. </title> <journal> International Journal of Policy Analysis and Information Systems, </journal> <volume> 4: </volume> <pages> 219-243. </pages>
Reference: <author> Michalski, R. S. </author> <year> 1983a. </year> <title> A Theory and Methodology of Inductive Learning. </title> <booktitle> Artificial Intelligence . Vol.20, </booktitle> <pages> 111-116. </pages>
Reference-contexts: AQ-family of inductive learning programs implements the representation space: A) the initial representation space; B) the improved representation space due to the AqBC approach STAR method of inductive learning <ref> ( Michalski 1983a) </ref>.It is based on the AQ algorithm for solvin g the general covering problem (Michalski 1969) .
Reference: <author> Michalski, R.S., and Stepp, R. E. </author> <year> 1983b. </year> <title> Learning from Observation: Conceptual Clustering. </title> <booktitle> In Machine Learning: An Artificial Intelligence Approach </booktitle> . 
Reference-contexts: While the rules shown in created by the statistical clustering, they do provide us with a great deal more insight about these clusters than would simply looking at the class member lists alone . The use of a conceptual clustering tool, such as Cluster/2 <ref> (Michalski & Stepp 1983b) </ref> could also prove helpful as the unsupervised classification system, since it generates descriptive rules as part of its clustering algorithm. <p> Another potential experiment would be to use the taxonomy produced via the AqBC method to perform supervised classification with many different decision attributes from the current set rather than just the population attribute. Other future research will focus on developing new strategies combining various statistical (Sharma 1996) and conceptual <ref> (Michalski & Stepp 1983b,c) </ref> classification methods for constructing better knowledge representation spaces from large data sets. Not only Bayesian classifiers benefit from the use of AQ15c to learn the descriptive representations of the generated classes.
Reference: <editor> Michalski, R. S., Carbonell, J. G. , and Mitchell, T. M. eds.: </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference: <author> Michalski, R. S. , and Stepp, R. E. </author> <year> 1983c. </year> <title> Automated Construction of Classifications: Conceptual Clustering Versus Numerical Taxonomy. </title> <journal> IEEE Transactions on PAMI 5:4, </journal> <pages> 396-410. </pages>
Reference: <author> Michalski, R. S., Mozetic, I., Hong, J., and Lavrac, N., </author> <year> 1986. </year> <title> The MultiPurpose Incremental Learning System AQ15 and its Testing Application to Three Medical Domains. </title> <booktitle> In Proceedings of AAAI-86, </booktitle> <pages> 1041-1045, </pages> <address> Philadelphia, PA. </address>
Reference-contexts: AqBC: A Multistrategy Approach for Constructive Induction-based Knowledge Discovery In order to present a self-contained paper, we describe each module of this proposed approach below. AQ15c Inductive Learning System AQ15c ( Wnek et al. 1995) is a C language reimplementation of AQ15 <ref> ( Michalski et al. 1986) </ref>.
Reference: <author> Michalski, R. S., and Kaufman, K. A. </author> <year> 1997. </year> <title> Data Mining and Knowledge Discovery : A Review of Issues and a Multistrategy Approach. In Machine Learning and Data Mining: </title> <editor> M ethods and Applications . Michalski, R. </editor> <publisher> S. </publisher>
Reference: <editor> Bratko, I., and Kubat M. eds.: </editor> <publisher> John Wiley & Sons. Forthcoming </publisher> . 
Reference: <author> Sharma, S. </author> <year> 1996. </year> <title> Applied Multivariate Techniques .: John Wiley, </title> <address> New York. </address>
Reference-contexts: For comparison, we applied standard statistical clustering methods to the data in place of AutoClass as the unsupervised classification engine. The two approaches compared are a simple K-means centroid method and a Ward hierarchical clustering <ref> (Sharma 1996) </ref>. The Ward clustering, shown in the dendrogram of three. The class attribute constructed by K-means has three classes without any clear meaning. The clusterings produced by the two methods are strikingly similar, but not especially meaningful. <p> Another potential experiment would be to use the taxonomy produced via the AqBC method to perform supervised classification with many different decision attributes from the current set rather than just the population attribute. Other future research will focus on developing new strategies combining various statistical <ref> (Sharma 1996) </ref> and conceptual (Michalski & Stepp 1983b,c) classification methods for constructing better knowledge representation spaces from large data sets. Not only Bayesian classifiers benefit from the use of AQ15c to learn the descriptive representations of the generated classes.
Reference: <author> Thrun, S. B. , et al. </author> <year> 1991. </year> <title> The MONK s problems: A Performance Comparison of Different Learning Algorithms. </title> <institution> Carnegie Mellon University. </institution>
Reference-contexts: This way, the hierarchical hypotheses structures discovered from the nested classifications provide valuable information that cannot be obtained from either system alone. The first MONK problem, MONK1, <ref> (Thrun et al. 1991) </ref> and US census data (obtained from the US Census Bureaus web page) have been used for experimentation. Other statistical classification methods (K-means centroid & Ward hierarchical clustering) were also applied to this data to compare and analyze the results.
Reference: <author> Wnek, J., Kaufman , K., Bloedorn , E., and Michalski, </author> <note> R. </note>
Reference: <author> S. </author> <year> 1995. </year> <title> Inductive Learning System AQ15c: The Method and Users Guide. Reports of the Machine Learning and Inference Laboratory, </title> <type> MLI 95-4, </type> <institution> George Mason University, VA. </institution>
Reference-contexts: AutoClass provides a maximum posterior probability grouping objects into classes. The constructed classes define abstract concepts, with descriptions learned from class members using the inductive learning system, Aq15c <ref> ( Wnek et al. 1995) </ref>. The abstract concept descriptions are then used to improve and expand the original representation space. This expanded representation space serves as a final setting for supervised concept learning of any attribute from the original examples by employing Aq15c. <p> Other statistical classification methods (K-means centroid & Ward hierarchical clustering) were also applied to this data to compare and analyze the results. The diagrammatic visualization system, DIAV <ref> (Wnek 1995) </ref> graphically interprets the knowledge representation spaces and shows the changes in the representation space caused by constructive induction. In this paper, we show that the newly created knowledge facilitates classification and, in turn, problem solving that employs classification or pattern recognition in large databases. <p> CI searches for patterns in data, learned hypotheses, and knowledge from experts , using them to create a new knowledge representation space (Wnek & Michalski 1994). The illustration of this problem is given by <ref> (Arciszewski et al. 1995) </ref>. Let us suppose that the problem is to construct a description that separates points marked by + from points marked by - (Figure 1A). <p> We will discuss in later sections how AqBC addresses this issue in an application to US census data. AqBC: A Multistrategy Approach for Constructive Induction-based Knowledge Discovery In order to present a self-contained paper, we describe each module of this proposed approach below. AQ15c Inductive Learning System AQ15c <ref> ( Wnek et al. 1995) </ref> is a C language reimplementation of AQ15 ( Michalski et al. 1986). <p> Not only Bayesian classifiers benefit from the use of AQ15c to learn the descriptive representations of the generated classes. In addition to the distance-based clustering methods described above, other subsymbolic systems such as SOFMs <ref> (Kohonen 1995) </ref> and k-NN methods can be employed as the unsupervised classification engine. Multiple engines may also be used to perform additional constructive induction prior to unsupervised classification. For example, AQ17-DCI (Bloedorn & Michalski 1991) and AQ17-HCI (Wnek & Michalski 1994) construct new features based on interrelationships among existing ones.
Reference: <author> Wnek, J., and Michalski , R. S. </author> <year> 1994. </year> <title> Hypothesis-driven Constructive Induction in AQ17-HCI: A method and experiments. </title> <journal> Machine Learning, Vol.14, No.2, </journal> <pages> 139-168. </pages>
Reference-contexts: CI searches for patterns in data, learned hypotheses, and knowledge from experts , using them to create a new knowledge representation space <ref> (Wnek & Michalski 1994) </ref>. The illustration of this problem is given by (Arciszewski et al. 1995). Let us suppose that the problem is to construct a description that separates points marked by + from points marked by - (Figure 1A). <p> By not providing the original classifications , the original properties of the testing data set can be preserved and AutoClasss ability to correctly classify previously unseen examples can be verified. The technique of constructing classification labels is based on previous constructive induction methodology <ref> ( Wnek & Michalski 1994) </ref>. The improvement of the know ledge representation space is already demonstrated with the M ONK1 problem in the previous section. <p> Multiple engines may also be used to perform additional constructive induction prior to unsupervised classification. For example, AQ17-DCI (Bloedorn & Michalski 1991) and AQ17-HCI <ref> (Wnek & Michalski 1994) </ref> construct new features based on interrelationships among existing ones. Varying the rule learner based on the application may also prove productive.
Reference: <author> Wnek, J. </author> <year> 1995. </year> <title> DIAV 2.0 User Manual: Specification and Guide through the Diagrammatic Visualization System, Reports of the Machine Learning and Inference Laboratory, </title> <type> MLI 95-4, </type> <institution> George Mason University, Fairfax, VA. </institution>
Reference-contexts: AutoClass provides a maximum posterior probability grouping objects into classes. The constructed classes define abstract concepts, with descriptions learned from class members using the inductive learning system, Aq15c <ref> ( Wnek et al. 1995) </ref>. The abstract concept descriptions are then used to improve and expand the original representation space. This expanded representation space serves as a final setting for supervised concept learning of any attribute from the original examples by employing Aq15c. <p> Other statistical classification methods (K-means centroid & Ward hierarchical clustering) were also applied to this data to compare and analyze the results. The diagrammatic visualization system, DIAV <ref> (Wnek 1995) </ref> graphically interprets the knowledge representation spaces and shows the changes in the representation space caused by constructive induction. In this paper, we show that the newly created knowledge facilitates classification and, in turn, problem solving that employs classification or pattern recognition in large databases. <p> We will discuss in later sections how AqBC addresses this issue in an application to US census data. AqBC: A Multistrategy Approach for Constructive Induction-based Knowledge Discovery In order to present a self-contained paper, we describe each module of this proposed approach below. AQ15c Inductive Learning System AQ15c <ref> ( Wnek et al. 1995) </ref> is a C language reimplementation of AQ15 ( Michalski et al. 1986).
Reference: <author> Zhang, J., and Michalski, R. S. </author> <year> 1989. </year> <title> Rule Optimization Via SG-TRUNC Method. </title> <booktitle> In Proceedings of the Fourth European Working Session on Learning. </booktitle>
Reference-contexts: A weakness of this second class is that it has a conjunction that covers only one example. It has been found that rule truncation (in which less important selectors are removed) can often produce more general and effective results <ref> (Zhang & Michalski 1989) </ref>, often by removing conjunctions that essentially cover exceptions (such as the one city covered by the third conjunction in this class). This class is a strong candidate for the application of such a method.
References-found: 27

