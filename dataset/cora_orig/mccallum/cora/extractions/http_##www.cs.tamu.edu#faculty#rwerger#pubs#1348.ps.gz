URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/1348.ps.gz
Refering-URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/
Root-URL: http://www.cs.tamu.edu
Title: Abstract  
Keyword: compiler, parallelization, Fortran, dependence analysis, privatization, symbolic, run-time, Polaris  
Abstract: The limited ability of compilers to find the parallelism in programs is a significant barrier to the use of high performance computers. It forces programmers to resort to parallelizing their programs by hand, adding another level of complexity to the programming task. We show evidence that compilers can be improved, through static and run-time techniques, to the extent that a significant group of scientific programs may be parallelized automatically. Symbolic dependence analysis and array privatization, plus run-time versions of those techniques are shown to be important to the success of this effort. If we can succeed to parallelize programs automatically, the acceptance and use of large-scale parallel processors will be enhanced greatly. 
Abstract-found: 1
Intro-found: 1
Reference: [BE94a] <author> William Blume and Rudolf Eigenmann. </author> <title> The Range Test: A Dependence Test for Symbolic, Non-linear Expressions. </title> <type> Technical report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> April </month> <year> 1994. </year> <note> CSRD Report No. 1345. </note>
Reference-contexts: be changed to A (N*M), and a reference A (I,J) will be changed to A (I + N*J). 4.1.2 Symbolic dependence analysis in Polaris To handle the nonlinear expressions that we have seen in the Perfect Benchmarks, we have implemented a symbolic dependence test in Polaris, called the range test <ref> [BE94a] </ref>.
Reference: [BE94b] <author> William Blume and Rudolf Eigenmann. </author> <title> Symbolic Analysis Techniques Needed for the Effective Parallelization of the Perfect Benchmarks. </title> <type> Technical report, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> January </month> <year> 1994. </year> <note> CSRD Report No. 1332. </note>
Reference-contexts: In fact, four of the twelve codes (i.e. DYFESM, QCD, OCEAN, and TRFD) that we parallelized by hand would exhibit a speedup of at most two if we could not parallelize loops with nonlinear array subscripts <ref> [BE94b] </ref>. For some of these loops, nonlinear expressions occurred in the original program text. For other loops, nonlinear expressions were introduced by the compiler. Two common compiler transformations can introduce nonlinearities into array subscript expressions: induction variable substitution and array linearization.
Reference: [BENP93] <author> Utpal Banerjee, Rudolf Eigenmann, Alexandru Nicolau, and David Padua. </author> <title> Automatic Program Parallelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2), </volume> <month> February </month> <year> 1993. </year>
Reference-contexts: Due to lack of space we will omit many details. The interested reader is referred to <ref> [BENP93] </ref>, a recent survey that includes an extensive list of references. We begin with a discussion of data dependence analysis, whose purpose is to determine whether two statement instances 1 must execute in the order specified in the source program to guarantee correct results.
Reference: [CH78] <author> Patrick Cousot and Nicolas Halbwachs. </author> <title> Automatic Discovery of Linear Restraints Among Variables of a Program. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 84-97, </pages> <year> 1978. </year>
Reference-contexts: Doing these techniques symbolically means that the analysis manipulates or propagates symbolic expressions, equations, and inequalities that contain program variables. Polaris implements two mechanisms for symbolic propagation. One is based on the techniques discussed in <ref> [CH78] </ref> for the forward substitution of symbolic equations and inequalities.
Reference: [EHJ + 93] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, Zhiyuan Li, and D. Padua. </author> <title> Restructuring Fortran Programs for Cedar. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(7) </volume> <pages> 553-573, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: Our study showed that current automatic restructurers seldom achieve good speedups. Although restructurers can achieve significant gains for small kernels or benchmarks, the typical gain for real programs is small <ref> [EHJ + 93] </ref>. Our experience has been only with coarse-grain loop parallelism, but a more accurate analysis of programs could be useful to detect vector and instruction-level parallelism. <p> Then, we will discuss how symbolic analysis can be used to improve array privatization, which is one of the most important techniques needed for effectively parallelizing programs. We will not discuss idiom recognition further in this paper. Some issues regarding this topic are discussed in <ref> [EHJ + 93] </ref>. Even the most powerful symbolic analysis techniques cannot detect parallelism if the information is unavailable at compile time.
Reference: [HKT92] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD Distributed-Memory Machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The success of MPP compilers rests heavily on the effectiveness of their techniques for automatic detection of parallelism. Techniques for dependence analysis, privatization, and symbolic analysis have all been mentioned as central to Fortran compilers for MPPs <ref> [HKT92] </ref>. For this reason, it is pertinent to discuss here the effectiveness of today's techniques for the automatic detection of parallelism and what we believe is necessary to improve their effectiveness. 2 Fundamental Techniques We now present a brief description of automatic parallelization techniques.
Reference: [HP91] <author> Mohammad Haghighat and Constantine Polychronopoulos. </author> <title> Symbolic Dependence Analysis for High-Performance Parallelizing Compilers. </title> <editor> In A. Nicolau D. Gelernter, T. Gross and 16 D. Padua, editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Processing, pages 310--330. </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference: [Poi90] <author> Lynn Pointer. </author> <title> Perfect: Performance Evaluation for Cost-Effective Transformations Report 2. </title> <type> Technical report, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Res & Dev, </institution> <month> March </month> <year> 1990. </year> <note> CSRD Report No. 964. </note>
Reference-contexts: In all programs we inspected, we found that our transformations could improve the program performance 5 the Cedar machine 6 by a significant factor. In fact, in most programs this was close to or matching the performance that resulted from the best reported manual effort <ref> [Poi90] </ref>. Having identified several techniques that can greatly improve the effectiveness of automatic paral-lelization, we are now implementing these techniques in the Polaris compiler. So far, a preliminary implementation of Polaris is able to parallelize half the programs shown in Figure 1 to the extent of the manually parallelized versions.
Reference: [PP93] <author> Paul M. Petersen and David A. Padua. </author> <title> Static and Dynamic Evaluation of Data Dependence Analysis. </title> <booktitle> In Proc. of ICS'93, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: The values of the loop limits is also necessary even though accurate results can sometimes be obtained by conservatively assuming that the upper limit is the largest possible integer value in the target machine <ref> [PP93] </ref>. If a compiler relies only on numerical techniques, as often is the case, it has to assume a dependence when the values of the coefficients or loop limits are not known. This is one of the main reasons why today's compilers fail to detect parallelism in sequential programs. <p> Because of this, modern day data dependence tests have become very accurate and efficient <ref> [PP93] </ref>. However, these tests place constraints upon loop bounds and array subscript expressions of the loops that they examine. If these constraints are not met, these tests fail, thus preventing the loop from being fully parallelized.
Reference: [Pug92] <author> William Pugh. </author> <title> A Practical Algorithm for Exact Array Dependence Analysis. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference: [RP94] <author> Lawrence Rauchwerger and David Padua. </author> <title> The PRIVATIZING DOALL Test: A Run-Time Technique for DOALL Loop Identification and Array Privatization . Technical report, </title> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. and Dev., </institution> <month> January </month> <year> 1994. </year> <note> CSRD Report No. 1329. </note>
Reference-contexts: If the subsequent test finds that the loop was not fully parallel, then it will be re-executed sequentially. In order to implement such a strategy, we have developed a run-time technique, called the Privatizing Doall test (PD test), for detecting the presence of cross-iteration dependences in a loop <ref> [RP94] </ref>. If there are any such dependences, this test does not identify them; it only flags their existence.
Reference: [TP92] <author> Peng Tu and David Padua. </author> <title> Array privatization for shared and distributed memory machines. </title> <booktitle> In Proc. 2nd Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Machines, ACM SIGPLAN Notices 1993, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: This is perhaps the main reason that they fail to parallelize many outer loops and thus are limited in their effectiveness. Privatization is not only important to detect parallelism, but also to increase the quality of the code generated by distributed-memory Fortran compilers <ref> [TP92] </ref> using the owner computes rule. Another important transformation to eliminate dependences is the recognition and replacement of idioms, usually simple recurrences. One recurrence found frequently is induction.
Reference: [TP93a] <author> Peng Tu and David Padua. </author> <title> Automatic array privatization. </title> <editor> In Utpal Banerjee, David Gel-ernter, Alex Nicolau, and David Padua, editors, </editor> <booktitle> Proc. Sixth Workshop on Languages and Compilers for Parallel Computing, volume 768 of Lecture Notes in Computer Science, </booktitle> <pages> pages 500-521, </pages> <address> Portland, OR, August 1993. </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: In our experience, the most important of these transformations is array privatization <ref> [TP93a] </ref>. As mentioned in Section 2, array privatization is used to eliminate memory-related dependences.
Reference: [TP93b] <author> Peng Tu and David Padua. </author> <title> Demand-driven symbolic analysis. </title> <type> CSRD Report 1336, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomp. R&D, </institution> <month> Dec </month> <year> 1993. </year> <month> 17 </month>
Reference-contexts: P CMHOG-solvex1/100 0.9 P P CMHOG-nudt/200 0.7 P S (A) CMHOG-setup/70 0.7 P P CMHOG-maxmin/10 0.3 P S (A) CMHOG-hdfall/800 0.2 P P X Notes: (R)=true recurrence; (IO)=input/output operations; (I)=not yet fully implemented; (A)=Automatable technique not being implemented Table 1: Transformation of the time-critical loops of our evaluation suite 8 <ref> [TP93b] </ref>. We have used the former to support dependence analysis and the latter to support array privatization and idiom recognition. Even though, in theory at least, either approach could support all three analysis techniques, the demand-driven approach is potentially more efficient because it derives only the information that is needed.
References-found: 14

