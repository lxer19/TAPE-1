URL: http://www.cs.toronto.edu/~parodi/CVPR-1996.ps.gz
Refering-URL: http://www.cs.toronto.edu/~parodi/abstract_cvpr96.html
Root-URL: 
Title: A fast and flexible statistical method for text extraction in document pages  
Author: Pietro Parodi Giulia Piccioli 
Address: 6 King's College Rd, Room 265 C Via Dodecaneso 33 Toronto (ON), Canada M5S 3H5 Genova, Italy 16146  
Affiliation: Department of Computer Science Dipartimento di Fisica University of Toronto University of Genova  
Abstract: This paper describes a fast and flexible method for extracting text regions from a document page containing text, graphics, and pictures. Such regions can be given as an input to an OCR system. The user fixes two parameters, the minimum width w of the text to be detected, and the precision * needed (both expressed as a percentage of the image width), according to the implementation needs. The method works by subdividing the page into overlapping columns whose width and inter-shift depend on w and *, and by performing text lines extraction on each column separately. Suc- cessively, a statistical analysis of the text line elements found in each column is performed, and they are connected to form complete text lines. Finally, related pieces of text are merged into blocks so that a sensible reading order is provided for the OCR system. The algorithm is very fast, is able to work on low-resolution document pages and is robust against skew. The algorithm is also very flexible: no assumptions are made on the layout of the document, the shape of the text regions, and the font size and style; the main assumption is that the background is uniform and the text approximately horizontal. Despite the statistical nature of the method, a single line of text of a certain font size is generally sufficient to warrant detection. Ex- perimental results are shown which demonstrate the effectiveness of the method on several different kinds of documents. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L.A. Fletcher and R. Kasturi. </author> <title> A robust al-gorithm for text string separation from mixed text/graphics images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 10(6):910918, </volume> <year> 1988. </year>
Reference-contexts: Many techniques for text extraction and document segmentation have been developed through the years. They are commonly subdivided into three main categories: top-down, bottom-up and hybrid techniques. Bottom-up techniques <ref> [1] </ref> are usually based on connected components analysis: they progressively clus <p>- ter neighboring pixels to form symbols, words, text lines and so on.
Reference: [2] <author> D. Wang and S.N. Srihari. </author> <title> Classification of news-paper image blocks using texture analysis. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 47 </volume> <pages> 327-352, </pages> <year> 1989. </year>
Reference-contexts: They do not make assumptions on font style and size and about page layout, but they are computationally expensive and rely on the use of appropriate thresholds. Top-down techniques <ref> [2, 3, 4] </ref> use a priori assumptions about the layout of the page in order to segment it.
Reference: [3] <author> F.M. Wahl, K.Y. Wong, and R.G. Casey. </author> <title> Block segmentation and text extraction in mixed text/image documents. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 20 </volume> <pages> 375-390, </pages> <year> 1982. </year>
Reference-contexts: They do not make assumptions on font style and size and about page layout, but they are computationally expensive and rely on the use of appropriate thresholds. Top-down techniques <ref> [2, 3, 4] </ref> use a priori assumptions about the layout of the page in order to segment it.
Reference: [4] <author> G. Nagy and S.C.Seth. </author> <title> Hierarchical representa-tion of optical scanned documents. </title> <booktitle> In Proceedings of the International Conference on Pattern Recognition, </booktitle> <pages> pages 347-349. </pages> <publisher> IEEE, </publisher> <address> Montreal, Canada, </address> <year> 1984. </year>
Reference-contexts: They do not make assumptions on font style and size and about page layout, but they are computationally expensive and rely on the use of appropriate thresholds. Top-down techniques <ref> [2, 3, 4] </ref> use a priori assumptions about the layout of the page in order to segment it.
Reference: [5] <author> H.S. Baird, S.E. Jones, and S.J. Fortune. </author> <title> Image segmentation by shape-directed covers. </title> <booktitle> In Proceedings of the International Conference on Pattern Recognition, </booktitle> <pages> pages 820-825. </pages> <publisher> IEEE, </publisher> <year> 1990. </year>
Reference-contexts: It is impossible to give an overview of these methods here (some more details can be found in the review section), so we will limit ourselves to cite three works based on background analysis <ref> [5, 6, 7] </ref> and two very general methods for segmentation: one based on Gabor filters [8] and the other based on wavelet analysis (Edemat et al., 1995). Despite the many efforts spent on the subject, document segmentation cannot be considered as a completely solved problem.
Reference: [6] <author> T. Pavlidis and J. Zhou. </author> <title> Page segmentation by white streams. </title> <booktitle> In Proceedings of the International Conference on Document Analysis and Recognition, </booktitle> <pages> pages 945-953, </pages> <year> 1991. </year>
Reference-contexts: It is impossible to give an overview of these methods here (some more details can be found in the review section), so we will limit ourselves to cite three works based on background analysis <ref> [5, 6, 7] </ref> and two very general methods for segmentation: one based on Gabor filters [8] and the other based on wavelet analysis (Edemat et al., 1995). Despite the many efforts spent on the subject, document segmentation cannot be considered as a completely solved problem.
Reference: [7] <author> A. Antonacopoulos and R. T. Ritchings. </author> <title> Flexible page segmentation using the background. </title> <booktitle> In Proceedings of the International Conference on Pattern Recognition. IEEE, </booktitle> <address> Jerusalem, Israel, </address> <year> 1994. </year>
Reference-contexts: It is impossible to give an overview of these methods here (some more details can be found in the review section), so we will limit ourselves to cite three works based on background analysis <ref> [5, 6, 7] </ref> and two very general methods for segmentation: one based on Gabor filters [8] and the other based on wavelet analysis (Edemat et al., 1995). Despite the many efforts spent on the subject, document segmentation cannot be considered as a completely solved problem.
Reference: [8] <author> A.K. Jain and S. Bhattacharjee. </author> <title> Text segmen-tation using Gabor filters for automatic docu-ment processing. </title> <journal> Machine Vision and Applications, </journal> <volume> 5 </volume> <pages> 169-184, </pages> <year> 1992. </year>
Reference-contexts: It is impossible to give an overview of these methods here (some more details can be found in the review section), so we will limit ourselves to cite three works based on background analysis [5, 6, 7] and two very general methods for segmentation: one based on Gabor filters <ref> [8] </ref> and the other based on wavelet analysis (Edemat et al., 1995). Despite the many efforts spent on the subject, document segmentation cannot be considered as a completely solved problem.
References-found: 8

