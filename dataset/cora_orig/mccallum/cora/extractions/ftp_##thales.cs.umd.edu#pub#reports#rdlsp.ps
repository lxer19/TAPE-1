URL: ftp://thales.cs.umd.edu/pub/reports/rdlsp.ps
Refering-URL: ftp://thales.cs.umd.edu/pub/reports/Contents.html
Root-URL: 
Title: Rank Degeneracy and Least Squares Problems  
Author: Gene Golub G. W. Stewart 
Affiliation: Stanford University Virginia Klema National Bureau of Economic Research  University of Maryland  
Abstract: This paper is concerned with least squares problems when the least squares matrix A is near a matrix that is not of full rank. A definition of numerical rank is given. It is shown that under certain conditions when A has numerical rank r there is a distinguished r dimensional subspace of the column space of A that is insensitive to how it is approximated by r independent columns of A. The consequences of this fact for the least squares problem are examined. Algorithms are described for approximating the stable part of the column space of A.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. N. Afriat. </author> <title> Orthogonal and oblique projectors and the characteristics of pairs of vector spaces. </title> <booktitle> Proceedings of the Cambridge Philosophical Society, </booktitle> <volume> 53 </volume> <pages> 800-816, </pages> <year> 1957. </year>
Reference-contexts: Then for two subspaces R (X) and R (Y ) we shall measure the distance between them by kP X P Y k 2 (for the various geometric interpretations of this number, which is related to canonical correlations and the angle between subspaces, see <ref> [1, 2, 13] </ref>).
Reference: [2] <author> A. Bjorck and G. H. Golub. </author> <title> Numerical methods for computing angles between linear subspaces. </title> <journal> Mathematics of Computation, </journal> <volume> 27 </volume> <pages> 579-594, </pages> <year> 1973. </year>
Reference-contexts: Then for two subspaces R (X) and R (Y ) we shall measure the distance between them by kP X P Y k 2 (for the various geometric interpretations of this number, which is related to canonical correlations and the angle between subspaces, see <ref> [1, 2, 13] </ref>).
Reference: [3] <author> P. Businger and G. H. Golub. </author> <title> Linear least squares solutions by Householder transformations. </title> <journal> Numerische Mathematik, </journal> <volume> 7 </volume> <pages> 269-276, </pages> <year> 1965. </year> <note> Also in [18, pp.111-118]. </note>
Reference-contexts: An algol program incorporating this "column pivoting" is given in <ref> [3] </ref> and a fortran program is given in [11].
Reference: [4] <author> J. M. Chambers. </author> <title> Stabilizing linear regression against observational error in independent variats. </title> <type> Manuscript, </type> <institution> Bell Laboratories, </institution> <address> Murray Hill, New Jersey., </address> <year> 1972. </year>
Reference-contexts: The use of principal components to eliminate collinearities has been proposed in the literature (e.g., see <ref> [4, 9, 16, 17] </ref>). This paper extends these proposals in two ways. First we prove theorems that express quantitatively the results of deciding that certain columns of A can be ignored. Second we describe in detail how existing computational techniques can be used to realize our methods. <p> Specifically, the rows and columns of A should be scaled so that the errors in the individual elements of A are as nearly as possible equal. This scaling has also been proposed in <ref> [4] </ref>, and an efficient algorithm for accomplishing it is described in [5]. The rationale for this scaling is the following.
Reference: [5] <author> A. R. Curtis and J. K. Reid. </author> <title> On the automatic scaling of matrices for Gaussian elimination. </title> <journal> Journal of the Institute for Mathematics and Applications, </journal> <volume> 10 </volume> <pages> 118-124, </pages> <year> 1972. </year>
Reference-contexts: Specifically, the rows and columns of A should be scaled so that the errors in the individual elements of A are as nearly as possible equal. This scaling has also been proposed in [4], and an efficient algorithm for accomplishing it is described in <ref> [5] </ref>. The rationale for this scaling is the following.
Reference: [6] <author> C. Daniel and F. S. Wood. </author> <title> Fitting Equations to Data. </title> <publisher> John Wiley, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: The results of this section have implications for a common practice in data analysis, namely that of fitting a large number of subsects of the columns of A in an attempt to obtain a good fit with fewer than the full complement of columns (for example, see <ref> [6] </ref>). We have, in effect, shown that if the ration *=ffi is reasonable, this procedure is not likely to be very productive. Any set of r independent columns will give about the same residual, and any large set that significantly reduces the residual must produce an unacceptably large solution.
Reference: [7] <author> G. H. Golub. </author> <title> Least squares, singular values, and matrix approximations. </title> <journal> Aplicace Mathematicky, </journal> <volume> 13 </volume> <pages> 44-51, </pages> <year> 1968. </year>
Reference-contexts: When the norm in definition 2.1 is either the 2-norm or the Frobenius norm, the problem of determining the numerical rank of a matrix can be solved in terms of the singular value decomposition of the matrix. This decomposition, which has many applications (e.g., see <ref> [7] </ref>), is described in the following theorem. Theorem 2.2. Let A be an m fi n matrix with m n.
Reference: [8] <author> G. H. Golub and C. Reinch. </author> <title> Singular value decomposition and least squares solution. </title> <journal> Numerische Mathematik, </journal> <volume> 14 </volume> <pages> 403-420, </pages> <year> 1970. </year> <note> Also in [18, pp.134-151]. </note>
Reference-contexts: Rank Degeneracy and Least Squares Problems 17 6. Extraction of Independent Columns: the Singular Value Decompo sition When the singular value decomposition of A has been computed (an algol program is given in <ref> [8] </ref> and a fortran program in [11]), a different way of selecting independent columns is available. The method is based on the following theorem. Theorem 6.1.
Reference: [9] <author> D. M. Hawkins. </author> <title> On the investigation of alternative regressions by principal component analysis. </title> <journal> Appl. Statist., </journal> <volume> 22 </volume> <pages> 275-286, </pages> <year> 1973. </year>
Reference-contexts: The use of principal components to eliminate collinearities has been proposed in the literature (e.g., see <ref> [4, 9, 16, 17] </ref>). This paper extends these proposals in two ways. First we prove theorems that express quantitatively the results of deciding that certain columns of A can be ignored. Second we describe in detail how existing computational techniques can be used to realize our methods. <p> First when it is hoped that fewer than r columns can produce a good fit, and second when the *-ffi ratio is not very small. An approach to the second problem that uses the singular value decomposition of the augmented matrix (A; b) is described in <ref> [9] </ref> and [16, 17]. Rank Degeneracy and Least Squares Problems 14 5. Extraction of Independent Columns: the QR Factorization We now turn to the problem of extracting a set of numerically independent columns. The first method we shall consider is based on the QR factorization of the matrix A.
Reference: [10] <author> H. Hotelling. </author> <title> Relation of the newer multivariate statistical methods to factor analysis. Br. </title> <journal> J. Static. Psychol., </journal> <volume> 10 </volume> <pages> 69-79, </pages> <year> 1957. </year> <title> Rank Degeneracy and Least Squares Problems 25 </title>
Reference-contexts: Hence kr * r W k 2 = k (P U * P AW )bk 2 fl Theorem 4.1 partially answers a question raised by Hotelling <ref> [10] </ref>; namely if carriers are chosen to eliminate dependencies, what guarantees that one set will not fit b better than another? The answer is that if there is a well defined gap between ffi and *, then any set of r strongly independent columns will give approximately the same residual.
Reference: [11] <author> C. L. Lawson and R. J. Hanson. </author> <title> Solving Least Squares Problems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1974. </year>
Reference-contexts: An algol program incorporating this "column pivoting" is given in [3] and a fortran program is given in <ref> [11] </ref>. <p> Rank Degeneracy and Least Squares Problems 17 6. Extraction of Independent Columns: the Singular Value Decompo sition When the singular value decomposition of A has been computed (an algol program is given in [8] and a fortran program in <ref> [11] </ref>), a different way of selecting independent columns is available. The method is based on the following theorem. Theorem 6.1.
Reference: [12] <author> J. W. Longley. </author> <title> An appraisal of least squares programs for the electronic computer from the point of view of the user. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 62 </volume> <pages> 819-841, </pages> <year> 1967. </year>
Reference-contexts: Example 7.3. To show that our theory may be of some use even where there is not a sharply defined gap in the singular values, we consider the Longley test data <ref> [12] </ref>, which has frequently been cited in the literature. Since it is a common practice to subtract means from raw data, we have included a column of ones in the model. Specifically the columns of A are 1. Ones 2. GNP Implicit Price Deflator, 1954-100 3. GNP 4.
Reference: [13] <author> G. W. Stewart. </author> <title> Error and perturbation bounds for subspaces associated with certain eigenvalue problems. </title> <journal> SIAM Review, </journal> <volume> 15 </volume> <pages> 727-764, </pages> <year> 1973. </year>
Reference-contexts: Then for two subspaces R (X) and R (Y ) we shall measure the distance between them by kP X P Y k 2 (for the various geometric interpretations of this number, which is related to canonical correlations and the angle between subspaces, see <ref> [1, 2, 13] </ref>).
Reference: [14] <author> G. W. Stewart. </author> <title> Introduction to Matrix Computations. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: They are also unitarily invariant; that is if U and B are orthogonal matrices then kAk p = kU T Ak p = kAV k p (p = 2; F): For more on these norms, see <ref> [14] </ref>. 2. Rank Degeneracy The usual mathematical notation of rank is not very useful when the matrices in question are not known exactly. <p> there is an orthogonal matrix U of order m and an orthogonal matrix V of order n such that U T AV = ! where = diag ( 1 ; 2 ; : : : ; n ) and For proofs of this theorem and the results cited below see <ref> [14] </ref>. The numbers 1 , 2 , : : : , n , which are unique, are called the singular values of A. <p> Probably the best numerical algorithm is one based on Householder transformations in which the QR factorizations A jk = Q jk R jk are computed successively for k = 1; 2; : : : ; n (e.g., see <ref> [14] </ref>). At the kth step, just before Q jk and R jk are computed, there is the possibility of replacing the kth column of A by one of the columns a k+1 , a k+2 , : : : , a n . <p> Alternatively one could apply an algorithm such as Gaussian elimination with complete pivoting to V T 1 (e.g., see <ref> [14] </ref>). If either of the above suggestions is followed, the final matrix V T *1 will be upper triangular, and ins infimum can be bounded by the method suggested in the last section.
Reference: [15] <author> G. W. Stewart. </author> <title> On the perturbation of pseudo-inverses, projections, and linear least squares problems. </title> <journal> SIAM Review, </journal> <volume> 19 </volume> <pages> 634-662, </pages> <year> 1977. </year>
Reference-contexts: We stress that such a solution cannot be be very stable. By (2.9) any matrix consisting of more than r columns of A must have a singular value less than or equal to *, and it follows from the perturbation theory for the least squares problem <ref> [15] </ref> that the solution must be sensitive to perturbations in A and b. (Another way of seeing this is to note that * 2 is a lower bound of k (A T A) 1 k 2 , so that the solution must have a large covariance matrix.) However, one might be <p> If r is small, significant savings can be obtained by observing that the singular values in [0; 1) of V *1 and ^ V *2 are the same (see the appendix of <ref> [15] </ref> for a proof). Thus one can start with the smaller matrix V T *1 ; ^ V T and use the QR factorization to determine the dependent columns of A.
Reference: [16] <author> J. Webster, R. Gunst, and R. Mason. </author> <title> Latent root regression analysis. </title> <journal> Tech-nometrics, </journal> <volume> 16 </volume> <pages> 513-522, </pages> <year> 1974. </year>
Reference-contexts: The use of principal components to eliminate collinearities has been proposed in the literature (e.g., see <ref> [4, 9, 16, 17] </ref>). This paper extends these proposals in two ways. First we prove theorems that express quantitatively the results of deciding that certain columns of A can be ignored. Second we describe in detail how existing computational techniques can be used to realize our methods. <p> First when it is hoped that fewer than r columns can produce a good fit, and second when the *-ffi ratio is not very small. An approach to the second problem that uses the singular value decomposition of the augmented matrix (A; b) is described in [9] and <ref> [16, 17] </ref>. Rank Degeneracy and Least Squares Problems 14 5. Extraction of Independent Columns: the QR Factorization We now turn to the problem of extracting a set of numerically independent columns. The first method we shall consider is based on the QR factorization of the matrix A.
Reference: [17] <author> J. Webster, R. Gunst, and R. Mason. </author> <title> A comparison of least squares and latent root regression estimators. </title> <journal> Technometrics, </journal> <volume> 18 </volume> <pages> 75-83, </pages> <year> 1976. </year>
Reference-contexts: The use of principal components to eliminate collinearities has been proposed in the literature (e.g., see <ref> [4, 9, 16, 17] </ref>). This paper extends these proposals in two ways. First we prove theorems that express quantitatively the results of deciding that certain columns of A can be ignored. Second we describe in detail how existing computational techniques can be used to realize our methods. <p> First when it is hoped that fewer than r columns can produce a good fit, and second when the *-ffi ratio is not very small. An approach to the second problem that uses the singular value decomposition of the augmented matrix (A; b) is described in [9] and <ref> [16, 17] </ref>. Rank Degeneracy and Least Squares Problems 14 5. Extraction of Independent Columns: the QR Factorization We now turn to the problem of extracting a set of numerically independent columns. The first method we shall consider is based on the QR factorization of the matrix A.
Reference: [18] <author> J. H. Wilkinson and C. Reinsch. </author> <title> Handbook for Automatic Computation. V.II Linear Algebra. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1971. </year>
References-found: 18

