URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/complexconsistency.ps.Z
Refering-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Root-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Email: mschmitt@neuro.informatik.uni-ulm.de  
Title: On the Complexity of Consistency Problems for Neurons with Binary Weights  
Author: Michael Schmitt Abteilung Neuroinformatik 
Address: D-89069 Ulm, Germany  
Affiliation: Universitat Ulm  
Abstract: We inquire into the complexity of training a neuron with binary weights when the training examples are Boolean and required to have bounded coincidence and heaviness. Coincidence of an example set is defined as the maximum inner product of two elements, heaviness of an example set is the maximum Hamming-weight of an element. We use both as parameters to define classes of restricted consistency problems and ask for which values they are NP-complete or solvable in polynomial time. The consistency problem is shown to be NP-complete when the example sets are allowed to have coincidence at least 1 and heaviness at least 4. On the other hand, we give linear-time algorithms for solving consistency problems with coincidence 0 or heaviness at most 3. Moreover, these results remain valid when the threshold of the neuron is bounded by a constant of value at least 2, whereas consistency can be decided in linear time for neurons with threshold at most 1. We also study maximum consistency problems and obtain NP-completeness for example sets of coincidence at least 1 and heaviness at least 2, whereas we again find linear-time algorithms for the complementary cases. The same is shown to be true for neurons with bounded thresholds the bound being at least 1. On the other hand, maximum consistency can be decided in linear time for a neuron with threshold 0. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. A. Aho, J. E. Hopcroft, and J. D. Ullman, </author> <title> The Design and Analysis of Computer Algorithms, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: Theorem 1 follows from Theorem 6, Corollary 11, and Theorem 12 below. Theorem 2 will be proved by Corollary 7, Theorem 10, and Corollary 13. The next two theorems are concerned with maximum consistency, at first regar ding neurons with arbitrary threshold. 1 see e.g. <ref> [1] </ref> for a definition and its relationship to other models of computation 7 Theorem 3 MAXIMUM B-CONSISTENCY WITH COINCIDENCE C AND HEAVINESS H is NP-complete whenever C 1 and H 2. <p> A linear-time algorithm for 2-SAT has been outlined by Even, Itai, and Shamir [9]. Aspvall, Plass, and Tarjan [6] give a full description of an algorithm that even decides satisfiability of quantified 2-SAT formulas in linear time and uses the strong components algorithm by Tarjan <ref> [1, 24] </ref>. All in all, Algorithm B-CONSISTENCY runs in linear time. Herewith, Theorem 1 is proved.
Reference: [2] <author> E. Amaldi, </author> <title> On the complexity of training Perceptrons, in Artificial Neural Networks, </title> <editor> T. Kohonen, K. Makisara, O. Simula, and J. Kangas, eds., </editor> <publisher> Elsevier Science Publishers B.V. (North-Holland), </publisher> <year> 1991, </year> <pages> pp. 55-60. </pages>
Reference-contexts: The complexity of consistency and maximum consistency problems for networks computing Boolean functions has been investigated by several theorists. Diverse constraints on architectures and weights have lead to NP-completeness results on the one hand and polynomial-time algorithms on the other <ref> [2, 3, 7, 12, 13, 15] </ref>. The most restrictive architecture we can think of is a network consisting of a single unit known as McCulloch-Pitts neuron [16]. <p> The maximum consistency problem for a set of functions F has as instance a set of examples S and a natural number K. The question is whether there is a subset of S with at least K elements and a function f 2 F consistent with that subset. Amaldi <ref> [2] </ref> has shown that the problem is NP-complete for single neurons with threshold t = 0 and arbitrary weights. The constraint t = 0 was removed by Hoffgen and Simon [12]. For natural numbers C and H we define maximum consistency problems for B as follows.
Reference: [3] <author> E. Amaldi and V. Kann, </author> <title> The complexity and approximability of finding maximum feasible subsystems of linear relations, </title> <type> Tech. Rep. </type> <institution> ORWP 93/11, Ecole Polytechnique Federale de Lausanne, Departement de Mathematiques, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: The complexity of consistency and maximum consistency problems for networks computing Boolean functions has been investigated by several theorists. Diverse constraints on architectures and weights have lead to NP-completeness results on the one hand and polynomial-time algorithms on the other <ref> [2, 3, 7, 12, 13, 15] </ref>. The most restrictive architecture we can think of is a network consisting of a single unit known as McCulloch-Pitts neuron [16].
Reference: [4] <author> M. Anthony and N. Biggs, </author> <title> Computational Learning Theory, </title> <booktitle> Cambridge Tracts in Theoretical Computer Science, </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cam-bridge, </address> <year> 1992. </year>
Reference-contexts: Alternative terms frequently used in the literature are training problem [7], loading problem [13], and fitting problem [17]. The name consistency problem often occurs in the context of Valiant's pac-learning paradigm <ref> [4, 8] </ref>. 5 The consistency problem for B was proved to be NP-complete by Pitt and Valiant [20, Section 5]. Now we make use of coincidence and heaviness to define the classes of consistency problems we are concerned with. We state them in the style of Garey and Johnson [10].
Reference: [5] <author> M. A. Arbib, </author> <title> Brains, Machines, and Mathematics, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <note> second ed., </note> <year> 1987. </year>
Reference-contexts: In their original model, McCulloch and Pitts also defined inhibitory synapses that, when active, absolutely prevent the activity of the neuron regardless of the sum of the excitatory synapses [16]. Later, the name McCulloch-Pitts neuron was commonly used to refer to a neuron with arbitrary real weights <ref> [5] </ref>. 2.2 Examples An example is a pair (x; a) where x is supposed to be an input value and a to be the corresponding output value for the function being learned.
Reference: [6] <author> B. Aspvall, M. F. Plass, and R. E. Tarjan, </author> <title> A linear-time algorithm for testing the truth of certain quantified Boolean formulas, </title> <journal> Information Processing Letters, </journal> <volume> 8 (1979), </volume> <pages> pp. 121-123. </pages>
Reference-contexts: Each clause contains at most two literals. Thus, we are solving a 2-SAT problem in statement III.3. A linear-time algorithm for 2-SAT has been outlined by Even, Itai, and Shamir [9]. Aspvall, Plass, and Tarjan <ref> [6] </ref> give a full description of an algorithm that even decides satisfiability of quantified 2-SAT formulas in linear time and uses the strong components algorithm by Tarjan [1, 24]. All in all, Algorithm B-CONSISTENCY runs in linear time. Herewith, Theorem 1 is proved.
Reference: [7] <author> A. L. Blum and R. L. Rivest, </author> <title> Training a 3-node neural network is NP-complete, Neural Networks, </title> <booktitle> 5 (1992), </booktitle> <pages> pp. 117-127. </pages>
Reference-contexts: The complexity of consistency and maximum consistency problems for networks computing Boolean functions has been investigated by several theorists. Diverse constraints on architectures and weights have lead to NP-completeness results on the one hand and polynomial-time algorithms on the other <ref> [2, 3, 7, 12, 13, 15] </ref>. The most restrictive architecture we can think of is a network consisting of a single unit known as McCulloch-Pitts neuron [16]. <p> The consistency problem for a set of functions F is the problem to decide if a given set of examples S has a function f 2 F that is consistent with S. Alternative terms frequently used in the literature are training problem <ref> [7] </ref>, loading problem [13], and fitting problem [17]. The name consistency problem often occurs in the context of Valiant's pac-learning paradigm [4, 8]. 5 The consistency problem for B was proved to be NP-complete by Pitt and Valiant [20, Section 5].
Reference: [8] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth, </author> <title> Lear-nability and the Vapnik-Chervonenkis dimension, </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36 (1989), </volume> <pages> pp. 929-965. </pages>
Reference-contexts: Alternative terms frequently used in the literature are training problem [7], loading problem [13], and fitting problem [17]. The name consistency problem often occurs in the context of Valiant's pac-learning paradigm <ref> [4, 8] </ref>. 5 The consistency problem for B was proved to be NP-complete by Pitt and Valiant [20, Section 5]. Now we make use of coincidence and heaviness to define the classes of consistency problems we are concerned with. We state them in the style of Garey and Johnson [10].
Reference: [9] <author> S. Even, A. Itai, and A. Shamir, </author> <title> On the complexity of timetable and multi-commodity flow problems, </title> <journal> SIAM Journal on Computing, </journal> <volume> 5 (1976), </volume> <pages> pp. 691-703. </pages>
Reference-contexts: The construction of the set of clauses in part III is possible in linear time as well. Each clause contains at most two literals. Thus, we are solving a 2-SAT problem in statement III.3. A linear-time algorithm for 2-SAT has been outlined by Even, Itai, and Shamir <ref> [9] </ref>. Aspvall, Plass, and Tarjan [6] give a full description of an algorithm that even decides satisfiability of quantified 2-SAT formulas in linear time and uses the strong components algorithm by Tarjan [1, 24]. All in all, Algorithm B-CONSISTENCY runs in linear time. Herewith, Theorem 1 is proved.
Reference: [10] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness, </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: Now we make use of coincidence and heaviness to define the classes of consistency problems we are concerned with. We state them in the style of Garey and Johnson <ref> [10] </ref>. Let C and H be arbitrary natural numbers. The classes of consistency problems for B with bounded coincidence and heaviness are defined as follows. Let B-CONSISTENCY WITH COINCIDENCE C Instance: Example set S with i (S) C. <p> Dichotomy properties are also known for other problem classes, the most famous among them might be k-SAT, the problem of deciding whether a set of clauses with k literals per clause has a satisfying assignment <ref> [10, p. 259] </ref>. A more general result has been shown by Schaefer for GENERALIZED SATISFIABILITY revealing a dichotomy as well [10, 21]. Finally, we mention GRAPH-K-COLORABILITY [10, p. 191]. <p> A more general result has been shown by Schaefer for GENERALIZED SATISFIABILITY revealing a dichotomy as well <ref> [10, 21] </ref>. Finally, we mention GRAPH-K-COLORABILITY [10, p. 191]. One should have in mind that all these dichotomies are proper only if P 6= NP. 3 NP-Complete Consistency Problems To show NP-completeness of the unrestricted consistency problem for B, Pitt and Valiant defined a reduction from ZERO-ONE INTEGER PROGRAMMING [20]. <p> A more general result has been shown by Schaefer for GENERALIZED SATISFIABILITY revealing a dichotomy as well [10, 21]. Finally, we mention GRAPH-K-COLORABILITY <ref> [10, p. 191] </ref>. One should have in mind that all these dichotomies are proper only if P 6= NP. 3 NP-Complete Consistency Problems To show NP-completeness of the unrestricted consistency problem for B, Pitt and Valiant defined a reduction from ZERO-ONE INTEGER PROGRAMMING [20]. <p> Question: Is there a truth assignment fi : U ! f0; 1g such that each subset in C has exactly one true variable? POSITIVE 1-IN-3SAT is already known to be NP-complete <ref> [10, p. 259] </ref>. We show that it remains NP-complete under the requirement of almost disjointness. Lemma 5 ALMOST DISJOINT POSITIVE 1-IN-3SAT is NP-complete. Proof. We give a reduction from POSITIVE 1-IN-3SAT. Let c and d be subsets violating the condition of almost disjointness. <p> Their result remains valid if the weights are to be from the set f1; 1g. It turns out that a slight modification of their construction is 10 sufficient for our purpose. The reduction is from VERTEX COVER <ref> [10, p. 190] </ref>. Let (V; E); K be an instance of the latter where V = fv 1 ; : : : ; v n g. The set of examples S where dom (S) f0; 1g 2n is constructed as follows.
Reference: [11] <author> M. Golea and M. Marchand, </author> <title> On learning perceptrons with binary weights, </title> <booktitle> Neural Computation, 5 (1993), </booktitle> <pages> pp. 767-782. </pages>
Reference-contexts: Venkatesh examined algorithms for finding binary weights of a single neuron consistent with an example set in a distribution dependent manner [25]. An inconsistent algorithm for pac-learning such a neuron under certain distributions has been presented by Golea and Marchand <ref> [11] </ref>. The previous short list of references is by no means intended to be complete, let alone unbiased. At least, we consider it sufficient to give evidence for the perseverating interest in simple neurons.
Reference: [12] <author> K.-U. H offgen and H.-U. Simon, </author> <title> Robust trainability of single neurons, </title> <booktitle> in Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1992, </year> <pages> pp. 428-439. </pages>
Reference-contexts: The complexity of consistency and maximum consistency problems for networks computing Boolean functions has been investigated by several theorists. Diverse constraints on architectures and weights have lead to NP-completeness results on the one hand and polynomial-time algorithms on the other <ref> [2, 3, 7, 12, 13, 15] </ref>. The most restrictive architecture we can think of is a network consisting of a single unit known as McCulloch-Pitts neuron [16]. <p> Amaldi [2] has shown that the problem is NP-complete for single neurons with threshold t = 0 and arbitrary weights. The constraint t = 0 was removed by Hoffgen and Simon <ref> [12] </ref>. For natural numbers C and H we define maximum consistency problems for B as follows. Let MAXIMUM B-CONSISTENCY WITH COINCIDENCE C Instance: Example set S with i (S) C, natural number K. <p> In the remainder of the section we address maximum consistency problems. Theorem 8 MAXIMUM B-CONSISTENCY WITH COINCIDENCE 1 AND HEAVINESS 2 is NP-complete. Proof. The proof is an adaptation of the proof of Theorem 3.1 in Hoffgen and Simon <ref> [12] </ref>. They have established NP-completeness of the maximum consistency problem for neurons with arbitrary weights. Their result remains valid if the weights are to be from the set f1; 1g. It turns out that a slight modification of their construction is 10 sufficient for our purpose.
Reference: [13] <author> J. S. Judd, </author> <title> Neural Network Design and the Complexity of Learning, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: The complexity of consistency and maximum consistency problems for networks computing Boolean functions has been investigated by several theorists. Diverse constraints on architectures and weights have lead to NP-completeness results on the one hand and polynomial-time algorithms on the other <ref> [2, 3, 7, 12, 13, 15] </ref>. The most restrictive architecture we can think of is a network consisting of a single unit known as McCulloch-Pitts neuron [16]. <p> The consistency problem for a set of functions F is the problem to decide if a given set of examples S has a function f 2 F that is consistent with S. Alternative terms frequently used in the literature are training problem [7], loading problem <ref> [13] </ref>, and fitting problem [17]. The name consistency problem often occurs in the context of Valiant's pac-learning paradigm [4, 8]. 5 The consistency problem for B was proved to be NP-complete by Pitt and Valiant [20, Section 5].
Reference: [14] <author> H. Karloff, </author> <title> Linear Programming, </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1991. </year>
Reference-contexts: Consistency for neurons with unconstrained weights, however, can be decided in polynomial time using algorithms for Linear Programming <ref> [14] </ref>. Therefore, with respect to arbitrary weights there is no dichotomy for consistency problems, whereas the dichotomy for maximum consistency coincides with the right-hand side of Figure 1. 2 In any case, this is not justified until after the results. 16
Reference: [15] <author> J.-H. Lin and J. S. Vitter, </author> <title> Complexity results on learning by neural nets, </title> <booktitle> Machine Learning, 6 (1991), </booktitle> <pages> pp. 211-230. 17 </pages>
Reference-contexts: The complexity of consistency and maximum consistency problems for networks computing Boolean functions has been investigated by several theorists. Diverse constraints on architectures and weights have lead to NP-completeness results on the one hand and polynomial-time algorithms on the other <ref> [2, 3, 7, 12, 13, 15] </ref>. The most restrictive architecture we can think of is a network consisting of a single unit known as McCulloch-Pitts neuron [16].
Reference: [16] <author> W. S. McCulloch and W. Pitts, </author> <title> A logical calculus of the ideas immanent in nervous activity, </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 5 (1943), </volume> <pages> pp. 115-133. </pages>
Reference-contexts: Diverse constraints on architectures and weights have lead to NP-completeness results on the one hand and polynomial-time algorithms on the other [2, 3, 7, 12, 13, 15]. The most restrictive architecture we can think of is a network consisting of a single unit known as McCulloch-Pitts neuron <ref> [16] </ref>. It has been introduced by McCulloch and Pitts to model the logical behavior of nerve cells and is based on the hypothesis of an all-or-none character of neural activity. <p> In their original model, McCulloch and Pitts also defined inhibitory synapses that, when active, absolutely prevent the activity of the neuron regardless of the sum of the excitatory synapses <ref> [16] </ref>. Later, the name McCulloch-Pitts neuron was commonly used to refer to a neuron with arbitrary real weights [5]. 2.2 Examples An example is a pair (x; a) where x is supposed to be an input value and a to be the corresponding output value for the function being learned.
Reference: [17] <author> B. K. Natarajan, </author> <title> Machine Learning: A Theoretical Approach, </title> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: The consistency problem for a set of functions F is the problem to decide if a given set of examples S has a function f 2 F that is consistent with S. Alternative terms frequently used in the literature are training problem [7], loading problem [13], and fitting problem <ref> [17] </ref>. The name consistency problem often occurs in the context of Valiant's pac-learning paradigm [4, 8]. 5 The consistency problem for B was proved to be NP-complete by Pitt and Valiant [20, Section 5].
Reference: [18] <author> G. Palm, </author> <title> On associative memory, </title> <journal> Biological Cybernetics, </journal> <volume> 36 (1980), </volume> <pages> pp. 19-31. </pages>
Reference-contexts: Willshaw et al. used neurons with binary weights as building blocks for networks being able to perform pattern association [27] and generalization tasks [26]. Information storage capacity for Hebbian learning in these models has been calculated by Palm <ref> [18] </ref> (see also the review [19] where binary synapses are compared to arbitrary synapses and arbitrary learning rules). Venkatesh examined algorithms for finding binary weights of a single neuron consistent with an example set in a distribution dependent manner [25].
Reference: [19] <author> G. Palm, </author> <title> Memory capacities of local rules for synaptic modification: A comparative review, </title> <booktitle> Concepts in Neuroscience, 1 (1991), </booktitle> <pages> pp. 97-128. </pages>
Reference-contexts: Willshaw et al. used neurons with binary weights as building blocks for networks being able to perform pattern association [27] and generalization tasks [26]. Information storage capacity for Hebbian learning in these models has been calculated by Palm [18] (see also the review <ref> [19] </ref> where binary synapses are compared to arbitrary synapses and arbitrary learning rules). Venkatesh examined algorithms for finding binary weights of a single neuron consistent with an example set in a distribution dependent manner [25].
Reference: [20] <author> L. Pitt and L. G. Valiant, </author> <title> Computational limitations on learning from examples, </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 35 (1988), </volume> <pages> pp. 965-984. </pages>
Reference-contexts: Thus, we obtain a plain computing device that still embodies the threshold principle of neurons and has a non-trivial consistency problem: it has been proved to be NP-complete by Pitt and Valiant <ref> [20] </ref>. In this report we take a closer look at consistency problems for this simple neuron. <p> Alternative terms frequently used in the literature are training problem [7], loading problem [13], and fitting problem [17]. The name consistency problem often occurs in the context of Valiant's pac-learning paradigm [4, 8]. 5 The consistency problem for B was proved to be NP-complete by Pitt and Valiant <ref> [20, Section 5] </ref>. Now we make use of coincidence and heaviness to define the classes of consistency problems we are concerned with. We state them in the style of Garey and Johnson [10]. Let C and H be arbitrary natural numbers. <p> Finally, we mention GRAPH-K-COLORABILITY [10, p. 191]. One should have in mind that all these dichotomies are proper only if P 6= NP. 3 NP-Complete Consistency Problems To show NP-completeness of the unrestricted consistency problem for B, Pitt and Valiant defined a reduction from ZERO-ONE INTEGER PROGRAMMING <ref> [20] </ref>. It turns out that the example sets used therein have coincidence at least (n 1)=2. So, nothing can be inferred from their proof concerning constant coincidence and heaviness. We obtain the result by reducing a variant of ONE-IN-THREE 3SAT which we call ALMOST DISJOINT POSITIVE 1-IN-3SAT.
Reference: [21] <author> T. J. Schaefer, </author> <title> The complexity of satisfiability problems, </title> <booktitle> in Proceedings of the 10th Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1978, </year> <pages> pp. 216-226. </pages>
Reference-contexts: A more general result has been shown by Schaefer for GENERALIZED SATISFIABILITY revealing a dichotomy as well <ref> [10, 21] </ref>. Finally, we mention GRAPH-K-COLORABILITY [10, p. 191]. One should have in mind that all these dichotomies are proper only if P 6= NP. 3 NP-Complete Consistency Problems To show NP-completeness of the unrestricted consistency problem for B, Pitt and Valiant defined a reduction from ZERO-ONE INTEGER PROGRAMMING [20].
Reference: [22] <author> K. Steinbuch, </author> <title> Die Lernmatrix, </title> <journal> Kybernetik, </journal> <volume> 1 (1961), </volume> <pages> pp. 36-45. </pages>
Reference-contexts: The learning matrices of Stein-buch consisted of conditioned connections that, in their simplest version, were able to attain two possible conductance values during a learning phase <ref> [22, 23] </ref>. Willshaw et al. used neurons with binary weights as building blocks for networks being able to perform pattern association [27] and generalization tasks [26].
Reference: [23] <author> K. Steinbuch and U. A. W. Piske, </author> <title> Learning matrices and their applications, </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 12 (1963), </volume> <pages> pp. 846-862. </pages>
Reference-contexts: The learning matrices of Stein-buch consisted of conditioned connections that, in their simplest version, were able to attain two possible conductance values during a learning phase <ref> [22, 23] </ref>. Willshaw et al. used neurons with binary weights as building blocks for networks being able to perform pattern association [27] and generalization tasks [26].
Reference: [24] <author> R. Tarjan, </author> <title> Depth-first search and linear graph algorithms, </title> <journal> SIAM Journal on Computing, </journal> <volume> 1 (1972), </volume> <pages> pp. 146-160. </pages>
Reference-contexts: A linear-time algorithm for 2-SAT has been outlined by Even, Itai, and Shamir [9]. Aspvall, Plass, and Tarjan [6] give a full description of an algorithm that even decides satisfiability of quantified 2-SAT formulas in linear time and uses the strong components algorithm by Tarjan <ref> [1, 24] </ref>. All in all, Algorithm B-CONSISTENCY runs in linear time. Herewith, Theorem 1 is proved.
Reference: [25] <author> S. S. Venkatesh, </author> <title> On learning binary weights for majority functions, </title> <booktitle> in Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <editor> L. G. Valiant and M. K. Warmuth, eds., </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991, </year> <pages> pp. 257-266. </pages>
Reference-contexts: Venkatesh examined algorithms for finding binary weights of a single neuron consistent with an example set in a distribution dependent manner <ref> [25] </ref>. An inconsistent algorithm for pac-learning such a neuron under certain distributions has been presented by Golea and Marchand [11]. The previous short list of references is by no means intended to be complete, let alone unbiased.
Reference: [26] <author> D. J. Willshaw, </author> <title> A simple network capable of inductive generalization, </title> <journal> Proc. R. Soc. Lond. B, </journal> <volume> 182 (1972), </volume> <pages> pp. 233-247. </pages>
Reference-contexts: Willshaw et al. used neurons with binary weights as building blocks for networks being able to perform pattern association [27] and generalization tasks <ref> [26] </ref>. Information storage capacity for Hebbian learning in these models has been calculated by Palm [18] (see also the review [19] where binary synapses are compared to arbitrary synapses and arbitrary learning rules).
Reference: [27] <author> D. J. Willshaw, O. Buneman, and H. C. Longuet-Higgins, </author> <title> Non-holographic associative memory, </title> <booktitle> Nature, 222 (1969), </booktitle> <pages> pp. 960-962. </pages>
Reference-contexts: The learning matrices of Stein-buch consisted of conditioned connections that, in their simplest version, were able to attain two possible conductance values during a learning phase [22, 23]. Willshaw et al. used neurons with binary weights as building blocks for networks being able to perform pattern association <ref> [27] </ref> and generalization tasks [26]. Information storage capacity for Hebbian learning in these models has been calculated by Palm [18] (see also the review [19] where binary synapses are compared to arbitrary synapses and arbitrary learning rules).
References-found: 27

