URL: http://polaris.cs.uiuc.edu/reports/1268.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: USING A CACHE IN PLACE OF A CEDAR-LIKE VECTOR PREFETCH UNIT  
Author: BY MAHDI SEDDIGHNEZHAD 
Degree: B.Engr., Shiraz University, 1986 THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the Graduate College of the  
Address: 1993 Urbana, Illinois  
Affiliation: University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. L. Baer and T. F. Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 176-186, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: The advantage of this method is that more than one line can be prefetched and held in the stream buffer. This is most beneficial when the sequential locations are referenced. Baer and Chen <ref> [1] </ref> proposed a different uniprocessor prefetching mechanism based on predicting the instruction stream with a look-ahead program counter (LA-PC). They can predict the address of the block to be prefetched whenever the LA-PC encounters a LOAD or STORE instruction. The LA-PC is several cycles ahead of the regular PC. <p> Completion time is the time needed for each processor to finish its assigned task and the execution time is shown as: T E (p) = M ax (T i Where T i cp is completion time for processor i, and i 2 <ref> [1; p] </ref>. Speedup is used to compare the performance of two different designs. In our study we have used three kinds of speedup, S p , S c , and S pc .
Reference: [2] <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, L. Pointer, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsuing, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, R. Goodrum, and J. Martin. </author> <title> The perfect club benchmarks: Effective performance evaluation of supercomputers. </title> <type> Technical Report 827, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomputing Research & Development, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: VECSUM is a simple loop to add two vectors. TRIDIA computes the multiplication of a tridiagonal matrix by a vector. EFLUX is a subroutine to compute the Euler fluxes in the FLO52Q benchmark from the Perfect Benchmarks <ref> [2] </ref>. CFD is a two-dimensional computational fluid dynamics kernel. The last benchmark is CG, which solves the sparse linear system of equations with the classical conjugate-gradient algorithm. All these programs use the stride-one accesses. Therefore, the spatial locality is high for these programs.
Reference: [3] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: As a result, the LA-PC may produce more memory and network traffic by making wrong predictions. Software techniques and compile-time analysis of programs have been studied by some researchers <ref> [3, 8, 9, 14] </ref> in order to improve memory latency, and as a result overall system performance. Klaiber and Levy [17] describe a software controlled data prefetching scheme to hide the memory latency in uniprocessor systems. <p> Two things are important in their method: first, predicting the probable sequence of references, and second, predicting the addresses accessed by those references. Extra time spent to issue the prefetch instructions is a major problem with this method. Callahan, Kennedy, and Porterfield <ref> [3] </ref> propose a software prefetching technique in order to reduce cache misses. They have shown that reusability of data in cache can be maximized by reordering computations.
Reference: [4] <author> S. C. Chen. </author> <title> Large-scale and high-speed multiprocessor system for scientific applications: Cray x-mp series. </title> <booktitle> In Proc. NATO Advanced Research Workshop on High-Speed Computing, </booktitle> <month> June </month> <year> 1983. </year>
Reference-contexts: In addition, during last decade researchers have become more interested in designing multiprocessor systems that consist of several processors connected to memory modules via interconnection networks in order to respond to the needs of computationally intensive applications. Several vector supercomputers like the CRAY-XMP <ref> [4] </ref> and CEDAR (developed at University of Illinois) [18] have been developed during the last decade. 2 Such systems have even longer memory latency time because of limited bandwidth and conflicts in interconnection networks and memory.
Reference: [5] <author> H. Cheong and A. V. Veidenbaum. </author> <title> Compiler-directed cache management in multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 23 </volume> <pages> 39-47, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: A fast and easy solution to the problem is just not to allow the shared data to be cached, but this method will cause a poor performance for the system. We have used a software-based coherence scheme <ref> [5] </ref> in this study. An invalidation instruction is executed by each processor starting the execution of a DOALL to invalidate its cache. After the DOALL, the processor responsible for executing the sequential part of the code (processor zero) also needs to execute the invalidation instruction after the ENDDOALL. <p> After the DOALL, the processor responsible for executing the sequential part of the code (processor zero) also needs to execute the invalidation instruction after the ENDDOALL. This is called a Simple invalidation approach in <ref> [5] </ref> and is shown in Example 2.2. 21 Example 2.2 DOALL j = 1 , p Invalidate DO i = 1 , N A (i) = ..... . = C (i) .
Reference: [6] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, Z. Li, and D. Padua. </author> <title> Restructuring fortran program for cedar. </title> <booktitle> International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: The restructuring strategy is such that the outermost parallel loops are distributed across the processors and the innermost loops are vectorized. Parallelism is expressed by DOALL loops <ref> [6] </ref>. The compiler does not generate nested parallel loops, and only the outermost loop is executed as parallel. 25 26 3.2 Code Generation and Optimization A pseudo-assembly code is generated by a code generator [11] from the parallel Fortran program restructured by Parafrase as described in the previous section.
Reference: [7] <author> John W.C. Fu and Janak H. Patel. </author> <title> Data prefetching in multiprocessor vector cache memories. </title> <booktitle> In In International Symposium on Computer Architecture, </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: However, they did not study the importance of stride and its effect on the cache performance. 9 Fu and Patel <ref> [7] </ref> also agree that choosing the best line size is important, but they have shown that finding the optimal line size is not easy for vector caches in vector multiprocessor systems because of varying access patterns of references, non-unit stride accesses and low data reusability.
Reference: [8] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global memory transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 587-616, </pages> <month> October </month> <year> 1988. </year> <month> 66 </month>
Reference-contexts: As a result, the LA-PC may produce more memory and network traffic by making wrong predictions. Software techniques and compile-time analysis of programs have been studied by some researchers <ref> [3, 8, 9, 14] </ref> in order to improve memory latency, and as a result overall system performance. Klaiber and Levy [17] describe a software controlled data prefetching scheme to hide the memory latency in uniprocessor systems.
Reference: [9] <author> E. Gornish, E. Granston, and A. Veidenbaum. </author> <title> Compiler directed data prefetch-ing in multiprocessors with memory hierarchies. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 354-368, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: As a result, the LA-PC may produce more memory and network traffic by making wrong predictions. Software techniques and compile-time analysis of programs have been studied by some researchers <ref> [3, 8, 9, 14] </ref> in order to improve memory latency, and as a result overall system performance. Klaiber and Levy [17] describe a software controlled data prefetching scheme to hide the memory latency in uniprocessor systems. <p> Reducing the cost of the system by using a smaller cache is an advantage of this method. However, there is some overhead involved that hardware should minimize in order to gain good performance from this system. Gornish <ref> [9, 10] </ref> finds the earliest time at which prefetching can be performed in a parallel program and shows that by using compile-time analysis and software prefetching, data needed within an iteration of the loop can be fetched one or more iterations ahead.
Reference: [10] <author> E. H. Gornish. </author> <title> Compile time analysis for data prefetching. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Reducing the cost of the system by using a smaller cache is an advantage of this method. However, there is some overhead involved that hardware should minimize in order to gain good performance from this system. Gornish <ref> [9, 10] </ref> finds the earliest time at which prefetching can be performed in a parallel program and shows that by using compile-time analysis and software prefetching, data needed within an iteration of the loop can be fetched one or more iterations ahead.
Reference: [11] <author> E. Granston. </author> <title> Strplp/codegn user's manual. </title> <type> Technical report, Internal Report, </type> <month> March </month> <year> 1990. </year>
Reference-contexts: Parallelism is expressed by DOALL loops [6]. The compiler does not generate nested parallel loops, and only the outermost loop is executed as parallel. 25 26 3.2 Code Generation and Optimization A pseudo-assembly code is generated by a code generator <ref> [11] </ref> from the parallel Fortran program restructured by Parafrase as described in the previous section. The pseudo-assembly code contains scalar and vector instructions as well as parallel constructs such as COBEGIN/COEND implementing DOALL loops.
Reference: [12] <author> E. D. Granston. </author> <title> Memory Management Techniques for Large-Scale, Shared-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1992. </year>
Reference-contexts: An example of a Fortran program to compute A i = B i + C i and its optimized pseudo-assembly code generated by Parafrase, code generator and optimizer are shown in Appendix A. 3.4 Simulator The simulator is a modified version of the simulator developed by Granston <ref> [12] </ref>. Each component of the system is modeled by a separate module in the simulator (processor, cache, network, memory). Required parameters for each component can be changed for different experiments to study the effect of each parameter on the behavior of each component or on the overall system. <p> For example, multiple network ports per processor can be used, more bandwidth can be achieved by a larger degree of interleaving or wider data path between processor and memory. More than one outstanding request can be allowed, although, it has been shown by Granston <ref> [12] </ref> that it does not work automatically with the VPU and while the processor may have any number 61 of memory access requests outstanding, the VPU processes these requests one at a time. More than one VPU can be used in each processor.
Reference: [13] <author> Elana Granston, Stephen Turner, and Alexander Veidenbaum. </author> <title> Design and analysis of a scalable, shared-memory system with support for burst traffic. </title> <booktitle> In Proceedings of the 1990 workshop on shared memory multiprocessors, at the Int'l. Symp. on computer architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Finally, the summary and conclusions are given in Chapter 5. 6 1.1 Related Work Several studies have been done by different researchers with respect to the behavior of the memory system and reduction of the memory access delay. Granston, Turner, and Veidenbaum <ref> [13] </ref> have studied the behavior of the memory system in multiprocessor systems in the presence of bursts of requests. They concentrated on optimization of read traffic because there are more reads than writes in real programs. <p> They have investigated different ways of reducing memory latency by decreasing the memory and network conflicts and increasing bandwidth. The effect of burst length and burst issue rate has also been studied by Granston, Turner, and Veidenbaum. Results in <ref> [13] </ref> show that a scalable memory system for vector multiprocessor can be constructed. Jouppi [16] has introduced a stream buffer mechanism for uniprocessor systems that begins prefetching successive lines into the stream buffer, starting at the miss address, when a miss occurs.
Reference: [14] <author> Elena Granston and Alexander Veidenbaum. </author> <title> An integrated hardware/software solution for effective management of local storage in high performance systems. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: As a result, the LA-PC may produce more memory and network traffic by making wrong predictions. Software techniques and compile-time analysis of programs have been studied by some researchers <ref> [3, 8, 9, 14] </ref> in order to improve memory latency, and as a result overall system performance. Klaiber and Levy [17] describe a software controlled data prefetching scheme to hide the memory latency in uniprocessor systems. <p> The first system (case (a)) uses a vector prefetch unit in each processor to fetch data from global memory into a prefetch data buffer. The second system (case (b)) is similar to the system used by Granston and Veidenbaum <ref> [14] </ref> to study the shared-memory multiprocessor. The main difference is that the vector prefetch unit is replaced with a private cache in each processor. We have also used more accurate timing for both scalar and vector instructions based on MIPS R3000 instruction timing.
Reference: [15] <author> K. Hwang and F. A. Briggs. </author> <title> Computer Architecture and Parallel Processing. </title> <publisher> McGraw-Hill, </publisher> <year> 1984. </year>
Reference-contexts: Therefore, hit ratios can be affected by timing in our study. 2.4.4 Write Policy We have used the write-through-with-write-allocate <ref> [15] </ref> policy, in which the main memory and the cache are updated at the same time. Also a line is loaded into the cache 20 on a write miss. In this policy both read and write references contribute to the hit ratio.
Reference: [16] <author> N. P. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In In International Symposium on Computer Architecture, </booktitle> <pages> pages 364-373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The effect of burst length and burst issue rate has also been studied by Granston, Turner, and Veidenbaum. Results in [13] show that a scalable memory system for vector multiprocessor can be constructed. Jouppi <ref> [16] </ref> has introduced a stream buffer mechanism for uniprocessor systems that begins prefetching successive lines into the stream buffer, starting at the miss address, when a miss occurs.
Reference: [17] <author> A. C. Klaiber and H. M. Levy. </author> <title> An architecture for software-controlled data prefetch-ing. </title> <booktitle> In In International Symposium on Computer Architecture, </booktitle> <pages> pages 43-53, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: As a result, the LA-PC may produce more memory and network traffic by making wrong predictions. Software techniques and compile-time analysis of programs have been studied by some researchers [3, 8, 9, 14] in order to improve memory latency, and as a result overall system performance. Klaiber and Levy <ref> [17] </ref> describe a software controlled data prefetching scheme to hide the memory latency in uniprocessor systems. The main idea in their approach is to insert FETCH instructions into the instruction stream at compile time. The FETCH instruction prefetches data, predicted to be used later, into a fetch buffer.
Reference: [18] <author> J. Konicek, T. Tilton, A. Veidenbaum, C. Q. Zhu, E. S. Davidson, R. Downing, M. Haney, M. Sharma, P. C. Yew, P. M. Farmwald, D. Kuck, D. Lavery, R. lind-sey, D. Pointer, J. Andrews, T. Beck, T. Murphy, S. Turner, and N. Warter. </author> <title> The organization of the cedar system. </title> <booktitle> International Conference on Parallel Processing, </booktitle> <pages> pages I-49-I-56, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Several vector supercomputers like the CRAY-XMP [4] and CEDAR (developed at University of Illinois) <ref> [18] </ref> have been developed during the last decade. 2 Such systems have even longer memory latency time because of limited bandwidth and conflicts in interconnection networks and memory. Memory latency is a critical problem in achieving high performance, particularly for shared-memory multiprocessor systems.
Reference: [19] <author> David J. Kuck, R. H. Kuhn, B. Leasure, and M. Wolfe. </author> <title> The structure of an advanced vectorizer for pipelined processors. </title> <booktitle> In Fourth International Computer Software and Applications Conference, </booktitle> <month> October </month> <year> 1980. </year>
Reference-contexts: The goal of this thesis is to investigate the performance of the two approaches described earlier in a Cedar-like system. We use a program-driven timing simulator. Real Fortran kernels have been used for our experiments. Parallelization and vectorization are applied to the sequential kernels by Parafrase <ref> [19, 24] </ref>, which is a Fortran program restructurer. A pseudo-assembly code is then generated from the parallel Fortran by a code generator. As the final step, the generated code is optimized and used to drive the simulator. <p> The next two sections describe the compilation, code generation and optimization and are followed by the simulator description. Figure 3.1 shows the simulation environment. 3.1 Compilation A sequential Fortran program is restructured by Parafrase <ref> [19, 24] </ref> such that the resulting code takes the advantage of the parallelism available in a particular machine or class of machines (in our case, a vector multiprocessor machine).
Reference: [20] <author> Krishnan Padmanabhan and Duncan H. Lawrie. </author> <title> A class of redundant path multistage interconnection networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32:1099-1108, </volume> <month> December </month> <year> 1983. </year> <month> 67 </month>
Reference-contexts: The main difference is that the vector prefetch unit is replaced with a private cache in each processor. We have also used more accurate timing for both scalar and vector instructions based on MIPS R3000 instruction timing. We have used packet-switched Omega networks <ref> [20] </ref> as interconnection networks between processors and modules in this study. We also assume that the number of memory modules and processors are the same; the 32 processors are connected to 32 memory modules using two separate unidirectional networks to maximize the bandwidth. <p> Memory response goes to the memory output buffer. Backward network accepts the response from the memory if its input buffer is not full. For more detail see [23]. 22 2.6 Network Model The networks are packet-switched Omega networks <ref> [20] </ref>. Depending on whether it is a read or write request, a packet can be one or two words long. An address/control word (ACM) which always comes as the first word of the packet, carries the requested memory address and also network routing information.
Reference: [21] <author> A. J. Smith. </author> <title> Cache memories. </title> <journal> In ACM Computing Surveys, </journal> <volume> Vol. 14, </volume> <pages> pages 473-530, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: This restriction limits the benefit of prefetching. The other problem with using the prefetch unit is the overhead involved in providing the start, stride, mask, and length for prefetching small blocks of data. Gather/scatter operations can not be handled by the vector prefetch unit. Temporal locality <ref> [21] </ref> can not be exploited by the VPU. The advantages of using a cache are that a cache fetches a line on a miss and if the line size is greater than one, additional words are prefetched. <p> Another problem with using a cache is misses that are part of cache overhead in fetching a vector. This problem can be solved by prefetching line (i + 1) when accessing line i. Prefetching can be done in two ways: miss prefetching, and tag prefetching <ref> [21] </ref>. There are other reasons for long latency in multiprocessor systems, but we do not study them in detail in this thesis.
Reference: [22] <author> K. So and V. Zecca. </author> <title> Cache performance of vector processors. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 261-268, </pages> <year> 1988. </year>
Reference-contexts: Therefore data would be available when they are needed. Zecca and So <ref> [22] </ref> have looked at cache performance for vector processors and have shown that understanding the memory access patterns for real applications is very important.
Reference: [23] <author> Stephen Wilson Turner. </author> <title> Shared memory and interconnection network performance for vector multiprocessors. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: The network contention is also a factor that increases the memory latency. Memory bandwidth is another limitation in the multiprocessor system. Increasing the degree of interleaving can increase the system memory bandwidth and reduce contention. Turner <ref> [23] </ref> has studied these problems in more detail. Another important factor affecting the performance of the memory system, and as a result latency time, is the number of outstanding memory requests that are allowed in the processor. For example, in Cedar, only one outstanding vector load request is allowed. <p> Memory requests are stored in the input buffer and then forwarded to memory. Memory response goes to the memory output buffer. Backward network accepts the response from the memory if its input buffer is not full. For more detail see <ref> [23] </ref>. 22 2.6 Network Model The networks are packet-switched Omega networks [20]. Depending on whether it is a read or write request, a packet can be one or two words long. <p> Switches are capable of transferring a single word into each output port during each cycle unless there are conflicts. The delay in the network to transfer a packet is one cycle per stage plus any queuing delays. Turner <ref> [23] </ref> describes the switch and network model in more detail. 23 CHAPTER 3 SIMULATION ENVIRONMENT An accurate simulation is needed to study the behavior of the vector caches, vector prefetch units, and their effects on performance of a large-scale vector multiprocessor system. <p> Since these three parts are non-overlapping because of the chosen system organization, the total execution time is approximated as: T E T C + T Hit + T W . The following two metrics are used as in <ref> [23] </ref> to determine network and shared memory performance. <p> As has been shown in <ref> [23] </ref> line size and issue probability are two parameters affecting latency and interarrival time and their increase leads to more contention and queuing delays. In the case of prefetch unit, the prefetch line size is fixed to the vector length.
Reference: [24] <author> Michael Joseph Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1982. </year>
Reference-contexts: The goal of this thesis is to investigate the performance of the two approaches described earlier in a Cedar-like system. We use a program-driven timing simulator. Real Fortran kernels have been used for our experiments. Parallelization and vectorization are applied to the sequential kernels by Parafrase <ref> [19, 24] </ref>, which is a Fortran program restructurer. A pseudo-assembly code is then generated from the parallel Fortran by a code generator. As the final step, the generated code is optimized and used to drive the simulator. <p> The next two sections describe the compilation, code generation and optimization and are followed by the simulator description. Figure 3.1 shows the simulation environment. 3.1 Compilation A sequential Fortran program is restructured by Parafrase <ref> [19, 24] </ref> such that the resulting code takes the advantage of the parallelism available in a particular machine or class of machines (in our case, a vector multiprocessor machine).
References-found: 24

