URL: ftp://ftp.cs.umass.edu/pub/techrept/techreport/1992/UM-CS-1992-083.ps
Refering-URL: http://www-ml.cs.umass.edu/
Root-URL: 
Title: Multivariate Decision Trees  
Author: Carla E. Brodley Paul E. Utgoff 
Address: Amherst, Massachusetts 01003 USA  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: COINS Technical Report 92-82 December 1992 Abstract Multivariate decision trees overcome a representational limitation of univariate decision trees: univariate decision trees are restricted to splits of the instance space that are orthogonal to the feature's axis. This paper discusses the following issues for constructing multivariate decision trees: representing a multivariate test, including symbolic and numeric features, learning the coefficients of a multivariate test, selecting the features to include in a test, and pruning of multivariate decision trees. We present some new and review some well-known methods for forming multivariate decision trees. The methods are compared across a variety of learning tasks to assess each method's ability to find concise, accurate decision trees. The results demonstrate that some multivariate methods are more effective than others. In addition, the experiments confirm that allowing multivariate tests improves the accuracy of the resulting decision tree over univariate trees. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. </author> <year> (1987). </year> <title> Learnability and the Vapnik-Chervonenkis dimension, </title> <institution> (UCSC-CRL-87-20), Santa Cruz, CA: University of California. </institution> <note> 34 Breiman, </note> <author> L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and regression trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group. </publisher>
Reference: <author> Brodley, C. E., & Utgoff, P. E. </author> <year> (1992). </year> <title> Multivariate versus univariate decision trees, </title> <type> (Coins Technical Report 92-8), </type> <institution> Amherst, MA: University of Massachusetts, Department of Computer and Information Science. </institution>
Reference-contexts: Empirical tests show that 10 setting ff = :01 is effective in reducing total training time without reducing the quality of the learned classifier <ref> (Brodley & Utgoff, 1992) </ref>. 3.4 CART: Explicit reduction of impurity CART searches explicitly for a set of coefficients that minimizes the impurity of the partition defined by the multivariate test (Breiman, et al. 1984). <p> This measure is analogous to the Euclidean interclass distance measure for estimating error (Kittler, 1986). This elimination procedure is used in the LMDT algorithm <ref> (Brodley & Utgoff, 1992) </ref>. 4.4 Heuristic Sequential Search The Heuristic Sequential Search (HSS) algorithm is a combination of the SFS algorithm and the SBE algorithm.
Reference: <author> Buntine, W., & Niblett, T. </author> <year> (1992). </year> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 75-85. </pages> <note> Detrano,R., </note> <author> Janosi,A., Steinbrunn,W., Pfisterer, M., Schmid, J., Sandhu, S., Guppy, K., Lee, S., & Froelicher, V. </author> <year> (1989). </year> <title> International application of a new probability algorithm for the diagnosis of coronary artery disese. </title> <journal> American Journal of Cardiology, </journal> <volume> 64, </volume> <pages> 304-310. </pages>
Reference: <author> Duda, R. O., & Fossum, H. </author> <year> (1966). </year> <title> Pattern classification by iteratively determined linear and piecewise linear discriminant functions. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> EC-15, </volume> <pages> 220-232. </pages>
Reference-contexts: We discuss its application to linear machines here. (See Frean (1990) for a discussion of its application to an LTU.) One well known method for training a linear machine is the absolute error correction rule <ref> (Duda & Fossum, 1966) </ref>, which adjusts W i and W j , where i is the class to which the instance belongs and j is the class to which the linear machine incorrectly assigns the instance.
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <address> New York: </address> <publisher> Wiley & Sons. </publisher>
Reference-contexts: If the number of unique instances is not greater than twice the dimensionality of the number of features in the test, then the test will underfit the training instances <ref> (Duda & Hart, 1973) </ref>. In other words, when there are too few instances, there are many possible orientations for the hyperplane defined by the test, and there is no basis for selecting one orientation over another. <p> The Pocket Algorithm fulfills a critical role when searching for a separating hyperplane because the classification accuracy of an LTU trained using the absolute error correction rule is unpredictable when the instances are not linearly separable <ref> (Duda & Hart, 1973) </ref>. <p> The criterion for when to reduce fi is motivated by the fact that the magnitude of the linear machine increases rapidly during the early training, stabilizing when the decision boundary is near its final location <ref> (Duda & Hart, 1973) </ref>. The default values for a and b remain the same throughout the experiments reported in this paper. A thermal linear machine has converged when the magnitude of each correction, k, to the linear machine is larger than fi for each instance in the training set.
Reference: <author> Fayyad, U. M., & Irani, K. B. </author> <year> (1990). </year> <booktitle> What should be minimized in a decision tree? Proceedings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 749-754). </pages> <address> Boston, MA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fayyad, U. M., & Irani, K. B. </author> <year> (1992a). </year> <title> On the handling of continuous-valued attribute in decision tree generation. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 87-102. </pages>
Reference: <author> Fayyad, U. M., & Irani, K. B. </author> <year> (1992b). </year> <title> The attribute selection problem in decision tree generation. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 104-110). </pages> <address> San Jose, CA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Fisher, R. A. </author> <year> (1936). </year> <title> Multiple measures in taxonomic problems. </title> <journal> Annals of Eugenics, </journal> <volume> 7, </volume> <pages> 179-188. </pages>
Reference-contexts: The examples were collected by B. German of the Home Office Forensic Science Service at Aldermaston, Reading, UK. Hepatitis: The task for this domain is to predict from test results whether a patient will live or die from hepatitis. Iris: Fisher's classic data set <ref> (Fisher, 1936) </ref>, contains three classes of 50 instances each. Each class is a type of iris plant. One class is linearly separable from the other two, but the latter two are not linearly separable from each other.
Reference: <author> Frean, M. </author> <year> (1990). </year> <title> Small nets and short paths: Optimising neural computation. </title> <type> Doctoral dissertation, </type> <institution> Center for Cognitive Science, University of Edinburgh. </institution>
Reference-contexts: The first method, Recursive Least Squares (RLS) (Young, 1984), minimizes the mean-squared error over the training data. The second method, the Pocket Algorithm (Gallant, 1986), maximizes the number of correct classifications on the training data. The third method, Thermal Training <ref> (Frean, 1990) </ref>, converges to a set of coefficients by paying decreasing attention to large errors. The fourth method, CART's coefficient learning method (Breiman, et al. 1984), explicitly searches for a set of coefficients that minimizes the impurity of the partition created by the multivariate test.
Reference: <author> Gallant, S. I. </author> <year> (1986). </year> <title> Optimal linear discriminants. </title> <booktitle> Proceedings of the International Conference on Pattern Recognition (pp. </booktitle> <pages> 849-852). </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The first method, Recursive Least Squares (RLS) (Young, 1984), minimizes the mean-squared error over the training data. The second method, the Pocket Algorithm <ref> (Gallant, 1986) </ref>, maximizes the number of correct classifications on the training data. The third method, Thermal Training (Frean, 1990), converges to a set of coefficients by paying decreasing attention to large errors.
Reference: <author> Hampson, S. E., & Volper, D. J. </author> <year> (1986). </year> <title> Linear function neurons: Structure and training. </title> <journal> Biological Cybernetics, </journal> <volume> 53, </volume> <pages> 203-217. </pages>
Reference-contexts: If the feature has more than two observed values, then each feature-value pair can be mapped to a propositional feature, which is TRUE if and only if the feature has the particular value in the instance <ref> (Hampson & Volper, 1986) </ref>. This avoids imposing any order on the unordered values of the feature. With this encoding, one can create linear combinations of both ordered and unordered features. 2.2 Filling in missing values For some instances, not all feature values may be available.
Reference: <author> Kittler, J. </author> <year> (1986). </year> <title> Feature selection and extraction. </title> <editor> In Young & Fu (Eds.), </editor> <booktitle> Handbook of pattern recognition and image processing. </booktitle> <address> New York: </address> <publisher> Academic Press. 35 Matheus, </publisher> <editor> C. J. </editor> <year> (1990). </year> <title> Feature construction: An analytic framework and an appli-cation to decision trees. </title> <type> Doctoral dissertation, </type> <institution> Department of Computer Science, University of Illinois, Urbana-Champaign, IL. </institution>
Reference-contexts: 4.1 Sequential Backward Elimination A Sequential Backward Elimination search is a top down search method that starts with all of the features and tries to remove the feature that will cause the smallest decrease of some merit criterion function that reflects the amount of classification information conveyed by the feature <ref> (Kittler, 1986) </ref>. Each feature of a test either contributes to, makes no difference to, or hinders the quality of the test. An SBE search iteratively removes the feature that contributes least to the quality of the test. It continues eliminating features until a specified stopping criterion is met. <p> To this end, GSBE computes, for each remaining feature, the average squared distance between the weights of the linear discriminant functions for each pair of classes and then eliminates the feature that has the smallest dispersion. This measure is analogous to the Euclidean interclass distance measure for estimating error <ref> (Kittler, 1986) </ref>. This elimination procedure is used in the LMDT algorithm (Brodley & Utgoff, 1992). 4.4 Heuristic Sequential Search The Heuristic Sequential Search (HSS) algorithm is a combination of the SFS algorithm and the SBE algorithm.
Reference: <author> Mingers, J. </author> <year> (1989a). </year> <title> An empirical comparison of selection measures for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 319-342. </pages>
Reference: <author> Mingers, J. </author> <year> (1989b). </year> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4, </volume> <pages> 227-243. </pages>
Reference: <author> Nilsson, N. J. </author> <year> (1965). </year> <title> Learning machines. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: A linear machine (LM) is a set of R linear discriminant functions that are used collectively to assign an instance to one of the R classes <ref> (Nilsson, 1965) </ref>. Let Y be an instance description (a pattern vector) consisting of a constant 1 and the n features that describe the instance. Then each discriminant function g i (Y) has the form W T i Y, where W i is a vector of n + 1 coefficients.
Reference: <author> Pagallo, G., & Haussler, D. </author> <year> (1990). </year> <title> Boolean feature discovery in empirical learning. </title> <booktitle> Machine Learning, </booktitle> <pages> 71-99. </pages>
Reference-contexts: This limitation reduces the ability to express concepts succinctly, which renders many classes of concepts difficult or impossible to express. This representational limitation manifests itself in two forms: features are tested in one or more subtrees of the decision tree <ref> (Pagallo & Haussler, 1990) </ref> and features are tested more than once along a path in the decision tree.
Reference: <author> Pagallo, G. M. </author> <year> (1990). </year> <title> Adaptive decision tree algorithms for learning from examples. </title> <type> Doctoral dissertation, </type> <institution> University of California at Santa Cruz. </institution>
Reference-contexts: This limitation reduces the ability to express concepts succinctly, which renders many classes of concepts difficult or impossible to express. This representational limitation manifests itself in two forms: features are tested in one or more subtrees of the decision tree <ref> (Pagallo & Haussler, 1990) </ref> and features are tested more than once along a path in the decision tree.
Reference: <author> Quinlan, J. R. </author> <year> (1986a). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: For example, a merit criterion function may measure the accuracy of the test when applied to the training data, or measure the entropy, as with the Gini (Breiman, et al. 1984) or information-gain ratio <ref> (Quinlan, 1986a) </ref> criteria. The stopping criterion determines when to stop eliminating features from the linear combination test. <p> In each of the feature selection algorithms we used the information-gain ratio merit criterion <ref> (Quinlan, 1986a) </ref> and the discrimination and underfitting criteria described in Section 2.5. In addition, because one of the feature selection algorithms, GSBE, requires that the input features be normalized, we normalize the instances at each node and retain the normalization information for testing. <p> We use Thermal Training procedure for multiclass data sets because, as discussed at the end of Section 6.2, RLS is restricted to binary partitions of the data. Each algorithm uses the gain-ratio merit criterion <ref> (Quinlan, 1986a) </ref>, the discrimination and underfitting criteria, and reduced error pruning. The algorithms differ only in the feature selection procedure used. This section seeks to answer the following questions: 1. Is SBE better than SFS because of starting from an informed position? 2.
Reference: <author> Quinlan, J. R. </author> <year> (1986b). </year> <title> The effect of noise on concept learning. </title> <editor> In Michalski, Car-bonell & Mitchell (Eds.), </editor> <booktitle> Machine learning: An artificial intelligence approach. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A noisy instance is one for which either the class label is incorrect, some number of the attribute values are incorrect or a combination of the two. Noise can be caused by many different factors which include faulty measurements, ill-defined thresholds and subjective interpretation <ref> (Quinlan, 1986b) </ref>. Overfitting occurs when the training data contain noisy instances and the decision tree algorithm induces a classifier that 3 classifies all instances in the training set correctly. Such a tree will usually perform poorly for previously unseen instances.
Reference: <author> Quinlan, J. R. </author> <year> (1987). </year> <title> Simplifying decision trees. </title> <journal> Internation Journal of Man-machine Studies, </journal> <volume> 27, </volume> <pages> 221-234. </pages>
Reference-contexts: In addition, because one of the feature selection algorithms, GSBE, requires that the input features be normalized, we normalize the instances at each node and retain the normalization information for testing. To prune the trees we use the reduced error pruning algorithm <ref> (Quinlan, 1987) </ref>, which uses a set of instances, the prune set, that is independent of the training instances to estimate the error of a decision tree. <p> In this experiment the best linear combination test found by a multivariate test procedure was added to the set of possible univariate tests, and the best from this new set was then chosen. We include two univariate decision tree algorithms, univariate CART and C4.5 <ref> (Quinlan, 1987) </ref> to assess the effect of adding multivariate tests on 25 accuracy, tree size and training time. We ran four versions of the CART program: CART with only univariate tests and three versions of CART with linear combinations added.
Reference: <author> Quinlan, J. R. </author> <year> (1989). </year> <title> Unknown attribute values in induction. </title> <booktitle> Proceedings of the Sixth International Workshop on Machine Learning (pp. </booktitle> <pages> 164-168). </pages> <address> Ithaca, NY: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Safavian, S. R., & Langrebe, D. </author> <year> (1991). </year> <title> A survey of decision tree classifier methodology. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 21, </volume> <pages> 660-674. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> NADALINE: A normalized adaptive linear element that learns efficiently, </title> <institution> (GTE TR88-509.4), GTE Laboratories Incorporated. </institution>
Reference-contexts: At each node in the tree, each encoded symbolic and numeric feature is normalized by mapping it to standard normal form, i.e., zero mean and unit standard deviation <ref> (Sutton, 1988) </ref>. After normalization, missing values can be filled in with the sample mean, which is equal to zero.
Reference: <author> Sutton, R. S., & Matheus, C. J. </author> <year> (1991). </year> <title> Learning polynomial functions by feature construction. </title> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop (pp. </booktitle> <pages> 208-212). </pages> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Utgoff, P. E., & Brodley, C. E. </author> <year> (1990). </year> <title> An incremental method for finding multivariate splits for decision trees. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 58-65). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The Pocket Algorithm was used in PT2, an incremental multivariate decision tree algorithm <ref> (Utgoff & Brodley, 1990) </ref>. 8 * o o o o x x x o 3.3 The Thermal Training Procedure The thermal training procedure can be applied to a linear threshold unit or a linear machine.
Reference: <author> Utgoff, P. E., & Brodley, C. E. </author> <year> (1991). </year> <title> Linear machine decision trees, </title> <type> (COINS Technical Report 91-10), </type> <institution> Amherst, MA: University of Massachusetts, Department of Computer and Information Science. </institution>
Reference-contexts: The second kind of problematic error occurs when a misclassified instance lies very close to the decision boundary, as shown to the right of the boundary in Figure 2. To ensure that the weights converge, one needs to reduce the amount of all corrections. Utgoff and Brodley <ref> (Utgoff & Brodley, 1991) </ref> extended these ideas to a linear machine, yielding a thermal linear machine.
Reference: <author> Young, P. </author> <year> (1984). </year> <title> Recursive estimation and time-series analysis. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher> <pages> 36 </pages>
Reference-contexts: The first method, Recursive Least Squares (RLS) <ref> (Young, 1984) </ref>, minimizes the mean-squared error over the training data. The second method, the Pocket Algorithm (Gallant, 1986), maximizes the number of correct classifications on the training data. The third method, Thermal Training (Frean, 1990), converges to a set of coefficients by paying decreasing attention to large errors.
References-found: 28

