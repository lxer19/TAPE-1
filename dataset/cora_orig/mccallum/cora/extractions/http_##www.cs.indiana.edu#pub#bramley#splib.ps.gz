URL: http://www.cs.indiana.edu/pub/bramley/splib.ps.gz
Refering-URL: http://www.cs.indiana.edu/pub/bramley/
Root-URL: http://www.cs.indiana.edu
Title: SPLIB: A LIBRARY OF ITERATIVE METHODS FOR SPARSE LINEAR SYSTEMS  
Author: RANDALL BRAMLEY AND XIAOGE WANG 
Date: May 22, 1997  
Address: BLOOMINGTON  
Affiliation: DEPARTMENT OF COMPUTER SCIENCE INDIANA UNIVERSITY  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> E. Anderson, </author> <title> Parallel implementation of preconditioned conjugate gradient methods for solving sparse systems of linear equations, </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <year> 1988. </year>
Reference-contexts: Also, it is a read indirection, which does not stall pipelin-ing as badly as write indirection. CSR requires four memory reads and one write per two flops. Tools are available in Yousef Saad's Sparskit [20] for converting from other sparse matrix data structures to and from CSR. Research <ref> [1, 19, 15] </ref> has shown that a jagged diagonal data structure is often more efficient for matrix-vector products, especially for vector or heavily pipelined machines. However, jagged diagonals is generally difficult to implement correctly for people who do not customarily work with sparse matrix data structures.
Reference: [2] <author> E. Anderson et al., </author> <title> LAPACK Users' Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1992. </year>
Reference-contexts: Floating point traps: SPLIB computes quantities such as machine epsilon and safe invertible minimum by calls to the LAPACK <ref> [2] </ref> routines dlamch. If you have set floating point traps, SPLIB is almost certain to trigger them. This can be handled by explicitly setting the parameters SMALL and pdr in the codes. 9.
Reference: [3] <author> M. Arioli, I. Duff, J. Noailles, and D. Ruiz, </author> <title> A block projection method for general sparse matrices, </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 13 (1992), </volume> <pages> pp. 47-70. </pages>
Reference-contexts: Another norm is important for understanding the convergence of iterative methods, one proposed by Oetli and Prager in [18] and used by Arioli et al in <ref> [3] </ref>.
Reference: [4] <author> O. Axelsson, </author> <title> Iterative Solution Methods, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, 1 ed., </address> <year> 1994. </year>
Reference-contexts: Common blocks are avoided except internally, and the code is heavily instrumented to provide valuable information about the convergence history, memory usage, and CPU time used. This document is intended primarily to describe SPLIB. For information about the methods implemented, see the references or general sources such as <ref> [21, 4, 6] </ref>. Note the date of this document; it supersedes earlier versions. SPLIB has a new interface, which avoids Fortran logical variables as arguments and includes provisions for scaling the linear system. 2. Comparison With Other Packages.
Reference: [5] <author> O. Axelsson and G. Lindskog, </author> <title> On the eigenvalue distribution of a class of preconditioning methods, </title> <journal> Numer. Math., </journal> <volume> 48 (1986), </volume> <pages> pp. 479-498. </pages>
Reference-contexts: For finite element problems, MILU (s,1) gives a preconditioner which is exact on constant functions. Although it can be shown to have better order of accuracy on model problems, on practical problems it generally gives a worse preconditioner; for an explanation of this phenemonen, see the results in <ref> [5] </ref>. ILUT is Yousef Saad's algorithm (and implementation) of thresholding as a dropping criterion. This allows dropping of elements during the incomplete factorization based on their relative size instead of their positions within the matrix.
Reference: [6] <author> R. Barrett et al., </author> <title> Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, </title> <publisher> SIAM, </publisher> <address> Philadelphia, 1 ed., </address> <year> 1994. </year> <title> [7] , Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods, </title> <publisher> SIAM, </publisher> <address> Philadelphia, 1 ed., </address> <year> 1994. </year>
Reference-contexts: Common blocks are avoided except internally, and the code is heavily instrumented to provide valuable information about the convergence history, memory usage, and CPU time used. This document is intended primarily to describe SPLIB. For information about the methods implemented, see the references or general sources such as <ref> [21, 4, 6] </ref>. Note the date of this document; it supersedes earlier versions. SPLIB has a new interface, which avoids Fortran logical variables as arguments and includes provisions for scaling the linear system. 2. Comparison With Other Packages. <p> (CGNE) [9]. * conjugate gradients for the normal equations A T Ax = A T b (CGNR) [16], * conjugate gradients squared (CGS) [23], * biconjugate gradients stabilized (CGSTAB) [10], * restarted generalized minimum residual (GMRES) [22], * Transpose-free QMR (QRMTF) [12], * Templates version of biconjugate gradients stabilized (BICGS) <ref> [6] </ref>, * Templates version of GMRES (GMREST) [6], * Jacobi iteration (JACOBI) [14], * Gauss-Seidel (GAUSS) [14], * Successive Over-relaxation (SOR) [14], 5 * Orthomin (m) (ORTHM) [24]. Note that two methods are repeated (GMRES (k) and biconjugage gradients stabilized), with the second versions modified from Templates [6]. <p> normal equations A T Ax = A T b (CGNR) [16], * conjugate gradients squared (CGS) [23], * biconjugate gradients stabilized (CGSTAB) [10], * restarted generalized minimum residual (GMRES) [22], * Transpose-free QMR (QRMTF) [12], * Templates version of biconjugate gradients stabilized (BICGS) <ref> [6] </ref>, * Templates version of GMRES (GMREST) [6], * Jacobi iteration (JACOBI) [14], * Gauss-Seidel (GAUSS) [14], * Successive Over-relaxation (SOR) [14], 5 * Orthomin (m) (ORTHM) [24]. Note that two methods are repeated (GMRES (k) and biconjugage gradients stabilized), with the second versions modified from Templates [6]. <p> gradients stabilized (BICGS) <ref> [6] </ref>, * Templates version of GMRES (GMREST) [6], * Jacobi iteration (JACOBI) [14], * Gauss-Seidel (GAUSS) [14], * Successive Over-relaxation (SOR) [14], 5 * Orthomin (m) (ORTHM) [24]. Note that two methods are repeated (GMRES (k) and biconjugage gradients stabilized), with the second versions modified from Templates [6]. The Templates GMRES uses left preconditioning, solving M 1 Ax = M 1 b, while the native SPLIB version uses right preconditioning, solving AM 1 y = b, x = M 1 y.
Reference: [8] <author> R. Bramley, X. Wang, and D. Pelletier, </author> <title> Orthogonalization based iterative methods for generalized Stokes problems, in Solution Techniques for Large-Scale CFD Problems, </title> <editor> W. G. Habashi, ed., </editor> <publisher> Centre de Recherce en Calcul Applique, </publisher> <year> 1994. </year>
Reference-contexts: and relaxation param eter ! (MILU (s,!)) [13], * incomplete LU factorization with threshold dropping (ILUT (s,*)) [20], * symmetric successive overrelaxation (SSOR (!)) [14], * block diagonal with tridiagonal blocks TRID (s), * incomplete LU factorization without modification of off-diagonal elements (ILU0), * extended compressed incomplete modified Gram-Schmidt (ECIMGS) <ref> [8] </ref>. All of the preconditioning methods are used as left preconditioners by the solvers, except for GMRES which uses right preconditioning (as noted above, GMREST is the same algorithm but with left preconditioning). <p> Note that the usual way of writing M is and so the scaling factor of 1=(2 !) has been omitted and the last two factors have been combined. The second form shows that when A is symmetric, so is the SSOR preconditioner. The ECIMGS preconditioner <ref> [8] </ref> is unique to SPLIB, and is derived from incomplete orthogonalization preconditioners [26, 25]. The parameter s provided to ECIMGS is a floating point number, which specifies the sparsity pattern to target.
Reference: [9] <author> E. J. Craig, </author> <title> The N-step iterations procedures, </title> <journal> J. Math. Physics, </journal> <volume> 34 (1955), </volume> <pages> pp. 64-73. </pages>
Reference-contexts: All of the preconditioners are parameterized, providing a rich variety of preconditioning strategies. The iterative methods currently are * bi-conjugate gradients (BICG) [11], * conjugate gradients for the normal equations AA T y = b; x = A T y (CGNE) <ref> [9] </ref>. * conjugate gradients for the normal equations A T Ax = A T b (CGNR) [16], * conjugate gradients squared (CGS) [23], * biconjugate gradients stabilized (CGSTAB) [10], * restarted generalized minimum residual (GMRES) [22], * Transpose-free QMR (QRMTF) [12], * Templates version of biconjugate gradients stabilized (BICGS) [6], *
Reference: [10] <author> H. V. der Vorst, </author> <title> Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of nonsymmetric linear systems, </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 13 (1992), </volume> <pages> pp. 631-644. </pages>
Reference-contexts: bi-conjugate gradients (BICG) [11], * conjugate gradients for the normal equations AA T y = b; x = A T y (CGNE) [9]. * conjugate gradients for the normal equations A T Ax = A T b (CGNR) [16], * conjugate gradients squared (CGS) [23], * biconjugate gradients stabilized (CGSTAB) <ref> [10] </ref>, * restarted generalized minimum residual (GMRES) [22], * Transpose-free QMR (QRMTF) [12], * Templates version of biconjugate gradients stabilized (BICGS) [6], * Templates version of GMRES (GMREST) [6], * Jacobi iteration (JACOBI) [14], * Gauss-Seidel (GAUSS) [14], * Successive Over-relaxation (SOR) [14], 5 * Orthomin (m) (ORTHM) [24].
Reference: [11] <author> R. Fletcher, </author> <title> Conjugate gradient methods for indefinite systems, </title> <booktitle> in Lecture Notes in Mathematics, </booktitle> <volume> No. 506, </volume> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, </address> <year> 1976, </year> <pages> pp. 73-89. </pages>
Reference-contexts: The preconditioners include ones that allow multiple levels of fill-in through positional and numerical strategies. All of the preconditioners are parameterized, providing a rich variety of preconditioning strategies. The iterative methods currently are * bi-conjugate gradients (BICG) <ref> [11] </ref>, * conjugate gradients for the normal equations AA T y = b; x = A T y (CGNE) [9]. * conjugate gradients for the normal equations A T Ax = A T b (CGNR) [16], * conjugate gradients squared (CGS) [23], * biconjugate gradients stabilized (CGSTAB) [10], * restarted generalized
Reference: [12] <author> R. Freund, </author> <title> A transpose-free quasi-minimum residual algorithm for non-Hermitian linear systems, </title> <journal> SIAM J. Sci. Stat. Comp.., </journal> <volume> 14 (1993), </volume> <pages> pp. 470-482. </pages>
Reference-contexts: T y = b; x = A T y (CGNE) [9]. * conjugate gradients for the normal equations A T Ax = A T b (CGNR) [16], * conjugate gradients squared (CGS) [23], * biconjugate gradients stabilized (CGSTAB) [10], * restarted generalized minimum residual (GMRES) [22], * Transpose-free QMR (QRMTF) <ref> [12] </ref>, * Templates version of biconjugate gradients stabilized (BICGS) [6], * Templates version of GMRES (GMREST) [6], * Jacobi iteration (JACOBI) [14], * Gauss-Seidel (GAUSS) [14], * Successive Over-relaxation (SOR) [14], 5 * Orthomin (m) (ORTHM) [24].
Reference: [13] <author> I. Gustafsson, </author> <title> Modified incomplete Cholesky methods, in Preconditioning Methods: Theory and Applications, </title> <editor> D. Evans, ed., </editor> <address> New York, 1983, </address> <publisher> Gordon and Breach, </publisher> <pages> pp. 265-293. </pages>
Reference-contexts: more accurate computations, but does not satisfy the IEEE standard for floating point arithmetic The preconditioners currently implemented in SPLIB are * incomplete LU factorization with s levels of fill (ILU (s)) [17], * modified incomplete LU factorization with s levels of fill and relaxation param eter ! (MILU (s,!)) <ref> [13] </ref>, * incomplete LU factorization with threshold dropping (ILUT (s,*)) [20], * symmetric successive overrelaxation (SSOR (!)) [14], * block diagonal with tridiagonal blocks TRID (s), * incomplete LU factorization without modification of off-diagonal elements (ILU0), * extended compressed incomplete modified Gram-Schmidt (ECIMGS) [8]. <p> ILU0 is distinct from ILU (s) with s = 0 in that its factorization process only modifies the diagonal entries of the matrix being factored. For PDE problems arising from quasi-uniform meshes, the two are equivalent, but for irregular meshes they differ (see <ref> [13] </ref>, where the two different algorithms are distinguished by using an asterisk). ILU0 requires only one additional vector of storage. SSOR (!) is a preconditioner based on a matrix splitting, and so requires no additional floating point storage.
Reference: [14] <author> L. Hageman and D. Young, </author> <title> Applied Iterative Methods, </title> <publisher> Academic Press, </publisher> <address> New York, 1 ed., </address> <year> 1981. </year>
Reference-contexts: = A T b (CGNR) [16], * conjugate gradients squared (CGS) [23], * biconjugate gradients stabilized (CGSTAB) [10], * restarted generalized minimum residual (GMRES) [22], * Transpose-free QMR (QRMTF) [12], * Templates version of biconjugate gradients stabilized (BICGS) [6], * Templates version of GMRES (GMREST) [6], * Jacobi iteration (JACOBI) <ref> [14] </ref>, * Gauss-Seidel (GAUSS) [14], * Successive Over-relaxation (SOR) [14], 5 * Orthomin (m) (ORTHM) [24]. Note that two methods are repeated (GMRES (k) and biconjugage gradients stabilized), with the second versions modified from Templates [6]. <p> (CGNR) [16], * conjugate gradients squared (CGS) [23], * biconjugate gradients stabilized (CGSTAB) [10], * restarted generalized minimum residual (GMRES) [22], * Transpose-free QMR (QRMTF) [12], * Templates version of biconjugate gradients stabilized (BICGS) [6], * Templates version of GMRES (GMREST) [6], * Jacobi iteration (JACOBI) <ref> [14] </ref>, * Gauss-Seidel (GAUSS) [14], * Successive Over-relaxation (SOR) [14], 5 * Orthomin (m) (ORTHM) [24]. Note that two methods are repeated (GMRES (k) and biconjugage gradients stabilized), with the second versions modified from Templates [6]. <p> squared (CGS) [23], * biconjugate gradients stabilized (CGSTAB) [10], * restarted generalized minimum residual (GMRES) [22], * Transpose-free QMR (QRMTF) [12], * Templates version of biconjugate gradients stabilized (BICGS) [6], * Templates version of GMRES (GMREST) [6], * Jacobi iteration (JACOBI) <ref> [14] </ref>, * Gauss-Seidel (GAUSS) [14], * Successive Over-relaxation (SOR) [14], 5 * Orthomin (m) (ORTHM) [24]. Note that two methods are repeated (GMRES (k) and biconjugage gradients stabilized), with the second versions modified from Templates [6]. <p> implemented in SPLIB are * incomplete LU factorization with s levels of fill (ILU (s)) [17], * modified incomplete LU factorization with s levels of fill and relaxation param eter ! (MILU (s,!)) [13], * incomplete LU factorization with threshold dropping (ILUT (s,*)) [20], * symmetric successive overrelaxation (SSOR (!)) <ref> [14] </ref>, * block diagonal with tridiagonal blocks TRID (s), * incomplete LU factorization without modification of off-diagonal elements (ILU0), * extended compressed incomplete modified Gram-Schmidt (ECIMGS) [8].
Reference: [15] <author> M. Heroux, P. Vu, and C. Yang, </author> <title> A parallel preconditioned conjugate gradient package for solving sparse linear systems on a Cray Y-MP, </title> <journal> Applied Numerical Mathematics, </journal> <volume> 8 (1991), </volume> <pages> pp. 93-115. </pages>
Reference-contexts: Also, it is a read indirection, which does not stall pipelin-ing as badly as write indirection. CSR requires four memory reads and one write per two flops. Tools are available in Yousef Saad's Sparskit [20] for converting from other sparse matrix data structures to and from CSR. Research <ref> [1, 19, 15] </ref> has shown that a jagged diagonal data structure is often more efficient for matrix-vector products, especially for vector or heavily pipelined machines. However, jagged diagonals is generally difficult to implement correctly for people who do not customarily work with sparse matrix data structures.
Reference: [16] <author> M. Hestenes and E. </author> <title> Stiefel, Methods of conjugate gradients for solving linear systems, </title> <journal> Journal of Research National Bureau of Standards, </journal> <volume> 49 (1952), </volume> <pages> pp. 409-436. </pages>
Reference-contexts: The iterative methods currently are * bi-conjugate gradients (BICG) [11], * conjugate gradients for the normal equations AA T y = b; x = A T y (CGNE) [9]. * conjugate gradients for the normal equations A T Ax = A T b (CGNR) <ref> [16] </ref>, * conjugate gradients squared (CGS) [23], * biconjugate gradients stabilized (CGSTAB) [10], * restarted generalized minimum residual (GMRES) [22], * Transpose-free QMR (QRMTF) [12], * Templates version of biconjugate gradients stabilized (BICGS) [6], * Templates version of GMRES (GMREST) [6], * Jacobi iteration (JACOBI) [14], * Gauss-Seidel (GAUSS) [14], *
Reference: [17] <author> J. Meijerink and H. van der Vorst, </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix, </title> <journal> Math. Comp., </journal> <volume> 31 (1977), </volume> <pages> pp. 148-162. </pages>
Reference-contexts: This provides more accurate computations, but does not satisfy the IEEE standard for floating point arithmetic The preconditioners currently implemented in SPLIB are * incomplete LU factorization with s levels of fill (ILU (s)) <ref> [17] </ref>, * modified incomplete LU factorization with s levels of fill and relaxation param eter ! (MILU (s,!)) [13], * incomplete LU factorization with threshold dropping (ILUT (s,*)) [20], * symmetric successive overrelaxation (SSOR (!)) [14], * block diagonal with tridiagonal blocks TRID (s), * incomplete LU factorization without modification of
Reference: [18] <author> W. Oetli and W. Prager, </author> <title> Compatibility of approximate solution of linear equations with given error bounds for coefficients and right hand sides, </title> <journal> Numer. Math, </journal> <volume> 6 (1964), </volume> <pages> pp. 405-409. </pages>
Reference-contexts: If instlvl is greater than or equal to 2, other files showing the preconditioned residual versus iteration number and elapsed time are also produced. Another norm is important for understanding the convergence of iterative methods, one proposed by Oetli and Prager in <ref> [18] </ref> and used by Arioli et al in [3].
Reference: [19] <author> G. V. Paolini and G. di Brozolo, </author> <title> Data structures to vectorize CG algorithms for general sparsity patterns, </title> <journal> BIT, </journal> <volume> 29 (1989), </volume> <pages> pp. 703-718. </pages>
Reference-contexts: Also, it is a read indirection, which does not stall pipelin-ing as badly as write indirection. CSR requires four memory reads and one write per two flops. Tools are available in Yousef Saad's Sparskit [20] for converting from other sparse matrix data structures to and from CSR. Research <ref> [1, 19, 15] </ref> has shown that a jagged diagonal data structure is often more efficient for matrix-vector products, especially for vector or heavily pipelined machines. However, jagged diagonals is generally difficult to implement correctly for people who do not customarily work with sparse matrix data structures.
Reference: [20] <author> Y. Saad, SPARSKIT: </author> <title> a basic tool kit for sparse matrix computations, </title> <type> tech. rep., </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, Illinois, </institution> <year> 1990. </year> <title> [21] , Iterative methods for sparse linear systems, </title> <publisher> PWS publishing, </publisher> <address> New York, </address> <year> 1995. </year> <note> to appear. </note>
Reference-contexts: Furthermore, SPLIB has outstanding instrumentation and provides more clues about 2 its performance and nonconvergence cases. Also unlike Templates and the solvers provided with Sparskit <ref> [20] </ref>, this code uses a forward instead of a reverse communication model. <p> Also, it is a read indirection, which does not stall pipelin-ing as badly as write indirection. CSR requires four memory reads and one write per two flops. Tools are available in Yousef Saad's Sparskit <ref> [20] </ref> for converting from other sparse matrix data structures to and from CSR. Research [1, 19, 15] has shown that a jagged diagonal data structure is often more efficient for matrix-vector products, especially for vector or heavily pipelined machines. <p> for floating point arithmetic The preconditioners currently implemented in SPLIB are * incomplete LU factorization with s levels of fill (ILU (s)) [17], * modified incomplete LU factorization with s levels of fill and relaxation param eter ! (MILU (s,!)) [13], * incomplete LU factorization with threshold dropping (ILUT (s,*)) <ref> [20] </ref>, * symmetric successive overrelaxation (SSOR (!)) [14], * block diagonal with tridiagonal blocks TRID (s), * incomplete LU factorization without modification of off-diagonal elements (ILU0), * extended compressed incomplete modified Gram-Schmidt (ECIMGS) [8].
Reference: [22] <author> Y. Saad and M. Schultz, </author> <title> Gmres: A generalized minimal residual algorithm for solving nonsymmetric linear systems, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7 (1986), </volume> <pages> pp. 856-869. </pages>
Reference-contexts: for the normal equations AA T y = b; x = A T y (CGNE) [9]. * conjugate gradients for the normal equations A T Ax = A T b (CGNR) [16], * conjugate gradients squared (CGS) [23], * biconjugate gradients stabilized (CGSTAB) [10], * restarted generalized minimum residual (GMRES) <ref> [22] </ref>, * Transpose-free QMR (QRMTF) [12], * Templates version of biconjugate gradients stabilized (BICGS) [6], * Templates version of GMRES (GMREST) [6], * Jacobi iteration (JACOBI) [14], * Gauss-Seidel (GAUSS) [14], * Successive Over-relaxation (SOR) [14], 5 * Orthomin (m) (ORTHM) [24].
Reference: [23] <author> P. Sonneveld, </author> <title> Cgs, a fast Lanczos-type solver for nonsymmetric linear systems, </title> <journal> SIAM J. Sci. 25 Stat. Comp., </journal> <volume> 10 (1989), </volume> <pages> pp. 36-52. </pages>
Reference-contexts: The iterative methods currently are * bi-conjugate gradients (BICG) [11], * conjugate gradients for the normal equations AA T y = b; x = A T y (CGNE) [9]. * conjugate gradients for the normal equations A T Ax = A T b (CGNR) [16], * conjugate gradients squared (CGS) <ref> [23] </ref>, * biconjugate gradients stabilized (CGSTAB) [10], * restarted generalized minimum residual (GMRES) [22], * Transpose-free QMR (QRMTF) [12], * Templates version of biconjugate gradients stabilized (BICGS) [6], * Templates version of GMRES (GMREST) [6], * Jacobi iteration (JACOBI) [14], * Gauss-Seidel (GAUSS) [14], * Successive Over-relaxation (SOR) [14], 5 *
Reference: [24] <author> P. Vinsome, Orthomin, </author> <title> an iterative method for solving sparse sets of simultaneous linear equations, </title> <booktitle> Proc. Fourth Symp. on Reservoir Simulation, </booktitle> <year> (1976), </year> <pages> pp. 149-159. </pages>
Reference-contexts: stabilized (CGSTAB) [10], * restarted generalized minimum residual (GMRES) [22], * Transpose-free QMR (QRMTF) [12], * Templates version of biconjugate gradients stabilized (BICGS) [6], * Templates version of GMRES (GMREST) [6], * Jacobi iteration (JACOBI) [14], * Gauss-Seidel (GAUSS) [14], * Successive Over-relaxation (SOR) [14], 5 * Orthomin (m) (ORTHM) <ref> [24] </ref>. Note that two methods are repeated (GMRES (k) and biconjugage gradients stabilized), with the second versions modified from Templates [6].
Reference: [25] <author> X. Wang, </author> <title> Incomplete Factorization Preconditioning for Linear Least Squares Problems, </title> <type> PhD thesis, </type> <institution> University of Illinois Urbana-Champaign, </institution> <year> 1993. </year> <note> Also available as Tech. Rep. </note> <institution> UIUCDCS-R-93-1834, Computer Science Department, University of Illinois - Urbana. </institution>
Reference-contexts: The second form shows that when A is symmetric, so is the SSOR preconditioner. The ECIMGS preconditioner [8] is unique to SPLIB, and is derived from incomplete orthogonalization preconditioners <ref> [26, 25] </ref>. The parameter s provided to ECIMGS is a floating point number, which specifies the sparsity pattern to target.
Reference: [26] <author> X. Wang, K. A. Gallivan, and R. Bramley, CIMGS: </author> <title> A incomplete orthogonalization pre-conditioner, </title> <type> Tech. Rep. 393, </type> <institution> Indiana University-Bloomington, Bloomington, </institution> <note> IN 47405, 1994. Accepted for publication in SIAM J. Sci. Comp. 26 </note>
Reference-contexts: The second form shows that when A is symmetric, so is the SSOR preconditioner. The ECIMGS preconditioner [8] is unique to SPLIB, and is derived from incomplete orthogonalization preconditioners <ref> [26, 25] </ref>. The parameter s provided to ECIMGS is a floating point number, which specifies the sparsity pattern to target.
References-found: 24

