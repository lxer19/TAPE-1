URL: http://www-cse.ucsd.edu/users/gary/pubs/noelle_attractor_cogsci96.ps
Refering-URL: http://www.cse.ucsd.edu/users/gary/
Root-URL: 
Email: fdnoelle,garyg@cs.ucsd.edu  
Title: In Search Of Articulated Attractors  
Author: David C. Noelle and Garrison W. Cottrell 
Address: La Jolla, CA 92093-0114  
Affiliation: Computer Science Engineering 0114 University of California, San Diego  
Abstract: Recurrent attractor networks offer many advantages over feed-forward networks for the modeling of psychological phenomena. Their dynamic nature allows them to capture the time course of cognitive processing, and their learned weights may often be easily interpreted as soft constraints between representational components. Perhaps the most significant feature of such networks, however, is their ability to facilitate generalization by enforcing well formedness constraints on intermediate and output representations. Attractor networks which learn the systematic regularities of well formed representations by exposure to a small number of examples are said to possess articulated attractors. This paper investigates the conditions under which articulated attractors arise in recurrent networks trained using variants of backpropagation. The results of computational experiments demonstrate that such structured attrac-tors can spontaneously appear in an emergence of systematic-ity, if an appropriate error signal is presented directly to the recurrent processing elements. We show, however, that distal error signals, backpropagated through intervening weights, pose serious problems for networks of this kind. We present simulation results, discuss the reasons for this difficulty, and suggest some directions for future attempts to surmount it. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bridle, J. S. </author> <year> (1990). </year> <title> Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parameters. </title> <editor> In Touretzky, D. S. (Ed.), </editor> <booktitle> Advances in Neural Information Processing Systems 2 (pp. </booktitle> <pages> 211-217). </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: We will also consider encouraging articulated attractors by constraining the network architecture. In particular, we plan to investigate the possibility of initializing weights at the recurrent layer to a configuration which embodies a collection of winner-take-all networks. These will be implemented using a softmax constraint <ref> (Bridle, 1990) </ref>, so backpropagated error can still successfully reach weights feeding into the at tractor network. Also, restricted receptive fields among the hidden to output connections might be used to approximate an orthogonality constraint on this mapping.
Reference: <author> Elman, J. L. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14(2), </volume> <pages> 179-211. </pages>
Reference: <author> French, R. M. </author> <year> (1991). </year> <title> Using semi-distributed representations to overcome catastrophic forgetting in connectionist networks. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 173-178). </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Polarization is taken as a goal for the sake of the stability of attractor learning. Even with corner attractors, however, these networks still need to avoid hidden to output mappings which restrict generalization. A technique such as activation sharpening <ref> (French, 1991) </ref> could potentially produce the kinds of representations needed, but this would require an a priori specification of the number of hidden elements on for each pattern. Still, an inductive bias of this sort may be the best that is possible under an indirect error signal.
Reference: <author> Hopfield, J. J. </author> <year> (1982). </year> <title> Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> 79, </volume> <pages> 2554-2558. </pages>
Reference: <author> Mathis, D. W. and Mozer, M. C. </author> <year> (1995). </year> <title> On the computational utility of consciousness. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 7 (pp. </booktitle> <pages> 11-18). </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Noelle, D. C. and Cottrell, G. W. </author> <year> (1995). </year> <title> A connectionist model of instruction following. </title> <editor> In Moore, J. D. and Lehman, J. F. (Eds.), </editor> <booktitle> Proceedings of the Seventeenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 369-374). </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Plaut, D. C. and McClelland, J. L. </author> <year> (1993). </year> <title> Generalization with componential attractors: Word and nonword reading in an attractor network. </title> <booktitle> In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 824-829). </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986a). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., McClelland, J. L., and the PDP Research Group (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition (ch. 8). </booktitle> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Rumelhart, D. E., Smolensky, P., McClelland, J. L., and Hin-ton, G. E. </author> <year> (1986b). </year> <title> Schemata and sequential thought processes in PDP models. </title> <editor> In Rumelhart, D. E., McClelland, J. L., and the PDP Research Group (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition (ch. 14). </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> St. John, M. F. </author> <year> (1992). </year> <title> Leaning language in the service of a task. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 271-276). </pages> <address> Hillsdale, NJ: </address> <publisher> Lawrence Erlbaum Associates. </publisher>
References-found: 10

