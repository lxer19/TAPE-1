URL: ftp://ftp.idsia.ch/pub/techrep/IDSIA-19-96.ps.gz
Refering-URL: http://www.idsia.ch/techrep.html
Root-URL: http://www.idsia.ch/techrep.html
Email: juergen@idsia.ch  hochreit@informatik.tu-muenchen.de  
Title: GUESSING CAN OUTPERFORM MANY LONG TIME LAG ALGORITHMS  
Author: Jurgen Schmidhuber Sepp Hochreiter 
Date: May 6, 1996  
Web: http://www.idsia.ch/~juergen  http://www7.informatik.tu-muenchen.de/~hochreit  
Address: Corso Elvezia 36 6900 Lugano, Switzerland  80290 Munchen, Germany  
Affiliation: IDSIA  Fakultat fur Informatik Technische Universitat Munchen  
Pubnum: Technical Note IDSIA-19-96  
Abstract: Numerous recent papers focus on standard recurrent nets' problems with long time lags between relevant signals. Some propose rather sophisticated, alternative methods. We show: many problems used to test previous methods can be solved more quickly by random weight guessing. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bengio, Y. and Frasconi, P. </author> <year> (1994). </year> <title> Credit assignment through time: Alternatives to backpropagation. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 75-82. </pages> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Bengio, Y. and Frasconi, P. </author> <year> (1995). </year> <title> An input output HMM architecture. </title> <editor> In Tesauro, G., Touretzky, D. S., and Leen, T. K., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 427-434. </pages> <publisher> MIT Press, </publisher> <address> Cambridge MA. </address>
Reference: <author> Bengio, Y., Simard, P., and Frasconi, P. </author> <year> (1994). </year> <title> Learning long-term dependencies with gradient descent is difficult. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2) </volume> <pages> 157-166. </pages>
Reference: <author> El Hihi, S. and Bengio, Y. </author> <year> (1995). </year> <title> Hierarchical recurrent neural networks for long-term dependencies. In Advances in Neural Information Processing Systems 8. </title> <note> To appear. </note>
Reference: <author> Hochreiter, J. </author> <year> (1991). </year> <title> Untersuchungen zu dynamischen neuronalen Netzen. </title> <type> Diploma thesis, </type> <institution> Institut fur Informatik, Lehrstuhl Prof. Brauer, Technische Universitat Munchen. </institution>
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1995). </year> <title> Long short term memory. </title> <type> Technical Report FKI-207-95, </type> <institution> Fakultat fur Informatik, Technische Universitat Munchen. </institution> <note> Revised version submitted to Neural Computation, </note> <year> 1996. </year>
Reference-contexts: There are many tasks that require either many free parameters (e.g., input weights) or high weight precision, such that random search becomes completely infeasible (e.g., Schmidhuber's task, 1992). For such problems, we recommend to try a novel method called "Long Short Term Memory", or LSTM for short <ref> (Hochreiter and Schmidhuber, 1995) </ref>. LSTM does not suffer from the above-mentioned problems of other gradient-based approaches. It can solve non-trivial, complex long time lag problems involving distributed, high-precision, continuous-valued representations. Acknowledgments This work was supported by DFG grant SCHM 942/3-1 from "Deutsche Forschungsgemeinschaft".
Reference: <author> Hochreiter, S. and Schmidhuber, J. </author> <year> (1996). </year> <title> Flat minima. Neural Computation. </title> <publisher> In press. </publisher>
Reference-contexts: Flat minima. It should be mentioned that successful guessing typically hits flat minima of the error function <ref> (Hochreiter and Schmidhuber, 1996) </ref>. Feedforward nets. It should also be mentioned that solutions to many well-known, simple, nontemporal tasks such as XOR can be guessed within less than 100 trials on numerous standard 2 feedforward architectures. Limitations of guessing.
Reference: <author> Lin, T., Horne, B. G., Tino, P., and Giles, C. L. </author> <year> (1995). </year> <title> Learning long-term dependencies is not as difficult with NARX recurrent neural networks. </title> <institution> Technical Report UMIACS-TR-95-78 and CS-TR-3500, Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742. </institution>
Reference: <author> Manolios, P. and Fanelli, R. </author> <year> (1994). </year> <title> First-order recurrent neural networks and deterministic finite state automata. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 1155-1173. </pages>
Reference: <author> Miller, C. B. and Giles, C. L. </author> <year> (1993). </year> <title> Experimental comparison of the effect of order in recurrent neural networks. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 7(4) </volume> <pages> 849-872. </pages>
Reference: <author> Pearlmutter, B. A. </author> <year> (1989). </year> <title> Learning state space trajectories in recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 263-269. </pages>
Reference: <author> Pollack, J. B. </author> <year> (1991). </year> <title> The induction of dynamical recognizers. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 227-252. </pages>
Reference: <author> Schmidhuber, J. H. </author> <year> (1992). </year> <title> Learning complex, extended sequences using the principle of history compression. </title> <journal> Neural Computation, </journal> <volume> 4(2) </volume> <pages> 234-242. </pages>
Reference: <author> Schmidhuber, J. H. </author> <year> (1995). </year> <title> Discovering solutions with low Kolmogorov complexity and high generalization capability. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 488-496. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: There are many tasks that require either many free parameters (e.g., input weights) or high weight precision, such that random search becomes completely infeasible (e.g., Schmidhuber's task, 1992). For such problems, we recommend to try a novel method called "Long Short Term Memory", or LSTM for short <ref> (Hochreiter and Schmidhuber, 1995) </ref>. LSTM does not suffer from the above-mentioned problems of other gradient-based approaches. It can solve non-trivial, complex long time lag problems involving distributed, high-precision, continuous-valued representations. Acknowledgments This work was supported by DFG grant SCHM 942/3-1 from "Deutsche Forschungsgemeinschaft".
Reference: <author> Tomita, M. </author> <year> (1982). </year> <title> Dynamic construction of finite automata from examples using hill-climbing. </title> <booktitle> In Proceedings of the Fourth Annual Cognitive Science Conference, </booktitle> <pages> pages 105-108. </pages> <address> Ann Arbor, MI. </address>
Reference: <author> Watrous, R. L. and Kuhn, G. M. </author> <year> (1992). </year> <title> Induction of finite-state languages using second-order recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 406-414. </pages>
Reference: <author> Williams, R. J. </author> <year> (1989). </year> <title> Complexity of exact gradient computation algorithms for recurrent neural networks. </title> <type> Technical Report Technical Report NU-CCS-89-27, </type> <institution> Boston: Northeastern University, College of Computer Science. </institution> <month> 3 </month>
References-found: 17

