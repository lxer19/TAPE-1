URL: http://www.ai.mit.edu/people/jpmellor/cvrmed_bw.ps.gz
Refering-URL: http://www.ai.mit.edu/people/jpmellor/erv.html
Root-URL: 
Email: email: jpmellor@ai.mit.edu  
Title: Realtime Camera Calibration for Enhanced Reality Visualization  
Author: J.P. Mellor 
Address: Cambridge, MA 02139 USA  
Affiliation: Massachusetts Institute of Technology,  
Abstract: The problem which must be solved to make realtime enhanced reality visualization possible is basically the camera calibration problem. The relationship between the coordinate frames of the patient, the patient's internal anatomy scans and the image plane of the camera observing the patient must be established. This paper presents a new approach to finding this relationship and develops a system for performing enhanced reality visualization. Given the locations of a few fiducials our method is fully automatic, runs in nearly real-time, is accurate to a fraction of a pixel, allows both patient and camera motion, automatically corrects for changes to the internal camera parameters (focal length, focus, aperture, etc.) and requires only a single video image.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Duane C. Brown. </author> <title> Decentering distortion of lenses. </title> <journal> Photogrammetric Engineering, </journal> <volume> 32(3) </volume> <pages> 444-462, </pages> <year> 1965. </year>
Reference-contexts: The effective focal length f also varies with focus and aperture settings. Zoom lenses take this variability to an extreme enabling large changes to f . Lens distortion also varies with changes to focus and aperture <ref> [1] </ref>. In enhanced reality visualization, we are interested in the total transformation from model to image coordinates. We do not need to separate intrinsic and extrinsic parameters to generate an enhanced reality image.
Reference: 2. <author> W.E.L. Grimson, T. Lozano-Perez, G.J. Ettinger W.M. Wells III, S.J. White, and R. Kikinis. </author> <title> An automatic registration method for frameless stereotaxy, image guided surgery, and enhanced reality visualization. </title> <booktitle> In Computer Vision and Pattern Recognition, </booktitle> <pages> pages 430-436. </pages> <publisher> IEEE, </publisher> <address> June 1994. Seattle, WA. </address>
Reference-contexts: The degree to which the wire frame matches the edges in the image is a measure of the accuracy of our method. The enhancement is accurate well outside the volume enclosed by the fiducials. locations of the fiducials are determined using a laser scanner <ref> [2] </ref>. A discussion of this initial calibration can be found in [3]. Given the locations of the fiducials, a model of the skull obtained from CT imaging (shown as black dots) and a video image taken from ~ 1m away with a 25mm lens, the enhanced reality visualizations are generated.
Reference: 3. <author> J.P. Mellor. </author> <title> Enhanced reality visualization in a surgical environment. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: In contrast, our approach uses video information exclusively and does not require a calibrated camera making it particularly well suited for enhanced reality visualization. For a summary of current enhanced reality visualization techniques see <ref> [3] </ref>. ? This work was supported in part by ARPA under Rome Laboratory contract F3060 94-C-0204 and ONR contract N00014-91-J-4038. 2 Our Method 2.1 A Perspective Transformation is Enough The camera calibration problem is typically posed as follows: I = M R Where I is a matrix of image points, M <p> Under perspective projection these relationships are no longer exact. However for our configuration, a 1cm fiducial viewed from 1m using a 16mm or 25mm lens, the effects of perspective distortion are negligible compared to noise. A more detailed discussion can be found in <ref> [3] </ref>. 2.4 Implementation The current system is implemented in Lucid Common Lisp and runs on a Sun SparcStation 2 using a VideoPix frame grabber. The two most limiting components are the frame grabber ( 4 frames/sec) and the rendering/display system. <p> The enhancement is accurate well outside the volume enclosed by the fiducials. locations of the fiducials are determined using a laser scanner [2]. A discussion of this initial calibration can be found in <ref> [3] </ref>. Given the locations of the fiducials, a model of the skull obtained from CT imaging (shown as black dots) and a video image taken from ~ 1m away with a 25mm lens, the enhanced reality visualizations are generated.
Reference: 4. <author> Reg G. Willson and Steven A. Shafer. </author> <title> What is the center of the image? Technical Report CMU-CS-93-122, </title> <institution> Carnegie-Mellon University, Computer Science Department, </institution> <month> April </month> <year> 1993. </year> <title> Fig. 1. Some Enhanced Reality Visualizations Using a Geometric Object. Fig. 2. Some Enhanced Reality Visualizations Using a Plastic Skull. </title>
Reference-contexts: This assumes that the intrinsic parameters are fixed. In general, they are not. They change with the focus and aperture settings. For example, the principle point can shift by 8 pixels or more with adjustments to focus <ref> [4] </ref>. The effective focal length f also varies with focus and aperture settings. Zoom lenses take this variability to an extreme enabling large changes to f . Lens distortion also varies with changes to focus and aperture [1].
References-found: 4

