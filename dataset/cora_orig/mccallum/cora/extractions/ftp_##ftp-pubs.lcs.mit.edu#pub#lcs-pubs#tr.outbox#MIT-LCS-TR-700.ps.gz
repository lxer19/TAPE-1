URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-700.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/tr700.html
Root-URL: 
Title: On Consulting a Set of Experts and Searching  
Author: Ronald L. Rivest 
Degree: by Igal Galperin Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  All rights reserved. Author  Certified by  E.S. Webster Professor of Computer Science Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, Departmental Committee on Graduate Students  
Date: September 1996  September 5, 1996  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology 1996.  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> G. M. Adel'son-Vel'ski and E. M. Landis. </author> <title> An algorithm for the organization of information. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 3 </volume> <pages> 1259-1263, </pages> <year> 1962. </year>
Reference-contexts: The prediction domain of either the manager or the experts may be modified to be the real interval <ref> [0; 1] </ref> as in Cesa-Bianchi et al. [17], Little-stone and Warmuth [44], Haussler et al. [32] and Vovk [70]. For this prediction domain various loss functions may be considered (Vovk [70], Haussler et al. [32]). <p> i is represented by a triple S i = hE i ; i; e i i; where E i 2 N is the number of errors made by Alice (zero is considered a natural number); i 2 N is the number of rounds played in the game; e i 2 <ref> [0; 1] </ref> fl the expert state vector. The coordinates of e i are real numbers that sum to one. Coordinate e i j is the measure of the 24 set of experts that made j mistakes in the first i rounds. <p> of cardinality no greater than the continuum then according to a standard theorem [58, Proposition 26.2] all non-atomic probability measures that can be defined over the set of experts X [G] that agree with [G] are unique up to isomorphism, and isomorphic to the Lebesgue measure on the real segment <ref> [0; 1] </ref>. Thus, up to isomorphism, there is a single non-atomic game associated with any given game. <p> Thus the properties of the pseudo value functions established in section 1.5.1 still hold, allowing Alice to use the PM algorithms in games with bigger decision domains for the same performance guarantees. 1.7.3 "Real" Managers Alice may be allowed to make decisions in the domain <ref> [0; 1] </ref>. If she decides d while the correct answer turns out to be c she is charged a loss of jd cj, known as the absolute loss. <p> If Alice's strategy is to make random binary predictions with probability d of predicting one, then the absolute loss measures Alice's expected loss in the game. Notice that if we replace Alice's <ref> [0; 1] </ref> decisions by binary decisions in the obvious way, making her vote one whenever d 1 2 , then the loss she incurs is at most twice the loss she would incur if she were allowed to make decisions in [0; 1]. <p> Notice that if we replace Alice's <ref> [0; 1] </ref> decisions by binary decisions in the obvious way, making her vote one whenever d 1 2 , then the loss she incurs is at most twice the loss she would incur if she were allowed to make decisions in [0; 1]. <p> The linear program relaxation of the above is allowing q j and p ij to take arbitrary values in the interval <ref> [0; 1] </ref>. The value of an optimal fractional solution (linear program solution) is a lower bound on the value of solutions of the discrete s-median problem. The Lin-Vitter algorithm works as follows: 1. <p> They are an elegant example of the height-balanced approach. Red-black trees implement the basic dictionary operations with a worst-case cost of O (log n) per operation, at the cost of storing one extra bit (the "color" of the node) at each node. AVL trees <ref> [1] </ref> are another well-known example of height-balanced trees. Other schemes are weight-balanced in that the size of subtrees causes restructuring. By ensuring that the weights of siblings are approximately equal, an overall bound on the height of the tree is enforced. <p> These he terms "superclasses" 106 containing all other classes of balanced trees. They satisfy the simplest possible criterion that guarantees logarithmic time searching - O (lg n) height. Most other tree based data structures, like AVL trees <ref> [1] </ref>, Red-Black trees [9, 29], BB (ff) trees [53], ffBB trees [54] and Andersson's BH (c) trees impose a balance condition that makes some of the trees of height O (lg n) not members of the class of allowed trees.
Reference: [2] <author> A. Andersson. </author> <title> Improving partial rebuilding by using simple balance criteria. </title> <booktitle> In Proceedings of the Workshop on Algorithms and Data Structures, </booktitle> <pages> pages 393-402. </pages> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: Our scheme combines the notions of height-balance and weight-balance to achieve an effective algorithm, without storing either height information or weight information at any node. It is most similar to Andersson's GB 0 (c) trees [3]. His first publication <ref> [2] </ref> has shortly preceded our independent discovery. Both GB 0 (c) trees and scapegoat trees use total rebuilding of subtrees to enforce an upper bound on the depth of the tree, and achieve the same asymptotic performance for the dictionary operations. <p> Condition (3.11) guarantees that this quantity does not exceed L. 2 Comment: The looser triggering condition for Delete induced restructuring specified by (3.11) applies to multi-key scapegoat trees as well. 3.9 Comparison to Andersson's Work We arrived at our result unaware of Andersson's publication <ref> [2] </ref> that has preceded our discovery by about a year. Even in light of his precedence scapegoat trees contribute to the theoretical understanding of the family of data structures that use partial rebuilding to enforce a bound on the tree's depth [3, 13, 55, 40, 25].
Reference: [3] <author> A. Andersson. </author> <title> Efficient Search Trees. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Lund University, Sweden, </institution> <month> January </month> <year> 1990. </year>
Reference-contexts: Our scheme combines the notions of height-balance and weight-balance to achieve an effective algorithm, without storing either height information or weight information at any node. It is most similar to Andersson's GB 0 (c) trees <ref> [3] </ref>. His first publication [2] has shortly preceded our independent discovery. Both GB 0 (c) trees and scapegoat trees use total rebuilding of subtrees to enforce an upper bound on the depth of the tree, and achieve the same asymptotic performance for the dictionary operations. <p> Even in light of his precedence scapegoat trees contribute to the theoretical understanding of the family of data structures that use partial rebuilding to enforce a bound on the tree's depth <ref> [3, 13, 55, 40, 25] </ref>. The first part of his thesis [3] culminates with the presentation of two data structures he calls GB (c) trees and GB 0 (c) trees General Balanced trees. These he terms "superclasses" 106 containing all other classes of balanced trees. <p> Even in light of his precedence scapegoat trees contribute to the theoretical understanding of the family of data structures that use partial rebuilding to enforce a bound on the tree's depth [3, 13, 55, 40, 25]. The first part of his thesis <ref> [3] </ref> culminates with the presentation of two data structures he calls GB (c) trees and GB 0 (c) trees General Balanced trees. These he terms "superclasses" 106 containing all other classes of balanced trees. They satisfy the simplest possible criterion that guarantees logarithmic time searching - O (lg n) height. <p> The usage of these schemes for multiway trees as well as their use for upgrading existing code are novel. Sleator and Tarjan [64] as well as Andersson <ref> [3] </ref> do not report experimental measurements of their suggested structures' performance.
Reference: [4] <author> D. Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 319-342, </pages> <year> 1988. </year>
Reference-contexts: We discuss optimal algorithms in the worst-case against a computationally unlimited adversary. This is the set-up commonly assumed in the analysis of algorithms for finitely many consultants. The PM (Pascal Matrix) algorithms can be 14 seen as a generalization of the Halving algorithm (Angluin <ref> [4] </ref>, Barzdin and Freivalds [8]) to the case when the candidate predictors are allowed multiple errors. A zero-sum multi-stage game is a competition between two players. Popular examples include chess, checkers, backgammon.
Reference: [5] <author> J. Aslam. </author> <title> Noise Tolerant Algorithms for Learning and Searching. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <month> February </month> <year> 1995. </year> <month> MIT/LCS/TR-657. </month>
Reference-contexts: Expert Games explored herein are closely related to, and for some variants reducible to, the well investigated (Rivest et al. [61], Pelc [60], Aslam and Dhagat [6], Spencer and Winkler [67], Aslam <ref> [5] </ref>) Paul-Carole chip games modeling a faulty search process. Our analysis leads to an extension of results by Rivest et al. [61] to cover searchers that incorporate an arbitrary prior on the values searched. <p> It was first introduced by Ulam [69] and addressed by numerous researchers <ref> [61, 60, 22, 30, 24, 6, 66, 67, 5] </ref>. Most of these papers model it by a multistage game. The searcher is commonly called Paul and his adversary Carole. The 48 problem most of these papers address is searching for a single value in a finite domain.
Reference: [6] <author> J. Aslam and A. Dhagat. </author> <title> Searching in the presence of linearly bounded errors. </title> <booktitle> In Proceedings of the Twenty Third Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 486-493. </pages> <publisher> ACM Press, </publisher> <year> 1991. </year>
Reference-contexts: Expert Games explored herein are closely related to, and for some variants reducible to, the well investigated (Rivest et al. [61], Pelc [60], Aslam and Dhagat <ref> [6] </ref>, Spencer and Winkler [67], Aslam [5]) Paul-Carole chip games modeling a faulty search process. Our analysis leads to an extension of results by Rivest et al. [61] to cover searchers that incorporate an arbitrary prior on the values searched. <p> It yields the same necessary and a different sufficient condition for Paul's victory than those specified by Spencer [66]. The heart of our proof method consists of establishing that the expert consulting problem can be modeled by a chip game (see e.g. Aslam and Dhagat <ref> [6] </ref>) in which the chooser tries to lengthen the game. We proceed to prove strong results about non-atomic expert consulting chip games. These imply somewhat weaker results for the interesting class of discrete games atomic games that represent consulting finitely many experts. <p> It was first introduced by Ulam [69] and addressed by numerous researchers <ref> [61, 60, 22, 30, 24, 6, 66, 67, 5] </ref>. Most of these papers model it by a multistage game. The searcher is commonly called Paul and his adversary Carole. The 48 problem most of these papers address is searching for a single value in a finite domain. <p> Thereby she decides the values in which one of the two subsets will accumulate one more vote against them being the target value. Some authors associate each candidate value with a chip, hence the name "chip games" <ref> [6] </ref>. The chips are positioned in a sequence of piles on a ray. The allowed positions for the various piles on this ray are indexed by the natural numbers. <p> Various limitations are placed on the ways in which Carole is allowed to lie. She might be allowed a constant number of lies [61, 59]. Games in which Carole is allowed to lie brlc times in a game of l rounds are addressed by Aslam and Dhagat <ref> [6] </ref> and Spencer and Winkler [67]. This limitation might be enforced when the game terminates, or at each round. In the expert consulting game the adversary (Carole) chooses a subset of X. Alice decides which set she want to vote with, and then the adversary announces Alice right or wrong.
Reference: [7] <author> J. L. Baer and B. Schwab. </author> <title> A comparison of tree balancing algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 20(5) </volume> <pages> 322-330, </pages> <month> May </month> <year> 1977. </year>
Reference-contexts: This is the first method ever proposed that achieves a worst-case search time of O (log n) without using such extra information, while maintaining optimal amortized update costs. In addition, the method is quite simple and practical. In their comparative study Baer and Schwab <ref> [7] </ref>, distinguish height-balanced schemes from weight-balanced schemes based on the criterion that triggers restructuring. In a height-balanced structure the extra information stored at each node helps to enforce a bound on the overall height of the tree by bounding the height of subtrees.
Reference: [8] <author> J.M. Barzdin and R.V. Freivalds. </author> <title> On the prediction of general recursive functions. </title> <journal> Soviet. Math. Dokl., </journal> <volume> 13 </volume> <pages> 1224-1228, </pages> <year> 1972. </year>
Reference-contexts: We discuss optimal algorithms in the worst-case against a computationally unlimited adversary. This is the set-up commonly assumed in the analysis of algorithms for finitely many consultants. The PM (Pascal Matrix) algorithms can be 14 seen as a generalization of the Halving algorithm (Angluin [4], Barzdin and Freivalds <ref> [8] </ref>) to the case when the candidate predictors are allowed multiple errors. A zero-sum multi-stage game is a competition between two players. Popular examples include chess, checkers, backgammon.
Reference: [9] <author> R. Bayer. </author> <title> Symmetric binary B-trees: Data structure and maintenance algorithms. </title> <journal> Acta Informatica, </journal> <volume> 1 </volume> <pages> 290-306, </pages> <year> 1972. </year> <month> 113 </month>
Reference-contexts: In a height-balanced structure the extra information stored at each node helps to enforce a bound on the overall height of the tree by bounding the height of subtrees. Red-black trees, were invented by Bayer <ref> [9] </ref> and refined by Guibas and Sedgewick [29]. They are an elegant example of the height-balanced approach. Red-black trees implement the basic dictionary operations with a worst-case cost of O (log n) per operation, at the cost of storing one extra bit (the "color" of the node) at each node. <p> These he terms "superclasses" 106 containing all other classes of balanced trees. They satisfy the simplest possible criterion that guarantees logarithmic time searching - O (lg n) height. Most other tree based data structures, like AVL trees [1], Red-Black trees <ref> [9, 29] </ref>, BB (ff) trees [53], ffBB trees [54] and Andersson's BH (c) trees impose a balance condition that makes some of the trees of height O (lg n) not members of the class of allowed trees.
Reference: [10] <author> S. Ben-David and E. Dichterman. </author> <title> Learning with restricted focus of attention. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 287-296. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: Extracting that which is essential is likewise a difficult algorithmic problem arising in various circumstances the clique problem, traveling salesman and many more. In the context of computational learning it was addressed explicitly by Blum [16], Blum et al. [15], Littlestone [43], Ben-David and Dichterman <ref> [10, 11] </ref>, Birkendorf et al. [14]. Consider a manager faced with the task of hiring experts from a pool of N candidates. We assume that he can find out the utility of hiring particular sets of experts by querying an oracle x : 2 N ! &lt;.
Reference: [11] <author> S. Ben-David and E. Dichterman. </author> <title> Learnability with restricted focus of attention guarantees noise-tolerance. </title> <booktitle> In Fifth International Workshop on Algorithmic Learning Theory, </booktitle> <pages> pages 248-259. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Extracting that which is essential is likewise a difficult algorithmic problem arising in various circumstances the clique problem, traveling salesman and many more. In the context of computational learning it was addressed explicitly by Blum [16], Blum et al. [15], Littlestone [43], Ben-David and Dichterman <ref> [10, 11] </ref>, Birkendorf et al. [14]. Consider a manager faced with the task of hiring experts from a pool of N candidates. We assume that he can find out the utility of hiring particular sets of experts by querying an oracle x : 2 N ! &lt;.
Reference: [12] <author> Jon L. Bentley. </author> <title> Multidimensional binary search trees used from associative searching. </title> <journal> Communications of the ACM, </journal> <volume> 19 </volume> <pages> 509-517, </pages> <year> 1975. </year>
Reference-contexts: It suggests a natural break-up of the code's development into two phases, the first of which produces code that supports all of the system's features except performance. We show scapegoat balancing can be used for a variety of tree-based data structures : Bentley's <ref> [12] </ref> kd trees, Leuker's [40] trees for orthogonal queries. Finkel and Bentley's [25] quad trees. <p> Hence, the total potential stored at the root is at least C 0 N F ((1 C 0 )N ) = O (NF (N), allowing it to pay for the rebuilding operation. 2 Scapegoat k d Trees. Bentley <ref> [12] </ref> introduced k d trees. He proved average-case bounds of O (lg n) for a tree of size n for both updates and searches. <p> Proof: To apply Theorem 3.7.1 we use the algorithm Bentley <ref> [12] </ref> proposes for building a perfectly balanced k d tree of N nodes in O (kN lg N ), by taking as a splitting point the median with respect to the splitting coordinate.
Reference: [13] <author> Jon L. Bentley. </author> <title> Multidimensional binary search trees in database applications. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 5(4) </volume> <pages> 333-340, </pages> <year> 1979. </year>
Reference-contexts: Bentley [12] introduced k d trees. He proved average-case bounds of O (lg n) for a tree of size n for both updates and searches. Bentley <ref> [13] </ref> and Overmars and van Leeuwen [55] propose a scheme for dynamic maintenance of k d trees that achieves a logarithmic worst-case bound for searches with an average-case bound of O ((lg n) 2 ) for updates. Both use an idea similar to ours of rebuilding weight-unbalanced subtrees. <p> Overmars and van Leeuwen called their structure pseudo k d trees. Scapegoat k d trees achieve logarithmic worst-case bounds for searches and a log 2 n amortized bound for updates. ( The analysis of updates of Overmars and van Leeuwen [55] and Bentley <ref> [13] </ref> can be improved to yield amortized rather than average-case bounds.) However, scapegoat k d trees do not require maintaining extra data at the nodes. <p> Even in light of his precedence scapegoat trees contribute to the theoretical understanding of the family of data structures that use partial rebuilding to enforce a bound on the tree's depth <ref> [3, 13, 55, 40, 25] </ref>. The first part of his thesis [3] culminates with the presentation of two data structures he calls GB (c) trees and GB 0 (c) trees General Balanced trees. These he terms "superclasses" 106 containing all other classes of balanced trees.
Reference: [14] <author> A. Birkendorf, E. Dichterman, J. Jackson, N. Klasner, and H. U. Simon. </author> <title> On restricted-focus-of-attention learnability of boolean functions. </title> <booktitle> In Proceedings of the Ninth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 205-216. </pages> <publisher> ACM Press, </publisher> <year> 1996. </year>
Reference-contexts: In the context of computational learning it was addressed explicitly by Blum [16], Blum et al. [15], Littlestone [43], Ben-David and Dichterman [10, 11], Birkendorf et al. <ref> [14] </ref>. Consider a manager faced with the task of hiring experts from a pool of N candidates. We assume that he can find out the utility of hiring particular sets of experts by querying an oracle x : 2 N ! &lt;.
Reference: [15] <author> A. Blum, L. Hellerstein, and N. Littlestone. </author> <title> Learning in the presence of finitely or infinitely many irrelevant attributes. </title> <booktitle> In Proceedings of the Fourth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 157-166. </pages> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: Extracting that which is essential is likewise a difficult algorithmic problem arising in various circumstances the clique problem, traveling salesman and many more. In the context of computational learning it was addressed explicitly by Blum [16], Blum et al. <ref> [15] </ref>, Littlestone [43], Ben-David and Dichterman [10, 11], Birkendorf et al. [14]. Consider a manager faced with the task of hiring experts from a pool of N candidates.
Reference: [16] <author> Avrim Blum. </author> <title> Learning boolean functions in an infinite attribute space. </title> <booktitle> In Proceedings of the Twenty Second Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 64-72, </pages> <address> Baltimore, Maryland, May 1990. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Extracting that which is essential is likewise a difficult algorithmic problem arising in various circumstances the clique problem, traveling salesman and many more. In the context of computational learning it was addressed explicitly by Blum <ref> [16] </ref>, Blum et al. [15], Littlestone [43], Ben-David and Dichterman [10, 11], Birkendorf et al. [14]. Consider a manager faced with the task of hiring experts from a pool of N candidates.
Reference: [17] <author> N. Cesa-Bianchi, Y. Freund, D. Helmbold, D. Haussler, R. Schapire, and M. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> In Proceedings of the Twenty Fifth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 382-391, </pages> <address> San Diego, CA, May 1993. </address> <publisher> ACM Press. </publisher>
Reference-contexts: The prediction domain of either the manager or the experts may be modified to be the real interval [0; 1] as in Cesa-Bianchi et al. <ref> [17] </ref>, Little-stone and Warmuth [44], Haussler et al. [32] and Vovk [70]. For this prediction domain various loss functions may be considered (Vovk [70], Haussler et al. [32]).
Reference: [18] <author> N. Cesa-Bianchi, Y. Freund, D.P. Helmbold, and M. Warmuth. </author> <title> On-line prediction and conversion strategies. </title> <booktitle> In EuroCOLT, </booktitle> <pages> pages 205-216. </pages> <publisher> Clarendon Press, </publisher> <year> 1993. </year>
Reference-contexts: of experts on-line has been investigated under a variety of assumptions. (The on-line problem is distinguished by the manager having to reach a decision in every round, before seeing all inputs.) Littlestone and War-muth's [44] WM (Weighted Majority) algorithm and later the BW (Binomial Weighting) algorithm by Cesa-Bianchi et al. <ref> [18] </ref> address the question of consulting a finite number of experts that provide boolean advice. <p> We prove the PM algorithms achieve similar bounds when the decision domain is a finite set of arbitrary size. This generalizes the problem previously addressed by Littlestone and Warmuth [44] and Cesa-Bianchi et al. <ref> [18] </ref> of decision domains of size two ("yes/no" questions). The PM algorithms can also be applied to tracing "good" sets of experts of arbitrary size 1, if such are known to exist, and allow the manager to incorporate a non-uniform prior on experts' quality. <p> We proceed to prove strong results about non-atomic expert consulting chip games. These imply somewhat weaker results for the interesting class of discrete games atomic games that represent consulting finitely many experts. Our results are similar to those established by Cesa-Bianchi et al. <ref> [18] </ref> for consulting finitely many experts and those by e.g. Spencer [66] for Paul-Carole games, showing the close relationship between these two problems. 17 It is said that "the hardest decisions in life are the least important ones". <p> The superiority of PM provides evidence in favor of weight updates in every round. The upper bounds on the worst-case asymptotic performance of PM are the same as those of BW. Cesa-Bianchi et al. <ref> [18] </ref> argue they are superior to those of WM. The next example proves that the opportunistic ratio of BW is unbounded. Example (BW's opportunistic ratio is unbounded): Let us define a family fG k g of expert consulting games.
Reference: [19] <author> V. Chvatal. </author> <title> A greedy heuristic for the set-covering problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 4(3) </volume> <pages> 233-235, </pages> <year> 1979. </year> <month> 114 </month>
Reference-contexts: Apply the greedy set cover algorithm <ref> [19, 36] </ref> to the covering of ~ by the sets fS j g, choosing iteratively the set S j that covers the most uncovered points. Repeat this process until all points of ~ are covered.
Reference: [20] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press/McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: Splay trees do have other desirable properties that make them of 78 considerable practical and theoretical interest, however, such as their near-optimality when handling an arbitrary sequence of operations. Our algorithm modifies the weight-balanced method of Varghese <ref> [20, Problem 18-3] </ref>, who presents an algorithm for maintaining weight-balanced trees with amortized cost O (log n) per operation. Our scheme combines the notions of height-balance and weight-balance to achieve an effective algorithm, without storing either height information or weight information at any node. <p> We wish to show that the amortized complexity per Insert is O (log n). For an overview of amortized analysis, see Cormen et al. <ref> [20] </ref>. We begin by defining a nonnegative potential function for the tree. Let (x) = jsize (left [x]) size (right [x])j; and define the potential of node x to be 0 if (x) &lt; 2, and (x) otherwise. <p> For the splay trees we used top-down splaying as suggested by Sleator and Tarjan [64]. The implementation of red-black trees follows Chapter 14 in Cormen, Leiserson and Rivest <ref> [20] </ref>. 3.11 Discussion and Conclusions Stout and Warren [68] present an algorithm which takes an arbitrary binary search tree and rebalances it to form what they call a route balanced tree using linear time and only constant space.
Reference: [21] <author> G. Cornuejols, </author> <title> M.L. Fisher, and G.L. Nemhauser. Location of bank accounts to optimize float: An analytical study of exact and approximate algorithms. </title> <journal> Management Science, </journal> <volume> 23 </volume> <pages> 789-810, </pages> <year> 1977. </year>
Reference-contexts: To approximate a Lipschitz function a memory-based learning system can be used, as proposed by Lin and Vitter [42]. We generalize the analysis of a greedy approximate solution of the s-median problem first considered by Cornuejols et al. <ref> [21] </ref>. We then compare its performance to the performance of Lin and Vitter's linear programming approximate solution of the same problem as a tool in the construction of memory-based learning systems. <p> Lund and Yanakakis's lower bounds for the set-covering problem imply that it is N P-hard to find *-approximate solutions of size o (s log jV j) to the s-median problem for an * sufficiently small [41, 46]. We generalize the analysis of Cornuejols et al. <ref> [21] </ref> to account for approximate solutions of the s-median problem that are not necessarily of size s. <p> of size at most s (1 + 1=*)(ln m + 1) such that ^ d ~ (U ) (1 + *) ^ D; where ^ D is the average distance of the optimal fractional solution for the discrete s-median problem. 2.3.2 A Simple and Efficient Greedy Algorithm Cornuejols et al. <ref> [21] </ref> were the first to derive a bound on the performance of the greedy heuristic for the s-median problem. The bound they showed is somewhat stronger than the bound in Theorem 2.3.2, as explained towards the end of this subsection. <p> Easy implementation is another potential advantage of a vanilla greedy approach. Lin and Vitter express the quality of approximation in terms of the optimal fractional solution, while Theorem 2.3.2 expresses the quality of approximation in terms of the optimal integral solution. Cornuejols et al. <ref> [21] </ref> and Nemhauser et al. [52] show the bounds of Theorem 2.2.3 hold relative to the optimal fractional solution of the linear programming formulation of the s-median problem for j = k and ff = 1. <p> Similarly, ~ D can be replaced by ^ D in Theorem 2.3.2. 71 Contributions of This Work to the Analysis of the Greedy Heuristic's Perfor--mance. Previous results on the performance of the greedy heuristic of Cornuejols et al. <ref> [21] </ref> and Nemhauser et al. [52] do not allow its comparison to the Lin-Vitter approximation algorithm, as they do not consider a relaxation of the requirement on the desired size of the approximating set.
Reference: [22] <author> J. Czyzowicz, D. Mundici, and A. Pelc. </author> <title> Ulam's searching game with lies. </title> <journal> Journal of Combinatorial Theory, Ser. A, </journal> <volume> 52 </volume> <pages> 62-76, </pages> <year> 1989. </year>
Reference-contexts: It was first introduced by Ulam [69] and addressed by numerous researchers <ref> [61, 60, 22, 30, 24, 6, 66, 67, 5] </ref>. Most of these papers model it by a multistage game. The searcher is commonly called Paul and his adversary Carole. The 48 problem most of these papers address is searching for a single value in a finite domain.
Reference: [23] <author> George B. Dantzig. </author> <title> Activity Analysis of Production and Allocation, chapter Programming of Interdependent Activities, II, </title> <booktitle> Mathematical Models, </booktitle> <pages> pages 19-32. </pages> <publisher> John Wiley and Sons Inc., </publisher> <address> New York, </address> <year> 1951. </year>
Reference-contexts: Output U = fx i g i2I U as the median set. The linear programming problem can be solved in provably polynomial time by the ellipsoid algorithm [39] or by the interior point method [37]. The simplex method <ref> [23] </ref> works very efficiently in practice, although in the worst case its performance is not polynomial-time.
Reference: [24] <author> U. Feige, D. Peleg, P. Raghavan, and Upfal E. </author> <title> Computing with unreliable information. </title> <booktitle> In Proceedings of the Twenty Second Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 128-137. </pages> <publisher> ACM Press, </publisher> <year> 1990. </year>
Reference-contexts: It was first introduced by Ulam [69] and addressed by numerous researchers <ref> [61, 60, 22, 30, 24, 6, 66, 67, 5] </ref>. Most of these papers model it by a multistage game. The searcher is commonly called Paul and his adversary Carole. The 48 problem most of these papers address is searching for a single value in a finite domain.
Reference: [25] <author> R. A. Finkel and J. L. Bentley. </author> <title> Quad-trees; a data structure for retrieval on composite keys. </title> <journal> Acta Informatica, </journal> <volume> 4 </volume> <pages> 1-9, </pages> <year> 1974. </year>
Reference-contexts: We show scapegoat balancing can be used for a variety of tree-based data structures : Bentley's [12] kd trees, Leuker's [40] trees for orthogonal queries. Finkel and Bentley's <ref> [25] </ref> quad trees. <p> Note that our algorithm improves Leuker's amortized bounds for updates, and does not require storage of balancing data at the nodes of the tree. Scapegoat Quad Trees. Quad trees were introduced by Finkel and Bentley <ref> [25] </ref>. <p> Even in light of his precedence scapegoat trees contribute to the theoretical understanding of the family of data structures that use partial rebuilding to enforce a bound on the tree's depth <ref> [3, 13, 55, 40, 25] </ref>. The first part of his thesis [3] culminates with the presentation of two data structures he calls GB (c) trees and GB 0 (c) trees General Balanced trees. These he terms "superclasses" 106 containing all other classes of balanced trees.
Reference: [26] <author> M.L. Fisher and D.S. Hochbaum. </author> <title> Probabilistic analysis of the planar k-median problem. </title> <journal> Mathematics of Operations Research, </journal> <volume> 5(1) </volume> <pages> 265-294, </pages> <month> Feb </month> <year> 1980. </year>
Reference-contexts: for m input points drawn from the uniform distribution on a region of area A in the plane with probability one the value of a solution to the s-median problem is lower bounded by fi (m s) q s , for some constant fi, as shown by Fisher and Hochbaum <ref> [26] </ref>. Then the vanilla greedy algorithm may be used to produce a system of size fi (s log Kdiam Y d* ).
Reference: [27] <author> I. Galperin. </author> <title> Analysis of greedy expert hiring and an application to memory-based learning. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 217-223. </pages> <publisher> ACM Press, </publisher> <year> 1996. </year>
Reference-contexts: The algorithms presented are named after a combinatorial entity they utilize, the Pascal Matrix. 10 0.2 Chapter 2: Greedy Expert Hiring and an Appli- cation The second chapter (based on Galperin <ref> [27] </ref>) addresses the problem of hiring a set of experts from a pool of candidates. Modeling this problem by a coalitional game, a uniform lower bound on the performance of the greedy heuristic for a family of games is derived.
Reference: [28] <author> I. Galperin and R. Rivest. </author> <title> Scapegoat trees. </title> <booktitle> In Proceedings of the 4th ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 165-174. </pages> <publisher> ACM SIAM, </publisher> <year> 1993. </year>
Reference-contexts: We find the greedy approximation is simpler, more efficient and in many cases yields a smaller system. 0.3 Chapter 3: Searching a Dynamically Changing Set The last chapter (based on Galperin and Rivest <ref> [28] </ref>) is dedicated to the problem of supporting searches of a dynamically changing set of keys. An algorithm for maintaining binary search trees is presented. The amortized complexity per Insert or Delete is O (log n) while the worst-case cost of a Search is O (log n).
Reference: [29] <author> Leo J. Guibas and Robert Sedgewick. </author> <title> A diochromatic framework for balanced trees. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 8-21. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1978. </year> <month> 115 </month>
Reference-contexts: In a height-balanced structure the extra information stored at each node helps to enforce a bound on the overall height of the tree by bounding the height of subtrees. Red-black trees, were invented by Bayer [9] and refined by Guibas and Sedgewick <ref> [29] </ref>. They are an elegant example of the height-balanced approach. Red-black trees implement the basic dictionary operations with a worst-case cost of O (log n) per operation, at the cost of storing one extra bit (the "color" of the node) at each node. <p> These he terms "superclasses" 106 containing all other classes of balanced trees. They satisfy the simplest possible criterion that guarantees logarithmic time searching - O (lg n) height. Most other tree based data structures, like AVL trees [1], Red-Black trees <ref> [9, 29] </ref>, BB (ff) trees [53], ffBB trees [54] and Andersson's BH (c) trees impose a balance condition that makes some of the trees of height O (lg n) not members of the class of allowed trees.
Reference: [30] <author> W. Guzicki. </author> <title> Ulam's searching game with two lies. </title> <journal> Journal of Combinatorial Theory, Ser. A, </journal> <volume> 54 </volume> <pages> 1-19, </pages> <year> 1990. </year>
Reference-contexts: Section 1.7.4 discusses the close relationship between expert consulting games and the better investigated faulty search games. In the latter opportunistic algorithms for finitely many candidate values are known only for the cases M = 1 (see Pelc [59]) and M = 2 (see Guzicki <ref> [30] </ref>). <p> It was first introduced by Ulam [69] and addressed by numerous researchers <ref> [61, 60, 22, 30, 24, 6, 66, 67, 5] </ref>. Most of these papers model it by a multistage game. The searcher is commonly called Paul and his adversary Carole. The 48 problem most of these papers address is searching for a single value in a finite domain. <p> Proving this conjecture will reduce the problem of computing the exact value of states in the relative game with finitely many experts, addressed by Pelc [59] and Guzicki <ref> [30] </ref> for M = 1 and M = 2 respectively to the problem of calculating the number of moves in a game with prespecified strategies for both sides. 1.8 Conclusion Chip games were explored previously as a model of a faulty search procedure.
Reference: [31] <author> Chang H. and Iyengar S. S. </author> <title> Efficient algorithms to globally balance a binary search tree. </title> <journal> Communications of the ACM, </journal> <volume> 27(7) </volume> <pages> 695-702, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: Then build the new 1=2-weight-balanced tree using a "divide and conquer" method. This yields O (n) time and space complexity. Chang and Iyengar <ref> [31] </ref> survey a few techniques for rebuilding trees using logarithmic auxiliary space, and present additional algorithms. 91 The algorithms we present in this section are not included in their survey. All of the algorithms they present require two traversals of the tree.
Reference: [32] <author> D. Haussler, J. Kivinen, and M.K. Warmuth. </author> <title> Tight worst-case loss bounds for predicting with expert advice. </title> <booktitle> In EuroCOLT, </booktitle> <pages> pages 205-216. </pages> <publisher> Clarendon Press, </publisher> <year> 1995. </year>
Reference-contexts: The prediction domain of either the manager or the experts may be modified to be the real interval [0; 1] as in Cesa-Bianchi et al. [17], Little-stone and Warmuth [44], Haussler et al. <ref> [32] </ref> and Vovk [70]. For this prediction domain various loss functions may be considered (Vovk [70], Haussler et al. [32]). <p> domain of either the manager or the experts may be modified to be the real interval [0; 1] as in Cesa-Bianchi et al. [17], Little-stone and Warmuth [44], Haussler et al. <ref> [32] </ref> and Vovk [70]. For this prediction domain various loss functions may be considered (Vovk [70], Haussler et al. [32]). We generalize the problem of consulting finitely many experts by considering arbitrary measurable sets of experts of finite measure, and letting the decision domain of the manager and her advisors be a finite set. We discuss optimal algorithms in the worst-case against a computationally unlimited adversary.
Reference: [33] <author> D. Haussler and P. </author> <title> Long. A generalization of Sauer's lemma. </title> <type> Technical Report UCSC-CRL-90-15, </type> <institution> U.C. Santa Cruz Computer Research Laboratory, </institution> <month> Apr </month> <year> 1990. </year>
Reference-contexts: If arbitrarily long sequences are shattered, then dim P F is infinite. If F is a class of f0, 1g-valued functions then the definition of the pseudo-dimension is the same as that of the V C dimension. Haussler and Long <ref> [33] </ref> showed an upper bound on the sample complexity required to guarantee the uniform convergence with confidence 1 ffi 75 of the empirical estimates of a given family of functions with a bounded pseudo-dimension.
Reference: [34] <author> David Haussler. </author> <title> Generalizing the PAC model for neural net and other learning applications. </title> <type> Technical Report UCSC-CRL-89-30, </type> <institution> University of California Santa Cruz, </institution> <month> Sep </month> <year> 1989. </year>
Reference-contexts: One of the prettiest applications of this general analysis is to the s-median problem. Approximation algorithms for the s-median problem are a useful tool in learning Lips-chitz functions in the generalized PAC learning model of Haussler <ref> [34, 35] </ref>. To approximate a Lipschitz function a memory-based learning system can be used, as proposed by Lin and Vitter [42]. We generalize the analysis of a greedy approximate solution of the s-median problem first considered by Cornuejols et al. [21]. <p> First we quote two definitions after Haussler <ref> [34, 35] </ref>. For r 2 &lt; let sign (r) = 1 iff r &gt; 0, and zero otherwise.
Reference: [35] <author> David Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Information and Computation, </journal> <volume> 100(1) </volume> <pages> 78-150, </pages> <month> Sep </month> <year> 1992. </year>
Reference-contexts: One of the prettiest applications of this general analysis is to the s-median problem. Approximation algorithms for the s-median problem are a useful tool in learning Lips-chitz functions in the generalized PAC learning model of Haussler <ref> [34, 35] </ref>. To approximate a Lipschitz function a memory-based learning system can be used, as proposed by Lin and Vitter [42]. We generalize the analysis of a greedy approximate solution of the s-median problem first considered by Cornuejols et al. [21]. <p> First we quote two definitions after Haussler <ref> [34, 35] </ref>. For r 2 &lt; let sign (r) = 1 iff r &gt; 0, and zero otherwise.
Reference: [36] <author> D. S. Johnson. </author> <title> Approximation algorithms for combinatorial problems. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 9 </volume> <pages> 256-278, </pages> <year> 1974. </year>
Reference-contexts: Apply the greedy set cover algorithm <ref> [19, 36] </ref> to the covering of ~ by the sets fS j g, choosing iteratively the set S j that covers the most uncovered points. Repeat this process until all points of ~ are covered.
Reference: [37] <author> N. Karmarkar. </author> <title> A new polynomial-time algorithm for linear programming. </title> <journal> Combina-torica, </journal> <volume> 4(4) </volume> <pages> 373-395, </pages> <year> 1984. </year>
Reference-contexts: Output U = fx i g i2I U as the median set. The linear programming problem can be solved in provably polynomial time by the ellipsoid algorithm [39] or by the interior point method <ref> [37] </ref>. The simplex method [23] works very efficiently in practice, although in the worst case its performance is not polynomial-time.
Reference: [38] <author> Michael J. Kearns and Umesh V. Vazirani. </author> <title> An Introduction to Computational Learning Theory. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Many, if not all human behaviors seem to be the result of processing incoming information from multiple sources. Turing's test suggests intelligence can be measured by its resemblance to human behavior. The first two chapters of this thesis evolved from research in the area of Computational Learning Theory <ref> [38] </ref>, the area of theoretical computer science that is most closely related to research on artificial intelligence. We do not address the problem of consulting experts in its full generality. <p> out of N candidate values is at least arg max fP l h1i 1 brlc 1 g = arg max f2 l @ &lt; brlc 1 1 g: (1.21) 2 l @ &lt; brlc 1 A = Prf&lt; brlc successes in l throws of an unbiased coing: Using Chernoff's bound <ref> [38] </ref>: Pr [S l &lt; (1 fl)pl] e lpfl 2 =2 : we let p = 1=2, fl = 1 2r and get or 4 ln N 2 Spencer [66] implicitly proposes to evaluate states in the search game by equation (1.17) for = 1.
Reference: [39] <author> L.G. Khachiyan. </author> <title> A polynomial algorithm in linear programming. </title> <journal> Soviet Mathematics Doklady, </journal> <volume> 20 </volume> <pages> 191-194, </pages> <year> 1979. </year>
Reference-contexts: Let I U be the set of indices of sets chosen by the greedy set-covering heuristic. Output U = fx i g i2I U as the median set. The linear programming problem can be solved in provably polynomial time by the ellipsoid algorithm <ref> [39] </ref> or by the interior point method [37]. The simplex method [23] works very efficiently in practice, although in the worst case its performance is not polynomial-time.
Reference: [40] <author> George S. Leuker. </author> <title> A data structure for orthogonal range queries. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 28-34. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1978. </year> <month> 116 </month>
Reference-contexts: It suggests a natural break-up of the code's development into two phases, the first of which produces code that supports all of the system's features except performance. We show scapegoat balancing can be used for a variety of tree-based data structures : Bentley's [12] kd trees, Leuker's <ref> [40] </ref> trees for orthogonal queries. Finkel and Bentley's [25] quad trees. <p> For keys which are d dimensional vectors one may wish to specify a range for each component of the key and ask how many keys have all components in the desired range. Leuker <ref> [40] </ref> proposed an algorithm that handles range queries in O (log d n) worst-case time where n is the size of the tree. Updates are handled in O (nlog d n) amortized time. <p> Even in light of his precedence scapegoat trees contribute to the theoretical understanding of the family of data structures that use partial rebuilding to enforce a bound on the tree's depth <ref> [3, 13, 55, 40, 25] </ref>. The first part of his thesis [3] culminates with the presentation of two data structures he calls GB (c) trees and GB 0 (c) trees General Balanced trees. These he terms "superclasses" 106 containing all other classes of balanced trees.
Reference: [41] <author> Jyh-Han Lin and Jeffrey Scott Vitter. </author> <title> *-approximation with minimum packing con-straint violation. </title> <booktitle> In Proceedings of the Twenty Fourth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 771-782. </pages> <publisher> ACM Press, </publisher> <month> May </month> <year> 1992. </year>
Reference-contexts: A class of functions F is uniformly Lipschitz bounded if there exists a bound K such that all functions in F are K-Lipschitz functions. Call such an F a class of Lipschitz functions. Now let us define the s-median problem (based on Lin and Vitter <ref> [41] </ref>). In section 2.4 we discuss its relevance to efficient memory-based learning of Lipschitz functions. The input is a complete (directed or undirected) graph G = (V; E) on n vertices. Non-negative weights c ij are associated with the edges. We call the c ij -s distances. <p> It is N P-hard even in the Euclidian space [48, 57]. Lund and Yanakakis's lower bounds for the set-covering problem imply that it is N P-hard to find *-approximate solutions of size o (s log jV j) to the s-median problem for an * sufficiently small <ref> [41, 46] </ref>. We generalize the analysis of Cornuejols et al. [21] to account for approximate solutions of the s-median problem that are not necessarily of size s. <p> The linear programming problem can be solved in provably polynomial time by the ellipsoid algorithm [39] or by the interior point method [37]. The simplex method [23] works very efficiently in practice, although in the worst case its performance is not polynomial-time. Lin and Vitter <ref> [41] </ref> show: Theorem 2.3.1 Given any * &gt; 0, the Lin-Vitter algorithm outputs a set U of size at most s (1 + 1=*)(ln m + 1) such that ^ d ~ (U ) (1 + *) ^ D; where ^ D is the average distance of the optimal fractional solution
Reference: [42] <author> Jyh-Han Lin and Jeffrey Scott Vitter. </author> <title> A theory for memory-based learning. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 103-115. </pages> <publisher> ACM Press, </publisher> <month> Jul </month> <year> 1992. </year>
Reference-contexts: Approximation algorithms for the s-median problem are a useful tool in learning Lips-chitz functions in the generalized PAC learning model of Haussler [34, 35]. To approximate a Lipschitz function a memory-based learning system can be used, as proposed by Lin and Vitter <ref> [42] </ref>. We generalize the analysis of a greedy approximate solution of the s-median problem first considered by Cornuejols et al. [21]. <p> The analysis of the performance of greedy hiring in coalitional games implies a lower bound on approximations of the s-median problem defined below. Approximation algorithms for the s-median problem are in turn, is a useful tool in the development of a learning algorithm for Lipschitz functions (Lin and Vitter <ref> [42] </ref>). A memory-based learning system is a system that approximates (learns) a given function, f : X ! Z, in the following manner: An instance of the input space X is mapped by an encoder fl to the addresses of one or more memory locations. <p> The encoder or the decoder or both of them can be learned. A memory-based learning system can be evaluated by its sample, time, and space complexities. Conceivably, such systems can be used to learn functions over both discrete and continuous domains. Lin and Vitter <ref> [42] </ref> give a historical overview of early research on memory-based learning systems. Their stated main result is a memory-based learning system that PAC-learns in polynomial time and space, to which we propose an alternative. A Voronoi system is a very simple memory-based learning system. <p> We then evaluate the usage of a 58 greedy approximation scheme as an alternative to the (also greedy) approximation scheme used by Lin and Vitter <ref> [42] </ref> in their algorithm for memory-based learning of Lipschitz functions. It is found to be easier to implement and to have better time complexity than the scheme proposed by Lin and Vitter. In many cases it also yields a smaller Voronoi system. <p> Proof (of Theorem 2.2.2): The theorem follows from Claim 2.2.1 and Corollary 2.2.1. 2 2.3 Two Approximation Algorithms for the s-Median Problem Lin and Vitter <ref> [42] </ref> present an algorithm that finds an approximate solution to the s-median problem by solving a linear programming problem and then applying the greedy heuristic to the solution. <p> It concludes with a review of the proof that the Lin Vitter algorithm indeed works (with either approximation subroutine). 2.4.1 The Learning Algorithm Lin and Vitter <ref> [42] </ref> propose to learn classes of uniformly Lipschitz bounded functions by Voronoi systems of polynomial size with respect to the the error measure er P X (f; g) = E X [d Y (g (x); f (x))] = X Let Q P x (X; *; d X ) denote the quantization <p> We use this to derive bounds on the quality of a greedy approximate solution to the s-median problem. We argue that in the context of memory-based learning of Lipschitz functions the greedy approximation algorithm is an attractive alternative to the approximation technique proposed by Lin and Vitter <ref> [42] </ref>.
Reference: [43] <author> Nick Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: Extracting that which is essential is likewise a difficult algorithmic problem arising in various circumstances the clique problem, traveling salesman and many more. In the context of computational learning it was addressed explicitly by Blum [16], Blum et al. [15], Littlestone <ref> [43] </ref>, Ben-David and Dichterman [10, 11], Birkendorf et al. [14]. Consider a manager faced with the task of hiring experts from a pool of N candidates.
Reference: [44] <author> Nick Littlestone and Manfred K. Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108(2) </volume> <pages> 212-261, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: The problem of algorithmically consulting a finite set of experts on-line has been investigated under a variety of assumptions. (The on-line problem is distinguished by the manager having to reach a decision in every round, before seeing all inputs.) Littlestone and War-muth's <ref> [44] </ref> WM (Weighted Majority) algorithm and later the BW (Binomial Weighting) algorithm by Cesa-Bianchi et al. [18] address the question of consulting a finite number of experts that provide boolean advice. <p> The prediction domain of either the manager or the experts may be modified to be the real interval [0; 1] as in Cesa-Bianchi et al. [17], Little-stone and Warmuth <ref> [44] </ref>, Haussler et al. [32] and Vovk [70]. For this prediction domain various loss functions may be considered (Vovk [70], Haussler et al. [32]). <p> We prove the PM algorithms achieve similar bounds when the decision domain is a finite set of arbitrary size. This generalizes the problem previously addressed by Littlestone and Warmuth <ref> [44] </ref> and Cesa-Bianchi et al. [18] of decision domains of size two ("yes/no" questions). The PM algorithms can also be applied to tracing "good" sets of experts of arbitrary size 1, if such are known to exist, and allow the manager to incorporate a non-uniform prior on experts' quality. <p> If we limit our discussions to games of tracking the best expert with a uniform prior, then PM is h2; 0i almost opportunistic, while the previously known algorithms are not. Littlestone and Warmuth <ref> [44] </ref> suggest WM can be used in schemes that update the weights 46 of experts in every round, or only in rounds in which the manager errs for the same asymptotic performance.
Reference: [45] <author> L.H. Loomis and S. Sternberg. </author> <title> Advanced Calculus. </title> <publisher> Addison-Wesley, </publisher> <year> 1968. </year>
Reference-contexts: )g X &lt; *g Thus, for a given ffi &gt; 0, with probability at least 1 ffi, ~ D 2 ffi 1 Since any two norms j:j 1 ; j:j 2 on &lt; n are equivalent, that is ajxj 1 jxj 2 bjxj 1 for some positive constants a; b <ref> [45] </ref>, for any norm on &lt; n : ~ D C ( P m 2 ) n ; for some C &gt; 0.
Reference: [46] <author> C. Lund and M. Yanakakis. </author> <title> On the hardness of approximating minimization problems. </title> <booktitle> In Proceedings of the Twenty Fifth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 286-293. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: It is N P-hard even in the Euclidian space [48, 57]. Lund and Yanakakis's lower bounds for the set-covering problem imply that it is N P-hard to find *-approximate solutions of size o (s log jV j) to the s-median problem for an * sufficiently small <ref> [41, 46] </ref>. We generalize the analysis of Cornuejols et al. [21] to account for approximate solutions of the s-median problem that are not necessarily of size s.
Reference: [47] <author> D. Marr. </author> <title> A theory for cerebral neocortex. </title> <journal> In Proceedings of the Royal Society of London B., </journal> <volume> 176, </volume> <pages> pages 161-234, </pages> <year> 1970. </year>
Reference-contexts: Some researchers conjecture this is the main function of the early processing carried out by the brain (Marr <ref> [47] </ref>). Extracting that which is essential is likewise a difficult algorithmic problem arising in various circumstances the clique problem, traveling salesman and many more.
Reference: [48] <author> N. Megiddo and K.J. Supowit. </author> <title> On the complexity of some common geometric location problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 13 </volume> <pages> 182-196, </pages> <year> 1984. </year>
Reference-contexts: Call U the median set. The s-median problem arises in data compression, network location, and clustering. It is N P-hard even in the Euclidian space <ref> [48, 57] </ref>. Lund and Yanakakis's lower bounds for the set-covering problem imply that it is N P-hard to find *-approximate solutions of size o (s log jV j) to the s-median problem for an * sufficiently small [41, 46].
Reference: [49] <author> K. Mehlhorn and A. Tsakalidis. </author> <title> Data structures. </title> <editor> In J. van Leeuwen, editor, </editor> <title> Algorithms and Complexity, volume A, </title> <booktitle> chapter 6, </booktitle> <pages> pages 301-341. </pages> <publisher> Elsevier, </publisher> <year> 1990. </year>
Reference-contexts: The problem of searching for a particular named element within a given set is one of the fundamental problems of theoretical computer science <ref> [49] </ref>. This thesis (in chapter 1) establishes a close connection between the problems of searching a set using unreliable information, and consulting experts on-line. <p> Mehlhorn and Tsakalidis <ref> [49] </ref> survey the recent literature on such data structures. In this paper we propose a new method that achieves optimal amortized costs for update operations (Insert and Delete) and optimal worst-case cost for Search, without requiring the extra information (e.g. colors or weights) normally required by many balanced-tree schemes.
Reference: [50] <author> G.L. Nemhauser and L.A. Wolsey. </author> <title> Best algorithms for approximating the maximum of a submodular set function. </title> <journal> Mathematics of Operations Research, </journal> <volume> 3(3) </volume> <pages> 177-188, </pages> <year> 1978. </year>
Reference-contexts: We would like the ratio P j;k = X j =X k to be lower bounded by a function of j and k that does not depend on x. This problem was considered by Nemhauser and Wolsey <ref> [51, 50] </ref> and by Nemhauser, Wolsey and Fisher [52]. They prove that for games that are monotone and concave P k;k 1 (1 k 1 : A similar bound can be proven on P j;k for arbitrary j.
Reference: [51] <author> G.L. Nemhauser and L.A. Wolsey. </author> <title> Maximizing submodular set functions: Formulations and analysis of algorithms. </title> <editor> In P. Hansen, editor, </editor> <booktitle> Studies on Graphs and Discrete Programming, volume 11 of Annals of Discrete Mathematics, </booktitle> <pages> pages 279-301. </pages> <publisher> North-Holland, </publisher> <year> 1981. </year> <month> 117 </month>
Reference-contexts: We would like the ratio P j;k = X j =X k to be lower bounded by a function of j and k that does not depend on x. This problem was considered by Nemhauser and Wolsey <ref> [51, 50] </ref> and by Nemhauser, Wolsey and Fisher [52]. They prove that for games that are monotone and concave P k;k 1 (1 k 1 : A similar bound can be proven on P j;k for arbitrary j.
Reference: [52] <author> G.L. Nemhauser, L.A. Wolsey, </author> <title> and M.L. Fisher. An analysis of approximations for maximizing submodular set functions-I. </title> <journal> Mathematical Programming, </journal> <volume> 14 </volume> <pages> 265-294, </pages> <year> 1978. </year>
Reference-contexts: For an unrestricted input no performance guarantees can be provided for this heuristic. However, for functions x that are monotone and concave a uniform lower bound on the performance of greedy hiring holds. Nemhauser et al. <ref> [52] </ref> present a bound on the ratio between the value of the greedy approximation and that of the optimal solution. When the value of sets of experts has to be estimated by sampling, the manager may only have access to approximate values of such sets, rather than exact values. <p> We would like the ratio P j;k = X j =X k to be lower bounded by a function of j and k that does not depend on x. This problem was considered by Nemhauser and Wolsey [51, 50] and by Nemhauser, Wolsey and Fisher <ref> [52] </ref>. They prove that for games that are monotone and concave P k;k 1 (1 k 1 : A similar bound can be proven on P j;k for arbitrary j. <p> The bound they showed is somewhat stronger than the bound in Theorem 2.3.2, as explained towards the end of this subsection. Subsequently, Nemhauser et al. <ref> [52] </ref> generalized the result to arbitrary coalitional games. We show a derivation of the bound for the s-median problem from the general bound for Coalitional Games. We also generalize their analysis to allow approximation of the s-median set by sets of size other than s. 68 Description of the Algorithm. <p> Easy implementation is another potential advantage of a vanilla greedy approach. Lin and Vitter express the quality of approximation in terms of the optimal fractional solution, while Theorem 2.3.2 expresses the quality of approximation in terms of the optimal integral solution. Cornuejols et al. [21] and Nemhauser et al. <ref> [52] </ref> show the bounds of Theorem 2.2.3 hold relative to the optimal fractional solution of the linear programming formulation of the s-median problem for j = k and ff = 1. <p> Similarly, ~ D can be replaced by ^ D in Theorem 2.3.2. 71 Contributions of This Work to the Analysis of the Greedy Heuristic's Perfor--mance. Previous results on the performance of the greedy heuristic of Cornuejols et al. [21] and Nemhauser et al. <ref> [52] </ref> do not allow its comparison to the Lin-Vitter approximation algorithm, as they do not consider a relaxation of the requirement on the desired size of the approximating set. Their analysis bounds only the ratio we denoted P k;k and not the more general P j;k .
Reference: [53] <author> J. Nievergelt and E. M. Reingold. </author> <title> Binary trees of bounded balance. </title> <journal> SIAM Journal on Computing, </journal> <volume> 2(1) </volume> <pages> 33-43, </pages> <year> 1973. </year>
Reference-contexts: AVL trees [1] are another well-known example of height-balanced trees. Other schemes are weight-balanced in that the size of subtrees causes restructuring. By ensuring that the weights of siblings are approximately equal, an overall bound on the height of the tree is enforced. Nievergelt and Reingold <ref> [53] </ref> introduce such trees and present algorithms for implementing the basic dictionary operations in O (log n) worst-case time. The first published data structure that does not store any extra information at each node are Splay trees due to Sleator and Tarjan [64]. <p> These he terms "superclasses" 106 containing all other classes of balanced trees. They satisfy the simplest possible criterion that guarantees logarithmic time searching - O (lg n) height. Most other tree based data structures, like AVL trees [1], Red-Black trees [9, 29], BB (ff) trees <ref> [53] </ref>, ffBB trees [54] and Andersson's BH (c) trees impose a balance condition that makes some of the trees of height O (lg n) not members of the class of allowed trees.
Reference: [54] <author> H. J. Olivie. </author> <title> A new class of binary search trees: Half balanced binary search trees. </title> <journal> International J. of Computer Mathematics, </journal> <volume> 9 </volume> <pages> 287-303, </pages> <year> 1981. </year>
Reference-contexts: These he terms "superclasses" 106 containing all other classes of balanced trees. They satisfy the simplest possible criterion that guarantees logarithmic time searching - O (lg n) height. Most other tree based data structures, like AVL trees [1], Red-Black trees [9, 29], BB (ff) trees [53], ffBB trees <ref> [54] </ref> and Andersson's BH (c) trees impose a balance condition that makes some of the trees of height O (lg n) not members of the class of allowed trees.
Reference: [55] <author> Mark H. Overmars and Jan van Leeuwen. </author> <title> Dynamic multi-dimensional data structures based on quad- and k d trees. </title> <journal> Acta Informatica, </journal> <volume> 17 </volume> <pages> 267-285, </pages> <year> 1982. </year>
Reference-contexts: Bentley [12] introduced k d trees. He proved average-case bounds of O (lg n) for a tree of size n for both updates and searches. Bentley [13] and Overmars and van Leeuwen <ref> [55] </ref> propose a scheme for dynamic maintenance of k d trees that achieves a logarithmic worst-case bound for searches with an average-case bound of O ((lg n) 2 ) for updates. Both use an idea similar to ours of rebuilding weight-unbalanced subtrees. <p> Overmars and van Leeuwen called their structure pseudo k d trees. Scapegoat k d trees achieve logarithmic worst-case bounds for searches and a log 2 n amortized bound for updates. ( The analysis of updates of Overmars and van Leeuwen <ref> [55] </ref> and Bentley [13] can be improved to yield amortized rather than average-case bounds.) However, scapegoat k d trees do not require maintaining extra data at the nodes. <p> Samet [62] proposed an algorithm 102 for deletions. Overmars and van Leeuwen <ref> [55] </ref> introduced pseudo-quad trees adynamic version of quad trees. <p> or following the original notations of Overmars and van Leeuwen log d+1ffi n for any positive constant ffi (note that we do not require 1 &lt; ffi). * The bounds on updates are improved from average-case to amortized bounds. (Though careful analysis of the algorithm of Overmars and van Leeuwen <ref> [55] </ref> can yield amor tized bounds too.) * Scapegoat trees do not require maintenance of extra data at the nodes regarding the weight of the children of each node. <p> We call a multi-way node, x, ff-weight-balanced, if the every child y of x, satisfies size (y) ffsize (x). Weight and height balanced trees are defined in a way similar to that used for binary trees. Theorem 2.2.3 in Overmars and van Leeuwen <ref> [55] </ref> suggests how to build a 1=(d + 1) weight balanced pseudo-quad tree in O (n log n) time. <p> Even in light of his precedence scapegoat trees contribute to the theoretical understanding of the family of data structures that use partial rebuilding to enforce a bound on the tree's depth <ref> [3, 13, 55, 40, 25] </ref>. The first part of his thesis [3] culminates with the presentation of two data structures he calls GB (c) trees and GB 0 (c) trees General Balanced trees. These he terms "superclasses" 106 containing all other classes of balanced trees.
Reference: [56] <author> Guillermo Owen. </author> <title> Game Theory. </title> <publisher> Academic Press, </publisher> <year> 1982. </year>
Reference-contexts: It defines the notions of an opportunistic and almost opportunistic strategies. Then it describes the particulars 20 of expert consulting games and defines them formally. These are the subject of discussion in what follows. 1.3.1 Multistage Two-Person Games A zero-sum multistage two-person game <ref> [56] </ref> is a seven-tuple G = hS 1 ; S 2 ; S T ; S 0 ; c; M 1 ; M 2 i. The set S = S 1 [ S 2 [ S T is the set of states of game G. <p> 1 ; 2 if it satisfies sup V (Gj 1 ; o A finite game, one which is guaranteed to terminate in a finite number of moves and in which the set of moves available to the players in each state is finite, is guaranteed by the famed Minimax Theorem <ref> [56] </ref> to have a value V (G) 2 &lt; [ f1; 1g when mixed strategies are allowed. Both players in such a game have, possibly mixed, minimax strategies. Yet if pure strategies guarantee the game's value, then neither player can improve upon them by using mixed strategies. <p> The set N = f1; : : : ; ng is commonly called the set of players, and the set of its subsets, 2 N , the set of coalitions. (Here we assume N is finite.) An introductory text on coalitional game theory is by Owen <ref> [56] </ref>. 59 A game is monotone if 8S; T N such that S T : v (S) v (T ): A game is additive if 8S; T N; S " T = ; : v (S) + v (T ) = v (S [ T ): A game is subadditive if
Reference: [57] <author> C.H. Papadimitriou. </author> <title> Worst-case and probabilistic analysis of a geometric location problem. </title> <journal> SIAM Journal on Computing, </journal> <volume> 10 </volume> <pages> 542-557, </pages> <year> 1981. </year>
Reference-contexts: Call U the median set. The s-median problem arises in data compression, network location, and clustering. It is N P-hard even in the Euclidian space <ref> [48, 57] </ref>. Lund and Yanakakis's lower bounds for the set-covering problem imply that it is N P-hard to find *-approximate solutions of size o (s log jV j) to the s-median problem for an * sufficiently small [41, 46].
Reference: [58] <author> K.R. Parthasarathy. </author> <title> Introduction to Probability and Measure. </title> <publisher> Springer-Verlag, </publisher> <year> 1977. </year>
Reference-contexts: For an atomic game we call a game with identical parameters but a non-atomic measure the associated non-atomic game. If we restrict our attention to sets of experts of cardinality no greater than the continuum then according to a standard theorem <ref> [58, Proposition 26.2] </ref> all non-atomic probability measures that can be defined over the set of experts X [G] that agree with [G] are unique up to isomorphism, and isomorphic to the Lebesgue measure on the real segment [0; 1].
Reference: [59] <author> A. Pelc. </author> <title> Solution of Ulam's problem on searching with a lie. </title> <journal> Journal of Combinatorial Theory, Seires A, </journal> <volume> 44 </volume> <pages> 129-140, </pages> <year> 1987. </year>
Reference-contexts: The problem of finding such algorithms that are efficient remains open. Section 1.7.4 discusses the close relationship between expert consulting games and the better investigated faulty search games. In the latter opportunistic algorithms for finitely many candidate values are known only for the cases M = 1 (see Pelc <ref> [59] </ref>) and M = 2 (see Guzicki [30]). <p> The game proceeds in rounds. Various limitations are placed on the ways in which Carole is allowed to lie. She might be allowed a constant number of lies <ref> [61, 59] </ref>. Games in which Carole is allowed to lie brlc times in a game of l rounds are addressed by Aslam and Dhagat [6] and Spencer and Winkler [67]. This limitation might be enforced when the game terminates, or at each round. <p> Proving this conjecture will reduce the problem of computing the exact value of states in the relative game with finitely many experts, addressed by Pelc <ref> [59] </ref> and Guzicki [30] for M = 1 and M = 2 respectively to the problem of calculating the number of moves in a game with prespecified strategies for both sides. 1.8 Conclusion Chip games were explored previously as a model of a faulty search procedure.
Reference: [60] <author> A. Pelc. </author> <title> Searching with known error probability. </title> <journal> Theoretical Computer Science, </journal> <volume> 63 </volume> <pages> 185-202, </pages> <year> 1989. </year>
Reference-contexts: Expert Games explored herein are closely related to, and for some variants reducible to, the well investigated (Rivest et al. [61], Pelc <ref> [60] </ref>, Aslam and Dhagat [6], Spencer and Winkler [67], Aslam [5]) Paul-Carole chip games modeling a faulty search process. Our analysis leads to an extension of results by Rivest et al. [61] to cover searchers that incorporate an arbitrary prior on the values searched. <p> It was first introduced by Ulam [69] and addressed by numerous researchers <ref> [61, 60, 22, 30, 24, 6, 66, 67, 5] </ref>. Most of these papers model it by a multistage game. The searcher is commonly called Paul and his adversary Carole. The 48 problem most of these papers address is searching for a single value in a finite domain. <p> Most of these papers model it by a multistage game. The searcher is commonly called Paul and his adversary Carole. The 48 problem most of these papers address is searching for a single value in a finite domain. In Rivest et al. [61] and Pelc <ref> [60] </ref> the authors consider finding the * vicinity of a single real value in a given segment. The various versions of games of searching by questioning a liar using arbitrary membership queries are comparable to expert consulting games.
Reference: [61] <author> R.L. Rivest, A. R. Meyer, D.J. Kleitman, Winklmann K., and J. Spencer. </author> <title> Coping with errors in binary search procedures. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 20 </volume> <pages> 396-404, </pages> <year> 1980. </year>
Reference-contexts: Expert Games explored herein are closely related to, and for some variants reducible to, the well investigated (Rivest et al. <ref> [61] </ref>, Pelc [60], Aslam and Dhagat [6], Spencer and Winkler [67], Aslam [5]) Paul-Carole chip games modeling a faulty search process. Our analysis leads to an extension of results by Rivest et al. [61] to cover searchers that incorporate an arbitrary prior on the values searched. <p> herein are closely related to, and for some variants reducible to, the well investigated (Rivest et al. <ref> [61] </ref>, Pelc [60], Aslam and Dhagat [6], Spencer and Winkler [67], Aslam [5]) Paul-Carole chip games modeling a faulty search process. Our analysis leads to an extension of results by Rivest et al. [61] to cover searchers that incorporate an arbitrary prior on the values searched. It yields the value of the continuous Mistake Bound game (Version A in Spencer and Winkler's [67]), implying a lower bound on the performance of Paul in the discrete game. <p> game with M (l) = M and a uniform prior is arg max fl lg 0 l 1 1 + M lg lg Any other strategy's expected loss for this problem is at least as high on some inputs. (The right side of (1.20) is due to Rivest et al. <ref> [61] </ref>.) 1.7 Extensions and Implications The results proven thus far for expert consulting games can be generalized in various ways. They have interesting parallels to those obtained for the problem of searching in the presence of errors. <p> It was first introduced by Ulam [69] and addressed by numerous researchers <ref> [61, 60, 22, 30, 24, 6, 66, 67, 5] </ref>. Most of these papers model it by a multistage game. The searcher is commonly called Paul and his adversary Carole. The 48 problem most of these papers address is searching for a single value in a finite domain. <p> Most of these papers model it by a multistage game. The searcher is commonly called Paul and his adversary Carole. The 48 problem most of these papers address is searching for a single value in a finite domain. In Rivest et al. <ref> [61] </ref> and Pelc [60] the authors consider finding the * vicinity of a single real value in a given segment. The various versions of games of searching by questioning a liar using arbitrary membership queries are comparable to expert consulting games. <p> The game proceeds in rounds. Various limitations are placed on the ways in which Carole is allowed to lie. She might be allowed a constant number of lies <ref> [61, 59] </ref>. Games in which Carole is allowed to lie brlc times in a game of l rounds are addressed by Aslam and Dhagat [6] and Spencer and Winkler [67]. This limitation might be enforced when the game terminates, or at each round. <p> We further analyze larger classes of M (l) and (l) functions. We thus extend results of Rivest et al. <ref> [61] </ref> for what they call continuous games. Mistake bounded adversaries making linearly many mistakes, M (l) = brlc against membership queries of Paul were explored by Spencer and Winkler [67].
Reference: [62] <author> Hanan Samet. </author> <title> Deletion in two-dimensional quad trees. </title> <journal> Communications of the ACM, </journal> <volume> 23(12) </volume> <pages> 703-710, </pages> <year> 1980. </year>
Reference-contexts: Samet <ref> [62] </ref> proposed an algorithm 102 for deletions. Overmars and van Leeuwen [55] introduced pseudo-quad trees adynamic version of quad trees.
Reference: [63] <author> Lloyd S. Shapley. </author> <title> Cores of convex games. </title> <journal> International Journal of Game Theory, </journal> <volume> 1(1) </volume> <pages> 11-26, </pages> <year> 1971. </year>
Reference-contexts: ): A game is concave if it satisfies the condition of diminishing returns for all i 2 N and for all S; T such that S T N n fig: v (S [ fig) v (S) v (T [ fig) v (T ): These naming conventions are due to Shapley <ref> [63] </ref> who defined and investigated convex games.
Reference: [64] <author> Daniel D. Sleator and Robert E. Tarjan. </author> <title> Self-adjusting binary search trees. </title> <journal> Journal of the ACM, </journal> <volume> 32(3) </volume> <pages> 652-686, </pages> <year> 1985. </year> <month> 118 </month>
Reference-contexts: Nievergelt and Reingold [53] introduce such trees and present algorithms for implementing the basic dictionary operations in O (log n) worst-case time. The first published data structure that does not store any extra information at each node are Splay trees due to Sleator and Tarjan <ref> [64] </ref>. They achieve O (log n) amortized complexity per operation. <p> The usage of these schemes for multiway trees as well as their use for upgrading existing code are novel. Sleator and Tarjan <ref> [64] </ref> as well as Andersson [3] do not report experimental measurements of their suggested structures' performance. <p> Hence, in practical applications, it would be advisable to use scapegoat trees when the inserted keys are expected to be roughly randomly distributed, or when the application is search intensive. For the splay trees we used top-down splaying as suggested by Sleator and Tarjan <ref> [64] </ref>.
Reference: [65] <author> D.D. Sleator and R.E. Tarjan. </author> <title> Amortized efficiency of list update rules. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 488-492. </pages> <publisher> ACM Press, </publisher> <year> 1984. </year>
Reference-contexts: The most commonly used measure of algorithms' quality is their asymptotic performance. Yet this measure is not sensitive enough to separate the LRU and FIFO paging algorithms from LFU and LIFO although they are not equivalent in practice. Sleator and Tarjan <ref> [65] </ref> used competitivity to better compare them theoretically.
Reference: [66] <author> J. Spencer. </author> <title> Ulam's searching game with a fixed number of lies. </title> <journal> Theoretical Computer Science, </journal> <volume> 95 </volume> <pages> 307-322, </pages> <year> 1992. </year>
Reference-contexts: It yields the value of the continuous Mistake Bound game (Version A in Spencer and Winkler's [67]), implying a lower bound on the performance of Paul in the discrete game. It yields the same necessary and a different sufficient condition for Paul's victory than those specified by Spencer <ref> [66] </ref>. The heart of our proof method consists of establishing that the expert consulting problem can be modeled by a chip game (see e.g. Aslam and Dhagat [6]) in which the chooser tries to lengthen the game. We proceed to prove strong results about non-atomic expert consulting chip games. <p> These imply somewhat weaker results for the interesting class of discrete games atomic games that represent consulting finitely many experts. Our results are similar to those established by Cesa-Bianchi et al. [18] for consulting finitely many experts and those by e.g. Spencer <ref> [66] </ref> for Paul-Carole games, showing the close relationship between these two problems. 17 It is said that "the hardest decisions in life are the least important ones". This principle is accounted for by the observation that a decision is difficult to make when the options presented are of similar utility. <p> It was first introduced by Ulam [69] and addressed by numerous researchers <ref> [61, 60, 22, 30, 24, 6, 66, 67, 5] </ref>. Most of these papers model it by a multistage game. The searcher is commonly called Paul and his adversary Carole. The 48 problem most of these papers address is searching for a single value in a finite domain. <p> 2 l @ &lt; brlc 1 A = Prf&lt; brlc successes in l throws of an unbiased coing: Using Chernoff's bound [38]: Pr [S l &lt; (1 fl)pl] e lpfl 2 =2 : we let p = 1=2, fl = 1 2r and get or 4 ln N 2 Spencer <ref> [66] </ref> implicitly proposes to evaluate states in the search game by equation (1.17) for = 1. His rationale for using these weights stems from considering a randomized strategy of Carole in which she uses a fair coin to decide her answers. <p> If P 2l e 0 1 M (l) then the adversary wins. This establishes the same necessary condition for Paul's (the adversary's) victory as that in Spencer's <ref> [66] </ref> and a different sufficient condition. <p> Thereby he will effectively play a strategy equivalent to D 1 2 . This gives a different proof of the validity of the sufficient condition (the main result) specified by Spencer <ref> [66] </ref>.
Reference: [67] <author> J. Spencer and P. Winkler. </author> <title> Three thresholds for a liar. Combinatorics, </title> <journal> Probability and Computing, </journal> <volume> 1 </volume> <pages> 81-93, </pages> <year> 1992. </year>
Reference-contexts: Expert Games explored herein are closely related to, and for some variants reducible to, the well investigated (Rivest et al. [61], Pelc [60], Aslam and Dhagat [6], Spencer and Winkler <ref> [67] </ref>, Aslam [5]) Paul-Carole chip games modeling a faulty search process. Our analysis leads to an extension of results by Rivest et al. [61] to cover searchers that incorporate an arbitrary prior on the values searched. <p> Our analysis leads to an extension of results by Rivest et al. [61] to cover searchers that incorporate an arbitrary prior on the values searched. It yields the value of the continuous Mistake Bound game (Version A in Spencer and Winkler's <ref> [67] </ref>), implying a lower bound on the performance of Paul in the discrete game. It yields the same necessary and a different sufficient condition for Paul's victory than those specified by Spencer [66]. <p> Spencer and Winkler <ref> [67] </ref> use an alternative equivalent definition. <p> Call it the alternating split. These splits were considered by Spencer and Winkler <ref> [67] </ref>. <p> It was first introduced by Ulam [69] and addressed by numerous researchers <ref> [61, 60, 22, 30, 24, 6, 66, 67, 5] </ref>. Most of these papers model it by a multistage game. The searcher is commonly called Paul and his adversary Carole. The 48 problem most of these papers address is searching for a single value in a finite domain. <p> She might be allowed a constant number of lies [61, 59]. Games in which Carole is allowed to lie brlc times in a game of l rounds are addressed by Aslam and Dhagat [6] and Spencer and Winkler <ref> [67] </ref>. This limitation might be enforced when the game terminates, or at each round. In the expert consulting game the adversary (Carole) chooses a subset of X. Alice decides which set she want to vote with, and then the adversary announces Alice right or wrong. <p> We further analyze larger classes of M (l) and (l) functions. We thus extend results of Rivest et al. [61] for what they call continuous games. Mistake bounded adversaries making linearly many mistakes, M (l) = brlc against membership queries of Paul were explored by Spencer and Winkler <ref> [67] </ref>. We show Theorem 1.7.1 Paul needs at least 4 ln N (12r) 2 questions to find the hidden number out of N candidates in a Mistake Bound (Version A of Spencer and Winkler [67]) game. <p> mistakes, M (l) = brlc against membership queries of Paul were explored by Spencer and Winkler <ref> [67] </ref>. We show Theorem 1.7.1 Paul needs at least 4 ln N (12r) 2 questions to find the hidden number out of N candidates in a Mistake Bound (Version A of Spencer and Winkler [67]) game. Proof: A Faulty Search Game with finitely many candidate values is related to a nonatomic search game, in a relationship similar to that existing in Expert Consulting Games.
Reference: [68] <author> Q. F. Stout and B. L. Warren. </author> <title> Tree rebalancing in optimal time and space. </title> <journal> Communications of the ACM, </journal> <volume> 29(9) </volume> <pages> 902-908, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: This simplified the code somewhat and yielded a 6% - 9% percent speedup over the version described by the pseudo-code. Stout and Warren <ref> [68] </ref> call these route balanced trees. This issue is discussed further in Section 3.11. <p> For the splay trees we used top-down splaying as suggested by Sleator and Tarjan [64]. The implementation of red-black trees follows Chapter 14 in Cormen, Leiserson and Rivest [20]. 3.11 Discussion and Conclusions Stout and Warren <ref> [68] </ref> present an algorithm which takes an arbitrary binary search tree and rebalances it to form what they call a route balanced tree using linear time and only constant space. This improves upon the logarithmic space required to output a perfectly balanced tree.
Reference: [69] <author> S. Ulam. </author> <title> Adventures of a Mathematician. </title> <address> Scribners (New York), </address> <year> 1977. </year>
Reference-contexts: It was first introduced by Ulam <ref> [69] </ref> and addressed by numerous researchers [61, 60, 22, 30, 24, 6, 66, 67, 5]. Most of these papers model it by a multistage game. The searcher is commonly called Paul and his adversary Carole.
Reference: [70] <author> V.G. Vovk. </author> <title> Agregating strategies. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year> <month> 119 </month>
Reference-contexts: The prediction domain of either the manager or the experts may be modified to be the real interval [0; 1] as in Cesa-Bianchi et al. [17], Little-stone and Warmuth [44], Haussler et al. [32] and Vovk <ref> [70] </ref>. For this prediction domain various loss functions may be considered (Vovk [70], Haussler et al. [32]). <p> The prediction domain of either the manager or the experts may be modified to be the real interval [0; 1] as in Cesa-Bianchi et al. [17], Little-stone and Warmuth [44], Haussler et al. [32] and Vovk <ref> [70] </ref>. For this prediction domain various loss functions may be considered (Vovk [70], Haussler et al. [32]). We generalize the problem of consulting finitely many experts by considering arbitrary measurable sets of experts of finite measure, and letting the decision domain of the manager and her advisors be a finite set.
References-found: 70

