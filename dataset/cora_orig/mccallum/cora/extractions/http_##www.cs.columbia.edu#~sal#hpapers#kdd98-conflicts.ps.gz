URL: http://www.cs.columbia.edu/~sal/hpapers/kdd98-conflicts.ps.gz
Refering-URL: http://www.cs.columbia.edu:80/~sal/recent-papers.html
Root-URL: 
Email: wfan@cs.columbia.edu  
Phone: Phone  
Title: Using Conflicts Among Multiple Base Classifiers to Measure the Performance of Stacking Author(s) with address(es):  
Author: David W. Fan Salvatore J. Stolfo Philip K. Chan 
Keyword: Conflict, Stacking, Metric, Accuracy Estimate, Performance  
Address: New York, NY 10027  Melbourne, FL 32901  
Affiliation: Department of Computer Science Columbia University  Computer Science Florida Institute of Technology  
Note: Title:  Email address of contact author:  number of contact author: 212-939-7078 I want my paper submission to be judged in the following category: Research Multiple submission statement (if applicable): N/A  
Abstract: 250 word maximum): We analyze stacking and point out the conflict problem. Conflicts are defined as base data with different class labels that produced the same predictions by a set of base classifiers. Based on conflicts, we propose conflict-based accuracy estimate to determine the overall accuracy of a stacked classifier and conflict-based accuracy improvement estimate to determine the overall accuracy improvement over base classifiers. We discuss some popular metrics for comparing and evaluating a set of classifiers: coverage, correlated error, diversity and specialty, and show that these metrics do not accurately estimate the overall accuracy of a stacked classifier system. From experimental results, we demonstrate that conflict-based accuracy estimate is an effective measure to predict overall performance and compare different stacked systems, and conflict-based accuracy improvement estimate is a good measure to project the overall accuracy improvement. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Ali. </author> <title> On Explaining Degree of Error Reduction due to Combining Multiple Decision Trees. Integrating Multiple Learning Model Workshop, </title> <booktitle> AAAI-96, </booktitle> <address> Portland, Oregon. </address>
Reference-contexts: 1 Introduction Stacking [20] is a widely known technique to combine classifiers [7]. Empirical studies have shown that stacking helps to increase both accuracy and efficiency. Many papers in recent years have concentrated on using various metrics, coverage [2], diversity [2, 4], correlated error <ref> [1] </ref> and specialty [4], to explain stacking and choose classifiers for combining. Choosing the best base classifiers is an important issue to increase accuracy and efficiency of a stacked classifier system. In this paper, we will discuss a problem called conflicts that hinders the accuracy improvement of stacking. <p> We propose to use CB-accuracy estimate to predict stacking's performance. We then discuss some popular metrics and show by example that it is relatively hard to use them to predict the overall accuracy. Measurements: Accuracy difference, accuracy improvement [4] and error ratio <ref> [1] </ref> were used previously to compare the performance of different combining structures. Let denote the overall accuracy of a stacked classifier system and denote the average accuracy of constituent base classifiers. The accuracy difference is defined as [4], accuracy dif f erence = ( ). <p> The accuracy difference is defined as [4], accuracy dif f erence = ( ). The accuracy improvement [4] is defined as, accuracy improvement = . Using this notation, the error ratio <ref> [1] </ref> is defined as, error ratio = 1 1 . These measures are appropriate when the base classifiers are fixed. However, it is problematic to use them to compare two different structures, since these measures are defined over the average predictive accuracy of all base classifiers. <p> c 3 ; c 4 ); y) num ((c 1 ; c 2 ; c 3 ; c 4 ); y) num ((0,0,1,0),1) 400 ((0,0,1,0),1) 100 ((0,1,1,0),1) 500 ((0,1,1,0),1) 800 correlated error = 0.2314 correlated error = 0.1667 = 0:6667 = 0:5263 sity [2, 4], specialty [4] and correlated error <ref> [1] </ref> to choose classifiers for combining and to explain how stacking increases accuracy. For a detailed description and formal definition of these metrics, please refer the cited papers. These metrics, except for coverage, are indirect measurements of stacking. <p> It has been observed that accuracy improvement increases with diversity. But it is not necessarily true that high diversity will increase overall accuracy. From the example in Figure 2, we see that when diversity increases, overall accuracy can either increase or decrease. Correlated Error, introduced by Ali and Pazzani <ref> [1] </ref>, measures the fraction of instances for which a pair of base classifiers make the same incorrect prediction. It has been observed that there is a decreasing trend in accuracy improvement when correlated error increases [1, 4]. <p> Correlated Error, introduced by Ali and Pazzani [1], measures the fraction of instances for which a pair of base classifiers make the same incorrect prediction. It has been observed that there is a decreasing trend in accuracy improvement when correlated error increases <ref> [1, 4] </ref>. But also, from the example in Figure 2, we observe that when correlated error decreases, overall accuracy also decreases. Specialty, introduced by Chan [4], measures how biased the base classifiers' predictions are towards certain classes. That is, they are more accurate in predicting certain classes than others.
Reference: [2] <author> C. Brodley and T. </author> <title> Lane Creating and Exploiting Coverage and Diversity Integrating Multiple Learning Model Workshop, </title> <booktitle> AAAI-96, </booktitle> <address> Portland, Oregon </address>
Reference-contexts: 1 Introduction Stacking [20] is a widely known technique to combine classifiers [7]. Empirical studies have shown that stacking helps to increase both accuracy and efficiency. Many papers in recent years have concentrated on using various metrics, coverage <ref> [2] </ref>, diversity [2, 4], correlated error [1] and specialty [4], to explain stacking and choose classifiers for combining. Choosing the best base classifiers is an important issue to increase accuracy and efficiency of a stacked classifier system. <p> 1 Introduction Stacking [20] is a widely known technique to combine classifiers [7]. Empirical studies have shown that stacking helps to increase both accuracy and efficiency. Many papers in recent years have concentrated on using various metrics, coverage [2], diversity <ref> [2, 4] </ref>, correlated error [1] and specialty [4], to explain stacking and choose classifiers for combining. Choosing the best base classifiers is an important issue to increase accuracy and efficiency of a stacked classifier system. <p> For brevity, we call conflict-based accuracy improvement estimate as CB-accuracy improvement estimate. Metrics: Recent work on stacking has concentrated effort on using coverage <ref> [2] </ref>, diver 3 Diversity vs Conflicts Example 1 Example 2 Conflict 1 Conflict 2 Conflict 1 Conflict 2 ((1,1),1) ((1,0),1) ((1,1),1) ((0,0),1) ((1,0),0) ((1,0),0) ((0,0),1) ((1,0),1) diversity = 0.5 diversity = 1 diversity = 0 diversity = 0.5 = 1 = 0:5 = 0:5 = 1 Conflicts vs Correlated Error Example <p> 2 ((c 1 ; c 2 ; c 3 ; c 4 ); y) num ((c 1 ; c 2 ; c 3 ; c 4 ); y) num ((0,0,1,0),1) 400 ((0,0,1,0),1) 100 ((0,1,1,0),1) 500 ((0,1,1,0),1) 800 correlated error = 0.2314 correlated error = 0.1667 = 0:6667 = 0:5263 sity <ref> [2, 4] </ref>, specialty [4] and correlated error [1] to choose classifiers for combining and to explain how stacking increases accuracy. For a detailed description and formal definition of these metrics, please refer the cited papers. These metrics, except for coverage, are indirect measurements of stacking. <p> As we shall see, it is relatively hard to use these metrics to estimate how well the overall stacked system will be. Coverage, introduced by Brodley and Lane <ref> [2] </ref>, measures the fraction of instances for which at least one of the base classifiers produces the correct predictions. Coverage actually measures 2 extreme cases of conflict, ((0; 0; : : : ; 0); 1) and ((1; 1; : : : ; 1); 0). <p> In terms of conflicts, we can define coverage as: coverage = 1 j ((0;0;:::;0);1)j+j ((1;1;:::;1);0)j jSj . But conflict is more general than coverage. It is reported that high coverage will increase stacking's accuracy improvement <ref> [2, 4] </ref>. This is natural since high coverage reduces the occurrences of the two extreme cases of conflicts. Diversity [2, 4] measures how different the base classifiers are, based on their predictions. <p> But conflict is more general than coverage. It is reported that high coverage will increase stacking's accuracy improvement <ref> [2, 4] </ref>. This is natural since high coverage reduces the occurrences of the two extreme cases of conflicts. Diversity [2, 4] measures how different the base classifiers are, based on their predictions. When the value of diversity grows, the predictions from the base classifiers are more evenly distributed and, therefore, more diverse. It has been observed that accuracy improvement increases with diversity.
Reference: [3] <author> W. Buntime and R. Caruana. </author> <title> Introduction to IND and Recursive Partitioning, </title> <institution> NASA Ames Research Center. </institution>
Reference-contexts: We used RIPPER [6], CART, ID3 and C4.5 <ref> [3] </ref> as the base learners. We combined 2 and 3 out of the 4 base classifiers trained by these programs and formed 10 different stacked systems. 2-fold CV was used to generate meta-level training data. We did not use any meta-learner here.
Reference: [4] <author> P. </author> <title> Chan An Extensible Meta-learning Approach for Scalable and Accurate Inductive Learning Ph.D. </title> <type> Thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, New York, </address> <year> 1996 </year>
Reference-contexts: 1 Introduction Stacking [20] is a widely known technique to combine classifiers [7]. Empirical studies have shown that stacking helps to increase both accuracy and efficiency. Many papers in recent years have concentrated on using various metrics, coverage [2], diversity <ref> [2, 4] </ref>, correlated error [1] and specialty [4], to explain stacking and choose classifiers for combining. Choosing the best base classifiers is an important issue to increase accuracy and efficiency of a stacked classifier system. <p> 1 Introduction Stacking [20] is a widely known technique to combine classifiers [7]. Empirical studies have shown that stacking helps to increase both accuracy and efficiency. Many papers in recent years have concentrated on using various metrics, coverage [2], diversity [2, 4], correlated error [1] and specialty <ref> [4] </ref>, to explain stacking and choose classifiers for combining. Choosing the best base classifiers is an important issue to increase accuracy and efficiency of a stacked classifier system. In this paper, we will discuss a problem called conflicts that hinders the accuracy improvement of stacking. <p> We propose to use CB-accuracy estimate to predict stacking's performance. We then discuss some popular metrics and show by example that it is relatively hard to use them to predict the overall accuracy. Measurements: Accuracy difference, accuracy improvement <ref> [4] </ref> and error ratio [1] were used previously to compare the performance of different combining structures. Let denote the overall accuracy of a stacked classifier system and denote the average accuracy of constituent base classifiers. The accuracy difference is defined as [4], accuracy dif f erence = ( ). <p> Measurements: Accuracy difference, accuracy improvement <ref> [4] </ref> and error ratio [1] were used previously to compare the performance of different combining structures. Let denote the overall accuracy of a stacked classifier system and denote the average accuracy of constituent base classifiers. The accuracy difference is defined as [4], accuracy dif f erence = ( ). The accuracy improvement [4] is defined as, accuracy improvement = . Using this notation, the error ratio [1] is defined as, error ratio = 1 1 . These measures are appropriate when the base classifiers are fixed. <p> Let denote the overall accuracy of a stacked classifier system and denote the average accuracy of constituent base classifiers. The accuracy difference is defined as <ref> [4] </ref>, accuracy dif f erence = ( ). The accuracy improvement [4] is defined as, accuracy improvement = . Using this notation, the error ratio [1] is defined as, error ratio = 1 1 . These measures are appropriate when the base classifiers are fixed. <p> 2 ((c 1 ; c 2 ; c 3 ; c 4 ); y) num ((c 1 ; c 2 ; c 3 ; c 4 ); y) num ((0,0,1,0),1) 400 ((0,0,1,0),1) 100 ((0,1,1,0),1) 500 ((0,1,1,0),1) 800 correlated error = 0.2314 correlated error = 0.1667 = 0:6667 = 0:5263 sity <ref> [2, 4] </ref>, specialty [4] and correlated error [1] to choose classifiers for combining and to explain how stacking increases accuracy. For a detailed description and formal definition of these metrics, please refer the cited papers. These metrics, except for coverage, are indirect measurements of stacking. <p> ; c 2 ; c 3 ; c 4 ); y) num ((c 1 ; c 2 ; c 3 ; c 4 ); y) num ((0,0,1,0),1) 400 ((0,0,1,0),1) 100 ((0,1,1,0),1) 500 ((0,1,1,0),1) 800 correlated error = 0.2314 correlated error = 0.1667 = 0:6667 = 0:5263 sity [2, 4], specialty <ref> [4] </ref> and correlated error [1] to choose classifiers for combining and to explain how stacking increases accuracy. For a detailed description and formal definition of these metrics, please refer the cited papers. These metrics, except for coverage, are indirect measurements of stacking. <p> In terms of conflicts, we can define coverage as: coverage = 1 j ((0;0;:::;0);1)j+j ((1;1;:::;1);0)j jSj . But conflict is more general than coverage. It is reported that high coverage will increase stacking's accuracy improvement <ref> [2, 4] </ref>. This is natural since high coverage reduces the occurrences of the two extreme cases of conflicts. Diversity [2, 4] measures how different the base classifiers are, based on their predictions. <p> But conflict is more general than coverage. It is reported that high coverage will increase stacking's accuracy improvement <ref> [2, 4] </ref>. This is natural since high coverage reduces the occurrences of the two extreme cases of conflicts. Diversity [2, 4] measures how different the base classifiers are, based on their predictions. When the value of diversity grows, the predictions from the base classifiers are more evenly distributed and, therefore, more diverse. It has been observed that accuracy improvement increases with diversity. <p> Correlated Error, introduced by Ali and Pazzani [1], measures the fraction of instances for which a pair of base classifiers make the same incorrect prediction. It has been observed that there is a decreasing trend in accuracy improvement when correlated error increases <ref> [1, 4] </ref>. But also, from the example in Figure 2, we observe that when correlated error decreases, overall accuracy also decreases. Specialty, introduced by Chan [4], measures how biased the base classifiers' predictions are towards certain classes. That is, they are more accurate in predicting certain classes than others. <p> It has been observed that there is a decreasing trend in accuracy improvement when correlated error increases [1, 4]. But also, from the example in Figure 2, we observe that when correlated error decreases, overall accuracy also decreases. Specialty, introduced by Chan <ref> [4] </ref>, measures how biased the base classifiers' predictions are towards certain classes. That is, they are more accurate in predicting certain classes than others. As this measure increases, the base classifiers are more biased and specialized to certain classes.
Reference: [5] <author> R.T. </author> <title> Clemen Combining Forcasts: A Review and Annotated Bibliography. </title> <journal> International Journal of Forcasting, </journal> <volume> 5 </volume> <pages> 559-583. </pages>
Reference: [6] <author> W. </author> <title> Cohen Fast Effective Rule Induction In Proc. </title> <booktitle> Twelfth Intl. Conf. on Machine Learning. </booktitle> <publisher> Morgan Kaufman </publisher>
Reference-contexts: We used RIPPER <ref> [6] </ref>, CART, ID3 and C4.5 [3] as the base learners. We combined 2 and 3 out of the 4 base classifiers trained by these programs and formed 10 different stacked systems. 2-fold CV was used to generate meta-level training data. We did not use any meta-learner here.
Reference: [7] <author> T. </author> <title> Dietterich Machine Learning Research: Four Current Directions. </title> <journal> AI Magazine, v.18, n4, p97-136 </journal>
Reference-contexts: 1 Introduction Stacking [20] is a widely known technique to combine classifiers <ref> [7] </ref>. Empirical studies have shown that stacking helps to increase both accuracy and efficiency. Many papers in recent years have concentrated on using various metrics, coverage [2], diversity [2, 4], correlated error [1] and specialty [4], to explain stacking and choose classifiers for combining.
Reference: [8] <author> T. Dietterich and G. </author> <title> Bakiri Solving Multiclass Learning Problems via Error-correcting Output Codes Journal of Aritificial Intelligence Research, </title> <booktitle> 2 </booktitle> <pages> 263-286 </pages>
Reference-contexts: We call the * portion data conflicts. Conflicts were first discussed in [10, 19]. (The reader is advised not to confuse conflicts with Dietterich and Bakiri's error-correction output code to handle multiclass classification problems <ref> [8] </ref>). In a companion paper [11], we propose Recursive-Stacking to reduce conflicts and improve accuracy for a stacked system with a fixed number of base classifiers. In a third paper [12], we propose using conflicts to select classifiers and prune a stacked structure.
Reference: [9] <author> C. </author> <note> Elkan Boosted Naive Bayes submitted for publication, </note> <year> 1997 </year>
Reference: [10] <author> D. Fan,P. Chan and S. </author> <title> Stolfo A Comparative Evaluation of Combiner and Stacked Generalization Integrating Multiple Learning Model Workshop, </title> <booktitle> AAAI-96, </booktitle> <address> Portland, Oregon </address>
Reference-contexts: We assume that the training data and testing data are of the same distribution; this means that the * portion will always be misclassified. We call the * portion data conflicts. Conflicts were first discussed in <ref> [10, 19] </ref>. (The reader is advised not to confuse conflicts with Dietterich and Bakiri's error-correction output code to handle multiclass classification problems [8]). In a companion paper [11], we propose Recursive-Stacking to reduce conflicts and improve accuracy for a stacked system with a fixed number of base classifiers.
Reference: [11] <author> D. Fan, S. Stolfo and Philip K. </author> <title> Chan Recursive-Stacking to Improve the Accuracy of Combined Classifiers to be submitted </title>
Reference-contexts: We call the * portion data conflicts. Conflicts were first discussed in [10, 19]. (The reader is advised not to confuse conflicts with Dietterich and Bakiri's error-correction output code to handle multiclass classification problems [8]). In a companion paper <ref> [11] </ref>, we propose Recursive-Stacking to reduce conflicts and improve accuracy for a stacked system with a fixed number of base classifiers. In a third paper [12], we propose using conflicts to select classifiers and prune a stacked structure.
Reference: [12] <author> D. Fan, S. Stolfo and Philip K. </author> <title> Chan Using Conflicts To Select Classifiers and Prune Stacking System to be submitted </title>
Reference-contexts: In a companion paper [11], we propose Recursive-Stacking to reduce conflicts and improve accuracy for a stacked system with a fixed number of base classifiers. In a third paper <ref> [12] </ref>, we propose using conflicts to select classifiers and prune a stacked structure. In this paper, we concentrate on measuring performance of a stacked classifier system based on conflicts.
Reference: [13] <author> C. </author> <note> Grammes GNUFIT ftp://ftp,dartmouth.edu/oub/gnuplot/gnufit12.tar.gz </note>
Reference-contexts: We didn't use their average in order to show the full spread of the points. We used polynomials to fit the data points to uncover any trend. We used the Marquardt-Levenberg algorithm [17] (a non-linear least square fitting procedure, available in the GNUFIT package <ref> [13] </ref>) for this purpose. The sum of residuals usually stabilized at a quadratic fit. Some plots have very scattered data points and the residual of fitting is very large. This implies that there is hardly any trend.
Reference: [14] <author> S. </author> <title> Hashem Optimal Linear Combination of Neural Networks. </title> <type> Phd Thesis, </type> <institution> Purdue University, School of Industrial Engineering, Lafayette, IN, </institution> <year> 1993 </year>
Reference: [15] <author> M.I. Jordan and R.A. </author> <title> Jacob Hierarchical Mixtures of Experts and the EM Algorithm. Neural Computation, </title> <type> 6(2), </type> <institution> p.181 p.214. </institution>
Reference: [16] <author> A. Prodromidis, S. Stolfo and P. </author> <note> Chan Pruning Classifiers in a Distributed Meta-learning System submitted for publication </note>
Reference-contexts: We anticipate 6 the accuracy improvement to increase with the specialty metric. On close inspection, the metric is flawed. For example, if a classifier always predicts one class, it has a high specialty value. An improved specialty-based metric is proposed in <ref> [16] </ref>. 4 Discussion and Conclusion The CB-accuracy estimate and CB-accuracy improvement estimate metrics are effective in predicting the performance of a stacked classifier, when the conflict ratio * of each type are different than 0.5. But they are inaccurate when the ratios are close to 0.5.
Reference: [17] <author> A. Ralston and P. </author> <title> Rabinowitz A First Course in Numerical Analysis McGrawl Hill, </title> <address> New York, New York, </address> <year> 1978 </year>
Reference-contexts: We plot all the original data points in the figures that follow. We didn't use their average in order to show the full spread of the points. We used polynomials to fit the data points to uncover any trend. We used the Marquardt-Levenberg algorithm <ref> [17] </ref> (a non-linear least square fitting procedure, available in the GNUFIT package [13]) for this purpose. The sum of residuals usually stabilized at a quadratic fit. Some plots have very scattered data points and the residual of fitting is very large. This implies that there is hardly any trend.
Reference: [18] <author> S. Stolfo, A. Prodromidis, S. Tselepsis, W. Lee, D. Fan and P. Chan JAM: </author> <title> Java Agents for Meta-learning over Distributed Databases. In Prod. </title> <booktitle> Third Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <year> 1997. </year>
Reference: [19] <author> S. Stolfo, D. Fan, W. Lee, A. Prodromidis and P. </author> <title> Chan Credit Card Fraud Detection Using Meta-learning: Issues and Initial Results. </title> <booktitle> In Working Notes AAAI-97, </booktitle> <year> 1997 </year>
Reference-contexts: We assume that the training data and testing data are of the same distribution; this means that the * portion will always be misclassified. We call the * portion data conflicts. Conflicts were first discussed in <ref> [10, 19] </ref>. (The reader is advised not to confuse conflicts with Dietterich and Bakiri's error-correction output code to handle multiclass classification problems [8]). In a companion paper [11], we propose Recursive-Stacking to reduce conflicts and improve accuracy for a stacked system with a fixed number of base classifiers. <p> We also ran experiments on two real world data sets, First Union Credit Card Transaction Data and Chase Credit Card Transaction Data. The target classification of the data is either legitimate or fraudulent. For a description of the data schema, refer to <ref> [19] </ref>. From each bank, we obtained 0.5 million data spanning a whole year. The First Union data was not uniformally sampled for each month. The percentage of fraud ranges from 4% to over 20% over each month and the size of data for each month varies.
Reference: [20] <author> D. </author> <title> Wolpert Stacked Generalization. Neural Networks, 5(2), p.241-p.260. 8 Normalized CB-Accuracy Estimate Correlated Error Diversity Specialty 9 Normalized CB-Accuracy Improvement Estimate Correlated Error Diversity Specialty 10 </title>
Reference-contexts: 1 Introduction Stacking <ref> [20] </ref> is a widely known technique to combine classifiers [7]. Empirical studies have shown that stacking helps to increase both accuracy and efficiency. <p> Experimental results demonstrate that conflict-based accuracy estimate is an effective measure to predict performance and compare different stacked systems. 2 Conflict and Measures 2.1 Stacking and Its Conflict Problem Following Wolpert <ref> [20] </ref>, the general scheme for stacking works as follows. We have t different algorithms A 1 ,. . . ,A t and a set S of training examples f (x 1 ; y 1 ); : : : ; (x m ; y m )g. <p> Their predictions form a meta-level testing item (C 1 (x); :::; C t (x)) that is fed into the meta-classifier. The meta-classifier's prediction M C ((C 1 (x); : : : ; C t (x)) is the final outcome. Here, we use different terminology from <ref> [20] </ref>. Classifier is the same as generalizer. Level-0 is base level and level-1 is meta-level. For simplicity, we consider bimodal problems (2 classifications, 0 and 1 ).
References-found: 20

