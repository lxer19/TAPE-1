URL: http://phobos.cs.ucdavis.edu:8001/papers/jeremygs.ps.gz
Refering-URL: http://phobos.cs.ucdavis.edu:8001/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: frank@cs.ucdavis.edu  
Phone: (916) 758-5925  
Title: A Study of Genetic Algorithms to Find Approximate Solutions to Hard 3CNF Problems  
Author: Jeremy Frank 
Date: June 3, 1994  
Address: Davis, CA. 95616  
Affiliation: Division of Computer Science University of California at Davis  
Abstract: Genetic algorithms have been used to solve hard optimization problems ranging from the Travelling Salesman problem to the Quadratic Assignment problem. We show that the Simple Genetic Algorithm can be used to solve an optimization problem derived from the 3-Conjunctive Normal Form problem. By separating the populations into small sub-populations, parallel genetic algorithms exploits the inherent parallelism in genetic algorithms and prevents premature convergence. Genetic algorithms using hill-climbing conduct genetic search in the space of local optima, and hill-climbing can be less com-putationally expensive than genetic search. We examine the effectiveness of these techniques in improving the quality of solutions of 3CNF problems. 
Abstract-found: 1
Intro-found: 1
Reference: [ChKa] <author> P. Cheeseman, Bob Kanefsky, William Taylor. </author> <title> "Where the Really Hard Problems Are." </title> <booktitle> Proceedings of the 12 th IJCAI, </booktitle> <year> 1991. </year>
Reference-contexts: We ran a variety of experiments varying each of the GA parameters available and measured the value and variance of the optima found on randomly generated problems. We tested problems of 25 variables. We tested the GAs on 100 randomly generated "hard" CNF problems <ref> [ChKa] </ref>. In order to analyze our results we measured the average number of terms solved by each algorithm.
Reference: [MiSe] <author> D. Mitchell, B. Selman, H. Levesque. </author> <title> "Hard and Easy distribution of SAT Problems." </title> <booktitle> Proceedings, AAAI, </booktitle> <year> 1992, </year> <pages> pp. 459-465 </pages>
Reference-contexts: Recent research has shown that, for randomly generated formulas, if the ratio of terms to variables is close to 4.3 then these problems are the most difficult to solve <ref> [MiSe] </ref>. Greedy local techniques, used to solve the optimization version of 3CNF, show improvement over even the best backtracking methods [Se], [SeKa]. However, there are problems which can fool greedy algorithms into finding local optima unless the algorithms are specially modified [SeKa].
Reference: [SeKa] <author> B. Selman, H. Kautz. </author> <title> "An Empirical Study of Greedy Local Search for Satisfiability Testing." </title> <booktitle> Proceedings, AAAI Spring Symposium on Artificial Intelligence and NP-Hard Problems, </booktitle> <year> 1993, </year> <pages> pp. 149-155. </pages>
Reference-contexts: Greedy local techniques, used to solve the optimization version of 3CNF, show improvement over even the best backtracking methods [Se], <ref> [SeKa] </ref>. However, there are problems which can fool greedy algorithms into finding local optima unless the algorithms are specially modified [SeKa]. This brings to mind the so-called "deceptive" functions which are shown to fool GAs into finding suboptimal solutions [Mu]. <p> Greedy local techniques, used to solve the optimization version of 3CNF, show improvement over even the best backtracking methods [Se], <ref> [SeKa] </ref>. However, there are problems which can fool greedy algorithms into finding local optima unless the algorithms are specially modified [SeKa]. This brings to mind the so-called "deceptive" functions which are shown to fool GAs into finding suboptimal solutions [Mu]. To improve the performance of GAs on deceptive problems researchers have introduced parallel genetic algorithms, (referred to generally as PGAs). PGAs are designed to enhance genetic diversity by separating sub-populations.
Reference: [Gl] <author> F. Glover. </author> <title> "Tabu Search for Nonlinear and Parametric Optimizations", </title> <journal> Discrete Applied Mathematics, </journal> <year> 1991 </year>
Reference-contexts: For our experiments we shall use steepest ascent hill-climbing; among the options to be explored later are tabu search <ref> [Gl] </ref>, simulated annealing and greedy local search [Se]. 2.2 The Parallel Genetic Algorithm Genetic algorithms are just as susceptible to premature convergence as other search procedures. To address this problem, genetic algorithms induce diversification throughout the lifetime of the algorithm using parallel populations which exchange individuals for genetic crossover [Mu].
Reference: [Go] <author> D. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <address> Menlo Park, </address> <publisher> Wiley and Sons, </publisher> <year> 1989. </year>
Reference-contexts: Successive generations are created by selecting solutions from the previous generation using a fitness function related to the objective function of the optimization problem. The new population is then subject to genetic operators such as crossover and mutation to extend the search to other parts of the search space <ref> [Go] </ref>. Genetic algorithms (hereafter called GAs) have been tested on a wide variety of problems, including the combi-natorially difficult problems in the class NP. A typical NP-Hard problem is the Boolean Satisfaction Problem, where we desire an assignment of boolean variables which satisfies a boolean formula.
Reference: [Go2] <author> D. Goldberg. </author> <title> "Sizing Populations for Serial and Parallel Genetic Algorithms." </title> <booktitle> Proceedings, 3d International Conference on Genetic Algorithms, </booktitle> <year> 1989. </year>
Reference-contexts: In order to analyze our results we measured the average number of terms solved by each algorithm. In many cases this average is skewed by problems which have satisfying assignments, so we also recorded the number of problems solved by each algorithm. 3.1 Population Size and Generations SGA Goldberg <ref> [Go2] </ref> presents a method of sizing populations and determining the number of generations to convergence for serial and parallel GAs. We used this work to find the optimum number of generations for SGA using both the model of constant time convergence and logarithmic time convergence.
Reference: [Ho] <author> J. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> Univ. of Michigan Press, </publisher> <year> 1975. </year> <month> 9 </month>
Reference-contexts: 1 Introduction The Genetic Algorithm proposed by Holland <ref> [Ho] </ref> has become the basis for a new approach to optimization problems. In a genetic algorithm, sets of solutions called populations are succeeded by new solutions which are closer to the desired optima.
Reference: [Se] <author> B. Selman. </author> <title> "A New Method for Solving Hard Satisfiability Problems." </title> <booktitle> Proceedings, AAAI, </booktitle> <year> 1992, </year> <pages> pp. 440-446. </pages>
Reference-contexts: Recent research has shown that, for randomly generated formulas, if the ratio of terms to variables is close to 4.3 then these problems are the most difficult to solve [MiSe]. Greedy local techniques, used to solve the optimization version of 3CNF, show improvement over even the best backtracking methods <ref> [Se] </ref>, [SeKa]. However, there are problems which can fool greedy algorithms into finding local optima unless the algorithms are specially modified [SeKa]. This brings to mind the so-called "deceptive" functions which are shown to fool GAs into finding suboptimal solutions [Mu]. <p> For our experiments we shall use steepest ascent hill-climbing; among the options to be explored later are tabu search [Gl], simulated annealing and greedy local search <ref> [Se] </ref>. 2.2 The Parallel Genetic Algorithm Genetic algorithms are just as susceptible to premature convergence as other search procedures. To address this problem, genetic algorithms induce diversification throughout the lifetime of the algorithm using parallel populations which exchange individuals for genetic crossover [Mu].
Reference: [MuSc] <author> H. Mulenbien, M. Schomisch, J. Born. </author> <title> "The Parallel Genetic Algorithm as a Function Optimizer." </title> <booktitle> Proceedings, 4th International Conference on Genetic Algorithms, </booktitle> <year> 1991, </year> <pages> pp. 271-278. </pages>
Reference-contexts: To improve some parallel algorithms, hill-climbing is introduced. Performing crossover on local optima gives better results than crossover on other points, and in cases hill-climbing improves individuals more quickly than the genetic operators <ref> [MuSc] </ref>. In this paper we will construct several GAs to solve the optimization 2 version of the 3CNF problem. We shall analyze these GAs and investigate the settings of parameters such as crossover probability, mutation probability, migration rate, migration interval and population parameters.
Reference: [Mu] <author> H. Mulenbien. </author> <title> "Evolution in Time and Space: The Parallel Genetic Algorithm." In Foundations of Genetic Algorithms, </title> <editor> G. Rawlins, ed., </editor> <publisher> Mor-gan Kaufman, </publisher> <year> 1991. </year>
Reference-contexts: However, there are problems which can fool greedy algorithms into finding local optima unless the algorithms are specially modified [SeKa]. This brings to mind the so-called "deceptive" functions which are shown to fool GAs into finding suboptimal solutions <ref> [Mu] </ref>. To improve the performance of GAs on deceptive problems researchers have introduced parallel genetic algorithms, (referred to generally as PGAs). PGAs are designed to enhance genetic diversity by separating sub-populations. <p> To address this problem, genetic algorithms induce diversification throughout the lifetime of the algorithm using parallel populations which exchange individuals for genetic crossover <ref> [Mu] </ref>. These algorithms are known as parallel genetic algorithms or PGAs. Wright argues that for PGAs to avoid local optima a PGA should be divided into isolated sub-populations, one of which will eventually take over the others [Mu]. <p> lifetime of the algorithm using parallel populations which exchange individuals for genetic crossover <ref> [Mu] </ref>. These algorithms are known as parallel genetic algorithms or PGAs. Wright argues that for PGAs to avoid local optima a PGA should be divided into isolated sub-populations, one of which will eventually take over the others [Mu]. We take this view 3 in designing a PGA to solve 3CNF problems. Accordingly, the PGA is di-vided into sub-populations arranged in a ring [Mu]. Each subpopulation sends a small number of randomly picked individuals to it's neighbors after a number of generations [Ta]. <p> argues that for PGAs to avoid local optima a PGA should be divided into isolated sub-populations, one of which will eventually take over the others <ref> [Mu] </ref>. We take this view 3 in designing a PGA to solve 3CNF problems. Accordingly, the PGA is di-vided into sub-populations arranged in a ring [Mu]. Each subpopulation sends a small number of randomly picked individuals to it's neighbors after a number of generations [Ta]. These individuals randomly replace members of the populations they are sent to.
Reference: [Mu2] <author> H. Mulenbien. </author> <title> "Asynchronous Parallel Search by the Parallel Genetic Algorithm." </title> <booktitle> Proceedings, 3d IEEE Symposium on Parallel and Distributed Systems, </booktitle> <year> 1992, </year> <pages> pp. 526-533. </pages>
Reference-contexts: To improve the performance of GAs on deceptive problems researchers have introduced parallel genetic algorithms, (referred to generally as PGAs). PGAs are designed to enhance genetic diversity by separating sub-populations. In some PGAs individuals are separated and only allowed to mate with certain neighboring individuals <ref> [Mu2] </ref>, while in other algorithms sub-populations are allowed to exchange individuals after a certain time period has elapsed [Ta]. To improve some parallel algorithms, hill-climbing is introduced. <p> It may also be less expensive to use hill-climbing to do some of the work of the GA, thereby reducing the population size and/or the number of generations required to converge <ref> [Mu2] </ref>. For our experiments we shall use steepest ascent hill-climbing; among the options to be explored later are tabu search [Gl], simulated annealing and greedy local search [Se]. 2.2 The Parallel Genetic Algorithm Genetic algorithms are just as susceptible to premature convergence as other search procedures.
Reference: [LiBa] <author> G. Liepens, S. Baluja. "apGA: </author> <title> An Adaptive Parallel Genetic Algorithm." In Computer Science and Operations Research, New Developments in Their Interfaces, </title> <editor> Balci, Sharda, Zeinos, ed., </editor> <publisher> Pergamon Press, </publisher> <year> 1992. </year>
Reference: [Ta] <author> R. Tanese. </author> <title> "Distributed Genetic Algorithms." </title> <booktitle> Proceedings, 3d International Conference on Genetic Algorithms, </booktitle> <year> 1989. </year> <month> 10 </month>
Reference-contexts: PGAs are designed to enhance genetic diversity by separating sub-populations. In some PGAs individuals are separated and only allowed to mate with certain neighboring individuals [Mu2], while in other algorithms sub-populations are allowed to exchange individuals after a certain time period has elapsed <ref> [Ta] </ref>. To improve some parallel algorithms, hill-climbing is introduced. Performing crossover on local optima gives better results than crossover on other points, and in cases hill-climbing improves individuals more quickly than the genetic operators [MuSc]. <p> We take this view 3 in designing a PGA to solve 3CNF problems. Accordingly, the PGA is di-vided into sub-populations arranged in a ring [Mu]. Each subpopulation sends a small number of randomly picked individuals to it's neighbors after a number of generations <ref> [Ta] </ref>. These individuals randomly replace members of the populations they are sent to. By keeping the rate of migration small and the intervals of migration reasonably large, we maintain diversity in the sub-populations, but allow opportunities for improvement by exchanging individuals.
References-found: 13

