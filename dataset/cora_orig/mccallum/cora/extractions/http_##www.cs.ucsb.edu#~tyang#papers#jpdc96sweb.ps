URL: http://www.cs.ucsb.edu/~tyang/papers/jpdc96sweb.ps
Refering-URL: http://www.cs.ucsb.edu/Research/rapid_sweb/SWEB.html
Root-URL: http://www.cs.ucsb.edu
Email: fdandrese, tyang, ibarrag@cs.ucsb.edu  
Title: Towards a Scalable Distributed WWW Server on Workstation Clusters  
Author: Daniel Andresen Tao Yang Oscar H. Ibarra 
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California  
Abstract: In this paper, we investigate the issues involved in developing a scalable World Wide Web (WWW) server called SWEB on a cluster of workstations. The objective is to strengthen the processing capabilities of such a server in order to match huge demands in simultaneous access requests from the Internet, especially when these requests involve delivery of large digitized documents. The scheduling component of the system actively monitors the usages of CPU, disk I/O channels and the interconnection network to effectively distribute HTTP requests across processing units to exploit task and I/O parallelism. We analyze the maximum number of requests that can be handled by the system and present several experiments to examine the performance of this system.
Abstract-found: 1
Intro-found: 1
Reference: [A96] <institution> The Alexandria Digital Library Project, </institution> <note> http://alexandria.sdc.ucsb.edu/. </note>
Reference-contexts: 1 Motivation The Scalable Web server (SWEB) project grew out of the needs of the Alexandria Digital Library (ADL) project at UCSB <ref> [A96] </ref>. Digital library systems, which provide the on-line retrieval and processing of digitized documents through Internet, have increasingly turned into a topic of national importance.
Reference: [APB96] <author> E. Anderson, D. Patterson, E. Brewer, </author> <title> "The Magicrouter, an Application of Fast Packet Interposing", </title> <booktitle> submitted to the Second Symposium on Operating System Design and Implementation (OSDI96), </booktitle> <year> 1996. </year> <note> Also available at http://HTTP.CS.Berkeley.EDU/~eanders/262/262paper.ps. </note>
Reference-contexts: We did not take this approach mainly because the single central distributor becomes a single point of failure, making the entire system more vulnerable, unless significant low-level precautions are taken <ref> [APB96] </ref>. The current version of SWEB uses a distributed scheduler. The user requests are first evenly routed to SWEB processors via DNS rotation, as indicated in Figure 2. The rotation on available workstation network IDs is in a round-robin fashion. This functionality is available in current DNS systems. <p> Two approaches, URL redirection or request forwarding, could be used to achieve reassignment and we use the former. Request forwarding, which would involve transferring the TCP/IP connection to another server without the client's knowledge, is difficult to implement within HTTP <ref> [APB96] </ref>. URL redirection gives us excellent compatibility with current browsers and near-invisibility to users. Any HTTP request is not allowed to be redirected more than once to avoid the ping-pong effect. The functional structure of the scheduler at each processor is depicted in Fig. 3.
Reference: [AC95+] <author> D.Andresen, L.Carver, R.Dolin, C.Fischer, J.Frew, M.Goodchild, O.Ibarra, R.Kothuri, M.Larsgaard, B.Manjunath, D.Nebert, J.Simpson, T.Smith, T.Yang, Q.Zheng, </author> <title> "The WWW Prototype of the Alexandria Digital Library", </title> <booktitle> Proceedings of ISDL'95: International Symposium on Digital Libraries, </booktitle> <address> Japan August 22 - 25, </address> <year> 1995. </year>
Reference: [AY95+] <author> D.Andresen, T.Yang, V.Holmedahl, O.Ibarra, "SWEB: </author> <title> Towards a Scalable World Wide Web Server on Multicomputers", </title> <institution> Dept. of Computer Science Tech Rpt. TRCS95-17 U.C. Santa Barbara, </institution> <month> Sept., </month> <year> 1995, </year> <title> http://www.cs.ucsb.edu/ Research/rapid sweb/SWEB.html. A short version appears in Proc. </title> <booktitle> of 10th IEEE International Symp. on Parallel Processing (IPPS'96), </booktitle> <pages> pp. 850-856, </pages> <month> April, </month> <year> 1996. </year>
Reference: [SHK95] <author> B. A. Shirazi, A. R. Hurson, and K. M. Kavi (Eds), </author> <title> Scheduling and Load Balancing in Parallel and Distributed Systems, </title> <publisher> IEEE CS Press, </publisher> <year> 1995. </year>
Reference-contexts: Our dynamic scheduling scheme is closely related to the previous work on load balancing on distributed systems, for which a collection of papers is available in <ref> [SHK95] </ref>. In these studies, tasks arrivals may temporarily be uneven among processors and the goal of load balancing is to adjust the imbalance between processors by appropriately transferring tasks from overloaded processors to underloaded processors. <p> In a single-faceted scheduling system, a processor can be classified as lightly loaded and heavily loaded based on one parameter, e.g. CPU load. One purpose of such a classification is to update load information only when a classification changes to reduce unnecessary overhead, e.g. <ref> [SHK95] </ref>. In our problem context, it is hard to classify a processor as heavily or lightly loaded since there are several load parameters. A processor could have a light CPU load but its local disk may receive many access requests from the network file system. <p> To avoid this unsynchronized overloading, we conservatively increase the CPU load of p x by . This strategy is found to be effective in <ref> [SHK95] </ref>. We use = 30%. * t net = # bytes required net bandwidth This term is used to estimate the time necessary to return the results back to the client over the network.
Reference: [BR96] <author> E. Brewer, </author> <type> Personal communication, </type> <address> http://inktomi.berkeley.edu, Jan., </address> <year> 1996. </year>
Reference-contexts: Numerous other initiatives to create high-performance HTTP servers have been reported. The Inktomi server at UC Berkeley is based on the NOW technology <ref> [BR96] </ref>. NCSA [KBM94] has built a multi-workstation HTTP server based on round-robin domain name resolution (DNS) to assign requests to workstations. The round-robin technique is effective when HTTP requests access HTML information of relatively uniform size and the load and computing powers of workstations are relatively comparable.
Reference: [FKC95+] <author> C. Fu, M. Kim, D. Chang, O. H. Ibarra, T. Yang, </author> <title> Performance Evaluation of Parallel I/O on the Meiko CS-2. </title> <type> Tech Report, </type> <month> UCSB </month> <year> 1995. </year> <month> 15 </month>
Reference-contexts: For the Meiko CS-2, the disk reading performance is affected by the disk file caching. It is shown in <ref> [FKC95+] </ref> that the local disk read bandwidth is about b 1 = 5M B=s for a set of files with size 1.5 MB and the remote disk read bandwidth is about b 2 = 4:5M B=s. Our experiments on the Meiko show that d is about 0:10.
Reference: [GDI93] <author> K. Goswami, M. Devarakonda, R. Iyer, </author> <title> Prediction-based Dynamic Load-sharing Heuristics, </title> <journal> IEEE Trans--actions on Parallel and Distributed Systems, </journal> <volume> vol. 4, no. 6, </volume> <pages> pp. 638-648, </pages> <month> June, </month> <year> 1993. </year>
Reference-contexts: The optimal HTTP request assignment to processors does not solely depend on CPU loads. Thus we need to develop a multi-faceted scheduling scheme that can effectively utilize the system resources by considering the aggregate impact of multiple parameters on system performance. In <ref> [GDI93] </ref>, multiple resource requirements are predicted and suggested to guide the load sharing, but the authors utilize only the CPU factor in predicting response times in their analysis. The paper is organized as follows: Section 2 gives the background and problem definition.
Reference: [HL95+] <author> J. Hsieh, M. Lin, J. Liu, D. Du, </author> <title> Performance of A Mass Storage System for Video-On-Demand, </title> <journal> Special Issue of the Journal of Parallel and Distributed Computing on Multimedia Processing and Technology, Vol.30, No.2, </journal> <volume> pp.147-167, </volume> <month> November, </month> <year> 1995. </year>
Reference-contexts: It should be noted that WWW applications only represent a special class of Internet information systems. There are other information servers dealing with huge file sizes and large numbers of users, for example multi-media servers <ref> [HL95+] </ref>. Our situation has several differences. First, our current system has no real-time processing constraints for displaying digital movies or audio clips. Secondly, many of our users tend to browse different text and images information, rather than focusing on one particular document such as one film beginning to end.
Reference: [HT95] <author> Hypertext Transfer Protocol(HTTP): </author> <title> A protocol for networked information, </title> <address> http://www.w3.org/hypertext/WWW/Protocols/, June 26, </address> <year> 1995. </year>
Reference-contexts: The URL defines which resource the user wishes to access, the HTML language allows the information to be presented in a platform-independent but still well-formatted manner, and the HTTP protocol is the application-level mechanism for achieving the transfer of information <ref> [HT95] </ref>. 2 A simple HTTP request would typically activate a sequence of events from initiation to completion as shown in server to determine its IP address.
Reference: [LYC95] <author> Lycos Usage: </author> <title> Accesses per Day, </title> <address> http://lycos.cs.cmu.edu/usage-day.html. </address>
Reference-contexts: Our work is motivated by the fact that the Alexandria digital library WWW server has a potential to become the bottleneck in delivering large digitized documents over high-speed Internet connections. Popular WWW sites such as the Lycos and AltaV ista <ref> [LYC95] </ref> receive three to ten million accesses a day. For WWW-based network information systems such as digital libraries, the servers involve much more intensive I/O and heterogeneous CPU activities than simple file servers..
Reference: [KBM94] <author> E.D. Katz, M. Butler, R. McGrath, </author> <title> A Scalable HTTP Server: the NCSA Prototype, </title> <journal> Computer Networks and ISDN Systems. </journal> <volume> vol. 27, </volume> <year> 1994, </year> <pages> pp. 155-164. </pages>
Reference-contexts: Numerous other initiatives to create high-performance HTTP servers have been reported. The Inktomi server at UC Berkeley is based on the NOW technology [BR96]. NCSA <ref> [KBM94] </ref> has built a multi-workstation HTTP server based on round-robin domain name resolution (DNS) to assign requests to workstations. The round-robin technique is effective when HTTP requests access HTML information of relatively uniform size and the load and computing powers of workstations are relatively comparable. <p> For example, the NCSA has performed a number of tests using high-end workstations, and discovered in their working environment approximately 5-10 RPS could be dealt with using the NCSA httpd server <ref> [KBM94] </ref>, which cannot match the current and future loads (e.g. a digital library server). Thus multiple servers are needed for achieving scalable performance. Our overall objective is to reduce and sustain the response time under large numbers of simultaneous requests. Goals considered in designing this system are twofold. <p> The rotation on available workstation network IDs is in a round-robin fashion. This functionality is available in current DNS systems. The major advantages of this technique are simplicity, ease of implementation, and reliability <ref> [KBM94] </ref>. The DNS assigns the requests without consulting dynamically-changing system load information. Then SWEB conducts a further assignment of requests. Each processor in SWEB contains a scheduler and those processors collaborate with each other to exchange system load information. <p> The first experiment was run to determine how many requests per second SWEB could process. This depends on the average file sizes requested and the number of nodes. In <ref> [KBM94] </ref>, it is reported that a high-end workstation running NCSA httpd could fulfill approximately 5 RPS. We examine how a one-node NCSA httpd 1.3 server performs, and compare it with the 6-node SWEB on Sparc/Elan and the 4-node SWEB on Sparc/Ethernet.
Reference: [MFM95] <author> D. Mosedale, W. Foss, R. McCool, </author> <title> "Administering Very High Volume Internet Services", </title> <booktitle> Proc. of 1995 LISA IX, </booktitle> <address> Monterey, CA, </address> <month> September, </month> <year> 1995. </year>
Reference-contexts: DNS caching enables a local DNS system to 1 cache the name-to-IP address mapping, so that most recently accessed hosts can quickly be mapped. The downside is that all requests for a period of time from a DNS server's domain will go to a particular IP address <ref> [MFM95] </ref>. It should be noted that WWW applications only represent a special class of Internet information systems. There are other information servers dealing with huge file sizes and large numbers of users, for example multi-media servers [HL95+]. Our situation has several differences. <p> It should be noted that in practice requests come in periodic bursts. Thus we use a 30-second test period in the rest of experiments, representing a non-trivial but limited burst of requests <ref> [MFM95] </ref>. Response time and drop rate. In Table 2, we report the response time (the time from after the client sends a request until the completion of this request) when we vary the number of server nodes. The system starts to drop requests if the server reaches its RPS limit.
Reference: [WA95+] <author> R. Wolski, C. Anglano, J. Schopf, F. Berman, </author> <title> Developing Heterogeneous Applications Using Zoom and HeNCE, </title> <booktitle> Proceedings of the Heterogeneous Computing Workshop, </booktitle> <volume> HCW '95, </volume> <pages> pp. 12-21, </pages> <address> Santa Barbara, CA, </address> <publisher> IEEE, </publisher> <month> April, </month> <year> 1995. </year> <month> 16 </month>
Reference-contexts: 1.5M (%) Request Parsing 70 4.4 Server Analysis (SWEB) 0.6 &lt; 0.01 Load monitoring (SWEB) &lt; 0.2 &lt; 0.2 Redirection (SWEB) &lt; 0.01 &lt; 0.01 Processing/fulfillment 28 95.5 We are currently migrating the system to be the primary server for the ADL and investigating its performance in a heterogeneous environment <ref> [WA95+] </ref>. Acknowledgments This work was supported in part by funding from NSF IRI94-11330 and CDA-9529418, and a Navy NRaD grant. We would like to thank Omer Egecioglu, Terry Smith, Cong Fu and the Alexandria Digital Library team for many valuable discussions, Vegard Holmedahl for programming and debugging the system.
References-found: 14

