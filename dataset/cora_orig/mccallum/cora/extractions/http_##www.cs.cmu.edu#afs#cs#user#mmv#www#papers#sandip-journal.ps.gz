URL: http://www.cs.cmu.edu/afs/cs/user/mmv/www/papers/sandip-journal.ps.gz
Refering-URL: http://www.cs.cmu.edu/~mmv/produce-bib.html
Root-URL: 
Email: fpstone,velosog@cs.cmu.edu  
Title: Towards Collaborative and Adversarial Learning: A Case Study in Robotic Soccer  
Author: Peter Stone and Manuela Veloso 
Keyword: Running title: Soccer: Collaborative and Adversarial  
Web: http://www.cs.cmu.edu/fpstone,mmvg  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Soccer is a rich domain for the study of multiagent learning issues. Not only must the players learn low-level skills, but they must also learn to work together and to adapt to the behaviors of different opponents. We are using a robotic soccer system to study these different types of multiagent learning: low-level skills, collaborative, and adversarial. Here we describe in detail our experimental framework. We present a learned, robust, low-level behavior that is necessitated by the multiagent nature of the domain, namely shooting a moving ball. We then discuss the issues that arise as we extend the learning scenario to require collaborative and adversarial learning.
Abstract-found: 1
Intro-found: 1
Reference: <author> Achim, S., Stone, P., & Veloso, M. </author> <year> (1996). </year> <title> Building a dedicated robotic soccer system. </title> <booktitle> In Working Notes of the IROS-96 Workshop on RoboCup. </booktitle>
Reference-contexts: Modeled closely after the Dynamo system, the authors have also developed a real-world robotic soccer system <ref> (Achim, Stone, & Veloso, 1996) </ref>. The main differences from the Dynamo system are that the robots are much smaller, there are five on each team, and they use Infra-red communication rather than radio frequency. <p> The behavior is specifically designed to be useful for more complex multiagent behaviors as described in Section 5. 3.2 The Simulator Although we are pursuing research directions with our real robotic system <ref> (Achim et al., 1996) </ref>, the simulator facilitates extensive training and testing of learning methods.
Reference: <author> Asada, M., Noda, S., Tawaratsumida, S., & Hosoda, K. </author> <year> (1994a). </year> <title> Purposive behavior acquisition on a real robot by vision-based reinforcement learning. </title> <booktitle> In Proc. of MLC-COLT (Machine Learning Confernce and Computer Learning Theory) Workshop on Robot Learning, </booktitle> <pages> pp. 19. </pages>
Reference-contexts: Asada's robots are larger and are equipped with on-board sensing capabilities. They have been used to develop some low-level behaviors such as shooting and avoiding as well as a RL technique for combining behaviors <ref> (Asada et al., 1994a, 1994b) </ref>. By reducing the state space significantly, he was able to use RL to learn to shoot 5 a stationary ball into a goal. His best result in simulation is a 70% scoring rate.
Reference: <author> Asada, M., Uchibe, E., Noda, S., Tawaratsumida, S., & Hosoda, K. </author> <year> (1994b). </year> <title> Coordination of multiple behaviors acquired by vision-based reinforcement learning. </title> <booktitle> In Proc. of IEEE/RSJ/GI International Conference on Intelligent Robots and Systems 1994 (IROS '94), </booktitle> <pages> pp. 917924. </pages>
Reference-contexts: His best result in simulation is a 70% scoring rate. He has also done some work on combining different learned behaviors with a separate learned decision mechanism on top <ref> (Asada et al., 1994b) </ref>. While the goals of this research are very similar to our own, the approach is different.
Reference: <author> Clouse, J. A. </author> <year> (1996). </year> <title> Learning from an automated training agent. </title> <editor> In Wei, G., & Sen, S. (Eds.), </editor> <title> Adaptation and Learning in Multiagent Systems. </title> <publisher> Springer Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: This example represents a class of multiagent learning in which a learning agent attempts to model other agents. One final example of multiagent learning in which only one of the agents learns is a training scenario in which a novice agent learns from a knowledgeable agent <ref> (Clouse, 1996) </ref>. The novice learns to drive on a simulated race track from an expert agent whose behavior is fixed. The one thing that all of the above learning systems have in common is that the learning agent is interacting with other agents.
Reference: <author> Ford, R., Boutilier, C., & Kanazawa, K. </author> <year> (1994). </year> <title> Exploiting natural structure in reinforcement learning: Experience in robot soccer-playing.. </title> <type> Unpublished Manuscript. </type> <note> 26 Kaelbling, </note> <author> L. P., Littman, M. L., & Moore, A. W. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4, </volume> <pages> 237285. </pages>
Reference-contexts: Several researchers have previously used simulated robotic soccer to study ML applications. Using the Dynasim soccer simulator (Sahota, 1996, 1993), Ford et al. used a Reinforcement Learning (RL) approach with sensory predicates to learn to choose among low-level behaviors <ref> (Ford, Boutilier, & Kanazawa, 1994) </ref>. Using the simulator described below, Stone and Veloso used Memory-based Learning to allow a player to learn when to shoot and when to pass the ball (Stone & Veloso, 1996a).
Reference: <author> Kim, J.-H. </author> <year> (1996). </year> <title> Third call for participation: Micro-robot world cup soccer tournament (mirosot'96).. </title> <note> Accessible from http://vivaldi.kaist.ac.kr/. </note>
Reference: <author> Kitano, H., Asada, M., Kuniyoshi, Y., Noda, I., & Osawa, E. </author> <year> (1995). </year> <title> Robocup: The robot world cup initiative. </title> <booktitle> In IJCAI-95 Workshop on Entertainment and AI/Alife, </booktitle> <pages> pp. </pages> <address> 1924 Montreal, Quebec. </address>
Reference: <author> Latombe, J.-C. </author> <year> (1991). </year> <title> A fast path planner for a car-like indoor mobile robot. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 659665. </pages>
Reference: <author> Matsubara, H., Noda, I., & Hiraki, K. </author> <year> (1996). </year> <title> Learning of cooperative actions in multi-agent systems: a case study of pass play in soccer. In Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <pages> pp. </pages> <address> 6367 Menlo Park,CA. </address> <publisher> AAAI Press. AAAI Technical Report SS-96-01. </publisher>
Reference-contexts: In the RoboCup Soccer Server (Noda, 1995) Matsubar et al. also used a neural network to allow a player to learn when to shoot and when to pass <ref> (Matsubara, Noda, & Hiraki, 1996) </ref>. The work described in this article builds on previous work by learning a more difficult behavior: shooting a moving ball into a goal.
Reference: <author> Noda, I. </author> <year> (1995). </year> <title> Soccer server : a simulator of robocup. </title> <booktitle> In Proceedings of AI symposium '95, </booktitle> <pages> pp. </pages> <month> 2934. </month> <journal> Japanese Society for Artificial Intelligence. </journal>
Reference-contexts: Using the simulator described below, Stone and Veloso used Memory-based Learning to allow a player to learn when to shoot and when to pass the ball (Stone & Veloso, 1996a). In the RoboCup Soccer Server <ref> (Noda, 1995) </ref> Matsubar et al. also used a neural network to allow a player to learn when to shoot and when to pass (Matsubara, Noda, & Hiraki, 1996). <p> However, in a game situation, given the positioning of all the other players on the field, the receiver of a pass must choose where it should send the ball next. In a richer and more widely used simulator environment <ref> (Noda, 1995) </ref>, the authors have used decision tree learning to enable a passer to choose from among possible receivers in the presence of defenders (Stone & Veloso, 1996b).
Reference: <author> Press, W. H. </author> <year> (1988). </year> <title> Numerical Recipes in C: </title> <booktitle> the art of scientific computing, </booktitle> <pages> pp. 710713. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge. </address>
Reference: <author> Reeds, J. A., & Shepp, R. A. </author> <year> (1991). </year> <title> Optimal paths for a car that goes both forward and backward.. </title> <journal> Pacific Journal of Mathematics, </journal> <volume> 145(2), </volume> <pages> 367393. </pages>
Reference: <author> Sahota, M. </author> <year> (1994). </year> <type> Personal correspondence.. </type>
Reference-contexts: In particular, the simulator's code is adapted from their own code, for which we thank Michael Sahota whose work (Sahota, 1993) and personal correspondence <ref> (Sahota, 1994) </ref> has been motivating and invaluable. The simulator facilitates the control of any number of agents and a ball within a designated playing area. Care has been taken to ensure that the simulator models real-world responses (friction, conservation of momentum, etc.) as closely as possible.
Reference: <author> Sahota, M. </author> <year> (1996). </year> <title> Dynasim user guide. </title> <note> Available at http://www.cs.ubc.ca/nest/lci/soccer. 27 Sahota, </note> <author> M. K. </author> <year> (1993). </year> <title> Real-time intelligent behaviour in dynamic environments: Soccer-playing robots.. </title> <type> Master's thesis, </type> <institution> University of British Columbia. </institution>
Reference-contexts: Several researchers have previously used simulated robotic soccer to study ML applications. Using the Dynasim soccer simulator <ref> (Sahota, 1996, 1993) </ref>, Ford et al. used a Reinforcement Learning (RL) approach with sensory predicates to learn to choose among low-level behaviors (Ford, Boutilier, & Kanazawa, 1994).
Reference: <author> Sahota, M. K., Mackworth, A. K., Barman, R. A., & Kingdon, S. J. </author> <year> (1995). </year> <title> Real-time control of soccer-playing robots using off-board vision: the dynamite testbed. </title> <booktitle> In IEEE International Conference on Systems, Man, and Cybernetics, </booktitle> <pages> pp. 36903663. </pages>
Reference-contexts: This section briefly summarizes previous research and describes our research platform. 3.1 Related Work A ground-breaking system for Robotic Soccer, and the one that served as the inspiration for our work, is the Dynamo System developed at the University of British Columbia <ref> (Sahota, Mackworth, Barman, & Kingdon, 1995) </ref>. This system was designed to be capable of supporting several robots per team, but most work has been done in a 1 vs. 1 scenario. <p> All of the research reported in this article was conducted in simulation. 6 Both the simulator and the real-world system are based closely on systems designed by the Laboratory for Computational Intelligence at the University of British Columbia <ref> (Sahota et al., 1995) </ref>. In particular, the simulator's code is adapted from their own code, for which we thank Michael Sahota whose work (Sahota, 1993) and personal correspondence (Sahota, 1994) has been motivating and invaluable.
Reference: <author> Stone, P., & Veloso, M. </author> <year> (1996a). </year> <title> Beating a defender in robotic soccer: Memory-based learning of a continuous function. </title> <editor> In Touretzky, D. S., Mozer, M. C., & Hasselmo, M. E. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 8 Cambridge, </booktitle> <address> MA. </address> <publisher> MIT press. </publisher>
Reference-contexts: Using the simulator described below, Stone and Veloso used Memory-based Learning to allow a player to learn when to shoot and when to pass the ball <ref> (Stone & Veloso, 1996a) </ref>. In the RoboCup Soccer Server (Noda, 1995) Matsubar et al. also used a neural network to allow a player to learn when to shoot and when to pass (Matsubara, Noda, & Hiraki, 1996). <p> This extra option would further complicate the defender's behavior. Some related results pertaining to the decision of whether to pass or shoot appear in <ref> (Stone & Veloso, 1996a) </ref>. 6 Discussion and Conclusion Robotic Soccer is a rich domain for the study of multiagent learning issues. There are opportunities to study both collaborative and adversarial situations.
Reference: <author> Stone, P., & Veloso, M. </author> <year> (1996b). </year> <title> A layered approach to learning client behaviors in the robocup soccer server. </title> <note> Submitted to Applied Artificial Intelligence (AAI) Journal. </note>
Reference-contexts: In a richer and more widely used simulator environment (Noda, 1995), the authors have used decision tree learning to enable a passer to choose from among possible receivers in the presence of defenders <ref> (Stone & Veloso, 1996b) </ref>.
Reference: <author> Stone, P., & Veloso, M. </author> <year> (1996c). </year> <title> Multiagent systems: A survey from a machine learning perspective. </title> <journal> Submitted to IEEE Transactions on Knowledge and Data Engineering (TKDE). </journal>
Reference-contexts: Examples of traditional Machine Learning tasks include function approximation, classification, and problem-solving performance improvement given empirical data. Meanwhile, the subfield of Multiagent Systems, as surveyed in <ref> (Stone & Veloso, 1996c) </ref>, deals with domains having multiple agents and considers mechanisms for the interaction of independent agents' behaviors. Thus, multiagent learning includes any situation in which an agent learns to interact with other agents, even if the other agents' behaviors are static.
Reference: <author> Tan, M. </author> <year> (1993). </year> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. 330337. </pages>
Reference-contexts: There are some previous examples of single agents learning in a multiagent environment which are included in the multiagent learning literature. One of the earliest multiagent learning papers describes a reinforcement learning agent which incorporates information that is gathered by another agent <ref> (Tan, 1993) </ref>. It is considered multiagent learning because the learning agent has a cooperating agent and an adversary agent with which it learns to interact. Another such example is a negotiation scenario in which one agent learns the negotiating techniques of another using Bayesian Learning methods (Zeng & Sycara, 1996).
Reference: <author> Wei, G. </author> <year> (1995). </year> <title> Distributed reinforcement learning. </title> <booktitle> Robotics and Autonomous Systems, </booktitle> <volume> 15, </volume> <pages> 135142. </pages>
Reference-contexts: As described by Wei, it is learning that is done by several agents and that becomes possible only because several agents are present <ref> (Wei, 1995) </ref>. In fact, in certain circumstances, the first clause of this definition is not necessary. We claim 2 that it is possible to engage in multiagent learning even if only one agent is actually learning.
Reference: <author> Zeng, D., & Sycara, K. </author> <year> (1996). </year> <title> Bayesian learning in negotiation. In Adaptation, Coevolution and Learning in Multiagent Systems: </title> <booktitle> Papers from the 1996 AAAI Spring Symposium, </booktitle> <pages> pp. </pages> <address> 99104 Menlo Park,CA. </address> <publisher> AAAI Press. AAAI Technical Report SS-96-01. </publisher>
Reference-contexts: It is considered multiagent learning because the learning agent has a cooperating agent and an adversary agent with which it learns to interact. Another such example is a negotiation scenario in which one agent learns the negotiating techniques of another using Bayesian Learning methods <ref> (Zeng & Sycara, 1996) </ref>. Again, this situation is considered multiagent learning because the learning agent is learning to interact with another agent: the situation only makes sense due to the presence of multiple agents.
References-found: 21

