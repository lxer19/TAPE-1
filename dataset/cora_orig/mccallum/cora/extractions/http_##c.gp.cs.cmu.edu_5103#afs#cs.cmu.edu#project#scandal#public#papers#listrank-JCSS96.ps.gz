URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/scandal/public/papers/listrank-JCSS96.ps.gz
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/scandal/public/papers/listrank-spaa94.html
Root-URL: http://www.cs.cmu.edu
Email: mrmiller@cs.cmu.edu  
Phone: (412) 268-3056  
Title: List Ranking and List Scan on the Cray C90  
Author: Margaret Reid-Miller 
Note: The views and conclusions contained in this document are those of the author and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of Wright Laboratory or the U. S. Government.  
Address: Pittsburgh, PA 15213-3891  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: fl This research was sponsored in part by the Wright Laboratory, Aeronautical Systems Center, Air Force Materiel Command, USAF, and the Advanced Research Projects Agency (ARPA) under grant number F33615-93-1-1330 and in part by the Pittsburgh Supercomputing Center (Grant ASC890018P) which provided Cray Y-MP C90 time. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Abrahamson, N. Dadoun, D. G. Kirpatrick, and T. Przytycka. </author> <title> A simple parallel tree contraction algorithm. </title> <journal> Journal of Algorithms, </journal> <volume> 10(2) </volume> <pages> 287-302, </pages> <year> 1989. </year> <month> 21 </month>
Reference-contexts: List ranking has been studied extensively by the theory community and is used as a primitive for many tree and graph algorithms <ref> [1, 12, 11, 20, 21, 25, 26, 29, 32] </ref>. Table II compares several list-ranking algorithms. The serial algorithm takes O (n) time and has small constants. The first parallel algorithm was developed by Wyllie [35] and is based on pointer jumping.
Reference: [2] <author> R. Anderson and G. L. Miller. </author> <title> Deterministic parallel list ranking. </title> <editor> In J. H. Reif, editor, </editor> <booktitle> VLSI Algorithms and Architectures: 3rd Aegean Workshop on Computing, AWOC88, volume 319 of Lecture Notes in Computer Science, </booktitle> <pages> pages 81-90. </pages> <publisher> Springer-Verlag, </publisher> <month> June/July </month> <year> 1988. </year>
Reference-contexts: Algorithm Time Work Constants Space Serial O (n) O (n) small c Wyllie [35] O ( n log n p + log n) O (n log n) small n + c Randomized [25, 3] O ( n p + log n) O (n) medium &gt; 2n Optimal <ref> [8, 9, 2] </ref> O ( n p + log n) O (n) very large &gt; n Ours O ( n p + log 2 n) O (n) small 5p + c Table II Comparison of several list-ranking algorithms, where n is the length of the linked list and p is the <p> Most use contract-scan-expand phases and address two considerations. One is how to find vertices on which to work to keep all the processors busy and the second is how to break symmetry so that two processors are not working on neighboring vertices <ref> [2] </ref>. We break symmetry by randomly dividing up the linked list of length n into m+1 sublists that can be processed independently and in parallel. We briefly describe the three phases of the algorithm below. Phase 1 Randomly divide the list into m + 1 sublists. <p> Because it is not work efficient and its constants are larger than Wyllie's or ours, we chose not to implement it. Anderson and Miller combined their randomized algorithm with the Cole/Viskin deterministic coin tossing to get an optimal O (log n) time deterministic list-ranking algorithm <ref> [2] </ref>. As with their randomized algorithm, the processors are assigned log n vertices to process. On each round each processor executes a case statement that either breaks symmetry, splices out a vertex in its queue, or splices out a vertex at another processor's queue.
Reference: [3] <author> R. Anderson and G. L. Miller. </author> <title> A simple randomized parallel algorithm for list-ranking. </title> <journal> Information Processing Letters, </journal> <volume> 33(5) </volume> <pages> 269-273, </pages> <year> 1990. </year>
Reference-contexts: The problem is that none of these algorithms simultaneously are work efficient and have small constants. Algorithm Time Work Constants Space Serial O (n) O (n) small c Wyllie [35] O ( n log n p + log n) O (n log n) small n + c Randomized <ref> [25, 3] </ref> O ( n p + log n) O (n) medium &gt; 2n Optimal [8, 9, 2] O ( n p + log n) O (n) very large &gt; n Ours O ( n p + log 2 n) O (n) small 5p + c Table II Comparison of several <p> To evaluate the performance of our algorithm, 4 we implemented the list-ranking algorithms that are likely to be the most competitive: serial, Wyllie, Miller/Reif [25] and Anderson/Miller <ref> [3] </ref>. The latter two use randomized pointer jumping. is faster than ours for lists shorter than 1000 vertices. But for longer lists, our algorithm outperforms other algorithms. <p> This algorithm is 20 times slower than our algorithm and 3.5 times slower than the serial algorithm for long linked lists. 2.4 Anderson/Miller random mate Anderson and Miller <ref> [3, 31] </ref> modified the above algorithm so that it avoids load balancing (packing). The p processors are assigned the work of splicing out a queue of n=p vertices. At each round a processor attempts to remove one vertex in its vertex queue.
Reference: [4] <author> S. Baase. </author> <title> Introduction to parallel connectivity, list ranking, and Euler tour techniques. </title> <editor> In J. Reif, editor, </editor> <booktitle> Synthesis of Parallel Algorithms, </booktitle> <pages> pages 61-114. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Except for Wyllie's pointer jumping algorithm on short linked lists we conclude that other algorithms are unlikely to be competitive. Cole and Viskin devised a parallel deterministic coin tossing technique [6] which they used to develop an optimal deterministic parallel list-ranking algorithm <ref> [7, 4] </ref>. This algorithm breaks the linked list into sublists of two or three vertices long (the heads of the sublists are called 2-ruling sets); reduces the sublists to a single vertices; and then compacts these single vertices into contiguous memory to create a new linked list. <p> Its constants are far too large to be practical. In addition, they give a much simpler 2-ruling set algorithm that is not work efficient but has smaller constants (see <ref> [4] </ref>). Because it is not work efficient and its constants are larger than Wyllie's or ours, we chose not to implement it. Anderson and Miller combined their randomized algorithm with the Cole/Viskin deterministic coin tossing to get an optimal O (log n) time deterministic list-ranking algorithm [2].
Reference: [5] <author> G. E. Blelloch, S. Chatterjee, and M. Zagha. </author> <title> Solving linear recurrences with loop raking. </title> <booktitle> In Proceedings Sixth International Parallel Processing Symposium, </booktitle> <pages> pages 416-424, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: To amortize latencies, we attempt to keep the vector lengths long, close to the length of the vector registers. When the work load is imbalanced so that processors finish at different time steps we can use strip-mining [28] or loop-raking <ref> [37, 5] </ref> to assign the work of several virtual processors to a single element processor.
Reference: [6] <author> R. Cole and U. Vishkin. </author> <title> Deterministic coin tossing and accelerating cascades: Micro and macro techniques for designing parallel algorithms. </title> <booktitle> In Proceedings ACM Symposium on Theory of Computing, </booktitle> <pages> pages 206-219, </pages> <year> 1986. </year>
Reference-contexts: Below we discuss some other deterministic algorithms that have been described in the literature. Except for Wyllie's pointer jumping algorithm on short linked lists we conclude that other algorithms are unlikely to be competitive. Cole and Viskin devised a parallel deterministic coin tossing technique <ref> [6] </ref> which they used to develop an optimal deterministic parallel list-ranking algorithm [7, 4].
Reference: [7] <author> R. Cole and U. Vishkin. </author> <title> Deterministic coin tossing with applications to optimal parallel list ranking. </title> <journal> Information and Control, </journal> <volume> 70(1) </volume> <pages> 31-53, </pages> <year> 1986. </year>
Reference-contexts: Except for Wyllie's pointer jumping algorithm on short linked lists we conclude that other algorithms are unlikely to be competitive. Cole and Viskin devised a parallel deterministic coin tossing technique [6] which they used to develop an optimal deterministic parallel list-ranking algorithm <ref> [7, 4] </ref>. This algorithm breaks the linked list into sublists of two or three vertices long (the heads of the sublists are called 2-ruling sets); reduces the sublists to a single vertices; and then compacts these single vertices into contiguous memory to create a new linked list.
Reference: [8] <author> R. Cole and U. Vishkin. </author> <title> Approximate parallel scheduling. Part I: The basic technique with applications to optimal parallel list ranking in logarithmic time. </title> <journal> SIAM Journal of Computing, </journal> <volume> 17(1) </volume> <pages> 128-142, </pages> <month> Feb. </month> <year> 1988. </year>
Reference-contexts: Algorithm Time Work Constants Space Serial O (n) O (n) small c Wyllie [35] O ( n log n p + log n) O (n log n) small n + c Randomized [25, 3] O ( n p + log n) O (n) medium &gt; 2n Optimal <ref> [8, 9, 2] </ref> O ( n p + log n) O (n) very large &gt; n Ours O ( n p + log 2 n) O (n) small 5p + c Table II Comparison of several list-ranking algorithms, where n is the length of the linked list and p is the <p> The problem is algorithms for finding 2-ruling sets that give either of these time bounds are quite complicated and have large constants. Cole and Viskin also developed the first O (log n) optimal deterministic list-ranking algorithm based on assigning processors to jobs and using expander graphs <ref> [8] </ref>. Its constants are far too large to be practical. In addition, they give a much simpler 2-ruling set algorithm that is not work efficient but has smaller constants (see [4]).
Reference: [9] <author> R. Cole and U. Vishkin. </author> <title> Faster optimal parallel prefix sums and list ranking. </title> <journal> Information and Computation, </journal> <volume> 81(3) </volume> <pages> 334-352, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Algorithm Time Work Constants Space Serial O (n) O (n) small c Wyllie [35] O ( n log n p + log n) O (n log n) small n + c Randomized [25, 3] O ( n p + log n) O (n) medium &gt; 2n Optimal <ref> [8, 9, 2] </ref> O ( n p + log n) O (n) very large &gt; n Ours O ( n p + log 2 n) O (n) small 5p + c Table II Comparison of several list-ranking algorithms, where n is the length of the linked list and p is the <p> The algorithm runs in O (log n log log n) parallel time and uses O (n) steps. Later they modified their algorithm to give a O (log n) time optimal deterministic algorithm <ref> [9, 34] </ref>. The problem is algorithms for finding 2-ruling sets that give either of these time bounds are quite complicated and have large constants. Cole and Viskin also developed the first O (log n) optimal deterministic list-ranking algorithm based on assigning processors to jobs and using expander graphs [8].
Reference: [10] <author> W. Feller. </author> <title> An Introduction to Probability Theory and Its Applications, Volume 2. Wiley Series in Probability and Mathematical Statistics. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: For truly random numbers Prob (X j = X k ) = 0 for j 6= k. Therefore, the numbers partition (0,1) into m+1 subintervals. Let X (1) ; : : : X (m) denote the X s ordered by their sizes from smallest to largest. Proposition 2 (Feller <ref> [10] </ref>) If X 1 ; : : : ; X m are independent and uniformly distributed over the range (0,1) then as m ! 1 the successive intervals in our partition behave as though they are mutually independent exponentially distributed variables with Exp (X (j+1) X (j) ) = 1 m
Reference: [11] <author> H. Gazit. </author> <title> Optimal EREW parallel algorithms for connectivity, ear decomposition and st-number of planar graphs. </title> <booktitle> In Proceedings International Conference on Parallel Processing, </booktitle> <pages> pages 84-91, </pages> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: List ranking has been studied extensively by the theory community and is used as a primitive for many tree and graph algorithms <ref> [1, 12, 11, 20, 21, 25, 26, 29, 32] </ref>. Table II compares several list-ranking algorithms. The serial algorithm takes O (n) time and has small constants. The first parallel algorithm was developed by Wyllie [35] and is based on pointer jumping.
Reference: [12] <author> H. Gazit, G. L. Miller, and S.-H. Teng. </author> <title> Optimal tree contraction in the EREW model. </title> <editor> In S. K. Tewsburg, B. W. Dickinson, and S. C. Schwartz, editors, </editor> <booktitle> Concurrent Computations, </booktitle> <pages> pages 139-156. </pages> <publisher> Plenum Publishing Corporation, </publisher> <year> 1988. </year>
Reference-contexts: List ranking has been studied extensively by the theory community and is used as a primitive for many tree and graph algorithms <ref> [1, 12, 11, 20, 21, 25, 26, 29, 32] </ref>. Table II compares several list-ranking algorithms. The serial algorithm takes O (n) time and has small constants. The first parallel algorithm was developed by Wyllie [35] and is based on pointer jumping.
Reference: [13] <author> J. Greiner. </author> <title> A comparison of data-parallel algorithms for connected components. </title> <booktitle> In Proceedings Sixth Annual Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 16-25, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: But its performance was quite poor on other meshes and is likely to be worse on arbitrary graphs. Hsu et al. [17, 19, 18] built a library of pointer-based algorithms, including connected components, open ear decomposition, list ranking etc.; Greiner <ref> [13] </ref> compared several parallel connected-components algorithms; Narayannan [27] implemented a single source shortest path algorithm; and Hillis and Steele [15] implemented list ranking. These studies showed little or no speed up over implementations on fast workstations.
Reference: [14] <author> A. R. Hainline, S. R. Thompson, and L. L. Halcomb. </author> <title> Vector performance estimation for CRAY X-MP/Y-MP supercomputers. </title> <journal> Journal of Supercomputing, </journal> <volume> 6 </volume> <pages> 49-70, </pages> <year> 1992. </year>
Reference-contexts: We attempted to reorder the statements within a loop in order to fill the multiple functional units for concurrent operations, to avoid contention between input/output memory ports and the gather/scatter hardware, and to avoid write after read dependencies <ref> [14] </ref>. With nested loops we unrolled the outer loop up to eight times to avoid unnecessary loading and storing of vector registers on each execution of the inner loop. Chaining is also possible within 10 loops. We made no attempt to avoid memory bank conflicts.
Reference: [15] <author> W. D. Hillis and G. L. Steele Jr. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: Hsu et al. [17, 19, 18] built a library of pointer-based algorithms, including connected components, open ear decomposition, list ranking etc.; Greiner [13] compared several parallel connected-components algorithms; Narayannan [27] implemented a single source shortest path algorithm; and Hillis and Steele <ref> [15] </ref> implemented list ranking. These studies showed little or no speed up over implementations on fast workstations. List ranking finds for each vertex in a linked list the number of vertices that precede it in the list.
Reference: [16] <author> R. W. Hockney. </author> <title> Characterization of parallel computers and algorithms. </title> <journal> Computer Physics Communications, </journal> <volume> 26 </volume> <pages> 285-291, </pages> <year> 1982. </year> <month> 22 </month>
Reference-contexts: Then each processor reads the index at its random location. If the index is not its own it knows that it is a duplicate processor and can drop out of the computation. The standard model <ref> [16] </ref> for the performance of vector operations on vectors of length n is: T (n) = t e (n + n 1=2 ); where t e is the incremental time per vector element and n 1=2 is the vector half performance length (the vector length that achieves half the peak performance).
Reference: [17] <author> T.-s. Hsu and V. Ramachandran. </author> <title> Efficient implementation of virtual processing for some combinatorial algorithms on the MasPar MP-1. </title> <booktitle> In Proceedings of the Seventh IEEE Symposium on Parallel and Distributed Computing, </booktitle> <pages> pages 154-159, </pages> <address> San Antonio, TX, </address> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: Their study showed that for certain classes of probabilistic meshes their algorithm performs reasonably well. But its performance was quite poor on other meshes and is likely to be worse on arbitrary graphs. Hsu et al. <ref> [17, 19, 18] </ref> built a library of pointer-based algorithms, including connected components, open ear decomposition, list ranking etc.; Greiner [13] compared several parallel connected-components algorithms; Narayannan [27] implemented a single source shortest path algorithm; and Hillis and Steele [15] implemented list ranking. <p> However, we did switch to the serial algorithm when only a few queues remained. This result is in contrast to that found by Hsu and Ramachandran <ref> [17] </ref> on the MasPar MP-1. They found that switching to Wyllie's algorithm greatly reduced the number of rounds needed. The difference is that they had 16,383 processors and were using an unbiased coin whereas we had only 128 element processors and a biased coin.
Reference: [18] <author> T.-s. Hsu, V. Ramachandran, and N. Dean. </author> <title> Implementation of parallel graph algorithms on the MasPar. In Computational Support for Discrete Mathematics, </title> <booktitle> DIMACS Series in Discrete Mathematics and Theoretical Computer Science, </booktitle> <volume> Vol. 15, </volume> <pages> pages 165-198. </pages> <publisher> American Mathematical Society, </publisher> <year> 1994. </year> <note> Also available as TR-92-38, </note> <institution> Dept. of Comp. Sci., University of Texas at Austin, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Their study showed that for certain classes of probabilistic meshes their algorithm performs reasonably well. But its performance was quite poor on other meshes and is likely to be worse on arbitrary graphs. Hsu et al. <ref> [17, 19, 18] </ref> built a library of pointer-based algorithms, including connected components, open ear decomposition, list ranking etc.; Greiner [13] compared several parallel connected-components algorithms; Narayannan [27] implemented a single source shortest path algorithm; and Hillis and Steele [15] implemented list ranking.
Reference: [19] <author> T.-s. Hsu, V. Ramachandran, and N. Dean. </author> <title> Implementation of parallel graph algorithms on a massively parallel SIMD computer with virtual processing. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <pages> pages 106-112, </pages> <address> Santa Barbara CA, </address> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Their study showed that for certain classes of probabilistic meshes their algorithm performs reasonably well. But its performance was quite poor on other meshes and is likely to be worse on arbitrary graphs. Hsu et al. <ref> [17, 19, 18] </ref> built a library of pointer-based algorithms, including connected components, open ear decomposition, list ranking etc.; Greiner [13] compared several parallel connected-components algorithms; Narayannan [27] implemented a single source shortest path algorithm; and Hillis and Steele [15] implemented list ranking.
Reference: [20] <author> S. R. Kosaraju and A. L. Delcher. </author> <title> Optimal parallel evaluation of tree-structured computation by ranking (extended abstract). </title> <editor> In J. H. Reif, editor, </editor> <booktitle> VLSI Algorithms and Architectures: 3rd Aegean Workshop on Computing, AWOC88, volume 319 of Lecture Notes in Computer Science, </booktitle> <pages> pages 101-110. </pages> <publisher> Springer-Verlag, </publisher> <month> June/July </month> <year> 1988. </year>
Reference-contexts: List ranking has been studied extensively by the theory community and is used as a primitive for many tree and graph algorithms <ref> [1, 12, 11, 20, 21, 25, 26, 29, 32] </ref>. Table II compares several list-ranking algorithms. The serial algorithm takes O (n) time and has small constants. The first parallel algorithm was developed by Wyllie [35] and is based on pointer jumping.
Reference: [21] <author> C. P. Kruskal, L. Rudolph, and M. Snir. </author> <title> Efficient parallel algorithms for graph problems. </title> <journal> Algorithmica, </journal> <volume> 5(1) </volume> <pages> 43-64, </pages> <year> 1990. </year>
Reference-contexts: List ranking has been studied extensively by the theory community and is used as a primitive for many tree and graph algorithms <ref> [1, 12, 11, 20, 21, 25, 26, 29, 32] </ref>. Table II compares several list-ranking algorithms. The serial algorithm takes O (n) time and has small constants. The first parallel algorithm was developed by Wyllie [35] and is based on pointer jumping.
Reference: [22] <author> S. S. Lumetta, A. Krishnamurthy, and D. E. Culler. </author> <title> Towards modeling the performance of a fast connected components algorithm on parallel machines. </title> <booktitle> In Proceedings Supercomputing '95, </booktitle> <year> 1995. </year>
Reference-contexts: Our aim was to understand the problems involved in implementing parallel pointer-based algorithms and their possible solutions, and to see how well these kinds of algorithms perform in practice. Implementations of parallel pointer-based algorithms have been conspicuously sparse. Lumetta et al. <ref> [22] </ref> implemented a hybrid parallel/serial connected components algorithm for distributed memory machines. Their study showed that for certain classes of probabilistic meshes their algorithm performs reasonably well. But its performance was quite poor on other meshes and is likely to be worse on arbitrary graphs.
Reference: [23] <author> Y. Mansour, N. Nisan, and U. Vishkin. </author> <title> Trade-offs between communication throughput and parallel time. </title> <booktitle> In Proceedings ACM Symposium on Theory of Computing, </booktitle> <pages> pages 372-381, </pages> <address> Mon-treal, P.Q., </address> <month> May </month> <year> 1994. </year>
Reference-contexts: High bandwidth is not sufficient because the algorithms with even moderate size constants are not much better than the serial implementation as shown in Figure 1. Small constants are not sufficient because reduce bandwidths results in longer parallel times, as indicated in <ref> [23] </ref> and seen in the reduced speedup of our algorithm for larger numbers of processors (see Figure 3). As with any implementation, there are a multitude of possible modifications and enhancements that could improve its performance. A large part of the performance loss is due to short vector lengths.
Reference: [24] <author> K. Mehlhorn and U. Vishkin. </author> <title> Randomized and deterministic simulations of PRAMs by parallel machines with restricted granularity of parallel memory. </title> <journal> Acta Informatica, </journal> <volume> 21 </volume> <pages> 339-374, </pages> <year> 1984. </year>
Reference-contexts: For vector multiprocessors, virtual processing allows for vectorization so that computation and communication can be pipelined to hide latencies. Virtual processing has been a common approach for hiding latencies <ref> [33, 24] </ref>. Using Zagha's programming model we implement PRAM algorithms by treating a vector processor as a SIMD (distributed memory) multiprocessor. Each element of the vector registers of length L act as L element processors of the SIMD machine, see Figure 2.
Reference: [25] <author> G. L. Miller and J. H. Reif. </author> <title> Parallel tree contraction and its application. </title> <booktitle> In Proceedings Symposium on Foundations of Computer Science, </booktitle> <pages> pages 478-489, </pages> <month> Oct. </month> <year> 1985. </year>
Reference-contexts: List ranking has been studied extensively by the theory community and is used as a primitive for many tree and graph algorithms <ref> [1, 12, 11, 20, 21, 25, 26, 29, 32] </ref>. Table II compares several list-ranking algorithms. The serial algorithm takes O (n) time and has small constants. The first parallel algorithm was developed by Wyllie [35] and is based on pointer jumping. <p> The problem is that none of these algorithms simultaneously are work efficient and have small constants. Algorithm Time Work Constants Space Serial O (n) O (n) small c Wyllie [35] O ( n log n p + log n) O (n log n) small n + c Randomized <ref> [25, 3] </ref> O ( n p + log n) O (n) medium &gt; 2n Optimal [8, 9, 2] O ( n p + log n) O (n) very large &gt; n Ours O ( n p + log 2 n) O (n) small 5p + c Table II Comparison of several <p> Therefore, our goal was to design an algorithm that both is work efficient and has small constants, even if it meant sacrificing optimal time. To evaluate the performance of our algorithm, 4 we implemented the list-ranking algorithms that are likely to be the most competitive: serial, Wyllie, Miller/Reif <ref> [25] </ref> and Anderson/Miller [3]. The latter two use randomized pointer jumping. is faster than ours for lists shorter than 1000 vertices. But for longer lists, our algorithm outperforms other algorithms. <p> As one can see from 7 it does scale almost linearly with the number of processors and so on multiple processors is faster than the serial algorithm for moderately long lists. 2.3 Miller/Reif random mate One of the simplest work-efficient parallel algorithms was devised by Miller and Reif <ref> [25, 31] </ref>. It uses randomization for symmetry breaking so that processors at neighboring vertices do not attempt to dereference their successor pointers simultaneously. Each processor flips an unbiased male/female coin. If it is a female and its successor is a male (a "random mate") then it "splices out" its successor.
Reference: [26] <author> G. L. Miller and J. H. Reif. </author> <title> Parallel tree contraction. Part 2: Further applications. </title> <journal> SIAM Journal of Computing, </journal> <volume> 20(6) </volume> <pages> 1128-1147, </pages> <month> Dec. </month> <year> 1991. </year>
Reference-contexts: List ranking has been studied extensively by the theory community and is used as a primitive for many tree and graph algorithms <ref> [1, 12, 11, 20, 21, 25, 26, 29, 32] </ref>. Table II compares several list-ranking algorithms. The serial algorithm takes O (n) time and has small constants. The first parallel algorithm was developed by Wyllie [35] and is based on pointer jumping.
Reference: [27] <author> P. Narayanan. </author> <title> Single source shortest path problem on processor arrays. </title> <booktitle> In Proceedings Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 553-556, </pages> <address> McLean, VA, </address> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: But its performance was quite poor on other meshes and is likely to be worse on arbitrary graphs. Hsu et al. [17, 19, 18] built a library of pointer-based algorithms, including connected components, open ear decomposition, list ranking etc.; Greiner [13] compared several parallel connected-components algorithms; Narayannan <ref> [27] </ref> implemented a single source shortest path algorithm; and Hillis and Steele [15] implemented list ranking. These studies showed little or no speed up over implementations on fast workstations. List ranking finds for each vertex in a linked list the number of vertices that precede it in the list.
Reference: [28] <author> D. A. Padua and M. Wolfe. </author> <title> Advanced Compiler Optimizations for Supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> Dec. </month> <year> 1986. </year>
Reference-contexts: To amortize latencies, we attempt to keep the vector lengths long, close to the length of the vector registers. When the work load is imbalanced so that processors finish at different time steps we can use strip-mining <ref> [28] </ref> or loop-raking [37, 5] to assign the work of several virtual processors to a single element processor.
Reference: [29] <author> V. Ramachandran. </author> <title> Parallel open ear decomposition with applications to graph biconnectivity and triconnectivity. </title> <editor> In J. Reif, editor, </editor> <booktitle> Synthesis of Parallel Algorithms, </booktitle> <pages> pages 275-340. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year> <month> 23 </month>
Reference-contexts: List ranking has been studied extensively by the theory community and is used as a primitive for many tree and graph algorithms <ref> [1, 12, 11, 20, 21, 25, 26, 29, 32] </ref>. Table II compares several list-ranking algorithms. The serial algorithm takes O (n) time and has small constants. The first parallel algorithm was developed by Wyllie [35] and is based on pointer jumping.
Reference: [30] <author> M. Reid-Miller and G. E. Blelloch. </author> <title> List ranking and list scan on the Cray C-90. </title> <type> Technical Report CMU-CS-94-101, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: Figure 3 shows the speedup relative to one processor for various size lists. 3 Our Algorithm on a Vector Processor This section describes our vector implementation of list scan (for a more detailed description see <ref> [30] </ref>). We used the C programming language and the standard Cray C compiler on a Cray C90. The time equations we give in this section are in Cray C90 clock cycles (4.2 nsec).
Reference: [31] <author> M. Reid-Miller, G. L. Miller, and F. Modugno. </author> <title> List ranking and parallel tree contraction. </title> <editor> In J. Reif, editor, </editor> <booktitle> Synthesis of Parallel Algorithms, </booktitle> <pages> pages 115-194. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: As one can see from 7 it does scale almost linearly with the number of processors and so on multiple processors is faster than the serial algorithm for moderately long lists. 2.3 Miller/Reif random mate One of the simplest work-efficient parallel algorithms was devised by Miller and Reif <ref> [25, 31] </ref>. It uses randomization for symmetry breaking so that processors at neighboring vertices do not attempt to dereference their successor pointers simultaneously. Each processor flips an unbiased male/female coin. If it is a female and its successor is a male (a "random mate") then it "splices out" its successor. <p> This algorithm is 20 times slower than our algorithm and 3.5 times slower than the serial algorithm for long linked lists. 2.4 Anderson/Miller random mate Anderson and Miller <ref> [3, 31] </ref> modified the above algorithm so that it avoids load balancing (packing). The p processors are assigned the work of splicing out a queue of n=p vertices. At each round a processor attempts to remove one vertex in its vertex queue.
Reference: [32] <author> B. Schieber. </author> <title> Parallel lowest common ancestor computation. </title> <editor> In J. Reif, editor, </editor> <booktitle> Synthesis of Parallel Algorithms, </booktitle> <pages> pages 259-274. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: List ranking has been studied extensively by the theory community and is used as a primitive for many tree and graph algorithms <ref> [1, 12, 11, 20, 21, 25, 26, 29, 32] </ref>. Table II compares several list-ranking algorithms. The serial algorithm takes O (n) time and has small constants. The first parallel algorithm was developed by Wyllie [35] and is based on pointer jumping.
Reference: [33] <author> L. G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: For vector multiprocessors, virtual processing allows for vectorization so that computation and communication can be pipelined to hide latencies. Virtual processing has been a common approach for hiding latencies <ref> [33, 24] </ref>. Using Zagha's programming model we implement PRAM algorithms by treating a vector processor as a SIMD (distributed memory) multiprocessor. Each element of the vector registers of length L act as L element processors of the SIMD machine, see Figure 2.
Reference: [34] <author> U. Vishkin. </author> <title> Advanced parallel prefix-sums, list ranking and connectivity. </title> <editor> In J. Reif, editor, </editor> <booktitle> Synthesis of Parallel Algorithms, </booktitle> <pages> pages 215-257. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: The algorithm runs in O (log n log log n) parallel time and uses O (n) steps. Later they modified their algorithm to give a O (log n) time optimal deterministic algorithm <ref> [9, 34] </ref>. The problem is algorithms for finding 2-ruling sets that give either of these time bounds are quite complicated and have large constants. Cole and Viskin also developed the first O (log n) optimal deterministic list-ranking algorithm based on assigning processors to jobs and using expander graphs [8].
Reference: [35] <author> J. C. Wyllie. </author> <title> The complexity of parallel computations. </title> <type> Technical Report TR-79-387, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <month> Aug. </month> <year> 1979. </year>
Reference-contexts: Table II compares several list-ranking algorithms. The serial algorithm takes O (n) time and has small constants. The first parallel algorithm was developed by Wyllie <ref> [35] </ref> and is based on pointer jumping. It is not work efficient since it takes O (n log n) operations. But because Wyllie's algorithm is very simple, it works well for short lists. To obtain work-efficient algorithms one approach is to use randomized pointer jumping. <p> On the other hand, optimal deterministic parallel PRAM list-ranking algorithms have even larger constants, which makes their implementation impractical. The problem is that none of these algorithms simultaneously are work efficient and have small constants. Algorithm Time Work Constants Space Serial O (n) O (n) small c Wyllie <ref> [35] </ref> O ( n log n p + log n) O (n log n) small n + c Randomized [25, 3] O ( n p + log n) O (n) medium &gt; 2n Optimal [8, 9, 2] O ( n p + log n) O (n) very large &gt; n Ours <p> On workstations the list-ranking execution times are substantially faster than the list-scan times and both times depend on whether all the linked-list data can be placed in the cache or not. 2.2 Wyllie's algorithm The first parallel list-ranking algorithm was introduced by Wyllie <ref> [35] </ref>. The algorithm uses a technique common to most parallel list-ranking algorithms, "pointer jumping" or "shortcutting". A processor is associated with every vertex and each processor repeatedly replaces its successor pointer with its successor's successor pointer in unison with the other processors.
Reference: [36] <author> M. Zagha. </author> <title> Efficient Irregular Computation on Pipelined-Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution> <note> In Preparation. </note>
Reference-contexts: Thus, the PRAM model assumption that often is cited as unrealistic, namely unit-time memory access, holds on vector multiprocessors as long as we can avoid memory bank conflicts and hide latencies. 5 Zagha <ref> [36] </ref> proposes several vector multiprocessing programming techniques for avoiding bank conflicts and hiding latencies. To address bank conflicts he proposes a data distribution technique to manage the memory system explicitly.
Reference: [37] <author> M. Zagha and G. E. Blelloch. </author> <title> Radix sort for vector multiprocessors. </title> <booktitle> In Proceedings Supercomputing '91, </booktitle> <pages> pages 712-721, </pages> <month> Nov. </month> <year> 1991. </year> <month> 24 </month>
Reference-contexts: To amortize latencies, we attempt to keep the vector lengths long, close to the length of the vector registers. When the work load is imbalanced so that processors finish at different time steps we can use strip-mining [28] or loop-raking <ref> [37, 5] </ref> to assign the work of several virtual processors to a single element processor.
References-found: 37

