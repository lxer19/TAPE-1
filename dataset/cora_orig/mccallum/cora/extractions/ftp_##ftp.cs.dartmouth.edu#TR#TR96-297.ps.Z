URL: ftp://ftp.cs.dartmouth.edu/TR/TR96-297.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR96-297/
Root-URL: http://www.cs.dartmouth.edu
Email: dfk@cs.dartmouth.edu  
Title: Applications of Parallel I/O  
Author: David Kotz 
Date: October 14, 1996  
Address: Hanover, NH 03755-3510  
Affiliation: Department of Computer Science Dartmouth College  
Web: URL ftp://ftp.cs.dartmouth.edu/TR/TR96-297.ps.Z  
Note: Available at  
Abstract: Technical Report PCS-TR96-297 Release 1 Abstract Scientific applications are increasingly being implemented on massively parallel supercomputers. Many of these applications have intense I/O demands, as well as massive computational requirements. This paper is essentially an annotated bibliography of papers and other sources of information about scientific applications using parallel I/O. It will be updated periodically.
Abstract-found: 1
Intro-found: 1
Reference: [Are91] <author> James W. Arendt. </author> <title> Parallel genome sequence comparison using a concurrent file system. </title> <type> Technical Report UIUCDCS-R-91-1674, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference-contexts: They would like to see a file system that can adapt dynamically to adjust its policies to the apparent access patterns. Automatic request aggregation of some kind seems like a good idea; of course, that is one feature of a buffer cache. * <ref> [Are91] </ref> They use a genome-sequence comparison program to study the performance of Intel CFS.
Reference: [AUB + 96] <author> Anurag Acharya, Mustafa Uysal, Robert Bennett, Assaf Mendelson, Michael Beynon, Jeffrey K. Hollingsworth, Joel Saltz, and Alan Sussman. </author> <title> Tuning the performance of I/O intensive parallel applications. </title> <booktitle> In Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 15-27, </pages> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Stored in block-structured form to improve locality on the disk. * [Die90] An out-of-core backpropagation application that reads large files, sequentially, on CM2 with DataVault. 3 Characterizations of specific applications These papers are detailed characterizations of the I/O access pattern of one or more parallel appli cations. * <ref> [AUB + 96] </ref> They discuss four application programs from the areas of satellite-data process ing and linear algebra. They tune each one of them by restructuring the program. * [CACR95] A detailed characterization of three applications: electron scattering, terrain rendering, and quantum chemistry.
Reference: [BC93] <author> P. E. Bjtrstad and J. Cook. </author> <title> Large scale structural analysis on massively parallel computers. </title> <booktitle> In Linear Algebra for Large Scale and Real-Time Applications, </booktitle> <pages> pages 3-11. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1993. </year> <note> ftp from ftp.ii.uib.no in pub/tech_reports/mpp_sestra.ps.Z. 3 http://www.cs.dartmouth.edu/cs archive/pario/anecdotes.html 4 http://www.cs.dartmouth.edu/cs archive/pario/examples.html 8 </note>
Reference-contexts: The history I/O is no problem. On distributed-memory machines with no SSD, out-of-core was impractical and the history file was only written once per simulated month. "The most significant weakness in the distributed-memory implementation is the treatment of I/O, [due to] file system maturity...." * <ref> [BC93] </ref> A substantial part of this structural-analysis application was involved in I/O, mov ing substructures in and out of RAM. * [Joh84] A paper about three-dimensional wave-equation computations in seismic modeling. They describe their need for large memory and fast paging and I/O in out-of-core solutions.
Reference: [Bel88] <author> Jean L. Bell. </author> <title> A specialized data management system for parallel execution of parti-cle physics codes. </title> <booktitle> In Proceedings of the ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 277-285, </pages> <year> 1988. </year>
Reference-contexts: They found results that are consistent with [KN95], in that they also found lots of small data requests, some large data requests, significant file sharing and interprocess locality. * <ref> [Bel88] </ref> They describe a specialized database system for particle physics codes. The paper is valuable for its description of access patterns and subsequent file access requirements. Particle-in-cell codes iterate over timesteps, updating the position of each particle, and then the characteristics of each cell in the grid.
Reference: [BW96] <author> Sandra Johnson Baylor and C. Eric Wu. </author> <title> Parallel I/O workload characteristics using Vesta. </title> <editor> In Ravi Jain, John Werth, and James C. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems, volume 362 of The Kluwer International Series in Engineering and Computer Science, chapter 7, </booktitle> <pages> pages 167-185. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: The application reads in a huge file of records, each a genome sequence, and compares each sequence against a given sequence. * <ref> [BW96] </ref> They characterize four parallel applications: sort, matrix multiply, seismic migration, and video server, in terms of their I/O activity.
Reference: [CACR95] <author> Phyllis E. Crandall, Ruth A. Aydt, Andrew A. Chien, and Daniel A. Reed. </author> <title> Input/output characteristics of scalable parallel applications. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: They tune each one of them by restructuring the program. * <ref> [CACR95] </ref> A detailed characterization of three applications: electron scattering, terrain rendering, and quantum chemistry. They look at the volume of data moved, the timing of I/O, and the periodic nature of I/O. They do a little bit with the access patterns of data within each file.
Reference: [CHKM93] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural requirements of parallel scientific applications with explicit communication. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <year> 1993. </year>
Reference-contexts: Note that many of these things can be provided by a file system, but that most are hard to come by in typical file systems, if not impossible. Many of these features are generalizable to other applications. 5 * <ref> [CHKM93] </ref> They look at many parallel applications, although there is little mention of I/O. They average 1207 Bytes/MFlop. Some of the applications do I/O throughout their run (2400 Bytes/MFlop avg.), while others only do I/O at the beginning or end (14 Bytes/MFlop avg).
Reference: [CSWM95] <author> Terry W. Clark, L. Ridgway Scott, Stanislaw Wlodek, and J. Andrew McCammon. </author> <title> I/O limitations in parallel molecular dynamics. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <year> 1995. </year>
Reference-contexts: It writes two files, one containing three three-dimensional matrices X, Y, and Z, 1 http://www.cs.dartmouth.edu/reports/abstracts/TR96-297.html. 2 and the other containing the four-dimensional matrix Q. The matrices are spread over all the nodes, and each file is written in parallel by the processors. * <ref> [CSWM95] </ref> They "discuss data production rates and their impact on the performance of scientific applications using parallel computers." They "present performance data for a biomolec-ular simulation of the enzyme, acetylcholinesterase, which uses the parallel molecular dynamics program EulerGROMOS.
Reference: [dC94] <author> Juan Miguel del Rosario and Alok Choudhary. </author> <title> High performance I/O for parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: NASA Ames * [PEK + 94, PEK + 95] Thinking Machines CM-5 at NCSA * [NKP + 95, NKP + 96] both 5 Other surveys of applications using parallel I/O There are a few other papers that have an extensive survey of several parallel scientific applications using parallel I/O. * <ref> [dC94] </ref> A nice summary of grand-challenge and other applications, and their I/O needs. * [Poo94] This paper from the Scalable I/O Initiative describes several applications and their I/O needs. They focus on four categories of I/O needs: input, output, checkpoint, and virtual memory ("out-of-core" scratch space).
Reference: [Die90] <author> Carl Diegert. </author> <title> Out-of-core backpropagation. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 97-103, </pages> <year> 1990. </year>
Reference-contexts: They describe their need for large memory and fast paging and I/O in out-of-core solutions. They used 4-way parallel I/O. They needed to transfer a 3-d matrix in and out of memory by rows, columns, and vertical columns. Stored in block-structured form to improve locality on the disk. * <ref> [Die90] </ref> An out-of-core backpropagation application that reads large files, sequentially, on CM2 with DataVault. 3 Characterizations of specific applications These papers are detailed characterizations of the I/O access pattern of one or more parallel appli cations. * [AUB + 96] They discuss four application programs from the areas of satellite-data process
Reference: [FAJL + 95] <author> Hassan Fallah-Adl, Joseph JaJa, Shunlin Liang, Yoram J. Kaufman, and John Town-shend. </author> <title> Efficient algorithms for atmospheric correction of remotely sensed data. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <year> 1995. </year>
Reference-contexts: The actual production rates are compared against a typical time frame for results analysis where we show that the rate limiting step is the simulation, and that to overcome this will require improved output rates." * <ref> [FAJL + 95] </ref> "Remotely sensed imagery has been used for developing and validating various studies regarding land cover dynamics." They "develop a parallel version of [their] algorithm that is scalable in terms of both computation and I/O.
Reference: [GGL93] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year>
Reference-contexts: Units of I/O seem to be either (sub)matrices (1-5 dimensions) or items in a collection of objects (100-10000 bytes each). Data set sizes varied up to 1 TB; bandwidth needs varied up to 1 GB/s. * <ref> [GGL93] </ref> They give a useful overview of the I/O requirements of many applications codes, in terms of input, output, scratch files, debugging, and checkpointing. * [Moo95] They briefly describe the I/O requirements for four production oceanography programs running at Oregon State University.
Reference: [HLDS95] <author> Steven W. Hammond, Richard D. Loft, John M. Dennis, and Rochard K. Sato. </author> <title> Implementation and performance issues of a massively parallel atmospheric model. </title> <journal> Parallel Computing, </journal> <volume> 21 </volume> <pages> 1593-1619, </pages> <year> 1995. </year>
Reference-contexts: I do not include papers about scientific kernels (LU factorization, matrix multiplication, sorting, FFT, and so forth). * <ref> [HLDS95] </ref> They discuss a weather code that runs on the CM-5. The code writes a history file, dumping some data every timestep, and periodically a restart file. They found that CM-5 Fortran met their needs, although required huge buffers to get much scalability.
Reference: [HRW + 95] <author> James J. Hack, James M. Rosinski, David L. Williamson, Byron A. Boville, and John E. Truesdale. </author> <title> Computational design of the NCAR community climate model. </title> <journal> Parallel Computing, </journal> <volume> 21 </volume> <pages> 1545-1569, </pages> <year> 1995. </year>
Reference-contexts: It uses MPI for portability, and has sustained 22 Mflops/sec/proc (compiled FORTRAN) on the Intel Paragon." There are two pages about their I/O scheme, mostly regarding a calculation of the optimal balance between compute nodes and I/O nodes to achieve perfect overlap. * <ref> [HRW + 95] </ref> They discuss a climate modeling code, which does some out-of-core work to communicate data between time steps. They also dump a 'history' file every simulated day, and periodic checkpoint files. They are flexible about the layout of the history file, assuming 3 postprocessing will clean it up.
Reference: [JKH95] <author> Philip W. Jones, Christopher L. Kerr, and Richard S. Hemler. </author> <title> Practical considerations in development of a parallel SKYHI general circulation model. </title> <journal> Parallel Computing, </journal> <volume> 21 </volume> <pages> 1677-1694, </pages> <year> 1995. </year>
Reference-contexts: They want to see a single, shared file-system image from all processors, have the file format be independent of processor count, use portable conventional interface, and have throughput scale with the number of computation processors. * <ref> [JKH95] </ref> This paper is about a weather code. There's a bit about the parallel I/O issues. They periodically write a restart file, and they write out several types of data files. They write out the data in any order, with a little mini-header in each chunk that describes the chunk.
Reference: [Joh84] <author> Olin G. Johnson. </author> <title> Three-dimensional wave equation computations on vector computers. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 72(1) </volume> <pages> 90-95, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: the history file was only written once per simulated month. "The most significant weakness in the distributed-memory implementation is the treatment of I/O, [due to] file system maturity...." * [BC93] A substantial part of this structural-analysis application was involved in I/O, mov ing substructures in and out of RAM. * <ref> [Joh84] </ref> A paper about three-dimensional wave-equation computations in seismic modeling. They describe their need for large memory and fast paging and I/O in out-of-core solutions. They used 4-way parallel I/O. They needed to transfer a 3-d matrix in and out of memory by rows, columns, and vertical columns.
Reference: [KFG94] <author> John F. Karpovich, James C. French, and Andrew S. Grimshaw. </author> <title> High performance access to radio astronomy data: A case study. </title> <booktitle> In Proceedings of the 7th International 9 Working Conference on Scientific and Statistical Database Management, </booktitle> <month> September </month> <year> 1994. </year> <note> Also available as Univ. of Virginia TR CS-94-25. </note>
Reference-contexts: But I/O is bursty, so larger bandwidths are suggested. The applications are parallel programs running on Intel Delta, nCUBE/1, or nCUBE/2; and are in C, FORTRAN, or both. * <ref> [KGF93, KFG94] </ref> They store a sparse, multidimensional radio-astronomy data set as a set of tagged data values, i.e., as a set of tuples, each with several keys and a data value.
Reference: [KGF93] <author> John F. Karpovich, Andrew S. Grimshaw, and James C. </author> <title> French. Breaking the I/O bottleneck at the National Radio Astronomy Observatory (NRAO). </title> <type> Technical Report CS-94-37, </type> <institution> University of Virginia, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: But I/O is bursty, so larger bandwidths are suggested. The applications are parallel programs running on Intel Delta, nCUBE/1, or nCUBE/2; and are in C, FORTRAN, or both. * <ref> [KGF93, KFG94] </ref> They store a sparse, multidimensional radio-astronomy data set as a set of tagged data values, i.e., as a set of tuples, each with several keys and a data value.
Reference: [KN94a] <author> David Kotz and Nils Nieuwejaar. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <type> Technical Report PCS-TR94-211, </type> <institution> Dept. of Math and Computer Science, Dartmouth College, </institution> <month> April </month> <year> 1994. </year> <note> Revised May 11, </note> <year> 1994. </year>
Reference-contexts: The CHARISMA project 2 traced the workload of two parallel machines running production scientific applications, and then characterized the workload in detail. * <ref> [KN94a, KN94b, KN95] </ref> Intel iPSC/860 at NASA Ames * [PEK + 94, PEK + 95] Thinking Machines CM-5 at NCSA * [NKP + 95, NKP + 96] both 5 Other surveys of applications using parallel I/O There are a few other papers that have an extensive survey of several parallel scientific
Reference: [KN94b] <author> David Kotz and Nils Nieuwejaar. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 640-649, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: The CHARISMA project 2 traced the workload of two parallel machines running production scientific applications, and then characterized the workload in detail. * <ref> [KN94a, KN94b, KN95] </ref> Intel iPSC/860 at NASA Ames * [PEK + 94, PEK + 95] Thinking Machines CM-5 at NCSA * [NKP + 95, NKP + 96] both 5 Other surveys of applications using parallel I/O There are a few other papers that have an extensive survey of several parallel scientific
Reference: [KN95] <author> David Kotz and Nils Nieuwejaar. </author> <title> File-system workload on a scientific multiprocessor. </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <pages> pages 51-60, </pages> <month> Spring </month> <year> 1995. </year>
Reference-contexts: The application reads in a huge file of records, each a genome sequence, and compares each sequence against a given sequence. * [BW96] They characterize four parallel applications: sort, matrix multiply, seismic migration, and video server, in terms of their I/O activity. They found results that are consistent with <ref> [KN95] </ref>, in that they also found lots of small data requests, some large data requests, significant file sharing and interprocess locality. * [Bel88] They describe a specialized database system for particle physics codes. The paper is valuable for its description of access patterns and subsequent file access requirements. <p> The CHARISMA project 2 traced the workload of two parallel machines running production scientific applications, and then characterized the workload in detail. * <ref> [KN94a, KN94b, KN95] </ref> Intel iPSC/860 at NASA Ames * [PEK + 94, PEK + 95] Thinking Machines CM-5 at NCSA * [NKP + 95, NKP + 96] both 5 Other surveys of applications using parallel I/O There are a few other papers that have an extensive survey of several parallel scientific
Reference: [Kot96] <author> David Kotz. </author> <title> BibTeX bibliography file: Parallel I/O. </title> <note> Available for ftp from cs.dartmouth.edu in pub/pario/pario.bib, and on the WWW at http://www.cs.dartmouth.edu/cs archive/pario/bib.html, May 1996. Eighth Edition. </note>
Reference-contexts: While I do not go into depth about the characteristics of each application, I hope that this paper helps researchers and application programmers to locate information that will help them to better understand the issues behind parallel I/O. See <ref> [Kot96] </ref> for a complete bibliography of parallel I/O. This research was funded by NSF under grant number CCR-9404919, by NASA Ames under agreement number NCC 2-849.
Reference: [Moo95] <author> Jason A. Moore. </author> <title> Parallel I/O requirements of four oceanography applications. </title> <type> Technical Report 95-80-1, </type> <institution> Oregon State University, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Data set sizes varied up to 1 TB; bandwidth needs varied up to 1 GB/s. * [GGL93] They give a useful overview of the I/O requirements of many applications codes, in terms of input, output, scratch files, debugging, and checkpointing. * <ref> [Moo95] </ref> They briefly describe the I/O requirements for four production oceanography programs running at Oregon State University. The applications all rely exclusively on array-oriented, sequential file operations.
Reference: [MPP + 95] <author> Craig Miller, David G. Payne, Thanh N. Phung, Herb Siegel, and Roy Williams. </author> <title> Parallel processing of spaceborne imaging radar data. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <year> 1995. </year>
Reference-contexts: Experimental results obtained show that a Thematic Mapper (TM) image (36 MB per band, 5 bands need to be corrected) can be handled in less than 4.3 minutes on a 32-node CM-5 machine, including I/O time." * <ref> [MPP + 95] </ref> They discuss "parallel processing of Synthetic Aperture Radar (SAR) data...," which is image data collected by satellite.
Reference: [NKP + 95] <author> Nils Nieuwejaar, David Kotz, Apratim Purakayastha, Carla Schlatter Ellis, and Michael Best. </author> <title> File-access characteristics of parallel scientific workloads. </title> <type> Technical Report PCS-TR95-263, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> August </month> <year> 1995. </year> <note> To appear in IEEE TPDS. </note>
Reference-contexts: The CHARISMA project 2 traced the workload of two parallel machines running production scientific applications, and then characterized the workload in detail. * [KN94a, KN94b, KN95] Intel iPSC/860 at NASA Ames * [PEK + 94, PEK + 95] Thinking Machines CM-5 at NCSA * <ref> [NKP + 95, NKP + 96] </ref> both 5 Other surveys of applications using parallel I/O There are a few other papers that have an extensive survey of several parallel scientific applications using parallel I/O. * [dC94] A nice summary of grand-challenge and other applications, and their I/O needs. * [Poo94] This
Reference: [NKP + 96] <author> Nils Nieuwejaar, David Kotz, Apratim Purakayastha, Carla Schlatter Ellis, and Michael Best. </author> <title> File-access characteristics of parallel scientific workloads. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 7(10) </volume> <pages> 1075-1089, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: The CHARISMA project 2 traced the workload of two parallel machines running production scientific applications, and then characterized the workload in detail. * [KN94a, KN94b, KN95] Intel iPSC/860 at NASA Ames * [PEK + 94, PEK + 95] Thinking Machines CM-5 at NCSA * <ref> [NKP + 95, NKP + 96] </ref> both 5 Other surveys of applications using parallel I/O There are a few other papers that have an extensive survey of several parallel scientific applications using parallel I/O. * [dC94] A nice summary of grand-challenge and other applications, and their I/O needs. * [Poo94] This
Reference: [OOVW96] <author> Curtis Ober, Ron Oldfield, John VanDyke, and David Womble. </author> <title> Seismic imaging on massively parallel computers. </title> <type> Technical Report SAND96-1112, </type> <institution> Sandia National Laboratories, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: They "parallelized the most compute-intensive SAR correlator phase of the Spaceborne Shuttle Imaging Radar-C/X-Band SAR (SIR-C/X-SAR) code, for the Intel Paragon." They "describe the data decomposition, the scalable high-performance I/O model, and the node-level optimizations which enable us to obtain efficient processing throughput." * <ref> [OOVW96] </ref> This paper is about "imaging of complex, oil-bearing geologies, such as overthrusts and salt domes.... ...highly accurate techniques involve the solution of the wave equation and are characterized by large data sets and large computational demands.
Reference: [PEK + 94] <author> Apratim Purakayastha, Carla Schlatter Ellis, David Kotz, Nils Nieuwejaar, and Michael Best. </author> <title> Characterizing parallel file-access patterns on a large-scale multiprocessor. </title> <type> Technical Report CS-1994-33, </type> <institution> Dept. of Computer Science, Duke University, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: The CHARISMA project 2 traced the workload of two parallel machines running production scientific applications, and then characterized the workload in detail. * [KN94a, KN94b, KN95] Intel iPSC/860 at NASA Ames * <ref> [PEK + 94, PEK + 95] </ref> Thinking Machines CM-5 at NCSA * [NKP + 95, NKP + 96] both 5 Other surveys of applications using parallel I/O There are a few other papers that have an extensive survey of several parallel scientific applications using parallel I/O. * [dC94] A nice summary
Reference: [PEK + 95] <author> Apratim Purakayastha, Carla Schlatter Ellis, David Kotz, Nils Nieuwejaar, and Michael Best. </author> <title> Characterizing parallel file-access patterns on a large-scale multiprocessor. </title> <booktitle> In Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <pages> pages 165-172, </pages> <month> April </month> <year> 1995. </year> <month> 10 </month>
Reference-contexts: The CHARISMA project 2 traced the workload of two parallel machines running production scientific applications, and then characterized the workload in detail. * [KN94a, KN94b, KN95] Intel iPSC/860 at NASA Ames * <ref> [PEK + 94, PEK + 95] </ref> Thinking Machines CM-5 at NCSA * [NKP + 95, NKP + 96] both 5 Other surveys of applications using parallel I/O There are a few other papers that have an extensive survey of several parallel scientific applications using parallel I/O. * [dC94] A nice summary
Reference: [Poo94] <author> James T. Poole. </author> <title> Preliminary survey of I/O intensive applications. </title> <type> Technical Report CCSF-38, </type> <institution> Scalable I/O Initiative, Caltech Concurrent Supercomputing Facilities, Cal-tech, </institution> <year> 1994. </year>
Reference-contexts: * [NKP + 95, NKP + 96] both 5 Other surveys of applications using parallel I/O There are a few other papers that have an extensive survey of several parallel scientific applications using parallel I/O. * [dC94] A nice summary of grand-challenge and other applications, and their I/O needs. * <ref> [Poo94] </ref> This paper from the Scalable I/O Initiative describes several applications and their I/O needs. They focus on four categories of I/O needs: input, output, checkpoint, and virtual memory ("out-of-core" scratch space).
Reference: [RB90] <author> A. L. Narasimha Reddy and Prithviraj Banerjee. </author> <title> A study of I/O behavior of Perfect benchmarks on a multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 312-321, </pages> <year> 1990. </year>
Reference-contexts: They decide on the splits based on a preliminary statistical survey of the data. Bucket overflow is handled by chaining. Then, they evaluate various kinds of queries, i.e., multidimensional range queries, for their performance. In this workload queries (reads) are much more common than updates (writes). * <ref> [RB90] </ref> Using five applications from the Perfect benchmark suite, they studied both implicit (paging) and explicit (file) I/O activity. They found that the paging activity was relatively small and that sequential access to VM was common.
Reference: [RHH95] <author> Bernardo Rodriguez, Leslie Hart, and Tom Henderson. </author> <title> Programming regular grid-based weather simulation models for portable and fast execution. </title> <booktitle> In Proceedings of the 1995 International Conference on Parallel Processing, </booktitle> <pages> pages III:51-59, </pages> <month> August </month> <year> 1995. </year>
Reference-contexts: They write out the data in any order, with a little mini-header in each chunk that describes the chunk. I/O was not a significant percentage of their run time on either the CM5 or C90. * <ref> [RHH95] </ref> weather simulation code * [RW93, Rya91] A paper, and corresponding I/O-template code, about aircraft simulation and computational fluid dynamics (Navier-Stokes flowfields). They describe their parallel implementation of the ARC3D code on the iPSC/860.
Reference: [RW93] <author> J. S. Ryan and S. K. Weeratunga. </author> <title> Parallel computation of 3-D Navier-Stokes flowfields for supersonic vehicles. </title> <booktitle> In 31st Aerospace Sciences Meeting and Exhibit, </booktitle> <address> Reno, NV, </address> <year> 1993. </year> <note> AIAA Paper 93-0064. </note>
Reference-contexts: They write out the data in any order, with a little mini-header in each chunk that describes the chunk. I/O was not a significant percentage of their run time on either the CM5 or C90. * [RHH95] weather simulation code * <ref> [RW93, Rya91] </ref> A paper, and corresponding I/O-template code, about aircraft simulation and computational fluid dynamics (Navier-Stokes flowfields). They describe their parallel implementation of the ARC3D code on the iPSC/860. A section of the paper considers I/O, which is to write out a large multidimensional matrix at each timestep.
Reference: [Rya91] <author> Steve Ryan. </author> <title> CFS workload demonstration code. </title> <address> WWW ftp://ftp.cs.dartmouth.edu/pub/pario/examples/CFS3D.tar.Z, </address> <month> July </month> <year> 1991. </year> <title> A simple program demonstrating CFS usage for ARC3D-like applications. </title>
Reference-contexts: They write out the data in any order, with a little mini-header in each chunk that describes the chunk. I/O was not a significant percentage of their run time on either the CM5 or C90. * [RHH95] weather simulation code * <ref> [RW93, Rya91] </ref> A paper, and corresponding I/O-template code, about aircraft simulation and computational fluid dynamics (Navier-Stokes flowfields). They describe their parallel implementation of the ARC3D code on the iPSC/860. A section of the paper considers I/O, which is to write out a large multidimensional matrix at each timestep.
Reference: [SACR96] <author> Evgenia Smirni, Ruth A. Aydt, Andrew A. Chien, and Daniel A. Reed. </author> <title> I/O requirements of scientific applications: An evolutionary view. </title> <booktitle> In Proceedings of the Fifth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 49-59, </pages> <year> 1996. </year>
Reference-contexts: They found a huge variation in request sizes, amount of I/O, number of files, and so forth. Their primary conclusion is thus that file systems should be adaptable to different access patterns, preferably under control of the application. * <ref> [SACR96] </ref> They study two applications (electron scattering and computational fluid dynamics) over several versions, using Pablo to capture the I/O activity. They thus watch as application developers improve the applications use of I/O modes and request sizes.
Reference: [TGL96a] <author> Rajeev Thakur, William Gropp, and Ewing Lusk. </author> <title> An experimental evaluation of the parallel I/O systems of the IBM SP and Intel Paragon using a production application. </title> <type> Technical Report MCS-P569-0296, </type> <institution> Argonne National Laboratory, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: Most of the I/O itself is not implemented in parallel, because they used UniTree on the SP, and because the Chameleon library sequentializes this kind of I/O through one node. * <ref> [TGL96a, TGL96b] </ref> They use an astrophysics application, which simulates the gravitational collapse of self-gravitating gaseous clouds, to compare the file systems of the Intel Paragon and the IBM SP-2. * [Woo93] This paper is interesting for its impressive usage of RAIDs and parallel networks to support scientific visualization.
Reference: [TGL96b] <author> Rajeev Thakur, William Gropp, and Ewing Lusk. </author> <title> An experimental evaluation of the parallel I/O systems of the IBM SP and Intel Paragon using a production application. </title> <booktitle> In Proceedings of the Third International Conference of the Austrian Center for Parallel Computation (ACPC), volume 1127 of Lecture Notes in Computer Science, </booktitle> <pages> pages 24-35. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1996. </year>
Reference-contexts: Most of the I/O itself is not implemented in parallel, because they used UniTree on the SP, and because the Chameleon library sequentializes this kind of I/O through one node. * <ref> [TGL96a, TGL96b] </ref> They use an astrophysics application, which simulates the gravitational collapse of self-gravitating gaseous clouds, to compare the file systems of the Intel Paragon and the IBM SP-2. * [Woo93] This paper is interesting for its impressive usage of RAIDs and parallel networks to support scientific visualization.
Reference: [TLG95] <author> Rajeev Thakur, Ewing Lusk, and William Gropp. </author> <title> I/O characterization of a portable astrophysics application on the IBM SP and Intel Paragon. </title> <type> Technical Report MCS-P534-0895, </type> <institution> Argonne National Laboratory, </institution> <month> August </month> <year> 1995. </year> <note> Revised October 1995. </note>
Reference-contexts: All access to files was sequential, though this may be due to the programmer's belief that the file system is sequential. Buffered I/O would help to make transfers bigger and more efficient, but there wasn't enough rereferencing to make caching useful. * <ref> [TLG95] </ref> They describe an astrophysics application, which "focuses on the study of the highly turbulent convective layers of late-type stars, like the sun, in which turbulent mixing plays a fundamental role in the redistribution of many physical ingredients of the star...." Its I/O usage is straightforward: it just writes its matrices
Reference: [Woo93] <author> Paul R. Woodward. </author> <title> Interactive scientific visualization of fluid flow. </title> <journal> IEEE Computer, </journal> <volume> 26(10) </volume> <pages> 13-25, </pages> <month> October </month> <year> 1993. </year> <month> 11 </month>
Reference-contexts: they used UniTree on the SP, and because the Chameleon library sequentializes this kind of I/O through one node. * [TGL96a, TGL96b] They use an astrophysics application, which simulates the gravitational collapse of self-gravitating gaseous clouds, to compare the file systems of the Intel Paragon and the IBM SP-2. * <ref> [Woo93] </ref> This paper is interesting for its impressive usage of RAIDs and parallel networks to support scientific visualization. In particular, the proposed Gigawall (a 10-foot by 6-foot gigapixel-per-second display) is run by 24 SGI processors and 32 9-disk RAIDs, connected to an MPP of some kind through an ATM switch.
References-found: 39

