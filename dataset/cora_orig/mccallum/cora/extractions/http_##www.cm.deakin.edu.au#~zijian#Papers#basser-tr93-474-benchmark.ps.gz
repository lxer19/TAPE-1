URL: http://www.cm.deakin.edu.au/~zijian/Papers/basser-tr93-474-benchmark.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/publications.html
Root-URL: 
Title: A BENCHMARK FOR CLASSIFIER LEARNING  
Author: ZIJIAN ZHENG 
Date: 2006  
Address: Sydney N.S.W. Australia  
Affiliation: Basser Department of Computer Science The University of  
Abstract: Technical Report 474 November 1993 
Abstract-found: 1
Intro-found: 1
Reference: [Aha et al., 1991] <author> D.W. Aha, D. Kibler, and M.K. Albert, </author> <title> Instance-based learning algorithms, </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 37-66, </pages> <year> 1991. </year>
Reference-contexts: divided into two types: * symbolic algorithms, such as C4.5 [Quinlan, 1993], CART [Breiman et al., 1984], PLS1 [Rendell, 1983, 1986], the AQ family of algorithms [Michalski and Chilausky, 1980]; and * sub-symbolic algorithms, such as Boole [Wilson, 1987], back-propagation [Ru-melhart et al., 1986], Perceptron [Rosenblatt, 1962], IB1, IB2, IB3 <ref> [Aha et al., 1991] </ref>, MDLA, B, C [Cameron-Jones, 1992], Nearest Neighbor, Bayes, and Linear Discriminant [Duda and Hart, 1973]. 1 All of these algorithms perform differently on different domains. In order to evaluate and compare the algorithms, many evaluation criteria have been developed. <p> Table 2 details the description of the benchmark using concrete dimension values of datasets. 4 A Demonstration of Using the Benchmark As a simple demonstration of using the benchmark, Table 3 gives the accuracy of the IB1 <ref> [Aha et al., 1991] </ref>, C4.5 [Quinlan, 1993], and CI2-2L [Zheng, 1992] algorithms obtained using a 10-fold cross-validation [Breiman et al., 1984], on each domain of the benchmark 4 . IB1 is a simple instance-based learning algorithm. C4.5 is the latest version of ID3, a well-known decision tree learning algorithm.
Reference: [Brazdil et al., 1993] <author> P. Brazdil, J. Gama, and B. Henery, </author> <title> Comparison of ML and statistical approaches using meta level learning, </title> <booktitle> Workshop Notes on Real-World Applications of Machine Learning, European Conference on Machine Learning, </booktitle> <year> 1993. </year>
Reference: [Breiman et al., 1984] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone, </author> <title> Classification And Regression Trees, </title> <address> Belmont, CA: </address> <publisher> Wadsworth, </publisher> <year> 1984. </year>
Reference-contexts: Induce: a theory that can predict the classes of unseen cases of the same domain. The learned theory defines classes in terms of attribute values. Algorithms for this task can be roughly divided into two types: * symbolic algorithms, such as C4.5 [Quinlan, 1993], CART <ref> [Breiman et al., 1984] </ref>, PLS1 [Rendell, 1983, 1986], the AQ family of algorithms [Michalski and Chilausky, 1980]; and * sub-symbolic algorithms, such as Boole [Wilson, 1987], back-propagation [Ru-melhart et al., 1986], Perceptron [Rosenblatt, 1962], IB1, IB2, IB3 [Aha et al., 1991], MDLA, B, C [Cameron-Jones, 1992], Nearest Neighbor, Bayes, and Linear <p> description of the benchmark using concrete dimension values of datasets. 4 A Demonstration of Using the Benchmark As a simple demonstration of using the benchmark, Table 3 gives the accuracy of the IB1 [Aha et al., 1991], C4.5 [Quinlan, 1993], and CI2-2L [Zheng, 1992] algorithms obtained using a 10-fold cross-validation <ref> [Breiman et al., 1984] </ref>, on each domain of the benchmark 4 . IB1 is a simple instance-based learning algorithm. C4.5 is the latest version of ID3, a well-known decision tree learning algorithm.
Reference: [Chatfield, 1978] <author> C. Chatfield, </author> <title> Statistics for Technology: A Course in Applied Statistics, </title> <publisher> Chapman abd Hall, </publisher> <year> 1978. </year>
Reference-contexts: The accuracy shown in boldface is significant higher than that in the preceding column at above the 95% level using an instance-based pairwised sign-test <ref> [Chatfield, 1978] </ref>. The accuracy shown in italics is significant lower than that in the preceding column at above the 95% level. From Table 3, we can see that the performance of C4.5 is obviously much better than that of IB1 on the LED-24 domain.
Reference: [Cameron-Jones, 1992] <author> R.M. Cameron-Jones, </author> <title> Minimum description length instance-based learning, </title> <booktitle> Proceedings of Australian Joint Conference on Artificial Intelligence, World Scientific Publisher, </booktitle> <pages> 368-373, </pages> <year> 1992. </year>
Reference-contexts: such as C4.5 [Quinlan, 1993], CART [Breiman et al., 1984], PLS1 [Rendell, 1983, 1986], the AQ family of algorithms [Michalski and Chilausky, 1980]; and * sub-symbolic algorithms, such as Boole [Wilson, 1987], back-propagation [Ru-melhart et al., 1986], Perceptron [Rosenblatt, 1962], IB1, IB2, IB3 [Aha et al., 1991], MDLA, B, C <ref> [Cameron-Jones, 1992] </ref>, Nearest Neighbor, Bayes, and Linear Discriminant [Duda and Hart, 1973]. 1 All of these algorithms perform differently on different domains. In order to evaluate and compare the algorithms, many evaluation criteria have been developed.
Reference: [Detrano et al., 1989] <author> R. Detrano, A. Janosi, W. Steinbrunn, M. Pfisterer, J. Schmid, S. Sandhu, K. Guppy, S. Lee, and V. Froelicher, </author> <title> International application of a new probability algorithm for the diagnosis of coronary artery disease, </title> <journal> American Journal of Cardiology, </journal> <volume> 64, </volume> <pages> 304-31, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Considerable advances have been made, in the field of classifier learning from examples, making it one of the most active research areas of machine learning. Many algorithms for this task have been developed and applied to problems from a variety of fields such as medical science <ref> [Detrano et al., 1989] </ref>, biology [Qian and Sejnowski, 1988], linguistics [Sejnowski and Rosenberg, 1987].
Reference: [Dietterich et al., 1990] <author> T.G. Dietterich, H. Hild, and G. Bakiri, </author> <title> A comparative study of ID3 and backpropagation for English text-to-speech mapping, </title> <booktitle> Proceedings of the Seventh International Workshop on Machine Learning, </booktitle> <pages> 24-31, </pages> <year> 1990. </year>
Reference-contexts: In real-world domains, such as medical domains, noise occurs due to errors introduced when measuring and diagnosing, and indeterminacy with respect to attributes. Noise in attribute-values and classes is inevitable, but the noise level often cannot be known. In the NetTalk domain <ref> [Dietterich et al., 1990] </ref>, because the window of 7 letters used is not large enough to uniquely determine the phoneme and stress of a letter, some instances with the same 7 letters might have different phonemes or stresses. We can therefore consider the class to be noisy or indeterminate.
Reference: [Duda and Hart, 1973] <author> R. Duda and P. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <address> New York: </address> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: al., 1984], PLS1 [Rendell, 1983, 1986], the AQ family of algorithms [Michalski and Chilausky, 1980]; and * sub-symbolic algorithms, such as Boole [Wilson, 1987], back-propagation [Ru-melhart et al., 1986], Perceptron [Rosenblatt, 1962], IB1, IB2, IB3 [Aha et al., 1991], MDLA, B, C [Cameron-Jones, 1992], Nearest Neighbor, Bayes, and Linear Discriminant <ref> [Duda and Hart, 1973] </ref>. 1 All of these algorithms perform differently on different domains. In order to evaluate and compare the algorithms, many evaluation criteria have been developed. The three most commonly used ones are predictive accuracy on a test set, the size of learned theory, and learning time.
Reference: [Holte, 1993] <author> R.C. Holte, </author> <title> Very simple classification rules perform well on most datasets, </title> <journal> Machine Learning, </journal> <volume> 11, </volume> <pages> 63-90, </pages> <year> 1993. </year> <month> 13 </month>
Reference: [Kononenko and Bratko, 1991] <author> I. Kononenko and I. Bratko, </author> <title> Information-based evalu-ation criterion for classifier's performance, </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 67-80, </pages> <year> 1991. </year>
Reference: [Lowe, 1993] <author> D.G. Lowe, </author> <title> Similarity metric learning for a variable-kernel classifier, </title> <type> Technical Report, </type> <institution> Computer Science Department, University of British Columbia, Canada, </institution> <year> 1993. </year>
Reference: [Michalski and Chilausky, 1980] <author> R.S. Michalski and R.L. Chilausky, </author> <title> Learning by being told and learning from examples, </title> <journal> International Journal of Policy Analysis and Information Systems, </journal> <volume> 4, </volume> <pages> 125-160, </pages> <year> 1980. </year>
Reference-contexts: The learned theory defines classes in terms of attribute values. Algorithms for this task can be roughly divided into two types: * symbolic algorithms, such as C4.5 [Quinlan, 1993], CART [Breiman et al., 1984], PLS1 [Rendell, 1983, 1986], the AQ family of algorithms <ref> [Michalski and Chilausky, 1980] </ref>; and * sub-symbolic algorithms, such as Boole [Wilson, 1987], back-propagation [Ru-melhart et al., 1986], Perceptron [Rosenblatt, 1962], IB1, IB2, IB3 [Aha et al., 1991], MDLA, B, C [Cameron-Jones, 1992], Nearest Neighbor, Bayes, and Linear Discriminant [Duda and Hart, 1973]. 1 All of these algorithms perform differently on
Reference: [Mooney et al., 1989] <author> R. Mooney, J. Shavlik, G. Towell, and A. Gove, </author> <title> An experimental comparison of symbolic and connectionist learning algorithms, </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> 775-780, </pages> <publisher> Morgan Kaufman, </publisher> <year> 1989. </year>
Reference: [Murphy and Aha, 1991] <author> P.M. Murphy and D.W. Aha, </author> <title> UCI Repository of machine learning databases [Machine-readable data repository]. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science, </institution> <year> 1991. </year>
Reference-contexts: It is very hard to judge an algorithm by seeing its performance on only a few arbitrarily selected domains. This problem was met when developing and evaluating the constructive induction learning algorithm CI [Zheng, 1992]. At first, twelve arbitrarily selected domains from the UCI Repository of machine learning databases <ref> [Murphy and Aha, 1991] </ref> were used. It was found that constructing new attributes can improve the performance of decision trees significantly on four domains, can improve the performance but without high significance level on five domains, and gains almost nothing on the three other domains. <p> 78.4 81.1 Monks-2 70.4 65.0 72.7 Mushroom 100.0 100.0 100.0 NetTalk (Phoneme) 73.9 81.1 82.8 Promoter 83.0 76.3 81.0 Soybean 91.1 91.5 93.9 Thyroid 97.1 99.1 99.1 Waveform-40 67.7 69.4 72.7 * The candidates were all real-world domains and well-known synthetic domains from the UCI Repository of machine learning databases <ref> [Murphy and Aha, 1991] </ref>. To ensure that the benchmark does not contain relatively unknown datasets, only datasets that have been referenced in the last three Proceedings of the International Workshop/Conference on Machine Learning were considered. This dual constraint guarantees that all datasets are commonly known and easily accessible.
Reference: [Qian and Sejnowski, 1988] <author> N. Qian and T.J. Sejnowski, </author> <title> Predicting the secondary structure of globular proteins using neural network models, </title> <journal> Journal of Molecular Biology, </journal> <volume> 202, </volume> <pages> 865-884, </pages> <publisher> Academic Press, </publisher> <year> 1988. </year>
Reference-contexts: Many algorithms for this task have been developed and applied to problems from a variety of fields such as medical science [Detrano et al., 1989], biology <ref> [Qian and Sejnowski, 1988] </ref>, linguistics [Sejnowski and Rosenberg, 1987]. The task of zeroth-order classifier learning from examples is generally in the form: Given: a set of examples, called the training set, in the form of a vector of attribute values and known class for each example.
Reference: [Quinlan, 1993] <author> J.R. Quinlan, C4.5: </author> <title> Programs for Machine Learning, </title> <address> San Meteo, CA: </address> <publisher> Morgan Kaufman, </publisher> <year> 1993. </year>
Reference-contexts: Induce: a theory that can predict the classes of unseen cases of the same domain. The learned theory defines classes in terms of attribute values. Algorithms for this task can be roughly divided into two types: * symbolic algorithms, such as C4.5 <ref> [Quinlan, 1993] </ref>, CART [Breiman et al., 1984], PLS1 [Rendell, 1983, 1986], the AQ family of algorithms [Michalski and Chilausky, 1980]; and * sub-symbolic algorithms, such as Boole [Wilson, 1987], back-propagation [Ru-melhart et al., 1986], Perceptron [Rosenblatt, 1962], IB1, IB2, IB3 [Aha et al., 1991], MDLA, B, C [Cameron-Jones, 1992], Nearest Neighbor, <p> Table 2 details the description of the benchmark using concrete dimension values of datasets. 4 A Demonstration of Using the Benchmark As a simple demonstration of using the benchmark, Table 3 gives the accuracy of the IB1 [Aha et al., 1991], C4.5 <ref> [Quinlan, 1993] </ref>, and CI2-2L [Zheng, 1992] algorithms obtained using a 10-fold cross-validation [Breiman et al., 1984], on each domain of the benchmark 4 . IB1 is a simple instance-based learning algorithm. C4.5 is the latest version of ID3, a well-known decision tree learning algorithm.
Reference: [Rendell, 1983] <author> L.A. Rendell, </author> <title> A new basis for state-space learning systems and a successful implementation, </title> <journal> Artificial Intelligence, </journal> <volume> 20, </volume> <pages> 369-392, </pages> <year> 1983. </year>
Reference-contexts: The learned theory defines classes in terms of attribute values. Algorithms for this task can be roughly divided into two types: * symbolic algorithms, such as C4.5 [Quinlan, 1993], CART [Breiman et al., 1984], PLS1 <ref> [Rendell, 1983, 1986] </ref>, the AQ family of algorithms [Michalski and Chilausky, 1980]; and * sub-symbolic algorithms, such as Boole [Wilson, 1987], back-propagation [Ru-melhart et al., 1986], Perceptron [Rosenblatt, 1962], IB1, IB2, IB3 [Aha et al., 1991], MDLA, B, C [Cameron-Jones, 1992], Nearest Neighbor, Bayes, and Linear Discriminant [Duda and Hart, 1973].
Reference: [Rendell, 1986] <author> L.A. Rendell, </author> <title> Induction, of and by probability, </title> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <address> Amsterdam: </address> <publisher> Elsevier Science Publishers, </publisher> <year> 1986. </year>
Reference: [Rosenlatt, 1962] <author> F. Rosenblatt, </author> <booktitle> Principles of Neuradynamics, </booktitle> <publisher> Spartan, </publisher> <address> New York, </address> <year> 1962. </year>
Reference: [Rumelhart et al., 1986] <author> D.E. Rumelhart, G.E. Hinton, and J.R. Williams, </author> <title> Learning internal representations by error propagation, </title> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. 1, </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, 318-362, </address> <year> 1986. </year>
Reference: [Schaffer, 1992] <author> C. Schaffer, </author> <title> Selecting a classification method by cross-validation, </title> <type> Technical Report, </type> <institution> Department of Computer Science, CUNY-Hunter College, </institution> <year> 1992. </year>
Reference: [Sejnowski and Rosenberg, 1987] <author> T.J. Sejnowski and C.R. Rosenberg, </author> <title> Parallel networks that learn to pronounce English text, </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 145-168, </pages> <year> 1987. </year>
Reference-contexts: Many algorithms for this task have been developed and applied to problems from a variety of fields such as medical science [Detrano et al., 1989], biology [Qian and Sejnowski, 1988], linguistics <ref> [Sejnowski and Rosenberg, 1987] </ref>. The task of zeroth-order classifier learning from examples is generally in the form: Given: a set of examples, called the training set, in the form of a vector of attribute values and known class for each example.
Reference: [Thrun et al., 1991] <author> S.B. Thrun, J. Bala, E. Bloedorn, I. Bratko, B. Cestnik, J. Cheng, K. De Jong, S. Dzeroski, S.E. Fahlman, D. Fisher, R. Hamann, K. Kauf-man, S. Keller, I. Kononenko, J. Kreuziger, R.S. Michalski, T. Mitchell, P. Pachowicz, Y. Reich, H. Vafaie, W. Van de Welde, W. Wenzel, J. Wnek, and J. Zhang, </author> <title> The MONK's problems a performance comparison of different learning algorithms, </title> <type> Technical Report: </type> <institution> CMU-CD-91-197, Carnegie Mellon University, </institution> <year> 1991. </year>
Reference: [Towell et al., 1991] <author> G.G. Towell, M.W. Craven, and J.W. Shavlik, </author> <title> Constructive induction in knowledge-based neural networks, </title> <booktitle> Proceedings of the Eighth International Conference on Machine Learning, </booktitle> <pages> 213-217, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference: [Weiss and Kapouleas, 1989] <author> S.M. Weiss and I. Kapouleas, </author> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods, </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> 781-787, </pages> <publisher> Morgan Kaufman, </publisher> <year> 1989. </year>
Reference: [Zheng, 1992] <author> Z. Zheng, </author> <title> Constructing conjunctive tests for decision trees, </title> <booktitle> Proceedings of Australian Joint Conference on Artificial Intelligence, World Scientific Publisher, </booktitle> <pages> 355-360, </pages> <year> 1992. </year> <month> 15 </month>
Reference-contexts: It is very hard to judge an algorithm by seeing its performance on only a few arbitrarily selected domains. This problem was met when developing and evaluating the constructive induction learning algorithm CI <ref> [Zheng, 1992] </ref>. At first, twelve arbitrarily selected domains from the UCI Repository of machine learning databases [Murphy and Aha, 1991] were used. <p> Table 2 details the description of the benchmark using concrete dimension values of datasets. 4 A Demonstration of Using the Benchmark As a simple demonstration of using the benchmark, Table 3 gives the accuracy of the IB1 [Aha et al., 1991], C4.5 [Quinlan, 1993], and CI2-2L <ref> [Zheng, 1992] </ref> algorithms obtained using a 10-fold cross-validation [Breiman et al., 1984], on each domain of the benchmark 4 . IB1 is a simple instance-based learning algorithm. C4.5 is the latest version of ID3, a well-known decision tree learning algorithm.
References-found: 26

