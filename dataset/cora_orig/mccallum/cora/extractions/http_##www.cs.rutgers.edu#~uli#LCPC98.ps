URL: http://www.cs.rutgers.edu/~uli/LCPC98.ps
Refering-URL: http://www.cs.rutgers.edu/~uli/pubs.html
Root-URL: http://www.cs.rutgers.edu
Email: email: uli@cs.rutgers.edu  
Title: Fortran RED A Retargetable Environment for Automatic Data Layout  
Author: Ulrich Kremer 
Note: This research was supported by DARPA contract DABT 63-93-C-0064 and experi ments were conducted using resources at the Cornell Theory Center.  
Affiliation: Department of Computer Science Rutgers University  
Abstract: The proliferation of parallel platforms over the last ten years has been dramatic. Parallel platforms come in different flavors, including desk-top multiprocessor PCs and workstations with a few processors, networks of PCs and workstations, and supercomputers with hundreds of processors or more. This diverse collection of parallel platforms provide not only computing cycles, but other important resources for scientific computing as well, such as large amounts of main memory and fast I/O capabilities. As a result of the proliferation of parallel platforms, the "typical profile" of a potential user of such systems has changed considerably. The specialist user who has a good understanding of the complexities of the target parallel system has been replaced by a user who is largely unfamiliar with the underlying system characteristics. While the specialist's main concern is peak performance, the non-specialist user may be willing to trade off performance for ease of programming. Recent languages such as High Performance Fortran (HPF) and SGI Parallel Fortran are a significant step towards making parallel platforms truly usable for a broadening user community. However, non-trivial user input is required to produce efficient parallel programs. The main challenge for a user is to understand the performance implications of a specified data layout, which requires knowledge about issues such as code generation and analysis strategies of the HPF compiler and its node compiler, and the performance characteristics of the target architecture. This paper discusses our preliminary experiences with the design and implementation of Fortran RED , a tool that supports Fortran as a deterministic, sequential programming model on different parallel target systems. The tool is not part of a compiler. Fortran RED uses HPF as its intermediate program representation since the language is portable across many parallel platforms, and commercial and research HPF compilers are widely available. Fortran RED is able to support different target HPF compilers and target architectures, and allows multi-dimensional distributions in addition to dynamic remapping. This paper focuses on the discussion of the performance prediction component of the tool and reports preliminary results for a single scientific kernel on two target systems, namely PGI's and IBM's HPF compilers with IBM's SP-2 as the target architecture. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> V. Adve, A. Carle, E. Granston, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, J. Mellor-Crummey, C-W. Tseng, and S. Warren. </author> <title> Requirements for data-parallel programming environments. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(3) </volume> <pages> 48-58, </pages> <year> 1994. </year>
Reference-contexts: This paper describes the design of a compositional performance predictor for an automatic data layout tool. A prototype tool based on our framework for automatic data layout has been implemented on top of the D System compiler infrastructure developed at Rice University <ref> [1] </ref>. The Fortran RED (Retargetable Environment for Datalayout) tool takes sequential Fortran programs as input and generates HPF programs as output. The input Fortran programs solve regular problems, i.e., use dense arrays as their main data structures.
Reference: 2. <author> J. Anderson. </author> <title> Automatic Computation and Data Decomposition for Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1997. </year>
Reference-contexts: The presented solutions differ significantly in the assumptions that are made about the input language, the possible set of data layouts, the compilation system, and the target machine architecture. Our work is similar in nature to the recent work done by Anderson and Lam at Stanford University <ref> [3, 2] </ref>, Chatterjee, Gilbert, Schreiber, She*er, and Pugh at RIACS, Xerox Parc, and the University of Maryland [37, 8], Palermo and Banerjee at the University of Illinois at Urbana Champaign [31, 30], Ayguade, Garcia, Girones, Labarta, Torres and Valero at the University of Catalunya in Barcelona, [14, 13], and Ning, Van
Reference: 3. <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 112-125, </pages> <year> 1993. </year>
Reference-contexts: However, it is interesting to note that we were able to find this bug because of our performance prediction results. 5 Related Work The problem of automatic data layout has been addressed by many researchers <ref> [3, 9, 16, 20, 23, 24, 25, 34, 6, 21] </ref>. The presented solutions differ significantly in the assumptions that are made about the input language, the possible set of data layouts, the compilation system, and the target machine architecture. <p> The presented solutions differ significantly in the assumptions that are made about the input language, the possible set of data layouts, the compilation system, and the target machine architecture. Our work is similar in nature to the recent work done by Anderson and Lam at Stanford University <ref> [3, 2] </ref>, Chatterjee, Gilbert, Schreiber, She*er, and Pugh at RIACS, Xerox Parc, and the University of Maryland [37, 8], Palermo and Banerjee at the University of Illinois at Urbana Champaign [31, 30], Ayguade, Garcia, Girones, Labarta, Torres and Valero at the University of Catalunya in Barcelona, [14, 13], and Ning, Van
Reference: 4. <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> An interactive environment for data partitioning and distribution. </title> <booktitle> In Proceedings of the 5th Distributed Memory Computing Conference, </booktitle> <pages> pages 1160-1170, </pages> <year> 1990. </year>
Reference-contexts: Currently supported communication patterns are shift, send receive, broadcast, and reduction. Compiler flags are used to enable communication optimizations such as message vectorization, message coalescing, and message aggregation <ref> [39, 15, 4] </ref>. These optimizations are simulated using array subscript dependence analysis to determine when the communication occurs and RSDs information to describe what data elements have to be communicated.
Reference: 5. <author> V. Balasundaram, G. Fox, K. Kennedy, and U. Kremer. </author> <title> A static performance es-timator to guide data partitioning decisions. </title> <booktitle> In Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 213-223, </pages> <year> 1991. </year>
Reference-contexts: A crucial component within Fortran RED is performance prediction. Our work on performance prediction in the context of Fortran RED has been based on our previous work on "training sets" <ref> [5] </ref>. This work is very similar to the micro-benchmarking approach developed by Saavedra and Smith [36, 35]. The importance of performance prediction for optimizing compilers has been recognized by many researchers and several proposals have been published in the literature, such as [40, 33, 10, 29].
Reference: 6. <author> D. Bau, I. Kodukula, V. Kotlyar, K. Pingali, and P. Stodghill. </author> <title> Solving alignment using elementary linear algebra. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, New York, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: However, it is interesting to note that we were able to find this bug because of our performance prediction results. 5 Related Work The problem of automatic data layout has been addressed by many researchers <ref> [3, 9, 16, 20, 23, 24, 25, 34, 6, 21] </ref>. The presented solutions differ significantly in the assumptions that are made about the input language, the possible set of data layouts, the compilation system, and the target machine architecture.
Reference: 7. <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Athens, Greece, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: In the first step, local iteration sets are computed that represent local loop iterations that have to be executed by each node processor. Based on these iterations, a data structure is built for each phase that maps phase loop levels to a sets of regular section descriptors (RSDs <ref> [7, 17] </ref>) that describe the generated communication patterns. Currently supported communication patterns are shift, send receive, broadcast, and reduction. Compiler flags are used to enable communication optimizations such as message vectorization, message coalescing, and message aggregation [39, 15, 4].
Reference: 8. <author> S. Chatterjee, J. R. Gilbert, R. Schreiber, and T. She*er. </author> <title> Array distribution in data-parallel programs. </title> <booktitle> In Proceedings of the Seventh Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, New York, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: Our work is similar in nature to the recent work done by Anderson and Lam at Stanford University [3, 2], Chatterjee, Gilbert, Schreiber, She*er, and Pugh at RIACS, Xerox Parc, and the University of Maryland <ref> [37, 8] </ref>, Palermo and Banerjee at the University of Illinois at Urbana Champaign [31, 30], Ayguade, Garcia, Girones, Labarta, Torres and Valero at the University of Catalunya in Barcelona, [14, 13], and Ning, Van Dongen, and Gao at CRIM and McGill University [28].
Reference: 9. <author> S. Chatterjee, J.R. Gilbert, R. Schreiber, and S-H. Teng. </author> <title> Automatic array alignment in data-parallel programs. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 16-28, </pages> <year> 1993. </year>
Reference-contexts: However, it is interesting to note that we were able to find this bug because of our performance prediction results. 5 Related Work The problem of automatic data layout has been addressed by many researchers <ref> [3, 9, 16, 20, 23, 24, 25, 34, 6, 21] </ref>. The presented solutions differ significantly in the assumptions that are made about the input language, the possible set of data layouts, the compilation system, and the target machine architecture.
Reference: 10. <author> T. Fahringer and H.P. Zima. </author> <title> A static parameter based performance prediction tool for parallel programs. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: This work is very similar to the micro-benchmarking approach developed by Saavedra and Smith [36, 35]. The importance of performance prediction for optimizing compilers has been recognized by many researchers and several proposals have been published in the literature, such as <ref> [40, 33, 10, 29] </ref>.
Reference: 11. <author> J. Ferrante, V. Sarkar, and W. Thrash. </author> <title> On estimating and enhancing cache effectiveness. </title> <booktitle> Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing - LNCS, </booktitle> <publisher> Springer Verlag, </publisher> <pages> 589 328-343, </pages> <year> 1991. </year>
Reference-contexts: Spatial or temporal reuse across iterations of the innermost loop, or cache conflict misses [18] are not considered in the current model. However, a model to predict the number of capacity misses is used similar to the model introduced by Ferrante, Sarkar, and Thrash <ref> [11] </ref>. All array references in the loop are partitioned into RefGroups, where each Re-fGroup contributes a single cache miss penalty to the overall cache miss penalty for an innermost loop iteration.
Reference: 12. <author> G. Fox, M. Johnson, G. Lyzenga, S. Otto, J. Salmon, and D. Walker. </author> <title> Solving Problems on Concurrent Processors. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: Communication inside a phase may lead to a pipelined execution of the loop. Communication outside of the phase may result in a loosely synchronous execution scheme <ref> [12] </ref>. In addition, special communication patterns may be recognized that represent global operations such as reductions. Based on the synchronization schemes and the costs for simple communication patterns and basic computations, the execution model determines the overall cost estimate for a candidate layout and its phase.
Reference: 13. <author> J. Garcia. </author> <title> Automatic Data Distribution for Massively Parallel Processors. </title> <type> PhD thesis, </type> <institution> Universitat Politecnica de Catalunya, Barcelona, </institution> <month> April </month> <year> 1997. </year>
Reference-contexts: Lam at Stanford University [3, 2], Chatterjee, Gilbert, Schreiber, She*er, and Pugh at RIACS, Xerox Parc, and the University of Maryland [37, 8], Palermo and Banerjee at the University of Illinois at Urbana Champaign [31, 30], Ayguade, Garcia, Girones, Labarta, Torres and Valero at the University of Catalunya in Barcelona, <ref> [14, 13] </ref>, and Ning, Van Dongen, and Gao at CRIM and McGill University [28]. In contrast to previous work, Fortran RED is design to target different compilation systems and parallel architectures, allowing program portability across a range of parallel platforms. A crucial component within Fortran RED is performance prediction.
Reference: 14. <author> J. Garcia, E. Ayguade, and J. Labarta. </author> <title> Dynamic data distribution with control flow analysis. </title> <booktitle> In Proceedings of Supercomputing '96, </booktitle> <year> 1996. </year>
Reference-contexts: Lam at Stanford University [3, 2], Chatterjee, Gilbert, Schreiber, She*er, and Pugh at RIACS, Xerox Parc, and the University of Maryland [37, 8], Palermo and Banerjee at the University of Illinois at Urbana Champaign [31, 30], Ayguade, Garcia, Girones, Labarta, Torres and Valero at the University of Catalunya in Barcelona, <ref> [14, 13] </ref>, and Ning, Van Dongen, and Gao at CRIM and McGill University [28]. In contrast to previous work, Fortran RED is design to target different compilation systems and parallel architectures, allowing program portability across a range of parallel platforms. A crucial component within Fortran RED is performance prediction.
Reference: 15. <author> M. Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency| Practice & Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Currently supported communication patterns are shift, send receive, broadcast, and reduction. Compiler flags are used to enable communication optimizations such as message vectorization, message coalescing, and message aggregation <ref> [39, 15, 4] </ref>. These optimizations are simulated using array subscript dependence analysis to determine when the communication occurs and RSDs information to describe what data elements have to be communicated.
Reference: 16. <author> M. Gupta. </author> <title> Automatic Data Partitioning on Distributed Memory Multicomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: However, it is interesting to note that we were able to find this bug because of our performance prediction results. 5 Related Work The problem of automatic data layout has been addressed by many researchers <ref> [3, 9, 16, 20, 23, 24, 25, 34, 6, 21] </ref>. The presented solutions differ significantly in the assumptions that are made about the input language, the possible set of data layouts, the compilation system, and the target machine architecture.
Reference: 17. <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: In the first step, local iteration sets are computed that represent local loop iterations that have to be executed by each node processor. Based on these iterations, a data structure is built for each phase that maps phase loop levels to a sets of regular section descriptors (RSDs <ref> [7, 17] </ref>) that describe the generated communication patterns. Currently supported communication patterns are shift, send receive, broadcast, and reduction. Compiler flags are used to enable communication optimizations such as message vectorization, message coalescing, and message aggregation [39, 15, 4].
Reference: 18. <author> J. Hennessy and D. Patterson. </author> <title> Computer Architecture A Quantitative Approach (2nd edition). </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1996. </year>
Reference-contexts: Our model is simpler in the sense that it relies on basic pattern matching to identify spatial and temporal reuse. Spatial or temporal reuse across iterations of the innermost loop, or cache conflict misses <ref> [18] </ref> are not considered in the current model. However, a model to predict the number of capacity misses is used similar to the model introduced by Ferrante, Sarkar, and Thrash [11].
Reference: 19. <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification. </title> <booktitle> Scientific Programming, </booktitle> <address> 2(1-2):1-170, </address> <year> 1993. </year>
Reference-contexts: The potential user of a parallel system may not be familiar or willing to deal with the complexities of efficient parallel programming. As a consequence, valuable computing resources may be underutilized or even wasted. Languages such as High Performance Fortran (HPF) <ref> [19] </ref> or SGI Parallel Fortran [38] provide a shared name space programming model augmented by directives that specify how the data is mapped onto the individual processors. The user of such languages is able to avoid many complexities of explicit parallel programming.
Reference: 20. <author> D. Hudak and S. Abraham. </author> <title> Compiler techniques for data partitioning of sequentially iterated parallel loops. </title> <booktitle> In Proceedings of the 1990 ACM International Conference on Supercomputing, </booktitle> <pages> pages 187-200, </pages> <year> 1990. </year>
Reference-contexts: However, it is interesting to note that we were able to find this bug because of our performance prediction results. 5 Related Work The problem of automatic data layout has been addressed by many researchers <ref> [3, 9, 16, 20, 23, 24, 25, 34, 6, 21] </ref>. The presented solutions differ significantly in the assumptions that are made about the input language, the possible set of data layouts, the compilation system, and the target machine architecture.
Reference: 21. <author> W. Kelly and W. Pugh. </author> <title> Minimizing communication while preserving parallelism. </title> <booktitle> In Proceedings of the 1996 ACM International Conference on Supercomputing, </booktitle> <pages> pages 52-60, </pages> <address> Philadelphia, PA, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: However, it is interesting to note that we were able to find this bug because of our performance prediction results. 5 Related Work The problem of automatic data layout has been addressed by many researchers <ref> [3, 9, 16, 20, 23, 24, 25, 34, 6, 21] </ref>. The presented solutions differ significantly in the assumptions that are made about the input language, the possible set of data layouts, the compilation system, and the target machine architecture.
Reference: 22. <author> K. Kennedy and U. Kremer. </author> <title> Automatic data layout for distributed memory machines. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 20(4) </volume> <pages> 869-916, </pages> <month> July </month> <year> 1998. </year>
Reference-contexts: The resulting measurements are stored in a performance data base and accessed by the tool during program analysis. A detailed discussion of our general framework for automatic data layout can be found in <ref> [22] </ref>. The paper is organized as follows. Section 2 presents the scientific program kernel used in our preliminary experiments. The example program that shows the need for a tool such as Fortran RED . Section 3 discusses our compositional performance prediction approach in detail.
Reference: 23. <author> K. Knobe, J.D. Lukas, and W.J. Dally. </author> <title> Dynamic alignment on distributed memory systems. </title> <booktitle> In Proceedings of the Third Workshop on Compilers for Parallel Computers, </booktitle> <address> Vienna, Austria, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: However, it is interesting to note that we were able to find this bug because of our performance prediction results. 5 Related Work The problem of automatic data layout has been addressed by many researchers <ref> [3, 9, 16, 20, 23, 24, 25, 34, 6, 21] </ref>. The presented solutions differ significantly in the assumptions that are made about the input language, the possible set of data layouts, the compilation system, and the target machine architecture.
Reference: 24. <author> P. Lee and T-B. Tsai. </author> <title> Compiling efficient programs for tightly-coupled distributed memory computers. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, pages II161-II165, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: However, it is interesting to note that we were able to find this bug because of our performance prediction results. 5 Related Work The problem of automatic data layout has been addressed by many researchers <ref> [3, 9, 16, 20, 23, 24, 25, 34, 6, 21] </ref>. The presented solutions differ significantly in the assumptions that are made about the input language, the possible set of data layouts, the compilation system, and the target machine architecture.
Reference: 25. <author> J. Li and M. Chen. </author> <title> Index domain alignment: Minimizing cost of cross-referencing between distributed arrays. </title> <booktitle> In Frontiers90: The 3rd Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, MD, </address> <month> October </month> <year> 1990. </year>
Reference-contexts: However, it is interesting to note that we were able to find this bug because of our performance prediction results. 5 Related Work The problem of automatic data layout has been addressed by many researchers <ref> [3, 9, 16, 20, 23, 24, 25, 34, 6, 21] </ref>. The presented solutions differ significantly in the assumptions that are made about the input language, the possible set of data layouts, the compilation system, and the target machine architecture.
Reference: 26. <author> J. Mellor-Crummey, V. Adve, and C. Koelbel. </author> <title> The Compiler's Role in Analysis and Tuning of Data-Parallel Programs. </title> <booktitle> In Proceedings of The Second Workshop on Environments and Tools for Parallel Scientific Computing, </booktitle> <pages> pages 211-220, </pages> <year> 1994. </year>
Reference-contexts: Performance prediction of a pipelined phase execution is more complicated than in the loosely synchronous case due to the structure of the underlying critical execution path. Execution models that can estimate pipelines of different granularity have been discussed in the literature, for instance in <ref> [26, 32] </ref>. For a pipelined phase, Fortran RED uses the innermost level that carries a true dependence to determine the granularity of the pipeline. This level is referred to as the pipeline level. <p> The pipeline model implemented in Fortran RED is similar to the model described by Mellor-Crummey, Adve, and Koelbel <ref> [26] </ref>, and allows pipelined execution across different distributed dimensions, i.e., multi-dimensional pipelines. For the special case of a reduction phase, Fortran RED determines the kind of reduction operation and its data type, for instance min, max, or sum of type double precision.
Reference: 27. <author> K. S. M c Kinley, S. Carr, and C.-W. Tseng. </author> <title> Improving data locality with loop transformations. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 18(4) </volume> <pages> 424-453, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: The cache model to determine the overall cache miss penalty is based on the work by McKinley, Carr and Tseng <ref> [27] </ref>. Our model is simpler in the sense that it relies on basic pattern matching to identify spatial and temporal reuse. Spatial or temporal reuse across iterations of the innermost loop, or cache conflict misses [18] are not considered in the current model.
Reference: 28. <author> Q. Ning, V. V. Dongen, and G. R. Gao. </author> <title> Automatic data and computation decomposition for distributed memory machines. </title> <booktitle> In Proceedings of the 28th Annual Hawaii International Conference on System Sciences, </booktitle> <address> Maui, Hawaii, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: RIACS, Xerox Parc, and the University of Maryland [37, 8], Palermo and Banerjee at the University of Illinois at Urbana Champaign [31, 30], Ayguade, Garcia, Girones, Labarta, Torres and Valero at the University of Catalunya in Barcelona, [14, 13], and Ning, Van Dongen, and Gao at CRIM and McGill University <ref> [28] </ref>. In contrast to previous work, Fortran RED is design to target different compilation systems and parallel architectures, allowing program portability across a range of parallel platforms. A crucial component within Fortran RED is performance prediction.
Reference: 29. <author> D.B. Noonburg and J.P. Shen. </author> <title> Theoretical modeling of superscalar processor performance. </title> <booktitle> In Proceedings of the 27th Annual International Symposium on Microar-chitecture, </booktitle> <address> San Jose, CA, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: This work is very similar to the micro-benchmarking approach developed by Saavedra and Smith [36, 35]. The importance of performance prediction for optimizing compilers has been recognized by many researchers and several proposals have been published in the literature, such as <ref> [40, 33, 10, 29] </ref>.
Reference: 30. <author> D. Palermo. </author> <title> Compiler Techniques for Optimizing Communication and Data Distribution for Distributed-Memory Multicomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> June </month> <year> 1996. </year> <note> Available as CRHC-96-09. </note>
Reference-contexts: Our work is similar in nature to the recent work done by Anderson and Lam at Stanford University [3, 2], Chatterjee, Gilbert, Schreiber, She*er, and Pugh at RIACS, Xerox Parc, and the University of Maryland [37, 8], Palermo and Banerjee at the University of Illinois at Urbana Champaign <ref> [31, 30] </ref>, Ayguade, Garcia, Girones, Labarta, Torres and Valero at the University of Catalunya in Barcelona, [14, 13], and Ning, Van Dongen, and Gao at CRIM and McGill University [28].
Reference: 31. <author> D. Palermo and P. Banerjee. </author> <title> Automatic selection of dynamic data partitioning schemes for distributed-memory multicomputers. </title> <type> Technical Report CRHC-95-09, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: Our work is similar in nature to the recent work done by Anderson and Lam at Stanford University [3, 2], Chatterjee, Gilbert, Schreiber, She*er, and Pugh at RIACS, Xerox Parc, and the University of Maryland [37, 8], Palermo and Banerjee at the University of Illinois at Urbana Champaign <ref> [31, 30] </ref>, Ayguade, Garcia, Girones, Labarta, Torres and Valero at the University of Catalunya in Barcelona, [14, 13], and Ning, Van Dongen, and Gao at CRIM and McGill University [28].
Reference: 32. <author> D. Palermo, E. Su, J. A. Chandy, and P. Banerjee. </author> <title> Communication optimizations used in the PARADIGM compiler for distributed-memory multicomputers. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <year> 1994. </year>
Reference-contexts: Performance prediction of a pipelined phase execution is more complicated than in the loosely synchronous case due to the structure of the underlying critical execution path. Execution models that can estimate pipelines of different granularity have been discussed in the literature, for instance in <ref> [26, 32] </ref>. For a pipelined phase, Fortran RED uses the innermost level that carries a true dependence to determine the granularity of the pipeline. This level is referred to as the pipeline level.
Reference: 33. <author> M. Parashar, S. Hariri, H. Haupt, and G. Fox. </author> <title> Interpreting the performance of HPF/Fortran90D. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <year> 1994. </year>
Reference-contexts: This work is very similar to the micro-benchmarking approach developed by Saavedra and Smith [36, 35]. The importance of performance prediction for optimizing compilers has been recognized by many researchers and several proposals have been published in the literature, such as <ref> [40, 33, 10, 29] </ref>.
Reference: 34. <author> J. Ramanujam and P. Sadayappan. </author> <title> A methodology for parallelizing programs for multicomputers and complex memory multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 637-646, </pages> <address> Reno, NV, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: However, it is interesting to note that we were able to find this bug because of our performance prediction results. 5 Related Work The problem of automatic data layout has been addressed by many researchers <ref> [3, 9, 16, 20, 23, 24, 25, 34, 6, 21] </ref>. The presented solutions differ significantly in the assumptions that are made about the input language, the possible set of data layouts, the compilation system, and the target machine architecture.
Reference: 35. <author> R.H. Saavedra and A.J. Smith. </author> <title> Performance characterization of optimizing compilers. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 21(7) </volume> <pages> 615-628, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: A crucial component within Fortran RED is performance prediction. Our work on performance prediction in the context of Fortran RED has been based on our previous work on "training sets" [5]. This work is very similar to the micro-benchmarking approach developed by Saavedra and Smith <ref> [36, 35] </ref>. The importance of performance prediction for optimizing compilers has been recognized by many researchers and several proposals have been published in the literature, such as [40, 33, 10, 29].
Reference: 36. <author> R.H. Saavedra-Barrera. </author> <title> CPU Performance Evaluation and Execution Time Prediction Using Narrow Spectrum Benchmarking. </title> <type> PhD thesis, </type> <institution> U.C. Berkeley, </institution> <month> February </month> <year> 1992. </year> <month> UCB/CSD-92-684. </month>
Reference-contexts: A crucial component within Fortran RED is performance prediction. Our work on performance prediction in the context of Fortran RED has been based on our previous work on "training sets" [5]. This work is very similar to the micro-benchmarking approach developed by Saavedra and Smith <ref> [36, 35] </ref>. The importance of performance prediction for optimizing compilers has been recognized by many researchers and several proposals have been published in the literature, such as [40, 33, 10, 29].
Reference: 37. <author> T. She*er, R. Schreiber, W. Pugh, J.R. Gilbert, and S. Chatterjee. </author> <title> Efficient distribution analysis via graph contraction. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 24(6) </volume> <pages> 599-620, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: Our work is similar in nature to the recent work done by Anderson and Lam at Stanford University [3, 2], Chatterjee, Gilbert, Schreiber, She*er, and Pugh at RIACS, Xerox Parc, and the University of Maryland <ref> [37, 8] </ref>, Palermo and Banerjee at the University of Illinois at Urbana Champaign [31, 30], Ayguade, Garcia, Girones, Labarta, Torres and Valero at the University of Catalunya in Barcelona, [14, 13], and Ning, Van Dongen, and Gao at CRIM and McGill University [28].
Reference: 38. <institution> Silicon Graphics Inc. </institution> <note> F77 User's Manual: Chapter 6 Parallel Programming on Origin2000, </note> <year> 1997. </year>
Reference-contexts: The potential user of a parallel system may not be familiar or willing to deal with the complexities of efficient parallel programming. As a consequence, valuable computing resources may be underutilized or even wasted. Languages such as High Performance Fortran (HPF) [19] or SGI Parallel Fortran <ref> [38] </ref> provide a shared name space programming model augmented by directives that specify how the data is mapped onto the individual processors. The user of such languages is able to avoid many complexities of explicit parallel programming.
Reference: 39. <author> C-W. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year> <institution> Rice COMP TR93-199. </institution>
Reference-contexts: Currently supported communication patterns are shift, send receive, broadcast, and reduction. Compiler flags are used to enable communication optimizations such as message vectorization, message coalescing, and message aggregation <ref> [39, 15, 4] </ref>. These optimizations are simulated using array subscript dependence analysis to determine when the communication occurs and RSDs information to describe what data elements have to be communicated. <p> However, runtime optimizations are more restricted since the placement and type of communication routines are determined at compile time. Both HPF compilers use message vectorization and message coalescing, but only IBM's compiler performs message aggregation <ref> [39] </ref>. Both compilation systems perform software pipeling at the node program level. A summary of the HPF compiler characteristics is given in Table 2. These characteristics were provided to Fortran RED as the description of the target compiler. For the experiments, our prototype system simulated message vector-ization and coalescing.
Reference: 40. <author> K-Y. Wang. </author> <title> Precise compile-time performance prediction for superscalar-based computers. </title> <booktitle> In Proceedings of the SIGPLAN '94 Conference on Programming Language Design and Implementation, </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: This work is very similar to the micro-benchmarking approach developed by Saavedra and Smith [36, 35]. The importance of performance prediction for optimizing compilers has been recognized by many researchers and several proposals have been published in the literature, such as <ref> [40, 33, 10, 29] </ref>.
Reference: 41. <author> D. Wood and M. Hill. </author> <title> Cost-effective parallel computing. </title> <booktitle> IEEE Computer, </booktitle> <year> 1995. </year>
Reference-contexts: These program parts are called program phases and are automatically determined by 2 In fact, some researchers argue that machines with a large main memory should always have multiple processors in order to make cost-effective use of the memory's capacity and bandwidth <ref> [41] </ref>. Fig. 1. Overview of Fortran RED the tool. After the generation of candidate layout search spaces for each phase, a single candidate layout is selected from each search space, resulting in a data layout for the entire program.
References-found: 41

