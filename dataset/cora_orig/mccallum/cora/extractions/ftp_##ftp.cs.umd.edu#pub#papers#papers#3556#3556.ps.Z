URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3556/3556.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Conjugate Gradients and Related KMP Algorithms: The Beginnings  
Author: Dianne P. O'Leary 
Date: November 7, 1995  
Abstract: In the late 1940's and early 1950's, newly available computing machines generated intense interest in solving "large" systems of linear equations. Among the algorithms developed were several related methods, all of which generated bases for Krylov subspaces and used the bases to minimize or orthogonally project a measure of error. These methods include the conjugate gradient algorithm and the Lanczos algorithm. We refer to these algorithms as the KMP family and discuss its origins, emphasizing research themes that continue to have central importance.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. E. </author> <title> Arnoldi, The principle of minimized iterations in the solution of the matrix eigenvalue problem, </title> <journal> Quarterly of Appl. Math., </journal> <volume> 9 (1951), </volume> <pages> pp. 17-29. </pages>
Reference-contexts: Lanczos' idea was further developed by W. E. Arnoldi, who reinterpreted the algorithm and then derived a new one, reducing a general matrix to upper Hessenberg form <ref> [1] </ref>. Arnoldi presented the algorithm as iterative in nature, and discussed filtering the right-hand side before generating the Krylov sequence.
Reference: [2] <author> O. Axelsson, </author> <title> A generalized SSOR method, </title> <journal> BIT, </journal> <volume> 12 (1972), </volume> <pages> pp. 443-467. </pages>
Reference-contexts: This work inspired the hope of having a library KMP Algorithms 5 of preconditioners that would apply to broad problem classes. Preconditioning was also discussed by Owe Axelsson <ref> [2] </ref>, by Paul Concus, Gene Golub, and Dianne O'Leary [10], and for nonlinear problems by Jim Douglas and Todd Dupont [15]. Two important extensions of the conjugate gradient algorithm were given during this period.
Reference: [3] <author> R. Barrett, M. Berry, T. F. Chan, J. Demmel, J. Donato, J. Dongarra, V. Eijkhout, R. Pozo, C. Romine, and H. van der Vorst, </author> <title> Templates for the Solution of Linear Systems, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1993. </year> <note> 6 O'Leary </note>
Reference-contexts: This includes the methods of Lanczos, Arnoldi, conjugate gradients, GMRES, CGS, QMR, and an alphabet soup of other algorithms. The following sections discuss some of the key early steps in the development of the KMP family. A reader interested in recent developments could begin with [24], <ref> [3] </ref>, and the papers in this volume. fl This work was supported by the National Science Foundation under Grant CCR-95-03126 y Department of Computer Science and Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742 (oleary@cs.umd.edu). 1 2 O'Leary 2 The Beginnings Researchers in the late 1940's and
Reference: [4] <author> G. G. Belford and E. H. Kaufman,Jr., </author> <title> An application of approximation theory to an error estimate in linear algebra, </title> <journal> Math. of Comp., </journal> <volume> 28 (1974), </volume> <pages> pp. 711-712. </pages>
Reference-contexts: Major steps in understanding the convergence behavior of the conjugate gradient algorithm were made by Kaniel [34] and Daniel [12]. Although both papers contained some errors <ref> [4, 13, 8] </ref>, the standard bounds on the convergence rate for conjugate gradients, derived using Chebyshev polynomials, can be found here.
Reference: [5] <author> B. L. Bierson, </author> <title> A discrete-variable approximation to optimal flight paths, </title> <journal> Astronautica Acta, </journal> <volume> 14 (1969), </volume> <pages> pp. 157-169. </pages>
Reference-contexts: not much in favor among numerical mathematicians, they did achieve considerable success in applications in spectral analysis [6], lens design [19], geodesy [16], polar circulation [7], infrared spectral analysis [45], optimal control [48], collision theory [18], structural analysis [23], pattern recognition [39], power system load flow [53], optimal flight paths <ref> [5] </ref>, nonrelativistic scattering [25], network analysis [35], and nuclear shell computation [47]. Some important work was also occurring in extending the usefulness of the KMP family. Wachspress showed the effectiveness of the algorithm preconditioned by alternating direction implicit methods (ADI) in solving discretizations of partial differential equations [52].
Reference: [6] <author> A. A. Bothner-By and C. Naar-Colin, </author> <title> The proton magnetic resonance spectra of 2,3-disubstituted n-butanes, </title> <journal> J. of the ACS, </journal> <volume> 84 (1962), </volume> <pages> pp. 743-747. </pages>
Reference-contexts: Although the algorithms were not much in favor among numerical mathematicians, they did achieve considerable success in applications in spectral analysis <ref> [6] </ref>, lens design [19], geodesy [16], polar circulation [7], infrared spectral analysis [45], optimal control [48], collision theory [18], structural analysis [23], pattern recognition [39], power system load flow [53], optimal flight paths [5], nonrelativistic scattering [25], network analysis [35], and nuclear shell computation [47].
Reference: [7] <author> W. J. Campbell, </author> <title> The wind-driven circulation of ice and water in a polar ocean, </title> <journal> J. of Geophysical Research, </journal> <volume> 70 (1965), </volume> <pages> pp. 3279-3301. </pages>
Reference-contexts: Although the algorithms were not much in favor among numerical mathematicians, they did achieve considerable success in applications in spectral analysis [6], lens design [19], geodesy [16], polar circulation <ref> [7] </ref>, infrared spectral analysis [45], optimal control [48], collision theory [18], structural analysis [23], pattern recognition [39], power system load flow [53], optimal flight paths [5], nonrelativistic scattering [25], network analysis [35], and nuclear shell computation [47].
Reference: [8] <author> A. I. Cohen, </author> <title> Rate of convergence of several conjugate gradient algorithms, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 9 (1972), </volume> <pages> pp. 248-259. </pages>
Reference-contexts: Major steps in understanding the convergence behavior of the conjugate gradient algorithm were made by Kaniel [34] and Daniel [12]. Although both papers contained some errors <ref> [4, 13, 8] </ref>, the standard bounds on the convergence rate for conjugate gradients, derived using Chebyshev polynomials, can be found here.
Reference: [9] <author> P. Concus and G. H. Golub, </author> <title> A generalized conjugate gradient method for nonsymmetric systems of linear equations, </title> <booktitle> in Computing Methods in Applied Sciences and Engineering (2nd Internat. Symposium, Versailles 1975), Part I, </booktitle> <editor> R. Glowinski and J. L. Lions, eds., </editor> <booktitle> Springer Lecture Notes in Econ. and Math. Systems 134, </booktitle> <address> New York, </address> <year> 1976, </year> <pages> pp. 56-65. </pages>
Reference-contexts: Two important extensions of the conjugate gradient algorithm were given during this period. Paul Concus, Gene Golub, and Olof Widlund solved problems in which the Hermitian part of the matrix was positive definite and could be used as a preconditioner <ref> [9, 54] </ref>. Chris Paige and Michael Saunders showed how to compute iterates in case a matrix was indefinite [44], resulting in SYMMLQ and related algorithms. Research on the eigenvalue problem also contributed to making the KMP algorithms more useful for solving linear systems.
Reference: [10] <author> P. Concus, G. H. Golub, and D. P. O'Leary, </author> <title> A generalized conjugate gradient method for the numerical solution of elliptic partial differential equations, in Sparse Matrix Computations, </title> <editor> J. R. Bunch and D. J. Rose, eds., </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976, </year> <pages> pp. 309-332. </pages>
Reference-contexts: This work inspired the hope of having a library KMP Algorithms 5 of preconditioners that would apply to broad problem classes. Preconditioning was also discussed by Owe Axelsson [2], by Paul Concus, Gene Golub, and Dianne O'Leary <ref> [10] </ref>, and for nonlinear problems by Jim Douglas and Todd Dupont [15]. Two important extensions of the conjugate gradient algorithm were given during this period.
Reference: [11] <author> J. Cullum and R. A. Willoughby, </author> <title> Lanczos and the computation in specified intervals of the spectrum of large, sparse real symmetric matrices, in Sparse Matrix Proceedings 1978, </title> <editor> I. S. Duff and G. W. Stewart, eds., </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979, </year> <pages> pp. 220-255. </pages>
Reference-contexts: Research on the eigenvalue problem also contributed to making the KMP algorithms more useful for solving linear systems. Chris Paige [43, 40, 41, 42], Beresford Parlett and W. Kahan [33], and Jane Cullum and Ralph Willoughby <ref> [11] </ref> all made important contributions to understanding the behavior of the Lanczos recursion under inexact arithmetic.
Reference: [12] <author> J. W. Daniel, </author> <title> The conjugate gradient method for linear and nonlinear operator equations, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 4 (1967a), </volume> <pages> pp. 10-26. </pages>
Reference-contexts: Major steps in understanding the convergence behavior of the conjugate gradient algorithm were made by Kaniel [34] and Daniel <ref> [12] </ref>. Although both papers contained some errors [4, 13, 8], the standard bounds on the convergence rate for conjugate gradients, derived using Chebyshev polynomials, can be found here.
Reference: [13] <author> J. W. Daniel, </author> <title> A correction concerning the convergence rate for the conjugate gradient method, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 7 (1970b), </volume> <pages> pp. 277-280. </pages>
Reference-contexts: Major steps in understanding the convergence behavior of the conjugate gradient algorithm were made by Kaniel [34] and Daniel [12]. Although both papers contained some errors <ref> [4, 13, 8] </ref>, the standard bounds on the convergence rate for conjugate gradients, derived using Chebyshev polynomials, can be found here.
Reference: [14] <author> W. C. Davidon, </author> <title> Variable metric method for minimization, </title> <type> tech. rep., </type> <institution> ANL-5990, Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1959. </year>
Reference-contexts: Golub and Kahan discussed the use of the Lanczos algorithm in computing the singular value decomposition [26]. Fletcher and Reeves extended the conjugate gradient algorithm to minimization of non-quadratic functions [20], opening an important area of research. Somewhat earlier, Davidon <ref> [14] </ref> had developed the first algorithm in the quasi-Newton family, but it had not yet been recognized as a relative of the conjugate gradient algorithm that generates the same sequence of iterates when applied to quadratic minimization.
Reference: [15] <author> J. Douglas, Jr. and T. Dupont, </author> <title> Preconditioned conjugate gradient iteration applied to Galerkin methods for a mildly-nonlinear Dirichlet problem, in Sparse Matrix Computations, </title> <editor> J. R. Bunch and D. J. Rose, eds., </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976, </year> <pages> pp. 333-348. </pages>
Reference-contexts: This work inspired the hope of having a library KMP Algorithms 5 of preconditioners that would apply to broad problem classes. Preconditioning was also discussed by Owe Axelsson [2], by Paul Concus, Gene Golub, and Dianne O'Leary [10], and for nonlinear problems by Jim Douglas and Todd Dupont <ref> [15] </ref>. Two important extensions of the conjugate gradient algorithm were given during this period. Paul Concus, Gene Golub, and Olof Widlund solved problems in which the Hermitian part of the matrix was positive definite and could be used as a preconditioner [9, 54].
Reference: [16] <author> H. M. Dufour, </author> <title> Resolution des systemes lineaires par la methode des residus conjugues, </title> <journal> Bulletin Geodesique, </journal> <volume> 71 (1964), </volume> <pages> pp. 65-87. </pages>
Reference-contexts: Although the algorithms were not much in favor among numerical mathematicians, they did achieve considerable success in applications in spectral analysis [6], lens design [19], geodesy <ref> [16] </ref>, polar circulation [7], infrared spectral analysis [45], optimal control [48], collision theory [18], structural analysis [23], pattern recognition [39], power system load flow [53], optimal flight paths [5], nonrelativistic scattering [25], network analysis [35], and nuclear shell computation [47].
Reference: [17] <author> M. Engeli, T. Ginsburg, H. Rutishauser, and E. </author> <title> Stiefel, Refined Iterative Methods for Computation of the Solution and the Eigenvalues of Self-Adjoint Boundary Value Problems, </title> <publisher> Birkhauser Verlag, </publisher> <address> Basel/Stuttgart, </address> <year> 1959. </year>
Reference-contexts: They reported the use of the algorithm to solve 106 "difference equations" in 90 iterations. By 1958, researchers had solved Laplace's equation over a 10 fi 10 grid in 11 Chebyshev iterations plus 2 conjugate gradient iterations <ref> [17] </ref>. Lanczos also published a paper in 1952 concerning iterative methods for linear systems [37]. He developed the use of his 1950 algorithm, and developed 2-term recurrences of biorthogonal polynomials. The conjugate gradient iteration was extended to Hilbert space in the Ph.D. dissertation of R. M.
Reference: [18] <author> B. E. Eu, </author> <title> Method of moments in collision theory, </title> <journal> J. Chem. Phys., </journal> <volume> 48 (1968), </volume> <pages> pp. 5611-5622. </pages>
Reference-contexts: Although the algorithms were not much in favor among numerical mathematicians, they did achieve considerable success in applications in spectral analysis [6], lens design [19], geodesy [16], polar circulation [7], infrared spectral analysis [45], optimal control [48], collision theory <ref> [18] </ref>, structural analysis [23], pattern recognition [39], power system load flow [53], optimal flight paths [5], nonrelativistic scattering [25], network analysis [35], and nuclear shell computation [47]. Some important work was also occurring in extending the usefulness of the KMP family.
Reference: [19] <author> D. P. Feder, </author> <title> Automatic lens design with a high-speed computer, </title> <journal> J. of the Optical Soc. of Amer., </journal> <volume> 52 (1962), </volume> <pages> pp. 177-183. </pages>
Reference-contexts: Although the algorithms were not much in favor among numerical mathematicians, they did achieve considerable success in applications in spectral analysis [6], lens design <ref> [19] </ref>, geodesy [16], polar circulation [7], infrared spectral analysis [45], optimal control [48], collision theory [18], structural analysis [23], pattern recognition [39], power system load flow [53], optimal flight paths [5], nonrelativistic scattering [25], network analysis [35], and nuclear shell computation [47].
Reference: [20] <author> R. Fletcher and C. M. Reeves, </author> <title> Function minimization by conjugate gradients, </title> <journal> Computer J., </journal> <volume> 7 (1964), </volume> <pages> pp. 149-154. </pages>
Reference-contexts: Golub and Kahan discussed the use of the Lanczos algorithm in computing the singular value decomposition [26]. Fletcher and Reeves extended the conjugate gradient algorithm to minimization of non-quadratic functions <ref> [20] </ref>, opening an important area of research. Somewhat earlier, Davidon [14] had developed the first algorithm in the quasi-Newton family, but it had not yet been recognized as a relative of the conjugate gradient algorithm that generates the same sequence of iterates when applied to quadratic minimization.
Reference: [21] <author> G. E. Forsythe, M. R. Hestenes, and J. B. Rosser, </author> <title> Iterative methods for solving linear equations, </title> <journal> Bull. Amer. Math. Soc., </journal> <note> 57 (1951), p. 480. </note>
Reference-contexts: The earliest report of NBS activity on the conjugate gradient algorithm for solving systems of equations involving symmetric positive definite matrices was in an abstract for the Summer Meeting of the American Mathematical Society held in Minneapolis in September, 1951 <ref> [21] </ref>. This discussion of the three-term recurrence form of conjugate gradients is coauthored by George Forsythe, Magnus Hestenes, and J. Barkley Rosser. Hestenes wrote a technical report on this work in July, 1951 [31].
Reference: [22] <author> L. Fox, H. D. Huskey, and J. H. Wilkinson, </author> <title> Notes on the solution of algebraic linear simultaneous equations, </title> <journal> Quart. J. of Mech. and Appl. Math., </journal> <volume> 1 (1948), </volume> <pages> pp. 149-173. </pages>
Reference-contexts: J. Paige for the usual 2-term recurrence. The conjugate direction algorithm is somewhat older, discussed in a 1948 paper by Leslie Fox, H. D. Huskey, and Jim Wilkinson <ref> [22] </ref>. Meanwhile, Eduard Stiefel of E.T.H., Zurich, visited the INA, and presented a paper on the conjugate gradient algorithm at a workshop in August, 1951. His description of the "n-step iteration" was published in 1952 and noted the connection with the Lanczos (1950) work [49].
Reference: [23] <author> R. L. Fox and E. L. Stanton, </author> <title> Developments in structural analysis by direct energy minimization, </title> <journal> AIAA J., </journal> <volume> 6 (1968), </volume> <pages> pp. 1036-1042. </pages>
Reference-contexts: Although the algorithms were not much in favor among numerical mathematicians, they did achieve considerable success in applications in spectral analysis [6], lens design [19], geodesy [16], polar circulation [7], infrared spectral analysis [45], optimal control [48], collision theory [18], structural analysis <ref> [23] </ref>, pattern recognition [39], power system load flow [53], optimal flight paths [5], nonrelativistic scattering [25], network analysis [35], and nuclear shell computation [47]. Some important work was also occurring in extending the usefulness of the KMP family.
Reference: [24] <author> R. W. Freund, G. H. Golub, and N. M. Nachtigal, </author> <title> Iterative solution of linear systems, </title> <journal> Acta Numerica, </journal> <year> (1991), </year> <pages> pp. 57-100. </pages>
Reference-contexts: This includes the methods of Lanczos, Arnoldi, conjugate gradients, GMRES, CGS, QMR, and an alphabet soup of other algorithms. The following sections discuss some of the key early steps in the development of the KMP family. A reader interested in recent developments could begin with <ref> [24] </ref>, [3], and the papers in this volume. fl This work was supported by the National Science Foundation under Grant CCR-95-03126 y Department of Computer Science and Institute for Advanced Computer Studies, University of Maryland, College Park, MD 20742 (oleary@cs.umd.edu). 1 2 O'Leary 2 The Beginnings Researchers in the late 1940's
Reference: [25] <author> C. R. Garibotti and M. Villani, </author> <title> Continuation in the coupling constant for the total K and T matrices, </title> <address> Il Nuovo Cimento, </address> <month> 59 </month> <year> (1969), </year> <pages> pp. 107-123. </pages>
Reference-contexts: favor among numerical mathematicians, they did achieve considerable success in applications in spectral analysis [6], lens design [19], geodesy [16], polar circulation [7], infrared spectral analysis [45], optimal control [48], collision theory [18], structural analysis [23], pattern recognition [39], power system load flow [53], optimal flight paths [5], nonrelativistic scattering <ref> [25] </ref>, network analysis [35], and nuclear shell computation [47]. Some important work was also occurring in extending the usefulness of the KMP family. Wachspress showed the effectiveness of the algorithm preconditioned by alternating direction implicit methods (ADI) in solving discretizations of partial differential equations [52].
Reference: [26] <author> G. Golub and W. Kahan, </author> <title> Calculating the singular values and pseudo-inverse of a matrix, </title> <journal> SIAM J. Numer. Anal., 2(Series B) (1965), </journal> <pages> pp. 205-224. </pages>
Reference-contexts: Wachspress showed the effectiveness of the algorithm preconditioned by alternating direction implicit methods (ADI) in solving discretizations of partial differential equations [52]. Golub and Kahan discussed the use of the Lanczos algorithm in computing the singular value decomposition <ref> [26] </ref>. Fletcher and Reeves extended the conjugate gradient algorithm to minimization of non-quadratic functions [20], opening an important area of research.
Reference: [27] <author> G. H. Golub and D. P. O'Leary, </author> <title> Some history of the conjugate gradient and Lanczos algorithms: 1948-1976, </title> <journal> SIAM Review, </journal> <volume> 31 (1989), </volume> <pages> pp. 50-102. </pages>
Reference-contexts: Golub, Richard Underwood, and Jim Wilkinson developed a block form of the Lanczos algorithm [28, 50] that later inspired the development of block KMP algorithms. 5 Closing Comments Further information on the early history of the KMP family, as well as a much more complete bibliography, can be found in <ref> [27] </ref>.
Reference: [28] <author> G. H. Golub, R. Underwood, and J. H. Wilkinson, </author> <title> The Lanczos algorithm for the symmetric KMP Algorithms 7 Ax = Bx problem, </title> <type> tech. rep., </type> <institution> Stanford University Computer Science Department, Stanford, California, </institution> <year> 1972. </year>
Reference-contexts: Chris Paige [43, 40, 41, 42], Beresford Parlett and W. Kahan [33], and Jane Cullum and Ralph Willoughby [11] all made important contributions to understanding the behavior of the Lanczos recursion under inexact arithmetic. Gene Golub, Richard Underwood, and Jim Wilkinson developed a block form of the Lanczos algorithm <ref> [28, 50] </ref> that later inspired the development of block KMP algorithms. 5 Closing Comments Further information on the early history of the KMP family, as well as a much more complete bibliography, can be found in [27].
Reference: [29] <author> R. M. Hayes, </author> <title> Iterative methods of solving linear problems on Hilbert space, in Contributions to the Solution of Systems of Linear Equations and the Determination of Eigenvalues, </title> <editor> O. Taussky, ed., </editor> <volume> vol. </volume> <booktitle> 39 Applied Mathematics Series, </booktitle> <institution> National Bureau of Standards, U.S. Government Printing Office, </institution> <address> Washington, D.C., </address> <year> 1954, </year> <pages> pp. 71-103. </pages>
Reference-contexts: He developed the use of his 1950 algorithm, and developed 2-term recurrences of biorthogonal polynomials. The conjugate gradient iteration was extended to Hilbert space in the Ph.D. dissertation of R. M. Hayes, a U.C.L.A. student working at the INA <ref> [29] </ref>. Hayes established a linear convergence rate for general operators, with superlinear convergence if the operator is of the form identity plus a completely continuous operator. The roots of the idea of clustering eigenvalues to improve the convergence of conjugate gradients seem to date from here.
Reference: [30] <author> M. Hestenes, Private communcation, </author> <month> (July 1, </month> <year> 1987). </year>
Reference-contexts: Recently, C. Lanczos [1952] developed a closely related routine based on his earlier paper on eigenvalue problem. Examples and numerical tests of the method have been by R. Hayes, U. Hochstrasser, and M. Stein. All of this supports Hestenes' memories in 1987 <ref> [30] </ref>: "I believe it was done in the following order. 1. Stiefel because he had carried out some large experiments which surely took place more than a month before he came to UCLA. I invented it within a month of his arrival. 2. Hestenes. 3. Lanczos.
Reference: [31] <author> M. R. Hestenes, </author> <title> Iterative methods for solving linear equations, </title> <type> tech. rep., NAML Report 52-9, </type> <month> July 2, </month> <year> 1951, </year> <institution> National Bureau of Standards, </institution> <address> Los Angeles, California, </address> <year> 1951. </year> <note> Reprinted in J. of Optimization Theory and Applications 11 (1973), pp. 323-334. </note>
Reference-contexts: This discussion of the three-term recurrence form of conjugate gradients is coauthored by George Forsythe, Magnus Hestenes, and J. Barkley Rosser. Hestenes wrote a technical report on this work in July, 1951 <ref> [31] </ref>. In it, he acknowledged discussions with "Forsythe, Lanczos, Paige, Rosser, Stein, and others." He specifically credited Forsythe and Rosser for the 3-term recurrence and L. J. Paige for the usual 2-term recurrence. The conjugate direction algorithm is somewhat older, discussed in a 1948 paper by Leslie Fox, H. D.
Reference: [32] <author> M. R. Hestenes and E. </author> <title> Stiefel, Methods of conjugate gradients for solving linear systems, </title> <institution> J. Res. Nat. Bur. Standards, </institution> <month> 49 </month> <year> (1952), </year> <pages> pp. 409-436. </pages>
Reference-contexts: His description of the "n-step iteration" was published in 1952 and noted the connection with the Lanczos (1950) work [49]. Hestenes and Stiefel decided to combine their efforts, and they published the conjugate gradient paper in 1952. They described conjugate gradients as "an iterative method that terminates" <ref> [32, p.410] </ref>. <p> in 1952 [49, p.23] was that, "After writing up the present work, I discovered on a visit to the Institute for Numerical Analysis (University of California) that these results were also developed somewhat later by a group there." Hestenes and Stiefel give a more complete citation in the 1952 paper <ref> [32, pp.409-410] </ref>: The method of conjugate gradients was developed independently by E. Stiefel of the Institute of Applied Mathematics at Zurich and by M. R. Hestenes with the cooperation of J. B. Rosser, G. Forsythe, and L. Paige of the Institute for Numerical Analysis, National Bureau of Standards.
Reference: [33] <author> W. Kahan and B. N. Parlett, </author> <title> How far should you go with the Lanczos process?, in Sparse Matrix Computations, </title> <editor> J. R. Bunch and D. J. Rose, eds., </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976, </year> <pages> pp. 131-144. </pages>
Reference-contexts: Research on the eigenvalue problem also contributed to making the KMP algorithms more useful for solving linear systems. Chris Paige [43, 40, 41, 42], Beresford Parlett and W. Kahan <ref> [33] </ref>, and Jane Cullum and Ralph Willoughby [11] all made important contributions to understanding the behavior of the Lanczos recursion under inexact arithmetic.
Reference: [34] <author> S. Kaniel, </author> <title> Estimates for some computational techniques in linear algebra, </title> <journal> Math. of Comp., </journal> <volume> 95 (1966), </volume> <pages> pp. 369-378. </pages>
Reference-contexts: Major steps in understanding the convergence behavior of the conjugate gradient algorithm were made by Kaniel <ref> [34] </ref> and Daniel [12]. Although both papers contained some errors [4, 13, 8], the standard bounds on the convergence rate for conjugate gradients, derived using Chebyshev polynomials, can be found here.
Reference: [35] <author> K. Kawamura and R. A. Volz, </author> <title> On the convergence of the conjugate gradient method in Hilbert space, </title> <journal> IEEE Trans. on Auto. Control, </journal> <month> AC-14 </month> <year> (1969), </year> <pages> pp. 296-297. </pages>
Reference-contexts: mathematicians, they did achieve considerable success in applications in spectral analysis [6], lens design [19], geodesy [16], polar circulation [7], infrared spectral analysis [45], optimal control [48], collision theory [18], structural analysis [23], pattern recognition [39], power system load flow [53], optimal flight paths [5], nonrelativistic scattering [25], network analysis <ref> [35] </ref>, and nuclear shell computation [47]. Some important work was also occurring in extending the usefulness of the KMP family. Wachspress showed the effectiveness of the algorithm preconditioned by alternating direction implicit methods (ADI) in solving discretizations of partial differential equations [52].
Reference: [36] <author> C. </author> <title> Lanczos, An iteration method for the solution of the eigenvalue problem of linear differential and integral operators, </title> <institution> J. Res. Nat. Bur. Standards, </institution> <month> 45 </month> <year> (1950), </year> <pages> pp. 255-282. </pages>
Reference-contexts: The study of iterative methods for eigenvalue problems and for linear systems was a central research theme at the INA, and a landmark paper was published by Cornelius Lanczos in 1950 in the Journal of Research of the National Bureau of Standards <ref> [36] </ref>. In this work, Lanczos developed a three-term recurrence relation for matrix polynomials (equivalently, for a basis for a Krylov subspace) and a biorthogonalization method for finding eigenvalues of nonsymmetric matrices. Lanczos' idea was further developed by W. E.
Reference: [37] <author> C. </author> <title> Lanczos, Solution of systems of linear equations by minimized iterations, </title> <institution> J. Res. Nat. Bur. Standards, </institution> <month> 49 </month> <year> (1952), </year> <pages> pp. 33-53. </pages>
Reference-contexts: By 1958, researchers had solved Laplace's equation over a 10 fi 10 grid in 11 Chebyshev iterations plus 2 conjugate gradient iterations [17]. Lanczos also published a paper in 1952 concerning iterative methods for linear systems <ref> [37] </ref>. He developed the use of his 1950 algorithm, and developed 2-term recurrences of biorthogonal polynomials. The conjugate gradient iteration was extended to Hilbert space in the Ph.D. dissertation of R. M. Hayes, a U.C.L.A. student working at the INA [29]. <p> Clearly early researchers of the KMP family recognized the algorithms' usefulness as iterative methods and devoted considerable thought to preconditioning either by rescaling the matrix or by filtering the initial error by the use of other iterative methods as Lanczos suggested in 1952 <ref> [37, p.45] </ref>. 2.2 Assigning Credit Considerable controversy has arisen regarding proper credit for the conjugate gradient algorithm. Lanczos's papers clearly focus on KMP algorithms for nonsymmetric matrices, while Hestenes and Stiefel limit their attention to Hermitian positive definite matrices. This extra restriction enabled the use of many additional important properties. <p> Lanczos's papers clearly focus on KMP algorithms for nonsymmetric matrices, while Hestenes and Stiefel limit their attention to Hermitian positive definite matrices. This extra restriction enabled the use of many additional important properties. The claims of Lanczos, Hestenes, and Stiefel do not really overlap. Lanczos says in 1952 <ref> [37, p.53] </ref>, "The latest publication of Hestenes [1951] and of Stiefel [1952] is closely related to the p; q algorithm of the present paper, although developed independently and from different considerations." Stiefel's claim in 1952 [49, p.23] was that, "After writing up the present work, I discovered on a visit to <p> nonsymmetric iterations and the influence of loss of othogonality of the basis vectors. * Important aspects of the convergence behavior of the conjugate gradient iteration were understood relatively early; important questions still remain open concerning its behavior on discretizations of ill-posed problems, even though Lanczos himself had a partial understanding <ref> [37] </ref>. Convergence behavior of the nonsymmetric iterations is not nearly as well understood. * Preconditioning remains the key to effective use of the KMP family of algorithms, and the search for preconditioners effective on broad problem classes continues.
Reference: [38] <author> J. Meijerink and H. A. V. der Vorst, </author> <title> An iterative solution method for linear systems of which the coefficient matrix is a symmetric M-matrix, </title> <journal> Math. Comp., </journal> <volume> 31 (1977), </volume> <pages> pp. 148-162. </pages>
Reference-contexts: It was clear that progress was needed on several fronts: preconditioners that would turn ill-conditioned problems into well-conditioned ones, stabilized forms of the algorithms, and extensions to broader problem classes. The revival of interest in preconditioning produced the important paper of J. Meijerink and Henk van der Vorst <ref> [38] </ref>, available in preprint form in the early 1970's. They developed an algorithm for computing an incomplete LU factorization of an M-matrix (as did Richard Varga in a 1960 paper [51]).
Reference: [39] <author> G. Nagy, </author> <title> Classification algorithms in pattern recognition, </title> <journal> IEEE Trans. on Audio and Electroa-coustics, </journal> <month> AU-16 </month> <year> (1968a), </year> <pages> pp. 203-212. </pages>
Reference-contexts: Although the algorithms were not much in favor among numerical mathematicians, they did achieve considerable success in applications in spectral analysis [6], lens design [19], geodesy [16], polar circulation [7], infrared spectral analysis [45], optimal control [48], collision theory [18], structural analysis [23], pattern recognition <ref> [39] </ref>, power system load flow [53], optimal flight paths [5], nonrelativistic scattering [25], network analysis [35], and nuclear shell computation [47]. Some important work was also occurring in extending the usefulness of the KMP family.
Reference: [40] <author> C. C. Paige, </author> <title> Practical use of the symmetric Lanczos process with re-orthogonalization, </title> <journal> BIT, </journal> <volume> 10 (1970), </volume> <pages> pp. </pages> <month> 183-195. </month> <title> [41] , The computation of eigenvalues and eigenvectors of very large sparse matrices, </title> <type> tech. rep., Ph. D. dissertation, </type> <institution> University of London, </institution> <year> 1971. </year> <title> [42] , Computational variants of the Lanczos method for the eigenproblem, </title> <journal> J. Inst. Maths. Applics., </journal> <volume> 10 (1972), </volume> <pages> pp. </pages> <month> 373-381. </month> <title> [43] , Error analysis of the Lanczos algorithm for tridiagonalizing a symmetric matrix, </title> <journal> J. Inst. Maths. Applics., </journal> <volume> 18 (1976), </volume> <pages> pp. 341-349. </pages>
Reference-contexts: Chris Paige and Michael Saunders showed how to compute iterates in case a matrix was indefinite [44], resulting in SYMMLQ and related algorithms. Research on the eigenvalue problem also contributed to making the KMP algorithms more useful for solving linear systems. Chris Paige <ref> [43, 40, 41, 42] </ref>, Beresford Parlett and W. Kahan [33], and Jane Cullum and Ralph Willoughby [11] all made important contributions to understanding the behavior of the Lanczos recursion under inexact arithmetic.
Reference: [44] <author> C. C. Paige and M. A. Saunders, </author> <title> Solution of sparse indefinite systems of linear equations, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 12 (1975), </volume> <pages> pp. 617-629. </pages>
Reference-contexts: Paul Concus, Gene Golub, and Olof Widlund solved problems in which the Hermitian part of the matrix was positive definite and could be used as a preconditioner [9, 54]. Chris Paige and Michael Saunders showed how to compute iterates in case a matrix was indefinite <ref> [44] </ref>, resulting in SYMMLQ and related algorithms. Research on the eigenvalue problem also contributed to making the KMP algorithms more useful for solving linear systems. Chris Paige [43, 40, 41, 42], Beresford Parlett and W.
Reference: [45] <author> J. Pitha and R. N. Jones, </author> <title> A comparison of optimization methods for fitting curves to infrared band envelopes, </title> <journal> Canadian J. of Chemistry, </journal> <volume> 44 (1966), </volume> <pages> pp. 3031-3050. </pages>
Reference-contexts: Although the algorithms were not much in favor among numerical mathematicians, they did achieve considerable success in applications in spectral analysis [6], lens design [19], geodesy [16], polar circulation [7], infrared spectral analysis <ref> [45] </ref>, optimal control [48], collision theory [18], structural analysis [23], pattern recognition [39], power system load flow [53], optimal flight paths [5], nonrelativistic scattering [25], network analysis [35], and nuclear shell computation [47]. Some important work was also occurring in extending the usefulness of the KMP family.
Reference: [46] <author> J. K. Reid, </author> <title> On the method of conjugate gradients for the solution of large sparse systems of linear equations, in Large Sparse Sets of Linear Equations, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1971, </year> <pages> pp. 231-254. </pages>
Reference-contexts: Atomic Energy Research Establishment, Harwell <ref> [46] </ref>. Reid compared the numerical performance of several variants of the conjugate gradient algorithm and emphasized the algorithm's usefulness on well-conditioned problems. This stimulated interest among a number of researchers, and activity in the area blossomed.
Reference: [47] <author> T. Sebe and J. Nachamkin, </author> <title> Variational buildup of nuclear shell model bases, </title> <journal> Annals of Physics, </journal> <volume> 51 (1969), </volume> <pages> pp. 100-123. </pages>
Reference-contexts: success in applications in spectral analysis [6], lens design [19], geodesy [16], polar circulation [7], infrared spectral analysis [45], optimal control [48], collision theory [18], structural analysis [23], pattern recognition [39], power system load flow [53], optimal flight paths [5], nonrelativistic scattering [25], network analysis [35], and nuclear shell computation <ref> [47] </ref>. Some important work was also occurring in extending the usefulness of the KMP family. Wachspress showed the effectiveness of the algorithm preconditioned by alternating direction implicit methods (ADI) in solving discretizations of partial differential equations [52].
Reference: [48] <author> J. F. Sinnott, Jr. and D. G. Luenberger, </author> <title> Solution of optimal control problems by the method of conjugate gradients, </title> <booktitle> in 1967 Joint Automatic Control Conference, Preprints of Papers, Lewis Winner, </booktitle> <address> New York, </address> <year> 1967, </year> <pages> pp. 566-574. </pages>
Reference-contexts: Although the algorithms were not much in favor among numerical mathematicians, they did achieve considerable success in applications in spectral analysis [6], lens design [19], geodesy [16], polar circulation [7], infrared spectral analysis [45], optimal control <ref> [48] </ref>, collision theory [18], structural analysis [23], pattern recognition [39], power system load flow [53], optimal flight paths [5], nonrelativistic scattering [25], network analysis [35], and nuclear shell computation [47]. Some important work was also occurring in extending the usefulness of the KMP family.
Reference: [49] <author> E. </author> <title> Stiefel, Uber einige methoden der relaxationsrechnung, </title> <journal> Zeitschrift fur angewandte Mathe-matik und Physik, </journal> <volume> 3 (1952), </volume> <pages> pp. 1-33. </pages>
Reference-contexts: D. Huskey, and Jim Wilkinson [22]. Meanwhile, Eduard Stiefel of E.T.H., Zurich, visited the INA, and presented a paper on the conjugate gradient algorithm at a workshop in August, 1951. His description of the "n-step iteration" was published in 1952 and noted the connection with the Lanczos (1950) work <ref> [49] </ref>. Hestenes and Stiefel decided to combine their efforts, and they published the conjugate gradient paper in 1952. They described conjugate gradients as "an iterative method that terminates" [32, p.410]. <p> Lanczos says in 1952 [37, p.53], "The latest publication of Hestenes [1951] and of Stiefel [1952] is closely related to the p; q algorithm of the present paper, although developed independently and from different considerations." Stiefel's claim in 1952 <ref> [49, p.23] </ref> was that, "After writing up the present work, I discovered on a visit to the Institute for Numerical Analysis (University of California) that these results were also developed somewhat later by a group there." Hestenes and Stiefel give a more complete citation in the 1952 paper [32, pp.409-410]: The
Reference: [50] <author> R. Underwood, </author> <title> An iterative block Lanczos method for the solution of large sparse symmetric eigenproblems, </title> <type> tech. rep., Ph.D. dissertation, </type> <institution> Stanford University Computer Science Dept. Report STAN-CS-75-496, Stanford, California, </institution> <year> 1975. </year>
Reference-contexts: Chris Paige [43, 40, 41, 42], Beresford Parlett and W. Kahan [33], and Jane Cullum and Ralph Willoughby [11] all made important contributions to understanding the behavior of the Lanczos recursion under inexact arithmetic. Gene Golub, Richard Underwood, and Jim Wilkinson developed a block form of the Lanczos algorithm <ref> [28, 50] </ref> that later inspired the development of block KMP algorithms. 5 Closing Comments Further information on the early history of the KMP family, as well as a much more complete bibliography, can be found in [27].
Reference: [51] <author> R. S. Varga, </author> <title> Factorization and normalized iterative methods, in Boundary Problems in Differential Equations, </title> <editor> R. E. Langer, ed., </editor> <publisher> University of Wisconsin Press, </publisher> <address> Madison, </address> <year> 1960, </year> <pages> 8 O'Leary pp. 121-142. </pages>
Reference-contexts: The revival of interest in preconditioning produced the important paper of J. Meijerink and Henk van der Vorst [38], available in preprint form in the early 1970's. They developed an algorithm for computing an incomplete LU factorization of an M-matrix (as did Richard Varga in a 1960 paper <ref> [51] </ref>). This work inspired the hope of having a library KMP Algorithms 5 of preconditioners that would apply to broad problem classes. Preconditioning was also discussed by Owe Axelsson [2], by Paul Concus, Gene Golub, and Dianne O'Leary [10], and for nonlinear problems by Jim Douglas and Todd Dupont [15].
Reference: [52] <author> E. L. </author> <title> Wachspress, Extended application of alternating direction implicit iteration model problem theory, </title> <journal> J. Soc. Industr. Appl. Math., </journal> <volume> 11 (1963), </volume> <pages> pp. 994-1016. </pages>
Reference-contexts: Some important work was also occurring in extending the usefulness of the KMP family. Wachspress showed the effectiveness of the algorithm preconditioned by alternating direction implicit methods (ADI) in solving discretizations of partial differential equations <ref> [52] </ref>. Golub and Kahan discussed the use of the Lanczos algorithm in computing the singular value decomposition [26]. Fletcher and Reeves extended the conjugate gradient algorithm to minimization of non-quadratic functions [20], opening an important area of research.
Reference: [53] <author> Y. Wallach, </author> <title> Gradient methods for load-flow problems, </title> <journal> IEEE Trans. on Power Apparatus and Systems, </journal> <month> PAS-87 </month> <year> (1968), </year> <pages> pp. 1314-1318. </pages>
Reference-contexts: Although the algorithms were not much in favor among numerical mathematicians, they did achieve considerable success in applications in spectral analysis [6], lens design [19], geodesy [16], polar circulation [7], infrared spectral analysis [45], optimal control [48], collision theory [18], structural analysis [23], pattern recognition [39], power system load flow <ref> [53] </ref>, optimal flight paths [5], nonrelativistic scattering [25], network analysis [35], and nuclear shell computation [47]. Some important work was also occurring in extending the usefulness of the KMP family.
Reference: [54] <author> O. Widlund, </author> <title> A Lanczos method for a class of nonsymmetric systems of linear equations, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 15 (1978), </volume> <pages> pp. 801-812. </pages>
Reference-contexts: Two important extensions of the conjugate gradient algorithm were given during this period. Paul Concus, Gene Golub, and Olof Widlund solved problems in which the Hermitian part of the matrix was positive definite and could be used as a preconditioner <ref> [9, 54] </ref>. Chris Paige and Michael Saunders showed how to compute iterates in case a matrix was indefinite [44], resulting in SYMMLQ and related algorithms. Research on the eigenvalue problem also contributed to making the KMP algorithms more useful for solving linear systems.
References-found: 51

