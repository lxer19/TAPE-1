URL: ftp://ftp.cc.gatech.edu/pub/people/panesar/ieee-comp.ps.Z
Refering-URL: http://www.cs.gatech.edu/grads/p/Kiran.Panesar/homepage.html
Root-URL: 
Email: ffujimoto,panesar,ingridg@cc.gatech.edu  
Phone: phone: 404/894-5615; fax: 404/894-9442  
Title: Parallel Discrete Event Simulation on a Shared-Memory Multiprocessor  
Author: Richard Fujimoto, Kiran Panesar, and Maria Hybinette 
Keyword: Time Warp, distributed simulation, buffer management, message passing, global virtual time  
Address: Atlanta, GA 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Abstract: Many large-scale discrete event simulation computations for modeling telecommunication networks, computer systems, transportation grids, and a variety of other applications are excessively time consuming, and are a natural candidate for parallel execution. However, discrete event simulations are challenging to parallelize because cause-and-effect relationships determine dependencies between simulation computations and are difficult or impossible to predict prior to execution. This makes synchronization a non-trivial issue. Time Warp is a well-known synchronization protocol that detects out-of-order executions of computations as they occur, and recovers using a rollback mechanism. This article discusses important issues concerning the design of a Time Warp-based parallel simulation executive for shared-memory multiprocessors. It is observed that interactions between memory management mechanisms and buffer management techniques can have a dramatic effect on the performance of message-passing software on multiprocessors using virtual shared memory. An approach to buffer management is developed that yields significantly better performance than conventional strategies for large simulation applications. Up to fourty-fold speedups are observed for simulations of multicomputer and personal communication services networks executing on a Kendall Square Research KSR-2 machine. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. D. Carothers, R. M. Fujimoto, Y-B. Lin, and P. </author> <title> England. Distributed simulation of large-scale pcs networks. </title> <booktitle> In Proceedings of the 1994 MASCOTS Conference, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: The performance differential increases as the number of processors is increased. 16 17 4.2 Personal Communications Services (PCS) Simulation PCS is a simulation of a wireless communication network with a set of radio ports structured as a square grid (one port per grid sector) <ref> [1] </ref>. Each grid sector, or cell, is assigned a fixed number of channels. A portable or mobile phone resides in a cell for a period of time, and then moves to another cell.
Reference: [2] <author> S. Das, R. Fujimoto, K. Panesar, D. Allison, and M. Hybinette. GTW: </author> <title> A Time Warp system for shared memory multiprocessors. </title> <booktitle> In 1994 Winter Simulation Conference Proceedings, </booktitle> <pages> pages 1332-1339, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: It is seen that again, the partitioned buffer pool scheme outperforms the other strategies by a substantial margin. Measurements of an optimized version of the simulation using compiler optimizations and other techniques (described in <ref> [2] </ref>) are also shown. Committed event rates as high as 500,000 events per second were observed using 32 processors.
Reference: [3] <author> Thomas H. Dunigan. </author> <title> Multi ring performance of the kendall square multiprocessor. </title> <type> Technical Report ORNL/TM-12331, </type> <institution> Engineering Physics and Mathematics Division, Oak Ridge National Lab, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Data that is in neither the sub-cache nor the local cache is fetched from another processor's cache, or if it does not reside in another cache, from secondary storage. Further details of the machine architecture and its performance are described in <ref> [3] </ref>. Accesses to the sub-cache require 2 clock cycles, and accesses to the local cache require 20 cycles. The time to access another processor's cache depends on the ring traffic. <p> Each lock or unlock operation requires 3 sec in the absence of contention, 14 sec for a pair or processors on the same ring, and 32 sec for a pair of processors on different rings <ref> [3, p 10] </ref>. All experiments described here use a single ring, except the 32 processor runs that use processors from two different rings. Each ring contains a small number of processors that perform I/O; the Time Warp simulation does not use these processors in the experiments described here. <p> The lock time is measured to be approximately 20 sec, which is consistent with times reported in <ref> [3] </ref> when one considers that contention for the lock increases the access time to some degree.
Reference: [4] <author> R. M. Fujimoto. </author> <title> Time Warp on a shared memory multiprocessor. </title> <journal> Transactions of the Society for Computer Simulation, </journal> <volume> 6(3) </volume> <pages> 211-239, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: GTW uses a technique called direct cancelation where each event contains pointers to the messages sent when processing that event <ref> [4] </ref>. These pointers result in a global data structure stored in shared memory linking together all of the events in the system. Direct cancelation eliminates the need to search through queues to locate message/anti-message pairs, and enables fast cancelation of incorrect computations. Low overhead computation of Global Virtual Time.
Reference: [5] <author> R. M. Fujimoto. </author> <title> Parallel discrete event simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 30-53, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: process this event? How does it know that LP B won't later schedule an event with timestamp less than 100? The problem of ensuring that each LP processes events in timestamp order is referred to as the synchronization problem in the literature, and has received a considerable amount of attention <ref> [5] </ref>. Time Warp is a well known mechanism that solves the synchronization problem by detecting when an event is received with timestamp smaller than one that has already been processed, and uses a rollback mechanism to erase event computations performed out of timestamp order [9].
Reference: [6] <author> R. M. Fujimoto. </author> <title> Performance of Time Warp under synthetic workloads. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> volume 22, </volume> <pages> pages 23-28. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> January </month> <year> 1990. </year>
Reference-contexts: If the buffer pool contains few buffers (five in the experiments described here), additional buffers are reclaimed from the global 9 pool (if available) to restore the processor to its initial allocation. Initial experiments used a synthetic workload model called PHold <ref> [6] </ref>. The model uses a fixed-sized message population. The experiments described here use 256 LPs and a message population of 1024. Each event generates one new message with timestamp increment selected from an exponential distribution. The destination logical process (LP) is selected from a uniform distribution.
Reference: [7] <author> R. M. Fujimoto and M. Hybinette. </author> <title> Computing global virtual time on shared-memory multiprocessors. </title> <type> Technical report, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Thus, computing GVT on shared-memory Time Warp systems requires negligible overhead. Performance measurements of this algorithm on a Kendall Square Research multiprocessor indicate that GVT can be computed every millisecond without significantly degrading performance <ref> [7] </ref>. It is perhaps noteworthy that the above algorithm relies on a property of shared-memory multiprocessors called sequential consistency [10]. Sequential consistency guarantees that all processors perceive the same order of read and write operations on shared memory.
Reference: [8] <author> F. Gomes, S. Franks, B. Unger, Z. Xiao, J. Cleary, and A. Covington. Simkit: </author> <title> A high performance logical process simulation class library in c++. </title> <booktitle> In 1995 Winter Simulation Conference Proceedings, </booktitle> <pages> pages 706-713, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Two other shared-memory Time Warp systems have been derived from the GTW system, namely, the WarpKit simulator developed at the University of Calgary and University of Waikato <ref> [8] </ref>, and the Tempo [12] system developed by SAIC. In addition to the results described here, GTW has been successfully used by researchers at MITRE to speed up simulations of air traffic in the United States.
Reference: [9] <author> D. R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Time Warp is a well known mechanism that solves the synchronization problem by detecting when an event is received with timestamp smaller than one that has already been processed, and uses a rollback mechanism to erase event computations performed out of timestamp order <ref> [9] </ref>. Once the computation is rolled back, the events are reprocessed in timestamp order. Recall that each event may (1) modify state variables, and/or (2) schedule new events. Therefore, rolling back an event requires one to undo each of these actions.
Reference: [10] <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Performance measurements of this algorithm on a Kendall Square Research multiprocessor indicate that GVT can be computed every millisecond without significantly degrading performance [7]. It is perhaps noteworthy that the above algorithm relies on a property of shared-memory multiprocessors called sequential consistency <ref> [10] </ref>. Sequential consistency guarantees that all processors perceive the same order of read and write operations on shared memory. This common, global ordering enables one to characterize all operations affecting the snapshot (e.g., creating a new message) as occuring either before or after the GVT computation is initiated.
Reference: [11] <author> Per Stenstrom. </author> <title> A survey of cache coherence scheme for multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6), </volume> <month> June </month> <year> 1990. </year>
Reference-contexts: It is assumed that some mechanism is in place to ensure that duplicate copies of the same memory location in different caches remain consistent. This is typically accomplished by either invalidating copies in other caches when one processor modifies a cached block of memory, or updating duplicate copies <ref> [11] </ref>. 2 Time Warp on a Shared Memory Multiprocessor Several techniques specific to shared-memory architectures have been developed and implemented in GTW to realize an efficient implementation of Time Warp.
Reference: [12] <author> D. West, L. Mellon, J. Ramsey, J. Cleary, and J. Hofmann. </author> <title> Infrastructure for rapid execution of strike-planning systems. </title> <booktitle> In 1995 Winter Simulation Conference Proceedings, </booktitle> <pages> pages 1207-1214, </pages> <month> December </month> <year> 1995. </year>
Reference-contexts: Two other shared-memory Time Warp systems have been derived from the GTW system, namely, the WarpKit simulator developed at the University of Calgary and University of Waikato [8], and the Tempo <ref> [12] </ref> system developed by SAIC. In addition to the results described here, GTW has been successfully used by researchers at MITRE to speed up simulations of air traffic in the United States.
References-found: 12

