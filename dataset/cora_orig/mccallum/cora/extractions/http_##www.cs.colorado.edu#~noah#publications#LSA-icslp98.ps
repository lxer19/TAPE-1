URL: http://www.cs.colorado.edu/~noah/publications/LSA-icslp98.ps
Refering-URL: http://www.cs.colorado.edu/~noah/publications/
Root-URL: http://www.cs.colorado.edu
Email: fnoah,jurafskyg@colorado.edu  
Title: TOWARDS BETTER INTEGRATION OF SEMANTIC PREDICTORS IN STATISTICAL LANGUAGE MODELING  
Author: Noah Coccaro* and Daniel Jurafskyy* 
Address: Boulder  
Affiliation: Department of Computer Science* Department of Linguisticsy University of Colorado at  
Note: To Appear In Proceedings of ICSLP-98, Sydney  
Abstract: We introduce a number of techniques designed to help integrate semantic knowledge with N-gram language models for automatic speech recognition. Our techniques allow us to integrate Latent Semantic Analysis (LSA), a word-similarity algorithm based on word co-occurrence information, with N-gram models. While LSA is good at predicting content words which are coherent with the rest of a text, it is a bad predictor of frequent words, has a low dynamic range, and is inaccurate when combined linearly with N-grams. We show that modifying the dynamic range, applying a per-word confidence metric, and using geometric rather than linear combinations with N-grams produces a more robust language model which has a lower perplexity on a Wall Street Journal test-set than a baseline N-gram model. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. Bellegarda, J.W. Butzburger, Y. Chow, N. Coccaro, and D. Naik. </author> <title> A novel word clustering algorithm based on latent semantic analysis. </title> <booktitle> In Proceedings of ICASSP-96, </booktitle> <year> 1996. </year>
Reference-contexts: 1. INTRODUCTION There has been a lot of recent work on augmenting n-gram language models with other information sources such as longer distance syntactic, and semantic constraints (e.g. [8], [6]). In previous work, we <ref> [1] </ref> and others [2], [5] have suggested the use of Latent Semantic Analysis (LSA) [3] as a model of semantic knowledge to be applied to ASR.
Reference: 2. <author> Jerome Bellegarda. </author> <title> A latent semantic analysis framework for large-span language modeling. </title> <booktitle> In Eurospeech, </booktitle> <year> 1997. </year>
Reference-contexts: 1. INTRODUCTION There has been a lot of recent work on augmenting n-gram language models with other information sources such as longer distance syntactic, and semantic constraints (e.g. [8], [6]). In previous work, we [1] and others <ref> [2] </ref>, [5] have suggested the use of Latent Semantic Analysis (LSA) [3] as a model of semantic knowledge to be applied to ASR. LSA is a model of word semantic similarity based on word co-occurrence tendencies, and has been successful in IR and NLP applications, such as spelling correction [7].
Reference: 3. <author> S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Lan-dauer, and R. Harshman. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(6):391407, </volume> <year> 1990. </year>
Reference-contexts: 1. INTRODUCTION There has been a lot of recent work on augmenting n-gram language models with other information sources such as longer distance syntactic, and semantic constraints (e.g. [8], [6]). In previous work, we [1] and others [2], [5] have suggested the use of Latent Semantic Analysis (LSA) <ref> [3] </ref> as a model of semantic knowledge to be applied to ASR. LSA is a model of word semantic similarity based on word co-occurrence tendencies, and has been successful in IR and NLP applications, such as spelling correction [7].
Reference: 4. <author> Susan. T. Dumais. </author> <title> Improving the retrieval of information from external sources. Behavior Resarch Methods, Instruments and Computers, </title> <address> 23(2):229236, </address> <year> 1991. </year>
Reference-contexts: Our confidence metric is a `global term weighting' found to be useful in IR applications: the entropy of the frequency of a word over all documents in the training corpus <ref> [4] </ref>.
Reference: 5. <author> Y. Gotoh and S. Renals. </author> <title> Document space models using latent semantic analysis. </title> <booktitle> In Eurospeech, </booktitle> <year> 1997. </year>
Reference-contexts: 1. INTRODUCTION There has been a lot of recent work on augmenting n-gram language models with other information sources such as longer distance syntactic, and semantic constraints (e.g. [8], [6]). In previous work, we [1] and others [2], <ref> [5] </ref> have suggested the use of Latent Semantic Analysis (LSA) [3] as a model of semantic knowledge to be applied to ASR. LSA is a model of word semantic similarity based on word co-occurrence tendencies, and has been successful in IR and NLP applications, such as spelling correction [7].
Reference: 6. <author> R. M. Iyer. </author> <title> Improving And Predicting Performance Of Statistical Language Models In Sparse Domains. </title> <type> PhD thesis, </type> <institution> Boston University, </institution> <year> 1998. </year>
Reference-contexts: 1. INTRODUCTION There has been a lot of recent work on augmenting n-gram language models with other information sources such as longer distance syntactic, and semantic constraints (e.g. [8], <ref> [6] </ref>). In previous work, we [1] and others [2], [5] have suggested the use of Latent Semantic Analysis (LSA) [3] as a model of semantic knowledge to be applied to ASR.
Reference: 7. <author> M. P. Jones and J. H. Martin. </author> <title> Contextual spelling correction using latent semantic analysis. </title> <booktitle> In Proceedings of the Fifth Conference on Applied Natural Language Processing, </booktitle> <year> 1997. </year>
Reference-contexts: LSA is a model of word semantic similarity based on word co-occurrence tendencies, and has been successful in IR and NLP applications, such as spelling correction <ref> [7] </ref>. LSA is good at predicting the presence of words in the domain of the text, but not good at predicting their exact location. The N-gram model complements the LSA model by filling in the missing information where exactly the content words should go.
Reference: 8. <author> R. Rosenfeld. </author> <title> A maximum entropy approach to adaptive statistical language modeling. </title> <booktitle> Computer, Speech and Language, </booktitle> <volume> 1, </volume> <year> 1996. </year>
Reference-contexts: 1. INTRODUCTION There has been a lot of recent work on augmenting n-gram language models with other information sources such as longer distance syntactic, and semantic constraints (e.g. <ref> [8] </ref>, [6]). In previous work, we [1] and others [2], [5] have suggested the use of Latent Semantic Analysis (LSA) [3] as a model of semantic knowledge to be applied to ASR.
References-found: 8

