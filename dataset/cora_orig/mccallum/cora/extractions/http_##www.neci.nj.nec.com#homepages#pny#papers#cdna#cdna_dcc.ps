URL: http://www.neci.nj.nec.com/homepages/pny/papers/cdna/cdna_dcc.ps
Refering-URL: http://www.neci.nj.nec.com/homepages/pny/papers/cdna/main.html
Root-URL: 
Title: Significantly Lower Entropy Estimates for Natural DNA Sequences  
Author: David Loewenstern Peter N. Yianilos 
Note: Extended Abstract  
Abstract: If DNA were a random string over its alphabet fA; C; G; Tg, an optimal code would assign 2 bits to each nucleotide. We imagine DNA to be a highly ordered, purposeful molecule, and might therefore reasonably expect statistical models of its string representation to produce much lower entropy estimates. Surprisingly this has not been the case for many natural DNA sequences, including portions of the human genome. We introduce a new statistical model (compression algorithm), the strongest reported to date, for naturally occurring DNA sequences. Conventional techniques code a nucleotide using only slightly fewer bits (1.90) than one obtains by relying only on the frequency statistics of individual nucleotides (1.95). Our method in some cases increases this gap by more than five-fold (1.66) and may lead to better performance in microbiological pattern recognition applications. One of our main contributions, and the principle source of these improvements, is the formal inclusion of inexact match information in the model. The existence of matches at various distances forms a panel of experts which are then combined into a single prediction. The structure of this combination is novel and its parameters are learned using Expectation Maximization (EM).
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. E. Baum and J. E. Eagon, </author> <title> An inequality with application to statistical estimation for probabalistic functions of a Markov process and to models for ecology, </title> <journal> Bull. AMS, </journal> <volume> 73 (1967), </volume> <pages> pp. 360-363. </pages>
Reference-contexts: The learning task before us is to estimate the parameters Pr (h = ijf = j; w = k) and Pr (w = k) by examining the training set. Our algorithm is an application of the Baum-Welch algorithm for Hidden Markov Models <ref> [1] </ref>, and may also be viewed as an instance of Expectation Maximization (EM), a later rediscovery [4] of essentially the same algorithm and underlying information theoretic inequality.
Reference: [2] <author> T. C. Bell, J. G. Cleary, and I. H. Witten, </author> <title> Text Compression, </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: Assuming each character (nucleotide) is drawn uniformly at random from the alphabet, and that all positions in the string are independent, we know from elementary information theory <ref> [3, 2] </ref> that an optimal code will devote 2 bits to representing each character. This is the maximum entropy case. <p> 1:90 bit result is then even more surprising since it implies that knowledge of the immediate past reduces the entropy estimate by a mere 0:05 bits. 1 Data compression techniques such as Lempel-Ziv (LZ) coding (see their original paper [11] and the discussion of the considerable work that followed in <ref> [2] </ref>) may be viewed as entropy estimators, with LZ corresponding to a model that predicts based on a historical context of variable length. It "compresses" humretblas to 2:14 bits per character 2 , which is actually worse than the flat random model we started with. <p> Longer contexts reach farther into the past, and might reasonably be expected to result in stronger models. However, as context length increases, it becomes increasingly unlikely that a given context has ever been seen. This problem has led to the development of variable length context language models <ref> [2] </ref> which simply stated, use long contexts when enough earlier observations exists, and otherwise use shorter ones.
Reference: [3] <author> T. M. Cover and J. A. Thomas, </author> <title> Elements of Information Theory, </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: Assuming each character (nucleotide) is drawn uniformly at random from the alphabet, and that all positions in the string are independent, we know from elementary information theory <ref> [3, 2] </ref> that an optimal code will devote 2 bits to representing each character. This is the maximum entropy case.
Reference: [4] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin, </author> <title> Maximum-likelihood from incomplete data via the EM algorithm, </title> <journal> J. Royal Statistical Society Ser. B (methodological), </journal> <volume> 39 (1977), </volume> <pages> pp. 1-38. </pages>
Reference-contexts: Our algorithm is an application of the Baum-Welch algorithm for Hidden Markov Models [1], and may also be viewed as an instance of Expectation Maximization (EM), a later rediscovery <ref> [4] </ref> of essentially the same algorithm and underlying information theoretic inequality. To ensure that our system deals only with finite code-lengths, and to handle the all-zero case simply, as a practical expedient, we add a small constant, 1 fi 10 10 , to each vector position before normalization.
Reference: [5] <author> B. Dujon, D. Alexandraki, B. Andre, W. Ansorge, and V. B. et al., </author> <title> Complete DNA sequence of yeast chromosome XI, </title> <booktitle> Nature, </booktitle> <month> 369 </month> <year> (1994). </year>
Reference-contexts: In this case of panmtpacga, nearly all of the redundancy is explained by the unigraph statistics H (1) = 1:88. The observed relationship between %(C + G) and gene density <ref> [6, 5] </ref> in yeast is reflected in the discrepancy in H (1) between the coding and non-coding region of Yeast chromosome III. Observe that the H (6) estimate is rarely better than H (4), and in some cases is markedly worse (a consequence of limited sequence length).
Reference: [6] <author> L. L. Gatlin, </author> <title> Information Theory and the Living System, </title> <publisher> Columbia University Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: In this case of panmtpacga, nearly all of the redundancy is explained by the unigraph statistics H (1) = 1:88. The observed relationship between %(C + G) and gene density <ref> [6, 5] </ref> in yeast is reflected in the discrepancy in H (1) between the coding and non-coding region of Yeast chromosome III. Observe that the H (6) estimate is rarely better than H (4), and in some cases is markedly worse (a consequence of limited sequence length).
Reference: [7] <author> S. Grumbach and F. Tahi, </author> <title> A new challenge for compression algorithms: genetic sequences, </title> <booktitle> Information Processing & Management, 30 (1994), </booktitle> <pages> pp. 875-886. </pages>
Reference-contexts: Our metric looks not only for matches in the forward direction, but also for matches that are reversed and complemented. By "complemented" we mean that A; G are interchanged with T; C. This technique was introduced by Grumbach and Tahi <ref> [7] </ref> and in almost all cases slightly improves the rate of compression. 2. The Model In this section we further motivate our model and then describe it in formal terms. One may view each row of table 1 (really each Hamming distance) as corresponding to a predictive expert. <p> The resulting sequence contains 484; 483 bases and is referred to as our nonredundant data set. Our model's performance on these sequences is summarized in table 2. In some cases our results may be compared directly with estimates from <ref> [7] </ref>, which are included in the table. Our values for H (4) (the 4-symbol entropy) may be compared with the redundancy estimates of [10] and are in agreement. We have grouped our results by general type (i.e. mammal, prokaryote, etc.). <p> For some sequences our model is the first to detect substantial deviations from random behavior, illustrating the importance of inexact string matching techniques in this field. In many cases the compressive estimates from cdna (i.e., cdna-compress) are lower than those of biocompress-2 <ref> [7] </ref>. In several they are comparable, and in only one case (vaccg) does biocompress-2 outperform cdna-compress by as much as 0.05 bits. <p> The H (1); H (4); H (6) columns contain conventional multigram entropy estimates. The cdna column reports our model's cross-validation entropy estimates. Entropy estimates to the right of the double line are calculated by compressive methods: the biocompress-2 program of <ref> [7] </ref>, and our model's compressive estimate, cdna-compress. Boldface indicates the lowest entropy estimate for each of the two classes of methods for a given sequence.
Reference: [8] <author> H. Herzel, </author> <title> Complexity of symbol sequences, </title> <journal> Syst. Anal. Modl. Simul., </journal> <volume> 5 (1988), </volume> <pages> pp. 435-444. </pages>
Reference-contexts: Our formalization of this idea in section 2 is one of the main contributions of this paper. That natural DNA includes near repeats is well known, and in <ref> [8] </ref> the statistics of their occurrence are discussed. We measure nearness using ordinary Hamming distance. Given a target string of length w, it is then natural to wonder how many h distant matches exist in a string of length `.
Reference: [9] <author> D. Loewenstern and P. N. Yianilos, </author> <title> Significantly lower entropy estimates for natural DNA sequences, </title> <type> Tech. Rep. TR 96-51, </type> <institution> DIMACS, </institution> <year> 1996. </year>
Reference-contexts: Table 3 gives the results of several models on our non-redundant data set. It is apparent that our non-redundant collection of coding material is harder to model than the entire genes considered in earlier sections. Nevertheless we are able to 3 Please refer to <ref> [9] </ref> for a more detailed discussion of parameter sensitivity. Model Description bits 1. Reference: entropy of a uniform random coding sequence 1.98 2. Nucleotide 6-grams (H (6)) 1.92 3. cdna nucleotide 1.91 4.
Reference: [10] <author> R. Mantegna, S. Buldyrev, A. Goldberger, S. Havlin, C.-K. Peng, M. Simons, and H. Stanley, </author> <title> Linguistic features of noncoding DNA sequences, </title> <journal> Physical Review Letters, </journal> <volume> 73 (1993), </volume> <pages> pp. 3169-3172. </pages>
Reference-contexts: A logical next step taken by several investigators, focuses instead on higher order entropy estimates arising from measurements of the frequencies of longer sequences. For natural languages (e.g. English) this step typically leads to significantly lower entropy estimates. But the best resulting estimate for humretblas is roughly 1:90 bits <ref> [10] </ref>, still not impressively different from our 2-bit random starting point. This may be something of a surprise, since such models reflect such known DNA structure as %(C + G) composition and CG suppression. But these known effects have little impact on the entropy of DNA sequences as a whole. <p> Our model's performance on these sequences is summarized in table 2. In some cases our results may be compared directly with estimates from [7], which are included in the table. Our values for H (4) (the 4-symbol entropy) may be compared with the redundancy estimates of <ref> [10] </ref> and are in agreement. We have grouped our results by general type (i.e. mammal, prokaryote, etc.). The cdna-compress estimates are generated by partitioning the sequence into 20 equal segments: s 1 ; s 2 ; :::s 20 .
Reference: [11] <author> J. Ziv and A. Lempel, </author> <title> A universal algorithm for sequential data compression, </title> <journal> IEEE Transactions on Information Theory, </journal> <month> IT-23 </month> <year> (1977). </year>
Reference-contexts: The 1:90 bit result is then even more surprising since it implies that knowledge of the immediate past reduces the entropy estimate by a mere 0:05 bits. 1 Data compression techniques such as Lempel-Ziv (LZ) coding (see their original paper <ref> [11] </ref> and the discussion of the considerable work that followed in [2]) may be viewed as entropy estimators, with LZ corresponding to a model that predicts based on a historical context of variable length.
References-found: 11

