URL: http://www.daimi.aau.dk/~depenau/PUB/fapp.ps
Refering-URL: http://www.daimi.aau.dk/~depenau/PUB/pub.html
Root-URL: http://www.daimi.aau.dk
Email: Email: jd.terma@login.dkuug.dk  Email: depenau@daimi.aau.dk  
Phone: Phone: +45 86222000,  Phone: +45 89423371,  
Title: A Global-Local Learning Architecture for Classification Self-Constructing methods that combine Sigmoid and Radial Basis Function units  
Author: Jan Depenau and 
Address: Ny Munkegade, Bldg. 540, DK-8000 Aarhus C  
Affiliation: DAIMI, Computer Science Department, Aarhus University,  
Web: DK-8520 Lystrup  
Note: TERMA Elektronik AS, Hovmarken 4,  F.1 Introduction 182  
Abstract: The Global-Local approach, or simply GLOCAL, is a supervised learning method for dynamically building and training neural networks. The GLOCAL method starts with a global classification by training a simple one-layer perceptron. The weights are then frozen and will never be changed. The patterns that the perceptron is unable to learn, are then corrected with a local classification made by inserting and training a suitable number of radial basis functions (RBF). The global and local classification are combined in a new output layer so the final network becomes a Multi-Layer Perceptron (MLP) with one hidden layer that contains both sigmoid units and RBF units. The described version of the method is faster than the normal MLP, because only one of the sigmoid units is trained at a time and the RBF units do not need a slow error propagation training. For training the RBF units a new cluster algorithm that takes the class membership into consideration is presented. GLOCAL is shown to converge to zero error whilst learning any consistent classification of real-valued input. A test made on a practical example shows that a GLOCAL-net performs better than the popular Cascade-Correlation Algorithm (CCA) and equally well as a fine-tuned MLP. The performance of a neural network is highly dependent on its architecture. The number of inputs and outputs to the network is often given by the nature of the problem, while the question of how to determine a suitable number of units and connections in the intermediate layers is still unanswered. In the last decade several more or less mathematically based methods for successively building a neural network during training have been proposed. A survey of some of the best known constructive algorithms can be found in Alpaydin [1]. These methods can be further categorised, one where the metric is global and one where the metric is local. The methods based on a global metric have attracted most attention in the academic world. The most popular among these methods is probably the CCA which along with the Marchand method [5] is related to the work presented in this paper. However, most of the methods seen today either have no proof of convergence or such a proof is limited to the binary case. From the other category several methods based on the use of Radial Basis Functions or kernel functions have shown good performance and are well suited for real life applications implemented in hardware, see RCE [2] and RAN [8]. This paper describes a hybrid method, called GLOCAL, that combines the two approaches by using different activity functions and a new clustering method. The overall philosophy is: first do as well as possible for as many as possible in a simple way and then apply special treatment, i.e. use a more refined method, for those causing problems. Experiments confirmed the usefulness of this idea. fl Supported by the Danish Academy of Technical Sciences.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. </author> <month> Alpaydin </month> <year> (1991): </year> <title> GAL : Networks that grow when they learn and shrink when they forget, </title> <type> TR 91-032, </type> <institution> International Computer Science Institute. </institution> <month> May </month> <year> 1991. </year>
Reference: [2] <author> Reilly D.E., Cooper, L.N. and Elbaum, C. </author> <year> (1982). </year> <title> A Neural Model for Category Learning. </title> <journal> Biological Cybernetics, </journal> <volume> Vol 45, </volume> <pages> pp 35-41, </pages> <publisher> Springer-Verlag Berlin-Heidelberg. </publisher>
Reference: [3] <author> S. E. Fahlman and C. </author> <booktitle> Lebiere (1990), The Cascade-Correlation Learning Architecture, in NIPS 2, </booktitle> <editor> Editors D. Touretzky, M. </editor> <publisher> Kaufmann Publishers, Inc., </publisher> <address> Denver, Colorado, </address> <pages> pp. 524-532. </pages>
Reference-contexts: The output units were also trained by BP with j = 0:001 and ff = 0:9. For the sake of comparison the CCA was tested as well. The original code made by Fahlman <ref> [3] </ref> was used with the following parameters: j = 0:1, offset = 0:1, the "patience factor" = 0 and pool size = 1. The training set contained 525 patterns and the test set 184 174 patterns. No validation set was used.
Reference: [4] <author> B. Fritzke. </author> <title> Growing Cell Structures- A Self-Organizing Network for Unsupervised and Supervised Learning, </title> <booktitle> Neural Networks, </booktitle> <volume> Vol 7, </volume> <pages> pp 1441-1460, </pages> <year> 1994. </year>
Reference-contexts: Further discussion of this item will not be given here, but it should be mentioned that the answer is not trivial and of great influence for the overall performance. In <ref> [4] </ref> and [6] there are good illustrations of the problem with metric and dimensionality. 183 Once the centre position is placed, the RBF unit's parameter is determined. How this is done depends among other things on the choice of .
Reference: [5] <author> M.M. Marchand, M Golea, and P Rujan. </author> <title> A Convergence Theorem for Sequential Learning in Two-Layer Perceptron, </title> <journal> Europhysics Letters 11, </journal> <pages> pp. 487-492, </pages> <year> 1990. </year>
Reference: [6] <author> M.T. Musavi, W. Ahmed, K.H. Chan, </author> <title> K.B. Faris and D.M. Hummels. On the Training of Radial Basis Function Classifiers, </title> <booktitle> Neural Networks, </booktitle> <volume> Vol 5, </volume> <pages> pp 595-603, </pages> <year> 1992 </year>
Reference-contexts: Further discussion of this item will not be given here, but it should be mentioned that the answer is not trivial and of great influence for the overall performance. In [4] and <ref> [6] </ref> there are good illustrations of the problem with metric and dimensionality. 183 Once the centre position is placed, the RBF unit's parameter is determined. How this is done depends among other things on the choice of . <p> How this is done depends among other things on the choice of . It may be calculated or trained, but the general idea is to adjust some radii oe ndim in order to control the overlap between different clusters, see e.g. <ref> [6] </ref>. Having determined the RBF units, the outputs from all units are now connected to units in a new output layer. The number of units in the new output layer is of course equal to the number of classes.
Reference: [7] <author> M. Moller and J. Depenau, </author> <title> Soft-Monotonic Error Functions, </title> <booktitle> Proceedings from the World Congress on Neural Networks, </booktitle> <volume> Vol 3, </volume> <pages> pp. 444-449, </pages> <address> San Diego 94 </address>
Reference: [8] <author> J.C. </author> <title> Platt (1991), Learning by Combining memorization and Gradient Descent, </title> <booktitle> in NIPS 3, </booktitle> <editor> Editors D. Touretzky, M. </editor> <publisher> Kaufmann Publishers, Inc., </publisher> <address> Denver, Colorado, </address> <pages> pp. 714-720. </pages>
Reference: [9] <author> L. Prechelt, </author> <title> Proben 1 A Set of Neural Network Benchmark Problems and Benchmarking Rules. Found on connectionist ftp list. </title> <type> 185 </type>
Reference-contexts: However, if the latter approach is used or does not fulfil the above condition, convergence is not guaranteed. F.3 Implementation and Experiment In order to show the potential of GLOCAL, a simple version was tested on a Benchmark problem, the cancer1 problem , taken from PROBEN-1 database <ref> [9] </ref>. The task is to classify a tumor as either benign or malign based on cell descriptions gathered by microscopic examination. <p> The CCA used between 1004 and 2435 epochs to learn the task while the GLOCAL was preconditioned to always run 150 epochs for the perceptron and the output layer. 60 trials were run for each experiment. The result along with the result for the MLP taken from <ref> [9] </ref> are shown in table 1.
References-found: 9

