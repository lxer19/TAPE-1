URL: http://daphne.cs.ucla.edu/elias/publications/paper-dk93.ps
Refering-URL: http://daphne.cs.ucla.edu/elias/publications/index.html
Root-URL: http://www.cs.ucla.edu
Email: deng@cs.yorku.ca  elias@cs.ucla.edu  philmac@cs.idbsu.edu  
Title: Competitive Implementation of Parallel Programs  
Author: Xiaotie Deng Elias Koutsoupias Philip MacKenzie 
Note: Partially supported by an NSERC grant. Partially supported by NSF grant CCR-9521606. Partially supported by TARP grant 003658480.  
Address: North York, Ontario M3J 1P3  Los Angeles Los Angeles, CA 90095  Boise, Idaho 83725  
Affiliation: Department of Computer Science York University  Computer Science Department University of California,  Department of Mathematics and Computer Science Boise State University  
Abstract: We apply the methodology of competitive analysis of algorithms to the implementation of programs on parallel machines. We consider the problem of finding the best on-line distributed scheduling strategy that executes in parallel an unknown directed acyclic graph 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon, J. H. Spencer, and P. Erdos. </author> <title> The probabilistic method. </title> <publisher> Wiley & Sons, Inc., </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: In fact, this is an overly conservative estimation, but it simplifies the analysis without affecting the competitive ratio. Therefore, the optimum execution time is at least maxf (r kct )ct + t=k; tg ((r kct )ct + t=k) + (1 )t for any real 2 <ref> [0; 1] </ref>. Let = (c=(c + 1) + (k 1)=k) 1 which is in the interval [0; 1] for c &gt; 1=(k 1). <p> Therefore, the optimum execution time is at least maxf (r kct )ct + t=k; tg ((r kct )ct + t=k) + (1 )t for any real 2 <ref> [0; 1] </ref>. Let = (c=(c + 1) + (k 1)=k) 1 which is in the interval [0; 1] for c &gt; 1=(k 1). <p> As in the proof of Theorem 2 we get the following lower bound on the optimal execution time: maxf (r kct )ct + t=k; t=2g ((r kct )ct + t=k) + (1 )t=2 for any real 2 <ref> [0; 1] </ref>. Choosing = (2c=(c + 1) + (k 2)=k) 1 , we get that the competitive ratio is at most 2 + k2 k c . <p> The optimal execution time, however, is bounded from below by both the average work, n=2, and the length of the longest path p: maxfn=2; pg. Therefore, for any 2 <ref> [0; 1] </ref>, the optimum execution time is at least (n=2) + (1 )p. For = 2=3, we have that the optimal execution time is at least (n + p)=3 (n + b)=3 t =3. <p> Proof. As it has already been mentioned, the upper bound holds for all dags. In order to obtain the lower bound we will use the probabilistic method <ref> [1] </ref>. Consider a random adversary for a chain of binary trees: Each leaf of a complete tree is chosen as the root of the next tree with equal probability. <p> We will need the following Chernoff bound (see <ref> [1] </ref> for a proof of the case ff = 2): P 4 j=1 ff + 1 fi 2 fi 5 &lt; e 3 2 =2 Choosing = q 2 ln k=3 and fi = 16 2 = 32 ln k=3, we have P 4 min fi X X ij &lt; 4
Reference: [2] <author> D. E. Culler, R. M. Karp, D. Patterson, A. Sahay, E. E. Santos, K. E. Schauser, R. Subramonian, and T. von Eicken. </author> <title> LogP: A practical model of parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 39(11) </volume> <pages> 78-85, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: Other models of parallel computation assume a communication delay and usually have more parameters for the communication cost. The BSP model, proposed as a bridge between software and hardware [14], and the LogP model, developed by Culler et al. <ref> [2] </ref> are two such models. These models are not limited to parallel computing but they try to capture the locality properties of a machine, a factor that is also important in distributed systems, especially for clusters of workstations. <p> These models are not limited to parallel computing but they try to capture the locality properties of a machine, a factor that is also important in distributed systems, especially for clusters of workstations. Similar trends for unifying parallel and distributed computing are observed in parallel architecture and parallel programming <ref> [2, 13] </ref>. <p> To derive a general lower bound, we assume a very weak protocol of point to point communication, which is adopted in many parallel computation models <ref> [2, 14] </ref>: At each step, exactly one processor can send the result of a single node to only one processor; the result arrives at that processor after a delay of t steps.
Reference: [3] <author> R. P. </author> <title> Dilworth. A decomposition theorem for partially ordered sets. </title> <journal> Annals of Mathematics, </journal> <volume> 51(1) </volume> <pages> 161-166, </pages> <year> 1950. </year> <month> 22 </month>
Reference-contexts: An important parameter of a dag G is its width W (G) which is the maximum number of pairwise disconnected vertices. By Dilworth's theorem <ref> [3] </ref>, the width of a dag G, is equal to the size of a mimimum chain cover, that is, a minimum size set of chains that covers all nodes of G.
Reference: [4] <author> R. L. Graham. </author> <title> Bounds for certain multiprocessor timing anomalies. </title> <journal> Bell System Technical Journal, </journal> <volume> 45 </volume> <pages> 1563-1581, </pages> <year> 1966. </year>
Reference-contexts: The concept of competitive analysis has been introduced recently by Sleator and Tarjan in [12] (see also [6, 8]). For the scheduling problem, however, Graham <ref> [4] </ref> recognized almost thirty years ago that, without knowing job lengths, a scheduler can achieve twice the optimum. To formally define the competitive ratio for our problem, we first introduce some notation. <p> In the extreme case of no communication delay (t = 0), the obvious algorithm is that each processor repeatedly chooses a path from the set of unexecuted paths and finishes it. This algorithm is optimal with competitive ratio 2 1=k <ref> [4] </ref>. Unfortunately, this algorithm can not be applied when t &gt; 0, because after a processor finishes a path, it does not instantly know the set of unexecuted paths; this information becomes available with a delay of t time units. <p> For two processors and general dags a lower bound 3=2 on the competitive ratio holds even with no communication delay (t = 0) <ref> [4] </ref>. Even for binary trees (input at the root and outputs at leaves) when we allow nodes with one child, the lower bound is still 3/2. <p> For example, a full binary tree with very long paths hung from the leaves behaves almost as a set of independent tasks with unknown execution times. But this is precisely the class of graphs used in proving a lower bound of 3/2 for general dags in <ref> [4] </ref>. However, this lower bound does not apply to full binary trees; a binary tree is full if and only if every internal node has exactly two children. For this case the communication delays become important. <p> After executing these nodes the processors wait for t time units so that each processor knows all the outcomes. The competitive ratio of this algorithm is (2 1=k)(t + 1). The reason is that if we ignore the communication delays, the competitive ratio is 2 1=k <ref> [4] </ref>, and the communication multiplies the on-line execution time by t + 1. On the other hand, a competitive ratio of k is also trivially achievable when only one processor is used by the on-line algorithm.
Reference: [5] <author> H. Jung, L. M. Kirousis, and P. Spirakis. </author> <title> Lower bounds and efficient algorithms for multiprocessor scheduling of directed acyclic graphs with communication delays. </title> <journal> Information and Computation, </journal> <volume> 105(1) </volume> <pages> 94-104, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The approach of [10] and [11] assumes that the computation dag is known before the algorithm actually runs, essentially at compile time (the algorithm in [11] can be made on-line, but only by using a very high number of processors <ref> [5] </ref>). This assumption may not be realistic for all the parallel programs, since conditional statements and loops are important parts of any programming language.
Reference: [6] <author> A. R. Karlin, M. S. Manasse, L. Rudolph, and D. D. Sleator. </author> <title> Competitive snoopy caching. </title> <journal> Algorithmica, </journal> <volume> 3(1) </volume> <pages> 79-119, </pages> <year> 1988. </year>
Reference-contexts: The concept of competitive analysis has been introduced recently by Sleator and Tarjan in [12] (see also <ref> [6, 8] </ref>). For the scheduling problem, however, Graham [4] recognized almost thirty years ago that, without knowing job lengths, a scheduler can achieve twice the optimum. To formally define the competitive ratio for our problem, we first introduce some notation.
Reference: [7] <author> W. Loewe and W. Zimmermann. </author> <title> Upper time bounds for executing pram-programs on the logp-machine. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 41-50, </pages> <year> 1995. </year>
Reference-contexts: The same approach may be applied to parallel algorithms specially designed for coarse grained parallel computers. Some recent results <ref> [7, 16] </ref> are very promising for special classes of parallel algorithms. Prior to our work, there has already been some work by Wu and Kung in [15] which acknowledges the on-line nature of the parallel implementation of algorithms, in a different model of parallel computation.
Reference: [8] <author> M. Manasse, L. A. McGeoch, and D. Sleator. </author> <title> Competitive algorithms for server problems. </title> <journal> Journal of Algorithms, </journal> <volume> 11 </volume> <pages> 208-230, </pages> <year> 1990. </year>
Reference-contexts: The concept of competitive analysis has been introduced recently by Sleator and Tarjan in [12] (see also <ref> [6, 8] </ref>). For the scheduling problem, however, Graham [4] recognized almost thirty years ago that, without knowing job lengths, a scheduler can achieve twice the optimum. To formally define the competitive ratio for our problem, we first introduce some notation.
Reference: [9] <author> R. Motwani, S. Phillips, and E. Torng. </author> <title> Nonclairvoyant scheduling. </title> <journal> Theoretical Computer Science, </journal> <volume> 130(1) </volume> <pages> 17-47, </pages> <year> 1994. </year>
Reference-contexts: Any scheduling which would not intentionally idle achieves the optimal completion time. Even for the system performance metric of mean response time, the Round-Robin scheduling strategy guarantees a mean response time at most twice the optimum, without using any information about the actual executed dag <ref> [9] </ref>. The situation becomes more complicated when we deal with parallel programs, since one of the intricacies of parallel computation is that the optimum algorithm may depend critically on the profile of the parallel machine.
Reference: [10] <author> C. H. Papadimitriou and J. D. Ullman. </author> <title> A communication-time tradeoff. </title> <journal> SIAM Journal on Computing, </journal> <volume> 16(4) </volume> <pages> 639-646, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: Our work focuses on a simplified distributed memory model, the communication delay model introduced in [11]. We must stress, however, that our main negative result 2 applies to most models with distributed memory. 1.1 Distributed Memory Models In this paper, we assume the Papadimitriou-Yannakakis communication delay model <ref> [10, 11] </ref> to study the implementation problem of parallel programs on general purpose multicomputer systems. In this model, a universal parameter t , the communication delay between processors measured in instruction cycles, is used to abstract communication in parallel machines. <p> The approach of <ref> [10] </ref> and [11] assumes that the computation dag is known before the algorithm actually runs, essentially at compile time (the algorithm in [11] can be made on-line, but only by using a very high number of processors [5]). <p> Unfortunately, our result (Theorem 7) shows that this is far from possible for the communication delay model <ref> [10, 11] </ref>: No scheduler can guarantee a competitive ratio for divide-&-conquer trees better than ( t log t ). We also extend this result to the LogP model for a lower bound of ( L log L ), where L is the communication latency in the LogP model.
Reference: [11] <author> C. H. Papadimitriou and M. Yannakakis. </author> <title> Towards an architecture-independent analysis of parallel algorithms. </title> <journal> SIAM Journal on Computing, </journal> <volume> 19(2) </volume> <pages> 322-328, </pages> <year> 1990. </year>
Reference-contexts: Nevertheless, we may classify parallel models into two major categories: models with shared memory, and models with distributed memory. Our work focuses on a simplified distributed memory model, the communication delay model introduced in <ref> [11] </ref>. We must stress, however, that our main negative result 2 applies to most models with distributed memory. 1.1 Distributed Memory Models In this paper, we assume the Papadimitriou-Yannakakis communication delay model [10, 11] to study the implementation problem of parallel programs on general purpose multicomputer systems. <p> Our work focuses on a simplified distributed memory model, the communication delay model introduced in [11]. We must stress, however, that our main negative result 2 applies to most models with distributed memory. 1.1 Distributed Memory Models In this paper, we assume the Papadimitriou-Yannakakis communication delay model <ref> [10, 11] </ref> to study the implementation problem of parallel programs on general purpose multicomputer systems. In this model, a universal parameter t , the communication delay between processors measured in instruction cycles, is used to abstract communication in parallel machines. <p> shared memory machines, there is a factor similar to communication delay: It is the ratio of the cost for accessing the shared memory over the cost for accessing private caches of individual processors (a widely used technology to improve performance of parallel computers.) 1.2 Competitive Analysis It was shown in <ref> [11] </ref> that there exists a polynomial time algorithm that approximates the optimum execution time of any dag within a factor of two when the number of available processors is unbounded. The approach of [10] and [11] assumes that the computation dag is known before the algorithm actually runs, essentially at compile <p> used technology to improve performance of parallel computers.) 1.2 Competitive Analysis It was shown in <ref> [11] </ref> that there exists a polynomial time algorithm that approximates the optimum execution time of any dag within a factor of two when the number of available processors is unbounded. The approach of [10] and [11] assumes that the computation dag is known before the algorithm actually runs, essentially at compile time (the algorithm in [11] can be made on-line, but only by using a very high number of processors [5]). <p> The approach of [10] and <ref> [11] </ref> assumes that the computation dag is known before the algorithm actually runs, essentially at compile time (the algorithm in [11] can be made on-line, but only by using a very high number of processors [5]). This assumption may not be realistic for all the parallel programs, since conditional statements and loops are important parts of any programming language. <p> Many values for the three parameters F , k, and t result in interesting special cases. For example, when the family F contains only one element, the input dag is essentially known to the algorithm; all problems studied in <ref> [11] </ref> are of this kind. Consider an on-line scheduling strategy S that executes a dag from a given family F with k processors and communication delay ratio t . <p> Unfortunately, our result (Theorem 7) shows that this is far from possible for the communication delay model <ref> [10, 11] </ref>: No scheduler can guarantee a competitive ratio for divide-&-conquer trees better than ( t log t ). We also extend this result to the LogP model for a lower bound of ( L log L ), where L is the communication latency in the LogP model. <p> We show that a variant of the algorithm in <ref> [11] </ref> can be implemented on-line with a competitive ratio 2 when the number of processors is at least equal to the maximum width of the dag. <p> For general trees we show a lower bound on the competitive ratio of (t = log t ). This lower bound is a general result that applies to many models of parallel computation. The Papadimitriou-Yannakakis model <ref> [11] </ref> assumes that each processor broadcasts the results of its computations to all other processors. The lower bound of O (t = log t ) holds for much weaker models of communication. <p> Notice that since the graph is known qualitatively, the chain cover number w (G) can be computed at the compile time. The following on-line algorithm is an adaptation of the approximation algorithm in <ref> [11] </ref>. It uses only w (G) processors by making effective use of a chain cover of a dag G; in contrast, 7 the algorithm in [11] needs a processor for each node of G. <p> The following on-line algorithm is an adaptation of the approximation algorithm in <ref> [11] </ref>. It uses only w (G) processors by making effective use of a chain cover of a dag G; in contrast, 7 the algorithm in [11] needs a processor for each node of G. Algorithm A: Let w = w (G) be the maximum width of G and let C 1 , C 2 ,: : :,C w be a chain cover of G. <p> Let us denote by q v the processor that is assigned to execute the chain that contains node v. We adapt the proof used in <ref> [11] </ref> for showing that there exists a 2-approximate algorithm: Let e be an integer function defined inductively on the nodes of the dag G as follows: For a node v with p t + 1 predecessors, let e (v) = p; otherwise, order the predecessors of v in decreasing order, according <p> Then e (v) = e (u t+1 ) + t + 1. It was shown in <ref> [11] </ref> that no algorithm, on-line or off-line one, can execute a node v in less than e (v) + 1 steps. So, it suffices to show that Algorithm A executes every node v by time 2e (v) + 1. We use induction on the depth of v.
Reference: [12] <author> D. D. Sleator and R. E. Tarjan. </author> <title> Amortized efficiency of list update and paging rules. </title> <journal> Communications of the ACM, </journal> <volume> 28(2) </volume> <pages> 202-208, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: The concept of competitive analysis has been introduced recently by Sleator and Tarjan in <ref> [12] </ref> (see also [6, 8]). For the scheduling problem, however, Graham [4] recognized almost thirty years ago that, without knowing job lengths, a scheduler can achieve twice the optimum. To formally define the competitive ratio for our problem, we first introduce some notation.
Reference: [13] <author> A. S. Tanenbaum, M. F. Kaashoek, and H. E. Bal. </author> <title> Parallel programming using shared objects and broadcasting. </title> <journal> Computers, </journal> <volume> 25(8) </volume> <pages> 10-20, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: These models are not limited to parallel computing but they try to capture the locality properties of a machine, a factor that is also important in distributed systems, especially for clusters of workstations. Similar trends for unifying parallel and distributed computing are observed in parallel architecture and parallel programming <ref> [2, 13] </ref>.
Reference: [14] <author> L. G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Other models of parallel computation assume a communication delay and usually have more parameters for the communication cost. The BSP model, proposed as a bridge between software and hardware <ref> [14] </ref>, and the LogP model, developed by Culler et al. [2] are two such models. These models are not limited to parallel computing but they try to capture the locality properties of a machine, a factor that is also important in distributed systems, especially for clusters of workstations. <p> To derive a general lower bound, we assume a very weak protocol of point to point communication, which is adopted in many parallel computation models <ref> [2, 14] </ref>: At each step, exactly one processor can send the result of a single node to only one processor; the result arrives at that processor after a delay of t steps.
Reference: [15] <author> I-Chen Wu and H. T. Kung. </author> <title> Communication complexity for parallel divide-and-conquer. </title> <booktitle> In 32nd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 151-162, </pages> <year> 1991. </year> <month> 23 </month>
Reference-contexts: The same approach may be applied to parallel algorithms specially designed for coarse grained parallel computers. Some recent results [7, 16] are very promising for special classes of parallel algorithms. Prior to our work, there has already been some work by Wu and Kung in <ref> [15] </ref> which acknowledges the on-line nature of the parallel implementation of algorithms, in a different model of parallel computation.
Reference: [16] <author> W. Zimmermann and W. Loewe. </author> <title> An approach to machine-independent parallel programming. </title> <booktitle> In Parallel Programming: CONPAR 94-VAPP VI, volume 854 of Lecture Notes in Computer Science, </booktitle> <pages> pages 277-288. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year> <month> 24 </month>
Reference-contexts: The same approach may be applied to parallel algorithms specially designed for coarse grained parallel computers. Some recent results <ref> [7, 16] </ref> are very promising for special classes of parallel algorithms. Prior to our work, there has already been some work by Wu and Kung in [15] which acknowledges the on-line nature of the parallel implementation of algorithms, in a different model of parallel computation.
References-found: 16

