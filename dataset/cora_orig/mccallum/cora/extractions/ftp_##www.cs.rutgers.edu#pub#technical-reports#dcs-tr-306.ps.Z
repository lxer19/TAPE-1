URL: ftp://www.cs.rutgers.edu/pub/technical-reports/dcs-tr-306.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: FOUNDATIONS OF RECURRENT NEURAL NETWORKS  Written under the direction of  
Author: BY HAVA (EVE) TOVA SIEGELMANN Professor Eduardo D. Sontag 
Degree: A dissertation submitted to the Graduate School|New Brunswick  in partial fulfillment of the requirements for the degree of Doctor of Philosophy  and approved by  
Date: October, 1993  
Note: Graduate Program in Computer Science  
Address: New Jersey  Brunswick, New Jersey  
Affiliation: Rutgers, The State University of  New  
Abstract-found: 0
Intro-found: 0
Reference: [AA87] <author> J. Alspector and R.B. Allen. </author> <title> A neuromorphic vlsi learning system. </title> <editor> In P. Loseleben, editor, </editor> <booktitle> Advanced Research in VLSI: Proceedings of the 1987 Stanford Conference, </booktitle> <pages> pages 313-349, </pages> <address> Cambridge, MA, 1987. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Special purpose analog chips are being built to implement these solutions directly in hardware; see for instance <ref> [AA87] </ref>, [EDK + 89]. However, very little work has been done in the direction of exploring the ultimate capabilities of such devices from a theoretical standpoint.
Reference: [ADO91] <author> N. Alon, A.K. Dewdney, and T.J. Ott. </author> <title> Efficient simulation of finite automata by neural nets. </title> <journal> J. A.C.M., </journal> <volume> 38 </volume> <pages> 495-514, </pages> <year> 1991. </year>
Reference-contexts: The use of multi-tape Turing Machines may reduce the bound. Furthermore, it is quite possible that with some care in the construction one may be able to drastically reduce this estimate. One useful tool here may be the result in <ref> [ADO91] </ref> applied to the control unit|here we used a very inefficient simulation.) 5.5 Removing the Sigmoid From the Main Level Here, we show how to construct an equivalent network to the above, in which neurons in the main level compute linear combinations only.
Reference: [ASM93] <author> F. Albertini, E.D. Sontag, and V. Maillot. </author> <title> Uniqueness of weights for neural networks. </title> <editor> In R. Mammone, editor, </editor> <title> Artificial Neural Networks with Applications in Speech and Vision. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1993. </year>
Reference: [Atk89] <author> K.E. Atkinson. </author> <title> An Introduction to Numerical Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Proof. We first measure the difference (error) between the activations of the corresponding internal processors of N 1 (n) and N at time t T (n). This calculation is analogous to that of the chop error in floating point computation, <ref> [Atk89] </ref>. 94 We use the following notations: - N is the number of processors, M is the number of input lines, L N + M + 1. - W 0 is the largest absolute value of the weights of N , W W 0 + 1. xi (t) is the value
Reference: [Bar92] <author> A.R. Barron. </author> <title> Neural net approximation. </title> <booktitle> In Proc. Seventh Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pages 69-72, </pages> <institution> Yale University, </institution> <year> 1992. </year>
Reference: [Bat91] <author> R. Batruni. </author> <title> A multilayer neural network with piecewise-linear structure and backpropagation learning. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2 </volume> <pages> 395-403, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Unless otherwise stated, the function is the simplest possible "sigmoid," namely the saturated-linear function: (x) := &gt; &gt; &gt; &gt; &gt; &lt; 0 if x &lt; 0 1 if x &gt; 1 : 7 This function has appeared in many applications of neural nets (e.g. <ref> [Bat91, BGV88, Lip87, ZZZ92] </ref>). We use it because theorems are easier to prove when using this particular activation.
Reference: [BDG90] <author> J.L. Balcazar, J. Daz, and J. Gabarro. </author> <title> Structural Complexity, volume I and II. </title> <publisher> Springer-Verlag EATCS Monographs, Berlin, </publisher> <pages> 1988-1990. </pages>
Reference-contexts: In the same time unit, the machine M is automatically changed into the "answered" state. Note that the answer to a membership query for L o takes a unit time even for languages which are not computable by any Turing Machine. For background on oracle TMs, see e.g. <ref> [BDG90] </ref>. 19 Chapter 2 A Neural Language In this chapter, we introduce a high-level language which will allow us to easily prove theorems about simulations by neural networks. This chapter can be skipped if one is interested mainly in the results rather than the proof techniques. <p> The class P (S), for a given sparse set S, is the class of all languages computed by Turing machines in polynomial time and using queries from the oracle S. From <ref> [BDG90] </ref>, volume I, Theorem 5.5, pg 112, and Corollary 6.5.1, we conclude as follows: Corollary 6.5.2 net-p = [ S sparse P (S) : From [BDG90], volume I, Theorem 5.11, pg 122 (originally, [Mul56]), we conclude as follows: Corollary 6.5.3 net-exp includes all possible binary languages. <p> From <ref> [BDG90] </ref>, volume I, Theorem 5.5, pg 112, and Corollary 6.5.1, we conclude as follows: Corollary 6.5.2 net-p = [ S sparse P (S) : From [BDG90], volume I, Theorem 5.11, pg 122 (originally, [Mul56]), we conclude as follows: Corollary 6.5.3 net-exp includes all possible binary languages. Furthermore, most Boolean functions require exponential time complexity. <p> Definition 8.3.5 Given a vector function f = ffi as above, we say that f is non-uniformly F (n)-approximable in time A f (n), if there is a Turing Machine M that computes T (n)-Chop (f) using an advice function (c.f. <ref> [BDG90] </ref>, volume I, pg 99-115) in K [F (n), poly (T (n))]. Example 8.3.6 Assume a generalized processor network D that computes in time T . <p> This class captures a very frequently observed features of parallelism, characterized by the Parallel Computation Thesis: time on these models corresponds, modulo polynomial overheads, to space on First Class models. Prominent members of the Second Machine Class are the alternating Turing machines and the Vector Machines ([PS76], see also <ref> [BDG90] </ref>). Neural nets are considered a very appropriate model of parallel computation, due to the fact that the net result embodies the activity of a large number of neurons (hence the name "Parallel Distributed Processing"). <p> However, a single division can turn this digit into the most significant one. We use this power of division, and bitwise AND, to simulate a model of unbounded parallelism introduced by Pratt and Stockmeyer, that of vector machines ([PS76], see also <ref> [BDG90, KR90] </ref>). Vector machines are machines that can implement boolean operations and left and right shifts on their potentially infinite registers; these capabilities give them the power of parallel machines. <p> To make vector machines equivalent in power to other Second Class models, we have to impose the following restriction: no register is ever shifted by more than 2 O (t (n)) positions in a single shift instruction, where t (n) is the machine's running time (see <ref> [BDG90] </ref>). In other words, the arguments V j in the shift instructions always consist of no more than O (t (n)) bits. We call machines with this property restricted. Let VECTOR TIME (t) be the class of languages accepted by restricted vector machines in time O (t (n)).
Reference: [BGSS93] <author> J. L. Balcazar, R. Gavalda, H.T. Siegelmann, and E. D. Sontag. </author> <title> Some structural complexity aspects of neural computation. </title> <booktitle> In IEEE Structure in Complexity Theory Conference, </booktitle> <pages> pages 253-265, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: We study the gap between networks utilizing rational and real weights in Chapter 7, where we reveal an infinite hierarchy in between them. We characterize the weights there in an information-theoretic manner using the notion of Kolmogorov complexity. (This has appeared in <ref> [BGSS93] </ref>.) The third part of the thesis deals with more general notion of networks. We start in Chapter 8, by proving formally that no increase in computational power|up to polynomial time|is achieved 11 by considering more complex networks, that is, allowing high-order connections, more complex activation functions, or heterogeneous neurons.
Reference: [BGV88] <author> J.R. Brown, </author> <title> M.M. Garber, and S.F. Vanable. Articifial neural network on a simd architecture. </title> <booktitle> In Proc. 2nd Symposium on the Frontier of Massively Parallel Computation, </booktitle> <pages> pages 43-47, </pages> <address> Fairfax, VA, </address> <year> 1988. </year>
Reference-contexts: Unless otherwise stated, the function is the simplest possible "sigmoid," namely the saturated-linear function: (x) := &gt; &gt; &gt; &gt; &gt; &lt; 0 if x &lt; 0 1 if x &gt; 1 : 7 This function has appeared in many applications of neural nets (e.g. <ref> [Bat91, BGV88, Lip87, ZZZ92] </ref>). We use it because theorems are easier to prove when using this particular activation.
Reference: [BH89] <author> E.B. Baum and D. Haussler. </author> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1 </booktitle> <pages> 151-160, </pages> <year> 1989. </year>
Reference: [BHM92] <author> J. L. Balcazar, M. Hermo, and E. Mayordomo. </author> <title> Characterizations of logarithmic advice complexity classes. Information Processing 92, </title> <journal> IFIP Transactions A-12, </journal> <volume> 1 </volume> <pages> 315-321, </pages> <year> 1992. </year>
Reference-contexts: Thus we set bounds on the resource-bounded Kolmogorov complexity of the reals used as weights in neural nets, and then prove that such bounds correspond precisely to the amount of advice allowed to nonuniform classes lying between P and P=poly, as studied previously in <ref> [BHM92] </ref>. It is known that P=poly and some of its subclasses can be characterized by polynomial time computation using tally oracles. This motivates us to compare various real-weight neural models in terms of the Kolmogorov complexity of these tally oracles. <p> The class of languages accepted in this class is P, as proved in Chapter 5. * S = K [log; poly]. In this case, the class of languages accepted is Full-P=log, described by Balcazar, Hermo and Mayordomo <ref> [BHM92] </ref>. This is the class of TMs that receive logarithmically long advices, where each advice aimed at words of length n is appropriate for all shorter words as well. <p> h A 2 LOG (LOG is the class of functions f defined on the naturals, such that for some constant c, f (n) c log n for all n 2 IN) such that: 8n 9! n (j! n j h A (n)) 8x (jxj n) This class was shown in <ref> [BHM92] </ref> to have several interesting features. In particular, Theorem 17 there states that Full-P=log coincides with the class of languages recognized in polynomial time by TMs that consult tally sets, having characteristic sequences in K [log; poly]. The equivalence there implies this observation.
Reference: [BR88] <author> J. Berstel and C. Reutenauer. </author> <title> Rational Series and their Languages. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1988. </year>
Reference-contexts: problem can be reduced to it); on the other hand, the problem is believed to be decidable if a linear activation is used (halting in that case is equivalent to a fact that is widely conjectured to follow from classical results due to Skolem and others on rational functions; see <ref> [BR88] </ref>, page 75), and is also decidable in the pure threshold case (there are only finitely many states). As our function is in a sense a combination of thresholds and linear functions, this gap in decidability is perhaps remarkable.
Reference: [BR90] <author> A. Blum and R.L. Rivest. </author> <title> Training a 3-node neural network is np-complete. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 9-18. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year> <month> 160 </month>
Reference: [BSS89] <author> L. Blum, M. Shub, and S. Smale. </author> <title> On a theory of computation and complexity over the real numbers: Np completeness, recursive functions, and universal machines. </title> <journal> Bull. A.M.S., </journal> <volume> 21 </volume> <pages> 1-46, </pages> <year> 1989. </year>
Reference-contexts: The work closest to our model when real numbers are utilized as weights is that on real-number based computation started by Blum, Shub and Smale (see e.g. <ref> [BSS89] </ref>); we believe that our setup is simpler, and is much more appropriate if one is interested in studying neural networks or distributed processor environments. Also related to our work is the standard computational model of register machines. 1.4 Thesis Organization This thesis consists of four parts. <p> The output was also digital. Other models of analog (continuous) input|like the real numbers ; e|are possible, and also possible are models of analog output. However, with a few exceptions such as <ref> [BSS89] </ref>, the field of theory of computation has focused by and large on digital models only. Our model computes functions with digital I/O.
Reference: [CD89] <author> S.M. Carroll and B.W. Dickinson. </author> <title> Construction of neural nets using the radon transform. </title> <booktitle> In Proc. Int. Joint Conf. Neural Networks, </booktitle> <volume> volume I, </volume> <pages> pages 607-611, </pages> <year> 1989. </year>
Reference: [Che80] <author> G.W. Cherry. </author> <title> Pascal Programming Structure: An Introduction To Systematic Programming. </title> <publisher> Reston Publishing, </publisher> <address> Reston Virginia, </address> <year> 1980. </year>
Reference-contexts: Record is a heterogeneous type, consisting of fixed fields of other previously defined types. More details about records can be found in any Pascal manual, e.g. <ref> [Che80] </ref>. 31 ftypeg ::= fordered typegj Counter j Real j fcompound typeg j frecord typeg fordered typeg ::= ffinite typeg j Integer ffinite typeg ::= Boolean j Char j fscalar typeg fscalar typeg ::= fordered type identifierg j fvalueg .. fvalueg j fidentifier listg fvalueg ::= finteger valueg j fscalar valueg <p> This loop repeats length (list) times. At each repetition i, the statement |that has one free variable| is applied to the ith element of the list. 5. With frecord variable listg do fstatementg 2.3.7 Subprograms Two types of subprograms are defined to facilitate top-down design using step-wise refinement <ref> [Che80] </ref>: function and procedure. Functions return one value of predefined type, while procedures return no value. We have previously seen the syntax of declaring functions and procedures, and the syntax used when invoking them. Here, we refine our discussion of subprograms. <p> A program may invoke another one which is either its child, sibling, or sibling of its ancestor. Cyclic invoking is not allowed nor is recursion. The conventions regarding the scope of parameters is as in Pascal (e.g. <ref> [Che80] </ref> Chapter 7.) 48 2.4 Appendix: Compilation of NEL 2.4.1 Compilation of Data Types * Chars are treated similarly to Scalars.
Reference: [CSSM89] <author> A. Cleeremans, D. Servan-Schreiber, and J. McClelland. </author> <title> Finite state automata and simple recurrent recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1, No. 3:372, </volume> <year> 1989. </year>
Reference-contexts: In speech processing applications and language induction, recurrent net models are used as identification models, and they are fit to experimental data by means of a gradient descent optimization (the so-called "backpropagation" technique) of some cost criterion (see <ref> [CSSM89, Elm90, GMC + 92, Pol90, WZ89] </ref>). 1.1.1 Analog Computation The work in this thesis could also be seen as exploring a particular approach to analog computation, one based on dynamical systems of the type used in neural networks research. <p> Pollack left as an open question establishing if high-order connections are really necessary in order to achieve universality, though he conjectured that they are. Pollack's conjecture was assumed as correct in the neural network literature ([SCLG91, GMC + 92]). High order networks (i.e. <ref> [CSSM89, Elm90, GMC + 92, Pol90, WZ89] </ref>) have been used in applications. One motivation often cited for the use of high-order nets was Pollack's conjecture that their computational power is superior to that of linearly interconnected nets. <p> A numerical (gradient-descent) technique is used to "infer" an accepting network from a set of training examples. Much effort has been directed towards practical implementations of this application. Several models of acceptors have been developed. See for instance <ref> [CSSM89, Elm90, GMC + 92, Pol90, WZ89] </ref>. However, all of this has been done heuristically; some languages were found to be "learned", others were not. One of the main difficulties is that of deciding a priori on the proper size of the network to be used.
Reference: [Cyb89] <author> G. Cybenko. </author> <title> Approximation by superpositions of a sigmoidal function. </title> <journal> Math. Control, Signals, and Systems, </journal> <volume> 2 </volume> <pages> 303-314, </pages> <year> 1989. </year>
Reference: [DDGS93] <author> C. Darken, M. Donahue, L. Gurvits, and E. Sontag. </author> <title> Rate of approximation results motivated by robust neural network learning. </title> <booktitle> In Proc. Sixth ACM Workshop on Computational Learning Theory, </booktitle> <address> Santa Cruz, </address> <month> July </month> <year> 1993. </year>
Reference: [DS92] <author> D. DasGupta and G. Schnitger. </author> <title> The power of approximating: a comparison of activation functions. </title> <booktitle> In Conf. on Neural Information Processing Systems, </booktitle> <address> Denver, </address> <year> 1992. </year>
Reference: [EDK + 89] <author> S. P. Eberhardt, T. Daud, D. A. Kerns, T. X. Brown, and A. P. Thakoor. </author> <title> Competitive neural architecture for hardware solution to the assignment problem. Neural Networks, </title> <type> 4, </type> <year> 1989. </year>
Reference-contexts: Special purpose analog chips are being built to implement these solutions directly in hardware; see for instance [AA87], <ref> [EDK + 89] </ref>. However, very little work has been done in the direction of exploring the ultimate capabilities of such devices from a theoretical standpoint.
Reference: [Elm90] <author> J.L. Elman. </author> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211, </pages> <year> 1990. </year>
Reference-contexts: In speech processing applications and language induction, recurrent net models are used as identification models, and they are fit to experimental data by means of a gradient descent optimization (the so-called "backpropagation" technique) of some cost criterion (see <ref> [CSSM89, Elm90, GMC + 92, Pol90, WZ89] </ref>). 1.1.1 Analog Computation The work in this thesis could also be seen as exploring a particular approach to analog computation, one based on dynamical systems of the type used in neural networks research. <p> Pollack left as an open question establishing if high-order connections are really necessary in order to achieve universality, though he conjectured that they are. Pollack's conjecture was assumed as correct in the neural network literature ([SCLG91, GMC + 92]). High order networks (i.e. <ref> [CSSM89, Elm90, GMC + 92, Pol90, WZ89] </ref>) have been used in applications. One motivation often cited for the use of high-order nets was Pollack's conjecture that their computational power is superior to that of linearly interconnected nets. <p> The validity of this conjecture would imply that our recurrent network model is less powerful than standard models of computation. This conjecture was for a time rather widely accepted by the neural network community. In particular, it was the basic motivation for using high-order models (i.e. ([CSSM89], <ref> [Elm90] </ref>, [GMC + 92], [Pol90], [WZ89]). In this chapter we refute this conjecture. We prove in this chapter that one can simulate all (multi-tape) Turing Machines by nets, using only first-order (i.e., linear) connections and rational weights. Furthermore, this simulation can be done in linear time. <p> A numerical (gradient-descent) technique is used to "infer" an accepting network from a set of training examples. Much effort has been directed towards practical implementations of this application. Several models of acceptors have been developed. See for instance <ref> [CSSM89, Elm90, GMC + 92, Pol90, WZ89] </ref>. However, all of this has been done heuristically; some languages were found to be "learned", others were not. One of the main difficulties is that of deciding a priori on the proper size of the network to be used.
Reference: [FG90] <author> S. Franklin and M. Garzon. </author> <title> Neural computability. </title> <editor> In O. M. Omidvar, editor, </editor> <booktitle> Progress In Neural Networks, </booktitle> <pages> pages 128-144. </pages> <publisher> Ablex, </publisher> <address> Norwood, NJ, </address> <year> 1990. </year>
Reference: [Fra89] <author> J.A. Franklin. </author> <title> On the approximate realization of continuous mappings by neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2 </volume> <pages> 183-192, </pages> <year> 1989. </year>
Reference: [GF89] <author> M. Garzon and S. Franklin. </author> <title> Neural computability. </title> <booktitle> In Proc. 3rd Int. Joint Conf. Neural Networks, </booktitle> <volume> volume II, </volume> <pages> pages 631-637, </pages> <year> 1989. </year>
Reference-contexts: In Chapter 5, we see that no such superiority of computational power exists, at least when formalized in terms of polynomial-time computation. Work that deals with infinite structure is reported by Hartley and Szu ([HS87]) and by Franklin and Garzon ([FG90] and <ref> [GF89] </ref>), some of which deals with cellular automata. There one assumes an unbounded number of neurons, as opposed to a finite number fixed in advance. See also the work by Hong [Hon88], which deals with nonuniform networks with real weights.
Reference: [GJ79] <author> M.R. Garey and D.S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <publisher> Freeman, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Similarly to the class P, the class NP is defined as the class of those 17 functions that can be computed by some nondeterministic TM in polynomial time. A well-known open research question is whether the classes P and NP coincide (see e.g. <ref> [GJ79] </ref>). A Turing Machine That Receives Advice This is a Turing Machine model that, in addition to its input, receives also another sequence that assists in the computation.
Reference: [GMC + 92] <author> C. L. Giles, C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun, and Y.C. Lee. </author> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 4(3), </volume> <year> 1992. </year>
Reference-contexts: In speech processing applications and language induction, recurrent net models are used as identification models, and they are fit to experimental data by means of a gradient descent optimization (the so-called "backpropagation" technique) of some cost criterion (see <ref> [CSSM89, Elm90, GMC + 92, Pol90, WZ89] </ref>). 1.1.1 Analog Computation The work in this thesis could also be seen as exploring a particular approach to analog computation, one based on dynamical systems of the type used in neural networks research. <p> Pollack left as an open question establishing if high-order connections are really necessary in order to achieve universality, though he conjectured that they are. Pollack's conjecture was assumed as correct in the neural network literature ([SCLG91, GMC + 92]). High order networks (i.e. <ref> [CSSM89, Elm90, GMC + 92, Pol90, WZ89] </ref>) have been used in applications. One motivation often cited for the use of high-order nets was Pollack's conjecture that their computational power is superior to that of linearly interconnected nets. <p> The validity of this conjecture would imply that our recurrent network model is less powerful than standard models of computation. This conjecture was for a time rather widely accepted by the neural network community. In particular, it was the basic motivation for using high-order models (i.e. ([CSSM89], [Elm90], <ref> [GMC + 92] </ref>, [Pol90], [WZ89]). In this chapter we refute this conjecture. We prove in this chapter that one can simulate all (multi-tape) Turing Machines by nets, using only first-order (i.e., linear) connections and rational weights. Furthermore, this simulation can be done in linear time. <p> Note that networks with high order polynomials have appeared especially in the language recognition literature (see e.g. <ref> [GMC + 92] </ref> and references there). We emphasize the relationship between these models: Let N 1 be neural network (of any order), which recognizes a language L in polynomial time. Then there is a first order network N 2 which recognizes the same language L in polynomial time. <p> A numerical (gradient-descent) technique is used to "infer" an accepting network from a set of training examples. Much effort has been directed towards practical implementations of this application. Several models of acceptors have been developed. See for instance <ref> [CSSM89, Elm90, GMC + 92, Pol90, WZ89] </ref>. However, all of this has been done heuristically; some languages were found to be "learned", others were not. One of the main difficulties is that of deciding a priori on the proper size of the network to be used. <p> Each neuron computes its next state as a combination of other neurons and the external input, using the following dynamics, as in <ref> [GMC + 92] </ref>: x k (t + 1) = @ i=1 j=1 ij I j (t)x i (t) A : (12.1) Here can be one of the following functions: * Linear L (x) := x * The classical Sigmoid s (x) = 1 + e x * Heaviside (also called threshold) <p> the previous literature had used this quantity as an estimate of "neural" complexity. (Clearly, the size of a minimal NFA is not larger than that of the minimal DFA.) The particular languages used for comparisons are those that were used in past grammatical inference studies based on neural networks, e.g. <ref> [GMC + 92] </ref>. (In the case of Dualparity, the l-complexity can be estimated as 2, since The Language DFA's Size H-Complexity Experiment Tomita1: 1* 2 1 1 Tomita2: (10*) 3 2 2 Tomita3: no (1 odd ) followed by (0 odd ) 5 3 3 Tomita4: does not contain 000 substring <p> Thus, for any finite sample of a strings, the number of neurons required to accept it in the sigmoidal model is bounded by the number in the linear-activation one. To estimate better the space required in the sigmoidal model, we used the second order neural network recognizer developed in <ref> [GMC + 92] </ref>, and trained the weights to generate acceptors for the languages shown in Figure 12.2. The number of neurons found experimentally as required to accept a language in the sigmoidal model is described in the fourth column.
Reference: [Has87] <author> J. H. Hastard. </author> <title> Computational limitations for small depth circuits. </title> <type> PhD thesis, </type> <institution> Mas-sachusetts Institute of Technology, </institution> <year> 1987. </year>
Reference-contexts: Using the carry-look-ahead method, [Sav76], the summation can be computed via a subcircuit of depth O (log (T N )), width O (T 2 N ), and size O (T 2 N ). (This depth is of the same order as the lower bound when polynomial size is imposed, see <ref> [Has87] </ref>, [Yao85].) As for the saturation, one gate, p u , is sufficient for the integer part.
Reference: [HMP92] <author> T.A. Henziger, Z. Manna, and A. Pnueli. </author> <title> Temporal proof methodologies for real-time systems. </title> <type> Technical report, </type> <institution> School of Formal techniques in real-time and fault-tolerant systems, Univ. of Nijmegen, </institution> <address> The Netherlands, </address> <year> 1992. </year>
Reference-contexts: When the program starts running, all input channels are opened automatically. Output channels are defined similarly, however, each of them is opened via an explicit command in the program. NEL defines reactive programs (see e.g. Henziger, Manna and Pnueli <ref> [HMP92] </ref>), those having an on-going interaction with their environments (as opposed to transformational systems which 32 interact in a limited way with humans only). In this sense, NEL defines real-time software.
Reference: [Hon88] <author> J.W. Hong. </author> <title> On connectionist models. </title> <journal> On Pure and Applied Mathematics, </journal> <volume> 41, </volume> <year> 1988. </year> <month> 161 </month>
Reference-contexts: What changes in time are the activation values, or outputs of each processor, which are used in the next iteration. (A synchronous update model is used.) In this sense our model is very "uniform" in contrast with certain models used in the past, including those used in <ref> [Hon88] </ref>, or in the cellular automata literature, which allow the number of units to increase over time and often even the structure to change depending on the length of inputs being presented. <p> There one assumes an unbounded number of neurons, as opposed to a finite number fixed in advance. See also the work by Hong <ref> [Hon88] </ref>, which deals with nonuniform networks with real weights. In the paper [Wol91], Wolpert studies a class of machines with just linear activation functions, and shows that this class is at least as powerful as any Turing Machine (and clearly has super-Turing capabilities as well).
Reference: [Hop84] <author> J.J. </author> <title> Hopfield. Neurons with graded responses have collective computational properties like those of two-state neurons. </title> <journal> In Proc. of the Natl. Acad. of Sciences, </journal> <volume> volume 81, </volume> <pages> pages 3088-3092, </pages> <address> USA, </address> <year> 1984. </year>
Reference-contexts: Electrical circuit implementations of recurrent networks, employing resistively connected networks of n identical nonlinear amplifiers, with the resistor characteristics used to reflect the desired weights, have been proposed as models of analog computers, in particular in the context of con straint satisfaction problems and in content-addressable memory applications (see e.g. <ref> [Hop84] </ref>).
Reference: [Hor91] <author> K. Hornik. </author> <title> Approximation capabilities of multilayer feedforward networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 251-257, </pages> <year> 1991. </year>
Reference: [HS87] <author> R. Hartley and H. Szu. </author> <title> A comparison of the computational power of neural network models. </title> <booktitle> In Proc. IEEE Conf. Neural Networks, </booktitle> <pages> pages 17-22, </pages> <year> 1987. </year>
Reference: [HSW90] <author> K. Hornik, M. Stinchcombe, and H. White. </author> <title> Universal approximation of an unknown mapping and its derivatives using multilayer feedforward networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 551-560, </pages> <year> 1990. </year>
Reference: [HU79] <author> J.E. Hopcroft and J.D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1979. </year>
Reference-contexts: All those strings to which M responds with 1 define the language accepted by the machine. A well-developed field in computer science deals with the characterization of languages into classes defined in terms of the complexity of the machines needed to recognize them (see for example <ref> [HU79] </ref>). These classes include for example the regular languages (those accepted by finite automata), recursive languages (those accepted by Turing machines), and many others. That is, a specific type of machine architecture is associated with the class of languages that it is capable of recognizing.
Reference: [Jud90] <author> J.S. Judd. </author> <title> Neural Network Design and the Complexity of Learning. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1990. </year>
Reference: [KL82] <author> R.M. Karp and R. Lipton. </author> <title> Turing machines that take advice. </title> <journal> Enseign. Math., </journal> <volume> 28, </volume> <year> 1982. </year>
Reference-contexts: The class obtained in this fashion is called P=poly. This model is equivalent to that of non-uniform families of polynomial size circuits, as will be further explained in Chapter 6. See e.g. <ref> [KL82] </ref> for background on TMs that take advice. When exponential advice is allowed, any binary language is computable. (This will correspond to a non-uniform family of circuits of exponential size). <p> Thus, from <ref> [KL82] </ref> we conclude: 104 Theorem 12 If net-np = net-p then the polynomial hierarchy collapses to 2 . The above result says that a theory of computation similar to that which arises in the classical case of Turing machine computation is also possible for our model of analog computation.
Reference: [Kle56] <author> S. C. Kleene. </author> <title> Representation of events in nerve nets and finite automata. </title> <editor> In C.E. Shannon and J. McCarthy, editors, </editor> <booktitle> Automata Studies, </booktitle> <pages> pages 3-41. </pages> <publisher> Princeton Univ. Press, </publisher> <year> 1956. </year>
Reference-contexts: There has been previous work concerned with computability by finite networks, however. The classical result of McCulloch and Pitts ([MP43]) in 1943 (and Kleene <ref> [Kle56] </ref>) showed how to implement logic gates by threshold networks, and therefore how to simulate finite automata by such nets. Another related result was due to Pollack [Pol87]. Pollack argued that a certain recurrent net model, which he called a "neuring machine," is universal. <p> The classical 1943 result of McCulloch and Pitts ([MP43]) (see also Kleene's work <ref> [Kle56] </ref>) shows how to implement logic gates by threshold networks, and therefore how to simulate finite automata by such nets. For us, however, neurons allow for analog values ([0; 1]) rather than the discrete 0-1 McCulloch and Pitts neurons. Thus, potentially our networks are more powerful.
Reference: [Kob81] <author> K. Kobayashi. </author> <title> On compressibility of infinite sequences. </title> <type> Technical Report Research Report C-34, </type> <institution> Department of Information Sciences, Tokyo Institute of Technology, </institution> <year> 1981. </year>
Reference-contexts: Our definition of Kolmogorov complexity of infinite 109 sequences is a time-bounded analog of that in <ref> [Kob81] </ref>. We denote by w 1:k the word consisting of the first k symbols of w. Definition 7.1.1 Fix a universal Turing Machine U . Let f be a nondecreasing function, g a time constructible function, and ff 2 f0; 1g 1 .
Reference: [KR90] <author> R.M. Karp and V. Ramachandran. </author> <title> Parallel algorithms for shared-memory machines. </title> <booktitle> In Handbook of Theoretical Computer Science, volume A, </booktitle> <pages> pages 869-941. </pages> <address> MIT/Elsevier, </address> <year> 1990. </year>
Reference-contexts: However, a single division can turn this digit into the most significant one. We use this power of division, and bitwise AND, to simulate a model of unbounded parallelism introduced by Pratt and Stockmeyer, that of vector machines ([PS76], see also <ref> [BDG90, KR90] </ref>). Vector machines are machines that can implement boolean operations and left and right shifts on their potentially infinite registers; these capabilities give them the power of parallel machines. <p> Recall that addition, product, division, and bitwise AND of m-bit numbers can be computed in parallel machines in time (log m) O (1) and m O (1) memory (see for example <ref> [KR90] </ref>). Thus, updating the state of each processor at each simulated step needs t (n) O (1) time and 2 O (t (n)) memory on the vector machine. In fact, the simulations show that amount of memory in vector machines is polynomially related to net precision.
Reference: [KS93] <author> J. Kilian and H.T. </author> <title> Siegelmann. </title> <booktitle> On the power of sigmoid neural networks. In Proc. Sixth ACM Workshop on Computational Learning Theory, </booktitle> <address> Santa Cruz, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: The work described in this thesis opens up many further questions regarding the foundations of recurrent networks and analog computation. Let me summarize a few of them: * The exact characterization of networks whose activation is the classical sigmoid is still open. They were proved in <ref> [KS93] </ref> to compute any recursive function, and in Chapter 8 we proved that they compute not more than the class P/poly (i.e., according to our "Analog Church's Thesis"). This range is, however, very large. * Our networks are deterministic (non-random) and may compute exact real values.
Reference: [Lip87] <author> R.P. Lippmann. </author> <title> An introduction to computing with neural nets. </title> <journal> IEEE Acoustics Speech and Signal Processing Magazine, </journal> <pages> pages 4-22, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: Unless otherwise stated, the function is the simplest possible "sigmoid," namely the saturated-linear function: (x) := &gt; &gt; &gt; &gt; &gt; &lt; 0 if x &lt; 0 1 if x &gt; 1 : 7 This function has appeared in many applications of neural nets (e.g. <ref> [Bat91, BGV88, Lip87, ZZZ92] </ref>). We use it because theorems are easier to prove when using this particular activation.
Reference: [LLPS93] <author> M. Leshno, V. Y. Lin, A. Pinkus, and S. Schocken. </author> <title> Multilayer feedforward networks with a non-polynomial activation function can approximate any function. Neural Networks, </title> <type> 6, </type> <year> 1993. </year>
Reference: [Maa93] <author> W.G. Maass. </author> <title> Bounds for the computational power and learning complexity of analog neural nets. </title> <booktitle> In Proceeding of the 25 Annual ACM Symposium on Theory of Computation, </booktitle> <address> San Diego, </address> <month> May </month> <year> 1993. </year>
Reference: [Mac92] <author> B. J. MacLennan. </author> <title> Continuous symbol systems: The logic of connectionism. </title> <editor> In D.S. Levine and M. ~ Aparicio IV, editors, </editor> <title> Neural Networks for Knowledge Representation and Inference. </title> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1992. </year>
Reference-contexts: Finally, we remark that human cognition seems to be clearly based on "subsymbolic" or "analog" 6 components and modes of operation. As pointed out by many authors, in particular in the work of <ref> [Mac92] </ref>, the issue of understanding how macroscopic symbolic behavior arises from such a substrate is one of the most challenging ones in science.
Reference: [Mat92] <author> M. Matthews. </author> <title> On the uniform approximation of nonlinear discrete-time fading-memory systems using neural network models. </title> <type> Technical Report Ph.D. Thesis, ETH No. 9635, </type> <institution> E.T.H. </institution> <address> Zurich, </address> <year> 1992. </year> <month> 162 </month>
Reference-contexts: First of all, they constitute a very powerful model of computation, as shown in this work. They are also capable of approximating rather arbitrary dynamical systems, and this is of use in adaptive control and signal processing applications (see [Son92c], <ref> [Mat92] </ref>, and [PI91]). Recurrent nets have also been proposed as models of large scale parallel computation, since they are built of potentially many simple processors or "neurons". One of the primary motivations for their study is as a first approximation of biological neural systems.
Reference: [Min67] <author> M. L. Minsky. </author> <title> Computation: Finite and Infinite Machines. </title> <publisher> Prentice Hall, </publisher> <address> Engelwood Cliffs, </address> <year> 1967. </year>
Reference-contexts: Minsky proved the existence of a universal Turing Machine having one tape with 4 letters and 7 control states, <ref> [Min67] </ref>. Shannon showed in [Sha56] how to control the number of letters and states in a Turing Machine. Following his construction, we obtain a 2-letter 63-state 1-tape Turing Machine. However, we are interested in a two-stack machine rather than one tape.
Reference: [Mot93] <author> L. Motus. </author> <title> Time concepts in real-time software. </title> <journal> Control Enginnering Practice, </journal> <volume> 1 </volume> <pages> 21-33, </pages> <month> Feb </month> <year> 1993. </year>
Reference-contexts: In this sense, NEL defines real-time software. To fully implement NEL, both time concepts and safety properties should be developed. (See more on crucial issues in real-time software in Motus <ref> [Mot93] </ref>.) As we are interested in NEL from a theoretical, rather than implementable, view, we do not fully address these issues.
Reference: [MP43] <author> W. S. McCulloch and W. Pitts. </author> <title> A logical calculus of the ideas immanent in nervous activity. </title> <journal> Bull. Math. Biophys, </journal> <volume> 5 </volume> <pages> 115-133, </pages> <year> 1943. </year>
Reference: [MP88] <author> M. Minsky and S. Papert. </author> <title> Perceptrons: An Introduction to Computational Geometry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference: [MS93] <author> M. Macintyre and E.D. Sontag. </author> <title> Finiteness results for sigmoidal `neural' networks. </title> <booktitle> In Proceeding of the 25 Annual ACM Symposium on Theory of Computation, </booktitle> <address> San Diego, </address> <month> May </month> <year> 1993. </year>
Reference: [MSS91] <author> W. Maass, G. Schnitger, and E.D. Sontag. </author> <title> On the computational power of sigmoid versus boolean threshold circuits. </title> <booktitle> In Proc. 32nd IEEE Symp. Foundations of Comp. Sci, </booktitle> <pages> pages 767-776, </pages> <year> 1991. </year>
Reference: [Mul56] <author> D.E. Muller. </author> <title> Complexity in electronic switching circuits. </title> <journal> IRE Trans. Electronic Comp., </journal> <volume> 5 </volume> <pages> 15-19, </pages> <year> 1956. </year>
Reference-contexts: From [BDG90], volume I, Theorem 5.5, pg 112, and Corollary 6.5.1, we conclude as follows: Corollary 6.5.2 net-p = [ S sparse P (S) : From [BDG90], volume I, Theorem 5.11, pg 122 (originally, <ref> [Mul56] </ref>), we conclude as follows: Corollary 6.5.3 net-exp includes all possible binary languages. Furthermore, most Boolean functions require exponential time complexity. Nondeterministic Neural Networks The concept of a nondeterministic circuit family is usually defined by means of an extra input, whose role is that of an oracle.
Reference: [Mur71] <author> S. Muroga. </author> <title> Threshold Logic and its Applications. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: Without loss of generality, one may assume that these constants can each be expressed in binary with at most n i log (n i ) bits; see <ref> [Mur71] </ref>. If x i is on the bottom level, its input is the external input.
Reference: [Par92] <author> I. Parberry. </author> <title> Knowledge, understanding, and computational complexity. </title> <type> Technical Report CRPDC-92-2, </type> <institution> Department of Computer Sciences, University of North Texas, </institution> <month> Feb </month> <year> 1992. </year>
Reference-contexts: It is interesting to point out that the work that we report here does allow for such non-Turing power, while keeping track of computational constraints -and thus embedding a possible answer to Penrose's challenge in more classical computer science. Note that Parberry, in <ref> [Par92] </ref>, also insists that possible non-Turing theories should take account of such constraints, though he suggests a very different approach, namely the use of probabilistic computations within the theory of circuit complexity. <p> It is this lack of classical computability that makes circuits a possible model of resource-bounded "computing," as emphasized in <ref> [Par92] </ref>. We will show that recurrent neural networks, although more "uniform" in the sense that they have an unchanging physical structure, share exactly the same power. If L is recognized by the formal net N in time T , we write N = L and T N = T .
Reference: [Par93] <author> I. Parberry. </author> <title> The Computational and Learning Complexity of Neural Networks. </title> <type> draft, </type> <year> 1993. </year>
Reference-contexts: If x i is on the bottom level, its input is the external input. The function H is the threshold function H (z) = &gt; &lt; 1 z 0 (6.5) The relationships between threshold circuits and Boolean circuits are well studied. (See for example <ref> [Par93] </ref>.) They are known to be polynomial equivalent in size. We provide here an alternative direct relationship between threshold circuits and real networks, without passing through Boolean circuits. <p> Each gate of N 0 computes an addition of N m-bit numbers; then, it applies the function to it. Using a technique similar to the one provided in <ref> [Par93] </ref> pg 156-157, we show how to simulate each gate of N 0 via a threshold circuit of size O (m) and depth 2. We achieve the simulation in two steps: First we add the N numbers and then we simulate the application of the saturation functions.
Reference: [Pen89] <author> R. Penrose. </author> <title> The Emperor's New Mind. </title> <publisher> Oxford University Press, Oxford, </publisher> <year> 1989. </year>
Reference-contexts: In classical models of (digital) computation, this type of robustness can not even be properly defined. Comments on Analog and non-Turing "Computation" In the recent, very popular -and very controversial- book <ref> [Pen89] </ref>, Penrose has argued that the standard model of computing is not appropriate for modeling true biological intelligence. The author argues that physical processes, evolving at a quantum level, may result in computations which cannot be incorporated in Church's Thesis.
Reference: [PI91] <author> M. M. Polycarpou and P.A. Ioannou. </author> <title> Identification and control of nonlinear systems using neural network models: Design and stability analysis. </title> <type> Technical Report Report 91-09-01, </type> <institution> Department of EE/Systems, USC, </institution> <address> Los Angeles, </address> <month> Sept </month> <year> 1991. </year>
Reference-contexts: First of all, they constitute a very powerful model of computation, as shown in this work. They are also capable of approximating rather arbitrary dynamical systems, and this is of use in adaptive control and signal processing applications (see [Son92c], [Mat92], and <ref> [PI91] </ref>). Recurrent nets have also been proposed as models of large scale parallel computation, since they are built of potentially many simple processors or "neurons". One of the primary motivations for their study is as a first approximation of biological neural systems.
Reference: [Pol87] <author> J. B. Pollack. </author> <title> On Connectionist Models of Natural Language Processing. </title> <type> PhD thesis, </type> <institution> Computer Science Dept, Univ. of Illinois, Urbana, </institution> <year> 1987. </year>
Reference-contexts: The classical result of McCulloch and Pitts ([MP43]) in 1943 (and Kleene [Kle56]) showed how to implement logic gates by threshold networks, and therefore how to simulate finite automata by such nets. Another related result was due to Pollack <ref> [Pol87] </ref>. Pollack argued that a certain recurrent net model, which he called a "neuring machine," is universal. The model in [Pol87] consisted of a finite number of neurons of two different kinds, having identity and threshold responses, respectively. <p> Another related result was due to Pollack <ref> [Pol87] </ref>. Pollack argued that a certain recurrent net model, which he called a "neuring machine," is universal. The model in [Pol87] consisted of a finite number of neurons of two different kinds, having identity and threshold responses, respectively. The machine was high-order , that is, the activations were combined (as in Equation 1.1) using multiplications as opposed to just linear combinations (as in Equation 1.2). <p> more complicated rational weights than those that can be written as n 1 n 2 , for n 1 ; n 2 2 IN, n 2 2 f1; : : : ; 4g Related work that asserted universality of a finite network|similar to the recurrent network model|was done by Pollack <ref> [Pol87] </ref>. His model consisted of a finite number of neurons with two different activation functions, identity and threshold. The machine was high-order (also called Sigma-Pi), that is, the inputs of each neuron were combined using multiplications as opposed to just linear combinations.
Reference: [Pol90] <author> J.B. Pollack. </author> <title> The induction of dynamical recognizers. </title> <type> Technical Report 90-JP-Automata, </type> <institution> Dept of Computer and Information Science, Ohio State U., </institution> <year> 1990. </year>
Reference-contexts: In speech processing applications and language induction, recurrent net models are used as identification models, and they are fit to experimental data by means of a gradient descent optimization (the so-called "backpropagation" technique) of some cost criterion (see <ref> [CSSM89, Elm90, GMC + 92, Pol90, WZ89] </ref>). 1.1.1 Analog Computation The work in this thesis could also be seen as exploring a particular approach to analog computation, one based on dynamical systems of the type used in neural networks research. <p> Pollack left as an open question establishing if high-order connections are really necessary in order to achieve universality, though he conjectured that they are. Pollack's conjecture was assumed as correct in the neural network literature ([SCLG91, GMC + 92]). High order networks (i.e. <ref> [CSSM89, Elm90, GMC + 92, Pol90, WZ89] </ref>) have been used in applications. One motivation often cited for the use of high-order nets was Pollack's conjecture that their computational power is superior to that of linearly interconnected nets. <p> This conjecture was for a time rather widely accepted by the neural network community. In particular, it was the basic motivation for using high-order models (i.e. ([CSSM89], [Elm90], [GMC + 92], <ref> [Pol90] </ref>, [WZ89]). In this chapter we refute this conjecture. We prove in this chapter that one can simulate all (multi-tape) Turing Machines by nets, using only first-order (i.e., linear) connections and rational weights. Furthermore, this simulation can be done in linear time. <p> A numerical (gradient-descent) technique is used to "infer" an accepting network from a set of training examples. Much effort has been directed towards practical implementations of this application. Several models of acceptors have been developed. See for instance <ref> [CSSM89, Elm90, GMC + 92, Pol90, WZ89] </ref>. However, all of this has been done heuristically; some languages were found to be "learned", others were not. One of the main difficulties is that of deciding a priori on the proper size of the network to be used.
Reference: [PS76] <author> V.R. Pratt and L.J. Stockmeyer. </author> <title> A characterization of the power of vector machines. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 12 </volume> <pages> 198-221, </pages> <year> 1976. </year>
Reference: [Rab66] <author> M. Rabin. </author> <title> Lectures on classical and probabilistic automata. In E.R. Caianiello, editor, Automata Theory. </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1966. </year>
Reference-contexts: Remark 12.2.3 The gap between l-complexity and H-complexity can be arbitrarily large. This is easy to see from counting arguments. Indeed, there are an uncountable number of languages with l-complexity equal to 2|see for instance <ref> [Rab66] </ref>, page 311|but it is easy to see that there are only a countable number of languages with finite H-complexity. 12.3 Different Activation Functions: Using The H-Complexity As a Bound The space complexity using networks employing sigmoidal activation functions: (x) = 1 + e x ; is in a sense bounded
Reference: [Ros62] <author> F. Rosenblatt. </author> <title> Principles of Neurodynamics. </title> <publisher> Spartan Books, </publisher> <address> New York, </address> <year> 1962. </year>
Reference: [Sav76] <author> J.E. Savage. </author> <title> The Complexity of Computing. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1976. </year> <month> 163 </month>
Reference-contexts: Hardwiring the weights, we can say that each processor computes a sum of (T N + 2) (2T )-bit numbers. Using the carry-look-ahead method, <ref> [Sav76] </ref>, the summation can be computed via a subcircuit of depth O (log (T N )), width O (T 2 N ), and size O (T 2 N ). (This depth is of the same order as the lower bound when polynomial size is imposed, see [Has87], [Yao85].) As for the
Reference: [SCLG91] <author> G.Z. Sun, H.H. Chen, Y.C. Lee, and C.L. Giles. </author> <title> Turing equivalence of neural networks with second order connection weights. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, IEEE, </booktitle> <year> 1991. </year>
Reference: [Sha56] <author> C. E. Shannon. </author> <title> A universal turing machine with two internal states. </title> <editor> In C.E. Shannon and J. McCarthy, editors, </editor> <booktitle> Automata Studies, </booktitle> <pages> pages 156-165. </pages> <institution> Princeton Univ., </institution> <year> 1956. </year>
Reference-contexts: Minsky proved the existence of a universal Turing Machine having one tape with 4 letters and 7 control states, [Min67]. Shannon showed in <ref> [Sha56] </ref> how to control the number of letters and states in a Turing Machine. Following his construction, we obtain a 2-letter 63-state 1-tape Turing Machine. However, we are interested in a two-stack machine rather than one tape.
Reference: [Son75] <author> E.D. Sontag. </author> <title> On certain questions of rationality and decidability. </title> <journal> J. Comp. Syst. Sci., </journal> <volume> 11 </volume> <pages> 375-381, </pages> <year> 1975. </year>
Reference-contexts: We first cite Theorem (5.2) in [SS78]: If r 2 IR rat t fl is unambiguous, then L (r; q) is regular for every q 2 IR. (Of course, the only interesting case is q = 1, or equivalently any q 2 (0; 1]. See also Theorem 2.3 in <ref> [Son75] </ref> for another proof.) Conversely, theorem (5.1) in [SS78] states: Let L be a regular language. Then, r c (L) is in IR t fl . Given a word ! = u 1 u n , we denote by ~! its transpose, u n u 1 . <p> In the more realistic case in which the language L is not available, and the only data are a set of "training" strings together with information regarding their membership in the language, then one can show that even the question of finiteness of rank for the Hankel matrix is undecidable <ref> [Son75] </ref>. However, different heuristics might be useful is this case, such as repetitively enlarging the matrix until no change in its rank results after a few iterations. Remark 12.2.3 The gap between l-complexity and H-complexity can be arbitrarily large. This is easy to see from counting arguments.
Reference: [Son79] <author> E. D. Sontag. </author> <title> Realization theory of discrete-time nonlinear systems: Part i- the bounded case. </title> <journal> IEEE Trans.Circuits and Syst., </journal> <volume> 26 </volume> <pages> 342-356, </pages> <year> 1979. </year>
Reference-contexts: This gives a rise to a linear-activation network accepting L with fl = 1 2 . This network is not necessarily the smallest in size. We can then use the algorithm described in <ref> [Son79] </ref> to get a minimal network. (The minimal network is not unique, but any two such networks can be shown to coincide up to a change of basis in the space of neuron states.) The algorithm takes time polynomial in the size of the regular expression. (See the related discussion in
Reference: [Son88] <author> E.D. Sontag. </author> <title> Controllability is harder to decide than accessibility. </title> <journal> SIAM J. Control and Optimization, </journal> <volume> 26(6) </volume> <pages> 1106-1118, </pages> <year> 1988. </year>
Reference-contexts: to get a minimal network. (The minimal network is not unique, but any two such networks can be shown to coincide up to a change of basis in the space of neuron states.) The algorithm takes time polynomial in the size of the regular expression. (See the related discussion in <ref> [Son88] </ref>.) From the above construction, we conclude that Observation 12.2.1 For every language L, H-complexity (L) j NFA (L)j ; where jNFA (L)j is the number of states of a minimal size non-deterministic finite automaton accepting L. 2 In Figure 12.2, the first to third columns (ignore the column labeled "experiment"
Reference: [Son90] <author> E. D. Sontag. </author> <title> Mathematical Control Theory: Deterministic Finite Dimensional Systems. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: The output (or read-out map) is given by y = Cx where C 2 IR pfiN for some integer p. The integer p is called the number of outputs. As controlled dynamical systems (see <ref> [Son90] </ref>), networks can be viewed as discrete time systems built by combining delay lines with memory-free elements, each of which performs a nonlinear transformation on 3 its input. When is the identity and all polynomials have degree one, they are the classical linear systems used in engineering.
Reference: [Son92a] <author> E. D. Sontag. </author> <title> Feedforward nets for interpolation and classification. </title> <journal> J. Comp. Syst. Sci, </journal> <volume> 45 </volume> <pages> 20-48, </pages> <year> 1992. </year>
Reference: [Son92b] <author> E.D. Sontag. </author> <title> Feedback stabilization using two-hidden-layer nets. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 3 </volume> <pages> 981-990, </pages> <year> 1992. </year>
Reference: [Son92c] <author> E.D. Sontag. </author> <title> Neural nets as systems models and controllers. </title> <booktitle> In Proc. Seventh Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pages 73-79, </pages> <institution> Yale University, </institution> <year> 1992. </year>
Reference-contexts: First of all, they constitute a very powerful model of computation, as shown in this work. They are also capable of approximating rather arbitrary dynamical systems, and this is of use in adaptive control and signal processing applications (see <ref> [Son92c] </ref>, [Mat92], and [PI91]). Recurrent nets have also been proposed as models of large scale parallel computation, since they are built of potentially many simple processors or "neurons". One of the primary motivations for their study is as a first approximation of biological neural systems.
Reference: [SS78] <author> A. Salomaa and M. Soittola. </author> <title> Automata-Theoretic Aspects of Formal Power Series. </title> <publisher> Springer-Verlag, </publisher> <address> New-York, </address> <year> 1978. </year>
Reference-contexts: This series is unambiguous. Note that L (r c (L); 1 2 ) L for every language L. From now on, we will assume for simplicity that we have a binary alphabet, I = f0; 1g fl , i.e. M = 2 in Equation 12.1. Definition 12.1.1 <ref> [SS78] </ref> The Hankel matrix, H r , of the power series r is the infinite matrix whose rows and columns are indexed by the strings over f0; 1g fl |listed in the lexicographic order|and is defined by H r (u; v) = C uv : Example 12.1.2 The Hankel matrix H <p> We first cite Theorem (5.2) in <ref> [SS78] </ref>: If r 2 IR rat t fl is unambiguous, then L (r; q) is regular for every q 2 IR. (Of course, the only interesting case is q = 1, or equivalently any q 2 (0; 1]. <p> See also Theorem 2.3 in [Son75] for another proof.) Conversely, theorem (5.1) in <ref> [SS78] </ref> states: Let L be a regular language. Then, r c (L) is in IR t fl . Given a word ! = u 1 u n , we denote by ~! its transpose, u n u 1 . Lemma 12.1.5 [SS78] Let r be a power series over fl . <p> in [Son75] for another proof.) Conversely, theorem (5.1) in <ref> [SS78] </ref> states: Let L be a regular language. Then, r c (L) is in IR t fl . Given a word ! = u 1 u n , we denote by ~! its transpose, u n u 1 . Lemma 12.1.5 [SS78] Let r be a power series over fl . Then, r 2 IR rat t fl iff the rank of H r is finite.
Reference: [SS91a] <author> H. T. Siegelmann and E. D. Sontag. </author> <title> Turing computability with neural nets. </title> <journal> Appl. Math. Lett., </journal> <volume> 4(6) </volume> <pages> 77-80, </pages> <year> 1991. </year>
Reference-contexts: This result is pretty straightforward, but is needed for completeness. Chapter 5 describes the computational power of networks with rational weights. Specifically, it shows their equivalence to the Turing machine model. (Previous versions of it have appeared in <ref> [SS91a] </ref> and [SS92].) In Chapter 6, we completely describe the computational power of networks that utilize real weights. (These findings have appeared in [SS93] and will appear in [SSar].) We establish a precise correspondence between this model and a non-uniform circuit model.
Reference: [SS91b] <author> E.D. Sontag and H.J. Sussmann. </author> <title> Backpropagation separates where perceptrons do. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 243-249, </pages> <year> 1991. </year>
Reference: [SS92] <author> H. T. Siegelmann and E. D. Sontag. </author> <title> On the computational power of neural nets. </title> <booktitle> In Proc. Fifth ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 440-449, </pages> <address> Pittsburgh, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: This result is pretty straightforward, but is needed for completeness. Chapter 5 describes the computational power of networks with rational weights. Specifically, it shows their equivalence to the Turing machine model. (Previous versions of it have appeared in [SS91a] and <ref> [SS92] </ref>.) In Chapter 6, we completely describe the computational power of networks that utilize real weights. (These findings have appeared in [SS93] and will appear in [SSar].) We establish a precise correspondence between this model and a non-uniform circuit model.
Reference: [SS93] <author> H. T. Siegelmann and E. D. Sontag. </author> <title> Analog computation via neural networks. </title> <booktitle> In The second Israel Symposium on Theory of Computing and Systems, </booktitle> <address> Natanya, Israel, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Specifically, it shows their equivalence to the Turing machine model. (Previous versions of it have appeared in [SS91a] and [SS92].) In Chapter 6, we completely describe the computational power of networks that utilize real weights. (These findings have appeared in <ref> [SS93] </ref> and will appear in [SSar].) We establish a precise correspondence between this model and a non-uniform circuit model. We study the gap between networks utilizing rational and real weights in Chapter 7, where we reveal an infinite hierarchy in between them.
Reference: [SSG92] <author> H. T. Siegelmann, E. D. Sontag, and C. L. Giles. </author> <title> The complexity of language recognition by neural networks. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Algorithms, Software, Architecture (Proceedings of IFIP 12th World Computer Congress), </booktitle> <pages> pages 329-335, </pages> <address> Amsterdam, 1992. </address> <publisher> North Holland. </publisher>
Reference-contexts: Given a language, we are interested in finding the minimum size of a network required to recognize it. Estimates are obtained using the theory of power series in noncommuting variables. (Part of this chapter has appeared in <ref> [SSG92] </ref>.) A final chapter summarizes our conclusions and lists several open problems and suggestions for further work. 1.5 Computational Power Preliminaries In the science of computing, machines are classified according to the tasks or functions that they are capable of executing.
Reference: [SSar] <author> H. T. Siegelmann and E. D. Sontag. </author> <title> Analog computation via neural networks. </title> <note> Theoretical Computer Science, to appear. 164 </note>
Reference-contexts: Specifically, it shows their equivalence to the Turing machine model. (Previous versions of it have appeared in [SS91a] and [SS92].) In Chapter 6, we completely describe the computational power of networks that utilize real weights. (These findings have appeared in [SS93] and will appear in <ref> [SSar] </ref>.) We establish a precise correspondence between this model and a non-uniform circuit model. We study the gap between networks utilizing rational and real weights in Chapter 7, where we reveal an infinite hierarchy in between them.
Reference: [Ste84] <author> G.L. Steeler. </author> <title> Common LISP: The language. </title> <institution> Digital Equipment Cooperation USA, </institution> <year> 1984. </year>
Reference: [Sus92] <author> H.J. Sussmann. </author> <title> Uniqueness of the weights for minimal feedforward nets with a given input-output map. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 589-593, </pages> <year> 1992. </year>
Reference: [SW90] <author> M. Stinchcombe and H. White. </author> <title> Approximating and learning unknown mappings using multilayer feedforward networks with bounded weights. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, IEEE, </booktitle> <year> 1990. </year>
Reference: [Tom82] <author> M. Tomita. </author> <title> Dynamic construction of finite-state automata from examples using hill-climbing. </title> <booktitle> In Proceedings of the Fourth Annual Cognitive Science Conference, </booktitle> <pages> pages 105-108, </pages> <address> Ann Arbor MI, </address> <year> 1982. </year>
Reference-contexts: The first seven are known by the name Tomita languages, see <ref> [Tom82] </ref>, and the last two are parity and dual parity.
Reference: [VEB90] <author> P. Van Emde Boas. </author> <title> Machine models and simulations. </title> <booktitle> In Handbook of Theoretical Computer Science, volume A, </booktitle> <pages> pages 1-66. </pages> <address> MIT/Elsevier, </address> <year> 1990. </year>
Reference-contexts: Many Parallel models exist, and not all of them are equivalent. Our parallel models are taken from the so-called Second Machine Class <ref> [VEB90] </ref>. This class captures a very frequently observed features of parallelism, characterized by the Parallel Computation Thesis: time on these models corresponds, modulo polynomial overheads, to space on First Class models. <p> Relativizations of these classes are also used; the oracle machine model used for defining them is standard. All these classes are invariant under changes of the machine model, provided that it stays within the so-called First Machine Class <ref> [VEB90] </ref>: they simulate and are simulated by multitape Turing machines within a polynomial time overhead and a linear space overhead. 140 The extended nets we consider in this chapter have processors with either an update equation of the form x i (t + 1) = P i (x 1 (t); :
Reference: [VSD86] <author> A. Vergis, K. Steiglitz, and B. Dickinson. </author> <title> The complexity of analog computation. </title> <journal> Math. and Computers in Simulation, </journal> <volume> 28 </volume> <pages> 91-113, </pages> <year> 1986. </year>
Reference-contexts: Part of the problem is that, much interesting work notwithstanding, analog computation is hard to model, as difficult questions about precision of data and readout of results are immediately encountered |see for instance <ref> [VSD86] </ref>, and the many references there.
Reference: [Wol91] <author> D. Wolpert. </author> <title> A computationally universal field computer which is purely linear. </title> <type> Technical Report LA-UR-91-2937, </type> <institution> Los Alamos National Laboratory, </institution> <year> 1991. </year>
Reference-contexts: There one assumes an unbounded number of neurons, as opposed to a finite number fixed in advance. See also the work by Hong [Hon88], which deals with nonuniform networks with real weights. In the paper <ref> [Wol91] </ref>, Wolpert studies a class of machines with just linear activation functions, and shows that this class is at least as powerful as any Turing Machine (and clearly has super-Turing capabilities as well). <p> It is essential in that model, again, that the number of "neurons" be allowed to be infinite |as a matter of fact, in <ref> [Wol91] </ref> the number of such units is even uncountable| as the construction relies 10 on using different neurons to encode different possible tape configurations in Turing Machines.
Reference: [Won92] <author> W.S. Wong. </author> <title> Solving combinatorial optimization problems by gradient flows. </title> <booktitle> In Proc. IEEE Conf. Decision and Control, </booktitle> <pages> pages 1494-1496, </pages> <address> Tucson, </address> <month> Dec </month> <year> 1992. </year> <note> IEEE. </note>
Reference-contexts: It is of a great importance to develop learning algorithms and design procedures that will allow this power to be usefully exploited. * Our models operate in discrete time. Other approaches to analog computing (see e.g. <ref> [Won92] </ref> and the references there) are based on differential equations. We strongly conjecture that the "Analog Church's Thesis," when properly formulated, will still be valid for continuous time systems. 159
Reference: [WZ89] <author> R.J. Williams and D. Zipser. </author> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1, No. 2, </volume> <year> 1989. </year>
Reference-contexts: In speech processing applications and language induction, recurrent net models are used as identification models, and they are fit to experimental data by means of a gradient descent optimization (the so-called "backpropagation" technique) of some cost criterion (see <ref> [CSSM89, Elm90, GMC + 92, Pol90, WZ89] </ref>). 1.1.1 Analog Computation The work in this thesis could also be seen as exploring a particular approach to analog computation, one based on dynamical systems of the type used in neural networks research. <p> Pollack left as an open question establishing if high-order connections are really necessary in order to achieve universality, though he conjectured that they are. Pollack's conjecture was assumed as correct in the neural network literature ([SCLG91, GMC + 92]). High order networks (i.e. <ref> [CSSM89, Elm90, GMC + 92, Pol90, WZ89] </ref>) have been used in applications. One motivation often cited for the use of high-order nets was Pollack's conjecture that their computational power is superior to that of linearly interconnected nets. <p> This conjecture was for a time rather widely accepted by the neural network community. In particular, it was the basic motivation for using high-order models (i.e. ([CSSM89], [Elm90], [GMC + 92], [Pol90], <ref> [WZ89] </ref>). In this chapter we refute this conjecture. We prove in this chapter that one can simulate all (multi-tape) Turing Machines by nets, using only first-order (i.e., linear) connections and rational weights. Furthermore, this simulation can be done in linear time. <p> A numerical (gradient-descent) technique is used to "infer" an accepting network from a set of training examples. Much effort has been directed towards practical implementations of this application. Several models of acceptors have been developed. See for instance <ref> [CSSM89, Elm90, GMC + 92, Pol90, WZ89] </ref>. However, all of this has been done heuristically; some languages were found to be "learned", others were not. One of the main difficulties is that of deciding a priori on the proper size of the network to be used.
Reference: [Yao85] <author> A. Yao. </author> <title> Separating the polynomial-time hierarchy by oracles. </title> <booktitle> In Proc. 22nd IEEE Symp. Foundations of Comp. Sci., </booktitle> <pages> pages 1-10, </pages> <year> 1985. </year>
Reference-contexts: the carry-look-ahead method, [Sav76], the summation can be computed via a subcircuit of depth O (log (T N )), width O (T 2 N ), and size O (T 2 N ). (This depth is of the same order as the lower bound when polynomial size is imposed, see [Has87], <ref> [Yao85] </ref>.) As for the saturation, one gate, p u , is sufficient for the integer part.
Reference: [Yas71] <author> A. Yasuhara. </author> <title> Recursive Function Theory and Logic. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: We show that a large class of different networks and dynamical systems has no more computational power than our neural (first-order) model with real weights. Analogously to Church's thesis of computability (see e.g. <ref> [Yas71] </ref> p.98), our results suggest the following Thesis of Time-bounded Analog Computing: "Any reasonable analog computer will have no more power (up to polynomial time) than first-order recurrent networks." In previous chapters, we studied the computational power of the neural network model (see Equation 1.2) presented in Chapter 1.
Reference: [ZZZ92] <author> B. Zhang, L. Zhang, and H. Zhang. </author> <title> A quantitative analysis of the behavior of the pln network. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 639-661, </pages> <year> 1992. </year> <month> 165 </month>
Reference-contexts: Unless otherwise stated, the function is the simplest possible "sigmoid," namely the saturated-linear function: (x) := &gt; &gt; &gt; &gt; &gt; &lt; 0 if x &lt; 0 1 if x &gt; 1 : 7 This function has appeared in many applications of neural nets (e.g. <ref> [Bat91, BGV88, Lip87, ZZZ92] </ref>). We use it because theorems are easier to prove when using this particular activation.
References-found: 92

