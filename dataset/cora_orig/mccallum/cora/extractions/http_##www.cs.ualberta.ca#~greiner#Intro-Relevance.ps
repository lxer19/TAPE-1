URL: http://www.cs.ualberta.ca/~greiner/Intro-Relevance.ps
Refering-URL: http://www.cs.ualberta.ca/~greiner/Intro-Relevance.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: The Relevance of Relevance  
Author: Devika Subramanian Russell Greiner Judea Pearl 
Address: Houston, TX 77005  755 College Road East Princeton, NJ 08540-6632 USA  Los Angeles, CA  
Affiliation: Department of Computer Science Rice University  Siemens Corporate Research, Inc  Department of Computer Science University of California  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Russell Greiner and Devika Subramanian. </author> <title> Proceedings of the "Relevance" symposium. </title> <type> Technical Report FS-94-02, </type> <institution> AAAI, </institution> <address> Menlo Park, </address> <year> 1995. </year>
Reference-contexts: Relevance reasoning is crucial for agents like us with limited memory and computational resources; the benefit obtained from being able to derive appropriate actions within given resource limits far outweighs the expense of such reasoning. This special issue, motivated by the very successful AAAI Fall Symposium on this topic <ref> [1] </ref>, addresses questions in automating relevance reasoning, focusing on autonomous systems with limited computational and sensory resources, embedded in dynamic environments.
Reference: [2] <author> Nick Littlestone. </author> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning Journal, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: Finally, Littlestone earlier presented a very clever algorithm for learning linear-seperators, called Winnow, which had the intriguing property that it scales linearly with the number of relevant attributes but only logarithmically with the total number of attributes <ref> [2] </ref>; by contrast, the typical Perceptron algorithm scales linearly here [3]. As most of these results were only upper bounds, it was not clear whether there really was a difference here.
Reference: [3] <author> Marvin Minsky and Seymour Papert. </author> <title> Perceptron: An Introduction to Computational Geometry. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, </address> <note> expanded edition, </note> <year> 1988. </year>
Reference-contexts: Finally, Littlestone earlier presented a very clever algorithm for learning linear-seperators, called Winnow, which had the intriguing property that it scales linearly with the number of relevant attributes but only logarithmically with the total number of attributes [2]; by contrast, the typical Perceptron algorithm scales linearly here <ref> [3] </ref>. As most of these results were only upper bounds, it was not clear whether there really was a difference here. The [Kivenen+Warmuth+Auer] paper answers this, by showing that an adversary can force the Perceptron algorithm to make exponentially more mistakes (in the context of on-line learning) than Winnow.
Reference: [4] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1988. </year>
Reference-contexts: This work motivates the need for approximations as considered by [Khardon+Roth] who demonstrate that reasoning can be efficient when restricted to relevant models (e.g., vivid knowledge bases of Levesque) of formulas under consideration. [Darwiche] recasts Pearl's definition of conditional independence (irrelevance) in probabilistic reasoning <ref> [4] </ref> in the logical context and provides several efficient graph-theoretic algorithms for decomposing complex logical calculations into simpler computations.
Reference: [5] <author> Devika Subramanian. </author> <title> Shift of vocabulary bias in speedup learning. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 155-191, </pages> <year> 1995. </year>
Reference-contexts: such an account is either unlikely to be comprehensive or not be very useful. [Kohavi+John] empirically demonstrate that the determination of attribute relevance in an induction task cannot be made independent of the induction algorithm (which therefore necessitates a symbol level analysis). [Levy+Fikes+Sagiv], building on earlier work by Subramanian in <ref> [5] </ref>, provide a proof-theoretic account of irrelevance in the context of efficient deductive reasoning with large Horn theories, in sharp contrast to [Lakemeyer]'s purely semantic account.
Reference: [6] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: They show that this relevance information can be extremely useful: e.g., while there is no known algorithm that can "PAC-learn" arbitrary decision trees in the standard model (where the learner sees the complete set of attribute values for each instance) <ref> [6] </ref>, this learning task become trivial if the learner is told exactly which features are relevant for each instance.
References-found: 6

