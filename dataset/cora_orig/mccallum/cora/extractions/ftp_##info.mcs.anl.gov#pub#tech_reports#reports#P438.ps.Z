URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P438.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts94.htm
Root-URL: http://www.mcs.anl.gov
Title: Parallel Spectral Transform Shallow Water Model: A Runtime-Tunable Parallel Benchmark Code  
Author: P. H. Worley I. T. Foster 
Address: P.O. Box 2008 9700 South Cass Avenue Oak Ridge, TN 37831-6367 Argonne, IL 60439  
Affiliation: Oak Ridge National Laboratory Argonne National Laboratory  
Abstract: Fairness is an important issue when benchmarking parallel computers using application codes. The best parallel algorithm on one platform may not be the best on another. While it is not feasible to reevaluate parallel algorithms and reimplement large codes whenever new machines become available, it is possible to embed algorithmic options into codes that allow them to be "tuned" for a particular machine without requiring code modifications. In this paper, we describe a code in which such an approach was taken. PSTSWM was developed for evaluating parallel algorithms for the spectral transform method in atmospheric circulation models. Many levels of runtime-selectable algorithmic options are supported. We discuss these options and our evaluation methodology. We also provide empirical results from a number of parallel machines, indicating the importance of tuning for each platform before making a comparison. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. H. Bailey et al., </author> <title> The NAS Parallel Benchmarks, </title> <journal> Internat. J. Supercomputer Applications, </journal> <volume> 5 (1991), </volume> <pages> pp. 63-73. </pages>
Reference-contexts: One solution to this problem is the "paper and pencil" benchmark, as exemplified by the NAS benchmark suite <ref> [1] </ref>. For such a benchmark, the numerical algorithm is specified, but the parallel implementation is left to the researcher or the vendor. But it can be very time-consuming to develop parallel application codes from scratch, and paper benchmarks are most useful for algorithmic kernels.
Reference: [2] <author> J. J. Dongarra and W. Gentzsch, eds., </author> <title> Computer Benchmarks, </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Benchmarking parallel (and sequential) computers is a varied activity. It includes determining low level machine and system characteristics, measuring the performance of important or representative kernels, and measuring the performance of full or compact application codes <ref> [2] </ref>. It is also an activity beset with many difficulties, for example, when trying to fairly compare/contrast the performance of different computer systems. There are special difficulties when using full or compact application code benchmarks in interplatform comparisons.
Reference: [3] <author> I. T. Foster, </author> <title> Language constructs for modular parallel programs, </title> <institution> MCS-P391-1093, Argonne National Laboratory, Argonne, IL, </institution> <year> 1993. </year>
Reference-contexts: A similar effort to ours to introduce runtime algorithm options into a parallel code may not always be reasonable, but some options are simple to insert and can make a big difference on some machines. It is generally a good idea to encapsulate message passing primitives in parallel codes <ref> [3] </ref>, and once that is accomplished, a variety of communication protocols and algorithm variants can be embedded easily. In summary, it is our opinion that the low-level tuning described in this paper is a generally applicable technique that should be used before commencing any benchmarking effort on message-passing systems.
Reference: [4] <author> I. T. Foster and P. H. Worley, </author> <title> Parallel algorithms for the spectral transform method, </title> <institution> ORNL/TM-12507, Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: It has the same characterization of communication costs and communication pattern as the fi (log Q) transpose algorithm. All parallel algorithms execute essentially the same computations and, modulo load imbalances, differ only in communication costs. Load balance issues are discussed in detail in <ref> [4] </ref>. Note that the simple characterizations described here ignore link contention in the physical network, and, for example, the fi (log Q) distributed Legendre transform algorithm is not automatically better than the fi (Q) distributed algorithm. <p> This information can then be saved and used whenever a given number of processors becomes available for a run. We defer the complete description of the algorithm comparison to <ref> [4] </ref>, and discuss only the high-level tuning for the largest configurations on each of the target machines. The best parallel algorithm combinations and logical aspect ratios are listed below. <p> As before, all of the parallel algorithms, if not the aspect ratios, are expected to be reasonable choices in some environment, and cannot be dismissed out of hand. Even nonsquare aspect ratios are optimal for smaller numbers of processors. See <ref> [4] </ref> for more details. 6 Summary and Conclusions We found runtime tuning options to be a useful technique for determining efficient programming tech niques (low-level tuning) and for determining appro-priate parallel algorithms (high-level tuning).
Reference: [5] <author> G. A. Geist, M. T. Heath, B. W. Peyton, and P. H. Worley, PICL: </author> <title> a portable instrumented communication library, </title> <institution> ORNL/TM-11130, Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: Structuring PSTSWM so as to support multiple parallel algorithms efficiently was not a simple task, but maintaining and porting the code to other message-passing systems will be straightforward. PSTSWM is primarily written in standard Fortran 77, with message passing implemented using the PICL message passing libraries <ref> [5] </ref>. All message passing is encapsulated in three high-level routines for broadcast, global minimum, and global maximum; two classes of low level routines representing variants and/or stages of the swap operation and the send/receive operation, respectively; and one synchronization primitive.
Reference: [6] <author> J. J. Hack et al., </author> <title> Description of the NCAR Community Climate Model (CCM2), </title> <institution> NCAR/TN-382+STR, National Center for Atmospheric Research, Boulder, CO, </institution> <year> 1992. </year>
Reference-contexts: The spectral transform algorithm of STSWM follows closely how CCM2, the NCAR Community Climate Model, handles the dynamical part of the primitive equations <ref> [6] </ref>. PSTSWM differs from STSWM in one major respect: fictitious vertical levels have been added in order to get the correct communication and computation granularity for three-dimensional weather and climate codes and to allow parallel algorithms that decompose the vertical dimension to be evaluated. <p> T42 is the design point for CCM2 <ref> [6] </ref>, and T85 is representative of the size of problem that climate researchers would like to solve next. The tuning experiments reported here were designed assuming a logical 16fi 32 processor for the nCUBE/2 and Intel Paragon, and assuming a logical 8 fi 16 processor mesh for the Intel iPSC/860.
Reference: [7] <author> J. J. Hack and R. Jakob, </author> <title> Description of a global shallow water model based on the spectral transform method, </title> <institution> NCAR/TN-343+STR, National Center for Atmospheric Research, Boulder, CO, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: We feel that encapsulating multiple parallel algorithm options into a code is a very powerful approach to benchmarking. 2 PSTSWM PSTSWM is a message-passing parallel implementation of the sequential Fortran 77 code STSWM 2.0 <ref> [7] </ref>. STSWM solves the nonlinear shallow wa ter equations on a rotating sphere using the spectral transform method. <p> For more details on the steps in solving the shallow water equations using the spectral transform algorithm, see <ref> [7] </ref>. 3 Parallel Algorithm Options Parallel algorithms are used to compute the FFTs and to compute the vector sums used to approximate the forward and inverse Legendre transforms.
Reference: [8] <author> S. L. Johnsson and C.-T. Ho, </author> <title> Algorithms for matrix transposition on Boolean N-cube configured ensemble architectures, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 9 (1988), </volume> <pages> pp. 419-454. </pages>
Reference-contexts: This algorithm proceeds in Q1 steps on Q processors, where, at each step, a processor sends 1=Q of its data to another processor <ref> [8] </ref>. To be efficient, some care must be taken with the order of the data communication. We support two different schedules in PSTSWM. In the XOR schedule, processor q swaps data with processor XOR (q; i) at step i. This avoids competition for bandwidth in hypercube interconnection networks [8] and is <p> another processor <ref> [8] </ref>. To be efficient, some care must be taken with the order of the data communication. We support two different schedules in PSTSWM. In the XOR schedule, processor q swaps data with processor XOR (q; i) at step i. This avoids competition for bandwidth in hypercube interconnection networks [8] and is often more efficient than a general send/receive pattern on other networks. In the MOD schedule, processor q sends to processor MOD (q + i; Q) at step i.
Reference: [9] <author> D. W. Walker, </author> <title> The design of a standard message passing interface for distributed memory concurrent computers, Parallel Computing, </title> <note> (1994) (to appear). </note>
Reference-contexts: Porting the code to another message-passing system will require simply porting the PICL library or reimplementing the few communication routines in PSTSWM using either native message-passing primitives or the proposed MPI message-passing standard <ref> [9] </ref>, which supports all of the communication protocols currently implemented in PSTSWM. A similar effort to ours to introduce runtime algorithm options into a parallel code may not always be reasonable, but some options are simple to insert and can make a big difference on some machines.
Reference: [10] <author> D. W. Walker, P. H. Worley, and J. B. Drake, </author> <title> Par-allelizing the spectral transform method. Part II, </title> <journal> Con-currency: Practice and Experience, </journal> <volume> 4 (1992), </volume> <pages> pp. 509-531. </pages>
Reference-contexts: But, if extra buffer space is allocated, the receive requests can be posted early, hopefully eliminating some buffer copying by guaranteeing that messages are placed in user buffer space when they arrive. Both send/receive (0) and receive-ahead/send (1) implementations are supported. Distributed FFT. Walker et al. <ref> [10] </ref> showed that on computers that permit overlapping of computation and communication, it can be advantageous to divide the single block FFT into two and interleave the communication for one block with the computation for the other.
References-found: 10

