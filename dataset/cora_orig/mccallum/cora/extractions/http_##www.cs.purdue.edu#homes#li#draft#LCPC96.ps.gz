URL: http://www.cs.purdue.edu/homes/li/draft/LCPC96.ps.gz
Refering-URL: http://www.cs.purdue.edu/homes/li/publications.html
Root-URL: http://www.cs.purdue.edu
Title: Compiler Techniques for Concurrent Multithreading with Hardware Speculation Support  
Author: Zhiyuan Li, Jenn-Yuan Tsai Xin Wang, Pen-Chung Yew, and Bess Zheng 
Address: Minneapolis, MN 55455 Urbana, IL 61801  
Affiliation: Department of Computer Science Department of Computer Science University of Minnesota University of Illinois  
Abstract: Recently proposed concurrent multithreading architectures employ sophisticated hardware to support speculation on control and data dependences as well as run-time data dependence check, which enables parallelization of program regions such as while-loops which previously were ignored. The new architectures demand compilers to put more emphasis on the formation and selection of parallel threads. Compilers also play an important role in reducing the cost of run-time data dependence check. This paper discusses these new issues. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Randy Allen and Steve Johnson. </author> <title> Compiler C for vectorization, parallelization, and inline expansion. </title> <booktitle> In Prof. of SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 241-249, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Pointer arithmetic is used to index through array sections. If not transformed, such references would render the traditional algorithms useless. By rewriting pointered array references in a subscripted form, a process called subscriptization, the compiler can apply known algebraic algorithms for the data dependence test <ref> [1] </ref>. The previous techniques for cleaning up array subscripts do not deal with array references which are not already in subscripted forms [5]. The code segment in Figure 3.1 (a) is from eqntott in SPEC92 benchmarks. For clarity of exposition, we use source programs as examples whenever appropriate.
Reference: 2. <author> Pradeep K. Dubey, Kevin O'Brien, Kathryn O'Brien, and Charles Barton. </author> <title> Single--program speculative multithreading (SPSM) architecture: Compiler-assisted fine-grained multithreading. </title> <booktitle> In Proceedings of the IFIP WG 10.3 Working Conference on Parallel Architectures and Compilation Techniques, PACT '95, </booktitle> <pages> pages 109-121, </pages> <month> June 27-29, </month> <year> 1995. </year>
Reference-contexts: For example, a compiler normally does not know how many iterations of a while-loop will be executed at run time and hence it cannot safely generate multiple threads to execute different loop iterations simultaneously. Recently, several hardware mechanisms have been proposed to allow speculative execution of multiple threads <ref> [3, 11, 2, 12] </ref>. A thread, whose execution may depend on run-time conditions, is allowed to execute before those conditions are resolved. Once those conditions are resolved, a correctly speculated thread can then write its results to the memory. On the other hand, an incorrectly speculated thread is squashed.
Reference: 3. <author> Manoj Franklin and Gurindar S. Sohi. </author> <title> The expandable split window paradigm for exploiting fine-grained parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 58-67, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: For example, a compiler normally does not know how many iterations of a while-loop will be executed at run time and hence it cannot safely generate multiple threads to execute different loop iterations simultaneously. Recently, several hardware mechanisms have been proposed to allow speculative execution of multiple threads <ref> [3, 11, 2, 12] </ref>. A thread, whose execution may depend on run-time conditions, is allowed to execute before those conditions are resolved. Once those conditions are resolved, a correctly speculated thread can then write its results to the memory. On the other hand, an incorrectly speculated thread is squashed. <p> In Section 5, we explore the issue of parallel threads selection. In Section 6, we describe our current experimentation effort. We summarize our discussion in Section 7. 2 Speculative Concurrent Multithreaded Architectures 2.1 Multiscalar architecture The multiscalar paradigm <ref> [3, 11] </ref> exploits thread-level parallelism with aggressive hardware support for both control and data speculation. The compiler for the multiscalar processor must partition the control flow graph of a program into threads, each to be executed by a processing unit at run-time.
Reference: 4. <author> M. Girkar and C. Polychronopoulos. </author> <title> The HTG: An intermediate representation for programs based on control and data dependences. </title> <type> CSRD Technical Report No. 1046, </type> <institution> Univ. of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference-contexts: Currently we are pursuing the following studies: Estimating the working set size of a thread to allow the working set to fit in the memory buffer or the ARB; Estimating the amount of parallelism based on the data dependence graph; Constructing a hierarchical task graph <ref> [4] </ref> which reflects speculation possi bilities; Affirming the existence of data dependences in addition to affirming data independences, which is important for parallelism estimate and for avoiding excessive incorrect data speculation on the multiscalar.
Reference: 5. <author> Justiani and L. J. Hendren. </author> <title> Supporting array dependence testing for an optimizing/parallelizing c compiler. </title> <booktitle> In Proc. of the 1994 International Conference on Compiler Construction. Volume 749 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <month> April </month> <year> 1994. </year>
Reference-contexts: By rewriting pointered array references in a subscripted form, a process called subscriptization, the compiler can apply known algebraic algorithms for the data dependence test [1]. The previous techniques for cleaning up array subscripts do not deal with array references which are not already in subscripted forms <ref> [5] </ref>. The code segment in Figure 3.1 (a) is from eqntott in SPEC92 benchmarks. For clarity of exposition, we use source programs as examples whenever appropriate. In this example, i and j are pointers to two array sections.
Reference: 6. <author> Z. Li. </author> <title> Compiler algorithms for event variable synchronization. </title> <booktitle> In Proc. of the Fifth International Conference on Supercomputing (ACM), </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: Also, if the variable causing the dependence can potentially be updated several times before it is read by the dependence sink, then the register should not be forwarded before the last update is done. The problem of identifying last writes for the purpose of synchronization has previously been discussed <ref> [8, 6] </ref>. Analysis of last writes has also been proposed to support array privatization and other optimizations [7] When it is not clear which writes are last writes, the compiler needs to find a program point that post-dominates all potential last writes in order to safely forward the data.
Reference: 7. <author> D.E. Maydan, S.P. Amarasinghe, and M.S. Lam. </author> <title> Array data-flow analysis and its use in array privatization. </title> <booktitle> In Proc. of the 20th ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 2-15, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: The problem of identifying last writes for the purpose of synchronization has previously been discussed [8, 6]. Analysis of last writes has also been proposed to support array privatization and other optimizations <ref> [7] </ref> When it is not clear which writes are last writes, the compiler needs to find a program point that post-dominates all potential last writes in order to safely forward the data.
Reference: 8. <author> S. P. Midkiff and D. A. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1485-1495, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: With the help of the thread descriptors, the hardware of the multiscalar processor can rapidly traverse the control flow graph of a program and assign threads to processing units on the fly. Fig. 1. The microarchitecture of the multiscalar processor <ref> [8] </ref> processor consists of multiple processing units, which are connected to each other with a unidirectional ring. Each processing unit has its own register file and functional units. A processing unit can pass register data to its down-stream processing units via the unidirectional ring connection. <p> Also, if the variable causing the dependence can potentially be updated several times before it is read by the dependence sink, then the register should not be forwarded before the last update is done. The problem of identifying last writes for the purpose of synchronization has previously been discussed <ref> [8, 6] </ref>. Analysis of last writes has also been proposed to support array privatization and other optimizations [7] When it is not clear which writes are last writes, the compiler needs to find a program point that post-dominates all potential last writes in order to safely forward the data.
Reference: 9. <author> Andreas I. Moshovos, Scott E. Breach, T. N. Vijaykumar, and Guri. S. Sohi. </author> <note> Submitted for a blind review to a conference. </note>
Reference-contexts: There are no synchronization instructions to force a thread to wait. Instead, the ARB hardware can speculate, based on memory reference history, that a flow dependence may occur at a particular memory address <ref> [9] </ref>. The hardware then forces the thread which may be the sink of the dependence to wait for a flag associated with that memory address. As soon as a preceding thread stores data to that address, the flag is raised, which permits the waiting thread to proceed.
Reference: 10. <author> B. A. Nayfeh and K. Olukotun. </author> <title> Exploring the design space for a shared-cache multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 166-175, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Growing evidences suggest that future processors need to take advantage of multiple threads of execution in order to find sufficient parallel operations [11]. Allowing multiple threads of execution is similar to, but not exactly like, placing multiprocessors on a single chip or on a multichip-module (MCM) <ref> [10] </ref>. ? This work was supported in part by NSF CAREER Award CCR-9502541, NSF Grant MIP 9496320, a gift from Intel Corporation, and by the U.S. Army Intelligence Center and Fort Huachuca under Contract DABT63-95-C-0127 and ARPA order no. D 346.
Reference: 11. <author> Gurindar S. Sohi, Scott E. Breach, and T. N. Vijaykumar. </author> <title> Multiscalar processors. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 414-425, </pages> <month> June 22-24, </month> <year> 1995. </year>
Reference-contexts: Although independent instructions can be statically reordered and packed into a window, such code motion is constrained by programs' control structures. Growing evidences suggest that future processors need to take advantage of multiple threads of execution in order to find sufficient parallel operations <ref> [11] </ref>. Allowing multiple threads of execution is similar to, but not exactly like, placing multiprocessors on a single chip or on a multichip-module (MCM) [10]. ? This work was supported in part by NSF CAREER Award CCR-9502541, NSF Grant MIP 9496320, a gift from Intel Corporation, and by the U.S. <p> For example, a compiler normally does not know how many iterations of a while-loop will be executed at run time and hence it cannot safely generate multiple threads to execute different loop iterations simultaneously. Recently, several hardware mechanisms have been proposed to allow speculative execution of multiple threads <ref> [3, 11, 2, 12] </ref>. A thread, whose execution may depend on run-time conditions, is allowed to execute before those conditions are resolved. Once those conditions are resolved, a correctly speculated thread can then write its results to the memory. On the other hand, an incorrectly speculated thread is squashed. <p> Once those conditions are resolved, a correctly speculated thread can then write its results to the memory. On the other hand, an incorrectly speculated thread is squashed. Such concurrent multithreading architectures may also provide hardware for run-time data dependence check <ref> [11, 12] </ref>. These new hardware features create new parallelization opportunities to the compiler, which may result in a myriad of potential parallel threads. The selection and scheduling of parallel threads is expected to have a great impact on the program's performance. <p> In this paper, we discuss several issues regarding compiler optimizations for concurrent multithreading architectures with hardware support for speculative execution and for run-time data dependence check. We use two particular designs, namely multiscalar <ref> [11] </ref> and superthreaded processors [12], as examples to show the implications of such hardware on compiler techniques. In the next section, we describe these two microarchitectures and their execution models. <p> In Section 5, we explore the issue of parallel threads selection. In Section 6, we describe our current experimentation effort. We summarize our discussion in Section 7. 2 Speculative Concurrent Multithreaded Architectures 2.1 Multiscalar architecture The multiscalar paradigm <ref> [3, 11] </ref> exploits thread-level parallelism with aggressive hardware support for both control and data speculation. The compiler for the multiscalar processor must partition the control flow graph of a program into threads, each to be executed by a processing unit at run-time.
Reference: 12. <author> Jenn-Yuan Tsai and Pen-Chung Yew. </author> <title> The superthreaded architecture: Thread pipelining with run-time data dependence checking and control speculation. </title> <booktitle> In Proceedings of International Conference on Parallel Architectures and Compilation Techniques, PACT '96, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: For example, a compiler normally does not know how many iterations of a while-loop will be executed at run time and hence it cannot safely generate multiple threads to execute different loop iterations simultaneously. Recently, several hardware mechanisms have been proposed to allow speculative execution of multiple threads <ref> [3, 11, 2, 12] </ref>. A thread, whose execution may depend on run-time conditions, is allowed to execute before those conditions are resolved. Once those conditions are resolved, a correctly speculated thread can then write its results to the memory. On the other hand, an incorrectly speculated thread is squashed. <p> Once those conditions are resolved, a correctly speculated thread can then write its results to the memory. On the other hand, an incorrectly speculated thread is squashed. Such concurrent multithreading architectures may also provide hardware for run-time data dependence check <ref> [11, 12] </ref>. These new hardware features create new parallelization opportunities to the compiler, which may result in a myriad of potential parallel threads. The selection and scheduling of parallel threads is expected to have a great impact on the program's performance. <p> In this paper, we discuss several issues regarding compiler optimizations for concurrent multithreading architectures with hardware support for speculative execution and for run-time data dependence check. We use two particular designs, namely multiscalar [11] and superthreaded processors <ref> [12] </ref>, as examples to show the implications of such hardware on compiler techniques. In the next section, we describe these two microarchitectures and their execution models. <p> The ARB also keeps track of all load and store operations performed by each active thread. A data dependence violation is detected if a thread writes to a memory location whose corresponding ARB entry records an earlier load operation by a successor thread. 2.2 Superthreaded architecture The superthreaded processor <ref> [12] </ref> is similar to the multiscalar processor, but it does not speculate on data dependences. Instead, the superthreaded processor performs run-time data dependence checking for load operations.
References-found: 12

