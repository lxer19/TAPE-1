URL: http://www.cs.unc.edu/~mcmillan/plenoptic.ps
Refering-URL: http://www.cs.unc.edu/Research/graphics/pubs.html
Root-URL: http://www.cs.unc.edu
Keyword: CR Descriptors: I.3.3 [Computer Graphics]: Picture/Image Generation display algorithms, viewing algorithms; I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism hidden line/ surface removal; I.4.3 [Image Processing]: Enhancement registration; I.4.7 [Image Processing]: Feature Measurement projections; I.4.8 [Image Processing]: Scene Analysis.  
Date: August 6-11, 1995)  
Address: (Los Angeles, California,  
Note: Proceedings of SIGGRAPH 95  
Abstract: Image-based rendering is a powerful new approach for generating real-time photorealistic computer graphics. It can provide convincing animations without an explicit geometric representation. We use the plenoptic function of Adelson and Bergen to provide a concise problem statement for image-based rendering paradigms, such as morphing and view interpolation. The plenoptic function is a parameterized function for describing everything that is visible from a given point in space. We present an image-based rendering system based on sampling, reconstructing, and resampling the plenoptic function. In addition, we introduce a novel visible surface algorithm and a geometric invariant for cylindrical projections that is equivalent to the epipolar constraint defined for planar projections. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adelson, E. H., and J. R. Bergen, </author> <title> The Plenoptic Function and the Elements of Early Vision, Computational Models of Visual Processing, Chapter 1, </title> <editor> Edited by Michael Landy and J. Anthony Movs-hon. </editor> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass. </address> <year> 1991. </year>
Reference-contexts: We then evaluate previous image-based rendering methods within this new framework. Finally, we present our own image-based rendering methodology and results from our prototype implementation. 2. THE PLENOPTIC FUNCTION Adelson and Bergen <ref> [1] </ref> assigned the name plenoptic function (from the latin root plenus, meaning complete or full, and optic pertaining to vision) to the pencil of rays visible from any point in space, at any time, and over any range of wavelengths.
Reference: [2] <author> Anderson, D., </author> <title> Hidden Line Elimination in Projected Grid Surfaces, </title> <journal> ACM Transactions on Graphics, </journal> <month> October </month> <year> 1982. </year>
Reference-contexts: Next, we enumerate each sheet such that the projected image of the desired viewpoint is the last point drawn. This simple partitioning and enumeration provides a back-to-front ordering for use by a painters style rendering algorithm. This hidden-surface algorithm is a generalization of Andersons <ref> [2] </ref> visible line algorithm to arbitrary projected grid surfaces. Additional details can be found in [21]. At this point, the plenoptic samples can be warped to their new position according to the image ow field.
Reference: [3] <author> Barnard, S.T. </author> <title> A Stochastic Approach to Stereo Vision, SRI International, </title> <type> Technical Note 373, </type> <month> April 4, </month> <year> 1986. </year>
Reference-contexts: This cylindrical epipolar relationship can be used to establish image ow fields using standard computer vision methods. We have used correlation methods [9], a simulated annealing-like relaxation method <ref> [3] </ref>, and the method of differences [20] to compute stereo disparities between cylinder pairs. Each method has its strengths and weaknesses.
Reference: [4] <author> Beier, T. and S. Neely, </author> <title> Feature-Based Image Metamorphosis, </title> <journal> Computer Graphics (SIGGRAPH92 Proceedings), </journal> <volume> Vol. 26, No. 2, </volume> <pages> pp. 35-42, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The Movie-Map form of image-based rendering can also be interpreted as a table-based evaluation of the plenoptic function. This interpretation reects the database structure common to most image-based systems. 3.2 Image Morphing Image morphing is a very popular image-based rendering technique <ref> [4] </ref>, [28]. Generally, morphing is considered to occur between two images. We can think of these images as endpoints along some path through time and/or space. In this interpretation, morphing becomes a method for reconstructing partial samples of the continuous ple-noptic function along this path.
Reference: [5] <author> Blinn, J. F. and M. E. Newell, </author> <title> Texture and Reection in Computer Generated Images, </title> <journal> Communications of the ACM, </journal> <volume> vol. 19, no. 10, </volume> <pages> pp. 542-547, </pages> <month> October </month> <year> 1976. </year>
Reference-contexts: In computer graphics, the progression toward image-based rendering systems was initially motivated by the desire to increase the visual realism of the approximate geometric descriptions by mapping images onto their surface (texture mapping) [7], [12]. Next, images were used to approximate global illumination effects (environment mapping) <ref> [5] </ref>, and, most recently, we have seen systems where the images themselves constitute the significant aspects of the scenes description [8]. Another reason for considering image-based rendering systems in computer graphics is that acquisition of realistic surface models is a difficult problem.
Reference: [6] <author> Bolles, R. C., H. H. Baker, and D. H. Marimont, </author> <title> Epipolar-Plane Image Analysis: An Approach to Determining Structure from Motion, </title> <journal> International Journal of Computer Vision, </journal> <volume> Vol. 1, </volume> <year> 1987. </year>
Reference: [7] <author> Catmull, E., </author> <title> A Subdivision Algorithm for Computer Display of Curved Surfaces (Ph. D. Thesis), Department of Computer Sci-FIGURE 5. Cylindrical images a and b are panoramic views separated by approximately 60 inches. Image c is a panoramic view of an operating room. In image d, several epipolar curves are superimposed onto cylindrical image a. </title> <booktitle> Proceedings of SIGGRAPH 95 (Los Angeles, </booktitle> <address> California, </address> <month> August 6-11, </month> <note> 1995) 8 ence, </note> <institution> University of Utah, Tech. Report UTEC-CSc-74-133, </institution> <month> December </month> <year> 1974. </year>
Reference-contexts: In computer graphics, the progression toward image-based rendering systems was initially motivated by the desire to increase the visual realism of the approximate geometric descriptions by mapping images onto their surface (texture mapping) <ref> [7] </ref>, [12]. Next, images were used to approximate global illumination effects (environment mapping) [5], and, most recently, we have seen systems where the images themselves constitute the significant aspects of the scenes description [8].
Reference: [8] <author> Chen, S. E. and L. Williams. </author> <title> View Interpolation for Image Synthesis, </title> <booktitle> Computer Graphics (SIGGRAPH93 Proceedings), </booktitle> <pages> pp. 279-288, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Next, images were used to approximate global illumination effects (environment mapping) [5], and, most recently, we have seen systems where the images themselves constitute the significant aspects of the scenes description <ref> [8] </ref>. Another reason for considering image-based rendering systems in computer graphics is that acquisition of realistic surface models is a difficult problem. While geometry-based rendering technology has made significant strides towards achieving photorealism, creating accurate models is still nearly as difficult as it was ten years ago. <p> This blending function is usually some linear combination of the two images based on what percentage of the paths length has been traversed. Thus, morphing is a plenoptic reconstruction method which interpolates between samples and uses local derivative information to construct approximations. 3.3 View Interpolation Chens and Williams <ref> [8] </ref> view interpolation employs incomplete plenoptic samples and image ow fields to reconstruct arbitrary viewpoints with some constraints on gaze angle. The reconstruction process uses information about the local neighborhood of a sample. <p> This involves computing the projected kernels size based on the current disparity value and the derivatives along the epipolar curves. While the visibility method properly handles mesh folds, we still must consider the tears (or excessive stretching) produced by the exposure of previously occluded image regions. In view interpolation <ref> [8] </ref> a simple distinguished color heuristic is used based on the screen space projection of the neighboring pixel on the same scan-line. This approach approximates stretching for small regions of occlusion, where the occluder still abuts the occluded region.
Reference: [9] <author> Faugeras, O., </author> <title> Three-dimensional Computer Vision: A Geometric Viewpoint, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1993. </year>
Reference-contexts: Next, we present a geometric constraint for cylindrical projections that determines the possible positions of a point given its position in some other cylinder. This constraint plays the same role that the epipolar geometries [18], <ref> [9] </ref>, used in the computer vision community for depth-from-stereo computations, play for planar projections. First, we will present an intuitive argument for the existence of such an invariant. Consider yourself at the center of a cylindrical projection. <p> This cylindrical epipolar relationship can be used to establish image ow fields using standard computer vision methods. We have used correlation methods <ref> [9] </ref>, a simulated annealing-like relaxation method [3], and the method of differences [20] to compute stereo disparities between cylinder pairs. Each method has its strengths and weaknesses.
Reference: [10] <author> Greene, N., </author> <title> Environment Mapping and Other Applications of World Projections, </title> <journal> IEEE Computer Graphics and Applications, </journal> <month> November </month> <year> 1986. </year>
Reference-contexts: Furthermore, those which do map to a plane with consistent neighborhood relationships are generally quite distorted and, therefore, non-uniform. A set of six planar projections in the form of a cube has been suggested by Greene <ref> [10] </ref> as an efficient representation for environment maps. While this representation can be easily stored and accessed by a computer, it provides significant problems relating to acquisition, alignment, and registration when used with real, non-computer-generated images. The orthogonal orientation of the cube faces requires precise camera positioning.
Reference: [11] <author> Hartley, R.I., </author> <title> Self-Calibration from Multiple Views with a Rotating Camera, </title> <booktitle> Proceedings of the European Conference on Computer Vision, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Other techniques for deriving these homogeneous transformations without specific point correspondences have also been described [22], [25]. The set of homogenous transforms, H i , can be decomposed into two parts which will allow for arbitrary reprojections in a manner similar to <ref> [11] </ref>. These two parts include an intrinsic transform, S, which is determined entirely by camera properties, and an extrinsic transform, R i , which is determined by the rotation around the cameras center of projection: (3) This decomposition decouples the projection and rotational components of the homogeneous transform.
Reference: [12] <author> Heckbert, P. S., </author> <title> Fundamentals of Texture Mapping and Image Warping, </title> <type> Masters Thesis, </type> <institution> Dept. of EECS, </institution> <note> UCB, Technical Report No. UCB/CSD 89/516, </note> <month> June </month> <year> 1989. </year>
Reference-contexts: In computer graphics, the progression toward image-based rendering systems was initially motivated by the desire to increase the visual realism of the approximate geometric descriptions by mapping images onto their surface (texture mapping) [7], <ref> [12] </ref>. Next, images were used to approximate global illumination effects (environment mapping) [5], and, most recently, we have seen systems where the images themselves constitute the significant aspects of the scenes description [8]. <p> This well known result has been reported by several authors <ref> [12] </ref>, [28], [22]. The images resulting from typical camera motions, such as pan, tilt, roll, and zoom, can all be related in this fashion. When creating a cylindrical projection, we will only need to consider panning camera motions. <p> In order to reproject an individual image into a cylindrical projection, we must first determine a model for the cameras projection or, equivalently, the appropriate homogenous transforms. Many different techniques have been developed for inferring the homogenous transformation between images sharing common centers of projection. The most common technique <ref> [12] </ref> involves establishing four corresponding points across each image pair. The resulting transforms provide a mapping of pixels from the planar projection of the first image to the planar projection of the second.
Reference: [13] <author> Horn, B., and B.G. Schunck, </author> <title> Determining Optical Flow, </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 17, </volume> <year> 1981. </year>
Reference-contexts: However, several authors in the field of computer vision have shown that this type of image ow information is equivalent to changes in the local intensity due to infinitesimal perturbations of the plenoptic functions independent variables [20], <ref> [13] </ref>. This local derivative behavior can be related to the intensity gradient via applications of the chain rule. In fact, morphing makes an even stronger assumption that the ow information is constant along the entire path, thus amounting to a locally linear approximation.
Reference: [14] <author> Kanatani, K., </author> <title> Transformation of Optical Flow by Camera Rotation, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 10, No. 2, </volume> <month> March </month> <year> 1988. </year>
Reference-contexts: This is accomplished by using a linear approximation to an infinitesimal rotation by the angle . This linear approximation results from substituting for the cosine terms and for the sine terms of the rotation matrix. This infinitesimal perturbation has been shown by <ref> [14] </ref> to reduce to the following approximate equations: (5) where f is the apparent focal length of the camera measured in pixels, and (C x , C y ) is the pixel coordinate of the intersection of the optical axis with the image plane. (C x , C y ) is
Reference: [15] <author> Laveau, S. and O. Faugeras, </author> <title> 3-D Scene Representation as a Collection of Images and Fundamental Matrices, </title> <type> INRIA, Technical Report No. 2205, </type> <month> February, </month> <year> 1994. </year>
Reference-contexts: View interpolation, however, adds a nonlinearity by allowing the visibility process to determine the blending function between reference frames in a closest-take-all (a.k.a. winner-take-all) fashion. 3.4 Laveau and Faugeras Laveau and Faugeras <ref> [15] </ref> have taken advantage of the fact that the epipolar geometries between images restrict the image ow field in such a way that it can be parameterized by a single disparity value and a fundamental matrix which represents the epipolar relationship.
Reference: [16] <author> Lenz, R. K. and R. Y. Tsai, </author> <title> Techniques for Calibration of the Scale Factor and Image Center for High Accuracy 3D Machine Vision Metrology, </title> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation, </booktitle> <month> March 31 - April 3, </month> <year> 1987. </year>
Reference: [17] <author> Lippman, A., Movie-Maps: </author> <title> An Application of the Optical Videodisc to Computer Graphics, </title> <booktitle> SIGGRAPH 80 Proceedings, </booktitle> <year> 1980. </year>
Reference-contexts: This problem statement provides for many avenues of exploration, such as how to optimally select sample points and how to best reconstruct a continuous function from these samples. 3. PREVIOUS WORK 3.1 Movie-Maps The Movie-Map system by Lippman <ref> [17] </ref> is one of the earliest attempts at constructing an image-based rendering system. In Movie-Maps, incomplete plenoptic samples are stored on interactive video laser disks. They are accessed randomly, primarily by a change in viewpoint; however, the system can also accommodate panning, tilting, or zooming about a fixed viewing position.
Reference: [18] <author> Longuet-Higgins, H. C., </author> <title> A Computer Algorithm for Reconstructing a Scene from Two Projections, </title> <journal> Nature, </journal> <volume> Vol. 293, </volume> <month> September </month> <year> 1981. </year>
Reference-contexts: Next, we present a geometric constraint for cylindrical projections that determines the possible positions of a point given its position in some other cylinder. This constraint plays the same role that the epipolar geometries <ref> [18] </ref>, [9], used in the computer vision community for depth-from-stereo computations, play for planar projections. First, we will present an intuitive argument for the existence of such an invariant. Consider yourself at the center of a cylindrical projection.
Reference: [19] <author> Longuet-Higgins, H. C., </author> <title> The Reconstruction of a Scene From Two Projections Configurations That Defeat the 8-Point Algorithm, </title> <booktitle> Proceedings of the First IEEE Conference on Artificial Intelligence Applications, </booktitle> <month> Dec </month> <year> 1984. </year>
Reference: [20] <author> Lucas, B., and T. Kanade, </author> <title> An Iterative Image Registration Technique with an Application to Stereo Vision, </title> <booktitle> Proceedings of the Seventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Vancouver, </address> <year> 1981. </year>
Reference-contexts: However, several authors in the field of computer vision have shown that this type of image ow information is equivalent to changes in the local intensity due to infinitesimal perturbations of the plenoptic functions independent variables <ref> [20] </ref>, [13]. This local derivative behavior can be related to the intensity gradient via applications of the chain rule. In fact, morphing makes an even stronger assumption that the ow information is constant along the entire path, thus amounting to a locally linear approximation. <p> This cylindrical epipolar relationship can be used to establish image ow fields using standard computer vision methods. We have used correlation methods [9], a simulated annealing-like relaxation method [3], and the method of differences <ref> [20] </ref> to compute stereo disparities between cylinder pairs. Each method has its strengths and weaknesses. We refer the reader to the references for further details. 4.4 Plenoptic Function Reconstruction Our image-based rendering system takes as input cylindrically projected panoramic reference images along with scalar disparity images relating each cylinder pair.
Reference: [21] <author> McMillan, Leonard, </author> <title> A List-Priority Rendering Algorithm for Redisplaying Projected Surfaces, </title> <institution> Department of Computer Science, UNC, </institution> <type> Technical Report TR95-005, </type> <year> 1995. </year>
Reference-contexts: This simple partitioning and enumeration provides a back-to-front ordering for use by a painters style rendering algorithm. This hidden-surface algorithm is a generalization of Andersons [2] visible line algorithm to arbitrary projected grid surfaces. Additional details can be found in <ref> [21] </ref>. At this point, the plenoptic samples can be warped to their new position according to the image ow field. In general, these new pixel positions lie on an irregular grid, thus requiring some sort of reconstruction and resampling.
Reference: [22] <author> Mann, S. and R. W. </author> <title> Picard, Virtual Bellows: Constructing High Quality Stills from Video, </title> <booktitle> Proceedings of the First IEEE International Conference on Image Processing, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: This well known result has been reported by several authors [12], [28], <ref> [22] </ref>. The images resulting from typical camera motions, such as pan, tilt, roll, and zoom, can all be related in this fashion. When creating a cylindrical projection, we will only need to consider panning camera motions. <p> This approach, in effect, avoids direct determination of an entire camera model by performing all mappings between different instances of the same camera. Other techniques for deriving these homogeneous transformations without specific point correspondences have also been described <ref> [22] </ref>, [25]. The set of homogenous transforms, H i , can be decomposed into two parts which will allow for arbitrary reprojections in a manner similar to [11].
Reference: [23] <author> Press, W. H., B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling, </author> <title> Numerical Recipes in C, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, Massachusetts, </address> <pages> pp. 309-317, </pages> <year> 1988. </year>
Reference-contexts: The recon Proceedings of SIGGRAPH 95 (Los Angeles, California, August 6-11, 1995) 3 struction process again takes advantage of both image data and local derivative information to reconstruct the plenoptic function. 3.5 Regan and Pose Regan and Pose <ref> [23] </ref> describe a hybrid system in which plenoptic samples are generated on the y by a geometry-based rendering system at available rendering rates, while interactive rendering is provided by the image-based subsystem. At any instant, a user interacts with a single plenoptic sample. <p> The structural matrix, S, is determined by minimizing the following error function: (11) where I i-1 and I i represent the center third of the pixels from images i-1 and i respectively. Using Powells multivariable minimization method <ref> [23] </ref> with the following initial values for our six parameters, (12) the solution typically converges in about six iterations. At this point we will have a new estimate for (C x , C y ) which can be fed back into stage one, and the entire process can be repeated.
Reference: [24] <author> Regan, M., and R. </author> <title> Pose, Priority Rendering with a Virtual Reality Address Recalculation Pipeline, </title> <booktitle> SIGGRAPH94 Proceedings, </booktitle> <year> 1994. </year>
Reference: [25] <author> Szeliski, R., </author> <title> Image Mosaicing for Tele-Reality Applications, </title> <institution> DEC and Cambridge Research Lab Technical Report, </institution> <type> CRL 94/ 2, </type> <month> May </month> <year> 1994. </year>
Reference-contexts: This approach, in effect, avoids direct determination of an entire camera model by performing all mappings between different instances of the same camera. Other techniques for deriving these homogeneous transformations without specific point correspondences have also been described [22], <ref> [25] </ref>. The set of homogenous transforms, H i , can be decomposed into two parts which will allow for arbitrary reprojections in a manner similar to [11].
Reference: [26] <author> Tomasi, C., and T. Kanade, </author> <title> Shape and Motion from Image Streams: a Factorization Method; Full Report on the Orthographic Case, </title> <type> Technical Report, </type> <institution> CMU-CS-92-104, Carnegie Mellon University, </institution> <month> March </month> <year> 1992. </year>
Reference: [27] <author> Tsai, R. Y., </author> <title> A Versatile Camera Calibration Technique for High-Accuracy 3D Machine Vision Metrology Using Off-the-Shelf TV Cameras and Lenses, </title> <journal> IEEE Journal of Robotics and Automation, </journal> <volume> Vol. RA-3, No. 4, </volume> <month> August </month> <year> 1987. </year>
Reference-contexts: The remaining terms, W x and W z , describe the combined effects of camera orientation and deviations of the viewplanes orientation from perpendicular to the optical axis. Ideally, the viewplane would be normal to the optical axis, but manufacturing tolerances allow these numbers to vary slightly <ref> [27] </ref>. q x' x fq f y' y f 2p f atan i 1= 0= P 0 r C y = (10) In addition, the w z term is indistinguishable from the cameras roll angle and, thus, represents both the image sensors and the cameras rotation. <p> At this point, the plenoptic samples can be warped to their new position according to the image ow field. In general, these new pixel positions lie on an irregular grid, thus requiring some sort of reconstruction and resampling. We use a forward-mapping [28] reconstruction approach in the spirit of <ref> [27] </ref> in our prototype. This involves computing the projected kernels size based on the current disparity value and the derivatives along the epipolar curves. While the visibility method properly handles mesh folds, we still must consider the tears (or excessive stretching) produced by the exposure of previously occluded image regions.
Reference: [28] <author> Westover, L. A., </author> <title> Footprint Evaluation for Volume Rendering, </title> <booktitle> SIGGRAPH90 Proceedings, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: The Movie-Map form of image-based rendering can also be interpreted as a table-based evaluation of the plenoptic function. This interpretation reects the database structure common to most image-based systems. 3.2 Image Morphing Image morphing is a very popular image-based rendering technique [4], <ref> [28] </ref>. Generally, morphing is considered to occur between two images. We can think of these images as endpoints along some path through time and/or space. In this interpretation, morphing becomes a method for reconstructing partial samples of the continuous ple-noptic function along this path. <p> This well known result has been reported by several authors [12], <ref> [28] </ref>, [22]. The images resulting from typical camera motions, such as pan, tilt, roll, and zoom, can all be related in this fashion. When creating a cylindrical projection, we will only need to consider panning camera motions. <p> Additional details can be found in [21]. At this point, the plenoptic samples can be warped to their new position according to the image ow field. In general, these new pixel positions lie on an irregular grid, thus requiring some sort of reconstruction and resampling. We use a forward-mapping <ref> [28] </ref> reconstruction approach in the spirit of [27] in our prototype. This involves computing the projected kernels size based on the current disparity value and the derivatives along the epipolar curves.
Reference: [29] <author> Wolberg, G., </author> <title> Digital Image Warping, </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1990. </year>
References-found: 29

