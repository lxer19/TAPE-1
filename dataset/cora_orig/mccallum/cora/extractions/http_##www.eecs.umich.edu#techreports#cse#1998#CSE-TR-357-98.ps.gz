URL: http://www.eecs.umich.edu/techreports/cse/1998/CSE-TR-357-98.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse98.html
Root-URL: http://www.cs.umich.edu
Email: gabandah,davidson@eecs.umich.edu  
Title: Configuration Independent Analysis for Characterizing Shared-Memory Applications  
Author: Gheith A. Abandah Edward S. Davidson 
Date: September 9, 1997  
Address: Michigan  
Affiliation: Advanced Computer Architecture Laboratory, EECS Department University of  
Abstract: Characterizing shared-memory applications provides insight to design efficient systems, and provides awareness to identify and correct application performance bottlenecks. Configuration dependent analysis is often used to simulate detailed application traces on a particular hardware model. The communication traffic and computation workload generated by the application trace is used as a characterization of this application. This paper demonstrates that configuration independent analysis is a useful tool to characterize shared-memory applications. Configuration independent analysis characterizes inherent application characteristics that do not change from one configuration to another. While configuration dependent analysis is repeated for each target configuration, configuration independent analysis is only performed once. Moreover, configuration independent analysis does not require developing models for the target configurations and is faster than detailed simulation. However, configuration dependent analysis directly provides more information about specific configurations. A combination of the two analysis types constitutes a comprehensive and efficient methodology for characterizing shared-memory applications. In this paper, we show how configuration independent analysis is used to characterize eight aspects of application behavior: general characteristics, working sets, concurrency, communication patterns, communication variation over time, communication slack, communication locality, and sharing behavior. We illustrate the advantages and limitations of this approach by analyzing eight case-study benchmarks from the scientific and commercial domains and interpreting the results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. J. Denning, </author> <title> Working set model for program behavior, </title> <journal> Commun. ACM, </journal> <volume> vol. 11, no. 6, </volume> <pages> pp. 323333, </pages> <year> 1968. </year>
Reference-contexts: This paper addresses eight characteristics of shared-memory applications: * General characteristics of the application, including dynamic instruction count, number of distinct touched instructions, a parallel execution profile (serial and parallel phases), number of synchronization barriers and locks, I/O traffic, and percentage of memory instructions (by type). * The working set <ref> [1] </ref> of an application in an execution interval is the number of distinct memory locations accessed in this interval. The working set often changes over time and may be hierarchical, e.g. multiple working sets may be accessed iteratively and collectively constitute a larger working set.
Reference: [2] <author> M. Wolfe, </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year>
Reference-contexts: This characterization is also useful to programmers; for example, when the working set size is larger than the cache size, the programmer can improve the application performance by reducing the working set, e.g. by segmenting a matrix computation into blocks <ref> [2] </ref>. * The amount of concurrency available in an application influences how well application performance scales as more processors are used. An application with high concurrency has the potential to efficiently utilize a large number of processors. Section 5.3 discusses factors that affect the concurrency of shared-memory applications.
Reference: [3] <author> G. Abandah, </author> <title> Tools for characterizing distributed shared memory applications, </title> <type> Tech. Rep. </type> <institution> HPL96 157, HP Laboratories, </institution> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: CDAT is used to characterize things like cache misses and false sharing that depend on configuration parameters, e.g. cache size and line width. Since this paper concentrates on CIAT, other tools are treated briefly. However, an interested reader can refer to <ref> [3] </ref>. CIAT analyzes application properties that do not change from one configuration to another, thus relieving CDAT from repeating this analysis for every configuration. CDAT, which uses fairly detailed models of the system coherence protocol and system state, is generally slower than CIAT.
Reference: [4] <author> Hewlett-Packard, </author> <title> PA-RISC 1.1 Architecture and Instruction Set, </title> <booktitle> third ed., </booktitle> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: CDAT, which uses fairly detailed models of the system coherence protocol and system state, is generally slower than CIAT. SMAIT has two parts: a perl script program for instrumenting PARISC <ref> [4] </ref> assembly language files (based on a tool called RYO [5]), and a run-time library that is linked with the instrumented program. The perl script program replaces some PA-RISC instructions with calls to the run-time library subroutines.
Reference: [5] <author> D. F. Zucker and A. H. Karp, RYO: </author> <title> a versatile instruction instrumentation tool for PARISC, </title> <type> Technical Report CSLTR95658, </type> <institution> Stanford University, </institution> <month> Jan. </month> <year> 1995. </year>
Reference-contexts: CDAT, which uses fairly detailed models of the system coherence protocol and system state, is generally slower than CIAT. SMAIT has two parts: a perl script program for instrumenting PARISC [4] assembly language files (based on a tool called RYO <ref> [5] </ref>), and a run-time library that is linked with the instrumented program. The perl script program replaces some PA-RISC instructions with calls to the run-time library subroutines. During program execution, the run-time library generates trace records for the instrumented memory instructions, taken branches, and synchronization and I/O calls.
Reference: [6] <author> S. Fortune and J. Wyllie, </author> <title> Parallelism in random access machines, </title> <booktitle> in Proc. Tenth ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 114118, </pages> <year> 1978. </year>
Reference-contexts: TDAT is also used to analyze CDAT's traffic trace. The characterization reported by these tools is used to support application tuning, early-design of scalable shared-memory systems, parameterizing synthetic work-load generators, comparing alternative design options, and investigating new design approaches. CIAT uses a model similar to the PRAM model <ref> [6] </ref> which assumes that p processors can execute p instructions concurrently and each instruction takes a fixed time. Therefore, CIAT keeps track of time in instruction units.
Reference: [7] <author> S. Woo, M. Ohara, E. Torrie, J. Singh, and A. Gupta, </author> <title> The SPLASH-2 programs: Characterization and methodology considerations, </title> <booktitle> in Proc. of the 22nd ISCA, </booktitle> <pages> pp. 2436, </pages> <year> 1995. </year>
Reference-contexts: CIAT performs analysis per phase, reports phase characteristics at the end of each phase, and reports the aggregate characteristics of all phases at the end. 4 Applications We have analyzed Radix, FFT, LU, and Cholesky from SPLASH-2 <ref> [7] </ref>, CG and SP from NPB [8], and TPC benchmarks C and D [9, 10]. SPLASH-2 consists of 8 applications and 4 computational kernels drawn from scientific, engineering, and graphics computing. <p> fixed number of times, e.g. saving the previous state at a procedure entry for a procedure that is called a fixed number of times. 5.2 Working Sets The size of an application's working set is sometimes characterized by conducting multiple simulation experiments using a fully-associative cache with LRU replacement policy <ref> [15, 7] </ref>. Each experiment uses one cache size and measures the cache miss ratio. A graph of the cache miss ratio versus cache size is used to deduce the working set sizes from the graph knees. <p> They suggested developing general-purpose simulation tools to obtain empirical information for supporting 23 the design of parallel algorithms. There are several studies that combine source-code analysis with configuration dependent analysis to characterize shared-memory applications <ref> [21, 15, 7] </ref>. Woo et al. have characterized several aspects of the SPLASH-2 suite of parallel applications [7]. Their characterization includes load balance, working sets, communication to computation ratio, system traffic, and sharing. They used execution-driven simulation with the Tango Lite [22] tracing tool. <p> There are several studies that combine source-code analysis with configuration dependent analysis to characterize shared-memory applications [21, 15, 7]. Woo et al. have characterized several aspects of the SPLASH-2 suite of parallel applications <ref> [7] </ref>. Their characterization includes load balance, working sets, communication to computation ratio, system traffic, and sharing. They used execution-driven simulation with the Tango Lite [22] tracing tool. In order to capture some of the fundamental properties of SPLASH-2, they adjusted model parameters between low and high values.
Reference: [8] <author> D. Bailey et al., </author> <title> The NAS parallel benchmarks, </title> <type> Technical Report RNR-94-07, </type> <institution> NASA Ames Research Center, </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: CIAT performs analysis per phase, reports phase characteristics at the end of each phase, and reports the aggregate characteristics of all phases at the end. 4 Applications We have analyzed Radix, FFT, LU, and Cholesky from SPLASH-2 [7], CG and SP from NPB <ref> [8] </ref>, and TPC benchmarks C and D [9, 10]. SPLASH-2 consists of 8 applications and 4 computational kernels drawn from scientific, engineering, and graphics computing.
Reference: [9] <author> Transaction Processing Performance Council, </author> <title> TPC Benchmark C, Standard Specification, </title> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: CIAT performs analysis per phase, reports phase characteristics at the end of each phase, and reports the aggregate characteristics of all phases at the end. 4 Applications We have analyzed Radix, FFT, LU, and Cholesky from SPLASH-2 [7], CG and SP from NPB [8], and TPC benchmarks C and D <ref> [9, 10] </ref>. SPLASH-2 consists of 8 applications and 4 computational kernels drawn from scientific, engineering, and graphics computing.
Reference: [10] <author> Transaction Processing Performance Council, </author> <title> TPC Benchmark D, Decision Support, Standard Specification, </title> <month> May </month> <year> 1995. </year>
Reference-contexts: CIAT performs analysis per phase, reports phase characteristics at the end of each phase, and reports the aggregate characteristics of all phases at the end. 4 Applications We have analyzed Radix, FFT, LU, and Cholesky from SPLASH-2 [7], CG and SP from NPB [8], and TPC benchmarks C and D <ref> [9, 10] </ref>. SPLASH-2 consists of 8 applications and 4 computational kernels drawn from scientific, engineering, and graphics computing.
Reference: [11] <author> G. Abandah, </author> <title> Characterizing shared-memory applications: A case study of the NAS parallel benchmarks, </title> <type> Tech. Rep. </type> <institution> HPL9724, HP Laboratories, </institution> <month> Jan. </month> <year> 1997. </year>
Reference-contexts: SPLASH-2 consists of 8 applications and 4 computational kernels drawn from scientific, engineering, and graphics computing. NPB are 5 kernels and 3 pseudo-applications that mimic the computation and data movement characteristics of large-scale computational fluid dynamic applications (an earlier report characterizes 5 benchmarks of NPB using CIAT and CDAT <ref> [11] </ref>). The TPC benchmarks are intended to compare commercial database platforms. The following is a short description of the eight benchmarks. Radix is an integer sort kernel that iterates on radix r digits of the keys.
Reference: [12] <institution> Transaction Processing Performance Council Home Page. </institution> <note> http://www.tpc.org/. </note>
Reference-contexts: Compared with other queries, although Query 3 takes a moderate run time, it has a high disk I/O and communication rates <ref> [12] </ref>. A comprehensive characterization of TPC-D's queries is beyond the scope of this paper. Table 1 shows the problem sizes analyzed in this study. The scientific benchmarks were analyzed using two problem sizes on a range of processors from 1 to 32.
Reference: [13] <author> T. Brewer, </author> <title> A highly scalable system utilizing up to 128 PA-RISC processors, </title> <booktitle> in Digest of papers, COMPCON'95, </booktitle> <pages> pp. 133140, </pages> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: The SPLASH-2 benchmarks were developed in Stanford University to facilitate shared-memory multiprocessor research and are written in C. The NPB are specified algorithmically so that computer vendors can implement them on a wide range of parallel machines. We analyzed the Convex Exemplar <ref> [13] </ref> implementation of NPB which is written in Fortran. The performance of an earlier version of this implementation is reported in [14]. However, to get a general characterization of these benchmarks, we undid some of the Exemplar-specific optimizations.
Reference: [14] <author> S. Saini and D. H. Bailey, </author> <title> NAS parallel benchmark results 1295, </title> <type> Tech. Rep. </type> <institution> NAS95021, NASA Ames Research Center, </institution> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: The NPB are specified algorithmically so that computer vendors can implement them on a wide range of parallel machines. We analyzed the Convex Exemplar [13] implementation of NPB which is written in Fortran. The performance of an earlier version of this implementation is reported in <ref> [14] </ref>. However, to get a general characterization of these benchmarks, we undid some of the Exemplar-specific optimizations. The six scientific benchmarks were instrumented, compiled, and analyzed on a 4-node Exemplar SPP1600 multiprocessor. Table 2 shows the configuration of this system.
Reference: [15] <author> E. Rothberg, J. Singh, and A. Gupta, </author> <title> Working sets, cache sizes, and node granularity issues for large-scale multiprocessors, </title> <booktitle> in Proc. of the 20th ISCA, </booktitle> <pages> pp. 1425, </pages> <year> 1993. </year> <month> 25 </month>
Reference-contexts: fixed number of times, e.g. saving the previous state at a procedure entry for a procedure that is called a fixed number of times. 5.2 Working Sets The size of an application's working set is sometimes characterized by conducting multiple simulation experiments using a fully-associative cache with LRU replacement policy <ref> [15, 7] </ref>. Each experiment uses one cache size and measures the cache miss ratio. A graph of the cache miss ratio versus cache size is used to deduce the working set sizes from the graph knees. <p> They suggested developing general-purpose simulation tools to obtain empirical information for supporting 23 the design of parallel algorithms. There are several studies that combine source-code analysis with configuration dependent analysis to characterize shared-memory applications <ref> [21, 15, 7] </ref>. Woo et al. have characterized several aspects of the SPLASH-2 suite of parallel applications [7]. Their characterization includes load balance, working sets, communication to computation ratio, system traffic, and sharing. They used execution-driven simulation with the Tango Lite [22] tracing tool.
Reference: [16] <author> G. M. </author> <title> Amdahl, Validity of single-processor approach to achieving large-scale computing capability, </title> <booktitle> in Proc. AFIPS Conf., </booktitle> <address> (Reston, VA.), </address> <pages> pp. 483485, </pages> <year> 1967. </year>
Reference-contexts: Amdahl's serial fraction <ref> [16] </ref>, ignoring the three parallel overheads, is Serial Fraction = Idle (p)=(p 1) Busy (1) Perfect speedup is only possible when the serial fraction, parallel overhead busy work, imbalance, and contention are zero.
Reference: [17] <author> J. Singh, E. Rothberg, and A. Gupta, </author> <title> Modeling communication in parallel algorithms: A fruitful interaction between theory and systems?, </title> <booktitle> in Proc. Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 189199, </pages> <year> 1994. </year>
Reference-contexts: In a machine with high synchronization overheads, the contention time can become more significant than reported by CIAT. 5.4 Communication Patterns In configuration dependent analysis, communication is characterized from the traffic that a processor generates to access data that is not allocated in its local memory <ref> [17] </ref>. This traffic includes traffic due to inherent coherence communication, cold-start misses, finite cache capacity, limited cache associativity, and false sharing. <p> Pablo [18], Medea [19], and Paradyn [20]. Nevertheless, there is some work that focuses on characterizing shared-memory applications. Singh et al. demonstrated that it is often difficult to model the communication of parallel algorithms analytically <ref> [17] </ref>. They suggested developing general-purpose simulation tools to obtain empirical information for supporting 23 the design of parallel algorithms. There are several studies that combine source-code analysis with configuration dependent analysis to characterize shared-memory applications [21, 15, 7].
Reference: [18] <author> D. A. Reed, R. A. Aydt, R. J. Noe, P. C. Roth, K. A. Shields, B. W. Schwartz, and L. F. Tavera, </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment, </title> <booktitle> in Proc. Scalable Parallel Libraries Conf., </booktitle> <pages> pp. 104113, </pages> <year> 1993. </year>
Reference-contexts: However, the large increase in the total number of accesses in Cholesky is due to the increase in both private and shared accesses. 6 Related Work Available parallel performance analysis tools have mainly been developed for analyzing message-passing applications, e.g. Pablo <ref> [18] </ref>, Medea [19], and Paradyn [20]. Nevertheless, there is some work that focuses on characterizing shared-memory applications. Singh et al. demonstrated that it is often difficult to model the communication of parallel algorithms analytically [17].
Reference: [19] <author> M. Calzarossa, L. Massari, A. Merlo, M. Pantano, and T. Daniele, Medea: </author> <title> A tool for workload characterization of parallel systems, </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> vol. 3, </volume> <pages> pp. 7280, </pages> <month> Winter </month> <year> 1995. </year>
Reference-contexts: However, the large increase in the total number of accesses in Cholesky is due to the increase in both private and shared accesses. 6 Related Work Available parallel performance analysis tools have mainly been developed for analyzing message-passing applications, e.g. Pablo [18], Medea <ref> [19] </ref>, and Paradyn [20]. Nevertheless, there is some work that focuses on characterizing shared-memory applications. Singh et al. demonstrated that it is often difficult to model the communication of parallel algorithms analytically [17].
Reference: [20] <author> B. Miller, M. Callaghan, J. Cargille, J. Hollingsworth, R. Irvin, K. Karavanic, K. Kunchithapadam, and T. Newhall, </author> <title> The Paradyn parallel performance measurement tool, </title> <journal> Computer, </journal> <volume> vol. 28, </volume> <pages> pp. 3746, </pages> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: However, the large increase in the total number of accesses in Cholesky is due to the increase in both private and shared accesses. 6 Related Work Available parallel performance analysis tools have mainly been developed for analyzing message-passing applications, e.g. Pablo [18], Medea [19], and Paradyn <ref> [20] </ref>. Nevertheless, there is some work that focuses on characterizing shared-memory applications. Singh et al. demonstrated that it is often difficult to model the communication of parallel algorithms analytically [17]. They suggested developing general-purpose simulation tools to obtain empirical information for supporting 23 the design of parallel algorithms.
Reference: [21] <author> J. Singh, W.-D. Weber, and A. Gupta, </author> <title> SPLASH: Stanford parallel applications for shared memory, </title> <journal> Computer Architecture News, </journal> <volume> vol. 20, </volume> <pages> pp. 544, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: They suggested developing general-purpose simulation tools to obtain empirical information for supporting 23 the design of parallel algorithms. There are several studies that combine source-code analysis with configuration dependent analysis to characterize shared-memory applications <ref> [21, 15, 7] </ref>. Woo et al. have characterized several aspects of the SPLASH-2 suite of parallel applications [7]. Their characterization includes load balance, working sets, communication to computation ratio, system traffic, and sharing. They used execution-driven simulation with the Tango Lite [22] tracing tool.
Reference: [22] <author> S. Goldschmidt and J. Hennessy, </author> <title> The accuracy of trace-driven simulations of multiprocessors, </title> <type> Tech. Rep. </type> <institution> CSL-TR-92-546, Stanford University, </institution> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: Woo et al. have characterized several aspects of the SPLASH-2 suite of parallel applications [7]. Their characterization includes load balance, working sets, communication to computation ratio, system traffic, and sharing. They used execution-driven simulation with the Tango Lite <ref> [22] </ref> tracing tool. In order to capture some of the fundamental properties of SPLASH-2, they adjusted model parameters between low and high values. Chandra et al. also used simulation to characterize the performance of a collection of applications [23].
Reference: [23] <author> S. Chandra, J. R. Larus, and A. Rogers, </author> <title> Where is time spent in message-passing and shared-memory programs, </title> <booktitle> in Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 6173, </pages> <year> 1994. </year>
Reference-contexts: They used execution-driven simulation with the Tango Lite [22] tracing tool. In order to capture some of the fundamental properties of SPLASH-2, they adjusted model parameters between low and high values. Chandra et al. also used simulation to characterize the performance of a collection of applications <ref> [23] </ref>. Their main objective was to analyze where time is spent in message-passing versus shared-memory programs. Perl and Sites [24] have studied some Windows NT applications on Alpha PCs. Their study includes analyzing the application bandwidth requirements, characterizing the memory access patterns, and analyzing application sensitivity to cache size.
Reference: [24] <author> S. E. Perl and R. L. </author> <title> Sites, Studies of Windows NT performance using dynamic execution traces, </title> <booktitle> in 2nd Symposium on Operating Systems Design and Implementation (OSDI '96), </booktitle> <month> October 2831, </month> <year> 1996. </year> <title> Seattle, </title> <booktitle> WA (USENIX, </booktitle> <publisher> ed.), </publisher> <pages> pp. 169183, </pages> <year> 1996. </year>
Reference-contexts: Chandra et al. also used simulation to characterize the performance of a collection of applications [23]. Their main objective was to analyze where time is spent in message-passing versus shared-memory programs. Perl and Sites <ref> [24] </ref> have studied some Windows NT applications on Alpha PCs. Their study includes analyzing the application bandwidth requirements, characterizing the memory access patterns, and analyzing application sensitivity to cache size.
Reference: [25] <author> S. Chodnekar, V. Srinivasan, A. Vaidya, A. Sivasubramaniam, and C. Das, </author> <title> Towards a communication characterization methodology for parallel applications, </title> <booktitle> in Proc. of HPCA-3, </booktitle> <pages> pp. 310319, </pages> <year> 1997. </year>
Reference-contexts: Their study includes analyzing the application bandwidth requirements, characterizing the memory access patterns, and analyzing application sensitivity to cache size. To get insight in designing interconnection networks, Chodnekar et al. analyzed the time distribution and locality of communication events in some message-passing and shared-memory applications <ref> [25] </ref>.
Reference: [26] <author> S. Leutenegger and D. Dias, </author> <title> A modeling study of the TPC-C benchmark, </title> <booktitle> in Proc. of ACM SIGMOD, </booktitle> <pages> pp. 2231, </pages> <year> 1993. </year>
Reference-contexts: Their study includes analyzing the application bandwidth requirements, characterizing the memory access patterns, and analyzing application sensitivity to cache size. To get insight in designing interconnection networks, Chodnekar et al. analyzed the time distribution and locality of communication events in some message-passing and shared-memory applications [25]. Leutenegger and Dias <ref> [26] </ref> analyzed the TPC-C disk accesses to model its disk access patterns and showed that TPC-C can achieve close to linear speedup in a distributed system when some read-only data is replicated. 7 Conclusions Splitting the application analysis into configuration independent and configuration dependent analysis provides a clean and efficient characterization
References-found: 26

