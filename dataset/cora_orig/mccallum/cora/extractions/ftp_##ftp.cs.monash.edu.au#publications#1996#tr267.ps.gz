URL: ftp://ftp.cs.monash.edu.au/publications/1996/tr267.ps.gz
Refering-URL: http://www.cs.monash.edu.au/~kelly/publications/group.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: A Comparison of the Effectiveness of Optimizations in a CLP(R) Compiler  
Author: Andrew D. Kelly Andrew Macdonald Kim Marriott Peter J. Stuckey Roland H.C. Yap 
Keyword: Key words: Constraint logic programming, compilation, program analysis, program optimization, source-to-source program transformation.  
Abstract: Constraint Logic Programming (CLP) languages extend logic programming by allowing constraints from different domains such as real numbers or Boolean functions. They have proved to be ideal for expressing problems that require interactive mathematical modelling and complex combinatorial optimization problems. However, CLP languages have mainly been considered as research systems, useful for rapid prototyping, but not really competitive with more conventional programming languages when performance is crucial. One promising approach to improving the performance of CLP systems is the use of powerful program optimizations to reduce the cost of constraint solving. We extend work in this area by describing a new optimizing compiler for the CLP language CLP(R). The compiler implements six powerful optimizations: reordering of constraints, bypass of the constraint solver, splitting and dead code elimination, removal of redundant constraints, removal of redundant variables, and specialization of constraints which cannot fail. Each program optimization is designed to remove the overhead of constraint solving when possible and keep the number of constraints in the store as small as possible. We systematically evaluate the effectiveness of each optimization in isolation and also in combination. Our empirical evaluation of the compiler verifies that optimizing compilation can be made efficient enough to allow compilation of real-world programs and that it is worth performing such compilation because it gives significant time and space performance improvements. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. At-Kaci. </author> <title> Warren's Abstract Machine: A Tutorial Reconstruction. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: The optimizing compiler has four main components (see Figure 2): the optimizer which performs the optimizations; a global analyzer and a constraint solver which provide information to guide the optimizer; and a code generator which produces CLAM abstract machine code. CLAM is an extension of the Prolog WAM <ref> [1] </ref> architecture which supports calls to constraint solvers. The current compiler consists of about 54,000 lines of C++ code. The main complication in the design of the optimizer is the number of different optimizations. This is exacerbated by non-trivial interaction between the optimizations.
Reference: [2] <author> T. Armstrong, K. Marriott, P. Schachte and H. Stndergaard. </author> <title> Boolean functions for dependency analysis: Algebraic properties and efficient representation. </title> <editor> In B. Le Charlier, editor, </editor> <title> Static Analysis: </title> <booktitle> Proc. First Int. Symp. (Lecture Notes in Computer Science 864), </booktitle> <pages> 266-280. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Currently the analyzer uses descriptions which are tuples of seven different domains: Pos This domain captures groundness information about variables. We use ROBDDs to represent the Boolean functions <ref> [2] </ref>. CallAlive This consists of lists of variables which may be directly referenced later in execution. Shar This captures information about possible structure sharing of variables between Prolog terms. It is based on descriptions introduced by Stndergaard [14] for eliminating occur checks in Prolog.
Reference: [3] <author> M. Hermenegildo, G. Puebla, K. Marriott, and P. Stuckey. </author> <title> Incremental Analysis of Logic Programs. </title> <booktitle> In Proc. of the Int. Conf. on Logic Programming, </booktitle> <address> Tokyo, Japan, </address> <publisher> MIT Press, </publisher> <month> June </month> <year> 1995, </year> <pages> 797-814. </pages>
Reference-contexts: Second, it handles "incremental widening" which means that the analysis will terminate for description domains such as arithmetic intervals which have infinite ascending chains of descriptions. Details about the algorithms used can be found in <ref> [3] </ref>. Details of the description domains are deliberately kept insulated from the optimizer so as to make it easier to change them.
Reference: [4] <author> A. Kelly, A. Macdonald, K. Marriott, H. Stndergaard, P. Stuckey, and R. Yap. </author> <title> An optimizing compiler for CLP(R). </title> <booktitle> In Proc of the First Int. Conf. on Principles and Practices of Constraint Programming, </booktitle> <publisher> LNCS 976, Springer-Verlag, </publisher> <pages> 222-239, </pages> <year> 1995. </year>
Reference-contexts: One promising approach to improving the performance of CLP systems is the use of powerful program optimizations to reduce the cost of constraint solving. Experience with such an approach to the compilation of CLP (R) was recently presented in <ref> [4] </ref>. CLP (R) is the prototypical constraint logic programming language which extends Prolog by incorporating real arithmetic constraints. The results described in [4] indicate that performing three optimizations bypass of the constraint solver, fl Dept. of Computer Science, Monash University, Clayton 3168, Australia. y Dept. of Computer Science, University of Melbourne, <p> Experience with such an approach to the compilation of CLP (R) was recently presented in <ref> [4] </ref>. CLP (R) is the prototypical constraint logic programming language which extends Prolog by incorporating real arithmetic constraints. The results described in [4] indicate that performing three optimizations bypass of the constraint solver, fl Dept. of Computer Science, Monash University, Clayton 3168, Australia. y Dept. of Computer Science, University of Melbourne, Parkville 3052, Australia. z Dept. of Information Systems and Computer Science, National University of Singapore, Singapore 119260. 1 redundant variable elimination and <p> This is the process of duplicating (splitting or specializing) predicates for the different modes in which they are called and applying optimizations to each split copy, or variant, in a different way from any other split variant. Unfortunately the system described in <ref> [4] </ref> did not prove the practicality of such an optimizing compiler as the cost of performing the required program analysis for these optimizations was found to be prohibitively high on larger benchmarks. The system described in [4] also did not implement the more powerful optimizations of constraint reordering and constraint removal. <p> Unfortunately the system described in <ref> [4] </ref> did not prove the practicality of such an optimizing compiler as the cost of performing the required program analysis for these optimizations was found to be prohibitively high on larger benchmarks. The system described in [4] also did not implement the more powerful optimizations of constraint reordering and constraint removal. These optimizations are potentially even more useful than those considered in [4]. For example, in [7] preliminary tests indicate that future redundancy, a type of constraint removal optimization, can lead to a forty-fold performance improvement. <p> The system described in <ref> [4] </ref> also did not implement the more powerful optimizations of constraint reordering and constraint removal. These optimizations are potentially even more useful than those considered in [4]. For example, in [7] preliminary tests indicate that future redundancy, a type of constraint removal optimization, can lead to a forty-fold performance improvement. The current paper describes a system which addresses these issues. It extends the work described in [4] in three ways. * Our empirical studies of the new <p> optimizations are potentially even more useful than those considered in <ref> [4] </ref>. For example, in [7] preliminary tests indicate that future redundancy, a type of constraint removal optimization, can lead to a forty-fold performance improvement. The current paper describes a system which addresses these issues. It extends the work described in [4] in three ways. * Our empirical studies of the new compiler show that it is an order of magnitude faster than the compiler in [4] and demonstrates that optimizing compilation is sufficiently fast for it to be employed in practical CLP compilation. * Second, we consider an additional two optimizations <p> The current paper describes a system which addresses these issues. It extends the work described in <ref> [4] </ref> in three ways. * Our empirical studies of the new compiler show that it is an order of magnitude faster than the compiler in [4] and demonstrates that optimizing compilation is sufficiently fast for it to be employed in practical CLP compilation. * Second, we consider an additional two optimizations constraint reordering and future redun dancy. * Third (and arguably most importantly), we systematically evaluate the effectiveness of each optimization and the cost of performing <p> We consider each optimization in isolation and also in combination. Our results are somewhat surprising. The optimizing compiler and analyser presented in <ref> [4] </ref> has been completely redesigned and rewritten. This was necessary because of its slow performance and because it could not support constraint reordering and constraint removal. <p> Constraint reordering and future redundancy were quite complex to implement and required the multi-pass analysis and optimization cycle of the new optimizing compiler as well as the new analysis domain. This paper continues our work on the implementation of CLP (R) <ref> [6, 5, 9, 4] </ref> and on preliminary studies of each of the optimizations considered here [7, 5, 12, 9, 11]. Our compiler is also related to experimental compilers for Prolog, which also make use of global program analysis [17, 15, 13]. These too have given rise to good performance improvements. <p> Also the scope for performance improvement is greater in CLP languages because of the high cost of constraint solving. Apart from our earlier work in <ref> [4] </ref>, the only other work on optimizing compilation of CLP languages is that of [16] describing work in-progress on a compiler for CLP (Lin). CLP (Lin) is essentially CLP (R) without term constraints or delayed non-linear arithmetic constraints. Their compiler uses constraint reordering, constraint removal and solver bypass (called refinement). <p> Conceptually, the analyzer takes a program and goal and annotates each point in the program with an approximate description of the constraints which will be encountered at that point when the goal is executed. An example of how a typical program is annotated can be found in <ref> [4] </ref>. Actually the analyzer does not exist as a separate entity in the compiler. Rather it is associated with the AnnotatedP rogram class of which the current annotated CLIC program is an instance. The optimizer obtains analysis information by way of methods on AnnotatedP rograms.
Reference: [5] <author> J. Jaffar, S. Michaylov, P. Stuckey and R. Yap. </author> <title> An abstract machine for CLP(R). </title> <booktitle> Proc. ACM Conf. Programming Language Design and Implementation, </booktitle> <pages> 128-139. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: Constraint reordering and future redundancy were quite complex to implement and required the multi-pass analysis and optimization cycle of the new optimizing compiler as well as the new analysis domain. This paper continues our work on the implementation of CLP (R) <ref> [6, 5, 9, 4] </ref> and on preliminary studies of each of the optimizations considered here [7, 5, 12, 9, 11]. Our compiler is also related to experimental compilers for Prolog, which also make use of global program analysis [17, 15, 13]. These too have given rise to good performance improvements. <p> This paper continues our work on the implementation of CLP (R) [6, 5, 9, 4] and on preliminary studies of each of the optimizations considered here <ref> [7, 5, 12, 9, 11] </ref>. Our compiler is also related to experimental compilers for Prolog, which also make use of global program analysis [17, 15, 13]. These too have given rise to good performance improvements. <p> If the constraint is made redundant by each such combination of constraints (which is determined using the constraint solver) it is marked as add remove. 1 Code Generator The code generator maps the CLIC code into CLAM <ref> [5] </ref> instructions which are executed by the CLAM emulator. The existing CLP (R) compiler also produced CLAM code, but used only a core set.
Reference: [6] <author> J. Jaffar, S. Michaylov, P. Stuckey and R. Yap. </author> <title> The CLP(R) language and system. </title> <booktitle> ACM Transactions on Programming Languages and Systems 14 (3): </booktitle> <pages> 339-395, </pages> <year> 1992. </year>
Reference-contexts: Constraint reordering and future redundancy were quite complex to implement and required the multi-pass analysis and optimization cycle of the new optimizing compiler as well as the new analysis domain. This paper continues our work on the implementation of CLP (R) <ref> [6, 5, 9, 4] </ref> and on preliminary studies of each of the optimizations considered here [7, 5, 12, 9, 11]. Our compiler is also related to experimental compilers for Prolog, which also make use of global program analysis [17, 15, 13]. These too have given rise to good performance improvements.
Reference: [7] <author> N. Jtrgensen, K. Marriott and S. Michaylov. </author> <title> Some global compile-time optimizations for CLP(R). </title> <editor> In V. Saraswat and K. Ueda, editors, </editor> <booktitle> Logic Programming: Proc. 1991 Int. Symp., </booktitle> <pages> 420-434. </pages> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: The system described in [4] also did not implement the more powerful optimizations of constraint reordering and constraint removal. These optimizations are potentially even more useful than those considered in [4]. For example, in <ref> [7] </ref> preliminary tests indicate that future redundancy, a type of constraint removal optimization, can lead to a forty-fold performance improvement. The current paper describes a system which addresses these issues. <p> This paper continues our work on the implementation of CLP (R) [6, 5, 9, 4] and on preliminary studies of each of the optimizations considered here <ref> [7, 5, 12, 9, 11] </ref>. Our compiler is also related to experimental compilers for Prolog, which also make use of global program analysis [17, 15, 13]. These too have given rise to good performance improvements.
Reference: [8] <author> B. Le Charlier and P. Van Hentenryck. </author> <title> Experimental evaluation of a generic abstract interpretation algorithm for Prolog. </title> <booktitle> ACM Transactions on Programming Languages and Systems 16 (1): </booktitle> <pages> 35-101, </pages> <year> 1994. </year>
Reference-contexts: For efficiency therefore, the analyzer is incremental and only re-analyzes those parts of the program which have been changed. The Analyzer The global analyzer consists of a generic abstract interpretation engine, similar to analysis engines such as PLAI [13] and GAIA <ref> [8] </ref> developed for Prolog. The generic analysis engine is designed not only to perform many different analyses but also to allow for the easy addition of new analyses. The core of the analyzer is an algorithm for efficient fixpoint computation.
Reference: [9] <author> A. Macdonald, P. Stuckey and R. Yap. </author> <title> Redundancy of variables in CLP(R). </title> <booktitle> In Logic Programming: Proc. 1993 Int. Symp., </booktitle> <pages> 75-93. </pages> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Constraint reordering and future redundancy were quite complex to implement and required the multi-pass analysis and optimization cycle of the new optimizing compiler as well as the new analysis domain. This paper continues our work on the implementation of CLP (R) <ref> [6, 5, 9, 4] </ref> and on preliminary studies of each of the optimizations considered here [7, 5, 12, 9, 11]. Our compiler is also related to experimental compilers for Prolog, which also make use of global program analysis [17, 15, 13]. These too have given rise to good performance improvements. <p> This paper continues our work on the implementation of CLP (R) [6, 5, 9, 4] and on preliminary studies of each of the optimizations considered here <ref> [7, 5, 12, 9, 11] </ref>. Our compiler is also related to experimental compilers for Prolog, which also make use of global program analysis [17, 15, 13]. These too have given rise to good performance improvements.
Reference: [10] <author> K. Marriott and H. Stndergaard. </author> <title> Analysis of constraint logic programs. </title> <editor> In S. Debray and M. Hermenegildo, </editor> <booktitle> Logic Programming: Proc. North American Conf. </booktitle> <year> 1990, </year> <pages> 531-547. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The analyzer performs global analysis of programs, that is, program information (descriptions) is inferred about the calls "between" different rules of the program, (as well as inferring the local information "within" each rule). The global analysis is based on abstract interpretation of constraint logic programs <ref> [10] </ref> in which operations in the execution of the goal are mimicked by abstract operations on the domain of descriptions, (the analyses).
Reference: [11] <author> K. Marriott, H. Stndergaard, P. Stuckey and R. Yap. </author> <title> Optimizing compilation for CLP(R). </title> <editor> In G. Gupta, editor, </editor> <booktitle> Proc. Seventeenth Australian Computer Science Conf., Australian Computer Science Comm. </booktitle> <volume> 16 (1): </volume> <pages> 551-560, </pages> <year> 1994. </year>
Reference-contexts: This paper continues our work on the implementation of CLP (R) [6, 5, 9, 4] and on preliminary studies of each of the optimizations considered here <ref> [7, 5, 12, 9, 11] </ref>. Our compiler is also related to experimental compilers for Prolog, which also make use of global program analysis [17, 15, 13]. These too have given rise to good performance improvements.
Reference: [12] <author> K. Marriott and P. Stuckey. </author> <title> The 3 R's of optimizing constraint logic programs: Refinement, removal and reordering. </title> <booktitle> Proc. Twentieth ACM Symp. Principles of Programming Languages, </booktitle> <pages> 334-344. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: This paper continues our work on the implementation of CLP (R) [6, 5, 9, 4] and on preliminary studies of each of the optimizations considered here <ref> [7, 5, 12, 9, 11] </ref>. Our compiler is also related to experimental compilers for Prolog, which also make use of global program analysis [17, 15, 13]. These too have given rise to good performance improvements.
Reference: [13] <author> K. Muthukumar and M. Hermenegildo. </author> <title> Compile-time derivation of variable dependency using abstract interpretation. </title> <journal> Journal of Logic Programming 13 (2&3): </journal> <pages> 315-347, </pages> <year> 1992. </year>
Reference-contexts: This paper continues our work on the implementation of CLP (R) [6, 5, 9, 4] and on preliminary studies of each of the optimizations considered here [7, 5, 12, 9, 11]. Our compiler is also related to experimental compilers for Prolog, which also make use of global program analysis <ref> [17, 15, 13] </ref>. These too have given rise to good performance improvements. However these compilers are quite simple when compared to our compiler as they apply only one or two non-interacting optimizations and only use simple non-incremental program analysis. <p> For efficiency therefore, the analyzer is incremental and only re-analyzes those parts of the program which have been changed. The Analyzer The global analyzer consists of a generic abstract interpretation engine, similar to analysis engines such as PLAI <ref> [13] </ref> and GAIA [8] developed for Prolog. The generic analysis engine is designed not only to perform many different analyses but also to allow for the easy addition of new analyses. The core of the analyzer is an algorithm for efficient fixpoint computation.
Reference: [14] <author> H. Stndergaard. </author> <title> An application of abstract interpretation of logic programs: Occur check reduction. </title> <editor> In B. Robinet and R. Wilhelm, editors, </editor> <booktitle> Proc. ESOP 86 (Lecture Notes in Computer Science 213), </booktitle> <pages> 327-338. </pages> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: We use ROBDDs to represent the Boolean functions [2]. CallAlive This consists of lists of variables which may be directly referenced later in execution. Shar This captures information about possible structure sharing of variables between Prolog terms. It is based on descriptions introduced by Stndergaard <ref> [14] </ref> for eliminating occur checks in Prolog. The description consists of a possible sharing relation for variables. Consider the goal Y = f (X), p (X), Y = f (Z), q (Z). After Y = f (X), X and Y possibly share, but Z does not share with anything else.
Reference: [15] <author> A. Taylor. </author> <title> LIPS on a MIPS: Results from a Prolog compiler for a RISC. </title> <editor> In D. Warren and P. Szeredi, editors, </editor> <booktitle> Logic Programming: Proc. Seventh Int. Conf., </booktitle> <pages> 174-185. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: This paper continues our work on the implementation of CLP (R) [6, 5, 9, 4] and on preliminary studies of each of the optimizations considered here [7, 5, 12, 9, 11]. Our compiler is also related to experimental compilers for Prolog, which also make use of global program analysis <ref> [17, 15, 13] </ref>. These too have given rise to good performance improvements. However these compilers are quite simple when compared to our compiler as they apply only one or two non-interacting optimizations and only use simple non-incremental program analysis.
Reference: [16] <author> V. Ramachandran and P. Van Hentenryck. </author> <title> Source to source optimizations of CLP(Lin). </title> <type> Technical Report. </type> <institution> Brown University, </institution> <year> 1995. </year>
Reference-contexts: Also the scope for performance improvement is greater in CLP languages because of the high cost of constraint solving. Apart from our earlier work in [4], the only other work on optimizing compilation of CLP languages is that of <ref> [16] </ref> describing work in-progress on a compiler for CLP (Lin). CLP (Lin) is essentially CLP (R) without term constraints or delayed non-linear arithmetic constraints. Their compiler uses constraint reordering, constraint removal and solver bypass (called refinement). <p> Their compiler uses constraint reordering, constraint removal and solver bypass (called refinement). They do not consider nofail or redundant variable removal optimizations nor do they appear to perform multi-variant specialization. It was developed independently and concurrently with the current version of our compiler. Although <ref> [16] </ref> gives some experimental evaluation of the compiler it does not evaluate the effectiveness of each optimization in isolation and it is difficult to judge the practicality or effectiveness of their compiler because of the small number and size of their benchmarks used in the evaluation.
Reference: [17] <author> P. Van Roy and A. Despain. </author> <title> The benefits of global dataflow analysis for an optimizing Prolog compiler. </title> <editor> In S. Debray and M. Hermenegildo, editors, </editor> <booktitle> Logic Programming: Proc. North American Conf. </booktitle> <year> 1990, </year> <pages> 501-515. </pages> <publisher> MIT Press, </publisher> <year> 1990. </year> <month> 16 </month>
Reference-contexts: This paper continues our work on the implementation of CLP (R) [6, 5, 9, 4] and on preliminary studies of each of the optimizations considered here [7, 5, 12, 9, 11]. Our compiler is also related to experimental compilers for Prolog, which also make use of global program analysis <ref> [17, 15, 13] </ref>. These too have given rise to good performance improvements. However these compilers are quite simple when compared to our compiler as they apply only one or two non-interacting optimizations and only use simple non-incremental program analysis.
References-found: 17

