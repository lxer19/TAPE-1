URL: http://www.cs.cornell.edu/Info/People/skalak/ml94-header.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/skalak/
Root-URL: 
Email: skalak@cs.umass.edu  
Title: Prototype and Feature Selection by Sampling and Random Mutation Hill Climbing Algorithms  
Author: David B. Skalak 
Address: Amherst, MA 01003  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: With the goal of reducing computational costs without sacrificing accuracy, we describe two algorithms to find sets of prototypes for nearest neighbor classification. Here, the term prototypes refers to the reference instances used in a nearest neighbor computation the instances with respect to which similarity is assessed in order to assign a class to a new data item. Both algorithms rely on stochastic techniques to search the space of sets of prototypes and are simple to implement. The first is a Monte Carlo sampling algorithm; the second applies random mutation hill climbing. On four datasets we show that only three or four prototypes sufficed to give predictive accuracy equal or superior to a basic nearest neighbor algorithm whose run-time storage costs were approximately 10 to 200 times greater. We briefly investigate how random mutation hill climbing may be applied to select features and prototypes simultaneously. Finally, we explain the performance of the sampling algorithm on these datasets in terms of a statistical measure of the extent of clustering displayed by the target classes.
Abstract-found: 1
Intro-found: 1
Reference: [Abelson et al., 1985] <author> Abelson, H.; Sussman, G.; and Sussman, J. </author> <year> 1985. </year> <title> Structure and Interpretation of Computer Programs. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: In particular, the results on the Cleveland database appear to be better than previously published results with only a very small percentage of stored instances used 3 . 2 To accelerate the algorithm, the calculated fitness of each prototype set was cached by memoizing <ref> [ Abelson et al., 1985 ] </ref> the evaluation function so that the predictive accuracy of a set of prototypes (and features) need only be computed once for each fold of the cross validation. Nonetheless, retrievals of previously cached evaluations are counted in the number evaluations reported. 3 D.
Reference: [Aha, 1990] <author> Aha, D. W. </author> <year> 1990. </year> <title> A Study of Instance-Based Algorithms for Supervised Learning Tasks: Mathemat ical, Empirical, and Psychological Evaluations. </title> <type> Ph.D. Dissertation, </type> <institution> Dept. of Information and Computer Science, University of California, Irvine. </institution>
Reference-contexts: Approaches to the problem have included storing misclassified instances (e.g., the Condensed Nearest Neighbor algorithm [ Hart, 1968 ] , the Reduced Nearest Neighbor algorithm [ Gates, 1972 ] , IB2 <ref> [ Aha, 1990 ] </ref> ); storing typical instances [ Zhang, 1992 ] ; storing only training instances that have been correctly classified by other training instances [ Wilson, 1972 ] ; exploiting domain knowledge [ Kurtzberg, 1987 ] ; and combining these techniques [ Voisin and Devijver, 1987 ] . <p> Other systems deal with reference selection by storing averages or abstractions of instances. For discussions of these approaches, see <ref> [ Aha, 1990 ] </ref> . Aha's research [ 1990 ] has shown that classification accuracy can be improved by limiting the number of prototypes and by weighting features. <p> approaches, see <ref> [ Aha, 1990 ] </ref> . Aha's research [ 1990 ] has shown that classification accuracy can be improved by limiting the number of prototypes and by weighting features. See for example the comparable or superior performance of Aha's instance-filtering algorithm IB3 over the baseline nearest neighbor algorithm IB1 [ Aha, 1990 ] . Aha's IB4 algorithm also improves performance by learning attribute relevance weights, resulting in a substantial increase in classification accuracy in some domains with irrelevant attributes.
Reference: [Aha, 1992] <author> Aha, D. W. </author> <year> 1992. </year> <title> Generalizing from Case Studies: A Case Study. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <address> Aberdeen, Scotland. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 1-10. </pages>
Reference-contexts: One direction for future research would be to characterize the databases for which these very simple Monte Carlo or RMHC approaches will work. (For research that attempts to characterize the qualities of different data sets, see <ref> [ Quinlan, 1993; Aha, 1992; Rendell and Cho, 1990 ] </ref> .) We take some initial steps in this direction in the following section. 4 When will Monte Carlo sampling work? It is clear that such naive sampling techniques will not always work, although their limits need to be determined experimentally and
Reference: [Almuallim and Dietterich, 1991] <author> Almuallim, H. and Di-etterich, T.G. </author> <year> 1991. </year> <title> Learning with Many Irrelevant Features. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher> <pages> 547-552. </pages>
Reference: [Bareiss, 1989] <author> Bareiss, E. R. </author> <year> 1989. </year> <title> Exemplar-Based Knowledge Acquisition. </title> <publisher> Academic Press, </publisher> <address> Boston, MA. </address>
Reference-contexts: While Holte used decision trees for concept representation and applied several simple inductive learning algorithms, we instead use sets of prototypical instances as partial concept descriptions for a simple nearest neighbor classification algorithm. Protos <ref> [ Bareiss, 1989 ] </ref> is a good example of a case-based reasoning system that relies on case prototypes for classification. An exemplar that is successfully matched to a problem has its prototypicality rating increased.
Reference: [Calinski and Harabasz, 1974] <author> Calinski, T. and Harabasz, J. </author> <year> 1974. </year> <title> A Dendrite Method for Cluster Analysis. </title> <journal> Communications in Statistics 3 </journal> <pages> 1-27. </pages>
Reference-contexts: In an empirical examination of stopping rules, the Calinski-Harabasz Index (sometimes, the Index) was the best performing of 30 procedures <ref> [ Milligan and Cooper, 1985; Calinski and Harabasz, 1974 ] </ref> . The Index is defined as [trace B=(k 1)]=[trace W=(n k)] where n is the total number of data instances and k is the number of clusters in a possible clustering 6 .
Reference: [Caruana and Freitag, 1994] <author> Caruana, Rich and Freitag, </author> <month> Dayne </month> <year> 1994. </year> <title> Greedy Attribute Selection. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Cognitive Systems, Inc., 1990] <institution> Cognitive Systems, Inc., </institution> <year> 1990. </year> <title> Case-Based Retrieval Shell, User's Manual V. 3.17. Cognitive Systems, </title> <publisher> Inc. </publisher>
Reference-contexts: Scaling in this way is designed to limit the effect of outlying values. The ReMind case-based reasoning development shell has previously incorporated a similar data pre-processing method <ref> [ Cognitive Systems, Inc., 1990 ] </ref> . In the pre-processing step, missing feature values are (naively) instantiated with the median value of that feature across all instances. To compute the similarity distance between two scaled instances, we use the Manhattan (city block or L 1 ) distance metric. <p> The pro-totypicality rating is used to determine the order in which exemplars are selected for further, knowledge-based pattern matching. Protos is an intelligent classification assistant and the prototypicality rating may be increased based in part on the actions of the supervising teacher. The ReMind case-based reasoning development shell <ref> [ Cognitive Systems, Inc., 1990 ] </ref> also incorporates a facility for the user to create prototypes to further index a case base.
Reference: [Dasarathy, 1991] <author> Dasarathy, Belur V. </author> <year> 1991. </year> <title> Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques. </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA. </address>
Reference-contexts: Reducing the number of instances used for nearest neighbor retrieval has been researched by the pattern recognition and instance-based learning communities for some time, where it is sometimes called the reference selection problem <ref> [ Dasarathy, 1991 ] </ref> . From one research perspective, this problem is a part of the general learning problem of determining the effect of the number and quality of training instances on predictive accuracy.
Reference: [DeJong and Spears, 1991] <author> DeJong, K. A. and Spears, W. M. </author> <year> 1991. </year> <title> Learning Concept Classification Rules Using Genetic Algorithms. </title> <booktitle> In 12th International Joint Conference on Artificial Intelligence, </booktitle> <address> Sydney, Australia. </address> <booktitle> International Joint Conferences on Artificial Intelligence. </booktitle> <pages> 651-656. </pages>
Reference: [Duda and Hart, 1973] <author> Duda, R. O. and Hart, P. E. </author> <year> 1973. </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley, </publisher> <address> New York. </address>
Reference-contexts: Finally, we offer evidence that the degree of clustering of the data set is a factor in determining how well our simple sampling algorithm will work. 1.1 The nearest neighbor algorithm To determine the classification accuracy of a set of prototypes, a 1-nearest neighbor classification algorithm is used <ref> [ Duda and Hart, 1973 ] </ref> . The similarity function used in this nearest neighbor computation is straightforward and relies on equally-weighted features. In a pre-processing step, all feature values are linearly scaled from 0 to 100.
Reference: [Gates, 1972] <author> Gates, G. W. </author> <year> 1972. </year> <title> The Reduced Nearest Neighbor Rule. </title> <journal> IEEE Transactions on Information Theory 431-433. </journal>
Reference-contexts: Approaches to the problem have included storing misclassified instances (e.g., the Condensed Nearest Neighbor algorithm [ Hart, 1968 ] , the Reduced Nearest Neighbor algorithm <ref> [ Gates, 1972 ] </ref> , IB2 [ Aha, 1990 ] ); storing typical instances [ Zhang, 1992 ] ; storing only training instances that have been correctly classified by other training instances [ Wilson, 1972 ] ; exploiting domain knowledge [ Kurtzberg, 1987 ] ; and combining these techniques [ Voisin
Reference: [Hart, 1968] <author> Hart, P. E. </author> <year> 1968. </year> <title> The Condensed Nearest Neighbor Rule. </title> <journal> IEEE Transactions on Information Theory (Corresp.) IT-14:515-516. </journal>
Reference-contexts: From one research perspective, this problem is a part of the general learning problem of determining the effect of the number and quality of training instances on predictive accuracy. Approaches to the problem have included storing misclassified instances (e.g., the Condensed Nearest Neighbor algorithm <ref> [ Hart, 1968 ] </ref> , the Reduced Nearest Neighbor algorithm [ Gates, 1972 ] , IB2 [ Aha, 1990 ] ); storing typical instances [ Zhang, 1992 ] ; storing only training instances that have been correctly classified by other training instances [ Wilson, 1972 ] ; exploiting domain knowledge [
Reference: [Holte, 1993] <author> Holte, R. C. </author> <year> 1993. </year> <title> Very Simple Classification Rules Perform Well on Most Commonly Used Datasets. </title> <booktitle> Machine Learning 11 </booktitle> <pages> 63-90. </pages>
Reference: [John et al., 1994] <author> John, George; Kohavi, Ron; and Pfleger, </author> <title> Karl 1994. Irrelevant Features and the Subset Selection Problem. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Kelly and Davis, 1991] <author> Kelly, J. D. J. and Davis, L. </author> <year> 1991. </year> <title> Hybridizing the Genetic Algorithm and the K Nearest Neighbors Classification Algorithm. </title> <booktitle> In Proceedings, Fourth International Conference on Genetic Algorithms, </booktitle> <address> San Diego, CA. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <pages> 377-383. </pages>
Reference: [Kira and Rendell, 1992] <author> Kira, Kenji and Rendell, Larry A. </author> <year> 1992. </year> <title> The Feature Selection Problem: Traditional Methods and a New Algorithm. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher> <pages> 129-134. </pages>
Reference: [Kurtzberg, 1987] <author> Kurtzberg, J. M. </author> <year> 1987. </year> <title> Feature analysis for symbol recognition by elastic matching. </title> <booktitle> International Business Machines Journal of Research and Development 31 </booktitle> <pages> 91-95. </pages>
Reference-contexts: [ Hart, 1968 ] , the Reduced Nearest Neighbor algorithm [ Gates, 1972 ] , IB2 [ Aha, 1990 ] ); storing typical instances [ Zhang, 1992 ] ; storing only training instances that have been correctly classified by other training instances [ Wilson, 1972 ] ; exploiting domain knowledge <ref> [ Kurtzberg, 1987 ] </ref> ; and combining these techniques [ Voisin and Devijver, 1987 ] . Other systems deal with reference selection by storing averages or abstractions of instances. For discussions of these approaches, see [ Aha, 1990 ] .
Reference: [Langley and Sage, 1994] <author> Langley, P. and Sage, S. </author> <year> 1994. </year> <title> Oblivious Decision Trees and Abstract Cases. </title> <booktitle> In Proceedings of the AAAI-94 Case-Based Reasoning Workshop (to appear), </booktitle> <address> Seattle, WA. </address> <booktitle> American Association for Artificial Intelligence, </booktitle> <address> Menlo Park, CA. </address>
Reference: [Milligan and Cooper, 1985] <author> Milligan, G. W. and Cooper, M. C. </author> <year> 1985. </year> <title> An Examination of Procedures for Determining the Number of Clusters in a Data set. </title> <booktitle> Psychome-trika 50 </booktitle> <pages> 159-179. </pages>
Reference-contexts: Characterizing clusters and determining the optimal number of clusters in a data set are classical problems, and many indicators have been proposed, usually under the heading stopping rules, used to determine where to terminate a hierarchical clustering algorithm <ref> [ Milligan and Cooper, 1985 ] </ref> . In an empirical examination of stopping rules, the Calinski-Harabasz Index (sometimes, the Index) was the best performing of 30 procedures [ Milligan and Cooper, 1985; Calinski and Harabasz, 1974 ] . <p> In an empirical examination of stopping rules, the Calinski-Harabasz Index (sometimes, the Index) was the best performing of 30 procedures <ref> [ Milligan and Cooper, 1985; Calinski and Harabasz, 1974 ] </ref> . The Index is defined as [trace B=(k 1)]=[trace W=(n k)] where n is the total number of data instances and k is the number of clusters in a possible clustering 6 .
Reference: [Mitchell and Holland, 1993] <author> Mitchell, M. and Holland, J. H. </author> <year> 1993. </year> <title> When Will a Genetic Algorithm Outperform Hill-Climbing? Technical report, </title> <institution> Santa Fe Institute. </institution>
Reference-contexts: Two algorithms are applied to select prototypes and features used in a nearest neighbor algorithm: (1) a Monte Carlo technique, which chooses the most accurate of a sample of random prototype sets; and (2) a random mutation hill climbing algorithm (e.g., <ref> [ Mitchell and Holland, 1993 ] </ref> ), which searches for sets of prototypes with demonstrably good classification power. In previous work we described a genetic algorithm approach to finding prototypes [ Skalak, 1993 ] .
Reference: [Moore and Lee, 1994] <author> Moore, Andrew W. and Lee, Mary S. </author> <year> 1994. </year> <title> Efficient Algorithms for Minimizing Cross Validation Error. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Murphy and Aha, 1994] <author> Murphy, P. M. and Aha, D. W. </author> <year> 1994. </year> <title> UCI repository of machine learning databases. For information contact ml-repository@ics.uci.edu. </title>
Reference-contexts: As usual, an instance is considered correctly classified by a prototype set if the instance's class equals the class of the prototype that is its 1-nearest neighbor taken from the prototype set. 1.2 Baseline storage requirements and classification accuracy Four databases from the UCI machine learning repository <ref> [ Murphy and Aha, 1994 ] </ref> were used: Iris, Cleveland Heart Disease (binary classes), Breast Cancer Ljubljana, and Soybean (small database). <p> An average of 7.6 features of 13 were used for classification in the Cleveland data set. The ca feature was used in all five partitions; cp, exang, and thal were applied in four; restecg in none. Descriptions of these features may be found in <ref> [ Murphy and Aha, 1994 ] </ref> . Table 5: Number of features in the original instance representation and average number features selected by RMHC-PF1.
Reference: [Papadimitriou and Steiglitz, 1982] <author> Papadimitriou, C.H. and Steiglitz, K. </author> <year> 1982. </year> <title> Combinatorial Optimization: Algorithms and Complexity. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference-contexts: An approach to prototype selection based on random mutation search is described next. 2.2 Random mutation hill climbing 2.2.1 The algorithm (RMHC) Random mutation hill climbing is a local search method that has a stochastic component <ref> [ Papadimitriou and Steiglitz, 1982 ] </ref> . The basic random mutation hill climbing algorithm (RMHC) is as described by Mitchell and Holland [ 1993 ] : 1. Choose a binary string at random. Call this string best-evaluated. 2. Mutate a bit chosen at random in best-evaluated. 3.
Reference: [Quinlan, 1993] <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: For general reference, since C4.5 <ref> [ Quinlan, 1993 ] </ref> is a benchmark learning algorithm, we also include in Table 1 classification accuracies on the four data sets from our own five-fold cross validation runs using pruned trees generated by C4.5 with its default option settings. <p> One direction for future research would be to characterize the databases for which these very simple Monte Carlo or RMHC approaches will work. (For research that attempts to characterize the qualities of different data sets, see <ref> [ Quinlan, 1993; Aha, 1992; Rendell and Cho, 1990 ] </ref> .) We take some initial steps in this direction in the following section. 4 When will Monte Carlo sampling work? It is clear that such naive sampling techniques will not always work, although their limits need to be determined experimentally and
Reference: [Rendell and Cho, 1990] <author> Rendell, L. and Cho, H. </author> <year> 1990. </year> <title> Empirical Learning as a Function of Concept Character. </title> <booktitle> Machine Learning 5 </booktitle> <pages> 267-298. </pages>
Reference-contexts: One direction for future research would be to characterize the databases for which these very simple Monte Carlo or RMHC approaches will work. (For research that attempts to characterize the qualities of different data sets, see <ref> [ Quinlan, 1993; Aha, 1992; Rendell and Cho, 1990 ] </ref> .) We take some initial steps in this direction in the following section. 4 When will Monte Carlo sampling work? It is clear that such naive sampling techniques will not always work, although their limits need to be determined experimentally and
Reference: [Schaffer, 1993] <author> Schaffer, C. </author> <year> 1993. </year> <title> Overfitting Avoidance as Bias. </title> <booktitle> Machine Learning 10 </booktitle> <pages> 153-178. </pages>
Reference-contexts: This second hypothesis may be tantamount to the assumption that a learning bias in favor of simpler models is appropriate for the data sets we use <ref> [ Schaffer, 1993 ] </ref> . Many previous approaches to selecting prototypes are instance-filtering techniques, where each member of the data set is examined in turn and some screen is used to sift the elements that should be retained in the emerging concept description.
Reference: [Skalak, 1993] <author> Skalak, D. B. </author> <year> 1993. </year> <title> Using a Genetic Algorithm to Learn Prototypes for Case Retrieval and Classification. </title> <booktitle> In Proceedings of the AAAI-93 Case-Based Reasoning Workshop (Technical Report WS-93-01), Wash-ington, D.C. American Association for Artificial Intelligence, </booktitle> <address> Menlo Park, CA. </address>
Reference-contexts: In previous work we described a genetic algorithm approach to finding prototypes <ref> [ Skalak, 1993 ] </ref> . This paper follows the theme of using adaptive search techniques to find sets of prototypes, but uses somewhat less computationally intensive algorithms to locate them.
Reference: [Sobol', 1974] <author> Sobol', I. M. </author> <year> 1974. </year> <title> The Monte Carlo Method. </title> <publisher> The University of Chicago Press, </publisher> <address> Chicago, IL. </address>
Reference-contexts: The results of independent trials are combined in some fashion, usually averaged <ref> [ Sobol', 1974 ] </ref> . The method has been applied to many problems in such domains as numerical integration, statistical physics, quality control and particle physics.
Reference: [Tan and Schlimmer, 1990] <author> Tan, M. and Schlimmer, J. C. </author> <year> 1990. </year> <title> Two Case Studies in Cost-Sensitive Concept Acquisition. </title> <booktitle> In Proceedings, Eighth National Conference on Artificial Intelligence, </booktitle> <address> Boston, MA. </address> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA. </address> <pages> 854-860. </pages>
Reference: [Vafaie and DeJong, 1992] <author> Vafaie, H. and DeJong, K. </author> <year> 1992. </year> <title> Genetic Algorithms as a Tool for Feature Selection in Machine Learning. </title> <booktitle> In Proceedings of the 1992 IEEE Int. Conf. on Tools with AI, </booktitle> <address> Arlington, VA (Nov. 1992). </address> <publisher> IEEE. </publisher> <pages> 200-203. </pages>
Reference: [Voisin and Devijver, 1987] <author> Voisin, J. and Devijver, P. A. </author> <year> 1987. </year> <title> An application of the Multiedit-Condensing technique to the reference selection problem in a print recognition system. </title> <booktitle> Pattern Recognition 5 </booktitle> <pages> 465-474. </pages>
Reference-contexts: algorithm [ Gates, 1972 ] , IB2 [ Aha, 1990 ] ); storing typical instances [ Zhang, 1992 ] ; storing only training instances that have been correctly classified by other training instances [ Wilson, 1972 ] ; exploiting domain knowledge [ Kurtzberg, 1987 ] ; and combining these techniques <ref> [ Voisin and Devijver, 1987 ] </ref> . Other systems deal with reference selection by storing averages or abstractions of instances. For discussions of these approaches, see [ Aha, 1990 ] .
Reference: [Wilson, 1972] <author> Wilson, D. </author> <year> 1972. </year> <title> Asymptotic Properties of Nearest Neighbor Rules using Edited Data. </title> <journal> Institute of Electrical and Electronic Engineers Transactions on Systems, Man and Cybernetics 2 </journal> <pages> 408-421. </pages>
Reference-contexts: misclassified instances (e.g., the Condensed Nearest Neighbor algorithm [ Hart, 1968 ] , the Reduced Nearest Neighbor algorithm [ Gates, 1972 ] , IB2 [ Aha, 1990 ] ); storing typical instances [ Zhang, 1992 ] ; storing only training instances that have been correctly classified by other training instances <ref> [ Wilson, 1972 ] </ref> ; exploiting domain knowledge [ Kurtzberg, 1987 ] ; and combining these techniques [ Voisin and Devijver, 1987 ] . Other systems deal with reference selection by storing averages or abstractions of instances. For discussions of these approaches, see [ Aha, 1990 ] .
Reference: [Zhang, 1992] <author> Zhang, J. </author> <year> 1992. </year> <title> Selecting Typical Instances in Instance-Based Learning. </title> <booktitle> In Proceedings of the Ninth International Machine Learning Workshop, </booktitle> <address> Aberdeen, Scotland. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <pages> 470-479. </pages>
Reference-contexts: Approaches to the problem have included storing misclassified instances (e.g., the Condensed Nearest Neighbor algorithm [ Hart, 1968 ] , the Reduced Nearest Neighbor algorithm [ Gates, 1972 ] , IB2 [ Aha, 1990 ] ); storing typical instances <ref> [ Zhang, 1992 ] </ref> ; storing only training instances that have been correctly classified by other training instances [ Wilson, 1972 ] ; exploiting domain knowledge [ Kurtzberg, 1987 ] ; and combining these techniques [ Voisin and Devijver, 1987 ] .
References-found: 34

