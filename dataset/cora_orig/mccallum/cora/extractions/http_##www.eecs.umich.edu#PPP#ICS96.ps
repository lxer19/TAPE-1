URL: http://www.eecs.umich.edu/PPP/ICS96.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Email: ktomko@cs.wright.edu davidson@eecs.umich.edu  
Title: Profile Driven Weighted Decomposition  
Author: Karen A. Tomko Edward S. Davidson 
Address: Dayton, OH 45435 Ann Arbor, MI 48109-2122  
Affiliation: Department of Computer Science Department of Electrical Engineering and Engineering and Computer Science Wright State University University of Michigan  
Abstract: Many large-scale computational problems are based on irregular (or unstructured) domains. Some examples are finite element methods in structural analysis, finite volume methods in fluid dynamics, and circuit simulation for VLSI design. Domain decomposition is a common technique for distributing the data and work of irregular scientific applications across a distributed memory parallel machine. To obtain efficiency, the subdomains must be constructed such that the work is divided with a reasonable balance among the processors while the subdomain boundary is kept small. Application and machine specific information can be used in conjunction with domain decomposition to achieve a level of performance not possible with traditional domain decomposition methods. Application profiling characterizes the performance of an application on a specific machine. We present a method that uses curve-fitting of application profile data to calculate vertex and edge weights for use with weighted graph decomposition algorithms. We demonstrate its potential on two routines from a production finite element application running on the IBM SP2. Our method combined with a multilevel partitioning algorithm reduced load imbalance from 52% to less than 10% for one routine in our study. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dan Anderson, Alex Akkerman, and Dave Strenski. FCRASH, </author> <title> optimization of an explicit FEA based crash simulation code for the Cray C916/16-51. </title> <booktitle> In Cray User Group Conference, </booktitle> <address> Denver, CO., </address> <month> March, </month> <year> 1995. </year>
Reference-contexts: It is a production application consisting of hundreds of thousands of lines of code. A general discussion of the application and the code development effort can be found in <ref> [1] </ref>. We choose two very different subroutines from FCRASH to analyze. The first routine, subroutine 1, is fairly well balanced under our default decomposition scheme, unweighted partitioning of elements of the FE mesh.
Reference: [2] <author> Stephen Barnard and Horst Simon. </author> <title> A fast multilevel implementation of recursive spectral bisections for partitioning unstructured problems. </title> <type> Technical Report RNR-92-33, </type> <institution> NASA Ames Research Center, NAS Systems Division, </institution> <month> April </month> <year> 1993. </year>
Reference: [3] <institution> CONVEX Computer Corporation. </institution> <note> CXpa Reference, second edition. </note>
Reference-contexts: The GIST tool from BBN allows user specified events to be monitored. CXpa from CONVEX extends gprof type profiling to a parallel computing environment. It provides loop iteration or routine execution counts, wall clock time, CPU time, and cache miss counts for each thread of a parallel application <ref> [3] </ref>. Similarly, PMON on the KSR1 and KSR2 provides CPU time, and cache miss data on a per thread basis. We make use of standard parallel profile information to guide parallel program optimizations.
Reference: [4] <author> Karen D. Devine, Joseph E. Flaherty, Stephen R. Wheat, and Arthur B. Maccabe. </author> <title> A massively parallel adaptive finite element method with dynamic load balancing. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 2-11, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Adaptive decomposition techniques also provide a means for load balancing heterogeneous applications and have been developed for use with applications which have dynamic run-time behavior. For example, researchers have used adaptive decomposition for Direct Simulation Monte Carlo applications [12], N-body Methods [15] and Adaptive Finite 1 Element methods <ref> [4] </ref>. These adaptive algorithms periodi-cally redistribute data in order to maintain load balance. Some implementations perform completely new decompositions at each rebalancing step while others simply adjust the boundaries between subdomains. We provide methods for balancing heterogeneous domains statically. <p> This information specifies the graph. The main loop structure of our example application, Figure 1, consists of an update of the matrix A. The calculation 2 integer array A mask <ref> [4; 4] </ref> real array A [4; 4] integer i; j do i 1 to 4 if (A mask (i; j) = 1) then A (i; j) F (A (i; j); A (i 1; j); A (i + 1; j); else if (a mask (i; j) = 2) then A (i; j) <p> This information specifies the graph. The main loop structure of our example application, Figure 1, consists of an update of the matrix A. The calculation 2 integer array A mask <ref> [4; 4] </ref> real array A [4; 4] integer i; j do i 1 to 4 if (A mask (i; j) = 1) then A (i; j) F (A (i; j); A (i 1; j); A (i + 1; j); else if (a mask (i; j) = 2) then A (i; j) G (A (i; j); A
Reference: [5] <author> T. Agerwala et al. </author> <title> SP2 system architecture. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 152-184, </pages> <year> 1995. </year>
Reference-contexts: The message-passing version of FCRASH running on the SP2 uses the IBM Message Passing Library (MPL) communication primitives. The SP2 system architecture is discussed at length in <ref> [5] </ref> and the MPL communication software is discussed in [16]. There are several steps in our experimental setup as diagramed in Figure 4. We perform domain decomposition on sample input files prior to running the FCRASH application, thus a separate partitioned input file is generated for each decomposition.
Reference: [6] <author> Charbel Farhat and Michel Lesoinne. </author> <title> Automatic partitioning of unstructured meshes for the parallel solution of problems in computational mechanics. </title> <journal> International Journal for Numerical Methods in Engineering, </journal> <volume> 36 </volume> <pages> 745-764, </pages> <year> 1993. </year>
Reference-contexts: We use domain decomposition algorithms to partition data structures of irregular applications. References [14], <ref> [6] </ref> and [11] provide comparisons of several proposed decomposition algorithms. Recently, very fast multilevel algorithms which use either spectral or greedy bisection algorithms have become available [9], [10].
Reference: [7] <author> M. Garey, D. Johnson, and L. Stockmeyer. </author> <title> Some simplified NP-complete graph problems. </title> <journal> Theoretical Computer Science, </journal> <volume> 1(3) </volume> <pages> 237-267, </pages> <year> 1976. </year>
Reference-contexts: Weighted graph partitioning is an NP-complete problem <ref> [7] </ref>, but there are several efficient heuristic algorithms available. Two prominent algorithms in the literature are spectral partitioning and greedy partitioning, both have been used in multilevel partitioning approaches that speed up partitioning for large graphs. We use multilevel spectral-based partitioning algorithms for the experiments presented in this paper.
Reference: [8] <author> Bruce Hendrickson and Robert Leland. </author> <title> The Chaco user's guide: Version 2.0. </title> <type> Technical Report SAND94-2692, </type> <institution> Sandia National Laboratories, </institution> <address> Albuquerque, NM, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: We perform domain decomposition on sample input files prior to running the FCRASH application, thus a separate partitioned input file is generated for each decomposition. For the experiments described in this paper we used the multilevel spectral-based algorithm as implemented in Chaco V1.0 <ref> [8] </ref> to decompose the input files. The scripts for converting input file formats and sifting through profile data were developed in Perl.
Reference: [9] <author> Bruce Hendrickson and Robert Leland. </author> <title> A multilevel algorithm for partitioning graphs. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <year> 1995. </year> <note> to appear in. </note>
Reference-contexts: We use domain decomposition algorithms to partition data structures of irregular applications. References [14], [6] and [11] provide comparisons of several proposed decomposition algorithms. Recently, very fast multilevel algorithms which use either spectral or greedy bisection algorithms have become available <ref> [9] </ref>, [10]. In this paper we give a method for determining reasonable input graph weights which we use in conjunction with a spectral-based multilevel decomposition algorithm. Like weighted domain decomposition, scatter decomposition provides a static means of load balancing heterogeneous domains. <p> This approach is used in many implementations and provides partitions which are as good or better than the partitions given by traditional single level algorithms <ref> [9] </ref>, [10]. 4 Profile driven approach to weight determination We have developed a method for empirically determining reasonable vertex and edge weights for applications with heterogeneous domains. The method is based on a simple linear model of application performance.
Reference: [10] <author> George Karypis and Vipin Kumar. </author> <title> A fast and high quality multilevel scheme for partitioning irregular graphs. </title> <type> Technical Report TR 95-035, </type> <institution> Dept. Computer Science, University of Minnesota, </institution> <year> 1995. </year>
Reference-contexts: We use domain decomposition algorithms to partition data structures of irregular applications. References [14], [6] and [11] provide comparisons of several proposed decomposition algorithms. Recently, very fast multilevel algorithms which use either spectral or greedy bisection algorithms have become available [9], <ref> [10] </ref>. In this paper we give a method for determining reasonable input graph weights which we use in conjunction with a spectral-based multilevel decomposition algorithm. Like weighted domain decomposition, scatter decomposition provides a static means of load balancing heterogeneous domains. <p> This approach is used in many implementations and provides partitions which are as good or better than the partitions given by traditional single level algorithms [9], <ref> [10] </ref>. 4 Profile driven approach to weight determination We have developed a method for empirically determining reasonable vertex and edge weights for applications with heterogeneous domains. The method is based on a simple linear model of application performance.
Reference: [11] <author> Robert Leland and Bruce Hendrickson. </author> <title> An empirical study of static load balancing algorithms. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 682-685, </pages> <year> 1994. </year>
Reference-contexts: We use domain decomposition algorithms to partition data structures of irregular applications. References [14], [6] and <ref> [11] </ref> provide comparisons of several proposed decomposition algorithms. Recently, very fast multilevel algorithms which use either spectral or greedy bisection algorithms have become available [9], [10]. In this paper we give a method for determining reasonable input graph weights which we use in conjunction with a spectral-based multilevel decomposition algorithm.
Reference: [12] <author> Bongki Moon and Joel Saltz. </author> <title> Adaptive runtime support for Direct Simulation Monte Carlo Methods on distributed memory architectures. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 176-183, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Adaptive decomposition techniques also provide a means for load balancing heterogeneous applications and have been developed for use with applications which have dynamic run-time behavior. For example, researchers have used adaptive decomposition for Direct Simulation Monte Carlo applications <ref> [12] </ref>, N-body Methods [15] and Adaptive Finite 1 Element methods [4]. These adaptive algorithms periodi-cally redistribute data in order to maintain load balance. Some implementations perform completely new decompositions at each rebalancing step while others simply adjust the boundaries between subdomains. We provide methods for balancing heterogeneous domains statically.
Reference: [13] <author> David M. Nicol and Joel H. Saltz. </author> <title> An analysis of scatter decomposition. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(11) </volume> <pages> 1337-1345, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: In this paper we give a method for determining reasonable input graph weights which we use in conjunction with a spectral-based multilevel decomposition algorithm. Like weighted domain decomposition, scatter decomposition provides a static means of load balancing heterogeneous domains. This technique has been analyzed by Nicol and Saltz in <ref> [13] </ref>. The main drawback of the method is that each processor is assigned several small disjoint regions of the domain which may increase the communication requirements of the application. We, instead, use weighted decomposition algorithms which attempt to provide connected subdomains to each processor.
Reference: [14] <author> Horst Simon. </author> <title> Partitioning of unstructured problems for parallel processing. </title> <journal> Computing Systems in Engineering, </journal> 2(2/3):135-148, 1991. 
Reference-contexts: We use domain decomposition algorithms to partition data structures of irregular applications. References <ref> [14] </ref>, [6] and [11] provide comparisons of several proposed decomposition algorithms. Recently, very fast multilevel algorithms which use either spectral or greedy bisection algorithms have become available [9], [10].
Reference: [15] <author> Jaswinder Pal Singh, Chris Holt, Takashi Totsuka, Anoop Gupta, and John L. Hennessy. </author> <title> Load balancing and data locality in adaptive hierarchical n-body methods: Barnes-hut, fast multipole, and radiosity. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> June </month> <year> 1995. </year>
Reference-contexts: Adaptive decomposition techniques also provide a means for load balancing heterogeneous applications and have been developed for use with applications which have dynamic run-time behavior. For example, researchers have used adaptive decomposition for Direct Simulation Monte Carlo applications [12], N-body Methods <ref> [15] </ref> and Adaptive Finite 1 Element methods [4]. These adaptive algorithms periodi-cally redistribute data in order to maintain load balance. Some implementations perform completely new decompositions at each rebalancing step while others simply adjust the boundaries between subdomains. We provide methods for balancing heterogeneous domains statically.
Reference: [16] <author> M. Snir, P. Hochschild, D. D. Frye, and K. J. Gildea. </author> <title> The communication software and parallel environment of the IBM SP2. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 185-204, </pages> <year> 1995. </year>
Reference-contexts: The message-passing version of FCRASH running on the SP2 uses the IBM Message Passing Library (MPL) communication primitives. The SP2 system architecture is discussed at length in [5] and the MPL communication software is discussed in <ref> [16] </ref>. There are several steps in our experimental setup as diagramed in Figure 4. We perform domain decomposition on sample input files prior to running the FCRASH application, thus a separate partitioned input file is generated for each decomposition.
Reference: [17] <author> Karen A. Tomko. </author> <title> Domain Decomposition, Irregular Applications and Parallel Computers. </title> <type> PhD thesis, </type> <institution> Department of Computer Science and Engineering, University of Michigan, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: The scripts for converting input file formats and sifting through profile data were developed in Perl. We used the MAPLE (version VR3) mathematics program [18] to find the linear least-squares approximation when calculating weights (See <ref> [17] </ref> for more information). 5.1 Experimental Results: Subroutine 1 The main loop of subroutine 1 iterates over all of the elements of the FE mesh and performs some calculation on each element. The calculation that is performed depends on the type of element and the status of the element.
Reference: [18] <author> Waterloo Maple Software, </author> <title> Waterloo, Ontario, Canada. Maple V Language Reference Manual. </title> <type> 8 </type>
Reference-contexts: For the experiments described in this paper we used the multilevel spectral-based algorithm as implemented in Chaco V1.0 [8] to decompose the input files. The scripts for converting input file formats and sifting through profile data were developed in Perl. We used the MAPLE (version VR3) mathematics program <ref> [18] </ref> to find the linear least-squares approximation when calculating weights (See [17] for more information). 5.1 Experimental Results: Subroutine 1 The main loop of subroutine 1 iterates over all of the elements of the FE mesh and performs some calculation on each element.
References-found: 18

