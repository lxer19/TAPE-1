URL: http://www.cs.cmu.edu/afs/cs/project/theo-11/www/wwkb/ling-ie.ps.gz
Refering-URL: http://www.cs.cmu.edu/~WebKB/
Root-URL: 
Email: dayne@cs.cmu.edu  
Title: Toward General-Purpose Learning for Information Extraction  
Author: Dayne Freitag 
Note: Submission Number: #333 Submission Type: Regular paper Topic Areas: A5, R1 Author of Record: Dayne Freitag Under consideration for other conferences (specify)? None  
Date: January 26, 1998  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Two trends are evident in the recent evolution of the field of information extraction: a preference for simple, often corpus-driven techniques over linguistically sophisticated ones; and a broadening of the central problem definition to include many non-traditional text domains. This development calls for information extraction systems which are as retargetable and general as possible. Here, we describe SRV, a learning architecture for information extraction which is designed for maximum generality and flexibility. SRV can exploit domain-specific information, including linguistic syntax and lexical information, in the form of features provided to the system explicitly as input for training. This process is illustrated on a domain created from Reuters corporate acquisitions articles. Features are derived from two general-purpose NLP systems, Sleator and Temperly's link grammar parser and Wordnet. Experiments compare the learner's performance with and without such linguistic information. Surprisingly, in many cases, the system performs as well without this information as with it. 
Abstract-found: 1
Intro-found: 1
Reference: <author> D. E. Appelt, J. R. Hobbs, J. Bear, D. Israel, and M. Tyson. </author> <year> 1993. </year> <title> FASTUS: a finite-state processor for information extraction from real-world text. </title> <booktitle> In Proceedings of the Twelvth International Con-verence on Artificial Intelligence (IJCAI-93). </booktitle>
Reference-contexts: Progress in the field of IE has been away from general NLP systems, that must be tuned to work in a particular domain, toward faster systems that perform less linguistic processing of documents and can be more readily targeted at novel domains (e.g., <ref> (Appelt et al., 1993) </ref>). A natural part of this development has been the introduction of machine learning techniques to facilitate the domain engineering effort (Riloff and Shoen, 1995) (Soderland and Lehnert, 1994).
Reference: <author> M. E. Califf and R. J. Mooney. </author> <year> 1997. </year> <title> Relational learning of pattern-match rules for information extraction. </title> <booktitle> In Working Papers of ACL-97 Workshop on Natural Language Learning. </booktitle>
Reference-contexts: A natural part of this development has been the introduction of machine learning techniques to facilitate the domain engineering effort (Riloff and Shoen, 1995) (Soderland and Lehnert, 1994). Several researchers have reported IE systems which use machine learning at their core (Soder-land, 1996) <ref> (Califf and Mooney, 1997) </ref>. Rather than spend human effort tuning a system for an IE domain, it becomes possible to conceive of training it on a sample of documents from the domain. <p> In addition to domains characterized by grammatical prose, we should be able to perform information extraction in domains involving less traditional structure, such as netnews articles and World Wide Web pages. In this paper we introduce a machine learning architecture designed to satisfy these two desiderata. Like <ref> (Califf and Mooney, 1997) </ref>, we frame the task of information extraction as a relational learning problem. <p> This general notion of relational features con 2 Note that this is not the only reasonable representational choice. In another important learning system for IE, for instance, examples are complete, syntactically tagged sentences (Soderland, 1996). 2 trasts with the only other relational learner for IE <ref> (Califf and Mooney, 1997) </ref>, in which only the adjacency relation is exploited. 2.3 Rule Construction In contrast with the general learning framework, which allows inference for multiple classes simultaneously, our representation defines a two-class problem, and we are really only interested in recognizing one class: the class of field instances.
Reference: <author> P. Clark and T. Niblett. </author> <year> 1989. </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3(4) </volume> <pages> 261-263. </pages>
Reference-contexts: The paradigm of rule inference (e.g, <ref> (Clark and Niblett, 1989) </ref>), including relational inference, fits our framework well, since algorithms from this paradigm learn for one class at a time.
Reference: <author> D. Freitag. </author> <year> 1997. </year> <title> Using grammatical inference to improve precision in information extraction. In Working Papers of the ICML-97 Workshop on Automata Induction, Grammatical Inference, and Language Acquisition. </title>
Reference-contexts: Under the dubious assumption that all such occurrences are indepedent, these counts are used to estimate the probability that a test fragment is a field instance. Our implementation follows that described in <ref> (Freitag, 1997) </ref>. Note that although these learners are "sim-ple," they are not necessarily ineffective. We have experimented with them in several domains and have been surprised by their level of performance in some cases. 4 Results The results presented here represent average performances over several separate experiments.
Reference: <author> D. Lewis. </author> <year> 1992. </year> <title> Representation and Learning in Information Retrieval. </title> <type> Ph.D. thesis, </type> <institution> Computer Science Dept, Univ. of Massachusetts. </institution>
Reference-contexts: The experiments described here were an exercise in the design of features to capture syntactic and lexical information. 3.1 Domain As part of these experiments we defined an information extraction problem using a publicly available corpus. 600 articles were sampled from the "acquisition" set in the Reuters corpus <ref> (Lewis, 1992) </ref> and tagged to identify instances of nine fields.
Reference: <author> G.A. Miller. </author> <year> 1995. </year> <title> WordNet: A lexical database for English. </title> <journal> Communications of the ACM, </journal> <volume> 38(11) </volume> <pages> 39-41. </pages>
Reference-contexts: Figure 1 shows part of a link grammar parse and its translation into features. Wordnet is a lexicographic database of English designed according to current linguistic theories regarding the cognitive organization of language in people <ref> (Miller, 1995) </ref>. Our object in using Wordnet is to enable SRV to recognize that the phrases, "A bought B," and, "X acquired Y," are instantiations of the same underlying pattern.
Reference: <author> T. M. Mitchell. </author> <year> 1997. </year> <title> Machine Learning. </title> <publisher> The McGraw-Hill Companies, Inc. </publisher>
Reference-contexts: The other simple learner, which we will call Bayes, is a statistical "bag-of-words" approach, based on the "Naive Bayes" algorithm <ref> (Mitchell, 1997) </ref>. The training set is used to record the frequency of literal tokens occurring in and within a small number of tokens around a field. <p> Rather than the absolute difficulty of a field, we speak of the suitability of a learner's inductive bias for a field <ref> (Mitchell, 1997) </ref>. For example, even though Rote is a "strawman" approach, its memorization bias is largely appropriate for the status field, which is typically instantiated as a short stock phrase from a small collection of candidates.
Reference: <author> J. R. Quinlan. </author> <year> 1990. </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266. </pages>
Reference-contexts: In this paper we introduce a machine learning architecture designed to satisfy these two desiderata. Like (Califf and Mooney, 1997), we frame the task of information extraction as a relational learning problem. We describe the implementation of a learning algorithm similar in spirit to FOIL <ref> (Quinlan, 1990) </ref>, which takes as input a set of tagged documents, and a set 1 of features that control generalization, and produces rules that describe how to extract information from novel documents. <p> The paradigm of rule inference (e.g, (Clark and Niblett, 1989)), including relational inference, fits our framework well, since algorithms from this paradigm learn for one class at a time. Our primary inspiration for SRV is FOIL <ref> (Quinlan, 1990) </ref>, a relational rule learner, which, in addition to simple attribute-value tests, can exploit relational structure in the domain to derive "new" features.
Reference: <author> E. Riloff and J. Shoen. </author> <year> 1995. </year> <title> Automatically ac quiring conceptual patterns without an annotated corpus. </title> <booktitle> In Proceedings of the Third Workshop on Very Large Corpora. </booktitle>
Reference-contexts: A natural part of this development has been the introduction of machine learning techniques to facilitate the domain engineering effort <ref> (Riloff and Shoen, 1995) </ref> (Soderland and Lehnert, 1994). Several researchers have reported IE systems which use machine learning at their core (Soder-land, 1996) (Califf and Mooney, 1997).
Reference: <author> D. Sleator and D. Temperley. </author> <year> 1993. </year> <title> Parsing En glish with a link grammar. </title> <booktitle> Third International Workshop on Parsing Technologies. </booktitle>
Reference-contexts: The link grammar parser takes a sentence as input and returns a complete parse in which terms are connected in typed binary relations ("links") which represent syntactic relationships <ref> (Sleator and Temperley, 1993) </ref>. The set of link types is quite large and covers a wide range of syntactic phenomena. Moreover, since links associate tokens with other tokens, they are essentially very similar to what we have called relational features.
Reference: <author> S. Soderland and W. Lehnert. </author> <year> 1994. </year> <title> Wrap-Up: a trainable discourse module for information extraction. </title> <journal> Journal of Artificial Intelligence Research. </journal>
Reference-contexts: A natural part of this development has been the introduction of machine learning techniques to facilitate the domain engineering effort (Riloff and Shoen, 1995) <ref> (Soderland and Lehnert, 1994) </ref>. Several researchers have reported IE systems which use machine learning at their core (Soder-land, 1996) (Califf and Mooney, 1997).
Reference: <author> S. Soderland. </author> <year> 1996. </year> <title> Learning Text Analysis Rules for Domain-specific Natural Language Processing. </title> <type> Ph.D. thesis, </type> <institution> Dept. of Computer Science, University of Massachusetts. </institution> <month> 7 </month>
Reference-contexts: A relational feature, on the other hand, maps a token to another token. This general notion of relational features con 2 Note that this is not the only reasonable representational choice. In another important learning system for IE, for instance, examples are complete, syntactically tagged sentences <ref> (Soderland, 1996) </ref>. 2 trasts with the only other relational learner for IE (Califf and Mooney, 1997), in which only the adjacency relation is exploited. 2.3 Rule Construction In contrast with the general learning framework, which allows inference for multiple classes simultaneously, our representation defines a two-class problem, and we are really
References-found: 12

