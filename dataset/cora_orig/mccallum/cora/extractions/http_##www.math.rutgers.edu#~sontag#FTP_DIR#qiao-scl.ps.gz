URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/qiao-scl.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Email: sontag@hilbert.rutgers.edu  yqiao@cs.utexas.edu  
Author: E. D. Sontag Y. Qiao 
Note: hypotheses made in previous work do not apply.  
Address: Piscataway, NJ 08854-8019, USA  Austin, TX 78712, USA  
Affiliation: Department of Mathematics Rutgers University,  Department of Computer Sciences C0500 University of Texas,  
Abstract: Further Results on Controllability Abstract This paper studies controllability properties of recurrent neural networks. The new contributions are: (1) an extension of a previous result to a slightly different model, (2) a formulation and proof of a necessary and sufficient condition, and (3) an analysis of a low-dimensional case for which the of Recurrent Neural Networks fl
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Albertini, F., and P. Dai Pra, </author> <title> "Forward accessibility for recurrent neural networks," </title> <journal> IEEE Trans. Automat. Control 40 (1995): </journal> <pages> 1962-1968 </pages>
Reference-contexts: The paper <ref> [1] </ref> initiated the study of controllability properties for recurrent nets, and provided conditions for the "forward accessibility" of such systems. In [16], a sufficient condition for complete controllability was obtained, and this condition is reviewed below.
Reference: [2] <editor> Albertini, F., and E.D. Sontag, </editor> <title> "For neural networks, function determines form," </title> <booktitle> Neural Networks 6(1993): </booktitle> <pages> 975-990. </pages>
Reference-contexts: the form _x (t) = A ~ (n) (x (t)) + Bu (t) ; (2) where A 2 R nfin and B 2 R nfim . 2 The block diagram representing such a system is as in Figure 2. l - +B A ~ (n) As often remarked, see e.g. <ref> [2] </ref>, it is sometimes possible to obtain results for input-affine recurrent networks from analogous results for (non input-affine) nets, by means of the change of variables z = Ax + Bu. <p> In summary, obtaining results for input-affine nets by this transformation is not completely trivial, and some work is required when following this approach. In <ref> [2] </ref>, it was shown how, for questions of parameter identification, this transformation can be fruitfully applied. Another of the contributions of this paper is to follow this 2 approach in order to provide a controllability result.
Reference: [3] <author> Chu, Y-C., and K. Glover, </author> <title> "Gain-scheduling for systems with repeated scalar nonlinearities," </title> <booktitle> IEEE Conf. Decision and Control , San Diego, </booktitle> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: 1 Introduction This paper deals with controllability properties of what are often called "recurrent neural networks". These constitute a class of nonlinear systems which, although formally analogous to linear systems, exhibit interesting nonlinear characteristics and arise often in applications, see e.g. <ref> [3, 4, 5, 7, 9, 10, 11, 14, 15, 18] </ref>. A general model of recurrent nets (see e.g. [15]) is as follows. Assume given a Lipschitz map : R ! R.
Reference: [4] <author> Feng, Z., </author> <title> and A.N. Michel,, "Uniqueness and stability of equilibria of a class of neural networks with applications to the Hopfield model," </title> <booktitle> IEEE Conf. Decision and Control , San Diego, </booktitle> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: 1 Introduction This paper deals with controllability properties of what are often called "recurrent neural networks". These constitute a class of nonlinear systems which, although formally analogous to linear systems, exhibit interesting nonlinear characteristics and arise often in applications, see e.g. <ref> [3, 4, 5, 7, 9, 10, 11, 14, 15, 18] </ref>. A general model of recurrent nets (see e.g. [15]) is as follows. Assume given a Lipschitz map : R ! R.
Reference: [5] <author> Giles, C.L., G.Z. Sun, H.H. Chen, Y.C. Lee, and D. Chen, </author> <title> "Higher order recurrent networks and grammatical inference", </title> <booktitle> in Advances in Neural Information Processing Systems 2, </booktitle> <editor> D.S. Touretzky (ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction This paper deals with controllability properties of what are often called "recurrent neural networks". These constitute a class of nonlinear systems which, although formally analogous to linear systems, exhibit interesting nonlinear characteristics and arise often in applications, see e.g. <ref> [3, 4, 5, 7, 9, 10, 11, 14, 15, 18] </ref>. A general model of recurrent nets (see e.g. [15]) is as follows. Assume given a Lipschitz map : R ! R.
Reference: [6] <author> Grasse, K.A., and H.J. Sussmann, </author> <title> "Global controllability by nice controls," in Nonlinear Controllability and Optimal Control (H.J. </title> <editor> Sussmann, ed.), </editor> <publisher> Dekker, </publisher> <address> New York, </address> <year> 1990, </year> <pages> pp. 33-79. </pages>
Reference-contexts: Another of the contributions of this paper is to follow this 2 approach in order to provide a controllability result. It turns out that a recent and nontrivial result about nonlinear controllability, from <ref> [6] </ref>, is instrumental in the proof. <p> A fundamental result in control theory makes a converse statement: Theorem 4 <ref> [6] </ref> Every completely controllable system is also completely normally controllable. The main technical fact that we need is given by the next result, which says that controllability is preserved under "backstepping" or "adding an integrator" ([8]).
Reference: [7] <editor> Hunt, K.J., G.W. Irwin, and K. Warwick, eds., </editor> <title> Neural Network Engineering in Dynamic Control Systems, </title> <publisher> Springer-Verlag, </publisher> <address> London, </address> <year> 1995. </year>
Reference-contexts: 1 Introduction This paper deals with controllability properties of what are often called "recurrent neural networks". These constitute a class of nonlinear systems which, although formally analogous to linear systems, exhibit interesting nonlinear characteristics and arise often in applications, see e.g. <ref> [3, 4, 5, 7, 9, 10, 11, 14, 15, 18] </ref>. A general model of recurrent nets (see e.g. [15]) is as follows. Assume given a Lipschitz map : R ! R.
Reference: [8] <author> Krstic, M., I. Kanellakopoulos, and P. Kokotovic, </author> <title> Nonlinear and adaptive control design, </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1995. </year>
Reference: [9] <author> Matthews, M., </author> <title> "A state-space approach to adaptive nonlinear filtering using recurrent neural networks," </title> <booktitle> Proc. 1990 IASTED Symp. on Artificial Intelligence Applications and Neural Networks, </booktitle> <address> Zurich, </address> <pages> pp. 197-200, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: 1 Introduction This paper deals with controllability properties of what are often called "recurrent neural networks". These constitute a class of nonlinear systems which, although formally analogous to linear systems, exhibit interesting nonlinear characteristics and arise often in applications, see e.g. <ref> [3, 4, 5, 7, 9, 10, 11, 14, 15, 18] </ref>. A general model of recurrent nets (see e.g. [15]) is as follows. Assume given a Lipschitz map : R ! R.
Reference: [10] <author> Michel, A.N., J.A. Farrell, and W. Porod. </author> <title> (1989) Qualitative analysis of neural networks. </title> <journal> IEEE Trans. Circuits and Sys., </journal> <volume> 36, </volume> <pages> 229-243. </pages>
Reference-contexts: 1 Introduction This paper deals with controllability properties of what are often called "recurrent neural networks". These constitute a class of nonlinear systems which, although formally analogous to linear systems, exhibit interesting nonlinear characteristics and arise often in applications, see e.g. <ref> [3, 4, 5, 7, 9, 10, 11, 14, 15, 18] </ref>. A general model of recurrent nets (see e.g. [15]) is as follows. Assume given a Lipschitz map : R ! R.
Reference: [11] <author> Polycarpou, </author> <title> M.M., and P.A. Ioannou, "Neural networks and on-line approximators for adaptive control," </title> <booktitle> in Proc. Seventh Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pp. 93-798, </pages> <institution> Yale University, </institution> <year> 1992. </year>
Reference-contexts: 1 Introduction This paper deals with controllability properties of what are often called "recurrent neural networks". These constitute a class of nonlinear systems which, although formally analogous to linear systems, exhibit interesting nonlinear characteristics and arise often in applications, see e.g. <ref> [3, 4, 5, 7, 9, 10, 11, 14, 15, 18] </ref>. A general model of recurrent nets (see e.g. [15]) is as follows. Assume given a Lipschitz map : R ! R.
Reference: [12] <author> Sontag, E.D., </author> <title> "Finite dimensional open-loop control generators for nonlinear systems," </title> <booktitle> Int. J. Control 47 (1988): </booktitle> <pages> 537-556. </pages>
Reference-contexts: The critical step is showing that if a system is controllable then it is controllable using differentiable controls with preassigned boundary values. The technique of proof is almost identical to the one used in the much older paper <ref> [12] </ref>, where it was shown that analytic systems are controllable using infinitely differentiable controls. We may now remove analyticity assumptions, thanks to the recent result that was cited in Theorem 4.
Reference: [13] <author> Sontag, E.D., </author> <title> Mathematical Control Theory: Deterministic Finite Dimensional Systems, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: For general terminology about systems and control, we follow <ref> [13] </ref>. <p> For those systems that are considered in this paper, solutions exist globally and are unique, so this is well-defined for all (t; x 0 ; u). (See <ref> [13] </ref> for generalities and basic facts about control systems.) We also write (t; x; u) as u (t; x) or t u (x), depending on the context. <p> that, for every pair of states y and z in W , y can be controlled to z without leaving V. 2 3 This definition amounts to the requirement that any two states which are sufficiently close to x 0 can be steered to one another without large excursions; see <ref> [13] </ref>. We will show, in Section 3: Theorem 2 Assume that 2 fi. Then the following two properties are equivalent, for (1): 1. B 2 B n;m . 2. The system (1) is strongly locally controllable around every state. <p> The main technical fact that we need is given by the next result, which says that controllability is preserved under "backstepping" or "adding an integrator" ([8]). This result is a counterpart of the well-known theorem which asserts that stabilizability is preserved under backstepping, cf. <ref> [13] </ref>, Chapter 4, but the proofs are very different. The critical step is showing that if a system is controllable then it is controllable using differentiable controls with preassigned boundary values.
Reference: [14] <author> Sontag, E.D., </author> <title> "Neural networks for control," in Essays on Control: Perspectives in the Theory and its Applications (H.L. </title> <editor> Trentelman and J.C. Willems, eds.), </editor> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1993, </year> <pages> pp. 339-380. </pages>
Reference-contexts: 1 Introduction This paper deals with controllability properties of what are often called "recurrent neural networks". These constitute a class of nonlinear systems which, although formally analogous to linear systems, exhibit interesting nonlinear characteristics and arise often in applications, see e.g. <ref> [3, 4, 5, 7, 9, 10, 11, 14, 15, 18] </ref>. A general model of recurrent nets (see e.g. [15]) is as follows. Assume given a Lipschitz map : R ! R.
Reference: [15] <author> Sontag, E.D., </author> <title> "Recurrent neural networks: Some systems-theoretic aspects," in Dealing with Complexity: a Neural Network Approach (M. </title> <editor> Karny, K. Warwick, and V. Kurkova, eds.), </editor> <publisher> Springer-Verlag, </publisher> <address> London, </address> <year> 1997, </year> <pages> pp. 1-12. </pages>
Reference-contexts: 1 Introduction This paper deals with controllability properties of what are often called "recurrent neural networks". These constitute a class of nonlinear systems which, although formally analogous to linear systems, exhibit interesting nonlinear characteristics and arise often in applications, see e.g. <ref> [3, 4, 5, 7, 9, 10, 11, 14, 15, 18] </ref>. A general model of recurrent nets (see e.g. [15]) is as follows. Assume given a Lipschitz map : R ! R. <p> These constitute a class of nonlinear systems which, although formally analogous to linear systems, exhibit interesting nonlinear characteristics and arise often in applications, see e.g. [3, 4, 5, 7, 9, 10, 11, 14, 15, 18]. A general model of recurrent nets (see e.g. <ref> [15] </ref>) is as follows. Assume given a Lipschitz map : R ! R. The most typical choice is (x) = tanh x : R ! R : x 7! e x e x which is also called the "sigmoid" or "logistic" map.
Reference: [16] <author> Sontag, E.D., and H.J. Sussmann, </author> <title> "Complete controllability of continuous-time recurrent neural networks," </title> <journal> Systems and Control Letters 30(1997): </journal> <pages> 177-183. </pages>
Reference-contexts: The paper [1] initiated the study of controllability properties for recurrent nets, and provided conditions for the "forward accessibility" of such systems. In <ref> [16] </ref>, a sufficient condition for complete controllability was obtained, and this condition is reviewed below. One of the contributions of this paper is to show that the condition in [16] can be turned into a necessary and sufficient condition for a stronger form of complete controllability. <p> In <ref> [16] </ref>, a sufficient condition for complete controllability was obtained, and this condition is reviewed below. One of the contributions of this paper is to show that the condition in [16] can be turned into a necessary and sufficient condition for a stronger form of complete controllability. There is a formal variant of the model given in the above definition, also sometimes referred to as a "recurrent neural network", namely as follows. <p> exists and is &gt; 0; 3. (r) &lt; 1 for all r 2 R; 4. for each a; b 2 R, b &gt; 1, lim 1 (a + bs) = 0 : (3) The activation function which appears most frequently in applications is = tanh, so the following result from <ref> [16] </ref> is worth mentioning: Lemma 2.1 The function tanh 2 fi. We recall the main result from [16]: Theorem 1 Assume that 2 fi and B 2 B n;m . Then the system (1) is controllable. <p> a; b 2 R, b &gt; 1, lim 1 (a + bs) = 0 : (3) The activation function which appears most frequently in applications is = tanh, so the following result from <ref> [16] </ref> is worth mentioning: Lemma 2.1 The function tanh 2 fi. We recall the main result from [16]: Theorem 1 Assume that 2 fi and B 2 B n;m . Then the system (1) is controllable. In order to state a necessary condition, we need the following concept, which we define for more general systems. <p> whose state space is R n ) is strongly locally controllable around every state, then it is clearly also completely controllable. (Strong locally controllability around x 0 implies that the reachable set R (x 0 ) is both open and closed.) In fact, the proof of Theorem 1 given in <ref> [16] </ref> already establishes strong local controllability. Thus, we only need to prove the necessity of B 2 B n;m . We now turn to input-affine nets.
Reference: [17] <author> Sussmann, H.J., </author> <title> "Local controllability and motion planning for some classes of systems with drift," </title> <booktitle> in Proc. 30th Conf. Decision and Control , Brighton, </booktitle> <address> UK, </address> <publisher> IEEE Publications, </publisher> <year> 1991, </year> <pages> pp. 1110-1114. </pages>
Reference-contexts: We may now remove analyticity assumptions, thanks to the recent result that was cited in Theorem 4. We should point out, also, that the special case of the result for "systems without drift", was previously given in <ref> [17] </ref>. Theorem 5 Suppose that the input-value set U is open and connected. Then, _x = f (x; u) is completely controllable if and only if the cascaded system _x = f (x; y) with state space X fi U and control space R m is completely controllable. 5 Proof.
Reference: [18] <author> Zbikowski, R., </author> <title> "Lie algebra of recurrent neural networks and identifiability," </title> <booktitle> Proc. Amer. Auto. Control Conf., </booktitle> <address> San Francisco, </address> <year> 1993, </year> <note> pp.2900-2901. 12 </note>
Reference-contexts: 1 Introduction This paper deals with controllability properties of what are often called "recurrent neural networks". These constitute a class of nonlinear systems which, although formally analogous to linear systems, exhibit interesting nonlinear characteristics and arise often in applications, see e.g. <ref> [3, 4, 5, 7, 9, 10, 11, 14, 15, 18] </ref>. A general model of recurrent nets (see e.g. [15]) is as follows. Assume given a Lipschitz map : R ! R.
References-found: 18

