URL: ftp://ftp.research.microsoft.com/pub/dtg/meek/mbn.ps
Refering-URL: http://www.research.microsoft.com/~thiesson/default.htm
Root-URL: http://www.research.microsoft.com
Email: fthiesson,meek,dmax,heckermag@microsoft.com  
Title: Learning Mixtures of Bayesian Networks  
Author: Bo Thiesson Christopher Meek David Maxwell Chickering David Heckerman 
Date: February, 1998  
Address: Redmond, WA 98052  One Microsoft Way Redmond, WA 98052  
Affiliation: Microsoft Research  Microsoft Research Advanced Technology Division Microsoft Corporation  
Abstract: Technical Report MSR-TR-97-30 
Abstract-found: 1
Intro-found: 1
Reference: [Banfield and Raftery, 1993] <author> Banfield, J. and Raftery, A. </author> <year> (1993). </year> <title> Model-based Gaussian and non-Gaussian clustering. </title> <journal> Biometrics, </journal> <volume> 49 </volume> <pages> 803-821. </pages>
Reference: [Bernardo and Smith, 1994] <author> Bernardo, J. and Smith, A. </author> <year> (1994). </year> <title> Bayesian Theory. </title> <publisher> John Wiley and Sons, </publisher> <address> New York. </address> <month> 21 </month>
Reference: [Celeux and Govart, 1995] <author> Celeux, G. and Govart, G. </author> <year> (1995). </year> <title> Gaussian parsimonius clus-tering models. </title> <journal> Pattern recognition, </journal> <volume> 28 </volume> <pages> 781-793. </pages>
Reference: [Cheeseman and Stutz, 1995] <author> Cheeseman, P. and Stutz, J. </author> <year> (1995). </year> <title> Bayesian classification (AutoClass): Theory and results. </title> <editor> In Fayyad, U., Piatesky-Shapiro, G., Smyth, P., and Uthurusamy, R., editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA. </address>
Reference: [Chickering, 1996] <author> Chickering, D. </author> <year> (1996). </year> <title> Learning Bayesian networks is NP-complete. </title> <editor> In Fisher, D. and Lenz, H., editors, </editor> <booktitle> Learning from Data, </booktitle> <pages> pages 121-130. </pages> <publisher> Springer-Verlag. </publisher>
Reference: [Chow and Liu, 1968] <author> Chow, C. and Liu, C. </author> <year> (1968). </year> <title> Approximating discrete probability distributions with dependence trees. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 14 </volume> <pages> 462-467. </pages>
Reference: [Clogg, 1995] <author> Clogg, C. </author> <year> (1995). </year> <title> Latent class models. </title> <booktitle> In Handbook of statistical modeling for the social and behavioral sciences, </booktitle> <pages> pages 311-359. </pages> <publisher> Plenum Press, </publisher> <address> New York. </address>
Reference: [Cooper and Herskovits, 1992] <author> Cooper, G. and Herskovits, E. </author> <year> (1992). </year> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347. </pages>
Reference: [Dempster et al., 1977] <author> Dempster, A., Laird, N., and Rubin, D. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B 39 </volume> <pages> 1-38. </pages>
Reference: [Geiger and Heckerman, 1996] <author> Geiger, D. and Heckerman, D. </author> <year> (1996). </year> <title> Beyond Bayesian networks: Similarity networks and Bayesian multinets. </title> <journal> Artificial Intelligence, </journal> <volume> 82 </volume> <pages> 45-74. </pages>
Reference: [Haughton, 1988] <author> Haughton, D. </author> <year> (1988). </year> <title> On the choice of a model to fit data from an exponential family. </title> <journal> Annals of Statistics, </journal> <volume> 16 </volume> <pages> 342-355. </pages>
Reference: [Heckerman, 1995] <author> Heckerman, D. </author> <year> (1995). </year> <title> A tutorial on learning Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research, </institution> <address> Redmond, WA. </address> <month> Revised November, </month> <year> 1996. </year>
Reference: [Heckerman and Geiger, 1995] <author> Heckerman, D. and Geiger, D. </author> <year> (1995). </year> <title> Likelihoods and priors for Bayesian networks. </title> <type> Technical Report MSR-TR-95-54, </type> <institution> Microsoft Research, </institution> <address> Redmond, WA. </address>
Reference-contexts: In particular, in both COMP1 and COMP2, all uncon 15 Graphical structure for second component. mixture components. ditional means are zero and all linear coefficients and conditional variances are one. (See e.g. <ref> [Heckerman and Geiger, 1995] </ref> for an explanation of Gaussian parameterization.) The third Bayesian network, which we refer to as COMP3, represents a Gaussian with same shape as the COMP1, but has its location shifted by setting all unconditional means to 5.0.
Reference: [Heckerman et al., 1995] <author> Heckerman, D., Geiger, D., and Chickering, D. </author> <year> (1995). </year> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 197-243. </pages>
Reference: [Hinton et al., 1997] <author> Hinton, G., Dayan, P., and Revow, M. </author> <year> (1997). </year> <title> Modeling the manifolds of images of handwritten digits. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 8 </volume> <pages> 65-74. </pages>
Reference: [Kass and Wasserman, 1995] <author> Kass, R. and Wasserman, L. </author> <year> (1995). </year> <title> A reference Bayesian test for nested hypotheses and its relationship to the Schwarz criterion. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 90 </volume> <pages> 928-934. 22 </pages>
Reference: [Lauritzen, 1992] <author> Lauritzen, S. </author> <year> (1992). </year> <title> Propagation of probabilities, means, and variances in mixed graphical association models. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 87 </volume> <pages> 1098-1108. </pages>
Reference: [Raftery, 1995] <author> Raftery, A. </author> <year> (1995). </year> <title> Bayesian model selection in social research. </title> <editor> In Marsden, P., editor, </editor> <booktitle> Sociological Methodology. </booktitle> <address> Blackwells, Cambridge, MA. </address>
Reference: [Schwarz, 1978] <author> Schwarz, G. </author> <year> (1978). </year> <title> Estimating the dimension of a model. </title> <journal> Annals of Statistics, </journal> <volume> 6 </volume> <pages> 461-464. </pages>
Reference: [Shachter and Kenley, 1989] <author> Shachter, R. and Kenley, C. </author> <year> (1989). </year> <title> Gaussian influence diagrams. </title> <journal> Management Science, </journal> <volume> 35 </volume> <pages> 527-550. </pages>
Reference: [Siebert, 1987] <author> Siebert, J. </author> <year> (1987). </year> <title> Vehicle recognition using rule-based methods. </title> <type> Technical Report TIRM-87-018, </type> <institution> Turing Institute. </institution>
Reference-contexts: Cheeseman-Stutz score decreases in only 4, 6, and 8 transitions out of the total 72, 76, and 78 transitions, respectively, suggesting that the heuristic score is good one. 7.2 Results on real data For experimental evaluation on real data we chose the Vehicle data set from the Machine Learning Repository <ref> [Siebert, 1987] </ref>. The data consists of 18 continuous 2-dimensional features extracted from the silhouettes of four types of vehicles: a double decker bus (218 18 mixture components. cases), a Saab 9000 (212 cases), an Opel Manta 400 (217 cases), and a Cheverolet van (199 cases).
Reference: [Spiegelhalter et al., 1993] <author> Spiegelhalter, D., Dawid, A., Lauritzen, S., and Cowell, R. </author> <year> (1993). </year> <title> Bayesian analysis in expert systems. </title> <journal> Statistical Science, </journal> <volume> 8 </volume> <pages> 219-282. </pages>
Reference: [Thiesson, 1997] <author> Thiesson, B. </author> <year> (1997). </year> <title> Score and information for recursive exponential models with incomplete data. </title> <booktitle> In Proceedings of Thirteenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Providence, RI. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Tipping and Bishop, 1997] <author> Tipping, M. and Bishop, C. </author> <year> (1997). </year> <title> Mixtures of probabilistic principle component analysers. </title> <type> Technical Report NCRG-97-003, </type> <institution> Neural Computing Research Group. </institution> <month> 23 </month>
References-found: 24

