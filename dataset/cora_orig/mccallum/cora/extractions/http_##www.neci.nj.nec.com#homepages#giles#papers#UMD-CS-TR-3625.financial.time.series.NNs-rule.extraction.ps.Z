URL: http://www.neci.nj.nec.com/homepages/giles/papers/UMD-CS-TR-3625.financial.time.series.NNs-rule.extraction.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/papers/
Root-URL: http://www.neci.nj.nec.com
Title: Noisy Time Series Prediction using Symbolic Representation and Recurrent Neural Network Grammatical Inference  
Author: Steve Lawrence fl Ah Chung Tsoi C. Lee Giles yz 
Note: http://www.neci.nj.nec.com/homepages/lawrence Lee Giles is also with the  
Address: 4 Independence Way, Princeton, NJ 08540  NSW 2522 Australia  College Park, MD 20742  College Park, MD 20742.  
Affiliation: NEC Research Institute,  Faculty of Informatics, University of Wollongong,  Institute for Advanced Computer Studies University of Maryland  Institute for Advanced Computer Studies, University of Maryland,  
Pubnum: Technical Report UMIACS-TR-96-27 and CS-TR-3625  
Email: flawrence,gilesg@research.nj.nec.com, Ah Chung Tsoi@uow.edu.au  
Phone: 1  2  
Web: http://www.neci.nj.nec.com/homepages/giles.html  
Date: December 16, 1997, 13:36  
Abstract: Financial forecasting is an example of a signal processing problem which is challenging due to small sample sizes, high noise, non-stationarity, and non-linearity. Neural networks have been very successful in a number of signal processing applications. We discuss fundamental limitations and inherent difficulties when using neural networks for the processing of high noise, small sample size signals. We introduce a new intelligent signal processing method which addresses the difficulties. The method uses conversion into a symbolic representation with a self-organizing map, and grammatical inference with recurrent neural networks. We apply the method to the prediction of daily foreign exchange rates, addressing difficulties with non-stationarity, overfitting, and unequal a priori class probabilities, and we find significant predictability in comprehensive experiments covering 5 different foreign exchange rates. The method correctly predicts the direction of change for the next day with an error rate of 47.1%. The error rate reduces to around 40% when rejecting examples where the system has low confidence in its prediction. The symbolic representation aids the extraction of symbolic knowledge from the recurrent neural networks in the form of deterministic finite state automata. These automata explain the operation of the system and are often relatively simple. Rules related to well known behavior such as trend following and mean reversal are extracted. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y.S. Abu-Mostafa. </author> <title> Learning from hints in neural networks. </title> <journal> Journal of Complexity, </journal> <volume> 6:192, </volume> <year> 1990. </year>
Reference-contexts: The use of a recurrent neural network instead of an MLP with a window of time delayed inputs introduces another assumption by explicitly addressing the temporal relationship of the inputs via the maintenance of an internal state. This can be seen as similar to the use of hints <ref> [1] </ref> and should help to make the problem less ill-posed. The curse of dimensionality refers to the exponential growth of hypervolume as a function of dimensionality [5]. Consider x i 2 R n . The regression, y = f (x) is a hypersurface in R n . <p> Each node represents one symbol in the resulting grammatical inference problem. A brief description of the self-organizing map is contained in the next section. The SOM can be represented by the following equation: S (k) = g (X (k; d)) (12) where S (k) 2 <ref> [1; 2; 3; : : : n s ] </ref>, and n s is the number of symbols (nodes) for the SOM. Each node in the SOM has been assigned an integer index ranging from 1 to the number of nodes.
Reference: [2] <author> J.A. Alexander and M.C. Mozer. </author> <title> Template-based algorithms for connectionist rule extraction. </title> <editor> In G. Tesauro, D. Touretzky, and T. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 7, </volume> <pages> pages 609616. </pages> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: There are 3645 data points for each exchange rate covering the period September 3, 1973 to May 18, 1987. In contrast to Weigend et al., this work 2 Rules can also be extracted from feedforward networks <ref> [21, 35, 44, 2, 39, 25] </ref>, however the recurrent network approach and deterministic finite state automata extraction seem particularly suitable for a time series problem. 3 December 16, 1997, 13:36 considers the prediction of all five exchange rates in the data and prediction for all days of the week instead of <p> Each node represents one symbol in the resulting grammatical inference problem. A brief description of the self-organizing map is contained in the next section. The SOM can be represented by the following equation: S (k) = g (X (k; d)) (12) where S (k) 2 <ref> [1; 2; 3; : : : n s ] </ref>, and n s is the number of symbols (nodes) for the SOM. Each node in the SOM has been assigned an integer index ranging from 1 to the number of nodes.
Reference: [3] <author> Martin Anthony and Norman L. Biggs. </author> <title> A computational learning theory view of economic forecasting with neural nets. </title> <editor> In A. Refenes, editor, </editor> <title> Neural Networks in the Capital Markets. </title> <publisher> John Wiley and Sons, </publisher> <year> 1995. </year>
Reference-contexts: Fama [11, 12] and found broad acceptance in the financial community <ref> [33, 45, 3] </ref>. The EMH, in its weak form, asserts that the price of an asset reflects all of the information that can be obtained from past prices of the asset, i.e. the movement of the price is unpredictable. <p> Each node represents one symbol in the resulting grammatical inference problem. A brief description of the self-organizing map is contained in the next section. The SOM can be represented by the following equation: S (k) = g (X (k; d)) (12) where S (k) 2 <ref> [1; 2; 3; : : : n s ] </ref>, and n s is the number of symbols (nodes) for the SOM. Each node in the SOM has been assigned an integer index ranging from 1 to the number of nodes.
Reference: [4] <author> A.R. Barron. </author> <title> Universal approximation bounds for superpositions of a sigmoidal function. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39(3):930945, </volume> <year> 1993. </year>
Reference-contexts: is the number of points for a given sampling density in 1 dimension, then in order to keep the same density as the dimensionality is increased, the number of points must increase according to N n It has been suggested that MLPs do not suffer from the curse of dimensionality <ref> [13, 4] </ref>. However, this is not true in general (although MLPs may cope better than other models). The apparent avoidance of the curse of dimensionality in [4] is due to the fact that the function spaces considered are more and more constrained as the input dimension increases [29]. <p> However, this is not true in general (although MLPs may cope better than other models). The apparent avoidance of the curse of dimensionality in <ref> [4] </ref> is due to the fact that the function spaces considered are more and more constrained as the input dimension increases [29]. Similarly, smoothness conditions must be satisfied for the results of [13].
Reference: [5] <author> R. E. Bellman. </author> <title> Adaptive Control Processes. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1961. </year>
Reference-contexts: This can be seen as similar to the use of hints [1] and should help to make the problem less ill-posed. The curse of dimensionality refers to the exponential growth of hypervolume as a function of dimensionality <ref> [5] </ref>. Consider x i 2 R n . The regression, y = f (x) is a hypersurface in R n . If f (x) is arbitrarily complex and unknown then dense samples are required to approximate the function ac 4 December 16, 1997, 13:36 curately. <p> If f (x) is arbitrarily complex and unknown then dense samples are required to approximate the function ac 4 December 16, 1997, 13:36 curately. However, it is hard to obtain dense samples in high dimensions 3 . This is the curse of dimensionality <ref> [5, 14, 15] </ref>. The relationship between the sampling density and the number of points required is N 1 n [15] where n is the dimensionality of the input space and N is the number of points.
Reference: [6] <author> Y. Bengio, </author> <title> editor. Neural Networks for Speech and Sequence Recognition. </title> <publisher> Thomson, </publisher> <year> 1996. </year>
Reference-contexts: However training RNNs tends to be difficult with high noise data, with a tendency for long-term dependencies to be neglected (experiments reported in <ref> [6] </ref> found a tendency for recurrent networks to take into account short-term dependencies but not long-term dependencies), and for the network to fall into a naive solution such as always predicting the most common output.
Reference: [7] <author> A. Cleeremans, D. Servan-Schreiber, and J.L. McClelland. </author> <title> Finite state automata and simple recurrent networks. </title> <booktitle> Neural Computation, </booktitle> <address> 1(3):372381, </address> <year> 1989. </year>
Reference-contexts: On the other hand, if M is small, this implies that there is a heavy filtering effect, resulting in only a small number of symbols. 5.2 Recurrent Network Prediction In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference <ref> [7, 19, 16] </ref>. The induction of relatively simple grammars has been addressed often e.g. [46, 47, 16] on learning Tomita grammars [43] . It has been shown that a particular class of recurrent networks are Turing equivalent [40].
Reference: [8] <author> C. Darken and J.E. Moody. </author> <title> Note on learning rate schedules for stochastic optimization. </title> <editor> In R.P. Lippmann, J.E. Moody, and D. S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 832838. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Such information can be very useful and important to the user of the technique. Appendix A: Simulation Details For the experiments reported in this paper, n h = 5 and n o = 2. The RNN learning rate was linearly reduced over the training period (see <ref> [8] </ref> regarding learning rate schedules) from an initial value of 0.5. All inputs were normalized to zero mean and unit variance. All nodes included a bias input which was part of the optimization process. Weights were initialized as shown in Haykin [22].
Reference: [9] <author> J.S. Denker, D. Schwartz, B. Wittner, S.A. Solla, R. Howard, L. Jackel, and J. </author> <title> Hopfield. Large automatic learning, rule extraction, and generalization. </title> <journal> Complex Systems, </journal> <volume> 1:877922, </volume> <year> 1987. </year>
Reference-contexts: A number 17 December 16, 1997, 13:36 of people have considered the extraction of symbolic knowledge from trained neural networks for both feedforward and recurrent neural networks <ref> [9, 18, 37] </ref>. For recurrent networks, the ordered triple of a discrete Markov process (fstate; input ! next stateg) can be extracted and used to form an equivalent deterministic finite state automata (DFA). This can be done by clustering the activation values of the recurrent state neurons [37].
Reference: [10] <author> J.L. Elman. </author> <title> Distributed representations, simple recurrent networks, and grammatical structure. </title> <booktitle> Machine Learning, </booktitle> <address> 7(2/3):195226, </address> <year> 1991. </year>
Reference-contexts: Our first step is to use recurrent neural networks (RNNs). Recurrent neural networks employ feedback connections and have the potential to represent certain computational structures in a more parsimonious fashion <ref> [10] </ref>. RNNs address the temporal relationship of their inputs by maintaining an internal state. RNNs are biased towards learning patterns which occur in temporal order i.e. they are less prone to learning random correlations which do not occur in temporal order. <p> An Elman recurrent neural network is then used 4 which is trained on the sequence of outputs from the SOM. The Elman network was chosen because it is suitable for the problem (a grammatical inference style problem) <ref> [10] </ref>, and because it has been shown to perform well in comparison to other recurrent architectures (e.g. see [31]).
Reference: [11] <author> E.F. Fama. </author> <title> The behaviour of stock market prices. </title> <journal> Journal of Business, </journal> <volume> January:34105, </volume> <year> 1965. </year>
Reference-contexts: As will be seen later, the topographical order of the SOM allows encoding a one dimensional SOM into a single input for the RNN. 4 The Efficient Market Hypothesis The Efficient Market Hypothesis (EMH) was developed in 1965 by E. Fama <ref> [11, 12] </ref> and found broad acceptance in the financial community [33, 45, 3]. The EMH, in its weak form, asserts that the price of an asset reflects all of the information that can be obtained from past prices of the asset, i.e. the movement of the price is unpredictable.
Reference: [12] <author> E.F. Fama. </author> <title> Efficient capital markets: A review of theory and empirical work. </title> <journal> Journal of Finance, </journal> <volume> May:383417, </volume> <year> 1970. </year> <month> 21 December 16, </month> <year> 1997, </year> <month> 13:36 </month>
Reference-contexts: As will be seen later, the topographical order of the SOM allows encoding a one dimensional SOM into a single input for the RNN. 4 The Efficient Market Hypothesis The Efficient Market Hypothesis (EMH) was developed in 1965 by E. Fama <ref> [11, 12] </ref> and found broad acceptance in the financial community [33, 45, 3]. The EMH, in its weak form, asserts that the price of an asset reflects all of the information that can be obtained from past prices of the asset, i.e. the movement of the price is unpredictable.
Reference: [13] <author> Andras Farago and Gabor Lugosi. </author> <title> Strong universal consistency of neural network classifiers. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39(4):11461151, </volume> <year> 1993. </year>
Reference-contexts: is the number of points for a given sampling density in 1 dimension, then in order to keep the same density as the dimensionality is increased, the number of points must increase according to N n It has been suggested that MLPs do not suffer from the curse of dimensionality <ref> [13, 4] </ref>. However, this is not true in general (although MLPs may cope better than other models). The apparent avoidance of the curse of dimensionality in [4] is due to the fact that the function spaces considered are more and more constrained as the input dimension increases [29]. <p> The apparent avoidance of the curse of dimensionality in [4] is due to the fact that the function spaces considered are more and more constrained as the input dimension increases [29]. Similarly, smoothness conditions must be satisfied for the results of <ref> [13] </ref>. The use of a recurrent neural network is important from the viewpoint of the curse of dimensionality because the RNN can take into account greater history of the input.
Reference: [14] <author> J.H. Friedman. </author> <title> An overview of predictive learning and function approximation. </title> <editor> In V. Cherkassky, J.H. Friedman, and H. Wechsler, editors, </editor> <title> From Statistics to Neural Networks, Theory and Pattern Recognition Applications, </title> <booktitle> volume 136 of NATO ASI Series F, </booktitle> <pages> pages 161. </pages> <publisher> Springer, </publisher> <year> 1994. </year>
Reference-contexts: In this case, there may be a unique solution which best fits the data from the range of solutions which the model can represent. The implicit assumption behind this constraint is that the underlying target function is smooth. Smoothness constraints are commonly used by learning algorithms <ref> [14] </ref>. Without smoothness or some other constraint, there is no reason to expect a model to perform well on unseen inputs (i.e. generalize well). Learning by example often operates in a vector space, i.e. a function mapping R n to R m is approximated. <p> If f (x) is arbitrarily complex and unknown then dense samples are required to approximate the function ac 4 December 16, 1997, 13:36 curately. However, it is hard to obtain dense samples in high dimensions 3 . This is the curse of dimensionality <ref> [5, 14, 15] </ref>. The relationship between the sampling density and the number of points required is N 1 n [15] where n is the dimensionality of the input space and N is the number of points.
Reference: [15] <author> J.H. Friedman. </author> <title> Introduction to computational learning and statistical prediction. </title> <booktitle> Tutorial Presented at Neural Information Processing Systems, </booktitle> <address> Denver, CO, </address> <year> 1995. </year>
Reference-contexts: If f (x) is arbitrarily complex and unknown then dense samples are required to approximate the function ac 4 December 16, 1997, 13:36 curately. However, it is hard to obtain dense samples in high dimensions 3 . This is the curse of dimensionality <ref> [5, 14, 15] </ref>. The relationship between the sampling density and the number of points required is N 1 n [15] where n is the dimensionality of the input space and N is the number of points. <p> However, it is hard to obtain dense samples in high dimensions 3 . This is the curse of dimensionality [5, 14, 15]. The relationship between the sampling density and the number of points required is N 1 n <ref> [15] </ref> where n is the dimensionality of the input space and N is the number of points. <p> The EMH is based on the assumption that all news is promptly incorporated in prices; since news is unpredictable (by definition), prices are 3 Kolmogorov's theorem shows that any continuous function of n dimensions can be completely characterized by a one dimensional continuous function. Specifically, Kolmogorov's theorem <ref> [28, 29, 30, 15] </ref> states that for any continuous function: f (x 1 ; x 2 ; : : : ; x n ) = j=1 n X i Q j (x i ) (1) where f i g n 1 are universal constants that do not depend on f , <p> In other words, for any continuous function of n arguments, there is a one dimensional continuous function that completely characterizes the original function. As such, it can be seen that the problem is not so much the dimensionality, but the complexity of the function <ref> [15] </ref>, i.e. the curse of dimensionality essentially says that in high dimensions, as fewer data points are available, the target function has to be simpler in order to learn it accurately from the given data. 5 December 16, 1997, 13:36 unpredictable.
Reference: [16] <author> C. Lee Giles, C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun, and Y.C. Lee. </author> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <booktitle> Neural Computation, </booktitle> <address> 4(3):393405, </address> <year> 1992. </year>
Reference-contexts: On the other hand, if M is small, this implies that there is a heavy filtering effect, resulting in only a small number of symbols. 5.2 Recurrent Network Prediction In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference <ref> [7, 19, 16] </ref>. The induction of relatively simple grammars has been addressed often e.g. [46, 47, 16] on learning Tomita grammars [43] . It has been shown that a particular class of recurrent networks are Turing equivalent [40]. <p> The induction of relatively simple grammars has been addressed often e.g. <ref> [46, 47, 16] </ref> on learning Tomita grammars [43] . It has been shown that a particular class of recurrent networks are Turing equivalent [40]. The Elman neural network has feedback from each of the hidden nodes to all of the hidden nodes, as shown in figure 3. <p> This can be done by clustering the activation values of the recurrent state neurons [37]. The automata extracted with this process can only recognize regular grammars 5 . The algorithm used for automata extraction is the same as that described in <ref> [16] </ref>. The algorithm is based on the observation that the activations of the recurrent state neurons in a trained network tend to cluster.
Reference: [17] <author> C. Lee Giles, C.B. Miller, D. Chen, G.Z. Sun, H.H. Chen, and Y.C. Lee. </author> <title> Extracting and learning an unknown grammar with recurrent neural networks. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 317324, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: December 16, 1997, 13:36 The use of a recurrent neural network is significant for two reasons: firstly, the temporal relationship of the series is explicitly modeled via internal states, and secondly, it is possible to extract rules from the trained recurrent networks in the form of deterministic finite state automata <ref> [17] </ref> 2 . The symbolic conversion is significant for a number of reasons: the quantization effectively filters the data or reduces the noise, the RNN training becomes more effective, and the symbolic input facilitates the extraction of rules from the trained networks.
Reference: [18] <author> C. Lee Giles and C.W. Omlin. </author> <title> Extraction, insertion and refinement of symbolic rules in dynamically-driven recurrent neural networks. </title> <booktitle> Connection Science, 5(3,4):307337, 1993. Special Issue on Architectures for Integrating Symbolic and Neural Processes. </booktitle>
Reference-contexts: A number 17 December 16, 1997, 13:36 of people have considered the extraction of symbolic knowledge from trained neural networks for both feedforward and recurrent neural networks <ref> [9, 18, 37] </ref>. For recurrent networks, the ordered triple of a discrete Markov process (fstate; input ! next stateg) can be extracted and used to form an equivalent deterministic finite state automata (DFA). This can be done by clustering the activation values of the recurrent state neurons [37].
Reference: [19] <author> C. Lee Giles, G.Z. Sun, H.H. Chen, Y.C. Lee, and D. Chen. </author> <title> Higher order recurrent networks & grammatical inference. </title> <editor> In D.S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 380387, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: On the other hand, if M is small, this implies that there is a heavy filtering effect, resulting in only a small number of symbols. 5.2 Recurrent Network Prediction In the past few years several recurrent neural network architectures have emerged which have been used for grammatical inference <ref> [7, 19, 16] </ref>. The induction of relatively simple grammars has been addressed often e.g. [46, 47, 16] on learning Tomita grammars [43] . It has been shown that a particular class of recurrent networks are Turing equivalent [40].
Reference: [20] <author> C.W.J. Granger and P. Newbold. </author> <title> Forecasting Economic Time Series. </title> <publisher> Academic Press, </publisher> <address> San Diego, </address> <note> second edition, </note> <year> 1986. </year>
Reference-contexts: 1 Introduction 1.1 Predicting Noisy Time Series Data The prediction of future events from noisy time series data is commonly done using various forms of statistical models <ref> [20] </ref>. Typical neural network models are closely related to statistical models, and estimate Bayesian a posteriori probabilities when given an appropriately formulated problem [38]. Neural networks have been very successful in a number of pattern recognition applications. <p> A is difficult because the model has not been trained with inputs in the range covered by section B. 6 December 16, 1997, 13:36 A common solution to this is to use a model which is based on the first order differences (equation 7) instead of the raw time series <ref> [20] </ref>: ffi (k + 1) = f (ffi (k); ffi (k 1); : : : ; ffi (k n + 1)) + (k) (6) where ffi (k + 1) = x (k + 1) x (k) (7) and (k) is a zero mean Gaussian variable with variance oe.
Reference: [21] <author> Yoichi Hayashi. </author> <title> A neural expert system with automated extraction of fuzzy if-then rules. </title> <editor> In Richard P. Lippmann, John E. Moody, and David S. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 578584. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1991. </year>
Reference-contexts: There are 3645 data points for each exchange rate covering the period September 3, 1973 to May 18, 1987. In contrast to Weigend et al., this work 2 Rules can also be extracted from feedforward networks <ref> [21, 35, 44, 2, 39, 25] </ref>, however the recurrent network approach and deterministic finite state automata extraction seem particularly suitable for a time series problem. 3 December 16, 1997, 13:36 considers the prediction of all five exchange rates in the data and prediction for all days of the week instead of
Reference: [22] <author> S. Haykin. </author> <title> Neural Networks, A Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: All inputs were normalized to zero mean and unit variance. All nodes included a bias input which was part of the optimization process. Weights were initialized as shown in Haykin <ref> [22] </ref>. Target outputs were -0.8 and 0.8 using the tanh output activation function and we used the quadratic cost function. In order to initialize the delays in the network, the RNN was run without training for 50 time steps prior to the start of the datasets.
Reference: [23] <author> J.E. Hopcroft and J.D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1979. </year>
Reference-contexts: Sample deterministic finite state automata (DFA) extracted from trained financial prediction networks using a quantization level of 5 can be seen in figure 10. The DFAs have been minimized using standard minimization techniques <ref> [23] </ref>. The DFAs were extracted from networks where the SOM embedding dimension was 1 and the SOM size was 2. As expected, the DFAs extracted in this case are relatively simple.
Reference: [24] <author> L. Ingber. </author> <title> Statistical mechanics of nonlinear nonequilibrium financial markets: Applications to optimized trading. </title> <journal> Mathematical Computer Modelling, </journal> <note> page in press, </note> <year> 1996. </year>
Reference-contexts: Much effort has been expended trying to prove or disprove the EMH. Current opinion is that the theory has been disproved <ref> [42, 24] </ref>, and much evidence suggests that the capital markets are not efficient [34].
Reference: [25] <author> M. Ishikawa. </author> <title> Rule extraction by successive regularization. </title> <booktitle> In International Conference on Neural Networks, </booktitle> <pages> pages 11391143. </pages> <publisher> IEEE Press, </publisher> <year> 1996. </year>
Reference-contexts: There are 3645 data points for each exchange rate covering the period September 3, 1973 to May 18, 1987. In contrast to Weigend et al., this work 2 Rules can also be extracted from feedforward networks <ref> [21, 35, 44, 2, 39, 25] </ref>, however the recurrent network approach and deterministic finite state automata extraction seem particularly suitable for a time series problem. 3 December 16, 1997, 13:36 considers the prediction of all five exchange rates in the data and prediction for all days of the week instead of
Reference: [26] <author> M.I. Jordan. </author> <title> Neural networks. </title> <editor> In A. Tucker, editor, </editor> <publisher> CRC Handbook of Computer Science, page in press. CRC Press, </publisher> <address> Boca Raton, FL, </address> <year> 1996. </year> <month> 22 December 16, </month> <year> 1997, </year> <month> 13:36 </month>
Reference-contexts: These help form the motivation for our new method. The problem of inferring an underlying probability distribution from a finite set of data is fundamentally an ill-posed problem because there are infinitely many solutions <ref> [26] </ref>, i.e. there are infinitely many functions which fit the training data exactly but differ in other regions of the input space. The problem becomes well-posed only when additional constraints are used. For example, the constraint that a limited size MLP will be used might be imposed.
Reference: [27] <author> T. Kohonen. </author> <title> Self-Organizing Maps. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Germany, </address> <year> 1995. </year>
Reference-contexts: y (k 3); y (k 4); y (k 5)) (17) The following sections describe the self-organizing map, recurrent network grammatical inference, dealing with non-stationarity, controlling overfitting, a priori class probabilities, and a method for comparison with a random walk. 5.1 The Self-Organizing Map 5.1.1 Introduction The self-organizing map, or SOM <ref> [27] </ref> is an unsupervised learning process which learns the distribution of a set of patterns without any class information. A pattern is projected from a possibly high dimensional input space S to a position in the map, a low dimensional display space D. <p> In other words, for similarity as measured using a metric in the input space S, the SOM attempts to preserve the similarity in the display space D. 5.1.2 Algorithm We give a brief description of the SOM algorithm, for more details see <ref> [27] </ref>. The SOM defines a mapping from an input space R n onto a topologically ordered set of nodes, usually in a lower dimensional space D. <p> A widely applied neighborhood function is: h ci = ff (t) exp 2oe 2 (t) (19) where ff (t) is a scalar valued learning rate and oe (t) defines the width of the kernel. They are generally both monotonically decreasing with time <ref> [27] </ref>. The use of the neighborhood function means that nodes which are topographically close in the SOM structure are moved towards the input pattern along with the winning node. This creates a smoothing effect which leads to a global ordering of the map. <p> The SOM can be considered a nonlinear projection of the probability density, p (x), of the input patterns x onto a (typically) smaller output space <ref> [27] </ref>. 5.1.3 Remarks The nodes in the display space D encode the information contained in the input space R n .
Reference: [28] <author> A.N. </author> <title> Kolmogorov. On the representation of continuous functions of several variables by super-positions of continuous functions of one variable and addition. </title> <journal> Dokl, </journal> <volume> 114:679681, </volume> <year> 1957. </year>
Reference-contexts: The EMH is based on the assumption that all news is promptly incorporated in prices; since news is unpredictable (by definition), prices are 3 Kolmogorov's theorem shows that any continuous function of n dimensions can be completely characterized by a one dimensional continuous function. Specifically, Kolmogorov's theorem <ref> [28, 29, 30, 15] </ref> states that for any continuous function: f (x 1 ; x 2 ; : : : ; x n ) = j=1 n X i Q j (x i ) (1) where f i g n 1 are universal constants that do not depend on f ,
Reference: [29] <author> V. K ffurkova. </author> <title> Kolmogorov's theorem is relevant. </title> <booktitle> Neural Computation, </booktitle> <address> 3(4):617622, </address> <year> 1991. </year>
Reference-contexts: However, this is not true in general (although MLPs may cope better than other models). The apparent avoidance of the curse of dimensionality in [4] is due to the fact that the function spaces considered are more and more constrained as the input dimension increases <ref> [29] </ref>. Similarly, smoothness conditions must be satisfied for the results of [13]. The use of a recurrent neural network is important from the viewpoint of the curse of dimensionality because the RNN can take into account greater history of the input. <p> The EMH is based on the assumption that all news is promptly incorporated in prices; since news is unpredictable (by definition), prices are 3 Kolmogorov's theorem shows that any continuous function of n dimensions can be completely characterized by a one dimensional continuous function. Specifically, Kolmogorov's theorem <ref> [28, 29, 30, 15] </ref> states that for any continuous function: f (x 1 ; x 2 ; : : : ; x n ) = j=1 n X i Q j (x i ) (1) where f i g n 1 are universal constants that do not depend on f ,
Reference: [30] <author> V. Kffurkova. </author> <title> Kolmogorov's theorem. </title> <editor> In Michael A. Arbib, editor, </editor> <booktitle> The Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pages 501502. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1995. </year>
Reference-contexts: The EMH is based on the assumption that all news is promptly incorporated in prices; since news is unpredictable (by definition), prices are 3 Kolmogorov's theorem shows that any continuous function of n dimensions can be completely characterized by a one dimensional continuous function. Specifically, Kolmogorov's theorem <ref> [28, 29, 30, 15] </ref> states that for any continuous function: f (x 1 ; x 2 ; : : : ; x n ) = j=1 n X i Q j (x i ) (1) where f i g n 1 are universal constants that do not depend on f ,
Reference: [31] <author> Steve Lawrence, C. Lee Giles, and Sandiway Fong. </author> <booktitle> Can recurrent neural networks learn natural language grammars? In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <pages> pages 18531858. </pages> <publisher> IEEE Press, </publisher> <address> Piscataway, NJ, </address> <year> 1996. </year>
Reference-contexts: The Elman network was chosen because it is suitable for the problem (a grammatical inference style problem) [10], and because it has been shown to perform well in comparison to other recurrent architectures (e.g. see <ref> [31] </ref>).
Reference: [32] <author> Jean Y. Lequarre. </author> <title> Foreign currency dealing: A brief introduction (data set C). In A.S. Weigend and N.A. Gershenfeld, editors, Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: The most important currencies in the market are the US dollar (which acts as a reference currency), the Japanese Yen, the British Pound, the German Mark, and the Swiss Franc <ref> [32] </ref>. Foreign exchange rates exhibit very high noise, and significant non-stationarity. Many financial institutions evaluate prediction algorithms using the percentage of times that the algorithm predicts the right trend from today until some time in the future [32]. <p> the Japanese Yen, the British Pound, the German Mark, and the Swiss Franc <ref> [32] </ref>. Foreign exchange rates exhibit very high noise, and significant non-stationarity. Many financial institutions evaluate prediction algorithms using the percentage of times that the algorithm predicts the right trend from today until some time in the future [32]. Hence, this paper considers the prediction of the direction of change in foreign exchange rates for the next business day.
Reference: [33] <author> B.G. Malkiel. </author> <title> Efficient Market Hypothesis. </title> <publisher> Macmillan, </publisher> <address> London, </address> <year> 1987. </year>
Reference-contexts: Fama [11, 12] and found broad acceptance in the financial community <ref> [33, 45, 3] </ref>. The EMH, in its weak form, asserts that the price of an asset reflects all of the information that can be obtained from past prices of the asset, i.e. the movement of the price is unpredictable.
Reference: [34] <author> B.G. Malkiel. </author> <title> A Random Walk Down Wall Street. </title> <publisher> Norton, </publisher> <address> New York, NY, </address> <year> 1996. </year>
Reference-contexts: Much effort has been expended trying to prove or disprove the EMH. Current opinion is that the theory has been disproved [42, 24], and much evidence suggests that the capital markets are not efficient <ref> [34] </ref>. If the EMH was true, then a financial series could be modeled as the addition of a noise component at each step: x (k + 1) = x (k) + *(k) (2) where *(k) is a zero mean Gaussian variable with variance oe.
Reference: [35] <author> C. McMillan, M.C. Mozer, and P. Smolensky. </author> <title> Rule induction through integrated symbolic and subsymbolic processing. </title> <editor> In John E. Moody, Steve J. Hanson, and Richard P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 4, </volume> <pages> pages 969976. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: There are 3645 data points for each exchange rate covering the period September 3, 1973 to May 18, 1987. In contrast to Weigend et al., this work 2 Rules can also be extracted from feedforward networks <ref> [21, 35, 44, 2, 39, 25] </ref>, however the recurrent network approach and deterministic finite state automata extraction seem particularly suitable for a time series problem. 3 December 16, 1997, 13:36 considers the prediction of all five exchange rates in the data and prediction for all days of the week instead of
Reference: [36] <author> J.E. Moody. </author> <title> Economic forecasting: Challenges and neural network solutions. </title> <booktitle> In Proceedings of the International Symposium on Artificial Neural Networks. </booktitle> <address> Hsinchu, Taiwan, </address> <year> 1995. </year>
Reference-contexts: The difficulty with this approach is the reduction in the already small number of training data points. The size of the training set controls a noise vs. non-stationarity tradeoff <ref> [36] </ref>. If the training set is too small, the noise makes it harder to estimate the appropriate mapping.
Reference: [37] <author> C.W. Omlin and C.L. Giles. </author> <title> Extraction of rules from discrete-time recurrent neural networks. Neural Networks, </title> <address> 9(1):4152, </address> <year> 1996. </year>
Reference-contexts: A number 17 December 16, 1997, 13:36 of people have considered the extraction of symbolic knowledge from trained neural networks for both feedforward and recurrent neural networks <ref> [9, 18, 37] </ref>. For recurrent networks, the ordered triple of a discrete Markov process (fstate; input ! next stateg) can be extracted and used to form an equivalent deterministic finite state automata (DFA). This can be done by clustering the activation values of the recurrent state neurons [37]. <p> For recurrent networks, the ordered triple of a discrete Markov process (fstate; input ! next stateg) can be extracted and used to form an equivalent deterministic finite state automata (DFA). This can be done by clustering the activation values of the recurrent state neurons <ref> [37] </ref>. The automata extracted with this process can only recognize regular grammars 5 . The algorithm used for automata extraction is the same as that described in [16]. The algorithm is based on the observation that the activations of the recurrent state neurons in a trained network tend to cluster.
Reference: [38] <author> D.W. Ruck, S.K. Rogers, K. Kabrisky, M.E. Oxley, and B.W. Suter. </author> <title> The multilayer perceptron as an approximation to an optimal Bayes estimator. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(4):296298, </volume> <year> 1990. </year>
Reference-contexts: 1 Introduction 1.1 Predicting Noisy Time Series Data The prediction of future events from noisy time series data is commonly done using various forms of statistical models [20]. Typical neural network models are closely related to statistical models, and estimate Bayesian a posteriori probabilities when given an appropriately formulated problem <ref> [38] </ref>. Neural networks have been very successful in a number of pattern recognition applications. For noisy time series prediction, neural networks typically take a delay embedding of previous inputs 1 which is mapped into a prediction.
Reference: [39] <author> R. Setiono and H. Liu. </author> <title> Symbolic representation of neural networks. </title> <booktitle> Computer, </booktitle> <address> 29(3):7177, </address> <year> 1996. </year>
Reference-contexts: There are 3645 data points for each exchange rate covering the period September 3, 1973 to May 18, 1987. In contrast to Weigend et al., this work 2 Rules can also be extracted from feedforward networks <ref> [21, 35, 44, 2, 39, 25] </ref>, however the recurrent network approach and deterministic finite state automata extraction seem particularly suitable for a time series problem. 3 December 16, 1997, 13:36 considers the prediction of all five exchange rates in the data and prediction for all days of the week instead of <p> Performance RNN Randomized Test graph is the average of the individual randomized test sets. 7 Automata Extraction A common complaint with neural networks is that the models are difficult to interpret i.e. it is not clear how the models arrive at the prediction or classification of a given input pattern <ref> [39] </ref>. A number 17 December 16, 1997, 13:36 of people have considered the extraction of symbolic knowledge from trained neural networks for both feedforward and recurrent neural networks [9, 18, 37].
Reference: [40] <author> H.T. Siegelmann and E.D. Sontag. </author> <title> On the computational power of neural nets. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 50(1):132150, </volume> <year> 1995. </year>
Reference-contexts: The induction of relatively simple grammars has been addressed often e.g. [46, 47, 16] on learning Tomita grammars [43] . It has been shown that a particular class of recurrent networks are Turing equivalent <ref> [40] </ref>. The Elman neural network has feedback from each of the hidden nodes to all of the hidden nodes, as shown in figure 3.
Reference: [41] <author> F. Takens. </author> <title> Detecting strange attractors in turbulence. In D.A. </title> <editor> Rand and L.-S. Young, editors, </editor> <booktitle> Dynamical Systems and Turbulence, volume 898 of Lecture Notes in Mathematics, </booktitle> <pages> pages 366 381, </pages> <address> Berlin, 1981. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: learn grammars [50] in order to capture any predictability in the evolution of the series. 1 For example, x (t); x (t 1); x (t 2); : : : ; x (t N + 1) form the inputs for a delay embedding of the previous N values of a series <ref> [49, 41] </ref>. 2 December 16, 1997, 13:36 The use of a recurrent neural network is significant for two reasons: firstly, the temporal relationship of the series is explicitly modeled via internal states, and secondly, it is possible to extract rules from the trained recurrent networks in the form of deterministic finite
Reference: [42] <author> S.J. Taylor, </author> <title> editor. Modelling Financial Time Series. </title> <editor> J. </editor> <publisher> Wiley & Sons, </publisher> <address> Chichester, </address> <year> 1994. </year> <month> 23 December 16, </month> <year> 1997, </year> <month> 13:36 </month>
Reference-contexts: Much effort has been expended trying to prove or disprove the EMH. Current opinion is that the theory has been disproved <ref> [42, 24] </ref>, and much evidence suggests that the capital markets are not efficient [34].
Reference: [43] <author> M. Tomita. </author> <title> Dynamic construction of finite-state automata from examples using hill-climbing. </title> <booktitle> In Proceedings of the Fourth Annual Cognitive Science Conference, </booktitle> <pages> pages 105108, </pages> <address> Ann Arbor, MI, </address> <year> 1982. </year>
Reference-contexts: The induction of relatively simple grammars has been addressed often e.g. [46, 47, 16] on learning Tomita grammars <ref> [43] </ref> . It has been shown that a particular class of recurrent networks are Turing equivalent [40]. The Elman neural network has feedback from each of the hidden nodes to all of the hidden nodes, as shown in figure 3.
Reference: [44] <author> G. Towell and J. Shavlik. </author> <title> The extraction of refined rules from knowledge based neural networks. </title> <booktitle> Machine Learning, </booktitle> <address> 131:71101, </address> <year> 1993. </year>
Reference-contexts: There are 3645 data points for each exchange rate covering the period September 3, 1973 to May 18, 1987. In contrast to Weigend et al., this work 2 Rules can also be extracted from feedforward networks <ref> [21, 35, 44, 2, 39, 25] </ref>, however the recurrent network approach and deterministic finite state automata extraction seem particularly suitable for a time series problem. 3 December 16, 1997, 13:36 considers the prediction of all five exchange rates in the data and prediction for all days of the week instead of
Reference: [45] <author> George Tsibouris and Matthew Zeidenberg. </author> <title> Testing the efficient markets hypothesis with gradient descent algorithms. </title> <editor> In A. Refenes, editor, </editor> <title> Neural Networks in the Capital Markets. </title> <publisher> John Wiley and Sons, </publisher> <year> 1995. </year>
Reference-contexts: Fama [11, 12] and found broad acceptance in the financial community <ref> [33, 45, 3] </ref>. The EMH, in its weak form, asserts that the price of an asset reflects all of the information that can be obtained from past prices of the asset, i.e. the movement of the price is unpredictable.
Reference: [46] <author> R.L. Watrous and G.M. Kuhn. </author> <title> Induction of finite state languages using second-order recurrent networks. </title> <editor> In J.E. Moody, S.J. Hanson, and R.P Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 309316, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: The induction of relatively simple grammars has been addressed often e.g. <ref> [46, 47, 16] </ref> on learning Tomita grammars [43] . It has been shown that a particular class of recurrent networks are Turing equivalent [40]. The Elman neural network has feedback from each of the hidden nodes to all of the hidden nodes, as shown in figure 3.
Reference: [47] <author> R.L. Watrous and G.M. Kuhn. </author> <title> Induction of finite-state languages using second-order recurrent networks. </title> <booktitle> Neural Computation, </booktitle> <address> 4(3):406, </address> <year> 1992. </year>
Reference-contexts: The induction of relatively simple grammars has been addressed often e.g. <ref> [46, 47, 16] </ref> on learning Tomita grammars [43] . It has been shown that a particular class of recurrent networks are Turing equivalent [40]. The Elman neural network has feedback from each of the hidden nodes to all of the hidden nodes, as shown in figure 3.
Reference: [48] <author> A.S. Weigend, B.A. Huberman, and D.E. Rumelhart. </author> <title> Predicting sunspots and exchange rates with connectionist networks. </title> <editor> In M. Casdagli and S. Eubank, editors, </editor> <booktitle> Nonlinear Modeling and Forecasting, SFI Studies in the Sciences of Complexity, Proceedings Vol. XII, </booktitle> <pages> pages 395432. </pages> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: Section 8 provides concluding remarks, and Appendix A provides full simulation details. 2 Exchange Rate Data The data we have used is publically available (http://www.cs.colorado.edu/~andreas/ Time-Series/Data/Exchange.Rates.Daily) and was first used by Weigend et al. <ref> [48] </ref>. The data consists of daily closing bids for five currencies (German Mark (DM), Japanese Yen, Swiss Franc, British Pound, and Canadian Dollar) with respect to the US Dollar and is from the Monetary Yearbook of the Chicago Mercantile Exchange. <p> of tests to be performed (i.e. 30 times as many predictions are made from the same number of training runs), thereby increasing confidence in the results. 5.4 Controlling Overfitting Highly noisy data has often been addressed using techniques such as weight decay, weight elimination and early stopping to control overfitting <ref> [48] </ref>. For this problem we use early stopping with a difference: the stopping point is chosen using multiple tests on a separate segment of data, rather than using a validation set in the normal manner. Tests showed that using a validation set in the normal manner seriously affected performance.
Reference: [49] <author> G.U. Yule. </author> <title> On a method of investigating periodicities in disturbed series with special reference to Wolfer's sunspot numbers. </title> <journal> Philosophical Transactions Royal Society London Series A, </journal> <volume> 226:267298, </volume> <year> 1927. </year>
Reference-contexts: learn grammars [50] in order to capture any predictability in the evolution of the series. 1 For example, x (t); x (t 1); x (t 2); : : : ; x (t N + 1) form the inputs for a delay embedding of the previous N values of a series <ref> [49, 41] </ref>. 2 December 16, 1997, 13:36 The use of a recurrent neural network is significant for two reasons: firstly, the temporal relationship of the series is explicitly modeled via internal states, and secondly, it is possible to extract rules from the trained recurrent networks in the form of deterministic finite <p> As is usual, a delay embedding of this series is then considered <ref> [49] </ref>: X (k; d 1 ) = (x (k); x (k 1); x (k 2); : : : ;x (k d 1 + 1)) (11) December 16, 1997, 13:36 where d 1 is the delay embedding dimension and is equal to 1 or 2 for the experiments reported here.
Reference: [50] <author> Z. Zeng, R.M. Goodman, and P. Smyth. </author> <title> Discrete recurrent neural networks for grammatical inference. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(2):320330, </volume> <year> 1994. </year> <month> 24 </month>
Reference-contexts: The problem then becomes one of grammatical inference the prediction of a given quantity from a sequence of symbols. It is then possible to take advantage of the known capabilities of recurrent neural networks to learn grammars <ref> [50] </ref> in order to capture any predictability in the evolution of the series. 1 For example, x (t); x (t 1); x (t 2); : : : ; x (t N + 1) form the inputs for a delay embedding of the previous N values of a series [49, 41]. 2
References-found: 50

