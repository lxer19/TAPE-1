URL: http://www.math.tau.ac.il/~megiddo/psfiles/lps106.ps.gz
Refering-URL: http://www.math.tau.ac.il/~megiddo/pub.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Phone: Tel  
Title: BOUNDARY BEHAVIOR OF INTERIOR POINT ALGORITHMS IN LINEAR PROGRAMMING  
Author: Nimrod Megiddoy and Michael Shubz 
Address: 650 Harry Road, San Jose, California 95120-6099, and  Box 218, Yorktown Heights, New York 10598.  
Affiliation: Sciences Research Institute, Berkeley, California. IBM Almaden Research Center,  Aviv University, Tel Aviv, Israel. IBM T. J. Watson Research Center,  
Note: This work was done in part while the authors were members at the Mathematical  Partially supported by NSF Grants.  
Abstract: This paper studies the boundary behavior of some interior point algorithms for linear programming. The algorithms considered are Karmarkar's projective rescaling algorithm, the linear rescaling algorithm which was proposed as a variation on Karmarkar's algorithm, and the logarithmic barrier technique. The study includes both the continuous trajectories of the vector fields induced by these algorithms and also the discrete orbits. It is shown that, although the algorithms are defined on the interior of the feasible polyhedron, they actually determine differentiable vector fields on the closed polyhedron. Conditions are given under which a vector field gives rise to trajectories that each visit the neighborhoods of all the vertices of the Klee-Minty cube. The linear rescaling algorithm satisfies these conditions. Thus, limits of such trajectories, obtained when a starting point is pushed to the boundary, may have an exponential number of breakpoints. It is shown that limits of projective rescaling trajectories may have only a linear number of such breakpoints. It is however shown that projective rescaling trajectories may visit the neighborhoods of linearly many vertices. The behavior of the linear rescaling algorithm near vertices is analyzed. It is shown that all the trajectories have a unique asymptotic direction of convergence to the optimum.
Abstract-found: 1
Intro-found: 1
Reference: [Bar] <author> E. R. Barnes, </author> <title> "A variation on Karmarkar's algorithm for solving linear programming problems", </title> <institution> Research Report RC11136, IBM T. J. Watson Research Center, </institution> <address> Yorktown Heights, New York, </address> <month> May </month> <year> 1985. </year>
Reference-contexts: This reflects the property that the algorithm moves in the direction of the gradient of the objective function after a projective scaling transformation has been applied. A variation on this algorithm, which was proposed in various forms by many people (e.g., <ref> [Bar, CaS, VMF] </ref>), is called the linear rescaling algorithm, reflecting the property that here a linear scaling transformation is applied before the gradient step is taken. The projective and the linear rescaling algorithms were shown in [GMSTW] to be related to the logarithmic barrier function technique using Newton's method. <p> The discrete version of the linear rescaling algorithm In this section we consider a specific choice of a step size in the linear rescaling algorithm (as in <ref> [Bar] </ref>). Given an interior point x, the algorithm determines a new point X ` (x) as follows X (x) = x kD 1 where 0 &lt; &lt; 1 is a constant. The choice of guarantees that X (x) is in the interior of the polytope (see [Bar]). <p> rescaling algorithm (as in <ref> [Bar] </ref>). Given an interior point x, the algorithm determines a new point X ` (x) as follows X (x) = x kD 1 where 0 &lt; &lt; 1 is a constant. The choice of guarantees that X (x) is in the interior of the polytope (see [Bar]). It has been proven [Bar, VMF] that for nondegenerate problems, for any interior point x, X q (x) converges to the optimal solution. In this section we study the asymptotic behavior and extensions to the boundary of this discrete algorithm. <p> The choice of guarantees that X (x) is in the interior of the polytope (see [Bar]). It has been proven <ref> [Bar, VMF] </ref> that for nondegenerate problems, for any interior point x, X q (x) converges to the optimal solution. In this section we study the asymptotic behavior and extensions to the boundary of this discrete algorithm.
Reference: [BayL] <author> D. A. Bayer and J. C. Lagarias, </author> <title> "The nonlinear geometry of linear programming I: affine and projective rescaling trajectories", </title> <type> AT&T preprint, </type> <year> 1986. </year>
Reference: [Blu] <author> L. G. Blum, </author> <title> "Towards an asymptotic analysis of Karmarkar's algorithm", Extended abstract, </title> <year> 1985. </year>
Reference-contexts: Proof: Inequality (i) implies inequality (ii) by Lemma 9.4. The equality of Lemma 9.4 divided by the inequality (ii) implies inequality (iii). Inequality (i) was proved in <ref> [Blu] </ref> and we provide here another proof. We have (D x c) n 1 (x) &gt; 0 : Now, (x) equals the projection of the vector D x c into the intersection of the nullspaces of the matrix AD x and the vector e T .
Reference: [CaS] <author> T. M. Cavalier and A. L. Soyster, </author> <title> "Some computational experience and a modification of the Karmarkar algorithm", </title> <booktitle> presented at the 12th Symposium on Mathematical Programming, </booktitle> <address> Cambridge, Mass., </address> <month> August </month> <year> 1985. </year>
Reference-contexts: This reflects the property that the algorithm moves in the direction of the gradient of the objective function after a projective scaling transformation has been applied. A variation on this algorithm, which was proposed in various forms by many people (e.g., <ref> [Bar, CaS, VMF] </ref>), is called the linear rescaling algorithm, reflecting the property that here a linear scaling transformation is applied before the gradient step is taken. The projective and the linear rescaling algorithms were shown in [GMSTW] to be related to the logarithmic barrier function technique using Newton's method.
Reference: [GMSTW] <author> P. E. Gill, W. Murray, M. A. Saunders, J. A. Tomlin and M. H. Wright, </author> <title> "On projected Newton barrier methods for linear programming and an equivalence to Karmarkar's projective method", </title> <type> Technical report SOL 85-11, </type> <institution> Systems Optimization Laboratory, Department of Operations Research, Stanford University, Stanford, </institution> <address> CA 94305, </address> <month> July </month> <year> 1985. </year>
Reference-contexts: The projective and the linear rescaling algorithms were shown in <ref> [GMSTW] </ref> to be related to the logarithmic barrier function technique using Newton's method. In this paper we study the behavior of all these algorithms. We consider both continuous and discrete versions of the algorithms. Our main interest here is in the boundary behavior of these algorithms. <p> Consider the problem (SF ()) where is fixed. As explained in <ref> [GMSTW] </ref>, the Newton search direction v at a point x is obtained by solving the following quadratic optimization problem: Minimize 1 v T r 2 F (x)v + (rF (x)) T v subject to Av = 0 ; where rF (x) = c D 1 and x : Let w denote <p> Thus, AD x O 0 : It follows that = I D x A T (AD 2 and x A T ) 1 AD x (D x c e) is the vector field corresponding to the fixed value of . It was noted in <ref> [GMSTW] </ref> that ~ ` (x) = lim V (x) : In this paper we study the boundary behavior of the above interior point algorithms for linear programming. We study both the continuous trajectories of the vector fields induced by these algorithms and the discrete orbits of the algorithms. <p> Recall that V (x) has a limit as tends to zero and, moreover, the direction of the limit V o (x) coincides with the direction assigned by the linear rescaling algorithm ~ ` (x) (see <ref> [GMSTW] </ref>). Thus, the vector field V o (x) is proper. It follows that although V (x) is not proper, it has "long" paths if is sufficiently small. More precisely, Proposition 3.5. <p> Let fi fl Thus, ~ p = Dt (x) (x T t (x) x : Notice the vector V (x) is well-defined even when is negative. The following proposition was first pointed out in <ref> [GMSTW] </ref>. Proposition 7.4. <p> In other words, v = (A T D 00 Remark D.1. We note that with g (~) = ln ~ this choice of yields the analogue of the linear rescaling method for the problem in inequality form (see also <ref> [GMSTW] </ref>). The latter can be seen as follows.
Reference: [G] <author> R. E. Gomory, </author> <title> "Trajectories tending to a critical point in 3-space", </title> <note> Annals of Mathematics 61 (1955) 140-153. </note>
Reference-contexts: It is convenient to express x in polar coordinates. We start with a slightly more general problem and follow Gomory <ref> [G] </ref>. Let be a real analytic vector field defined in the neighborhood of the origin. Consider the differential equation _x = F (x) : S n1 = fx 2 R n : kxk = 1g denote, as usual, the unit sphere in R n .
Reference: [H] <author> P. Hartman, </author> <title> Ordinary differential equations, </title> <editor> J. </editor> <publisher> Wiley and Sons, </publisher> <address> New York, </address> <year> 1964. </year>
Reference-contexts: Thus every point tends to a zero. The stable sets of zeros in the boundary stay in the boundary since the boundary is invariant. Thus the orbit of any interior point tends to the point (0; 1 p n e). It does so with a definite limiting direction (see <ref> [H] </ref> on C 1 linearization for contractions). This implies that the projected curve in the x-variable is tangent to the ray through e at the origin. Note that throughout this section we used differentiability only up to second order.
Reference: [Kar1] <author> N. Karmarkar, </author> <title> "A new polynomial-time algorithm for linear programming ", 74 in: </title> <booktitle> Proceedings of the 16th Annual ACM Symposium on Theory of Computing (1984), ACM, </booktitle> <address> New York, </address> <year> 1984, </year> <pages> pp. </pages> <note> 302-311; revised version: Combinatorica 4 (1984) pp. 373-395. </note>
Reference-contexts: 1. Introduction Interest in interior point algorithms for linear programming was revived by the work of Karmarkar <ref> [Kar1] </ref>. In this paper we sometimes refer to Karmarkar's algorithm also as the projective rescaling algorithm. This reflects the property that the algorithm moves in the direction of the gradient of the objective function after a projective scaling transformation has been applied. <p> We note that since the problem is in the minimization form, the new point has the form x ff (x)~ ` (x) where ff (x) is positive. 2. The projective rescaling algorithm. Following <ref> [Kar1] </ref>, the algorithm is stated with respect to the linear programming problem given in the following form ("Karmarkar's 3 standard form"): (KSF ) Minimize c T x subject to Ax = 0 e T x = 1 where A 2 R (m1)fin (1 m n), x; c 2 R n and <p> Since the two vectors are multiples of each other they are simultaneously orthogonal to J . Now it is not hard to see (as in <ref> [Kar1] </ref>) that when J (x) 6= 0, the potential function evaluated at Y J (x) is strictly less that its value at x and the same for any point on the line segment between x and Y J (x).
Reference: [Me1] <author> N. Megiddo, </author> <title> "A variation on Karmarkar's algorithm", </title> <type> unpublished manuscript, </type> <month> December </month> <year> 1984. </year>
Reference: [Me2] <author> N. Megiddo, </author> <title> "Pathways to the optimal set in linear programming", </title> <type> Research Report RJ 5295, </type> <institution> IBM Almaden Research Center, </institution> <year> 1986. </year>
Reference-contexts: Then any interior solution curve is tangent to the vector ~c m+1 1 at the origin where the vector (~c m+1 ; ; ~c n ) is the reduced cost vector. The discrete analog of this fact was observed experimentally by Earl Barnes. Subsequent to this analysis Megiddo <ref> [Me2] </ref> found different behavior for a class of differential equations related to the barrier method. 30 6.
Reference: [Re] <author> J. Renegar, </author> <title> "A polynomial-time algorithm, based on Newton's method, for linear programming", </title> <type> report MSRI 07118-86, </type> <institution> Mathematical Sciences Research Institute, Berkeley, California, </institution> <year> 1986. </year>
Reference: [Sh] <author> M. Shub, </author> <title> Global stability of dynamical systems, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: The transformation ~ t is called the time t map of the flow. For the proof of the following proposition the reader is referred to to chapter 2 in <ref> [Sh] </ref>: Proposition 5.2. <p> This type of argument can be found in center-unstable manifold theory in <ref> [Sh] </ref>. Given a point x in the interior of the polytope and an optimal point x fl , let a ` (x) = lim sup 1 log kX q (x) x fl k be the asymptotic rate of convergence to the optimum. We have shown Corollary 8.8. <p> Thus, dY (x) = (1 + (x)(x))I and hence x is a repeller (a source; see <ref> [Sh] </ref>). Lemma 9.8.
Reference: [VMF] <author> R. J. Vanderbei, M. J. Meketon and B. A. Freedman, </author> <title> "A modification of Kar-markar's linear programming algorithm", </title> <note> Algorithmica 1 (1986), to appear. 75 </note>
Reference-contexts: This reflects the property that the algorithm moves in the direction of the gradient of the objective function after a projective scaling transformation has been applied. A variation on this algorithm, which was proposed in various forms by many people (e.g., <ref> [Bar, CaS, VMF] </ref>), is called the linear rescaling algorithm, reflecting the property that here a linear scaling transformation is applied before the gradient step is taken. The projective and the linear rescaling algorithms were shown in [GMSTW] to be related to the logarithmic barrier function technique using Newton's method. <p> The linear rescaling algorithm. Following the description of <ref> [VMF] </ref>, the algorithm is stated with respect to the linear programming problem in the standard form. Also, it is assumed that a point x o is known such that Ax o = b and x o &gt; 0. <p> The choice of guarantees that X (x) is in the interior of the polytope (see [Bar]). It has been proven <ref> [Bar, VMF] </ref> that for nondegenerate problems, for any interior point x, X q (x) converges to the optimal solution. In this section we study the asymptotic behavior and extensions to the boundary of this discrete algorithm.
References-found: 13

