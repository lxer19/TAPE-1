URL: http://www.cs.toronto.edu/~paullu/Papers/aurora.ipps97.ps.Z
Refering-URL: http://www.cs.toronto.edu/~paullu/cvpublications.html
Root-URL: 
Email: paullu@sys.utoronto.ca  
Title: Aurora: Scoped Behaviour for Per-Context Optimized Distributed Data Sharing  
Author: Paul Lu 
Address: Toronto, Ontario, Canada M5S 3G4  
Affiliation: Dept. of Computer Science University of Toronto  
Abstract: We introduce the all-software, standard C++-based Aurora distributed shared data system. As with related systems, it provides a shared data abstraction on distributed memory hardware. An innovation in Aurora is the use of scoped be-haviour for per-context data sharing optimizations (i.e., portion of source code, such as a loop or phase). With scoped behaviour, a new language scope (e.g., nested braces) can be used to optimize the data sharing behaviour of the selected source code. Different scopes and different shared data can be optimized in different ways. Thus, scoped be-haviour provides a novel level of flexibility to incrementally tune the parallel performance of an application. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Amza, A. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Ra-jamony, W. Yu, and W. Zwaenepoel. TreadMarks: </author> <title> Shared Memory Computing on Networks of Workstations. </title> <journal> IEEE Computer, </journal> <volume> 29(2) </volume> <pages> 18-28, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: Accessing local and remote data using the same programming interface (i.e., reads and writes) is often more convenient than mixing local accesses with message passing. Consequently, the desire to support logically shared data, even on distributed memory hardware, has resulted in a variety of distributed shared memory (DSM) <ref> [15, 5, 1] </ref> and distributed shared data (DSD) [3, 20, 12] systems. DSM systems are usually based on fixed-sized units of sharing, often a page, and DSD systems usually have a variable granularity of sharing. <p> Discussion and related work Since Ivy [15], the first DSM system, a large body of work has emerged in the area of DSM and DSD systems. These systems vary in their strategies to overcome false sharing, tolerance for high communication costs <ref> [5, 1] </ref>, reliance on compiler [4, 3, 6] and hardware support [13, 19], implementation techniques [20, 12], and their extensibility. Related work in parallel array classes (for example, [14]) has also addressed the basic problem of transparently sharing data.
Reference: [2] <author> E. Arjomandi, W. O'Farrell, and G. Wilson. </author> <title> Smart Messages: An Object-Oriented Communication Mechanism for Parallel Systems. </title> <booktitle> In Proc. USENIX 1996 Conference on Object-Oriented Technologies, </booktitle> <address> Toronto, Ontario, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Operations on the member data of an active object are handled by the member functions, as is the case with passive objects. However, unlike passive objects, the member functions of an active object can also be remotely invoked with a smart message <ref> [2] </ref>. In ABC++, smart messages resemble normal method invocations and automate parameter marshalling and de-marshalling. Distributed memory and shared memory ABC++ run-time systems are available to Aurora applications.
Reference: [3] <author> H. Bal, M. Kaashoek, and A. Tanenbaum. Orca: </author> <title> A Language for Parallel Programming of Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3), </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: Consequently, the desire to support logically shared data, even on distributed memory hardware, has resulted in a variety of distributed shared memory (DSM) [15, 5, 1] and distributed shared data (DSD) <ref> [3, 20, 12] </ref> systems. DSM systems are usually based on fixed-sized units of sharing, often a page, and DSD systems usually have a variable granularity of sharing. <p> Discussion and related work Since Ivy [15], the first DSM system, a large body of work has emerged in the area of DSM and DSD systems. These systems vary in their strategies to overcome false sharing, tolerance for high communication costs [5, 1], reliance on compiler <ref> [4, 3, 6] </ref> and hardware support [13, 19], implementation techniques [20, 12], and their extensibility. Related work in parallel array classes (for example, [14]) has also addressed the basic problem of transparently sharing data.
Reference: [4] <author> M. Beltrametti, K. Bobey, and J. Zorbas. </author> <title> The Control Mechanism for the Myrias Parallel Computer System. </title> <journal> Computer Architecture News, </journal> <volume> 16(4) </volume> <pages> 21-30, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Discussion and related work Since Ivy [15], the first DSM system, a large body of work has emerged in the area of DSM and DSD systems. These systems vary in their strategies to overcome false sharing, tolerance for high communication costs [5, 1], reliance on compiler <ref> [4, 3, 6] </ref> and hardware support [13, 19], implementation techniques [20, 12], and their extensibility. Related work in parallel array classes (for example, [14]) has also addressed the basic problem of transparently sharing data.
Reference: [5] <author> J. Bennett, J. Carter, and W. Zwaenepoel. Munin: </author> <title> Distributed Shared Memory Based on Type-Specific Memory Coherence. </title> <booktitle> In Proc. 1990 Conference on Principles and Practice of Parallel Programming. </booktitle> <publisher> ACM Press, </publisher> <year> 1990. </year>
Reference-contexts: Accessing local and remote data using the same programming interface (i.e., reads and writes) is often more convenient than mixing local accesses with message passing. Consequently, the desire to support logically shared data, even on distributed memory hardware, has resulted in a variety of distributed shared memory (DSM) <ref> [15, 5, 1] </ref> and distributed shared data (DSD) [3, 20, 12] systems. DSM systems are usually based on fixed-sized units of sharing, often a page, and DSD systems usually have a variable granularity of sharing. <p> Discussion and related work Since Ivy [15], the first DSM system, a large body of work has emerged in the area of DSM and DSD systems. These systems vary in their strategies to overcome false sharing, tolerance for high communication costs <ref> [5, 1] </ref>, reliance on compiler [4, 3, 6] and hardware support [13, 19], implementation techniques [20, 12], and their extensibility. Related work in parallel array classes (for example, [14]) has also addressed the basic problem of transparently sharing data.
Reference: [6] <author> B. Bershad, M. Zekauskas, and W. Sawdon. </author> <title> The Midway Distributed Shared Memory System. </title> <booktitle> In Proc. 38th IEEE International Computer Conference (COMPCON Spring'93), </booktitle> <pages> pages 528-537, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Discussion and related work Since Ivy [15], the first DSM system, a large body of work has emerged in the area of DSM and DSD systems. These systems vary in their strategies to overcome false sharing, tolerance for high communication costs [5, 1], reliance on compiler <ref> [4, 3, 6] </ref> and hardware support [13, 19], implementation techniques [20, 12], and their extensibility. Related work in parallel array classes (for example, [14]) has also addressed the basic problem of transparently sharing data. <p> We refer to this as per-object release consistency. In this way, Aurora partitions the consistency namespace, as does scope consistency [11] and entry consistency <ref> [6] </ref>. However, Aurora does not require the hardware support of scope consistency and Aurora has a per-object granularity of coherence, instead of per-page. Also, Aurora does not require the special compiler support of, say, the Midway implementation of entry consistency. The basic ideas behind scoped behaviour are not new.
Reference: [7] <author> E. Brooks III. PCP: </author> <title> A Parallel Extension of C That is 99% Fat Free. </title> <type> Technical Report UCRL-99673, </type> <institution> Lawrence Livermore National Laboratory, Livermore, </institution> <address> CA, </address> <year> 1988. </year>
Reference-contexts: Aurora also provides a fairly standard single-program, multiple-data (SPMD) process model (similar to the PCP system <ref> [7] </ref>). The model includes teams of threads, sequential sections (i.e., code to be executed by only one thread, usually the team's master), and synchronization barriers. 2.1.
Reference: [8] <author> J. Coplien. </author> <title> Advanced C++: Programming Styles and Idioms. </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: By default, updates to scalar1 are immediately visible to other processors, which may require a message send for each update. Immediate updates may be expensive, but it is the overall least error-prone default behaviour. Any of the C++ built-in types or any user-defined concrete type <ref> [8] </ref>, as with scalar2, can be an independent unit of sharing. Shared vectors, such as vector1, are similar to shared scalars, except that the vector elements can be distributed across nodes and the subscript operator is overloaded to support indexing. <p> The standard C++ compiler generates the correct access code using the Aurora class library. 2.3. Basic architecture and implementation Both shared scalars and vectors in Aurora are implemented through the handle-body idiom <ref> [8] </ref>. The handle class implements the appropriate overloaded operators and type constructors. The handle also provides the location and distribution transparency required to access remote data. In contrast, the body objects contain the actual data. The separation of handle and body is important to Aurora's flexibility.
Reference: [9] <author> N. Doss, W. Gropp, E. Lusk, and A. Skjellum. </author> <title> A Model Implementation of MPI. </title> <type> Technical Report MCS-P393-1193, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL, </institution> <year> 1993. </year>
Reference-contexts: Each workstation has a single 133 MHz PowerPC 604 processor and 64 MB of memory. The software includes IBM's AIX 4.1 operating system, POSIX threads (pthreads), the xlC C++ compiler, a prototype version of ABC++, and the MPICH <ref> [9] </ref> implementation of MPI for inter-workstation communication. A single 100 Mbit/s Ethernet network is used for these experiments. 3.1. Matrix multiplication implemented in Figure 1. Speedups are given for multiplying two 512 fi 512 matrices and for two 640 fi 640 matrices.
Reference: [10] <author> K. Gharachorloo, D. E. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. L. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proc. 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Data placement mechanisms in Aurora enhance the effectiveness of owner-computes. Caching read-only data in local memory further eliminates remote read accesses. A special form of prefetching into a read cache is also supported. As well, Aurora provides a form of release consistency <ref> [10] </ref> to amortize the cost of remote write accesses by batching updates. Aurora is unique in its use of the scoped behaviour mechanism for applying optimizations.
Reference: [11] <author> L. Iftode, J. Singh, and K. Li. </author> <title> Scope Consistency: a Bridge between Release Consistency and Entry Consistency. </title> <booktitle> In Proc. 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: We refer to this as per-object release consistency. In this way, Aurora partitions the consistency namespace, as does scope consistency <ref> [11] </ref> and entry consistency [6]. However, Aurora does not require the hardware support of scope consistency and Aurora has a per-object granularity of coherence, instead of per-page. Also, Aurora does not require the special compiler support of, say, the Midway implementation of entry consistency.
Reference: [12] <author> K. Johnson, M. Kaashoek, and D. Wallach. </author> <title> CRL: High-Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proc. 15th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 213-228, </pages> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: Consequently, the desire to support logically shared data, even on distributed memory hardware, has resulted in a variety of distributed shared memory (DSM) [15, 5, 1] and distributed shared data (DSD) <ref> [3, 20, 12] </ref> systems. DSM systems are usually based on fixed-sized units of sharing, often a page, and DSD systems usually have a variable granularity of sharing. <p> These systems vary in their strategies to overcome false sharing, tolerance for high communication costs [5, 1], reliance on compiler [4, 3, 6] and hardware support [13, 19], implementation techniques <ref> [20, 12] </ref>, and their extensibility. Related work in parallel array classes (for example, [14]) has also addressed the basic problem of transparently sharing data. <p> Aurora's innovation is the application of scoped behaviour, with the handle-body architecture, to the problem of per-context and per-object data sharing optimization. In a largely C context, shared regions [20], SAM [21], and CRL <ref> [12] </ref>, associate run-time coherence actions with programmer-inserted code annotations (i.e., function calls). One advantage of the automatic invocation of constructors and destructors in Aurora is that it is impossible to omit an annotation and miss a coherence action.
Reference: [13] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Si-moni, K. Gharachorloo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proc. 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: These systems vary in their strategies to overcome false sharing, tolerance for high communication costs [5, 1], reliance on compiler [4, 3, 6] and hardware support <ref> [13, 19] </ref>, implementation techniques [20, 12], and their extensibility. Related work in parallel array classes (for example, [14]) has also addressed the basic problem of transparently sharing data.
Reference: [14] <author> M. Lemke and D. Quinlan. </author> <title> P++, a C++ Virtual Shared Grids Based Programming Environment for Architecture-Independent Development of Structured Grid Applications. </title> <booktitle> In Proc. </booktitle> <publisher> CONPAR 92-VAPP V. Springer-Verlag, </publisher> <month> September </month> <year> 1992. </year>
Reference-contexts: These systems vary in their strategies to overcome false sharing, tolerance for high communication costs [5, 1], reliance on compiler [4, 3, 6] and hardware support [13, 19], implementation techniques [20, 12], and their extensibility. Related work in parallel array classes (for example, <ref> [14] </ref>) has also addressed the basic problem of transparently sharing data. One refinement in Aurora's implementation of release consistency is that only the affected data objects are made consistent at the end of a scope, instead of an entire page or all of memory.
Reference: [15] <author> K. Li. IVY: </author> <title> A Shared Virtual Memory System for Parallel Computing. </title> <booktitle> In Proc. 1988 International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 94-101, </pages> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Accessing local and remote data using the same programming interface (i.e., reads and writes) is often more convenient than mixing local accesses with message passing. Consequently, the desire to support logically shared data, even on distributed memory hardware, has resulted in a variety of distributed shared memory (DSM) <ref> [15, 5, 1] </ref> and distributed shared data (DSD) [3, 20, 12] systems. DSM systems are usually based on fixed-sized units of sharing, often a page, and DSD systems usually have a variable granularity of sharing. <p> Adding more processors and more keys result in more contention for the Ethernet, especially during Phase 3. Experimenting with different software and hardware techniques to deal with contention will be the topic of future work. 4. Discussion and related work Since Ivy <ref> [15] </ref>, the first DSM system, a large body of work has emerged in the area of DSM and DSD systems.
Reference: [16] <author> X. Li, P. Lu, J. Schaeffer, J. Shillington, P. Wong, and H. Shi. </author> <title> On the Versatility of Parallel Sorting by Regular Sampling. </title> <journal> Parallel Computing, </journal> <volume> 19 </volume> <pages> 1079-1103, </pages> <year> 1993. </year>
Reference-contexts: However, Aurora provides it in the context of scoped behaviour, it abstracts the implementation details of the padding, and it automates the prefetching behaviour required for this archetypical data sharing pattern. 3.3. Parallel sorting by regular sampling The Parallel Sorting by Regular Sampling (PSRS) application kernel <ref> [22, 16] </ref> has been implemented using Aurora. PSRS is a general purpose, comparison-based sort with key exchange. It can be communication intensive since the number of keys exchanged grows linearly with the problem size. The basic PSRS algorithm consists of four distinct phases.
Reference: [17] <author> H. Lu, S. Dwarkadas, A. Cox, and W. Zwaenepoel. </author> <title> Message-Passing vs. Distributed Shared Memory on Networks of Workstations. </title> <booktitle> In Proc. of Supercomputing'95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: False sharing, when performance is degraded by the interference between updates to logically independent data located in the same unit of data management, is a potential problem with DSM systems <ref> [17] </ref>. In DSD systems, false sharing is avoided by managing independent data as separate objects. Another challenge for distributed systems is to overcome the network latency and bandwidth limitations of typical distributed memory platforms. In particular, remote accesses of shared data can be expensive on general-purpose networks.
Reference: [18] <author> W. O'Farrell, F. Eigler, S. Pullara, and G. Wilson. </author> <title> ABC++. </title> <editor> In G. V. Wilson and P. Lu, editors, </editor> <title> Parallel Programming Using C++. </title> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Aurora's mixed-parallelism process model includes both task parallelism and data parallelism. The various process models can be mixed, in reasonable ways, within a single application. Currently, the task parallelism is provided by the underlying ABC++ library through active objects <ref> [18] </ref>. Active objects are normal C++ objects that contain their own thread of control. <p> The notion of nested scopes is fundamental to block-structured sequential languages. The association of actions with C++ constructors and destructors is also not new. For example, ABC++ also associates data movement actions with the scoped lock handles of its parametric shared regions <ref> [18] </ref>. Aurora's innovation is the application of scoped behaviour, with the handle-body architecture, to the problem of per-context and per-object data sharing optimization. In a largely C context, shared regions [20], SAM [21], and CRL [12], associate run-time coherence actions with programmer-inserted code annotations (i.e., function calls).
Reference: [19] <author> S. Reinhardt, J. Larus, and D. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proc. 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: These systems vary in their strategies to overcome false sharing, tolerance for high communication costs [5, 1], reliance on compiler [4, 3, 6] and hardware support <ref> [13, 19] </ref>, implementation techniques [20, 12], and their extensibility. Related work in parallel array classes (for example, [14]) has also addressed the basic problem of transparently sharing data.
Reference: [20] <author> H. Sandhu, B. Gamsa, and S. Zhou. </author> <title> The Shared Regions Approach to Software Cache Coherence. </title> <booktitle> In Proc. Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 229-238, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Consequently, the desire to support logically shared data, even on distributed memory hardware, has resulted in a variety of distributed shared memory (DSM) [15, 5, 1] and distributed shared data (DSD) <ref> [3, 20, 12] </ref> systems. DSM systems are usually based on fixed-sized units of sharing, often a page, and DSD systems usually have a variable granularity of sharing. <p> These systems vary in their strategies to overcome false sharing, tolerance for high communication costs [5, 1], reliance on compiler [4, 3, 6] and hardware support [13, 19], implementation techniques <ref> [20, 12] </ref>, and their extensibility. Related work in parallel array classes (for example, [14]) has also addressed the basic problem of transparently sharing data. <p> For example, ABC++ also associates data movement actions with the scoped lock handles of its parametric shared regions [18]. Aurora's innovation is the application of scoped behaviour, with the handle-body architecture, to the problem of per-context and per-object data sharing optimization. In a largely C context, shared regions <ref> [20] </ref>, SAM [21], and CRL [12], associate run-time coherence actions with programmer-inserted code annotations (i.e., function calls). One advantage of the automatic invocation of constructors and destructors in Aurora is that it is impossible to omit an annotation and miss a coherence action.
Reference: [21] <author> D. Scales and M. Lam. </author> <title> The Design and Evaluation of a Shared Object System for Distributed Memory Machines. </title> <booktitle> In Proc. 1st Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 101-114, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Aurora's innovation is the application of scoped behaviour, with the handle-body architecture, to the problem of per-context and per-object data sharing optimization. In a largely C context, shared regions [20], SAM <ref> [21] </ref>, and CRL [12], associate run-time coherence actions with programmer-inserted code annotations (i.e., function calls). One advantage of the automatic invocation of constructors and destructors in Aurora is that it is impossible to omit an annotation and miss a coherence action.
Reference: [22] <author> H. Shi and J. Schaeffer. </author> <title> Parallel Sorting by Regular Sampling. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14(4) </volume> <pages> 361-372, </pages> <year> 1992. </year>
Reference-contexts: However, Aurora provides it in the context of scoped behaviour, it abstracts the implementation details of the padding, and it automates the prefetching behaviour required for this archetypical data sharing pattern. 3.3. Parallel sorting by regular sampling The Parallel Sorting by Regular Sampling (PSRS) application kernel <ref> [22, 16] </ref> has been implemented using Aurora. PSRS is a general purpose, comparison-based sort with key exchange. It can be communication intensive since the number of keys exchanged grows linearly with the problem size. The basic PSRS algorithm consists of four distinct phases.
References-found: 22

