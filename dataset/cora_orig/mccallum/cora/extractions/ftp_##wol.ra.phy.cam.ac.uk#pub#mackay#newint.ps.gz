URL: ftp://wol.ra.phy.cam.ac.uk/pub/mackay/newint.ps.gz
Refering-URL: http://131.111.48.24/mackay/README.html
Root-URL: 
Email: Email: mackay@mrao.cam.ac.uk.  Email: takeuchi@matsumoto.elec.waseda.ac.jp.  
Title: Interpolation Models with Multiple  
Author: Hyperparameters David J C MacKay Ryo Takeuchi 
Address: Cambridge, United Kingdom.  Japan.  
Affiliation: Laboratory,  Electrical Engineering department, Waseda University, Tokyo,  
Note: David MacKay is with the Cavendish  Ryo Takeuchi is with the  
Date: January 3, 1997  
Abstract: A traditional interpolation model is characterized by the choice of reg-ularizer applied to the interpolant, and the choice of noise model. Typically, the regularizer has a single regularization constant ff, and the noise model has a single parameter fi. The ratio ff=fi alone is responsible for determining globally all these attributes of the interpolant: its `complexity', `flexibility', `smoothness', `characteristic scale length', and `characteristic amplitude'. We suggest that interpolation models should be able to capture more than just one flavour of simplicity and complexity. We describe Bayesian models in which the interpolant has a smoothness that varies spatially. We emphasize the importance, in practical implementation, of the concept of `conditional convexity' when designing models with many hyperparameters. We apply the new models to the interpolation of neuronal spike data and demonstrate a substantial improvement in generalization error. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Blake, A., and Zisserman, A. </author> <title> (1987) Visual Reconstruction. </title> <address> Cambridge Mass.: </address> <publisher> MIT Press. </publisher>
Reference: <author> Carter, C. K., and Kohn, R. </author> <title> (1994) On Gibbs sampling for state-space models. </title> <type> Biometrika 81 (3): </type> <pages> 541-553. </pages> <note> 22 Gilks, </note> <author> W., and Wild, P. </author> <title> (1992) Adaptive rejection sampling for Gibbs sampling. </title> <journal> Applied Statistics 41: </journal> <pages> 337-348. </pages>
Reference: <author> Gu, C., and Wahba, G. </author> <title> (1991) Minimizing GCV/GML scores with multiple smoothing parameters via the Newton method. </title> <journal> SIAM J. Sci. Stat. Comput. </journal> <volume> 12: </volume> <pages> 383-398. </pages>
Reference-contexts: Some authors view this `empirical Bayes' approach as controversial and inaccurate (Wolpert 1993), but it is widely used under various names such as `ML-II', and is closely related to `generalized maximum likelihood' <ref> (Gu and Wahba 1991) </ref>. The ideal Bayesian method would put a proper prior on the hyperparameters and marginalize over them, but optimization of the hyperparameters is computationally more convenient and often gives predictive distributions that are indistinguishable (MacKay 1996).
Reference: <author> Gull, S. F. </author> <title> (1989) Developments in maximum entropy data analysis. In Maximum Entropy and Bayesian Methods, Cambridge 1988 , ed. by J. </title> <booktitle> Skilling, </booktitle> <pages> pp. 53-71, </pages> <address> Dordrecht. </address> <publisher> Kluwer. </publisher>
Reference: <author> Kimeldorf, G. S., and Wahba, G. </author> <title> (1970) A correspondence between Bayesian estimation of stochastic processes and smoothing by splines. </title> <journal> Annals of Mathematical Statistics 41 (2): </journal> <pages> 495-502. </pages>
Reference-contexts: Typically the regularizer is a quadratic functional of the interpolant and has a single associated regularization constant ff, and the noise model is also quadratic and has a single parameter fi. For example, the splines prior for the function y (x) <ref> (Kimeldorf and Wahba 1970) </ref> is: 1 log P (y (x)jff; H 1 ) = 2 Z dx [y (p) (x)] 2 + const; (1) where y (p) denotes the pth derivative of y.
Reference: <author> Lewicki, M. </author> <title> (1994) Bayesian modeling and classification of neural signals. </title> <booktitle> Neural Computation 6 (5): </booktitle> <pages> 1005-1030. </pages>
Reference-contexts: simplicity and complexity? And should not the interpolant's smoothness, for example, be able to vary spatially? 1.1 Example: Neural spike modelling An example of a function from a real system is shown in figure 1; this is the action potential of a neuron deduced from recordings of 40 distinct events <ref> (Lewicki 1994) </ref>. The graph was created by fitting a simple spline model (with p = 1) to the data. This function has one `spiky' region with large characteristic amplitude and short spatial scale. Elsewhere the true function is smooth.
Reference: <author> MacKay, D. J. C. </author> <title> (1992) Bayesian interpolation. </title> <booktitle> Neural Computation 4 (3): </booktitle> <pages> 415-447. </pages>
Reference: <author> MacKay, D. J. C. </author> <title> (1994) Bayesian non-linear modelling for the prediction competition. </title> <journal> In ASHRAE Transactions, </journal> <volume> V.100, </volume> <pages> Pt.2 , pp. 1053-1062, </pages> <address> Atlanta Georgia. ASHRAE. </address>
Reference: <author> MacKay, D. J. C. </author> <title> (1995) Probabilistic networks: New models and new methods. </title> <booktitle> In ICANN '95 , pp. </booktitle> <pages> 331-337, </pages> <address> Paris. </address> <publisher> EC2 and Cie. </publisher>
Reference-contexts: We have used the Gibbs sampling software `BUGS' (Thomas et al. 1992) to implement a similar interpolation model in which the Gaussian noise level is a spatially varying function fi (x) <ref> (MacKay 1995) </ref>. 4 Some Generalizations 4.1 Strategies for making models with multiple hyperpa rameters We now discuss more generally the construction of hierarchical models with multiple hyperparameters. Consider a Gaussian prior on some parameters w, equivalent to the function y (x) in the earlier example.
Reference: <author> MacKay, D. J. C. </author> <year> (1996) </year> <month> Hyperparameters: </month> <title> Optimize, or integrate out? In Maximum Entropy and Bayesian Methods, </title> <editor> Santa Barbara 1993 , ed. by G. </editor> <booktitle> Hei-dbreder, </booktitle> <pages> pp. 43-60, </pages> <address> Dordrecht. </address> <publisher> Kluwer. </publisher>
Reference-contexts: That a model is conditionally convex does not guarantee that marginal distributions of all variables are unimodal. For example the traditional model's posterior marginals P (wjD) and P (ffjD) are not necessarily unimodal; but good unimodal approximations to them can often be made <ref> (MacKay 1996) </ref>. So we conjecture that conditional convexity is a desirable property for a tractable model. We now generalize the spline model of equation (1) to a model with multiple hyperparameters that is conditionally convex, and demonstrate it on the neural spike data. <p> The ideal Bayesian method would put a proper prior on the hyperparameters and marginalize over them, but optimization of the hyperparameters is computationally more convenient and often gives predictive distributions that are indistinguishable <ref> (MacKay 1996) </ref>. We use a discrete representation of y (x) and ff (x) on a finely spaced grid, fx c g, writing y (x) ! y, ff (x; u) ! fff c ff (x c ; u)g and hc h (x c ).
Reference: <author> Muller, H. G., and Stadtmuller, U. </author> <title> (1987) Variable bandwidth kernel estimators of regression-curves. </title> <journal> Annals of Statistics 15 (1): </journal> <pages> 182-201. </pages> <note> 23 Neal, </note> <author> R. M. </author> <title> (1993) Probabilistic inference using Markov chain Monte Carlo methods. </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Dept. of Computer Science, University of Toronto. </institution>
Reference-contexts: Prior work making use of variable hyperparameters includes the modelling of data with non-Gaussian innovations or observation noise (see, e.g., (West 1984; Carter and Kohn 1994; Shephard 1994)). The interpolation models we propose might be viewed as Bayesian versions of the `variable bandwidth' kernel regression technique <ref> (Muller and Stadtmuller 1987) </ref>. The aim of our new model is also similar to the goal of inferring the locations of discontinuities in a function, studied by Blake and Zisserman (1987).
Reference: <author> Neal, R. M. </author> <title> (1996) Bayesian Learning for Neural Networks. </title> <booktitle> Number 118 in Lecture Notes in Statistics. </booktitle> <address> New York: </address> <publisher> Springer. </publisher>
Reference: <author> Shephard, N. </author> <title> (1994) Partial non-Gaussian state-space. </title> <type> Biometrika 81 (1): </type> <pages> 115-131. </pages>
Reference: <author> Smith, A. </author> <title> (1991) Bayesian computational methods. </title> <journal> Philosophical Transactions of the Royal Society of London A 337: </journal> <pages> 369-386. </pages>
Reference-contexts: By contrast we attempt to 4 create new hierarchical models that are, for practical purposes, convex. 2 Tractable hierarchical modelling: Convexity Bayesian statistical inference is often implemented either by Gaussian approximations about modes of distributions, or by Markov Chain Monte Carlo methods <ref> (Smith 1991) </ref>. Both methods clearly have a better chance of success if the posterior probability distribution over the model parameters and hyperparame-ters is not dominated by multiple distinct optima.
Reference: <author> Thomas, A., Spiegelhalter, D. J., and Gilks, W. R. </author> <title> (1992) BUGS: A program to perform Bayesian inference using Gibbs sampling. In Bayesian Statistics 4 , ed. </title> <editor> by J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. </editor> <volume> Smith, </volume> <pages> pp. 837-842. </pages> <publisher> Oxford: Clarendon Press. </publisher>
Reference-contexts: One could use Markov chain Monte Carlo methods such as Gibbs sampling or hybrid Monte Carlo, both of which would involve a similar computational load (see Neal (1993) for an excellent review). We have used the Gibbs sampling software `BUGS' <ref> (Thomas et al. 1992) </ref> to implement a similar interpolation model in which the Gaussian noise level is a spatially varying function fi (x) (MacKay 1995). 4 Some Generalizations 4.1 Strategies for making models with multiple hyperpa rameters We now discuss more generally the construction of hierarchical models with multiple hyperparameters.
Reference: <author> West, M. </author> <title> (1984) Outlier models and prior distributions in Bayesian linear-regression. </title> <journal> Journal of the Royal Statistical Society Series B-Methodological 46 (3): </journal> <pages> 431-439. </pages>
Reference: <author> Williams, C. K. I., and Rasmussen, C. E. </author> <title> (1996) Gaussian processes for regression. </title> <booktitle> In Advances in Neural Information Processing Systems 8 , ed. </booktitle> <editor> by D. S. Touretzky, M. C. Mozer, and M. E. Hasselmo. </editor> <publisher> MIT Press. </publisher>
Reference-contexts: This is why the sum model is convex. The exponential sum model, we conjecture, pushes flexibility to the limits of convexity. We believe these ideas may be relevant to the design of computationally tractable Gaussian process models for non-linear regression <ref> (Williams and Rasmussen 1996) </ref>. 4.4 How to represent a covariance matrix In this paper we have used interpolation of neural spike data as a test bed for the new models. We now discuss another task to which the general principles we have discussed may apply.
Reference: <author> Wolpert, D. H. </author> <title> (1993) On the use of evidence in neural networks. </title> <booktitle> In Advances in Neural Information Processing Systems 5 , ed. </booktitle> <editor> by C. L. Giles, S. J. Hanson, and J. D. </editor> <booktitle> Cowan, </booktitle> <pages> pp. 539-546, </pages> <address> San Mateo, California. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 24 </pages>
Reference-contexts: Some authors view this `empirical Bayes' approach as controversial and inaccurate <ref> (Wolpert 1993) </ref>, but it is widely used under various names such as `ML-II', and is closely related to `generalized maximum likelihood' (Gu and Wahba 1991).
References-found: 18

