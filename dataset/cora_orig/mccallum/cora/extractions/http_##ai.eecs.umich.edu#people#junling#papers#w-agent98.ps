URL: http://ai.eecs.umich.edu/people/junling/papers/w-agent98.ps
Refering-URL: http://ai.eecs.umich.edu/people/junling/publication.html
Root-URL: http://www.cs.umich.edu
Email: fjunling, wellmang@umich.edu  
Title: Online Learning about Other Agents in a Dynamic Multiagent System  
Author: Junling Hu and Michael P. Wellman 
Web: http://ai.eecs.umich.edu/people/fjunling,wellmang  
Address: Ann Arbor, MI 48109-2110, USA  
Affiliation: Artificial Intelligence Laboratory University of Michigan  
Abstract: We analyze the problem of learning about other agents in a class of dynamic multiagent systems, where performance of the primary agent depends on behavior of the others. We consider an online version of the problem, where agents must learn models of the others in the course of continual interactions. We implement various levels of recursive model in a simulated double auction market. Our experiments show that performance of an agent can be quite sensitive to its assumptions about the policies of other agents, and (not surprisingly), when there is substantial uncertainty about the level of sophistication of other agents, minimizing assumptions might be the best policy. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Tucker Balch. </author> <title> Learning roles: Behavioral diversity in robot teams. </title> <note> In Sen [13]. </note>
Reference-contexts: 1 Introduction A dynamic multiagent system, loosely speaking, is a system that changes over time due to the interaction of multiple agents. Examples of such systems abound; no doubt many will be represented at this conference. Popular demonstration applications include robotic soccer <ref> [1] </ref>, the predator-prey pursuit game [5], and net-worked information systems [17]. Another example is a marketplace, where prices of goods continuously change as agents announce offers. As the rise of the Internet and electronic commerce continues, dynamic automated markets will be an increasingly important domain for software agents. <p> This is where learning about other agents comes in. Since agents receive payoffs while they learn, their learning must also be online. One can study online learning without explicitly modeling other agents. Research in this category has studied dynamic multiagent systems such as robotic soccer <ref> [1, 10] </ref> and the pursuit game [5]. The online learning method usually applied in such studies is reinforcement learning, of which Q-learning is the most popular. In most studies, Q-learning is used to derive the relationship (s i ; a i ) ! r i .
Reference: [2] <author> Ann L. C. Bazzan. </author> <title> Evolution of coordination as a metaphor for learning in multi-agent systems. </title> <booktitle> In Weiss [19]. Seleted papers from ICMAS'96 Workshop LIOME and ECAI'96 workshop LDAIS. </booktitle>
Reference-contexts: They study how agents can simultaneously learn about rewards coordinate strategies by judiciously combining exploitation and exploration. Most studies of repeated games have considered two-player games. A few have admitted more than two players (e.g., the traffic light coordination game of Bazzan <ref> [2] </ref>, and a stochastic social game investigated by Shoham and Tennenholtz [14]), but usually by having agents play two-player games with each other in a round-robin style. Schmidhuber and Zhao [12] consider a system with three self-interested agents.
Reference: [3] <author> David Carmel and Shaul Markovitch. </author> <title> Opponent modeling in multi-agent systems. </title> <publisher> Springer, </publisher> <year> 1996. </year> <booktitle> Proceedings of IJCAI'95 Workshop. </booktitle>
Reference-contexts: Online learning methods employed include fictitious play and Q-learning. Because the state is fixed, agents in a repeated game do not need to consider relations of actions to states. A theoretical framework is presented by Carmel and Markovitch <ref> [3] </ref> for learning in two-player repeated games. They propose that agents can learn about the other agent's action a j by observing the history of joint actions f (a i ; a j )g.
Reference: [4] <author> Caroline Claus and Boutilier Craig. </author> <title> The dynamics of reinforcement learning in cooperative multiagent systems. </title> <note> In Sen [13]. </note>
Reference-contexts: The models to be learned are described by deterministic finite automata, which assume that the strategy of the other agent never changes and actions are discrete. Claus and Boutilier <ref> [4] </ref> apply Q-learning to a repeated coordination game. They study how agents can simultaneously learn about rewards coordinate strategies by judiciously combining exploitation and exploration. Most studies of repeated games have considered two-player games.
Reference: [5] <author> Edwin De Jong. </author> <title> Non-random exploration bonuses for online reinforcement learning. </title> <note> In Sen [13]. </note>
Reference-contexts: 1 Introduction A dynamic multiagent system, loosely speaking, is a system that changes over time due to the interaction of multiple agents. Examples of such systems abound; no doubt many will be represented at this conference. Popular demonstration applications include robotic soccer [1], the predator-prey pursuit game <ref> [5] </ref>, and net-worked information systems [17]. Another example is a marketplace, where prices of goods continuously change as agents announce offers. As the rise of the Internet and electronic commerce continues, dynamic automated markets will be an increasingly important domain for software agents. <p> Since agents receive payoffs while they learn, their learning must also be online. One can study online learning without explicitly modeling other agents. Research in this category has studied dynamic multiagent systems such as robotic soccer [1, 10] and the pursuit game <ref> [5] </ref>. The online learning method usually applied in such studies is reinforcement learning, of which Q-learning is the most popular. In most studies, Q-learning is used to derive the relationship (s i ; a i ) ! r i . <p> In most studies, Q-learning is used to derive the relationship (s i ; a i ) ! r i . This ignores the impact of s i and a i on agent i's reward, r i . De Jong <ref> [5] </ref> did, however, account for s i in his neural network model of the relationship (s i ; s i ) ! a i , where agents choose the best action based on current states.
Reference: [6] <author> Daniel Friedman and John Rust, </author> <title> editors. The Double Auction Market. </title> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: Double auctions can be further classified according to such criteria as the timing of the bidding protocol. For example, in a synchronous double auction, all agents submit their bids in lockstep. The book edited by Friedman and Rust <ref> [6] </ref> collects several studies of double auctions, including both simulations and game-theoretic analyses. In most simulations described, agents adopt simple bidding strategies without much modeling of other agents.
Reference: [7] <author> Piotr J. Gmytrasiewicz and Edmund H. Durfee. </author> <title> A rigorous, operational formalization of recursive modeling. </title> <booktitle> In Proceedings of the First International Conference of Multiagent Systems. </booktitle> <publisher> AAAI Press, </publisher> <year> 1995. </year>
Reference-contexts: A soccer agent learns about the linear relation between shooting error and shooting distance, and then decide to shoot or not based on the predicted error. Explicit recursive modeling of other agents in multi-agent settings was proposed by Gmytrasiewicz and Dur-fee <ref> [7] </ref>. Vidal and Durfee [17] implemented a recursive modeling approach in a particular exchange market. In further work, Vidal and Durfee [18] developed a more abstract frameworks of recursive modeling, for purposes of analyzing general multiagent learning behavior.
Reference: [8] <author> Junling Hu and Michael P. Wellman. </author> <title> Self-fulfilling bias in multiagent learning. </title> <booktitle> In Proceedings of the Second International Conference on Multiagent Systems. </booktitle> <publisher> AAAI Press, </publisher> <month> December </month> <year> 1996. </year>
Reference-contexts: Since all agents have the same observations (except about their own policies), i's model of j's model of k (for k 6= i) is exactly what i's model of k would be if i were acting as a 1-level agent. In our previous study <ref> [8] </ref>, we investigated two types of agents: a competitive agent and a 1-level agent that modeled the others in the aggregate as non-estimating. Our learning model was also online, implemented in a market system called WALRAS [20]. <p> Although the particulars differ, our results are consistent with recent studies of strategic versus competitive behavior in computational markets, conducted by ourselves and others <ref> [8, 11] </ref>.
Reference: [9] <author> R. Preston McAfee and John McMillan. </author> <title> Auctions and bidding. </title> <journal> Journal of Economic Literature, </journal> <volume> 25 </volume> <pages> 699-738, </pages> <year> 1987. </year>
Reference-contexts: In this paper, we designed and tested four types of agent in a different, auction-based market system. 4 The Synchronous Double Auction Market Auctions come in a wide range of types, distinguished by the way bidders submit their bids and how the allocations and prices are determined <ref> [9] </ref>. In a double-sided (or just double) auction, both buyers and sellers submit bids. A single agent may even submit both, offering to buy or sell depending on the price. Double auctions can be further classified according to such criteria as the timing of the bidding protocol.
Reference: [10] <author> Rajami Nadella and Sandip Sen. </author> <title> Correlating internal parameters and external performance. </title> <booktitle> In Weiss [19]. Seleted papers from ICMAS'96 Workshop LI-OME and ECAI'96 workshop LDAIS. </booktitle>
Reference-contexts: This is where learning about other agents comes in. Since agents receive payoffs while they learn, their learning must also be online. One can study online learning without explicitly modeling other agents. Research in this category has studied dynamic multiagent systems such as robotic soccer <ref> [1, 10] </ref> and the pursuit game [5]. The online learning method usually applied in such studies is reinforcement learning, of which Q-learning is the most popular. In most studies, Q-learning is used to derive the relationship (s i ; a i ) ! r i . <p> The agents use history data to choose actions that have been successful and remove actions that have failed. The success is measured by the accumulated reward at each point of evaluation. Linear regression as an online learning methods has been adopted by Nadella and Sen <ref> [10] </ref> in designing soccer agents. A soccer agent learns about the linear relation between shooting error and shooting distance, and then decide to shoot or not based on the predicted error. Explicit recursive modeling of other agents in multi-agent settings was proposed by Gmytrasiewicz and Dur-fee [7].
Reference: [11] <author> Tuomas Sandholm and Fredrik Ygge. </author> <title> On the gains and losses of speculation in equilibrium markets. </title> <booktitle> In Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 632-638, </pages> <address> Nagoya, Japan, </address> <year> 1997. </year>
Reference-contexts: Although the particulars differ, our results are consistent with recent studies of strategic versus competitive behavior in computational markets, conducted by ourselves and others <ref> [8, 11] </ref>.
Reference: [12] <author> Jurgen Schmidhuber and Jieyu Zhao. </author> <title> Multi-agent learning with the success-story algorithm. </title> <booktitle> In Weiss [19]. Seleted papers from ICMAS'96 Workshop LI-OME and ECAI'96 workshop LDAIS. </booktitle>
Reference-contexts: A few have admitted more than two players (e.g., the traffic light coordination game of Bazzan [2], and a stochastic social game investigated by Shoham and Tennenholtz [14]), but usually by having agents play two-player games with each other in a round-robin style. Schmidhuber and Zhao <ref> [12] </ref> consider a system with three self-interested agents. The agents use history data to choose actions that have been successful and remove actions that have failed. The success is measured by the accumulated reward at each point of evaluation.
Reference: [13] <editor> Sandip Sen, editor. </editor> <booktitle> Collected papers from the AAAI-97 workshop on multiagent learning. </booktitle> <publisher> AAAI, AAAI Press, </publisher> <year> 1997. </year>
Reference: [14] <author> Yoav Shoham and Moshe Tennenholtz. </author> <title> On the emergence of social conventions: Modeling, analysis, and simulations. </title> <journal> Artificial Intelligence, </journal> <volume> 94(1-2):139-166, </volume> <month> July </month> <year> 1997. </year>
Reference-contexts: Most studies of repeated games have considered two-player games. A few have admitted more than two players (e.g., the traffic light coordination game of Bazzan [2], and a stochastic social game investigated by Shoham and Tennenholtz <ref> [14] </ref>), but usually by having agents play two-player games with each other in a round-robin style. Schmidhuber and Zhao [12] consider a system with three self-interested agents. The agents use history data to choose actions that have been successful and remove actions that have failed.
Reference: [15] <author> Hirofumi Uzawa. </author> <title> On the stability of Edgeworth barter process. </title> <journal> International Economic Review, </journal> <volume> 3(2) </volume> <pages> 218-232, </pages> <month> May </month> <year> 1962. </year>
Reference-contexts: In our system, termination is mandated when, for all goods, all the buy prices are lower than all the sell prices. That means no agents can be matched as a buyer or a seller, and no trading can happen. Our auction model satisfies the conditions of an Edge-worth process <ref> [15] </ref>, where the utility of agents never de crease through trade. In traditional study of Edgeworth processes, it is assumed that all agents are competitive. But in our system, agents can take strategic actions, which change the system behavior.
Reference: [16] <author> Hal R. </author> <title> Varian. Microeconomic Analysis. </title> <editor> W. W. </editor> <publisher> Norton & Company, </publisher> <address> New York, third edition, </address> <year> 1992. </year>
Reference-contexts: We can characterize this in terms of the agents' reservation prices <ref> [16] </ref>. The reservation price is defined as the maximum (minimum) price an agent is willing to pay (accept) for the good it wants to buy (sell).
Reference: [17] <author> Jose M. Vidal and Edmund H. Durfee. </author> <title> The impact of nested agent models in an information economy. </title> <booktitle> In Proceedings of the Second International Conference on Multiagent Systems. </booktitle> <publisher> AAAI Press, </publisher> <month> Decem-ber </month> <year> 1996. </year>
Reference-contexts: Examples of such systems abound; no doubt many will be represented at this conference. Popular demonstration applications include robotic soccer [1], the predator-prey pursuit game [5], and net-worked information systems <ref> [17] </ref>. Another example is a marketplace, where prices of goods continuously change as agents announce offers. As the rise of the Internet and electronic commerce continues, dynamic automated markets will be an increasingly important domain for software agents. <p> A soccer agent learns about the linear relation between shooting error and shooting distance, and then decide to shoot or not based on the predicted error. Explicit recursive modeling of other agents in multi-agent settings was proposed by Gmytrasiewicz and Dur-fee [7]. Vidal and Durfee <ref> [17] </ref> implemented a recursive modeling approach in a particular exchange market. In further work, Vidal and Durfee [18] developed a more abstract frameworks of recursive modeling, for purposes of analyzing general multiagent learning behavior.
Reference: [18] <author> Jose M. Vidal and Edmund H. Durfee. </author> <title> Agents learning about agents: A framework and analysis. </title> <note> In Sen [13]. </note>
Reference-contexts: Explicit recursive modeling of other agents in multi-agent settings was proposed by Gmytrasiewicz and Dur-fee [7]. Vidal and Durfee [17] implemented a recursive modeling approach in a particular exchange market. In further work, Vidal and Durfee <ref> [18] </ref> developed a more abstract frameworks of recursive modeling, for purposes of analyzing general multiagent learning behavior. In between the fully general abstraction and particular ad hoc models will be specifications of recursive modeling agents specialized to well-defined problem classes. We proceed below to develop this specialization in two stages. <p> Agents taking this approach do not explicitly attempt to reason about what determines the a i . Adopting the terminology of Vidal and Durfee <ref> [18] </ref>, we call such agents 0-level agents, meaning that they do not model the underlying behavior of other agents.
Reference: [19] <author> Gerhard Weiss, </author> <title> editor. </title> <booktitle> Distributed Artificial Intelligence Meets Machine Learning: learning in multi-agent environment. </booktitle> <publisher> Springer, </publisher> <year> 1997. </year> <note> Seleted papers from ICMAS'96 Workshop LIOME and ECAI'96 workshop LDAIS. </note>
Reference: [20] <author> Michael P. Wellman. </author> <title> A market-oriented programming environment and its application to distributed multicommodity flow problems. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 1-22, </pages> <year> 1993. </year>
Reference-contexts: In our previous study [8], we investigated two types of agents: a competitive agent and a 1-level agent that modeled the others in the aggregate as non-estimating. Our learning model was also online, implemented in a market system called WALRAS <ref> [20] </ref>. Our experiments showed that when states are not observable, such incomplete information can lead the learning agent to a self-fulfilling suboptimal equilibrium.
References-found: 20

