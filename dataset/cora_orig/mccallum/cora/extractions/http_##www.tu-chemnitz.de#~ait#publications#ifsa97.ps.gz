URL: http://www.tu-chemnitz.de/~ait/publications/ifsa97.ps.gz
Refering-URL: http://www.tu-chemnitz.de/~ait/more_infos.html
Root-URL: 
Email: E-mail: fait,jzei,ros,wdig@informatik.tu-chemnitz.de  E-mail: schlosser@koblenz.fh-rpl.de  
Phone: Phone: +49 371 531-1643, Fax: +49 371 531-1465,  Phone: +49 261 9528-187, Fax: +49 261 56953,  
Title: FEATURE SPACE PARTITIONING BY NON-LINEAR AND FUZZY DECISION TREES  
Author: Andreas Ittner a Jens Zeidler a Rolf Rossius a Werner Dilger a Michael Schlosser b 
Keyword: Non-linear and Fuzzy Decision Trees, Feature Space Partitioning  
Address: D-09107 Chemnitz,  Am Finkenherd 4, D-56075 Koblenz,  
Affiliation: a Chemnitz University of Technology, Department of Computer Science,  b FH Koblenz, Department of Electrical Engineering,  
Abstract: This paper focuses on a unified sight in the field of non-linear feature space partitioning. We present two well-known approaches of growing decision trees from data and show that these methods have a lot in common regarding non-linearity. The aim of this paper is to clarify that the application of simple mathematical operations broadens the capabilities to split the feature space in a non-linear fashion. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. Ittner. </author> <title> Ermittlung von funktionalen Attributabhangigkeiten und deren Einflu auf maschinelle Lernverfahren. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Science, Chemnitz University of Technology, Germany, </institution> <year> 1995. </year> <note> Only in German available. </note>
Reference-contexts: Here we demonstrate the influence of membership functions and the fuzzy operators on the feature space partitioning. Section 5 summarizes the lessons learned from this unification and outlines an avenue for further work. 2 Non-Linear Decision Trees The method of NDT, first introduced in <ref> [1] </ref>, pro <br>- ceeds in two consecutive steps: * augmentation of the feature space * application of a decision tree algorithm.
Reference: 2. <author> A. Ittner and M. Schlosser. </author> <title> Discovery of relevant new features by generating non-linear decision trees. </title> <editor> In E. Simoudis, J. Han, and U. Fayyad, ed-itors, </editor> <booktitle> Proc. of 2 nd International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 108-113. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, Portland, Oregon, USA, </address> <year> 1996. </year> <note> http://www.tuchemnitz.de/~ait/publications/kdd96.ps.gz. </note>
Reference-contexts: This can be done for instance within the process of generating new features from a set of given task-supplied primitive ones. Using these newly created 'non-linear' features allows for a non-linear partitioning of the original feature space. The studies in [3] and <ref> [2] </ref> showed that non-linear decision tree algorithms (NDT's) produce more accurate trees than their axis-parallel or oblique counterparts. On the other hand, multiplication is a quite important operation to define logical operators in fuzzy theory and its application on fuzzy decision trees (FDT's). <p> In general, these hyperplanes corresponded to non-linear hypersurfaces in the original feature space of primitive features. One of the data sets we used in our experiments <ref> [2] </ref> was an artificial set, called SPIRAL [Figure 1], which allows for an exemplary demonstration of the ability of the NDT method [Figure 2].
Reference: 3. <author> A. Ittner and M. Schlosser. </author> <title> Non-linear deci-sion trees - NDT. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Proc. of 13 th International Machine Learning Conference, </booktitle> <pages> pages 252-257. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, Bari, Italy, </address> <year> 1996. </year> <note> http://www.tuchemnitz.de/~ait/publications/icml96.ps.gz. </note>
Reference-contexts: This can be done for instance within the process of generating new features from a set of given task-supplied primitive ones. Using these newly created 'non-linear' features allows for a non-linear partitioning of the original feature space. The studies in <ref> [3] </ref> and [2] showed that non-linear decision tree algorithms (NDT's) produce more accurate trees than their axis-parallel or oblique counterparts. On the other hand, multiplication is a quite important operation to define logical operators in fuzzy theory and its application on fuzzy decision trees (FDT's).
Reference: 4. <author> C. Z. Janikow. </author> <title> Fuzzy processing in decision trees. </title> <booktitle> In Proceedings of International Symposium on Artifical Intelligence, </booktitle> <pages> pages 360-367, </pages> <year> 1993. </year>
Reference-contexts: However, there is also a number of fuzzy decision tree solutions <ref> [4] </ref>, [6], [7] and [8]. In the field of FDT's, the learning examples are labelled with membership grades. These grades represent the affiliations of examples to the classes. Fuzzy borders for discretisation of continuous-valued attributes [Figure 3] are used in almost all approaches mentioned above.
Reference: 5. <author> S. Murthy, S. Kasif, S. Salzberg, and R. Beigel. </author> <title> OC1: Randomized induction of oblique deci-sion trees. </title> <booktitle> In Proc. of the 11 th Nat. Conf. on AI AAAI-93, </booktitle> <pages> pages 322-327, </pages> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: In the second step of the NDT method a decision tree algorithm is applied to construct an oblique decision tree in the augmented feature space which is now of higher dimension.In our experiments we used OC1 (Oblique Classifier 1) <ref> [5] </ref>. This algorithm generated hyperplanes with an oblique orientation as a test of a linear combination of primitive and new created features at each internal node. In general, these hyperplanes corresponded to non-linear hypersurfaces in the original feature space of primitive features.
Reference: 6. <author> M. Umano, H. Okamoto, I. Hatono, H. Tamura, F. Kawachi, S. Umedzu, and J. Kinoshita. </author> <title> Fuzzy decision trees by fuzzy ID3 algorithm and its ap-plication to diagnosis systems. </title> <booktitle> In Proc. of 3 rd IEEE International Conference on Fuzzy Systems, </booktitle> <pages> pages 2113-2118, </pages> <address> Orlando, FL, </address> <year> 1994. </year>
Reference-contexts: However, there is also a number of fuzzy decision tree solutions [4], <ref> [6] </ref>, [7] and [8]. In the field of FDT's, the learning examples are labelled with membership grades. These grades represent the affiliations of examples to the classes. Fuzzy borders for discretisation of continuous-valued attributes [Figure 3] are used in almost all approaches mentioned above.
Reference: 7. <author> X. Wu and P. M-ahlen. </author> <title> Fuzzy interpretation of induction results. </title> <editor> In U. M. Fayyad and R. Uthurusamy, editors, </editor> <booktitle> Proc. of 1 st International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 325-330, </pages> <address> Montreal, Quebec, Canada, </address> <year> 1995. </year>
Reference-contexts: However, there is also a number of fuzzy decision tree solutions [4], [6], <ref> [7] </ref> and [8]. In the field of FDT's, the learning examples are labelled with membership grades. These grades represent the affiliations of examples to the classes. Fuzzy borders for discretisation of continuous-valued attributes [Figure 3] are used in almost all approaches mentioned above.
Reference: 8. <author> J. Zeidler and M. Schlosser. </author> <title> Fuzzy handling of continuous-valued attributes in decision trees. </title> <editor> In Y. Kodratoff, G. Nakhaeizadeh, and Ch. Taylor, editors, </editor> <booktitle> Proc. of MLNet Familiarization Workshop: Statistics, Machine Learning and Knowledge Discovery in Databases, </booktitle> <pages> pages 41-46, </pages> <address> Heraklion, Crete/Greece, </address> <year> 1995. </year> <note> http://www.tuchemnitz.de/~jzei/VEROEFF/ecws3.ps. </note>
Reference-contexts: On the other hand, multiplication is a quite important operation to define logical operators in fuzzy theory and its application on fuzzy decision trees (FDT's). In the area of FDT's <ref> [8] </ref> we can find non-linear partitionings too. Here the calculation with Fuzzy-AND and Fuzzy- OR in conjunction with defuzzification creates new non-linear geometrical forms of feature space partition (see [9]). Section 2 of this paper is dedicated to non-linear feature space partitioning with NDT's. <p> However, there is also a number of fuzzy decision tree solutions [4], [6], [7] and <ref> [8] </ref>. In the field of FDT's, the learning examples are labelled with membership grades. These grades represent the affiliations of examples to the classes. Fuzzy borders for discretisation of continuous-valued attributes [Figure 3] are used in almost all approaches mentioned above. The resulting FDT consists of nodes and leafs.
Reference: 9. <author> J. Zeidler and M. Schlosser. </author> <title> Continuous-valued attributes in fuzzy decision trees. </title> <booktitle> In Proc. of 6 th International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems, </booktitle> <pages> pages 395400, </pages> <address> Granada, Spain, </address> <year> 1996. </year> <note> http://www.tuchemnitz.de/~jzei/VEROEFF/ipmu.ps. </note>
Reference-contexts: In the area of FDT's [8] we can find non-linear partitionings too. Here the calculation with Fuzzy-AND and Fuzzy- OR in conjunction with defuzzification creates new non-linear geometrical forms of feature space partition (see <ref> [9] </ref>). Section 2 of this paper is dedicated to non-linear feature space partitioning with NDT's. Section 3 deals with fundamentals of fuzzy-decision trees generation. In section 4 we try to develop a unified view of the partitioning problem as a synthesis of these two types of trees.
References-found: 9

