URL: http://www.cs.ucsb.edu/~acha/publications/sigmetrics98-submitted.ps.gz
Refering-URL: http://www.cs.ucsb.edu/~acha/publications/sigmetrics98-submitted.html
Root-URL: http://www.cs.ucsb.edu
Title: The Utility of Exploiting Idle Memory for Data-Intensive Computations  
Author: Anurag Acharya Sanjeev Setia 
Address: Santa Barbara, CA 93106  Fairfax, VA 22030  
Affiliation: Dept. of Computer Science University of California  Dept. of Computer Science George Mason University  
Abstract: In this paper, we examine the utility of exploiting idle memory in workstation pools. We attempt to answer the following questions. First, given a workstation pool, what fraction of the memory can be expected to be idle? This provides an estimate of the opportunity for hosting guest data. Second, what fraction of a individual host's memory can be expected to be idle? This helps determine the recruitment policy what is the maximum amount of memory that should be recruited on a single host? Third, what is the distribution of memory idle-times? That is, what is the probability that a chunk of memory that is currently idle will be idle for longer than time t? This information indicates how long guest data can be expected to survive; applications that access their data-sets frequently within the expected life-time of guest data are more likely to benefit from exploiting idle memory. Fourth, how much benefit can a user expect? We use two metrics for the benefit of exploiting idle memory: (1) if I have a pool with w workstations, how much memory should I expect to get for free by harvesting idle memory; (2) how much improvement can be achieved in end-to-end execution time? Finally, how long and how frequently might a user have to wait to reclaim her machine if she volunteers to host guest pages on her machine? This helps answer the question of social acceptability. To answer the questions relating to the availability of idle memory, we have analyzed two-week long traces from five workstation pools with different sizes, locations, and patterns of use. To evaluate the expected benefits and costs, we have simulated three data-intensive applications (0.5 GB-5 GB) on these workstation pools.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Acharya, G. Edjlali, and J. Saltz. </author> <title> The utility of exploiting idle workstations for parallel computation. </title> <booktitle> In Proceedings of SIGMETRICS'97, </booktitle> <year> 1997. </year>
Reference-contexts: Systems that utilize idle workstations for running sequential jobs have been in production use for many years (e.g. Condor [15], which manages about 300 workstations at the University of Wisconsin); there has also been significant amount of research on exploiting idle workstations for hosting parallel computations <ref> [1, 3, 5, 6, 16, 19] </ref>. With the current growth in the number and the size of data-intensive tasks, exploiting idle workstations for their memory is an attractive option. <p> At this point, the object being replaced can be explicitly stored at a remote idle node by making a library call. Only clean objects are stored in remote 1 Following Condor, many resource harvesting systems are named after birds (eg Finch <ref> [1] </ref>). All of these systems deal with the CPU and other resources, dodo is just for memory (being an extinct bird, dodo is just in memory :)) 6 memory.
Reference: [2] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules in large databases. </title> <booktitle> In Proc. of 20th Int'l Conf. on Very Large Databases (VLDB), </booktitle> <address> Santiago, Chile, </address> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: The total database size was 5.2 GB (including the indices). DMINE: this application tries to extract association rules from retail data [17]. The dataset consists of 50 million transactions, with an average transaction size of 10 items and maximal potentially frequent set size of 3 (see <ref> [2, 17] </ref> for details). The dataset size for this program was 4 GB. LU: this application computes the dense LU decomposition of an out-of-core matrix [12]. The dataset consisted of an 8192 fi 8192 double precision matrix (total size 536 MB) with a slab size of 64 columns.
Reference: [3] <author> R. Arpaci, A. Dusseau, A. Vahdat, L. Liu, T. Anderson, and D. Patterson. </author> <title> The Interaction of Parallel and Sequential Workloads on a Network of Workstations. </title> <booktitle> In Proceedings of the 1995 ACM SIGMETRICS Joint International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 26778, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: Systems that utilize idle workstations for running sequential jobs have been in production use for many years (e.g. Condor [15], which manages about 300 workstations at the University of Wisconsin); there has also been significant amount of research on exploiting idle workstations for hosting parallel computations <ref> [1, 3, 5, 6, 16, 19] </ref>. With the current growth in the number and the size of data-intensive tasks, exploiting idle workstations for their memory is an attractive option.
Reference: [4] <author> M. Baker, J. Hartman, M. Kupfer, K. Shirriff, and J. Ousterhout. </author> <title> Measurements of a distributed file system. </title> <booktitle> In Proceedings of the 13th Symposium on Operating Systems Principles, </booktitle> <pages> pages 198212, </pages> <year> 1991. </year>
Reference-contexts: Dahlin et al [8] and Sarkar et al [20] propose schemes to use idle memory to increase the effective file cache size. Both these studies use the Sprite distributed file system traces <ref> [4] </ref> to determine both memory availability and program behavior. The Sprite traces were acquired from a cluster of workstations used for computer science research and as such are not focused towards data intensive applications. In fact, there is no information about the identity or the memory footprints of individual applications.
Reference: [5] <author> N. Carriero, D. Gelernter, M. Jourdenais, and D. Kaminsky. </author> <title> Piranha Scheduling: Strategies and Their Implementation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 23(1):533, </volume> <month> Feb </month> <year> 1995. </year>
Reference-contexts: Systems that utilize idle workstations for running sequential jobs have been in production use for many years (e.g. Condor [15], which manages about 300 workstations at the University of Wisconsin); there has also been significant amount of research on exploiting idle workstations for hosting parallel computations <ref> [1, 3, 5, 6, 16, 19] </ref>. With the current growth in the number and the size of data-intensive tasks, exploiting idle workstations for their memory is an attractive option.
Reference: [6] <author> A. Chowdhury, L. Nicklas, S. Setia, and E. Whie. </author> <title> Supporting dynamic space-sharing on clusters of non-dedicated workstations. </title> <booktitle> In Proceedings of the 17th International conference on distributed computing, </booktitle> <year> 1997. </year>
Reference-contexts: Systems that utilize idle workstations for running sequential jobs have been in production use for many years (e.g. Condor [15], which manages about 300 workstations at the University of Wisconsin); there has also been significant amount of research on exploiting idle workstations for hosting parallel computations <ref> [1, 3, 5, 6, 16, 19] </ref>. With the current growth in the number and the size of data-intensive tasks, exploiting idle workstations for their memory is an attractive option.
Reference: [7] <author> D. Comer and J. Griffioen. </author> <title> Efficient order dependent communication in distributed virtual memory environment. </title> <booktitle> In Proceedings of the Symposium on Experiences with Distributed and Multiprocessor Systems III, </booktitle> <pages> pages 24962, </pages> <year> 1992. </year>
Reference-contexts: Felten and Zahorjan [10] examined the idea of using remote client memory instead of disk for virtual memory paging. Schilit and Duchamp [21] investigated the use of remote memory paging for diskless portable machines. Comer and Griffioen <ref> [7] </ref> proposed a communication protocol for remote paging. 15 6 Conclusions There are three primary conclusions of our study. First, there is significant benefit to harvesting idle memory on workstation clusters for data-intensive applications.
Reference: [8] <author> M. Dahlin, R. Wang, T. Anderson, and D. Patterson. </author> <title> Cooperative Caching: Using Remote Memory to Improve File System Performance. </title> <booktitle> In Proceedings of the First Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 26780, </pages> <month> Nov </month> <year> 1994. </year>
Reference-contexts: Their study uses synthetic models of both memory availability and program behavior. It is difficult to draw conclusions about behavior of real data-intensive programs on real workstation pools from their work. Dahlin et al <ref> [8] </ref> and Sarkar et al [20] propose schemes to use idle memory to increase the effective file cache size. Both these studies use the Sprite distributed file system traces [4] to determine both memory availability and program behavior.
Reference: [9] <author> M. Feely, W. Morgan, F. Pighin, A. Karlin, H. Levy, and C. Thekkath. </author> <title> Implementing Global Memory Management in a Workstation Cluster. </title> <booktitle> In Proceedings of the fifteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 20112, </pages> <month> Dec </month> <year> 1995. </year>
Reference-contexts: The Sprite traces were acquired from a cluster of workstations used for computer science research and as such are not focused towards data intensive applications. In fact, there is no information about the identity or the memory footprints of individual applications. Feely et al <ref> [9] </ref> describe a low-level global memory management system that uses idle memory to back up not just file pages but all of virtual memory as well. They show that this scheme is able to use idle memory to improve the performance of a suite of data-intensive tasks. <p> We have used actual traces from three data-intensive applications running realistic workloads; the size of the datasets of these applications are significantly larger than those of the applications used by Feely et al <ref> [9] </ref>. We first examine the memory availability questions. Section 2 describes the methodology used to monitor the memory usage in a workstation pool, and the metrics computed to estimate the opportunity for hosting guest memory. Next, we examine the benefits and costs of exploiting idle memory for data-intensive computations. <p> Each application is linked to a run-time library that implements the functionality needed by the application in order to create, read, write, and delete remote memory objects. In contrast to global memory systems such as GMS <ref> [9] </ref> where applications use remote memory pages implicitly, our simulator assumes a system in which applications are written to make use of remote memory explicitly. Thus, applications create, read, write, and delete remote memory objects by invoking the corresponding routines in the run-time library.
Reference: [10] <author> E. Felten and J. Zahorjan. </author> <title> Issues in implementation of a remote memory paging system. </title> <type> Technical Report 91-03-09, </type> <institution> Department of Computer Science, University of Washington, </institution> <year> 1991. </year> <month> 16 </month>
Reference-contexts: It keeps track of the idle memory available and ships memory objects to the corresponding machines as needed. Iftode et al propose extending the memory hierarchy of multicomputers by introducing a remote memory server layer. Felten and Zahorjan <ref> [10] </ref> examined the idea of using remote client memory instead of disk for virtual memory paging. Schilit and Duchamp [21] investigated the use of remote memory paging for diskless portable machines.
Reference: [11] <author> M. Franklin, M. Carey, and M. Livny. </author> <title> Global Memory Management in Client-Server DBMS Architectures. </title> <booktitle> In Proceedings of the eighteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 596609, </pages> <month> Aug </month> <year> 1992. </year>
Reference-contexts: This indicates that in all three clusters, there are some nodes that are always idle. Objects stored in the memory of these nodes do not get dropped resulting in the performance benefits observed in our experiments. 5 Other Related Work Franklin et al <ref> [11] </ref> describe a unified memory management scheme for the servers and all the clients in a client-server database system. Their goal was to avoid replication of pages between the buffer pools of all the clients as well as the buffer pools of the servers.
Reference: [12] <author> B. Hendrickson and D. Womble. </author> <title> The torus-wrap mapping for dense matrix calculations on massively parallel computers. </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 15(5), </volume> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: The dataset consists of 50 million transactions, with an average transaction size of 10 items and maximal potentially frequent set size of 3 (see [2, 17] for details). The dataset size for this program was 4 GB. LU: this application computes the dense LU decomposition of an out-of-core matrix <ref> [12] </ref>. The dataset consisted of an 8192 fi 8192 double precision matrix (total size 536 MB) with a slab size of 64 columns. We chose these three applications as representatives of three classes of applications.
Reference: [13] <author> L. Iftode, K. Li, and K. Petersen. </author> <title> Memory Servers for Multicomputers. </title> <booktitle> In COMPCON Spring'93 Digest of Papers, </booktitle> <pages> pages 53847, </pages> <month> Feb </month> <year> 1993. </year>
Reference-contexts: Their goal was to avoid replication of pages between the buffer pools of all the clients as well as the buffer pools of the servers. Explicit memory servers have been proposed by Narten&Yavatkar [18] and Iftode et al <ref> [13] </ref>. They describe a memory server similar in spirit to the Dodo central manager. It keeps track of the idle memory available and ships memory objects to the corresponding machines as needed. Iftode et al propose extending the memory hierarchy of multicomputers by introducing a remote memory server layer.
Reference: [14] <author> A. Leff, P. Yu, and J. Wolf. </author> <title> Policies for efficient memory utilization in a remote caching architecture. </title> <booktitle> In Proceedings of First International Conf. on Parallel and distributed Information Systems, </booktitle> <year> 1991. </year>
Reference-contexts: To drive these simulations, we used traces captured by running these applications on realistic workloads [22]. The datasets for these applications vary from 0.5 GB to about 5 GB. Previous research into using idle memory for hosting guest data has taken one of three approaches. Leff et al <ref> [14] </ref> use an analytical and simulation model based approach to study the feasibility of remote caching architectures. Their study uses synthetic models of both memory availability and program behavior. It is difficult to draw conclusions about behavior of real data-intensive programs on real workstation pools from their work.
Reference: [15] <author> M. Litzkow and M. Livny. </author> <title> Experiences with the Condor Distributed Batch System. </title> <booktitle> In Proceedings of the IEEE Workshop on Experimental Distributed Systems, </booktitle> <pages> pages 97101, </pages> <month> Oct </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Idle workstations have traditionally been harvested for their cycles. Exploiting idle workstations for hosting guest computation has been a popular research area. Systems that utilize idle workstations for running sequential jobs have been in production use for many years (e.g. Condor <ref> [15] </ref>, which manages about 300 workstations at the University of Wisconsin); there has also been significant amount of research on exploiting idle workstations for hosting parallel computations [1, 3, 5, 6, 16, 19]. <p> Next, we describe our system and workload models used in the simulations. 3.1 Run-time Software for Exploiting Idle Memory Our simulations assumed the existence of a run-time system for harvesting idle memory (similar in several respects to the well-known Condor system <ref> [15] </ref> which harvests idle processors). This run-time system, which we refer to as Dodo, 1 has four components: (i) resource monitors, (ii) idle memory daemons, (iii) a central memory manager and scheduler, and (iv) a run-time library that is linked in with the application.
Reference: [16] <author> J. Moreira, V. Naik, and R. Konuru. </author> <title> A Programming Environment for Dynamic Resource Allocation and Data Distribution. </title> <type> Technical Report RC 20239, </type> <institution> IBM Research, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: Systems that utilize idle workstations for running sequential jobs have been in production use for many years (e.g. Condor [15], which manages about 300 workstations at the University of Wisconsin); there has also been significant amount of research on exploiting idle workstations for hosting parallel computations <ref> [1, 3, 5, 6, 16, 19] </ref>. With the current growth in the number and the size of data-intensive tasks, exploiting idle workstations for their memory is an attractive option.
Reference: [17] <author> A. Mueller. </author> <title> Fast sequential and parallel algorithms for association rule mining: A comparison. </title> <type> Technical Report CS-TR-3515, </type> <institution> University of Maryland, College Park, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: These queries perform complex join, set, and aggregate operations on indexed and non-indexed relations. The data was stored in DB2 (from IBM). The total database size was 5.2 GB (including the indices). DMINE: this application tries to extract association rules from retail data <ref> [17] </ref>. The dataset consists of 50 million transactions, with an average transaction size of 10 items and maximal potentially frequent set size of 3 (see [2, 17] for details). The dataset size for this program was 4 GB. <p> The total database size was 5.2 GB (including the indices). DMINE: this application tries to extract association rules from retail data [17]. The dataset consists of 50 million transactions, with an average transaction size of 10 items and maximal potentially frequent set size of 3 (see <ref> [2, 17] </ref> for details). The dataset size for this program was 4 GB. LU: this application computes the dense LU decomposition of an out-of-core matrix [12]. The dataset consisted of an 8192 fi 8192 double precision matrix (total size 536 MB) with a slab size of 64 columns.
Reference: [18] <author> T. Narten and R. Yavatkar. </author> <title> Remote Memory as a Resource in Distributed Systems. </title> <booktitle> In Proceedings of the third Workshop on Workstation Operating Systems, </booktitle> <pages> pages 1326, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Their goal was to avoid replication of pages between the buffer pools of all the clients as well as the buffer pools of the servers. Explicit memory servers have been proposed by Narten&Yavatkar <ref> [18] </ref> and Iftode et al [13]. They describe a memory server similar in spirit to the Dodo central manager. It keeps track of the idle memory available and ships memory objects to the corresponding machines as needed.
Reference: [19] <author> J. Pruyne and M. Livny. </author> <title> Parallel Processing on Dynamic Resources with CARMI. </title> <booktitle> In Proceedings of the Workshop on Job Scheduling Strategies for Parallel Processing, </booktitle> <pages> pages 25978, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Systems that utilize idle workstations for running sequential jobs have been in production use for many years (e.g. Condor [15], which manages about 300 workstations at the University of Wisconsin); there has also been significant amount of research on exploiting idle workstations for hosting parallel computations <ref> [1, 3, 5, 6, 16, 19] </ref>. With the current growth in the number and the size of data-intensive tasks, exploiting idle workstations for their memory is an attractive option.
Reference: [20] <author> P. Sarkar and J. Hartman. </author> <title> Efficient cooperative caching using hints. </title> <booktitle> In Proceedings of The Second Symposium on Operating Systems Design and Implementation, </booktitle> <year> 1996. </year>
Reference-contexts: Their study uses synthetic models of both memory availability and program behavior. It is difficult to draw conclusions about behavior of real data-intensive programs on real workstation pools from their work. Dahlin et al [8] and Sarkar et al <ref> [20] </ref> propose schemes to use idle memory to increase the effective file cache size. Both these studies use the Sprite distributed file system traces [4] to determine both memory availability and program behavior.
Reference: [21] <author> B. Schilit and D. Duchamp. </author> <title> Adaptive remote paging for mobile computers. </title> <type> Technical Report CUCS-004-91, </type> <institution> Department of Computer Science, Columbia University, </institution> <year> 1991. </year>
Reference-contexts: Iftode et al propose extending the memory hierarchy of multicomputers by introducing a remote memory server layer. Felten and Zahorjan [10] examined the idea of using remote client memory instead of disk for virtual memory paging. Schilit and Duchamp <ref> [21] </ref> investigated the use of remote memory paging for diskless portable machines. Comer and Griffioen [7] proposed a communication protocol for remote paging. 15 6 Conclusions There are three primary conclusions of our study. First, there is significant benefit to harvesting idle memory on workstation clusters for data-intensive applications.
Reference: [22] <author> M. Uysal, A. Acharya, and J. Saltz. </author> <title> Requirements of I/O systems for parallel machines: An application-driven study. </title> <type> Technical Report CS-TR-3802, </type> <institution> Department of Computer Science, University of Maryland, </institution> <year> 1997. </year>
Reference-contexts: To drive these simulations, we used traces captured by running these applications on realistic workloads <ref> [22] </ref>. The datasets for these applications vary from 0.5 GB to about 5 GB. Previous research into using idle memory for hosting guest data has taken one of three approaches. Leff et al [14] use an analytical and simulation model based approach to study the feasibility of remote caching architectures. <p> The motivation for using a first in policy comes from the fact that the applications used in our simulations exhibit scan and triangle-scan I/O access patterns <ref> [22] </ref>. For such applications, a first in policy is a good caching policy. To allocate remote memory for an object, the application makes a request to the central manager. <p> The I/O characteristics of these applications have previously been studied by Uysal et al <ref> [22] </ref>. OLAP: the workload consisted of five consecutive queries against a simulated database containing records for one year's operation of a large department store (one million customers, 100 thousand products, 10 thousand employees, 200 departments and 100 million transactions). <p> The workstation availability and memory usage traces for each workstation were obtained as described in Section 2.1. The I/O traces used to drive our simulation were obtained by running the benchmark applications on eight processors of an IBM SP-2. These traces were acquired by Uysal et al <ref> [22] </ref> using the AIX trace utility.
Reference: [23] <author> G. Voelker, H. Jamrozik, M. Vernon, H. Levy, and E. Lazowska. </author> <title> Managing Server Load in Global Memory Systems. </title> <booktitle> In Proceedings of the 1997 ACM Sigmetrics Conference, </booktitle> <pages> pages 127138, </pages> <month> June </month> <year> 1997. </year> <month> 17 </month>
Reference-contexts: First, even though hosting guest memory pages is likely to have a smaller impact on the owner of the workstation than hosting guest processes, many workstation owners will probably be unwilling to permit such usage because of social reasons. Second, as shown by a recent study <ref> [23] </ref>, in some situations, the cpu cycles consumed for handling remote memory requests can degrade the performance of the owner's jobs. An idle memory daemon is started on a workstation when its memory is recruited for hosting remote objects.
References-found: 23

