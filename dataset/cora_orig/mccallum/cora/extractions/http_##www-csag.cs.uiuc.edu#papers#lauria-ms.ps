URL: http://www-csag.cs.uiuc.edu/papers/lauria-ms.ps
Refering-URL: http://www-csag.cs.uiuc.edu/papers/index.html
Root-URL: http://www.cs.uiuc.edu
Title: HIGH PERFORMANCE MPI IMPLEMENTATION ON A NETWORK OF WORKSTATIONS  
Author: BY MARIO LAURIA 
Degree: 1992 THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the Graduate College of the  
Address: 1996 Urbana, Illinois  
Affiliation: Laur., Universita degli Studi di Napoli "Federico II",  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> T. Anderson, D. Culler, and D. Patterson. </author> <title> A case for NOW (networks of workstations). </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 54-64, </pages> <year> 1995. </year>
Reference-contexts: A few crucial differences between the two worlds can help explain the disparity. One main problem of the more sophisticated MPP architectures is their increased development time. As shown in Table 1.1 MPPs tend to lag by one or two years workstations with the same or comparable processor <ref> [1] </ref>. With commodity processor speed doubling roughly every two years, a lag of two years in the time to market translates into a twofold performance disadvantage. MPP CPU Year Equiv. <p> elapsed_time [j+1]; elapsed_time [j+1] = swap; - - /* Subtract overhead; calculate the ave time */ ave_time = 0; for (i = 1; i &lt; num_sets; i++) - elapsed_time [i] -= ave_overhead; ave_time += elapsed_time [i]; - ave_time /= (num_sets - 1); if (verbose) - printf (""nMinimum time:"n"); show_time_for_set (elapsed_time <ref> [1] </ref>); printf (""nAverage over all but the first trial:"n"); show_time_for_set (ave_time); - median_time = (elapsed_time [num_sets/2] + elapsed_time [(num_sets + 1)/2])/2; printf (""nMedian time:"n"); show_time_for_set (median_time); - void node1 () - int i; for (i = 0; i &lt; data_size; i++) - send_data [i] = -i; - if (verbose) - printf <p> recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Send (send_data, data_size, send_type, 0, SEND_TAG, MPI_COMM_WORLD); - void command_line (int argc, char *argv [], int *num_sets, int *num_reps, int *data_size, int *verbose) - *num_sets = 5; *num_reps = 20; *data_size = 16; *verbose = 1; if (argc &gt; 1) - *num_sets = atoi (argv <ref> [1] </ref>); if (*num_sets &gt; max_num_sets) - printf ("Too many sets (%d vs %d)."n", *num_sets, max_num_sets); exit (-1); - if (argc &gt; 2) - *num_reps = atoi (argv [2]); if (*num_reps &gt; max_num_reps) - printf ("Too many reps (%d vs %d)."n", *num_reps, max_num_reps); exit (-1); - if (argc &gt; 3) - *data_size <p> - MPI_Waitall (num_reps, request, status); MPI_Send (send_data, 0, send_type, 0, SEND_TAG, MPI_COMM_WORLD); - void command_line (int argc, char *argv [], int *num_sets, int *num_reps, int *data_size, int *verbose) - *num_sets = 5; *num_reps = 20; *data_size = 16; *verbose = 1; if (argc &gt; 1) - *num_sets = atoi (argv <ref> [1] </ref>); if (*num_sets &gt; max_num_sets) - printf ("Too many sets (%d vs %d)."n", *num_sets, max_num_sets); exit (-1); - if (argc &gt; 2) - *num_reps = atoi (argv [2]); if (*num_reps &gt; max_num_reps) - printf ("Too many reps (%d vs %d)."n", *num_reps, max_num_reps); exit (-1); 53 - if (argc &gt; 3) - <p> 0; i &lt; num_sets; i++) - MPI_Barrier (MPI_COMM_WORLD); for (i = 0; i &lt; num_reps; i++) - MPI_Barrier (MPI_COMM_WORLD); - - void command_line (int argc, char *argv []) - int cmd_line [] = -12, 100, 128, 1-; /* cmd_line = malloc (sizeof (int) * 4); cmd_line [0] = 12; cmd_line <ref> [1] </ref> = 100; cmd_line [2] = 128; cmd_line [3] = 1; */ if (argc &gt; 1) - cmd_line [0] = atoi (argv [1]); if (cmd_line [0] &gt; max_num_sets) - printf ("Too many sets (%d vs %d)."n", cmd_line [0], max_num_sets); exit (-1); - if (argc &gt; 2) - cmd_line [1] = atoi <p> command_line (int argc, char *argv []) - int cmd_line [] = -12, 100, 128, 1-; /* cmd_line = malloc (sizeof (int) * 4); cmd_line [0] = 12; cmd_line <ref> [1] </ref> = 100; cmd_line [2] = 128; cmd_line [3] = 1; */ if (argc &gt; 1) - cmd_line [0] = atoi (argv [1]); if (cmd_line [0] &gt; max_num_sets) - printf ("Too many sets (%d vs %d)."n", cmd_line [0], max_num_sets); exit (-1); - if (argc &gt; 2) - cmd_line [1] = atoi (argv [2]); if (cmd_line [1] &gt; max_num_reps) - printf ("Too many reps (%d vs %d)."n", cmd_line [1], max_num_reps); exit (-1); - if <p> 12; cmd_line <ref> [1] </ref> = 100; cmd_line [2] = 128; cmd_line [3] = 1; */ if (argc &gt; 1) - cmd_line [0] = atoi (argv [1]); if (cmd_line [0] &gt; max_num_sets) - printf ("Too many sets (%d vs %d)."n", cmd_line [0], max_num_sets); exit (-1); - if (argc &gt; 2) - cmd_line [1] = atoi (argv [2]); if (cmd_line [1] &gt; max_num_reps) - printf ("Too many reps (%d vs %d)."n", cmd_line [1], max_num_reps); exit (-1); - if (argc &gt; 3) - cmd_line [2] = atoi (argv [3]); if (cmd_line [2] &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", cmd_line [2], max_data_size); <p> = 128; cmd_line [3] = 1; */ if (argc &gt; 1) - cmd_line [0] = atoi (argv <ref> [1] </ref>); if (cmd_line [0] &gt; max_num_sets) - printf ("Too many sets (%d vs %d)."n", cmd_line [0], max_num_sets); exit (-1); - if (argc &gt; 2) - cmd_line [1] = atoi (argv [2]); if (cmd_line [1] &gt; max_num_reps) - printf ("Too many reps (%d vs %d)."n", cmd_line [1], max_num_reps); exit (-1); - if (argc &gt; 3) - cmd_line [2] = atoi (argv [3]); if (cmd_line [2] &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", cmd_line [2], max_data_size); exit (-1); - if (argc &gt; 4) <p> cmd_line [0] = atoi (argv <ref> [1] </ref>); if (cmd_line [0] &gt; max_num_sets) - printf ("Too many sets (%d vs %d)."n", cmd_line [0], max_num_sets); exit (-1); - if (argc &gt; 2) - cmd_line [1] = atoi (argv [2]); if (cmd_line [1] &gt; max_num_reps) - printf ("Too many reps (%d vs %d)."n", cmd_line [1], max_num_reps); exit (-1); - if (argc &gt; 3) - cmd_line [2] = atoi (argv [3]); if (cmd_line [2] &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", cmd_line [2], max_data_size); exit (-1); - if (argc &gt; 4) - cmd_line [3] = atoi (argv [4]); - MPI_Bcast (cmd_line, sizeof (int) <p> (argv [3]); if (cmd_line [2] &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", cmd_line [2], max_data_size); exit (-1); - if (argc &gt; 4) - cmd_line [3] = atoi (argv [4]); - MPI_Bcast (cmd_line, sizeof (int) * 4, MPI_INT, 0, MPI_COMM_WORLD); num_sets = cmd_line [0]; 59 num_reps = cmd_line <ref> [1] </ref>; data_size = cmd_line [2]; verbose = cmd_line [3]; total_messages = num_sets * num_reps; - int main (int argc, char *argv []) - /* allocate send and receive buffers */ send_data = malloc (max_data_size * sizeof (int)); recv_data = malloc (max_data_size * sizeof (int)); MPI_Init (&argc, &argv); MPI_Comm_size (MPI_COMM_WORLD, &max_nodes); MPI_Comm_rank <p> identity of the node */ for (ii=0; ii&lt;np; node_id [ii] = ii++); sort_with_id (np, global_median_time, node_id); /* print/log results */ printf ("Size: %d"tMedian time:", size); printf (" max %f [%d];", global_median_time [np-1], node_id [np-1]); printf (" 2nd max %f [%d];", global_median_time [np-2], node_id [np-2]); printf (" min %f [%d];", global_median_time <ref> [1] </ref>, node_id [1]); printf (""n"); fflush (stdout); recordlog (240496, np, SETS, REPS, size, median_time, 0, 0, 0); /* throw away the alternate group/communicator */ MPI_Comm_free (&SWITCH_ROOT_COMM); 65 MPI_Group_free (&SWITCH_ROOT_GROUP); - /* end SIZES */ closelog (); - void nodei ()- int i, size, set, reps, alt_root; /* for each message size, <p> the node */ for (ii=0; ii&lt;np; node_id [ii] = ii++); sort_with_id (np, global_median_time, node_id); /* print/log results */ printf ("Size: %d"tMedian time:", size); printf (" max %f [%d];", global_median_time [np-1], node_id [np-1]); printf (" 2nd max %f [%d];", global_median_time [np-2], node_id [np-2]); printf (" min %f [%d];", global_median_time <ref> [1] </ref>, node_id [1]); printf (""n"); fflush (stdout); recordlog (240496, np, SETS, REPS, size, median_time, 0, 0, 0); /* throw away the alternate group/communicator */ MPI_Comm_free (&SWITCH_ROOT_COMM); 65 MPI_Group_free (&SWITCH_ROOT_GROUP); - /* end SIZES */ closelog (); - void nodei ()- int i, size, set, reps, alt_root; /* for each message size, take a <p> identity of the node */ for (ii=0; ii&lt;np; node_id [ii] = ii++); sort_with_id (np, global_median_time, node_id); /* print/log results */ printf ("Size: %d"tMedian time:", size); printf (" max %f [%d];", global_median_time [np-1], node_id [np-1]); printf (" 2nd max %f [%d];", global_median_time [np-2], node_id [np-2]); printf (" min %f [%d];", global_median_time <ref> [1] </ref>, node_id [1]); printf (""n"); fflush (stdout); recordlog (240496, np, SETS, REPS, size, median_time, 0, 0, 0); /* throw away the alternate group/communicator */ MPI_Comm_free (&SWITCH_ROOT_COMM); MPI_Group_free (&SWITCH_ROOT_GROUP); - /* end SIZES */ closelog (); - void nodei ()- int i, size, set, reps, alt_root; /* for each message size, take <p> the node */ for (ii=0; ii&lt;np; node_id [ii] = ii++); sort_with_id (np, global_median_time, node_id); /* print/log results */ printf ("Size: %d"tMedian time:", size); printf (" max %f [%d];", global_median_time [np-1], node_id [np-1]); printf (" 2nd max %f [%d];", global_median_time [np-2], node_id [np-2]); printf (" min %f [%d];", global_median_time <ref> [1] </ref>, node_id [1]); printf (""n"); fflush (stdout); recordlog (240496, np, SETS, REPS, size, median_time, 0, 0, 0); /* throw away the alternate group/communicator */ MPI_Comm_free (&SWITCH_ROOT_COMM); MPI_Group_free (&SWITCH_ROOT_GROUP); - /* end SIZES */ closelog (); - void nodei ()- int i, size, set, reps, alt_root; /* for each message size, take a set
Reference: [2] <author> T. M. Anderson and R. S. Cornelius. </author> <title> High-performance switching with Fibre Channel. </title> <booktitle> In Digest of Papers Compcon 1992, </booktitle> <pages> pages 261-268. </pages> <publisher> IEEE Computer Society Press, </publisher> <address> 1992. Los Alamitos, Calif. </address>
Reference-contexts: What is making workstation clusters more attractive than before is the introduction of new communication technologies, with much improved performance with respect to the current networks. The fast LANs available today (ATM [6], FDDI [9], Fibrechannel <ref> [2] </ref>, Myrinet [4]), in terms of latency and bandwidth can be comparable to the proprietary interconnection networks found in MPPs. 1.3 The Approach The simple substitution of new, fast network hardware for the old, without a corresponding change to the existing communication software represents only a partial solution, giving less than <p> *verbose) - *num_sets = 5; *num_reps = 20; *data_size = 16; *verbose = 1; if (argc &gt; 1) - *num_sets = atoi (argv [1]); if (*num_sets &gt; max_num_sets) - printf ("Too many sets (%d vs %d)."n", *num_sets, max_num_sets); exit (-1); - if (argc &gt; 2) - *num_reps = atoi (argv <ref> [2] </ref>); if (*num_reps &gt; max_num_reps) - printf ("Too many reps (%d vs %d)."n", *num_reps, max_num_reps); exit (-1); - if (argc &gt; 3) - *data_size = atoi (argv [3]); if (*data_size &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", *data_size, max_data_size); exit (-1); 48 - if (argc &gt; 4) - <p> (max_nodes &gt; 2) - printf ("Error [%i]: test not set up for more than two nodes."n", node_id); exit (-1); - send_type = MPI_INT; recv_type = MPI_INT; cmd_line = malloc (sizeof (double) * 8); if (node_id == 0) - command_line (argc, argv, &num_sets, &num_reps, &data_size, &verbose); cmd_line [0] = num_sets; cmd_line <ref> [2] </ref> = num_reps; cmd_line [4] = data_size; cmd_line [6] = verbose; MPI_Send (&cmd_line [0], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [2], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [4], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [6], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); total_messages = num_sets * num_reps; node0 (); else <p> = MPI_INT; recv_type = MPI_INT; cmd_line = malloc (sizeof (double) * 8); if (node_id == 0) - command_line (argc, argv, &num_sets, &num_reps, &data_size, &verbose); cmd_line [0] = num_sets; cmd_line <ref> [2] </ref> = num_reps; cmd_line [4] = data_size; cmd_line [6] = verbose; MPI_Send (&cmd_line [0], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [2], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [4], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [6], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); total_messages = num_sets * num_reps; node0 (); else MPI_Recv (&cmd_line [0], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv (&cmd_line [2], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); 49 MPI_Recv (&cmd_line <p> (&cmd_line [0], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line <ref> [2] </ref>, 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [4], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [6], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); total_messages = num_sets * num_reps; node0 (); else MPI_Recv (&cmd_line [0], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv (&cmd_line [2], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); 49 MPI_Recv (&cmd_line [4], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv (&cmd_line [6], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); num_sets = cmd_line [0]; num_reps = cmd_line [2]; data_size = cmd_line [4]; verbose = cmd_line [6]; total_messages = num_sets * num_reps; node1 (); - MPI_Finalize <p> num_sets * num_reps; node0 (); else MPI_Recv (&cmd_line [0], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv (&cmd_line <ref> [2] </ref>, 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); 49 MPI_Recv (&cmd_line [4], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv (&cmd_line [6], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); num_sets = cmd_line [0]; num_reps = cmd_line [2]; data_size = cmd_line [4]; verbose = cmd_line [6]; total_messages = num_sets * num_reps; node1 (); - MPI_Finalize (); - A.2 Bandwidth Test Code /* -- bandwidth.c testing the bandwidth of MPI */ #include &lt;stdio.h&gt; #include &lt;string.h&gt; #include &lt;malloc.h&gt; #include "mpi.h" #define max_data_size (64 * 1024) /* 27 gives best bw <p> *verbose) - *num_sets = 5; *num_reps = 20; *data_size = 16; *verbose = 1; if (argc &gt; 1) - *num_sets = atoi (argv [1]); if (*num_sets &gt; max_num_sets) - printf ("Too many sets (%d vs %d)."n", *num_sets, max_num_sets); exit (-1); - if (argc &gt; 2) - *num_reps = atoi (argv <ref> [2] </ref>); if (*num_reps &gt; max_num_reps) - printf ("Too many reps (%d vs %d)."n", *num_reps, max_num_reps); exit (-1); 53 - if (argc &gt; 3) - *data_size = atoi (argv [3]); if (*data_size &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", *data_size, max_data_size); exit (-1); - if (argc &gt; 4) - <p> i++) - MPI_Barrier (MPI_COMM_WORLD); for (i = 0; i &lt; num_reps; i++) - MPI_Barrier (MPI_COMM_WORLD); - - void command_line (int argc, char *argv []) - int cmd_line [] = -12, 100, 128, 1-; /* cmd_line = malloc (sizeof (int) * 4); cmd_line [0] = 12; cmd_line [1] = 100; cmd_line <ref> [2] </ref> = 128; cmd_line [3] = 1; */ if (argc &gt; 1) - cmd_line [0] = atoi (argv [1]); if (cmd_line [0] &gt; max_num_sets) - printf ("Too many sets (%d vs %d)."n", cmd_line [0], max_num_sets); exit (-1); - if (argc &gt; 2) - cmd_line [1] = atoi (argv [2]); if (cmd_line <p> 100; cmd_line <ref> [2] </ref> = 128; cmd_line [3] = 1; */ if (argc &gt; 1) - cmd_line [0] = atoi (argv [1]); if (cmd_line [0] &gt; max_num_sets) - printf ("Too many sets (%d vs %d)."n", cmd_line [0], max_num_sets); exit (-1); - if (argc &gt; 2) - cmd_line [1] = atoi (argv [2]); if (cmd_line [1] &gt; max_num_reps) - printf ("Too many reps (%d vs %d)."n", cmd_line [1], max_num_reps); exit (-1); - if (argc &gt; 3) - cmd_line [2] = atoi (argv [3]); if (cmd_line [2] &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", cmd_line [2], max_data_size); exit (-1); - if <p> - printf ("Too many sets (%d vs %d)."n", cmd_line [0], max_num_sets); exit (-1); - if (argc &gt; 2) - cmd_line [1] = atoi (argv <ref> [2] </ref>); if (cmd_line [1] &gt; max_num_reps) - printf ("Too many reps (%d vs %d)."n", cmd_line [1], max_num_reps); exit (-1); - if (argc &gt; 3) - cmd_line [2] = atoi (argv [3]); if (cmd_line [2] &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", cmd_line [2], max_data_size); exit (-1); - if (argc &gt; 4) - cmd_line [3] = atoi (argv [4]); - MPI_Bcast (cmd_line, sizeof (int) * 4, MPI_INT, 0, MPI_COMM_WORLD); num_sets = cmd_line [0]; 59 num_reps <p> %d)."n", cmd_line [0], max_num_sets); exit (-1); - if (argc &gt; 2) - cmd_line [1] = atoi (argv <ref> [2] </ref>); if (cmd_line [1] &gt; max_num_reps) - printf ("Too many reps (%d vs %d)."n", cmd_line [1], max_num_reps); exit (-1); - if (argc &gt; 3) - cmd_line [2] = atoi (argv [3]); if (cmd_line [2] &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", cmd_line [2], max_data_size); exit (-1); - if (argc &gt; 4) - cmd_line [3] = atoi (argv [4]); - MPI_Bcast (cmd_line, sizeof (int) * 4, MPI_INT, 0, MPI_COMM_WORLD); num_sets = cmd_line [0]; 59 num_reps = cmd_line [1]; data_size = cmd_line [2]; <p> cmd_line [1] = atoi (argv <ref> [2] </ref>); if (cmd_line [1] &gt; max_num_reps) - printf ("Too many reps (%d vs %d)."n", cmd_line [1], max_num_reps); exit (-1); - if (argc &gt; 3) - cmd_line [2] = atoi (argv [3]); if (cmd_line [2] &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", cmd_line [2], max_data_size); exit (-1); - if (argc &gt; 4) - cmd_line [3] = atoi (argv [4]); - MPI_Bcast (cmd_line, sizeof (int) * 4, MPI_INT, 0, MPI_COMM_WORLD); num_sets = cmd_line [0]; 59 num_reps = cmd_line [1]; data_size = cmd_line [2]; verbose = cmd_line [3]; total_messages = num_sets * num_reps; - int main <p> <ref> [2] </ref> &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", cmd_line [2], max_data_size); exit (-1); - if (argc &gt; 4) - cmd_line [3] = atoi (argv [4]); - MPI_Bcast (cmd_line, sizeof (int) * 4, MPI_INT, 0, MPI_COMM_WORLD); num_sets = cmd_line [0]; 59 num_reps = cmd_line [1]; data_size = cmd_line [2]; verbose = cmd_line [3]; total_messages = num_sets * num_reps; - int main (int argc, char *argv []) - /* allocate send and receive buffers */ send_data = malloc (max_data_size * sizeof (int)); recv_data = malloc (max_data_size * sizeof (int)); MPI_Init (&argc, &argv); MPI_Comm_size (MPI_COMM_WORLD, &max_nodes); MPI_Comm_rank (MPI_COMM_WORLD, &node_id); command_line (argc,
Reference: [3] <author> H. E. Bal, M. F. Kaashoek, and A. S. Tanenbaum. Orca: </author> <title> A language for parallel programming of distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(3) </volume> <pages> 190-205, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Besides the MPI message passing library, these are the BSD socket interface, the Converse compiler backend [15], the Tempest runtime library [12], the Orca Project parallel object language <ref> [3] </ref>. FM performance exceeds the performance of messaging layers presently available on a number of MPPs [14, 24, 8]. FM 1.1 is the version which has been used to build the MPI library upon. The two graphs of figure 2.1 show the performance of FM. <p> max_num_sets) - printf ("Too many sets (%d vs %d)."n", *num_sets, max_num_sets); exit (-1); - if (argc &gt; 2) - *num_reps = atoi (argv [2]); if (*num_reps &gt; max_num_reps) - printf ("Too many reps (%d vs %d)."n", *num_reps, max_num_reps); exit (-1); - if (argc &gt; 3) - *data_size = atoi (argv <ref> [3] </ref>); if (*data_size &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", *data_size, max_data_size); exit (-1); 48 - if (argc &gt; 4) - *verbose = atoi (argv [4]); - int main (int argc, char *argv []) - /* allocate send and receive buffers */ send_data = malloc (max_data_size * sizeof <p> - printf ("Too many sets (%d vs %d)."n", *num_sets, max_num_sets); exit (-1); - if (argc &gt; 2) - *num_reps = atoi (argv [2]); if (*num_reps &gt; max_num_reps) - printf ("Too many reps (%d vs %d)."n", *num_reps, max_num_reps); exit (-1); 53 - if (argc &gt; 3) - *data_size = atoi (argv <ref> [3] </ref>); if (*data_size &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", *data_size, max_data_size); exit (-1); - if (argc &gt; 4) - *verbose = atoi (argv [4]); - int main (int argc, char *argv []) - /* allocate send and receive buffers */ send_data = malloc (max_data_size * sizeof (int)); <p> for (i = 0; i &lt; num_reps; i++) - MPI_Barrier (MPI_COMM_WORLD); - - void command_line (int argc, char *argv []) - int cmd_line [] = -12, 100, 128, 1-; /* cmd_line = malloc (sizeof (int) * 4); cmd_line [0] = 12; cmd_line [1] = 100; cmd_line [2] = 128; cmd_line <ref> [3] </ref> = 1; */ if (argc &gt; 1) - cmd_line [0] = atoi (argv [1]); if (cmd_line [0] &gt; max_num_sets) - printf ("Too many sets (%d vs %d)."n", cmd_line [0], max_num_sets); exit (-1); - if (argc &gt; 2) - cmd_line [1] = atoi (argv [2]); if (cmd_line [1] &gt; max_num_reps) - <p> sets (%d vs %d)."n", cmd_line [0], max_num_sets); exit (-1); - if (argc &gt; 2) - cmd_line [1] = atoi (argv [2]); if (cmd_line [1] &gt; max_num_reps) - printf ("Too many reps (%d vs %d)."n", cmd_line [1], max_num_reps); exit (-1); - if (argc &gt; 3) - cmd_line [2] = atoi (argv <ref> [3] </ref>); if (cmd_line [2] &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", cmd_line [2], max_data_size); exit (-1); - if (argc &gt; 4) - cmd_line [3] = atoi (argv [4]); - MPI_Bcast (cmd_line, sizeof (int) * 4, MPI_INT, 0, MPI_COMM_WORLD); num_sets = cmd_line [0]; 59 num_reps = cmd_line [1]; data_size <p> - printf ("Too many reps (%d vs %d)."n", cmd_line [1], max_num_reps); exit (-1); - if (argc &gt; 3) - cmd_line [2] = atoi (argv <ref> [3] </ref>); if (cmd_line [2] &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", cmd_line [2], max_data_size); exit (-1); - if (argc &gt; 4) - cmd_line [3] = atoi (argv [4]); - MPI_Bcast (cmd_line, sizeof (int) * 4, MPI_INT, 0, MPI_COMM_WORLD); num_sets = cmd_line [0]; 59 num_reps = cmd_line [1]; data_size = cmd_line [2]; verbose = cmd_line [3]; total_messages = num_sets * num_reps; - int main (int argc, char *argv []) - /* allocate send and receive <p> printf ("Too much data (%d vs %d)."n", cmd_line [2], max_data_size); exit (-1); - if (argc &gt; 4) - cmd_line <ref> [3] </ref> = atoi (argv [4]); - MPI_Bcast (cmd_line, sizeof (int) * 4, MPI_INT, 0, MPI_COMM_WORLD); num_sets = cmd_line [0]; 59 num_reps = cmd_line [1]; data_size = cmd_line [2]; verbose = cmd_line [3]; total_messages = num_sets * num_reps; - int main (int argc, char *argv []) - /* allocate send and receive buffers */ send_data = malloc (max_data_size * sizeof (int)); recv_data = malloc (max_data_size * sizeof (int)); MPI_Init (&argc, &argv); MPI_Comm_size (MPI_COMM_WORLD, &max_nodes); MPI_Comm_rank (MPI_COMM_WORLD, &node_id); command_line (argc, argv); if (node_id ==
Reference: [4] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. </author> <title> Myrinet|a gigabit-per-second local-area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> February </month> <year> 1995. </year> <note> Available from http://www.myri.com/research/publications/Hot.ps. </note>
Reference-contexts: What is making workstation clusters more attractive than before is the introduction of new communication technologies, with much improved performance with respect to the current networks. The fast LANs available today (ATM [6], FDDI [9], Fibrechannel [2], Myrinet <ref> [4] </ref>), in terms of latency and bandwidth can be comparable to the proprietary interconnection networks found in MPPs. 1.3 The Approach The simple substitution of new, fast network hardware for the old, without a corresponding change to the existing communication software represents only a partial solution, giving less than satisfactory results <p> In the present version of FM, this represents a major bottleneck for bandwidth and directly limits performance for long messages. 9 2.2 The Network Myrinet is a high speed LAN interconnect which uses bidirectional byte-wide copper links to achieve physical bandwidth of nearly 80 MB/s in each direction <ref> [4] </ref>. It uses the interconnect technology developed for the Caltech Mosaic project [20]. A Myrinet network is composed of network interfaces connected to crossbar switches by point-to-point links. <p> - printf ("Too many reps (%d vs %d)."n", *num_reps, max_num_reps); exit (-1); - if (argc &gt; 3) - *data_size = atoi (argv [3]); if (*data_size &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", *data_size, max_data_size); exit (-1); 48 - if (argc &gt; 4) - *verbose = atoi (argv <ref> [4] </ref>); - int main (int argc, char *argv []) - /* allocate send and receive buffers */ send_data = malloc (max_data_size * sizeof (int)); recv_data = malloc (max_data_size * sizeof (int)); MPI_Init (&argc, &argv); MPI_Comm_size (MPI_COMM_WORLD, &max_nodes); MPI_Comm_rank (MPI_COMM_WORLD, &node_id); if (max_nodes &gt; 2) - printf ("Error [%i]: test not set <p> printf ("Error [%i]: test not set up for more than two nodes."n", node_id); exit (-1); - send_type = MPI_INT; recv_type = MPI_INT; cmd_line = malloc (sizeof (double) * 8); if (node_id == 0) - command_line (argc, argv, &num_sets, &num_reps, &data_size, &verbose); cmd_line [0] = num_sets; cmd_line [2] = num_reps; cmd_line <ref> [4] </ref> = data_size; cmd_line [6] = verbose; MPI_Send (&cmd_line [0], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [2], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [4], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [6], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); total_messages = num_sets * num_reps; node0 (); else MPI_Recv (&cmd_line [0], 1, <p> (sizeof (double) * 8); if (node_id == 0) - command_line (argc, argv, &num_sets, &num_reps, &data_size, &verbose); cmd_line [0] = num_sets; cmd_line [2] = num_reps; cmd_line <ref> [4] </ref> = data_size; cmd_line [6] = verbose; MPI_Send (&cmd_line [0], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [2], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [4], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [6], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); total_messages = num_sets * num_reps; node0 (); else MPI_Recv (&cmd_line [0], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv (&cmd_line [2], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); 49 MPI_Recv (&cmd_line [4], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv <p> 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line <ref> [4] </ref>, 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [6], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); total_messages = num_sets * num_reps; node0 (); else MPI_Recv (&cmd_line [0], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv (&cmd_line [2], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); 49 MPI_Recv (&cmd_line [4], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv (&cmd_line [6], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); num_sets = cmd_line [0]; num_reps = cmd_line [2]; data_size = cmd_line [4]; verbose = cmd_line [6]; total_messages = num_sets * num_reps; node1 (); - MPI_Finalize (); - A.2 Bandwidth Test Code /* -- bandwidth.c testing <p> (); else MPI_Recv (&cmd_line [0], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv (&cmd_line [2], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); 49 MPI_Recv (&cmd_line <ref> [4] </ref>, 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv (&cmd_line [6], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); num_sets = cmd_line [0]; num_reps = cmd_line [2]; data_size = cmd_line [4]; verbose = cmd_line [6]; total_messages = num_sets * num_reps; node1 (); - MPI_Finalize (); - A.2 Bandwidth Test Code /* -- bandwidth.c testing the bandwidth of MPI */ #include &lt;stdio.h&gt; #include &lt;string.h&gt; #include &lt;malloc.h&gt; #include "mpi.h" #define max_data_size (64 * 1024) /* 27 gives best bw */ #define max_num_reps max_data_size <p> - printf ("Too many reps (%d vs %d)."n", *num_reps, max_num_reps); exit (-1); 53 - if (argc &gt; 3) - *data_size = atoi (argv [3]); if (*data_size &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", *data_size, max_data_size); exit (-1); - if (argc &gt; 4) - *verbose = atoi (argv <ref> [4] </ref>); - int main (int argc, char *argv []) - /* allocate send and receive buffers */ send_data = malloc (max_data_size * sizeof (int)); recv_data = malloc (max_data_size * sizeof (int)); /* status = (MPI_Status *)malloc (num_reps*sizeof (MPI_Status)); request = (MPI_Request *)malloc (num_reps*sizeof (MPI_Request)); */ MPI_Init (&argc, &argv); MPI_Comm_size (MPI_COMM_WORLD, &max_nodes); <p> reps (%d vs %d)."n", cmd_line [1], max_num_reps); exit (-1); - if (argc &gt; 3) - cmd_line [2] = atoi (argv [3]); if (cmd_line [2] &gt; max_data_size) - printf ("Too much data (%d vs %d)."n", cmd_line [2], max_data_size); exit (-1); - if (argc &gt; 4) - cmd_line [3] = atoi (argv <ref> [4] </ref>); - MPI_Bcast (cmd_line, sizeof (int) * 4, MPI_INT, 0, MPI_COMM_WORLD); num_sets = cmd_line [0]; 59 num_reps = cmd_line [1]; data_size = cmd_line [2]; verbose = cmd_line [3]; total_messages = num_sets * num_reps; - int main (int argc, char *argv []) - /* allocate send and receive buffers */ send_data =
Reference: [5] <author> Greg Buzzard, David Jacobson, Scott Marovich, and John Wilkes. Hamlyn: </author> <title> A high-performance network interface with sender-based memory management. </title> <booktitle> In Proceedings of the IEEE Hot Interconnects Symposium, </booktitle> <year> 1995. </year> <note> Available from ftp://ftp.hpl.hp.com/ wilkes/HamlynHotIntIII.ps.Z. </note>
Reference-contexts: The problem of ATM networks is the cost of the switch, much higher the cost of a comparable Myrinet switch. Hamlyn <ref> [5] </ref> implements a sender-based memory management scheme, and give applications direct access to the network interface. It is planned to be implemented on a proprietary version of Myrinet for HP workstations.
Reference: [6] <author> CCITT, </author> <title> SG XVIII, Report R34. Draft Recommendation I.150: B-ISDN ATM functional characteristics, </title> <month> June </month> <year> 1990. </year>
Reference-contexts: What is making workstation clusters more attractive than before is the introduction of new communication technologies, with much improved performance with respect to the current networks. The fast LANs available today (ATM <ref> [6] </ref>, FDDI [9], Fibrechannel [2], Myrinet [4]), in terms of latency and bandwidth can be comparable to the proprietary interconnection networks found in MPPs. 1.3 The Approach The simple substitution of new, fast network hardware for the old, without a corresponding change to the existing communication software represents only a partial <p> not set up for more than two nodes."n", node_id); exit (-1); - send_type = MPI_INT; recv_type = MPI_INT; cmd_line = malloc (sizeof (double) * 8); if (node_id == 0) - command_line (argc, argv, &num_sets, &num_reps, &data_size, &verbose); cmd_line [0] = num_sets; cmd_line [2] = num_reps; cmd_line [4] = data_size; cmd_line <ref> [6] </ref> = verbose; MPI_Send (&cmd_line [0], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [2], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [4], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [6], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); total_messages = num_sets * num_reps; node0 (); else MPI_Recv (&cmd_line [0], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, <p> - command_line (argc, argv, &num_sets, &num_reps, &data_size, &verbose); cmd_line [0] = num_sets; cmd_line [2] = num_reps; cmd_line [4] = data_size; cmd_line <ref> [6] </ref> = verbose; MPI_Send (&cmd_line [0], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [2], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [4], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line [6], 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); total_messages = num_sets * num_reps; node0 (); else MPI_Recv (&cmd_line [0], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv (&cmd_line [2], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); 49 MPI_Recv (&cmd_line [4], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv (&cmd_line [6], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); <p> send_type, 1, SEND_TAG, MPI_COMM_WORLD); MPI_Send (&cmd_line <ref> [6] </ref>, 1, send_type, 1, SEND_TAG, MPI_COMM_WORLD); total_messages = num_sets * num_reps; node0 (); else MPI_Recv (&cmd_line [0], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv (&cmd_line [2], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); 49 MPI_Recv (&cmd_line [4], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv (&cmd_line [6], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); num_sets = cmd_line [0]; num_reps = cmd_line [2]; data_size = cmd_line [4]; verbose = cmd_line [6]; total_messages = num_sets * num_reps; node1 (); - MPI_Finalize (); - A.2 Bandwidth Test Code /* -- bandwidth.c testing the bandwidth of MPI */ #include &lt;stdio.h&gt; #include &lt;string.h&gt; <p> [0], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv (&cmd_line [2], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); 49 MPI_Recv (&cmd_line [4], 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); MPI_Recv (&cmd_line <ref> [6] </ref>, 1, recv_type, 0, MPI_ANY_TAG, MPI_COMM_WORLD, &status); num_sets = cmd_line [0]; num_reps = cmd_line [2]; data_size = cmd_line [4]; verbose = cmd_line [6]; total_messages = num_sets * num_reps; node1 (); - MPI_Finalize (); - A.2 Bandwidth Test Code /* -- bandwidth.c testing the bandwidth of MPI */ #include &lt;stdio.h&gt; #include &lt;string.h&gt; #include &lt;malloc.h&gt; #include "mpi.h" #define max_data_size (64 * 1024) /* 27 gives best bw */ #define max_num_reps max_data_size #define max_num_sets 256 #define
Reference: [7] <author> D. Clark. </author> <title> The structure of systems using upcalls. </title> <booktitle> In ACM Symp. OS Principle '85, </booktitle> <pages> pages 171-180, </pages> <year> 1985. </year>
Reference-contexts: Rather than trying to relax these constraints, for example by duplicating on the send side the knowledge about which buffer to use, an inquiry mechanism employing an upcall <ref> [7] </ref> has been adopted. An upcall is a function defined inside an upper layer, and invoked by the layer below. Whenever the first fragment of a long message arrives, FM asks the application code for a buffer to use as reassembly buffer.
Reference: [8] <author> Cray Research, Inc. </author> <title> Cray T3D System Architecture Overview, </title> <month> March </month> <year> 1993. </year>
Reference-contexts: Besides the MPI message passing library, these are the BSD socket interface, the Converse compiler backend [15], the Tempest runtime library [12], the Orca Project parallel object language [3]. FM performance exceeds the performance of messaging layers presently available on a number of MPPs <ref> [14, 24, 8] </ref>. FM 1.1 is the version which has been used to build the MPI library upon. The two graphs of figure 2.1 show the performance of FM. The experimental setup is composed of two SPARCstation 20 with 75 MHz SuperSPARC-II processors and 1 MB cache.
Reference: [9] <institution> Fiber-distributed data interface (FDDI)|Token ring media access control (MAC). American National Standard for Information Systems ANSI X3.139-1987, </institution> <month> July </month> <year> 1987. </year> <institution> American National Standards Institute. </institution>
Reference-contexts: What is making workstation clusters more attractive than before is the introduction of new communication technologies, with much improved performance with respect to the current networks. The fast LANs available today (ATM [6], FDDI <ref> [9] </ref>, Fibrechannel [2], Myrinet [4]), in terms of latency and bandwidth can be comparable to the proprietary interconnection networks found in MPPs. 1.3 The Approach The simple substitution of new, fast network hardware for the old, without a corresponding change to the existing communication software represents only a partial solution, giving
Reference: [10] <author> Message Passing Interface Forum. </author> <title> The MPI message passing interface standard. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, </institution> <month> April </month> <year> 1994. </year> <note> Available from http://www.mcs.anl.gov/mpi/mpi-report.ps. </note>
Reference-contexts: The MPI standard appears to be rapidly gaining support since the initial draft was presented at the Supercomputing 93 conference (November 1993) and its introduction at the Message Passing Interface Forum <ref> [10] </ref>. One of its largest attractions is the number of free implementations that have been made available.
Reference: [11] <author> H. Franke, C. E. Wu, M Riviere, P Pattnik, and M Snir. </author> <title> MPI programming environment for IBM SP1/SP2. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: Using a faster medium than Ethernet does not bring much improvement [17]. Some of the same libraries have been ported to MPPs. For example, two implementations of MPI are available on the SP2. One is a port of MPICH, the other (MPI-F) is a native implementation <ref> [11] </ref>. Both achieve 33 MB/s of peak bandwidth, with a 0-byte latency of 40.5 s for MPI-F and 55 s for the MPICH port.
Reference: [12] <author> Mark D. Hill, James R. Larus, and David A. Wood. </author> <title> Tempest: A sub-strate for portable parallel programs. </title> <booktitle> In Compcon, </booktitle> <month> March </month> <year> 1995. </year> <note> Available from ftp://ftp.cs.wisc.edu/wwt/compcon95 tempest.ps. </note>
Reference-contexts: The range of applications being considered for development on FM, or already in the works, is testimonial of the flexibility of its interface. Besides the MPI message passing library, these are the BSD socket interface, the Converse compiler backend [15], the Tempest runtime library <ref> [12] </ref>, the Orca Project parallel object language [3]. FM performance exceeds the performance of messaging layers presently available on a number of MPPs [14, 24, 8]. FM 1.1 is the version which has been used to build the MPI library upon.
Reference: [13] <author> Norman C. Hutchinson and Larry L. Peterson. </author> <title> The x-Kernel: An Architecture for Implementing Network Protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: In a sense, this goes against a common tendency to design a communication stack using blocks with independently defined interfaces, and in which service invocations occur from the top down only. An example of such approach is the x-Kernel architecture <ref> [13] </ref>, which in defining a uniform abstraction for encapsulating protocols, prescribes the primitive calls that protocol objects can use to interact.
Reference: [14] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview, </title> <year> 1991. </year>
Reference-contexts: Besides the MPI message passing library, these are the BSD socket interface, the Converse compiler backend [15], the Tempest runtime library [12], the Orca Project parallel object language [3]. FM performance exceeds the performance of messaging layers presently available on a number of MPPs <ref> [14, 24, 8] </ref>. FM 1.1 is the version which has been used to build the MPI library upon. The two graphs of figure 2.1 show the performance of FM. The experimental setup is composed of two SPARCstation 20 with 75 MHz SuperSPARC-II processors and 1 MB cache.
Reference: [15] <author> Laxmikant V. Kale, Milind Bhandarkar, Narain Jagathesan, Sanjeev Krishnan, and Joshua M. Yelon. </author> <title> Converse: an interoperable framework for parallel programming. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium, </booktitle> <year> 1996. </year> <note> Available from http://charm.cs.uiuc.edu/papers/converse-ipps96.ps. </note>
Reference-contexts: The range of applications being considered for development on FM, or already in the works, is testimonial of the flexibility of its interface. Besides the MPI message passing library, these are the BSD socket interface, the Converse compiler backend <ref> [15] </ref>, the Tempest runtime library [12], the Orca Project parallel object language [3]. FM performance exceeds the performance of messaging layers presently available on a number of MPPs [14, 24, 8]. FM 1.1 is the version which has been used to build the MPI library upon.
Reference: [16] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Software overhead in messaging layers: </title> <booktitle> Where does the time go? In Proceedings of the Sixth Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <year> 1994. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/asplos94.ps. </note>
Reference-contexts: This scheme is simple and efficient, only requires a counter for each of the other nodes of the network, and has the benefit of preserving the order of the packets. Reliable and in-order delivery guarantees can be expensive if implemented in the higher level messaging layers <ref> [16] </ref>. Their cost can be decreased if built directly into the lower level layer, where there is an opportunity to take advantage of some useful features of the network.
Reference: [17] <author> M. Liu, J. Hsieh, D. Hu, J. Thomas, and J. MacDonald. </author> <title> Distributed network computing over Local ATM Networks. </title> <booktitle> In Supercomputing '94, </booktitle> <year> 1995. </year>
Reference-contexts: in terms of latency and bandwidth can be comparable to the proprietary interconnection networks found in MPPs. 1.3 The Approach The simple substitution of new, fast network hardware for the old, without a corresponding change to the existing communication software represents only a partial solution, giving less than satisfactory results <ref> [17] </ref>. One order of magnitude less for bandwidth (megabytes vs tens of megabytes), two orders less for latency (milliseconds vs tens of microseconds), can be expected at the software level with respect to the hardware performance. <p> Using a faster medium than Ethernet does not bring much improvement <ref> [17] </ref>. Some of the same libraries have been ported to MPPs. For example, two implementations of MPI are available on the SP2. One is a port of MPICH, the other (MPI-F) is a native implementation [11].
Reference: [18] <author> Neil R. McKenzie, Kevin Bolding, Carl Ebeling, and Lawrence Snyder. Cranium: </author> <title> An interface for message passing on adaptive packet routing networks. </title> <booktitle> In Proceedings of the 1994 Parallel Computer Routing and Communication Workshop, </booktitle> <month> May </month> <year> 1994. </year> <note> Available from ftp://shrimp.cs.washington.edu/pub/chaos/docs/cranium-pcrcw.ps.Z. </note>
Reference-contexts: Hamlyn [5] implements a sender-based memory management scheme, and give applications direct access to the network interface. It is planned to be implemented on a proprietary version of Myrinet for HP workstations. Cranium <ref> [18] </ref> is in many respect similar to Hamlyn, and is to be implemented on a experimental interconnection network, Chaos. However, to our knowledge, at the time of this writing there are no high performance MPI implementations available on these interfaces.
Reference: [19] <author> Scott Pakin, Mario Lauria, and Andrew Chien. </author> <title> High performance messaging on workstations: Illinois Fast Messages (FM) for Myrinet. </title> <booktitle> In Supercomputing, </booktitle> <month> December </month> <year> 1995. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/myrinet-fm-sc95.ps. </note>
Reference-contexts: One solution is to build new communication software, designed from the start with these objectives in mind. This has been the objective of the Fast Messages (FM) project <ref> [19] </ref>. In such project, the Myrinet network was selected due to its programmability and price/performance ratio. Then, an highly optimized, low latency messaging layer providing a virtual interface to the hardware was developed. <p> FM achieves a short message latency of only 14 s and a peak bandwidth of 17.6 MB/s, with an Active Messages style interface. The first part of the research has been completed by the Concurrent Systems Architecture Group (CSAG) FM team (including the author) <ref> [19] </ref>. Only the main results and the essentials of the FM interface will be presented here. The second phase of the research constitutes the object of this work, and will be described in greater detail. After this first phase, the problem of optimizing the FM interface was tackled. <p> This technique requires additional queue management, the handling of acknowledgments, a timeout device, and provisions to deal with duplicated 39 packets. Instead, the FM layer is built in a way not only to offer this guarantee, but also to pay little or no cost for it <ref> [19] </ref>. Some low level messaging layers offer different primitives, each targeted to a specific use, like low latency transfer, or remote memory copy. The typical utilization domain of each primitive can be best represented using a multidimensional space, with each dimension representing one of the major features characterizing the primitives.
Reference: [20] <author> C. Seitz, N. Boden, J. Seizovic, and W. Su. </author> <title> The design of the Caltech Mosaic C multicom-puter. </title> <booktitle> In Proceedings of the University of Washington Symposium on Integrated Systems, </booktitle> <year> 1993. </year> <note> Available from http://www.myri.com/research/publications/sbss.ps. </note>
Reference-contexts: It uses the interconnect technology developed for the Caltech Mosaic project <ref> [20] </ref>. A Myrinet network is composed of network interfaces connected to crossbar switches by point-to-point links. The reasons for this particular network has been preferred to others are its speed, its low price, and the availability of development tools to create the control program of the interface.
Reference: [21] <author> David Sitsky, David Walsh, and Chris Johnson. </author> <title> Implementation and performance of the MPI Message Passing Interface on the Fujistu AP1000 Multicomputer. </title> <note> In Proceedings of ACSC'95. Available from ftp://dcssoft.anu.edu.au/pub/www/dcs/cap/mpi/mpi.html. </note>
Reference-contexts: It achieves a minimum latency of 43 s; we measured a peak bandwidth of only 31.7 MB/s, despite a network which can provide up to 300 MB/s. MPI is available on the AP1000, where it achieves 332 s minimum latency and 2.85 MB/s peak bandwidth <ref> [21] </ref>. 3 Available at http://www.epcc.ed.ac.uk/t3dmpi/Product/index.html 17 Chapter 3 Design The approach followed in creating MPI-FM has been one of incremental refinements to a straightforward implementation of the ADI.
Reference: [22] <author> C.B. Stunkel, D.G. Shea, D.G. Grice, P.H. Hochschild, and M. Tsao. </author> <title> The SP1 high-performance switch. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference, </booktitle> <pages> pages 150-157, </pages> <address> Knoxville, TN,, </address> <month> May </month> <year> 1994. </year> <note> Available from http://ibm.tc.cornell.edu/ibm/pps/doc/hps.ps. </note>
Reference-contexts: Second, from an architectural point of view the SP2 itself is very close to a network of workstations. Its nodes are ordinary, rack-mounted IBM RISC workstations running an unmodified AIX; the only difference is in the interconnect, a proprietary network based on the Vulcan switch <ref> [22] </ref>. The graphs show that MPI-FM has better or comparable latency and bandwidth for message sizes up to 2 KB.
Reference: [23] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency, Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-340, </pages> <month> [12] </month> <year> 1990. </year>
Reference-contexts: Having examined the advantages of workstation clusters, let's discuss what has slowed their progress so far. The use of networked machines for distributed computing is not new, and a number of communication libraries exploiting the ubiquitous Ethernet have been around for a while now (BSD sockets, PVM <ref> [23] </ref>). But Ethernet and its associated networking protocols were not designed for parallel high performance computing, and their limitations when used for this purpose restrict the usefulness of this approach. For example, TCP/IP was designed to achieve reliable and efficient internetworking over slow, unreliable, LANs and WANs.
Reference: [24] <institution> Thinking Machines Corporation, </institution> <address> 245 First Street, Cambridge, MA 02154-1264. </address> <booktitle> The Connection Machine CM-5 Technical Summary, </booktitle> <month> October </month> <year> 1991. </year> <month> 80 </month>
Reference-contexts: Besides the MPI message passing library, these are the BSD socket interface, the Converse compiler backend [15], the Tempest runtime library [12], the Orca Project parallel object language [3]. FM performance exceeds the performance of messaging layers presently available on a number of MPPs <ref> [14, 24, 8] </ref>. FM 1.1 is the version which has been used to build the MPI library upon. The two graphs of figure 2.1 show the performance of FM. The experimental setup is composed of two SPARCstation 20 with 75 MHz SuperSPARC-II processors and 1 MB cache.
Reference: [25] <author> T. von Eicken, D. Culler, S. Goldstein, and K. Schauser. </author> <title> Active Messages: a mecha-nism for integrated communication and computation. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: The FM send4 () is a specialized version for messages of no more than four words, optimized for latency. The FM interface is a generalization of the Active Message model <ref> [25] </ref>, in that there are no restrictions on the communication operations that can be carried out by the handler (the user is responsible for avoiding deadlock situations).
Reference: [26] <author> Thorsten von Eicken, Anindya Basu, Vineet Buch, and Werner Vogels. U-Net: </author> <title> A user-level network interface for parallel and distributed computing. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year> <note> Available from http://www.cs.cornell.edu/Info/Projects/ATM/sosp.ps. 81 </note>
Reference-contexts: Table 2.2 summarizes the core message passing routines, representing the minimum set required to have a fully functional implementation. 2.5 Related Work A number of other research projects are focusing on the construction of an integrated high performance hardware-software communication subsystem for network of workstations. U-Net <ref> [26] </ref> is built using FORE ATM interface cards, and presents an AAL5 programming interface to the applications.
References-found: 26

