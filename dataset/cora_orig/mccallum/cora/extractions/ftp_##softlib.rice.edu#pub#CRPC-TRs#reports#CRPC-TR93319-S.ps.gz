URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR93319-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Slicing Analysis and Indirect Access to Distributed Arrays  
Author: Raja Das, Joel Saltz, Reinhard von Hanxleden 
Address: P.O. Box 1892 Houston, TX 77251-1892  
Affiliation: Rice University  
Note: Center for Research on Parallel Computation  
Date: June 1993  
Pubnum: CRPC-TR93319-S  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: E-mail: fraja|saltzg@cs.umd.edu. x Center for Research on Parallel Computation, Rice University, Houston, TX 77251. E-mail: reinhard@rice.edu. Based on initial projects like Fortran D [5, 10] and Vienna Fortran <ref> [1, 17] </ref>, the High Performance Fortran Forum has proposed the first version of High Performance Fortran (HPF) [4], which can be thought of as Fortran 90 enhanced with data distribution annotations. Long term storage of distributed array data is assigned to specific memory locations in the distributed machine. <p> Current prototypes of compilers for HPF-like languages produce Single Program Multiple Data (SPMD) code with message passing and/or runtime communication primitives. Reducing communication costs is crucial in achieving good performance on applications [7, 9]. While current systems like the Fortran D project [10] and the Vienna Fortran Compilation system <ref> [1] </ref> have implemented a number of optimizations for reducing communication costs (like message blocking, collective communication, message coalescing and aggregation), these optimizations have been developed mostly in the context of regular problems (i.e., for codes having only regular data access patterns).
Reference: [2] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis. </author> <title> Distributed memory compiler methods for irregular problems data copy reuse and runtime partitioning. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <booktitle> Languages, Compilers and Runtime Environments for Distributed Memory Machines, </booktitle> <pages> pages 185-220. </pages> <address> Else-vier, </address> <year> 1992. </year>
Reference-contexts: In earlier work, we have developed runtime support, analysis techniques, and compiler prototypes designed to handle loops where distributed arrays are accessed through a single level of indirection <ref> [2, 6, 11, 15] </ref>.
Reference: [3] <author> R. Das, J. Saltz, D. Mavriplis, and R. Ponnusamy. </author> <title> The incremental scheduler. In Unstructured Scientific Computation on Scalable Multiprocessors, </title> <address> Cambridge Mass, 1992. </address> <publisher> MIT Press. </publisher>
Reference-contexts: We have also carried out extensive research on the design of runtime support to support the communication requirements associated with indirectly accessed distributed arrays. One of the key optimizations is to reduce the communication volume associated with a prefetch by recognizing duplicated data references <ref> [14, 3] </ref>. While this paper did not focus on these runtime support issues, we have implicitly assumed the existence of such runtime support. We are implementing the algorithm presented in Section 4 in the Fortran D compiler being developed at Rice University.
Reference: [4] <author> D. Loveman (Ed.). </author> <title> Draft High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: E-mail: fraja|saltzg@cs.umd.edu. x Center for Research on Parallel Computation, Rice University, Houston, TX 77251. E-mail: reinhard@rice.edu. Based on initial projects like Fortran D [5, 10] and Vienna Fortran [1, 17], the High Performance Fortran Forum has proposed the first version of High Performance Fortran (HPF) <ref> [4] </ref>, which can be thought of as Fortran 90 enhanced with data distribution annotations. Long term storage of distributed array data is assigned to specific memory locations in the distributed machine.
Reference: [5] <author> Geoffrey Fox, Seema Hiranandani, Ken Kennedy, Charles Koelbel, Uli Kremer, Chau-Wen Tseng, and Min-You Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report CRPC-TR90079, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: E-mail: fraja|saltzg@cs.umd.edu. x Center for Research on Parallel Computation, Rice University, Houston, TX 77251. E-mail: reinhard@rice.edu. Based on initial projects like Fortran D <ref> [5, 10] </ref> and Vienna Fortran [1, 17], the High Performance Fortran Forum has proposed the first version of High Performance Fortran (HPF) [4], which can be thought of as Fortran 90 enhanced with data distribution annotations.
Reference: [6] <author> R. v. Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz. </author> <title> Compiler analysis for irregular problems in Fortran D. </title> <booktitle> In Proceedings of the Fifth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> New Haven, CT, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: In earlier work, we have developed runtime support, analysis techniques, and compiler prototypes designed to handle loops where distributed arrays are accessed through a single level of indirection <ref> [2, 6, 11, 15] </ref>. <p> In previous work we had presented a dataflow framework that determines the right placement of communication routines such that maximum reuse of off-processor data is possible <ref> [6] </ref>. This dataflow analysis in combination with the transformation presented in this paper will allow to generate efficient parallel code for any problem where irregular data access patterns exist.
Reference: [7] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Evaluation of compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the Sixth International Conference on Supercomputing. </booktitle> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: Current prototypes of compilers for HPF-like languages produce Single Program Multiple Data (SPMD) code with message passing and/or runtime communication primitives. Reducing communication costs is crucial in achieving good performance on applications <ref> [7, 9] </ref>.
Reference: [8] <author> S. Hiranandani, J. Saltz, P. Mehrotra, and H. Berry-man. </author> <title> Performance of hashed cache data migration schemes on multicomputers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12 </volume> <pages> 415-422, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Here some set representation, like a hash table, which collects subscripts and stores each of them at most once, would be an appropriate trace recording mechanism. Using a hash table to store off-processor data values was first introduced in <ref> [8] </ref>. The space requirements are only O (R), and we also do not need any counting slices. The time per access, however, will be O (log (R)) for most common set representations.
Reference: [9] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiler optimizations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings Supercomputing '91, </booktitle> <pages> pages 86-100. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1991. </year>
Reference-contexts: Current prototypes of compilers for HPF-like languages produce Single Program Multiple Data (SPMD) code with message passing and/or runtime communication primitives. Reducing communication costs is crucial in achieving good performance on applications <ref> [7, 9] </ref>.
Reference: [10] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: E-mail: fraja|saltzg@cs.umd.edu. x Center for Research on Parallel Computation, Rice University, Houston, TX 77251. E-mail: reinhard@rice.edu. Based on initial projects like Fortran D <ref> [5, 10] </ref> and Vienna Fortran [1, 17], the High Performance Fortran Forum has proposed the first version of High Performance Fortran (HPF) [4], which can be thought of as Fortran 90 enhanced with data distribution annotations. <p> Current prototypes of compilers for HPF-like languages produce Single Program Multiple Data (SPMD) code with message passing and/or runtime communication primitives. Reducing communication costs is crucial in achieving good performance on applications [7, 9]. While current systems like the Fortran D project <ref> [10] </ref> and the Vienna Fortran Compilation system [1] have implemented a number of optimizations for reducing communication costs (like message blocking, collective communication, message coalescing and aggregation), these optimizations have been developed mostly in the context of regular problems (i.e., for codes having only regular data access patterns).
Reference: [11] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In earlier work, we have developed runtime support, analysis techniques, and compiler prototypes designed to handle loops where distributed arrays are accessed through a single level of indirection <ref> [2, 6, 11, 15] </ref>.
Reference: [12] <author> R. Mirchandaney, J. H. Saltz, R. M. Smith, D. M. Nicol, and Kay Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the 1988 ACM International Conference on Supercomputing, </booktitle> <pages> pages 140-152, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: (m) REAL x (n), y (n) !HPF$ DISTRIBUTE (BLOCK) :: col, x, y !HPF$ EXECUTE (i) ON HOME x (i) K1 FORALL i = 1, n K2 x (i) = x (i) + y (col (i)) K3 ENDFORALL K4 END the loops into two constructs called an inspector and executor <ref> [12] </ref>. During program execution, the inspector examines the data references made by a processor and calculates what off-processor data need to be fetched and where these data will be stored once they are received. The executor loop then uses the information from the inspector to implement the actual computation.
Reference: [13] <author> Y. Saad. Sparsekit: </author> <title> a basic tool kit for sparse matrix computations. </title> <type> Report 90-20, </type> <institution> RIACS, </institution> <year> 1990. </year>
Reference-contexts: Section 5 concludes with a brief discussion and an overview of the status of our implementation. 2 Example Transformation This section illustrates the effect of applying our transformation to the HPF subroutine CSR shown in Row format <ref> [13] </ref>. An n by n sparse matrix is multiplied by an n-element array x, the results are stored in an n-element array y. The matrix values are all assumed to be equal to zero or one.
Reference: [14] <author> J. Saltz, R. Das, R. Ponnusamy, D. Mavriplis, H Berryman, and J. Wu. </author> <title> Parti procedures for realistic loops. </title> <booktitle> In Proceedings of the 6th Distributed Memory Computing Conference, </booktitle> <address> Portland, Oregon, </address> <month> April-May </month> <year> 1991. </year> <month> 10 </month>
Reference-contexts: We have also carried out extensive research on the design of runtime support to support the communication requirements associated with indirectly accessed distributed arrays. One of the key optimizations is to reduce the communication volume associated with a prefetch by recognizing duplicated data references <ref> [14, 3] </ref>. While this paper did not focus on these runtime support issues, we have implicitly assumed the existence of such runtime support. We are implementing the algorithm presented in Section 4 in the Fortran D compiler being developed at Rice University.
Reference: [15] <author> Joel Saltz, Harry Berryman, and Janet Wu. </author> <title> Multi--processors and runtime compilation. </title> <type> Technical Report 90-59, </type> <institution> ICASE, NASA Langley Research Center, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: In earlier work, we have developed runtime support, analysis techniques, and compiler prototypes designed to handle loops where distributed arrays are accessed through a single level of indirection <ref> [2, 6, 11, 15] </ref>.
Reference: [16] <author> G. A. Venkatesh. </author> <title> The semantic approach to program slicing. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 107-119, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Each of the slices has the following properties with respect to the original program P : * Inserting s code at s target in P is legal; i.e., it does not change the meaning of P . The s code is similar to a dynamic backward executable slice <ref> [16] </ref>. * After executing s code , s ident will have stored the values of s vn . * If s is a collecting slice, then s vn will be the value number of a subscript s ast of a nonlocal array reference arr (sub ast ) in P , and
Reference: [17] <author> H. Zima, P. Brezany, B. Chapman, P. Mehrotra, and A. Schwald. </author> <title> Vienna Fortran | a language specification, version 1.1. </title> <type> Interim Report 21, </type> <institution> ICASE, NASA Langley Research Center, </institution> <month> March </month> <year> 1992. </year> <month> 11 </month>
Reference-contexts: E-mail: fraja|saltzg@cs.umd.edu. x Center for Research on Parallel Computation, Rice University, Houston, TX 77251. E-mail: reinhard@rice.edu. Based on initial projects like Fortran D [5, 10] and Vienna Fortran <ref> [1, 17] </ref>, the High Performance Fortran Forum has proposed the first version of High Performance Fortran (HPF) [4], which can be thought of as Fortran 90 enhanced with data distribution annotations. Long term storage of distributed array data is assigned to specific memory locations in the distributed machine.
References-found: 17

