URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P454.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts94.htm
Root-URL: http://www.mcs.anl.gov
Title: A Subgradient Algorithm for Nonlinear Integer Programming  
Author: Zhijun Wu John E. Dennis and Robert E. Bixby 
Keyword: Abbreviated title: Nonlinear Integer Programming Key words: Nonlinear integer programming, subgradient methods, nonlinear least squares, nonlinear constrained optimization, linear integer programming, branch-and-bound methods  
Address: Current Address:  9700 South Cass Avenue, Argonne, IL 60439.  
Affiliation: at the Department of Mathematical Sciences, Rice University.  Mathematics and Computer Science Division, Argonne National Laboratory,  
Note: AMS (MOS) subject classification: 65K05, 65K10, 90C10, 90C27, 90C30 This work is part of author's Ph.D. thesis directed by Professors  
Abstract: This paper describes a subgradient approach to nonlinear integer programming and, in particular, nonlinear 0-1 integer programming. In this approach, the objective function for a nonlinear integer program is considered as a nonsmooth function over the integer points. The subgradient and the supporting plane for the function are defined, and a necessary and sufficient condition for the optimal solution is established, based on the theory of nonsmooth analysis. A new algorithm, called the subgradient algorithm, is developed. The algorithm is in some sense an extension of Newton's method to discrete problems: The algorithm searches for a solution iteratively among the integer points. In each iteration, it generates the next point by solving the problem for a local piecewise linear model. Each local model is constructed using the supporting planes for the objective function at a set of previously generated integer points. A solution is found when either the optimality condition is satisfied or an iterate is repeated. In either case, the algorithm terminates in finite steps. The theory and the algorithm are presented. The methods for computing the supporting planes and solving the linear subproblems are described. Test results for a small set of problems are given. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Balas and J. B. </author> <month> Mazzola </month> <year> [1984a]. </year> <title> Nonlinear 0-1 Programming: I. Linearization Techniques. </title> <booktitle> Mathematical Programming 30, </booktitle> <pages> 1-21. </pages>
Reference-contexts: Therefore, we focus only on Problem (1) in this work. Several approaches to the solution of Problem (1) have been studied. The main ones are enumeration, algebraic, and linearization approaches <ref> [1, 2, 7, 16, 17, 20, 21] </ref>. For a general review, readers are referred to [8, 13, 19, 22, 26]. Most of these approaches consider problems with special structures. <p> Proof: For any x, let s 1 ; s 2 2 @f r (x). We show that s 1 + (1 )s 2 2 @f r (x) for any 2 <ref> [0; 1] </ref>: (15) Since s 1 ; s 2 2 @f r (x), s 1 (x x) f r (x) f r (x) 8x 2 B n ; and (16) So, for any x 2 B n and 2 [0; 1], = s 1 (x x) + (1 )s 2 (x <p> + (1 )s 2 2 @f r (x) for any 2 <ref> [0; 1] </ref>: (15) Since s 1 ; s 2 2 @f r (x), s 1 (x x) f r (x) f r (x) 8x 2 B n ; and (16) So, for any x 2 B n and 2 [0; 1], = s 1 (x x) + (1 )s 2 (x x) = f (x) f (x); (18) which, by the definition of a subgradient (8), implies s 1 + (1 )s 2 2 @f r (x) for any 2 [0; 1]: (19) Theorem 3 A necessary and sufficient condition <p> So, for any x 2 B n and 2 <ref> [0; 1] </ref>, = s 1 (x x) + (1 )s 2 (x x) = f (x) f (x); (18) which, by the definition of a subgradient (8), implies s 1 + (1 )s 2 2 @f r (x) for any 2 [0; 1]: (19) Theorem 3 A necessary and sufficient condition for x fl 2 B n to be a minimizer of f r (and also f ) over B n is 0 2 @f r (x fl ). <p> Theorem 9 For any s 2 R n and convex function f , the projection set S of s with respect to f at x is convex. Proof: Let x; x 0 2 S. We show that x + (1 )x 0 2 S 8 2 <ref> [0; 1] </ref>: (55) This follows immediately from the fact that f (x + (1 )x 0 ) f (x) + (1 )f (x 0 ) = g (x + (1 )x 0 ); (56) because that f is convex, x; x 0 2 S, and g is linear. 2 15 Theorem <p> The above problem is a linear integer programming problem with one continuous variable and can be solved with an enumeration method <ref> [1, 2] </ref>. Also, observe that p (i+1) is generated by adding one more supporting plane to p (i) , which implies that problems in the ith and (i + 1)th iterations are almost the same except that the problem in the (i + 1)th iteration has one 25 more constraint.
Reference: [2] <author> E. Balas and J. B. </author> <month> Mazzola </month> <year> [1984b]. </year> <title> Nonlinear 0-1 Programming: II. Dominance Relations and Algorithms. </title> <booktitle> Mathematical Programming 30, </booktitle> <pages> 22-45. </pages>
Reference-contexts: Therefore, we focus only on Problem (1) in this work. Several approaches to the solution of Problem (1) have been studied. The main ones are enumeration, algebraic, and linearization approaches <ref> [1, 2, 7, 16, 17, 20, 21] </ref>. For a general review, readers are referred to [8, 13, 19, 22, 26]. Most of these approaches consider problems with special structures. <p> The above problem is a linear integer programming problem with one continuous variable and can be solved with an enumeration method <ref> [1, 2] </ref>. Also, observe that p (i+1) is generated by adding one more supporting plane to p (i) , which implies that problems in the ith and (i + 1)th iterations are almost the same except that the problem in the (i + 1)th iteration has one 25 more constraint.
Reference: [3] <author> R. E. </author> <title> Bixby [1987]. Notes on Combinatorial Optimization. </title> <type> Technical Report TR87-21, </type> <institution> Dept. of Math. Sci., Rice Univ., Houston. </institution>
Reference-contexts: Otherwise, the sub-problem is divided, and two more subproblems are generated. The process continues until all subproblems are either eliminated or solved. Among all 0-1 solutions obtained for the subproblems, the one that yields the smallest objective value is the optimal solution to the original problem <ref> [3, 13, 26] </ref>.
Reference: [4] <author> R. E. </author> <title> Bixby [1990]. Implementing the Simplex Method: The Initial Basis. </title> <type> Technical Report TR90-32, </type> <institution> Dept. of Math. Sci., Rice Univ., Hous-ton. </institution>
Reference-contexts: k = ^ 8k = 1; : : : ; m: Then, a branching variable x k for this problem is selected if k solves the problem: min f max fb j + a jk gg: (113) The relaxed problems can be solved by using a standard dual simplex method <ref> [4, 5] </ref>. Let p (i) and p (i+1) be the relaxed problems for the ith and (i + 1)th linear integer minimax subproblems, respectively. As we have mentioned before, p (i+1) is the same as p (i) except that it has one more constraint.
Reference: [5] <author> V. </author> <month> Chvatal </month> <year> [1980]. </year> <title> Linear Programming. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York. </address>
Reference-contexts: k = ^ 8k = 1; : : : ; m: Then, a branching variable x k for this problem is selected if k solves the problem: min f max fb j + a jk gg: (113) The relaxed problems can be solved by using a standard dual simplex method <ref> [4, 5] </ref>. Let p (i) and p (i+1) be the relaxed problems for the ith and (i + 1)th linear integer minimax subproblems, respectively. As we have mentioned before, p (i+1) is the same as p (i) except that it has one more constraint.
Reference: [6] <author> F. H. </author> <title> Clarke [1983]. Optimization and Nonsmooth Analysis. </title> <publisher> John Wiley, </publisher> <address> New York. </address> <month> 35 </month>
Reference-contexts: The subgradient and the supporting plane for the function are defined, and a necessary and sufficient condition for the optimal solution is established, based on the theory of nonsmooth analysis <ref> [6, 28, 29] </ref>. A new algorithm, called the subgradient algorithm, is developed. The algorithm is in some sense an extension of Newton's method to discrete problems: The algorithm searches for a solution iteratively among the integer points.
Reference: [7] <author> Y. Crama, P. Hansen and B. </author> <title> Jaumard [1990]. The Basic Algorithm for Pseudo-Boolean Programming Revisited. </title> <note> Discrete Applied Mathematics. </note>
Reference-contexts: Therefore, we focus only on Problem (1) in this work. Several approaches to the solution of Problem (1) have been studied. The main ones are enumeration, algebraic, and linearization approaches <ref> [1, 2, 7, 16, 17, 20, 21] </ref>. For a general review, readers are referred to [8, 13, 19, 22, 26]. Most of these approaches consider problems with special structures.
Reference: [8] <author> G. B. </author> <title> Dantzig [1960]. On the Significance of Solving Linear Programming Problems with Some Integer Variables. The Rand Corporation, Document P1486. </title>
Reference-contexts: Therefore, we focus only on Problem (1) in this work. Several approaches to the solution of Problem (1) have been studied. The main ones are enumeration, algebraic, and linearization approaches [1, 2, 7, 16, 17, 20, 21]. For a general review, readers are referred to <ref> [8, 13, 19, 22, 26] </ref>. Most of these approaches consider problems with special structures. For problems with general objective functions, such as Problem (3) and Problem (4), they usually do not apply, owing to their special requirements for the form of the objective function.
Reference: [9] <author> J. E. Dennis, Jr., H. J. Martinez and R. A. </author> <title> Tapia [1989]. A Convergence Theory for the Structured BFGS Secant Method with an Application to Nonlinear Least Squares. </title> <journal> Journal of Optimization Theory and Applications 61, </journal> <pages> 159-176. </pages>
Reference-contexts: Therefore, to solve the system (77), we can use the quasi-Newton method with a structural BFGS secant update by <ref> [30, 9] </ref>. Also, in computing the Newton step, we can take advantage of this special property to solve each linear system efficiently.
Reference: [10] <author> J. E. Dennis, Jr., and R. B. </author> <title> Schnabel [1983]. Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J. </address>
Reference-contexts: While Algorithm 2 usually requires many updates, another approach is to solve the problem without considering the constraints: min k d (s) k : (62) The solution to this problem, for the case of l 2 norm, can be obtained by a standard nonlinear least squares method <ref> [10, 27] </ref>. Let s fl be the solution, and * =k d (s fl ) k be the optimal value.
Reference: [11] <author> P. van Emde-Boas [1981]. </author> <title> Another NP-Complete Partition Problem and the Complexity of Computing Short Vectors in a Lattice. </title> <type> Report 81-04, </type> <institution> Mathematical Institute, Univ. of Amsterdam, </institution> <address> Amsterdam. </address>
Reference-contexts: This problem, called the closest vector problem in integer programming, has been proven to be NP-complete even for simple norms such as l 2 and l 1 <ref> [11, 24, 25] </ref>. Another example is related to the solution of a class of more general problems: mixed-integer nonlinear programming problems.
Reference: [12] <author> R. </author> <title> Fletcher [1987]. Practical Methods of Optimization. </title> <publisher> John Wiley & Sons, </publisher> <address> New York. </address>
Reference-contexts: s i ) = 0 (71) x i+1 (x i ) s i+1 ) = 0 u i (f 0 u i (f (x i ) f (x) s T (x i x)) = 0 Proof: Necessity follows directly from the first-order necessary condition for a nonlinear constrained optimization problem <ref> [12, 15, 31] </ref>. 20 Note that the ith equation of (71) implies that u i &gt; 0, and r 2 x l i (x i ; u i ) = u i r 2 f (x i ) is positive definite. <p> So, the necessary condition is also sufficient by the second-order sufficiency condition for a nonlinear constrained opti mization problem <ref> [12, 15, 31] </ref>. 2 Lemma 3 Given ^s 2 R n and its corresponding projection set ^ S, let ^x i be an extreme point of ^ S along the x i direction, then there is a neighborhood N (^s; *) of ^s and a function x i : R n <p> We will again 22 only consider the formulation (66). The results can be extended to general cases. A constrained optimization problem can be expensive to solve <ref> [12, 15] </ref>. However, the problem (66) has a special structure. It is a problem with a linear objective function and a nonlinear convex constraint.
Reference: [13] <author> R. S. Garfinkel and G. L. </author> <title> Nemhauser [1972]. Integer Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York. </address>
Reference-contexts: Therefore, we focus only on Problem (1) in this work. Several approaches to the solution of Problem (1) have been studied. The main ones are enumeration, algebraic, and linearization approaches [1, 2, 7, 16, 17, 20, 21]. For a general review, readers are referred to <ref> [8, 13, 19, 22, 26] </ref>. Most of these approaches consider problems with special structures. For problems with general objective functions, such as Problem (3) and Problem (4), they usually do not apply, owing to their special requirements for the form of the objective function. <p> Otherwise, the sub-problem is divided, and two more subproblems are generated. The process continues until all subproblems are either eliminated or solved. Among all 0-1 solutions obtained for the subproblems, the one that yields the smallest objective value is the optimal solution to the original problem <ref> [3, 13, 26] </ref>.
Reference: [14] <author> M. R. Garey and D. S. </author> <title> Johnson [1979]. Computers and Intractability: </title>
Reference-contexts: However, the subgradient algorithm did not need to enumerate many of these points, but stopped immediately after an optimal solution was reached. We understand that the nonlinear integer programming problem is NP-hard, and therefore, it is impossible to find both an efficient and a general solution <ref> [14, 23] </ref>. Hence we do not expect the subgradient algorithm to be able to solve all the problems efficiently, either. Problem #4 shows that the subgradient algorithm cannot find an optimal solution in a small number of steps.
References-found: 14

