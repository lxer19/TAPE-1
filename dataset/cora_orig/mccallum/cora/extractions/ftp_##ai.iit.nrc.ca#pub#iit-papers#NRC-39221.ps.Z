URL: ftp://ai.iit.nrc.ca/pub/iit-papers/NRC-39221.ps.Z
Refering-URL: http://ai.iit.nrc.ca/cgi-bin/ftpsearch/?turney
Root-URL: 
Email: peter@ai.iit.nrc.ca  
Title: The Management of Context-Sensitive Features: A Review of Strategies  
Author: Peter Turney 
Address: Ottawa, Ontario, Canada, K1A 0R6  
Affiliation: Institute for Information Technology National Research Council Canada  
Abstract: In this paper, we review five heuristic strategies for handling context-sensitive features in supervised machine learning from examples. We discuss two methods for recovering lost (implicit) contextual information. We mention some evidence that hybrid strategies can have a synergetic effect. We then show how the work of several machine learning researchers fits into this framework. While we do not claim that these strategies exhaust the possibilities, it appears that the framework includes all of the techniques that can be found in the published literature on context-sensitive learning.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D.W. </author> <year> (1989). </year> <title> Incremental, instance-based learning of independent and graded concept descriptions. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <pages> pp. 387-391. </pages> <address> California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Aha, D.W., & Goldstone, R.L. </author> <year> (1992). </year> <title> Concept learning and exible weighting. </title> <booktitle> In Proceedings of the Fourteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pp. 534-539. </pages> <address> Illinois: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference: <author> Bergadano, F., Matwin, S., Michalski, R.S., and Zhang, J. </author> <year> (1992). </year> <title> Learning two-tiered descriptions of exible concepts: The POSEIDON system. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 5-43. </pages>
Reference: <author> Domingos, P. </author> <year> (1996). </year> <title> Context-sensitive feature selection for lazy learners. </title> <note> To appear in Artificial Intelligence Review. </note>
Reference: <author> Katz, A.J., Gately, M.T., and Collins, D.R. </author> <year> (1990). </year> <title> Robust classifiers without robust features, </title> <journal> Neural Computation, </journal> <volume> 2, </volume> <pages> 472-479. </pages>
Reference: <author> Kubat, M. </author> <year> (1989). </year> <title> Floating approximation in time-varying knowledge bases. </title> <journal> Pattern Recognition Letters, </journal> <volume> 10, </volume> <pages> 223-227. </pages>
Reference: <author> Kubat, M. </author> <year> (1996). </year> <title> Second tier for decision trees. </title> <booktitle> Machine Learning: Proceedings of the 13th International Conference, </booktitle> <address> California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Michalski, R.S. </author> <year> (1987). </year> <title> How to learn imprecise concepts: A method employing a two-tiered knowledge representation for learning. </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <pages> pp. 50-58, </pages> <address> California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Michalski, R.S. </author> <year> (1989). </year> <title> Two-tiered concept meaning, inferential matching and conceptual cohesiveness. </title> <booktitle> In S. </booktitle>
Reference: <editor> Vosniadu and A. Ortony (editors), </editor> <title> Similarity and Analogy. </title> <publisher> Cambridge University Press. </publisher>
Reference: <author> Michalski, R.S. </author> <year> (1990). </year> <title> Learning exible concepts: Fundamental ideas and methodology. </title> <editor> In Y. Kodratoff and R.S. Michalski (editors), </editor> <booktitle> Machine Learning: An Artificial Intel-ligence Approach, Vol. III, </booktitle> <address> California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Murphy, P.M., & Aha, D.W. </author> <year> (1996). </year> <title> UCI Repository of Machine Learning Databases. </title> <institution> University of California at Irvine, Department of Information and Computer Science. </institution> <note> [Available on the Internet at URL http://www.ics.uci.edu/ AI/ML/Machine-Learning.html.] </note> <author> Pratt, L.Y., Mostow, J, and Kamm, C.A. </author> <year> (1991). </year> <title> Direct transfer of learned information among neural networks. </title> <booktitle> Proceedings of the 9th National Conference on Artificial Intelligence (AAAI-91), </booktitle> <pages> pp. 584-580, </pages> <address> Anaheim, California. </address>
Reference-contexts: The paper is motivated by the belief that contextual features are pervasive. In support of this claim, Table 1 lists some of the examples of contextual features that have been examined in the machine learning literature. Many standard machine learning datasets <ref> (Murphy & Aha, 1996) </ref> contain contextual features, although this is rarely (explicitly) exploited. For example, in medical diagnosis problems, the patients gender, age, and weight are often available. These features are contextual, since they (typically) do not inuence the diag-nosis when they are considered in isolation.
Reference: <author> Turney, P.D. </author> <year> (1993a). </year> <title> Exploiting context when learning to classify. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, ECML-93, </booktitle> <pages> pp. 402-407. </pages> <address> Vienna, Austria: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: It is the task of the machine learning system to induce a model for pre-dicting the class of an example from its features. In many learning tasks, we may distinguish three different types of features: primary, contextual, and irrelevant features <ref> (Turney, 1993a, 1993b) </ref>. Primary features are useful for classification when considered in isolation, without regard for the other features. Contextual features are not useful in isolation, but can be useful when combined with other features. <p> When the two contexts are considered separately, diagnosis is relatively simple. If we mix the contexts together, correct diagnosis becomes more difficult. Katz et al. (1990) list four strategies for using contextual information when classifying. In earlier work <ref> (Turney, 1993a, 1993b) </ref>, we named these strategies contextual nor Table 1: Some examples from the machine learning literature. <p> The first step (classification using primary features alone) may be done by either a single classifier or multiple classifiers. For example, we might combine multiple specialized classifiers, each trained in a different con-text. See Figure 5. In our previous work <ref> (Turney, 1993a, 1993b) </ref>, we discussed a strategy that was not included in the list of four strategies given by Katz et al. (1990). We called this strategy contextual weighting. Strategy 5: Contextual weighting: The contextual features can be used to weight the primary features, prior to classification. <p> For example, we experimented with all eight possible combinations of three of the strategies (contextual normalization, contextual expansion, and contextual weighting) in two different domains, vowel recognition and hepatitis prognosis <ref> (Turney 1993a, 1993b) </ref>. In the vowel recognition task, the accuracy of a nearest-neighbour algorithm with no mechanism for handling context was 56%. With contextual normalization, contextual expansion, and contextual weighting, the accuracy of the nearest-neighbour algorithm was 66%. <p> With contextual normalization, contextual expansion, and contextual weighting, the accuracy of the nearest-neighbour algorithm was 66%. The sum of the improvement for the three strategies used separately was 3%, but the improvement for the three strategies together was 10% <ref> (Turney, 1993a, 1993b) </ref>. There is a statistically significant synergetic effect in this domain. In the hepatitis prognosis task, the accuracy of a nearest-neighbour algorithm with no mechanism for handling context was 71%. With contextual normalization, contextual expansion, and contextual weighting, the accuracy of the nearest-neighbour algorithm was 84%.
Reference: <author> Turney, P.D. </author> <year> (1993b). </year> <title> Robust classification with context-sensitive features. </title> <journal> In Industrial and Engineering Applications of Artificial Intelligence and Expert Systems, IEA/ AIE-93, </journal> <pages> pp. 268-276. </pages> <address> Edinburgh, Scotland: </address> <publisher> Gordon and Breach. </publisher>
Reference-contexts: With contextual normalization, contextual expansion, and contextual weighting, the accuracy of the nearest-neighbour algorithm was 84%. The sum of the improvement for the three strategies used separately was 12%, but the improvement for the three strategies together was 13% <ref> (Turney, 1993b) </ref>. The synergetic effect is not sta-tistically significant in this domain. One area for future research is to discover the circumstances under which there will be a synergy when strategies are combined.
Reference: <author> Turney, P.D., and Halasz, M. </author> <year> (1993). </year> <title> Contextual normalization applied to aircraft gas turbine engine diagnosis. </title> <journal> Journal of Applied Intelligence, </journal> <volume> 3, </volume> <pages> 109-129. </pages>
Reference: <author> Watrous, R.L. </author> <year> (1991). </year> <title> Context-modulated vowel discrimination using connectionist networks. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5, </volume> <pages> 341-362. </pages>
Reference: <author> Watrous, R.L. </author> <year> (1993). </year> <title> Speaker normalization and adaptation using second-order connectionist networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4, </volume> <pages> 21-30. </pages>
Reference: <author> Watrous, R.L. and Towell, G. </author> <year> (1995). </year> <title> A patient-adaptive neural network ECG patient monitoring algorithm. </title> <booktitle> In Pro-ceedings Computers in Cardiology 1995, </booktitle> <address> Vienna, Austria. </address>
Reference: <author> Widmer, G. and Kubat, M. </author> <year> (1992). </year> <title> Learning exible concepts from streams of examples: </title> <booktitle> FLORA2. In Proceedings of the 10th European Conference on Artificial Intelligence (ECAI-92), </booktitle> <address> Vienna. Chichester: </address> <publisher> Wiley and Sons. </publisher>
Reference: <author> Widmer, G. and Kubat, M. </author> <year> (1993). </year> <title> Effective learning in dynamic environments by explicit context tracking. </title> <booktitle> In Proceedings of the European Conference on Machine Learning (ECML-93), </booktitle> <pages> 227-243, </pages> <address> Vienna, Austria. Berlin: </address> <publisher> Springer Verlag. </publisher>
Reference: <author> Widmer, G. and Kubat, M. </author> <year> (1996). </year> <title> Learning in the presence of concept drift and hidden contexts. </title> <journal> Machine Learning, </journal> <volume> 23, </volume> <pages> pp. 69-101. </pages>
Reference: <author> Widmer, G. </author> <year> (1996). </year> <title> Recognition and exploitation of contextual clues via incremental meta-learning. </title> <booktitle> Machine Learning: Proceedings of the 13th International Conference, </booktitle> <address> California: </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 22

