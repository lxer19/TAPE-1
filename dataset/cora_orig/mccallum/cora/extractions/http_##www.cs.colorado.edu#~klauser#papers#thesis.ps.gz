URL: http://www.cs.colorado.edu/~klauser/papers/thesis.ps.gz
Refering-URL: http://www.cs.colorado.edu/~klauser/publications.html
Root-URL: http://www.cs.colorado.edu
Email: aklauser@iaik.tu-graz.ac.at  
Title: A Simulation Study for Distributed File Caching in High-Performance Parallel Architectures  
Author: Artur Klauser O. Univ.-Prof. Dipl.-Ing. Dr. techn. Reinhard Posch 
Degree: Master Thesis  Supervisor:  
Date: January 1994  
Note: in  
Address: Graz  
Affiliation: Institut fur Angewandte Informationsverarbeitung und Kommunikationstechnologie Technische Universitat  Graz,  
Abstract-found: 0
Intro-found: 1
Reference: [ABHN91] <author> Mustaque Ahamad, James E. Burns, Philip W. Hutto, and Gil Neiger. </author> <title> Causal Memory. </title> <type> Technical Report GIT-ICS-91/42, </type> <institution> School of Information and Computer Science, Georgia Institute of Technology, </institution> <year> 1991. </year>
Reference: [ACD + 92] <author> Mustaque Ahamad, Muthusamy Chelliah, Partha Dasgupta, Richard J. LeBlanc, and Mark Pearson. </author> <title> Shared Memory Programming in a Distributed System. </title> <type> Technical Report GIT-CC-63, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: Cache Coherency A large number of people have studied various forms of coherency and non-coherency. Among these is Lamport with two fundamental papers on event orderings in distributed systems [Lam78, Lam79], Ahamad et al. with papers on causal memory <ref> [AHJ91, ACD + 92, 59 CHAPTER 7. RELATED WORK 60 ABHN91] </ref>, Birman with virtual synchrony [Bir91a, Bir91b], Landin et al. with causal and total orderings [LLSG92], and Herlihy and Moss with transactional memory [HM92].
Reference: [AHJ91] <author> Mustaque Ahamad, Philip W. Hutto, and Ranjit John. </author> <title> Implementing and Programming Causal Distributed Shared Memory. </title> <booktitle> In 11th International Conference on Dist. </booktitle> <institution> Comput., </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Cache Coherency A large number of people have studied various forms of coherency and non-coherency. Among these is Lamport with two fundamental papers on event orderings in distributed systems [Lam78, Lam79], Ahamad et al. with papers on causal memory <ref> [AHJ91, ACD + 92, 59 CHAPTER 7. RELATED WORK 60 ABHN91] </ref>, Birman with virtual synchrony [Bir91a, Bir91b], Landin et al. with causal and total orderings [LLSG92], and Herlihy and Moss with transactional memory [HM92].
Reference: [Bas90] <author> Forest Basket. </author> <title> Cache-coherent Multiprocessors: An Easy Approach to High Performance Computing, </title> <booktitle> 1990. Distinguished Lecture Series, Volume III, </booktitle> <publisher> University Video Communications. </publisher>
Reference: [BC92] <author> Ingrid Y. Bucher and Donald A. Calahan. </author> <title> Models of Access Delays in Multiprocessor Memories. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(3) </volume> <pages> 270-280, </pages> <month> May </month> <year> 1992. </year>
Reference: [BD90] <author> Gregory D. Burns and Raja B. Daoud. </author> <title> The Performance / Functionality Dilemma of Multicomputer Message Passing. </title> <booktitle> In Proceedings of theFifth Distributed Memory Computing Conference, </booktitle> <year> 1990. </year>
Reference-contexts: These models include causal consistency ([ABHN91, AHJ91, ACD + 92], [HS92a, HS92b]), transactional consistency ([HM92]), virtual synchrony ([Bir91a, Bir91b]), and lazy replication techniques ([LLSG92]). Some of the models also leave a programmer the choice which consistency model is likely to perform best for the specific application ([Bas90], [Bir91a], <ref> [BD90] </ref>). Most of these consistency models were proposed in the context of distributed shared memory ([MR91], [ZSLW90], [MF90, MF89]). The aim of all these techniques is to reduce the coherency traffic incurred by sharing.
Reference: [BD92] <author> Eric A. Brewer and Chrysanthos N. Dellarocas. </author> <title> Proteus User Documentation, </title> <note> version 0.5, </note> <month> December </month> <year> 1992. </year>
Reference-contexts: providing a means to match the accuracy of the results with the patience of the experimenter. 3.1 Proteus | an Overview The simulator of choice for this study is a public domain program called Proteus 3.0 which was developed at the MIT Laboratory for Computer Science by Brewer et al. <ref> [BDCW91, BD92] </ref>. Proteus is an execution-driven parallel architecture simulator. It is capable of simulating any MIMD architectures of both, shared and distributed memory types, using bus or point-to-point interconnection structures. This simulation is performed on ordinary single-processor machines such as DEC or Sun workstations. <p> Proteus Simulator A general overview of Proteus is provided in Brewer et al. [BDCW91]. A more detailed description of the simulator can be found in Brewer and Dellarocas <ref> [BD92] </ref>. Sprite File System Trace Data An introduction to the measurements in the Sprite file system can be found in Baker et al. [BHK + 91]. A more detailed description of the caching approaches taken in the Sprite file system is provided in Nelson, Welch, and Ousterhout [NWO88].
Reference: [BDCW91] <author> Eric A. Brewer, Chrysanthos N. Dellarocas, Adrian Colbrool, and William E. Weihl. Proteus: </author> <title> A High-Performance Parallel-Architecture Simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> Massachusetts Institute of Technology, Laboratory for Computer Science, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: providing a means to match the accuracy of the results with the patience of the experimenter. 3.1 Proteus | an Overview The simulator of choice for this study is a public domain program called Proteus 3.0 which was developed at the MIT Laboratory for Computer Science by Brewer et al. <ref> [BDCW91, BD92] </ref>. Proteus is an execution-driven parallel architecture simulator. It is capable of simulating any MIMD architectures of both, shared and distributed memory types, using bus or point-to-point interconnection structures. This simulation is performed on ordinary single-processor machines such as DEC or Sun workstations. <p> Proteus Simulator A general overview of Proteus is provided in Brewer et al. <ref> [BDCW91] </ref>. A more detailed description of the simulator can be found in Brewer and Dellarocas [BD92]. Sprite File System Trace Data An introduction to the measurements in the Sprite file system can be found in Baker et al. [BHK + 91].
Reference: [BDG + 93] <author> A. Beguelin, J. Dongarra, G. Geist, W. Jiang, R. Manchek, K. Moore, and V. Sunderam. </author> <title> The PVM Project, 1993. </title> <institution> Oak Ridge National Laboratory, Oak Ridge, Tennessee. </institution>
Reference-contexts: The communications libraries utilized in parallel supercomputers as well as workstation clusters provide basically the same functionality. Public domain libraries like PVM <ref> [GBD + 93, BDG + 93] </ref>, P4, and Parmacs are available for almost any machine on both kinds of architectures. Furthermore, most companies have announced to support the coming message passing standard of MPI [DHHW93, Mes93]. To provide an adequate structuring for distributed systems, the client-server approach is often used.
Reference: [BHK + 91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout. </author> <title> Measurements of a Distributed File System. </title> <type> Technical 72 BIBLIOGRAPHY 73 report, </type> <institution> University of California at Berkeley, Computer Science Division, </institution> <month> July </month> <year> 1991. </year> <booktitle> Also appeared in Proceedings of the 13th Symposium on Operating Systems Principles, </booktitle> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: However, to get realistic data it is necessary to restrict the generated traces by some constraints that have been reported by measurements in existing file systems, such as conducted by Baker et al. <ref> [BHK + 91] </ref> for the Sprite file system or Maffeis [Maf92b] and Danzig, Hall, and Schwartz [DHS93] for global FTP traffic. <p> A more detailed description of the simulator can be found in Brewer and Dellarocas [BD92]. Sprite File System Trace Data An introduction to the measurements in the Sprite file system can be found in Baker et al. <ref> [BHK + 91] </ref>. A more detailed description of the caching approaches taken in the Sprite file system is provided in Nelson, Welch, and Ousterhout [NWO88]. Hartman provides a very detailed description of the trace file contents [Har93].
Reference: [Bir91a] <author> Kenneth P. Birman. </author> <title> Maintaining Consistency in Distributed Systems. </title> <type> Technical Report TR91-1240, </type> <institution> Cornell University, Department of Computer Science, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: Additionally, there is some overhead to query and change various policy related information on both, the server and the clients. Although this software model provides a perfect form of consistency, which is called sequential consistency ([Lam78, Lam79], [HS92b, HS92a], <ref> [Bir91a] </ref>), it also produces a substantial network load for shared files. The higher the sharing ratio, the worse it is for this model. There have been some other consistency models proposed, some of which also have been implemented for evaluation purposes. <p> These models include causal consistency ([ABHN91, AHJ91, ACD + 92], [HS92a, HS92b]), transactional consistency ([HM92]), virtual synchrony ([Bir91a, Bir91b]), and lazy replication techniques ([LLSG92]). Some of the models also leave a programmer the choice which consistency model is likely to perform best for the specific application ([Bas90], <ref> [Bir91a] </ref>, [BD90]). Most of these consistency models were proposed in the context of distributed shared memory ([MR91], [ZSLW90], [MF90, MF89]). The aim of all these techniques is to reduce the coherency traffic incurred by sharing. <p> Among these is Lamport with two fundamental papers on event orderings in distributed systems [Lam78, Lam79], Ahamad et al. with papers on causal memory [AHJ91, ACD + 92, 59 CHAPTER 7. RELATED WORK 60 ABHN91], Birman with virtual synchrony <ref> [Bir91a, Bir91b] </ref>, Landin et al. with causal and total orderings [LLSG92], and Herlihy and Moss with transactional memory [HM92]. Heddaya and Sinha provide an attempt to give a formalism to cover all these different kinds of coherency [HS92a, HS92b].
Reference: [Bir91b] <author> Kenneth P. Birman. </author> <title> The Process Group Approach to Reliable Distributed Computing. </title> <type> Technical Report TR91-1216, </type> <institution> Cornell University, Department of Computer Science, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Among these is Lamport with two fundamental papers on event orderings in distributed systems [Lam78, Lam79], Ahamad et al. with papers on causal memory [AHJ91, ACD + 92, 59 CHAPTER 7. RELATED WORK 60 ABHN91], Birman with virtual synchrony <ref> [Bir91a, Bir91b] </ref>, Landin et al. with causal and total orderings [LLSG92], and Herlihy and Moss with transactional memory [HM92]. Heddaya and Sinha provide an attempt to give a formalism to cover all these different kinds of coherency [HS92a, HS92b].
Reference: [BWF91] <author> Didier Badouel, Charles A. Wuthrich, and Eugene L. Fiume. </author> <title> Routing Strategies and Message Contention on Low-Dimensional Interconnection Networks. </title> <type> Technical report, </type> <institution> Computer Systems Research Institute, University of Toron-to, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: A mesh is a special case of a k-ary n-cube network with the dimension n = 2. The k-ary n-cube networks are discussed in some detail by Badouel, Wuthrich, and Fiume <ref> [BWF91, BWF92] </ref> and Dally [Dal90]. Mesh interconnection structures are actually used in existing systems such as the Paragon. The network used for this simulation study is modeled after this computer. <p> Interconnection Network Basic performance analyses of the k-ary n-cubes, the kind of network used for this study, can be found in Dally [Dal90], and Badouel, Wuthrich and Fiume [BWF92] who also analyze routing strategies and message contention in <ref> [BWF91] </ref>. Proteus Simulator A general overview of Proteus is provided in Brewer et al. [BDCW91]. A more detailed description of the simulator can be found in Brewer and Dellarocas [BD92].
Reference: [BWF92] <author> Didier Badouel, Charles A. Wuthrich, and Eugene L. Fiume. </author> <title> An Analysis of Connectivity of k-ary n-cube m-diag Interconnection Networks. </title> <type> Technical report, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: A mesh is a special case of a k-ary n-cube network with the dimension n = 2. The k-ary n-cube networks are discussed in some detail by Badouel, Wuthrich, and Fiume <ref> [BWF91, BWF92] </ref> and Dally [Dal90]. Mesh interconnection structures are actually used in existing systems such as the Paragon. The network used for this simulation study is modeled after this computer. <p> A special implementation is presented in Minnich and Farber [MF89], who also analyze the network behavior of the system in [MF90]. Interconnection Network Basic performance analyses of the k-ary n-cubes, the kind of network used for this study, can be found in Dally [Dal90], and Badouel, Wuthrich and Fiume <ref> [BWF92] </ref> who also analyze routing strategies and message contention in [BWF91]. Proteus Simulator A general overview of Proteus is provided in Brewer et al. [BDCW91]. A more detailed description of the simulator can be found in Brewer and Dellarocas [BD92].
Reference: [CD88] <author> George F. Coulouris and Jean Dollimore. </author> <title> Distributed Systems: Concepts and Design. </title> <address> Addison-Wessley, </address> <year> 1988. </year> <note> ISBN 0-201-18059-6. </note>
Reference-contexts: A work especially aimed towards distributed systems, with lots of information on distributed file systems is Coulouris and Dollimore <ref> [CD88] </ref>. Cache Replacement Strategies Lots of work has been conducted on cache replacement strategies.
Reference: [Dal90] <author> William J. Dally. </author> <title> Performance Analysis of k-ary n-cube Interconnection Networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(6) </volume> <pages> 775-785, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: A mesh is a special case of a k-ary n-cube network with the dimension n = 2. The k-ary n-cube networks are discussed in some detail by Badouel, Wuthrich, and Fiume [BWF91, BWF92] and Dally <ref> [Dal90] </ref>. Mesh interconnection structures are actually used in existing systems such as the Paragon. The network used for this simulation study is modeled after this computer. <p> A special implementation is presented in Minnich and Farber [MF89], who also analyze the network behavior of the system in [MF90]. Interconnection Network Basic performance analyses of the k-ary n-cubes, the kind of network used for this study, can be found in Dally <ref> [Dal90] </ref>, and Badouel, Wuthrich and Fiume [BWF92] who also analyze routing strategies and message contention in [BWF91]. Proteus Simulator A general overview of Proteus is provided in Brewer et al. [BDCW91]. A more detailed description of the simulator can be found in Brewer and Dellarocas [BD92].
Reference: [DHHW93] <author> Jack Dongarra, Rolf Hempel, Anthony J. G. Hey, and David W. Walker. </author> <title> A Proposal for a User-Level, Message Passing Interface in a Distributed Memory Environment. </title> <type> Technical Report ORNL/TM-12231, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, Tennessee, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Public domain libraries like PVM [GBD + 93, BDG + 93], P4, and Parmacs are available for almost any machine on both kinds of architectures. Furthermore, most companies have announced to support the coming message passing standard of MPI <ref> [DHHW93, Mes93] </ref>. To provide an adequate structuring for distributed systems, the client-server approach is often used. In this approach some of the processing nodes implement servers. Servers are software modules which provide specialized services to all interested parties, called clients.
Reference: [DHS93] <author> Peter B. Danzig, Richard S. Hall, and Michael F. Schwartz. </author> <title> A Case for Caching File Objects Inside Internetworks. </title> <type> Technical Report CU-CS-642-93, </type> <institution> University of Colorado at Boulder, </institution> <month> March </month> <year> 1993. </year>
Reference-contexts: However, to get realistic data it is necessary to restrict the generated traces by some constraints that have been reported by measurements in existing file systems, such as conducted by Baker et al. [BHK + 91] for the Sprite file system or Maffeis [Maf92b] and Danzig, Hall, and Schwartz <ref> [DHS93] </ref> for global FTP traffic. Otherwise, if the data is generated completely from scratch without having a real trace study controlling its parameters, there are high chances of producing access patterns that will not be seen in the real system. <p> Cache Replacement Strategies Lots of work has been conducted on cache replacement strategies. Starting with work analyzing various kinds of traffic characteristics, such as internetwork traffic by Danzig, Hall, and Schwartz <ref> [DHS93] </ref>, FTP traffic by Maffeis [Maf92b, Maf92a] and Maffeis and Cap [MC92], and processor-level instruction streams by Voldman et al. [VMH + 83].
Reference: [ES93] <author> Maria R. Ebling and M. Satyanarayanan. SynRGen: </author> <title> A Extensible Synthetic File Reference Generator (Draft Version). </title> <type> Technical report, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: If some of these restrictions are not enforced, the generated data might be unrealistic and thus render simulations with this data useless. A different approach to get a realistic load is reported by Ebling and Satyanarayanan <ref> [ES93] </ref>. In this case a virtual file system is set up. The number, location, and length of the files in this file system can be strictly controlled.
Reference: [Far91] <author> Keith I. Farkas. </author> <title> A Decentralized Hierarchical Cache-Consistency Scheme for Shared-Memory Multiprocessors. </title> <type> Master's thesis, </type> <institution> University of Toronto, Department of Electrical Engineering, </institution> <month> April </month> <year> 1991. </year> <month> TR-EECG-91-04-01. </month>
Reference: [FVS92] <author> Keith Farkas, Zvonko Vranesic, and Michael Stumm. </author> <title> Cache Consistency in Hierarchical-Ring-Based Multiprocessors. </title> <type> Technical Report EECG TR-92-09-01, </type> <institution> Department of Electrical Engineering, University of Toronto, </institution> <month> September </month> <year> 1992. </year> <note> BIBLIOGRAPHY 74 </note>
Reference-contexts: Yang, Thangadurai and Bhuyan present an adaptive cache coherency protocol for a special-case architecture [YTB92], and Farkas, Vranesic, and Stumm also develop a cache consistency scheme for yet another special architecture <ref> [FVS92] </ref>. Distributed Shared Memory (DSM) To provide the abstraction of shared memory on distributed memory machines a scheme called distributed shared memory is employed. This scheme has many similarities to the distribution of client caches over many clients as presented earlier in this work.
Reference: [GBD + 93] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Robert Manchek, and Vaidy Sunderam. </author> <title> PVM 3 User's Guide and Reference Manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, Tennessee, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The communications libraries utilized in parallel supercomputers as well as workstation clusters provide basically the same functionality. Public domain libraries like PVM <ref> [GBD + 93, BDG + 93] </ref>, P4, and Parmacs are available for almost any machine on both kinds of architectures. Furthermore, most companies have announced to support the coming message passing standard of MPI [DHHW93, Mes93]. To provide an adequate structuring for distributed systems, the client-server approach is often used.
Reference: [Gha75] <author> M. Z. Ghanem. </author> <title> Dynamic Partitioning of the Main Memory Using the Working Set Concept. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 19(9) </volume> <pages> 445-450, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: There are many replacement strategies known. Some are rather simple ([SP88], [Tan92]) like FIFO, LFU (least frequently used), LRU (least recently used), and LR (longest resident), while others are quite sophisticated as in Gahanem <ref> [Gha75] </ref> and Maffeis [Maf92a]. However, LRU is the replacement policy mostly used and is also the policy of choice for this study. <p> Other work is aimed towards optimal partitioning the cache memory between various parties, like various file systems in Thiebaut, Stone and Wolf [TSW92], and partitioning between interleaved data and instruction streams in Stone, Turek, and Wolf [STW92]. A work from Ghanem <ref> [Gha75] </ref> investigates the partitioning of main memory between a number of processes. Cache Coherency A large number of people have studied various forms of coherency and non-coherency.
Reference: [Har93] <author> John H. Hartman. </author> <title> Private communication, </title> <year> 1993. </year>
Reference-contexts: A more detailed description of the caching approaches taken in the Sprite file system is provided in Nelson, Welch, and Ousterhout [NWO88]. Hartman provides a very detailed description of the trace file contents <ref> [Har93] </ref>. Chapter 8 Conclusions and Future Work In the presented work a simulation study was conducted, investigating the field of distributed file caching in parallel architectures. The simulations were performed for a general distributed memory MIMD architecture with a 2D-mesh interconnection topology.
Reference: [Hay88] <author> John P. Hayes. </author> <title> Computer Architecture and Organization. </title> <publisher> McGraw-Hill, </publisher> <year> 1988. </year> <note> Second Edition. ISBN 0-07-100479-3. </note>
Reference-contexts: Basic knowledge about computer architecture of parallel and distributed systems with some case studies of real systems can be found in Hayes <ref> [Hay88] </ref>. Hennessy and Patterson [HP90] also provide some basic material on computer architecture in general, and on cache coherency in multiprocessor systems in particular. A similar approach is taken by Stone [Sto93], which also contains a section on trace length considerations and efficient cache simulations.
Reference: [HM92] <author> Maurice Herlihy and J. Eliot B. Moss. </author> <title> Transactional Memory: Architectural Support for Lock-Free Data Structures. </title> <type> Technical Report CRL 97/07, </type> <institution> Digital Equipment Corporation, Cambridge Research Lab, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: RELATED WORK 60 ABHN91], Birman with virtual synchrony [Bir91a, Bir91b], Landin et al. with causal and total orderings [LLSG92], and Herlihy and Moss with transactional memory <ref> [HM92] </ref>. Heddaya and Sinha provide an attempt to give a formalism to cover all these different kinds of coherency [HS92a, HS92b].
Reference: [HP90] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year> <note> ISBN 1-55860-188-0. </note>
Reference-contexts: Basic knowledge about computer architecture of parallel and distributed systems with some case studies of real systems can be found in Hayes [Hay88]. Hennessy and Patterson <ref> [HP90] </ref> also provide some basic material on computer architecture in general, and on cache coherency in multiprocessor systems in particular. A similar approach is taken by Stone [Sto93], which also contains a section on trace length considerations and efficient cache simulations.
Reference: [HS92a] <author> Abdelsalam Heddaya and Himanshu Sinha. </author> <title> An Overview of Mermera: A System and Formalism for Non-coherent Distributed Shared Memory. </title> <type> Technical Report BU-CS-92-009, </type> <institution> Computer Sciences Department, Boston University, </institution> <year> 1992. </year> <note> to appear in Proceedings of the HICSS-26, Maui, Hawaii, </note> <month> Jan. </month> <pages> 5-8, </pages> <year> 1993. </year>
Reference-contexts: Additionally, there is some overhead to query and change various policy related information on both, the server and the clients. Although this software model provides a perfect form of consistency, which is called sequential consistency ([Lam78, Lam79], <ref> [HS92b, HS92a] </ref>, [Bir91a]), it also produces a substantial network load for shared files. The higher the sharing ratio, the worse it is for this model. There have been some other consistency models proposed, some of which also have been implemented for evaluation purposes. <p> The higher the sharing ratio, the worse it is for this model. There have been some other consistency models proposed, some of which also have been implemented for evaluation purposes. These models include causal consistency ([ABHN91, AHJ91, ACD + 92], <ref> [HS92a, HS92b] </ref>), transactional consistency ([HM92]), virtual synchrony ([Bir91a, Bir91b]), and lazy replication techniques ([LLSG92]). Some of the models also leave a programmer the choice which consistency model is likely to perform best for the specific application ([Bas90], [Bir91a], [BD90]). <p> RELATED WORK 60 ABHN91], Birman with virtual synchrony [Bir91a, Bir91b], Landin et al. with causal and total orderings [LLSG92], and Herlihy and Moss with transactional memory [HM92]. Heddaya and Sinha provide an attempt to give a formalism to cover all these different kinds of coherency <ref> [HS92a, HS92b] </ref>. Yang, Thangadurai and Bhuyan present an adaptive cache coherency protocol for a special-case architecture [YTB92], and Farkas, Vranesic, and Stumm also develop a cache consistency scheme for yet another special architecture [FVS92].
Reference: [HS92b] <author> Abdelsalam Heddaya and Himanshu Sinha. </author> <title> Coherence, Non-coherence and Local Consistency in Distributed Shared Memory for Parallel Computing. </title> <type> Technical Report BU-CS-92-004, </type> <institution> Computer Sciences Department, Boston University, </institution> <year> 1992. </year>
Reference-contexts: Additionally, there is some overhead to query and change various policy related information on both, the server and the clients. Although this software model provides a perfect form of consistency, which is called sequential consistency ([Lam78, Lam79], <ref> [HS92b, HS92a] </ref>, [Bir91a]), it also produces a substantial network load for shared files. The higher the sharing ratio, the worse it is for this model. There have been some other consistency models proposed, some of which also have been implemented for evaluation purposes. <p> The higher the sharing ratio, the worse it is for this model. There have been some other consistency models proposed, some of which also have been implemented for evaluation purposes. These models include causal consistency ([ABHN91, AHJ91, ACD + 92], <ref> [HS92a, HS92b] </ref>), transactional consistency ([HM92]), virtual synchrony ([Bir91a, Bir91b]), and lazy replication techniques ([LLSG92]). Some of the models also leave a programmer the choice which consistency model is likely to perform best for the specific application ([Bas90], [Bir91a], [BD90]). <p> RELATED WORK 60 ABHN91], Birman with virtual synchrony [Bir91a, Bir91b], Landin et al. with causal and total orderings [LLSG92], and Herlihy and Moss with transactional memory [HM92]. Heddaya and Sinha provide an attempt to give a formalism to cover all these different kinds of coherency <ref> [HS92a, HS92b] </ref>. Yang, Thangadurai and Bhuyan present an adaptive cache coherency protocol for a special-case architecture [YTB92], and Farkas, Vranesic, and Stumm also develop a cache consistency scheme for yet another special architecture [FVS92].
Reference: [KR88] <author> Brian W. Kernighan and Dennis M. Ritchie. </author> <title> The C Programming Language. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year> <note> Second Edition. ISBN 0-13-110362-8. </note>
Reference: [Lam78] <author> Leslie Lamport. </author> <title> Time, Clocks, and the Ordering of Events in a Distributed System. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: A work from Ghanem [Gha75] investigates the partitioning of main memory between a number of processes. Cache Coherency A large number of people have studied various forms of coherency and non-coherency. Among these is Lamport with two fundamental papers on event orderings in distributed systems <ref> [Lam78, Lam79] </ref>, Ahamad et al. with papers on causal memory [AHJ91, ACD + 92, 59 CHAPTER 7. RELATED WORK 60 ABHN91], Birman with virtual synchrony [Bir91a, Bir91b], Landin et al. with causal and total orderings [LLSG92], and Herlihy and Moss with transactional memory [HM92].
Reference: [Lam79] <author> Leslie Lamport. </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: A work from Ghanem [Gha75] investigates the partitioning of main memory between a number of processes. Cache Coherency A large number of people have studied various forms of coherency and non-coherency. Among these is Lamport with two fundamental papers on event orderings in distributed systems <ref> [Lam78, Lam79] </ref>, Ahamad et al. with papers on causal memory [AHJ91, ACD + 92, 59 CHAPTER 7. RELATED WORK 60 ABHN91], Birman with virtual synchrony [Bir91a, Bir91b], Landin et al. with causal and total orderings [LLSG92], and Herlihy and Moss with transactional memory [HM92].
Reference: [LLSG92] <author> Rivka Landin, Barbara Liskov, Liuba Shrira, and Sanjay Ghemawat. </author> <title> Providing High Availability Using Lazy Replication. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(4) </volume> <pages> 360-391, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Among these is Lamport with two fundamental papers on event orderings in distributed systems [Lam78, Lam79], Ahamad et al. with papers on causal memory [AHJ91, ACD + 92, 59 CHAPTER 7. RELATED WORK 60 ABHN91], Birman with virtual synchrony [Bir91a, Bir91b], Landin et al. with causal and total orderings <ref> [LLSG92] </ref>, and Herlihy and Moss with transactional memory [HM92]. Heddaya and Sinha provide an attempt to give a formalism to cover all these different kinds of coherency [HS92a, HS92b].
Reference: [Maf92a] <author> Silvano Maffeis. </author> <title> Cache Management Algorithms for Flexible Filesystems. </title> <type> Technical report, </type> <institution> Institut fur Informatik der Universitat Zurich (IFI), </institution> <note> De-cember 1992. BIBLIOGRAPHY 75 </note>
Reference-contexts: There are many replacement strategies known. Some are rather simple ([SP88], [Tan92]) like FIFO, LFU (least frequently used), LRU (least recently used), and LR (longest resident), while others are quite sophisticated as in Gahanem [Gha75] and Maffeis <ref> [Maf92a] </ref>. However, LRU is the replacement policy mostly used and is also the policy of choice for this study. <p> Cache Replacement Strategies Lots of work has been conducted on cache replacement strategies. Starting with work analyzing various kinds of traffic characteristics, such as internetwork traffic by Danzig, Hall, and Schwartz [DHS93], FTP traffic by Maffeis <ref> [Maf92b, Maf92a] </ref> and Maffeis and Cap [MC92], and processor-level instruction streams by Voldman et al. [VMH + 83].
Reference: [Maf92b] <author> Silvano Maffeis. </author> <title> File Access Patterns in Public FTP Archives and an Index for Locality of Reference. </title> <type> Technical Report IFI TR 92.13, </type> <institution> Institut fur Informatik der Universitat Zurich (IFI), </institution> <month> August </month> <year> 1992. </year>
Reference-contexts: However, to get realistic data it is necessary to restrict the generated traces by some constraints that have been reported by measurements in existing file systems, such as conducted by Baker et al. [BHK + 91] for the Sprite file system or Maffeis <ref> [Maf92b] </ref> and Danzig, Hall, and Schwartz [DHS93] for global FTP traffic. Otherwise, if the data is generated completely from scratch without having a real trace study controlling its parameters, there are high chances of producing access patterns that will not be seen in the real system. <p> Cache Replacement Strategies Lots of work has been conducted on cache replacement strategies. Starting with work analyzing various kinds of traffic characteristics, such as internetwork traffic by Danzig, Hall, and Schwartz [DHS93], FTP traffic by Maffeis <ref> [Maf92b, Maf92a] </ref> and Maffeis and Cap [MC92], and processor-level instruction streams by Voldman et al. [VMH + 83].
Reference: [MC92] <author> Silvano Maffeis and Clemens H. </author> <title> Cap. Replication Heuristics and Polling Algorithms for Object Replication and a Replicating File Transfer Protocol. </title> <type> Technical Report IFI TR 92.06, </type> <institution> Institut fur Informatik der Universitat Zurich (IFI), </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: Cache Replacement Strategies Lots of work has been conducted on cache replacement strategies. Starting with work analyzing various kinds of traffic characteristics, such as internetwork traffic by Danzig, Hall, and Schwartz [DHS93], FTP traffic by Maffeis [Maf92b, Maf92a] and Maffeis and Cap <ref> [MC92] </ref>, and processor-level instruction streams by Voldman et al. [VMH + 83]. Other work is aimed towards optimal partitioning the cache memory between various parties, like various file systems in Thiebaut, Stone and Wolf [TSW92], and partitioning between interleaved data and instruction streams in Stone, Turek, and Wolf [STW92].
Reference: [Mes93] <author> Message Passing Interface Forum. </author> <title> Draft Document for a Standard Message-Passing Interface. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, Ten-nessee, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Public domain libraries like PVM [GBD + 93, BDG + 93], P4, and Parmacs are available for almost any machine on both kinds of architectures. Furthermore, most companies have announced to support the coming message passing standard of MPI <ref> [DHHW93, Mes93] </ref>. To provide an adequate structuring for distributed systems, the client-server approach is often used. In this approach some of the processing nodes implement servers. Servers are software modules which provide specialized services to all interested parties, called clients.
Reference: [MF89] <author> Ronald G. Minnich and David J. Farber. </author> <title> The Mether System: Distributed Shared Memory for SunOS 4.0. </title> <booktitle> Usenix Summer 89, </booktitle> <year> 1989. </year>
Reference-contexts: Some of the models also leave a programmer the choice which consistency model is likely to perform best for the specific application ([Bas90], [Bir91a], [BD90]). Most of these consistency models were proposed in the context of distributed shared memory ([MR91], [ZSLW90], <ref> [MF90, MF89] </ref>). The aim of all these techniques is to reduce the coherency traffic incurred by sharing. This is done by loosening some of the restrictions that are normally expected to hold in centralized and distributed systems that provide sequential consistency. <p> This scheme has many similarities to the distribution of client caches over many clients as presented earlier in this work. Some basic work can be found in Mohindra and Ramachandran [MR91], as well as in Zhou et al. [ZSLW90]. A special implementation is presented in Minnich and Farber <ref> [MF89] </ref>, who also analyze the network behavior of the system in [MF90].
Reference: [MF90] <author> Ronald G. Minnich and David J. Farber. </author> <title> Reducing Host Load, Network Load and Latency in a Distributed Shared Memory. </title> <booktitle> In 10th International Conference on Distributed Computing Systems, </booktitle> <address> Paris, France, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Some of the models also leave a programmer the choice which consistency model is likely to perform best for the specific application ([Bas90], [Bir91a], [BD90]). Most of these consistency models were proposed in the context of distributed shared memory ([MR91], [ZSLW90], <ref> [MF90, MF89] </ref>). The aim of all these techniques is to reduce the coherency traffic incurred by sharing. This is done by loosening some of the restrictions that are normally expected to hold in centralized and distributed systems that provide sequential consistency. <p> Some basic work can be found in Mohindra and Ramachandran [MR91], as well as in Zhou et al. [ZSLW90]. A special implementation is presented in Minnich and Farber [MF89], who also analyze the network behavior of the system in <ref> [MF90] </ref>. Interconnection Network Basic performance analyses of the k-ary n-cubes, the kind of network used for this study, can be found in Dally [Dal90], and Badouel, Wuthrich and Fiume [BWF92] who also analyze routing strategies and message contention in [BWF91].
Reference: [MR91] <author> Ajay Mohindra and Umkishore Ramachandran. </author> <title> A Survey of Distributed Shared Memory in Loosely-coupled Systems. </title> <type> Technical Report GIT-CC-91/01, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: This scheme has many similarities to the distribution of client caches over many clients as presented earlier in this work. Some basic work can be found in Mohindra and Ramachandran <ref> [MR91] </ref>, as well as in Zhou et al. [ZSLW90]. A special implementation is presented in Minnich and Farber [MF89], who also analyze the network behavior of the system in [MF90].
Reference: [NWO88] <author> M. N. Nelson, B. B. Welch, and J. K. Ousterhout. </author> <title> Caching in the Sprite Network File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 134-154, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: TRACE DATA 22 at the same University analyzing the UNIX 4.2 BSD file system. This early work formed the basis for the design of the Sprite file system caching strategy by means of simulation studies performed by Nelson, Welch, and Ousterhout <ref> [NWO88] </ref>. Data recorded during the Sprite file system study was made publically available. This data is used as input data to the simulation of distributed file caching. The complete data set contains 8 traces of 24 hours recording time each. <p> Sprite File System Trace Data An introduction to the measurements in the Sprite file system can be found in Baker et al. [BHK + 91]. A more detailed description of the caching approaches taken in the Sprite file system is provided in Nelson, Welch, and Ousterhout <ref> [NWO88] </ref>. Hartman provides a very detailed description of the trace file contents [Har93]. Chapter 8 Conclusions and Future Work In the presented work a simulation study was conducted, investigating the field of distributed file caching in parallel architectures.
Reference: [SGZ92] <author> Harjinder S. Sandhu, Benjamin Gamsa, and Songnian Zhou. </author> <title> Region-Oriented Memory Management in Shared-Memory Multiprocessors. </title> <type> Technical Report CSRI-269, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Rings and hierarchies of rings have also been used in parallel computers, both, in research ([FVS92, Far91], <ref> [SGZ92] </ref>) and industry (the KSR1 machine). Star Star networks, as shown in Fig. 2.3 (d), have been very popular for large centralized computer systems, with the central computer in the middle of the network controlling all activities.
Reference: [SP88] <author> Abraham Silberschatz and James L. Peterson. </author> <title> Operating System Concepts. </title> <address> Addison-Wessley, </address> <year> 1988. </year> <note> Alternate Edition. ISBN 0-201-18760-4. </note>
Reference-contexts: A similar approach is taken by Stone [Sto93], which also contains a section on trace length considerations and efficient cache simulations. Tanenbaum [Tan88] is a standard textbook containing lots of computer network information. On the software side there is an introductory work into operating systems by Sil-berschatz and Peterson <ref> [SP88] </ref>, and some further information, especially concerning distributed systems, can be found in Tanenbaum [Tan92]. A work especially aimed towards distributed systems, with lots of information on distributed file systems is Coulouris and Dollimore [CD88]. Cache Replacement Strategies Lots of work has been conducted on cache replacement strategies.
Reference: [Sto93] <author> Harold S. Stone. </author> <booktitle> High-Performance Computer Architecture. Addison-Wessley, 1993. Third Edition. </booktitle> <address> ISBN 0-201-52688-3. </address>
Reference-contexts: There is evidence that forthcoming standards such as ATM will push clusters in the same performance class as parallel architectures. 2.2.1 Topologies A large variety of topologies for interconnection networks have been studied in the past ([Hay88], <ref> [Sto93] </ref>). Besides the topology, there is also the differentiation between direct and indirect networks. Whereas the former directly connect two nodes, the latter achieve their connections via a switching network. <p> TRACE DATA 27 Parameter Estimates Mathematical statistics provides the tools to answer the above question. Similar considerations for trace lengths in direct mapped and k-way set-associative cache simulations can be found in Stone <ref> [Sto93] </ref>. Assume that the statistical process producing cache misses is a Bernoulli process, i.e. each reference has a probability h of being a hit and a probability m = 1 h of being a miss. <p> One of these design choices is the internal organization of the cache, i.e. its associativity. Processor-level caches are usually direct mapped caches or k-way set associative caches with k being in the range of 2-8. This is due to the increase in hardware complexity as the associativity increases ([HP90], <ref> [Sto93] </ref>). However, file caches do not use special hardware for maintaining a cache. A file cache is held in ordinary random-access memory and is purely operated by software. This fact makes it relatively easy to use a fully associative design in the case of file caches ([CD88]). <p> Usually processor-level caches experience a higher hit rate than file caches, as a result of the increased spatial locality of the instruction and data access streams, as compared to a typical file access stream. The main reason for this phenomenon has been explained by Stone <ref> [Sto93] </ref> as the nested structure of programs, executing for long periods of their time in nested loops, which is exactly what caches are designed to CHAPTER 5. SIMULATED SOFTWARE MODELS 34 support. In file access traffic, however, this behavior is not as pronounced as in processor-level instruction streams. <p> Hennessy and Patterson [HP90] also provide some basic material on computer architecture in general, and on cache coherency in multiprocessor systems in particular. A similar approach is taken by Stone <ref> [Sto93] </ref>, which also contains a section on trace length considerations and efficient cache simulations. Tanenbaum [Tan88] is a standard textbook containing lots of computer network information.
Reference: [STW92] <author> Harold S. Stone, John Turek, and Joel L. Wolf. </author> <title> Optimal Partitioning of Cache Memory. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(9) </volume> <pages> 1054-1068, </pages> <month> Septem-ber </month> <year> 1992. </year>
Reference-contexts: Moreover, if such a client reduces the size of its cache, it will not suffer very much from that reduction, as the miss-rate derivative with respect to the cache size is only small. In their work, Stone, Turek, and Wolf <ref> [STW92] </ref> present a scheme to partition processor-level caches between interleaved data and instruction reference streams. The partitioning is considered optimal if the miss-rate derivatives with respect to the cache size of both streams are equal. An extension to their work for use in this study can easily be made. <p> Other work is aimed towards optimal partitioning the cache memory between various parties, like various file systems in Thiebaut, Stone and Wolf [TSW92], and partitioning between interleaved data and instruction streams in Stone, Turek, and Wolf <ref> [STW92] </ref>. A work from Ghanem [Gha75] investigates the partitioning of main memory between a number of processes. Cache Coherency A large number of people have studied various forms of coherency and non-coherency.
Reference: [Tan88] <author> Andrew S. Tanenbaum. </author> <title> Computer Networks. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year> <note> Second Edition. ISBN 0-13-166836-6. </note>
Reference-contexts: Hennessy and Patterson [HP90] also provide some basic material on computer architecture in general, and on cache coherency in multiprocessor systems in particular. A similar approach is taken by Stone [Sto93], which also contains a section on trace length considerations and efficient cache simulations. Tanenbaum <ref> [Tan88] </ref> is a standard textbook containing lots of computer network information. On the software side there is an introductory work into operating systems by Sil-berschatz and Peterson [SP88], and some further information, especially concerning distributed systems, can be found in Tanenbaum [Tan92].
Reference: [Tan92] <author> Andrew S. Tanenbaum. </author> <title> Modern Operating Systems. </title> <publisher> Prentice Hall, </publisher> <year> 1992. </year> <note> ISBN 0-13-595752-4. BIBLIOGRAPHY 76 </note>
Reference-contexts: Although the optimal cache replacement strategy is well known, it is impossible to implement it. Unfortunately the optimal strategy needs information about future events which is not available in a real-world implementation. There are many replacement strategies known. Some are rather simple ([SP88], <ref> [Tan92] </ref>) like FIFO, LFU (least frequently used), LRU (least recently used), and LR (longest resident), while others are quite sophisticated as in Gahanem [Gha75] and Maffeis [Maf92a]. However, LRU is the replacement policy mostly used and is also the policy of choice for this study. <p> Tanenbaum [Tan88] is a standard textbook containing lots of computer network information. On the software side there is an introductory work into operating systems by Sil-berschatz and Peterson [SP88], and some further information, especially concerning distributed systems, can be found in Tanenbaum <ref> [Tan92] </ref>. A work especially aimed towards distributed systems, with lots of information on distributed file systems is Coulouris and Dollimore [CD88]. Cache Replacement Strategies Lots of work has been conducted on cache replacement strategies.
Reference: [TSW92] <author> Dominique Thiebaut, Harold S. Stone, and Joel L. Wolf. </author> <title> Improving Disk Cache Hit-Ratios Through Cache Partitioning. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(6) </volume> <pages> 665-676, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Other work is aimed towards optimal partitioning the cache memory between various parties, like various file systems in Thiebaut, Stone and Wolf <ref> [TSW92] </ref>, and partitioning between interleaved data and instruction streams in Stone, Turek, and Wolf [STW92]. A work from Ghanem [Gha75] investigates the partitioning of main memory between a number of processes. Cache Coherency A large number of people have studied various forms of coherency and non-coherency.
Reference: [VMH + 83] <author> J. Voldman, B. Mandelbrot, L. W. Hoevel, J Knight, and P. Rosenfeld. </author> <title> Fractal Nature of Software-Cache Interaction. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 27(2) </volume> <pages> 164-170, </pages> <month> March </month> <year> 1983. </year>
Reference-contexts: On the other hand, cache misses as well as cache hits tend to be clustered in time | a phenomenon called temporal locality which, among others, was studied by Voldman et al. <ref> [VMH + 83] </ref>. Due to this difference between reality and the mathematical model, the model only provides a lower bound for the number of references needed to bound the accuracy of statistical estimates. <p> Starting with work analyzing various kinds of traffic characteristics, such as internetwork traffic by Danzig, Hall, and Schwartz [DHS93], FTP traffic by Maffeis [Maf92b, Maf92a] and Maffeis and Cap [MC92], and processor-level instruction streams by Voldman et al. <ref> [VMH + 83] </ref>. Other work is aimed towards optimal partitioning the cache memory between various parties, like various file systems in Thiebaut, Stone and Wolf [TSW92], and partitioning between interleaved data and instruction streams in Stone, Turek, and Wolf [STW92].
Reference: [YTB92] <author> Qing Yang, George Thangadurai, and Laxmi N. Bhuyan. </author> <title> Design of an Adaptive Cache Coherence Protocol for Large Scale Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(3) </volume> <pages> 281-293, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Heddaya and Sinha provide an attempt to give a formalism to cover all these different kinds of coherency [HS92a, HS92b]. Yang, Thangadurai and Bhuyan present an adaptive cache coherency protocol for a special-case architecture <ref> [YTB92] </ref>, and Farkas, Vranesic, and Stumm also develop a cache consistency scheme for yet another special architecture [FVS92]. Distributed Shared Memory (DSM) To provide the abstraction of shared memory on distributed memory machines a scheme called distributed shared memory is employed.
Reference: [ZSLW90] <author> Songnian Zhou, Michael Stumm, Kai Li, and David Wortman. </author> <title> Heterogeneous Distributed Shared Memory. </title> <type> Technical Report CSRI-244, </type> <institution> Computer Systems Research Institute, University of Toronto, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Some of the models also leave a programmer the choice which consistency model is likely to perform best for the specific application ([Bas90], [Bir91a], [BD90]). Most of these consistency models were proposed in the context of distributed shared memory ([MR91], <ref> [ZSLW90] </ref>, [MF90, MF89]). The aim of all these techniques is to reduce the coherency traffic incurred by sharing. This is done by loosening some of the restrictions that are normally expected to hold in centralized and distributed systems that provide sequential consistency. <p> This scheme has many similarities to the distribution of client caches over many clients as presented earlier in this work. Some basic work can be found in Mohindra and Ramachandran [MR91], as well as in Zhou et al. <ref> [ZSLW90] </ref>. A special implementation is presented in Minnich and Farber [MF89], who also analyze the network behavior of the system in [MF90].
References-found: 51

