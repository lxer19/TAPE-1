URL: http://www.cs.jhu.edu/grad/murthy/jair94.ps.Z
Refering-URL: http://www.cs.jhu.edu/grad/murthy/home.html
Root-URL: http://www.cs.jhu.edu
Email: murthy@cs.jhu.edu  kasif@cs.jhu.edu  salzberg@cs.jhu.edu  
Title: A System for Induction of Oblique Decision Trees  
Author: Sreerama K. Murthy Simon Kasif Steven Salzberg 
Address: Baltimore, MD 21218 USA  
Affiliation: Department of Computer Science Johns Hopkins University,  
Note: Journal of Artificial Intelligence Research 2 (1994) 1-32 Submitted 4/94; published 8/94  
Abstract: This article describes a new system for induction of oblique decision trees. This system, OC1, combines deterministic hill-climbing with two forms of randomization to find a good oblique split (in the form of a hyperplane) at each node of a decision tree. Oblique decision tree methods are tuned especially for domains in which the attributes are numeric, although they can be adapted to symbolic or mixed symbolic/numeric attributes. We present extensive empirical studies, using both real and artificial data, that analyze OC1's ability to construct oblique trees that are smaller and more accurate than their axis-parallel counterparts. We also examine the benefits of randomization for the construction of oblique decision trees. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. </author> <year> (1990). </year> <title> A Study of Instance-Based Algorithms for Supervised Learning: Mathematical, empirical and psychological evaluations. </title> <type> Ph.D. thesis, </type> <institution> Department of Information and Computer Science, University of California, Irvine. </institution> <note> 27 Murthy, </note> <author> Kasif & Salzberg Almuallin, H., & Dietterich, T. </author> <year> (1991). </year> <title> Learning with many irrelevant features. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 547-552. </pages> <address> San Jose, CA. </address>
Reference: <author> Belsley, D. </author> <year> (1980). </year> <title> Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. </title> <publisher> Wiley & Sons, </publisher> <address> New York. </address>
Reference: <author> Bennett, K., & Mangasarian, O. </author> <year> (1992). </year> <title> Robust linear programming discrimination of two linearly inseparable sets. </title> <journal> Optimization Methods and Software, </journal> <volume> 1, </volume> <pages> 23-34. </pages>
Reference: <author> Bennett, K., & Mangasarian, O. </author> <year> (1994a). </year> <title> Multicategory discrimination via linear programming. </title> <journal> Optimization Methods and Software, </journal> <volume> 3, </volume> <pages> 29-39. </pages>
Reference: <author> Bennett, K., & Mangasarian, O. </author> <year> (1994b). </year> <title> Serial and parallel multicategory discrimination. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4 (4). </volume>
Reference: <author> Blum, A., & Rivest, R. </author> <year> (1988). </year> <title> Training a 3-node neural network is NP-complete. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pp. 9-18. </pages> <address> Boston, MA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Breiman, L., Friedman, J., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: Figure 3 illustrates these upper limits for two points in two dimensions. For axis-parallel splits, there are only n d distinct possibilities, and axis-parallel methods such as C4.5 (Quinlan, 1993a) and CART <ref> (Breiman et al., 1984) </ref> can exhaustively search for the best split at each node. The problem of searching for the best oblique split is therefore much more difficult than that of searching for the best axis-parallel split. In fact, the problem is NP-hard. <p> There is also some less closely related work on algorithms to train artificial neural networks to build decision tree-like classifiers (Brent, 1991; Cios & Liu, 1992; Herman & Yeung, 1992). The first oblique decision tree algorithm to be proposed was CART with linear combinations <ref> (Breiman et al., 1984, chapter 5) </ref>. This algorithm, referred to henceforth as CART-LC, is an important basis for OC1. Figure 4 summarizes (using Breiman et al.'s notation) what the CART-LC algorithm does at each node in the decision tree. <p> OC1 uses an oblique split only when it improves over the best axis-parallel split. 4 4. As pointed out in <ref> (Breiman et al., 1984, Chapter 5) </ref>, it does not make sense to use an oblique split when the number of examples at a node n is less than or almost equal to the number of features d, because the 9 Murthy, Kasif & Salzberg The search strategy for the space of <p> The two main alternatives considered in the past have been simulated annealing, used in the SADT system (Heath et al., 1993b), and deterministic heuristic search, as in CART-LC <ref> (Breiman et al., 1984) </ref>.
Reference: <author> Brent, R. P. </author> <year> (1991). </year> <title> Fast training algorithms for multilayer neural nets. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2 (3), </volume> <pages> 346-354. </pages>
Reference: <author> Brodley, C. E., & Utgoff, P. E. </author> <year> (1992). </year> <title> Multivariate versus univariate decision trees. </title> <type> Tech. rep. COINS CR 92-8, </type> <institution> Dept. of Computer Science, University of Massachusetts at Amherst. </institution>
Reference-contexts: This allows us to quantify more precisely how the parameters of our algorithm affect its performance. A second purpose of this experiment is to compare OC1's search strategy with that of two existing oblique decision tree induction systems - LMDT <ref> (Brodley & Utgoff, 1992) </ref> and SADT (Heath et al., 1993b). We show that the quality of trees induced by OC1 is as good as, if not better than, that of the trees induced by these existing systems on three 19 Murthy, Kasif & Salzberg artificial domains.
Reference: <author> Brodley, C. E., & Utgoff, P. E. </author> <year> (1994). </year> <title> Multivariate decision trees. </title> <journal> Machine Learning, </journal> <note> to appear. </note>
Reference-contexts: Because these tests are equivalent to hy-perplanes at an oblique orientation to the axes, we call this class of decision trees oblique decision trees. (Trees of this form have also been called "multivariate" <ref> (Brodley & Utgoff, 1994) </ref>. We prefer the term "oblique" because "multivariate" includes non-linear combinations of the variables, i.e., curved surfaces.
Reference: <author> Buntine, W. </author> <year> (1992). </year> <title> Tree classification software. </title> <booktitle> Technology 2002: The Third National Technology Transfer Conference and Exposition. </booktitle>
Reference-contexts: We also included axis-parallel CART and C4.5 in our comparisons. We used the implementations of these algorithms from the IND 2.1 package <ref> (Buntine, 1992) </ref>. The default cart0 and c4.5 "styles" defined in the package were used, without altering any parameter settings. The cart0 style uses the Twoing Rule and 0-SE cost complexity pruning with 10-fold cross validation.
Reference: <author> Buntine, W., & Niblett, T. </author> <year> (1992). </year> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 75-85. </pages>
Reference-contexts: We also included axis-parallel CART and C4.5 in our comparisons. We used the implementations of these algorithms from the IND 2.1 package <ref> (Buntine, 1992) </ref>. The default cart0 and c4.5 "styles" defined in the package were used, without altering any parameter settings. The cart0 style uses the Twoing Rule and 0-SE cost complexity pruning with 10-fold cross validation.
Reference: <author> Cardie, C. </author> <year> (1993). </year> <title> Using decision trees to improve case-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. 25-32. </pages> <institution> University of Massachusetts, Amherst. </institution>
Reference: <author> Cestnik, G., Kononenko, I., & Bratko, I. </author> <year> (1987). </year> <title> Assistant 86: A knowledge acquisition tool for sophisticated users. </title> <editor> In Bratko, I., & Lavrac, N. (Eds.), </editor> <booktitle> Progress in Machine Learning. </booktitle> <publisher> Sigma Press. </publisher>
Reference: <author> Cios, K. J., & Liu, N. </author> <year> (1992). </year> <title> A machine learning method for generation of a neural network architecture: A continuous ID3 algorithm. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3 (2), </volume> <month> 280-291. </month> <title> Induction of Oblique Decision Trees Cohen, </title> <editor> W. </editor> <year> (1993). </year> <title> Efficient pruning methods for separate-and-conquer rule learning systems. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 988-994. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Fayyad, U. M., & Irani, K. B. </author> <year> (1992). </year> <title> The attribute specification problem in decision tree generation. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pp. 104-110. </pages> <address> San Jose CA. </address> <publisher> AAAI Press. </publisher>
Reference: <author> Frean, M. </author> <year> (1990). </year> <title> Small Nets and Short Paths: Optimising neural computation. </title> <type> Ph.D. thesis, </type> <institution> Centre for Cognitive Science, University of Edinburgh. </institution>
Reference-contexts: The training algorithm presents examples repeatedly at each node until the linear machine converges. Because convergence cannot be guaranteed, LMDT uses heuristics to determine when the node has stabilized. To make the training stable even when the set of training instances is not linearly separable, a "thermal training" method <ref> (Frean, 1990) </ref> is used, similar to simulated annealing. A third system that creates oblique trees is Simulated Annealing of Decision Trees (SADT) (Heath et al., 1993b) which, like OC1, uses randomization.
Reference: <author> Gupta, R., Smolka, S., & Bhaskar, S. </author> <year> (1994). </year> <title> On randomization in sequential and distributed algorithms. </title> <journal> ACM Computing Surveys, </journal> <volume> 26 (1), </volume> <pages> 7-86. </pages>
Reference-contexts: It is natural to ask why randomization helps OC1 in the task of inducing decision trees. Researchers in combinatorial optimization have observed that randomized search usually succeeds when the search space holds an abundance of good solutions <ref> (Gupta, Smolka, & Bhaskar, 1994) </ref>. Furthermore, randomization can improve upon deterministic search when many of the local maxima in a search space lead to poor solutions.
Reference: <author> Hampson, S., & Volper, D. </author> <year> (1986). </year> <title> Linear function neurons: Structure and training. </title> <journal> Biological Cybernetics, </journal> <volume> 53, </volume> <pages> 203-217. </pages>
Reference: <author> Harrison, D., & Rubinfeld, D. </author> <year> (1978). </year> <title> Hedonic prices and the demand for clean air. </title> <journal> Journal of Environmental Economics and Management, </journal> <volume> 5, </volume> <pages> 81-102. </pages>
Reference-contexts: Housing Costs in Boston. This data set, also available as a part of the UCI ML repository, describes housing values in the suburbs of Boston as a function of 12 continuous attributes and 1 binary attribute <ref> (Harrison & Rubinfeld, 1978) </ref>. The category variable (median value of owner-occupied homes) is actually continuous, but we discretized it so that category = 1 if value &lt; $21000, and 2 otherwise. For other uses of this data, see (Belsley, 1980; Quinlan, 1993b). Diabetes diagnosis.
Reference: <author> Hassibi, B., & Stork, D. </author> <year> (1993). </year> <title> Second order derivatives for network pruning: optimal brain surgeon. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <pages> pp. 164-171. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Heath, D. </author> <year> (1992). </year> <title> A Geometric Framework for Machine Learning. </title> <type> Ph.D. thesis, </type> <institution> Johns Hopkins University, Baltimore, Maryland. </institution>
Reference: <author> Heath, D., Kasif, S., & Salzberg, S. </author> <year> (1993a). </year> <title> k-DT: A multi-tree learning method. </title> <booktitle> In Proceedings of the Second International Workshop on Multistrategy Learning, </booktitle> <pages> pp. 138-149. </pages> <address> Harpers Ferry, WV. </address> <institution> George Mason University. </institution>
Reference: <author> Heath, D., Kasif, S., & Salzberg, S. </author> <year> (1993b). </year> <title> Learning oblique decision trees. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1002-1007. </pages> <address> Chambery, France. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: To make the training stable even when the set of training instances is not linearly separable, a "thermal training" method (Frean, 1990) is used, similar to simulated annealing. A third system that creates oblique trees is Simulated Annealing of Decision Trees (SADT) <ref> (Heath et al., 1993b) </ref> which, like OC1, uses randomization. SADT uses simulated annealing (Kirkpatrick, Gelatt, & Vecci, 1983) to find good values for the coefficients of the hyperplane at each node of a tree. <p> Because there are an exponential number of distinct ways to partition the examples with a hyperplane, any procedure that simply enumerates all of them will be unreasonably costly. The two main alternatives considered in the past have been simulated annealing, used in the SADT system <ref> (Heath et al., 1993b) </ref>, and deterministic heuristic search, as in CART-LC (Breiman et al., 1984). <p> This allows us to quantify more precisely how the parameters of our algorithm affect its performance. A second purpose of this experiment is to compare OC1's search strategy with that of two existing oblique decision tree induction systems - LMDT (Brodley & Utgoff, 1992) and SADT <ref> (Heath et al., 1993b) </ref>. We show that the quality of trees induced by OC1 is as good as, if not better than, that of the trees induced by these existing systems on three 19 Murthy, Kasif & Salzberg artificial domains.
Reference: <author> Herman, G. T., & Yeung, K. D. </author> <year> (1992). </year> <title> On piecewise-linear classification. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 14 (7), </volume> <pages> 782-786. </pages>
Reference: <author> Holte, R. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11 (1), </volume> <pages> 63-90. </pages>
Reference-contexts: It is known that many of the commonly-used data sets from the UCI repository are easy to learn with very simple representations <ref> (Holte, 1993) </ref>; therefore those data sets may not be ideal for our purposes. Thus we created a number of artificial data sets that present different problems for learning, and for which we know the "correct" concept definition.
Reference: <author> Hyafil, L., & Rivest, R. L. </author> <year> (1976). </year> <title> Constructing optimal binary decision trees is NP-complete. </title> <journal> Information Processing Letters, </journal> <volume> 5 (1), </volume> <pages> 15-17. </pages>
Reference-contexts: Note that although they do find the optimal test (with respect to an impurity measure) for each node of a tree, the complete tree may not be optimal: as is well known, the problem of finding the smallest tree is NP-Complete <ref> (Hyafil & Rivest, 1976) </ref>. Thus even axis-parallel decision tree methods do not produce "ideal" decision trees.
Reference: <author> Kira, K., & Rendell, L. </author> <year> (1992). </year> <title> A practical approach to feature selection. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. 249-256. </pages> <address> Aberdeen, Scotland. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kirkpatrick, S., Gelatt, C., & Vecci, M. </author> <year> (1983). </year> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220 (4598), </volume> <pages> 671-680. </pages> <note> 29 Murthy, </note> <author> Kasif & Salzberg Kodratoff, Y., & Manago, M. </author> <year> (1987). </year> <title> Generalization and noise. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27, </volume> <pages> 181-204. </pages>
Reference-contexts: A third system that creates oblique trees is Simulated Annealing of Decision Trees (SADT) (Heath et al., 1993b) which, like OC1, uses randomization. SADT uses simulated annealing <ref> (Kirkpatrick, Gelatt, & Vecci, 1983) </ref> to find good values for the coefficients of the hyperplane at each node of a tree. SADT first places a hyperplane in a canonical location, and then iteratively perturbs all the coefficients by small random amounts.
Reference: <author> Langley, P., & Sage, S. </author> <year> (1993). </year> <title> Scaling to domains with many irrelevant features. </title> <institution> Learning Systems Department, Siemens Corporate Research, Princeton, NJ. </institution>
Reference: <author> Mangasarian, O., Setiono, R., & Wolberg, W. </author> <year> (1990). </year> <title> Pattern recognition via linear programming: Theory and application to medical diagnosis. </title> <note> In SIAM Workshop on Optimization. </note>
Reference: <author> Mingers, J. </author> <year> (1989a). </year> <title> An empirical comparison of pruning methods for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4 (2), </volume> <pages> 227-243. </pages>
Reference: <author> Mingers, J. </author> <year> (1989b). </year> <title> An empirical comparison of selection measures for decision tree induction. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 319-342. </pages>
Reference: <author> Moret, B. M. </author> <year> (1982). </year> <title> Decision trees and diagrams. </title> <journal> Computing Surveys, </journal> <volume> 14 (4), </volume> <pages> 593-623. </pages>
Reference-contexts: However, any non-exhaustive deterministic algorithm for searching through all these hyperplanes is prone to getting stuck in local minima. 3. Throughout the paper, we use the terms "split" and "hyperplane" interchangeably to refer to the test at a node of a decision tree. The first usage is standard <ref> (Moret, 1982) </ref>, and refers to the fact that the test splits the data into two partitions.
Reference: <author> Murphy, P., & Aha, D. </author> <year> (1994). </year> <title> UCI repository of machine learning databases a machine-readable data repository. </title> <institution> Maintained at the Department of Information and Computer Science, University of California, Irvine. </institution> <note> Anonymous FTP from ics.uci.edu in the directory pub/machine-learning-databases. </note>
Reference-contexts: The first line for each method gives accuracies, and the second line gives average tree sizes. The highest accuracy for each domain appears in boldface. and is available from the UC Irvine machine learning repository <ref> (Murphy & Aha, 1994) </ref>. Heath et al. (1993b) reported 94.9% accuracy on a subset of this data set (it then had only 470 instances), with an average decision tree size of 4.6 nodes, using SADT. Salzberg (1991) reported 96.0% accuracy using 1-NN on the same (smaller) data set.
Reference: <author> Murthy, S. K., Kasif, S., Salzberg, S., & Beigel, R. </author> <year> (1993). </year> <title> OC1: Randomized induction of oblique decision trees. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. 322-327. </pages> <address> Washington, D.C. </address> <publisher> MIT Press. </publisher>
Reference-contexts: This constant can be set by the user. P stag is reset to 1 every time the global impurity measure is improved. 11 Murthy, Kasif & Salzberg Our previous experiments <ref> (Murthy et al., 1993) </ref> indicated that the order of perturbation of the coefficients does not affect the classification accuracy as much as other parameters, especially the randomization parameters (see below).
Reference: <author> Murthy, S. K., & Salzberg, S. </author> <year> (1994). </year> <title> Using structure to improve decision trees. </title> <type> Tech. rep. </type> <institution> JHU-94/12, Department of Computer Science, Johns Hopkins University. </institution>
Reference-contexts: However, it is easy to include any of several standard methods (e.g., stepwise forward selection or stepwise backward selection) or even an ad hoc method to select features before running the tree-building process. For example, in separate experiments on data from the Hubble Space Telescope <ref> (Salzberg, Chandar, Ford, Murthy, & White, 1994) </ref>, we used feature selection methods as a preprocessing step to OC1, and reduced the number of attributes from 20 to 2. The resulting decision trees were both simpler and more accurate. <p> This concept is more difficult to learn than a single linear separator, but the minimal-size tree is still quite small. RCB RCB stands for "rotated checker board"; this data set has been the subject of other experiments on hard classification problems for decision trees <ref> (Murthy & Salzberg, 1994) </ref>. The data set, shown in Figure 9, has 2000 instances in 2-D, each belonging to one of eight categories. This concept is difficult to learn for any axis-parallel method, for obvious reasons. It is also quite difficult for oblique methods, for several reasons.
Reference: <author> Niblett, T. </author> <year> (1986). </year> <title> Constructing decision trees in noisy domains. </title> <editor> In Bratko, I., & Lavrac, N. (Eds.), </editor> <booktitle> Progress in Machine Learning. </booktitle> <publisher> Sigma Press, </publisher> <address> England. </address>
Reference: <author> Nilsson, N. </author> <year> (1990). </year> <title> Learning Machines. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Each internal node in an LMDT tree is a Linear Machine <ref> (Nilsson, 1990) </ref>. The training algorithm presents examples repeatedly at each node until the linear machine converges. Because convergence cannot be guaranteed, LMDT uses heuristics to determine when the node has stabilized.
Reference: <author> Odewahn, S., Stockwell, E., Pennington, R., Humphreys, R., & Zumach, W. </author> <year> (1992). </year> <title> Automated star-galaxy descrimination with neural networks. </title> <journal> Astronomical Journal, </journal> <volume> 103 (1), </volume> <pages> 318-331. </pages>
Reference-contexts: In the next section we will consider artificial data, for which the concept definition can be precisely characterized. 4.2.1 Description of Data Sets Star/Galaxy Discrimination. Two of our data sets came from a large set of astronomical images collected by Odewahn et al. <ref> (Odewahn, Stockwell, Pennington, Humphreys, & Zumach, 1992) </ref>. In their study, they used these images to train artificial neural networks running the perceptron and back propagation algorithms.
Reference: <author> Pagallo, G. </author> <year> (1990). </year> <title> Adaptive Decision Tree Algorithms for Learning From Examples. </title> <type> Ph.D. thesis, </type> <institution> University of California at Santa Cruz. </institution>
Reference: <author> Pagallo, G., & Haussler, D. </author> <year> (1990). </year> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5 (1), </volume> <pages> 71-99. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1983). </year> <title> Learning efficient classification procedures and their application to chess end games. </title> <editor> In Michalski, R., Carbonell, J., & Mitchell, T. (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Decision trees have been used for classification and other tasks since the 1960s (Moret, 1982; Safavin & Landgrebe, 1991). In the 1980's, Breiman et al.'s book on classification and regression trees (CART) and Quin-lan's work on ID3 <ref> (Quinlan, 1983, 1986) </ref> provided the foundations for what has become a large body of research on one of the central techniques of experimental machine learning. Many variants of decision tree (DT) algorithms have been introduced in the last decade.
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <month> 81-106. </month> <title> 30 Induction of Oblique Decision Trees Quinlan, </title> <editor> J. R. </editor> <year> (1987). </year> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27, </volume> <pages> 221-234. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1993a). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Figure 3 illustrates these upper limits for two points in two dimensions. For axis-parallel splits, there are only n d distinct possibilities, and axis-parallel methods such as C4.5 <ref> (Quinlan, 1993a) </ref> and CART (Breiman et al., 1984) can exhaustively search for the best split at each node. The problem of searching for the best oblique split is therefore much more difficult than that of searching for the best axis-parallel split. In fact, the problem is NP-hard. <p> Thus even axis-parallel decision tree methods do not produce "ideal" decision trees. Quinlan has suggested that his windowing algorithm might be used as a way of introducing randomization into C4.5, even though the algorithm was designed for another purpose <ref> (Quinlan, 1993a) </ref>. (The windowing 23 Murthy, Kasif & Salzberg algorithm selects a random subset of the training data and builds a tree using that.) We believe that randomization is a powerful tool in the context of decision trees, and our experiments are just one example of how it might be exploited.
Reference: <author> Quinlan, J. R. </author> <year> (1993b). </year> <title> Combining instance-based and model-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <institution> 236-243 University of Massachusetts, </institution> <address> Amherst. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Roth, R. H. </author> <year> (1970). </year> <title> An approach to solving linear discrete optimization problems. </title> <journal> Journal of the ACM, </journal> <volume> 17 (2), </volume> <pages> 303-313. </pages>
Reference: <author> Safavin, S. R., & Landgrebe, D. </author> <year> (1991). </year> <title> A survey of decision tree classifier methodology. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 21 (3), </volume> <pages> 660-674. </pages>
Reference: <author> Sahami, M. </author> <year> (1993). </year> <title> Learning non-linearly separable boolean functions with linear threshold unit trees and madaline-style networks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. 335-341. </pages> <publisher> AAAI Press. </publisher>
Reference: <author> Salzberg, S. </author> <year> (1991). </year> <title> A nearest hyperrectangle learning method. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 251-276. </pages>
Reference: <author> Salzberg, S. </author> <year> (1992). </year> <title> Combining learning and search to create good classifiers. </title> <type> Tech. rep. </type> <institution> JHU-92/12, Johns Hopkins University, Baltimore MD. </institution>
Reference: <author> Salzberg, S., Chandar, R., Ford, H., Murthy, S. K., & White, R. </author> <year> (1994). </year> <title> Decision trees for automated identification of cosmic rays in Hubble Space Telescope images. </title> <journal> Publications of the Astronomical Society of the Pacific, </journal> <note> to appear. </note>
Reference-contexts: However, it is easy to include any of several standard methods (e.g., stepwise forward selection or stepwise backward selection) or even an ad hoc method to select features before running the tree-building process. For example, in separate experiments on data from the Hubble Space Telescope <ref> (Salzberg, Chandar, Ford, Murthy, & White, 1994) </ref>, we used feature selection methods as a preprocessing step to OC1, and reduced the number of attributes from 20 to 2. The resulting decision trees were both simpler and more accurate. <p> This concept is more difficult to learn than a single linear separator, but the minimal-size tree is still quite small. RCB RCB stands for "rotated checker board"; this data set has been the subject of other experiments on hard classification problems for decision trees <ref> (Murthy & Salzberg, 1994) </ref>. The data set, shown in Figure 9, has 2000 instances in 2-D, each belonging to one of eight categories. This concept is difficult to learn for any axis-parallel method, for obvious reasons. It is also quite difficult for oblique methods, for several reasons.
Reference: <author> Schaffer, C. </author> <year> (1993). </year> <title> Overfitting avoidance as bias. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 153-178. </pages>
Reference: <author> Schlimmer, J. </author> <year> (1993). </year> <title> Efficiently inducing determinations: A complete and systematic search algorithm that uses optimal pruning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. 284-290. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Smith, J., Everhart, J., Dickson, W., Knowler, W., & Johannes, R. </author> <year> (1988). </year> <title> Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. </title> <booktitle> In Proceedings of the Symposium on Computer Applications and Medical Care, </booktitle> <pages> pp. 261-265. </pages> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Utgoff, P. E. </author> <year> (1989). </year> <title> Perceptron trees: A case study in hybrid concept representations. </title> <journal> Connection Science, </journal> <volume> 1 (4), </volume> <pages> 377-391. </pages>
Reference: <author> Utgoff, P. E., & Brodley, C. E. </author> <year> (1990). </year> <title> An incremental method for finding multivariate splits for decision trees. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pp. 58-65. </pages> <address> Los Altos, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Utgoff, P. E., & Brodley, C. E. </author> <year> (1991). </year> <title> Linear machine decision trees. </title> <type> Tech. rep. 10, </type> <institution> University of Massachusetts at Amherst. </institution> <note> 31 Murthy, </note> <author> Kasif & Salzberg Van de Merckt, T. </author> <year> (1992). </year> <title> NFDT: A system that learns flexible concepts based on decision trees for numerical attributes. </title> <booktitle> In Proceedings of the Ninth International Workshop on Machine Learning, </booktitle> <pages> pp. 322-331. </pages>
Reference: <author> Van de Merckt, T. </author> <year> (1993). </year> <title> Decision trees in numerical attribute spaces. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1016-1021. </pages>
Reference: <author> Weiss, S., & Kapouleas, I. </author> <year> (1989). </year> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods. </title> <booktitle> In Proceedings of the 11th International Joint Conference of Artificial Intelligence, </booktitle> <pages> pp. 781-787. </pages> <address> Detroit, MI. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Wolpert, D. </author> <year> (1992). </year> <title> On overfitting avoidance as bias. </title> <type> Tech. rep. SFI TR 92-03-5001, </type> <institution> The Santa Fe Institute, </institution> <address> Santa Fe, New Mexico. </address> <month> 32 </month>
References-found: 61

