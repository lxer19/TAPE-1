URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95632-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: facha,uysal,robertb,assaf,beynon,hollings,saltz,alsg@cs.umd.edu  
Title: Tuning the Performance of I/O Intensive Parallel Applications 1  
Author: Anurag Acharya, Mustafa Uysal, Robert Bennett, Assaf Mendelson, Michael Beynon, Jeff Hollingsworth, Joel Saltz, Alan Sussman 
Address: College Park MD 20742  
Affiliation: Dept of Computer Science, University of Maryland,  
Abstract: Getting good I/O performance from parallel programs is a critical problem for many application domains. In this paper, we report our experience tuning the I/O performance of four application programs from the areas of sensor data processing and linear algebra. After tuning, three of the four applications achieve effective I/O rates of over 100Mb/s, on 16 processors. The total volume of I/O required by the programs ranged from about 75MB to over 200GB. We report the lessons learned in achieving high I/O performance from these applications, including the need for code restructuring, local disks on every node and overlapping I/O with computation. We also report our experience on achieving high performance on peer-to-peer configurations. Finally, we comment on the necessity of complex I/O interfaces like collective I/O and strided requests to achieve high performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Robert Bennett, Kelvin Bryant, Alan Sussman, Raja Das, and Joel Saltz. Jovian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proceedings of the 1994 Scalable Parallel Libraries Conference, </booktitle> <pages> pages 10-20. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year>
Reference-contexts: In addition, local accesses are guaranteed not to interfere with I/O requests from other processors. This increases the utility of the file cache and makes the overall behavior of the application becomes more predictable. Exploiting locality in this manner is beneficial for out-of-core applications <ref> [1, 2, 12] </ref> on both client-server and peer-to-peer configurations. In either configuration, exploiting locality improves I/O performance as well as total execution time. <p> Relatively straightforward loop restructuring, including interchanging the order of nested loops [15] and fusing multiple requests were sufficient to coalesce these requests into large block I/O requests. * None of the applications studied required collective I/O <ref> [1, 3, 13] </ref>. This is not surprising given the size of the requests after code restructuring. All of the applications are parallelized in SPMD fashion. In our earth science applications all processes are independent (apart from initial and possibly final synchronization).
Reference: [2] <author> R. Bordawekar, A. Choudhary, K. Kennedy, C. Koelbel, and M. Paleczny. </author> <title> A model and compilation strategy for out-of-core data parallel programs. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 1-10. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1995. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 30, No. </volume> <pages> 8. </pages>
Reference-contexts: As a result, significant effort has been put into trying to improve parallel I/O systems. To date, most of the effort has focused on observing the I/O behavior of existing applications and on trying to improve the ability of I/O systems to execute existing applications <ref> [2, 3, 4, 6, 8] </ref>. We decided to take a different approach; rather than tuning the I/O system to optimize fixed applications, we concentrated on tuning the applications to improve their I/O performance (and hopefully also improve their execution time). <p> In addition, local accesses are guaranteed not to interfere with I/O requests from other processors. This increases the utility of the file cache and makes the overall behavior of the application becomes more predictable. Exploiting locality in this manner is beneficial for out-of-core applications <ref> [1, 2, 12] </ref> on both client-server and peer-to-peer configurations. In either configuration, exploiting locality improves I/O performance as well as total execution time.
Reference: [3] <author> Alok Choudhary, Rajesh Bordawekar, Michael Harry, Rakesh Krishnaiyer, Ravi Ponnusamy, Tarvinder Singh, and Rajeev Thakur. </author> <title> PASSION: Parallel and scalable software for input-output. </title> <type> Technical Report SCCS-636, </type> <institution> NPAC, </institution> <month> September </month> <year> 1994. </year> <note> Also available as CRPC Report CRPC-TR94483. </note>
Reference-contexts: As a result, significant effort has been put into trying to improve parallel I/O systems. To date, most of the effort has focused on observing the I/O behavior of existing applications and on trying to improve the ability of I/O systems to execute existing applications <ref> [2, 3, 4, 6, 8] </ref>. We decided to take a different approach; rather than tuning the I/O system to optimize fixed applications, we concentrated on tuning the applications to improve their I/O performance (and hopefully also improve their execution time). <p> Relatively straightforward loop restructuring, including interchanging the order of nested loops [15] and fusing multiple requests were sufficient to coalesce these requests into large block I/O requests. * None of the applications studied required collective I/O <ref> [1, 3, 13] </ref>. This is not surprising given the size of the requests after code restructuring. All of the applications are parallelized in SPMD fashion. In our earth science applications all processes are independent (apart from initial and possibly final synchronization).
Reference: [4] <author> P. Crandall, R. Aydt, A. Chien, and D. Reed. </author> <title> Input/output characteristics of scalable parallel applications. </title> <booktitle> In Proceedings Supercomputing'95, </booktitle> <month> December </month> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: As a result, significant effort has been put into trying to improve parallel I/O systems. To date, most of the effort has focused on observing the I/O behavior of existing applications and on trying to improve the ability of I/O systems to execute existing applications <ref> [2, 3, 4, 6, 8] </ref>. We decided to take a different approach; rather than tuning the I/O system to optimize fixed applications, we concentrated on tuning the applications to improve their I/O performance (and hopefully also improve their execution time).
Reference: [5] <author> J.J.Dongarra, J.DuCroz, I.S.Duff, and S.Hammarling. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16(1) </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: Our parallel out-of-core sparse Cholesky factorization is a parallelization of a left-looking supernodal Cholesky factorization algorithm [10]. This particular formulation of Cholesky factorization enables the use of efficient dense linear algebra kernels <ref> [5] </ref>, as well as large transfers between secondary storage and primary memory. 10 1 for i = 1 to S do 2 for all S j with j &lt; i and S ij 6= ; 3 Read S j 4 Update S i with S j 5 Discard S j 6
Reference: [6] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <booktitle> In Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 61-74. </pages> <publisher> ACM Press, </publisher> <month> November </month> <year> 1994. </year>
Reference-contexts: As a result, significant effort has been put into trying to improve parallel I/O systems. To date, most of the effort has focused on observing the I/O behavior of existing applications and on trying to improve the ability of I/O systems to execute existing applications <ref> [2, 3, 4, 6, 8] </ref>. We decided to take a different approach; rather than tuning the I/O system to optimize fixed applications, we concentrated on tuning the applications to improve their I/O performance (and hopefully also improve their execution time).
Reference: [7] <author> David Kotz and Ting Cai. </author> <title> Exploring the use of I/O nodes for computation in a MIMD processor. </title> <booktitle> In Proceedings of the IPPS'95 Third Annual Workshop on Input/Output in Parallel and Distributed Systems (IOPADS), </booktitle> <pages> pages 78-89, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: This allowed it to overlap the I/O and communication with the computation. The problem of achieving good computation performance on processors that are serving data to others has been previously noted by Kotz and Cai <ref> [7] </ref>. In their experiments on a cluster of RS6000s, they found that serving off-processor I/O requests can slow a relatively simple parallel program by between 17% and 98%. Given the trend towards commodity hardware, we believe peer-to-peer configurations are important for applications that do substantial I/O and computation.
Reference: [8] <author> David Kotz and Nils Nieuwejaar. </author> <title> File-system workload on a scientific multiprocessor. </title> <journal> IEEE Parallel & Distributed Technology, </journal> <volume> 3(1) </volume> <pages> 51-60, </pages> <month> Spring </month> <year> 1995. </year>
Reference-contexts: As a result, significant effort has been put into trying to improve parallel I/O systems. To date, most of the effort has focused on observing the I/O behavior of existing applications and on trying to improve the ability of I/O systems to execute existing applications <ref> [2, 3, 4, 6, 8] </ref>. We decided to take a different approach; rather than tuning the I/O system to optimize fixed applications, we concentrated on tuning the applications to improve their I/O performance (and hopefully also improve their execution time).
Reference: [9] <author> Joseph W. H. Liu. </author> <title> The role of elimination trees in sparse factorization. </title> (11):134-172, 1990. 
Reference-contexts: For all the experiments in this section, we have not used local disk striping; each node uses a single local disk. A key data structure in factor is the elimination tree <ref> [9] </ref> generated by the symbolic factorization using the structure of the sparse matrix. This structure contains dependency information between different supernodes and allows all processors to generate a schedule for their I/O and communication.
Reference: [10] <author> Esmond G. NG and Barry W. Peyton. </author> <title> Block sparse cholesky algorithms on advanced uniprocessor computers. </title> <booktitle> 14(5) </booktitle> <pages> 1034-1056, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: The Cholesky factor of a symmetric positive definite matrix A is a lower triangular matrix L with positive diagonal, such that A = LL T . Our parallel out-of-core sparse Cholesky factorization is a parallelization of a left-looking supernodal Cholesky factorization algorithm <ref> [10] </ref>.
Reference: [11] <author> Nils Nieuwejaar and David Kotz. </author> <title> Low-level interfaces for high-level parallel I/O. </title> <booktitle> In Proceedings of the IPPS'95 Third Annual Workshop on Input/Output in Parallel and Distributed Systems (IOPADS), </booktitle> <pages> pages 47-62, </pages> <month> April </month> <year> 1995. </year>
Reference-contexts: Complex I/O interfaces are not required: * After code restructuring, most requests in the studied applications were large. For large requests, the interface is usually less important. * Small strided requests were a recurrent pattern in the original versions of pathfinder and climate. Nested-strided requests <ref> [11] </ref> have been proposed for just such patterns. However we found that these patterns were caused by the embedding of small I/O requests in the innermost loops.
Reference: [12] <author> M. Paleczny, K. Kennedy, and C. Koelbel. </author> <title> Compiler support for out-of-core arrays on parallel machines. </title> <booktitle> In Proceedings of the Fifth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 110-118. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> February </month> <year> 1995. </year>
Reference-contexts: In addition, local accesses are guaranteed not to interfere with I/O requests from other processors. This increases the utility of the file cache and makes the overall behavior of the application becomes more predictable. Exploiting locality in this manner is beneficial for out-of-core applications <ref> [1, 2, 12] </ref> on both client-server and peer-to-peer configurations. In either configuration, exploiting locality improves I/O performance as well as total execution time.
Reference: [13] <author> Juan Miguel del Rosario and Alok N. Choudhary. </author> <title> High-performance I/O for massively parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Relatively straightforward loop restructuring, including interchanging the order of nested loops [15] and fusing multiple requests were sufficient to coalesce these requests into large block I/O requests. * None of the applications studied required collective I/O <ref> [1, 3, 13] </ref>. This is not surprising given the size of the requests after code restructuring. All of the applications are parallelized in SPMD fashion. In our earth science applications all processes are independent (apart from initial and possibly final synchronization).
Reference: [14] <author> Edward Rothberg and Anoop Gupta. </author> <title> An efficient block-oriented approach to parallel sparse cholesky factorization. </title> <address> pages 503-512, Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: The index file contains the sparsity structure of L (which is obtained from symbolic factorization). The index file is distributed among the processors, with each processor having a block of columns. We use a 2-D partitioning strategy, originally developed in <ref> [14] </ref>. The processors are organized in a k fi m grid. Let P i;j denote the processor number at the ith row and jth column of the processor grid. Supernode i of matrix A is mapped to processors in the (i mod m) th column of the processor grid.
Reference: [15] <author> Michael Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year> <month> 16 </month>
Reference-contexts: Nested-strided requests [11] have been proposed for just such patterns. However we found that these patterns were caused by the embedding of small I/O requests in the innermost loops. Relatively straightforward loop restructuring, including interchanging the order of nested loops <ref> [15] </ref> and fusing multiple requests were sufficient to coalesce these requests into large block I/O requests. * None of the applications studied required collective I/O [1, 3, 13]. This is not surprising given the size of the requests after code restructuring. All of the applications are parallelized in SPMD fashion.
References-found: 15

