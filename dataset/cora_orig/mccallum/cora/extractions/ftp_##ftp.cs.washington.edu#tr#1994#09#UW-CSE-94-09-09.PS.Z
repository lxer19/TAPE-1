URL: ftp://ftp.cs.washington.edu/tr/1994/09/UW-CSE-94-09-09.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: Architectural Support for Compiler-Generated Data-Parallel Programs  
Author: by Alexander C. Klaiber 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Approved by (Chairperson of Supervisory Committee)  
Note: Program Authorized to Offer Degree Date  
Date: 1994  
Affiliation: University of Washington  
Abstract-found: 0
Intro-found: 1
Reference: [Adve & Hill 90] <author> S. Adve and M. Hill. </author> <title> Weak ordering a new definition. </title> <booktitle> In Proceedings of 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <year> 1990. </year>
Reference: [Agarwal et al. 88] <author> A. Agarwal, R. Simoni, J. Hennessy, and M. Horowitz. </author> <title> An evaluation of directory schemes for cache coherence. </title> <booktitle> In Proceedings of 15th International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <year> 1988. </year>
Reference: [Agarwal et al. 91] <author> A. Agarwal, D. Chaiken, G. D'Souza, K. Johnson, D. Kranz, J. Kubia-towicz, K. Kurihara, B.-H. Lim, G. Maa, D. Nussbaum, M. Parkin, and D. Yeung. </author> <title> The MIT Alewife machine: A large-scale distributed-memory multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: To fully realize the advantages of parallel processing, we need to design efficient communication mechanisms. Existing communication architectures span a spectrum ranging from message passing [Arlauskas 88, Intel 91a, Dally 90, TMC 91b] to remote-memory access [Crowther et al. 85, Cray 93], shared memory <ref> [Sequent 87, Lenoski et al. 92, Agarwal et al. 91] </ref> and cache-only architectures [Hagersten 92a, KSR 92]. These communication architectures are often used directly by the programmer a fact that has influenced their design, much as assembly language programming has influenced the design of CISC instruction sets. <p> We argue that those machines would benefit greatly from architectural support, such as adaptive or user-selectable cache coherence protocols [Carter et al. 91, Bennett et al. 92], full/empty bits on memory words <ref> [Agarwal et al. 91, Alverson et al. 90] </ref>, a network or network interface dedicated to barrier synchronization and reduction (e.g., the control network on the CM-5 [TMC 91b]), or direct access to message-passing primitives as implemented on the Alewife machine [Agarwal et al. 91]. <p> 91, Bennett et al. 92], full/empty bits on memory words [Agarwal et al. 91, Alverson et al. 90], a network or network interface dedicated to barrier synchronization and reduction (e.g., the control network on the CM-5 [TMC 91b]), or direct access to message-passing primitives as implemented on the Alewife machine <ref> [Agarwal et al. 91] </ref>. As far as our latency measurements are concerned, we find that even the basic message-passing machine can easily hide network latencies by using asynchronous message exchanges. <p> One way of combining synchronization and data transfer in the framework of a shared-memory architecture is through the use of full/empty bits on memory words <ref> [Agarwal et al. 91, Alverson et al. 90] </ref>, though we would argue that this approach can be very costly, and certainly is overkill for the needs of the C* compiler. <p> Machines like FLASH [Kuskin et al. 94] or Typhoon [Reinhardt et al. 94] would be ideal testbeds for such an approach, since their network interface is fully programmable. Another promising approach is to build hybrid architectures that support both shared-memory and message-passing, such as the Alewife <ref> [Agarwal et al. 91] </ref>. The challenge here 111 is to carefully integrate the different communication models in the compiler.
Reference: [Alverson et al. 90] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> In Proceedings of International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <year> 1990. </year>
Reference-contexts: We argue that those machines would benefit greatly from architectural support, such as adaptive or user-selectable cache coherence protocols [Carter et al. 91, Bennett et al. 92], full/empty bits on memory words <ref> [Agarwal et al. 91, Alverson et al. 90] </ref>, a network or network interface dedicated to barrier synchronization and reduction (e.g., the control network on the CM-5 [TMC 91b]), or direct access to message-passing primitives as implemented on the Alewife machine [Agarwal et al. 91]. <p> One way of combining synchronization and data transfer in the framework of a shared-memory architecture is through the use of full/empty bits on memory words <ref> [Agarwal et al. 91, Alverson et al. 90] </ref>, though we would argue that this approach can be very costly, and certainly is overkill for the needs of the C* compiler.
Reference: [Anderson et al. 91] <author> T. E. Anderson, H. M. Levy, B. N. Bershad, and E. D. Lazowska. </author> <title> Performance measurements on a 128-node Butterfly parallel processor. </title> <booktitle> In Proceedings of 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 108-120, </pages> <year> 1991. </year>
Reference-contexts: Since our compiler largely eliminates protocol overhead, the NI management overhead becomes more important. In older message-passing machines, such as the Intel Delta [Intel 91b], only the operating system can access the NI. With such a design, most message-passing operations therefore incur the additional cost of a system call <ref> [Anderson et al. 91] </ref>. More modern machines, such as the CM-5 [TMC 91b], use a network interface that can be safely accessed from user-mode. We assume that all network interfaces used in our study have this property as well.
Reference: [Archibald 88] <author> J. K. Archibald. </author> <title> A cache coherence approach for large multiprocessor systems. </title> <booktitle> In 1988 International Conference on Supercomputing, </booktitle> <pages> pages 337-345, </pages> <year> 1988. </year>
Reference: [Arlauskas 88] <author> R. Arlauskas. </author> <title> iPSC/2 System: A Second Generation Hypercube, </title> <month> January </month> <year> 1988. </year> <month> 113 </month>
Reference-contexts: Unfortunately, the cost of communication may limit the performance of parallel computers. To fully realize the advantages of parallel processing, we need to design efficient communication mechanisms. Existing communication architectures span a spectrum ranging from message passing <ref> [Arlauskas 88, Intel 91a, Dally 90, TMC 91b] </ref> to remote-memory access [Crowther et al. 85, Cray 93], shared memory [Sequent 87, Lenoski et al. 92, Agarwal et al. 91] and cache-only architectures [Hagersten 92a, KSR 92]. <p> Again, the performance of off-chip logic may be worse than dedicated on-chip hardware, but both designs provide the same functionality. Using off-the-shelf processors reduces overall design time for the machine and results in faster time-to-market. In fact, most existing parallel computers, such as Intel's series of message-passing machines <ref> [Arlauskas 88, Bokhari 90, Intel 91b, Intel 91a] </ref>, the Thinking Machines CM-5 [TMC 91b], or the Cray T3D [Cray 93] (a NUMA machine), use processing nodes built around a commercial off-the-shelf microprocessor. Instead of requiring special-purpose processor instructions, these machines control communication through hardware external to the processor. <p> The run-time system can use such a facility to force the CPU to receive and process a message immediately rather than waiting for the next time the receive FIFO fills up. Some machines, such as the iPSC/2 <ref> [Arlauskas 88] </ref>, include DMA hardware to facilitate transferring data between memory and the NI. However, the CPU must still initiate all DMA 85 transfers.
Reference: [Baer & Chen 91] <author> J. L. Baer and T. F. Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <booktitle> In Proceedings Supercomputing '91, </booktitle> <pages> pages 176-186, </pages> <year> 1991. </year>
Reference: [Barrusio 94] <author> R. Barrusio, </author> <year> 1994. </year> <title> Usenet Communication on comp.sys.super. </title>
Reference-contexts: The write messages, of course, propagate through the data network which has much higher latency than the dedicated barrier network <ref> [Barrusio 94] </ref>. This drastically limits the potential performance gains from the dedicated network. Protection Parallel machines that support time- or space-sharing among multiple users must address the issue of protecting different jobs from each other. One user's job should not be allowed to send messages to another user's job.
Reference: [Bennett et al. 90] <author> J. K. Bennett, J. B. Carter, and W. Zwaenepoel. </author> <title> Adaptive software cache management. </title> <booktitle> In Proceedings of 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 125-134, </pages> <year> 1990. </year>
Reference: [Bennett et al. 92] <author> J. K. Bennett, S. Dwarkadas, J. Greenwood, and E. Speight. </author> <title> Willow: a scalable shared memory multiprocessor. </title> <booktitle> In Proceedings. Supercomputing '92, </booktitle> <pages> pages 336-345, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: For example, the results show that a significant fraction of the total traffic used by the shared-memory architectures for these benchmarks is explicit synchronization. We argue that those machines would benefit greatly from architectural support, such as adaptive or user-selectable cache coherence protocols <ref> [Carter et al. 91, Bennett et al. 92] </ref>, full/empty bits on memory words [Agarwal et al. 91, Alverson et al. 90], a network or network interface dedicated to barrier synchronization and reduction (e.g., the control network on the CM-5 [TMC 91b]), or direct access to message-passing primitives as implemented on the
Reference: [Blumrich et al. 94] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> To appear in Proceedings of 1994 International Symposium on Computer Architecture, </booktitle> <year> 1994. </year>
Reference-contexts: Given that the coprocessor is identical to the compute processor, it may be more efficient to use it for general computation, performing communication through the kernel on both processors. The Shrimp architecture <ref> [Blumrich et al. 94] </ref> implements a low-overhead data transport mechanism by marking memory pages as mapped out; store operations to those pages cause the written data to be automatically forwarded to the memory of another node. The mapping, established by the OS kernel, specifies destination node and address.
Reference: [Bokhari 90] <author> S. Bokhari. </author> <title> Communication overhead on the intel iPSC-860 hypercube. </title> <type> Technical Report Interim Report 10, </type> <institution> ICASE, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: Again, the performance of off-chip logic may be worse than dedicated on-chip hardware, but both designs provide the same functionality. Using off-the-shelf processors reduces overall design time for the machine and results in faster time-to-market. In fact, most existing parallel computers, such as Intel's series of message-passing machines <ref> [Arlauskas 88, Bokhari 90, Intel 91b, Intel 91a] </ref>, the Thinking Machines CM-5 [TMC 91b], or the Cray T3D [Cray 93] (a NUMA machine), use processing nodes built around a commercial off-the-shelf microprocessor. Instead of requiring special-purpose processor instructions, these machines control communication through hardware external to the processor.
Reference: [Brustoloni & Bershad 92] <author> J. C. Brustoloni and B. N. Bershad. </author> <title> Simple protocol processing for high-bandwidth low-latency networking. </title> <type> Technical Report CMU-CS-93-132, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: Usually, the sender must save a copy of each message until it is acknowledged; this in turn may incur overhead for managing the required buffer space. Brustoloni and Bershad <ref> [Brustoloni & Bershad 92] </ref> have developed an efficient protocol for ATM-based networks that can handle message loss. For our studies, we assume that the communication primitives used by the programmer or compiler are reliable, i.e., the network neither loses nor corrupts messages, and it does not introduce spurious messages.
Reference: [Callahan et al. 91] <author> D. Callahan, K. Kennedy, and A. Porterfield. </author> <title> Software prefetching. </title> <booktitle> In Proceedings of 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 40-52, </pages> <year> 1991. </year>
Reference: [Carter et al. 91] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year> <month> 114 </month>
Reference-contexts: For example, the results show that a significant fraction of the total traffic used by the shared-memory architectures for these benchmarks is explicit synchronization. We argue that those machines would benefit greatly from architectural support, such as adaptive or user-selectable cache coherence protocols <ref> [Carter et al. 91, Bennett et al. 92] </ref>, full/empty bits on memory words [Agarwal et al. 91, Alverson et al. 90], a network or network interface dedicated to barrier synchronization and reduction (e.g., the control network on the CM-5 [TMC 91b]), or direct access to message-passing primitives as implemented on the <p> In the CACHE model, this is not possible most cache coherence mechanisms are oblivious to the application's communication patterns, and even adaptive protocols <ref> [Carter et al. 91, Stenstrom et al. 93] </ref> may take some time to recognize a pattern; there is generally no way for the application to inform the hardware ahead of time of an upcoming communication pattern. <p> For example, write-invalidate protocols perform badly on synchronization operations. Even adaptive protocols <ref> [Carter et al. 91, Stenstrom et al. 93] </ref> may take some time to recognize a pattern; there is generally no way for the application to inform the hardware ahead of time of an upcoming communication pattern.
Reference: [Censier & Feautrier 78] <author> L. M. Censier and P. Feautrier. </author> <title> A new solution to coherence problems in multicache systems. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 1112-1118, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: The choice of interconnect may influence the design of the cache coherency protocol. For example, if the network supports efficient broadcast operations, then a snoopy cache coherence protocol (e.g. [McCreight 84]) can be used. Otherwise, directory-based protocols (such as <ref> [Censier & Feautrier 78] </ref>) are more attractive. . However, the difference 2 The Cray T3D uses an approach similar in spirit.
Reference: [Chapman et al. 92] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Vienna Fortran a Fortran language extension for distributed memory systems. </title> <editor> In J. Saltz and P. Mehro-tra, editors, </editor> <title> Languages, Compilers, and Run-time Environments for Distributed Memory Machines. </title> <publisher> Elsevier Press, </publisher> <year> 1992. </year>
Reference-contexts: There exists a significant body of work on C* as well as other data-parallel languages such as Force [Jordan 87], Dino [Rosing et al. 90], Kali [Koelbel & Mehrotra 91], Vienna Fortran <ref> [Chapman et al. 92] </ref> or High-Performance Fortran (HPF) [HPFF 93], to name just a few. This reflects the growing popularity of this type of language.
Reference: [Clark et al. 92] <author> T. W. Clark, R. von Hanxleden, K. Kennedy, C. Koelbel, and L. Scott. </author> <title> Evaluating parallel languages for molecular dynamics computations. </title> <booktitle> In Proceedings. Scalable High Performance Computing Conference SHPCC-92, </booktitle> <pages> pages 98-105, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: A general solution to this problem requires extra communication and computation at run-time, just to determine which nodes have to send which data items where <ref> [Clark et al. 92, von Hanxleden et al. 92] </ref>. To reduce this overhead, the run-time system can analyze a communication pattern once at run-time, and amortize the cost of the analysis over many reuses of the pattern [Wu et al. 91].
Reference: [Cox & Fowler 89] <author> A. Cox and R. Fowler. </author> <title> The implementation of a coherent memory abstraction on a NUMA multiprocessor. experiences with PLATINUM. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 32-44, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: In comparison, the message-passing architecture achieves its advantages with minimal communication hardware. The NUMA architecture suffers primarily from being limited to transferring at most one word per request. An efficient block transfer mechanism is essential for competitive performance (as was already noted in <ref> [Cox & Fowler 89] </ref>). Chapter 6 IMPROVING MESSAGE-PASSING The previous chapter has focused on the amount of interconnect traffic generated by the different architectures. In this chapter, we study a different metric, namely the CPU overhead required to send a message. Communication-related CPU overhead affects performance in two ways.
Reference: [Cray 93] <author> Cray Research, Inc., </author> <title> 2360 Pilot Knob Road, Mendota Heights, MN 55120. CRAY T3D System Architecture Overview Manual (HR-04033), </title> <year> 1993. </year>
Reference-contexts: Unfortunately, the cost of communication may limit the performance of parallel computers. To fully realize the advantages of parallel processing, we need to design efficient communication mechanisms. Existing communication architectures span a spectrum ranging from message passing [Arlauskas 88, Intel 91a, Dally 90, TMC 91b] to remote-memory access <ref> [Crowther et al. 85, Cray 93] </ref>, shared memory [Sequent 87, Lenoski et al. 92, Agarwal et al. 91] and cache-only architectures [Hagersten 92a, KSR 92]. <p> Using off-the-shelf processors reduces overall design time for the machine and results in faster time-to-market. In fact, most existing parallel computers, such as Intel's series of message-passing machines [Arlauskas 88, Bokhari 90, Intel 91b, Intel 91a], the Thinking Machines CM-5 [TMC 91b], or the Cray T3D <ref> [Cray 93] </ref> (a NUMA machine), use processing nodes built around a commercial off-the-shelf microprocessor. Instead of requiring special-purpose processor instructions, these machines control communication through hardware external to the processor. <p> In our work, we therefore examine different packet sizes. Special-Purpose Networks Some machines, such as the Thinking Machines CM-5 [TMC 91b] or the Cray T3D <ref> [Cray 93] </ref>, use dedicated networks for certain communication operations. For example, the CM-5 includes a control network that has been specifically optimized to perform efficient reduction and broadcast operations. Similarly, the T3D has a synchronization network that provides very low-latency barrier synchronization operations. <p> Another model, MSG blk, is based on MSG inf but attempts (at runtime) to aggregate multiple per-VP requests into a single message. Remote-Memory Model Our remote-memory (NUMA) model is based on architectures such as the BBN Butterfly [Crowther et al. 85] or the Cray T3D <ref> [Cray 93] </ref>. Memory is statically distributed among processors and there are no caches. Processors access memory through 1, 2, 4, or 8-byte load and store instructions. When a processor issues a memory reference, the hardware decides whether that reference is to local or remote memory.
Reference: [Crowther et al. 85] <author> W. Crowther, J. Goodhue, E. Starr, R. Thomas, W. Milliken, and T. Blackadar. </author> <title> Performance measurements on a 128-node Butterfly parallel processor. </title> <booktitle> In Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pages 531-540, </pages> <year> 1985. </year>
Reference-contexts: Unfortunately, the cost of communication may limit the performance of parallel computers. To fully realize the advantages of parallel processing, we need to design efficient communication mechanisms. Existing communication architectures span a spectrum ranging from message passing [Arlauskas 88, Intel 91a, Dally 90, TMC 91b] to remote-memory access <ref> [Crowther et al. 85, Cray 93] </ref>, shared memory [Sequent 87, Lenoski et al. 92, Agarwal et al. 91] and cache-only architectures [Hagersten 92a, KSR 92]. <p> Another model, MSG blk, is based on MSG inf but attempts (at runtime) to aggregate multiple per-VP requests into a single message. Remote-Memory Model Our remote-memory (NUMA) model is based on architectures such as the BBN Butterfly <ref> [Crowther et al. 85] </ref> or the Cray T3D [Cray 93]. Memory is statically distributed among processors and there are no caches. Processors access memory through 1, 2, 4, or 8-byte load and store instructions.
Reference: [Culler et al. 91a] <author> D. Culler, A. Sah, K. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine grain parallelism with minimal hardware support: A compiler-controlled treaded abstract machine. </title> <booktitle> In Proceedings of 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: In [Henry & Joerg 92b], the authors propose a network interface design that provides special support for Id [Nikhil 90] programs that have been compiled to Berkeley's Threaded Abstract Machine <ref> [Culler et al. 91a] </ref>. They reduce communication overhead by implementing message dispatching, forwarding and replying in hardware, and by mapping the network interface into the processor's general-purpose registers. Compared to our work, they target a much more fine-grained computation model, yet reach similar conclusions.
Reference: [Culler et al. 91b] <author> D. E. Culler, A. Sah, K. E. Schauser, T. von Eicken, and J. Wawrzynek. </author> <title> Fine-grain parallelism with minimal hardware support: A compiler-controlled 115 threaded abstract machine. </title> <booktitle> In Proceedings of 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-175, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: It would be useful to examine how the communication requirements for other programming models differ from those of data-parallel languages. For example, Henry and Joerg designed a NI for use with the TAM <ref> [Culler et al. 91b] </ref> model of execution and reached conclusions that are somewhat different from ours [Henry & Joerg 92a].
Reference: [Cypher et al. 93] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural requirements of parallel scientific applications with explicit communication. </title> <booktitle> In Proceedings of 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <year> 1993. </year>
Reference-contexts: Overall, there appears to be no clear consensus which of these approaches is better, or what packet sizes are desirable; in fact, the ideal packet size depends heavily on the workload <ref> [Cypher et al. 93] </ref>. In our work, we therefore examine different packet sizes. Special-Purpose Networks Some machines, such as the Thinking Machines CM-5 [TMC 91b] or the Cray T3D [Cray 93], use dedicated networks for certain communication operations.
Reference: [Dahlgren et al. 94] <author> F. Dahlgren, M. Dubois, and P. Stenstrom. </author> <title> Combined performance gains of simple cache protocol extensios. </title> <booktitle> In Proceedings of 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 187-197, </pages> <year> 1994. </year>
Reference: [Dally 90] <author> W. J. Dally. </author> <title> The J-machine system. </title> <editor> In P. Winston and S. Shellard, editors, </editor> <booktitle> Artificial Intelligence at MIT: Expanding Frontiers, </booktitle> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Unfortunately, the cost of communication may limit the performance of parallel computers. To fully realize the advantages of parallel processing, we need to design efficient communication mechanisms. Existing communication architectures span a spectrum ranging from message passing <ref> [Arlauskas 88, Intel 91a, Dally 90, TMC 91b] </ref> to remote-memory access [Crowther et al. 85, Cray 93], shared memory [Sequent 87, Lenoski et al. 92, Agarwal et al. 91] and cache-only architectures [Hagersten 92a, KSR 92].
Reference: [Eggers & Jeremiassen 91] <author> S. Eggers and T. Jeremiassen. </author> <title> Eliminating false sharing. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages I:377-381, </pages> <month> August </month> <year> 1991. </year>
Reference: [Felten 93a] <author> E. W. Felten. </author> <title> Protocol Compilation: High-Performance Communication for Parallel Programs. </title> <type> PhD dissertation, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, WA 98195, </address> <month> September </month> <year> 1993. </year> <note> Available as technical report 93-09-09. </note>
Reference-contexts: For message-passing systems, researchers have largely focused on reducing the high per-message overhead typically found in message-passing systems. For example, active messages [von Eicken et al. 92] are a low-level transport mechanism that achieves low latency by efficiently dispatching to a message handler on the receiving node. Felten <ref> [Felten 93a] </ref> proposes using a protocol compiler to custom-generate message-passing protocols for a given program and thus reduce protocol overhead. The above two approaches rely entirely on software techniques; however, hardware approaches have been suggested as well. <p> Unfortunately, the message-passing communication model also has several drawbacks. First, traditional message-passing incurs run-time protocol overhead, e.g., for managing the buffers required for the just-in-time delivery semantics. Felten has shown that protocol overhead degrades the performance of message-passing codes <ref> [Felten 93a] </ref>. Since the C* communication operations read and write parallel variables that are already allocated by the compiler, most of the buffer management overhead is completely unnecessary. <p> On the other hand, we have introduced explicit synchronization operations that were performed implicitly in the message-passing model. We can think of the synchronization operations as another form of protocol overhead, taking the place of the traditional message-passing protocol overhead <ref> [Felten 93a, Felten 94] </ref>. At this point, we make no attempt to quantify this tradeoff. In Chapter 6, we will see that the true advantage of compiling for a remote-memory model is that it uses a small set of simple, well-defined communication primitives. <p> A study by Felten of several scientific message-passing applications running on an iPSC/860 under NX/2 showed that these programs spend between 20% to 70% communicating, and an average of 33% of that time is communication overhead <ref> [Felten 93a] </ref>. Clearly, communication overhead can dramatically degrade the performance of parallel programs. We distinguish two types of CPU overhead, protocol and NI management overhead. The former is a result of the semantics of message-passing; it is largely independent of the design of the NI hardware. <p> We briefly review both kinds of CPU overhead. 6.1.1 Protocol Overhead Protocol overhead is an inevitable result of any form on inter-node communication <ref> [Felten 93a] </ref>. For example, nodes executing a parallel program must synchronize their actions in order to avoid race conditions; following [Felten 93a], we consider the associated work a form of protocol overhead. <p> We briefly review both kinds of CPU overhead. 6.1.1 Protocol Overhead Protocol overhead is an inevitable result of any form on inter-node communication <ref> [Felten 93a] </ref>. For example, nodes executing a parallel program must synchronize their actions in order to avoid race conditions; following [Felten 93a], we consider the associated work a form of protocol overhead. A major source of CPU overhead in traditional message-passing libraries is due to buffer management: receiving nodes must dynamically allocate buffer space for messages that arrive before the receiver has issued a matching msg recv call. <p> Between buffer management and other overheads due to the rich semantics of most message-passing libraries (e.g., message matching, implicit synchronization, etc.), the CPU overhead can be significant. 83 Felten <ref> [Felten 93a] </ref> has proposed a compiler-based approach for reducing the protocol overhead in data-parallel programs. The compiler analyzes the source program (which makes conventional message-passing calls) and extracts information about the program's communication pattern. <p> Active messages [von Eicken et al. 92] are a low-level transport mechanism that achieves low latency by efficiently dispatching to a message handler on the receiving node. Felten <ref> [Felten 93a] </ref> proposes using a protocol compiler to custom-generate message-passing protocols for a given program and thus reduce protocol overhead. Neither approach can 104 completely overcome inadequacies of existing network interface hardware. The Intel Paragon [Intel 91a] uses a second i860 microprocessor on each node to handle communication operations. <p> By using a high-level parallel compiler, it is possible to exploit information about communication patterns and perform at compile-time many of the tasks traditionally provided by a communication library. Specifically, the compiler can eliminate most of the protocol overhead of traditional message-passing libraries <ref> [Felten 93a] </ref>. However, the communication primitives offered by traditional message-passing network interfaces do not match well the needs of compilers. We have found that implementations of C* on traditional message-passing hardware require significant CPU overhead for communication.
Reference: [Felten 93b] <author> E. W. Felten. </author> <title> Protocol Compilation: High-Performance Communication for Parallel Programs. </title> <type> PhD dissertation, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <address> Seattle, WA 98195, </address> <month> September </month> <year> 1993. </year> <note> Available as technical report 93-09-09. </note>
Reference-contexts: The NI hardware performs all the communication work and (assuming nonblocking caches) the CPU can proceed with local computations while the communication is in progress. In contrast, CPU overhead is very high in traditional message-passing machines, often over an order of magnitude higher than the interconnect latency <ref> [Felten 93b] </ref>. However, we have seen in the previous chapter that the message-passing model has many desirable features. <p> However, the software cost of sending and receiving data in a message-passing machine is generally high, negating any possible performance advantages <ref> [Felten 93b] </ref>. Our goal is to reduce the overhead of data transfer and synchronization in message-passing distributed-memory architectures.
Reference: [Felten 94] <author> E. W. Felten, </author> <year> 1994. </year> <type> Personal Communication. </type>
Reference-contexts: On the other hand, we have introduced explicit synchronization operations that were performed implicitly in the message-passing model. We can think of the synchronization operations as another form of protocol overhead, taking the place of the traditional message-passing protocol overhead <ref> [Felten 93a, Felten 94] </ref>. At this point, we make no attempt to quantify this tradeoff. In Chapter 6, we will see that the true advantage of compiling for a remote-memory model is that it uses a small set of simple, well-defined communication primitives.
Reference: [Fox 88] <author> G. C. Fox. </author> <title> What have we learnt from using real parallel machines to solve real problems. </title> <booktitle> In Proceedings of Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 897-955, </pages> <year> 1988. </year> <month> 116 </month>
Reference-contexts: To make this task manageable, we focus on the class of data-parallel languages, and we pick the C* language as one representative for our experiments. The data-parallel model is an important one; a study by Fox <ref> [Fox 88] </ref> has shown that a majority of existing scientific applications fit that model well. As a framework for our architectural studies, we concentrate on MIMD parallel computers and three competing communication architectures message-passing, remote-memory access and cache-coherent shared-memory. The core of this dissertation consists of two parts. <p> This execution model avoids race conditions and thus greatly simplifies the understanding and debugging of data-parallel programs. While the data-parallel model of execution is not as general as arbitrary MIMD computation, it is nonetheless a very powerful model. A study by Fox <ref> [Fox 88] </ref> showed that 70 out of 84 scientific applications studied, or over 80%, fit the data-parallel model.
Reference: [Frank & Vernon 93] <author> M. I. Frank and M. K. Vernon. </author> <title> A hybrid shared memory / message passing parallel machine. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <pages> pages I:232-237, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: As a result, shared-memory systems take on some of the characteristics of message-passing systems (where data movement is entirely under explicit application control). A related approach <ref> [Frank & Vernon 93] </ref> integrates message passing and shared memory by introducing a new cache line state, possibly-stale, into a conventional cache coherence protocol. The proposed architecture lets user programs move data between nodes without the overhead of cache coherence operations. <p> The results of the experiments are expressed in terms of execution time on the Alewife, and are therefore somewhat specific to that implementation, though their conclusions agree with ours. A related approach <ref> [Frank & Vernon 93] </ref> integrates message passing and shared memory 77 by introducing a new cache line state, possibly-stale, into a conventional cache coherence protocol. The proposed architecture permits data to be moved between nodes without the overhead of cache coherence operations.
Reference: [Fu et al. 92] <author> J. W. C. Fu, J. H. Patel, and B. L. Janssens. </author> <title> Stride directed prefetching in scalar processors. </title> <booktitle> In 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 102-110, </pages> <year> 1992. </year>
Reference: [Gharachorloo et al. 90] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <year> 1990. </year>
Reference-contexts: Clearly, this can be a problem for application programs. For example, it is harder to implement efficiently a sequentially consistent shared-memory system if data packets can be delivered out of order [Thapar et al. 93]. Weaker memory consistency models <ref> [Hutto & Ahamad 90, Gharachorloo et al. 90] </ref> may be able to tolerate out-of-order delivery more easily. <p> In effect, this is a software implementation of a relaxed memory consistency model, similar to release consistency <ref> [Gharachorloo et al. 90] </ref>. Several optimizations can further reduce the cost of synchronization. First, for some communication patterns that are amenable to static analysis, point-to-point synchronization can be used instead of barrier synchronization. <p> For example, the cache coherence protocol could be made adaptive [Archibald 88, Stenstrom et al. 93, Bennett et al. 90, Carter et al. 91], or it could implement a weaker memory consistency model <ref> [Hutto & Ahamad 90, Gharachorloo et al. 90] </ref>. In this section, we evaluate our three communication architectures with respect to the amount of communication latency they incur. <p> For the latter technique, we assume a relaxed memory consistency model, similar to release consistency <ref> [Gharachorloo et al. 90] </ref>. Figure 5.15 summarizes the different models. Note that the study of latency reducing techniques for cache-coherent shared-memory machines is beyond the scope of this dissertation, so our list is necessarily incomplete. <p> Uses a write-invalidate protocol and implements sequential consistency. CACHE 32 wu As CACHE 32, but can selectively use a write-update proto col where beneficial. CACHE 32 aw Uses asynchronous propagation of writes, and implements a relaxed memory consistency model similar to release consis tency <ref> [Gharachorloo et al. 90] </ref>. CACHE 32 pre Uses sequential prefetching of data for reads: only the first of a series of sequential cache line accesses incurs latency. lowers the latency of those operations. <p> The individual write operations are effectively pipelined; the processor only has to wait at the next synchronization point for the last write to be acknowledged. This is similar to the release consistency protocol described in <ref> [Gharachorloo et al. 90] </ref>. Note that CACHE aw still uses an invalidation-based protocol, so a consumer of newly written data still incurs the latency of requesting the data from the producer.
Reference: [Gupta 89] <author> R. Gupta. </author> <title> The fuzzy barrier: A mechanism for the high speed synchronization of processors. </title> <booktitle> In Proceedings of 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 54-63, </pages> <year> 1989. </year>
Reference-contexts: The reason for this is that, as discussed before, the MSG model can sometimes combine data transfer and synchronization, hence there are fewer synchronization operations to perform in the first place. Note that our compiler does not currently use fuzzy barriers <ref> [Gupta 89] </ref>.
Reference: [Hagersten 92a] <author> E. Hagersten. </author> <title> DDM a cache-only memory architecture. </title> <booktitle> Computer, </booktitle> <pages> pages 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Existing communication architectures span a spectrum ranging from message passing [Arlauskas 88, Intel 91a, Dally 90, TMC 91b] to remote-memory access [Crowther et al. 85, Cray 93], shared memory [Sequent 87, Lenoski et al. 92, Agarwal et al. 91] and cache-only architectures <ref> [Hagersten 92a, KSR 92] </ref>. These communication architectures are often used directly by the programmer a fact that has influenced their design, much as assembly language programming has influenced the design of CISC instruction sets. <p> A different approach is taken in so-called COMA (Cache-Only Memory Architecture) shared-memory machines, such as the DDM <ref> [Hagersten 92a] </ref> or the KSR-1 [KSR 92].
Reference: [Hagersten 92b] <author> E. Hagersten. </author> <title> Toward Scalable Cache Only Memory Architectures. </title> <type> PhD dissertation, </type> <institution> Swedish Institute of Computer Science, </institution> <month> October </month> <year> 1992. </year> <title> SICS Dissertation Series 08. </title>
Reference-contexts: Request messages contain enough information for the remote node to create a reply message. Cache-Coherent Model For the cache-coherent shared-memory model (CACHE), we examine several different variants. For most of our results in this chapter, we show measurements for a COMA (cache-only memory architecture <ref> [Hagersten 92b] </ref>) roughly based on the KSR-1 [KSR 92], which assumes an interconnect capable of broadcast. 1 We also simulate an interconnect similar to the DASH [Lenoski et al. 92], where, based on its address, a memory block is assigned a home processor that handles all requests for that memory block.
Reference: [Hatcher & Quinn 91] <author> P. J. Hatcher and M. J. Quinn. </author> <title> Data-Parallel Programming on MIMD Computers. </title> <publisher> MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: We particularly highlight the similarity between C* and HPF in the next section. Significant work has been done on the compilation of an older version of C* (defined in [Rose & Steele Jr. 87]) for both distributed-memory and shared-memory multiprocessors; see for example <ref> [Hatcher & Quinn 91] </ref> for a summary. For a detailed description of the current language, the reader is referred to [TMC 91a]; we give a brief overview here. C* distinguishes between scalar and parallel variables; the latter have a shape associated with them that describes how the data is organized. <p> 1 In practice, one would presumably inline most of the communication operations in order to obtain better performance, but this was not necessary for the purpose of this dissertation. 26 4.1 Alternative Communication Models Hatcher and Quinn describe two alternative approaches for compiling C* for shared-memory and distributed-memory targets, respectively <ref> [Hatcher & Quinn 91] </ref>. In this section, we briefly review and contrast their approaches, and then outline the compilation strategy of the C* compiler used in this dissertation. 4.1.1 Compiling for a Shared-Memory Target In a shared-memory architecture, all nodes can access each other's memory through load and store instructions. <p> compiler must perform three major tasks: * generate code for parallel computation and manage the virtual processor model * generate code for inter-node communication * insert synchronization to maintain inter-node data dependencies. 3 The original C* compiler by Hatcher and Quinn stored parallel variables as contiguous arrays in shared memory <ref> [Hatcher & Quinn 91] </ref>; it therefore did not address the issue of false sharing. 33 4.2.1 Parallel Computation We first describe how to generate code for parallel statements (except communication), and how the virtual processor model is managed.
Reference: [Hatcher et al. 91] <author> P. J. Hatcher, M. J. Quinn, and B. K. Seevers. </author> <title> Implementing a data-parallel language on a tightly coupled multiprocessor. </title> <booktitle> In Proc. 3rd Workshop Programming Languages Compilers Parallel Computers, </booktitle> <year> 1991. </year>
Reference-contexts: This greatly simplifies how the compiler handles C* communication operations, since C* get and send operations translate directly into load and store instructions. However, the compiler must insert explicit synchronization operations in the generated code to prevent race conditions. Hatcher and Quinn describe this approach in <ref> [Hatcher et al. 91] </ref>. Consider the C* code in Figure 4.2a, where each VP sends a copy of its y variable to its left neighbor, which stores the data in its x variable. <p> The minimal set of barriers can be found in linear time for sequential code, but for general control flow graphs, this is an NP-hard problem <ref> [Hatcher et al. 91] </ref>. Our compiler uses a set of heuristics to achieve near-optimal barrier placement in practice, e.g., it places barriers before rather than inside loops whenever possible. <p> When compiling for a distributed-memory target, we express communication in terms of remote memory accesses instead of message exchanges. This approach, similar to Hatcher and Quinn's compilation strategy for shared-memory machines <ref> [Hatcher et al. 91] </ref>, avoids most of the overheads associated with traditional message-passing. The more significant advantage of our approach is that communication is performed by a small set of well-defined primitives which are much less general than message-passing.
Reference: [Henry & Joerg 92a] <author> D. S. Henry and C. F. Joerg. </author> <title> A tightly-coupled processor-network interface. </title> <booktitle> In Proceedings of 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 111-122, </pages> <month> October </month> <year> 1992. </year> <month> 117 </month>
Reference-contexts: A custom processor design can improve communication performance; for example, the architect can integrate the network interface more tightly with the CPU <ref> [Henry & Joerg 92a] </ref>, or include special-purpose communication instructions in the CPU. The Kendall Square KSR-1 shared-memory computer [KSR 92] uses both approaches; its processors provide instructions for prefetching or post-storing cache lines, plus a host of instructions that control the memory system, especially the caching strategy. <p> It would be useful to examine how the communication requirements for other programming models differ from those of data-parallel languages. For example, Henry and Joerg designed a NI for use with the TAM [Culler et al. 91b] model of execution and reached conclusions that are somewhat different from ours <ref> [Henry & Joerg 92a] </ref>.
Reference: [Henry & Joerg 92b] <author> D. S. Henry and C. F. Joerg. </author> <title> A tightly-coupled processor-network interface. </title> <booktitle> In Proceedings of 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 111-122, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: Though our approach is similar in nature, our emphasis lies on user-level communication and compiler support for parallel programming; their emphasis is on distributed applications. In <ref> [Henry & Joerg 92b] </ref>, the authors propose a network interface design that provides special support for Id [Nikhil 90] programs that have been compiled to Berkeley's Threaded Abstract Machine [Culler et al. 91a].
Reference: [HPFF 93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <note> Version 1.0, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: There exists a significant body of work on C* as well as other data-parallel languages such as Force [Jordan 87], Dino [Rosing et al. 90], Kali [Koelbel & Mehrotra 91], Vienna Fortran [Chapman et al. 92] or High-Performance Fortran (HPF) <ref> [HPFF 93] </ref>, to name just a few. This reflects the growing popularity of this type of language. The following two properties are shared by almost all (imperative) data-parallel languages. * Parallelism is obtained by performing similar (or identical) operations in parallel on the elements of a large data set.
Reference: [Hutto & Ahamad 90] <author> P. W. Hutto and M. Ahamad. </author> <title> Slow memory: Weakening consistency to enhance concurrency in distributed shared memories. </title> <booktitle> In Proceedings of 10th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 302-309, </pages> <year> 1990. </year>
Reference-contexts: Clearly, this can be a problem for application programs. For example, it is harder to implement efficiently a sequentially consistent shared-memory system if data packets can be delivered out of order [Thapar et al. 93]. Weaker memory consistency models <ref> [Hutto & Ahamad 90, Gharachorloo et al. 90] </ref> may be able to tolerate out-of-order delivery more easily. <p> For example, the cache coherence protocol could be made adaptive [Archibald 88, Stenstrom et al. 93, Bennett et al. 90, Carter et al. 91], or it could implement a weaker memory consistency model <ref> [Hutto & Ahamad 90, Gharachorloo et al. 90] </ref>. In this section, we evaluate our three communication architectures with respect to the amount of communication latency they incur.
Reference: [Intel 91a] <institution> Intel Supercomputer Systems Division. Paragon XP/S Product Overview, </institution> <year> 1991. </year>
Reference-contexts: Unfortunately, the cost of communication may limit the performance of parallel computers. To fully realize the advantages of parallel processing, we need to design efficient communication mechanisms. Existing communication architectures span a spectrum ranging from message passing <ref> [Arlauskas 88, Intel 91a, Dally 90, TMC 91b] </ref> to remote-memory access [Crowther et al. 85, Cray 93], shared memory [Sequent 87, Lenoski et al. 92, Agarwal et al. 91] and cache-only architectures [Hagersten 92a, KSR 92]. <p> Again, the performance of off-chip logic may be worse than dedicated on-chip hardware, but both designs provide the same functionality. Using off-the-shelf processors reduces overall design time for the machine and results in faster time-to-market. In fact, most existing parallel computers, such as Intel's series of message-passing machines <ref> [Arlauskas 88, Bokhari 90, Intel 91b, Intel 91a] </ref>, the Thinking Machines CM-5 [TMC 91b], or the Cray T3D [Cray 93] (a NUMA machine), use processing nodes built around a commercial off-the-shelf microprocessor. Instead of requiring special-purpose processor instructions, these machines control communication through hardware external to the processor. <p> Felten [Felten 93a] proposes using a protocol compiler to custom-generate message-passing protocols for a given program and thus reduce protocol overhead. Neither approach can 104 completely overcome inadequacies of existing network interface hardware. The Intel Paragon <ref> [Intel 91a] </ref> uses a second i860 microprocessor on each node to handle communication operations. However, the network interface is not accessible at the user level and all communication must pass through the coprocessor.
Reference: [Intel 91b] <institution> Intel Supercomputer Systems Division. </institution> <note> A Touchstone DELTA System Description, </note> <month> February </month> <year> 1991. </year>
Reference-contexts: Again, the performance of off-chip logic may be worse than dedicated on-chip hardware, but both designs provide the same functionality. Using off-the-shelf processors reduces overall design time for the machine and results in faster time-to-market. In fact, most existing parallel computers, such as Intel's series of message-passing machines <ref> [Arlauskas 88, Bokhari 90, Intel 91b, Intel 91a] </ref>, the Thinking Machines CM-5 [TMC 91b], or the Cray T3D [Cray 93] (a NUMA machine), use processing nodes built around a commercial off-the-shelf microprocessor. Instead of requiring special-purpose processor instructions, these machines control communication through hardware external to the processor. <p> This generally involves reading and writing data and control words from and into memory-mapped NI registers, moving data between memory and the NI, fielding interrupts, etc. Since our compiler largely eliminates protocol overhead, the NI management overhead becomes more important. In older message-passing machines, such as the Intel Delta <ref> [Intel 91b] </ref>, only the operating system can access the NI. With such a design, most message-passing operations therefore incur the additional cost of a system call [Anderson et al. 91]. More modern machines, such as the CM-5 [TMC 91b], use a network interface that can be safely accessed from user-mode. <p> Figure 6.1 shows a traditional design for a message-passing architecture; machines like the Intel Delta <ref> [Intel 91b] </ref> or Thinking Machines CM-5 [TMC 91b] use this basic network interface design. The NI itself consists of little more than two FIFOs (one to receive data from the network and one to hold data that is to be injected into the network) and simple control circuitry.
Reference: [Jordan 87] <author> H. Jordan. </author> <title> The Force. </title> <type> Technical Report ECE 87-1-1, </type> <institution> Dept. of Electrical and Computer Engineering University of Colorado, </institution> <month> January </month> <year> 1987. </year>
Reference-contexts: Chapter 3 DATA-PARALLEL LANGUAGES We have chosen C* [Rose & Steele Jr. 87, TMC 90], a data-parallel language, as the high-level parallel language for our experiments. There exists a significant body of work on C* as well as other data-parallel languages such as Force <ref> [Jordan 87] </ref>, Dino [Rosing et al. 90], Kali [Koelbel & Mehrotra 91], Vienna Fortran [Chapman et al. 92] or High-Performance Fortran (HPF) [HPFF 93], to name just a few. This reflects the growing popularity of this type of language.
Reference: [Jouppi 93] <author> N. P. Jouppi. </author> <title> Cache write policies and performance. </title> <booktitle> In Proceedings of 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 191-201, </pages> <year> 1993. </year>
Reference-contexts: In the write-update protocol several nodes may hold writable copies; whenever a node writes (part of) a cache line, the changed words are forwarded to all other nodes holding a copy. We assume that a write-cache <ref> [Jouppi 93] </ref> is used, i.e., successive writes to the same cache line generate a single forwarding message.
Reference: [Ju & Dietz 91] <author> Y. Ju and H. Dietz. </author> <title> Reduction of cache coherence overhead by compiler data layout and loop transformation. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Fourth Workshop on Languages and Compilers for Parallelism. </booktitle> <publisher> Springer Verlag, </publisher> <month> August </month> <year> 1991. </year>
Reference: [Klaiber & Frankel 93] <author> A. C. Klaiber and J. L. Frankel. </author> <title> Comparing data-parallel and message-passing paragigms. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <pages> pages II:11-II:20, </pages> <year> 1993. </year>
Reference-contexts: While the data-parallel model of execution is not as general as arbitrary MIMD computation, it is nonetheless a very powerful model. A study by Fox [Fox 88] showed that 70 out of 84 scientific applications studied, or over 80%, fit the data-parallel model. Klaiber and Frankel <ref> [Klaiber & Frankel 93] </ref> have demonstrated that even an application that intuitively does not seem to fit the data-parallel model a distributed event-driven simulation can be expressed cleanly and efficiently in a data-parallel language. <p> One drawback of C* (or similar languages) is that sometimes the synchronous semantics overly constrain a solution, as one cannot efficiently express arbitrary asynchronous operations. Nave compilation may exacerbate this problem, but some language extensions and compiler techniques can alleviate the problem <ref> [Klaiber & Frankel 93] </ref>. 3.2 C* versus HPF The C* language originated as a language for SIMD architectures, so there may be concerns over how well it represents other data-parallel languages. In this section, we compare C* with the more recently developed High Performance Fortran (HPF), another data-parallel language. <p> In <ref> [Klaiber & Frankel 93] </ref> the authors discuss other optimizations that reduce the cost of VP emulation. We reiterate that the experiments described later in this dissertation focus on communication operations rather than efficient code generation for local (i.e., non-communication) operations.
Reference: [Klaiber & Levy 91] <author> A. C. Klaiber and H. M. Levy. </author> <title> An architecture for software-controlled data prefetching. </title> <booktitle> In Proceedings of 18th International Symposium on Computer Architecture, </booktitle> <pages> pages 43-53, </pages> <month> May </month> <year> 1991. </year> <month> 118 </month>
Reference: [Koelbel & Mehrotra 91] <author> C. Koelbel and P. Mehrotra. </author> <title> Compiling global name-space parallel loops for distributed execution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: There exists a significant body of work on C* as well as other data-parallel languages such as Force [Jordan 87], Dino [Rosing et al. 90], Kali <ref> [Koelbel & Mehrotra 91] </ref>, Vienna Fortran [Chapman et al. 92] or High-Performance Fortran (HPF) [HPFF 93], to name just a few. This reflects the growing popularity of this type of language.
Reference: [Konstantinidou & Snyder 91] <author> S. Konstantinidou and L. Snyder. </author> <title> Chaos router: Architecture and performance. </title> <booktitle> In Proceedings of 18th International Symposium on Computer Architecture, </booktitle> <pages> pages 212-221, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Adaptive routing algorithms have the advantage that they may use the interconnect's aggregate bandwidth more efficiently <ref> [Ngai & Seitz 89, Snyder 92, Konstantinidou & Snyder 91] </ref>. However, they also do not generally guarantee FIFO delivery of messages; in other words, if node A sends messages m 1 and m 2 to node B in that order, the messages may arrive in reverse order at node B.
Reference: [Kranz et al. 93] <author> D. Kranz, K. Johnson, A. Agarwal, J. Kubiatowicz, and B.-H. Lim. </author> <title> Integrating message-passing and shared-memory: Early experience. </title> <booktitle> In Proceedings of Fourth SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <year> 1993. </year>
Reference-contexts: This is similar in nature to what happens in message-passing systems where copies of remote data are created under the control of the application program. Yet another approach, taken by the Alewife machine, is to offer both a shared-memory system and message-passing primitives. In <ref> [Kranz et al. 93] </ref>, the authors identify several scenarios where a compiler or programmer could implement operations more cheaply through message passing than through shared memory. Finally, the FLASH [Kuskin et al. 94] and Typhoon [Reinhardt et al. 94] shared-memory architectures include fully programmable network interfaces. <p> Similar research comparing the performance of shared-memory and message-passing implementations of a standard cell router was performed by [Martonosi & Gupta 89]. This study, too, focused on the programming style, not on the architectural mechanisms. In <ref> [Kranz et al. 93] </ref>, the authors argue that traditional shared-memory machines suffer from the limitations of shared memory as the only communication mechanism available. They identify several scenarios where a compiler or programmer could implement operations more cheaply through message passing than through shared memory.
Reference: [KSR 92] <institution> Kendall Square Research. </institution> <type> KSR-1 Technical Summary, </type> <year> 1992. </year>
Reference-contexts: Existing communication architectures span a spectrum ranging from message passing [Arlauskas 88, Intel 91a, Dally 90, TMC 91b] to remote-memory access [Crowther et al. 85, Cray 93], shared memory [Sequent 87, Lenoski et al. 92, Agarwal et al. 91] and cache-only architectures <ref> [Hagersten 92a, KSR 92] </ref>. These communication architectures are often used directly by the programmer a fact that has influenced their design, much as assembly language programming has influenced the design of CISC instruction sets. <p> For example, researchers have studied adaptive or user/compiler selectable cache coherence mechanisms that use different coherency protocols for different sharing patterns [Carter et al. 91, Bennett et al. 92, Stenstrom et al. 93]. Some machines like the KSR-1 <ref> [KSR 92] </ref> provide processor instructions to prefetch or poststore data, or load data in a state that facilitates future writes. Most of these techniques try to improve performance by giving the application more explicit control over how and when data is moved between processing nodes. <p> A custom processor design can improve communication performance; for example, the architect can integrate the network interface more tightly with the CPU [Henry & Joerg 92a], or include special-purpose communication instructions in the CPU. The Kendall Square KSR-1 shared-memory computer <ref> [KSR 92] </ref> uses both approaches; its processors provide instructions for prefetching or post-storing cache lines, plus a host of instructions that control the memory system, especially the caching strategy. <p> A different approach is taken in so-called COMA (Cache-Only Memory Architecture) shared-memory machines, such as the DDM [Hagersten 92a] or the KSR-1 <ref> [KSR 92] </ref>. <p> Cache-Coherent Model For the cache-coherent shared-memory model (CACHE), we examine several different variants. For most of our results in this chapter, we show measurements for a COMA (cache-only memory architecture [Hagersten 92b]) roughly based on the KSR-1 <ref> [KSR 92] </ref>, which assumes an interconnect capable of broadcast. 1 We also simulate an interconnect similar to the DASH [Lenoski et al. 92], where, based on its address, a memory block is assigned a home processor that handles all requests for that memory block.
Reference: [Kuskin et al. 94] <author> J. Kuskin, D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachor-loo, J. Chapin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, and J. Hennessy. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> To appear in Proceedings of 1994 International Symposium on Computer Architecture, </booktitle> <year> 1994. </year>
Reference-contexts: Yet another approach, taken by the Alewife machine, is to offer both a shared-memory system and message-passing primitives. In [Kranz et al. 93], the authors identify several scenarios where a compiler or programmer could implement operations more cheaply through message passing than through shared memory. Finally, the FLASH <ref> [Kuskin et al. 94] </ref> and Typhoon [Reinhardt et al. 94] shared-memory architectures include fully programmable network interfaces. In principle, this would allow coherence protocols to be tailored to specific applications; it is even conceivable to turn these machines into NUMA or message-passing machines simply by reprogramming the network interfaces. <p> We assume that the network provides two separate (virtual or physical) channels; by using one for requests and the other for replies we prevent cycles and hence deadlock. This approach to deadlock avoidance is similar to the one used in the DASH [Lenoski et al. 92] and FLASH <ref> [Kuskin et al. 94] </ref> multiprocessors. An interesting property of our design is that it can be readily used with a network that delivers messages out of order. Our compilation strategy guarantees that messages need only be ordered with respect to the synchronization points. <p> They reduce communication overhead by implementing message dispatching, forwarding and replying in hardware, and by mapping the network interface into the processor's general-purpose registers. Compared to our work, they target a much more fine-grained computation model, yet reach similar conclusions. The FLASH multiprocessor <ref> [Kuskin et al. 94] </ref> and Typhoon [Reinhardt et al. 94] use very flexible network interfaces built around a fully programmable microprocessor core. While the aim of these architectures is to support shared-memory systems, it should be possible to implement our communication primitives on their network interfaces. <p> We have already mentioned enhancements such as prefetching, compiler-selected coherence protocols, asynchronous write propagation, relaxed consistency models or full/empty bits. Machines like FLASH <ref> [Kuskin et al. 94] </ref> or Typhoon [Reinhardt et al. 94] would be ideal testbeds for such an approach, since their network interface is fully programmable. Another promising approach is to build hybrid architectures that support both shared-memory and message-passing, such as the Alewife [Agarwal et al. 91].
Reference: [Lenoski et al. 92] <author> D. Lenoski, K. Gharachorloo, J. Laudon, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: To fully realize the advantages of parallel processing, we need to design efficient communication mechanisms. Existing communication architectures span a spectrum ranging from message passing [Arlauskas 88, Intel 91a, Dally 90, TMC 91b] to remote-memory access [Crowther et al. 85, Cray 93], shared memory <ref> [Sequent 87, Lenoski et al. 92, Agarwal et al. 91] </ref> and cache-only architectures [Hagersten 92a, KSR 92]. These communication architectures are often used directly by the programmer a fact that has influenced their design, much as assembly language programming has influenced the design of CISC instruction sets. <p> In most existing shared-memory machines, all data in the system's caches is backed by main memory. In the case where main memory is physically distributed with the processing nodes, e.g. on the Stanford DASH <ref> [Lenoski et al. 92] </ref>, each cache line has a home node, namely the processing node holding the portion of main memory that backs the cache line. <p> For most of our results in this chapter, we show measurements for a COMA (cache-only memory architecture [Hagersten 92b]) roughly based on the KSR-1 [KSR 92], which assumes an interconnect capable of broadcast. 1 We also simulate an interconnect similar to the DASH <ref> [Lenoski et al. 92] </ref>, where, based on its address, a memory block is assigned a home processor that handles all requests for that memory block. <p> We assume that the network provides two separate (virtual or physical) channels; by using one for requests and the other for replies we prevent cycles and hence deadlock. This approach to deadlock avoidance is similar to the one used in the DASH <ref> [Lenoski et al. 92] </ref> and FLASH [Kuskin et al. 94] multiprocessors. An interesting property of our design is that it can be readily used with a network that delivers messages out of order. Our compilation strategy guarantees that messages need only be ordered with respect to the synchronization points.
Reference: [Leung & Zahorjan 93] <author> S. Leung and J. Zahorjan. </author> <title> Improving the performance of runtime parallelization. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 83-91, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: To reduce this overhead, the run-time system can analyze a communication pattern once at run-time, and amortize the cost of the analysis over many reuses of the pattern [Wu et al. 91]. Also, researchers have optimized the analysis phase itself <ref> [Leung & Zahorjan 93] </ref>. However, we wish to reduce this overhead further, even in cases where the above techniques are not applicable. 4.1.3 Our Compilation Strategy We now describe the strategy our compiler uses to generate code for shared-memory and distributed-memory target architectures.
Reference: [Lin & Snyder 90] <author> C. Lin and L. Snyder. </author> <title> A comparison of programming models for shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pages II:163-170, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: For a given size machine, and assuming equal technology parameters for the interconnect, one would expect message round-trip latencies to be lower in the DASH. 5.7 Related Work Several studies have examined the performance impact of shared-memory versus message-passing programming styles, e.g. <ref> [Lin & Snyder 90] </ref> and [Ngo & Snyder 92]. Their experiments, performed on different shared-memory machines, show that frequently the message-passing version of a program outperforms the shared-memory version, due to better locality.
Reference: [MacDonald & Barrusio 94] <author> T. MacDonald and R. Barrusio, </author> <year> 1994. </year> <type> Personal Communication. 119 </type>
Reference-contexts: Instead, the T3D dedicates a smaller number of high-order address bits to encode a node identifier, and each processing node uses a lookup table (the TLB Annex) <ref> [MacDonald & Barrusio 94] </ref> to translate this identifier into a full node number. 3 Such as design usually requires a very tight coupling between the NI and the processing node's memory system, so one could argue whether the caches are part of the NI or part of the processing node. <p> Third, we expect programs to send sequences of messages with the same type and options; the NI can provide a way to initialize the type and option word once, and send several messages using that control word. Finally, by using extra mapping hardware as on the Cray T3D <ref> [MacDonald & Barrusio 94] </ref> or Typhoon [Reinhardt et al. 94] we can implement NUMA-style access to remote memory though conventional load and store instructions. 93 6.4 Experimental Methodology We evaluate several variants of the two base architectures.
Reference: [Martonosi & Gupta 89] <author> M. Martonosi and A. Gupta. </author> <title> Tradeoffs in message-passing and shared-memory implementations of a standard cell router. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages III:88-96, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Their experiments, performed on different shared-memory machines, show that frequently the message-passing version of a program outperforms the shared-memory version, due to better locality. Similar research comparing the performance of shared-memory and message-passing implementations of a standard cell router was performed by <ref> [Martonosi & Gupta 89] </ref>. This study, too, focused on the programming style, not on the architectural mechanisms. In [Kranz et al. 93], the authors argue that traditional shared-memory machines suffer from the limitations of shared memory as the only communication mechanism available.
Reference: [McCreight 84] <author> E. McCreight. </author> <title> The dragon computer system: An early overview. </title> <type> Technical report, </type> <institution> Xerox Corp., </institution> <month> September </month> <year> 1984. </year>
Reference-contexts: Whenever the cache line is written, the changes are forwarded to all other nodes holding a copy of the line. The choice of interconnect may influence the design of the cache coherency protocol. For example, if the network supports efficient broadcast operations, then a snoopy cache coherence protocol (e.g. <ref> [McCreight 84] </ref>) can be used. Otherwise, directory-based protocols (such as [Censier & Feautrier 78]) are more attractive. . However, the difference 2 The Cray T3D uses an approach similar in spirit.
Reference: [Mellor-Crummey & Scott 91] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <pages> pages 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: The message-passing model implements the tree operations directly in terms of sending messages, whereas the NUMA and CACHE implementations are based on the lock-free barriers described in <ref> [Mellor-Crummey & Scott 91] </ref>, which achieve the theoretical minimum of (2p 2) remote accesses on p processors. 45 (a) Combined data transfer and synchronization in MSG. (b) Separate data transfer and synchronization in NUMA. broadcast Broadcasts are implemented using a fanout tree (essentially the second half of a reduction): the node
Reference: [Minzer 89] <author> S. E. Minzer. </author> <title> Broadband ISDN and Asynchronous Transfer Mode (ATM). </title> <journal> IEEE Communications Magazine, </journal> <volume> 27(9) 17-24,57, </volume> <month> September </month> <year> 1989. </year>
Reference-contexts: Reliability Different networks offer different degrees of reliability. For example, due to congestion or transmission errors, the network may lose or corrupt data packets, or it may deliver spurious messages; the network may or may not detect and report these conditions. ATM network switches <ref> [Minzer 89] </ref> are allowed to drop packets without notification if network congestion gets too high. If the network does not deal with such events, the communication endpoints (i.e., the NI or processing node) must execute a protocol that can handle them.
Reference: [Mowry & Gupta 91] <author> T. Mowry and A. Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference: [Ngai & Seitz 89] <author> J. Y. Ngai and C. L. Seitz. </author> <title> A framework for adaptive routing in mul-ticomputer networks. </title> <booktitle> In Proceedings of the 1989 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 1-9, </pages> <year> 1989. </year>
Reference-contexts: Adaptive routing algorithms have the advantage that they may use the interconnect's aggregate bandwidth more efficiently <ref> [Ngai & Seitz 89, Snyder 92, Konstantinidou & Snyder 91] </ref>. However, they also do not generally guarantee FIFO delivery of messages; in other words, if node A sends messages m 1 and m 2 to node B in that order, the messages may arrive in reverse order at node B.
Reference: [Ngo & Snyder 92] <author> T. Ngo and L. Snyder. </author> <title> On the influence of programming models on shared memory computer performance. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <pages> pages 284-291, </pages> <year> 1992. </year>
Reference-contexts: For a given size machine, and assuming equal technology parameters for the interconnect, one would expect message round-trip latencies to be lower in the DASH. 5.7 Related Work Several studies have examined the performance impact of shared-memory versus message-passing programming styles, e.g. [Lin & Snyder 90] and <ref> [Ngo & Snyder 92] </ref>. Their experiments, performed on different shared-memory machines, show that frequently the message-passing version of a program outperforms the shared-memory version, due to better locality. Similar research comparing the performance of shared-memory and message-passing implementations of a standard cell router was performed by [Martonosi & Gupta 89].
Reference: [Nikhil 90] <author> R. S. Nikhil. </author> <title> Id version 90.0 reference manual. </title> <type> Technical Report CSG Memo 284-1, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Though our approach is similar in nature, our emphasis lies on user-level communication and compiler support for parallel programming; their emphasis is on distributed applications. In [Henry & Joerg 92b], the authors propose a network interface design that provides special support for Id <ref> [Nikhil 90] </ref> programs that have been compiled to Berkeley's Threaded Abstract Machine [Culler et al. 91a]. They reduce communication overhead by implementing message dispatching, forwarding and replying in hardware, and by mapping the network interface into the processor's general-purpose registers.
Reference: [Pierce 88] <author> P. Pierce. </author> <title> The NX/2 operating system. </title> <booktitle> In Proceedings of Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 384-390. </pages> <publisher> ACM Press, </publisher> <year> 1988. </year> <month> 120 </month>
Reference-contexts: Other NI registers hold information about the status of the FIFOs or allow the processor to control the NI's mode of operation; e.g., whether or not to interrupt the CPU when the receive FIFO fills up. A traditional message-passing library (like Intel's NX <ref> [Pierce 88] </ref> or Thinking Machines' CMMD [TMC 92]) can be implemented on top of these primitives to provide higher levels of abstraction to the programmer. Some machines include DMA hardware to speed transfer of data between main memory and NI.
Reference: [Quinn et al. 88] <author> M. J. Quinn, P. J. Hatcher, and K. C. Jourdenais. </author> <title> Compiling C* programs for a hypercube multicomputer. </title> <booktitle> In Proc ACM/SIGPLAN PPEALS, </booktitle> <pages> pages 57-65, </pages> <year> 1988. </year>
Reference: [Reinhardt et al. 94] <author> S. Reinhardt, J. Larus, and D. Wood. Typhoon: </author> <title> A user-level shared-memory system. </title> <booktitle> To appear in Proceedings of 1994 International Symposium on Computer Architecture, </booktitle> <year> 1994. </year>
Reference-contexts: In [Kranz et al. 93], the authors identify several scenarios where a compiler or programmer could implement operations more cheaply through message passing than through shared memory. Finally, the FLASH [Kuskin et al. 94] and Typhoon <ref> [Reinhardt et al. 94] </ref> shared-memory architectures include fully programmable network interfaces. In principle, this would allow coherence protocols to be tailored to specific applications; it is even conceivable to turn these machines into NUMA or message-passing machines simply by reprogramming the network interfaces. <p> Finally, by using extra mapping hardware as on the Cray T3D [MacDonald & Barrusio 94] or Typhoon <ref> [Reinhardt et al. 94] </ref> we can implement NUMA-style access to remote memory though conventional load and store instructions. 93 6.4 Experimental Methodology We evaluate several variants of the two base architectures. The traditional NI design is the one shown in Figure 6.1 on page 85. <p> They reduce communication overhead by implementing message dispatching, forwarding and replying in hardware, and by mapping the network interface into the processor's general-purpose registers. Compared to our work, they target a much more fine-grained computation model, yet reach similar conclusions. The FLASH multiprocessor [Kuskin et al. 94] and Typhoon <ref> [Reinhardt et al. 94] </ref> use very flexible network interfaces built around a fully programmable microprocessor core. While the aim of these architectures is to support shared-memory systems, it should be possible to implement our communication primitives on their network interfaces. <p> We have already mentioned enhancements such as prefetching, compiler-selected coherence protocols, asynchronous write propagation, relaxed consistency models or full/empty bits. Machines like FLASH [Kuskin et al. 94] or Typhoon <ref> [Reinhardt et al. 94] </ref> would be ideal testbeds for such an approach, since their network interface is fully programmable. Another promising approach is to build hybrid architectures that support both shared-memory and message-passing, such as the Alewife [Agarwal et al. 91].
Reference: [Rose & Steele Jr. 87] <author> J. R. Rose and G. L. Steele Jr. </author> <title> C*: An extended C language for data parallel programming. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <volume> volume ii, </volume> <pages> pages 2-16, </pages> <year> 1987. </year>
Reference-contexts: Unless otherwise stated, we assume in our experiments that the underlying interconnection network is reliable (e.g., it does not lose, duplicate, or corrupt packets without signalling an error) and provides FIFO ordering. Chapter 3 DATA-PARALLEL LANGUAGES We have chosen C* <ref> [Rose & Steele Jr. 87, TMC 90] </ref>, a data-parallel language, as the high-level parallel language for our experiments. <p> Klaiber and Frankel [Klaiber & Frankel 93] have demonstrated that even an application that intuitively does not seem to fit the data-parallel model a distributed event-driven simulation can be expressed cleanly and efficiently in a data-parallel language. Several data-parallel languages, including an older version of C* <ref> [Rose & Steele Jr. 87] </ref>, originated as a programming language for SIMD machines. <p> We particularly highlight the similarity between C* and HPF in the next section. Significant work has been done on the compilation of an older version of C* (defined in <ref> [Rose & Steele Jr. 87] </ref>) for both distributed-memory and shared-memory multiprocessors; see for example [Hatcher & Quinn 91] for a summary. For a detailed description of the current language, the reader is referred to [TMC 91a]; we give a brief overview here.
Reference: [Rosing et al. 90] <author> M. Rosing, R. Schnabel, and R. Weaver. </author> <title> The Dino parallel programming language. </title> <type> Technical Report CU-CS-457-90, </type> <institution> Dept. of Computer Science, University of Colorado, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: Chapter 3 DATA-PARALLEL LANGUAGES We have chosen C* [Rose & Steele Jr. 87, TMC 90], a data-parallel language, as the high-level parallel language for our experiments. There exists a significant body of work on C* as well as other data-parallel languages such as Force [Jordan 87], Dino <ref> [Rosing et al. 90] </ref>, Kali [Koelbel & Mehrotra 91], Vienna Fortran [Chapman et al. 92] or High-Performance Fortran (HPF) [HPFF 93], to name just a few. This reflects the growing popularity of this type of language.
Reference: [Sequent 87] <institution> Sequent Computer Systems Incorporated. </institution> <note> Symmetry Technical Summary, rev. 1.5 edition, </note> <year> 1987. </year>
Reference-contexts: To fully realize the advantages of parallel processing, we need to design efficient communication mechanisms. Existing communication architectures span a spectrum ranging from message passing [Arlauskas 88, Intel 91a, Dally 90, TMC 91b] to remote-memory access [Crowther et al. 85, Cray 93], shared memory <ref> [Sequent 87, Lenoski et al. 92, Agarwal et al. 91] </ref> and cache-only architectures [Hagersten 92a, KSR 92]. These communication architectures are often used directly by the programmer a fact that has influenced their design, much as assembly language programming has influenced the design of CISC instruction sets.
Reference: [Snyder 92] <author> L. Snyder. </author> <title> Chaos router: finally, a practical adaptive router? In Parallel Architectures and Their Efficient Use. </title> <booktitle> First Heinz Nixdorf Symposium Proceedings, </booktitle> <pages> pages 146-155, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Adaptive routing algorithms have the advantage that they may use the interconnect's aggregate bandwidth more efficiently <ref> [Ngai & Seitz 89, Snyder 92, Konstantinidou & Snyder 91] </ref>. However, they also do not generally guarantee FIFO delivery of messages; in other words, if node A sends messages m 1 and m 2 to node B in that order, the messages may arrive in reverse order at node B.
Reference: [Stenstrom et al. 93] <author> P. Stenstrom, M. Brorsson, and L. Sandberg. </author> <title> An adaptive cache coherence protocol optimized for migratory sharing. </title> <booktitle> In Proceedings of 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 109-118, </pages> <year> 1993. </year>
Reference-contexts: In the CACHE model, this is not possible most cache coherence mechanisms are oblivious to the application's communication patterns, and even adaptive protocols <ref> [Carter et al. 91, Stenstrom et al. 93] </ref> may take some time to recognize a pattern; there is generally no way for the application to inform the hardware ahead of time of an upcoming communication pattern. <p> For example, write-invalidate protocols perform badly on synchronization operations. Even adaptive protocols <ref> [Carter et al. 91, Stenstrom et al. 93] </ref> may take some time to recognize a pattern; there is generally no way for the application to inform the hardware ahead of time of an upcoming communication pattern.
Reference: [Su et al. 93] <author> E. Su, D. J. Palermo, and P. Banerjee. </author> <title> Automating parallelization of regular computations for distributed-memory multicomputers in the Paradigm compiler. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <pages> pages II:39-46, </pages> <year> 1993. </year>
Reference-contexts: Compilers for data-parallel languages essentially produce SPMD (single-program, multiple-data) style code, a common programming paradigm on MIMD machines. Par-allelizing compilers for sequential languages, such as Paradigm <ref> [Su et al. 93] </ref>, generate similar code. In fact, since most parallelizing compilers exploit mainly data parallelism (e.g., by parallelizing loops), we expect programs generated this way to exhibit execution behavior similar to compiled programs written in high-level data-parallel languages. <p> The C* benchmarks we study do not require particularly elaborate data distributions, and would not benefit from HPF's added flexibility. Also, researchers have recently had great success in deriving data distributions automatically <ref> [Su et al. 93] </ref>; this may obviate HPF's complex data distribution specifications. To express communication, C* and HPF again use different syntax but very similar semantics. C* uses left index expressions to describe communication operations, whereas in HPF, any array index operation potentially causes communication.
Reference: [Thapar et al. 93] <author> M. Thapar, B. Delagi, and M. J. Flynn. </author> <title> Linked list cache coherence for scalabe shared memory. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 34-43, </pages> <year> 1993. </year> <month> 121 </month>
Reference-contexts: Clearly, this can be a problem for application programs. For example, it is harder to implement efficiently a sequentially consistent shared-memory system if data packets can be delivered out of order <ref> [Thapar et al. 93] </ref>. Weaker memory consistency models [Hutto & Ahamad 90, Gharachorloo et al. 90] may be able to tolerate out-of-order delivery more easily.
Reference: [Thekkath et al. 93] <author> C. A. Thekkath, H. M. Levy, and E. D. Lazowska. </author> <title> Efficient support for multicomputing on ATM networks. </title> <type> Technical Report 93-04-03, </type> <institution> University of Washington, Department of Computer Science & Engineering, </institution> <address> Seattle, WA 98195, </address> <year> 1993. </year>
Reference-contexts: The mapping, established by the OS kernel, specifies destination node and address. Our work focuses more specifically on the needs of a parallel compiler, and on the separation of data transport and synchronization. <ref> [Thekkath et al. 93] </ref> propose a remote memory access model instead of traditional message-passing for streamlining data and control transfer between workstations on a local area network.
Reference: [TMC 90] <institution> Thinking Machines Corp., </institution> <address> 245 First St., Cambridge MA 02142. </address> <note> C* Programming Guide, Version 6.0, </note> <month> November </month> <year> 1990. </year>
Reference-contexts: Unless otherwise stated, we assume in our experiments that the underlying interconnection network is reliable (e.g., it does not lose, duplicate, or corrupt packets without signalling an error) and provides FIFO ordering. Chapter 3 DATA-PARALLEL LANGUAGES We have chosen C* <ref> [Rose & Steele Jr. 87, TMC 90] </ref>, a data-parallel language, as the high-level parallel language for our experiments. <p> For this work, we chose the new revision of C* <ref> [TMC 90, TMC 91a] </ref> as a representative data-parallel language. As we will see shortly, C*'s communication operations are typical of what we would expect from other data-parallel languages, hence our findings should extend to those languages as well.
Reference: [TMC 91a] <author> J. Frankel. </author> <title> C* language reference manual. </title> <type> Technical report, </type> <institution> Thinking Machines Corp., </institution> <address> 245 First St., Cambridge MA 02142, </address> <year> 1991. </year>
Reference-contexts: For this work, we chose the new revision of C* <ref> [TMC 90, TMC 91a] </ref> as a representative data-parallel language. As we will see shortly, C*'s communication operations are typical of what we would expect from other data-parallel languages, hence our findings should extend to those languages as well. <p> For a detailed description of the current language, the reader is referred to <ref> [TMC 91a] </ref>; we give a brief overview here. C* distinguishes between scalar and parallel variables; the latter have a shape associated with them that describes how the data is organized. Attributes of a shape are its rank, layout and number of positions; there is one virtual processor (VP) per position. <p> The code below 1 The semantics of C* specify that scalar code inside branches of a where statement is always executed, i.e., independent of the condition in the where. This is in keeping with C*'s global model of execution <ref> [TMC 91a] </ref>. 20 transposes matrix a by sending each element at position (i; j) to position (j; i). Matrix b is transposed using an equivalent (but not necessarily equally efficient) get operation.
Reference: [TMC 91b] <institution> Thinking Machines Corporation. </institution> <type> CM-5 Technical Summary, </type> <year> 1991. </year>
Reference-contexts: Unfortunately, the cost of communication may limit the performance of parallel computers. To fully realize the advantages of parallel processing, we need to design efficient communication mechanisms. Existing communication architectures span a spectrum ranging from message passing <ref> [Arlauskas 88, Intel 91a, Dally 90, TMC 91b] </ref> to remote-memory access [Crowther et al. 85, Cray 93], shared memory [Sequent 87, Lenoski et al. 92, Agarwal et al. 91] and cache-only architectures [Hagersten 92a, KSR 92]. <p> Using off-the-shelf processors reduces overall design time for the machine and results in faster time-to-market. In fact, most existing parallel computers, such as Intel's series of message-passing machines [Arlauskas 88, Bokhari 90, Intel 91b, Intel 91a], the Thinking Machines CM-5 <ref> [TMC 91b] </ref>, or the Cray T3D [Cray 93] (a NUMA machine), use processing nodes built around a commercial off-the-shelf microprocessor. Instead of requiring special-purpose processor instructions, these machines control communication through hardware external to the processor. <p> In our work, we therefore examine different packet sizes. Special-Purpose Networks Some machines, such as the Thinking Machines CM-5 <ref> [TMC 91b] </ref> or the Cray T3D [Cray 93], use dedicated networks for certain communication operations. For example, the CM-5 includes a control network that has been specifically optimized to perform efficient reduction and broadcast operations. Similarly, the T3D has a synchronization network that provides very low-latency barrier synchronization operations. <p> from architectural support, such as adaptive or user-selectable cache coherence protocols [Carter et al. 91, Bennett et al. 92], full/empty bits on memory words [Agarwal et al. 91, Alverson et al. 90], a network or network interface dedicated to barrier synchronization and reduction (e.g., the control network on the CM-5 <ref> [TMC 91b] </ref>), or direct access to message-passing primitives as implemented on the Alewife machine [Agarwal et al. 91]. As far as our latency measurements are concerned, we find that even the basic message-passing machine can easily hide network latencies by using asynchronous message exchanges. <p> Performance of fanin/fanout tree operations can of course be improved dramatically by providing dedicated hardware, even dedicated networks, as is done on the CM-5 <ref> [TMC 91b] </ref>. However, we know of no shared-memory machine that incorporates such hardware. 5.8 Summary We have compiled a suite of scientific C* applications for message-passing, NUMA and cache-coherent architectures. We have simulated execution of the benchmarks and the respective architectures and measured technology-independent information about interconnect traffic and latency. <p> In older message-passing machines, such as the Intel Delta [Intel 91b], only the operating system can access the NI. With such a design, most message-passing operations therefore incur the additional cost of a system call [Anderson et al. 91]. More modern machines, such as the CM-5 <ref> [TMC 91b] </ref>, use a network interface that can be safely accessed from user-mode. We assume that all network interfaces used in our study have this property as well. <p> Figure 6.1 shows a traditional design for a message-passing architecture; machines like the Intel Delta [Intel 91b] or Thinking Machines CM-5 <ref> [TMC 91b] </ref> use this basic network interface design. The NI itself consists of little more than two FIFOs (one to receive data from the network and one to hold data that is to be injected into the network) and simple control circuitry. <p> This requires synchronization between the two processors, which for small messages may be more costly than allowing either processor to directly access the network interface, as is done on the CM-5 <ref> [TMC 91b] </ref>. Given that the coprocessor is identical to the compute processor, it may be more efficient to use it for general computation, performing communication through the kernel on both processors.
Reference: [TMC 92] <institution> Thinking Machines Corp. </institution> <note> CMMD 2.0 Reference Manual, </note> <year> 1992. </year>
Reference-contexts: Other NI registers hold information about the status of the FIFOs or allow the processor to control the NI's mode of operation; e.g., whether or not to interrupt the CPU when the receive FIFO fills up. A traditional message-passing library (like Intel's NX [Pierce 88] or Thinking Machines' CMMD <ref> [TMC 92] </ref>) can be implemented on top of these primitives to provide higher levels of abstraction to the programmer. Some machines include DMA hardware to speed transfer of data between main memory and NI.
Reference: [von Eicken et al. 92] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: For message-passing systems, researchers have largely focused on reducing the high per-message overhead typically found in message-passing systems. For example, active messages <ref> [von Eicken et al. 92] </ref> are a low-level transport mechanism that achieves low latency by efficiently dispatching to a message handler on the receiving node. Felten [Felten 93a] proposes using a protocol compiler to custom-generate message-passing protocols for a given program and thus reduce protocol overhead. <p> Note that the remote read and write operations can be implemented efficiently on a message-passing communication architecture using a low-level mechanism like Active Messages <ref> [von Eicken et al. 92] </ref>, which provide a very low overhead data transfer mechanism, without the protocol overhead of message passing. On the other hand, we have introduced explicit synchronization operations that were performed implicitly in the message-passing model. <p> Instead of generating code for a message-passing communication model, it compiles code for a remote memory access model which does not require any buffer management. Our run-time libraries for distributed-memory architectures implement the C* communication primitives using active messages <ref> [von Eicken et al. 92] </ref>, a very lightweight data transport mechanism. <p> Recall that our compiler has already eliminated most of the overhead present in traditional message-passing libraries. Our run-time libraries use active messages <ref> [von Eicken et al. 92] </ref> to implement the remote read and write operations; they do not need to perform any buffer management. With most protocol overhead eliminated, a significant amount of CPU overhead remains, namely the NI management overhead discussed earlier. <p> We conclude that while larger packets improve absolute performance of all NI designs, the new design can take better advantage of the larger packets. 6.6 Related Work Several researchers have proposed software techniques to reduce message-passing overhead. Active messages <ref> [von Eicken et al. 92] </ref> are a low-level transport mechanism that achieves low latency by efficiently dispatching to a message handler on the receiving node. Felten [Felten 93a] proposes using a protocol compiler to custom-generate message-passing protocols for a given program and thus reduce protocol overhead.
Reference: [von Hanxleden et al. 92] <author> R. von Hanxleden, K. Kennedy, C. Koelbel, R. Das, and J. Saltz. </author> <title> Compiler analysis for irregular problems in Fortran D. </title> <booktitle> In Languages and Compilers for Parallel Computing. 5th International Workshop Proceedings, </booktitle> <pages> pages 97-111, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: A general solution to this problem requires extra communication and computation at run-time, just to determine which nodes have to send which data items where <ref> [Clark et al. 92, von Hanxleden et al. 92] </ref>. To reduce this overhead, the run-time system can analyze a communication pattern once at run-time, and amortize the cost of the analysis over many reuses of the pattern [Wu et al. 91].
Reference: [Wu et al. 91] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <year> 1991. </year>
Reference-contexts: To reduce this overhead, the run-time system can analyze a communication pattern once at run-time, and amortize the cost of the analysis over many reuses of the pattern <ref> [Wu et al. 91] </ref>. Also, researchers have optimized the analysis phase itself [Leung & Zahorjan 93].
References-found: 86

