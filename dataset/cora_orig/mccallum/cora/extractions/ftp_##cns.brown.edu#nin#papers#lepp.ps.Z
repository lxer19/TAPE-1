URL: ftp://cns.brown.edu/nin/papers/lepp.ps.Z
Refering-URL: http://www.math.tau.ac.il/~nin/research.html
Root-URL: 
Title: Localized Exploratory Projection Pursuit  
Author: Nathan Intrator 
Address: Providence, RI 02912  
Affiliation: Center for Neural Science Brown University  
Abstract: Based on CART, we introduce a recursive partitioning method for high dimensional space which partitions the data using low dimensional features. The low dimensional features are extracted via an exploratory projection pursuit (EPP) method, localized to each node in the tree. In addition, we present an exploratory splitting rule that is potentially less biased to the training data. This leads to a nonparametric classifier for high dimensional space that has local feature extractors optimized to different regions in the input space. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bellman, R. E. </author> <year> (1961). </year> <title> Adaptive Control Processes. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference-contexts: 1 Introduction Due to the curse of dimensionality <ref> (Bellman, 1961) </ref> it is desirable to extract features from a high dimensional data space before attempting a classification. This may be done in those cases where the important structure is assumed to lie in a low dimensional subspace of the original data.
Reference: <author> Bienenstock, E. L., Cooper, L. N., and Munro, P. W. </author> <year> (1982). </year> <title> Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex. </title> <journal> Journal Neuroscience, </journal> <volume> 2 </volume> <pages> 32-48. </pages>
Reference-contexts: The feature extraction part of a node is implemented by an EPP method that seeks multimodality in the projected distributions (Intrator, 1990). This method is based on a biologically motivated synaptic modification equations <ref> (Bienenstock et al., 1982) </ref>, and is computa-tionally practical for high dimensional spaces, making it suitable to be used as the feature extractor in the proposed hybrid EPP/CART method. 3 Pseudo-Supervised Network Although the proposed hybrid EPP/CART is able to use any of the CART splitting rules, we would like to consider
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <booktitle> The Wadsworth Statistics/Probability Series, </booktitle> <address> Bel-mont, CA. </address>
Reference-contexts: This observation is one of the motivations of recursive partitioning methods, including tree structured algorithms. The proposed method is based on the classification and regression tree algorithm of CART <ref> (Breiman et al., 1984) </ref>. Section 2 discusses CART briefly, and indicates how the hybrid tree is constructed. A new splitting criterion based on a variation of a back-propagation network is presented in section 3. <p> This leads to a combination of feature extraction and recursive partitioning that has the potential to be much more powerful than each of the methods by itself. Moreover, this method is still consistent with the mono-tonicity requirement of the cost at each split <ref> (Breiman et al., 1984) </ref>, and therefore allows the use of the powerful pruning mechanism of CART. The construction of the hybrid tree is the same as in the CART method (Breiman et al., 1984) with the exception that every node can perform additional feature extraction based on the high dimensional input <p> Moreover, this method is still consistent with the mono-tonicity requirement of the cost at each split <ref> (Breiman et al., 1984) </ref>, and therefore allows the use of the powerful pruning mechanism of CART. The construction of the hybrid tree is the same as in the CART method (Breiman et al., 1984) with the exception that every node can perform additional feature extraction based on the high dimensional input patterns that arrive at that node, and based on the features extracted so far.
Reference: <author> Duda, R. O. and Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classifica tion and Scene Analysis. </title> <publisher> John Wiley, </publisher> <address> New York. </address>
Reference: <author> Friedman, J. H. </author> <year> (1977). </year> <title> A recursive partitioning decision rule for nonparametric classification. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 26 </volume> <pages> 404-408. </pages>
Reference: <author> Friedman, J. H. </author> <year> (1987). </year> <title> Exploratory projection pur suit. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 82 </volume> <pages> 249-266. </pages>
Reference: <author> Friedman, J. H. and Tukey, J. W. </author> <year> (1974). </year> <title> A projec tion pursuit algorithm for exploratory data analysis. </title> <journal> IEEE Transactions on Computers, C(23):881-889. </journal>
Reference: <author> Hinton, G. E. and Nowlan, S. J. </author> <year> (1990). </year> <title> The bootstrap widrow-hoff rule as a cluster-formation algorithm. </title> <journal> Neural Computation, </journal> <volume> 2(3) </volume> <pages> 355-362. </pages>
Reference-contexts: An intuitive explanation to this target definition is similar to the reasoning behind hard and soft competition approaches <ref> (Hinton and Nowlan, 1990) </ref>. If a hard target (0 or 1) is imposed, then whenever the output is close to .5 which means that the input is close to the boundary, the error signal would be large.
Reference: <author> Huber, P. J. </author> <year> (1985). </year> <title> Projection pursuit. (with discus sion). </title> <journal> The Annals of Statistics, </journal> <volume> 13 </volume> <pages> 435-475. </pages>
Reference-contexts: A more general and powerful method for feature extraction is Projection Pursuit, and its unsupervised version Exploratory Projection Pursuit (Friedman and Tukey, 1974; Friedman, 1987). This method has been extended in various directions, and is reviewed in <ref> (Huber, 1985) </ref>. One of the advantages of EPP is the use of locally smooth objective functions in the search for interesting features. Such functions are not related to the class labels, and have the potential of avoiding the curse of dimensionality (Huber, 1985). <p> been extended in various directions, and is reviewed in <ref> (Huber, 1985) </ref>. One of the advantages of EPP is the use of locally smooth objective functions in the search for interesting features. Such functions are not related to the class labels, and have the potential of avoiding the curse of dimensionality (Huber, 1985). The method has an underlying assumption of homogeneity of the input space.
Reference: <author> Intrator, N. </author> <year> (1990). </year> <title> Feature extraction using an unsuper vised neural network. </title> <editor> In Touretzky, D. S., Ellman, J. L., Sejnowski, T. J., and Hinton, G. E., editors, </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School, </booktitle> <pages> pages 310-318. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: The feature extraction part of a node is implemented by an EPP method that seeks multimodality in the projected distributions <ref> (Intrator, 1990) </ref>.
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. </author> <year> (1991). </year> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3(1) </volume> <pages> 79-87. </pages>
Reference: <author> Lucky, R. W. </author> <year> (1966). </year> <title> Techniques for adaptive equaliza tion of digital communications systems. </title> <journal> Bell Sys-tems,Technical Journal, </journal> <volume> 45 </volume> <pages> 255-286. </pages>
Reference: <author> Riskin, E. A., Lookabaugh, T., Chou, P. A., and Gray, R. M. </author> <year> (1990). </year> <title> Variable rate vector quantization for medical image compression. </title> <journal> IEEE Transactions on Medical Imaging, </journal> <volume> 9(3) </volume> <pages> 290-298. </pages>
Reference-contexts: The pruning mechanism is a very powerful tool, and may be useful in remote applications of CART such as image compression using vector quantization <ref> (Riskin et al., 1990) </ref>. CART is not directly applicable to classification problems in very high dimensional spaces, such as gray level pixel images, since splitting based on a single dimension (single pixel in this case) is unlikely to increase the ho mogeneity of sub regions in the space.
Reference: <author> Rumelhart, D. E., Hinton, G. E., and Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Therefore, a continuous approximation to t R is used. We shall follow the notations presented in <ref> (Rumelhart et al., 1986) </ref>, and present a splitting rule that is based on a variation of error backpropagation network. <p> MSE, is equivalent to minimizing the shaded area in the picture. In practice, there is no need to have be greater than 5. The calculation of the gradient with respect to the weight w ij follows in the same way as in <ref> (Rumelhart et al., 1986) </ref>, when taking into account the fact that the target depends on the network output as well. <p> @t pj @net pj @w ji and it follows that for ffi pj = (t pj o pj ) o pj (1 o pj ) t pj (1 t pj ) ; @w ji The calculation of the gradient with respect to a hidden unit weight is exactly as in <ref> (Rumelhart et al., 1986) </ref>, and will not be repeated here. An intuitive explanation to this target definition is similar to the reasoning behind hard and soft competition approaches (Hinton and Nowlan, 1990).
Reference: <author> Sankar, A. S. and Mammone, R. J. </author> <year> (1991). </year> <title> Neural tree networks. </title> <editor> In Mammone, R. J. and Zeevi, Y., editors, </editor> <booktitle> Neural Networks: Theory and Apllications. </booktitle> <publisher> Academic Press, </publisher> <address> New York. </address>
References-found: 15

