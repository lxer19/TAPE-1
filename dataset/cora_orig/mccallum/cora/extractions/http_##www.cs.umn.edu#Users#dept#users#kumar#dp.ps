URL: http://www.cs.umn.edu/Users/dept/users/kumar/dp.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: karypis@cs.umn.edu and kumar@cs.umn.edu  
Title: Efficient Parallel Formulations for Some Dynamic Programming Algorithms  
Author: George Karypis and Vipin Kumar 
Note: This work was supported by IST/SDIO through the Army Research Office grant #28408-MA-SDI and by the United States Army Research Office, Contract Number DAAL03-89-C-0038 at the University of Minnesota Army High Performance Computing Research Center. nCUBE/2 is a registered trademark of nCUBE Corporation.  
Date: TR 92-59, October 1992  
Address: Minneapolis, MN 55414  
Affiliation: Department of Computer Science University of Minnesota  
Abstract: In this paper we are concerned with Dynamic Programming (DP) algorithms whose solution is given by a recurrence relation similar to that for the matrix parenthesization problem. Guibas, Kung and Thompson presented a systolic array algorithm for this problem that uses O(n 2 ) processing cells and solves the problem in O(n) time. In this paper, we present three different mappings of this systolic algorithm on a mesh connected parallel computer. The first two mappings use commonly known techniques for mapping systolic arrays to mesh computers. Both of them are able to obtain only a fraction of maximum possible performance. The primary reason for the poor performance of these formulations is that different nodes at different levels in the multistage graph in the DP formulation require different amounts of computation. Any adaptation has to take this into consideration and evenly distribute the work among the processors. Our third mapping balances the work load among processors and thus is capable of providing efficiency approximately equal to 1 (i:e: speedup approximately equal to the number of processors) for any number of processors and sufficiently large problem. We present a theoretical analysis of these mappings and experimentally evaluate them on a mesh embedded onto a 256 processor nCUBE/2 fl . It can be shown that our mapping can be used to efficiently map a wide class of two dimension systolic array algorithms onto mesh connected parallel computers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.V. Aho and J.D. Ullman. </author> <title> The Theory of Parsing, Translation and Compiling, </title> <booktitle> Vol. 1, Parsing. </booktitle> <address> Englewood Cliffs, </address> <publisher> NJ Prentice Hall, </publisher> <year> 1972. </year>
Reference-contexts: In this paper we are concerned with the polyadic-nonserial DP algorithms whose solution is given by a recurrence relation similar to that for the matrix parenthesization problem [6]. Examples of these problems are: optimal triangularization of polygons, optimal binary search trees [6], and the CYK parser <ref> [1] </ref>. The serial complexity of these problems is O (n 3 ). A number of parallel formulations have been proposed in [9] that use O (n) processors on a hypercube and solves the problem in O (n 2 ) time.
Reference: [2] <author> S. G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: The purpose of this paper is to investigate a number of possible mappings of the GKT algorithm onto a mesh connected parallel computer <ref> [2] </ref> with p processors. A mesh connected parallel computer has a structure similar to that of the TSA hence, the mapping of the TSA algorithm onto a mesh can be done in a natural way. Furthermore, we will assume that the mesh connected parallel computer has wrap-around communication links. <p> Hence, shu*ing mapping can effectively utilize O (n 2 ) processors. 13 6 Experimental Results We implemented all three mappings of the GKT algorithm presented in Section 4 on an nCUBE/2 parallel computer. nCUBE/2 is a hypercube connected parallel computer and a well known mapping <ref> [2] </ref> was used to embed a wrap around mesh on it. A large number of experiments were made with different values of p and n. Some of these results are shown in Table 3.
Reference: [3] <author> R. Bellman and S. Dreyfus. </author> <title> Applied Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ, </address> <year> 1962. </year>
Reference-contexts: 1 Introduction Dynamic programming (DP) is a widely used problem solving paradigm for optimization problems that is widely applied to a large number of areas including optimal control, industrial engineering, economics and artificial intelligence <ref> [3, 4, 14, 18] </ref>. Many practical problems involving a sequence of interrelated decisions can be efficiently solved by DP. The essence of many DP algorithms lies in computing solutions of the smallest subproblems and storing the results for usage in computing larger subproblems.
Reference: [4] <author> U. Bertele and F. Brioschi. </author> <title> Nonserial Dynamic Programming. </title> <publisher> Academic Press, </publisher> <address> NY, </address> <year> 1972. </year>
Reference-contexts: 1 Introduction Dynamic programming (DP) is a widely used problem solving paradigm for optimization problems that is widely applied to a large number of areas including optimal control, industrial engineering, economics and artificial intelligence <ref> [3, 4, 14, 18] </ref>. Many practical problems involving a sequence of interrelated decisions can be efficiently solved by DP. The essence of many DP algorithms lies in computing solutions of the smallest subproblems and storing the results for usage in computing larger subproblems.
Reference: [5] <author> D. P. Bertsekas and J.N. Tsitsiklis. </author> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <year> 1989. </year>
Reference-contexts: A DP formulation is serial if the subproblems can be grouped in levels, and the solution to any subproblem in a certain level can be found using subproblems that belong only in the immediately preceding levels, otherwise it is non-serial . As it was shown in <ref> [11, 5] </ref> monadic-serial DP problems can be solved by a series of matrix-vector multiplication which is easy to parallelize [5]. On the other hand there is no general parallel formulation for polyadic-nonserial DP problems. <p> As it was shown in [11, 5] monadic-serial DP problems can be solved by a series of matrix-vector multiplication which is easy to parallelize <ref> [5] </ref>. On the other hand there is no general parallel formulation for polyadic-nonserial DP problems. In this paper we are concerned with the polyadic-nonserial DP algorithms whose solution is given by a recurrence relation similar to that for the matrix parenthesization problem [6]. <p> If there are I nodes in a diagonal, we assign I=p nodes to each of the p processors. Each processor computes the cost of the entries c (i; j) assigned to 3 it. This is followed by an all-to-all broadcast <ref> [5] </ref> during which solution costs of the subproblems at that diagonal are made known to all the processors. Since each processor has complete information about subproblem costs at preceding diagonals, no communication is needed other than the all-to-all broadcast. <p> Figure 2 illustrates this mapping. This mapping scheme is often called checkerboarding and it has been used in a number of applications <ref> [8, 5, 10] </ref>.
Reference: [6] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: On the other hand there is no general parallel formulation for polyadic-nonserial DP problems. In this paper we are concerned with the polyadic-nonserial DP algorithms whose solution is given by a recurrence relation similar to that for the matrix parenthesization problem <ref> [6] </ref>. Examples of these problems are: optimal triangularization of polygons, optimal binary search trees [6], and the CYK parser [1]. The serial complexity of these problems is O (n 3 ). <p> In this paper we are concerned with the polyadic-nonserial DP algorithms whose solution is given by a recurrence relation similar to that for the matrix parenthesization problem <ref> [6] </ref>. Examples of these problems are: optimal triangularization of polygons, optimal binary search trees [6], and the CYK parser [1]. The serial complexity of these problems is O (n 3 ). A number of parallel formulations have been proposed in [9] that use O (n) processors on a hypercube and solves the problem in O (n 2 ) time. <p> Section 6 presents experimental results, and finally Section 7 provides some concluding remarks. 2 The Parenthesization Problem and the Dynamic Program ming Algorithm The parenthesization and other isomorphic problems, can be efficiently solved using a dynamic programming algorithm <ref> [6] </ref>. Consider the evaluation of the product of n matrices, A 1 ; A 2 ; : : : A n , where each A i is a matrix with r i1 rows and r i columns. <p> We can graphically visualize this if we think of filling in the tables in a diagonal order (see Figure 1). This concept of diagonal oriented computations will be extensively used in the rest of this paper. For a more detailed description refer to <ref> [6] </ref>. Computing entry c (i; j) requires computing the cost of (j i) possible parenthesizations and taking their minimum. This can be done in (j i) operations. Hence, the exact sequential complexity of the algorithm is: P n1 6 n 6 .
Reference: [7] <author> L. J. Guibas, H. T. Kung, and C. D. Thompson. </author> <title> Direct VLSI Implementation of Combinatorial Algorithms. </title> <booktitle> In Proceedings of Conference on Very Large Scale Integration, California Institute of Technology, </booktitle> <pages> pages 509-525, </pages> <year> 1979. </year>
Reference-contexts: The serial complexity of these problems is O (n 3 ). A number of parallel formulations have been proposed in [9] that use O (n) processors on a hypercube and solves the problem in O (n 2 ) time. A systolic array algorithm has been proposed in <ref> [7] </ref> that uses O (n 2 ) processing cells and solves the problem in O (n) time. Finally, there are some non-cost-optimal parallel formulations for PRAM machines that solve the problem in O (log 2 n) time using n 6 log n processors [15, 16]. <p> By choosing big enough block sizes, the ratio of communication to computation at each processor can be made arbitrarily small. In this paper, we present three different formulations of the systolic algorithm <ref> [7] </ref> on a mesh connected parallel computer. The first formulation is a mapping of the systolic algorithm on a two dimension mesh computer along the lines proposed in [10]. This formulation results an upper bound on the efficiency equal to 1 12 for sufficiently large number of processors. <p> The exact processor-time product of the PRAM formulation is n (n+1) 2 fi 2n n 3 ; hence, even though the PRAM algorithm is significantly faster, it does 6 times more work than the sequential algorithm therefore its efficiency is only 0:167. Guibas, Kung and Thomson <ref> [7] </ref> have developed a systolic algorithm for the parenthesization problem. Their algorithm uses n (n+1) 2 processing elements (cells) connected as a two dimension systolic array (TSA) as shown in Figure 1, and solves the problem in in essentially the same time as the PRAM algorithm outlined above. <p> For the rest of this paper we will refer to this algorithm as GKT. A brief description of the algorithm follows. For a more detailed description the reader should refer to <ref> [7] </ref>. The inputs c (i; i) are applied in parallel to the cells with coordinates (i; i) and each cell (i; j) computes c (i; j). If a cell is computing an element of diagonal t, then its result is ready at time 2t.
Reference: [8] <author> Anshul Gupta and Vipin Kumar. </author> <title> On the scalability of Matrix Multiplication Algorithms on Parallel Computers. </title> <type> Technical Report TR 91-54, </type> <institution> Computer Science Department, University of Minnesota, </institution> <address> Minneapolis, MN 55455, </address> <year> 1991. </year>
Reference-contexts: Figure 2 illustrates this mapping. This mapping scheme is often called checkerboarding and it has been used in a number of applications <ref> [8, 5, 10] </ref>.
Reference: [9] <author> Oscar H. Ibara, Ting-Chuen Pong, and Stephen M. Sohn. </author> <title> Parallel Recognition and Parsing on the Hypercube. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(6) </volume> <pages> 764-770, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Examples of these problems are: optimal triangularization of polygons, optimal binary search trees [6], and the CYK parser [1]. The serial complexity of these problems is O (n 3 ). A number of parallel formulations have been proposed in <ref> [9] </ref> that use O (n) processors on a hypercube and solves the problem in O (n 2 ) time. A systolic array algorithm has been proposed in [7] that uses O (n 2 ) processing cells and solves the problem in O (n) time. <p> If n is sufficiently larger than p, then the communication time can be made to be an arbitrarily small fraction of the computation time, and linear speedups can be obtained. An alternative mapping was proposed by Ibara , Pong and Sohn in the context of the CYK parser <ref> [9] </ref>. Their formulation uses p = O (n) processors, connected in a hypercube topology, and solves the problem in O ( n 3 p ) time, which is cost optimal. The formulation of Ibara et:al: has properties similar to the formulation for linear array mentioned above.
Reference: [10] <author> Oscar H. Ibara and Stephen M. Sohn. </author> <title> On Mapping Systolic Algorithms onto the Hypercube. </title> <journal> IEEE Transaction on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 48-63, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: This mapping leads to poor utilization, because in general purpose parallel computers, the communication cost for sending a unit message is much higher than unit computations. As shown in Ibara, Pong and Sohn <ref> [10] </ref>, this problem can be corrected by assigning a block of of cells to each processor. Now the computation at each processor becomes proportional to the area of the block, and communication becomes proportional to the periphery. <p> In this paper, we present three different formulations of the systolic algorithm [7] on a mesh connected parallel computer. The first formulation is a mapping of the systolic algorithm on a two dimension mesh computer along the lines proposed in <ref> [10] </ref>. This formulation results an upper bound on the efficiency equal to 1 12 for sufficiently large number of processors. The second formulation is a slightly modified version of the first scheme but also has an upper bound of 1 3 in efficiency. <p> Figure 2 illustrates this mapping. This mapping scheme is often called checkerboarding and it has been used in a number of applications <ref> [8, 5, 10] </ref>.
Reference: [11] <author> Guo jie Li and Benjamin W. Wah. </author> <title> Parallel Processing of Serial Dynamic Programming Problems. </title> <booktitle> In Proc. COMPSAC 85, </booktitle> <pages> pages 81-89, </pages> <year> 1985. </year>
Reference-contexts: A DP formulation is expressed as a recursive functional equation whose left-hand side is an expression involving the maximization (or minimization) of values of some cost functions. Li and Wah <ref> [11] </ref>, have developed a classification of DP programming schemes according to the form of the functional equations and the nature of the recursion. A DP formulation is called monadic if its cost function involves only one recursive term, otherwise it is called polyadic. <p> A DP formulation is serial if the subproblems can be grouped in levels, and the solution to any subproblem in a certain level can be found using subproblems that belong only in the immediately preceding levels, otherwise it is non-serial . As it was shown in <ref> [11, 5] </ref> monadic-serial DP problems can be solved by a series of matrix-vector multiplication which is easy to parallelize [5]. On the other hand there is no general parallel formulation for polyadic-nonserial DP problems.
Reference: [12] <author> Vipin Kumar and Anshul Gupta. </author> <title> Analyzing Scalability of Parallel Algorithms and Architectures. </title> <type> Technical report, </type> <institution> TR-91-18, Computer Science Department, University of Minnesota, </institution> <month> June </month> <year> 1991. </year> <note> A short version of the paper appears in the Proceedings of the 1991 International Conference on Supercomputing, Germany, and as an invited paper in the Proc. of 29th Annual Allerton Conference on Communuication, Control and Computing, Urbana,IL, </note> <month> October </month> <year> 1991. </year> <month> 18 </month>
Reference-contexts: This is because the TSA algorithm requires O (n 2 ) processors (i:e n 2 p ) n p For a more rigorous discussion on the isoefficiency metric and scalability analysis, the reader is referred to <ref> [13, 12] </ref>. 5.3 Analysis of Checkerboarding Mapping In analyzing the performance of the checkerboarding mapping we will concentrate on the amount of computation T P 1; p p calc , performed by processor P 1; p p because this processor performs more com putation than any other processor.
Reference: [13] <author> Vipin Kumar and V. Nageshwara Rao. </author> <title> Parallel Depth-First Search, Part II: Analysis. </title> <journal> Inter--national Journal of Parallel Programming, </journal> <volume> 16 </volume> (6):501-519, 1987. 
Reference-contexts: This is because the TSA algorithm requires O (n 2 ) processors (i:e n 2 p ) n p For a more rigorous discussion on the isoefficiency metric and scalability analysis, the reader is referred to <ref> [13, 12] </ref>. 5.3 Analysis of Checkerboarding Mapping In analyzing the performance of the checkerboarding mapping we will concentrate on the amount of computation T P 1; p p calc , performed by processor P 1; p p because this processor performs more com putation than any other processor.
Reference: [14] <author> H. Ney. </author> <title> Dynamic programming as a technique for Pattern Recognition. </title> <booktitle> In Proc. 6th Intl. Conf. Pattern Recognition, </booktitle> <pages> pages 1119 - 1125, </pages> <month> Oct. </month> <year> 1982. </year>
Reference-contexts: 1 Introduction Dynamic programming (DP) is a widely used problem solving paradigm for optimization problems that is widely applied to a large number of areas including optimal control, industrial engineering, economics and artificial intelligence <ref> [3, 4, 14, 18] </ref>. Many practical problems involving a sequence of interrelated decisions can be efficiently solved by DP. The essence of many DP algorithms lies in computing solutions of the smallest subproblems and storing the results for usage in computing larger subproblems.
Reference: [15] <author> Wojciech Rytter. </author> <title> On Efficienct Parallel Computations for Some Dynamic Programming Problems. </title> <journal> Theoretical Computer Science, </journal> <volume> 59 </volume> <pages> 297-307, </pages> <year> 1988. </year>
Reference-contexts: Finally, there are some non-cost-optimal parallel formulations for PRAM machines that solve the problem in O (log 2 n) time using n 6 log n processors <ref> [15, 16] </ref>. The systolic algorithm for two dimension systolic arrays can be directly mapped onto a mesh connected parallel computer by assigning each cell to a different processor.
Reference: [16] <author> L. Valiant, S. Skyum, S. Berkowitz, and C. Rackoff. </author> <title> Fast Parallel Computation of Polynomials Using Few Processors. </title> <journal> SIAM J. Computing, </journal> <volume> 12(4), </volume> <year> 1983. </year>
Reference-contexts: Finally, there are some non-cost-optimal parallel formulations for PRAM machines that solve the problem in O (log 2 n) time using n 6 log n processors <ref> [15, 16] </ref>. The systolic algorithm for two dimension systolic arrays can be directly mapped onto a mesh connected parallel computer by assigning each cell to a different processor.
Reference: [17] <author> Kumiko Wada and Nobuyuki Ichiyoshi. </author> <title> A Distributed Shortest Path Algorithm and Its Mapping on the Multi-PSI. </title> <booktitle> In Proceedings of Distributed Massively Concurent Computers., </booktitle> <year> 1989. </year>
Reference-contexts: This mapping is illustrated in Figure 4. For the rest of this paper this mapping will be referred to as shu*ing. A variation of this mapping was used in the context of shortest path on sparse graphs in <ref> [17] </ref>.
Reference: [18] <author> D. White. </author> <title> Dynamic Programming. </title> <publisher> Oliver and Boyd, Edinburgh, </publisher> <address> England, </address> <year> 1969. </year> <month> 19 </month>
Reference-contexts: 1 Introduction Dynamic programming (DP) is a widely used problem solving paradigm for optimization problems that is widely applied to a large number of areas including optimal control, industrial engineering, economics and artificial intelligence <ref> [3, 4, 14, 18] </ref>. Many practical problems involving a sequence of interrelated decisions can be efficiently solved by DP. The essence of many DP algorithms lies in computing solutions of the smallest subproblems and storing the results for usage in computing larger subproblems.
References-found: 18

