URL: ftp://ftp.cs.dartmouth.edu/TR/TR94-226.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR94-226/
Root-URL: http://www.cs.dartmouth.edu
Title: Disk-directed I/O for MIMD Multiprocessors  
Author: David Kotz 
Address: Hanover, NH 03755  
Affiliation: Department of Computer Science Dartmouth College  
Note: Available at  
Pubnum: Dartmouth PCS-TR94-226  
Email: dfk@cs.dartmouth.edu  
Date: July 22, 1994 Revised November 8, 1994  
Web: URL ftp://ftp.cs.dartmouth.edu/TR/TR94-226.ps.Z  
Abstract: Many scientific applications that run on today's multiprocessors, such as weather forecasting and seismic analysis, are bottlenecked by their file-I/O needs. Even if the multiprocessor is configured with sufficient I/O hardware, the file-system software often fails to provide the available bandwidth to the application. Although libraries and enhanced file-system interfaces can make a significant improvement, we believe that fundamental changes are needed in the file-server software. We propose a new technique, disk-directed I/O, to allow the disk servers to determine the flow of data for maximum performance. Our simulations show that tremendous performance gains are possible. Indeed, disk-directed I/O provided consistent high performance that was largely independent of data distribution, obtained up to 93% of peak disk bandwidth, and was as much as 18 times faster than traditional parallel file systems.
Abstract-found: 1
Intro-found: 1
Reference: [BBS + 94] <author> Robert Bennett, Kelvin Bryant, Alan Sussman, Raja Das, and Joel Saltz. Jovian: </author> <title> A framework for optimizing parallel I/O. </title> <booktitle> In Proceedings of the 1994 Scalable Parallel Libraries Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: ELFS [KGF94] provides an object-oriented interface that encourages operations on large objects, and could lead to support for collective I/O. Finally, there are several interfaces for collective matrix I/O <ref> [GGL93, BdC93, BBS + 94] </ref>. <p> While this concept is roughly similar to our disk-directed I/O, it is primarily a speed-matching buffer used for load balancing. The Jovian collective-I/O library <ref> [BBS + 94] </ref> tries to coalesce fragmented requests from many CPs into larger requests that can be passed to the IOPs. Their "coalescing processes" are essentially a dynamic implementation of the two-phase-I/O permutation phase.
Reference: [BdC93] <author> Rajesh Bordawekar, Juan Miguel del Rosario, and Alok Choudhary. </author> <title> Design and evaluation of primitives for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <year> 1993. </year>
Reference-contexts: ELFS [KGF94] provides an object-oriented interface that encourages operations on large objects, and could lead to support for collective I/O. Finally, there are several interfaces for collective matrix I/O <ref> [GGL93, BdC93, BBS + 94] </ref>. <p> Recall that each application process must call ReadCP once for each contiguous chunk of the file, no matter how small. Each IOP attempts to dynamically optimize the use of the disk, cache, and network interface. Two-phase I/O. Figure 2b sketches an alternative proposed by del Rosario, Bordawekar, and Choudhary <ref> [dBC93, BdC93] </ref>, which permutes the data among the CP memories before writing or after reading. Thus, there are two phases, one for I/O and one for an in-memory permutation.
Reference: [BDCW91] <author> Eric A. Brewer, Chrysanthos N. Dellarocas, Adrian Colbrook, and William E. Weihl. Proteus: </author> <title> A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> MIT, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: only memory-memory copy we used. 5 While our cache implementation does not model any specific commercial cache implementation, we believe it is reasonable and better than most, and thus a fair competitor for our disk-directed-I/O implementation. 4.1 Simulator The implementations described above ran on top of the Proteus parallel-architecture simulator <ref> [BDCW91] </ref>, which in turn ran on a DEC-5000 workstation. We configured Proteus using the parameters listed in Table 1. <p> 8 KB I/O buses (one per IOP) 16 * I/O bus type SCSI I/O bus peak bandwidth 10 Mbytes/s Interconnect topology 6 fi 6 torus Interconnect bandwidth 200 fi 10 6 bytes/s bidirectional Interconnect latency 20 ns per router Routing wormhole Proteus itself has been validated against real message-passing machines <ref> [BDCW91] </ref>.
Reference: [BGST93] <author> Michael L. Best, Adam Greenberg, Craig Stanfill, and Lewis W. Tucker. </author> <title> CMMD I/O: A parallel Unix I/O. </title> <booktitle> In Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <pages> pages 489-495, </pages> <year> 1993. </year>
Reference-contexts: Indeed, since most multiprocessor file systems <ref> [CF94, FPD93, Pie89, Roy93, DdR92, LIN + 93, BGST93, Dib90, DSE88] </ref> decluster file data across many disks, each application request may be broken into even smaller requests that are sent to different IOPs.
Reference: [CDG + 93] <author> David E. Culler, Andrea Drusseau, Seth Copen Goldstein, Arvind Krishnamurthy, Steven Lumetta, Thorsten von Eicken, and Katherine Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 262-283, </pages> <year> 1993. </year>
Reference-contexts: Finally, our Memput and Memget operations are not unusual. Similar remote-memory-access mechanisms are supported in a variety of distributed-memory systems <ref> [WMR + 94, CDG + 93] </ref>. 7.1 Comparison to Two-phase I/O The above results clearly show the benefits of disk-directed I/O over traditional caching.
Reference: [CF94] <author> Peter F. Corbett and Dror G. Feitelson. </author> <title> Design and implementation of the Vesta parallel file system. </title> <booktitle> In Proceedings of the Scalable High-Performance Computing Conference, </booktitle> <pages> pages 63-70, </pages> <year> 1994. </year>
Reference-contexts: Indeed, since most multiprocessor file systems <ref> [CF94, FPD93, Pie89, Roy93, DdR92, LIN + 93, BGST93, Dib90, DSE88] </ref> decluster file data across many disks, each application request may be broken into even smaller requests that are sent to different IOPs. <p> Unfortunately, few multiprocessor file systems provide a collective interface. Most have an interface based on simple parallel extensions to the traditional read/write/seek model, focusing on coordination of the file pointer. Vesta <ref> [CF94] </ref> and the nCUBE file system [DdR92] support logical mappings between the file and processor memories, defining separate "subfiles" for each processor.
Reference: [CK93] <author> Thomas H. Cormen and David Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dart-mouth Institute for Advanced Graduate Studies. </institution> <note> Revised from Dartmouth PCS-TR93-188. </note>
Reference-contexts: Finally, scientific applications use files for more than loading raw data and storing results; files are used as scratch space for very large problems as application-controlled virtual memory <ref> [CK93] </ref>. In short, multiprocessors need new file systems that are designed for parallel scientific applications. In this paper we describe a technique that is designed specifically for high performance on parallel scientific applications. <p> Collective I/O need not involve matrices. Many out-of-core parallel algorithms do I/O in "mem-oryloads," that is, they repeatedly load some subset of the file into memory, process it, and write it out <ref> [CK93] </ref>. Each transfer is a large, but not necessarily contiguous, set of data. Traditional caching and prefetching policies, geared for sequential access, would be ineffective or even detrimental for 3 this type of I/O. Unfortunately, few multiprocessor file systems provide a collective interface.
Reference: [CLVW93] <author> Pei Cao, Swee Boon Lim, Shivakumar Venkataraman, and John Wilkes. </author> <title> The TickerTAIP parallel RAID architecture. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 52-63, </pages> <year> 1993. </year>
Reference-contexts: Our model for managing a disk-directed request, that is, sending a high-level request to all IOPs which then operate independently under the assumption that they can determine the necessary actions to accomplish the task, is an example of collaborative execution like that used in the TickerTAIP RAID controller <ref> [CLVW93] </ref>. Finally, our Memput and Memget operations are not unusual. Similar remote-memory-access mechanisms are supported in a variety of distributed-memory systems [WMR + 94, CDG + 93]. 7.1 Comparison to Two-phase I/O The above results clearly show the benefits of disk-directed I/O over traditional caching.
Reference: [dBC93] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 56-70, </pages> <year> 1993. </year> <note> Also published in Computer Architecture News 21(5), </note> <month> December </month> <year> 1993, </year> <pages> pages 31-38. </pages>
Reference-contexts: Valuable semantic information | that a large, contiguous, parallel file transfer is in progress | is lost through this low-level interface. A collective-I/O interface, in which all CPs cooperate to make a single, large request, retains this semantic information, making it easier to coordinate I/O for better performance <ref> [dBC93, Nit92, PGK88] </ref>. Collective I/O need not involve matrices. Many out-of-core parallel algorithms do I/O in "mem-oryloads," that is, they repeatedly load some subset of the file into memory, process it, and write it out [CK93]. Each transfer is a large, but not necessarily contiguous, set of data. <p> Recall that each application process must call ReadCP once for each contiguous chunk of the file, no matter how small. Each IOP attempts to dynamically optimize the use of the disk, cache, and network interface. Two-phase I/O. Figure 2b sketches an alternative proposed by del Rosario, Bordawekar, and Choudhary <ref> [dBC93, BdC93] </ref>, which permutes the data among the CP memories before writing or after reading. Thus, there are two phases, one for I/O and one for an in-memory permutation. <p> The access patterns. Our read- and write-access patterns differed in the way the array elements (records) were mapped into CP memories. We chose to evaluate the array-distribution possibilities available in High-Performance Fortran <ref> [HPF93, dBC93] </ref>, as shown in Figure 3. Thus, elements in each dimension of the array could be mapped entirely to one CP (NONE), distributed among CPs in contiguous blocks (BLOCK; note this is a different "block" than the file system "block"), or distributed round-robin among the CPs (CYCLIC). <p> Finally, our Memput and Memget operations are not unusual. Similar remote-memory-access mechanisms are supported in a variety of distributed-memory systems [WMR + 94, CDG + 93]. 7.1 Comparison to Two-phase I/O The above results clearly show the benefits of disk-directed I/O over traditional caching. Two-phase I/O <ref> [dBC93] </ref> was designed to avoid the worst of traditional caching while using the same IOP software, by reading data in a "conforming distribution," then permuting it among the CPs. At first glance, disk-directed I/O is two-phase I/O implemented by rewriting IOP software so the IOPs do both phases simultaneously.
Reference: [DdR92] <author> Erik DeBenedictis and Juan Miguel del Rosario. </author> <title> nCUBE parallel I/O software. </title> <booktitle> In Eleventh Annual IEEE International Phoenix Conference on Computers and Communications (IPCCC), </booktitle> <pages> pages 0117-0124, </pages> <month> April </month> <year> 1992. </year>
Reference-contexts: Indeed, since most multiprocessor file systems <ref> [CF94, FPD93, Pie89, Roy93, DdR92, LIN + 93, BGST93, Dib90, DSE88] </ref> decluster file data across many disks, each application request may be broken into even smaller requests that are sent to different IOPs. <p> Unfortunately, few multiprocessor file systems provide a collective interface. Most have an interface based on simple parallel extensions to the traditional read/write/seek model, focusing on coordination of the file pointer. Vesta [CF94] and the nCUBE file system <ref> [DdR92] </ref> support logical mappings between the file and processor memories, defining separate "subfiles" for each processor. Although these mappings remove the burden of managing the file pointer from the programmer, and allow the programmer to request noncontiguous data in a single request, there is no support for collective I/O.
Reference: [Dib90] <author> Peter C. Dibble. </author> <title> A Parallel Interleaved File System. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: Indeed, since most multiprocessor file systems <ref> [CF94, FPD93, Pie89, Roy93, DdR92, LIN + 93, BGST93, Dib90, DSE88] </ref> decluster file data across many disks, each application request may be broken into even smaller requests that are sent to different IOPs. <p> while the relative benefit of disk-directed I/O over traditional caching varied, disk-directed I/O consistently provided excellent performance, at least as good as traditional caching, often independent of access pattern, and often close to hardware limits. 25 7 Related work Disk-directed I/O is somewhat reminiscent of the PIFS (Bridge) "tools" interface <ref> [Dib90] </ref>, in that the data flow is controlled by the file system rather by than the application. PIFS focuses on managing where data flows (for memory locality), whereas disk-directed I/O focuses more on when data flows (for better disk and cache performance).
Reference: [DSE88] <author> Peter Dibble, Michael Scott, and Carla Ellis. </author> <title> Bridge: A high-performance file system for parallel processors. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computer Systems, </booktitle> <pages> pages 154-161, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Indeed, since most multiprocessor file systems <ref> [CF94, FPD93, Pie89, Roy93, DdR92, LIN + 93, BGST93, Dib90, DSE88] </ref> decluster file data across many disks, each application request may be broken into even smaller requests that are sent to different IOPs.
Reference: [EGKS90] <author> Susanne Englert, Jim Gray, Terrye Kocher, and Praful Shah. </author> <title> A benchmark of NonStop SQL Release 2 demonstrating near-linear speedup and scaleup on large databases. </title> <booktitle> In Proceedings of the 1990 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 245-246, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Some parallel database machines use an architecture similar to disk-directed I/O, in that certain operations are moved closer to the disks to allow for more optimization. In the Tandem NonStop system <ref> [EGKS90] </ref> each query is sent to all IOPs, which scan the local database partition and send only the relevant tuples back to the requesting node.
Reference: [FPD93] <author> James C. French, Terrence W. Pratt, and Mriganka Das. </author> <title> Performance measurement of the Concurrent File System of the Intel iPSC/2 hypercube. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):115-121, January and February 1993. </note>
Reference-contexts: Indeed, since most multiprocessor file systems <ref> [CF94, FPD93, Pie89, Roy93, DdR92, LIN + 93, BGST93, Dib90, DSE88] </ref> decluster file data across many disks, each application request may be broken into even smaller requests that are sent to different IOPs.
Reference: [GGL93] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year>
Reference-contexts: Poor performance is not surprising because the Unix file system was designed for a general-purpose workload [OCH + 85], rather than for a parallel, scientific workload. Scientific applications use larger files and have more sequential access <ref> [MK91, GGL93, PP93] </ref>. Parallel scientific programs access the file with patterns not seen in uniprocessor or distributed-system workloads, in particular, complex strided access to discontiguous pieces of the file [KN94, NK94]. <p> ELFS [KGF94] provides an object-oriented interface that encourages operations on large objects, and could lead to support for collective I/O. Finally, there are several interfaces for collective matrix I/O <ref> [GGL93, BdC93, BBS + 94] </ref>. <p> ELFS [KGF94] provides an object-oriented interface that encourages operations on large objects, and could lead to support for collective I/O. Finally, there are several interfaces for collective matrix I/O [GGL93, BdC93, BBS + 94]. For example, to read a two-dimensional matrix of integers in the notation of <ref> [GGL93] </ref>, every processor executes the following code: /* describes my part of matrix */ PIFArrayPart mypart [2] = ... ; /* memory for my part */ int *A = malloc (...); PIFILE *fp = PIFOpen (...); PIFReadDistributedArray (fp, NULL, sizeof (int), mypart, 2, A, MSG_INT); Thus, the groundwork for collective I/O <p> The matrix is distributed among the CPs in various ways, but within each CP the data is contiguous in memory. We discuss three implementation alternatives: traditional caching, two-phase I/O, and disk-directed I/O. The latter two require a collective-I/O interface similar to that of Galbreath et al <ref> [GGL93] </ref>, above. Traditional caching. This alternative mimics a "traditional" parallel file system like Intel CFS [Pie89], with no explicit collective-I/O interface and with IOPs that each manage a file cache. 4 the corresponding function executed at the IOP to service each incoming CP request.
Reference: [HPF93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <address> 1.0 edition, </address> <month> May 3 </month> <year> 1993. </year>
Reference-contexts: The access patterns. Our read- and write-access patterns differed in the way the array elements (records) were mapped into CP memories. We chose to evaluate the array-distribution possibilities available in High-Performance Fortran <ref> [HPF93, dBC93] </ref>, as shown in Figure 3. Thus, elements in each dimension of the array could be mapped entirely to one CP (NONE), distributed among CPs in contiguous blocks (BLOCK; note this is a different "block" than the file system "block"), or distributed round-robin among the CPs (CYCLIC).
Reference: [KE93] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Caching and writeback policies in parallel file systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):140-145, January and February 1993. </note>
Reference-contexts: enough to double-buffer an independent stream of requests from each CP to each disk. 4 The cache used an LRU-replacement strategy, prefetched one block ahead after each read request, and flushed dirty buffers to disk when they were full (i.e., after n bytes had been written to an n-byte buffer <ref> [KE93] </ref>). As described above, we transferred data as a part of request and reply messages, using DMA to avoid most extraneous copies.
Reference: [KGF94] <author> John F. Karpovich, Andrew S. Grimshaw, and James C. </author> <title> French. Extensible file systems ELFS: An object-oriented approach to high performance file I/O. </title> <booktitle> In Proceedings of the Ninth Annual Conference on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <pages> pages 191-204, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: CM-Fortran for the CM-5 does provide a collective-I/O interface, which leads to high performance through cooperation among the compiler, run-time, operating system, and hardware. ELFS <ref> [KGF94] </ref> provides an object-oriented interface that encourages operations on large objects, and could lead to support for collective I/O. Finally, there are several interfaces for collective matrix I/O [GGL93, BdC93, BBS + 94].
Reference: [KHH + 92] <author> Masaru Kitsuregawa, Satoshi Hirano, Masanobu Harada, Minoru Nakamura, and Mikio Takagi. </author> <title> The Super Database Computer (SDC): System architecture, algorithm and preliminary evaluation. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual Hawaii International Conference on System Sciences, </booktitle> <volume> volume I, </volume> <pages> pages 308-319, </pages> <year> 1992. </year>
Reference-contexts: In the Tandem NonStop system [EGKS90] each query is sent to all IOPs, which scan the local database partition and send only the relevant tuples back to the requesting node. The Super Database Computer <ref> [KHH + 92] </ref> has disk controllers that continuously produce tasks from the input data set, which are consumed and processed by CPs as they become available. While this concept is roughly similar to our disk-directed I/O, it is primarily a speed-matching buffer used for load balancing.
Reference: [KN94] <author> David Kotz and Nils Nieuwejaar. </author> <title> Dynamic file-access characteristics of a production parallel scientific workload. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Scientific applications use larger files and have more sequential access [MK91, GGL93, PP93]. Parallel scientific programs access the file with patterns not seen in uniprocessor or distributed-system workloads, in particular, complex strided access to discontiguous pieces of the file <ref> [KN94, NK94] </ref>. Finally, scientific applications use files for more than loading raw data and storing results; files are used as scratch space for very large problems as application-controlled virtual memory [CK93]. In short, multiprocessors need new file systems that are designed for parallel scientific applications. <p> If that processor's data is not logically contiguous in the file, as is often the case <ref> [KN94] </ref>, a separate file-system call is needed for each contiguous chunk of the file. The file system is thus faced with 1 This scenario arises in many situations. The file may contain raw input data or may be a scratch file written in a previous phase of the application. <p> The small record size was 8 bytes, the size of a double-precision floating point number. The large record size was 8192 bytes, the size of a file-system block and cache buffer. These record-size choices are reasonable <ref> [KN94] </ref>. We also tried 1024-byte and 4096-byte records (Figure 12), leading to results between the 8-byte and 8192-byte results; we present only the extremes here. 12 6 Results A note on the results: the numbers have been updated since the earlier version of this TR and since the OSDI paper.
Reference: [KTR94] <author> David Kotz, Song Bac Toh, and Sriram Radhakrishnan. </author> <title> A detailed simulation model of the HP 97560 disk drive. </title> <type> Technical Report PCS-TR94-220, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Thus, our experiments used the modeled network. We added a disk model, a reimplementation of Ruemmler and Wilkes' HP 97560 model <ref> [RW94, KTR94] </ref>. We validated our model against disk traces provided by HP, using the same technique and measure as Ruemmler and Wilkes.
Reference: [LIN + 93] <author> Susan J. LoVerso, Marshall Isman, Andy Nanopoulos, William Nesheim, Ewan D. Milne, and Richard Wheeler. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer USENIX Conference, </booktitle> <pages> pages 291-305, </pages> <year> 1993. </year>
Reference-contexts: Indeed, since most multiprocessor file systems <ref> [CF94, FPD93, Pie89, Roy93, DdR92, LIN + 93, BGST93, Dib90, DSE88] </ref> decluster file data across many disks, each application request may be broken into even smaller requests that are sent to different IOPs.
Reference: [MK91] <author> Ethan L. Miller and Randy H. Katz. </author> <title> Input/output behavior of supercomputer applications. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <pages> pages 567-576, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: Poor performance is not surprising because the Unix file system was designed for a general-purpose workload [OCH + 85], rather than for a parallel, scientific workload. Scientific applications use larger files and have more sequential access <ref> [MK91, GGL93, PP93] </ref>. Parallel scientific programs access the file with patterns not seen in uniprocessor or distributed-system workloads, in particular, complex strided access to discontiguous pieces of the file [KN94, NK94].
Reference: [Nit92] <author> Bill Nitzberg. </author> <title> Performance of the iPSC/860 Concurrent File System. </title> <type> Technical Report RND-92-020, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Valuable semantic information | that a large, contiguous, parallel file transfer is in progress | is lost through this low-level interface. A collective-I/O interface, in which all CPs cooperate to make a single, large request, retains this semantic information, making it easier to coordinate I/O for better performance <ref> [dBC93, Nit92, PGK88] </ref>. Collective I/O need not involve matrices. Many out-of-core parallel algorithms do I/O in "mem-oryloads," that is, they repeatedly load some subset of the file into memory, process it, and write it out [CK93]. Each transfer is a large, but not necessarily contiguous, set of data.
Reference: [NK94] <author> Nils Nieuwejaar and David Kotz. </author> <title> A multiprocessor extension to the conventional file system interface. </title> <type> Technical Report PCS-TR94-230, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: Scientific applications use larger files and have more sequential access [MK91, GGL93, PP93]. Parallel scientific programs access the file with patterns not seen in uniprocessor or distributed-system workloads, in particular, complex strided access to discontiguous pieces of the file <ref> [KN94, NK94] </ref>. Finally, scientific applications use files for more than loading raw data and storing results; files are used as scratch space for very large problems as application-controlled virtual memory [CK93]. In short, multiprocessors need new file systems that are designed for parallel scientific applications.
Reference: [OCH + 85] <author> John Ousterhout, Herve Da Costa, David Harrison, John Kunze, Mike Kupfer, and James Thompson. </author> <title> A trace driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-24, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: This revised technical report updates a few of the traditional-caching numbers from the OSDI paper (see page 13), but makes no qualitative changes. 1 is often poor. Poor performance is not surprising because the Unix file system was designed for a general-purpose workload <ref> [OCH + 85] </ref>, rather than for a parallel, scientific workload. Scientific applications use larger files and have more sequential access [MK91, GGL93, PP93].
Reference: [PGK88] <author> David Patterson, Garth Gibson, and Randy Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In ACM SIGMOD Conference, </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Valuable semantic information | that a large, contiguous, parallel file transfer is in progress | is lost through this low-level interface. A collective-I/O interface, in which all CPs cooperate to make a single, large request, retains this semantic information, making it easier to coordinate I/O for better performance <ref> [dBC93, Nit92, PGK88] </ref>. Collective I/O need not involve matrices. Many out-of-core parallel algorithms do I/O in "mem-oryloads," that is, they repeatedly load some subset of the file into memory, process it, and write it out [CK93]. Each transfer is a large, but not necessarily contiguous, set of data.
Reference: [Pie89] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year> <month> 30 </month>
Reference-contexts: Indeed, since most multiprocessor file systems <ref> [CF94, FPD93, Pie89, Roy93, DdR92, LIN + 93, BGST93, Dib90, DSE88] </ref> decluster file data across many disks, each application request may be broken into even smaller requests that are sent to different IOPs. <p> We discuss three implementation alternatives: traditional caching, two-phase I/O, and disk-directed I/O. The latter two require a collective-I/O interface similar to that of Galbreath et al [GGL93], above. Traditional caching. This alternative mimics a "traditional" parallel file system like Intel CFS <ref> [Pie89] </ref>, with no explicit collective-I/O interface and with IOPs that each manage a file cache. 4 the corresponding function executed at the IOP to service each incoming CP request. Recall that each application process must call ReadCP once for each contiguous chunk of the file, no matter how small.
Reference: [PP93] <author> Barbara K. Pasquale and George C. Polyzos. </author> <title> A static analysis of I/O characteristics of scientific applications in a production workload. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 388-397, </pages> <year> 1993. </year>
Reference-contexts: Poor performance is not surprising because the Unix file system was designed for a general-purpose workload [OCH + 85], rather than for a parallel, scientific workload. Scientific applications use larger files and have more sequential access <ref> [MK91, GGL93, PP93] </ref>. Parallel scientific programs access the file with patterns not seen in uniprocessor or distributed-system workloads, in particular, complex strided access to discontiguous pieces of the file [KN94, NK94].
Reference: [Roy93] <author> Paul J. Roy. </author> <title> Unix file access and caching in a multicomputer environment. </title> <booktitle> In Proceedings of the Usenix Mach III Symposium, </booktitle> <pages> pages 21-37, </pages> <year> 1993. </year>
Reference-contexts: Indeed, since most multiprocessor file systems <ref> [CF94, FPD93, Pie89, Roy93, DdR92, LIN + 93, BGST93, Dib90, DSE88] </ref> decluster file data across many disks, each application request may be broken into even smaller requests that are sent to different IOPs.
Reference: [RW94] <author> Chris Ruemmler and John Wilkes. </author> <title> An introduction to disk drive modeling. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 17-28, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Thus, our experiments used the modeled network. We added a disk model, a reimplementation of Ruemmler and Wilkes' HP 97560 model <ref> [RW94, KTR94] </ref>. We validated our model against disk traces provided by HP, using the same technique and measure as Ruemmler and Wilkes.
Reference: [WMR + 94] <author> Stephen R. Wheat, Arthur B. Maccabe, Rolf Riesen, David W. van Dresser, and T. Mack Stallcup. PUMA: </author> <title> An operating system for massively parallel systems. </title> <booktitle> In Proceedings of the Twenty-Seventh Annual Hawaii International Conference on System Sciences, </booktitle> <year> 1994. </year> <note> Many of these papers can be found at http://www.cs.dartmouth.edu/pario.html The disk-model software can be found at http://www.cs.dartmouth.edu/cs archive/diskmodel.html 31 </note>
Reference-contexts: Finally, our Memput and Memget operations are not unusual. Similar remote-memory-access mechanisms are supported in a variety of distributed-memory systems <ref> [WMR + 94, CDG + 93] </ref>. 7.1 Comparison to Two-phase I/O The above results clearly show the benefits of disk-directed I/O over traditional caching.
References-found: 32

