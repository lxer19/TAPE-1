URL: http://www.cs.columbia.edu/~leana/ps/ieee-deb.ps
Refering-URL: http://www.cs.columbia.edu/~leana/E6998-42/
Root-URL: http://www.cs.columbia.edu
Title: Fault Tolerance Issues in Data Declustering for Parallel Database Systems  
Author: Leana Golubchik Richard R. Muntz 
Date: September, 1994.)  
Note: (Appeared in Bulletin of the Technical Committee on Data Engineering,  
Affiliation: UCLA Computer Science Department  
Abstract: Maintaining the integrity of data and its accessibility are crucial tasks in database systems. Although each component in the storage hierarchy can be fairly reliable, a large collection of such components is prone to failure; this is especially true of the secondary storage system which normally contains a large number of magnetic disks. In designing a fault tolerant secondary storage system, one should keep in mind that failures, although potentially devastating, are expected to occur fairly infrequently; hence, it is important to provide reliability techniques that do not (significantly) hinder the system's performance during normal operation. Furthermore, it is desirable to maintain a reasonable level of performance under failure as well. Since high degrees of reliability are traditionally achieved through the use of duplicate components and redundant information, it is also reasonable to use these redundancies in improving the system's performance during normal operation. In this article we concentrate on techniques for improving reliability of secondary storage systems as well as the resulting system performance during normal operation and under failure.
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> Fault Tolerant Disk Drive Matrix, Patent 5,303,244, </editor> <address> Granted April 12, </address> <year> 1994. </year> <title> AT&T Global Information Solutions. </title>
Reference-contexts: However, the orthogonal organization described above does not exhibit this property. Note that in that organization, a disk failure creates an addition load only on the disks belonging to the same disk array as the failed disk. In <ref> [1] </ref>, the authors propose another approach, termed a disk matrix 24 , which also guards against single points of failure but with an additional benefit of evenly distributing the additional load due to a failure over all the disks in the system. <p> among the disks in the disk matrix according to the following rules: 1) no two blocks from the same parity group end up on the same disk string and 2) the increase in the load due to a disk failure is evenly distributed among all the disks in the matrix <ref> [1] </ref>; this is illustrated in Figure 7 (b). Due to a lack of space we do not describe this scheme any further but refer the interested reader to [1]. <p> and 2) the increase in the load due to a disk failure is evenly distributed among all the disks in the matrix <ref> [1] </ref>; this is illustrated in Figure 7 (b). Due to a lack of space we do not describe this scheme any further but refer the interested reader to [1].
Reference: [2] <author> NonStop SQL, </author> <title> A Distributed, High-performance, High-reliablity Implementaion of SQL. </title> <type> Technical Report No. 82317, </type> <institution> Tandem Database Group, March,1987. </institution>
Reference-contexts: Examples of parity based schemes include RAIDs [31], clustered RAIDs [30] and various parity striping schemes [16]. Proper reliability techniques can increase mean time to data loss (MTTDL) to millions of hours [21]. Full mirroring <ref> [2] </ref> is a special case of parity striping (with d = 1), where each disk is replicated on another disk; whenever a disk fails, its mirror can be used to retrieve the missing data. The disk farm is composed of a number of such pairs. <p> Note that, one can have logical level replication and not do dynamic load balancing, i.e., just use the replication for reliability and (static) redistribution of load after failure. 6 These schemes were originally suggested as logical level schemes; thus we discuss them in that context. 4 3.1 Mirroring/Shadowing Disk shadowing <ref> [5, 2] </ref> refers to maintaining two (mirrored disk) or more (shadow set) identical disk images on different disks, mainly for the purpose of providing a highly reliable disk subsystem.
Reference: [3] <institution> DBC/1012 database computer system manual release 2.0. </institution> <type> Technical Report Document No. </type> <institution> C10-0001-02, Teradata Corporation, </institution> <month> Nov </month> <year> 1985. </year>
Reference-contexts: The I/O controller generally handles the replication and higher levels of software, such as the query optimizer, are not concerned 4 , i.e., higher levels of software just see a collection of reliable disks with some changes in performance characteristics. With logical fragmentation as in the Teradata <ref> [3] </ref> and Gamma [12] shared nothing database machines, relations are fragmented and relation fragments are stored on independent nodes of the system. Replication is visible to the query processing software and is managed by the database system software. <p> The authors argue that two copies are sufficient to provide a high degree of reliability, but that more than two copies can result in significant performance improvements. 5 3.2 Interleaved Declustering In <ref> [3, 11] </ref> interleaved declustering is considered as a replication scheme at the logical level (see Section 2). It can also provide an alternative to the mirroring 7 scheme, if applied at the physical level. <p> possible to use both copies of the data to service read requests; however, with logical level replication, concurrency control and buffer management issues must be considered (see Section 2). 9 This could be a significant problem, for instance, in a shared-nothing database machine (see Section 1), such as the DBC/1012 <ref> [3] </ref>, where the performance of the "slowest" node limits the performance of the entire system. 10 Note that, this argument is an approximation, i.e., it only takes into consideration combinations of 2 failures. <p> This scheme can also provide an alternative to the classical mirroring scheme when applied to physical level replication, as well as to the interleaved declustering scheme described in <ref> [3, 11] </ref>. We briefly describe the concept of chained declustering from [20]. Chained declustering has the same storage overhead as compared to the classic mirroring scheme and interleaved declustering, but, like interleaved declustering, it offers better performance degradation properties when a single disk failure occurs.
Reference: [4] <author> D. Bitton. </author> <title> Arm scheduling in shadowed disks. </title> <booktitle> COMPCON, </booktitle> <pages> pages 132-136, </pages> <month> Spring </month> <year> 1989. </year>
Reference-contexts: With multiple data paths, a shadow set can service several read requests in parallel, thus improving the throughput of the disk subsystem. Furthermore, expected seek times for read requests can be improved by chosing the disk in the shadow set with the minimum seek distance <ref> [5, 4] </ref>. This leads to a need for disk scheduling policies to exploit these possibilities. Such policies for mirrored disk subsystems are studied in [36]; disk scheduling policies for real-time applications using mirrored disks are studied in [9].
Reference: [5] <author> D. Bitton and J. Gray. </author> <title> Disk shadowing. </title> <booktitle> VLDB, </booktitle> <pages> pages 331-338, </pages> <year> 1988. </year>
Reference-contexts: Full mirroring has a higher storage overhead than other parity based schemes with d &gt; 1 (e.g., RAID) because data is fully duplicated, but it can offer better performance in terms of throughput and response time [16] than the parity based schemes. For instance, in <ref> [5] </ref>, the authors exploit the availability of two copies of the data to optimize seek times in a mirrored disks environment. The amount of redundant information stored determines the storage overhead for providing reliability and the system's resiliency to disk failure. <p> Since high degrees of reliability and availability are achieved through the use of redundant information (and duplicate components), it is also reasonable to use these redundancies in improving the system's performance during normal operation, e.g., as in mirrored disk systems <ref> [5] </ref> (see Section 3 for more details). <p> Note that, one can have logical level replication and not do dynamic load balancing, i.e., just use the replication for reliability and (static) redistribution of load after failure. 6 These schemes were originally suggested as logical level schemes; thus we discuss them in that context. 4 3.1 Mirroring/Shadowing Disk shadowing <ref> [5, 2] </ref> refers to maintaining two (mirrored disk) or more (shadow set) identical disk images on different disks, mainly for the purpose of providing a highly reliable disk subsystem. <p> With multiple data paths, a shadow set can service several read requests in parallel, thus improving the throughput of the disk subsystem. Furthermore, expected seek times for read requests can be improved by chosing the disk in the shadow set with the minimum seek distance <ref> [5, 4] </ref>. This leads to a need for disk scheduling policies to exploit these possibilities. Such policies for mirrored disk subsystems are studied in [36]; disk scheduling policies for real-time applications using mirrored disks are studied in [9]. <p> This leads to a need for disk scheduling policies to exploit these possibilities. Such policies for mirrored disk subsystems are studied in [36]; disk scheduling policies for real-time applications using mirrored disks are studied in [9]. One interesting question that is addressed in <ref> [5] </ref> is whether it makes sense to have more than 2 disks in a shadow set.
Reference: [6] <author> P. Chen. </author> <title> An evaluation of redundant arrays of disks using an Amdahl 5890. </title> <type> Technical Report UCB/CSD 89/506, </type> <institution> UC Berkeley, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: The MTTDL of a RAID is easily shown to be inversely proportional to the rebuild time <ref> [6, 11] </ref>; in the RAID system described in [31], rebuilding the failed disk contents at maximum speed (the capacity of the standby disk) results in the use of the 2 entire capacity of the surviving disks in the array.
Reference: [7] <author> P. Chen, G. A. Gibson, R. H. Katz, and D. A. Patterson. </author> <title> An evaluation of redundant arrays of disks using an Amdahl 5890. </title> <booktitle> ACM SIGMETRICS Conference, </booktitle> <pages> pages 74-85, </pages> <year> 1990. </year>
Reference-contexts: When designing a fault tolerance scheme, the following aspects of the disk subsystem must be examined: a) performance under normal operation (e.g., <ref> [7, 31] </ref>), b) mean time to data loss (or system failure) (e.g. [14]), and c) performance of the disk subsystem under failure, i.e., when one or more disks are inoperable or inaccessible (e.g., [35, 30, 17, 19, 18]).
Reference: [8] <author> Peter M. Chen and David A. Patterson. </author> <title> Maximizing Performance in a Striped Disk Array. </title> <publisher> ISCA, </publisher> <pages> pages 322-331, </pages> <year> 1990. </year>
Reference-contexts: is due to the common assumption that rotationally synchronized disks do not perform independent accesses; hence, they are viewed as a single unit, with N fl rate of a single disk, and which can satisfy one request at a time. (An exception to this view is the work presented in <ref> [8] </ref>, where the authors describe workloads under which it would be beneficial to use larger striping units in synchronized, i.e., RAID3, disk array organizations.) The advantages of a traditional byte interleaved RAID3 are: 1) high bandwidth, 2) high reliability, and 3) its performance in degraded modes (since every request results in <p> Since most disk controllers can detect which disk has failed, this is not necessary. Thus, we do not discuss the RAID2 organization any further. 17 What is the desirable block size depends on the system's expected workload (e.g., see <ref> [8] </ref>). 18 Performance consequences of several parity placement schemes for RAID systems are investigated in [24, 25], where the authors show that, for certain types of workloads, a "proper" choice of parity placement can result in a significant performance improvement. 10 a dedicated parity disk, as in the example of Figure
Reference: [9] <author> S. Chen and D. Towsley. </author> <title> Performance of a mirrored disk in a real-time transaction system. </title> <booktitle> ACM Sigmetrics 1991, </booktitle> <pages> pages 198-207, </pages> <year> 1991. </year>
Reference-contexts: This leads to a need for disk scheduling policies to exploit these possibilities. Such policies for mirrored disk subsystems are studied in [36]; disk scheduling policies for real-time applications using mirrored disks are studied in <ref> [9] </ref>. One interesting question that is addressed in [5] is whether it makes sense to have more than 2 disks in a shadow set.
Reference: [10] <author> S. Chen and D. Towsley. </author> <title> The Design and Evaluation of RAID5 and Parity Striping Disk Array Architecture. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 58-74, </pages> <year> 1993. </year>
Reference-contexts: In [16], a system using parity striping is analyzed and its performance is compared to that of a system using mirrored disks and a system using RAID5. Another comparison of RAID5 and parity striping performance (under normal operation) can be found in <ref> [10] </ref>. Due to a lack of space, we do not discuss these works here. 5 Summary In summary, we have discussed two basic categories of schemes that store redundant information for the purpose of reliability; these are: 1) full replication schemes and 2) schemes using parity information.
Reference: [11] <author> G. Copeland and T. Keller. </author> <title> A Comparison of High-Availability Media Recovery Techniques. </title> <booktitle> ACM SIGMOD Conference, </booktitle> <pages> pages 98-109, </pages> <year> 1989. </year>
Reference-contexts: The MTTDL of a RAID is easily shown to be inversely proportional to the rebuild time <ref> [6, 11] </ref>; in the RAID system described in [31], rebuilding the failed disk contents at maximum speed (the capacity of the standby disk) results in the use of the 2 entire capacity of the surviving disks in the array. <p> The authors argue that two copies are sufficient to provide a high degree of reliability, but that more than two copies can result in significant performance improvements. 5 3.2 Interleaved Declustering In <ref> [3, 11] </ref> interleaved declustering is considered as a replication scheme at the logical level (see Section 2). It can also provide an alternative to the mirroring 7 scheme, if applied at the physical level. <p> This scheme can also provide an alternative to the classical mirroring scheme when applied to physical level replication, as well as to the interleaved declustering scheme described in <ref> [3, 11] </ref>. We briefly describe the concept of chained declustering from [20]. Chained declustering has the same storage overhead as compared to the classic mirroring scheme and interleaved declustering, but, like interleaved declustering, it offers better performance degradation properties when a single disk failure occurs.
Reference: [12] <author> David J. Dewitt, R. Gerber, G. Graefe, M. Heytens, K.Kumar, and M.Muralikrishna. </author> <title> Gamma : A high performance dataflow database machine. </title> <booktitle> VLDB Conference, </booktitle> <pages> pages 228-240, </pages> <year> 1986. </year>
Reference-contexts: With logical fragmentation as in the Teradata [3] and Gamma <ref> [12] </ref> shared nothing database machines, relations are fragmented and relation fragments are stored on independent nodes of the system. Replication is visible to the query processing software and is managed by the database system software.
Reference: [13] <author> S. Ghandeharizadeh and D. J. DeWitt. </author> <title> Hybrid-Range Partitioning Strategy: A New Declustering Strategy for Multiprocessor Database Machines. </title> <booktitle> VLDB, </booktitle> <pages> pages 481-492, </pages> <year> 1990. </year>
Reference-contexts: For instance, if the size of the fragments becomes very small (which would happen if a relatively small file was distributed over many disks), then some types of queries would have to be serviced by accessing multiple disks, and this can result in increased overhead <ref> [13] </ref>. 8 data devices, where N is the stripe width. Each set of N data blocks is protected by one parity block; for instance in Figure 3, p 0 = d 0 d 1 d 2 .
Reference: [14] <author> Garth A. Gibson. </author> <title> Performance and Reliability in Redundant Arrays of Inexpensive Disks. </title> <booktitle> 1989 Computer Measurement Group (CMG) Annual Conference Proceedings, </booktitle> <month> December </month> <year> 1989. </year>
Reference-contexts: When designing a fault tolerance scheme, the following aspects of the disk subsystem must be examined: a) performance under normal operation (e.g., [7, 31]), b) mean time to data loss (or system failure) (e.g. <ref> [14] </ref>), and c) performance of the disk subsystem under failure, i.e., when one or more disks are inoperable or inaccessible (e.g., [35, 30, 17, 19, 18]). <p> the additional failure of one of the surviving disks can cause data loss.) To this purpose, "hot standby" disks (or spares) are often provided, and the system is designed to automatically rebuild the contents of the failed disk on the standby disk, using the redundant information on the surviving disks <ref> [14] </ref>. The parity group size effects: a) the time required to rebuild a failed disk (and therefore the MTTDL) and, b) the workload (measured in accesses per second per disk) that can be supported during the rebuild process, and c) the system's performance under failure. <p> by relaxing this condition, e) interleave unit size (stripe granularity) determines the number of devices that are involved in an access, and and hence it affects the system's performance during normal operation (we do not discuss this any further due to lack of space but refer the interested reader to <ref> [14] </ref> for a performance comparison between byte interleaved, block interleaved, and mirrored systems under normal operation), f ) number of spares affects the reliability of the I/O subsystem (see Section 4.3), and g) reconstruction time (or vulnerability window) is of crucial importance, because a system operating under failure is not only <p> The various approaches to improving the reconstruction process are discussed in Section 4.5. In this section, we first address the question of "how many spares do we need?". In <ref> [14] </ref>, the authors address this issue by simulating a disk array with 7 parity groups (or strings as they are called in [14]) and varying the size of the spare disk pool. <p> In this section, we first address the question of "how many spares do we need?". In <ref> [14] </ref>, the authors address this issue by simulating a disk array with 7 parity groups (or strings as they are called in [14]) and varying the size of the spare disk pool. <p> However, there are other components in the I/O subsystem that deserve attention, such as controllers, power supplies, cabling, cooling systems, etc. In <ref> [14] </ref>, the authors point out that such support hardware is normally shared by a disk string (all the disks on one bus), as illustrated in Figure 7 (a). <p> Thus, disks sharing the same support hardware should not belong to the same disk array. In fact, the disk arrays should be constructed orthogonally to the support hardware groups [32]. In <ref> [14] </ref>, the authors compare the MTTDL of an array with an non-orthogonal organization to that of an array with an orthogonal organization and show a significant improvement in reliability.
Reference: [15] <author> Leana Golubchik, John C.S. Lui, and Richard R. Muntz. </author> <title> Chained declustering: Load balancing and robustness to skew and failure. </title> <booktitle> RIDE-TQP Workshop, </booktitle> <month> February </month> <year> 1992. </year> <month> 16 </month>
Reference-contexts: For reads, load balancing decisions can be made by the query processing software, i.e., at the logical level 5 , as in [20], or they can be deferred until the time of the actual I/O operation, i.e., performed by the disk controller at the physical level, as in <ref> [15] </ref>. Note that, the dynamic scheduling studies that are discussed in this article, specifically in the context of chained declustering (see Section 3.3), can be applied to both physical and logical replication methods. <p> As already mentioned, several dynamic balancing schemes are discussed in [36], in the context of mirrored disks systems. In <ref> [15] </ref>, authors investigate the degree to which a dynamic load balancing disk scheduling algorithm in conjunction with chained declustering can respond robustly to variations in workload and disk failures (which destroy the symmetry of the system and introduce skewed load); they demonstrate that simple dynamic scheduling algorithms can greatly improve the
Reference: [16] <author> Jim Gray, Bob Horst, and Mark Walker. </author> <title> Parity striping of disk arrays: Low-cost reliable storage with acceptable throughput. </title> <booktitle> VLDB Conference, </booktitle> <pages> pages 148-172, </pages> <year> 1990. </year>
Reference-contexts: For instance, transaction processing systems (i.e., with OLTP workload) are under stringent system responsiveness requirements, e.g., 99 percent of all transactions must be completed within 1 second. Such systems are configured according to the number of I/Os/second desired, rather than the number of MBs necessary to store the data <ref> [16] </ref>. <p> Whenever a disk fails, a data block on the failed disk can be reconstructed by reading and computing the exclusive OR of the corresponding parity and data blocks. Examples of parity based schemes include RAIDs [31], clustered RAIDs [30] and various parity striping schemes <ref> [16] </ref>. Proper reliability techniques can increase mean time to data loss (MTTDL) to millions of hours [21]. <p> The disk farm is composed of a number of such pairs. Full mirroring has a higher storage overhead than other parity based schemes with d &gt; 1 (e.g., RAID) because data is fully duplicated, but it can offer better performance in terms of throughput and response time <ref> [16] </ref> than the parity based schemes. For instance, in [5], the authors exploit the availability of two copies of the data to optimize seek times in a mirrored disks environment. The amount of redundant information stored determines the storage overhead for providing reliability and the system's resiliency to disk failure. <p> There are several disadvantages to disk shadowing. Firstly, there is the cost. Mirroring has a 100% storage overhead. This is not a severe problem if the expected workload is of the OLTP type. According to <ref> [16] </ref>, OLTP systems have stringent responsiveness requirements; in order to avoid long queues of requests for the data, the disks in such systems are usually purchased for their arms and not for their capacity. Secondly, there is the "write" overhead. <p> To remedy this problem, we can use a parity based scheme. In this section we discuss three variations on such schemes: 1) redundant array of inexpensive (or independent) disks (RAID) [31], 2) clustered RAID [30], and 3) parity striping <ref> [16] </ref>. <p> The parallel reconstruction process requires additional buffer space to hold the data blocks that have been read from the surviving disk, but have not (yet) been used to reconstruct the missing data. 24 The disk matrix is a generalization of the clustered disk array idea. 14 4.7 Parity Striping In <ref> [16] </ref>, the authors point out why traditional RAID5 organization [31] might not be the best solution for all types of workloads, and more specifically for OLTP workloads (i.e., workloads with relatively small accesses). <p> In <ref> [16] </ref>, a system using parity striping is analyzed and its performance is compared to that of a system using mirrored disks and a system using RAID5. Another comparison of RAID5 and parity striping performance (under normal operation) can be found in [10].
Reference: [17] <author> M. Holland and G. A. Gibson. </author> <title> Parity Declustring for Continuous Operation in Redundant Disk Arrays. </title> <booktitle> In 5th Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> Boston, MA, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: scheme, the following aspects of the disk subsystem must be examined: a) performance under normal operation (e.g., [7, 31]), b) mean time to data loss (or system failure) (e.g. [14]), and c) performance of the disk subsystem under failure, i.e., when one or more disks are inoperable or inaccessible (e.g., <ref> [35, 30, 17, 19, 18] </ref>). We should keep in mind that failures are expected to occur relatively infrequently, so most of the time a system is in a fully operational mode. Thus, it is important to provide reliability techniques that do not (significantly) hinder the system's performance during normal operation. <p> This is the problem of computing, for a given data block, the location of its "buddy" data blocks and parity block (i.e., the rest of the blocks in the parity group), which is addressed in <ref> [17, 29] </ref>. 4.5 Recovery Procedures Several reconstruction schemes are suggested in [30]; these include: a) basic rebuild, where the data is read from the surviving disks, reconstructed through a parity computation, and then written to the spare disk, b) rebuild with read-redirect, where, in addition, read requests, for the portion of <p> In all three schemes, the authors [30] suggest that the write requests to the failed disk should always be redirected to the standby disk. In <ref> [17] </ref> the authors question this decision and investigate another recovery algorithm, in addition to the three proposed in [30], which they refer to as the minimal-update algorithm; in this scheme, updates to the failed disk are ignored, whenever possible. <p> In particular, in light to moderate loads with G1 C1 &lt; 0:5, the schemes with no redirection result in a shorter reconstruction period. The reason <ref> [17] </ref> is that the benefits of o*oading the surviving disks do not outweigh the penalty of loading the replacement disk with random workload, unless the surviving disks are highly utilized. <p> Another way to reduce the reconstruction period is to start multiple (independent) reconstruction processes in parallel. In <ref> [17] </ref>, the authors note that a single reconstruction process (or in lock step reconstruction) 22 is not always able to highly utilize a disk array, especially when G1 C1 is relative small; in that paper, the authors investigate the benefits of using an 8-way parallel 22 By a single reconstruction process
Reference: [18] <author> M. Holland, G. A. Gibson, and D. P. Siewiorek. </author> <title> Architectures and Algorithms for On-Line Failure Recovery in Redundant Disk Arrays. </title> <note> Submitted to the Journal of Distributed and Parallel Databases. </note>
Reference-contexts: scheme, the following aspects of the disk subsystem must be examined: a) performance under normal operation (e.g., [7, 31]), b) mean time to data loss (or system failure) (e.g. [14]), and c) performance of the disk subsystem under failure, i.e., when one or more disks are inoperable or inaccessible (e.g., <ref> [35, 30, 17, 19, 18] </ref>). We should keep in mind that failures are expected to occur relatively infrequently, so most of the time a system is in a fully operational mode. Thus, it is important to provide reliability techniques that do not (significantly) hinder the system's performance during normal operation.
Reference: [19] <author> M. Holland, G. A. Gibson, and D. P. Siewiorek. </author> <title> Fast, On-Line Failure Recovery in Redundant Disk Arrays. </title> <booktitle> In 23rd Annual International Symposium on Fault-Tolerant Computing, </booktitle> <year> 1993. </year>
Reference-contexts: scheme, the following aspects of the disk subsystem must be examined: a) performance under normal operation (e.g., [7, 31]), b) mean time to data loss (or system failure) (e.g. [14]), and c) performance of the disk subsystem under failure, i.e., when one or more disks are inoperable or inaccessible (e.g., <ref> [35, 30, 17, 19, 18] </ref>). We should keep in mind that failures are expected to occur relatively infrequently, so most of the time a system is in a fully operational mode. Thus, it is important to provide reliability techniques that do not (significantly) hinder the system's performance during normal operation.
Reference: [20] <author> H. Hsiao and D. J. DeWitt. </author> <title> Chained Declustering: A New Availability Strategy for Multiprocessor Database Machines. </title> <booktitle> Proc. of Data Engineering, </booktitle> <pages> pages 456-465, </pages> <year> 1990. </year>
Reference-contexts: Such complex operations are typically limited by any imbalance 3 in the system, which can be caused either by a skew in the workload [23] or by a disk array with a diminished capacity, due to a failure <ref> [20] </ref>. For example, in a one hundred disk system, a single failed disk represents a loss of only 1% of the raw I/O capacity of the system. <p> For reads, load balancing decisions can be made by the query processing software, i.e., at the logical level 5 , as in <ref> [20] </ref>, or they can be deferred until the time of the actual I/O operation, i.e., performed by the disk controller at the physical level, as in [15]. <p> To make precise calculations, we must take into consideration combinations of 3 or more failures; however, these are much less probable than combinations of 2 failures. 6 3.3 Chained Declustering In <ref> [20] </ref>, chained declustering is considered as a replication scheme at the logical level of a shared nothing database machine. This scheme can also provide an alternative to the classical mirroring scheme when applied to physical level replication, as well as to the interleaved declustering scheme described in [3, 11]. <p> This scheme can also provide an alternative to the classical mirroring scheme when applied to physical level replication, as well as to the interleaved declustering scheme described in [3, 11]. We briefly describe the concept of chained declustering from <ref> [20] </ref>. Chained declustering has the same storage overhead as compared to the classic mirroring scheme and interleaved declustering, but, like interleaved declustering, it offers better performance degradation properties when a single disk failure occurs. Figure 2 illustrates the chained declustering concept. <p> When a disk failure occurs (e.g. disk 1 in Figure 2), the chained declustering scheme is able to adjust the additional read workload to both copies of the data in such a way as to balance it evenly among the surviving disks; this results in a less degraded performance (see <ref> [20] </ref> for more details). There are several ways to perform the load adjustment depending on table declustering methods, storage organization, and access plans. Since data is logically replicated, the query scheduler chooses an access plan in order to balance the load. <p> Chained declustering has the same storage overhead as mirroring and interleaved declustering. But, it has a higher reliability than interleaved declustering (but not as high as mirroring) <ref> [20] </ref>.
Reference: [21] <author> R. Katz, D. W. Gordon, and J. A. Tuttle. </author> <title> Storage System Metrics for Evaluating Disk Array Organization. </title>
Reference-contexts: Examples of parity based schemes include RAIDs [31], clustered RAIDs [30] and various parity striping schemes [16]. Proper reliability techniques can increase mean time to data loss (MTTDL) to millions of hours <ref> [21] </ref>. Full mirroring [2] is a special case of parity striping (with d = 1), where each disk is replicated on another disk; whenever a disk fails, its mirror can be used to retrieve the missing data. The disk farm is composed of a number of such pairs.
Reference: [22] <author> M. Y. Kim. </author> <title> Synchronized Disk Iterleaving. </title> <journal> IEEE Trans. on Computers, </journal> <pages> pages 978-988, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: The synchronized RAID3 organization is traditionally byte interleaved, as in <ref> [22, 31] </ref>.
Reference: [23] <author> M. S. Lakshmi and P. S. Yu. </author> <title> Effect of skew on join performance in parallel architectures. </title> <booktitle> In Int. Symposium on Databases in Parallel and Distributed Systems, </booktitle> <pages> pages 107-120, </pages> <year> 1988. </year>
Reference-contexts: Such complex operations are typically limited by any imbalance 3 in the system, which can be caused either by a skew in the workload <ref> [23] </ref> or by a disk array with a diminished capacity, due to a failure [20]. For example, in a one hundred disk system, a single failed disk represents a loss of only 1% of the raw I/O capacity of the system.
Reference: [24] <author> E. Lee. </author> <title> Software and Performance Issues in the Implementation of a RAID Prototype. </title> <month> May </month> <year> 1990. </year>
Reference-contexts: In general, the more redundant information is stored, the lower is the probability that a failure results in data loss, but the higher is the cost of providing reliability. Furthermore, the placement of the redundant information on the disks <ref> [24, 25] </ref> influences the system's behavior during normal operation and under failure as well as its ability to recover quickly and return to the fully operational state. <p> Thus, we do not discuss the RAID2 organization any further. 17 What is the desirable block size depends on the system's expected workload (e.g., see [8]). 18 Performance consequences of several parity placement schemes for RAID systems are investigated in <ref> [24, 25] </ref>, where the authors show that, for certain types of workloads, a "proper" choice of parity placement can result in a significant performance improvement. 10 a dedicated parity disk, as in the example of Figure 3.
Reference: [25] <author> E. Lee and R. Katz. </author> <title> Performance Consequences of Parity Placement in Disk Arrays. </title> <address> pages 190-199, </address> <year> 1991. </year>
Reference-contexts: In general, the more redundant information is stored, the lower is the probability that a failure results in data loss, but the higher is the cost of providing reliability. Furthermore, the placement of the redundant information on the disks <ref> [24, 25] </ref> influences the system's behavior during normal operation and under failure as well as its ability to recover quickly and return to the fully operational state. <p> Thus, we do not discuss the RAID2 organization any further. 17 What is the desirable block size depends on the system's expected workload (e.g., see [8]). 18 Performance consequences of several parity placement schemes for RAID systems are investigated in <ref> [24, 25] </ref>, where the authors show that, for certain types of workloads, a "proper" choice of parity placement can result in a significant performance improvement. 10 a dedicated parity disk, as in the example of Figure 3.
Reference: [26] <author> J. Menon and J. Cortney. </author> <title> The Architecture of a Fault-Tolerant Cached RAID Controller. </title> <booktitle> In 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 76-86, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: However, we should note that RAID4 and RAID5 suffer from performance degradation on "small" write requests, since each non-full stripe write request results in four I/O operations; due to lack of space, we do not discuss this problem here but refer the interested reader to <ref> [27, 26, 33] </ref>. 4.3 Spares As mentioned earlier, reconstructing a failed disk as soon as possible contributes significantly to improving the MTTDL. Of course, to reconstruct a disk, we need a spare one.
Reference: [27] <author> J. Menon and J. Kasson. </author> <title> Methods for Improved Update Performance of Disk Arrays. </title> <booktitle> Proceedings of the Hawaii International Conference on System Sciences, </booktitle> <pages> pages 74-83, </pages> <year> 1992. </year>
Reference-contexts: However, we should note that RAID4 and RAID5 suffer from performance degradation on "small" write requests, since each non-full stripe write request results in four I/O operations; due to lack of space, we do not discuss this problem here but refer the interested reader to <ref> [27, 26, 33] </ref>. 4.3 Spares As mentioned earlier, reconstructing a failed disk as soon as possible contributes significantly to improving the MTTDL. Of course, to reconstruct a disk, we need a spare one.
Reference: [28] <author> J. Menon and D. Mattson. </author> <title> Comparison of Sparing Alternatives for Disk Arrays. </title> <booktitle> Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: Another way to use spare disks to improve system performance, both during normal operation and under failure, is to use a distributed spare <ref> [28] </ref> (instead of a dedicated spare).
Reference: [29] <author> A. Merchant and P. S. Yu. </author> <title> Design and Modeling of Clustered RAID. </title> <booktitle> Proceedings of the International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 140-149, </pages> <year> 1992. </year>
Reference-contexts: This is the problem of computing, for a given data block, the location of its "buddy" data blocks and parity block (i.e., the rest of the blocks in the parity group), which is addressed in <ref> [17, 29] </ref>. 4.5 Recovery Procedures Several reconstruction schemes are suggested in [30]; these include: a) basic rebuild, where the data is read from the surviving disks, reconstructed through a parity computation, and then written to the spare disk, b) rebuild with read-redirect, where, in addition, read requests, for the portion of
Reference: [30] <author> Richard R. Muntz and John C.S. Lui. </author> <title> Performance analysis of disk arrays under failure. </title> <booktitle> VLDB Conference, </booktitle> <pages> pages 162-173, </pages> <year> 1990. </year>
Reference-contexts: Whenever a disk fails, a data block on the failed disk can be reconstructed by reading and computing the exclusive OR of the corresponding parity and data blocks. Examples of parity based schemes include RAIDs [31], clustered RAIDs <ref> [30] </ref> and various parity striping schemes [16]. Proper reliability techniques can increase mean time to data loss (MTTDL) to millions of hours [21]. <p> scheme, the following aspects of the disk subsystem must be examined: a) performance under normal operation (e.g., [7, 31]), b) mean time to data loss (or system failure) (e.g. [14]), and c) performance of the disk subsystem under failure, i.e., when one or more disks are inoperable or inaccessible (e.g., <ref> [35, 30, 17, 19, 18] </ref>). We should keep in mind that failures are expected to occur relatively infrequently, so most of the time a system is in a fully operational mode. Thus, it is important to provide reliability techniques that do not (significantly) hinder the system's performance during normal operation. <p> To remedy this problem, we can use a parity based scheme. In this section we discuss three variations on such schemes: 1) redundant array of inexpensive (or independent) disks (RAID) [31], 2) clustered RAID <ref> [30] </ref>, and 3) parity striping [16]. <p> Each set of N data blocks is protected by one parity block; for instance in Figure 3, p 0 = d 0 d 1 d 2 . In general, there are three modes of operation for a disk array <ref> [30] </ref>: 1) normal mode, where all disks are operational, 2) degraded mode, where one (or more) disks have failed, and 3) rebuild mode, where the disks are still down, but the process of reconstructing the missing information on spare disks is in progress. <p> One way to improve the system's performance under failure and at the same time speed up the reconstruction process is to use the clustered array organization, proposed in <ref> [30] </ref>. <p> Hence, an array with G &lt; C would perform better under failure and would have a shorter reconstruction process. An analysis of clustered array's performance under failure, using three different reconstruction schemes (see Section 4.5) can be found in <ref> [30] </ref>; this analysis indicates that there are significant advantages to using the clustered disk array scheme. There remains one problem with respect to implementing the clustered array architecture, which is left open in [30]. <p> clustered array's performance under failure, using three different reconstruction schemes (see Section 4.5) can be found in <ref> [30] </ref>; this analysis indicates that there are significant advantages to using the clustered disk array scheme. There remains one problem with respect to implementing the clustered array architecture, which is left open in [30]. This is the problem of computing, for a given data block, the location of its "buddy" data blocks and parity block (i.e., the rest of the blocks in the parity group), which is addressed in [17, 29]. 4.5 Recovery Procedures Several reconstruction schemes are suggested in [30]; these include: a) <p> left open in <ref> [30] </ref>. This is the problem of computing, for a given data block, the location of its "buddy" data blocks and parity block (i.e., the rest of the blocks in the parity group), which is addressed in [17, 29]. 4.5 Recovery Procedures Several reconstruction schemes are suggested in [30]; these include: a) basic rebuild, where the data is read from the surviving disks, reconstructed through a parity computation, and then written to the spare disk, b) rebuild with read-redirect, where, in addition, read requests, for the portion of the data on the missing disk that has already been reconstructed <p> In all three schemes, the authors <ref> [30] </ref> suggest that the write requests to the failed disk should always be redirected to the standby disk. In [17] the authors question this decision and investigate another recovery algorithm, in addition to the three proposed in [30], which they refer to as the minimal-update algorithm; in this scheme, updates to <p> In all three schemes, the authors <ref> [30] </ref> suggest that the write requests to the failed disk should always be redirected to the standby disk. In [17] the authors question this decision and investigate another recovery algorithm, in addition to the three proposed in [30], which they refer to as the minimal-update algorithm; in this scheme, updates to the failed disk are ignored, whenever possible. A simulation of all four reconstruction algorithms reveals that the two more complex schemes, i.e., read-redirect and piggy-backing, do not consistently reduce the length of the reconstruction period.
Reference: [31] <author> David A. Patterson, Garth Gibson, and Randy H. Katz. </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID). </title> <booktitle> ACM SIGMOD Conference, </booktitle> <pages> pages 109-116, </pages> <year> 1988. </year>
Reference-contexts: To improve the reliability and availability of the secondary storage system, some form of data redundancy must be introduced. One way to introduce redundancy into the system is to use parity based schemes <ref> [31] </ref> which construct a parity block for every d data blocks; the parity block plus the d data blocks constitute a parity group. <p> Whenever a disk fails, a data block on the failed disk can be reconstructed by reading and computing the exclusive OR of the corresponding parity and data blocks. Examples of parity based schemes include RAIDs <ref> [31] </ref>, clustered RAIDs [30] and various parity striping schemes [16]. Proper reliability techniques can increase mean time to data loss (MTTDL) to millions of hours [21]. <p> When designing a fault tolerance scheme, the following aspects of the disk subsystem must be examined: a) performance under normal operation (e.g., <ref> [7, 31] </ref>), b) mean time to data loss (or system failure) (e.g. [14]), and c) performance of the disk subsystem under failure, i.e., when one or more disks are inoperable or inaccessible (e.g., [35, 30, 17, 19, 18]). <p> The MTTDL of a RAID is easily shown to be inversely proportional to the rebuild time [6, 11]; in the RAID system described in <ref> [31] </ref>, rebuilding the failed disk contents at maximum speed (the capacity of the standby disk) results in the use of the 2 entire capacity of the surviving disks in the array. Thus rebuilding at maximum rate means that the array can perform no other work during the rebuild period. <p> To remedy this problem, we can use a parity based scheme. In this section we discuss three variations on such schemes: 1) redundant array of inexpensive (or independent) disks (RAID) <ref> [31] </ref>, 2) clustered RAID [30], and 3) parity striping [16]. <p> availability of data after a failure would not mean much if this data can not be accessed in a "reasonable" amount of time (see Section 4.5 for a discussion of several reconstruction schemes). 4.2 RAID Organizations In this section we describe the different RAID organizations, as they are presented in <ref> [31] </ref>. <p> The synchronized RAID3 organization is traditionally byte interleaved, as in <ref> [22, 31] </ref>. <p> In a traditional RAID architecture, as in <ref> [31] </ref>, it is assumed that the group size is always equal to the cluster size. An example of a system where the group size (G = 4) is less 20 than the cluster size (C = 5) is illustrated in Figure 6. <p> hold the data blocks that have been read from the surviving disk, but have not (yet) been used to reconstruct the missing data. 24 The disk matrix is a generalization of the clustered disk array idea. 14 4.7 Parity Striping In [16], the authors point out why traditional RAID5 organization <ref> [31] </ref> might not be the best solution for all types of workloads, and more specifically for OLTP workloads (i.e., workloads with relatively small accesses).
Reference: [32] <author> M. Schulze, G. Gibson, R. Katz, and D. Patterson. </author> <booktitle> How-Reliable is a RAID? COMPCON, </booktitle> <pages> pages 118-123, </pages> <year> 1989. </year>
Reference-contexts: Thus, disks sharing the same support hardware should not belong to the same disk array. In fact, the disk arrays should be constructed orthogonally to the support hardware groups <ref> [32] </ref>. In [14], the authors compare the MTTDL of an array with an non-orthogonal organization to that of an array with an orthogonal organization and show a significant improvement in reliability.
Reference: [33] <author> D. Stodolsky, G. A. Gibson, and M. Holland. </author> <title> Parity Logging, Overcoming the Small Writes Problem in Redundant Disk Arrays. </title> <booktitle> In 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 64-75, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: However, we should note that RAID4 and RAID5 suffer from performance degradation on "small" write requests, since each non-full stripe write request results in four I/O operations; due to lack of space, we do not discuss this problem here but refer the interested reader to <ref> [27, 26, 33] </ref>. 4.3 Spares As mentioned earlier, reconstructing a failed disk as soon as possible contributes significantly to improving the MTTDL. Of course, to reconstruct a disk, we need a spare one.
Reference: [34] <author> M. Stonebraker. </author> <title> A Case for Shared Nothing. </title> <journal> Database Engineering, </journal> <volume> 9(1), </volume> <year> 1986. </year>
Reference-contexts: In the worst case (a workload of all reads and no writes) this can double the access rate to the surviving disks and thus in effect, cut the capacity of the array in half. Consider for example a shared-nothing <ref> [34] </ref> database machine architecture, where each node contains one or more disk arrays.
Reference: [35] <author> M. Stonebraker and G. A. Schloss. </author> <title> Distributed RAID ANew Multiple Copy Algorithm. </title> <booktitle> Sixth Int'l. Conf on Data Engineering, </booktitle> <pages> pages 430-437, </pages> <year> 1990. </year>
Reference-contexts: scheme, the following aspects of the disk subsystem must be examined: a) performance under normal operation (e.g., [7, 31]), b) mean time to data loss (or system failure) (e.g. [14]), and c) performance of the disk subsystem under failure, i.e., when one or more disks are inoperable or inaccessible (e.g., <ref> [35, 30, 17, 19, 18] </ref>). We should keep in mind that failures are expected to occur relatively infrequently, so most of the time a system is in a fully operational mode. Thus, it is important to provide reliability techniques that do not (significantly) hinder the system's performance during normal operation. <p> there exists another variation on the RAID idea, termed RADD (Redundant Array of Distributed Disks), which is a distributed version of a RAID5 system (refer to Section 4.2 for a discussion on RAID5); we do not discuss it here due to lack of space but refer the interested reader to <ref> [35] </ref>. 4.1 Disk Array Basics The basic organization of an N + 1 disk array is illustrated in Figure 3, where there is a cluster of N + 1 devices with N data devices and one parity device (N = 3).
Reference: [36] <author> D. Towsley, S. Chen, and S. P. Yu. </author> <title> Performance analysis of a fault tolerant mirrored disk system. </title> <booktitle> Proceeding of Performance '90, </booktitle> <pages> pages 239-253, </pages> <year> 1990. </year>
Reference-contexts: Furthermore, expected seek times for read requests can be improved by chosing the disk in the shadow set with the minimum seek distance [5, 4]. This leads to a need for disk scheduling policies to exploit these possibilities. Such policies for mirrored disk subsystems are studied in <ref> [36] </ref>; disk scheduling policies for real-time applications using mirrored disks are studied in [9]. One interesting question that is addressed in [5] is whether it makes sense to have more than 2 disks in a shadow set. <p> Another way to balance the load of the system is to apply some dynamic load balancing scheme, since it can adjust the load on each node in real time to respond to statistical variations 12 . As already mentioned, several dynamic balancing schemes are discussed in <ref> [36] </ref>, in the context of mirrored disks systems.
Reference: [37] <author> Philip S. Yu and Asit Dan. </author> <title> Effect of system dynamics on coupling architectures for transaction processing. </title> <type> Technical Report RC 16606, </type> <institution> IBM T.J. Watson Research Division, </institution> <month> Feb </month> <year> 1991. </year>
Reference-contexts: There are however significant problems associated with dynamic data sharing across multiple nodes of a system, e.g., concurrency control, and efficient use of buffer space <ref> [38, 37] </ref>. We do not address these problems here due to lack of space. With respect to logical replication one can view such studies as an investigation of the potential benefits of dynamic load balancing, particularly with respect to robustness to workload imbalance and disk failure.
Reference: [38] <author> Philip S. Yu and Asit Dan. </author> <title> Impact of affinity on the performance of coupling architectures for transaction processing. </title> <type> Technical Report RC 16431, </type> <institution> IBM T.J. Watson Research Division, </institution> <month> Jan </month> <year> 1991. </year>
Reference-contexts: There are however significant problems associated with dynamic data sharing across multiple nodes of a system, e.g., concurrency control, and efficient use of buffer space <ref> [38, 37] </ref>. We do not address these problems here due to lack of space. With respect to logical replication one can view such studies as an investigation of the potential benefits of dynamic load balancing, particularly with respect to robustness to workload imbalance and disk failure.
References-found: 38

