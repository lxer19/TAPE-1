URL: ftp://ftp.cs.brown.edu/pub/techreports/95/cs95-17.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-95-17.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [AACS87] <author> A. Aggarwal, B. Alpern, A.K. Chandra and M. Snir. </author> <title> A Model for Hierarchical Memory. </title> <booktitle> In Proc. of the 19th ACM Symposium on Theory of Computing, </booktitle> <year> (1987), </year> <pages> 305-314. </pages>
Reference-contexts: In this scenario, the speedup of the n-processor mesh is fi (n 3=2 ). The preceding estimate refers to a straightforward implementation of matrix multiplication; however, as noted in <ref> [AACS87] </ref>, a careful exploitation of locality would enable to contain the access overhead in the uniprocessor to within a factor fi (log n). This simple example illustrates the potential for superlinear speedups under the limiting technology. <p> Machines. We shall consider parallel machines built as interconnections of (processing-element, memory-module) pairs. Such a pair is modeled as a Hierarchical Random Access Machine, or H-RAM, a generalization of the RAM [CR73] introduced by <ref> [AACS87] </ref> (under the name of Hierarchical Memory Model) to capture the higher cost of remote memory access. (See also [S95] and its bibliography.) Definition 1 An f (x)-H-RAM is a random access machine where an access to address x takes time f (x).
Reference: [BP92] <author> G. </author> <title> Bilardi and F.P. Preparata. </title> <booktitle> Horizons of Parallel Computing. In Proc. of INRIA 25th Anniversary Symposium, invited paper, </booktitle> <year> 1992, </year> <note> LNCS Springer-Verlag. To appear in Journal on Parallel and Distributed Computing. </note>
Reference-contexts: Although most parameters are still being improved, there is an emerging consensus that physical limitations to signal propagation speed and device size are becoming increasingly significant. We have recently undertaken an analysis <ref> [BP92] </ref> of a hypothetical environment, called the "limiting technology", where provocatively- the limits of physics are assumed to have been attained, and no further improvements are feasible. <p> Clearly, data locality plays no role in models with uniform access cost, such as the RAM and the P-RAM. The physical computing system we shall adopt (to be formally defined in Section 2) is what appears to be the only scalable machine in the limiting technology <ref> [BP92] </ref>: the mesh, i.e., a uniform lattice (in d 3 dimensions) of processors. In our study, a mesh node is a (CPU, hierarchical memory module) pair for which worst-case private-memory access time is of the same order as the data-exchange time with a near-neighbor unit.
Reference: [BP95] <author> G. Bilardi and F.P. Preparata. </author> <title> Processor-Time Tradeoffs under Bounded-Speed Message Propagation: Part II, Lower Bounds. </title> <type> Manuscript, </type> <year> 1995. </year>
Reference-contexts: The significance of Theorem 1 is in relation to applications for which the larger machine computation exactly reflects the locality. Such computation exist, as we have shown in a companion paper <ref> [BP95] </ref> yielding matching lower bounds (for most values of n; p, and m). Therefore, no improvement is possible beyond the results of Theorem 1, thereby showing that locality slowdown is an inherent feature of limiting technology simulations.
Reference: [B74] <author> R.P. Brent. </author> <title> The Parallel Evaluation of General Arithmetic Expressions. </title> <journal> Journal of the ACM, </journal> (21)2:201-206, 1974. 
Reference-contexts: Moreover, the processor-time tradeoff in the limiting technology can be different from the classical tradeoff embodied by Brent's Principle <ref> [B74, J92] </ref>, whereby a computation running for T steps on n processors can be emulated in at most dn=peT steps on p &lt; n processors of the same type.
Reference: [CR73] <author> S.A. Cook and R.A. Reckhow. </author> <title> Time Bounded Random Access Machines. </title> <journal> Journal of Comput. System Science, </journal> <volume> 7 </volume> <pages> 354-375, </pages> <year> 1973. </year> <month> 20 </month>
Reference-contexts: Machines. We shall consider parallel machines built as interconnections of (processing-element, memory-module) pairs. Such a pair is modeled as a Hierarchical Random Access Machine, or H-RAM, a generalization of the RAM <ref> [CR73] </ref> introduced by [AACS87] (under the name of Hierarchical Memory Model) to capture the higher cost of remote memory access. (See also [S95] and its bibliography.) Definition 1 An f (x)-H-RAM is a random access machine where an access to address x takes time f (x).
Reference: [J92] <author> J. JaJa. </author> <title> An Introduction to Parallel Algorithms Addison-Wesley Reading Mass., </title> <year> 1992. </year>
Reference-contexts: Moreover, the processor-time tradeoff in the limiting technology can be different from the classical tradeoff embodied by Brent's Principle <ref> [B74, J92] </ref>, whereby a computation running for T steps on n processors can be emulated in at most dn=peT steps on p &lt; n processors of the same type.
Reference: [S95] <author> J.E. Savage. </author> <title> Space-Time Tradeoffs in Memory Hierarchies. </title> <type> TR, </type> <institution> Dept. of Comp.Sci., Brown University, </institution> <year> 1995. </year>
Reference-contexts: Such a pair is modeled as a Hierarchical Random Access Machine, or H-RAM, a generalization of the RAM [CR73] introduced by [AACS87] (under the name of Hierarchical Memory Model) to capture the higher cost of remote memory access. (See also <ref> [S95] </ref> and its bibliography.) Definition 1 An f (x)-H-RAM is a random access machine where an access to address x takes time f (x). According to this definition, a (processing-element, memory-module) pair is an f (x)- H-RAM.
Reference: [S86] <author> L. Snyder. </author> <title> Type Architectures, Shared Memory, and the Corollary of Modest Potential. </title> <booktitle> Annual Review of Computer Science, </booktitle> <volume> 1 </volume> <pages> 289-317, </pages> <year> 1986. </year> <month> 21 </month>
Reference-contexts: A corollary of Brent's Principle is that the best parallel algorithm on p processors cannot be more than p time faster than the best sequential algorithm (the Fundamental Principle of Parallel Computation <ref> [S86] </ref>). Informally, when communication delays are proportional to physical distances, the deployment of p processors can lead to speed-ups in two ways. A p-fold parallelism in the computation translates into a an O (p) speed-up due to simultaneous execution of operations (and the corresponding data access), as in Brent's Principle.
References-found: 8

