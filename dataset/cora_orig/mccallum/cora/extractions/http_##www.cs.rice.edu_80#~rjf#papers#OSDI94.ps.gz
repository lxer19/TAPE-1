URL: http://www.cs.rice.edu:80/~rjf/papers/OSDI94.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~rjf/pubs.html
Root-URL: 
Email: fkoch,fowler,ericg@diku.dk  
Title: Message-Driven Relaxed Consistency in a Software Distributed Shared Memory  
Author: Povl T. Koch Robert J. Fowler Eric Jul 
Address: Universitetsparken 1, 2100 Copenhagen, Denmark  
Affiliation: Department of Computer Science, University of Copenhagen (DIKU)  
Date: November 14-17, 1994  
Note: Appeared in Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI) pp. 75-85, Monterey, California,  
Abstract: Message-passing and distributed shared memory have their respective advantages and disadvantages in distributed parallel programming. We approach the problem of integrating both mechanisms into a single system by proposing a new message-driven coherency mechanism. Messages carrying explicit causality annotations are exchanged to trigger memory coherency actions. By adding annotations to standard message-based protocols, it is easy to construct efficient implementations of common synchronization and communication mechanisms. Because these are user-level messages, the set of available primitives is extended easily with language- or application-specific mechanisms. CarlOS, an experimental prototype for evaluating this approach, is derived from the lazy release consistent memory of TreadMarks. We describe the message-driven coherency memory model used in CarlOS, and we examine the performance of several applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(6) </volume> <pages> 613-624, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Interprocess coordination operations (synchronization and scheduling) are especially important, so many systems, whether hardware- or software-based, supplement their shared-memory implementations with additional mechanisms for synchronization. Additional incentives to give synchronization special treatment are provided by models of memory consistency <ref> [1, 6, 8, 12] </ref> that require that synchronization operations be identified to the memory coherence mechanism. On message-based hardware, this often means that the implementation of shared memory relies on a set of message-based synchronization mechanisms. In our message-driven consistency strategy, we make those mechanisms visible at the user-level.
Reference: [2] <author> John B. Carter. </author> <title> Efficient distributed shared memory based on multi-protocol release consistency. </title> <type> Ph.D. thesis, </type> <institution> Rice University, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: To avoid the cost of locking and migration, Carter experimented with an RPC-based queue implementation using a fixed manager <ref> [2] </ref>. Because the items in the queue could contain references to shared memory, each node explicitly forced memory into a consistent state by performing a flush operation before accessing the queue.
Reference: [3] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Techniques for reducing consistency-related communication in distributed shared memory systems. </title> <journal> ACM Transactions on Computer Systems. </journal> <note> To appear. </note>
Reference-contexts: Mechanisms to reduce or hide memory latencies [9], including update-based coherence protocols [20], are intended specifically to allow memory operations to be overlapped with computation. Related techniques can be used in software implementations to control the overhead of using shared memory by delaying and coalescing communication overhead <ref> [3, 12] </ref>. 1.1 Shared Memory: Problems and Fixes There are many common operations whose shared-memory implementations perform worse than do their message-based equivalents. Mismatches in alignment and size between the system's memory blocks and user data structures 1 reduce the control that memory-based protocols have over which data are communicated. <p> See [9] for a comparative evaluation of these alternatives. Software DSMs have fewer opportunities to use buffering and pipelining to overlap computation with communication. Even so, the LRC protocol delays or eliminates many memory-related communication events, update-based protocols can decrease read latencies, and multiple-writer protocols <ref> [3] </ref> greatly reduce the impact of false sharing. Although sophisticated implementation techniques im prove some aspects of the performance of shared memory, there still are many cases in which message-passing retains its advantages.
Reference: [4] <author> Jeffrey S. Chase, Franz G. Amador, Edward D. La-zowska, Henry M. Levy, and Richard J. Littlefield. </author> <title> The Amber system: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 147-158, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Although CarlOS does not maintain the consistency of this region, the single address map provides a consistent interpretation for pointers to objects in this region. The intention is that memory consistency for this region will be provided by run-time libraries using message-passing protocols such as those used in Amber <ref> [4] </ref>.
Reference: [5] <author> Digital. </author> <title> Alpha architecture handbook. </title> <publisher> Digital Press, </publisher> <year> 1992. </year>
Reference-contexts: The performance measurements were made on a cluster of DEC 3000/300 workstations with 150 MHz Alpha AXP processors running DEC OSF/1 v1.3 on an isolated Ethernet segment. Timings were obtained using a fine-grain low-overhead timing facility based on AXP architecture's processor cycle counter <ref> [5] </ref>. We examine at least two versions of each of three applications running on CarlOS: TSP, Quicksort, and Water. One version of each application is a strictly shared memory program that runs on TreadMarks and is synchronized using locks and barriers.
Reference: [6] <author> M. Dubois, C. Scheurich, and F.A. Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Interprocess coordination operations (synchronization and scheduling) are especially important, so many systems, whether hardware- or software-based, supplement their shared-memory implementations with additional mechanisms for synchronization. Additional incentives to give synchronization special treatment are provided by models of memory consistency <ref> [1, 6, 8, 12] </ref> that require that synchronization operations be identified to the memory coherence mechanism. On message-based hardware, this often means that the implementation of shared memory relies on a set of message-based synchronization mechanisms. In our message-driven consistency strategy, we make those mechanisms visible at the user-level.
Reference: [7] <author> Sandhay Dwarkadas, Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 244-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The consistency information appended to the message comprises interval descriptions that the sender has determined are needed to make the immediate receiver consistent. If an invalidation-based consistency strategy is used, the interval descriptions contain only write notices. If an update or hybrid strategy <ref> [7] </ref> is used, the message also will contain a set of diffs. Thus far, we have used only the invalidation strategy in CarlOS. When a node accepts a RELEASE message, it performs the actions required by an acquire event in LRC.
Reference: [8] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Interprocess coordination operations (synchronization and scheduling) are especially important, so many systems, whether hardware- or software-based, supplement their shared-memory implementations with additional mechanisms for synchronization. Additional incentives to give synchronization special treatment are provided by models of memory consistency <ref> [1, 6, 8, 12] </ref> that require that synchronization operations be identified to the memory coherence mechanism. On message-based hardware, this often means that the implementation of shared memory relies on a set of message-based synchronization mechanisms. In our message-driven consistency strategy, we make those mechanisms visible at the user-level.
Reference: [9] <author> A. Gupta, J. Hennessy, K. Gharachorloo, T. Mowry, and W.-D. Weber. </author> <title> Comparative evaluation of latency reducing and tolerating techniques. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In hardware implementations of distributed shared memory, communication occurs without the overhead of processor interrupts, context switching, and software network protocols, even when the system is based on a low-level message-passing network. Mechanisms to reduce or hide memory latencies <ref> [9] </ref>, including update-based coherence protocols [20], are intended specifically to allow memory operations to be overlapped with computation. <p> Relaxed models of memory consistency have been used effectively to overlap communication and computation, thus reducing and hiding the latencies of writes and of coherence operations. The read-latency problem has been addressed by using update-based protocols [20], by augmenting invalidation-based protocols with prefetching [17], and by multithreading. See <ref> [9] </ref> for a comparative evaluation of these alternatives. Software DSMs have fewer opportunities to use buffering and pipelining to overlap computation with communication.
Reference: [10] <author> E. Jul, H. Levy, N. Hutchinson, and A. Black. </author> <title> Fine-grained mobility in the Emerald system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 109-133, </pages> <month> Febru-ary </month> <year> 1988. </year>
Reference-contexts: A major goal of the CarlOS project is to explore hybrid distributed computing environments that integrate aspects of distributed shared memory based on flat memory spaces with aspects of distributed object-based languages <ref> [10] </ref>. This objective affects the design in several ways. In particular, the development of our message-driven memory consistency model was motivated by the need to combine explicit user-controlled object migration, function shipping in the form of remote invocation, and a flat, coherent address space.
Reference: [11] <author> Peter Keleher, Alan L. Cox, Sandhya Dwarkadas, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In Proceedings of the 1994 Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: We present the message-driven memory model, the problems that it ad dresses, and examples of its use. The model is compatible with hardware- as well as software implementations. We describe a software prototype called CarlOS that was built by applying extensive modifications to the Tread-Marks DSM system <ref> [11] </ref>. There are advantages and disadvantages to message-passing as well as to shared memory. Message-passing gives programmers and compilers explicit control over the choice of data communicated and over the time of transmission. With appropriate interfaces and protocols, it is relatively easy to overlap computation with communication. <p> Currently, CarlOS runs entirely in Unix user mode and uses the standard Unix system interface. The implementation strategy is evolutionary and will entail some kernel-level support, particularly in the network drivers. We began with the TreadMarks <ref> [11] </ref> code. While the basic mechanisms of lazy release consistency are intact, data structures and internal protocols have been restructured extensively to support the internal concurrency and asynchrony implicit in the message-driven consistency model. <p> Our presentation focuses on the differences between the systems, and space limitations prevent us from including too much detail that has appeared elsewhere. A detailed description of the TreadMarks implementation can be found in <ref> [11] </ref>. tervals whose endpoints occur at the acquire and release events executed on that node. In TreadMarks, these occur within the execution of built-in synchronization primitives. In CarlOS, they occur when RELEASE messages are sent and accepted, respectively (see Section 4.3).
Reference: [12] <author> Peter Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Specifically, messages carrying causality annotations are used explicitly to drive and control a set of memory-consistency mechanisms derived from the lazy release consistent (LRC) protocol <ref> [12] </ref>. Some annotations allow messages to be transmitted without incurring memory-consistency overhead, while others force the receiver to a state consistent with the sender. This approach is a simple and sound architectural basis for the construction of hybrid systems that provide both shared memory and message-passing. <p> Mechanisms to reduce or hide memory latencies [9], including update-based coherence protocols [20], are intended specifically to allow memory operations to be overlapped with computation. Related techniques can be used in software implementations to control the overhead of using shared memory by delaying and coalescing communication overhead <ref> [3, 12] </ref>. 1.1 Shared Memory: Problems and Fixes There are many common operations whose shared-memory implementations perform worse than do their message-based equivalents. Mismatches in alignment and size between the system's memory blocks and user data structures 1 reduce the control that memory-based protocols have over which data are communicated. <p> Interprocess coordination operations (synchronization and scheduling) are especially important, so many systems, whether hardware- or software-based, supplement their shared-memory implementations with additional mechanisms for synchronization. Additional incentives to give synchronization special treatment are provided by models of memory consistency <ref> [1, 6, 8, 12] </ref> that require that synchronization operations be identified to the memory coherence mechanism. On message-based hardware, this often means that the implementation of shared memory relies on a set of message-based synchronization mechanisms. In our message-driven consistency strategy, we make those mechanisms visible at the user-level.
Reference: [13] <author> D. Kranz, K. Johnson, A. Agarwal, J. Kubiatowicz, and B-H. Lim. </author> <title> Integrating message-passing and shared-memory: Early experience. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <address> San Diego, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: A conceptually simple approach is to deliver user messages only after all preceding memory operations have been performed. 1 Several groups <ref> [13, 14, 15] </ref> are working on systems that combine some form of message-passing with shared memory at the hardware level. By using the same pipelines to carry user messages and memory-system messages in these designs, in-order delivery can be ensured without adversely affecting the performance of the memory subsystem.
Reference: [14] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a message in the Alewife multiprocessor. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <pages> pages 195-206, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: A conceptually simple approach is to deliver user messages only after all preceding memory operations have been performed. 1 Several groups <ref> [13, 14, 15] </ref> are working on systems that combine some form of message-passing with shared memory at the hardware level. By using the same pipelines to carry user messages and memory-system messages in these designs, in-order delivery can be ensured without adversely affecting the performance of the memory subsystem.
Reference: [15] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenbaum, and John Hennessy. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Conference on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: A conceptually simple approach is to deliver user messages only after all preceding memory operations have been performed. 1 Several groups <ref> [13, 14, 15] </ref> are working on systems that combine some form of message-passing with shared memory at the hardware level. By using the same pipelines to carry user messages and memory-system messages in these designs, in-order delivery can be ensured without adversely affecting the performance of the memory subsystem.
Reference: [16] <author> Leslie Lamport. </author> <title> Time, clocks, and the ordering of events in a distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: If all messages are synchronizing messages, the ordering of memory events is consistent with happened before as defined by Lamport <ref> [16] </ref> for message-passing systems. Since all reasoning is in terms of synchronizing messages, they are the only mechanism needed to drive the memory system into consistent states. A consequence of 1 In-order point-to-point delivery of messages is not a sufficient condition for correctness.
Reference: [17] <author> T. Mowry and A. Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12 </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Relaxed models of memory consistency have been used effectively to overlap communication and computation, thus reducing and hiding the latencies of writes and of coherence operations. The read-latency problem has been addressed by using update-based protocols [20], by augmenting invalidation-based protocols with prefetching <ref> [17] </ref>, and by multithreading. See [9] for a comparative evaluation of these alternatives. Software DSMs have fewer opportunities to use buffering and pipelining to overlap computation with communication.
Reference: [18] <author> J. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stan-ford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The relatively small contribution of consistency overhead to total execution time, however, limits the magnitude of the effect. 5.3 The Water Application Water is a molecular dynamics simulation application from the SPLASH suite <ref> [18] </ref>. Each iteration of the program consists of several phases separated by barriers. In a problem instance with P processors and N molecules, each processor is assigned N=P molecules. In the most 5 CarlOS uses TreadMarks' memory management mechanisms for system data structures (intervals, diffs, etc.).
Reference: [19] <author> Thomas von Eicken, David E. Culler, Seth Copen Goldstein, and Karl Erik Schauser. </author> <title> Active messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: CarlOS messages currently are implemented using UDP/IP datagrams supplemented with a sliding window protocol to assure reliable, in-order delivery. The CarlOS message interface defines a form of active messages <ref> [19] </ref>. Each message contains a pointer to a handler function that is invoked when the message is received and to which the body of the message is passed as an argument.
Reference: [20] <author> Larry D. Wittie, Gudjun Hermannsson, and Ai Li. </author> <title> Eager sharing for efficient massive parallelism. </title> <booktitle> In 1992 International Conference on Parallel Processing, </booktitle> <pages> pages 251-255, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1992. </year> <month> 11 </month>
Reference-contexts: In hardware implementations of distributed shared memory, communication occurs without the overhead of processor interrupts, context switching, and software network protocols, even when the system is based on a low-level message-passing network. Mechanisms to reduce or hide memory latencies [9], including update-based coherence protocols <ref> [20] </ref>, are intended specifically to allow memory operations to be overlapped with computation. <p> Relaxed models of memory consistency have been used effectively to overlap communication and computation, thus reducing and hiding the latencies of writes and of coherence operations. The read-latency problem has been addressed by using update-based protocols <ref> [20] </ref>, by augmenting invalidation-based protocols with prefetching [17], and by multithreading. See [9] for a comparative evaluation of these alternatives. Software DSMs have fewer opportunities to use buffering and pipelining to overlap computation with communication.
References-found: 20

