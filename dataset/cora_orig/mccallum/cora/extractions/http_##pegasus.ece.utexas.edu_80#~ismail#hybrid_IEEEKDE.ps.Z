URL: http://pegasus.ece.utexas.edu:80/~ismail/hybrid_IEEEKDE.ps.Z
Refering-URL: 
Root-URL: 
Email: E-mail: fismail,ghoshg@pine.ece.utexas.edu  
Title: Symbolic Interpretation of Artificial Neural Networks  
Author: Ismail Taha and Joydeep Ghosh 
Date: September 20, 1996  
Address: Austin, TX 78712-1084  
Affiliation: University of Texas,  
Abstract: H ybrid Intelligent Systems that combine knowledge based and artificial neural network systems typically have four phases involving domain knowledge representation, mapping of this knowledge into an initial connectionist architecture, network training, and rule extraction respectively. The final phase is important because it can provide a trained connectionist architecture with explanation power and validate its output decisions. Moreover, it can be used to refine and maintain the initial knowledge acquired from domain experts. In this paper, we introduce three new rule extraction techniques. The first technique extracts a set of binary rules from any neural network regardless of its kind (MLP, RBF etc.,). The second technique extracts partial rules that represent the most important embedded knowledge in a trained network. The fidelity of the second technique is adjustable to the desired level of knowledge extraction. The third technique is a comprehensive and universal approach. A rule evaluation technique that orders extracted rules based on three performance measures is then proposed. The three techniques are applied to the iris and breast cancer data sets. The extracted rules are evaluated qualitatively and quantitatively, and compared with those obtained by other approaches. fl This research was supported in part by ARO contract DAAH04-94-G-0417, DAAH04-95-10494, and ATP grant #442. Ismail Taha was also supported by the Egyptian Government Ph.D. Fellowship in Electrical and Computer Engineering. A preliminary version of this paper appears in Proceedings of ANNIE'96, St. Louis, November 1996. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Andrews, J. Diederich, and A. Tickle. </author> <title> A survey and critique of techniques for extracting rules from trained artificial neural networks. </title> <type> Technical report, </type> <institution> Neurocomputing Research Center, Queensland University of Technology, Queensland, Australia, </institution> <year> 1995. </year>
Reference-contexts: This subsection summarizes some of the existing approaches with emphasis on extracting rules from feedforward (specifically, MLP) ANN architectures. A very rich source of 2 RULE EXTRACTION 6 literature review of different rule extraction approaches is a technical report written by Andrews et al. <ref> [1] </ref>. 2.2.1 Link Rule Extraction Techniques The methodology behind most of the techniques for rule extraction from MLPs can be summarized in two main steps: (i) For each hidden or output node in the network, search for different combinations of input links whose weighted sum exceeds the bias of the current <p> Heuristic methods are commonly used in the LRE category to bound the search space for rules and to increase the comprehensibility of the extracted rules. Some researchers use the term "decompositional methods" to refer to the LRE type techniques <ref> [1, 11] </ref>. Several other rule extraction approaches that extract rules from feedforward ANNs have been reported. The main difference between them and the approaches mentioned above is that they extract rules from specialized ANNs. RuleNet [30] and RULEX [3, 2] are two examples of this class of approaches.
Reference: [2] <author> R. Andrews and S. Geva. </author> <title> Rule extraction from a constrained error backpropagation MLP. </title> <booktitle> In Proceedings of Fifth Australian Conference on Neural Networks, </booktitle> <pages> pages 9-12, </pages> <address> Brisbane, Queensland, </address> <year> 1994. </year>
Reference-contexts: Several other rule extraction approaches that extract rules from feedforward ANNs have been reported. The main difference between them and the approaches mentioned above is that they extract rules from specialized ANNs. RuleNet [30] and RULEX <ref> [3, 2] </ref> are two examples of this class of approaches. RULEX extracts rules from a Constrained Error Back-Propagation (CEBP) MLP network, similar to Radial Basis Function (RBF) networks. Each hidden node in this CEBP network is localized in a disjoint region of the training examples.
Reference: [3] <author> R. Andrews and S. Geva. </author> <title> Inserting and extracting knowledge from constrained error backpropagation networks. </title> <booktitle> In Proceedings of Sixth Australian Conference on Neural Networks, </booktitle> <address> Sydney NSW, </address> <year> 1995. </year>
Reference-contexts: Several other rule extraction approaches that extract rules from feedforward ANNs have been reported. The main difference between them and the approaches mentioned above is that they extract rules from specialized ANNs. RuleNet [30] and RULEX <ref> [3, 2] </ref> are two examples of this class of approaches. RULEX extracts rules from a Constrained Error Back-Propagation (CEBP) MLP network, similar to Radial Basis Function (RBF) networks. Each hidden node in this CEBP network is localized in a disjoint region of the training examples. <p> In this paper we refer to this class as the Black-box Rule Extraction (BRE) category because rules are extracted regardless the type or the structure of the neural network. Another given name to this class of rule extraction techniques is 00 pedagogical 00 approaches <ref> [3] </ref>. For example, DEDEC extracts rules by ranking the inputs of an ANN according to their importance (contribution) to the ANN outputs [51]. This ranking process is done by examining the weight vectors of the ANN, which puts DEDEC on the border between LRE and BRE techniques.
Reference: [4] <author> R. Brayton, G. Hachtel, C. McMullen, and A. Sangiovanni-Vincentelli. </author> <title> Logic Minimization Algorithm for VLSI Synthesis. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1984. </year>
Reference-contexts: Any available boolean simplification method can be used to perform step 3 of the BIO-RE algorithm (e.g. Karnough map [22], algebraic manipulation, or a tabulation method [29]). We used Espresso 1 to generate the extracted rules <ref> [4] </ref>. Rules extracted by BIO-RE are represented in the format: If [N ot] Input-Variable [And [N ot] Input-Variable] fl !Consequent j where: [] is an optional term and [] fl means that the term [] can be repeated 0 or n times.
Reference: [5] <author> J. Catlett. </author> <title> On changing continuous attributes into ordered discrete attributes. </title> <booktitle> In European Working Session on Learning, </booktitle> <year> 1991. </year>
Reference-contexts: Different discretization approaches can be exploited to compute discretization boundaries of input features X i s <ref> [46, 5, 23, 26, 56] </ref>. Full-RE uses the Chi2 [25] algorithm 2 , a powerful dis cretization tool, to compute discretization boundaries of input features.
Reference: [6] <author> R. Challo, R.A. McLauchlan, D.A. Clark, and S.I. Omar. </author> <title> A fuzzy neural hybrid system. </title> <booktitle> In IEEE International Conference on Neural Networks, volume III, </booktitle> <pages> pages 1654-1657, </pages> <address> Orlando, FL., </address> <year> 1994. </year>
Reference-contexts: Researchers have also combined connectionist systems with fuzzy logic systems to obtain Fuzzy Logic Neural Networks (FLNN or NeuroFuzzy) hybrid systems. In FLNNs, the neural network subsystem is typically used to adapt membership functions of fuzzy variables <ref> [6] </ref>, or to refine and extract fuzzy rules [48, 47, 24]. Extracting symbolic rules from trained ANNs is an important feature of comprehensive hybrid 2 RULE EXTRACTION 4 systems, as it helps in: 1. Alleviating the knowledge acquisition problem and refining initial domain knowledge. 2.
Reference: [7] <author> M.W. Craven and J.W. Shavlik. </author> <title> Using sampling and queries to extract rules from trained neural networks. </title> <booktitle> In Machine Learning: Proceedings of the Eleventh international Conference, </booktitle> <year> 1994. </year>
Reference-contexts: An example of such a rule extraction approach 2 RULE EXTRACTION 8 is the algorithm developed by Saito and Nakano to extract medical diagnostic rules from a trained network [39]. BRAINNE [40], Rule-extraction-as-learning <ref> [7] </ref>, and DEDEC [50] are other examples of extracting rules by investigating the input-output mapping of a trained network. In this paper we refer to this class as the Black-box Rule Extraction (BRE) category because rules are extracted regardless the type or the structure of the neural network.
Reference: [8] <author> S. Dzeroski and N. Lavrac. </author> <title> Learning relations from noisy examples: An empirical comparison of linus and foil. </title> <booktitle> In Proceedings of the Eighth International Machine Learning Workshop, </booktitle> <pages> pages 399-402. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: MofN extracts rules from the KBANN trained network through six main procedures. Rules extracted by MofN are significantly superior than rules extracted by other symbolic approaches such as C4.5 [37], Either [35] and LINUS <ref> [8] </ref> at least for problems like "promoter recognition in DNA nucleotides" for which it is a natural fit [52]. NeuroRule is another rule extraction approach that uses different combinations of weighted links to extract rules [43].
Reference: [9] <author> L.M. Fu. </author> <title> Rule learning by searching on adapted nets. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence (Anaheim CA), </booktitle> <pages> pages 590-595, </pages> <year> 1991. </year>
Reference-contexts: All premises of a rule are conjuncted. Either [35], KT <ref> [9] </ref> and Subset [52] are three notable rule extraction algorithms in this category.
Reference: [10] <author> L.M. Fu. </author> <title> Knowledge-based connectionism for revising domain theories. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 23(1) </volume> <pages> 173-182, </pages> <year> 1993. </year> <note> REFERENCES 36 </note>
Reference-contexts: 1 Introduction Several researchers have investigated the design of hybrid systems that combine expert and connectionist subsystems <ref> [44, 45, 54, 10, 16, 15, 27] </ref>. <p> KBNNs attempt to exploit the complementary properties of knowledge based and neural network paradigms to obtain more powerful and robust systems. HIA [44], KBANN [53, 34], RAPTURE [27] and KBCNN <ref> [10, 11] </ref> are examples of KBNN hybrid systems. Figure 1 sketches typical components of a KBNN system that combines rule-based and connectionist paradigms. Researchers have also combined connectionist systems with fuzzy logic systems to obtain Fuzzy Logic Neural Networks (FLNN or NeuroFuzzy) hybrid systems.
Reference: [11] <author> L.M. Fu. </author> <booktitle> Neural Networks in Computer Intelligence. </booktitle> <publisher> McGraw-Hill, Inc., </publisher> <year> 1994. </year>
Reference-contexts: KBNNs attempt to exploit the complementary properties of knowledge based and neural network paradigms to obtain more powerful and robust systems. HIA [44], KBANN [53, 34], RAPTURE [27] and KBCNN <ref> [10, 11] </ref> are examples of KBNN hybrid systems. Figure 1 sketches typical components of a KBNN system that combines rule-based and connectionist paradigms. Researchers have also combined connectionist systems with fuzzy logic systems to obtain Fuzzy Logic Neural Networks (FLNN or NeuroFuzzy) hybrid systems. <p> Heuristic methods are commonly used in the LRE category to bound the search space for rules and to increase the comprehensibility of the extracted rules. Some researchers use the term "decompositional methods" to refer to the LRE type techniques <ref> [1, 11] </ref>. Several other rule extraction approaches that extract rules from feedforward ANNs have been reported. The main difference between them and the approaches mentioned above is that they extract rules from specialized ANNs. RuleNet [30] and RULEX [3, 2] are two examples of this class of approaches. <p> Moreover, they both extract comprehensive rules with relatively high correct classification rate as reported by the authors of NeuroRule [42]. For iris problem, we also compare the set of rules extracted by Full-RE with the corresponding set of rules extracted by KT algorithm <ref> [11] </ref>. Before analyzing the extracted rules, we summarize the computational complexity of Neu-roRule and C4.5rules: * NeuroRule: as a starting point of the NeuroRule algorithm, 100 fully connected MLPs are generated. <p> Like ID3 [36], C4.5rules [20] generates decision tree rules based on the available input samples. Therefore, the complexity is moderate, but the performance of the rules generated by C4.5rules is highly affected by the noise level in the available data samples <ref> [11] </ref>. 5 PERFORMANCE EVALUATION 30 5.1 Comparison using iris data set The rules extracted by the Full-RE techniques for the iris problem were given in Table 3.
Reference: [12] <author> J. Ghosh and K. Tumer. </author> <title> Structural adaptation and generalization in supervised feed-forward networks. </title> <journal> Journal of Artificial Neural Networks, </journal> <volume> 1(4) </volume> <pages> 431-458, </pages> <year> 1994. </year>
Reference-contexts: Training procedure: in all experiments, an MLP network is trained using the backpropa gation algorithm with momentum as well as a regularization term P which adds 2w jk w 0 2 jk to the weight update term in the backpropagation equation <ref> [12] </ref>. Cross validation is used for the stopping criteria. 2. Network architectures and data reduction: for the iris problem, an MLP with 4 input, 6 hidden, and 3 output nodes is used for the three experiments but trained with different data sets each time, as described later.
Reference: [13] <author> C.L. Giles, B.G. Horne, and T. Lin. </author> <title> Learning a class of large finite state machines with a recurrent neural network. </title> <type> Technical Report UMIACS-TR-94-94, </type> <institution> Institute of Advanced Computer Studies, University of Maryland, College Park, MD 20742, </institution> <year> 1994. </year>
Reference-contexts: Some FLNN systems include a fuzzy rule extraction module for refining fuzzy sets membership functions and explaining the trained neural network [17, 48, 47, 24] 2.2.4 Extracting rules from recurrent networks Recurrent networks have shown great success in representing finite state languages [14, 55] and deterministic finite state automata <ref> [13] </ref>. Omlin and Giles, [33] have developed a heuristic algorithm to extract grammar rules in the form of Deterministic Finite-state Automata (DFA) from discrete-time neural networks and specifically from second-order networks.
Reference: [14] <author> C.L. Giles, C.B. Miller, D. Chen, H.H. Chen, G.Z. Sun, and Y.C. Lee. </author> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 394-405, </pages> <year> 1992. </year>
Reference-contexts: Some FLNN systems include a fuzzy rule extraction module for refining fuzzy sets membership functions and explaining the trained neural network [17, 48, 47, 24] 2.2.4 Extracting rules from recurrent networks Recurrent networks have shown great success in representing finite state languages <ref> [14, 55] </ref> and deterministic finite state automata [13]. Omlin and Giles, [33] have developed a heuristic algorithm to extract grammar rules in the form of Deterministic Finite-state Automata (DFA) from discrete-time neural networks and specifically from second-order networks.
Reference: [15] <author> C.W. Glover, M. Silliman, M. Walker, and P. Spelt. </author> <title> Hybrid neural network and rule-based pattern recognition system capable of self-modification. </title> <booktitle> In Proceedings of SPIE, Application of Artificial Intelligence VIII, </booktitle> <pages> pages 290-300, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Several researchers have investigated the design of hybrid systems that combine expert and connectionist subsystems <ref> [44, 45, 54, 10, 16, 15, 27] </ref>.
Reference: [16] <author> J.A. Hendler. </author> <title> Marker-passing over microfeatures: Towards a hybrid symbolic/connectionist model. </title> <journal> Cognitive Science, </journal> <volume> 13 </volume> <pages> 79-106, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Several researchers have investigated the design of hybrid systems that combine expert and connectionist subsystems <ref> [44, 45, 54, 10, 16, 15, 27] </ref>.
Reference: [17] <author> S. Horikawa, T. Furuhashi, and Y. Uchikawa. </author> <title> On fuzzy modeling using fuzzy neural networks with back-propagation algorithm. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 3(5) </volume> <pages> 801-806, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Some FLNN systems include a fuzzy rule extraction module for refining fuzzy sets membership functions and explaining the trained neural network <ref> [17, 48, 47, 24] </ref> 2.2.4 Extracting rules from recurrent networks Recurrent networks have shown great success in representing finite state languages [14, 55] and deterministic finite state automata [13].
Reference: [18] <author> P. Howes and N. Crook. </author> <title> Rule extraction from neural networks. </title> <editor> In R. Andrews and J. Diederich, editors, </editor> <title> Rules and Networks: </title> <booktitle> Proceedings of the Rule Extraction From Trained Artificial Neural Networks Workshop, </booktitle> <pages> pages 60-67. </pages> <institution> Queensland University of Technology, Neurocomputing Research Center QUT NRC, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: The main difference between NeuroRule and MofN is that the former extracts rules from networks after pruning their architectures and then discretizing their hidden 2 RULE EXTRACTION 7 units activation values. Recently, Howes and Crook introduced another algorithm that extracts rules from feedfor-ward neural networks <ref> [18] </ref>. The network architecture used by this algorithm is restricted to one hidden layer network trained with a binary sigmoid activation function.
Reference: [19] <author> R. Jacobs, M. Jordan, and S. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87, </pages> <year> 1991. </year>
Reference-contexts: A distinctive feature of RULEX is that it controls the search space through its network while other approaches use heuristic measures to do the same. RuleNet, on the other hand, uses the idea of adaptive mixture of local expert <ref> [19] </ref> to train a localized ANN then extracts binary rules in a LRE approach. Both RULEX and RuleNet can be classified as "localized LRE" techniques. 2.2.2 Black-box Rule Extraction Techniques Another class of rule extraction approaches extracts rules from feedforward networks only by examining their input-output mapping behavior.
Reference: [20] <author> Quinlan J.R. </author> <title> C4.5 Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1992. </year>
Reference-contexts: However, the pruning and clustering processes lead to substantial overheads. * C4.5rules: C4.5rules was used by the authors of NeuroRule to extract rules from the iris and breast-cancer databases for comparison reasons. Like ID3 [36], C4.5rules <ref> [20] </ref> generates decision tree rules based on the available input samples.
Reference: [21] <author> Schlimmer J.S. </author> <title> Concept acquisition through representational adjustment. </title> <type> PhD thesis, </type> <institution> University of California, Irvine Department of Information and Science, </institution> <month> May </month> <year> 1996. </year> <note> REFERENCES 37 </note>
Reference-contexts: These instances are divided into a training set of size 341 and a test set of size 342. Other popular data sets that have been used as benchmarks for rule extraction approaches are the Monk [49], Mushroom <ref> [21] </ref> and the DNA promoter [54] data sets. All three of these data sets inputs are symbolic/discrete by nature.
Reference: [22] <author> M. Karnough. </author> <title> A map method for synthesis of combinational logic circuits. </title> <journal> Trans. AIEE, Comm. and Electronics, </journal> <volume> 72(1) </volume> <pages> 593-599, </pages> <month> November </month> <year> 1953. </year>
Reference-contexts: Generate the corresponding boolean function (represented in the previously described binary rule format) from the truth table of step 2. Any available boolean simplification method can be used to perform step 3 of the BIO-RE algorithm (e.g. Karnough map <ref> [22] </ref>, algebraic manipulation, or a tabulation method [29]). We used Espresso 1 to generate the extracted rules [4].
Reference: [23] <author> R. Kerber. Chimerge: </author> <title> Discretization of numeric attributes. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence AAAI, </booktitle> <pages> pages 123-128, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Different discretization approaches can be exploited to compute discretization boundaries of input features X i s <ref> [46, 5, 23, 26, 56] </ref>. Full-RE uses the Chi2 [25] algorithm 2 , a powerful dis cretization tool, to compute discretization boundaries of input features.
Reference: [24] <author> C.T. Lin and C.S.G. Lee. </author> <title> Neural-network-based fuzzy logic control and decision system. </title> <journal> IEEE Transaction on Computers, </journal> <volume> 40(12) </volume> <pages> 1320-1326, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: Researchers have also combined connectionist systems with fuzzy logic systems to obtain Fuzzy Logic Neural Networks (FLNN or NeuroFuzzy) hybrid systems. In FLNNs, the neural network subsystem is typically used to adapt membership functions of fuzzy variables [6], or to refine and extract fuzzy rules <ref> [48, 47, 24] </ref>. Extracting symbolic rules from trained ANNs is an important feature of comprehensive hybrid 2 RULE EXTRACTION 4 systems, as it helps in: 1. Alleviating the knowledge acquisition problem and refining initial domain knowledge. 2. Providing reasoning and explanation capabilities. 3. Supporting cross-referencing and verification capabilities. 4. <p> Some FLNN systems include a fuzzy rule extraction module for refining fuzzy sets membership functions and explaining the trained neural network <ref> [17, 48, 47, 24] </ref> 2.2.4 Extracting rules from recurrent networks Recurrent networks have shown great success in representing finite state languages [14, 55] and deterministic finite state automata [13].
Reference: [25] <author> H. Liu and R. Setiono. Chi2: </author> <title> Feature selection and discretization of numeric attributes. </title> <booktitle> In Proceedings of the Seventh International Conference on Tools with Artificial Intelligence, </booktitle> <pages> pages 388-391, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: Different discretization approaches can be exploited to compute discretization boundaries of input features X i s [46, 5, 23, 26, 56]. Full-RE uses the Chi2 <ref> [25] </ref> algorithm 2 , a powerful dis cretization tool, to compute discretization boundaries of input features.
Reference: [26] <author> H. Liu and R. Setiono. </author> <title> Discretization of ordinal attributes and feature selection, </title> <type> Technical Report TRB4/95. Technical report, </type> <institution> National University of Singapore, Department of Information Systems and Computer Science, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: Different discretization approaches can be exploited to compute discretization boundaries of input features X i s <ref> [46, 5, 23, 26, 56] </ref>. Full-RE uses the Chi2 [25] algorithm 2 , a powerful dis cretization tool, to compute discretization boundaries of input features.
Reference: [27] <author> J.J. Mahoney and R.J. Mooney. </author> <title> Combining connectionist and symbolic learning to refine certainty factor rule bases. </title> <booktitle> Connection Science, </booktitle> <address> 5(3-4):339-364, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Several researchers have investigated the design of hybrid systems that combine expert and connectionist subsystems <ref> [44, 45, 54, 10, 16, 15, 27] </ref>. <p> KBNNs attempt to exploit the complementary properties of knowledge based and neural network paradigms to obtain more powerful and robust systems. HIA [44], KBANN [53, 34], RAPTURE <ref> [27] </ref> and KBCNN [10, 11] are examples of KBNN hybrid systems. Figure 1 sketches typical components of a KBNN system that combines rule-based and connectionist paradigms. Researchers have also combined connectionist systems with fuzzy logic systems to obtain Fuzzy Logic Neural Networks (FLNN or NeuroFuzzy) hybrid systems.
Reference: [28] <author> O.L. Mangasarian and H.W. Wolberg. </author> <title> Cancer diagnosis via linear programming. </title> <journal> SIAM News, </journal> <volume> 23(5) </volume> <pages> 1-18, </pages> <year> 1990. </year>
Reference-contexts: Each input pattern has four continuous input features: I 1 = Sepal-length , I 2 = Sepal-width, I 3 = Petal-length, and I 4 =Petal-width. 3. Breast-Cancer data set which has nine inputs and two output classes <ref> [28, 32] </ref>.
Reference: [29] <author> M. Morris Mano. </author> <title> Digital Logic and Computer Design. </title> <publisher> Prentice-Hall, </publisher> <year> 1990. </year>
Reference-contexts: Generate the corresponding boolean function (represented in the previously described binary rule format) from the truth table of step 2. Any available boolean simplification method can be used to perform step 3 of the BIO-RE algorithm (e.g. Karnough map [22], algebraic manipulation, or a tabulation method <ref> [29] </ref>). We used Espresso 1 to generate the extracted rules [4].
Reference: [30] <author> C. McMillan, M.C. Mozer, and P. Smolensky. </author> <title> The connectionist scientist game: Rule extraction and refinement in a neural network. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of The Cognitive Science Society, </booktitle> <year> 1991. </year>
Reference-contexts: Some researchers use the term "decompositional methods" to refer to the LRE type techniques [1, 11]. Several other rule extraction approaches that extract rules from feedforward ANNs have been reported. The main difference between them and the approaches mentioned above is that they extract rules from specialized ANNs. RuleNet <ref> [30] </ref> and RULEX [3, 2] are two examples of this class of approaches. RULEX extracts rules from a Constrained Error Back-Propagation (CEBP) MLP network, similar to Radial Basis Function (RBF) networks. Each hidden node in this CEBP network is localized in a disjoint region of the training examples.
Reference: [31] <author> W. Mendenhall. </author> <title> Introduction to Probability and Statistics, Fifth Edition. </title> <publisher> Wadsworth Publishing Company, Inc., </publisher> <year> 1979. </year>
Reference-contexts: In Equation 2, i is multiplied by "2" to provide a 3 PROPOSED RULE EXTRACTION APPROACHES 12 wider distribution of input X i ( a range of i 2 i will contain approximately 95 percent of the X i measurements <ref> [31] </ref> if X i is normally distributed). If more detailed rules are required (i.e. the comprehensibility measure p &gt; 1), then Partial-RE starts looking for combinations of two unmarked links starting from the first (maximum) element of the positive set.
Reference: [32] <author> P.M. Murphy and D.W. Aha. </author> <title> UCI repository of machine learning database. </title> <type> Technical report, </type> <institution> University of California, Department of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: Iris database, a simple classification problem which contains 50 examples each of classes Iris Setosa, Iris Versicolor, and Iris Virginica <ref> [32] </ref>. These 150 instances were divides into two subsets, the first subset, used for training, is of size 89 and the second is of size 61 and used for testing. <p> Each input pattern has four continuous input features: I 1 = Sepal-length , I 2 = Sepal-width, I 3 = Petal-length, and I 4 =Petal-width. 3. Breast-Cancer data set which has nine inputs and two output classes <ref> [28, 32] </ref>.
Reference: [33] <author> C.W. Omlin and C.L. Giles. </author> <title> Extraction of rules from discrete-time recurrent neural networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 9(1) </volume> <pages> 41-52, </pages> <year> 1996. </year> <note> REFERENCES 38 </note>
Reference-contexts: Omlin and Giles, <ref> [33] </ref> have developed a heuristic algorithm to extract grammar rules in the form of Deterministic Finite-state Automata (DFA) from discrete-time neural networks and specifically from second-order networks.
Reference: [34] <author> D.W. Optiz and J.W. Shavlik. </author> <title> Heuristically expanding knowledge-based neural network. </title> <booktitle> In Proceedings of Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 512-517, </pages> <year> 1993. </year>
Reference-contexts: KBNNs attempt to exploit the complementary properties of knowledge based and neural network paradigms to obtain more powerful and robust systems. HIA [44], KBANN <ref> [53, 34] </ref>, RAPTURE [27] and KBCNN [10, 11] are examples of KBNN hybrid systems. Figure 1 sketches typical components of a KBNN system that combines rule-based and connectionist paradigms. Researchers have also combined connectionist systems with fuzzy logic systems to obtain Fuzzy Logic Neural Networks (FLNN or NeuroFuzzy) hybrid systems.
Reference: [35] <author> D. Ourston and R.J. Mooney. </author> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 815-820. </pages> <publisher> AAAI Press, </publisher> <year> 1990. </year>
Reference-contexts: All premises of a rule are conjuncted. Either <ref> [35] </ref>, KT [9] and Subset [52] are three notable rule extraction algorithms in this category. <p> MofN extracts rules from the KBANN trained network through six main procedures. Rules extracted by MofN are significantly superior than rules extracted by other symbolic approaches such as C4.5 [37], Either <ref> [35] </ref> and LINUS [8] at least for problems like "promoter recognition in DNA nucleotides" for which it is a natural fit [52]. NeuroRule is another rule extraction approach that uses different combinations of weighted links to extract rules [43].
Reference: [36] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: However, the pruning and clustering processes lead to substantial overheads. * C4.5rules: C4.5rules was used by the authors of NeuroRule to extract rules from the iris and breast-cancer databases for comparison reasons. Like ID3 <ref> [36] </ref>, C4.5rules [20] generates decision tree rules based on the available input samples.
Reference: [37] <author> J.R. Quinlan. </author> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27 </volume> <pages> 221-234, </pages> <year> 1987. </year>
Reference-contexts: MofN extracts rules from the KBANN trained network through six main procedures. Rules extracted by MofN are significantly superior than rules extracted by other symbolic approaches such as C4.5 <ref> [37] </ref>, Either [35] and LINUS [8] at least for problems like "promoter recognition in DNA nucleotides" for which it is a natural fit [52]. NeuroRule is another rule extraction approach that uses different combinations of weighted links to extract rules [43].
Reference: [38] <author> R. Ruddel and A. Sangiovanni-Vincentelli. Espresso-MV: </author> <title> Algorithms for multiple-Valued logic minimization. </title> <booktitle> In Proceedings of Cust. Int. Circ. Conf. </booktitle> <address> Portland, </address> <month> May </month> <year> 1985. </year>
Reference-contexts: Given that the above conditions are satisfied, BIO-RE has some advantages: 1. It allows the use of available logic minimization tools. 2. Extracted rules are optimal and cannot be simplified any further. Hence, no rewriting procedure is required. 1 Espresso is a software package for logic design <ref> [38] </ref>. 3 PROPOSED RULE EXTRACTION APPROACHES 11 3. The extracted rules do not depend on the number of layers of the trained network. 4. The set of rules extracted by BIO-RE is comprehensive and understandable. 5. All premises of the extracted rules are conjuncted. 6.
Reference: [39] <author> K. Saito and R. Nakano. </author> <title> Medical diagnostic expert system based on DPD model. </title> <booktitle> In Proceedings of IEEE International Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 255-262, </pages> <year> 1988. </year>
Reference-contexts: An example of such a rule extraction approach 2 RULE EXTRACTION 8 is the algorithm developed by Saito and Nakano to extract medical diagnostic rules from a trained network <ref> [39] </ref>. BRAINNE [40], Rule-extraction-as-learning [7], and DEDEC [50] are other examples of extracting rules by investigating the input-output mapping of a trained network.
Reference: [40] <author> S. Sestito and T. Dillon. </author> <title> Automated knowledge acquisition of rules with continuously valued attributes. </title> <booktitle> In Proceedings of Twelfth International Conference on Expert Systems and their Applications (AVIGNON), </booktitle> <pages> pages 645-656, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: An example of such a rule extraction approach 2 RULE EXTRACTION 8 is the algorithm developed by Saito and Nakano to extract medical diagnostic rules from a trained network [39]. BRAINNE <ref> [40] </ref>, Rule-extraction-as-learning [7], and DEDEC [50] are other examples of extracting rules by investigating the input-output mapping of a trained network. In this paper we refer to this class as the Black-box Rule Extraction (BRE) category because rules are extracted regardless the type or the structure of the neural network.
Reference: [41] <author> R. Setiono. </author> <title> Extracting rules from pruned neural networks for breast cancer diagnosis. </title> <booktitle> Artificial Intelligence in Medicine, </booktitle> <pages> pages 37-51, </pages> <month> February </month> <year> 1996. </year>
Reference-contexts: The rules extracted by NeuroRule from the best among the pruned 100 MLP network architectures (6 inputs, 1 hidden, and 2 output nodes) are <ref> [43, 41] </ref>: Rule 1: If X 1 &lt; 7:0 and X 2 &lt; 8:0 and X 3 &lt; 3:0 and X 8 &lt; 9:0, then Benign Rule 2: If X 1 &lt; 7:0 and X 2 &lt; 8:0 and X 3 &lt; 3:0 and X 6 &lt; 9:0, then Benign Rule
Reference: [42] <author> R. Setiono and H. Liu. </author> <title> Understanding neural networks via rule extraction. </title> <booktitle> In Proceedings of Fourteenth International Joint Conference on Artificial Intelligence (IJCAI), </booktitle> <pages> pages 480-485, </pages> <year> 1995. </year>
Reference-contexts: The main reason of choosing NeuroRule and C4.5rules is that they have previously been used to extract rules for the same two databases used by Full-RE <ref> [42] </ref>. Moreover, they both extract comprehensive rules with relatively high correct classification rate as reported by the authors of NeuroRule [42]. For iris problem, we also compare the set of rules extracted by Full-RE with the corresponding set of rules extracted by KT algorithm [11]. <p> The main reason of choosing NeuroRule and C4.5rules is that they have previously been used to extract rules for the same two databases used by Full-RE <ref> [42] </ref>. Moreover, they both extract comprehensive rules with relatively high correct classification rate as reported by the authors of NeuroRule [42]. For iris problem, we also compare the set of rules extracted by Full-RE with the corresponding set of rules extracted by KT algorithm [11].
Reference: [43] <author> R. Setiono and H. Liu. </author> <title> Symbolic representation of neural networks. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 71-77, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: NeuroRule is another rule extraction approach that uses different combinations of weighted links to extract rules <ref> [43] </ref>. The main difference between NeuroRule and MofN is that the former extracts rules from networks after pruning their architectures and then discretizing their hidden 2 RULE EXTRACTION 7 units activation values. Recently, Howes and Crook introduced another algorithm that extracts rules from feedfor-ward neural networks [18]. <p> There is no need to prune the trained network since Full-RE is capable of extracting rules from MLPs of any size. In this section, we compare the performance of the extracted rules from the iris and the breast-cancer databases with the rules extracted by both NeuroRule and C4.5rules algorithms <ref> [43] </ref>. The main reason of choosing NeuroRule and C4.5rules is that they have previously been used to extract rules for the same two databases used by Full-RE [42]. Moreover, they both extract comprehensive rules with relatively high correct classification rate as reported by the authors of NeuroRule [42]. <p> The rules extracted by NeuroRule from the best among the pruned 100 MLP network architectures (6 inputs, 1 hidden, and 2 output nodes) are <ref> [43, 41] </ref>: Rule 1: If X 1 &lt; 7:0 and X 2 &lt; 8:0 and X 3 &lt; 3:0 and X 8 &lt; 9:0, then Benign Rule 2: If X 1 &lt; 7:0 and X 2 &lt; 8:0 and X 3 &lt; 3:0 and X 6 &lt; 9:0, then Benign Rule <p> &lt; 8:0 and X 3 &lt; 3:0 and X 6 &lt; 9:0, then Benign Rule 3: If X 2 &lt; 8:0 and X 3 &lt; 3:0 and X 6 &lt; 3:0 and X 8 &lt; 9:0, then Benign Rule 4: Default Rule (Malignant) The corresponding rules extracted by C4.5rules are <ref> [43] </ref>: Rule 1: If X 1 &lt; 7:0 and X 2 &lt; 8:0 and X 3 &lt; 3:0, then Benign Rule 2: If X 1 &lt; 7:0 and X 2 &lt; 2:0, then Benign Rule 3: If X 2 5:0, then Malignant Rule 4: If X 6 9:0, then Malignant Rule
Reference: [44] <author> I. Taha and J. Ghosh. </author> <title> Controlling water reservoirs using a hybrid intelligent architecture. </title> <booktitle> In Intelligent Engineering Systems Through Artificial Neural Networks ANNIE, </booktitle> <volume> volume 5, </volume> <pages> pages 63-68. </pages> <publisher> ASME Press, </publisher> <month> November </month> <year> 1995. </year> <note> REFERENCES 39 </note>
Reference-contexts: 1 Introduction Several researchers have investigated the design of hybrid systems that combine expert and connectionist subsystems <ref> [44, 45, 54, 10, 16, 15, 27] </ref>. <p> KBNNs attempt to exploit the complementary properties of knowledge based and neural network paradigms to obtain more powerful and robust systems. HIA <ref> [44] </ref>, KBANN [53, 34], RAPTURE [27] and KBCNN [10, 11] are examples of KBNN hybrid systems. Figure 1 sketches typical components of a KBNN system that combines rule-based and connectionist paradigms. <p> The remaining 6 input features are then used for training and testing an MLP with 9 hidden and 2 output nodes. 3. Network initialization: for the artificial problem the six initial rules are used by the Node Link Algorithm <ref> [44] </ref>, to initialize a network of 4 input, 6 hidden, and 4 output nodes. For both the iris and breast-cancer data sets, there is no prior knowledge so the corresponding networks are initialized randomly. 4.
Reference: [45] <author> I. Taha and J. Ghosh. </author> <title> A hybrid intelligent architecture and its application to water reservoir control. </title> <note> Submitted to Journal of Smart Engineering Systems, </note> <month> December </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Several researchers have investigated the design of hybrid systems that combine expert and connectionist subsystems <ref> [44, 45, 54, 10, 16, 15, 27] </ref>.
Reference: [46] <author> I. Taha and J. Ghosh. </author> <title> A hybrid intelligent architecture for refining input characterization and domain knowledge. </title> <booktitle> In Proceedings of World Congress on Neural Networks, </booktitle> <volume> volume II, </volume> <pages> pages 284-287, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Different discretization approaches can be exploited to compute discretization boundaries of input features X i s <ref> [46, 5, 23, 26, 56] </ref>. Full-RE uses the Chi2 [25] algorithm 2 , a powerful dis cretization tool, to compute discretization boundaries of input features.
Reference: [47] <author> H. Takagi and I. Hayashi. </author> <title> NN-driven fuzzy reasoning. </title> <editor> In J.C. Bezdek and S.K. Pal, editors, </editor> <booktitle> Fuzzy Models for Pattern Recognition, </booktitle> <pages> pages 496-512. </pages> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: Researchers have also combined connectionist systems with fuzzy logic systems to obtain Fuzzy Logic Neural Networks (FLNN or NeuroFuzzy) hybrid systems. In FLNNs, the neural network subsystem is typically used to adapt membership functions of fuzzy variables [6], or to refine and extract fuzzy rules <ref> [48, 47, 24] </ref>. Extracting symbolic rules from trained ANNs is an important feature of comprehensive hybrid 2 RULE EXTRACTION 4 systems, as it helps in: 1. Alleviating the knowledge acquisition problem and refining initial domain knowledge. 2. Providing reasoning and explanation capabilities. 3. Supporting cross-referencing and verification capabilities. 4. <p> Some FLNN systems include a fuzzy rule extraction module for refining fuzzy sets membership functions and explaining the trained neural network <ref> [17, 48, 47, 24] </ref> 2.2.4 Extracting rules from recurrent networks Recurrent networks have shown great success in representing finite state languages [14, 55] and deterministic finite state automata [13].
Reference: [48] <author> E. Tazaki and N. Inoue. </author> <title> A generation methods for fuzzy rules using neural networks with planar lattice architecture. </title> <booktitle> In IEEE International Conference on Neural Networks, volume III, </booktitle> <pages> pages 1743-1748, </pages> <address> Orlando, FL., </address> <year> 1994. </year>
Reference-contexts: Researchers have also combined connectionist systems with fuzzy logic systems to obtain Fuzzy Logic Neural Networks (FLNN or NeuroFuzzy) hybrid systems. In FLNNs, the neural network subsystem is typically used to adapt membership functions of fuzzy variables [6], or to refine and extract fuzzy rules <ref> [48, 47, 24] </ref>. Extracting symbolic rules from trained ANNs is an important feature of comprehensive hybrid 2 RULE EXTRACTION 4 systems, as it helps in: 1. Alleviating the knowledge acquisition problem and refining initial domain knowledge. 2. Providing reasoning and explanation capabilities. 3. Supporting cross-referencing and verification capabilities. 4. <p> Some FLNN systems include a fuzzy rule extraction module for refining fuzzy sets membership functions and explaining the trained neural network <ref> [17, 48, 47, 24] </ref> 2.2.4 Extracting rules from recurrent networks Recurrent networks have shown great success in representing finite state languages [14, 55] and deterministic finite state automata [13].
Reference: [49] <author> S.B. Thrun, J. Bala, E. Bloedorn, B. Cheng I. Bratko, S. Dzeroski k. De-Jong, S. Fahlman, D. Fisher, R. Hamann, K. Kaufman, S. Keller, I. Kononenko, J. Kreuziger, R. Michalski, T. Mitchell, P. Pachowicz, Y. Reich, H. Vafaie, K. Van de Welde, W. Wenzel, J. Wnek, and J. Zhang. </author> <title> The monk's problem: a performance comparison of different learning algorithms. </title> <type> Technical report, </type> <institution> Carnegie Mellon University CMU-CS-91-197, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Each of the 683 available instances is labeled as Benign (444 instances) or Malignant. These instances are divided into a training set of size 341 and a test set of size 342. Other popular data sets that have been used as benchmarks for rule extraction approaches are the Monk <ref> [49] </ref>, Mushroom [21] and the DNA promoter [54] data sets. All three of these data sets inputs are symbolic/discrete by nature.
Reference: [50] <author> A.B. Tickle, M. Orlowski, and J. Diederich. DEDEC: </author> <title> Decision Detection by Rule Extraction from Neural Networks. </title> <institution> Queensland University of Technology, Neurocomputing Research Center QUT NRC, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: An example of such a rule extraction approach 2 RULE EXTRACTION 8 is the algorithm developed by Saito and Nakano to extract medical diagnostic rules from a trained network [39]. BRAINNE [40], Rule-extraction-as-learning [7], and DEDEC <ref> [50] </ref> are other examples of extracting rules by investigating the input-output mapping of a trained network. In this paper we refer to this class as the Black-box Rule Extraction (BRE) category because rules are extracted regardless the type or the structure of the neural network.
Reference: [51] <author> A.B. Tickle, M. Orlowski, and J. Diederich. DEDEC: </author> <title> A methodology for extracting rules from trained artificial neural networks. </title> <editor> In R. Andrews and J. Diederich, editors, </editor> <title> Rules and Networks: </title> <booktitle> Proceedings of the Rule Extraction From Trained Artificial Neural Networks Workshop, </booktitle> <pages> pages 90-102. </pages> <institution> Queensland University of Technology, Neurocomputing Research Center QUT NRC, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: Another given name to this class of rule extraction techniques is 00 pedagogical 00 approaches [3]. For example, DEDEC extracts rules by ranking the inputs of an ANN according to their importance (contribution) to the ANN outputs <ref> [51] </ref>. This ranking process is done by examining the weight vectors of the ANN, which puts DEDEC on the border between LRE and BRE techniques.
Reference: [52] <author> G.G. Towell and J.W. Shavlik. </author> <title> The extraction of refined rules from knowledge-based neural networks. </title> <journal> Machine Learning, </journal> <volume> 13(1) </volume> <pages> 71-101, </pages> <year> 1993. </year>
Reference-contexts: All premises of a rule are conjuncted. Either [35], KT [9] and Subset <ref> [52] </ref> are three notable rule extraction algorithms in this category. <p> The size of the extracted rules can be limited by specifying the number of premises of the rules. Generally, the rules extracted by both KT and Subset algorithms are tractable specially in small application domains. Based on the shortcomings of the Subset algorithm, Towell and Shavlik <ref> [52] </ref> developed another rule extraction algorithm called MofN. The name of the algorithm reflects the rule format that the algorithm uses to represent the extracted rules: If ("at least" M of the following N premises are true) then (the concept designated by the unit is true). <p> Rules extracted by MofN are significantly superior than rules extracted by other symbolic approaches such as C4.5 [37], Either [35] and LINUS [8] at least for problems like "promoter recognition in DNA nucleotides" for which it is a natural fit <ref> [52] </ref>. NeuroRule is another rule extraction approach that uses different combinations of weighted links to extract rules [43]. The main difference between NeuroRule and MofN is that the former extracts rules from networks after pruning their architectures and then discretizing their hidden 2 RULE EXTRACTION 7 units activation values.
Reference: [53] <author> G.G. Towell and J.W. Shavlik. </author> <booktitle> Knowledge-based artificial neural networks. Artificial Intelligence, </booktitle> <address> 70(1-2):119-165, </address> <year> 1994. </year> <note> REFERENCES 40 </note>
Reference-contexts: KBNNs attempt to exploit the complementary properties of knowledge based and neural network paradigms to obtain more powerful and robust systems. HIA [44], KBANN <ref> [53, 34] </ref>, RAPTURE [27] and KBCNN [10, 11] are examples of KBNN hybrid systems. Figure 1 sketches typical components of a KBNN system that combines rule-based and connectionist paradigms. Researchers have also combined connectionist systems with fuzzy logic systems to obtain Fuzzy Logic Neural Networks (FLNN or NeuroFuzzy) hybrid systems.
Reference: [54] <author> G.G. Towell, J.W. Shavlik, and M.O. Noordwier. </author> <title> Refinement of approximate domain theories by knowledge-based artificial neural network. </title> <booktitle> In Proceedings of Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 861-866, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Several researchers have investigated the design of hybrid systems that combine expert and connectionist subsystems <ref> [44, 45, 54, 10, 16, 15, 27] </ref>. <p> These instances are divided into a training set of size 341 and a test set of size 342. Other popular data sets that have been used as benchmarks for rule extraction approaches are the Monk [49], Mushroom [21] and the DNA promoter <ref> [54] </ref> data sets. All three of these data sets inputs are symbolic/discrete by nature.
Reference: [55] <author> R.L. Watrous and G.M. Kuhn. </author> <title> Induction of finite-state languages using second-order recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 406-414, </pages> <year> 1992. </year>
Reference-contexts: Some FLNN systems include a fuzzy rule extraction module for refining fuzzy sets membership functions and explaining the trained neural network [17, 48, 47, 24] 2.2.4 Extracting rules from recurrent networks Recurrent networks have shown great success in representing finite state languages <ref> [14, 55] </ref> and deterministic finite state automata [13]. Omlin and Giles, [33] have developed a heuristic algorithm to extract grammar rules in the form of Deterministic Finite-state Automata (DFA) from discrete-time neural networks and specifically from second-order networks.
Reference: [56] <author> S. Wolfram. </author> <title> Mathematica: A System for Doing Mathematics by Computer. </title> <publisher> Addison Wesley, </publisher> <year> 1991. </year>
Reference-contexts: Different discretization approaches can be exploited to compute discretization boundaries of input features X i s <ref> [46, 5, 23, 26, 56] </ref>. Full-RE uses the Chi2 [25] algorithm 2 , a powerful dis cretization tool, to compute discretization boundaries of input features.
References-found: 56

