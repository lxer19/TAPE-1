URL: http://www.cri.ensmp.fr/doc/A-302.ps.gz
Refering-URL: http://www.cri.ensmp.fr/rapports.html
Root-URL: 
Email: (coelho@cri.ensmp.fr)  
Phone: phone: (+33j0) 1 64 69 48 52, fax: (+33j0) 1 64 69 47 09  
Title: Compiling Dynamic Mappings with Array  
Author: Fabien Coelho 
Address: 35, rue Saint-Honore, F-77305 Fontainebleau cedex, France.  
Affiliation: Centre de Recherche en Informatique, Ecole des mines de Paris,  
Web: PPoPP'97)  URL: http://www.cri.ensmp.fr/pips  
Note: Copies (TR EMP CRI A 302. To appear in  
Abstract: Array remappings are useful to many applications on distributed memory parallel machines. They are available in High Performance Fortran, a Fortran-based data-parallel language. This paper describes techniques to handle dynamic mappings through simple array copies: array remap-pings are translated into copies between statically mapped distinct versions of the array. It discusses the language restrictions required to do so. The remapping graph which captures all remapping and liveness information is presented, as well as additional data-flow optimizations that can be performed on this graph, so as to avoid useless remap-pings at run time. Such useless remappings appear for arrays that are not used after a remapping. Live array copies are also kept to avoid other flow-dependent useless remappings. Finally the code generation and runtime required by our scheme are discussed. These techniques are implemented in our prototype HPF compiler. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: Following [11] we plan to exploit as much as possible this information to remove useless remappings that can be detected at compile time, or even some that may occur under particular run time conditions. These optimizations on G R are expressed as standard data flow problems <ref> [14, 13, 1] </ref>. 4.1 Removing useless remappings Leaving copies that are not live appear in G R with the N (not used) label. It means that although some remapping on an array was required by the user, this array is not referenced afterwards in its new mapping.
Reference: [2] <author> Jean-Yves Berthou and Laurent Colombet. </author> <title> Experiences in Data Parallel Programming on Cray MPP machines. First High Performance Fortran (HPF) Users Group Conference, </title> <address> Santa Fe, NM, USA, </address> <month> February </month> <year> 1997. </year>
Reference-contexts: 1 Introduction Array remappings, i.e. the ability to change array mappings at runtime, are definitely useful to applications and kernels such as ADI [16], linear algebra solvers [19], 2d FFT [10], signal processing [17] or tensor computations <ref> [2] </ref> for efficient execution on distributed memory parallel computers. HPF [15, 12] provides explicit remappings through realign and redistribute directives and implicit ones at subroutine calls and returns for array arguments.
Reference: [3] <author> Fabien Coelho. </author> <title> Contributions to High Performance Fortran Compilation. </title> <type> PhD thesis, </type> <institution> Ecole des mines de Paris, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: All issues are addressed: languages restrictions (or corrections) required for this scheme to be applicable, actual management of simple references in the code, data-flow optimizations, down to the runtime system requirements. This technique is implemented in our HPF compiler <ref> [3] </ref>. First, Section 2 presents the language restrictions, the handling of subroutine calls and our general approach to compile remappings. Second, Section 3 focuses on the definition and construction of the remapping graph which captures all necessary remapping and liveness information on a contracted control flow graph. <p> We have also presented optimizations enabled by our technique, to remove useless remappings and to detect live copies that can be reused without communication. Finally runtime implications have been discussed. Most of the techniques described in this paper are implemented in our prototype HPF compiler <ref> [3] </ref>. It is available from http://www.cri.ensmp.fr/pips/hpfc.html. The standard statically mapped HPF code generated is then compiled, with a special code generation phase for handling remapping communication due to the explicit array copies.
Reference: [4] <author> Fabien Coelho. </author> <title> Discussing HPF Design Issues. </title> <booktitle> In Euro-Par'96, </booktitle> <address> Lyon, France, pages I.571-I.578, </address> <month> August </month> <year> 1996. </year> <note> LNCS 1123. Also report EMP CRI A-284, </note> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Both schemes have been developed concurrently. Such techniques require well behaved programs: remappings should not appear anywhere in the program, to avoid references with ambiguous mappings as shown in Figure 5. We go a step further by suggesting <ref> [4] </ref> that the language should forbid such cases. This is supported by our experience with real applications that require dynamic mappings: ambiguous mappings are rather bugs to be reported. Our approach is unique from several points. First, our !hpf$ distribute A (block) ...
Reference: [5] <author> Fabien Coelho and Corinne Ancourt. </author> <title> Optimal Compilation of HPF Remappings. </title> <journal> Jounal of Parallel and Distributed Computing, </journal> <volume> 38(2) </volume> <pages> 229-236, </pages> <month> November </month> <year> 1996. </year> <note> Also TR EMP CRI A-277 (October 1995). </note>
Reference-contexts: Further powerful extensions can be delayed until applications need them and when compilation techniques are proven practical and efficient. These language restrictions are also required to compile remappings rather than to rely on generic library functions. Indeed, for compiling a remapping into a message passing SPMD code <ref> [5] </ref> both source and target mappings must be known. Then the compiler can take advantage of all available information to generate efficient code. The implicit philosophy is that the compiler handles most of the issues at compile time, with minimum left to run time.
Reference: [6] <author> Fabien Coelho and Henry Zongaro. </author> <title> ASSUME directive proposal. </title> <type> TR A 287, </type> <institution> CRI, Ecole des mines de Paris, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: Interfaces describing mappings of arguments of called subroutines are mandatory. Thus all necessary information is available for the caller to comply to the ar gument mapping of its callees. 3. Transcriptive mappings associated to subroutine arguments are forbidden. This feature can be replaced by a more precise mapping descriptions <ref> [6] </ref>, or could be enabled but would then require an interprocedural com pilation such as cloning [18].
Reference: [7] <author> Charles Consel and Fran~cois Noel. </author> <title> A General Approach for Run-Time Specialization and its Application to C. </title> <booktitle> In Symposium on Principles of Programming Language, </booktitle> <pages> pages 145-156, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: Another technique is run time partial evaluation that dynamically generates an optimized code once enough information is available <ref> [7] </ref>. However even though there are overheads and the runtime is complex. As HPF is expected to bring high performance, tran-scriptive and ambiguous mappings seem useless. They restrict the implementor choices and possible optimizations.
Reference: [8] <author> Beatrice Creusillet. </author> <title> Array Region Analyses and Applications. </title> <type> PhD thesis, </type> <institution> Ecole des mines de Paris, </institution> <month> Decem-ber </month> <year> 1996. </year>
Reference-contexts: This is detailed in Appendix D. 4.3 Other optimizations Further optimization can be thought of, as discussed in [11]. Array kill analysis, for instance based on array regions <ref> [8, 9] </ref>, tells whether the values of an array are dead at a given point in the program.
Reference: [9] <author> Beatrice Creusillet and Fran~cois Irigoin. </author> <title> Interprocedu-ral array region analyses. </title> <journal> Int. J. of Parallel Programming (special issue on LCPC), </journal> <volume> 24(6) </volume> <pages> 513-546, </pages> <year> 1996. </year>
Reference-contexts: This is detailed in Appendix D. 4.3 Other optimizations Further optimization can be thought of, as discussed in [11]. Array kill analysis, for instance based on array regions <ref> [8, 9] </ref>, tells whether the values of an array are dead at a given point in the program.
Reference: [10] <author> S.K.S. Gupta, C.-H. Huang, and P. Sadayappan. </author> <title> Implementing Fast Fourier Transforms on Distributed-Memory Multiprocessors using Data Redistributions. </title> <journal> Parallel Processing Letters, </journal> <volume> 4(4) </volume> <pages> 477-488, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Array remappings, i.e. the ability to change array mappings at runtime, are definitely useful to applications and kernels such as ADI [16], linear algebra solvers [19], 2d FFT <ref> [10] </ref>, signal processing [17] or tensor computations [2] for efficient execution on distributed memory parallel computers. HPF [15, 12] provides explicit remappings through realign and redistribute directives and implicit ones at subroutine calls and returns for array arguments.
Reference: [11] <author> Mary W. Hall, Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Interprocedural Compilation of Fortran D for MIMD Distributed-Memory Machines. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 522-534, </pages> <year> 1992. </year>
Reference-contexts: A ... if (...) then !hpf$ realign with T 2 :: A ... A ... endif !hpf$ redistribute T 2 ... A ... 1.2 Related work Such optimizations to avoid useless remapping communications, especially interprocedural ones, have been discussed <ref> [11, 18] </ref>. It is shown [11] that the best approach to handle subroutine calls is that callers must comply to callee requirements. We follow this approach. In contrast to these papers we rely on standard explicit interfaces to provide the needed information about callees while enabling similar optimizations. <p> A ... if (...) then !hpf$ realign with T 2 :: A ... A ... endif !hpf$ redistribute T 2 ... A ... 1.2 Related work Such optimizations to avoid useless remapping communications, especially interprocedural ones, have been discussed [11, 18]. It is shown <ref> [11] </ref> that the best approach to handle subroutine calls is that callers must comply to callee requirements. We follow this approach. In contrast to these papers we rely on standard explicit interfaces to provide the needed information about callees while enabling similar optimizations. <p> Third, our runtime can handle arrays with an ambiguous mapping, provided that it is not referenced in such a state. This requirement is weaker than the one for well behaved programs <ref> [18, 11] </ref>, since it enables cases such as Figure 6. 1.3 Outline This paper describes a practical approach to handle HPF remappings. <p> Another side-effect of optional interfaces, transcriptive mappings and weak descriptive mappings is that the compiler must make the callee handle remappings as a default case. But the callee has both less information and optimization opportunities <ref> [11] </ref>. These features improve expressiveness, but at the price of performance. Delaying to run time the array mapping handling of references means delaying the actual address calculations and reduces compile time optimizations which are mandatory to cache-based processors. Also compiling for an unknown mapping makes many communication optimizations impractical. <p> Following <ref> [11] </ref> we plan to exploit as much as possible this information to remove useless remappings that can be detected at compile time, or even some that may occur under particular run time conditions. <p> This is detailed in Appendix D. 4.3 Other optimizations Further optimization can be thought of, as discussed in <ref> [11] </ref>. Array kill analysis, for instance based on array regions [8, 9], tells whether the values of an array are dead at a given point in the program. <p> The directive creates a remapping vertex tagged D. Remappings can be moved around in the control flow graph, especially out of loops. From the code in Figure 15 we suggest to move the remappings as shown in Figure 16. This differs from <ref> [11] </ref>: the initial remapping is not moved out of the loop, because if t &lt; 1 this would induce a useless remapping. The remapping from block to cyclic will only occur at the first iteration of the loop.
Reference: [12] <author> HPF Forum. </author> <title> High Performance Fortran Language Specification. </title> <institution> Rice University, Houston, Texas, </institution> <note> Novem-ber 1996. version 2.0. </note>
Reference-contexts: 1 Introduction Array remappings, i.e. the ability to change array mappings at runtime, are definitely useful to applications and kernels such as ADI [16], linear algebra solvers [19], 2d FFT [10], signal processing [17] or tensor computations [2] for efficient execution on distributed memory parallel computers. HPF <ref> [15, 12] </ref> provides explicit remappings through realign and redistribute directives and implicit ones at subroutine calls and returns for array arguments. <p> However it must be noted that: (1) software engineering is an issue that deserves consideration; (2) the current status of the language definition is to drop remappings as a whole (by moving them out of the core language as simple approved extensions <ref> [12] </ref>) because they are considered too difficult to handle; (3) we have not encountered any real application so far that would benefit from the full expressiveness of arbitrary flow-dependent remappings.
Reference: [13] <author> Ken Kennedy. </author> <title> A survey of data flow analysis techniques. </title> <editor> In S. Muchnick and N. Jones, editors, </editor> <title> Program Flow Analysis: </title> <booktitle> Theory and Applications, </booktitle> <pages> pages 5-54. </pages> <publisher> Prentice-Hall, Inc., </publisher> <address> Engelwood Cliffs, </address> <year> 1979. </year>
Reference-contexts: Following [11] we plan to exploit as much as possible this information to remove useless remappings that can be detected at compile time, or even some that may occur under particular run time conditions. These optimizations on G R are expressed as standard data flow problems <ref> [14, 13, 1] </ref>. 4.1 Removing useless remappings Leaving copies that are not live appear in G R with the N (not used) label. It means that although some remapping on an array was required by the user, this array is not referenced afterwards in its new mapping.
Reference: [14] <author> Gary A. Kildall. </author> <title> A unified approach to global program optimization. </title> <booktitle> In Symposium on Principles of Programming Language, </booktitle> <pages> pages 194-206, </pages> <year> 1973. </year>
Reference-contexts: Following [11] we plan to exploit as much as possible this information to remove useless remappings that can be detected at compile time, or even some that may occur under particular run time conditions. These optimizations on G R are expressed as standard data flow problems <ref> [14, 13, 1] </ref>. 4.1 Removing useless remappings Leaving copies that are not live appear in G R with the N (not used) label. It means that although some remapping on an array was required by the user, this array is not referenced afterwards in its new mapping.
Reference: [15] <author> Charles Koelbel, David Loveman, Robert Schreiber, Guy Steele, and Mary Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Array remappings, i.e. the ability to change array mappings at runtime, are definitely useful to applications and kernels such as ADI [16], linear algebra solvers [19], 2d FFT [10], signal processing [17] or tensor computations [2] for efficient execution on distributed memory parallel computers. HPF <ref> [15, 12] </ref> provides explicit remappings through realign and redistribute directives and implicit ones at subroutine calls and returns for array arguments.
Reference: [16] <author> Ulrich Kremer. </author> <title> Automatic Data Layout for Distributed Memory Machines. </title> <type> PhD thesis, </type> <institution> Rice University, Hous-ton, Texas, </institution> <month> October </month> <year> 1995. </year> <note> Available as CRPC-TR95-559-S. </note>
Reference-contexts: 1 Introduction Array remappings, i.e. the ability to change array mappings at runtime, are definitely useful to applications and kernels such as ADI <ref> [16] </ref>, linear algebra solvers [19], 2d FFT [10], signal processing [17] or tensor computations [2] for efficient execution on distributed memory parallel computers. HPF [15, 12] provides explicit remappings through realign and redistribute directives and implicit ones at subroutine calls and returns for array arguments.
Reference: [17] <author> Peter G. Meisl, Mabo R. Ito, and Ian G. Cumming. </author> <title> Parallel synthetic aperture radar processing on workstation networks. </title> <booktitle> In International Parallel Processing Symposium, </booktitle> <pages> pages 716-723, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Array remappings, i.e. the ability to change array mappings at runtime, are definitely useful to applications and kernels such as ADI [16], linear algebra solvers [19], 2d FFT [10], signal processing <ref> [17] </ref> or tensor computations [2] for efficient execution on distributed memory parallel computers. HPF [15, 12] provides explicit remappings through realign and redistribute directives and implicit ones at subroutine calls and returns for array arguments.
Reference: [18] <author> Daniel J. Palermo, Eugene W. Hodges IV, and Prithvi-raj Banerjee. </author> <title> Interprocedural Array Redistribution Data-Flow Analysis. </title> <booktitle> In Language and Compilers for Parallel Computing, pages aa.1-aa.15, August 1996. </booktitle> <address> San Jose, CA. </address>
Reference-contexts: A ... if (...) then !hpf$ realign with T 2 :: A ... A ... endif !hpf$ redistribute T 2 ... A ... 1.2 Related work Such optimizations to avoid useless remapping communications, especially interprocedural ones, have been discussed <ref> [11, 18] </ref>. It is shown [11] that the best approach to handle subroutine calls is that callers must comply to callee requirements. We follow this approach. In contrast to these papers we rely on standard explicit interfaces to provide the needed information about callees while enabling similar optimizations. <p> Both the alignment and distribution problems must be solved to extract actual mappings associated to arrays in the program. The Static Distribution Assignment scheme <ref> [18] </ref> to handle dynamic array references is very similar to our approach which uses distinct copies for each array mapping. Both schemes have been developed concurrently. <p> A ... optimizations are expressed on the remapping graph which captures all mapping and use information for a routine. This graph can be seen as the dual of a contracted control-flow graph as noted in <ref> [18] </ref>. The advantage is that our graph is much smaller than the usual control-flow graph. Second, read and write uses of arrays are distinguished, enabling the detection of live copies that can be reused without communication in case of a remapping. <p> Third, our runtime can handle arrays with an ambiguous mapping, provided that it is not referenced in such a state. This requirement is weaker than the one for well behaved programs <ref> [18, 11] </ref>, since it enables cases such as Figure 6. 1.3 Outline This paper describes a practical approach to handle HPF remappings. <p> Transcriptive mappings associated to subroutine arguments are forbidden. This feature can be replaced by a more precise mapping descriptions [6], or could be enabled but would then require an interprocedural com pilation such as cloning <ref> [18] </ref>. Condition 1 is illustrated in Figure 5: Array A mapping is modified by the redistribute if the realign was executed before at runtime, otherwise A is aligned with template T 1 and get through T 2 redistribution unchanged. <p> Also compiling for an unknown mapping makes many communication optimizations impractical. Expensive and more complex techniques can be used to generate good code when lacking information: partial or full cloning of subroutines to be compiled with different assumptions, that requires a full interprocedural analysis and compilation <ref> [18] </ref>. Another technique is run time partial evaluation that dynamically generates an optimized code once enough information is available [7]. However even though there are overheads and the runtime is complex. As HPF is expected to bring high performance, tran-scriptive and ambiguous mappings seem useless.
Reference: [19] <author> Loc Prylli and Bernard Tourancheau. </author> <title> Efficient Block Cyclic Data Redistribution. </title> <booktitle> In Euro-Par'96, </booktitle> <address> Lyon, France, pages I.155-I.164, </address> <month> August </month> <year> 1996. </year> <note> LNCS 1123. Also INRIA RR 2766. 8 !hpf$ distribute T(*,block) !hpf$ align A(i,j) with T(i,j) if (...) then !hpf$ realign A(i,j) with T(j,i) endif !hpf$ redistribute T(block,*) </note>
Reference-contexts: 1 Introduction Array remappings, i.e. the ability to change array mappings at runtime, are definitely useful to applications and kernels such as ADI [16], linear algebra solvers <ref> [19] </ref>, 2d FFT [10], signal processing [17] or tensor computations [2] for efficient execution on distributed memory parallel computers. HPF [15, 12] provides explicit remappings through realign and redistribute directives and implicit ones at subroutine calls and returns for array arguments.
References-found: 19

