URL: http://www.cse.unsw.edu.au/~achim/PostScripts/Hoffmann-ISANN94.ps
Refering-URL: http://www.cse.unsw.edu.au/~achim/index.html
Root-URL: http://www.cse.unsw.edu.au
Email: E-mail: achim@cse.unsw.edu.au  
Title: A Greedy Learning Approach for Multi-Layer Perceptrons  
Author: Achim G. Hoffmann 
Address: Sydney, 2052 NSW, Australia  
Affiliation: School of Computer Science Engineering University of New South Wales  
Abstract: Neural networks as a general mechanism for learning and adaptation became increasingly popular in recent years. Mainly due to the development of the backpropagation learning procedure which allowed to train Multi-Layer Perceptrons. Unfortunately, back propagation is well-known for its particularly low learning speed. Thus, it is desirable to have more efficient learning mechanisms. In this paper a new learning algorithm for multi-layer Perceptrons is presented. It will be argued that by using a four-layer Perceptron instead of a three-layer Perceptron substantial improvements in the learning speed can be obtained. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. I. Gallant. </author> <title> Perceptron-based learning algorithms. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(2) </volume> <pages> 179-191, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: It will be shown that the new approach requires a fourth layer in order to be capable of learning arbitrarily complex functions in a greedy manner. The Perceptron pocket algorithm <ref> [1] </ref> and some variations of it are used for the independent training of each single neuron. This algorithm is always advisable for training a single Perceptron if no linear threshold function can be found which classifies all training examples correctly.
Reference: [2] <author> R. Hecht-Nielsen. </author> <title> Kolmogorov's mapping neural network existence theorem. </title> <booktitle> In Proceedings of IEEE first International Joint Conference of Neural Networks, volume III, </booktitle> <pages> pages 11-14, </pages> <year> 1987. </year>
Reference-contexts: This compares to the proved result that the three layer Perceptron is capable to generate any particular (even continuous) function if the weights and thresholds are properly adjusted and sufficiently many neurons are on the hidden layer (see e.g. <ref> [2] </ref>). 1 Actually, Werbos [6] had developed the generalized delta rule already in 1974 without putting it into the context of MLPs. 3 output layer input layer second hidden layer first hidden layer 11 12 13 22 23 24 NNNN 21 14 a linear threshold function.
Reference: [3] <author> M. Minsky and S. Papert. </author> <title> Perceptrons. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1969. </year>
Reference-contexts: Linear threshold functions are quite limited in their ability to divide a feature space into two different areas. For many classes of sets of patterns an appropriate separation cannot be achieved by a linear threshold function. Even reasonable approximations may not exist as linear threshold functions. Minsky and Papert <ref> [3] </ref> studied in their very influential book Perceptrons the limitations of linear threshold functions in depth.
Reference: [4] <author> F. Rosenblatt. </author> <title> Two theorems of statistical separability in the perceptron. </title> <booktitle> In Proceedings of the Symposium on the Mechanization of thought, </booktitle> <pages> pages 421-456, </pages> <address> London, </address> <year> 1959. </year> <title> Her Majesty's Stationary Office. </title>
Reference-contexts: In the n-dimensional feature space the separating linear decision function can be thought of a hyperplane dividing the feature space into two separate areas. To the end of the 1950s, Rosenblatt <ref> [4] </ref> developed his so-called Perceptron algorithm, which computes an appropriate linear threshold function for a given set of patterns.
Reference: [5] <author> D. E. Rumelhart, J. L. McClelland, </author> <title> and the PDP Research Group. Parallel Distributed Processing: Explorations in the Microstructure of Cognition,I & II. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: Unfortunately, the training of a combination of Perceptrons appears much harder than the training of a single Perceptron, since it is not clear how to distribute the feedback from each example over the various neurons. 2.3 Backpropagation Nevertheless, in 1986 Rumelhart, McClelland & Williams, see <ref> [5] </ref>, introduced in the context of multi-layer Perceptrons (MLPs) the generalized delta rule, which became famous as the backpropagation algorithm. 1 It is a general learning rule, which allows to find even in an MLP an appropriate set of threshold values and input weights for each neuron.
Reference: [6] <author> P. J. Werbos. </author> <title> Beyond regression: New tools for prediction and analysis in the behavioral sciences. </title> <type> PhD thesis, </type> <year> 1974. </year> <month> 6 </month>
Reference-contexts: This compares to the proved result that the three layer Perceptron is capable to generate any particular (even continuous) function if the weights and thresholds are properly adjusted and sufficiently many neurons are on the hidden layer (see e.g. [2]). 1 Actually, Werbos <ref> [6] </ref> had developed the generalized delta rule already in 1974 without putting it into the context of MLPs. 3 output layer input layer second hidden layer first hidden layer 11 12 13 22 23 24 NNNN 21 14 a linear threshold function.
References-found: 6

