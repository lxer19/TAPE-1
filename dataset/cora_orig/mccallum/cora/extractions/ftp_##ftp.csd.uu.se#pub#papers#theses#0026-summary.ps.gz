URL: ftp://ftp.csd.uu.se/pub/papers/theses/0026-summary.ps.gz
Refering-URL: http://www.csd.uu.se/papers/long-theses.html
Root-URL: 
Title: Compilation Techniques for Prolog  
Author: Thomas Lindgren 
Degree: Thesis for the Degree of Doctor of Philosophy  
Date: UPPSALA 1996  
Address: Uppsala University  
Affiliation: Computing Science Department  
Note: UPPSALA THESES IN COMPUTING SCIENCE 26  
Abstract-found: 0
Intro-found: 0
Reference: 1. <author> H. At-Kaci, </author> <title> The WAM: A (Real) Tutorial, </title> <publisher> MIT Press, </publisher> <year> 1991. </year> <month> f5g </month>
Reference-contexts: The traditional solution to this problem is to translate the Prolog code into abstract machine instructions that manage the control structures of Prolog, typically using a variant of WAM. The disadvantage with this approach is that the WAM is quite complex <ref> [132, 1] </ref> and thus difficult to implement, optimize and reason about. While most Prolog implementations use a Prolog-to-WAM compiler, an alternative approach, binarization, has been proposed by Tarau [115, 116, 117] and Demoen and Marien [42, 41].
Reference: 2. <author> K.A.M. Ali, R. Karlsson, </author> <title> The Muse or-parallel Prolog model and its performance, </title> <booktitle> in Proceedings of the North American Conference on Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year> <month> f3g </month>
Reference-contexts: If several goals are resolved simultaneously, the system extracts and-parallelism. When the resolved goals share free variables, the parallelism is classified as dependent and-parallelism. When goals do not share free variables, the parallelism is said to be independent and-parallelism. Or-parallel systems have been successfully built for some years <ref> [2, 75] </ref> and have been applied to search problems. In recent years, or-parallel constraint solving has emerged as a potential application area [24, 122]. For and-parallel systems, the main problem is the management of parallel backtracking.
Reference: 3. <author> A.W. Appel, </author> <title> Simple generational garbage collection and fast alloca tion, </title> <journal> Software|Practice and Experience 19(2) </journal> <pages> 171-183, </pages> <year> 1989. </year> <month> f10g </month>
Reference-contexts: That stack is then collected using a mark-sweep algorithm to retain the ordering between variables, while copying is used for the rest of the heap. Finally, the paper shows how to extend the copying algorithm to generational collection. Generational garbage collection <ref> [71, 3] </ref> relies on the observation that newly created objects tend to be short-lived. Thus, garbage collection should concentrate on recently created data. The heap is split into two or more generations, and the most recent generation is collected most frequently.
Reference: 4. <author> A.W. Appel, </author> <title> Compiling With Continuations, </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year> <month> f16g </month>
Reference-contexts: We have found that explicit operations that save and restore state can be useful, since they then can be moved and removed by program transformations. Since they generate higher-order terms in their translation, a subsequent pass of closure conversion <ref> [4] </ref> converts the term to a first-order representation. Our algorithm, in contrast, directly generates first-order terms as continuations. Neumerkel [86, 85] has proposed Continuation Prolog, which allows compilers to manipulate continuations and remove some auxilliary output variables. The new program is then translated to binary or standard Prolog.
Reference: 5. <author> K. Appleby, M. Carlsson, S. Haridi, and D. Sahlin, </author> <title> Garbage Collection for Prolog Based on WAM, </title> <journal> Communications of the ACM, </journal> <volume> 31(6) </volume> <pages> 719-741, </pages> <month> June </month> <year> 1988. </year> <note> f19, 20g </note>
Reference-contexts: Paper D Prolog implementations such as SICStus Prolog use a mark-sweep algorithm that first marks the live data, then compacts the heap. We take the 1.3. Related work 19 implementation of Appleby et al. <ref> [5] </ref> as typical. This algorithm works in four steps and is based on the Deutsch-Schorr-Waite [99, 29] algorithm for marking and on Morris' algorithm [82, 29] for compaction (a more extensive summary is given in Paper D). <p> Demoen et al do not directly compare their algorithm's efficiency with ours. However, their mark-copy algorithm appears to be approximately as efficient as ours. They do not present measurements for generational mark-copy. Sahlin [93] has developed a method that makes the execution time of the Appleby et al. <ref> [5] </ref> algorithm proportional to the size of the live data. The main drawback of Sahlin's algorithm is that implementing the mark-sweep algorithm becomes more difficult. To our knowledge it has never been implemented.
Reference: 6. <author> U. Banerjee, R. Eigenmann, A. Nicolau, D.A. Padua, </author> <title> Automatic pro gram parallelization, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 81, no. 2, </volume> <month> February </month> <year> 1993 </year> <month> f22g </month>
Reference-contexts: Synchronization requirements are derived from dependence analysis, which determines what loads and stores can interfere. Recently, array dataflow analysis [44] has been developed to deal more precisely with this problem. Automatic loop-level parallelization of imperative programs has met some difficulties. For instance, Banerjee et al <ref> [6] </ref> note: "Dependence analysis in the presence of pointers has been found to be a particularly difficult problem. . . .
Reference: 7. <author> J. Barklund, J. Bevemyr, </author> <title> Executing bounded quantifications on shared memory multiprocessors, </title> <booktitle> in Programming Language Implementation and Logic Programming 1993, </booktitle> <publisher> LNCS 714, Springer Verlag, </publisher> <year> 1993. </year> <month> f22g </month>
Reference-contexts: Barklund and Millroth [8] constructed Nova Prolog, a data-parallel logic language, which later was generalized into bounded quantifications [9]. A bounded quantification expresses some action to be taken over a finite set of elements, which often allows data-parallel execution <ref> [7] </ref>. Voronkov [130] independently laid theoretical foundations for bounded quantifications. Using bounded quantifications allowed a concise data parallel approach to logic programming. Finally, Sehr [101, 102] has independently proposed extraction of data-parallelism in the context of Prolog; see also Section 1.3.
Reference: 8. <author> J. Barklund, H. Millroth, </author> <title> Nova Prolog, </title> <type> UPMAIL Technical Report 52, </type> <institution> Computing Science Department, Uppsala University, </institution> <year> 1988. </year> <title> f4, </title> <publisher> 22g </publisher>
Reference-contexts: The single-program, multiple-data (SPMD) paradigm, where each process is a parallel loop iteration, exploits data-parallelism and is arguably the most popular programming model for conventional multiprocessors. In logic programming, data-parallelism has been used in the contexts of or-parallelism [109, 122] and and-parallelism <ref> [8, 81, 9, 130] </ref>. The sixth and final approach combines the previous methods. A program may lack parallelism of a given form, which will yield poor performance on systems that extract only that form of parallelism. Some researchers 1.2. <p> The data-parallel paradigm, where the user specifies a single thread of control that operates simultaneously on many data objects, is suitable for this purpose. Barklund and Millroth <ref> [8] </ref> constructed Nova Prolog, a data-parallel logic language, which later was generalized into bounded quantifications [9]. A bounded quantification expresses some action to be taken over a finite set of elements, which often allows data-parallel execution [7]. Voronkov [130] independently laid theoretical foundations for bounded quantifications.
Reference: 9. <author> J. Barklund, H. Millroth, </author> <title> Providing iteration and concurrency in logic programs through bounded quantifications, </title> <booktitle> in Proc. International Conference on Fifth Generation Systems, </booktitle> <publisher> Ohmsha, </publisher> <year> 1992. </year> <note> f4, 22g 25 26 </note>
Reference-contexts: The single-program, multiple-data (SPMD) paradigm, where each process is a parallel loop iteration, exploits data-parallelism and is arguably the most popular programming model for conventional multiprocessors. In logic programming, data-parallelism has been used in the contexts of or-parallelism [109, 122] and and-parallelism <ref> [8, 81, 9, 130] </ref>. The sixth and final approach combines the previous methods. A program may lack parallelism of a given form, which will yield poor performance on systems that extract only that form of parallelism. Some researchers 1.2. <p> The data-parallel paradigm, where the user specifies a single thread of control that operates simultaneously on many data objects, is suitable for this purpose. Barklund and Millroth [8] constructed Nova Prolog, a data-parallel logic language, which later was generalized into bounded quantifications <ref> [9] </ref>. A bounded quantification expresses some action to be taken over a finite set of elements, which often allows data-parallel execution [7]. Voronkov [130] independently laid theoretical foundations for bounded quantifications. Using bounded quantifications allowed a concise data parallel approach to logic programming.
Reference: 10. <author> J. Barklund, H. Millroth, </author> <title> Garbage cut for garbage collection of iterative Prolog programs, </title> <booktitle> 3rd Symposium on Logic Programming, </booktitle> <address> Salt Lake City, </address> <month> September </month> <year> 1986, </year> <note> IEEE. f19g </note>
Reference-contexts: For the older generation they use a mark-sweep algorithm. The technique is similar to that described by Barklund and Millroth <ref> [10] </ref> and later by Older and Rummell [88]. We show in Paper D how a simpler copying collector can be implemented, how the troublesome primitives can be accomodated better and how generational collection can be done in a simple and intuitive way.
Reference: 11. <author> J. Beer, </author> <title> The occur-check problem revisited, </title> <journal> Journal of Logic Program ming Vol. </journal> <volume> 5, </volume> <pages> pp. 243-261, </pages> <publisher> North-Holland, </publisher> <year> 1988. </year> <month> f17g </month>
Reference-contexts: We have studied control flow analysis for a language with less general and somewhat different control structures, and have shown that the solution can be found quickly and represented compactly. Paper C Beer <ref> [11] </ref> was the first to exploit uninitialized variables. This was done by a runtime approach, where registers and heap cells were tagged as uninitial-ized when created and modified during execution when unifications and similar operations occurred. He found that a large portion of the dynamically occurring variables actually were uninitialized.
Reference: 12. <author> Y. Bekkers, O. Ridoux and L. Ungaro, </author> <title> Dynamic Memory Management for Sequential Logic Programming Languages, </title> <booktitle> Proceedings of the International Workshop on Memory Management 92, </booktitle> <publisher> LNCS 637, Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1992. </year> <month> f19g </month>
Reference-contexts: However, where Touati and Hama still wish to retain properties such as memory recovery on backtracking, we take a more radical approach: ease of garbage collection is more important than recovering memory on backtracking. Bekkers, Ridoux and Ungaro <ref> [12] </ref> observe that it is possible to reclaim garbage collected data on backtracking if copying collection starts at the oldest choice point (bottom-to-top). Their method has several differences to ours. * Their algorithm does not preserve the heap order, which means prim itives such as @&lt;/2 will work incorrectly.
Reference: 13. <author> J. Bevemyr, </author> <title> A Recursion Parallel Prolog Engine, </title> <institution> Licentiate of Philos ophy Thesis, Uppsala Theses in Computer Science 16/93, </institution> <year> 1993. </year> <month> f13g </month>
Reference-contexts: Bindings to variables shared between loop iterations may never be conditional [84]. 3. When a loop iteration is finished, it is always deterministic. 4. A (parallel) loop iteration can not start a parallel loop. Further details, e.g., on handling predicates with side-effects, can be found in Refs. <ref> [13, 72] </ref>. By making the first restriction, we ensure that parallel and sequential execution yield the same answer.
Reference: 14. <author> J. Bevemyr, </author> <title> A scheme for executing nested recursion parallelism, </title> <booktitle> in JICSLP'96 Post-Conference Workshop on Implementation Techniques, </booktitle> <month> September </month> <year> 1996. </year> <month> f15g </month>
Reference-contexts: Examples of such systems are &-Prolog [57] and ACE [52]. Results have generally been encouraging. Recently, Bevemyr <ref> [14] </ref> and the author [73] have proposed methods to extend the `flat' Reform Prolog system into efficiently executing arbitrarily nested recursion-parallel loops. 1.3 RELATED WORK Paper A There has been considerable interest in transformations of Prolog into Scheme and partial continuation passing styles and the relation of Prolog to intermediate representations,
Reference: 15. <author> P. Bigot, D. Gudeman, S.K. Debray, </author> <title> Output value placement in moded logic programs, </title> <booktitle> in Logic Programming: Proceedings of the Eleventh International Conference, </booktitle> <publisher> MIT Press, </publisher> <year> 1994. </year> <month> f18g </month>
Reference-contexts: No indication of the number of uninitialized arguments derived was given. The recently developed strongly-typed, strongly-moded logic programming language Mercury [110] restricts programs so that outputs are always unini-tialized. A recent release performs a mode analysis similar to the one proposed in Paper C. Bigot, Gudeman and Debray <ref> [15] </ref> have developed an alternative to Van Roy's algorithm, which they use to decide which output arguments should be returned in registers, and which should be returned in memory.
Reference: 16. <author> K. De Bosschere, S.K. Debray, D. Gudeman, S. Kannan, </author> <title> Call forward ing: A simple interprocedural optimization technique for dynamically typed languages, </title> <booktitle> in Proc. Principles of Programming Languages, </booktitle> <publisher> ACM Press, </publisher> <year> 1994. </year> <month> f18g </month>
Reference-contexts: It may be interesting to consider this for our benchmark set, and to possibly use multiple versions of a predicate for different call sites. The Bigot-Gudeman-Debray algorithm uses a single version per predicate. Gudeman, De Bosschere, Debray and Kannan <ref> [16] </ref> have defined call forwarding as a way to hoist type tests out of loops or in general to elide type tests when the call site can statically decide tests in the callee.
Reference: 17. <author> P. Brisset, O. Ridoux, </author> <title> Continuations in Prolog, </title> <booktitle> in Proceedings of the Tenth International Conference on Logic Programming, </booktitle> <editor> ed. D.S. Warren, </editor> <publisher> MIT Press, </publisher> <year> 1993. </year> <month> f16g </month>
Reference-contexts: Nilsson [87] shows how to derive the WAM choicepoint instructions by partial evaluation of a meta interpreter. Our work is distinct from the control aspects of the WAM and adds translations of other control constructs such as cut. Brisset and Ridoux <ref> [17] </ref> propose a CPS for Prolog. As that language has -abstractions, their translation is similar to Scheme or Lisp translations discussed below, but does not explicitly manage substitutions.
Reference: 18. <author> M. Carlsson, </author> <title> On Implementing Prolog in Functional Programming, </title> <booktitle> NGC 2 (1984), </booktitle> <pages> pp. 347-359. </pages> <month> f17g </month>
Reference-contexts: Translations into Scheme and Lisp Several translations of Prolog into Lisp have been proposed. We take the work of Kahn and Carlsson as representa 1.3. Related work 17 tive of this approach. Kahn and Carlsson <ref> [64, 18] </ref> propose the use of upward failure or downward success continuations. The former relies on using lazy streams to produce solutions, while the latter performs a procedure return on failure.
Reference: 19. <author> M. Carlsson, </author> <title> Design and Implementation of an Or-Parallel Prolog En gine, </title> <type> Ph.D. Thesis, </type> <institution> SICS-RITA/02, </institution> <year> 1990. </year> <month> f2g </month>
Reference-contexts: Warren's design has been the starting point of two major directions of research. The first of these is to improve on Warren's engine by local optimizations which target each clause or predicate in isolation <ref> [19] </ref>. An example of this is when clauses fail early. In this case, backtracking can be optimized [20, 78]. A second example is that unifications can be compiled into very efficient code [124, 76, 77].
Reference: 20. <author> M. Carlsson, </author> <title> On the efficiency of optimizing shallow backtracking in compiled Prolog, </title> <booktitle> in Proc. Sixth International Conference on Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1989. </year> <month> f2g </month>
Reference-contexts: The first of these is to improve on Warren's engine by local optimizations which target each clause or predicate in isolation [19]. An example of this is when clauses fail early. In this case, backtracking can be optimized <ref> [20, 78] </ref>. A second example is that unifications can be compiled into very efficient code [124, 76, 77]. As a third example, clause indexing can be improved by sophisticated source-to-source transformation or code generation [34, 22].
Reference: 21. <author> J.-H. Chang, </author> <title> High performance execution of Prolog programs based on a static dependency analysis, </title> <type> Ph.D. Thesis, </type> <institution> UCB/CSD 86/263, Univ. Calif. Berkeley, </institution> <year> 1986. </year> <note> f21, 24g </note>
Reference-contexts: In general, the performance is quite good when independent and-parallelism can be exploited. The overhead for parallel execution is mainly 1.3. Related work 21 related to process management and runtime tests for independence. An optimizing compiler can reduce the overhead of independence tests <ref> [83, 21] </ref>. Subsequent work combined independent and-parallelism with or-parallel [52] and dependent and-parallel [49] execution to extend the scope for parallel execution. <p> Get-zinger [47] found that a domain similar to Taylor's provided the best cost-benefit ratio for sequential compilation, out of a large collection of domains. 24 Summary The treatment of aliases in Reform Prolog is similar to that of Chang's SDDA <ref> [21] </ref>, in that aliases are treated as equivalence classes of possibly or certainly aliased variables. Subsequently, more powerful tracking of aliases has been proposed, e.g., by Muthukumar and Hermenegildo [83], Sundarara-jan [111], Jacobs and Langen [60] and the PROP domain [32, 126].
Reference: 22. <author> T. Chen, I.V. Ramakrishnan, R. Ramesh, </author> <title> Multistage indexing algo rithms, </title> <booktitle> in Proc. Joint International Conference & Symposium on Logic Programming'92, </booktitle> <publisher> MIT Press, </publisher> <year> 1992. </year> <note> f2g 27 </note>
Reference-contexts: In this case, backtracking can be optimized [20, 78]. A second example is that unifications can be compiled into very efficient code [124, 76, 77]. As a third example, clause indexing can be improved by sophisticated source-to-source transformation or code generation <ref> [34, 22] </ref>. The drawback of local techniques is that they cannot take advantage of the calling context. For example, it may be the case that a predicate is always 1.1. Introduction 3 called in a single mode.
Reference: 23. <author> C.J. </author> <title> Cheney, A nonrecursive list compacting algorithm, </title> <journal> Communica tions of the ACM, </journal> <volume> 13(11) </volume> <pages> 677-678, </pages> <month> November </month> <year> 1970. </year> <note> f9, 10g </note>
Reference-contexts: In particular, the paper shows that copying and generational copying garbage collection can be implemented straightforwardly and efficiently on standard hardware. The basic idea of the paper is to adapt an efficient collector, such as Cheney's algorithm <ref> [23] </ref>. The paper solves a number of problems that appear when adapting generational copying collection to Prolog. Interior pointers are common in Prolog. An interior pointer is one that points directly to a field in a structure, without referring to the head of the structure. <p> On backtracking, a Prolog implementation can reclaim the topmost segment, since all data therein belong to a failed branch of the proof tree. This has led to the general use of mark-sweep algorithms that carefully preserve the segments. However, Cheney-style 10 Summary copying <ref> [23] </ref> does not preserve the segment ordering of data. Instead, it traverses the live data in breadth-first order. There are three problems with not preserving the ordering of data on the heap. First, memory can not always be reclaimed on backtracking.
Reference: 24. <author> D.A. Clark, C.J. Rawlings, J. Shirazi, L-L. Li, K. Schuerman, M. Reeve, A. Veron, </author> <title> Solving large combinatorial problems in molecular biology using the ElipSys parallel constraint logic programming system, </title> <note> in Computer Journal 36(4). f3g </note>
Reference-contexts: When goals do not share free variables, the parallelism is said to be independent and-parallelism. Or-parallel systems have been successfully built for some years [2, 75] and have been applied to search problems. In recent years, or-parallel constraint solving has emerged as a potential application area <ref> [24, 122] </ref>. For and-parallel systems, the main problem is the management of parallel backtracking.
Reference: 25. <author> K.L. Clark, F. McCabe, </author> <title> The control facilities of IC-Prolog, in Expert Systems in the Micro-Electronic World (ed. </title> <editor> D. Michie), </editor> <publisher> Edinburgh University Press, </publisher> <year> 1979. </year> <month> f21g </month>
Reference-contexts: By requiring programs to be binding determinate, i.e., not undo any bindings during parallel execution, the binding conflict problem was once again reduced to global failure. Binding determinism was a major influence on the Reform Prolog execution model. Clark and McCabe introduced explicitly concurrent language constructs in IC-Prolog <ref> [25] </ref> and further developed by Clark and Gregory in their work on the Relational Language [26]. From this school of thought sprang three concurrent logic languages, Concurrent Prolog [103], Parlog [27] and Guarded Horn Clauses (GHC) [125].
Reference: 26. <author> K.L. Clark, S. Gregory, </author> <title> A relational language for parallel program ming, </title> <booktitle> in Proceedings ACM Symposium on Functional Programming and Computer Architecture, </booktitle> <year> 1981. </year> <month> f21g </month>
Reference-contexts: Binding determinism was a major influence on the Reform Prolog execution model. Clark and McCabe introduced explicitly concurrent language constructs in IC-Prolog [25] and further developed by Clark and Gregory in their work on the Relational Language <ref> [26] </ref>. From this school of thought sprang three concurrent logic languages, Concurrent Prolog [103], Parlog [27] and Guarded Horn Clauses (GHC) [125]. A restriction of the latter was chosen as the base language of the Japanese Fifth Generation Project.
Reference: 27. <author> K.L. Clark, S. Gregory, </author> <title> PARLOG: A parallel logic programming lan guage, </title> <type> report DOC 83/5, </type> <institution> Department of Computing, Imperial College, </institution> <address> London, </address> <year> 1983. </year> <title> f4, </title> <publisher> 21g </publisher>
Reference-contexts: There are several solutions to these problems. The first, and perhaps simplest, is to define a new language, possibly better suited to concurrent execution. This approach was taken for languages such as Concurrent Prolog [103], Parlog <ref> [27] </ref> and GHC [125], and was termed stream and-parallelism: processes are goals, and communicate by means of shared variables that implement, e.g., streams of messages. Tick [121] gives a good overview of the state of the art in this field. <p> Clark and McCabe introduced explicitly concurrent language constructs in IC-Prolog [25] and further developed by Clark and Gregory in their work on the Relational Language [26]. From this school of thought sprang three concurrent logic languages, Concurrent Prolog [103], Parlog <ref> [27] </ref> and Guarded Horn Clauses (GHC) [125]. A restriction of the latter was chosen as the base language of the Japanese Fifth Generation Project.
Reference: 28. <author> M. Codish, A. Mulkers, M. Bruynooghe, M. Garcia de la Banda, M. Hermenegildo, </author> <title> Improving abstract interpretations by combining domains, </title> <booktitle> in Proceedings of the 1993 Symposium on Partial Evaluation and Program Manipulation, </booktitle> <publisher> ACM Press, </publisher> <year> 1993. </year> <month> f24g </month>
Reference-contexts: The linearity domain of Reform Prolog keeps track of whether a term contains repeated occurrences of variables [62]. The approach taken by Reform Prolog is less sophisticated than other proposed domains <ref> [62, 112, 28, 61] </ref>, but appears to work well in practice.
Reference: 29. <author> J. Cohen, </author> <title> Garbage Collection of Linked Data Structure, </title> <journal> Computing Surveys, </journal> <volume> 13(3) </volume> <pages> 341-367, </pages> <month> September, </month> <year> 1981. </year> <month> f19g </month>
Reference-contexts: Paper D Prolog implementations such as SICStus Prolog use a mark-sweep algorithm that first marks the live data, then compacts the heap. We take the 1.3. Related work 19 implementation of Appleby et al. [5] as typical. This algorithm works in four steps and is based on the Deutsch-Schorr-Waite <ref> [99, 29] </ref> algorithm for marking and on Morris' algorithm [82, 29] for compaction (a more extensive summary is given in Paper D). Our copying collector uses a similar mark-phase, but copies data rather than compacting them. Touati and Hama [123] developed a generational copying garbage collector. <p> We take the 1.3. Related work 19 implementation of Appleby et al. [5] as typical. This algorithm works in four steps and is based on the Deutsch-Schorr-Waite [99, 29] algorithm for marking and on Morris' algorithm <ref> [82, 29] </ref> for compaction (a more extensive summary is given in Paper D). Our copying collector uses a similar mark-phase, but copies data rather than compacting them. Touati and Hama [123] developed a generational copying garbage collector. The heap is split into an old and a new generation.
Reference: 30. <author> A. Colmerauer, H. Kanoui, R. Pasero, P. Roussel, </author> <title> Un Systeme de Com munication Homme-Machine en Fran~cais, </title> <institution> Groupe de Recherche en Intelligence Artificielle, Univ. de Aix-Marseille, </institution> <address> Luminy, </address> <year> 1972. </year> <month> f1g </month>
Reference-contexts: 1.1 INTRODUCTION Prolog Prolog is a programming language based on a subset of predicate logic where programs consist of Horn clauses. Robinson [92] showed that resolution could be used to efficiently prove theorems for such theories; Colmerauer <ref> [30] </ref> and Kowalski [67] subsequently noted that such clauses could be viewed as programs in addition to logical theories. For example, consider a predicate specifying list concatenation such that the third argument is the concatenation of the first two arguments.
Reference: 31. <author> J.S. Conery, D.F. Kibler, </author> <title> Parallel interpretation of logic programs, </title> <booktitle> in Proceedings ACM Symposium on Functional Programming and Computer Architecture, </booktitle> <year> 1981. </year> <title> f4, </title> <publisher> 20g </publisher>
Reference-contexts: This is also called determinism-driven and-parallelism, and has been the basis of a number of research systems. A fourth approach is to record the dependences between processes dynamically, so that backtracking can be done properly. This approach is taken by Conery and Kibler <ref> [31] </ref>, and subsequently by Shen [104, 105, 106]. Dependence recording and dependence checking delegated entirely to runtime is too inefficient. <p> Papers E,F and G And-parallelism A parallel logic programming system exploiting dependent and-parallelism as well as or-parallelism was designed by Conery and Kibler <ref> [31] </ref>, though with impractical overheads. De Groot [40] and Hermenegildo [56] restricted parallel execution to independent and-parallelism, by performing runtime tests to determine whether a given conjunction is to be run in parallel or sequentially. When goals do not share variables, they can be executed in parallel.
Reference: 32. <author> A. Cortesi, G. File, W. Winsborough, </author> <title> Prop revisited: propositional formulas as abstract domain for groundness analysis, </title> <booktitle> in Proc. Sixth Annual IEEE Symposium on Logic in Computer Science, </booktitle> <publisher> IEEE Press, </publisher> <year> 1991. </year> <month> f24g </month>
Reference-contexts: Subsequently, more powerful tracking of aliases has been proposed, e.g., by Muthukumar and Hermenegildo [83], Sundarara-jan [111], Jacobs and Langen [60] and the PROP domain <ref> [32, 126] </ref>. These proposals have been used for accurate groundness information, however, while Reform Prolog uses aliases to keep track of freeness. The linearity domain of Reform Prolog keeps track of whether a term contains repeated occurrences of variables [62].
Reference: 33. <author> P. Cousot, R. Cousot, </author> <title> Abstract interpretation and application to logic programs, </title> <journal> Journal of Logic Programming, 1992:13:103-179. f18g </journal>
Reference-contexts: We finally note that the proposed transformation can be seen as an abstract interpretation <ref> [33] </ref> followed by a program transformation based on the derived results. Paper D Prolog implementations such as SICStus Prolog use a mark-sweep algorithm that first marks the live data, then compacts the heap. We take the 1.3. Related work 19 implementation of Appleby et al. [5] as typical.
Reference: 34. <author> S. Dawson, C.R. Ramakrishnan, I.V. Ramakrishnan, K. Sagonas, S. Skiena, T. Swift, D.S. Warren, </author> <title> Unification factoring for efficient execution of logic programs, </title> <booktitle> in Proc. Principles of Programming Languages, </booktitle> <publisher> MIT Press, </publisher> <year> 1996. </year> <note> f2g 28 </note>
Reference-contexts: In this case, backtracking can be optimized [20, 78]. A second example is that unifications can be compiled into very efficient code [124, 76, 77]. As a third example, clause indexing can be improved by sophisticated source-to-source transformation or code generation <ref> [34, 22] </ref>. The drawback of local techniques is that they cannot take advantage of the calling context. For example, it may be the case that a predicate is always 1.1. Introduction 3 called in a single mode.
Reference: 35. <author> S.K. Debray, </author> <title> Static inference of modes and data dependencies in logic programs, </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> Vol. 11, No. 3, </volume> <month> July </month> <year> 1989, </year> <pages> pp. 418-450. </pages> <month> f23g </month>
Reference-contexts: Sehr's system was, as far as we know, never implemented. Static analysis The dataflow analyzer of Reform Prolog was built by extending a framework proposed by Debray [37]. The abstract domain used is an extension of that of Debray and Warren <ref> [38, 35] </ref>. While their domain tracked may-aliases and modes, Reform Prolog tracks modes and list types (including some difference lists), may- and must-aliases, linearity, locality and determinism. Aquarius Prolog [127] used a simple mode analysis to improve sequential code.
Reference: 36. <author> S.K.Debray, </author> <title> A simple code improvement scheme for Prolog, </title> <journal> Journal of Logic Programming, 1992:13:57-88. f5, </journal> <volume> 16, </volume> <month> 17g </month>
Reference-contexts: In Prolog programs, the clause and goal selection rules are implicit. Paper A shows how to translate Prolog into a form where control is explicit, which simplifies the underlying implementation. Paper B shows how to derive a control flow graph from a Prolog program. Debray <ref> [36] </ref> has shown that the control flow graph is a useful tool for an optimizing compiler. Paper C proposes a robust static analysis to detect uninitialized arguments to predicates; this information can be used for important compiler optimizations. <p> Neumerkel [86, 85] has proposed Continuation Prolog, which allows compilers to manipulate continuations and remove some auxilliary output variables. The new program is then translated to binary or standard Prolog. The transformation is manual in nature, but could possibly be automated. Optimization of Prolog using control flow graphs Debray <ref> [36] </ref> proposes several optimizations based on Prolog predicates translated into control flow graphs with success and failure edges. <p> In particular, a generalized trail was required to suspend and resume a proof tree branch. Such operations are beyond the scope of this paper; all our failure continuations obey a stack-like discipline. Paper B Debray <ref> [36] </ref> and Sehr [101, 102] use control flow graphs to optimize sequential programs and extract parallelism, respectively. Both authors considered only intraprocedural control flow. In our formulation, procedure calls disappear in an interprocedural sea of assignments, continuation creation and primitive operations.
Reference: 37. <author> S.K. Debray, </author> <title> Efficient dataflow analysis of logic programs, </title> <journal> Journal of the ACM, </journal> <volume> Vol 39, No. 4, </volume> <month> October </month> <year> 1992. </year> <month> f23g </month>
Reference-contexts: Sehr's system was, as far as we know, never implemented. Static analysis The dataflow analyzer of Reform Prolog was built by extending a framework proposed by Debray <ref> [37] </ref>. The abstract domain used is an extension of that of Debray and Warren [38, 35]. While their domain tracked may-aliases and modes, Reform Prolog tracks modes and list types (including some difference lists), may- and must-aliases, linearity, locality and determinism.
Reference: 38. <author> S.K. Debray, D.S. Warren, </author> <title> Automatic mode inference for logic pro grams, </title> <journal> Journal of Logic Programming, </journal> <volume> Vol. 5, No. 3, </volume> <year> 1988. </year> <month> f23g </month>
Reference-contexts: Sehr's system was, as far as we know, never implemented. Static analysis The dataflow analyzer of Reform Prolog was built by extending a framework proposed by Debray [37]. The abstract domain used is an extension of that of Debray and Warren <ref> [38, 35] </ref>. While their domain tracked may-aliases and modes, Reform Prolog tracks modes and list types (including some difference lists), may- and must-aliases, linearity, locality and determinism. Aquarius Prolog [127] used a simple mode analysis to improve sequential code.
Reference: 39. <author> S.K. Debray, T. Proebsting, </author> <title> Inter-procedural control flow analysis of first order programs with tail call optimization, </title> <type> Draft, </type> <institution> University of Arizona, </institution> <month> May </month> <year> 1996. </year> <month> f17g </month>
Reference-contexts: Paper B Debray [36] and Sehr [101, 102] use control flow graphs to optimize sequential programs and extract parallelism, respectively. Both authors considered only intraprocedural control flow. In our formulation, procedure calls disappear in an interprocedural sea of assignments, continuation creation and primitive operations. Debray and Proebsting <ref> [39] </ref> have recently shown that control flow analyses can be transformed into parsing problems, and use LR (0) and LR (1)-items to perform the analysis. They consider languages with tail recursion, but lacking backtracking.
Reference: 40. <author> D. De Groot, </author> <title> Restricted AND-parallelism, </title> <booktitle> in Proceedings of the In ternational Conference on Fifth Generation Systems, </booktitle> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1984. </year> <title> f4, </title> <publisher> 20g </publisher>
Reference-contexts: Tick [121] gives a good overview of the state of the art in this field. An example of a recent implementation of such a language is the Monaco compiler [120]. A second approach, compatible with Prolog, is to extract independent and-parallelism <ref> [40, 56] </ref>. A failed process needs in this case only notify its siblings of the failure, which is substantially simpler than full dependence checking. A number of systems have been built to exploit independent and-parallelism [57, 52]. <p> Papers E,F and G And-parallelism A parallel logic programming system exploiting dependent and-parallelism as well as or-parallelism was designed by Conery and Kibler [31], though with impractical overheads. De Groot <ref> [40] </ref> and Hermenegildo [56] restricted parallel execution to independent and-parallelism, by performing runtime tests to determine whether a given conjunction is to be run in parallel or sequentially. When goals do not share variables, they can be executed in parallel.
Reference: 41. <author> B. Demoen, A. Marien, </author> <title> Implementation of Prolog as Binary Definite Programs, </title> <booktitle> Proceedings of the Second Russian Conference on Logic Programming, </booktitle> <publisher> Springer Verlag. f5, 16g </publisher>
Reference-contexts: The disadvantage with this approach is that the WAM is quite complex [132, 1] and thus difficult to implement, optimize and reason about. While most Prolog implementations use a Prolog-to-WAM compiler, an alternative approach, binarization, has been proposed by Tarau [115, 116, 117] and Demoen and Marien <ref> [42, 41] </ref>. The idea is to manage control by transforming general Prolog programs into a subset of Prolog that can be implemented more easily. When binarization is applied globally (i.e., for the entire program), the result is that every clause has a single goal. <p> This simplifies compilation but excludes some optimizations. Our representation directly admits a translation into control flow graphs, since control flow is explicit. Translations in Prolog Binarization, as proposed by Tarau [115] and Demoen and Marien <ref> [42, 41] </ref> ignores the failure continuation that relates the different clauses of a predicate.
Reference: 42. <author> B. Demoen, </author> <title> On the Transformation of a Prolog Program to a More Efficient Binary Program, </title> <booktitle> Proceedings of the LOPSTR'92 workshop, </booktitle> <address> Manchester, </address> <month> July </month> <year> 1992. </year> <note> f5, 16g </note>
Reference-contexts: The disadvantage with this approach is that the WAM is quite complex [132, 1] and thus difficult to implement, optimize and reason about. While most Prolog implementations use a Prolog-to-WAM compiler, an alternative approach, binarization, has been proposed by Tarau [115, 116, 117] and Demoen and Marien <ref> [42, 41] </ref>. The idea is to manage control by transforming general Prolog programs into a subset of Prolog that can be implemented more easily. When binarization is applied globally (i.e., for the entire program), the result is that every clause has a single goal. <p> This simplifies compilation but excludes some optimizations. Our representation directly admits a translation into control flow graphs, since control flow is explicit. Translations in Prolog Binarization, as proposed by Tarau [115] and Demoen and Marien <ref> [42, 41] </ref> ignores the failure continuation that relates the different clauses of a predicate.
Reference: 43. <author> B. Demoen, G. Engels, P. Tarau, </author> <title> Segment order preserving copying garbage collection for WAM based Prolog, </title> <booktitle> in ACM Symposium on Applied Computing, </booktitle> <publisher> ACM Press, </publisher> <year> 1995. </year> <month> f20g </month>
Reference-contexts: In contrast, our algorithm only supports partial reclamation of memory by backtracking. Our measurements indicate that this is sufficient: the copying algorithms we describe do not reclaim appreciably less 20 Summary memory on backtracking than the standard mark-sweep algorithm on the measured benchmarks. Subsequently, Demoen, Engels and Tarau <ref> [43] </ref> proposed and implemented an extension of the Bekkers-Ridoux-Ungaro bottom-up copying algorithm, combined with our mark-copy algorithm. They note that instant reclaiming of segments can be improved by moving data between segments.
Reference: 44. <author> P. Feautrier, </author> <title> Dataflow analysis of array and scalar references, </title> <journal> Interna tional Journal of Parallel Programming, </journal> <volume> Vol. 20, No. 1, </volume> <month> February </month> <year> 1991, </year> <note> Plenum Press f22g </note>
Reference-contexts: Vectorization translates program statements into vector instructions for supercomputers, and we will not consider it further. Concurrentization entails adding synchronization primitives to the loop body to ensure correct execution. Synchronization requirements are derived from dependence analysis, which determines what loads and stores can interfere. Recently, array dataflow analysis <ref> [44] </ref> has been developed to deal more precisely with this problem. Automatic loop-level parallelization of imperative programs has met some difficulties. For instance, Banerjee et al [6] note: "Dependence analysis in the presence of pointers has been found to be a particularly difficult problem. . . .
Reference: 45. <author> M. Felleisen, </author> <title> Transliterating Prolog into Scheme, </title> <institution> Computer Science Department Technical Report 182, Indiana University, </institution> <year> 1985. </year> <month> f17g </month>
Reference-contexts: Kahn and Carlsson [65] then employ partial evaluation to produce quite good Lisp code for naive reverse with modes. Research at Indiana University showed that Horn clauses could be translated into Scheme by fairly straightforward means <ref> [45, 66] </ref> and that some care was needed to extend Prolog with continuations [55]. In particular, a generalized trail was required to suspend and resume a proof tree branch. Such operations are beyond the scope of this paper; all our failure continuations obey a stack-like discipline.
Reference: 46. <author> T. Getzinger, </author> <title> Abstract Interpretation for the Compile-Time Analysis of Logic Programs, </title> <type> Ph.D. Thesis, Technical Report ACAL-TR-93-09, </type> <institution> University of South California, </institution> <month> September </month> <year> 1993. </year> <note> f8, 18g </note>
Reference-contexts: While some systems require the user to declare output arguments, Prolog compilers traditionally derive such information by global analysis. A simple and effective approach is taken by Van Roy [128] and Getzinger <ref> [46] </ref>. However, their analysis has the drawback of being monovariant : all calls to a predicate in the entire program are used to summarize which arguments are uninitialized. <p> Van Roy also developed an algorithm that could return approximately 1/3 of the outputs in registers. Getzinger improved on Van Roy's analysis and explored some alternatives, but remained within the monovariant framework <ref> [46, 47] </ref>. Taylor [118] subsequently incorporated uninitialized variables into his Parma compiler, and reported substantial performance gains. No indication of the number of uninitialized arguments derived was given. The recently developed strongly-typed, strongly-moded logic programming language Mercury [110] restricts programs so that outputs are always unini-tialized.
Reference: 47. <author> T. Getzinger, </author> <title> The Costs and Benefits of Abstract Interpretation Driven Prolog Optimization, </title> <booktitle> in Proc. First International Static Analysis Symposium, </booktitle> <publisher> LNCS 864, Springer Verlag, </publisher> <year> 1994. </year> <note> f18, 23g 29 </note>
Reference-contexts: Van Roy also developed an algorithm that could return approximately 1/3 of the outputs in registers. Getzinger improved on Van Roy's analysis and explored some alternatives, but remained within the monovariant framework <ref> [46, 47] </ref>. Taylor [118] subsequently incorporated uninitialized variables into his Parma compiler, and reported substantial performance gains. No indication of the number of uninitialized arguments derived was given. The recently developed strongly-typed, strongly-moded logic programming language Mercury [110] restricts programs so that outputs are always unini-tialized. <p> Taylor [118, 119] proposed a somewhat stronger domain, based on depth-k tracking of term structure, modes, constant types and recursive list types, but did not publish any precision results. Get-zinger <ref> [47] </ref> found that a domain similar to Taylor's provided the best cost-benefit ratio for sequential compilation, out of a large collection of domains. 24 Summary The treatment of aliases in Reform Prolog is similar to that of Chang's SDDA [21], in that aliases are treated as equivalence classes of possibly or
Reference: 48. <author> D. Gudeman, K. De Bosschere, S.K. Debray, </author> <title> jc: an efficient and portable sequential implementation of Janus, </title> <booktitle> in Logic Programming: Proceedings of the Joint International Conference and Symposium on Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1992. </year> <month> f8g </month>
Reference-contexts: An uninitialized variable need not be trailed or dereferenced when it is bound, nor need we initialize its memory location beforehand. Optimizing uninitialized variables can lead to considerable execution time savings, and is used by all high-performance logic programming implementations today <ref> [128, 118, 110, 48] </ref>. While some systems require the user to declare output arguments, Prolog compilers traditionally derive such information by global analysis. A simple and effective approach is taken by Van Roy [128] and Getzinger [46].
Reference: 49. <author> G. Gupta, V. Santos Costa, R. Yang, M.V. Hermenegildo, IDIOM: </author> <title> Intergrating Dependent and-, Independent and- and Or-parallelism, </title> <booktitle> in Logic Programming: Proceedings of the 1991 International Symposium, </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year> <note> f5, 21g </note>
Reference-contexts: A program may lack parallelism of a given form, which will yield poor performance on systems that extract only that form of parallelism. Some researchers 1.2. Summary 5 have set the goal to extract maximal parallelism from Prolog programs, by simultaneously exploiting multiple forms of parallelism <ref> [49, 52, 51, 50, 96] </ref>. 1.2 SUMMARY This section summarizes the papers in this thesis and discusses the scientific contributions, and my personal contributions. In Prolog programs, the clause and goal selection rules are implicit. <p> The overhead for parallel execution is mainly 1.3. Related work 21 related to process management and runtime tests for independence. An optimizing compiler can reduce the overhead of independence tests [83, 21]. Subsequent work combined independent and-parallelism with or-parallel [52] and dependent and-parallel <ref> [49] </ref> execution to extend the scope for parallel execution. The execution engines for the combination proposals are far more complex than a sequential Prolog engine, which is unfortunate if the goal is high absolute speedups with respect to a sequential system.
Reference: 50. <author> G. Gupta, B. Jayaraman, </author> <title> Compiled and-or parallel execution of logic programs, </title> <booktitle> in Proc. North American Conference on Logic Programming '89, </booktitle> <publisher> MIT Press, </publisher> <year> 1989. </year> <month> f5g </month>
Reference-contexts: A program may lack parallelism of a given form, which will yield poor performance on systems that extract only that form of parallelism. Some researchers 1.2. Summary 5 have set the goal to extract maximal parallelism from Prolog programs, by simultaneously exploiting multiple forms of parallelism <ref> [49, 52, 51, 50, 96] </ref>. 1.2 SUMMARY This section summarizes the papers in this thesis and discusses the scientific contributions, and my personal contributions. In Prolog programs, the clause and goal selection rules are implicit.
Reference: 51. <author> G. Gupta, B. Jayaraman, </author> <title> Optimizing and-or parallel implementations, </title> <booktitle> in Proc. North American Conference on Logic Programming '90, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year> <month> f5g </month>
Reference-contexts: A program may lack parallelism of a given form, which will yield poor performance on systems that extract only that form of parallelism. Some researchers 1.2. Summary 5 have set the goal to extract maximal parallelism from Prolog programs, by simultaneously exploiting multiple forms of parallelism <ref> [49, 52, 51, 50, 96] </ref>. 1.2 SUMMARY This section summarizes the papers in this thesis and discusses the scientific contributions, and my personal contributions. In Prolog programs, the clause and goal selection rules are implicit.
Reference: 52. <author> G. Gupta, </author> <title> M.V. Hermenegildo, ACE: And/Or-parallel copying-based execution of logic programs, in Parallel Execution of Logic Programs, </title> <publisher> LNCS 569, Springer Verlag, </publisher> <year> 1991. </year> <title> f4, </title> <type> 5, 15, </type> <institution> 21g </institution>
Reference-contexts: A second approach, compatible with Prolog, is to extract independent and-parallelism [40, 56]. A failed process needs in this case only notify its siblings of the failure, which is substantially simpler than full dependence checking. A number of systems have been built to exploit independent and-parallelism <ref> [57, 52] </ref>. Subsequently, the notion of independence has been refined so that processes may share variables but still are independent as long as they do not affect each other's control [58]. The third approach (again used with Prolog) is to only bind variables when the binding process is deterministic. <p> A program may lack parallelism of a given form, which will yield poor performance on systems that extract only that form of parallelism. Some researchers 1.2. Summary 5 have set the goal to extract maximal parallelism from Prolog programs, by simultaneously exploiting multiple forms of parallelism <ref> [49, 52, 51, 50, 96] </ref>. 1.2 SUMMARY This section summarizes the papers in this thesis and discusses the scientific contributions, and my personal contributions. In Prolog programs, the clause and goal selection rules are implicit. <p> Related work 15 Reflections and later work Our results inspired a number of subsequent papers [59, 90, 89] which incorporate some of the Reform Prolog techniques into control-parallel systems, in particular the `fast unfolding' of Reform compilation. Examples of such systems are &-Prolog [57] and ACE <ref> [52] </ref>. Results have generally been encouraging. <p> The overhead for parallel execution is mainly 1.3. Related work 21 related to process management and runtime tests for independence. An optimizing compiler can reduce the overhead of independence tests [83, 21]. Subsequent work combined independent and-parallelism with or-parallel <ref> [52] </ref> and dependent and-parallel [49] execution to extend the scope for parallel execution. The execution engines for the combination proposals are far more complex than a sequential Prolog engine, which is unfortunate if the goal is high absolute speedups with respect to a sequential system.
Reference: 53. <author> S. Haridi, </author> <title> A logic programming language based on the Andorra model, </title> <booktitle> New Generation Computing 7(1990), </booktitle> <pages> pp. 109-125, </pages> <publisher> Springer Verlag, </publisher> <year> 1990. </year> <month> f21g </month>
Reference-contexts: A consequence was that binding conflicts caused global failure rather than backtracking. (Certain proposed metaprimitives allow programmers to encapsulate a computation to recover from failure.) The Andorra Kernel Language <ref> [53] </ref> was proposed to join concurrent logic programming with Prolog's don't-know nondeterminism by splitting nondeterministic states into several deterministic computations. 22 Summary A different approach is to provide the programmer with powerful linguistic constructs to specify large, regular parallel computations.
Reference: 54. <author> W.L. Harrison III, </author> <title> The interprocedural analysis and parallelization of Scheme programs, </title> <journal> Lisp and Symbolic Computation, </journal> <volume> Vol. 2, no. 3/4, </volume> <year> 1989 </year> <month> f22g </month>
Reference-contexts: We believe this is in part due to the inability to parallelize loops with procedure calls. Reform Prolog allows procedure calls in parallel procedures and checks statically that they conform to the execution model. Harrison <ref> [54] </ref> developed a system that parallelizes recursive calls in Scheme. In particular, the implementation of recursion-parallelism is similar to ours, though presented in a more general context. Harrison performs side-effect and dependence analysis to restructure and parallelize Scheme programs. 1.3.
Reference: 55. <author> C. Haynes, </author> <title> Logic continuations, </title> <journal> Journal of Logic Programming, </journal> <volume> 4(2) </volume> <pages> 157-176, </pages> <month> June </month> <year> 1987. </year> <month> f17g </month>
Reference-contexts: Kahn and Carlsson [65] then employ partial evaluation to produce quite good Lisp code for naive reverse with modes. Research at Indiana University showed that Horn clauses could be translated into Scheme by fairly straightforward means [45, 66] and that some care was needed to extend Prolog with continuations <ref> [55] </ref>. In particular, a generalized trail was required to suspend and resume a proof tree branch. Such operations are beyond the scope of this paper; all our failure continuations obey a stack-like discipline.
Reference: 56. <author> M.V. Hermenegildo, </author> <title> An abstract machine for restricted AND-parallel execution of logic programs, </title> <booktitle> in Third International Conference on Logic Programming, </booktitle> <publisher> LNCS 225, Springer Verlag, </publisher> <year> 1986. </year> <title> f4, </title> <publisher> 20g </publisher>
Reference-contexts: Tick [121] gives a good overview of the state of the art in this field. An example of a recent implementation of such a language is the Monaco compiler [120]. A second approach, compatible with Prolog, is to extract independent and-parallelism <ref> [40, 56] </ref>. A failed process needs in this case only notify its siblings of the failure, which is substantially simpler than full dependence checking. A number of systems have been built to exploit independent and-parallelism [57, 52]. <p> Papers E,F and G And-parallelism A parallel logic programming system exploiting dependent and-parallelism as well as or-parallelism was designed by Conery and Kibler [31], though with impractical overheads. De Groot [40] and Hermenegildo <ref> [56] </ref> restricted parallel execution to independent and-parallelism, by performing runtime tests to determine whether a given conjunction is to be run in parallel or sequentially. When goals do not share variables, they can be executed in parallel. <p> When goals do not share variables, they can be executed in parallel. Backtracking can then be managed locally, possibly followed by killing all sibling goals when one goal in a parallel conjunction failed. Systems that rely solely on independent and-parallelism have been investigated by Hermenegildo <ref> [56, 57, 58] </ref> and others. In general, the performance is quite good when independent and-parallelism can be exploited. The overhead for parallel execution is mainly 1.3. Related work 21 related to process management and runtime tests for independence. An optimizing compiler can reduce the overhead of independence tests [83, 21].
Reference: 57. <author> M.V. Hermenegildo, K.J. Greene, </author> <title> &-Prolog and its performance: ex ploiting independent and-parallelism, </title> <booktitle> in Proceedings of the Seventh International Conference on Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year> <note> f4, 15, 20g </note>
Reference-contexts: A second approach, compatible with Prolog, is to extract independent and-parallelism [40, 56]. A failed process needs in this case only notify its siblings of the failure, which is substantially simpler than full dependence checking. A number of systems have been built to exploit independent and-parallelism <ref> [57, 52] </ref>. Subsequently, the notion of independence has been refined so that processes may share variables but still are independent as long as they do not affect each other's control [58]. The third approach (again used with Prolog) is to only bind variables when the binding process is deterministic. <p> Related work 15 Reflections and later work Our results inspired a number of subsequent papers [59, 90, 89] which incorporate some of the Reform Prolog techniques into control-parallel systems, in particular the `fast unfolding' of Reform compilation. Examples of such systems are &-Prolog <ref> [57] </ref> and ACE [52]. Results have generally been encouraging. <p> When goals do not share variables, they can be executed in parallel. Backtracking can then be managed locally, possibly followed by killing all sibling goals when one goal in a parallel conjunction failed. Systems that rely solely on independent and-parallelism have been investigated by Hermenegildo <ref> [56, 57, 58] </ref> and others. In general, the performance is quite good when independent and-parallelism can be exploited. The overhead for parallel execution is mainly 1.3. Related work 21 related to process management and runtime tests for independence. An optimizing compiler can reduce the overhead of independence tests [83, 21].
Reference: 58. <author> M.V. Hermenegildo, F. Rossi, </author> <title> Non-strict independent and-parallelism, </title> <booktitle> in Proceedings of the Seventh International Conference on Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year> <title> f4, </title> <publisher> 20g </publisher>
Reference-contexts: A number of systems have been built to exploit independent and-parallelism [57, 52]. Subsequently, the notion of independence has been refined so that processes may share variables but still are independent as long as they do not affect each other's control <ref> [58] </ref>. The third approach (again used with Prolog) is to only bind variables when the binding process is deterministic. No dependence checking is then needed, because the producer of the binding cannot backtrack to produce another binding [135, 133, 84]. <p> When goals do not share variables, they can be executed in parallel. Backtracking can then be managed locally, possibly followed by killing all sibling goals when one goal in a parallel conjunction failed. Systems that rely solely on independent and-parallelism have been investigated by Hermenegildo <ref> [56, 57, 58] </ref> and others. In general, the performance is quite good when independent and-parallelism can be exploited. The overhead for parallel execution is mainly 1.3. Related work 21 related to process management and runtime tests for independence. An optimizing compiler can reduce the overhead of independence tests [83, 21].
Reference: 59. <author> M. Hermenegildo, M. Carro, </author> <title> Relating data-parallelism and (And )parallelism in logic programs, </title> <journal> Computer Languages Journal, </journal> <note> to appear. f15g 30 </note>
Reference-contexts: The Reform Prolog compiler was written by me, while the execution engine was written by Bevemyr. The compiler optimizations were designed in collaboration with with Johan Bevemyr and implemented by me. 1.3. Related work 15 Reflections and later work Our results inspired a number of subsequent papers <ref> [59, 90, 89] </ref> which incorporate some of the Reform Prolog techniques into control-parallel systems, in particular the `fast unfolding' of Reform compilation. Examples of such systems are &-Prolog [57] and ACE [52]. Results have generally been encouraging.
Reference: 60. <author> D. Jacobs, A. Langen, </author> <title> Accurate and efficient approximation of variable aliasing in logic programs, </title> <booktitle> in Proc. North American Conference on Logic Programming 1989, </booktitle> <publisher> MIT Press, </publisher> <year> 1989. </year> <month> f24g </month>
Reference-contexts: Subsequently, more powerful tracking of aliases has been proposed, e.g., by Muthukumar and Hermenegildo [83], Sundarara-jan [111], Jacobs and Langen <ref> [60] </ref> and the PROP domain [32, 126]. These proposals have been used for accurate groundness information, however, while Reform Prolog uses aliases to keep track of freeness. The linearity domain of Reform Prolog keeps track of whether a term contains repeated occurrences of variables [62].
Reference: 61. <author> G. Janssens, M. Bruynooghe, </author> <title> Deriving descriptions of possible values of program variables by means of abstract interpretation, </title> <journal> Journal of Logic Programming 1992:13:205-258. f24g </journal>
Reference-contexts: The linearity domain of Reform Prolog keeps track of whether a term contains repeated occurrences of variables [62]. The approach taken by Reform Prolog is less sophisticated than other proposed domains <ref> [62, 112, 28, 61] </ref>, but appears to work well in practice.
Reference: 62. <author> N. Jones & H. Stndergaard, </author> <title> A semantics-based framework for the abstract interpretation of Prolog, </title> <type> report 86/14, </type> <institution> University of Copen-hagen, </institution> <year> 1986. </year> <month> f24g </month>
Reference-contexts: These proposals have been used for accurate groundness information, however, while Reform Prolog uses aliases to keep track of freeness. The linearity domain of Reform Prolog keeps track of whether a term contains repeated occurrences of variables <ref> [62] </ref>. The approach taken by Reform Prolog is less sophisticated than other proposed domains [62, 112, 28, 61], but appears to work well in practice. <p> The linearity domain of Reform Prolog keeps track of whether a term contains repeated occurrences of variables [62]. The approach taken by Reform Prolog is less sophisticated than other proposed domains <ref> [62, 112, 28, 61] </ref>, but appears to work well in practice.
Reference: 63. <author> N. Jones, C. Gomard, P. Sestoft, </author> <title> Partial Evaluation and Automatic Program Generation, </title> <publisher> Prentice-Hall, </publisher> <year> 1993. </year> <month> f17g </month>
Reference-contexts: Debray and Proebsting [39] have recently shown that control flow analyses can be transformed into parsing problems, and use LR (0) and LR (1)-items to perform the analysis. They consider languages with tail recursion, but lacking backtracking. Shivers [107] and Jones, Gomard and Sestoft <ref> [63] </ref> proposed control flow analysis for the purpose of recovering the control flow graph of higher-order functional programs. We have studied control flow analysis for a language with less general and somewhat different control structures, and have shown that the solution can be found quickly and represented compactly.
Reference: 64. <author> K. Kahn, M. Carlsson, </author> <title> How To Implement Prolog on a Lisp Machine, in Implementations of Prolog, </title> <editor> ed. J. Campbell, </editor> <publisher> Ellis Horwood, </publisher> <year> 1984. </year> <month> f17g </month>
Reference-contexts: Translations into Scheme and Lisp Several translations of Prolog into Lisp have been proposed. We take the work of Kahn and Carlsson as representa 1.3. Related work 17 tive of this approach. Kahn and Carlsson <ref> [64, 18] </ref> propose the use of upward failure or downward success continuations. The former relies on using lazy streams to produce solutions, while the latter performs a procedure return on failure.
Reference: 65. <author> K. Kahn, M. Carlsson, </author> <title> The Compilation of Prolog Programs without the Use of a Prolog Compiler, </title> <type> UPMAIL Technical Report 27, </type> <institution> Uppsala University, </institution> <year> 1984. </year> <month> f17g </month>
Reference-contexts: The former relies on using lazy streams to produce solutions, while the latter performs a procedure return on failure. We note that both methods to some extent bury the symmetry of success and failure continuations, and that neither method eliminates all control stacks. Kahn and Carlsson <ref> [65] </ref> then employ partial evaluation to produce quite good Lisp code for naive reverse with modes. Research at Indiana University showed that Horn clauses could be translated into Scheme by fairly straightforward means [45, 66] and that some care was needed to extend Prolog with continuations [55].
Reference: 66. <author> E. Kohlbecker, eu-Prolog, </author> <type> Technical Report 155, </type> <institution> Computer Science Department, Indiana University, </institution> <year> 1984. </year> <month> f17g </month>
Reference-contexts: Kahn and Carlsson [65] then employ partial evaluation to produce quite good Lisp code for naive reverse with modes. Research at Indiana University showed that Horn clauses could be translated into Scheme by fairly straightforward means <ref> [45, 66] </ref> and that some care was needed to extend Prolog with continuations [55]. In particular, a generalized trail was required to suspend and resume a proof tree branch. Such operations are beyond the scope of this paper; all our failure continuations obey a stack-like discipline.
Reference: 67. <author> R.A. Kowalski, </author> <title> Predicate logic as a computer language, </title> <booktitle> in Information Processing 74, </booktitle> <pages> pp. 569-574, </pages> <publisher> North-Holland, </publisher> <year> 1974. </year> <month> f1g </month>
Reference-contexts: 1.1 INTRODUCTION Prolog Prolog is a programming language based on a subset of predicate logic where programs consist of Horn clauses. Robinson [92] showed that resolution could be used to efficiently prove theorems for such theories; Colmerauer [30] and Kowalski <ref> [67] </ref> subsequently noted that such clauses could be viewed as programs in addition to logical theories. For example, consider a predicate specifying list concatenation such that the third argument is the concatenation of the first two arguments.
Reference: 68. <author> J.R. Larus, </author> <title> P.N. Hilfinger, Detecting conflicts between structure ac cesses, </title> <booktitle> in Proceedings of the SIGPLAN'88 Conference on Programming Language Design and Implementation, </booktitle> <publisher> ACM Press, </publisher> <year> 1988 </year> <month> f23g </month>
Reference-contexts: The Reform compiler handles general doacross loops. Harrison performs a thorough job of restructuring the computation, which could be a useful future extension to our system. Larus and Hilfinger describe an advanced parallelizing compiler for Lisp, Curare [69], that performs alias analysis prior to restructuring <ref> [68] </ref>. By computing the program dependence graph of the program, they can extract parallelism from the program. The alias analysis then serves as a data dependence analysis. Reform Prolog also computes an alias analysis, but extracts parallelism less freely.
Reference: 69. <author> J.R. Larus, </author> <title> P.N. Hilfinger, Restructuring Lisp programs for concur rent execution, </title> <booktitle> in Proceedings of the ACM/SIGPLAN PPEALS 1988 Parallel Programming: Experience with Applications, Languages and Systems, </booktitle> <publisher> ACM Press, </publisher> <year> 1988 </year> <month> f23g </month>
Reference-contexts: The Reform compiler handles general doacross loops. Harrison performs a thorough job of restructuring the computation, which could be a useful future extension to our system. Larus and Hilfinger describe an advanced parallelizing compiler for Lisp, Curare <ref> [69] </ref>, that performs alias analysis prior to restructuring [68]. By computing the program dependence graph of the program, they can extract parallelism from the program. The alias analysis then serves as a data dependence analysis. Reform Prolog also computes an alias analysis, but extracts parallelism less freely.
Reference: 70. <author> S. Le Huitouze, </author> <title> A new datastructure for implementing extensions to Prolog, </title> <booktitle> Proc. Programming Language Implementation and Logic Programming 1990, </booktitle> <publisher> LNCS 456, Springer Verlag, </publisher> <year> 1990. </year> <month> f19g </month>
Reference-contexts: We think our approach leads to better locality of reference. However, we have not found any published measurements of the efficiency of the Bekkers-Ridoux-Ungaro algorithm. * Variable shunting <ref> [70, 94] </ref> is used to avoid duplication of variables inside structures. This may introduce new variable chains, as shown in Paper D. We want to avoid this situation. Their algorithm does preserve the segment structure of the heap (but not the ordering within a segment).
Reference: 71. <author> H. Lieberman, C. Hewitt, </author> <title> A real-time garbage collector based on the lifetimes of objects, </title> <journal> Communications of the ACM, </journal> <volume> 26(6) </volume> <pages> 419-429, </pages> <month> June </month> <year> 1983. </year> <month> f10g </month>
Reference-contexts: That stack is then collected using a mark-sweep algorithm to retain the ordering between variables, while copying is used for the rest of the heap. Finally, the paper shows how to extend the copying algorithm to generational collection. Generational garbage collection <ref> [71, 3] </ref> relies on the observation that newly created objects tend to be short-lived. Thus, garbage collection should concentrate on recently created data. The heap is split into two or more generations, and the most recent generation is collected most frequently.
Reference: 72. <author> T. Lindgren, </author> <title> The compilation and execution of recursion-parallel Pro log on shared-memory multiprocessors, </title> <booktitle> Licentiate of Philosophy Thesis, Uppsala Theses in Computer Science 18/93, </booktitle> <month> November </month> <year> 1993. </year> <note> f13g 31 </note>
Reference-contexts: Bindings to variables shared between loop iterations may never be conditional [84]. 3. When a loop iteration is finished, it is always deterministic. 4. A (parallel) loop iteration can not start a parallel loop. Further details, e.g., on handling predicates with side-effects, can be found in Refs. <ref> [13, 72] </ref>. By making the first restriction, we ensure that parallel and sequential execution yield the same answer. <p> Finally, the analyzer also keeps track of whether loop iterations are deterministic or not, at every goal and at the end of an iteration. Code generation using this information is described in greater detail in the author's licentiate thesis <ref> [72] </ref>. Briefly, the compiler uses knowledge of locality, modes and determinism to optimize operations.
Reference: 73. <author> T. Lindgren, </author> <title> Compiling for nested recursion-parallelism, </title> <booktitle> in JICSLP'96 post-conference workshop on implementation, </booktitle> <month> September </month> <year> 1996. </year> <month> f15g </month>
Reference-contexts: Examples of such systems are &-Prolog [57] and ACE [52]. Results have generally been encouraging. Recently, Bevemyr [14] and the author <ref> [73] </ref> have proposed methods to extend the `flat' Reform Prolog system into efficiently executing arbitrarily nested recursion-parallel loops. 1.3 RELATED WORK Paper A There has been considerable interest in transformations of Prolog into Scheme and partial continuation passing styles and the relation of Prolog to intermediate representations, such as Warren's abstract
Reference: 74. <author> J. Lloyd, </author> <booktitle> Foundations of Logic Programming (2nd ed.), </booktitle> <publisher> Springer Ver lag, f2g </publisher>
Reference-contexts: The use of such a strategy separates Prolog from pure theorem proving. Many good books on the basics of Prolog and logic programming have been published, such as Sterling and Shapiro's textbook [114] on Prolog programming, or Lloyd's text on logic programming theory <ref> [74] </ref>. The novice reader is directed to those publications; we will in the rest of this thesis assume a working knowledge of the concepts and programming methods of Prolog and logic programming, such as resolution, unification, backtracking, cut, clause indexing, higher order predicates, and so on.
Reference: 75. <author> E. Lusk, D.H.D. Warren, S. Haridi, P. Brand, R. Butler, A. Calder wood, M. Carlsson, A. Ciepielewski, T. Disz, B. Hausman, R. Olson, R. Overbeek, R. Stevens, P. </author> <title> Szeredi, The Aurora or-parallel system, </title> <journal> New Generation Computing, </journal> <volume> vol 7(2-3), </volume> <year> 1990. </year> <month> f3g </month>
Reference-contexts: If several goals are resolved simultaneously, the system extracts and-parallelism. When the resolved goals share free variables, the parallelism is classified as dependent and-parallelism. When goals do not share free variables, the parallelism is said to be independent and-parallelism. Or-parallel systems have been successfully built for some years <ref> [2, 75] </ref> and have been applied to search problems. In recent years, or-parallel constraint solving has emerged as a potential application area [24, 122]. For and-parallel systems, the main problem is the management of parallel backtracking.
Reference: 76. <author> A. Marien, B. Demoen, </author> <title> A new scheme for unification in WAM, </title> <booktitle> in International Logic Programming Symposium 1991, </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year> <month> f2g </month>
Reference-contexts: An example of this is when clauses fail early. In this case, backtracking can be optimized [20, 78]. A second example is that unifications can be compiled into very efficient code <ref> [124, 76, 77] </ref>. As a third example, clause indexing can be improved by sophisticated source-to-source transformation or code generation [34, 22]. The drawback of local techniques is that they cannot take advantage of the calling context. For example, it may be the case that a predicate is always 1.1.
Reference: 77. <author> M. Meier, </author> <title> Compilation of compound terms in Prolog, </title> <type> technical report ECRC-95-12, </type> <institution> ECRC, </institution> <month> July </month> <year> 1990. </year> <month> f2g </month>
Reference-contexts: An example of this is when clauses fail early. In this case, backtracking can be optimized [20, 78]. A second example is that unifications can be compiled into very efficient code <ref> [124, 76, 77] </ref>. As a third example, clause indexing can be improved by sophisticated source-to-source transformation or code generation [34, 22]. The drawback of local techniques is that they cannot take advantage of the calling context. For example, it may be the case that a predicate is always 1.1.
Reference: 78. <author> M. Meier, </author> <title> Shallow backtracking in Prolog programs, </title> <type> technical report ECRC-95-11, </type> <institution> ECRC, </institution> <month> February </month> <year> 1987. </year> <month> f2g </month>
Reference-contexts: The first of these is to improve on Warren's engine by local optimizations which target each clause or predicate in isolation [19]. An example of this is when clauses fail early. In this case, backtracking can be optimized <ref> [20, 78] </ref>. A second example is that unifications can be compiled into very efficient code [124, 76, 77]. As a third example, clause indexing can be improved by sophisticated source-to-source transformation or code generation [34, 22].
Reference: 79. <author> H. Millroth, </author> <title> Reforming the compilation of logic programs, </title> <type> Ph.D. </type> <institution> The sis, Uppsala Theses in Computer Science 10, </institution> <year> 1991. </year> <month> f11g </month>
Reference-contexts: Bevemyr wrote the actual collector code, collected the benchmarking data and integrated the three collectors with Reform Prolog. Reform Prolog Papers E and F introduced Reform Prolog, a language built on Millroth's principle of recursion parallelism <ref> [79] </ref>. Paper G evaluates compiler optimizations that reduce the number of synchronizations and locking operations needed during parallel execution. <p> Paper G evaluates compiler optimizations that reduce the number of synchronizations and locking operations needed during parallel execution. Consider the list recursive predicate p. p ([]; : : :) p ([XjXs]; : : :) ^ p (Xs; : : :) ^ Millroth <ref> [79, 81] </ref> showed that such predicates could be compiled into efficient loop code, using Reform compilation. 1. Compute the length of the input list as n and build the n instances of and . 2. For i ranging from 1 to n, execute i . 3.
Reference: 80. <author> H. Millroth, </author> <title> Reform compilation for non-linear recursion, </title> <booktitle> Proceedings of the International Conference on Logic Programming and Automated Reasoning, </booktitle> <publisher> LNCS 624, Springer Verlag, </publisher> <year> 1992. </year> <month> f11g </month>
Reference-contexts: For i ranging from 1 to n, execute i . 3. Execute the remaining recursive call to p. 4. For i ranging from n to 1, execute i . Millroth also showed that nonlinear recursion could be compiled into loop code <ref> [80] </ref>; this is beyond the scope of our work. The Reform Prolog project was started in September 1990 to study the par-allelization of Prolog programs by means of Reform compilation. Originally, 12 Summary the project members were H-akan Millroth, Margus Veanes and the author.
Reference: 81. <author> H. Millroth, SLDR-resolution: </author> <title> parallelizing structural recursion in logic programs, </title> <journal> Journal of Logic Programming, </journal> <volume> Vol 25(2), </volume> <month> Nov. </month> <year> 1995, </year> <pages> pp. 93-117. </pages> <address> f4, 11g </address>
Reference-contexts: The single-program, multiple-data (SPMD) paradigm, where each process is a parallel loop iteration, exploits data-parallelism and is arguably the most popular programming model for conventional multiprocessors. In logic programming, data-parallelism has been used in the contexts of or-parallelism [109, 122] and and-parallelism <ref> [8, 81, 9, 130] </ref>. The sixth and final approach combines the previous methods. A program may lack parallelism of a given form, which will yield poor performance on systems that extract only that form of parallelism. Some researchers 1.2. <p> Paper G evaluates compiler optimizations that reduce the number of synchronizations and locking operations needed during parallel execution. Consider the list recursive predicate p. p ([]; : : :) p ([XjXs]; : : :) ^ p (Xs; : : :) ^ Millroth <ref> [79, 81] </ref> showed that such predicates could be compiled into efficient loop code, using Reform compilation. 1. Compute the length of the input list as n and build the n instances of and . 2. For i ranging from 1 to n, execute i . 3.
Reference: 82. <author> F. Morris, </author> <title> A Time- and Space- Efficient Compaction Algorithm, </title> <journal> Com munications of the ACM, </journal> <volume> 12(9) </volume> <pages> 662-665, </pages> <month> August </month> <year> 1978. </year> <month> f19g </month>
Reference-contexts: We take the 1.3. Related work 19 implementation of Appleby et al. [5] as typical. This algorithm works in four steps and is based on the Deutsch-Schorr-Waite [99, 29] algorithm for marking and on Morris' algorithm <ref> [82, 29] </ref> for compaction (a more extensive summary is given in Paper D). Our copying collector uses a similar mark-phase, but copies data rather than compacting them. Touati and Hama [123] developed a generational copying garbage collector. The heap is split into an old and a new generation.
Reference: 83. <author> K. Muthukumar, </author> <title> M.V. Hermenegildo, Compile-time derivation of vari able dependency using abstract interpretation, </title> <journal> Journal of Logic Programming, 1992:13:315-347. f21, 24g </journal>
Reference-contexts: In general, the performance is quite good when independent and-parallelism can be exploited. The overhead for parallel execution is mainly 1.3. Related work 21 related to process management and runtime tests for independence. An optimizing compiler can reduce the overhead of independence tests <ref> [83, 21] </ref>. Subsequent work combined independent and-parallelism with or-parallel [52] and dependent and-parallel [49] execution to extend the scope for parallel execution. <p> Subsequently, more powerful tracking of aliases has been proposed, e.g., by Muthukumar and Hermenegildo <ref> [83] </ref>, Sundarara-jan [111], Jacobs and Langen [60] and the PROP domain [32, 126]. These proposals have been used for accurate groundness information, however, while Reform Prolog uses aliases to keep track of freeness.
Reference: 84. <author> L. Naish, </author> <title> Parallelizing NU-Prolog, </title> <booktitle> Logic Programming: Proceedings of the Fifth International Conference and Symposium, </booktitle> <publisher> MIT Press, </publisher> <year> 1988. </year> <title> f4, </title> <type> 13, 14, </type> <institution> 21g </institution>
Reference-contexts: The third approach (again used with Prolog) is to only bind variables when the binding process is deterministic. No dependence checking is then needed, because the producer of the binding cannot backtrack to produce another binding <ref> [135, 133, 84] </ref>. This is also called determinism-driven and-parallelism, and has been the basis of a number of research systems. A fourth approach is to record the dependences between processes dynamically, so that backtracking can be done properly. <p> Testing the value of X must delay until X is bound or the loop iteration is leftmost. 2. Bindings to variables shared between loop iterations may never be conditional <ref> [84] </ref>. 3. When a loop iteration is finished, it is always deterministic. 4. A (parallel) loop iteration can not start a parallel loop. Further details, e.g., on handling predicates with side-effects, can be found in Refs. [13, 72]. <p> and G contribute a number of insights to the design and implementation of and-parallel logic programming systems. * Data-parallelism is a rich source of parallel work even for logic pro grams, and recursion-parallelism is an appropriate tool to exploit such work. * Reform Prolog confirms Naish's insight that binding determinism <ref> [84] </ref> is a crucial implementation concept for dependent and-parallel systems. * Static analysis can accurately separate shared data from local data, which reduces the need for locking instructions (e.g., reduce the dynamic number of lockings to 52% of the unoptimized total). * Static analysis can accurately identify the data accesses that <p> The resulting engine executes at about 25% the speed of SICStus Prolog, a standard Prolog implementation, and provides almost transparent parallelization of Prolog programs. A different approach to binding conflicts was proposed by Naish <ref> [84] </ref> with PNU-Prolog. By requiring programs to be binding determinate, i.e., not undo any bindings during parallel execution, the binding conflict problem was once again reduced to global failure. Binding determinism was a major influence on the Reform Prolog execution model.
Reference: 85. <author> U. Neumerkel, </author> <title> A transformation based on the equality between terms, in Logic Programming Synthesis and Transformation, </title> <publisher> LOPSTR'93, Springer Verlag, </publisher> <year> 1993. </year> <month> f16g </month>
Reference-contexts: Since they generate higher-order terms in their translation, a subsequent pass of closure conversion [4] converts the term to a first-order representation. Our algorithm, in contrast, directly generates first-order terms as continuations. Neumerkel <ref> [86, 85] </ref> has proposed Continuation Prolog, which allows compilers to manipulate continuations and remove some auxilliary output variables. The new program is then translated to binary or standard Prolog. The transformation is manual in nature, but could possibly be automated.
Reference: 86. <author> U. Neumerkel, </author> <title> Continuation Prolog: A new intermediary language for WAM and BinWAM code generation, </title> <booktitle> in Post-ILPS'95 Workshop on Implementation of Logic Programming Languages. </booktitle> <address> f16g 32 </address>
Reference-contexts: Since they generate higher-order terms in their translation, a subsequent pass of closure conversion [4] converts the term to a first-order representation. Our algorithm, in contrast, directly generates first-order terms as continuations. Neumerkel <ref> [86, 85] </ref> has proposed Continuation Prolog, which allows compilers to manipulate continuations and remove some auxilliary output variables. The new program is then translated to binary or standard Prolog. The transformation is manual in nature, but could possibly be automated.
Reference: 87. <author> U. Nilsson, </author> <title> Towards a methodology for the design of abstract ma chines for logic programming languages, </title> <journal> Journal of Logic Programming, 1993:16:163-189. f16g </journal>
Reference-contexts: Our binary continuation style avoids these problems and also allows us to directly describe indexing as a source-to-source transformation, which binarization delegates to the abstract machine. Nilsson <ref> [87] </ref> shows how to derive the WAM choicepoint instructions by partial evaluation of a meta interpreter. Our work is distinct from the control aspects of the WAM and adds translations of other control constructs such as cut. Brisset and Ridoux [17] propose a CPS for Prolog.
Reference: 88. <author> W.J. Older and J.A. Rummell, </author> <title> An Incremental Garbage Collector for WAM-Based Prolog, </title> <booktitle> Proceedings of the Joint International Conference and Symposium on Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1992. </year> <month> f19g </month>
Reference-contexts: For the older generation they use a mark-sweep algorithm. The technique is similar to that described by Barklund and Millroth [10] and later by Older and Rummell <ref> [88] </ref>. We show in Paper D how a simpler copying collector can be implemented, how the troublesome primitives can be accomodated better and how generational collection can be done in a simple and intuitive way.
Reference: 89. <author> E. Pontelli, G. Gupta, </author> <title> Data Parallel Logic Programming in &ACE, </title> <booktitle> Proc. of the IEEE International Symposium on Parallel and Distributed Processing, </booktitle> <publisher> IEEE Press, </publisher> <year> 1995. </year> <month> f15g </month>
Reference-contexts: The Reform Prolog compiler was written by me, while the execution engine was written by Bevemyr. The compiler optimizations were designed in collaboration with with Johan Bevemyr and implemented by me. 1.3. Related work 15 Reflections and later work Our results inspired a number of subsequent papers <ref> [59, 90, 89] </ref> which incorporate some of the Reform Prolog techniques into control-parallel systems, in particular the `fast unfolding' of Reform compilation. Examples of such systems are &-Prolog [57] and ACE [52]. Results have generally been encouraging.
Reference: 90. <author> E. Pontelli, G. Gupta, </author> <title> Determinacy Driven Optimization of And Parallel Prolog Implementations, </title> <booktitle> Int. Conf. Logic Programming, </booktitle> <publisher> MIT Press, </publisher> <year> 1995. </year> <month> f15g </month>
Reference-contexts: The Reform Prolog compiler was written by me, while the execution engine was written by Bevemyr. The compiler optimizations were designed in collaboration with with Johan Bevemyr and implemented by me. 1.3. Related work 15 Reflections and later work Our results inspired a number of subsequent papers <ref> [59, 90, 89] </ref> which incorporate some of the Reform Prolog techniques into control-parallel systems, in particular the `fast unfolding' of Reform compilation. Examples of such systems are &-Prolog [57] and ACE [52]. Results have generally been encouraging.
Reference: 91. <author> G. Puebla, M. Hermenegildo, </author> <title> Implementation of multiple specializa tion in logic programs, in Proc. Partial Evaluation and Program Manipulation, </title> <publisher> ACM Press, </publisher> <year> 1995. </year> <month> f18g </month>
Reference-contexts: We are only aware of two implementations of multiple specialization for Prolog programs: Sahlin's partial evaluator Mixtus [95] can generate multiple versions of a predicate, and Puebla and Hermenegildo <ref> [91] </ref> have recently used multiple specialization to improve the parallelization of &-Prolog. We finally note that the proposed transformation can be seen as an abstract interpretation [33] followed by a program transformation based on the derived results.
Reference: 92. <author> J. A. Robinson, </author> <title> A machine-oriented logic based on the resolution prin ciple, </title> <journal> Journal of the ACM, </journal> <volume> 12(1) </volume> <pages> 23-41, </pages> <year> 1965. </year> <month> f1g </month>
Reference-contexts: 1.1 INTRODUCTION Prolog Prolog is a programming language based on a subset of predicate logic where programs consist of Horn clauses. Robinson <ref> [92] </ref> showed that resolution could be used to efficiently prove theorems for such theories; Colmerauer [30] and Kowalski [67] subsequently noted that such clauses could be viewed as programs in addition to logical theories.
Reference: 93. <author> D. Sahlin, </author> <title> Making garbage collection independent of the amount of garbage, </title> <institution> Research Report R87008, Swedish Institute of Computer Science, </institution> <year> 1987. </year> <month> f20g </month>
Reference-contexts: Demoen et al do not directly compare their algorithm's efficiency with ours. However, their mark-copy algorithm appears to be approximately as efficient as ours. They do not present measurements for generational mark-copy. Sahlin <ref> [93] </ref> has developed a method that makes the execution time of the Appleby et al. [5] algorithm proportional to the size of the live data. The main drawback of Sahlin's algorithm is that implementing the mark-sweep algorithm becomes more difficult. To our knowledge it has never been implemented.
Reference: 94. <author> D. Sahlin, M. Carlsson, </author> <title> Variable shunting for the WAM, </title> <institution> SICS research report R91:07, SICS, </institution> <year> 1991. </year> <month> f19g </month>
Reference-contexts: We think our approach leads to better locality of reference. However, we have not found any published measurements of the efficiency of the Bekkers-Ridoux-Ungaro algorithm. * Variable shunting <ref> [70, 94] </ref> is used to avoid duplication of variables inside structures. This may introduce new variable chains, as shown in Paper D. We want to avoid this situation. Their algorithm does preserve the segment structure of the heap (but not the ordering within a segment).
Reference: 95. <author> D. Sahlin, </author> <title> The Mixtus approach to automatic partial evaluation of full Prolog, </title> <booktitle> in Proc. North American Conference on Logic Program-ming'90, </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year> <month> f18g </month>
Reference-contexts: Van Roy's compiler [127] merges multiple calls that have produced the same intermediate code, which can be seen as the reverse of call forwarding. We are only aware of two implementations of multiple specialization for Prolog programs: Sahlin's partial evaluator Mixtus <ref> [95] </ref> can generate multiple versions of a predicate, and Puebla and Hermenegildo [91] have recently used multiple specialization to improve the parallelization of &-Prolog. We finally note that the proposed transformation can be seen as an abstract interpretation [33] followed by a program transformation based on the derived results.
Reference: 96. <author> V. Santos Costa, D.H.D. Warren, R. Yang, Andorra-I: </author> <title> A parallel Pro log system that transparently exploits both and- and or-parallelism, </title> <booktitle> in Third ACM SIGPLAN Symposium on Principles & Practices of Parallel Programming, </booktitle> <publisher> ACM Press, </publisher> <year> 1991. </year> <note> f5, 21g </note>
Reference-contexts: A program may lack parallelism of a given form, which will yield poor performance on systems that extract only that form of parallelism. Some researchers 1.2. Summary 5 have set the goal to extract maximal parallelism from Prolog programs, by simultaneously exploiting multiple forms of parallelism <ref> [49, 52, 51, 50, 96] </ref>. 1.2 SUMMARY This section summarizes the papers in this thesis and discusses the scientific contributions, and my personal contributions. In Prolog programs, the clause and goal selection rules are implicit. <p> Determinism is detected at runtime. Yang's static method was thus replaced with a dynamic method of detecting and exploiting parallelism. This approach is also called determinism-driven parallel execution. An implementation of the Andorra principle, Andorra-I, is described in several papers <ref> [96, 97, 98, 136] </ref>. Shen [106, 104] developed a method that allows general dependent and-parallelism, by maintaining enough information to restart processes that have consumed bindings that are then invalid, and by restricting access to shared variables.
Reference: 97. <author> V. Santos Costa, D.H.D. Warren, R. Yang, </author> <title> The Andorra-I preproces sor: supporting full Prolog on the basic Andorra model, </title> <booktitle> in Logic Programming: Proceedings of the Eighth International Conference, </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year> <month> f21g </month>
Reference-contexts: Determinism is detected at runtime. Yang's static method was thus replaced with a dynamic method of detecting and exploiting parallelism. This approach is also called determinism-driven parallel execution. An implementation of the Andorra principle, Andorra-I, is described in several papers <ref> [96, 97, 98, 136] </ref>. Shen [106, 104] developed a method that allows general dependent and-parallelism, by maintaining enough information to restart processes that have consumed bindings that are then invalid, and by restricting access to shared variables.
Reference: 98. <author> V. Santos Costa, D.H.D. Warren, R. Yang, </author> <title> The Andorra-I engine: a parallel implementation of the basic Andorra model, </title> <booktitle> in Logic Programming: Proceedings of the Eighth International Conference, </booktitle> <publisher> MIT Press, </publisher> <year> 1991. </year> <note> f21g 33 </note>
Reference-contexts: Determinism is detected at runtime. Yang's static method was thus replaced with a dynamic method of detecting and exploiting parallelism. This approach is also called determinism-driven parallel execution. An implementation of the Andorra principle, Andorra-I, is described in several papers <ref> [96, 97, 98, 136] </ref>. Shen [106, 104] developed a method that allows general dependent and-parallelism, by maintaining enough information to restart processes that have consumed bindings that are then invalid, and by restricting access to shared variables.

References-found: 98

