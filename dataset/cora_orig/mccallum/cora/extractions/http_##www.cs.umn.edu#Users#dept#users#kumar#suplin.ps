URL: http://www.cs.umn.edu/Users/dept/users/kumar/suplin.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: On the Efficiency of Parallel Backtracking  
Author: V. Nageshwara Rao and Vipin Kumar 
Date: January 1992  
Address: Orlando, Florida 32816  Minneapolis, MN 55455  
Affiliation: Department of Computer Sciences, University of Central Florida,  Computer Science Department University of Minnesota,  
Abstract: It is known that isolated executions of parallel backtrack search exhibit speedup anomalies. In this paper we present analytical models and experimental results on the average case behavior of parallel backtracking. We consider two types of backtrack search algorithms: (i) simple backtracking (which does not use any heuristic information); (ii) heuristic backtracking (which uses heuristics to order and prune search). We present analytical models to compare the average number of nodes visited in sequential and parallel search for each case. For simple backtracking, we show that the average speedup obtained is (i) linear when distribution of solutions is uniform and (ii) super-linear when distribution of solutions is non-uniform. For heuristic backtracking, the average speedup obtained is at least linear (i.e., either linear or superlinear), and the speedup obtained on a subset of instances (called difficult instances) is superlinear. We also present experimental results over many synthetic and practical problems on various parallel machines, that validate our theoretical analysis. fl This work was supported by Army Research Office grant # DAAG29-84-K-0060 to the Artificial Intelligence Laboratory, University of Texas at Austin and by Army Research Office grant # 28408-MA-SDI to the University of Minnesota and by the Army High Performance Computing Research Center at the University of Minnesota This paper will also appear in IEEE Transaction on Parallel and Distributed Systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Arvindam, Vipin Kumar, V. Nageshwara Rao, and Vineet Singh. </author> <title> Automatic test pattern generation on multiprocessors. </title> <journal> Parallel Computing, </journal> <volume> 17, number 12 </volume> <pages> 1323-1342, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: We implemented sequential and parallel versions of PODEM on a 128 processor Symult 2010 multiprocessor. We performed an experiment using the ISCAS-85 benchmark files as test data. More details on our implementation and experimental results can be found in <ref> [1] </ref>. Our experiments were conducted as follows. The HTD faults were first filtered out by picking those faults from the seven files whose test patterns could not be found within 25 backtracks using the sequential algorithm.
Reference: [2] <author> Raphael A. Finkel and Udi Manber. </author> <title> DIB a distributed implementation of backtracking. </title> <journal> ACM Trans. of Progr. Lang. and Systems, </journal> <volume> 9 No. 2 </volume> <pages> 235-256, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: In this paper we deal with two important ones : (i) simple backtracking (which does not use any heuristic information); (ii) heuristic backtracking (which uses ordering and/or pruning heuristics to reduce search complexity). A number of parallel formulations of DFS have been developed by various researchers <ref> [12, 7, 22, 2, 25, 23] </ref>. In one such formulation [23], N processors concurrently perform backtracking in disjoint parts of a state-space tree. The parts of the state-space searched by different processors are roughly of equal sizes. <p> It may also use heuristics to prune nodes of the search space so that search can be avoided under these nodes. This method is also referred to as ordered DFS [13]. 3 Parallel DFS There are many different parallel formulations of DFS <ref> [7, 15, 19, 34, 2, 8, 23] </ref> that are suitable for execution on asynchronous MIMD multiprocessors. The formulation discussed here is used quite commonly [22, 23, 2, 4, 14]. In this formulation, each processor searches a disjoint part of the search space. <p> This method is also referred to as ordered DFS [13]. 3 Parallel DFS There are many different parallel formulations of DFS [7, 15, 19, 34, 2, 8, 23] that are suitable for execution on asynchronous MIMD multiprocessors. The formulation discussed here is used quite commonly <ref> [22, 23, 2, 4, 14] </ref>. In this formulation, each processor searches a disjoint part of the search space. Whenever a processor completely searches its assigned part, it requests a busy processor for work. <p> One cannot guarantee the optimality of a solution unless an exhaustive search is performed. Saletore and Kale [30] present a parallel formulation of DFS which is quite different than the ones in <ref> [22, 23, 2] </ref>. Their formulation explicitly ensures that the number of nodes searched by sequential and parallel formulations are nearly equal. The results of our paper do not apply to their parallel DFS.
Reference: [3] <author> Prabhakar Goel. </author> <title> An implicit enumeration algorithm to generate tests for combinatorial logic circuits. </title> <journal> IEEE Transactions on Computers, C-30,No. </journal> <volume> 3 </volume> <pages> 215-222, </pages> <year> 1981. </year>
Reference-contexts: The theoretical analysis is validated by experimental 2 analysis on example problems such as the problem of generating test-patterns for digital circuits <ref> [3] </ref>, N queens, 15-puzzle [26], and the hackers problem [31]. The result that "parallel backtrack search gives at least a linear speedup on the average" is important since DFS is currently the best known and practically useful algorithm to solve a number of important problems. <p> An input pattern is said to be a test for a given fault if, in the presence of the fault, it produces an output that is different for the faulty and fault-free circuits. We studied sequential and parallel implementations of an algorithm called PODEM (Path-Oriented Decision Making 16 <ref> [3] </ref>) used for combinational circuits (and for sequential circuits based on the level-sensitive scan design approach). This is one of the most successful algorithms for the problem and it is widely used. The number of faults possible in a circuit is proportional to the number of signal lines in it.
Reference: [4] <author> Ananth Grama, Vipin Kumar, and V. Nageshwara Rao. </author> <title> Experimental evaluation of load balancing techniques for the hypercube. </title> <booktitle> In Proceedings of the Parallel Computing 91 Conference, </booktitle> <year> 1991. </year>
Reference-contexts: Search overhead is caused because sequential and parallel DFS search the nodes in a different order. Communication overhead is dependent upon the target architecture and the load balancing technique. The communication overhead in parallel DFS was analyzed in our previously published papers <ref> [17, 16, 4, 14] </ref>, and was experimentally validated on a variety of problems and architectures. 1 In this paper, we only analyze search overhead. However, in the experiments, which were run only on real multiprocessors, both overheads are incurred. <p> This method is also referred to as ordered DFS [13]. 3 Parallel DFS There are many different parallel formulations of DFS [7, 15, 19, 34, 2, 8, 23] that are suitable for execution on asynchronous MIMD multiprocessors. The formulation discussed here is used quite commonly <ref> [22, 23, 2, 4, 14] </ref>. In this formulation, each processor searches a disjoint part of the search space. Whenever a processor completely searches its assigned part, it requests a busy processor for work.
Reference: [5] <author> D. P. Helmbold and C. E. McDowell. </author> <title> Modeling speedup (n) greater than n. </title> <booktitle> In Proceedings of International conference on Parallel Processing, </booktitle> <pages> pages 8-12, </pages> <year> 1988. </year>
Reference-contexts: Their formulation explicitly ensures that the number of nodes searched by sequential and parallel formulations are nearly equal. The results of our paper do not apply to their parallel DFS. In <ref> [5] </ref>, a general model for explaining the occurrence of superlinear speedups in a variety of search problems is presented. It is shown that if the parallel algorithm performs less work than the corresponding sequential algorithm, superlinear speedup is possible.
Reference: [6] <author> Ellis Horowitz and Sartaj Sahni. </author> <title> Fundamentals of Computer Algorithms. </title> <publisher> Computer Science Press, </publisher> <address> Rockville, Maryland, </address> <year> 1978. </year>
Reference-contexts: 1 Introduction Consider the problem of finding a solution in a state-space tree containing one or more solutions <ref> [10, 28, 26, 6] </ref>. Backtracking, also called Depth-first Search, is a widely used technique for solving such problems because of its storage efficiency [13, 28]. Throughout the paper, we use the two names interchangeably. We use the acronym DFS to denote backtracking or depth-first search on state-space trees. <p> This characteristic happens to be true for a variety of problem spaces searched by simple backtracking. 4.3 Experimental Results We present experimental results of the performance of parallel DFS on three problems: (i) the hacker's problem [31] (ii) the 15-puzzle problem [26] and (iii) the N queens problem <ref> [6] </ref>. In all the experiments discussed in this section, both sequential and parallel DFS visit the newly generated successors of a node in a random order.
Reference: [7] <author> M. Imai, Y. Yoshida, and T. Fukumura. </author> <title> A parallel searching scheme for multiprocessor systems and its application to combinatorial problems. </title> <booktitle> In IJCAI, </booktitle> <pages> pages 416-418, </pages> <year> 1979. </year>
Reference-contexts: In this paper we deal with two important ones : (i) simple backtracking (which does not use any heuristic information); (ii) heuristic backtracking (which uses ordering and/or pruning heuristics to reduce search complexity). A number of parallel formulations of DFS have been developed by various researchers <ref> [12, 7, 22, 2, 25, 23] </ref>. In one such formulation [23], N processors concurrently perform backtracking in disjoint parts of a state-space tree. The parts of the state-space searched by different processors are roughly of equal sizes. <p> This type of behavior is common for a variety of parallel search algorithms, and is referred to as `speedup anomaly' [18, 19]. The superlinear speedup in isolated executions of parallel DFS has been reported by many researchers <ref> [12, 25, 22, 7, 33, 20] </ref>. It may appear that on the average the speedup would be either linear or sublinear; otherwise, even parallel DFS executed on sequential processor via time-slicing would perform better than sequential DFS. <p> It may also use heuristics to prune nodes of the search space so that search can be avoided under these nodes. This method is also referred to as ordered DFS [13]. 3 Parallel DFS There are many different parallel formulations of DFS <ref> [7, 15, 19, 34, 2, 8, 23] </ref> that are suitable for execution on asynchronous MIMD multiprocessors. The formulation discussed here is used quite commonly [22, 23, 2, 4, 14]. In this formulation, each processor searches a disjoint part of the search space. <p> What is the optimal combination? This paper analyzed efficiency of parallel DFS for certain models. It would be interesting to perform similar analysis for other models and also for other parallel formulations of backtrack search such as those given in <ref> [7] </ref> and in [30]. Acknowledgements: We would like to thank Sunil Arvindam and Hang He Ng for helping us with some of the experiments. We would also like to thank Dr. James C.
Reference: [8] <author> Virendra K. Janakiram, Dharma P. Agrawal, and Ram Mehrotra. </author> <title> Randomized parallel algorithms for prolog programs and backtracking applications. </title> <booktitle> In Proceedings of International conference on Parallel Processing, </booktitle> <pages> pages 278-281, </pages> <year> 1987. </year>
Reference-contexts: It may also use heuristics to prune nodes of the search space so that search can be avoided under these nodes. This method is also referred to as ordered DFS [13]. 3 Parallel DFS There are many different parallel formulations of DFS <ref> [7, 15, 19, 34, 2, 8, 23] </ref> that are suitable for execution on asynchronous MIMD multiprocessors. The formulation discussed here is used quite commonly [22, 23, 2, 4, 14]. In this formulation, each processor searches a disjoint part of the search space. <p> The width of the banded region is expected to reduce if a lot more repetitions (say a 1000 for every instance) were tried. The instances with non-uniform distribution of solutions give superlinear speedups. The problem is naturally known to exhibit non uniformity in solution density <ref> [8] </ref>. Each data point shown was obtained by averaging over 100 trials. As we can see, parallel DFS exhibits better efficiency than sequential DFS. <p> Superlinearity for hard-to-detect faults was experimentally observed for other ATPG heuristics by Patil and Banerjee in [27]. 6 Related Research The occurrence of speedup anomalies in simple backtracking was studied in [22] and <ref> [8] </ref>. Monien, et. al.[22] studied a parallel formulation of DFS for solving the satisfiability problem. In this formulation, each processor tries to prove the satisfiability of a different subformula of the input formula. <p> One very simple parallel formulation of DFS presented in <ref> [21, 8] </ref> is to let the same search space be searched by many processors in an independent random order until one of the processors finds a solution. The total number of nodes expanded by a processor in this formulation is again a random variable (let's call it T (N)). <p> Clearly, T (N) = minfV 1 ,...,V N g, where each V i is a random variable. If the average value of T (N) is less than 1 N times T (1), then also we can expect superlinear speedup <ref> [21, 8] </ref>. For certain distributions of T (1), this happens to be the case. For example, if the probability of finding a solution at any level of the state-space tree is the same, then T (1) has this property [21]. <p> For certain distributions of T (1), this happens to be the case. For example, if the probability of finding a solution at any level of the state-space tree is the same, then T (1) has this property [21]. Note that our parallel formulation of DFS dominates the one in <ref> [21, 8] </ref> in terms of efficiency, as in our parallel formulation there is no duplication of work. Hence, our parallel formulation will exhibit superlinear speedup on any search space for which the formulation in [21, 8] exhibits superlinear speedup, but the converse is not true. <p> Note that our parallel formulation of DFS dominates the one in <ref> [21, 8] </ref> in terms of efficiency, as in our parallel formulation there is no duplication of work. Hence, our parallel formulation will exhibit superlinear speedup on any search space for which the formulation in [21, 8] exhibits superlinear speedup, but the converse is not true. For certain problems, probabilistic algorithms [29] 5 can perform substantially better than simple backtracking. <p> For ordered DFS, randomized parallel DFS algorithms such as those given in <ref> [21, 8] </ref> will perform poorly, they are not able to benefit from the ordering heuristics. Probabilistic algorithms have the same weakness. <p> In the case of optimization problems (i.e., when we are interested in finding a least-cost solution), randomized parallel DFS algorithms in <ref> [21, 8] </ref> as well as probabilistic algorithms are not useful. One cannot guarantee the optimality of a solution unless an exhaustive search is performed. Saletore and Kale [30] present a parallel formulation of DFS which is quite different than the ones in [22, 23, 2].
Reference: [9] <author> L. V. Kale. </author> <title> A perfect heuristic for the n non-attacking queens problem. </title> <journal> Information Processing Letters, </journal> <volume> 34 </volume> <pages> 173-178, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: On the other hand, in problems such as N queens, the ordering information improves the performance of DFS substantially <ref> [9, 32] </ref>. 8 The experiments for 15-puzzle were performed on the BBN Butterfly parallel processor for up to 9 processors. The experiments involved instances of the 15-puzzle with uniform distribution and non-uniform distribution of solutions. Depth bounded DFS was used to limit the search space for each instance.
Reference: [10] <editor> Laveen Kanal and Vipin Kumar. </editor> <booktitle> Search in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction Consider the problem of finding a solution in a state-space tree containing one or more solutions <ref> [10, 28, 26, 6] </ref>. Backtracking, also called Depth-first Search, is a widely used technique for solving such problems because of its storage efficiency [13, 28]. Throughout the paper, we use the two names interchangeably. We use the acronym DFS to denote backtracking or depth-first search on state-space trees.
Reference: [11] <author> Richard Korf. </author> <type> Personal Communication. </type> <year> 1988. </year>
Reference-contexts: For example, experience with the IDA* algorithm on the 15-puzzle problem <ref> [11] </ref> indicates that 7 experiment is repeated many times. Note that, besides the random ordering of successors, there is another source of variability in execution time of parallel DFS.
Reference: [12] <author> W. Kornfeld. </author> <title> The use of parallelism to implement a heuristic search. </title> <booktitle> In IJCAI, </booktitle> <pages> pages 575-580, </pages> <year> 1981. </year>
Reference-contexts: In this paper we deal with two important ones : (i) simple backtracking (which does not use any heuristic information); (ii) heuristic backtracking (which uses ordering and/or pruning heuristics to reduce search complexity). A number of parallel formulations of DFS have been developed by various researchers <ref> [12, 7, 22, 2, 25, 23] </ref>. In one such formulation [23], N processors concurrently perform backtracking in disjoint parts of a state-space tree. The parts of the state-space searched by different processors are roughly of equal sizes. <p> This type of behavior is common for a variety of parallel search algorithms, and is referred to as `speedup anomaly' [18, 19]. The superlinear speedup in isolated executions of parallel DFS has been reported by many researchers <ref> [12, 25, 22, 7, 33, 20] </ref>. It may appear that on the average the speedup would be either linear or sublinear; otherwise, even parallel DFS executed on sequential processor via time-slicing would perform better than sequential DFS.
Reference: [13] <author> Vipin Kumar. </author> <title> Depth-first search. </title> <editor> In Stuart C. Shapiro, editor, </editor> <booktitle> Encyclopaedia of Artifi--cial Intelligence: </booktitle> <volume> Vol 2, </volume> <pages> pages 1004-1005. </pages> <publisher> John Wiley and Sons, Inc., </publisher> <address> New York, </address> <year> 1987. </year> <note> Revised version appears in the second edition of the encyclopedia to be published in 1992. </note>
Reference-contexts: 1 Introduction Consider the problem of finding a solution in a state-space tree containing one or more solutions [10, 28, 26, 6]. Backtracking, also called Depth-first Search, is a widely used technique for solving such problems because of its storage efficiency <ref> [13, 28] </ref>. Throughout the paper, we use the two names interchangeably. We use the acronym DFS to denote backtracking or depth-first search on state-space trees. There are many variants of DFS algorithms, each of which is tuned to certain types of problems. <p> It may use heuristics for ordering the successors of an expanded node. It may also use heuristics to prune nodes of the search space so that search can be avoided under these nodes. This method is also referred to as ordered DFS <ref> [13] </ref>. 3 Parallel DFS There are many different parallel formulations of DFS [7, 15, 19, 34, 2, 8, 23] that are suitable for execution on asynchronous MIMD multiprocessors. The formulation discussed here is used quite commonly [22, 23, 2, 4, 14].
Reference: [14] <author> Vipin Kumar, Ananth Grama, and V. Nageshwara Rao. </author> <title> Scalable load balancing techniques for parallel computers. </title> <type> Technical report, Tech Report 91-55, </type> <institution> Computer Science Department, University of Minnesota, </institution> <year> 1991. </year>
Reference-contexts: Search overhead is caused because sequential and parallel DFS search the nodes in a different order. Communication overhead is dependent upon the target architecture and the load balancing technique. The communication overhead in parallel DFS was analyzed in our previously published papers <ref> [17, 16, 4, 14] </ref>, and was experimentally validated on a variety of problems and architectures. 1 In this paper, we only analyze search overhead. However, in the experiments, which were run only on real multiprocessors, both overheads are incurred. <p> This method is also referred to as ordered DFS [13]. 3 Parallel DFS There are many different parallel formulations of DFS [7, 15, 19, 34, 2, 8, 23] that are suitable for execution on asynchronous MIMD multiprocessors. The formulation discussed here is used quite commonly <ref> [22, 23, 2, 4, 14] </ref>. In this formulation, each processor searches a disjoint part of the search space. Whenever a processor completely searches its assigned part, it requests a busy processor for work.
Reference: [15] <editor> Vipin Kumar and Laveen Kanal. </editor> <title> Parallel branch-and-bound formulations for and/or tree search. </title> <journal> IEEE Trans. Pattern. Anal. and Machine Intell., </journal> <volume> PAMI-6:768-778, </volume> <year> 1984. </year>
Reference-contexts: It may also use heuristics to prune nodes of the search space so that search can be avoided under these nodes. This method is also referred to as ordered DFS [13]. 3 Parallel DFS There are many different parallel formulations of DFS <ref> [7, 15, 19, 34, 2, 8, 23] </ref> that are suitable for execution on asynchronous MIMD multiprocessors. The formulation discussed here is used quite commonly [22, 23, 2, 4, 14]. In this formulation, each processor searches a disjoint part of the search space.
Reference: [16] <author> Vipin Kumar and V. N. Rao. </author> <title> Scalable parallel formulations of depth-first search. </title> <editor> In Vipin Kumar, P. S. Gopalakrishnan, and Laveen Kanal, editors, </editor> <booktitle> Parallel Algorithms for Machine Intelligence and Vision. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Search overhead is caused because sequential and parallel DFS search the nodes in a different order. Communication overhead is dependent upon the target architecture and the load balancing technique. The communication overhead in parallel DFS was analyzed in our previously published papers <ref> [17, 16, 4, 14] </ref>, and was experimentally validated on a variety of problems and architectures. 1 In this paper, we only analyze search overhead. However, in the experiments, which were run only on real multiprocessors, both overheads are incurred. <p> From all these results, it is clear that superlinearity increases with the increasing hardness of instances. The degree of superlinearity decreases with the increasing number of processors because the efficiency of parallel DFS decreases if the problem size is fixed and the number of processors is increased <ref> [16] </ref>. Note that the above experimental results validate only the discussion in Section 5.2.1. To validate Theorem 5.3, it would be necessary to find the number of nodes expanded by parallel DFS even for easy to detect faults.
Reference: [17] <author> Vipin Kumar and V. Nageshwara Rao. </author> <title> Parallel depth-first search, part II: Analysis. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16 </volume> (6):501-519, 1987. 
Reference-contexts: This paper considers the average case speedup anomalies in parallel DFS algorithms that are based on the techniques developed in <ref> [23, 17] </ref>. Though simple backtracking and heuristic backtracking algorithms we analyze here use a DFS strategy, their behavior is very different, and they are analyzed separately. We develop abstract models for the search spaces that are traversed by these two types of DFS algorithms. <p> Search overhead is caused because sequential and parallel DFS search the nodes in a different order. Communication overhead is dependent upon the target architecture and the load balancing technique. The communication overhead in parallel DFS was analyzed in our previously published papers <ref> [17, 16, 4, 14] </ref>, and was experimentally validated on a variety of problems and architectures. 1 In this paper, we only analyze search overhead. However, in the experiments, which were run only on real multiprocessors, both overheads are incurred.
Reference: [18] <author> T. H. Lai and Sartaj Sahni. </author> <title> Anomalies in parallel branch and bound algorithms. </title> <journal> Communications of the ACM, </journal> <pages> pages 594-602, </pages> <year> 1984. </year>
Reference-contexts: This type of behavior is common for a variety of parallel search algorithms, and is referred to as `speedup anomaly' <ref> [18, 19] </ref>. The superlinear speedup in isolated executions of parallel DFS has been reported by many researchers [12, 25, 22, 7, 33, 20].
Reference: [19] <author> Guo-Jie Li and Benjamin W. Wah. </author> <title> Computational efficiency of parallel approximate branch-and-bound algorithms. </title> <booktitle> In International Conf on Paralle Processing, </booktitle> <pages> pages 473-480, </pages> <year> 1984. </year>
Reference-contexts: This type of behavior is common for a variety of parallel search algorithms, and is referred to as `speedup anomaly' <ref> [18, 19] </ref>. The superlinear speedup in isolated executions of parallel DFS has been reported by many researchers [12, 25, 22, 7, 33, 20]. <p> It may also use heuristics to prune nodes of the search space so that search can be avoided under these nodes. This method is also referred to as ordered DFS [13]. 3 Parallel DFS There are many different parallel formulations of DFS <ref> [7, 15, 19, 34, 2, 8, 23] </ref> that are suitable for execution on asynchronous MIMD multiprocessors. The formulation discussed here is used quite commonly [22, 23, 2, 4, 14]. In this formulation, each processor searches a disjoint part of the search space.
Reference: [20] <author> Kai Li. Ivy: </author> <title> A shared virtual memory system for parallel computing. </title> <booktitle> In Proceedings of International conference on Parallel Processing: </booktitle> <volume> Vol II, </volume> <pages> pages 94-101, </pages> <year> 1988. </year>
Reference-contexts: This type of behavior is common for a variety of parallel search algorithms, and is referred to as `speedup anomaly' [18, 19]. The superlinear speedup in isolated executions of parallel DFS has been reported by many researchers <ref> [12, 25, 22, 7, 33, 20] </ref>. It may appear that on the average the speedup would be either linear or sublinear; otherwise, even parallel DFS executed on sequential processor via time-slicing would perform better than sequential DFS.
Reference: [21] <author> R. Mehrotra and E. Gehringer. </author> <title> Superlinear speedup through randomized algorithms. </title> <booktitle> In Proceedings of International conference on Parallel Processing, </booktitle> <pages> pages 291-300, </pages> <year> 1985. </year>
Reference-contexts: One very simple parallel formulation of DFS presented in <ref> [21, 8] </ref> is to let the same search space be searched by many processors in an independent random order until one of the processors finds a solution. The total number of nodes expanded by a processor in this formulation is again a random variable (let's call it T (N)). <p> Clearly, T (N) = minfV 1 ,...,V N g, where each V i is a random variable. If the average value of T (N) is less than 1 N times T (1), then also we can expect superlinear speedup <ref> [21, 8] </ref>. For certain distributions of T (1), this happens to be the case. For example, if the probability of finding a solution at any level of the state-space tree is the same, then T (1) has this property [21]. <p> For certain distributions of T (1), this happens to be the case. For example, if the probability of finding a solution at any level of the state-space tree is the same, then T (1) has this property <ref> [21] </ref>. Note that our parallel formulation of DFS dominates the one in [21, 8] in terms of efficiency, as in our parallel formulation there is no duplication of work. <p> For certain distributions of T (1), this happens to be the case. For example, if the probability of finding a solution at any level of the state-space tree is the same, then T (1) has this property [21]. Note that our parallel formulation of DFS dominates the one in <ref> [21, 8] </ref> in terms of efficiency, as in our parallel formulation there is no duplication of work. Hence, our parallel formulation will exhibit superlinear speedup on any search space for which the formulation in [21, 8] exhibits superlinear speedup, but the converse is not true. <p> Note that our parallel formulation of DFS dominates the one in <ref> [21, 8] </ref> in terms of efficiency, as in our parallel formulation there is no duplication of work. Hence, our parallel formulation will exhibit superlinear speedup on any search space for which the formulation in [21, 8] exhibits superlinear speedup, but the converse is not true. For certain problems, probabilistic algorithms [29] 5 can perform substantially better than simple backtracking. <p> For ordered DFS, randomized parallel DFS algorithms such as those given in <ref> [21, 8] </ref> will perform poorly, they are not able to benefit from the ordering heuristics. Probabilistic algorithms have the same weakness. <p> In the case of optimization problems (i.e., when we are interested in finding a least-cost solution), randomized parallel DFS algorithms in <ref> [21, 8] </ref> as well as probabilistic algorithms are not useful. One cannot guarantee the optimality of a solution unless an exhaustive search is performed. Saletore and Kale [30] present a parallel formulation of DFS which is quite different than the ones in [22, 23, 2].
Reference: [22] <author> B. Monien, O. Vornberger, and E. Spekenmeyer. </author> <title> Superlinear speedup for parallel backtracking. </title> <type> Technical Report 30, </type> <institution> Univ. of Paderborn, </institution> <address> FRG, </address> <year> 1986. </year>
Reference-contexts: In this paper we deal with two important ones : (i) simple backtracking (which does not use any heuristic information); (ii) heuristic backtracking (which uses ordering and/or pruning heuristics to reduce search complexity). A number of parallel formulations of DFS have been developed by various researchers <ref> [12, 7, 22, 2, 25, 23] </ref>. In one such formulation [23], N processors concurrently perform backtracking in disjoint parts of a state-space tree. The parts of the state-space searched by different processors are roughly of equal sizes. <p> This type of behavior is common for a variety of parallel search algorithms, and is referred to as `speedup anomaly' [18, 19]. The superlinear speedup in isolated executions of parallel DFS has been reported by many researchers <ref> [12, 25, 22, 7, 33, 20] </ref>. It may appear that on the average the speedup would be either linear or sublinear; otherwise, even parallel DFS executed on sequential processor via time-slicing would perform better than sequential DFS. <p> This method is also referred to as ordered DFS [13]. 3 Parallel DFS There are many different parallel formulations of DFS [7, 15, 19, 34, 2, 8, 23] that are suitable for execution on asynchronous MIMD multiprocessors. The formulation discussed here is used quite commonly <ref> [22, 23, 2, 4, 14] </ref>. In this formulation, each processor searches a disjoint part of the search space. Whenever a processor completely searches its assigned part, it requests a busy processor for work. <p> Superlinearity for hard-to-detect faults was experimentally observed for other ATPG heuristics by Patil and Banerjee in [27]. 6 Related Research The occurrence of speedup anomalies in simple backtracking was studied in <ref> [22] </ref> and [8]. Monien, et. al.[22] studied a parallel formulation of DFS for solving the satisfiability problem. In this formulation, each processor tries to prove the satisfiability of a different subformula of the input formula. <p> Our analysis in Section 4 shows that any non-uniformity in solution densities among the regions searched by different processors leads to a superlinear speedup on the average. The other two types of heuristic DFS algorithms we discuss are outside the scope of both <ref> [22] </ref> and [24]. 19 . <p> One cannot guarantee the optimality of a solution unless an exhaustive search is performed. Saletore and Kale [30] present a parallel formulation of DFS which is quite different than the ones in <ref> [22, 23, 2] </ref>. Their formulation explicitly ensures that the number of nodes searched by sequential and parallel formulations are nearly equal. The results of our paper do not apply to their parallel DFS. <p> While isolated occurrences of speedup anomalies in parallel DFS had been reported earlier by various researchers, no experimental or analytical results showing possibility of superlinear speedup on the average (with the exception of results in <ref> [22, 24] </ref>) were available for parallel DFS. A number of questions need to be addressed by further research.
Reference: [23] <author> V. Nageshwara Rao and Vipin Kumar. </author> <title> Parallel depth-first search, part I: Implementation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16 </volume> (6):479-499, 1987. 
Reference-contexts: In this paper we deal with two important ones : (i) simple backtracking (which does not use any heuristic information); (ii) heuristic backtracking (which uses ordering and/or pruning heuristics to reduce search complexity). A number of parallel formulations of DFS have been developed by various researchers <ref> [12, 7, 22, 2, 25, 23] </ref>. In one such formulation [23], N processors concurrently perform backtracking in disjoint parts of a state-space tree. The parts of the state-space searched by different processors are roughly of equal sizes. <p> A number of parallel formulations of DFS have been developed by various researchers [12, 7, 22, 2, 25, 23]. In one such formulation <ref> [23] </ref>, N processors concurrently perform backtracking in disjoint parts of a state-space tree. The parts of the state-space searched by different processors are roughly of equal sizes. <p> This paper considers the average case speedup anomalies in parallel DFS algorithms that are based on the techniques developed in <ref> [23, 17] </ref>. Though simple backtracking and heuristic backtracking algorithms we analyze here use a DFS strategy, their behavior is very different, and they are analyzed separately. We develop abstract models for the search spaces that are traversed by these two types of DFS algorithms. <p> It may also use heuristics to prune nodes of the search space so that search can be avoided under these nodes. This method is also referred to as ordered DFS [13]. 3 Parallel DFS There are many different parallel formulations of DFS <ref> [7, 15, 19, 34, 2, 8, 23] </ref> that are suitable for execution on asynchronous MIMD multiprocessors. The formulation discussed here is used quite commonly [22, 23, 2, 4, 14]. In this formulation, each processor searches a disjoint part of the search space. <p> This method is also referred to as ordered DFS [13]. 3 Parallel DFS There are many different parallel formulations of DFS [7, 15, 19, 34, 2, 8, 23] that are suitable for execution on asynchronous MIMD multiprocessors. The formulation discussed here is used quite commonly <ref> [22, 23, 2, 4, 14] </ref>. In this formulation, each processor searches a disjoint part of the search space. Whenever a processor completely searches its assigned part, it requests a busy processor for work. <p> One cannot guarantee the optimality of a solution unless an exhaustive search is performed. Saletore and Kale [30] present a parallel formulation of DFS which is quite different than the ones in <ref> [22, 23, 2] </ref>. Their formulation explicitly ensures that the number of nodes searched by sequential and parallel formulations are nearly equal. The results of our paper do not apply to their parallel DFS.
Reference: [24] <author> V. Nageshwara Rao and Vipin Kumar. </author> <title> Superlinear speedup in state-space search. </title> <booktitle> In Proceedings of the 1988 Foundation of Software Technology and Theoretcal Computer Science, </booktitle> <month> December </month> <year> 1988. </year> <booktitle> Lecture Notes in Computer Science number 338, </booktitle> <publisher> Springer Verlag. </publisher> <pages> 28 </pages>
Reference-contexts: Our analysis of simple backtracking (in Section 4) is done for a similar model, but our results are general and stronger. We had also analyzed the average case behavior of parallel (simple) backtracking in <ref> [24] </ref>. The theoretical results we present here are much stronger than those in [24]. In [24], we showed that if the regions searched by a few of the processors had all the solutions uniformly distributed and the regions searched by all the rest of the processors had no solutions at all, <p> Our analysis of simple backtracking (in Section 4) is done for a similar model, but our results are general and stronger. We had also analyzed the average case behavior of parallel (simple) backtracking in <ref> [24] </ref>. The theoretical results we present here are much stronger than those in [24]. In [24], we showed that if the regions searched by a few of the processors had all the solutions uniformly distributed and the regions searched by all the rest of the processors had no solutions at all, the average speedup in parallel backtracking would be superlinear. <p> Our analysis of simple backtracking (in Section 4) is done for a similar model, but our results are general and stronger. We had also analyzed the average case behavior of parallel (simple) backtracking in <ref> [24] </ref>. The theoretical results we present here are much stronger than those in [24]. In [24], we showed that if the regions searched by a few of the processors had all the solutions uniformly distributed and the regions searched by all the rest of the processors had no solutions at all, the average speedup in parallel backtracking would be superlinear. <p> Our analysis in Section 4 shows that any non-uniformity in solution densities among the regions searched by different processors leads to a superlinear speedup on the average. The other two types of heuristic DFS algorithms we discuss are outside the scope of both [22] and <ref> [24] </ref>. 19 . <p> While isolated occurrences of speedup anomalies in parallel DFS had been reported earlier by various researchers, no experimental or analytical results showing possibility of superlinear speedup on the average (with the exception of results in <ref> [22, 24] </ref>) were available for parallel DFS. A number of questions need to be addressed by further research.
Reference: [25] <author> V. Nageshwara Rao, Vipin Kumar, and K. Ramesh. </author> <title> A parallel implementation of iterative-deepening-a*. </title> <booktitle> In Proceedings of the National Conf. on Artificial Intelligence (AAAI-87), </booktitle> <pages> pages 878-882, </pages> <year> 1987. </year>
Reference-contexts: In this paper we deal with two important ones : (i) simple backtracking (which does not use any heuristic information); (ii) heuristic backtracking (which uses ordering and/or pruning heuristics to reduce search complexity). A number of parallel formulations of DFS have been developed by various researchers <ref> [12, 7, 22, 2, 25, 23] </ref>. In one such formulation [23], N processors concurrently perform backtracking in disjoint parts of a state-space tree. The parts of the state-space searched by different processors are roughly of equal sizes. <p> This type of behavior is common for a variety of parallel search algorithms, and is referred to as `speedup anomaly' [18, 19]. The superlinear speedup in isolated executions of parallel DFS has been reported by many researchers <ref> [12, 25, 22, 7, 33, 20] </ref>. It may appear that on the average the speedup would be either linear or sublinear; otherwise, even parallel DFS executed on sequential processor via time-slicing would perform better than sequential DFS.
Reference: [26] <author> Nils J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Tioga Press, </publisher> <year> 1980. </year>
Reference-contexts: 1 Introduction Consider the problem of finding a solution in a state-space tree containing one or more solutions <ref> [10, 28, 26, 6] </ref>. Backtracking, also called Depth-first Search, is a widely used technique for solving such problems because of its storage efficiency [13, 28]. Throughout the paper, we use the two names interchangeably. We use the acronym DFS to denote backtracking or depth-first search on state-space trees. <p> The theoretical analysis is validated by experimental 2 analysis on example problems such as the problem of generating test-patterns for digital circuits [3], N queens, 15-puzzle <ref> [26] </ref>, and the hackers problem [31]. The result that "parallel backtrack search gives at least a linear speedup on the average" is important since DFS is currently the best known and practically useful algorithm to solve a number of important problems. <p> This characteristic happens to be true for a variety of problem spaces searched by simple backtracking. 4.3 Experimental Results We present experimental results of the performance of parallel DFS on three problems: (i) the hacker's problem [31] (ii) the 15-puzzle problem <ref> [26] </ref> and (iii) the N queens problem [6]. In all the experiments discussed in this section, both sequential and parallel DFS visit the newly generated successors of a node in a random order.
Reference: [27] <author> S. Patil and P. Banerjee. </author> <title> A parallel branch-and-bound algorithm for test generation. </title> <journal> IEEE Transactions on Computer Aided Design, </journal> <volume> 9-3:313-322, </volume> <year> 1990. </year>
Reference-contexts: As a result, the execution of the algorithm is terminated when it fails to generate a test after a predefined number of node expansions or backtracks. Those faults that cannot be solved in reasonable time by the serial algorithm are called hard-to-detect (HTD) faults <ref> [27] </ref>. In practice, it is very important to generate tests for as many faults as possible. Higher fault coverage results in more reliable chips. <p> For such faults, experimental run time will not be roughly proportional to the number of nodes searched by parallel DFS because for small trees communication overhead becomes significant. Superlinearity for hard-to-detect faults was experimentally observed for other ATPG heuristics by Patil and Banerjee in <ref> [27] </ref>. 6 Related Research The occurrence of speedup anomalies in simple backtracking was studied in [22] and [8]. Monien, et. al.[22] studied a parallel formulation of DFS for solving the satisfiability problem. In this formulation, each processor tries to prove the satisfiability of a different subformula of the input formula.
Reference: [28] <author> Judea Pearl. </author> <title> Heuristics Intelligent Search Strategies for Computer Problem Solving. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1984. </year>
Reference-contexts: 1 Introduction Consider the problem of finding a solution in a state-space tree containing one or more solutions <ref> [10, 28, 26, 6] </ref>. Backtracking, also called Depth-first Search, is a widely used technique for solving such problems because of its storage efficiency [13, 28]. Throughout the paper, we use the two names interchangeably. We use the acronym DFS to denote backtracking or depth-first search on state-space trees. <p> 1 Introduction Consider the problem of finding a solution in a state-space tree containing one or more solutions [10, 28, 26, 6]. Backtracking, also called Depth-first Search, is a widely used technique for solving such problems because of its storage efficiency <ref> [13, 28] </ref>. Throughout the paper, we use the two names interchangeably. We use the acronym DFS to denote backtracking or depth-first search on state-space trees. There are many variants of DFS algorithms, each of which is tuned to certain types of problems.
Reference: [29] <author> Michael O. Rabin. </author> <title> Probabilistic algorithms. </title> <editor> In J. Traub, editor, </editor> <booktitle> Algorithms and Complexity: New Directions and Results, </booktitle> <pages> pages 21-39. </pages> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1988. </year>
Reference-contexts: Hence, our parallel formulation will exhibit superlinear speedup on any search space for which the formulation in [21, 8] exhibits superlinear speedup, but the converse is not true. For certain problems, probabilistic algorithms <ref> [29] </ref> 5 can perform substantially better than simple backtracking.
Reference: [30] <author> Vikram Saletore and L. V. Kale. </author> <title> Consistent linear speedup to a first solution in parallel state-space search. </title> <booktitle> In Proceedings of the 1990 National Conference on Artificial Intelligence, </booktitle> <pages> pages 227-233, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: In the case of optimization problems (i.e., when we are interested in finding a least-cost solution), randomized parallel DFS algorithms in [21, 8] as well as probabilistic algorithms are not useful. One cannot guarantee the optimality of a solution unless an exhaustive search is performed. Saletore and Kale <ref> [30] </ref> present a parallel formulation of DFS which is quite different than the ones in [22, 23, 2]. Their formulation explicitly ensures that the number of nodes searched by sequential and parallel formulations are nearly equal. The results of our paper do not apply to their parallel DFS. <p> What is the optimal combination? This paper analyzed efficiency of parallel DFS for certain models. It would be interesting to perform similar analysis for other models and also for other parallel formulations of backtrack search such as those given in [7] and in <ref> [30] </ref>. Acknowledgements: We would like to thank Sunil Arvindam and Hang He Ng for helping us with some of the experiments. We would also like to thank Dr. James C. Browne and Dr Vineet Singh for many helpful discussions. 23 APPENDICES A Details of Analysis for Ordered DFS Algorithms.
Reference: [31] <author> H. Stone and P. Sipala. </author> <title> The average complexity of depth-first search with backtracking and cutoff. </title> <journal> IBM Journal of Research and Development, </journal> <month> May </month> <year> 1986. </year>
Reference-contexts: The theoretical analysis is validated by experimental 2 analysis on example problems such as the problem of generating test-patterns for digital circuits [3], N queens, 15-puzzle [26], and the hackers problem <ref> [31] </ref>. The result that "parallel backtrack search gives at least a linear speedup on the average" is important since DFS is currently the best known and practically useful algorithm to solve a number of important problems. <p> This characteristic happens to be true for a variety of problem spaces searched by simple backtracking. 4.3 Experimental Results We present experimental results of the performance of parallel DFS on three problems: (i) the hacker's problem <ref> [31] </ref> (ii) the 15-puzzle problem [26] and (iii) the N queens problem [6]. In all the experiments discussed in this section, both sequential and parallel DFS visit the newly generated successors of a node in a random order.
Reference: [32] <author> H. S. Stone and J. Stone. </author> <title> Efficient search techniques an empirical study of the n-queens problem. </title> <type> Technical Report RC12057, </type> <institution> IBM TJ-Watson Research Center, </institution> <address> NY, </address> <year> 1986. </year>
Reference-contexts: On the other hand, in problems such as N queens, the ordering information improves the performance of DFS substantially <ref> [9, 32] </ref>. 8 The experiments for 15-puzzle were performed on the BBN Butterfly parallel processor for up to 9 processors. The experiments involved instances of the 15-puzzle with uniform distribution and non-uniform distribution of solutions. Depth bounded DFS was used to limit the search space for each instance.
Reference: [33] <author> Peter Tinker. </author> <title> Performance and pragmatics of an OR-parallel logic programming system. </title> <journal> International Journal of Parallel Programming, </journal> ?, <year> 1988. </year>
Reference-contexts: This type of behavior is common for a variety of parallel search algorithms, and is referred to as `speedup anomaly' [18, 19]. The superlinear speedup in isolated executions of parallel DFS has been reported by many researchers <ref> [12, 25, 22, 7, 33, 20] </ref>. It may appear that on the average the speedup would be either linear or sublinear; otherwise, even parallel DFS executed on sequential processor via time-slicing would perform better than sequential DFS.
Reference: [34] <author> Benjamin W. Wah and Y. W. Eva Ma. </author> <title> Manip amulticomputer architecture for solving combinatorial extremum-search problems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-33, </volume> <month> May </month> <year> 1984. </year>
Reference-contexts: It may also use heuristics to prune nodes of the search space so that search can be avoided under these nodes. This method is also referred to as ordered DFS [13]. 3 Parallel DFS There are many different parallel formulations of DFS <ref> [7, 15, 19, 34, 2, 8, 23] </ref> that are suitable for execution on asynchronous MIMD multiprocessors. The formulation discussed here is used quite commonly [22, 23, 2, 4, 14]. In this formulation, each processor searches a disjoint part of the search space.
References-found: 34

