URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/aml-turing.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Title: TURING COMPUTABILITY WITH NEURAL NETS  
Author: Hava T. Siegelmann Eduardo D. Sontag 
Address: New Brunswick, NJ 08903  New Brunswick, NJ 08903  
Affiliation: Department of Computer Science Rutgers University,  Department of Mathematics Rutgers University,  
Date: 1-15, 1900 0893-9659/00 $3:00 0:00  (Received June 17, 1991)  
Note: Appl. Math. Lett. Vol. 0, No. 0, pp.  Printed in Great Britain. All rights reserved Copyright c 1900 Pergamon Press plc  
Abstract: This paper shows the existence of a finite neural network, made up of sigmoidal neurons, which simulates a universal Turing machine. It is composed of less than 10 5 synchronously evolving processors, interconnected linearly. High-order connections are not required.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Alon, N., A.K. Dewdney, and T.J. Ott, </author> <title> "Efficient simulation of finite automata by neural nets," </title> <journal> J. A.C.M. </journal> <note> (1991): to appear. </note>
Reference-contexts: Note that at least since the classical work of McCulloch and Pitts in the 1940s, it has been clear how to simulate logic gates by networks of threshold (binary-valued) neurons, and hence how to obtain finite automata using such nets (see e.g. <ref> [1] </ref> for more recent work on that problem). One can simulate Turing machines if one allows a potentially unbounded number of neurons; see e.g. [4] for variations on this theme and relations to cellular automata. <p> 1g; there exist eight vectors v 1 ; v 2 ; : : :; v 8 2 Q 5 and scalars c 1 ; c 2 ; : : :; c 8 2 Q such that, for each (a; b; d; x) 2 f0; 1g 4 and each q 2 <ref> [0; 1] </ref>, fi (a; b; d)xq = i=1 ! 8 X c i (v i ) + 1 1 ; where = (1; a; b; d; x) and "" = dot product in Q 5 .
Reference: 2. <author> Berstel, J. and C. Reutenauer, </author> <title> Rational Series and their Languages, </title> <address> Sringer-Verlag, Berlin, </address> <year> 1988. </year>
Reference-contexts: are given in the full paper); on the other hand, the problem appears to become decidable 4 if a linear activation is used (halting in that case is equivalent to a fact that is widely conjectured to follow from classical results due to Skolem and others on rational functions; see <ref> [2] </ref>, page 75), and is also decidable in the pure threshold case (there are only finitely many states). As our function is in a sense a combination of thresholds and linear functions, this gap in decidability is perhaps remarkable. One obvious question deals with the use of other activation functions.
Reference: 3. <author> Blum, L., M. Shub, and S. Smale, </author> <title> "On a theory of computation and complexity over the real numbers: NP completeness, recursive functions, and universal machines," </title> <journal> Bull. A.M.S. </journal> <volume> 21(1989): </volume> <pages> 1-46. </pages>
Reference-contexts: See also <ref> [3] </ref> for other work on continuous-valued models of computation.
Reference: 4. <author> Franklin, S., and M. Garzon, </author> <title> "Neural computability," </title> <booktitle> in Progress In Neural Networks, </booktitle> <volume> Vol 1 )(O. </volume> <editor> M. Omidvar, ed.), </editor> <publisher> Ablex, </publisher> <address> Norwood, NJ, </address> <year> 1990, </year> <pages> pp. 128-144. </pages>
Reference-contexts: One can simulate Turing machines if one allows a potentially unbounded number of neurons; see e.g. <ref> [4] </ref> for variations on this theme and relations to cellular automata. Since we insist on a fixed number of neurons, which does not increase during the computation, our problem is different. fl Supported in part by US Air Force Grant AFOSR-880235 and by Siemens Corporate Research. 1 2 2.
Reference: 5. <author> Maass, W., G. Schnitger, and E.D. Sontag, </author> <title> "On the computational power of sigmoid versus boolean threshold circuits," </title> <booktitle> Proc. of the 32nd Annual Symp. on Foundations of Computer Science, </booktitle> <address> San Juan, PR, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: In closing, we note that the idea of using continuous-valued neurons in order to attain gains in computational capabilities compared with threshold gates had been explored in other work, for the special case of feedforward nets -see for instance [10] for questions of approximation and function interpolation, and <ref> [5] </ref> for questions of Boolean circuit complexity. See also [3] for other work on continuous-valued models of computation.
Reference: 6. <author> Marcus, </author> <title> C.M., and R.M. Westervelt, "Dynamics of iterated-map neural networks," </title> <journal> Phys. Rev. Ser. </journal> <volume> A 40(1989): </volume> <pages> 3355-3364. </pages>
Reference-contexts: Processor nets as above appear frequently in neural network studies, and their dynamic properties are of interest (see for instance <ref> [6] </ref>); the continuous-time analogue is also studied in the literature (see some comments in the concluding section below).
Reference: 7. <author> Pollack, J.B., </author> <title> On Connectionist Models of Natural Language Processing, </title> <type> Ph.D. Dissertation, </type> <institution> Com puter Science Dept, Univ. of Illinois, Urbana, </institution> <year> 1987. </year> <note> (Available as MCCS-87-100, </note> <institution> Computing Research Laboratory, </institution> <address> NMSU, Las Cruces, NM.) </address>
Reference-contexts: Reading a stack is straighforward: if q s encodes the stack value, then (2q s ) = 0 if and only if the stack is empty, and (2q s ) = 1 otherwise. (A different encoding of a stack, as in <ref> [7] </ref>, cannot be read in this simple manner.) The critical point is to show that the whole design can be integrated (stack operations and state transitions gated by states of control unit and symbols at tops of stacks) without introducing high-order connections, that is, products.
Reference: 8. <author> Siegelmann, H.T., and E.D. Sontag, </author> <title> "Neural nets are universal computing devices," </title> <type> Report SYCON 91-08, </type> <institution> Rutgers Center for Systems and Control, Rutgers University, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: Iff (n) is undefined, m = 0; otherwise (n) = m 1. We next describe the main ideas of the proof; details can be found in the technical report <ref> [8] </ref>. As in the textbook approach of "counter machines," one can encode an infinite tape into real-valued activations; the only problem is how to do this while preserving a purely linear-interconnection architecture.
Reference: 9. <author> Sontag, E.D., </author> <title> Mathematical Control Theory: Deterministic Finite Dimensional Systems, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: On the other hand, an equation of the type x + = t (Ax + bu + c), where t is a hard threshold (Heaviside) function, can only simulate a finite automaton, as all states are essentially binary. Many other types of "machines" may be used for universality (see <ref> [9] </ref>, especially Chapter 2, for general definitions of continuous machines).
Reference: 10. <author> Sontag, E.D., </author> <title> "Remarks on interpolation and recognition using neural nets," </title> <booktitle> in Advances in Neural Information Processing Systems 3 (R.P. </booktitle> <editor> Lippmann, J. Moody, and D.S. Touretzky, eds), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991, </year> <pages> pp. 939-945. </pages>
Reference-contexts: In closing, we note that the idea of using continuous-valued neurons in order to attain gains in computational capabilities compared with threshold gates had been explored in other work, for the special case of feedforward nets -see for instance <ref> [10] </ref> for questions of approximation and function interpolation, and [5] for questions of Boolean circuit complexity. See also [3] for other work on continuous-valued models of computation.
Reference: 11. <author> Sun, G.Z., H.-H. Chen, Y.-C. Lee, and C.L. Giles, </author> <title> "Turing equivalence of neural networks with second order connection weights," </title> <booktitle> in Proc. Int. Joint Conf. Neural Networks, </booktitle> <address> Seattle, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: In his model, all neurons synchronously update their states according to a quadratic combination of past activation values. In general, one calls high-order nets those in which activations are combined using multiplications; see <ref> [11] </ref> for related work and many other references to such nets. Pollack left open the question of establishing if high-order connections are really necessary in order to achive universality; the feeling among people working in the area has been that they are.
References-found: 11

