URL: http://http.cs.berkeley.edu/~sethg/papers/tr94-818.ps
Refering-URL: http://http.cs.berkeley.edu/~sethg/papers.html
Root-URL: http://www.cs.berkeley.edu
Title: The Implementation of a Threaded Abstract Machine  
Author: Seth Copen Goldstein 
Address: Berkeley, California 94720  
Affiliation: Computer Science Division (EECS) University of California  
Date: May 1994  
Pubnum: Report No. UCB/CSD 94-818  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Arvind, R. S. Nikhil, and K. K. Pingali. I-Structures: </author> <title> Data Structures for Parallel Computing. </title> <type> Technical Report CSG Memo 269, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> February </month> <year> 1987. </year> <title> Also in Proc. of the Graph Reduction Workshop, </title> <address> Santa Fe, NM. </address> <month> October </month> <year> 1986. </year>
Reference-contexts: However, if a different frame is active when the response returns, the inlet deposits the value into the inactive frame and posts a thread in that frame without disturbing the register usage of the currently active frame. The global heap supports synchronization on an element-by-element basis, as with I-structures <ref> [1] </ref>. Thus, there are two sources of latency in global accesses: A hardware communication latency occurs if the accessed element is remote to the issuing processor and, regardless of placement, a synchronization latency occurs if the accessed element is not present, causing the request to be deferred. <p> The call is initiated with an FALLOC and ends when an inlet is invoked by the callee. This structure is also followed for all non-local data access operations, e.g. for heap operations. There are two kinds of heap objects: packed and unpacked. The unpacked heap objects, I/M-structures <ref> [1] </ref>, provide per-element synchronization. The packed heap objects, A-structures, provide synchronization on a per-object basis. 2.2.1 I-Structures and M-Structures Every location of an I/M-structure consists of a data element and a synchronization tag. The data element is large enough to hold any data type (i.e., it is a general). <p> For instance, a local-only policy could be created by using the distribution list [0], which allocates each frame on the same processor. A policy which allocates every other frame on the current processor or one of its immediate neighbors results from the list <ref> [1; 0; 1; 0] </ref>. The random range policy also uses a distribution list, but chooses the processor on which to allocate the frame by picking an offset within a random range specified in the list. This works well for problems with 64 low locality needs [4]. <p> We use the notation sXXnYY to denote a policy, where XX is the percent of selfishness and YY is the size of the neighborhood. s100n1 denotes a completely local policy. The distributed policy with the list <ref> [1; 0; 1; 0] </ref> is described by s50n3. While, s64n7 describes the random policy of [3; 0]. 3.6.6 The Heap Instructions The parallel heap is divided into three sections: local, spread, and constant.
Reference: [2] <author> Boon. </author> <title> Runtime of Monsoon. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> July </month> <year> 1993. </year> <note> Special Issue on Dataflow. </note>
Reference-contexts: Indeed, the performance of our current implementation on a Sparc-10 rivals the performance of Id90 on special-purpose machines like the Monsoon [12]. All of our benchmark programs run faster on a single Sparc-10 than on a single-processor Monsoon <ref> [2] </ref>. In addition to providing users with fast Id90 executables on stock workstations and commercially available parallel machines, we have created a tool that allows us to study the dynamic behavior of large parallel applications on real machines.
Reference: [3] <author> P. J. Burns, M. Christon, R. Schweitzer, O. M. Lubeck, H. J. Wasserman, M. L. Simmons, and D. V. Pryor. </author> <title> Vectorization of Monte-Carlo Particle Transport: An Architectural Study using the LANL Benchmark Gamteb. </title> <booktitle> In Proc. Supercomputing '89. IEEE Computer Society and ACM SIGARCH, </booktitle> <address> New York, NY, </address> <month> November </month> <year> 1989. </year>
Reference-contexts: For instance, to randomly allocate approximately 42% of the frames within a neighborhood of seven processors and 58% on the local processor, the following distribution list could be used: <ref> [3; 0] </ref>. Finally, the sensing policy allocates the frame on the local processor unless another processor has requested a frame. Processors request frames by sending request messages to other processors when they are idle. <p> The distributed policy with the list [1; 0; 1; 0] is described by s50n3. While, s64n7 describes the random policy of <ref> [3; 0] </ref>. 3.6.6 The Heap Instructions The parallel heap is divided into three sections: local, spread, and constant. When heap objects are allocated, they are either allocated on the local heap or they are spread among the processors in the spread heap. <p> In addition to evaluating TAM itself, we have also used TL0 to study different approaches to resource allocation on parallel machines. In [7], we investigate frame allocation polices, I-structure caching, I-structure spreading, and k-bounded loops. We studied our two largest benchmarks, Gamteb <ref> [3] </ref> and Simple [5], on the CM-5. Both programs exhibit linear speed-up from two to sixty-four processors. (See Figure 4.1.
Reference: [4] <author> Soumen Chakrabarti, Abhiram Ranade, and Katherine Yelick. </author> <title> Randomized load balancing for tree structured computation. </title> <booktitle> In IEEE Scalable High Performance Computing Conference, </booktitle> <institution> Oak Ridge, Tennessee, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: The random range policy also uses a distribution list, but chooses the processor on which to allocate the frame by picking an offset within a random range specified in the list. This works well for problems with 64 low locality needs <ref> [4] </ref>. If the distribution list contains the number n, it will allocate the frame on a random processor n from the current one.
Reference: [5] <author> W. P. Crowley, C. P. Hendrickson, and T. E. Rudy. </author> <title> The SIMPLE code. </title> <type> Technical Report UCID 17715, </type> <institution> Lawrence Livermore Laboratory, </institution> <month> February </month> <year> 1978. </year>
Reference-contexts: In addition to evaluating TAM itself, we have also used TL0 to study different approaches to resource allocation on parallel machines. In [7], we investigate frame allocation polices, I-structure caching, I-structure spreading, and k-bounded loops. We studied our two largest benchmarks, Gamteb [3] and Simple <ref> [5] </ref>, on the CM-5. Both programs exhibit linear speed-up from two to sixty-four processors. (See Figure 4.1.
Reference: [6] <author> D. E. Culler. </author> <title> Managing Parallelism and Resources in Scientific Dataflow Programs. </title> <type> Technical Report 446, </type> <institution> MIT Lab for Comp. Sci., </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: The above example describes the call interface using the default, or non-strict, frame allocation policy. There are two other polices: the strict policy, used to invoke strict functions, and the kloop policy, used to invoke k-bounded loop <ref> [6] </ref> frames. The strict protocol allocates the required amount of memory and returns a pointer to the newly allocated and as yet uninitialized memory. It is up to the caller to initialize the frame by sending a message to inlet 0.
Reference: [7] <author> D. E. Culler, S. C. Goldstein, K. E. Schauser, and T. von Eicken. </author> <title> Empirical study of a dataflow language on the CM-5. </title> <note> To appear in IEEE Publication on Hamilton Island Workshop. </note>
Reference-contexts: In addition to evaluating TAM itself, we have also used TL0 to study different approaches to resource allocation on parallel machines. In <ref> [7] </ref>, we investigate frame allocation polices, I-structure caching, I-structure spreading, and k-bounded loops. We studied our two largest benchmarks, Gamteb [3] and Simple [5], on the CM-5. Both programs exhibit linear speed-up from two to sixty-four processors. (See Figure 4.1.
Reference: [8] <author> David E. Culler, Seth Copen Goldstein, Klaus Erik Schauser, and Thorsten von Eicken. </author> <title> TAM A Compiler Controlled Threaded Abstract Machine. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 347-370, </pages> <month> July </month> <year> 1993. </year> <note> Special Issue on Dataflow. </note>
Reference-contexts: Either way, it increases frame size due to the requirement that the RCV be large enough to act as an LCV. In addition, the threads posted while the frame is running require half as many cycles. Using our benchmark suite <ref> [8] </ref>, we discovered 59 that on sequential machines the average RCV size is 3:3 and 1:2 threads arrive on average while the frame is running, making the last implementation on average about six memory references cheaper than the copying implementation. <p> The TLC translator is able to produce instrumented executables which allow us to analyze the dynamic runtime behavior of our programs. The first studies we performed were used to determine if our front-end compilation strategy was effective in reducing the overhead normally found in dataflow implementations <ref> [8, 13] </ref>. This work, which looked at various partitioning schemes of the dataflow graph, forms the basis for the front end of the compiler. <p> The programs are described in <ref> [8] </ref>. 73 reasonable limit. One measure that can be used to evaluate the success of TL0 is the number of quanta per invocation. As we reported in [8], we found that there are generally between three and four quanta per invocation even in the presence of long remote latency as on <p> The programs are described in <ref> [8] </ref>. 73 reasonable limit. One measure that can be used to evaluate the success of TL0 is the number of quanta per invocation. As we reported in [8], we found that there are generally between three and four quanta per invocation even in the presence of long remote latency as on the CM-5. Compare this to at least two quanta per activation on a sequential machine for a sequential language.
Reference: [9] <institution> Free Software Foundation, Cambridge, Massachusetts. Using and Porting GNU CC, </institution> <year> 1993. </year> <title> Found on most systems by using emacs and meta-x info. </title>
Reference-contexts: This section addresses this problem by presenting a compromise between a completely native mode translatorwith its loss of portability and reduced ease of experimentationand the C version described above. All of the extensions used here are described in the chapter on C extensions in <ref> [9] </ref>. We keep the overall structure used by TLC, but implement the macros using GCC extensions to C.
Reference: [10] <author> R. S. Nikhil. </author> <note> Id (Version 90.0) Reference Manual. Technical Report CSG Memo, to appear, </note> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Third, it is aimed at allowing us to study the architectural mechanisms that are needed to support parallel programming. Finally, it is designed to facilitate the study of the dynamic behavior of large control-parallel applications. TL0 provides an execution vehicle for Id90, an implicitly parallel functional programming language <ref> [10] </ref>. Its functional nature, synchronizing data structures, and non-strict execution model hold the promise of considerable parallelism, but also provide a serious challenge to the compiler writer. The compilation process starts in the MIT front end [15], which converts the Id90 program into program graphs.
Reference: [11] <author> Michael D. Noakes, Deborah A. Wallach, and W. J. Dally. </author> <title> The J-Machine Multicomputer: An Architectural Evaluation. </title> <booktitle> In Proc. of the 20th Int'l Symposium on Computer Architecture, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The distributed policy performs the bestit chooses the middle road. On a completely different tack, the TL0 back end was used in evaluating two different machine architectures in [14]. We compared the J-Machine <ref> [11] </ref>, developed at MIT as a study in universal mechanisms for fine-grained parallelism, and the CM-5, developed at Thinking Machines Corporation as a commercial product supporting data-parallel programs.
Reference: [12] <author> G. M. Papadopoulos and D. E. Culler. Monsoon: </author> <title> an Explicit Token-Store Architecture. </title> <booktitle> In Proc. of the 17th Annual Int. Symp. on Comp. Arch., </booktitle> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year> <month> 76 </month>
Reference-contexts: In addition, TLC produces executables for the Sequent Symmetry, the nCube-2, and the CM-5. To date, this is the only viable implementation of Id90 on conventional machines. Indeed, the performance of our current implementation on a Sparc-10 rivals the performance of Id90 on special-purpose machines like the Monsoon <ref> [12] </ref>. All of our benchmark programs run faster on a single Sparc-10 than on a single-processor Monsoon [2].
Reference: [13] <author> Klaus Erik Schauser. </author> <title> Compiling Dataflow into Threads. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, Computer Science Div., </institution> <month> July </month> <year> 1991. </year> <note> Report No. UCB/CSD 91/644. </note>
Reference-contexts: The compilation process starts in the MIT front end [15], which converts the Id90 program into program graphs. These are converted by the Berkeley compiler into dual graphs, on which the program is partitioned into threads <ref> [13] </ref>. The partitioned graph is then converted into TL0 code. The last step in the compilation process is to translate the TL0 code into an executable that can be run on a real machine. <p> The TLC translator is able to produce instrumented executables which allow us to analyze the dynamic runtime behavior of our programs. The first studies we performed were used to determine if our front-end compilation strategy was effective in reducing the overhead normally found in dataflow implementations <ref> [8, 13] </ref>. This work, which looked at various partitioning schemes of the dataflow graph, forms the basis for the front end of the compiler.
Reference: [14] <author> E. Spertus, S. C. Goldstein, K. E. Schauser, T. von Eicken, D. E. Culler, and W. J. Dally. </author> <title> Evaluation of Mechanisms for Fine-Grained Parallel Programs in the J-Machine and the CM-5. </title> <booktitle> In Proc. of the 20th Int'l Symposium on Computer Architecture, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The random policy produces the most balanced distribution of frames, but is the slowest due to a high remote access rate. The distributed policy performs the bestit chooses the middle road. On a completely different tack, the TL0 back end was used in evaluating two different machine architectures in <ref> [14] </ref>. We compared the J-Machine [11], developed at MIT as a study in universal mechanisms for fine-grained parallelism, and the CM-5, developed at Thinking Machines Corporation as a commercial product supporting data-parallel programs.
Reference: [15] <author> K. R. Traub. </author> <title> A Compiler for the MIT Tagged-Token Dataflow Architecture. </title> <type> Technical Report TR-370, </type> <institution> MIT Lab for Comp. Sci., </institution> <type> 545 Tech. </type> <institution> Square, </institution> <address> Cambridge, MA, </address> <month> August </month> <year> 1986. </year> <type> (MS Thesis, </type> <institution> Dept. of EECS, MIT). </institution>
Reference-contexts: TL0 provides an execution vehicle for Id90, an implicitly parallel functional programming language [10]. Its functional nature, synchronizing data structures, and non-strict execution model hold the promise of considerable parallelism, but also provide a serious challenge to the compiler writer. The compilation process starts in the MIT front end <ref> [15] </ref>, which converts the Id90 program into program graphs. These are converted by the Berkeley compiler into dual graphs, on which the program is partitioned into threads [13]. The partitioned graph is then converted into TL0 code.
Reference: [16] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proc. of the 19th Int'l Symposium on Computer Architecture, </booktitle> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year> <note> Also available as Technical Report UCB/CSD 92/675, </note> <institution> CS Div., University of California at Berkeley. </institution> <month> 77 </month>
Reference-contexts: Instead, all SEND instructions occurring in inlets use the reply network. 9 The TL0 SEND instruction coupled with inlets are the embryonic ideas behind active messages <ref> [16] </ref>. 10 Or a network with two priorities. 13 may not start any operation for which resources have not yet been committed. <p> : : arg n ); =) if (toProc (destFp) == MYPID) (*dest)(asLocalPtr (destFp), arg 1 ; : : : arg n ); else am_sendn (toProc (destFp), dest, asLocalPtr (destFp), arg 1 ; : : : arg n ); endif The am_sendn macros are used to invoke the active message layer <ref> [16] </ref>. The translator generates only sends that can fit in a single message packet, which is determined by the machine. (See Figure 3.5.) 3.6.5 Falloc In TL0 at the higher level of scheduling, the frame is the basic unit of work.
References-found: 16

