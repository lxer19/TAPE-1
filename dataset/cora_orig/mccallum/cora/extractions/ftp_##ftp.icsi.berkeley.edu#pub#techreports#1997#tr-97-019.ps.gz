URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1997/tr-97-019.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1997.html
Root-URL: http://www.icsi.berkeley.edu
Title: Optimization with the Hopfield network based on correlated noises: an empirical approach Gaussian noises were
Author: Jacek Mandziuk 
Note: Optimization with SM, unlike in previous related models, in which ffi-correlated  Senior Fulbright  UCBerkeley. On leave from  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  Plac Politechniki 1, 00-661 Warsaw, Poland,  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  Scholar visiting ICSI and EECS Dept.  Institute of Mathematics, Warsaw University of Technology,  
Pubnum: TR-97-019  
Email: e-mail: mandziuk@alpha.im.pw.edu.pl  
Phone: (510) 643-9153 FAX (510) 643-7684  
Abstract: This paper presents two simple optimization techniques based on combining the Langevin Equation with the Hopfield Model. Proposed models referred as Stochastic Model (SM) and Pulsed Noise Model (PNM) can be viewed as straightforward stochastic extensions of the Hopfield optimization network. With the above strong simplifications neither SM nor PNM is expected to rigorously maintain Thermal Equilibrium (TE). However, approximate numerical tests based on the canonical Gibbs-Boltzmann distribution show, that differences between rigorous and estimated values of TE parameters are relatively low (within a few percent). In this sense both models are said to perform Quasi Thermal Equilibrium. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. H. Ackley, G. W. Hinton and T. J. Sejnowski, </author> <title> "A learning algorithm for Boltzmann Machines", </title> <journal> Cognitive Sci., 1985, </journal> <volume> 9, </volume> <pages> pp. 147-169 </pages>
Reference-contexts: TSP is mapped onto the Hopfield network by a square n fi n matrix V of "nearly binary" elements. Expression "nearly binary" means that gain ff in the neuron's activation function is big enough to eventually drive the network towards one of the corners of the <ref> [0; 1] </ref> nfin hypercube. The solution of the problem is read out from V after binarization of its elements. <p> Improvement is based on the observation that the global inhibition term in (8), that is the term multiplied by C, tends to average the influence of particular neurons, and in turn forces the system back towards the center of the hypercube <ref> [0; 1] </ref> nfin . <p> The network used for our simulations was not optimized in any of the above aspects. 1.3 The Langevin Equation The Boltzmann Machine <ref> [1] </ref> combined with the simulated annealing technique [12] is a well known model for stochastic optimization over binary variables. <p> In [8] it is showed that under some restrictive conditions for E and X, when the minimization is confined to the hypercube <ref> [0; 1] </ref> N by so-called "reflecting boundaries", and with the inverse logarithmic temperature annealing (cooling) schedule T (t) = ln (2 + t) and for "sufficiently large" c, system (11), regardless of starting point X (0), converges weakly with the probability measure to the Gibbs distribution (12). <p> (t); (i; j = 0; : : : ; N 1; i 6= j) are pairwise independent ffi-correlated Gaussian noises with intensity (15): Cov [fl i (t); fl i (s)] = dv i In such a case, it can be shown that the probability of observing a configuration V 2 <ref> [0; 1] </ref> N at temperature T is given by p V;T = Z (T ) n E (V ) o 6 where Z (T ) = [0;1] N n E (V ) o The above combination of the HM and the Langevin algorithm results in, what authors of [13] called, Stochastic <p> j , value of in (23) In order to examine the above properties, for various temperatures T , distributions of configurations are collected in the following way: (A1) system (19) is cooled to T according to (20), and T is fixed, (A2) M = 16 000 sample configurations v 2 <ref> [0; 1] </ref> nfin are collected, one at each pt 1 time, (p = 1; : : : ; 16 000), i.e. right before a noise injection, 11 (a) (b) temperature pair (60; 50). <p> By definition r 2 2 <ref> [0; 1] </ref>, and the greater r 2 the closer linear dependence of the data. In particular, r 2 = 1 means perfect match between the experimental data and the estimated straight line.
Reference: [2] <author> Y. Akiyama, A. Yamashira, M. Kajiura, Y. Anzai and H. Aiso, </author> <title> "The Gaussian Machine: A Stochastic Neural Network for Solving Assignment Problems", </title> <journal> Journal of Neural Network Computing, </journal> <year> 1991, </year> <month> Winter, </month> <pages> pp. 43-51 </pages>
Reference-contexts: More efficient energy forms can be found in <ref> [2, 3, 4, 6] </ref>, to cite only a few papers.
Reference: [3] <author> A. R. Bizzarri, </author> <title> "Convergence properties of a modified Hopfield-Tank model", </title> <journal> Biological Cybernetics, 1991, </journal> <volume> 64, </volume> <pages> pp. 293-300 </pages>
Reference-contexts: More efficient energy forms can be found in <ref> [2, 3, 4, 6] </ref>, to cite only a few papers.
Reference: [4] <author> R. D. Brandt, Y. Wang, A. J. Laub and S. K. Mitra, </author> <title> "Alternative Networks for Solving the Travelling Salesman Problem and the List-Matching Problem", </title> <booktitle> Intern. Conf. Neural Networks, </booktitle> <address> San Diego, </address> <year> 1988, </year> <pages> pp. 333-340 </pages>
Reference-contexts: More efficient energy forms can be found in <ref> [2, 3, 4, 6] </ref>, to cite only a few papers.
Reference: [5] <author> J. H. Cervantes and R. R. Hildebrant, </author> <title> "Comparison of three neuron- based computation schemes", </title> <booktitle> IEEE Conf. on Neural Networks, </booktitle> <address> San Diego, USA, </address> <year> 1987, </year> <month> III-657:671 </month>
Reference-contexts: The proof of the ability of SNN to maintain TE with the inverse logarithmic cooling (13) presented in [13] is a significant theoretical result. Unfortunately, there are some practical limitations that prevented SNN from straightforward implementation in hardware as well as in computer simulations <ref> [5] </ref>. Mainly the two of them: * an extremely slow, inverse logarithmic cooling schedule (13) is required, which makes the implementation ineffective. <p> Coordination of both temperature and gain schedules makes solving (14) a complicated and time-consuming task. This also seems to be a real obstacle in computer simulations <ref> [5] </ref> and in analogue implementation. In fact, in [13] it is suggested that increase of ff in (14) should be done much slower than temperature decrease. This is a necessary condition since cosh function should be restricted from a rapid growth.
Reference: [6] <author> L. Chen and K. Aihara. </author> <title> Chaotic Simulated Annealing by a Neural Network Model with Transient Chaos, </title> <booktitle> Neural Networks, </booktitle> <volume> 8(6) </volume> <pages> 915-930, </pages> <year> 1995 </year>
Reference-contexts: More efficient energy forms can be found in <ref> [2, 3, 4, 6] </ref>, to cite only a few papers.
Reference: [7] <author> T-S. Chiang, C-R. Hwang and S-J. Sheu, </author> <title> "Diffusion for global optimization in R n ", SIAM J. </title> <booktitle> Control and Optimization 1987, </booktitle> <pages> 25(3) pp. 737-753 </pages>
Reference-contexts: In other words, while maintaining the Thermal Equilibrium, system (11) gradually (as T decreases) converges with probability to the global minimum of E. This result was extended in <ref> [7] </ref> to the case of minimization over R N . 2 Previous related work Mathematical proof for the ability of the Langevin algorithm to eventually converge to the global minimum forms a strong basis for the research on practical implementation of the Langevin Equation-based minimization methods.
Reference: [8] <author> S. Geman and C-R. Hwang, </author> <title> "Diffusions for global optimization", </title> <journal> SIAM J. Control and Optimization 1986, </journal> <pages> 24(4) pp. 1031-1043 </pages>
Reference-contexts: For temperatures T near zero T is concen-trated on the global minima of E. The advantage of the fact that steady-state probability density of (11) is given by the Gibbs-Boltzmann distribution is taken in the optimization algorithm proposed by Geman and Hwang <ref> [8] </ref>. In [8] it is showed that under some restrictive conditions for E and X, when the minimization is confined to the hypercube [0; 1] N by so-called "reflecting boundaries", and with the inverse logarithmic temperature annealing (cooling) schedule T (t) = ln (2 + t) and for "sufficiently large" c, <p> For temperatures T near zero T is concen-trated on the global minima of E. The advantage of the fact that steady-state probability density of (11) is given by the Gibbs-Boltzmann distribution is taken in the optimization algorithm proposed by Geman and Hwang <ref> [8] </ref>. In [8] it is showed that under some restrictive conditions for E and X, when the minimization is confined to the hypercube [0; 1] N by so-called "reflecting boundaries", and with the inverse logarithmic temperature annealing (cooling) schedule T (t) = ln (2 + t) and for "sufficiently large" c, system (11),
Reference: [9] <author> J. J. </author> <title> Hopfield, "Neural networks and physical systems with emergent collective computational abilities", </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci. USA, </institution> <year> 1982, </year> <pages> 79, pp. 2554-2558 </pages>
Reference-contexts: The distinction will also be clearly indicated by the context. 1.1 The Hopfield Model In 1982 Hopfield <ref> [9] </ref> introduced a neural network model of a Content Addressable Memory (CAM) composed of many, highly-interconnected two-state, McCulloch-Pitts neurons [18]. The subsequent papers described the continuous version of the model, which was composed of the collection of continuous (graded) response neurons.
Reference: [10] <author> J. J. </author> <title> Hopfield, "Neurons with graded response have collective computational properties like those of two-state neurons", </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci. USA, </institution> <year> 1984, </year> <pages> 81, pp. 3088-3092 </pages>
Reference-contexts: The subsequent papers described the continuous version of the model, which was composed of the collection of continuous (graded) response neurons. The application domain of the continuous model was either the construction of a CAM <ref> [10] </ref> or solving combinatorial optimization problems [11]. Since after the classical paper of Hopfield and Tank [11] lots of theoretical and experimental research has been published on this subject, the description of the Hop-field Model (HM) presented in this Section is confined to the minimum indispensable for introducing the notation. <p> based on mini mization of the energy function (Lyapunov function for (4)), which is of the following generic form: E = 2 i=0 j=0 N1 X I i v i + RC i=0 0 For sufficiently high gain ff in (3) and in the regime du i dt ! 0, <ref> [10, 20] </ref> energy (5) reduces to E = 2 i=0 j=0 N1 X I i v i (6) Therefore, while solving optimization problem with the Hopfield network one is to choose weights t ij and external inputs I i so as (global) minima of (6) correspond to (optimal) solutions of the
Reference: [11] <author> J. J. Hopfield and D. W. Tank, </author> <title> "Neural Computation of Decisions in Optimization Problems", </title> <journal> Biol. Cyber., 1985, </journal> <volume> 52, </volume> <pages> pp. 141-152 </pages>
Reference-contexts: The subsequent papers described the continuous version of the model, which was composed of the collection of continuous (graded) response neurons. The application domain of the continuous model was either the construction of a CAM [10] or solving combinatorial optimization problems <ref> [11] </ref>. Since after the classical paper of Hopfield and Tank [11] lots of theoretical and experimental research has been published on this subject, the description of the Hop-field Model (HM) presented in this Section is confined to the minimum indispensable for introducing the notation. <p> The application domain of the continuous model was either the construction of a CAM [10] or solving combinatorial optimization problems <ref> [11] </ref>. Since after the classical paper of Hopfield and Tank [11] lots of theoretical and experimental research has been published on this subject, the description of the Hop-field Model (HM) presented in this Section is confined to the minimum indispensable for introducing the notation. <p> The solution of the problem is read out from V after binarization of its elements. The same generic form of the energy function for the TSP as the one presented in <ref> [11] </ref> is used, with only different coefficients: A = 5; B = 5; C = 10; D = 5; n = n and ff = 10. The set of coefficients is chosen so as to be "reasonable" and no effort has been spent on optimizing coefficients' values. <p> A detailed explanation of the above choice of the energy function as well as a description of a mapping of the problem to matrix V are presented in <ref> [11] </ref>. 4 Many results published afterwards support the following remarks: * the choice of the energy form (8) regardless of coefficient values is not optimal. More efficient energy forms can be found in [2, 3, 4, 6], to cite only a few papers. <p> In fact, in [13] it is suggested that increase of ff in (14) should be done much slower than temperature decrease. This is a necessary condition since cosh function should be restricted from a rapid growth. An idea of slow increase of ff was also proposed in <ref> [11] </ref>, in the context of the classical (not stochastic) HM. The last remark concerning SNN is about the nature of noises in (14). Certainly, ffi-correlated noises used for the mathematical proof does not exist in practical situations.
Reference: [12] <author> S. Kirkpatrick, C. D. Gelatt Jr., and M. P. Vecchi, </author> <title> "Optimization by Simulated Annealing", </title> <booktitle> Science, 1983, </booktitle> <volume> 220, </volume> <pages> pp. 671-680 </pages>
Reference-contexts: Instead of classical Hopfield differential equations, which for a given starting point deterministically describe a trajectory in the search space, new models are defined by stochastic differential equations, obtained by adding a noise term to the Hopfield Model. Similarly to the simulated annealing method <ref> [12] </ref>, a noise term is multiplied by the coefficient (temperature), which decreases in time. Both models follow the idea of Stochastic Neural Network [13] and Diffusion Machine [21]. <p> The network used for our simulations was not optimized in any of the above aspects. 1.3 The Langevin Equation The Boltzmann Machine [1] combined with the simulated annealing technique <ref> [12] </ref> is a well known model for stochastic optimization over binary variables.
Reference: [13] <author> B. C. Levy and M. B. Adams, </author> <title> "Global optimization with stochastic neural networks", </title> <booktitle> IEEE Conf. on Neural Networks, </booktitle> <address> San Diego, USA, </address> <year> 1987, </year> <note> III-681:689 19 </note>
Reference-contexts: Similarly to the simulated annealing method [12], a noise term is multiplied by the coefficient (temperature), which decreases in time. Both models follow the idea of Stochastic Neural Network <ref> [13] </ref> and Diffusion Machine [21]. Optimization with Stochastic Model, unlike in referred works, in which ffi-correlated Gaussian noises were considered, is based on Gaussian noises with positive autocorrelation times. This is a reasonable assumption from the hardware implementation point of view. <p> Thus, as reported in <ref> [13] </ref>, in order to keep up with the Boltzmann law (the Gibbs distribution) the noise has to be injected to the Hopfield Model in the following way: du i (t) = j=0 u i + 2 T cosh where fl i (t); fl j (t); (i; j = 0; : : <p> V 2 [0; 1] N at temperature T is given by p V;T = Z (T ) n E (V ) o 6 where Z (T ) = [0;1] N n E (V ) o The above combination of the HM and the Langevin algorithm results in, what authors of <ref> [13] </ref> called, Stochastic Neural Network (SNN), which given enough time converges with probability to the global minimum. The proof of the ability of SNN to maintain TE with the inverse logarithmic cooling (13) presented in [13] is a significant theoretical result. <p> The above combination of the HM and the Langevin algorithm results in, what authors of <ref> [13] </ref> called, Stochastic Neural Network (SNN), which given enough time converges with probability to the global minimum. The proof of the ability of SNN to maintain TE with the inverse logarithmic cooling (13) presented in [13] is a significant theoretical result. Unfortunately, there are some practical limitations that prevented SNN from straightforward implementation in hardware as well as in computer simulations [5]. Mainly the two of them: * an extremely slow, inverse logarithmic cooling schedule (13) is required, which makes the implementation ineffective. <p> Coordination of both temperature and gain schedules makes solving (14) a complicated and time-consuming task. This also seems to be a real obstacle in computer simulations [5] and in analogue implementation. In fact, in <ref> [13] </ref> it is suggested that increase of ff in (14) should be done much slower than temperature decrease. This is a necessary condition since cosh function should be restricted from a rapid growth. <p> The main advantage of these approaches is their simplicity and implementation feasibility. Unlike in the previous related works regarding Stochastic Neural Networks <ref> [13] </ref> and Diffusion Machine [21], in SM and PNM, intensities of Gaussian noises injected to the system are independent of neurons' potentials. Moreover, instead of impractically long inverse logarithmic cooling schedules, the linear cooling is tested.
Reference: [14] <author> J. Mandziuk, </author> <title> "Solving the N-Queens Problem with a binary Hopfield-type net-work. Synchronous and asynchronous model", </title> <journal> Biological Cybernetics, 1995, </journal> <volume> 72, </volume> <pages> pp. 439-446 </pages>
Reference-contexts: However, despite some obvious limitations, the Hopfield continuous model (4), as well as two-state model can be successfully used for solving combinatorial optimization problems, especially in situations when finding a good (not optimal) solution is sufficient or for the problems with relatively many global minima, e.g. the N-Queens Problem <ref> [14, 17] </ref>. 1.2 The Travelling Salesman Problem The Travelling Salesman Problem is one of the standard benchmark problems for evaluation of the optimization methods.
Reference: [15] <author> J. Mandziuk, </author> <title> "Pulsed noise based stochastic optimization with the Hopfield model", </title> <booktitle> Proceedings of the IEEE International Conference on Neural Networks (ICNN'97), </booktitle> <address> Houston, Texas, USA, </address> <month> June </month> <year> 1997, </year> <note> to appear </note>
Reference-contexts: Moreover, in both models, improvement is expected with slower cooling schedules. Results presented in this paper are also presented in part in <ref> [15] </ref> and [16] for PNM and SM, respectively. 1 The paper is organized as follows: the next three subsections briefly introduce the background of this work: the Hopfield Model, the Travelling Salesman Problem, and the Langevin (Diffusion) Equation all of them in the context of solving N P Hard optimization problems.
Reference: [16] <author> J. Mandziuk, </author> <title> "Non-delta-correlated Gaussian Noises and the Hopfield Optimization Circuit: an Empirical Study", </title> <booktitle> Proceedings of the 2nd International Conference on Computational Intelligence and Neuroscience (ICCIN'97), </booktitle> <address> Research Triangle Park, North Carolina, USA, </address> <month> March </month> <year> 1997, </year> <journal> vol. </journal> <volume> 2: </volume> <pages> 110-113 </pages>
Reference-contexts: Moreover, in both models, improvement is expected with slower cooling schedules. Results presented in this paper are also presented in part in [15] and <ref> [16] </ref> for PNM and SM, respectively. 1 The paper is organized as follows: the next three subsections briefly introduce the background of this work: the Hopfield Model, the Travelling Salesman Problem, and the Langevin (Diffusion) Equation all of them in the context of solving N P Hard optimization problems.
Reference: [17] <author> J. Mandziuk and B. Macukow, </author> <title> "A neural network designed to solve the N-Queens Problem", </title> <journal> Biological Cybernetics, 1992, </journal> <volume> 66, </volume> <pages> pp. 375-379 </pages>
Reference-contexts: However, despite some obvious limitations, the Hopfield continuous model (4), as well as two-state model can be successfully used for solving combinatorial optimization problems, especially in situations when finding a good (not optimal) solution is sufficient or for the problems with relatively many global minima, e.g. the N-Queens Problem <ref> [14, 17] </ref>. 1.2 The Travelling Salesman Problem The Travelling Salesman Problem is one of the standard benchmark problems for evaluation of the optimization methods.
Reference: [18] <author> W. A. McCulloch and W. Pitts, </author> <title> "A logical calculus of ideas immanent in nervous activity", </title> <journal> Bulletin of Mathematical Biophysics, 1943, </journal> <volume> 5, </volume> <pages> pp. 115-133 </pages>
Reference-contexts: The distinction will also be clearly indicated by the context. 1.1 The Hopfield Model In 1982 Hopfield [9] introduced a neural network model of a Content Addressable Memory (CAM) composed of many, highly-interconnected two-state, McCulloch-Pitts neurons <ref> [18] </ref>. The subsequent papers described the continuous version of the model, which was composed of the collection of continuous (graded) response neurons. The application domain of the continuous model was either the construction of a CAM [10] or solving combinatorial optimization problems [11].
Reference: [19] <author> D. Prevost, Ph. Lalanne, J. C. Rodier, P. Chavel, A. Dupret E. Belhaire and P. Garda, </author> <title> "Video-rate Simulated Annealing for stochastic artificial retinas", </title> <journal> Optics Communication, 1996, </journal> <volume> 132, </volume> <pages> pp. 427-431 </pages>
Reference-contexts: The particular choice of g as in (3), leads to (14). The work presented in this paper was also inspired by ref. <ref> [19] </ref>, where an optoelec-tronic system performing video-rate simulated annealing is presented and accuracy of its hardware implementation versus computational simulation results is analyzed. 3 Stochastic Model In order to avoid some implementation limitations of Stochastic Neural Network and Diffusion Machine mentioned in the previous section and to simplify the computational model, <p> However, in the experimental approach, the huge number of potential binary configurations V , V 2 f0; 1g N , makes direct observation of the Gibbs-Boltzmann distribution infeasible. Therefore, the approximate method, based on the canonical Gibbs-Boltzmann distribution (22) is used (cf. <ref> [19] </ref>). In the canonical Gibbs-Boltzmann distribution, p T = Z 0 (T ) n E o (E) denotes the number of binary configurations V with energies between E and E + ffiE, is the scaling coefficient, and Z 0 (T ) is the normalizing constant.
Reference: [20] <author> M. J. S. Smith and C. L. Protman, </author> <title> "Practical design and analysis of a simple "neural" optimization circuit", </title> <journal> IEEE Trans. Circiuts and Systems, 1989, </journal> <volume> 36, </volume> <pages> pp. 42-50 </pages>
Reference-contexts: based on mini mization of the energy function (Lyapunov function for (4)), which is of the following generic form: E = 2 i=0 j=0 N1 X I i v i + RC i=0 0 For sufficiently high gain ff in (3) and in the regime du i dt ! 0, <ref> [10, 20] </ref> energy (5) reduces to E = 2 i=0 j=0 N1 X I i v i (6) Therefore, while solving optimization problem with the Hopfield network one is to choose weights t ij and external inputs I i so as (global) minima of (6) correspond to (optimal) solutions of the
Reference: [21] <author> E. Wong, </author> <title> "Stochastic neural networks", </title> <journal> Algorithmica, 1991, </journal> <volume> 6, </volume> <pages> pp. 466-478 20 </pages>
Reference-contexts: Similarly to the simulated annealing method [12], a noise term is multiplied by the coefficient (temperature), which decreases in time. Both models follow the idea of Stochastic Neural Network [13] and Diffusion Machine <ref> [21] </ref>. Optimization with Stochastic Model, unlike in referred works, in which ffi-correlated Gaussian noises were considered, is based on Gaussian noises with positive autocorrelation times. This is a reasonable assumption from the hardware implementation point of view. <p> However in that case, stochastic processes (14) are not Markovian, since the transitions at time t are not independent of the past. Similar mathematical result concerning the convergence of the combination of HM and LE optimization was independently proved in <ref> [21] </ref>. In the proof presented in [21] there is no requirement for changing of gain ff. This, as mentioned above, significantly simplifies the method. The model presented in [21] is described by the set of equations (18): du i = @v i s g 0 (u i ) 7 and is <p> However in that case, stochastic processes (14) are not Markovian, since the transitions at time t are not independent of the past. Similar mathematical result concerning the convergence of the combination of HM and LE optimization was independently proved in <ref> [21] </ref>. In the proof presented in [21] there is no requirement for changing of gain ff. This, as mentioned above, significantly simplifies the method. The model presented in [21] is described by the set of equations (18): du i = @v i s g 0 (u i ) 7 and is called Diffusion Machine. <p> Similar mathematical result concerning the convergence of the combination of HM and LE optimization was independently proved in <ref> [21] </ref>. In the proof presented in [21] there is no requirement for changing of gain ff. This, as mentioned above, significantly simplifies the method. The model presented in [21] is described by the set of equations (18): du i = @v i s g 0 (u i ) 7 and is called Diffusion Machine. The particular choice of g as in (3), leads to (14). <p> The main advantage of these approaches is their simplicity and implementation feasibility. Unlike in the previous related works regarding Stochastic Neural Networks [13] and Diffusion Machine <ref> [21] </ref>, in SM and PNM, intensities of Gaussian noises injected to the system are independent of neurons' potentials. Moreover, instead of impractically long inverse logarithmic cooling schedules, the linear cooling is tested.
References-found: 21

