URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/isca-april.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/april.html
Root-URL: 
Title: APRIL: A Processor Architecture for Multiprocessing  
Author: Anant Agarwal, Beng-Hong Lim, David Kranz, and John Kubiatowicz 
Address: Cambridge, MA 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: Processors in large-scale multiprocessors must be able to tolerate large communication latencies and synchronization delays. This paper describes the architecture of a rapid-context-switching processor called APRIL with support for fine-grain threads and synchronization. APRIL achieves high single-thread performance and supports virtual dynamic threads. A commercial RISC-based implementation of APRIL and a run-time software system that can switch contexts in about 10 cycles is described. Measurements taken for several parallel applications on an APRIL simulator show that the overhead for supporting parallel tasks based on futures is reduced by a factor of two over a corresponding implementation on the Encore Multimax. The scalability of a multiprocessor based on APRIL is explored using a performance model. We show that the SPARC-based implementation of APRIL can achieve close to 80% processor utilization with as few as three resident threads per processor in a large-scale cache-based machine with an average base network latency of 55 cycles. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal. </author> <title> Performance Tradeoffs in Multithreaded Processors. </title> <month> September </month> <year> 1989. </year> <title> MIT VLSI Memo 89-566, </title> <institution> Laboratory for Computer Science. </institution>
Reference-contexts: We have developed a model for multithreaded processor utilization that includes the cache, network, and switching overhead effects. A detailed analysis is presented in <ref> [1] </ref>. This section will summarize the model and our chief results.
Reference: [2] <author> Arvind and Robert A. </author> <title> Iannucci. Two Fundamental Issues in Multiprocessing. </title> <type> Technical Report TM 330, </type> <institution> MIT, Laboratory for Computer Science, </institution> <month> October </month> <year> 1987. </year>
Reference-contexts: 1 Introduction The requirements placed on a processor in a large-scale multiprocessing environment are different from those in a uniprocessing setting. A processor in a parallel machine must be able to tolerate high memory latencies and handle process synchronization efficiently <ref> [2] </ref>. This need increases as more processors are added to the system. Parallel applications impose processing and communication bandwidth demands on the parallel machine. An efficient and cost-effective machine design achieves a balance between the processing power and the communication bandwidth provided.
Reference: [3] <author> Arvind, R. S. Nikhil, and K. K. Pingali. IStructures: </author> <title> Data Structures for Parallel Computing. </title> <booktitle> In Proceedings of the Workshop on Graph Reduction, (Springer-Verlag Lecture Notes in Computer Science 279), </booktitle> <month> September/October </month> <year> 1986. </year>
Reference-contexts: Traps also obviate the additional software tests of the lock in test&set operations. A similar mechanism is used to implement I-structures in dataflow machines <ref> [3] </ref>, however APRIL is different in that it implements such synchronizations through software trap handlers. 3.4 Multimodel Support Mechanisms APRIL is designed primarily for a shared-memory multiprocessor with strongly coherent caches.
Reference: [4] <author> William C. Athas and Charles L. Seitz. </author> <title> Multicomputers: Message-Passing Concurrent Computers. </title> <journal> Computer, </journal> <volume> 21(8) </volume> <pages> 9-24, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Therefore APRIL continues executing a single thread until a memory operation involving a remote request (or an unsuccessful synchronization attempt) is encountered. The controller forces the processor to switch to another thread, while it services the request. This approach is called coarse-grain multithreading. Processors in message passing multicomputers <ref> [21, 27, 6, 4] </ref> have traditionally taken this approach to allow overlapping of communication with computation. Context switching in APRIL is achieved by changing the frame pointer. Since APRIL has four task frames, it can have up to four threads loaded.
Reference: [5] <author> David Chaiken, Craig Fields, Kiyoshi Kurihara, and Anant Agarwal. </author> <title> Directory-Based CacheCoherence in Large-Scale Multiprocessors. </title> <month> June </month> <year> 1990. </year> <note> To appear in IEEE Computer. </note>
Reference-contexts: We evaluate the scalability of multithreaded processors in Section 8. 2 The ALEWIFE System APRIL is the processing element of ALEWIFE, a large- scale multiprocessor being designed at MIT. ALEWIFE is a cache-coherent machine with distributed, globally- shared memory. Cache coherence is maintained using a directory-based protocol <ref> [5] </ref> over a low-dimension direct network [20]. The directory is distributed with the processing nodes. 2.1 Hardware As shown in Figure 1, each ALEWIFE node consists of a processing element, floating-point unit, cache, main memory, cache/directory controller and a network routing switch.
Reference: [6] <author> W. J. Dally et al. </author> <title> Architecture of a Message-Driven Processor. </title> <booktitle> In Proceedings of the 14th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 189-196, </pages> <publisher> IEEE, </publisher> <address> New York, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Therefore APRIL continues executing a single thread until a memory operation involving a remote request (or an unsuccessful synchronization attempt) is encountered. The controller forces the processor to switch to another thread, while it services the request. This approach is called coarse-grain multithreading. Processors in message passing multicomputers <ref> [21, 27, 6, 4] </ref> have traditionally taken this approach to allow overlapping of communication with computation. Context switching in APRIL is achieved by changing the frame pointer. Since APRIL has four task frames, it can have up to four threads loaded.
Reference: [7] <author> Michel Dubois, Christoph Scheurich, and Faye A. Briggs. </author> <title> Synchronization, coherence, and event or 12 dering in multiprocessors. </title> <booktitle> IEEE Computer, </booktitle> <pages> 9-21, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Multiple nodes are connected via a direct, packet-switched network. The controller synthesizes a global shared memory space via messages to other nodes, and satisfies requests from other nodes directed to its local memory. It maintains strong cache coherence <ref> [7] </ref> for memory accesses. On exception conditions, such as cache misses and failed synchronization attempts, the controller can choose to trap the processor or to make the processor wait. A multithreaded processor reduces the ill effects of the long-latency acknowledgment messages resulting from a strong cache coherence protocol.
Reference: [8] <author> R.H. Halstead and T. Fujita. MASA: </author> <title> A Multithreaded Processor Architecture for Parallel Symbolic Computing. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 443-451, </pages> <publisher> IEEE, </publisher> <address> New York, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: This reduces the negative effects of synchronization on processor utilization. This paper describes the architecture of APRIL, a processor designed for large-scale multiprocessing. APRIL builds on previous research on processors for parallel architectures such as HEP [22], MASA <ref> [8] </ref>, P- RISC [19], [14], [15], and [18]. Most of these processors support fine-grain interleaving of instruction streams from multiple threads, but suffer from poor single- thread performance. In the HEP, for example, instructions from a single thread can only be executed once every 8 cycles. <p> The PC chain represents the instruction addresses corresponding to a thread, and the PSR holds various pieces of process- specific state. Each register set, together with a single PC-chain and PSR, is conceptually grouped into a single entity called a task frame (using terminology from <ref> [8] </ref>). Only one task frame is active at a given time and is designated by a current frame pointer (FP). All register accesses are made to the active register set and instructions are fetched using the active PC-chain. <p> The trap handler executes in the same task frame as the thread that trapped so that it can access all of the thread's registers. 3.1 Coarse-Grain Multithreading In most processor designs to date (e.g. <ref> [8, 22, 19, 15] </ref>), multithreading has involved cycle-by-cycle interleaving of threads. Such fine-grain multithreading has been used to hide memory latency and also to achieve high pipeline utilization. Pipeline dependencies are avoided by maintaining instructions from different threads in the pipeline, at the price of poor single-thread performance.
Reference: [9] <author> Robert H. Halstead. </author> <title> Multilisp: A Language for Parallel Symbolic Computation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-539, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: In APRIL, thread scheduling is done in software, and unlimited virtual dynamic threads are supported. APRIL supports full/empty bit synchronization, and provides tag support for futures <ref> [9] </ref>. In this paper the terms process, thread, context, and task are used equivalently. By taking a systems-level design approach that considers not only the processor, but also the compiler and run-time system, we were able to migrate several noncritical operations into the software system, greatly simplifying processor design.
Reference: [10] <author> J. L. Hennessy and T. R. Gross. </author> <title> Postpass Code Optimization of Pipeline Constraints. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 422-448, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: This simplifies processor design considerably because context switches can be more expensive (4 to 10 cycles), and functionality such as scheduling can be migrated into run-time software. Single-thread performance is optimized, and techniques 2 used in RISC processors for enhancing pipeline perfor-mance can be applied <ref> [10] </ref>. Custom design of a processing element is not required in the ALEWIFE system; indeed, we are using a modified version of a commercial RISC processor for our first-round implementation. 2.2 Programming Model Our experimental programming language for ALEWIFE is Mul-T [16], an extended version of Scheme.
Reference: [11] <author> J. L. Hennessy et al. </author> <title> Hardware/Software Tradeoffs for Increased Performance. </title> <booktitle> In Proc. SIGARCH/SIGPLAN Symp. Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2-11, </pages> <month> March </month> <year> 1982. </year> <booktitle> ACM, </booktitle> <address> Palo Alto, CA. </address>
Reference-contexts: This introduces the restriction that objects in memory cannot be allocated at byte boundaries. This, however, is not a problem because object allocation at word boundaries is favored for other reasons <ref> [11] </ref>.
Reference: [12] <author> M. D. Hill et al. </author> <title> Design Decisions in SPUR. </title> <journal> Computer, </journal> 19(10) 8-22, November 1986. 
Reference-contexts: The basic instruction categories are summarized in Table 1. The remainder of this section describes features of APRIL instructions used for supporting multiprocessing. Data Type Formats APRIL supports tagged pointers for Mul-T, as in the Berkeley SPUR processor <ref> [12] </ref>, by encoding the pointer type in the low order bits of a data word. Associating the type with the pointer has the advantage of saving an additional memory reference when accessing type information. Figure 3 lists the different type encodings.
Reference: [13] <author> Mark Horowitz et al. </author> <title> A 32-Bit Microprocessor with 2K-Byte On-Chip Instruction Cache. </title> <journal> IEEE Journal of Solid-State Circuits, </journal> <month> October </month> <year> 1987. </year>
Reference-contexts: This assumes a single-cycle branch delay slot. Condition codes are set as a side effect of compute instructions. A longer branch delay might be necessary if the branch instruction itself does a compare so that condition codes need not be saved <ref> [13] </ref>; in this case the PC chain is correspondingly longer. Words in memory have a 32 bit data field, and have an additional synchronization bit called the full/empty bit. Use of multiple register sets on the processor, as in the HEP, allows rapid context switching.
Reference: [14] <editor> R.A. Iannucci. </editor> <title> Toward a Dataflow/von Neumann Hybrid Architecture. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, Hawaii, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: This reduces the negative effects of synchronization on processor utilization. This paper describes the architecture of APRIL, a processor designed for large-scale multiprocessing. APRIL builds on previous research on processors for parallel architectures such as HEP [22], MASA [8], P- RISC [19], <ref> [14] </ref>, [15], and [18]. Most of these processors support fine-grain interleaving of instruction streams from multiple threads, but suffer from poor single- thread performance. In the HEP, for example, instructions from a single thread can only be executed once every 8 cycles.
Reference: [15] <author> W. J. Kaminsky and E. S. Davidson. </author> <title> Developing a Multiple-Instruction-Stream Single-Chip Processor. </title> <booktitle> Computer, </booktitle> <pages> 66-78, </pages> <month> December </month> <year> 1979. </year>
Reference-contexts: This reduces the negative effects of synchronization on processor utilization. This paper describes the architecture of APRIL, a processor designed for large-scale multiprocessing. APRIL builds on previous research on processors for parallel architectures such as HEP [22], MASA [8], P- RISC [19], [14], <ref> [15] </ref>, and [18]. Most of these processors support fine-grain interleaving of instruction streams from multiple threads, but suffer from poor single- thread performance. In the HEP, for example, instructions from a single thread can only be executed once every 8 cycles. <p> The trap handler executes in the same task frame as the thread that trapped so that it can access all of the thread's registers. 3.1 Coarse-Grain Multithreading In most processor designs to date (e.g. <ref> [8, 22, 19, 15] </ref>), multithreading has involved cycle-by-cycle interleaving of threads. Such fine-grain multithreading has been used to hide memory latency and also to achieve high pipeline utilization. Pipeline dependencies are avoided by maintaining instructions from different threads in the pipeline, at the price of poor single-thread performance.
Reference: [16] <author> D. Kranz, R. Halstead, and E. Mohr. Mul-T: </author> <title> A High-Performance Parallel Lisp. </title> <booktitle> In Proceedings of SIGPLAN '89, Symposium on Programming Languages Design and Implemenation, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Custom design of a processing element is not required in the ALEWIFE system; indeed, we are using a modified version of a commercial RISC processor for our first-round implementation. 2.2 Programming Model Our experimental programming language for ALEWIFE is Mul-T <ref> [16] </ref>, an extended version of Scheme. Mul-T's basic mechanism for generating concurrent tasks is the future construct. The expression (future X), where X is an arbitrary expression, creates a task to evaluate X and also creates an object known as a future to eventually hold the value of X.
Reference: [17] <author> Eric Mohr, David A. Kranz, and Robert H. Halstead. </author> <title> Lazy task creation: a technique for increas-ing the granularity of parallel tasks. </title> <booktitle> In Proceedings of Symposium on Lisp and Functional Programming, </booktitle> <month> June </month> <year> 1990. </year> <note> To appear. </note>
Reference-contexts: In many programs the possibility of creating an excessive number of fine-grain tasks exists. Our solution to this problem is called lazy task creation <ref> [17] </ref>. With lazy task creation a future expression does not create a new task, but computes the expression as a local procedure call, leaving behind a marker indicating that a new task could have been created.
Reference: [18] <author> Nigel P. Topham, Amos Omondi and Roland N. Ibbett. </author> <title> Context Flow: An Alternative to Conventional Pipelined Architectures. </title> <journal> The Journal of Supercomputing, </journal> <volume> 2(1) </volume> <pages> 29-53, </pages> <year> 1988. </year>
Reference-contexts: This reduces the negative effects of synchronization on processor utilization. This paper describes the architecture of APRIL, a processor designed for large-scale multiprocessing. APRIL builds on previous research on processors for parallel architectures such as HEP [22], MASA [8], P- RISC [19], [14], [15], and <ref> [18] </ref>. Most of these processors support fine-grain interleaving of instruction streams from multiple threads, but suffer from poor single- thread performance. In the HEP, for example, instructions from a single thread can only be executed once every 8 cycles.
Reference: [19] <author> Rishiyur S. Nikhil and Arvind. </author> <booktitle> Can Dataflow Subsume von Neumann Computing? In Proceedings 16th Annual International Symposium on Computer Architecture, IEEE, </booktitle> <address> New York, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: This reduces the negative effects of synchronization on processor utilization. This paper describes the architecture of APRIL, a processor designed for large-scale multiprocessing. APRIL builds on previous research on processors for parallel architectures such as HEP [22], MASA [8], P- RISC <ref> [19] </ref>, [14], [15], and [18]. Most of these processors support fine-grain interleaving of instruction streams from multiple threads, but suffer from poor single- thread performance. In the HEP, for example, instructions from a single thread can only be executed once every 8 cycles. <p> The trap handler executes in the same task frame as the thread that trapped so that it can access all of the thread's registers. 3.1 Coarse-Grain Multithreading In most processor designs to date (e.g. <ref> [8, 22, 19, 15] </ref>), multithreading has involved cycle-by-cycle interleaving of threads. Such fine-grain multithreading has been used to hide memory latency and also to achieve high pipeline utilization. Pipeline dependencies are avoided by maintaining instructions from different threads in the pipeline, at the price of poor single-thread performance.
Reference: [20] <author> Charles L. Seitz. </author> <title> Concurrent VLSI Architectures. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-33(12), </volume> <month> December </month> <year> 1984. </year>
Reference-contexts: ALEWIFE is a cache-coherent machine with distributed, globally- shared memory. Cache coherence is maintained using a directory-based protocol [5] over a low-dimension direct network <ref> [20] </ref>. The directory is distributed with the processing nodes. 2.1 Hardware As shown in Figure 1, each ALEWIFE node consists of a processing element, floating-point unit, cache, main memory, cache/directory controller and a network routing switch. Multiple nodes are connected via a direct, packet-switched network.
Reference: [21] <author> Charles L. Seitz. </author> <title> The Cosmic Cube. </title> <journal> CACM, </journal> <volume> 28(1) </volume> <pages> 22-33, </pages> <month> January </month> <year> 1985. </year>
Reference-contexts: Therefore APRIL continues executing a single thread until a memory operation involving a remote request (or an unsuccessful synchronization attempt) is encountered. The controller forces the processor to switch to another thread, while it services the request. This approach is called coarse-grain multithreading. Processors in message passing multicomputers <ref> [21, 27, 6, 4] </ref> have traditionally taken this approach to allow overlapping of communication with computation. Context switching in APRIL is achieved by changing the frame pointer. Since APRIL has four task frames, it can have up to four threads loaded.
Reference: [22] <author> B.J. Smith. </author> <title> A Pipelined, Shared Resource MIMD Computer. </title> <booktitle> In Proceedings of the 1978 International Conference on Parallel Processing, </booktitle> <pages> pages 6-8, </pages> <year> 1978. </year>
Reference-contexts: Processor utilization also diminishes due to synchronization latency. Spin lock accesses have a low overhead of memory requests, but busy-waiting on a synchronization event wastes processor cycles. Synchro- nization mechanisms that avoid busy-waiting through process blocking incur a high overhead. Full/empty bit synchronization <ref> [22] </ref> in a rapid context switching processor allows efficient fine-grain synchronization. This scheme associates synchronization information with objects at the granularity of a data word, allowing a low-overhead expression of maximum con- currency. <p> This reduces the negative effects of synchronization on processor utilization. This paper describes the architecture of APRIL, a processor designed for large-scale multiprocessing. APRIL builds on previous research on processors for parallel architectures such as HEP <ref> [22] </ref>, MASA [8], P- RISC [19], [14], [15], and [18]. Most of these processors support fine-grain interleaving of instruction streams from multiple threads, but suffer from poor single- thread performance. In the HEP, for example, instructions from a single thread can only be executed once every 8 cycles. <p> The trap handler executes in the same task frame as the thread that trapped so that it can access all of the thread's registers. 3.1 Coarse-Grain Multithreading In most processor designs to date (e.g. <ref> [8, 22, 19, 15] </ref>), multithreading has involved cycle-by-cycle interleaving of threads. Such fine-grain multithreading has been used to hide memory latency and also to achieve high pipeline utilization. Pipeline dependencies are avoided by maintaining instructions from different threads in the pipeline, at the price of poor single-thread performance.
Reference: [23] <author> SPARC Architecture Manual. </author> <year> 1988. </year> <institution> SUN Microsystems, Mountain View, California. </institution>
Reference-contexts: APRIL's simplicity allows an implementation based on minor modifications to an existing RISC processor design. We describe such an implementation based on Sun Microsystem's SPARC processor <ref> [23] </ref>. A compiler for APRIL, a run-time system, and an APRIL simulator are operational. We present simulation results for several parallel applications on APRIL's efficiency in handling fine-grain threads and assess the scalability of multiprocessors based on a coarse-grain multithreaded processor using an analytical model.
Reference: [24] <author> P. A. Steenkiste and J. L. Hennessy. </author> <title> A Simple Interprocedural Register Allocation Algorithm and Its Effectiveness for LISP. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Although we are not using multiple register windows for procedure calls within a single thread, this should not significantly hurt performance <ref> [25, 24] </ref>. To implement coarse-grain multithreading, we use two register windows per task frame a user window and a trap window. The SPARC processor chosen for our implementation has eight register windows, allowing a maximum of four hardware task frames.
Reference: [25] <author> David W. Wall. </author> <title> Global Register Allocation at Link Time. </title> <booktitle> In SIGPLAN '86, Conference on Compiler Construction, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: Although we are not using multiple register windows for procedure calls within a single thread, this should not significantly hurt performance <ref> [25, 24] </ref>. To implement coarse-grain multithreading, we use two register windows per task frame a user window and a trap window. The SPARC processor chosen for our implementation has eight register windows, allowing a maximum of four hardware task frames.
Reference: [26] <author> Wolf-Dietrich Weber and Anoop Gupta. </author> <title> Exploring the Benefits of Multiple Hardware Contexts in a Multiprocessor Architecture: Preliminary Results. </title> <booktitle> In Proceedings 16th Annual International Symposium on Computer Architecture, IEEE, </booktitle> <address> New York, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: We see that as few as three processes yield close to 80% utilization for a ten-cycle context-switch overhead components that affect processor utilization. which corresponds to our initial SPARC-based implementation of APRIL. This result is similar to that reported by Weber and Gupta <ref> [26] </ref> for coarse-grain mul- tithreaded processors. The main reason a low degree of multithreading is sufficient is that context switches are forced only on cache misses, which are expected to happen infrequently. The marginal benefits of additional processes is seen to decrease due to network and cache interference.
Reference: [27] <author> Colin Whitby-Strevens. </author> <title> The Transputer. </title> <booktitle> In Proceedings 12th Annual International Symposium on Computer Architecture, IEEE, </booktitle> <address> New York, </address> <month> June </month> <year> 1985. </year>
Reference-contexts: Therefore APRIL continues executing a single thread until a memory operation involving a remote request (or an unsuccessful synchronization attempt) is encountered. The controller forces the processor to switch to another thread, while it services the request. This approach is called coarse-grain multithreading. Processors in message passing multicomputers <ref> [21, 27, 6, 4] </ref> have traditionally taken this approach to allow overlapping of communication with computation. Context switching in APRIL is achieved by changing the frame pointer. Since APRIL has four task frames, it can have up to four threads loaded.
References-found: 27

