URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/unsupervised.ps.Z
Refering-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Root-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Email: E-mail: fbruf, mschmittg@igi.tu-graz.ac.at  
Title: Unsupervised Learning in Networks of Spiking Neurons Using Temporal Coding  
Author: Berthold Ruf and Michael Schmitt 
Address: Klosterwiesgasse 32/2, A-8010 Graz, Austria  
Affiliation: Institute for Theoretical Computer Science, Technische Universitat Graz  
Abstract: We propose a mechanism for unsupervised learning in networks of spiking neurons which is based on the timing of single firing events. Our results show that a topology preserving behaviour quite similar to that of Kohonen's self-organizing map can be achieved using temporal coding. In contrast to previous approaches, which use rate coding, the winner among competing neurons can be determined fast and locally. Hence our model is a further step towards a more realistic description of unsupervised learning in biological neural systems. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Arbib, M. A.: </author> <title> The Handbook of Brain Theory and Neural Networks. </title> <publisher> MIT Press, </publisher> <address> Cambridge (1995). </address>
Reference-contexts: We extend this idea to a learning mechanism that is closely related to one of the most successful paradigms of unsupervised learning: the self-organizing map (SOM) by Kohonen [6]. Topology preserving maps have been found in many regions of the brain, e.g. in the visual, auditory, or somatosensory cortex <ref> [1] </ref>. The SOM provides a ? To whom correspondence should be addressed. possible explanation how such maps can develop. Previous versions of the SOM assume that the output of a neuron is characterized by its firing rate and not by the timing of single firing events. <p> In order to take the random initialization of the weights into account we computed the relative neighbourhood distortion which is the actual value of M MDS divided by the maximum initial value of M MDS . This yields an initial value from the interval <ref> [0; 1] </ref> and makes the results for different initializations comparable.
Reference: 2. <author> Choe, Y., Miikkulainen, R.: </author> <title> Self-organization and segmentation with laterally connected spiking neurons. </title> <type> Technical Report AI TR 96-251, </type> <institution> Department of Computer Science, University of Texas at Austin, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: In addition to these conventional implementations there has also been some research on biologically more realistic models of self-organizing map algorithms, e.g. by Kohonen [5], Sirosh and Miikkulainen [14], Choe and Miikkulainen <ref> [2] </ref>. Also in these approaches the output of a neuron is assumed to correspond to its firing rate, and learning takes place in terms of this rate after the network has reached a stable state of firing.
Reference: 3. <author> Gerstner, W., van Hemmen, L. H.: </author> <title> How to describe neuronal activity: spikes, rates, </title> <booktitle> or assemblies? In Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo (1994) 463-470. </address>
Reference: 4. <author> Goodhill, G. J., Sejnowski, T. J.: </author> <title> Quantifying neighbourhood preservation in topographic mappings. </title> <booktitle> Proceedings of the 3rd Joint Symposium on Neural Computation, </booktitle> <address> San Diego, CA (1996) 61-82. </address>
Reference-contexts: In a series of computer simulations we have investigated the capability of the model to form topology preserving mappings. For the evaluation of these mappings, instead of relying on visual inspection, we used a measure for quantifying the neighbourhood preservation which was recently studied <ref> [4] </ref>. Our results show that the model exhibits the same characteristic behaviour as the SOM. The typical emergence of topology preserving behaviour could be observed for a wide range of parameters. We also studied the effect of weight normalization. <p> In the first the feedforward weights were normalized after each application of (1), in the second they remained unnormalized. In order to quantify the neighbourhood preservation we used the method of "metric multidimensional scaling" (see e.g. <ref> [4] </ref>) which is based on the measure M MDS = i=1 j&lt;i where N = jSj. The function M represents the mapping of the network, i.e. M (i) is the index of the winner neuron in the competitive layer for input s i .
Reference: 5. <author> Kohonen, T.: </author> <title> Physiological interpretation of the self-organizing map algorithm. </title> <booktitle> Neural Networks 6 (1993) 895-905. </booktitle>
Reference-contexts: In addition to these conventional implementations there has also been some research on biologically more realistic models of self-organizing map algorithms, e.g. by Kohonen <ref> [5] </ref>, Sirosh and Miikkulainen [14], Choe and Miikkulainen [2]. Also in these approaches the output of a neuron is assumed to correspond to its firing rate, and learning takes place in terms of this rate after the network has reached a stable state of firing.
Reference: 6. <author> Kohonen, T.: </author> <title> Self-organizing maps. </title> <publisher> Springer Verlag, </publisher> <address> Berlin (1995). </address>
Reference-contexts: We extend this idea to a learning mechanism that is closely related to one of the most successful paradigms of unsupervised learning: the self-organizing map (SOM) by Kohonen <ref> [6] </ref>. Topology preserving maps have been found in many regions of the brain, e.g. in the visual, auditory, or somatosensory cortex [1]. The SOM provides a ? To whom correspondence should be addressed. possible explanation how such maps can develop.
Reference: 7. <author> Maass, W.: </author> <title> Lower bounds for the computational power of networks of spiking neurons. </title> <booktitle> Neural Computation 8 (1996) 1-40. </booktitle>
Reference: 8. <author> Maass, W.: </author> <title> Fast sigmoidal networks via spiking neurons. </title> <booktitle> Neural Computation 9 (1997) 279-304. </booktitle>
Reference-contexts: In this paper we investigate unsupervised learning processes in SNNs. On the basis of a construction introduced in <ref> [8] </ref> we show how competitive learning can be performed by SNNs using temporal coding. We extend this idea to a learning mechanism that is closely related to one of the most successful paradigms of unsupervised learning: the self-organizing map (SOM) by Kohonen [6]. <p> In Section 3 we propose the mechanism for unsupervised learning in networks of these model neurons. The simulation results are presented in Section 4. 2 The Spiking Neuron Model Recently Maass has shown in <ref> [8] </ref> how leaky integrate-and-fire neurons can compute weighted sums in temporal coding, where the firing time of a neuron encodes a value in the sense that an early firing of the neuron represents a large value. <p> This computation can also be performed on the basis of "competitive temporal coding", such that no explicit reference times T 1 and T 2 are necessary (see <ref> [8] </ref> for details). 3 Unsupervised Learning On the basis of this construction it is now possible to implement various types of unsupervised learning in the context of SNNs as follows: Given a set S of m-dimensional input vectors s l = (s l 1 ; : : : ; s l <p> This work is a further step towards showing that biological neurons can indeed achieve a topology preserving behaviour using a similar learning procedure like the one suggested by Kohonen. As mentioned above we have assumed as in <ref> [8] </ref> that the neurons are of the leaky integrate-and-fire type, where the initial segment of the postsynaptic potentials rises linearly.
Reference: 9. <author> Maass, W.: </author> <title> Networks of spiking neurons: the third generation of neural network models. Neural Networks, to appear; extended abstract in Proceedings of the Seventh Australian Conference on Neural Networks, </title> <note> Canberra (1996) 1-10; available from ftp-host: archive.cis.ohio-state.edu, ftp-filename: /pub/neuroprose/maass.third-generation.ps.Z. </note>
Reference-contexts: Furthermore, spiking neuron networks (SNNs), where the computations are based on this coding scheme, have recently been shown to be computationally more powerful than networks consisting of threshold or sigmoidal gates with respect to time and network complexity <ref> [9] </ref>. In this paper we investigate unsupervised learning processes in SNNs. On the basis of a construction introduced in [8] we show how competitive learning can be performed by SNNs using temporal coding.
Reference: 10. <author> Murray, A., Tarassenko, L.: </author> <title> Analogue Neural VLSI: A Pulse Stream Approach. </title> <publisher> Chapman & Hall, </publisher> <address> London (1994). </address>
Reference-contexts: The model of unsupervised learning that we propose in this paper is therefore a candidate for a more realistic description of fast analogue computation in biological neural systems. Moreover, it also provides a link to possible industrial applications via silicon implementations in pulse coded VLSI <ref> [10] </ref>. The paper is organized as follows: In Section 2 we introduce the formal model of a spiking neuron. In Section 3 we propose the mechanism for unsupervised learning in networks of these model neurons.
Reference: 11. <author> Rieke, F., Warland, D., de Ruyter van Steveninck, R., and Bialek, W.: SPIKES: </author> <title> Exploring the Neural Code. </title> <publisher> MIT Press, </publisher> <address> Cambridge (1996). </address>
Reference: 12. <author> Ruf, B.: </author> <title> Computing functions with networks of spiking neurons. To appear in: </title> <booktitle> Proceedings of the International Work-Conference on Artificial and Biological Neural Networks (1997). </booktitle>
Reference-contexts: As mentioned above we have assumed as in [8] that the neurons are of the leaky integrate-and-fire type, where the initial segment of the postsynaptic potentials rises linearly. Recent simulations <ref> [12] </ref> have shown that even when using a more detailed neural model which includes nonlinear effects and more realistic shapes (e.g. ff-functions) for the postsynaptic potentials, spiking neurons can still compute weighted sums in the way described in Section 2. This indicates that these simplifying assumptions can be dropped.
Reference: 13. <author> Sejnowski, T.: </author> <title> Time for a new neural code? Nature 376 (1995) 21-22. </title>
Reference: 14. <author> Sirosh, J., Miikkulainen, R.: </author> <title> Topographic receptive fields and patterned lateral interaction in a self-organizing model of the primary visual cortex. </title> <booktitle> Neural Computation 9 (1997) 577-594. </booktitle>
Reference-contexts: In addition to these conventional implementations there has also been some research on biologically more realistic models of self-organizing map algorithms, e.g. by Kohonen [5], Sirosh and Miikkulainen <ref> [14] </ref>, Choe and Miikkulainen [2]. Also in these approaches the output of a neuron is assumed to correspond to its firing rate, and learning takes place in terms of this rate after the network has reached a stable state of firing.
References-found: 14

