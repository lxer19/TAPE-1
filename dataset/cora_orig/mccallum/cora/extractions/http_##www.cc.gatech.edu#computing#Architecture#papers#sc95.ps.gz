URL: http://www.cc.gatech.edu/computing/Architecture/papers/sc95.ps.gz
Refering-URL: http://www.cs.gatech.edu/people/home/yanasak/
Root-URL: 
Title: Architectural Mechanisms for Explicit Communication in Shared Memory Multiprocessors  
Author: Umakishore Ramachandran Gautam Shah Anand Sivasubramaniam Aman Singla Ivan Yanasak 
Keyword: Key Words: Shared memory multiprocessors, cache coherence, latency tolerating techniques, synchronization, explicit communication  
Note: To appear in the Proceedings of Supercomputing '95, to be held in San Diego in  This work has been funded in part by NSF grants MIPS-9058430 and MIPS-9200005, and an equipment grant from DEC.  
Address: Atlanta, GA 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Email: e-mail: frama,gautam,anand,aman,yanasakg@cc.gatech.edu  
Phone: Phone: (404) 894-5136 Fax: (404) 894-9442  
Date: December 1995.  
Abstract: The goal of this work is to explore architectural mechanisms for supporting explicit communication in cache-coherent shared memory multiprocessors. The motivation stems from the observation that applications display wide diversity in terms of sharing characteristics and hence impose different communication requirements on the system. Explicit communication mechanisms would allow tailoring the coherence management under software control to match these differing needs and strive to provide a close approximation to a zero overhead machine from the application perspective. Toward achieving these goals, we first analyze the characteristics of sharing observed in certain specific applications. We then use these characteristics to synthesize explicit communication primitives. The proposed primitives allow selectively updating a set of processors, or requesting a stream of data ahead of its intended use. These primitives are essentially generalizations of prefetch and poststore, with the ability to specify the sharer set for poststore either statically or dynamically. The proposed primitives are to be used in conjunction with an underlying invalidation based protocol. Used in this manner, the resulting memory system can dynamically adapt itself to performing either invalidations or updates to match the communication needs. Through application driven performance study we show the utility of these mechanisms in being able to reduce and tolerate communication latencies. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve and M. D. Hill. </author> <title> Weak Ordering ANew Definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Recently, use of some form of relaxed memory consistency model has been proposed as a means to improve performance of cache-based shared memory multiprocessors <ref> [27, 1, 13] </ref>. The basic premise is that most shared memory applications follow some synchronization model and expect consistent views of data only at well-defined synchronization points.
Reference: [2] <author> A. Agarwal et al. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Messages are circuit-switched and are routed along the row until they reach the destination column, upon which they are routed along the column. The above hardware is not significantly different from the node and directory structure of other CC-NUMA machines such as Stanford Dash [27], or MIT Alewife <ref> [2] </ref>. To accommodate the diversity of the explicit communication mechanisms we plan to propose in the next few subsections, we require the memory directory controller to be slightly more flexible.
Reference: [3] <author> R. J. Anderson and J. C. Setubal. </author> <title> On the parallel implementation of goldberg's maximum flow algor ithm. </title> <booktitle> In 4th Annual ACM Symposium on Parallel Algorithms and Archite ctures, </booktitle> <pages> pages 168-77, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: In the implementation <ref> [3] </ref>, each processor accesses a local work queue for tasks to perform. These may in turn generate new tasks which are added to this local work queue. Each task involves read and write accesses to shared data.
Reference: [4] <author> D. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 5(3) </volume> <pages> 63-73, </pages> <year> 1991. </year>
Reference-contexts: 1981 Write request 831 834 1488 837 Invalidate 8450 8454 0 3538 Update 0 0 62757 7680 Table 3: Barnes-Hut (producer-consumer): number of messages sent by a processor 6.2 IS Integer Sort is a kernel that occurs in Numerical Aerodynamic Simulation applications and is part of the NAS benchmark suite <ref> [4] </ref>. The kernel uses a parallel bucket sort to rank a list of integers. The parallel implementation that we study has been described in [31]. The problem size is 32K and the bucket size is 1K.
Reference: [5] <author> P. Bitar and A. M. Despain. </author> <title> Multiprocessor cache synchronization: Issues, innovations, evolution. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 424-433, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Firstly, P1 will not incur a miss penalty for data access subsequent to procuring the lock. This is one of the advantages of combining synchronization with data transfer <ref> [5, 25, 18, 33] </ref>. Secondly and more importantly, the transfer of data from P0 to P1 can be started as soon as the data item is ready to be written and does not have to be delayed until the unlock point.
Reference: [6] <author> H. Cheong and A. V. Veidenbaum. </author> <title> A cache coherence scheme with fast selective invalidation. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 299-307, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Considerably more experience is needed in analyzing applications to determine how much information can be easily gleaned to automate the process of using these primitives in the compiler. The work by Cytron et al [10], Cheong and Veidenbaum <ref> [7, 6] </ref>, and Min and Baer [29] have goals similar to ours in attempting to reduce the global communication by providing software-directed cache coherence. 10 6 Performance Evaluation In this section, we compare the performance of the above-mentioned memory systems using a set of applications.
Reference: [7] <author> H. Cheong and A. V. Veidenbaum. </author> <title> Stale data detection and coherence enforcement using flow analysis. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages I: 138-145, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Considerably more experience is needed in analyzing applications to determine how much information can be easily gleaned to automate the process of using these primitives in the compiler. The work by Cytron et al [10], Cheong and Veidenbaum <ref> [7, 6] </ref>, and Min and Baer [29] have goals similar to ours in attempting to reduce the global communication by providing software-directed cache coherence. 10 6 Performance Evaluation In this section, we compare the performance of the above-mentioned memory systems using a set of applications.
Reference: [8] <author> Cray Research, Inc., </author> <title> Minnesota. The Cray T3D System Architecture Overview Manual, </title> <year> 1993. </year>
Reference-contexts: Further an application might need to switch between protocols as its data sharing and access patterns change. These issues need further investigation and are part of our ongoing work. 7 Related Work There are many related projects that have similar goals to ours. One commercial product, the Cray-T3D <ref> [8] </ref> provides shared memory but does not maintain cache coherence. It is totally up to the system software to ensure coherence of cached data. In terms of primitives for explicit communication, they offer mechanisms to access the remote memory, block transfer, and prefetch.
Reference: [9] <editor> D. E. Culler et al. </editor> <booktitle> Parallel Programming in Split-C. In Proceedings of Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <month> November </month> <year> 1993. </year> <month> 19 </month>
Reference-contexts: For such applications, it would be useful to have a mechanism that adapts itself to a new sharing pattern dynamically and sends updates to the consumers in the new set. For another class of applications such as EM3D <ref> [9] </ref> and CG in the dynamic category, the communication pattern may not be apparent at compile time but the communication becomes deterministic once the program begins execution (i.e. the input data is read in).
Reference: [10] <author> R. Cytron, S. Marlovsky, and K. P. McAuliffe. </author> <title> Automatic management of programmable caches. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages II-229-238, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Considerably more experience is needed in analyzing applications to determine how much information can be easily gleaned to automate the process of using these primitives in the compiler. The work by Cytron et al <ref> [10] </ref>, Cheong and Veidenbaum [7, 6], and Min and Baer [29] have goals similar to ours in attempting to reduce the global communication by providing software-directed cache coherence. 10 6 Performance Evaluation In this section, we compare the performance of the above-mentioned memory systems using a set of applications.
Reference: [11] <author> F. Dahlgren, M. Dubois, and P. Stenstrom. </author> <title> Combined performance gains of simple cache protocol extensions. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 187-197, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The basic protocol in this scheme is update-based; however, a history of redundant updates to each individual cache line is tracked and used by a processor to self-invalidate and stifle future updates to 2 this line. Similarly, Dahlgren et al. <ref> [11] </ref> show that we can benefit by extending hardware cache coherence mechanisms with techniques such as adaptive prefetching and migratory sharing. But such mechanisms which rely entirely on the hardware may not provide the flexibility to adapt to the inherent characteristics of different applications.
Reference: [12] <author> F. Dahlgren and P. Stenstrom. </author> <title> Using write caches to improve performance of cache coherence protocols in shared memory multiprocessors. </title> <type> Technical report, </type> <institution> Dept. of Comp. Eng., Lund Univ., </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Each node has a piece of the shared memory with its associated full-mapped directory information, a private cache, a write buffer [27], and a write merge buffer <ref> [12] </ref>. Figure 2 shows the 5 relevant details of the node architecture. A cache block can exist in one of three states, INVALID, VALID and DIRTY. VALID state is a potentially shared clean state, while DIRTY state is an exclusive state requiring a write-back on cache line replacement. <p> The write buffer is flushed when a processor reaches a synchronization fence. Read operations are assumed to be blocking, and the processor is stalled until the operation is complete. The write-merge-buffer <ref> [12] </ref> is a small associative cache that records updates to a cache block from the processor. It is specific to memory systems that use updates and helps combine updates to the same cache line into a single message to the directory controller.
Reference: [13] <author> M. Dubois, C. Scheurich, and F. Briggs. </author> <title> Memory Access Buffering in Multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: Recently, use of some form of relaxed memory consistency model has been proposed as a means to improve performance of cache-based shared memory multiprocessors <ref> [27, 1, 13] </ref>. The basic premise is that most shared memory applications follow some synchronization model and expect consistent views of data only at well-defined synchronization points.
Reference: [14] <institution> Encore Computer Corporation, </institution> <address> 257 Cedar Hill St., Marlboro, MA 01752. </address> <note> Multimax Technical Summary, </note> <year> 1986. </year>
Reference-contexts: As we mentioned earlier, the consistency model presented by the memory system and the coherence protocols used for the caches combine to keep the overheads associated with locality management low. Early designs <ref> [28, 14] </ref> used sequential consistency [24] for the memory model which presents a uniformly consistent view of shared memory for all processors at all times.
Reference: [15] <author> B. Falsafi et al. </author> <title> Application-Specific Protocols for User-Level Shared Memory. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: Recognizing that a fixed protocol for cache consistency may not suffice, there have been recent proposals [21, 32] to have a menu of coherence protocols implemented in software so that an application can choose the right one depending on its communication pattern. Falsafi el al. <ref> [15] </ref> show that we can gain substantially by matching the coherence protocol to an application's communication pattern and memory semantics. However, there is a concern of increasing the programming complexity by requiring the application developer to choose the right coherence protocol. <p> Since communication is dependent on the input data set, the compiler cannot explicitly give the consumer set when the data item is produced. Dynamically providing an explicit consumer set as is done in <ref> [15] </ref> would involve a substantial change in the application code. However it can be seen that this sharing pattern is just a special case of the dynamic sharing pattern observed in applications such as Barnes-Hut.
Reference: [16] <author> Matthew I. Frank and Mary K. Vernon. </author> <title> A hybrid Shared Memory/Message Passing parallel machine. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Further, there have been several recent proposals to provide message-passing style communication primitives in a shared memory machine <ref> [16, 20, 22] </ref>. We refer to a specific combination of memory model and coherence protocol, together with some explicit communication primitives as a memory system in this paper.
Reference: [17] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P.Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <year> 1990. </year>
Reference-contexts: In this study we will consider Sequential Consistency (SC) [24] and Release Consistency (RC) <ref> [17] </ref> as the two choices for the memory model. Write-invalidate and write-update are the choices for the cache coherence protocol. The explicit communication primitives that we consider are the mechanisms that we proposed in the previous section.
Reference: [18] <author> J. R. Goodman and P. J. Woest. </author> <title> The Wisconsin Multicube: A new large-scale cache-coherent multiprocessor. </title> <booktitle> In Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 422-431, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Firstly, P1 will not incur a miss penalty for data access subsequent to procuring the lock. This is one of the advantages of combining synchronization with data transfer <ref> [5, 25, 18, 33] </ref>. Secondly and more importantly, the transfer of data from P0 to P1 can be started as soon as the data item is ready to be written and does not have to be delayed until the unlock point.
Reference: [19] <author> A. Gupta, J. Hennessy, K. Gharachorloo, T. Mowry, and W-D. Weber. </author> <title> Comparative evaluation of latency reducing and tolerating techniques. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: The appropriateness of a particular technique very much depends on the specifics of the communication pattern in the application, and it has been shown <ref> [19] </ref> that no one technique is universally applicable for all applications.
Reference: [20] <author> J. Heinlein, K. Gharachorloo, S. A. Dresser, and A. Gupta. </author> <title> Integration of Message Passing and Shared Memory in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Further, there have been several recent proposals to provide message-passing style communication primitives in a shared memory machine <ref> [16, 20, 22] </ref>. We refer to a specific combination of memory model and coherence protocol, together with some explicit communication primitives as a memory system in this paper.
Reference: [21] <author> M. Heinrich et al. </author> <title> The performance impact of flexibility in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Our work is in line with the current trend in shared memory multiprocessor design <ref> [32, 21] </ref> that emphasizes the need for flexibility in the choice of the coherence protocol that a machine provides to meet the changing needs of an application. Using an application-driven approach, this research makes several contributions. <p> But such mechanisms which rely entirely on the hardware may not provide the flexibility to adapt to the inherent characteristics of different applications. Recognizing that a fixed protocol for cache consistency may not suffice, there have been recent proposals <ref> [21, 32] </ref> to have a menu of coherence protocols implemented in software so that an application can choose the right one depending on its communication pattern. Falsafi el al. [15] show that we can gain substantially by matching the coherence protocol to an application's communication pattern and memory semantics.
Reference: [22] <author> D. Kranz, K. Johnson, A. Agarwal, J. Kubiatowicz, and B-H Lim. </author> <title> Integrating message-passing and shared memory: Early experience. </title> <booktitle> In Proceedings of the 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Further, there have been several recent proposals to provide message-passing style communication primitives in a shared memory machine <ref> [16, 20, 22] </ref>. We refer to a specific combination of memory model and coherence protocol, together with some explicit communication primitives as a memory system in this paper. <p> The important point to note is that these mechanisms are integrated with the basic underlying directory-based coherence maintenance. In this sense they are very different from the explicit communication primitives such as those proposed in MIT Alewife <ref> [22] </ref> or Stanford Flash [39], in that there is no address space management nor explicit coherence maintenance burden at the application level on the programmer for using our primitives.
Reference: [23] <author> J. Kuskin et al. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: It should be pointed out that we can implement our primitives on top of the Tempest interface. The Stanford Flash project <ref> [23] </ref> implements a shared memory system using a separate protocol processor. However, the primary goal is to reduce the hardware complexity of maintaining the directory structure. Therefore, it implements the traditional directory-based cache coherence protocol in software.
Reference: [24] <author> L. Lamport. </author> <title> How to make a Multiprocessor Computer that Correctly executes Multiprocess Programs. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> C-28(9), </volume> <year> 1979. </year>
Reference-contexts: As we mentioned earlier, the consistency model presented by the memory system and the coherence protocols used for the caches combine to keep the overheads associated with locality management low. Early designs [28, 14] used sequential consistency <ref> [24] </ref> for the memory model which presents a uniformly consistent view of shared memory for all processors at all times. Recently, use of some form of relaxed memory consistency model has been proposed as a means to improve performance of cache-based shared memory multiprocessors [27, 1, 13]. <p> In this study we will consider Sequential Consistency (SC) <ref> [24] </ref> and Release Consistency (RC) [17] as the two choices for the memory model. Write-invalidate and write-update are the choices for the cache coherence protocol. The explicit communication primitives that we consider are the mechanisms that we proposed in the previous section.
Reference: [25] <author> J. Lee and U. Ramachandran. </author> <title> Synchronization with Multiprocessor Caches. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 27-37, </pages> <year> 1990. </year>
Reference-contexts: Firstly, P1 will not incur a miss penalty for data access subsequent to procuring the lock. This is one of the advantages of combining synchronization with data transfer <ref> [5, 25, 18, 33] </ref>. Secondly and more importantly, the transfer of data from P0 to P1 can be started as soon as the data item is ready to be written and does not have to be delayed until the unlock point.
Reference: [26] <author> J. Lee and U. Ramachandran. </author> <title> Architectural Primitives for a Scalable Shared Memory Multiprocessor. </title> <booktitle> In Proceedings of the Third Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 103-114, </pages> <address> Hilton Head, South Carolina, </address> <month> July </month> <year> 1991. </year> <month> 20 </month>
Reference-contexts: Ideally, we would like for the uninterested processors to be dropped from the sharer set. This can be accomplished either by an explicit primitive (such as the reset-update primitive proposed in <ref> [26] </ref>) that can be issued under software control, or an implicit mechanism such as competitive update [30] that allows a processor to self-invalidate a block based on the history of updates to that block.
Reference: [27] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W-D Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Recently, use of some form of relaxed memory consistency model has been proposed as a means to improve performance of cache-based shared memory multiprocessors <ref> [27, 1, 13] </ref>. The basic premise is that most shared memory applications follow some synchronization model and expect consistent views of data only at well-defined synchronization points. <p> Each node has a piece of the shared memory with its associated full-mapped directory information, a private cache, a write buffer <ref> [27] </ref>, and a write merge buffer [12]. Figure 2 shows the 5 relevant details of the node architecture. A cache block can exist in one of three states, INVALID, VALID and DIRTY. <p> Messages are circuit-switched and are routed along the row until they reach the destination column, upon which they are routed along the column. The above hardware is not significantly different from the node and directory structure of other CC-NUMA machines such as Stanford Dash <ref> [27] </ref>, or MIT Alewife [2]. To accommodate the diversity of the explicit communication mechanisms we plan to propose in the next few subsections, we require the memory directory controller to be slightly more flexible.
Reference: [28] <author> T. Lovett and S. Thakkar. </author> <title> The symmetry multiprocessor system. </title> <booktitle> In Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <pages> pages 303-310, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: As we mentioned earlier, the consistency model presented by the memory system and the coherence protocols used for the caches combine to keep the overheads associated with locality management low. Early designs <ref> [28, 14] </ref> used sequential consistency [24] for the memory model which presents a uniformly consistent view of shared memory for all processors at all times.
Reference: [29] <author> S. L. Min and J-L. Baer. </author> <title> A Timestamp-based Cache Coherence Scheme. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing, </booktitle> <pages> pages I: 23-32, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: Considerably more experience is needed in analyzing applications to determine how much information can be easily gleaned to automate the process of using these primitives in the compiler. The work by Cytron et al [10], Cheong and Veidenbaum [7, 6], and Min and Baer <ref> [29] </ref> have goals similar to ours in attempting to reduce the global communication by providing software-directed cache coherence. 10 6 Performance Evaluation In this section, we compare the performance of the above-mentioned memory systems using a set of applications.
Reference: [30] <author> H. Nilsson, P. Stenstrom, and M. Dubois. </author> <title> Implementation and evaluation of update-based cache protocols under relaxed memory consistency models. </title> <type> Technical report, </type> <institution> Dept. of Comp. Eng., Lund Univ., </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: Invalidations are useful when an application changes its sharing pattern, and updates are useful to effect direct communication once a sharing pattern is established. By dynamically switching between updating and invalidating, the competitive update <ref> [30] </ref> scheme attempts to get the advantages of both schemes. The basic protocol in this scheme is update-based; however, a history of redundant updates to each individual cache line is tracked and used by a processor to self-invalidate and stifle future updates to 2 this line. <p> Ideally, we would like for the uninterested processors to be dropped from the sharer set. This can be accomplished either by an explicit primitive (such as the reset-update primitive proposed in [26]) that can be issued under software control, or an implicit mechanism such as competitive update <ref> [30] </ref> that allows a processor to self-invalidate a block based on the history of updates to that block.
Reference: [31] <author> U. Ramachandran, G. Shah, S. Ravikumar, and J. Muthukumarasamy. </author> <title> Scalability study of the KSR-1. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <pages> pages I-237-240, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: The kernel uses a parallel bucket sort to rank a list of integers. The parallel implementation that we study has been described in <ref> [31] </ref>. The problem size is 32K and the bucket size is 1K. It should be noted that this implementation has very good scalability on KSR-2 for large problem sizes (1 M).
Reference: [32] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Our work is in line with the current trend in shared memory multiprocessor design <ref> [32, 21] </ref> that emphasizes the need for flexibility in the choice of the coherence protocol that a machine provides to meet the changing needs of an application. Using an application-driven approach, this research makes several contributions. <p> But such mechanisms which rely entirely on the hardware may not provide the flexibility to adapt to the inherent characteristics of different applications. Recognizing that a fixed protocol for cache consistency may not suffice, there have been recent proposals <ref> [21, 32] </ref> to have a menu of coherence protocols implemented in software so that an application can choose the right one depending on its communication pattern. Falsafi el al. [15] show that we can gain substantially by matching the coherence protocol to an application's communication pattern and memory semantics. <p> Similarly, there is no guarantee in KSR-2 that a poststore will result in a data item being placed in a specific remote cache. The approach used in the Tempest project <ref> [32] </ref> from University of Wisconsin comes closest to our approach. They propose an interface for developing either shared memory or message passing programs. The interface is fairly general and is independent of the underlying hardware. For shared memory programming, the interface allows realizing a variety of coherence protocols.
Reference: [33] <institution> Kendall Square Research. Technical summary, </institution> <year> 1992. </year>
Reference-contexts: Firstly, P1 will not incur a miss penalty for data access subsequent to procuring the lock. This is one of the advantages of combining synchronization with data transfer <ref> [5, 25, 18, 33] </ref>. Secondly and more importantly, the transfer of data from P0 to P1 can be started as soon as the data item is ready to be written and does not have to be delayed until the unlock point. <p> It is totally up to the system software to ensure coherence of cached data. In terms of primitives for explicit communication, they offer mechanisms to access the remote memory, block transfer, and prefetch. Another shared memory multiprocessor, the KSR-2 <ref> [33] </ref>, uses a hardware write-invalidate protocol to maintain sequential consistency. The KSR-2 provides prefetch and poststore primitives to overlap computation with communication. However, they are not as general as the mechanisms we propose.
Reference: [34] <author> J. P. Singh, W-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1991. </year>
Reference-contexts: The results presented are for 16 processors. 6.1 Barnes-Hut Barnes-Hut is an N-body simulation application. The application simulates over time the movement of these bodies due to the gravitational forces exerted on one another, given some set of initial conditions. The parallel implementation <ref> [34] </ref> statically allocates a set of bodies to each processor and goes through three phases for each simulated time step. The regions of space are organized in a tree data structure with the bodies forming the leaves. <p> The sparse nature of the input matrix results in an algorithm with a data dependent dynamic access pattern. The algorithm requires an initial symbolic factorization of the input matrix which is done sequentially because it requires only a small fraction of the total compute time. Only numerical factorization <ref> [34] </ref> is parallelized and analyzed. Sets of columns having similar non-zero structure are combined into supernodes at the end of symbolic factorization. Processors get tasks from a central task queue. Each supernode is a potential task which is used to modify subsequent supernodes.
Reference: [35] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> An Approach to Scalability Study of Shared Memory Parallel Systems. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1994 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 171-180, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: However, there are several applications with dynamic communication characteristics where the sharing information is simply not available at the time the data is produced. For instance, in applications such as IS and CHOLESKY where there is no lock contention <ref> [35] </ref>, the producer cannot effect the data transfer since it does not know the next consumer. For such scenarios, we need to resort to receiver-initiated communication where the consumer explicitly requests the data item. <p> We simulate the relevant details of the shared memory hardware on top of SPASM <ref> [35, 36] </ref>, an execution-driven parallel architecture simulator. Table 1 gives the specific parameters of the base hardware used in the simulation. Our focus is on designing and evaluating mechanisms for tolerating communication overhead resulting from true sharing in applications. <p> Synchronization occurs in locking the task queue when fetching or adding tasks, and locking columns when they are being modified. The dynamic access pattern of the application precludes the use of an explicit static primitive such as PSET WRITE. Further, an earlier scalability study of this application <ref> [35] </ref> revealed that there is little lock contention, suggesting that we may not hope to gain significantly by the use of SYNC WRITE.
Reference: [36] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> A Simulation-based Scalability Study of Parallel Systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 411-426, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: We simulate the relevant details of the shared memory hardware on top of SPASM <ref> [35, 36] </ref>, an execution-driven parallel architecture simulator. Table 1 gives the specific parameters of the base hardware used in the simulation. Our focus is on designing and evaluating mechanisms for tolerating communication overhead resulting from true sharing in applications.
Reference: [37] <author> C. P. Thacker and L. C. Stewart. Firefly: </author> <title> A Multiprocessor Workstation. </title> <booktitle> In Proceedings of the First International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-172, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: It may also have to stall if the buffer is non-empty at a release point. RCupd: This memory system uses RC memory model, a simple write-update protocol not unlike the one used in Firefly <ref> [37] </ref> for state transitions, and no explicit communication primitives. From the point of view of the processor, 8 writes are handled exactly similarly as in RCinv. At a release point, the processor stalls until the pending writes in the write-buffer and the write-merge-buffer are complete.
Reference: [38] <author> W-D. Weber and A. Gupta. </author> <title> Exploring the benefits of multiple hardware contexts in a multiprocessor architecture: Preliminary Results. </title> <booktitle> In Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 273-280, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: To cut down on the number of messages, and to enable streaming of the data back to the requesting processor it would be beneficial to have a prefetch-like primitive that is decoupled from the cache line size. Multithreading <ref> [38] </ref> is another latency tolerating technique that can be useful in some cases where the prefetch cannot be initiated early enough. But this issue is beyond the scope of this paper and we do not discuss it further.
Reference: [39] <author> S. C. Woo, J. P. Singh, and J. L. Hennessy. </author> <title> The performance advantages of integrating block data transfer in cache-coherent multiprocessors. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year> <pages> 21 22 </pages>
Reference-contexts: The important point to note is that these mechanisms are integrated with the basic underlying directory-based coherence maintenance. In this sense they are very different from the explicit communication primitives such as those proposed in MIT Alewife [22] or Stanford Flash <ref> [39] </ref>, in that there is no address space management nor explicit coherence maintenance burden at the application level on the programmer for using our primitives.
References-found: 39

