URL: http://www.cs.huji.ac.il/labs/learning/Papers/apfa.ps.gz
Refering-URL: http://www.cs.huji.ac.il/labs/learning/Papers/MLT_list.html
Root-URL: http://www.cs.huji.ac.il
Email: danar@theory.lcs.mit.edu  singer@research.att.com  tishby@cs.huji.ac.il  
Title: On the Learnability and Usage of Acyclic Probabilistic Finite Automata  
Author: Dana Ron Yoram Singer Naftali Tishby 
Address: Cambridge, MA 02139  600 Mountian Avenue Murray Hill, NJ 07974  Jerusalem 91904, Israel  
Affiliation: Laboratory of Computer Science MIT  AT&T Bell Laboratories  Institute of Computer Science and Center for Neural Computation Hebrew University,  
Abstract: We propose and analyze a distribution learning algorithm for a subclass of Acyclic Probabilistic Finite Automata (APFA). This subclass is characterized by a certain distinguishability property of the automata's states. Though hardness results are known for learning distributions generated by general APFAs, we prove that our algorithm can indeed efficiently learn distributions generated by the subclass of APFAs we consider. In particular, we show that the KL-divergence between the distribution generated by the target source and the distribution generated by our hypothesis can be made small with high confidence in polynomial time. We present two applications of our algorithm. In the first, we show how to model cursively written letters. The resulting models are part of a complete cursive handwriting recognition system. In the second application we demonstrate how APFAs can be used to build multiple-pronunciation models for spoken words. We evaluate the APFA based pronunciation models on labeled speech data. The good performance (in terms of the log-likelihood obtained on test data) achieved by the APFAs and the incredibly small amount of time needed for learning suggests that the learning algorithm of APFAs might be a powerful alternative to commonly used probabilistic models. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Abe and M. Warmuth. </author> <title> On the computational complexity of approximating distributions by probabilistic automata. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 205-260, </pages> <year> 1992. </year> <month> 25 </month>
Reference-contexts: However, the commonly used training procedure for HMMs which is based on the forward-backward algorithm [2] is guaranteed to converge only to a local maximum of the likelihood function. Furthermore, there are theoretical results indicating that the problem of learning distributions generated by HMMs is hard <ref> [1, 8] </ref>. In addition, the successful applications of the HMM approach occur mostly in cases where its full power is not utilized, and the hypothesis constructed is essentially a PFA (or even an APFA). <p> 0 2 Q is the starting state; * q f =2 Q is the final state; * is a finite alphabet; * =2 is the final symbol ; * t : Q fi fg ! Q fq f g is the transition function; * fl : Q fi fg ! <ref> [0; 1] </ref> is the next symbol probability function. The function fl must satisfy the following requirement: for every q 2 Q, P We allow the transition function t to be undefined only on states q and symbols , for which fl (q; ) = 0.
Reference: [2] <author> L. E. Baum and T. Petrie. </author> <title> Statistical inference for probabilistic functions of finite state markov chains. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 37, </volume> <year> 1966. </year>
Reference-contexts: HMMs (which PFAs are a special case of) are popular in speech recognition and have better ability than the string matching based techniques to capture context dependent variations. However, the commonly used training procedure for HMMs which is based on the forward-backward algorithm <ref> [2] </ref> is guaranteed to converge only to a local maximum of the likelihood function. Furthermore, there are theoretical results indicating that the problem of learning distributions generated by HMMs is hard [1, 8].
Reference: [3] <author> Y. Bengio, Y. le Cun, and D. Henderson. </author> <title> Globally trained handwritten word recognizer using spatial representation, convolutional neural networks, and hidden Markov models. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Examples and reviews on practical models and algorithms for multiple-pronunciation can be found in [5, 13], and for cursive handwriting recognition in <ref> [10, 9, 19, 3] </ref>. Organization of the Paper The paper is organized as follows. In Sections 2 and 3 we give several definitions related to APFAs, and define our learning model. In Section 4 we present our learning algorithm.
Reference: [4] <author> R. C. Carrasco and J. Oncina. </author> <title> Learning stochastic regular grammars by means of a state merging method. </title> <booktitle> In The 2nd Intl. Collo. on Grammatical Inference and Applications, </booktitle> <pages> pages 139-152, </pages> <year> 1994. </year>
Reference-contexts: Another drawback of HMMs is that the current HMM training algorithms are neither online nor adaptive in the model's topology. A technique of merging states which is similar to the one used in this paper was also applied by Carrasco and Oncina <ref> [4] </ref>, and by Stolcke and Omohundro [18]. Carrasco and Oncina give an algorithm which identifies in the limit distributions generated by PFAs. Stolcke and Omohundro describe a learning algorithm for HMMs which merges states based on a Bayesian approach, and apply their algorithm to build pronunciation models for spoken words. <p> Acknowledgements We would like to thank an anonymous COLT'95 committee member for her/his careful reading and very helpful comments. Special thanks to Andreas Stolcke for helpful comments and for pointing us to reference <ref> [4] </ref>. We would also like to thank Ilan Kremer, Yoav Freund, Mike Kearns, Ronitt Rubinfeld, and Rob Schapire for helpful discussions. This research has been supported in part by the Israeli Ministry of Sciences and Arts and by the Bruno Goldberg endowment fund.
Reference: [5] <author> F. R. Chen. </author> <title> Identification of contextual factos for pronounciation networks. </title> <booktitle> In Proc. of IEEE Conf. on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 753-756, </pages> <year> 1990. </year>
Reference-contexts: Stolcke and Omohundro describe a learning algorithm for HMMs which merges states based on a Bayesian approach, and apply their algorithm to build pronunciation models for spoken words. Examples and reviews on practical models and algorithms for multiple-pronunciation can be found in <ref> [5, 13] </ref>, and for cursive handwriting recognition in [10, 9, 19, 3]. Organization of the Paper The paper is organized as follows. In Sections 2 and 3 we give several definitions related to APFAs, and define our learning model. In Section 4 we present our learning algorithm.
Reference: [6] <author> Y. Freund, M. Kearns, D. Ron, R. Rubinfeld, R. E. Schapire, and L. Sellie. </author> <title> Efficient learning of typical finite automata from random walks. </title> <booktitle> In Proceedings of the 24th Annual ACM Symp. on Theory of Computing, </booktitle> <pages> pages 315-324, </pages> <year> 1993. </year>
Reference-contexts: This technique was presented in the pioneering work of Trakhtenbrot and Brazdin' [20] in the context of learning deterministic finite automata (DFAs). The same idea was later applied by Freund et. al. <ref> [6] </ref> in their work on learning typical DFAs 1 .
Reference: [7] <author> W. Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> American Statistical Association Journal, </journal> <volume> 58 </volume> <pages> 13-30, </pages> <year> 1963. </year>
Reference-contexts: There are at most 1=* 0 states in each of the D levels for which P M 0 (q 0 ) * 0 , and hence, using Hoeffding's inequality <ref> [7] </ref> and the fact that m 1 2* 2 ln (2D=(* 0 ffi)) 2 jjn 2 D , with probability at least 1 (ffi=2)2 (jjn 2 D) , for each such q 0 , m q 0 =m P M 0 (q 0 ) * 0 .
Reference: [8] <author> M. Kearns, Y. Mansour, D. Ron, R. Rubinfeld, R. E. Schapire, and L. Sellie. </author> <title> On the learnability of discrete distributions. </title> <booktitle> In The 25th Annual ACM Symp. on Theory of Computing, </booktitle> <year> 1994. </year>
Reference-contexts: The learning algorithm is efficient in the sense that its running time is polynomial in the parameters of the problem. Our result should be contrasted with the intractability result for learning PFAs described by Kearns et. al. <ref> [8] </ref>. They show that PFAs are not efficiently learnable under the widely acceptable assumption that there is no efficient algorithm for learning parity functions in the presence of noise in the PAC model. <p> However, the commonly used training procedure for HMMs which is based on the forward-backward algorithm [2] is guaranteed to converge only to a local maximum of the likelihood function. Furthermore, there are theoretical results indicating that the problem of learning distributions generated by HMMs is hard <ref> [1, 8] </ref>. In addition, the successful applications of the HMM approach occur mostly in cases where its full power is not utilized, and the hypothesis constructed is essentially a PFA (or even an APFA). <p> Every state is copied at most D 1 times, therefore the total number of states in f M is at most n (D 1). 3 The Learning Model In this section we describe our learning model which is similar to the one introduced in <ref> [8] </ref>.
Reference: [9] <editor> R. Plamondon and C. G. Leedham, editors. </editor> <booktitle> Computer Processing of Handwriting. World Scientific, </booktitle> <year> 1990. </year>
Reference-contexts: Examples and reviews on practical models and algorithms for multiple-pronunciation can be found in [5, 13], and for cursive handwriting recognition in <ref> [10, 9, 19, 3] </ref>. Organization of the Paper The paper is organized as follows. In Sections 2 and 3 we give several definitions related to APFAs, and define our learning model. In Section 4 we present our learning algorithm.
Reference: [10] <author> R. Plamondon, C.Y Suen, and M. L. Simner, </author> <title> editors. Computer Recognition and Human Production of Handwriting. </title> <publisher> World Scientific, </publisher> <year> 1989. </year>
Reference-contexts: Examples and reviews on practical models and algorithms for multiple-pronunciation can be found in [5, 13], and for cursive handwriting recognition in <ref> [10, 9, 19, 3] </ref>. Organization of the Paper The paper is organized as follows. In Sections 2 and 3 we give several definitions related to APFAs, and define our learning model. In Section 4 we present our learning algorithm.
Reference: [11] <author> L. R. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <booktitle> Proc. of the IEEE, </booktitle> <year> 1989. </year>
Reference-contexts: Other Related Work The most common approaches to the modeling and recognition of sequences such as those studied in this paper are string matching algorithms (e.g. Dynamic Time Warping [15]) on the one hand, and Hidden Markov Models (in particular left-to-right HMMs) on the other hand <ref> [11, 12] </ref>. The string matching approach usually assumes the existence of a sequence prototype (reference template) together with a local noise model, from which the probabilities of deletions, insertions, and substitutions, can be deduced.
Reference: [12] <author> L.R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden markov models. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 3(1) </volume> <pages> 4-16, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: Other Related Work The most common approaches to the modeling and recognition of sequences such as those studied in this paper are string matching algorithms (e.g. Dynamic Time Warping [15]) on the one hand, and Hidden Markov Models (in particular left-to-right HMMs) on the other hand <ref> [11, 12] </ref>. The string matching approach usually assumes the existence of a sequence prototype (reference template) together with a local noise model, from which the probabilities of deletions, insertions, and substitutions, can be deduced.
Reference: [13] <author> M. D. Riley. </author> <title> A statistical model for generating pronounication networks. </title> <booktitle> In Proc. of IEEE Conf. on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 737-740, </pages> <year> 1991. </year>
Reference-contexts: Stolcke and Omohundro describe a learning algorithm for HMMs which merges states based on a Bayesian approach, and apply their algorithm to build pronunciation models for spoken words. Examples and reviews on practical models and algorithms for multiple-pronunciation can be found in <ref> [5, 13] </ref>, and for cursive handwriting recognition in [10, 9, 19, 3]. Organization of the Paper The paper is organized as follows. In Sections 2 and 3 we give several definitions related to APFAs, and define our learning model. In Section 4 we present our learning algorithm.
Reference: [14] <author> D. Ron, Y. Singer, and N. Tishby. </author> <title> Learning probabilistic automata with variable memory length. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Computational Learning Theory, </booktitle> <year> 1994. </year> <note> (To appear in Machine Learning). </note>
Reference-contexts: In the first application we construct models for cursive handwritten letters, and in the second we build pronunciation models for spoken words. These application use in part an online version of our algorithm which is also given in this paper. In a previous work <ref> [14] </ref> we introduced an algorithm for learning distributions (on long strings) generated by ergodic Markovian sources that can be described by a different subclass of probabilistic finite automata (PFAs) which we refer to as Variable Memory PFAs. Our two learning algorithm complement each other. <p> Here we briefly demonstrate how a new word can be broken into its different letter constituents. Recognition of completely unlabeled data is more involved, but can be performed efficiently using a higher level language model (see <ref> [14] </ref> for an example of such a model). A complete description of the cursive handwriting recognition system is given in [16]. When a transcription of a cursively written word (i.e., the letters that constitute the word) is given, we find the most likely segmentation of that word as follows.
Reference: [15] <author> D. Sankoff and J. B. Kruskal. </author> <title> Time warps, string edits and macromolecules: the theory and practice of sequence comparison. </title> <publisher> Addison-Wesley, </publisher> <address> Reading Mass, </address> <year> 1983. </year>
Reference-contexts: Other Related Work The most common approaches to the modeling and recognition of sequences such as those studied in this paper are string matching algorithms (e.g. Dynamic Time Warping <ref> [15] </ref>) on the one hand, and Hidden Markov Models (in particular left-to-right HMMs) on the other hand [11, 12].
Reference: [16] <author> Y. Singer. </author> <title> "What has been will be again": A Machine Learning Approach to the Analysis of Natural Language. </title> <type> PhD thesis, </type> <institution> The Hebrew University of Jerusalem, </institution> <year> 1995. </year> <month> 26 </month>
Reference-contexts: Our two learning algorithm complement each other. Whereas the variable memory PFAs capture the long range, stationary, statistical properties of the source, the APFAs capture the short sequence statistics. Together, these algorithm constitute a complete language modeling scheme, which we applied to cursive handwriting recognition <ref> [16] </ref>. The algorithm described in this paper is an efficient algorithm for learning distributions on strings generated by all APFAs M which have the following property. <p> to each new trial node is O (jjD) since we need only consider nodes on the path corresponding to the trail string, and their successors. 2 7 Applications A slightly modified version of our learning algorithm was applied and tested on various problems such as: stochastic modeling of cursive handwriting <ref> [16] </ref>, locating noun phrases in natural English text, and building multiple-pronunciation models for spoken words from their phonetic transcription. This modified version of the algorithm allows folding states from different levels, thus the resulting hypothesis is more compact. <p> Recognition of completely unlabeled data is more involved, but can be performed efficiently using a higher level language model (see [14] for an example of such a model). A complete description of the cursive handwriting recognition system is given in <ref> [16] </ref>. When a transcription of a cursively written word (i.e., the letters that constitute the word) is given, we find the most likely segmentation of that word as follows. The segmentation partitions the motor control commands into non-overlapping segments, where each segment corresponds to a different letter.
Reference: [17] <author> Y. Singer and N. Tishby. </author> <title> Dynamical encoding of cursive handwriting. </title> <journal> Biological Cybernetics, </journal> <volume> 71(3) </volume> <pages> 227-237, </pages> <year> 1994. </year>
Reference-contexts: Here we give a brief overview of the usage of acyclic PFAs and their learning scheme for the following applications: (a) A part of a complete cursive handwriting recognition system (b) Pronunciation models for spoken words. 7.1 Building Stochastic Models for Cursive Handwriting In <ref> [17] </ref>, a dynamic encoding scheme for cursive handwriting based on an oscillatory model of handwriting was proposed and analysed. The process described in [17] performs inverse mapping from continuous pen trajectories to strings over a discrete set of symbols which efficiently encode cursive handwriting. <p> the following applications: (a) A part of a complete cursive handwriting recognition system (b) Pronunciation models for spoken words. 7.1 Building Stochastic Models for Cursive Handwriting In <ref> [17] </ref>, a dynamic encoding scheme for cursive handwriting based on an oscillatory model of handwriting was proposed and analysed. The process described in [17] performs inverse mapping from continuous pen trajectories to strings over a discrete set of symbols which efficiently encode cursive handwriting. These symbols are named motor control commands.
Reference: [18] <author> A. Stolcke and S. Omohundro. </author> <title> Hidden Markov model induction by Bayesian model merging. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Another drawback of HMMs is that the current HMM training algorithms are neither online nor adaptive in the model's topology. A technique of merging states which is similar to the one used in this paper was also applied by Carrasco and Oncina [4], and by Stolcke and Omohundro <ref> [18] </ref>. Carrasco and Oncina give an algorithm which identifies in the limit distributions generated by PFAs. Stolcke and Omohundro describe a learning algorithm for HMMs which merges states based on a Bayesian approach, and apply their algorithm to build pronunciation models for spoken words. <p> This probability depends on the distribution of the different speakers that uttered the words in the training set. Such models can be used as a component in a speech recognition system. The same problem was studied in <ref> [18] </ref>. Here, we briefly discuss how our algorithm for learning APFAs can be used to efficiently build probabilistic pronunciation models for words. We used the TIMIT (Texas Instruments-MIT) database. <p> In order to quantitatively check the performance of the models, we filtered and partitioned the data in the same way as in <ref> [18] </ref>. That is, words occurring between 20 and 100 times in the data set were used for training and evaluation according to the following partition. 75% of the occurrences of each word were used as training data for the learning algorithm and the remaining 25% were used for evaluation. <p> The results are summarized in Table 1. The performance of the resulting APFAs is surprisingly good, compared to the performance of the Hidden Markov Model reported in <ref> [18] </ref>. To be cautious, we note that it is 24 not certain whether the better performance (in the sense that the likelihood of the APFAs on the test data is higher) indeed indicates better performance in terms of recognition error rate. <p> Yet, the much smaller time needed for the learning suggests that our algorithm might be the tool of choice for this problem when large amounts of training data are presented. Model Log-Likelihood Perplexity States Transitions Training Time APFA -2142.8 1.563 1398 2197 23 seconds HMM <ref> [18] </ref> -2343.0 1.849 1204 1542 29:49 minutes Table 1: The performance of APFAs compared to Hidden Markov Models (HMM) as reported in [18] by Stolcke and Omohundro. <p> Model Log-Likelihood Perplexity States Transitions Training Time APFA -2142.8 1.563 1398 2197 23 seconds HMM <ref> [18] </ref> -2343.0 1.849 1204 1542 29:49 minutes Table 1: The performance of APFAs compared to Hidden Markov Models (HMM) as reported in [18] by Stolcke and Omohundro.
Reference: [19] <author> C. C. Tappert, C. Y. Suen, and T. Wakahara. </author> <title> The state of art in on-line handwriting recognition. </title> <journal> IEEE Trans. on Pat. Anal. and Mach. Int., </journal> <volume> 12(8) </volume> <pages> 787-808, </pages> <year> 1990. </year>
Reference-contexts: Examples and reviews on practical models and algorithms for multiple-pronunciation can be found in [5, 13], and for cursive handwriting recognition in <ref> [10, 9, 19, 3] </ref>. Organization of the Paper The paper is organized as follows. In Sections 2 and 3 we give several definitions related to APFAs, and define our learning model. In Section 4 we present our learning algorithm.
Reference: [20] <author> B. A. Trakhtenbrot and Ya. M. Brazdin'. </author> <title> Finite Automata: Behavior and Synthesis. </title> <publisher> North-Holland, </publisher> <year> 1973. </year> <month> 27 </month>
Reference-contexts: One of the key techniques applied in this work is that of using some form of signatures of states in order to distinguish between the states of the target automaton. This technique was presented in the pioneering work of Trakhtenbrot and Brazdin' <ref> [20] </ref> in the context of learning deterministic finite automata (DFAs). The same idea was later applied by Freund et. al. [6] in their work on learning typical DFAs 1 .
References-found: 20

