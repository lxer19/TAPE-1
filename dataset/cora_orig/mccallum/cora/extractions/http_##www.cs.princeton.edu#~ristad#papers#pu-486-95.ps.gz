URL: http://www.cs.princeton.edu/~ristad/papers/pu-486-95.ps.gz
Refering-URL: http://www.cs.princeton.edu/~ristad/papers/pu-486-95.html
Root-URL: http://www.cs.princeton.edu
Title: Nonmonotonic Extension Models  
Author: Eric Sven Ristad Robert G. Thomas 
Abstract: Research Report CS-TR-486-95 February 1995 Abstract We introduce three new techniques for statistical language models: extension modeling, nonmonotonic contexts, and the divergence heuristic. Together these techniques result in language models that have few states, even fewer parameters, and low message entropies. For example, our techniques achieve a message entropy of 1.97 bits/char on the Brown corpus using only 89,325 parameters. In contrast, the character 4-gram model requires more than 250 times as many parameters in order to achieve a message entropy of only 2.47 bits/char. The fact that our model performs significantly better while using vastly fewer parameters indicates that it is a better probability model of natural language text. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bell, T. C., Cleary, J. G., and Witten, I. H. </author> <title> Text Compression. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: Formally, an extension model = h; D; E; i consists of a finite alphabet , jj = m, a dictionary D of contexts, D fl , a set of available context extensions E, E D fi , and a probability function : E ! <ref> [0; 1] </ref>. For every context w in D, E (w) is the set of symbols available in the context w and (jw) is the conditional probability of the symbol in the context w. Note that P 2 (jw) 1 for all contexts w in the dictionary D.
Reference: [2] <author> Brown, P., Pietra, V. D., Pietra, S. D., Lai, J., and Mercer, R. </author> <title> An estimate of an upper bound for the entropy of English. </title> <booktitle> Computational Linguistics 18 (1992), </booktitle> <pages> 31-40. 23 </pages>
Reference-contexts: The best reported results on the Brown Corpus are 1.75 bits/char using a large interpolated trigram word model whose parameters are estimated using over 600,000,000 words of proprietary training data <ref> [2] </ref>. The use of proprietary training data means that these results are not independently repeatable. In contrast, our results are obtained using only 900,000 words of widely available training data and may be independently verified by anyone with the inclination to do so. <p> For these three reasons repeatability, training corpus size, and the advantage of word models over character models the results reported in <ref> [2] </ref> are not directly comparable to those reported here. Section 3.1 compares the statistical efficiency of the various context model classes.
Reference: [3] <author> Cleary, J., and Witten, I. </author> <title> Data compression using adaptive coding and partial string matching. </title> <journal> IEEE Trans. Comm. COM-32, </journal> <volume> 4 (1984), </volume> <pages> 396-402. </pages>
Reference-contexts: The fact that our model performs significantly better using vastly fewer parameters argues that it is a much better probability model of natural language text. Our first two techniques nonmonotonic contexts and extension modeling are generalizations of the traditional context model <ref> [3, 10, 11] </ref>. Our third technique the divergence heuristic is an incremental model selection criterion based directly on Rissanen's minimum description length (MDL) principle [9]. The MDL principle states that the best model is the simplest model that provides a compact description of the observed data. <p> A dictionary D is monotonic iff it is either prefix or suffix monotonic. Conversely, a dictionary is nonmonotonic iff it is neither prefix nor suffix monotonic. Most context models, such as Rissanen's context algorithms [10, 11] and PPM <ref> [3] </ref>, include all strings whose frequency exceeds a fixed threshold. As a result, such models are both prefix and suffix monotonic. The same is true of the powerful class of interpolated Markov sources introduced by Jelinek and Mercer [6].
Reference: [4] <author> Cleary, J. G., Teahan, W., and Witten, I. H. </author> <title> Unboundedd length contexts for PPM. </title> <booktitle> In Proceedings of 5th Data Compression Conference (Los Alamitos, </booktitle> <address> CA, </address> <month> March 28-30 </month> <year> 1995), </year> <editor> J. Storer and M. Cohn, Eds., </editor> <publisher> IEEE, IEEE Computer Society Press, </publisher> <pages> pp. 52-61. </pages>
Reference-contexts: As a result, such models are both prefix and suffix monotonic. The same is true of the powerful class of interpolated Markov sources introduced by Jelinek and Mercer [6]. More recent context models, such as PPM* <ref> [4] </ref>, are prefix nonmonotonic but remain suffix monotonic. To the best of our knowledge, we are the first to propose a truely nonmonotonic context model [12].
Reference: [5] <author> Francis, W. N., and Kucera, H. </author> <title> Frequency analysis of English usage: lexicon and grammar. </title> <publisher> Houghton Mi*in, </publisher> <address> Boston, </address> <year> 1982. </year>
Reference-contexts: Next, in section 3, we demonstrate the efficacy of our techniques on the Brown Corpus, an eclectic collection of English prose containing approximately one million words of text drawn from 500 different sources and 15 different genres <ref> [5] </ref>. The brutally nonstationary nature of this corpus poses a stringent test for language models. The final section, section 4, briefly discusses related work. Appendix A contains a summary of our notation. 2 Extension Model Class This section consists of four parts. <p> All results are based on the Brown corpus <ref> [5] </ref>, an eclectic collection of English prose drawn from 500 sources across 15 genres. The irregular and nonstationary nature of this corpus is known to pose an exacting test for statistical language models.
Reference: [6] <author> Jelinek, F., and Mercer, R. L. </author> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Pattern Recognition in Practice (Amsterdam, </booktitle> <month> May 21-23 </month> <year> 1980), </year> <editor> E. S. Gelsema and L. N. Kanal, Eds., </editor> <publisher> North Holland, </publisher> <pages> pp. 381-397. </pages>
Reference-contexts: As a result, such models are both prefix and suffix monotonic. The same is true of the powerful class of interpolated Markov sources introduced by Jelinek and Mercer <ref> [6] </ref>. More recent context models, such as PPM* [4], are prefix nonmonotonic but remain suffix monotonic. To the best of our knowledge, we are the first to propose a truely nonmonotonic context model [12].
Reference: [7] <author> Knuth, D. E. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> 1 ed., vol. 1. </volume> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1968. </year>
Reference-contexts: The tree contains n = P m i=1 n i internal vertices and n 0 leaf vertices. There are (n 0 +n 1 +: : :+n m 1)!=n 0 !n 1 ! : : : n m ! such trees <ref> [7, p.587] </ref>. Accordingly, this tree may be encoded with an enumerative code using L (D) bits.
Reference: [8] <author> Moffat, A. </author> <title> A note on the PPM data compression algorithm. </title> <type> Research Report 88/7, </type> <institution> Department of Computer Science, University of Melbourne, </institution> <address> Parkville, Victoria, Australia, </address> <year> 1988. </year>
Reference-contexts: In 2.1, we formally define the class of extension models and prove that they satisfy the axioms of probability. We also classify language models based on the structure of their context dictionary. In 2.2, we show to estimate the parameters of an extension model using Mof-fat's "method C" <ref> [8] </ref>. In 2.3, we provide codelength formulas for our model class, based on efficient enumerative codes. These codelength formulas will be used to match the complexity of the model to the complexity of the data. <p> A (jw) = c (w) + jj Unfortunately, Laplace's rule leads to unnecessarily high model entropies when the alphabet size is large, the true entropy in a given context is small, and the training data is limited. Following Moffat <ref> [8] </ref>, we first partition the conditional event space in a given context w into two subevents: the symbols q (w) that have previously occurred in context w and those that q (w) that have not. Formally, q (w) : : = q (w).
Reference: [9] <author> Rissanen, J. </author> <title> Modeling by shortest data description. </title> <booktitle> Automatica 14 (1978), </booktitle> <pages> 465-471. </pages>
Reference-contexts: Our first two techniques nonmonotonic contexts and extension modeling are generalizations of the traditional context model [3, 10, 11]. Our third technique the divergence heuristic is an incremental model selection criterion based directly on Rissanen's minimum description length (MDL) principle <ref> [9] </ref>. The MDL principle states that the best model is the simplest model that provides a compact description of the observed data. In the traditional context model, every prefix and every suffix of a context is also a context. Three consequences follow from this property.
Reference: [10] <author> Rissanen, J. </author> <title> A universal data compression system. </title> <journal> IEEE Trans. Information Theory IT-29, </journal> <volume> 5 (1983), </volume> <pages> 656-664. </pages>
Reference-contexts: The fact that our model performs significantly better using vastly fewer parameters argues that it is a much better probability model of natural language text. Our first two techniques nonmonotonic contexts and extension modeling are generalizations of the traditional context model <ref> [3, 10, 11] </ref>. Our third technique the divergence heuristic is an incremental model selection criterion based directly on Rissanen's minimum description length (MDL) principle [9]. The MDL principle states that the best model is the simplest model that provides a compact description of the observed data. <p> In contrast, the traditional selection heuristic adds a more specific context to the model only if it's entropy is less than the entropy of the more general context <ref> [10, 11] </ref>. The traditional minimum entropy heuristic is a special case of the more effective and more powerful divergence heuristic. The divergence heuristic allows our models to generalize from the training corpus to the testing corpus, even for nonstationary sources such as the Brown corpus. <p> A dictionary D is monotonic iff it is either prefix or suffix monotonic. Conversely, a dictionary is nonmonotonic iff it is neither prefix nor suffix monotonic. Most context models, such as Rissanen's context algorithms <ref> [10, 11] </ref> and PPM [3], include all strings whose frequency exceeds a fixed threshold. As a result, such models are both prefix and suffix monotonic. The same is true of the powerful class of interpolated Markov sources introduced by Jelinek and Mercer [6]. <p> We include three variants of the context model: our nonmonotonic context model (NCM) as well as Rissanen's original two context models (CM 1 <ref> [10] </ref> and CM 2 [11]). The comparison is based on two criteria of statistical efficiency: bits/parameter on the test message, and bits/order on the test message. <p> We sought to recast Rissanen's online universal data compression algorithm <ref> [10, 11] </ref> in an o*ine setting using the traditional MDL criterion. <p> Our central technical contributions to Rissanen's pioneering work are to generalize the context model class and to better manage the interaction among contexts. In Rissanen's first model selection algorithm <ref> [10] </ref>, contexts are added to the model if they make strong predictions, ie., they have low entropy. In Rissanen's second model selection algorithm [11], a context w and all of its siblings w are added to the model if they reduce the codelength of their parent context w.
Reference: [11] <author> Rissanen, J. </author> <title> Complexity of strings in the class of Markov sources. </title> <journal> IEEE Trans. Information Theory IT-32, </journal> <volume> 4 (1986), </volume> <pages> 526-532. </pages>
Reference-contexts: The fact that our model performs significantly better using vastly fewer parameters argues that it is a much better probability model of natural language text. Our first two techniques nonmonotonic contexts and extension modeling are generalizations of the traditional context model <ref> [3, 10, 11] </ref>. Our third technique the divergence heuristic is an incremental model selection criterion based directly on Rissanen's minimum description length (MDL) principle [9]. The MDL principle states that the best model is the simplest model that provides a compact description of the observed data. <p> In contrast, the traditional selection heuristic adds a more specific context to the model only if it's entropy is less than the entropy of the more general context <ref> [10, 11] </ref>. The traditional minimum entropy heuristic is a special case of the more effective and more powerful divergence heuristic. The divergence heuristic allows our models to generalize from the training corpus to the testing corpus, even for nonstationary sources such as the Brown corpus. <p> A dictionary D is monotonic iff it is either prefix or suffix monotonic. Conversely, a dictionary is nonmonotonic iff it is neither prefix nor suffix monotonic. Most context models, such as Rissanen's context algorithms <ref> [10, 11] </ref> and PPM [3], include all strings whose frequency exceeds a fixed threshold. As a result, such models are both prefix and suffix monotonic. The same is true of the powerful class of interpolated Markov sources introduced by Jelinek and Mercer [6]. <p> We include three variants of the context model: our nonmonotonic context model (NCM) as well as Rissanen's original two context models (CM 1 [10] and CM 2 <ref> [11] </ref>). The comparison is based on two criteria of statistical efficiency: bits/parameter on the test message, and bits/order on the test message. According to the first criteria of statistical efficiency, the best model is the one that achieves the lowest test message entropy using the fewest parameters. <p> We sought to recast Rissanen's online universal data compression algorithm <ref> [10, 11] </ref> in an o*ine setting using the traditional MDL criterion. <p> In Rissanen's first model selection algorithm [10], contexts are added to the model if they make strong predictions, ie., they have low entropy. In Rissanen's second model selection algorithm <ref> [11] </ref>, a context w and all of its siblings w are added to the model if they reduce the codelength of their parent context w. In our estimation algorithm, a context is added to the model if it's conditional probabilities are significantly different than those of its maximal proper suffix.
Reference: [12] <author> Ristad, E. S., and Thomas, R. G. </author> <title> Context models in the MDL framework. </title> <booktitle> In Proceedings of 5th Data Compression Conference (Los Alamitos, </booktitle> <address> CA, </address> <month> March 28-30 </month> <year> 1995), </year> <editor> J. Storer and M. Cohn, Eds., </editor> <publisher> IEEE, IEEE Computer Society Press, </publisher> <pages> pp. 62-71. </pages>
Reference-contexts: Each symbol selects a single context, and the conditional probability of that symbol in that context will be estimated with respect to all histories for which that context is a candidate. We consider the problem of context mixing in other work <ref> [12] </ref>. 2.1.1 Validity Let us now introduce a sufficient condition for an extension model to induce a probabilty function on all strings n of a given length n. <p> More recent context models, such as PPM* [4], are prefix nonmonotonic but remain suffix monotonic. To the best of our knowledge, we are the first to propose a truely nonmonotonic context model <ref> [12] </ref>. Intuitively, a prefix monotonic model is one in which the length of the longest candidate context can increase by at most one symbol at each time step. <p> Therefore, our estimate of (jy) should be conditioned on the fact that the longer context xy did not occur. The interaction between candidate contexts can become quite complex, and we consider this problem in other work <ref> [12] </ref>. Parameter estimation is only a small part of the overall model estimation problem. Not only do we have to estimate the parameters for a model, we have to find the right parameters to use! To do this, we proceed in two steps. <p> Since ^c (jw) can be significantly less than c (jw), we should encode ^c (j) instead of c (j) and then calculate the true conditional frequencies c (j) from ^c (j) <ref> [12] </ref>. To take advan tage of the former fact, that c (y) c min , we should encode c (y) c min fi fi dye fi fi instead of c (y), where dye + denotes all contexts xy that are not proper suffixes of any other context in D.
Reference: [13] <author> Ron, D., Singer, Y., and Tishby, N. </author> <title> The power of amnesia. </title> <type> Tech. rep., </type> <institution> Institute of Computer Science, Hebrew University, Jerusalem, </institution> <year> 1994. </year> <month> 24 </month>
Reference-contexts: This is the principal reason that our models outperform Rissanen's models. Aspects of our work are similar to (but independent of) the recent work of Ron et.al <ref> [13] </ref>. Our model class properly includes their class of prediction suffix trees, and we both use a form of the Kullback-Liebler (KL) divergence to decide whether or not to add a given context to the model.
References-found: 13

