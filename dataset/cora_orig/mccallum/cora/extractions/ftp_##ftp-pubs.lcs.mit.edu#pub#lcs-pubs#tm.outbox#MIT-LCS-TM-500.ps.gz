URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tm.outbox/MIT-LCS-TM-500.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/catatm.html
Root-URL: 
Email: rosario@theory.lcs.mit.edu  
Title: PAC-Learning PROLOG clauses with or without errors  
Author: Rosario Gennaro 
Keyword: Inductive Logic Programming, PAC-learning, noise models.  
Note: Supported by NSF grant no. 9121466-CCR and by a graduate fellowship of Consiglio Nazionale delle Ricerche, Italy  
Date: April 7, 1994  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: In a nutshell we can describe a generic ILP problem as following: given a set E of (positive and negative) examples of a target predicate, and some background knowledge B about the world (usually a logic program including facts and auxiliary predicates), the task is to find a logic program H (our hypothesis) such that all positive examples can be deduced from B and H, while no negative example can. In this paper we review some of the results achieved in this area and discuss the techniques used. Moreover we prove the following new results: * Predicates described by non-recursive, local clauses of at most k literals are PAC-learnable under any distribution. This generalizes a previous result that was valid only for constrained clauses. * Predicates that are described by k non-recursive local clauses are PAC-learnable under any distribution. This generalizes a previous result that was non construc tive and valid only under some class of distributions. Finally we introduce what we believe is the first theoretical framework for learning Prolog clauses in the presence of errors. To this purpose we introduce a new noise model, that we call the fixed attribute noise model, for learning propositional concepts over the Boolean domain. This new noise model can be of its own interest. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin, </author> <title> Queries and Concept Learning, </title> <booktitle> Machine Learning 2, </booktitle> <year> 1988. </year>
Reference-contexts: In the language of Angluin <ref> [1] </ref> we would say that the results hold if the learner is allowed membership and disjointness queries about the target predicate. In the logic programming community disjointness queries are known as existential queries. We will use this term since in this context is more intuitive.
Reference: [2] <author> A. Blum, M. Furst, J. Jackson, M. Kearns, Y. Mansour, S. Rudich, </author> <note> Weakly learning DNF, to appear in STOC 1994 </note>
Reference-contexts: There are still various interesting classes of Prolog clauses that are not known to be PAC-learnable or to be hard to PAC-learn. For many of them the transformation to the propositional domain yields a DNF learning problem. Maybe some recent results about PAC-learning DNF (see <ref> [2] </ref>) could be useful in this setting. Open Problem 4 Finally lot has to be done regarding the new noise model we introduced. For example * We have just analyzed the learning algorithms (conjunctions, k-DNF, k-term DNF) that arise in the transformation of restricted ILP problems to the propositional domain.
Reference: [3] <author> A. Blum, M. Singh, </author> <title> Learning functions of k terms, </title> <month> COLT </month> <year> 1990 </year>
Reference-contexts: The result in [16] however is not constructive, in fact it requires sampling according to a universal distribution that is not computable (or in some restricted case exponential time computable). We follow a different approach. We use the algorithm by Blum and Singh <ref> [3] </ref> to learn k-term DNF by the larger class of general DNF formulae. They show that k-term DNF over f0; 1g n are learnable by DNF formulae containing O (n k1 ) terms under any distribution. <p> example in which if the original rate is &gt; 1 k then in the disjunction problem we are going to have = 1 i.e. for some bit we see no information at all. k-term DNF: A reasoning similar to the one above applies to the algorithm of Blum and Singh <ref> [3] </ref> to learn k-term DNF by general DNF formulae. <p> I would like to acknowledge the help of the following people: Mona Singh explained her paper <ref> [3] </ref> to me, making my reading of it easier and faster; Donna Slonim suggested the "easy way out" in Section 6.3; Lisa Tucker-Kellogg explained to me a lot of stuff about proteins.
Reference: [4] <author> W.W. Cohen, </author> <title> PAC-Learning Non-Recursive Logic Programs, </title> <type> preprint, </type> <year> 1993. </year>
Reference-contexts: Cohen <ref> [4] </ref> improves on (2) by proving that predicates defined by k non-recursive local clauses is PAC-learnable under the same broad class of distributions. <p> This is the standard PAC-learning formalization. In the ILP case however we have to take in account the fact that the learner is not provided just with examples but with a database of background knowledge B. Cohen in <ref> [4] </ref> addresses this issue and proposes the following approach: if C is a family of logic programs, each database B will define a particular concept class C [B] = f&lt; P; B &gt; for P 2 Cg. <p> All of these restrictions are not very limiting, except for the ones dealing with the structure of the clause (constrained, determinate, local). Unfortunately as we will see in the following, Cohen <ref> [4] </ref>, proves some negative results on the PAC-learnability of more expressive classes. 3 A case study: predicting protein structure The family tree example of the previous section was adequate to exemplify the definitions, but not to give a real flavor of what kind of problems ILP can be applied to. <p> Notice that to be able to output a logic program we have to learn in a DNF form, otherwise we will not be able to reconstruct the clauses defining q. Using a technique due to Cohen <ref> [4] </ref> we show how to reduce the problem of learning such a predicate to the one of learning a propositional concept over f0; 1g poly (n) . <p> Li and Vitanyi [16] prove that under the class of simple distributions (it includes all the computable ones) we can learn k-term 11 DNF by k-term DNF. Cohen <ref> [4] </ref> uses this result to prove that the class of predicates defined by k local clauses is learnable under simple distribution. <p> So we can state the following: Theorem 4 The class of predicates defined by k non-recursive clauses of constant locality i is PAC-learnable under any distribution by the larger class of predicates defined by clauses of constant locality i. This result improves on <ref> [4] </ref> in two ways: it is constructive and works under any distribution. Of course by the result of Cohen, both theorems hold for determinate clauses of constant depth as well. <p> This observation applies of course to our stronger results in Section 5.4 as well. So Theorems 3 and 4 hold for the recursive case as well if we allow membership and existential queries (and assume n a). 5.5 Limits on PAC-learning logic programs An entire section of Cohen's paper <ref> [4] </ref> is devoted to prove negative results for restricted classes of logic programs. He proves that relaxing in various ways the assumptions we made 12 above, results in unlearnabilty. These results are obtained using the framework of prediction preserving reducibility devised by Pitt and Warmuth in [22]. <p> We have surveyed the following results: Muggleton and Feng's approach to learn Prolog clauses by relative least general generalization [19]; Cohen's non-constructive results on PAC-learning local Prolog clauses under broad classes of distributions <ref> [4] </ref>. We have improved some of these results by giving algorithms that PAC-learn local Prolog clauses under any distribution. Moreover we presented the first (up to our knowledge) theoretical framework for Inductive Logic Programming in the presence of imperfect data. This investigation leaves many questions open for further research.
Reference: [5] <author> B. Dolsak, S. Muggleton, </author> <title> The application of Inductive Logic Programming to finite element design, </title> <editor> in S. Muggleton ed., </editor> <booktitle> Inductive Logic Programming, </booktitle> <publisher> Academic Press 1992. </publisher>
Reference-contexts: The interest in this area has been paying off especially in terms of practical applications: systems that infer restricted class of Prolog programs have been successfully applied to real-world problems like structure-activity prediction for drug design [14], protein secondary structure prediction [20] or finite element mesh design <ref> [5] </ref>. Born as a branch of logic programming, ILP has been attracting more attention from the Computational Learning Theory community in an effort to provide this new area with solid theoretical grounds.
Reference: [6] <author> S. Dzeroski, S. Muggleton, S. Russell, </author> <booktitle> PAC-learnability of Determinate Logic Programs, </booktitle> <month> COLT </month> <year> 1992. </year>
Reference-contexts: Numerous papers have appeared recently on the subject of PAC-learning logic programs. In particular the following results have been achieved: 1. in [7] the authors prove that predicates defined by non-recursive, constrained clauses of at most k literals are PAC-learnable under any distribution. 2. in <ref> [6] </ref> the authors prove PAC-learnability of predicates that are described by k deter minate non-recursive clauses under a broad class of distributions. 3. Cohen [4] improves on (2) by proving that predicates defined by k non-recursive local clauses is PAC-learnable under the same broad class of distributions. <p> This result improves on [4] in two ways: it is constructive and works under any distribution. Of course by the result of Cohen, both theorems hold for determinate clauses of constant depth as well. However in that case it could be more efficient to use directly the transformation in <ref> [6] </ref> instead of rewriting a determinate clause as a local one. 5.4 Dealing with recursion In [6] the authors point out that the general technique we have been using for non-recursive clauses works as well in the recursive case provided we allow the learner to ask questions about the target predicate <p> Of course by the result of Cohen, both theorems hold for determinate clauses of constant depth as well. However in that case it could be more efficient to use directly the transformation in <ref> [6] </ref> instead of rewriting a determinate clause as a local one. 5.4 Dealing with recursion In [6] the authors point out that the general technique we have been using for non-recursive clauses works as well in the recursive case provided we allow the learner to ask questions about the target predicate (and we bound the arity of the target predicate by a as well).
Reference: [7] <author> S. Dzeroski, S. Muggleton, S. Russell, </author> <title> Learnability of Constrained Logic Programs, </title> <booktitle> ECML 1993. </booktitle>
Reference-contexts: Muggleton and Feng in [19] show that, if restricted to the class of determinate logic programs 1 , Plotkin's method works efficiently. Numerous papers have appeared recently on the subject of PAC-learning logic programs. In particular the following results have been achieved: 1. in <ref> [7] </ref> the authors prove that predicates defined by non-recursive, constrained clauses of at most k literals are PAC-learnable under any distribution. 2. in [6] the authors prove PAC-learnability of predicates that are described by k deter minate non-recursive clauses under a broad class of distributions. 3. <p> Theorem 3 The class of predicates defined by non-recursive clauses of constant locality i containing at most k literals is PAC-learnable under any distribution by the larger class of predicates defined by clauses of constant locality i. This improves on <ref> [7] </ref> where a similar result was obtained for constrained clauses. They learn the class of predicates defined by constrained clauses containing at most k literals by itself.
Reference: [8] <author> S.A. Goldman, R.H. Sloan, </author> <title> Can PAC-learning Algorithms Tolerate Random Attribute Noise?, </title> <type> Tech. Rep. Wash. </type> <institution> Univ. WUCS-92-25, </institution> <year> 1992 </year>
Reference-contexts: The most studied version of this model is random attribute noise, where we assume that c 0 i = c i with probability - &lt; 1 2 independently for each single feature and example. For this model we can use the results of Goldman and Sloan <ref> [8] </ref> that show how to learn conjunctions and Shackelford and Volper [27] that show how to learn k-DNF with random attribute noise. Nothing is known on learning k-term DNF with random attribute noise. <p> This would in turn imply that different attributes have different noise rates i in the corresponding propositional problem (remember that each attribute corresponds to a particular background predicate applied to some variables). But Goldman and Sloan prove in <ref> [8] </ref> that for this kind of attribute noise, even the simple task of learning conjunctions become impossible unless i &lt; 2* for all i where * is the admissible error of the output hypothesis. So we can tolerate only very small amount of noise.
Reference: [9] <author> D. Haussler, </author> <title> Learning conjunctive concepts in structural domains, </title> <journal> Machine Learning, </journal> <volume> 5 (2), </volume> <year> 1990. </year>
Reference-contexts: Despite these results (both positive and negative) propositional learning has the drawback of the limited expressiveness of the hypothesis language. Recently, researchers have been interested in trying to expand the domain of learnability to subsets of first-order logic. Prior work has established a strong negative result: Haussler in <ref> [9] </ref> shows that (even highly restricted) conjunctions in first-order logic cannot be learned in the Valiant (PAC) model [28]. These negative results prompted people to look at other more specific subsets of first-order logic. In particular a natural choice was to look at Prolog programs.
Reference: [10] <author> M. Kearns, </author> <title> Efficient Noise-Tolerant Learning from Statistical Queries, </title> <note> STOC 1993. </note>
Reference-contexts: If we look back at the transformations outlined in Section 5 we can see how random classification noise in the ILP problem induces the same kind of noise in the corresponding propositional problem. Recently Kearns in <ref> [10] </ref> introduced a very powerful technique to deal with random classification noise. Using so-called statistical queries he proves that a large class of propositional learning algorithms works in the presence of random classification noise with error rate &lt; 1 2 .
Reference: [11] <author> M. Kearns, M. Li, </author> <title> Learning in the Presence of Malicious Errors, </title> <note> STOC 1988 </note>
Reference-contexts: The errors in the attributes are not really randomly distributed, but there is some determinism involved in the process. If we think of an adversary choosing the places where to flip the attribute bits, then we fall in the model considered by Kearns and Li <ref> [11] </ref>. And again they prove that only a * 1+* fraction of errors can be tolerated.
Reference: [12] <author> M. Kearns, L. Valiant, </author> <title> Cryptographic limitations on learning Boolean formulae and finite automata, </title> <note> STOC 1989. </note>
Reference-contexts: Log-depth clauses: The first class that he considers is the one of determinate clauses with depth bounded by log n. The proof of unpredictability comes from a reduction of Boolean circuits of depth i to determinate clauses of depth i. Using a result by Kearns and Valiant <ref> [12] </ref> (they prove log-depth circuits are hard to predict under suitable cryptographic assumptions), we know that log-depth clauses are hard to predict as well. Actually this result can be strenghtned using a recent result by Kharitonov [13], stating that log-depth circuits are hard to learn even under the uniform distribution.
Reference: [13] <author> M. Kharitonov, </author> <title> Cryptographic lower bounds on the learnability of Boolean functions on the uniform distributions, </title> <month> COLT </month> <year> 1992 </year>
Reference-contexts: Using a result by Kearns and Valiant [12] (they prove log-depth circuits are hard to predict under suitable cryptographic assumptions), we know that log-depth clauses are hard to predict as well. Actually this result can be strenghtned using a recent result by Kharitonov <ref> [13] </ref>, stating that log-depth circuits are hard to learn even under the uniform distribution. The reduction uses a database of only eleven facts (the definitions of the boolean predicates and, or, not, true) and it is shown in Figure 1.
Reference: [14] <author> R. King, S. Muggleton, R. Lewis, M. Sternberg, </author> <title> Drug design by machine learning, </title> <booktitle> Proceedings of the National Academy of Science, </booktitle> <volume> 89 (23), </volume> <year> 1992. </year> <month> 20 </month>
Reference-contexts: The interest in this area has been paying off especially in terms of practical applications: systems that infer restricted class of Prolog programs have been successfully applied to real-world problems like structure-activity prediction for drug design <ref> [14] </ref>, protein secondary structure prediction [20] or finite element mesh design [5]. Born as a branch of logic programming, ILP has been attracting more attention from the Computational Learning Theory community in an effort to provide this new area with solid theoretical grounds.
Reference: [15] <author> N. Lavrac, S. Dzeroski, </author> <title> Inductive Learning of Relations from Noisy Examples, </title> <editor> in S. Muggleton ed., </editor> <booktitle> Inductive Logic Programming, </booktitle> <publisher> Academic Press 1992. </publisher>
Reference-contexts: However there has been little work done on the effect of noise or errors in the training of ILP system, mostly empirical comparisons of the heuristics used by various implementations when dealing with noise (see for example <ref> [15] </ref>). In this paper we will present what we believe is the first theoretical treatment of the problem of the effects of noise and errors in ILP. 1.2 Outline of the paper The paper continues as following. In the next section we will give a brief overview of logic programming. <p> GOLEM has a very rudimentary noise-handling mechanism. It works by fixing a fraction of the negative examples that the hypothesis is allowed to cover. Other ILP systems like LINUS and FOIL have more sophisticated noise-handling mechanisms (see <ref> [15] </ref>) 5 PAC-learnability results As we said in the introduction, recently there has been some attempt to provide ILP with solid theoretical grounds. In particular some work has been done trying to formalize ILP in terms of the Valiant PAC model [28]. <p> An empirical study of the performance of various noise-handling heuristics is presented in <ref> [15] </ref>. <p> It usually involves methods to avoid the output hypothesis from over-fitting the data, i.e. to prevent concept descriptions that are too specific. In GOLEM the system allows the output hypothesis to cover some of the negative examples. In <ref> [15] </ref> the authors describe the noise-handling mechanisms of two other ILP systems: LINUS and FOIL. LINUS transforms the ILP problem into a attribute-value problem that is learned as a decision tree. Overfit-ting avoidance is then obtained by various tree pruning heuristics.
Reference: [16] <author> M. Li, P. Vitanyi, </author> <title> Learning simple concepts under simple distributions, </title> <journal> SIAM J. of Comp. </journal> <volume> 20 (5), </volume> <year> 1991. </year>
Reference-contexts: As we said above it does not help to be able to learn k-term DNF by k-CNF since we will not then be able to reconstruct and output a logic program as an answer. Li and Vitanyi <ref> [16] </ref> prove that under the class of simple distributions (it includes all the computable ones) we can learn k-term 11 DNF by k-term DNF. Cohen [4] uses this result to prove that the class of predicates defined by k local clauses is learnable under simple distribution. The result in [16] however <p> Vitanyi <ref> [16] </ref> prove that under the class of simple distributions (it includes all the computable ones) we can learn k-term 11 DNF by k-term DNF. Cohen [4] uses this result to prove that the class of predicates defined by k local clauses is learnable under simple distribution. The result in [16] however is not constructive, in fact it requires sampling according to a universal distribution that is not computable (or in some restricted case exponential time computable). We follow a different approach.
Reference: [17] <author> J.W. Lloyd, </author> <title> Foundations of Logic Programming, </title> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: Our presentation is somewhat simplified according to the scope of this paper. For a complete description of the field the reader is referred to Lloyd's classic text <ref> [17] </ref>.
Reference: [18] <author> S. Muggleton, W. Buntine, </author> <title> Machine Invention of First-Order Predicate by Inverting Resolution, </title> <editor> in S. Muggleton ed., </editor> <booktitle> Inductive Logic Programming, </booktitle> <publisher> Academic Press 1992. </publisher>
Reference-contexts: By that we mean a background knowledge that introduces facts and predicates that are not relevant to the learning problem at hand. This appears to be a very serious problem. Maybe some of the work done in Machine Predicate Invention (see <ref> [18] </ref>) could be useful in this case. Open Problem 3 In spite of the progress done in this paper, the question about learning subclasses of Prolog clauses is far from being settled.
Reference: [19] <author> S. Muggleton, C. Feng, </author> <title> Efficient induction of logic programs, </title> <editor> in S. Muggleton ed., </editor> <booktitle> Inductive Logic Programming, </booktitle> <publisher> Academic Press 1992. </publisher>
Reference-contexts: His thesis [23] is not limited just to Horn clause logic (Prolog was not around yet). He gave an algorithm that returns the least general clause covering all the positive examples and no negative example. Muggleton and Feng in <ref> [19] </ref> show that, if restricted to the class of determinate logic programs 1 , Plotkin's method works efficiently. Numerous papers have appeared recently on the subject of PAC-learning logic programs. <p> A constrained logic program is one composed just by constrained clauses. Determinate clauses of constant depth: This restriction was first introduced by Muggleton and Feng in <ref> [19] </ref>. It's a generalization of the constrained condition. Consider the clause A B 1 ^ : : : ^ B l . <p> GOLEM deals with this problem by allowing the inferred rule to cover some of the negative examples. We will come back to the issue of noise (intended either as misclassification of examples or as incompleteness of the background knowledge) later in the paper. 4 GOLEM: the logic-based approach In <ref> [19] </ref> Muggleton and Feng describe GOLEM, a system that infers efficiently determinate constant-depth clauses. It uses Plotkin's results on Relative Least General Generalization (RLGG) of clauses. <p> This approach is of course semi-decidable (or in any case highly inefficient). Moreover even after reduction the inferred clause may still have a large number of literals. Muggleton and Feng in <ref> [19] </ref> show that if we assume that the hidden predicate is defined by a determinate clause of depth i over a background knowledge of arity a, then the size of the RLGG of m examples does not depend on m (it is however O (b i a )). <p> This area is concerned with the inductive inference of Prolog programs from examples of their behavior and a general background knowledge. We have surveyed the following results: Muggleton and Feng's approach to learn Prolog clauses by relative least general generalization <ref> [19] </ref>; Cohen's non-constructive results on PAC-learning local Prolog clauses under broad classes of distributions [4]. We have improved some of these results by giving algorithms that PAC-learn local Prolog clauses under any distribution.
Reference: [20] <author> S. Muggleton, R. King, M. Sternberg, </author> <title> Protein secondary structure prediction using logic-based machine learning, </title> <journal> Protein Engineering, </journal> <volume> 5 (7), </volume> <year> 1992. </year>
Reference-contexts: The interest in this area has been paying off especially in terms of practical applications: systems that infer restricted class of Prolog programs have been successfully applied to real-world problems like structure-activity prediction for drug design [14], protein secondary structure prediction <ref> [20] </ref> or finite element mesh design [5]. Born as a branch of logic programming, ILP has been attracting more attention from the Computational Learning Theory community in an effort to provide this new area with solid theoretical grounds. <p> This section is intended to give a relevant example and some motivation. It may be skipped since it does not prejudicate the comprehension of the rest of the paper. The work synthesized in this section is reported in <ref> [20] </ref>. Interested readers are referred to that paper for more details. A very active research area in molecular biology is the prediction of secondary structure of proteins from their primary structure. Just to make things understandable we can think of proteins as long sequences of aminoacids or residues. <p> An example of clause that could be inferred by the program could be (we are in the realm of science fiction here, for a list of the real clauses inferred by GOLEM please refer to <ref> [20] </ref>) alpha (P rot; P os) 1structure (P rot; P os; Res) ^ aromatic (Res) ^ P os3 = P os + 3 ^ 1structure (P rot; P os3; Res1) ^ hydrophobic (Res1) This clause expresses the theory that a residue is in an ff-helix when it's aromatic and the residue
Reference: [21] <author> L. Pitt, L. Valiant, </author> <title> Computational limitations on learning from examples, </title> <type> JACM 35 (4), </type> <year> 1988 </year>
Reference-contexts: Now the corresponding propositional problem becomes the one of learning a k-term DNF formula. Unfortunately for k 2, k-term DNF are not PAC-learnable by k-term DNF (under the assumption NP6=RP <ref> [21] </ref>). As we said above it does not help to be able to learn k-term DNF by k-CNF since we will not then be able to reconstruct and output a logic program as an answer.
Reference: [22] <author> L. Pitt, M. Warmuth, </author> <title> Prediction Preserving Reducilibity, </title> <type> JCSS 41 (3), </type> <year> 1990 </year>
Reference-contexts: He proves that relaxing in various ways the assumptions we made 12 above, results in unlearnabilty. These results are obtained using the framework of prediction preserving reducibility devised by Pitt and Warmuth in <ref> [22] </ref>. Typically the reduction is conducted over a particular background knowledge B. Log-depth clauses: The first class that he considers is the one of determinate clauses with depth bounded by log n.
Reference: [23] <author> G.D. Plotkin, </author> <title> Automatic Methods of Inductive Inference, </title> <type> Ph.D. thesis, </type> <institution> Edinburgh University, </institution> <year> 1971. </year>
Reference-contexts: In this paper we will try to describe and continue this effort. 1.1 Previous and new results Probably the father of ILP can be considered Plotkin. His thesis <ref> [23] </ref> is not limited just to Horn clause logic (Prolog was not around yet). He gave an algorithm that returns the least general clause covering all the positive examples and no negative example.
Reference: [24] <author> J.A. Robinson, </author> <title> A machine-oriented logic based on the resolution principle, </title> <type> JACM 12 (1), </type> <year> 1965. </year>
Reference-contexts: It uses Plotkin's results on Relative Least General Generalization (RLGG) of clauses. In this section we will present their approach. 4.1 Relative Least General Generalization Informally we can consider Plotkin's notion of RLGG as the inverse of the concept of most general unifier of Robinson <ref> [24] </ref>. As unification (substitution of variables with terms) proves helpful in deduction, so generalization (substituting terms with variables) is useful for induction. We will discuss the aspects of Plotkin's work that are relevant to the ILP framework.
Reference: [25] <author> C. Schaffer, </author> <title> Overfitting Avoidance as Bias, </title> <booktitle> Machine Learning 10, </booktitle> <year> 1993. </year>
Reference-contexts: All mechanisms are just heuristics with little theoretical justification besides empirical results. Moreover recently in the literature there have been various criticisms on the general validity of overfitting avoidance heuristics (see for example <ref> [25] </ref>) In this section we will expand our quest for solid theoretical grounds for ILP to the problem of learning in the presence of imperfect data. We saw in the previous section that 14 PAC-learning restricted classes of logic program can be transformed into learning over the propositional domain.
Reference: [26] <author> R.E. Schapire, </author> <title> The strength of weak learnability, </title> <booktitle> Machine Learning 5 (2), </booktitle> <year> 1990. </year>
Reference-contexts: The reduction uses a database of only eleven facts (the definitions of the boolean predicates and, or, not, true) and it is shown in Figure 1. Constant-depth indeterminate clauses: This class is not predictable because the corresponding language includes an NP-complete problem. Schapire <ref> [26] </ref> proved that this is a sufficient condition for unpredictability.
Reference: [27] <author> G. Shackelford, D. Volper, </author> <title> Learning k-DNF with Noise in the Attribute, </title> <month> COLT </month> <year> 1988 </year>
Reference-contexts: For this model we can use the results of Goldman and Sloan [8] that show how to learn conjunctions and Shackelford and Volper <ref> [27] </ref> that show how to learn k-DNF with random attribute noise. Nothing is known on learning k-term DNF with random attribute noise. However we feel that modeling errors in the background knowledge with random attribute noise in the corresponding propositional problem is inappropriate.
Reference: [28] <author> L. Valiant, </author> <title> A Theory of the Learnable, </title> <journal> Comm of the ACM, </journal> <volume> 27 (11), </volume> <year> 1984 </year> <month> 21 </month>
Reference-contexts: Recently, researchers have been interested in trying to expand the domain of learnability to subsets of first-order logic. Prior work has established a strong negative result: Haussler in [9] shows that (even highly restricted) conjunctions in first-order logic cannot be learned in the Valiant (PAC) model <ref> [28] </ref>. These negative results prompted people to look at other more specific subsets of first-order logic. In particular a natural choice was to look at Prolog programs. This new research area has been named Inductive Logic Programming (ILP). <p> For that, we should try to formalize the problems in terms of the PAC model as defined by Valiant in <ref> [28] </ref>. Let's just recall it for sake of completeness. Let X be a set called the domain. A concept c over X is just a subset of X . A concept class C is a family of concepts. <p> In particular some work has been done trying to formalize ILP in terms of the Valiant PAC model <ref> [28] </ref>. In this section we will describe these results and our improvements to them. Section 2.2 contains all the definitions about the subclasses we are going to consider. <p> It can be shown that restricting the hidden rate to be &lt; 1 k we can PAC-learn with only a constant factor increase in the sample complexity ((1 k) 1 to be precise) Notice that in the algorithms above, we need to use the original 2-buttons model of Valiant <ref> [28] </ref> i.e. we need to be able to ask for a positive or a negative example. Consider the algorithm for learning conjunctions. There we use only positive examples.
References-found: 28

