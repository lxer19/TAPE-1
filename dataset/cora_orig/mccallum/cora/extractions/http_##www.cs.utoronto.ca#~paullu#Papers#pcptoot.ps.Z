URL: http://www.cs.utoronto.ca/~paullu/Papers/pcptoot.ps.Z
Refering-URL: http://www.cs.toronto.edu/~paullu/Pcp/pcp.html
Root-URL: 
Title: Programming in PCP  
Author: Brent Gorda, Karen Warren and Eugene D. Brooks III 
Note: Work performed under the auspices of the U. S. Department of Energy by the Lawrence Livermore National Laboratory under Contract W-7405-ENG-48.  
Date: June, 1991  
Abstract: PCP is an implementation of the split-join parallel programming paradigm for the C programming language. In the split-join paradigm a team of processors executes the user program from main() to exit(). The model allows for exploitation of nested parallelism via a mechanism called team splitting. All of the features of PCP are block structured and allow for arbitrary nesting of parallel constructs. In this manual we document PCP and give examples of its use in writing portable parallel programs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Eugene D. Brooks III, PCP: </author> <title> A Parallel Extension of C that is 99% Fat Free, </title> <institution> UCRL-99673, Lawrence Livermore National Laboratory, </institution> <year> 1988. </year>
Reference-contexts: It is the opinion of some researchers working on these problems that such automatic tools will not exist within a decade, if ever. The PCP <ref> [1] </ref> programming language puts the issues of scheduling, communication and synchronization directly into the hands of the programmer so that efficient parallel code can be written for the multiprocessors that are available today.
Reference: [2] <author> Harry F. Jordon, </author> <title> The Force: A Highly Portable Parallel Programming Language, </title> <booktitle> Proceeding of the International Conference on Parallel Processing, </booktitle> <month> August, </month> <year> 1989. </year>
Reference-contexts: PCP supports an extension of the Single-Program-Multiple-Data (SPMD) model that is available in H. Jordan's FORCE <ref> [2] </ref> and IBM EPEX/FORTRAN [3]. The key additional concept is the notion of team splitting that allows an arbitrary subdivision of the team of processors executing the code, and allows each sub-team to execute arbitrarily different code within the constraint of block structure.
Reference: [3] <author> F. Darema, D. A. George, V. A. Norton and G. F. Pfister, </author> <title> A single-program-multiple data computational model for EPEX/FORTRAN , Parallel Computing, </title> <month> April, </month> <year> 1988. </year>
Reference-contexts: PCP supports an extension of the Single-Program-Multiple-Data (SPMD) model that is available in H. Jordan's FORCE [2] and IBM EPEX/FORTRAN <ref> [3] </ref>. The key additional concept is the notion of team splitting that allows an arbitrary subdivision of the team of processors executing the code, and allows each sub-team to execute arbitrarily different code within the constraint of block structure.
Reference: [4] <author> Eugene D. Brooks III, </author> <title> The Butterfly Barrier , UCRL-95737, </title> <institution> Lawrence Livermore National Laboratory, </institution> <month> November, </month> <year> 1986. </year>
Reference-contexts: Each team has its own distinct barrier. A barrier is often used after a master block, or a forall loop, to ensure that the preceding work is complete before any processor is allowed to continue on. A fast algorithm <ref> [4, 5] </ref> which has no hot spots 4 or critical regions has been implemented for the PCP runtime support. 4 A hot spot is a shared memory variable for which all processors are vying. 6 In the example below, one team member performs the summing and printing of the result 5
Reference: [5] <author> D. Hensgen, R. Finkel, U. Manber, </author> <title> Two Algorithms for Barrier Synchronization, </title> <journal> International Journal of Parallel Programming,vol 17(1) </journal> <month> 1-17 </month> <year> (1988). </year>
Reference-contexts: Each team has its own distinct barrier. A barrier is often used after a master block, or a forall loop, to ensure that the preceding work is complete before any processor is allowed to continue on. A fast algorithm <ref> [4, 5] </ref> which has no hot spots 4 or critical regions has been implemented for the PCP runtime support. 4 A hot spot is a shared memory variable for which all processors are vying. 6 In the example below, one team member performs the summing and printing of the result 5
Reference: [6] <author> Brent C. Gorda and Eugene D. Brooks, </author> <title> The MPCI Gang Scheduler, The 1991 MPCI Yearly Report: Harnessing the Killer Micros, </title> <type> Draft, </type> <institution> Lawrence Livermore National Laboratory, </institution> <month> March, </month> <year> 1991 </year>
References-found: 6

