URL: http://www.cis.ohio-state.edu/~clam/papers/filesys.ps.Z
Refering-URL: http://www.cis.ohio-state.edu/~clam/
Root-URL: http://www.cis.ohio-state.edu
Title: Comparison of File Transfer Policies Using Simulation  
Author: Chi Chung Lam 
Date: December 1997  
Abstract: This paper compares four file transfer policies, namely on-demand fetching, prefetch-ing, whole-file transfer, and large-trunk transfer. The comparison is done using a custom-built CSIM-based distributed file system simulator, which models a dedicated file server machine and multiple client machines connected by a shared local area network. Simulation results indicate that prefetching does not improve a lot over on-demand fetching. Whole-file transfer shows its advantage only when there are over ten clients each equipped with a local disk. Among the four file transfer policies, large-trunk transfer is found to be far more superior than the others. At high load, large-trunk transfer yields a throughput rate three times that of whole-file transfer and six times of on-demanding fetching or prefetching. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Howard, John H. et al. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> Feb </month> <year> 1988. </year>
Reference-contexts: If not, it is at least on its way. Anyway, the waiting time at the client side can be reduced. Another file transfer policy is called whole-file transfer, which is an important feature in early versions of the Andrew File System (AFS) <ref> [1] </ref>. This policy is based on the observations that whole-file reads/writes are very common and that the file server could easily become the bottleneck. <p> Next, we consider the configuration where each client has a 1M bytes local disk cache. The client disks are as fast as the server disk. Other parameters are kept unchanged. We choose this configuration because the effectiveness of whole-file transfer relies on client disk cache <ref> [1] </ref>. Figure 2 shows the scalability and the resource utilization for the four file transfer policies under such a system configuration. For less than 5 clients, whole-file transfer is not as good as prefetching.
Reference: [2] <author> Sandberg, R. </author> <title> Design and implementation of the Sun network file system. </title> <booktitle> In proceedings of the USENIX 1985 Summer Conference (Partland, Ore). </booktitle> <pages> pp. 119-130, </pages> <month> Jun </month> <year> 1985. </year>
Reference-contexts: An obvious improvement to on-demand fetching is prefetching and it is the policy adopted by many file systems, including Sun NFS <ref> [2] </ref>. Because file accesses are mostly sequential, the file block to be accessed next is very often the next block in the file.
Reference: [3] <author> Schwetman, Herb. </author> <title> CSIM Users' Guide. </title> <institution> Microelectronics and Computer Technology Corporation, </institution> <month> Jun </month> <year> 1992. </year>
Reference-contexts: In 3 other words, each cache block in memory is more recently used than any cache block on disk, and no block resides both in memory and on disk. The simulator is written in CSIM <ref> [3] </ref>, a process-oriented discrete-event simulator. Client processors and disks (if any), server processor and disk, and the network are modeled as single-server, first-come-first-server facilities. Contents of the memory cache and disk cache (if any) in each machine are maintained in bidirectional linked list to facilitates LRU replacement.
Reference: [4] <author> Spector, Alfred Z. and Michael L. Kazar. </author> <title> Uniting File Systems. </title> <journal> Unix Review, </journal> <volume> 7(3) </volume> <pages> 61-70, </pages> <month> Mar </month> <year> 1989. </year> <month> 11 </month>
Reference-contexts: With this policy, AFS is able to scale gracefully and supports more clients. The last policy is large-trunk transfer. AFS 3.0 <ref> [4] </ref> is said to have incorporated this policy. However, since detail description of this policy could not be found in the literature, we created our own interpretation, which may or may not be the same as what is in AFS 3.0.
References-found: 4

