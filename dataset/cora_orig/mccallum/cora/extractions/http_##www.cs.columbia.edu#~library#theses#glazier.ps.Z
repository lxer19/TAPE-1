URL: http://www.cs.columbia.edu/~library/theses/glazier.ps.Z
Refering-URL: http://www.cs.columbia.edu/home/phd_prog/alumni.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Dynamic Neighborhood Bounding: An Error Reduction Technique for Monte Carlo Simulation  
Author: Jason Samuel Glazier 
Degree: Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the Graduate School of Arts and Sciences.  
Date: 1995  
Affiliation: Columbia University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Russell C.H. Cheng, </author> <title> Variance Reduction Methods, </title> <booktitle> Proceedings of the 1986 Winter Simulation Conference </booktitle>
Reference-contexts: the relationship: E n (f ) = p where (f ) is called the volatility 1 of f and is defined as: (f ) = D f (x) f ) dx and the mean of, f , is given by: f = D where both are calculated on D = <ref> [0; 1] </ref> d , the d dimensional unit cube. However, in practice because the true means and volatilities are not known, they are estimated by the sample mean, m (f ), given by: m (f ) = n i=1 1 We use the term volatility to refer to standard deviation. <p> 3 and the sample volatility, s (f ), is given by: s (f ) = u t 1 n X (f (t i ) m (f )) 2 The vectors t 1 ; : : : ; t n are n independent, uniformly distributed, randomly selected vectors over D = <ref> [0; 1] </ref> d , the d dimensional unit cube. We can clearly see from this relationship that if the volatility of the function f is reduced, then the expected error will also be reduced. <p> This is desirable since if the variance of the function is reduced, then the expected error is reduced. There are many surveys on VRTs <ref> [ 1; 19; 3; 4; 5; 6 ] </ref> . The most widely known VRTs are control variates, stratified sampling, common random numbers, and importance sampling. The similarities and differences between DNB and both stratified sampling, control variates, and importance sampling (static and adaptive) is discussed in section 2.3. <p> Classical Monte Carlo algorithms are best illustrated though multivariate integration. Let t 1 ; : : : ; t n be n randomly selected points which are independent and uniformly distributed over D = <ref> [0; 1] </ref> d , the d dimensional unit cube. Let B (L 2 (D)) be the unit ball of the space of L 2 -integrable functions. The L 2 -integrable functions, are the functions where R D f 2 dx &lt; 1. <p> Because DNB is a VRT for some specific cases, and because DNB is closely related to VRTs, it is important to contrast DNB to the four VRTs: stratified sampling, importance sampling, Vegas MC (adaptive importance sampling), and control variates (CV). VRT surveys published in the last few years including <ref> [ 1; 19; 3; 4; 5; 6 ] </ref> , plus many other chapters appearing in most books on MCS including [ 30; 31; 32; 37 ] . With the preponderance of surveys, one would be lead to believe that VRTs is a quickly changing field.
Reference: [2] <author> John C. Hull, </author> <title> Options, Futures, and other Derivative Securities, </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989 </year>
Reference-contexts: As we have mentioned, we use this 7.2% value to `center' our simulation, which we and most financial sources (see Hull <ref> [ 2 ] </ref> ), assumes would give more accurate results than centering the values around 7%. <p> 70.2 2 40.2 34.1 36.3 36.4 36.2 35.9 35.9 40.0 4 34.4 32.3 29.2 30.0 30.5 31.6 34.8 38.6 6 35.1 28.1 29.2 28.6 28.3 30.9 28.6 34.7 8 32.3 30.0 27.7 27.1 26.9 28.3 29.4 35.0 131 Chapter 8 PSR CASE STUDY We chose to use pre-settlement risk (see <ref> [ 2 ] </ref> as our case study to demonstrate a very sophisticated real world problem. Evaluating pre-settlement risk is an important financial problem that must be considered by every financial investment firm. In this problem, a firm holds a portfolio of unsettled contracts.
Reference: [3] <author> Barry L. Nelson, </author> <title> A Decomposition Approach To Variance Reduction, </title> <booktitle> Proceedings of the 1985 Winter Simulation Conference </booktitle>
Reference-contexts: This is desirable since if the variance of the function is reduced, then the expected error is reduced. There are many surveys on VRTs <ref> [ 1; 19; 3; 4; 5; 6 ] </ref> . The most widely known VRTs are control variates, stratified sampling, common random numbers, and importance sampling. The similarities and differences between DNB and both stratified sampling, control variates, and importance sampling (static and adaptive) is discussed in section 2.3. <p> Because DNB is a VRT for some specific cases, and because DNB is closely related to VRTs, it is important to contrast DNB to the four VRTs: stratified sampling, importance sampling, Vegas MC (adaptive importance sampling), and control variates (CV). VRT surveys published in the last few years including <ref> [ 1; 19; 3; 4; 5; 6 ] </ref> , plus many other chapters appearing in most books on MCS including [ 30; 31; 32; 37 ] . With the preponderance of surveys, one would be lead to believe that VRTs is a quickly changing field.
Reference: [4] <author> Barry L. Nelson, </author> <title> Variance Reduction for Simulation Practitioners, </title> <booktitle> Proceedings of the 1987 Winter Simulation Conference </booktitle>
Reference-contexts: This is desirable since if the variance of the function is reduced, then the expected error is reduced. There are many surveys on VRTs <ref> [ 1; 19; 3; 4; 5; 6 ] </ref> . The most widely known VRTs are control variates, stratified sampling, common random numbers, and importance sampling. The similarities and differences between DNB and both stratified sampling, control variates, and importance sampling (static and adaptive) is discussed in section 2.3. <p> Because DNB is a VRT for some specific cases, and because DNB is closely related to VRTs, it is important to contrast DNB to the four VRTs: stratified sampling, importance sampling, Vegas MC (adaptive importance sampling), and control variates (CV). VRT surveys published in the last few years including <ref> [ 1; 19; 3; 4; 5; 6 ] </ref> , plus many other chapters appearing in most books on MCS including [ 30; 31; 32; 37 ] . With the preponderance of surveys, one would be lead to believe that VRTs is a quickly changing field. <p> X j may be any series with known , although the input is commonly chosen since a strong correlation would commonly exist between input and output. It has been proven (see <ref> [ 4 ] </ref> ) that the variance of Z is minimized when fi = Cov [Y j ; X j ]=V ar [X j ]. However since Cov [Y j ; X j ] is seldom ever known, fi is an estimate.
Reference: [5] <author> James R. Willson, </author> <title> Variance Reduction Techniques for Digital Simulation, </title> <journal> American Journal of Mathematical and Management Sciences, </journal> <volume> Vol. 4, Nos. 3 & 4, p277-312, </volume> <year> 1984 </year>
Reference-contexts: This is desirable since if the variance of the function is reduced, then the expected error is reduced. There are many surveys on VRTs <ref> [ 1; 19; 3; 4; 5; 6 ] </ref> . The most widely known VRTs are control variates, stratified sampling, common random numbers, and importance sampling. The similarities and differences between DNB and both stratified sampling, control variates, and importance sampling (static and adaptive) is discussed in section 2.3. <p> Because DNB is a VRT for some specific cases, and because DNB is closely related to VRTs, it is important to contrast DNB to the four VRTs: stratified sampling, importance sampling, Vegas MC (adaptive importance sampling), and control variates (CV). VRT surveys published in the last few years including <ref> [ 1; 19; 3; 4; 5; 6 ] </ref> , plus many other chapters appearing in most books on MCS including [ 30; 31; 32; 37 ] . With the preponderance of surveys, one would be lead to believe that VRTs is a quickly changing field.
Reference: [6] <author> James R. Willson, </author> <title> Variance Reduction in Simulation, </title> <booktitle> Proceedings of the 1984 Winter Simulation Conference </booktitle>
Reference-contexts: This is desirable since if the variance of the function is reduced, then the expected error is reduced. There are many surveys on VRTs <ref> [ 1; 19; 3; 4; 5; 6 ] </ref> . The most widely known VRTs are control variates, stratified sampling, common random numbers, and importance sampling. The similarities and differences between DNB and both stratified sampling, control variates, and importance sampling (static and adaptive) is discussed in section 2.3. <p> Because DNB is a VRT for some specific cases, and because DNB is closely related to VRTs, it is important to contrast DNB to the four VRTs: stratified sampling, importance sampling, Vegas MC (adaptive importance sampling), and control variates (CV). VRT surveys published in the last few years including <ref> [ 1; 19; 3; 4; 5; 6 ] </ref> , plus many other chapters appearing in most books on MCS including [ 30; 31; 32; 37 ] . With the preponderance of surveys, one would be lead to believe that VRTs is a quickly changing field.
Reference: [7] <author> V.C. Bhavsar, J.R. Isaac, </author> <title> Design and Analysis of Parallel MC Algorithms SIAM J. </title> <journal> Sci. Stat. Computing, </journal> <volume> Vol 8, </volume> <pages> n1, </pages> <month> Jan </month> <year> 1987 </year>
Reference-contexts: Because of the excessive amount of computations that must be performed to obtain sufficiently accurate estimations, MCS is a popular application of parallel computing. MCS has traditionally mapped extremely well to parallel computers and has enjoyed large speed ups <ref> [ 7; 9; 37 ] </ref> . The MCS problem can be formalized as follows. <p> Parallel processing is an important technique for delivering more computing cycles than traditional serial processing. Although MCS as a general technique appears to be easily parallelized, in fact it can be irregular and difficult to parallelize (see <ref> [ 40; 10; 7 ] </ref> ). Some of the sources of the irregularity are due to memory limitations that do not alow one PEC per processor, or difficulties of efficiently using all the processors of a SIMD 2 machines. <p> This level of depth is sufficient for our discussion. 2.4.1 Intrinsic Parallelism In order to determine the intrinsic parallelism of MCS, we must first look at the properties of PECs. Bhavsar <ref> [ 7 ] </ref> has identified the following characteristics similar to most PECs: * PECs are essentially independent of each other because candidate vectors are sampled independently from the probability space. 34 * PECs usually consist of the execution of identical code. * A PECs execution time is in general a random <p> See [ 30; 31; 32 ] for problem descriptions. 37 2.4.3 Scheduling Once the parallelism of the problem has been determined, rules are needed to assign K PECs to P processors. We can make assignments either statically or dynamically. Scheduling seeks to maximize processor utilization in parallel processing. Bhavsar <ref> [ 7 ] </ref> explored static and dynamic scheduling algorithms for different magnitudes of K and P. In static computation assignment (SCA) schemes for parallel Monte Carlo algorithms, a fixed number of PECs is assigned to the processors before any PEC computations are initiated. <p> Sometimes this inspires very creative solutions. 55 4.1 Parallel DNB Algorithm We propose a parallel algorithm that we believe will overlap communication and computation. We have chosen to use the parallel model DCA scheme 1 put forth by <ref> [ 7 ] </ref> that was discussed in section 2.4.3. In DCA Scheme 1, given P processors, first we initiate P PECs in parallel. Whenever a PEC is completed on a processor, another PEC is initiated on that processor.
Reference: [8] <author> C. Zhao, J. Wood, </author> <title> The Monte Carlo Method on a Parallel Computer , Annals of Nuclear Energy, </title> <journal> vol. </journal> <volume> 16, </volume> <editor> n. </editor> <volume> 12, </volume> <pages> pp. 649-657, </pages> <year> 1989 </year>
Reference-contexts: Whenever a PEC is completed, we initiate another PEC on that processor only if the required K PECs have not been already initiated. Thus, in this scheme we initiate only K PECs and therefore no PECs get aborted as in DCA Scheme 1. Zhao <ref> [ 8 ] </ref> gives an example of successfully implementing multitasking and dynamic assignment for SIMD machines. Bhavsar's and Zhao's work is relevant if one employs geometric parallelism or very limited algorithmic parallelism, which in fact does cover most of the literature on parallel MCS.
Reference: [9] <author> CR Askew, </author> <title> Simulation of statistical mechanical systems on transputer arrays, </title> <journal> Computer Physics Communications, </journal> <volume> v42, </volume> <year> 1986, </year> <month> p21-26 </month>
Reference-contexts: Because of the excessive amount of computations that must be performed to obtain sufficiently accurate estimations, MCS is a popular application of parallel computing. MCS has traditionally mapped extremely well to parallel computers and has enjoyed large speed ups <ref> [ 7; 9; 37 ] </ref> . The MCS problem can be formalized as follows. <p> When the PECs are calculated on variables that are completely independent of each other, the PECs can be 35 executed in parallel. Because in the case of MCS, these computations are usually identical, they are commonly suitable for implementation on both SIMD and MIMD computers. Askew <ref> [ 9 ] </ref> and others refer to this type of parallelism as geometric parallelism . Although the term geometric parallelism usually also includes parallelism between computations of variables. * Parallelism in a PEC. <p> In general, a PEC decomposes into three computations, multidimensional random number generation, updating the value of a primary estimate based on a sample from a random number generator, and determination of the value of a termination condition for the computation. Intrinsic parallelism exists in all these steps. Askew <ref> [ 9 ] </ref> and others refer to this type of parallelism as algorithmic parallelism . 2.4.2 Processor Farms Processor farms are the predominantly implemented parallel model.
Reference: [10] <author> Reinhard Hanxleden, L. Ridgway Scott, </author> <title> Load Balancing on Message Passing Architectures , Journal of Parallel and Distributed Computing, </title> <booktitle> vol 13, </booktitle> <pages> pp. 312-324, </pages> <year> 1991 </year>
Reference-contexts: Parallel processing is an important technique for delivering more computing cycles than traditional serial processing. Although MCS as a general technique appears to be easily parallelized, in fact it can be irregular and difficult to parallelize (see <ref> [ 40; 10; 7 ] </ref> ). Some of the sources of the irregularity are due to memory limitations that do not alow one PEC per processor, or difficulties of efficiently using all the processors of a SIMD 2 machines. <p> However, none of these parallelization strategies deal with scheduling complicated algorithmic parallelism, as would be needed to implement problems such as semiconductor simulations. For complicated algorithmic parallelism, more traditional load balancing strategies have been investigated. Hanxleden <ref> [ 10 ] </ref> factored in load balancing issues, and extended Bhavar's scheduling schemes to include full algorithmic parallelism in message passing MIMD architectures. Hanxleden created a test bed on an iPSC/2 hypercube, for load balancing techniques. MCS was the test application. They also investigated both static and dynamic strategies.
Reference: [11] <author> GS Pawley, </author> <title> The implementation of Lattice Calculations on the DAP, </title> <journal> Journal of computational Physics, </journal> <note> v47, 1982, p165-178 148 </note>
Reference-contexts: However, because of this, the parallel MCS literature does not usually treat parallel processing VRTs as a special topic. 1 Parallel MCS has actually been run on machines as primitive as Bit-serial machines (see Pawley <ref> [ 11 ] </ref> ). Because of the cost of parallel machines, most researchers have to use whatever they have available to them. Sometimes this inspires very creative solutions. 55 4.1 Parallel DNB Algorithm We propose a parallel algorithm that we believe will overlap communication and computation.
Reference: [12] <author> D.B. Skillicorn, </author> <title> Practical Parallel Computation: I. Models of Computation, </title> <institution> CS Department, Queen's University, Kingston, Canada, </institution> <month> August 12, </month> <year> 1991 </year>
Reference-contexts: Few of the papers present general techniques. This survey seeks to provide a general view of the sources of intrinsic parallelism, parallel computational models, scheduling, and scalability. Two recent papers provide an excellent survey of computation models <ref> [ 12 ] </ref> and languages [ 13 ] for parallel and distributed processing. We discuss parallel processing from the simple view of SIMD or MIMD, shared or distributed memory, and the intercommunication topology.
Reference: [13] <author> Henri E. Bal, Jennifer G. Steiner, Andrew S. Tanenbaum, </author> <title> Programming Languages for Distributed Computing Systems, </title> <journal> ACM Computing Surveys, v21, </journal> <volume> n3, </volume> <month> September </month> <year> 1989, </year> <month> p261-321 </month>
Reference-contexts: Few of the papers present general techniques. This survey seeks to provide a general view of the sources of intrinsic parallelism, parallel computational models, scheduling, and scalability. Two recent papers provide an excellent survey of computation models [ 12 ] and languages <ref> [ 13 ] </ref> for parallel and distributed processing. We discuss parallel processing from the simple view of SIMD or MIMD, shared or distributed memory, and the intercommunication topology.
Reference: [14] <author> Makoto Yokozawa, Yosiaki Oka, Shunsuke Kondo, Yasumasa Togo, </author> <title> Development of Vectorized Monte Carlo Calculation and Application of Stratified Sampling, </title> <journal> Journal of Nuclear Science and Technology, v24, </journal> <volume> n7, </volume> <month> June </month> <year> 1987, </year> <month> p507-515 </month>
Reference: [15] <author> David Alan Grier, </author> <title> A System for Monte Carlo Experimentation, </title> <booktitle> Proceedings of the 1986 Winter Simulation Conference, </booktitle> <address> p876-887 </address>
Reference-contexts: DNBinc tool is a uniprocessing tool. Note that the tools are not for testing and prototyping general MCS applications. The tools are specific to testing and implementing DNB. There have been literally hundreds of non-MCS specific simulation tools developed. Grier <ref> [ 15 ] </ref> developed an MCS specific experimentation tool and language to assist in prototyping and testing MCSs and apply the traditional MCS VRTs. We know of no popular MCS specific tools in general wide-spread use, so there was no compelling reason to build our tools layered upon other tools.
Reference: [16] <author> V. Nageshwara Rao, Vipin Kumar, </author> <title> On the Efficiency of Parallel Backtracking, </title> <note> Tech Report 90-55 (Revised Jan 1992) , University of Minnesota, to appear in IEEE Transactions of Parallel and Distributed Systems. </note>
Reference-contexts: Independent communication domains can be easily configured on many architectures such as meshes, trees, and hyper-cubes (see [ 29 ] for descriptions of parallel architectures). These architectures follow the global coordinator and multiple coordinator architectures studied by <ref> [ 16; 17; 18 ] </ref> . DNB allows for incremental update of an MCS (for distribution changes only). Incremental update techniques allow a full solution of an evolving system to be calculated by reusing part of a prior solution. <p> Notice that this architecture appears to be scalable since the processors are being used as a processor farm. However, since we are using a global coordinator algorithm, the coordinator will always represent a serial bottleneck. To alleviate this negative situation, the multiple coordinator approach discussed by <ref> [ 16; 17; 18 ] </ref> can be used. The multiple coordinator approach is graphically displayed in figure 4.4. If multiple distinct communication domains can be created (or partitioned), then each communication domain would not be overloaded with messages.
Reference: [17] <author> George Karypis, Vipin Kumar, </author> <title> Unstructured Tree Search on SIMD Parallel Computers, </title> <type> Tech Report 92-21, </type> <institution> University of Minnesota, </institution> <month> April </month> <year> 1992, </year> <note> to appear in Proceedings of Supercomputing 92. </note>
Reference-contexts: Independent communication domains can be easily configured on many architectures such as meshes, trees, and hyper-cubes (see [ 29 ] for descriptions of parallel architectures). These architectures follow the global coordinator and multiple coordinator architectures studied by <ref> [ 16; 17; 18 ] </ref> . DNB allows for incremental update of an MCS (for distribution changes only). Incremental update techniques allow a full solution of an evolving system to be calculated by reusing part of a prior solution. <p> Notice that this architecture appears to be scalable since the processors are being used as a processor farm. However, since we are using a global coordinator algorithm, the coordinator will always represent a serial bottleneck. To alleviate this negative situation, the multiple coordinator approach discussed by <ref> [ 16; 17; 18 ] </ref> can be used. The multiple coordinator approach is graphically displayed in figure 4.4. If multiple distinct communication domains can be created (or partitioned), then each communication domain would not be overloaded with messages.
Reference: [18] <author> Hasanat M. Dewan, Salvatore J. Stolfo, </author> <title> System Reorganization and Load Balancing of Parallel Database Rule Processing, </title> <booktitle> To appear in the proceedings of the 7th Intl. Symposium on the Methodologies for Intelligent Systems (IS-MIS'93), </booktitle> <address> Trondheim, Norway, </address> <month> June 15-18, </month> <year> 1993 </year>
Reference-contexts: Independent communication domains can be easily configured on many architectures such as meshes, trees, and hyper-cubes (see [ 29 ] for descriptions of parallel architectures). These architectures follow the global coordinator and multiple coordinator architectures studied by <ref> [ 16; 17; 18 ] </ref> . DNB allows for incremental update of an MCS (for distribution changes only). Incremental update techniques allow a full solution of an evolving system to be calculated by reusing part of a prior solution. <p> Notice that this architecture appears to be scalable since the processors are being used as a processor farm. However, since we are using a global coordinator algorithm, the coordinator will always represent a serial bottleneck. To alleviate this negative situation, the multiple coordinator approach discussed by <ref> [ 16; 17; 18 ] </ref> can be used. The multiple coordinator approach is graphically displayed in figure 4.4. If multiple distinct communication domains can be created (or partitioned), then each communication domain would not be overloaded with messages.
Reference: [19] <author> Catherine McGeoch, </author> <title> Analyzing Algorithms by Simulation: Variance Reduction Techniques and Simulation Speedups, </title> <journal> ACM Computing Surveys, v24, </journal> <volume> n2, </volume> <month> June </month> <year> 1992, </year> <pages> p 195-212 </pages>
Reference-contexts: This is desirable since if the variance of the function is reduced, then the expected error is reduced. There are many surveys on VRTs <ref> [ 1; 19; 3; 4; 5; 6 ] </ref> . The most widely known VRTs are control variates, stratified sampling, common random numbers, and importance sampling. The similarities and differences between DNB and both stratified sampling, control variates, and importance sampling (static and adaptive) is discussed in section 2.3. <p> Because DNB is a VRT for some specific cases, and because DNB is closely related to VRTs, it is important to contrast DNB to the four VRTs: stratified sampling, importance sampling, Vegas MC (adaptive importance sampling), and control variates (CV). VRT surveys published in the last few years including <ref> [ 1; 19; 3; 4; 5; 6 ] </ref> , plus many other chapters appearing in most books on MCS including [ 30; 31; 32; 37 ] . With the preponderance of surveys, one would be lead to believe that VRTs is a quickly changing field.
Reference: [20] <author> PVM 3.0: </author> <title> Parallel Virtual Machine System 3.0, </title> <institution> University of Tennessee, Knoxville TN. Oak Ridge National Laboratory, Oak Ridge TN. Emory University, </institution> <address> Atlanta GA. </address> <note> Authors: </note> <author> A. L. Beguelin, J. J. Dongarra, G. A. Geist, R. J. Manchek, B. K. Moore, and V. S. </author> <note> Sunderam </note>
Reference-contexts: Every processor will have approximately the same vector store, so only the local store must be searched to determine if a new candidate vector needs to be evaluated. We have chosen to use Parallel Virtual Machine 3.0 (pvm3) <ref> [ 20 ] </ref> as the communication and process distribution software. Our software runs in this environment, and since pvm3 is currently portable to over 20 platforms, and runs `cross platform', pvm3 provides extensive portability for our tools. Our software implementation will be detailed in section 6.3.
Reference: [21] <editor> Frank J. Fabozzi (Editor), </editor> <booktitle> Handbook of Mortgage-Backed Securities , 3rd edition, </booktitle> <publisher> Probus Publishing Corp, </publisher> <address> Chicago, IL, </address> <year> 1992. </year>
Reference: [22] <author> M.E. Dyer, </author> <title> A.M. Frieze, A Randomized Algorithm for Fixed-Dimensional Linear Programming, </title> <booktitle> Mathematical Programming, </booktitle> <address> n44, </address> <year> 1989, </year> <month> p203-212 </month>
Reference-contexts: C Code for the Vegas algorithm can be found in the book Numerical Recipes in C, Vol. 2 [ 27 ] . Unlike the other VRTs, Vegas is not usually referenced in the VRT surveys (see the following recent references <ref> [ 22; 23; 27 ] </ref> ). Vegas relates to DNB in the same way as importance sampling. Vegas will be re-visited in section 5.1 when we discuss VRTs that support incremental update. 2.3.4 Control Variates Control Variates (CV) uses an auxiliary variable to correct for disproportionate sampling.
Reference: [23] <author> Rudolf Fleischer, </author> <title> Communication Complexity of Multi-Processor Systems, </title> <journal> Information Processing Letters, </journal> <volume> n30, </volume> <year> 1989, </year> <month> p57-65. </month>
Reference-contexts: C Code for the Vegas algorithm can be found in the book Numerical Recipes in C, Vol. 2 [ 27 ] . Unlike the other VRTs, Vegas is not usually referenced in the VRT surveys (see the following recent references <ref> [ 22; 23; 27 ] </ref> ). Vegas relates to DNB in the same way as importance sampling. Vegas will be re-visited in section 5.1 when we discuss VRTs that support incremental update. 2.3.4 Control Variates Control Variates (CV) uses an auxiliary variable to correct for disproportionate sampling.
Reference: [24] <author> The Public Securities Association, </author> <title> Uniform Practices for the clearance and settlement of mortgage backed securities and other related securities, 40 Broad Street, </title> <address> New York, New York, 10004. </address> <month> 149 </month>
Reference-contexts: A certain amount of prepayment is normal: people move because of job relocation, they require more or less space, and so on. To model this behavior, we use the PSA standard, which is published by the Public Securities Association (PSA) <ref> [ 24 ] </ref> . After beginning a new mortgage, it is rare that a home buyer will immediately refinance. In fact, for the first 30 months after starting a new mortgage, the rate of prepayment grows slowly.
Reference: [25] <author> Xian-He Sun, Lionel M. Ni, </author> <title> Scalable Problems and Memory-Bounded Speedup, </title> <journal> Journal of Parallel and Distributed Computing 19, </journal> <volume> p27-37, </volume> <year> 1993 </year>
Reference-contexts: We discuss the three models of parallel speedups as defined by Sun <ref> [ 25 ] </ref> for non-specific parallel processing. They are fixed-size, fixed-time, and memory-bounded speedup. Fixed-size speedup fixes the problem size and assumes that more processors can be used to solve a problem faster. <p> Fixed-size speedup 121 and network overhead were defined in section 4.2, where parallel speedup was defined based on the definitions by <ref> [ 25 ] </ref> . To come up with the theoretical speeds, the time to compute the simulation using DNBU (455.7 seconds) was divided by the number of slaves processors. The time returned by DNBU would be referred to by Sun [ 25 ] as T 1 (W ). <p> where parallel speedup was defined based on the definitions by <ref> [ 25 ] </ref> . To come up with the theoretical speeds, the time to compute the simulation using DNBU (455.7 seconds) was divided by the number of slaves processors. The time returned by DNBU would be referred to by Sun [ 25 ] as T 1 (W ). Also since pvm3 takes a non-trivial amount of time to initialize and setup, we factored out the startup costs to give a more accurate time comparison. From this table we notice a few interesting trends.
Reference: [26] <author> Stephen C. Dewhurst, Kathy T. Stark, </author> <title> Programming in C++, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1989. </year>
Reference-contexts: All optional functions have default functionality, but the user may override this default functionality. The optional functions should be overridden in cases where there is some domain specific speed up that is difficult to embody in the APIs default functionality. The code is written in C++ <ref> [ 26 ] </ref> . The dnbobj class encapsulates the core DNB API. The class has only two pure virtual functions, and ten optional functions that have default functionality. However, the default functionality is not sufficient to run DNB effectively.
Reference: [27] <author> W.H. Press, B.P. Flannery, S.A. Teukolsky, W.T. Vetterling, </author> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing, </booktitle> <volume> volume 2, </volume> <publisher> Cambridge University Press, </publisher> <address> Cambridge England, </address> <year> 1988. </year>
Reference-contexts: This is a symptom of the trade off to avoid the exponential explosion in the amount of information that must be stored. C Code for the Vegas algorithm can be found in the book Numerical Recipes in C, Vol. 2 <ref> [ 27 ] </ref> . Unlike the other VRTs, Vegas is not usually referenced in the VRT surveys (see the following recent references [ 22; 23; 27 ] ). Vegas relates to DNB in the same way as importance sampling. <p> C Code for the Vegas algorithm can be found in the book Numerical Recipes in C, Vol. 2 [ 27 ] . Unlike the other VRTs, Vegas is not usually referenced in the VRT surveys (see the following recent references <ref> [ 22; 23; 27 ] </ref> ). Vegas relates to DNB in the same way as importance sampling. Vegas will be re-visited in section 5.1 when we discuss VRTs that support incremental update. 2.3.4 Control Variates Control Variates (CV) uses an auxiliary variable to correct for disproportionate sampling. <p> This is an ideal environment for DNB since every function evaluation is extremely costly. Home buyers take mortgages at local banks. Local banks then sell these mort 1 We would like to point out that we used the rand2 () function from Numerical Recipes in `C' <ref> [ 27 ] </ref> , to generate our uniform random numbers. This can be seen in our source code in the appendix. 98 gages to an issuing agency such as the companies FreddieMac or GNMA. <p> Notice that the pre-payment for each iteration is calculated as: ppay = factor [i-1] * prepay [i] * fm Based on the unscheduled principal payments, we calculate q, which represents the percentage of prepayments. The function rtbis () is a `zero-finding' function, from Numerical Recipes in C <ref> [ 27 ] </ref> , as part of our IRR calculation. The rtbis () procedure implements the well known `bisection method'. <p> The CRASH RATE is the post crash rate. The function gasdev is from Numerical Recipes in C <ref> [ 27 ] </ref> . The gasdev routine calculates and returns a normal deviate with a volatility of 1. In reality interest rates seem to follow more of a `log normal mean reverting' process, rather than a strict log normal process, as we have implemented.
Reference: [28] <author> A.V. Aho, J.E. Hopcroft, and J.D. Ullman, </author> <title> Data Structures and Algorithms, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass, </address> <year> 1983. </year>
Reference-contexts: We present two software tools that aid in the testing process in sections 6.5 and 6.6, and the notion of speedup is formalized in section 4.2. * What algorithms are efficient for locating neighborhoods? We present one specific data structure and algorithm based on chained hashing <ref> [ 28 ] </ref> that we also use in our software tools. With this algorithm we have reduced the number of points to search by about a factor of 8 to 10 for our experiments. <p> We have chosen a variant of a hash table with a chaining conflict resolution strategy as our default data structure for storing evaluated vectors. See <ref> [ 28 ] </ref> for more information on hash tables and conflict resolution strategies. The hash table has DNBTABLEN buckets. DNBTABLEN can be any length set by the user. Any collisions will be added to the buckets collision list (chained).
Reference: [29] <author> Kai Hwang, Faye A. </author> <title> Briggs,Computer Architecture and Parallel Processing, </title> <publisher> Mc Graw-Hill, </publisher> <year> 1984. </year>
Reference-contexts: We can also have multiple masters to accommodate larger parallel processors and networks with distinct communication domains, as displayed in figure 1.4 (bus 1, 2, and 3 are independent communication domains). Independent communication domains can be easily configured on many architectures such as meshes, trees, and hyper-cubes (see <ref> [ 29 ] </ref> for descriptions of parallel architectures). These architectures follow the global coordinator and multiple coordinator architectures studied by [ 16; 17; 18 ] . DNB allows for incremental update of an MCS (for distribution changes only). <p> It is conceivable that DNB could be run on a SIMD machine (Single Instructions Multiple Data). 1 We decided not to skater our research focus, and thus do not consider, the older and less popular SIMD parallel model. For more information on parallel architectures and machines see <ref> [ 29 ] </ref> . Unlike the classical VRTs, DNB is dependent upon previously evaluated vectors. Therefore, DNB requires special consideration and algorithms when executed in a parallel environment. Most VRTs are basically identical on uniprocessors and multiprocessors.
Reference: [30] <editor> Hammersley, J.M. and Handscomb, D. C., </editor> <title> Monte Carlo Methods, </title> <publisher> Nethuen & Co., Ltd., </publisher> <address> London, </address> <year> 1964. </year>
Reference-contexts: INTRODUCTION AND STATEMENT OF PROBLEM Monte Carlo Simulation (MCS) is an important numerical tool in many areas of research (see <ref> [ 30; 37 ] </ref> ). MCS is used to perform function approximations when the function is unknown or poorly understood. If the function has a closed form integral solution, integration is the technique of choice for accurately computing means and quantiles. <p> Randomization is widely used to solve both continuous and discrete problems. Examples of these problems appear in almost every branch of science <ref> [ 30; 37 ] </ref> . The first publication which considers the Monte Carlo algorithm is probably the 1949 paper of Metropolis and Ulam [ 36 ] . In this section we present the ideas underlying Monte Carlo algorithms and randomization in general for the problem of multivariate integration. <p> We closely follow the introductory results given by Spassimir Paskov in his survey paper on 23 random points versus quasi-random points for multivariate integration [ 35 ] . We present a very light introduction, for a more detailed description of Monte Carlo algorithms we refer the reader to <ref> [ 35; 30; 37 ] </ref> , and for an in depth general randomization discussion see [ 38 ] . For the purposes of this discussion, we will assume that the known pseudo-random number algorithms are sufficiently random to produce good MC approximations. <p> VRT surveys published in the last few years including [ 1; 19; 3; 4; 5; 6 ] , plus many other chapters appearing in most books on MCS including <ref> [ 30; 31; 32; 37 ] </ref> . With the preponderance of surveys, one would be lead to believe that VRTs is a quickly changing field. In fact, very little has changed in the last few years, and the surveys contain essentially the same information. <p> All these characteristics make geometric parallelism a clear choice over algorithmic parallelism, if geometric parallelism can be implemented in the memory constraints of the system. 3 The Ising spin problem and the transport problem are common molecular physics problem. Semiconductor simulation is an electrical engineering/physics problem. See <ref> [ 30; 31; 32 ] </ref> for problem descriptions. 37 2.4.3 Scheduling Once the parallelism of the problem has been determined, rules are needed to assign K PECs to P processors. We can make assignments either statically or dynamically. Scheduling seeks to maximize processor utilization in parallel processing.
Reference: [31] <author> Bratley, P., B.L. Fox, and L.E. </author> <title> Schrage, A Guide to Simulation, </title> <publisher> Springer-Verlag, </publisher> <address> NY, </address> <year> 1983. </year>
Reference-contexts: VRT surveys published in the last few years including [ 1; 19; 3; 4; 5; 6 ] , plus many other chapters appearing in most books on MCS including <ref> [ 30; 31; 32; 37 ] </ref> . With the preponderance of surveys, one would be lead to believe that VRTs is a quickly changing field. In fact, very little has changed in the last few years, and the surveys contain essentially the same information. <p> All these characteristics make geometric parallelism a clear choice over algorithmic parallelism, if geometric parallelism can be implemented in the memory constraints of the system. 3 The Ising spin problem and the transport problem are common molecular physics problem. Semiconductor simulation is an electrical engineering/physics problem. See <ref> [ 30; 31; 32 ] </ref> for problem descriptions. 37 2.4.3 Scheduling Once the parallelism of the problem has been determined, rules are needed to assign K PECs to P processors. We can make assignments either statically or dynamically. Scheduling seeks to maximize processor utilization in parallel processing.
Reference: [32] <author> Law, A.M. and W.D. </author> <title> Kelton,Simulation Modeling and Analysis, </title> <publisher> McGraw-Hill, </publisher> <address> NY, </address> <year> 1982. </year>
Reference-contexts: VRT surveys published in the last few years including [ 1; 19; 3; 4; 5; 6 ] , plus many other chapters appearing in most books on MCS including <ref> [ 30; 31; 32; 37 ] </ref> . With the preponderance of surveys, one would be lead to believe that VRTs is a quickly changing field. In fact, very little has changed in the last few years, and the surveys contain essentially the same information. <p> All these characteristics make geometric parallelism a clear choice over algorithmic parallelism, if geometric parallelism can be implemented in the memory constraints of the system. 3 The Ising spin problem and the transport problem are common molecular physics problem. Semiconductor simulation is an electrical engineering/physics problem. See <ref> [ 30; 31; 32 ] </ref> for problem descriptions. 37 2.4.3 Scheduling Once the parallelism of the problem has been determined, rules are needed to assign K PECs to P processors. We can make assignments either statically or dynamically. Scheduling seeks to maximize processor utilization in parallel processing.
Reference: [33] <author> John A. Rice, </author> <title> Mathematical Statistics and Data Analysis, </title> <publisher> Wadsworth & Brooks/Cole, </publisher> <address> Pacific Grove, California, </address> <year> 1988. </year>
Reference-contexts: In this section we look at the sources of bias and an experimental solution to detect bias. "A population mean estimator is said to be unbiased if its expectation equals the quantity we wish to estimate" <ref> [ 33 ] </ref> . However this seemingly clear definition is slightly ambiguous. We will interpret this definition to mean that if the mean estimator shows a systematic error during the computation, even though its expectation may eventually converge to the quantity we wish to estimate, it is biased. <p> Due to the strong law of large numbers <ref> [ 33 ] </ref> , the Monte Carlo algorithm converges almost surely. The expected error of the Monte Carlo algorithm for a function f is defined by Z (m (f ) U n (f )) dt 1 : : : dt n ; where stands for the Lebesgue measure. <p> However, this does not guarantee anything about one particular run of the algorithm, although, random sampling theorems tell us that as more function samples are taken, the sample variance of the output approaches the true variance of the function <ref> [ 33 ] </ref> . VRTs are interesting, because they attempt to reduce the expected error. <p> DNBinc does not replace DNBtool, but rather should be used after DNBtool has determined that the simulation is properly applied, to test issues unique to incremental update. 7 This is a standard minimum number quoted in many statistics books including <ref> [ 33 ] </ref> . 95 Like DNBtool, all the analysis tables are in Latex format. DNBinc tool outputs the following data tables: * Hit ratio For each trial and bin, h * is reported.
Reference: [34] <author> Roger T. Stevens, </author> <title> Fractal programming in C, M&T Books, </title> <address> Redwook City, California, </address> <year> 1989. </year>
Reference-contexts: involved in DNB by investigating the easy to understand and extremely visual problem of predicting the number of black pixels in a fractal-like curve. 1 A fractal can be formally stated with Benoit Mandelbrot's classical definition: "a fractal is a curve whose Hausdorff-Besicovitch dimension is larger than its Euclidean dimension"(see <ref> [ 34 ] </ref> ). Fractals are a natural way to represent some of the objects that occur in nature. <p> Our goal is to estimate the number of black pixels in figure 3.1. The fractal curve equation we will use is very similar to the well known Mandelbrot Set <ref> [ 34 ] </ref> . We present a minor modification by adding a sine wave function to make the border more `messy' and therefore more difficult to estimate.
Reference: [35] <author> Spassimir Paskov, </author> <title> Random Points Versus Quasi-Random Points for Mul-tivariate Integration, </title> <institution> Columbia University Deptment of Computer Science, </institution> <note> technical papers, </note> <month> Sept. </month> <year> 1991 </year>
Reference-contexts: In this section we present the ideas underlying Monte Carlo algorithms and randomization in general for the problem of multivariate integration. We closely follow the introductory results given by Spassimir Paskov in his survey paper on 23 random points versus quasi-random points for multivariate integration <ref> [ 35 ] </ref> . We present a very light introduction, for a more detailed description of Monte Carlo algorithms we refer the reader to [ 35; 30; 37 ] , and for an in depth general randomization discussion see [ 38 ] . <p> We closely follow the introductory results given by Spassimir Paskov in his survey paper on 23 random points versus quasi-random points for multivariate integration [ 35 ] . We present a very light introduction, for a more detailed description of Monte Carlo algorithms we refer the reader to <ref> [ 35; 30; 37 ] </ref> , and for an in depth general randomization discussion see [ 38 ] . For the purposes of this discussion, we will assume that the known pseudo-random number algorithms are sufficiently random to produce good MC approximations. <p> This makes sense since the vectors closer to the candidate vector might help produce a better estimate. This would be a good area for future research. Also, experimentation should be done to see how DNB interacts with quasi-Monte Carlo <ref> [ 35 ] </ref> . Normally DNB assumes random numbers, and pseudo-random numbers are used in most cases. In quasi-Monte Carlo algorithms, pseudo-random numbers are replaced by cleverly devised deterministic numbers. One of the best known deterministic number streams is the Hammersley points [ 35 ] . <p> to see how DNB interacts with quasi-Monte Carlo <ref> [ 35 ] </ref> . Normally DNB assumes random numbers, and pseudo-random numbers are used in most cases. In quasi-Monte Carlo algorithms, pseudo-random numbers are replaced by cleverly devised deterministic numbers. One of the best known deterministic number streams is the Hammersley points [ 35 ] . We have previously mentioned, our incremental update analysis made the assumption that using a previous iteration's data points would not add an additional bias to the simulation for subsequent iterations.
Reference: [36] <author> Metropolis, N. and Ulam, S., </author> <title> The Monte Carlo method, </title> <journal> Journal American Statistics Assoc., </journal> <volume> vol 44, </volume> <year> 1949. </year>
Reference-contexts: Randomization is widely used to solve both continuous and discrete problems. Examples of these problems appear in almost every branch of science [ 30; 37 ] . The first publication which considers the Monte Carlo algorithm is probably the 1949 paper of Metropolis and Ulam <ref> [ 36 ] </ref> . In this section we present the ideas underlying Monte Carlo algorithms and randomization in general for the problem of multivariate integration.
Reference: [37] <author> Kalos, Malvin H. and Whitlock, Paula A., </author> <title> Monte Carlo Methods, Volume I, </title> <publisher> John Whiley and Sons, </publisher> <year> 1986. </year>
Reference-contexts: INTRODUCTION AND STATEMENT OF PROBLEM Monte Carlo Simulation (MCS) is an important numerical tool in many areas of research (see <ref> [ 30; 37 ] </ref> ). MCS is used to perform function approximations when the function is unknown or poorly understood. If the function has a closed form integral solution, integration is the technique of choice for accurately computing means and quantiles. <p> Because of the excessive amount of computations that must be performed to obtain sufficiently accurate estimations, MCS is a popular application of parallel computing. MCS has traditionally mapped extremely well to parallel computers and has enjoyed large speed ups <ref> [ 7; 9; 37 ] </ref> . The MCS problem can be formalized as follows. <p> Randomization is widely used to solve both continuous and discrete problems. Examples of these problems appear in almost every branch of science <ref> [ 30; 37 ] </ref> . The first publication which considers the Monte Carlo algorithm is probably the 1949 paper of Metropolis and Ulam [ 36 ] . In this section we present the ideas underlying Monte Carlo algorithms and randomization in general for the problem of multivariate integration. <p> We closely follow the introductory results given by Spassimir Paskov in his survey paper on 23 random points versus quasi-random points for multivariate integration [ 35 ] . We present a very light introduction, for a more detailed description of Monte Carlo algorithms we refer the reader to <ref> [ 35; 30; 37 ] </ref> , and for an in depth general randomization discussion see [ 38 ] . For the purposes of this discussion, we will assume that the known pseudo-random number algorithms are sufficiently random to produce good MC approximations. <p> VRT surveys published in the last few years including [ 1; 19; 3; 4; 5; 6 ] , plus many other chapters appearing in most books on MCS including <ref> [ 30; 31; 32; 37 ] </ref> . With the preponderance of surveys, one would be lead to believe that VRTs is a quickly changing field. In fact, very little has changed in the last few years, and the surveys contain essentially the same information.
Reference: [38] <author> Traub, J.F., Wasilokowski, G.W. and Wozniakowski, H., </author> <title> Information-based Complexity, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: We present a very light introduction, for a more detailed description of Monte Carlo algorithms we refer the reader to [ 35; 30; 37 ] , and for an in depth general randomization discussion see <ref> [ 38 ] </ref> . For the purposes of this discussion, we will assume that the known pseudo-random number algorithms are sufficiently random to produce good MC approximations. Classical Monte Carlo algorithms are best illustrated though multivariate integration.
Reference: [39] <author> Jason Glazier and Salvatore Stolvo, </author> <title> Dynamic Neighborhood Bounding for Monte Carlo Simulation, </title> <booktitle> Proceedings of the 1993 Winter Simulation Conference. </booktitle>
Reference-contexts: If the algorithm used the same search order every time, points in the beginning of the search order will have a disproportionately large number of hits and will bias the output. We actually experienced this phenomenon in the PSR case study presented in our first DNB paper <ref> [ 39 ] </ref> . When single hits were used, the strategy introduced significantly more bias than multiple hits. To experimentally combat bias, we have produced the software tool called DNBtool to be discussed in section 6.5. <p> We saw the value in having good testing tools. DNBtool and DNBinc tool 143 greatly simplified the testing process. The Latex formated tables they produced were used in this thesis without modification. For the initial case studies (see <ref> [ 39 ] </ref> ), many simulations were executed over the course of weeks, and the results had to be summarised `by hand'. With DNBtool, the simulations were automatically completed and summarised in a matter of hours.
Reference: [40] <author> A. McKerrell and L.M. Delves, </author> <title> Monte Carlo simulation of neutron diffusion on SIMD architectures, </title> <booktitle> Parallel Computing, </booktitle> <address> n8, </address> <year> 1988. </year> <month> 150 </month>
Reference-contexts: Parallel processing is an important technique for delivering more computing cycles than traditional serial processing. Although MCS as a general technique appears to be easily parallelized, in fact it can be irregular and difficult to parallelize (see <ref> [ 40; 10; 7 ] </ref> ). Some of the sources of the irregularity are due to memory limitations that do not alow one PEC per processor, or difficulties of efficiently using all the processors of a SIMD 2 machines.
References-found: 40

