URL: file://ftp.ai.univie.ac.at/papers/oefai-tr-96-01.ps.Z
Refering-URL: http://www.cse.unsw.edu.au/~mbh/context/context_data.html
Root-URL: 
Email: gerhard@ai.univie.ac.at  
Title: Recognition and Exploitation of Contextual Clues via Incremental Meta-Learning (Extended Version)  
Author: Gerhard WIDMER 
Keyword: Key words: on-line learning, concept drift, context dependence, transfer  
Note: The meta-learning framework is very general, and a number of instantiations and extensions of the model are conceivable. Some of these are briefly discussed.  
Address: Schottengasse 3, A-1010 Vienna, Austria  
Affiliation: Department of Medical Cybernetics and Artificial Intelligence, University of Vienna, and Austrian Research Institute for Artificial Intelligence,  
Abstract: Daily experience shows that in the real world, the meaning of many concepts heavily depends on some implicit context, and changes in that context can cause more or less radical changes in the concepts. Incremental concept learning in such domains requires the ability to recognize and adapt to such changes. This paper presents a solution for incremental learning tasks where the domain provides explicit clues as to the current context (e.g., attributes with characteristic values). We present a general two-level learning model, and its realization in a system named MetaL(B), that can learn to detect certain types of contextual clues, and can react accordingly when a context change is suspected. The model consists of a base level learner that performs the regular on-line learning and classification task, and a meta-learner that identifies potential contextual clues. Context learning and detection occur during regular on-line learning, without separate training phases for context recognition. Experiments with synthetic domains as well as a `real-world' problem show that MetaL(B) is robust in a variety of dimensions and produces substantial improvement over simple object-level learning in situations with changing contexts. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D., Kibler D., and Albert, </author> <title> M.K (1991). Instance-Based Learning. Machine Learning 6(1), </title> <publisher> pp.37-66. </publisher>
Reference-contexts: The relative frequencies required for the application of Bayes' rule must now be computed dynamically from the set of currently relevant instances (which are a subset of the window). On the other hand, this is no more expensive than the lookup in an instance-based learning algorithm <ref> (Aha et al., 1991) </ref> would be, and the fixed window size at least puts an upper bound on the number of examples that must be examined at each point in the learning process. In the learning experiments performed so far, we have not encountered any practical problems in this respect. <p> The effect can be most clearly seen in the Schubert experiment above. In terms of the dynamic selection of predictors, there is some relation between MetaL (B) and instance-based learners like IB3 <ref> (Aha et al., 1991) </ref>. IB3 determines predictive exemplars by monitoring each stored instance's individual predictive accuracy. Only those exemplars are used for prediction that have established a stable classification record. MetaL (B), on the other hand, uses identified contextual attributes to select predictive exemplars.
Reference: <author> Bergadano, F., Matwin, S., Michalski, R.S., and Zhang, J. </author> <year> (1992). </year> <title> Learning Two-tiered Descriptions of Flexible Contexts: The POSEIDON System. Machine Learning 8(1), </title> <publisher> pp.5-43. </publisher>
Reference-contexts: Algorithms for learning both tiers were implemented in the system POSEIDON <ref> (Bergadano et al., 1992) </ref>. Making adjustments (before or after base-level learning) to account for context effects has been the focus of some recent application-oriented research. Watrous and Towell (1995) describe a neural network for ECG monitoring that is augmented with an explicit `patient model'.
Reference: <author> Caruana, R.A. </author> <year> (1993). </year> <title> Multitask Learning: A Knowledge-based Source of Inductive Bias. </title> <booktitle> In Proceedings of the 10th International Conference on Machine Learning (ML-93), </booktitle> <address> Amherst, MA. San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Katz, A.J., Gately, M.T, and Collins, D.R. </author> <year> (1990). </year> <title> Robust Classifiers Without Robust Features. Neural Computation 2(4), </title> <publisher> pp.472-479. </publisher>
Reference: <author> Kilander, F. and Jansson, C.G. </author> <year> (1993). </year> <title> COBBIT A Control Procedure for COBWEB in the Presence of Concept Drift. </title> <booktitle> In Proceedings of the European Conference on Machine Learning (ECML-93), </booktitle> <address> Vienna, Austria. </address>
Reference-contexts: Forgetting is controlled by a window over the stream of incoming examples, and the window size is dynamically adjusted by a heuristic that monitors the learning process. Similar ideas were also put forward for unsupervised learning situations <ref> (Kilander and Jansson, 1993) </ref>. In addition, FLORA3 (Widmer and Kubat, 1993) introduces a mechanism for storing and reusing learned concept descriptions in environments with changing contexts; the goal is faster re-adaptation to concepts that had already appeared in the past.
Reference: <author> Kononenko, I. </author> <year> (1991). </year> <title> Semi-naive Bayesian classifier. </title> <booktitle> In Proceedings of the 5th European Working Session on Learning (ECML-91), Porto. </booktitle> <address> London: </address> <publisher> Pitman. </publisher>
Reference: <author> Langley, P. </author> <year> (1993). </year> <title> Induction of Recursive Bayesian Classifiers. </title> <booktitle> In Proceedings of the European Conference on Machine Learning (ECML-93), </booktitle> <address> Vienna, Austria. </address>
Reference-contexts: A simple Bayesian classifier is a probabilistic classification scheme that uses Bayes' theorem to determine the probability that an instance belongs to a particular class, given the instance's description. A very readable introduction to simple Bayesian classifiers can be found, e.g., in <ref> (Langley, 1993) </ref>. In the following, we assume that examples are described in terms of (discrete) attributes a i ; we will use the term feature for a specific attribute-value combination, notated as a i : v ij . Examples are assumed to belong to mutually exclusive classes c i .
Reference: <author> Littlestone, N. </author> <year> (1988). </year> <title> Learning Quickly When Irrelevant Attributes Abound: A New Linear-threshold Algorithm. Machine Learning 2(4), </title> <publisher> pp.285-318. </publisher>
Reference-contexts: 1 Motivation This article is concerned with incremental supervised concept learning, also known as on-line learning <ref> (e.g., Littlestone, 1988) </ref>. A learning algorithm is incrementally processing a stream of examples coming in. Each instance is associated with a given class label. The learner first attempts to classify the instance, then learns the correct class label, and updates some internal concept hypothesis, if appropriate.
Reference: <author> Michalski, R.S. </author> <year> (1987). </year> <title> How to Learn Imprecise Concepts: A Method Employing a Two-tiered Knowledge Representation for Learning. </title> <booktitle> In Proceedings of the 4th International Workshop on Machine Learning, </booktitle> <address> Irvine, CA. Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ourston, D. and Mooney, R.J. </author> <year> (1991). </year> <title> Improving Shared Rules in Multiple Category Domain Theories. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning (ML-91), </booktitle> <address> Evanston, Ill. San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Pratt, L.Y., Mostow, J., and Kamm, C.A. </author> <year> (1991). </year> <title> Direct Transfer of Learned Information Among Neural Networks. </title> <booktitle> In Proceedings of the 9th National Conference on Artificial Intelligence (AAAI-91), </booktitle> <address> Anaheim, CA. </address> <note> 21 Schlimmer, J.C. </note> <author> and Granger, R.H. </author> <year> (1986). </year> <title> Beyond Incremental Processing: Tracking Concept Drift. </title> <booktitle> In Proceedings of the 5th National Conference on Artificial Intelligence (AAAI-86), </booktitle> <address> Philadelphia, PA. </address>
Reference: <author> Schlimmer, J.C. and Granger, R.H. </author> <year> (1986). </year> <title> Incremental Learning from Noisy Data. Machine Learning 1(3), </title> <publisher> pp.317-354. </publisher>
Reference: <author> Thrun, S. and Mitchell, T.M. </author> <year> (1995). </year> <title> Learning One More Thing. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <address> Montreal. San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Turney, P.D. </author> <year> (1993). </year> <title> Robust Classification with Context-Sensitive Features. </title> <booktitle> In Proceedings of the 6th International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems (IEA/AIE-93), </booktitle> <address> Edinburgh, Scotland. </address>
Reference-contexts: The intent is to reduce the context-sensitivity of certain features. The methods assume that the contextual and context-sensitive features are known a priori. The methods are demonstrated in a number of practical applications, among them, the diagnosis of aircraft engines <ref> (Turney and Halasz, 1993) </ref>. All these approaches assume that contextually relevant attributes are known, and that the learner is in some way explicitly trained on different contexts.
Reference: <author> Turney, P.D. and Halasz, M. </author> <year> (1993). </year> <title> Contextual Normalization Applied to Aircraft Gas Turbine Engine Diagnosis. </title> <journal> Journal of Applied Intelligence 3, pp.109-129. </journal>
Reference-contexts: The intent is to reduce the context-sensitivity of certain features. The methods assume that the contextual and context-sensitive features are known a priori. The methods are demonstrated in a number of practical applications, among them, the diagnosis of aircraft engines <ref> (Turney and Halasz, 1993) </ref>. All these approaches assume that contextually relevant attributes are known, and that the learner is in some way explicitly trained on different contexts.
Reference: <author> Watrous, R.L. </author> <year> (1993). </year> <title> Speaker Normalization and Adaptation Using Second-order Connectionist Networks. </title> <journal> IEEE Transactions on Neural Networks 4(1), pp.21-30. </journal>
Reference-contexts: The model can be trained on individual patients. The network thus has an explicit context model that can be adjusted to the current context in a separate training phase. A similar approach was taken to adapt a classifier to different speakers in speech recognition <ref> (Watrous, 1993) </ref>. Earlier, Katz et al. (1990) had described a two-stage neural network classifier, where a higher-level network learned to switch between a set of n base-level classifier. The application domain was the recognition of targets on infrared and daytime television images.
Reference: <author> Watrous, R.L. and Towell, G. </author> <year> (1995). </year> <title> A Patient-adaptive Neural Network ECG Patient Monitoring Algorithm. </title> <booktitle> In Proceedings Computers in Cardiology 1995, </booktitle> <address> Vienna, Austria. </address>
Reference: <author> Widmer, G. and Kubat, M. </author> <year> (1992). </year> <title> Learning Flexible Concepts from Streams of Examples: </title> <booktitle> FLORA2. In Proceedings of the 10th European Conference on Artifical Intelligence (ECAI-92), </booktitle> <address> Vienna. Chichester, UK: </address> <publisher> Wiley. </publisher>
Reference: <author> Widmer, G. and Kubat, M. </author> <year> (1993). </year> <title> Effective Learning in Dynamic Environments by Explicit Context Tracking. </title> <booktitle> In Proceedings of the 6th European Conference on Machine Learning (ECML-93), </booktitle> <address> Vienna. Berlin: </address> <publisher> Springer Verlag. </publisher>
Reference-contexts: Forgetting is controlled by a window over the stream of incoming examples, and the window size is dynamically adjusted by a heuristic that monitors the learning process. Similar ideas were also put forward for unsupervised learning situations (Kilander and Jansson, 1993). In addition, FLORA3 <ref> (Widmer and Kubat, 1993) </ref> introduces a mechanism for storing and reusing learned concept descriptions in environments with changing contexts; the goal is faster re-adaptation to concepts that had already appeared in the past.
Reference: <author> Widmer, G. </author> <year> (1994). </year> <title> Combining Robustness and Flexibility in Learning Drifting Concepts. </title> <booktitle> In Proceedings of the 11th European Conference on Artificial Intelligence (ECAI-94), </booktitle> <address> Amsterdam. Chich-ester, UK: </address> <publisher> Wiley. </publisher>
Reference-contexts: The results are as expected. Naturally, the slower the change, the longer the valley of decreased accuracy. But it is evident that the effectivity of meta-learning is more or less unaffected by the speed of change. In particular, unlike systems like FLORA4 <ref> (Widmer, 13 1994) </ref>, MetaL (B) has no problems with slow drift | it is the overall distributions of the predictivity of features that determine MetaL (B)'s behavior, not necessarily dramatic short-term changes. 4.5 Complex and imperfect contextual clues In real life, contexts may be defined by more complex combinations of features.
Reference: <editor> Widmer, G. and Kubat, M. (in press). </editor> <title> Learning in the Presence of Concept Drift and Hidden Contexts. Machine Learning (in press). </title> <type> 22 </type>
References-found: 21

