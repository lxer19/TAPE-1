URL: http://www.cs.berkeley.edu/~soumen/thesis.ps.Z
Refering-URL: http://www.cs.berkeley.edu/~soumen/
Root-URL: http://www.cs.berkeley.edu
Title: Efficient Resource Scheduling in Multiprocessors  
Author: by Soumen Chakrabarti 
Degree: Master of Science, University of California, Berkeley, 1992 Bachelor of Technology, Indian Institute of Technology, Kharagpur, 1991 A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Computer Science in the GRADUATE DIVISION of the UNIVERSITY of CALIFORNIA at BERKELEY Committee in charge: Professor Katherine Yelick, Computer Science, Chair Professor James Demmel, Computer Science and Mathematics Professor Dorit Hochbaum,  
Date: 1996  
Affiliation: Industrial Engineering and Operations Research  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. Adler, S. Chakrabarti, M. Mitzenmacher, and L. Rasmussen. </author> <title> Parallel randomized load balancing. </title> <booktitle> In Symposium on the Theory of Computing (STOC). ACM, </booktitle> <year> 1995. </year>
Reference-contexts: At one extreme, one can send each new block to a random server; at the other, one can get optimal load balance by maintaining exact global load information. We give a precise characterization of this tradeoff <ref> [1] </ref>. Initial simulation results agree with theoretical predictions. Extensive work has been done on all the problems studied. In Chapter 6 we will present a survey and classification of known results based on job and machine models. <p> Consider the following heuristic, which we call Prefix-Suffix. 1. Sort tasks in decreasing order of t j (1); i.e., let t 1 (1) t 2 (1) t L (1). 43 2. For 1 i L + 1, define p [i] = P 1j&lt;i t j (P ) (p <ref> [1] </ref> = 0), and define s [i] = Pack (P; fi; : : : ; Lg) (s [L + 1] = 0). 3. <p> Estimates of sizes of intermediate results can be used to estimate the memory and disk bandwidth resource vector ~r j . The running time of a job is roughly inversely proportional to the number of processors in the range <ref> [1; m j ] </ref>, but not the total available memory. <p> In what follows, consider the `th interval in time, namely, (t `1 ; t ` ]; other intervals are processed similarly. Step 1. Let J ` be the set of jobs that have arrived within time <ref> [1; t ` ] </ref> but have not already been scheduled. We remove from consideration any job which cannot be scheduled within the `-th interval because of dependence and consequent critical path length. <p> : : ; q h (s) ) is an ordered list, where q ` is the processor that executed s ` , for 1 ` h (s). * R ~ J is a set of jobs, and * 1 ; : : : ; h (s) is a partition of <ref> [1; t ] </ref> such that - R " fs 1 ; : : : ; s h (s) g = ;. Each job in R become "ready" and arrives into q j during interval j , for some j, 1 j h (s). 92 Proof. <p> Include R h (s) into R, and set h (s) = [t 0 ; t ]. Then continue the construction from t 0 1 in an iterative manner. From Figure 5.1 (b), it can be seen that the processing times of nodes in R must cover all of <ref> [1; t ] </ref>, except for the time spent in processing nodes s 1 ; : : : ; s h (s) , which is at most ( ~ J ), and the time spent finishing jobs in progress when s ` arrives at q ` , which accounts for at most <p> Thus let R = S ` R ` and observe that t (R) t ( ~ J ) h ( ~ J )T (J ). We have also constructed a ordered partition = ( 1 ; : : : ; h (s) ) of <ref> [1; t ] </ref>. We also note the following fact, which follows, e.g., from considering the two cases b log 2a a and b log 2a &gt; a. <p> It would be interesting to tighten the makespan estimates in this section, as well as to provide randomized lower bounds. The performance of multiple round strategies <ref> [9, 1] </ref> for dynamic job graphs remains open, as is their effect on further reducing the load imbalance. <p> On the other hand, reducing T limits the maximum load at the expense of many rounds. We will quantify this tradeoff in x5.3.3. Our analysis exploits a basic tool that expresses the relation between occupancy distributions and Poisson distributions <ref> [63, 1] </ref>. Using this tool, we show that after a fixed number of rounds r, the final maximum load is O q log log n with high probability. We also show via lower bounds that no better load balance can be achieved by a large class of load balancing algorithms. <p> We can show a corresponding upper bound <ref> [1] </ref>; we omit the details. Notice that this is a synchronous protocol; bins need to know when all balls are done sending requests before sending out load information. <p> The results above can be extended to d-way choices by the balls, where d is constant or grows sufficiently slowly with n. The details can be found in Adler et al's paper <ref> [1] </ref>. Also, the general case of m n balls and n bins can be handled readily. Since the maximum load grows very slowly with n for all these schemes, and our analysis is asymptotic and probabilistic, we were interested in performance seen in actual simulations.
Reference: [2] <author> M. Adler, S. Chakrabarti, M. Mitzenmacher, and L. Rasmussen. </author> <title> Parallel randomized load balancing. </title> <note> 1996. Journal version of [1] in preparation. </note>
Reference-contexts: We omit the manipulations. The techniques carry over directly to the case where there are m balls and n bins, and each ball sends requests to d &gt; 2 bins, by generalizing each edge to a d-vertex hyperedge. The details can be found in <ref> [2] </ref>. 5.3.4 Other protocols Other parallelizations of the Azar et al (n)-round protocol are possible. For example, we can run the following protocol, called Sibling (2): 1. In parallel each ball picks two random bins and sends them requests. 2.
Reference: [3] <author> F. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante. </author> <title> An overview of the ptran analysis system for multiprocessing. </title> <booktitle> Proc. ACM 1987 International Conference on Supercomputing, </booktitle> <year> 1987. </year> <note> Also published in Journal of Parallel and Distributed Computing, </note> <month> Oct., </month> <year> 1988, </year> <pages> 5(5) pages 617-640. </pages>
Reference-contexts: shown in the last column for comparison. running example to illustrate the operation of the steps of the algorithm. 2.4.1 Representation and notation We represent the program using the augmented control flow graph (CFG), which makes loop structure more explicit than the standard CFG by placing preheader and postexit nodes <ref> [3, 100] </ref> before and after loops. These extra nodes also provide convenient locations for summarizing dataflow information for the loop. The CFG is a directed graph where each node is a basic block, a sequence of statements without jumps. Execution starts at the ENTRY node.
Reference: [4] <author> N. Alon. </author> <title> Eigenvalues and expanders. </title> <journal> Combinatorica, </journal> <volume> 6(2) </volume> <pages> 83-96, </pages> <year> 1986. </year>
Reference-contexts: Arbitrary DAGs. While we have handled hierarchical job graphs such as forests or series-parallel graphs, the general DAG case is open. It is known that precedence-constrained knapsack with general precedence is strongly N Phard [75], unlike forests. We show that settling the approximability issue will be challenging <ref> [4] </ref>. This shows that the framework of [70] may need modification to handle DAG's, not necessarily that the scheduling problem is difficult.
Reference: [5] <author> S. P. Amarasinghe and M. S. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Programming Language Design and Implementation (PLDI), </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year> <journal> ACM SIGPLAN. </journal>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation and generating the communication necessary to fetch values of non-local data referenced by a processor <ref> [72, 123, 20, 5, 21, 68] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data, for the following reasons. It is getting increasingly cost-effective to build multiprocessors from commodity hardware components and system software. <p> Particular technical references and their relation with this thesis are given in specific chapters. 6.1 Communication scheduling The results in Chapter 2 follow (and generalize) a large number of preliminary results in communication optimization, such as message vectorizing, coalescing, and redundancy elimination <ref> [72, 123, 20, 5, 21, 68] </ref>. Message vectorization is the technique of hoisting communication of array elements or sections out of loops to produce a single large message. Typically this works locally on single loop-nests [123, 72, 67, 83, 68].
Reference: [6] <author> T. Anderson, M. Dahlin, J. Neefe, D. Patterson, D. Roselli, and R. Wang. </author> <title> Serverless network file systems. </title> <booktitle> In ACM Symposium on Operating Systems Principle (SOSP). ACM, </booktitle> <month> Dec. </month> <year> 1995. </year> <note> To appear in TOCS, </note> <month> February </month> <year> 1996. </year>
Reference-contexts: In addition to the task scheduling application, this setting can be motivated by the problem of allocating file blocks in distributed "serverless" file systems like xFS <ref> [6] </ref>. In this setting there are n client workstations whose local disks comprise the file system, which can be abstracted as n servers. As clients write files, new disk blocks must be allocated in a decentralized way without overloading any particular server.
Reference: [7] <author> R. Arpaci, D. Culler, A. Krishnamurthy, S. Steinberg, and K. Yelick. </author> <title> Empirical evaluation of the CRAY-T3D: A compiler perspective. </title> <booktitle> In International Symposium on Computer Architecture. ACM SIGARCH, </booktitle> <year> 1995. </year>
Reference-contexts: Estimates are in part from <ref> [107, 117, 88, 7] </ref>; the network software are described in these references. to be very large on most distributed memory machines, although reasonable bandwidth can be supported for sufficiently large messages [109, 106]. See Table 2.1 for some idea of the CPU and communication speeds of current multiprocessors. <p> The curves were fit for 2 P 500 and 100 n = N 1=2 10000. An estimate of memory per processor in megabytes is given in the column marked M/P. Estimates for ff and fi are in part from <ref> [107, 117, 88, 7] </ref>. of processors, network latency, and network bandwidth. Using these given functions, we first estimate the parallel running time r (N; P ) for a given machine and problem, then fit Equation (3.1) to it using Matlab.
Reference: [8] <author> G. Attardi and C. Traverso. </author> <title> Strategy-accurate parallel Buchberger algorithms. </title> <booktitle> In International Conference on Parallel Symbolic Computation, </booktitle> <year> 1994. </year> <month> 116 </month>
Reference-contexts: In many application such as parallel search or branch and bound, the total work done is very sensitive to the job order, and one wishes to deviate from the best sequential order as little as possible <ref> [47, 8] </ref>. 5.2.3 Weighted occupancy At first we consider the weighted occupancy problem, where there is no precedence among jobs. There are n weighted balls (jobs), ball j having weight t j . These balls are thrown uniformly at random into P bins (processors).
Reference: [9] <author> Y. Azar, A. Broder, A. Karlin, and E. Upfal. </author> <title> Balanced allocations. </title> <booktitle> In Symposium on the Theory of Computing (STOC). ACM, </booktitle> <year> 1994. </year>
Reference-contexts: It would be interesting to tighten the makespan estimates in this section, as well as to provide randomized lower bounds. The performance of multiple round strategies <ref> [9, 1] </ref> for dynamic job graphs remains open, as is their effect on further reducing the load imbalance. <p> Recently, an important extension of this result was proven by Azar et al <ref> [9] </ref>. Suppose we place the balls sequentially, one at a time; for each ball, we choose two bins independently and uniformly at random, and place the ball in the less full bin. <p> Unfortunately, the new method requires the resting place of the balls to be determined sequentially <ref> [9] </ref>. This limits its applicability in parallel and distributed settings, a major drawback when compared to the simple approach, which has one parallel round of random assignment (we will give a precise definition of rounds later).
Reference: [10] <author> Y. Azar, B. Kalyanasundaram, S. Plotkin, K. Pruhs, and O. Waarts. </author> <title> Online load balancing of temporary tasks. </title> <booktitle> In Workshop on Algorithms and Data Structures (WADS), </booktitle> <volume> LNCS 709, </volume> <pages> pages 119-130, </pages> <year> 1993. </year>
Reference-contexts: Equivalently we scale jobs so that t min = 1 and t max = T . This is in keeping with recent analyses of online load balancing algorithms <ref> [10] </ref>, and is more broadly applicable than results with assumptions about distribution or variance. J will have an associated acyclic precedence relation J fi J.
Reference: [11] <author> S. B. Baden. </author> <title> Programming abstractions for dynamically partitioning and coordinating localized scientific calculations running on multiprocessors. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12(1) </volume> <pages> 145-157, </pages> <year> 1991. </year>
Reference-contexts: In adaptive mesh refinement (AMR) algorithms, there is task parallelism between meshes and data-parallelism within a mesh <ref> [11] </ref>. In computing eigenvalues of nonsymmetric matrices, the sign function algorithm does divide and conquer with matrix factorizations at each division [12]. In timing-level circuit simulation there is parallelism between separate subcircuits and parallelism within the model evaluation of each subcircuit [116].
Reference: [12] <author> Z. Bai and J. Demmel. </author> <title> Design of a parallel nonsymmetric eigenroutine toolbox, Part I. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Proceesing for Scientific Computing. </booktitle> <publisher> SIAM, </publisher> <year> 1993. </year> <note> Long version available as UC Berkeley Computer Science report all.ps.Z via anonymous ftp from tr-ftp.cs.berkeley.edu, directory pub/tech-reports/csd/csd-92-718. </note>
Reference-contexts: To find the n eigenvalues of A, which are complex in general, a new algorithm due to Bai and Demmel divides the complex plane into two half-planes and counts the number k of eigenvalues in one half-plane <ref> [12] </ref>. In terms of matrix operations, this is a data-parallel operation consisting mostly of several dense LU factorizations. k is not known before this operation. <p> In adaptive mesh refinement (AMR) algorithms, there is task parallelism between meshes and data-parallelism within a mesh [11]. In computing eigenvalues of nonsymmetric matrices, the sign function algorithm does divide and conquer with matrix factorizations at each division <ref> [12] </ref>. In timing-level circuit simulation there is parallelism between separate subcircuits and parallelism within the model evaluation of each subcircuit [116]. In sparse matrix factorization, multi-frontal algorithms expose task parallelism between separate dense sub-matrices and data parallelism within those dense matrices [85].
Reference: [13] <author> B. S. Baker, E. G. Coffman, and R. L. Rivest. </author> <title> Orthogonal packings in two dimensions. </title> <journal> SIAM Journal on Computing, </journal> <volume> 9(4) </volume> <pages> 846-855, </pages> <month> Nov. </month> <year> 1980. </year>
Reference-contexts: For independent jobs that need a fixed given number of processors, various rectangle packing algorithms naturally model the problem: every job can be regarded as a rectangle of width equal to the number of processors, and of height representing the running time <ref> [14, 13] </ref>. It is more realistic to assume that processors are a malleable resource: i.e., they can be traded for time. This tradeoff can be specified as an arbitrary function for the running time on any given number of processors.
Reference: [14] <author> B. S. Baker and J. S. Schwarz. </author> <title> Shelf algorithms for two-dimensional packing problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 12(3) </volume> <pages> 508-525, </pages> <year> 1983. </year>
Reference-contexts: For independent jobs that need a fixed given number of processors, various rectangle packing algorithms naturally model the problem: every job can be regarded as a rectangle of width equal to the number of processors, and of height representing the running time <ref> [14, 13] </ref>. It is more realistic to assume that processors are a malleable resource: i.e., they can be traded for time. This tradeoff can be specified as an arbitrary function for the running time on any given number of processors.
Reference: [15] <author> Y. Bartal, S. L. A. Marchetti-Spaccamela, J. Sgall, and L. Stougie. </author> <title> Multiprocessor scheduling with rejection. </title> <booktitle> In Symposium on Discrete Algorithms (SODA). ACM-SIAM, </booktitle> <year> 1996. </year>
Reference-contexts: Every task has a running time, and a penalty for rejection; the goal is to minimize the sum of the makespan of accepted tasks and the penalty of rejected tasks <ref> [15] </ref>. <p> For this model, constant factor makespan approximations are known that handle precedence between jobs exposed on-line. Furthermore, the algorithm is non-clairvoyant; i.e., it does not need to know the running time of a job before completion [51]. These and similar algorithms <ref> [15, 34] </ref> are essentially based on online gambling techniques. 6.3 Minsum metrics Most makespan results are relatively straightforward in that the approximation algorithms are compared against a very simple lower bound: the sum of average work per processor and critical path length [64, 51].
Reference: [16] <author> K. Belkhale and P. Banerjee. </author> <title> An approximate algorithm for the partitionable independent task scheduling problem. </title> <booktitle> In International Conference on Parallel Processing (ICPP). IEEE, </booktitle> <month> August </month> <year> 1990. </year> <note> Full version in technical reports UILU-ENG-90-2253 and CRHC-90-15, </note> <institution> University of Illinois, Urbana. </institution>
Reference-contexts: Several researchers have proposed support to take advantage of this mixed parallelism. In the theory area, the best known online scheduling algorithm for mixed parallelism is 2:62-optimal <ref> [16, 52] </ref>, and the best offline algorithm is 2-optimal [113, 87]. But these are worst-case guarantees. <p> This tradeoff can be specified as an arbitrary function for the running time on any given number of processors. For jobs with this model of trade-off, various constant-factor approximations for makespan are known for the special case where there is no precedence relation; i.e., the jobs are independent <ref> [16, 87] </ref>. Our results in Chapters 3 and 4 extend these results. In a slightly simpler but still reasonably realistic model of malleability, every job specifies a maximum number of processor up to which it will give perfect speedup.
Reference: [17] <author> C. Bischof, S. Huss-Lederman, X. Sun, A. Tsao, and T. Turnbull. </author> <title> Parallel performance of a symmetric eigensolver based on the invariant subspace decomposition approach. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <pages> pages 32-39, </pages> <address> Knoxville, TN, </address> <month> May </month> <year> 1994. </year> <journal> IEEE. </journal> <volume> 117 </volume>
Reference-contexts: It is clear that there is an optimal switched schedule that switches exactly once between the data-parallel and task-parallel phase. A first cut solution is to define J P = fj : t j (P ) &lt; t j (1)g; this has been used in practice <ref> [17] </ref>. This is overly conservative since it does not consider the complete ready list to decide the fate of a single task. <p> This successive separation process forms a binary tree with c = 4, d = 2, and a = 3=2. (We scale time so that the constant in O (N 3=2 ) becomes one.) For symmetric matrices, an algorithm similar in spirit is the beta-function technique of Bischof et al <ref> [17] </ref>.
Reference: [18] <author> R. D. Blumofe, M. Frigo, C. F. Joerg, C. E. Leiserson, and K. H. Randall. </author> <title> An analysis of DAG-consistent distributed shared-memory algorithms. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <address> Italy, </address> <month> June </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: Second, for evaluating efficiency gains with high accuracy, only regular trees could be considered. In spite of their simplicity, regular trees provide a good starting point for preliminary analysis <ref> [18] </ref>. 3.4.2.2 Examples Eigenvalue algorithms. Several eigenvalue algorithms exhibit mixed parallelism because they use divide and conquer. One such algorithm was described in Example 3.2.1 on page 40.
Reference: [19] <author> R. Blumwofe and C. Leiserson. </author> <title> Scheduling multithreaded computations by work stealing. </title> <booktitle> In Foundations of Computer Science (FOCS), </booktitle> <address> Santa Fe, NM, </address> <month> November </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: Third, a global data remap phase moves arrays distributed over all processors to the appropriate singleton context. At this point the work-lists of different processors are different. Finally, the ready-list behaves as a task-parallel distributed queue with work-stealing <ref> [19] </ref>. 3.5.2 Results We measured performance on two machines: a 25-processor Paragon and a 36-processor SP-2. The results are shown in Figure 3.9. Each graph plots performance (MFLOPS) against problem size. <p> We study work sharing, where busy processors forward jobs to random processors. Some other techniques are work stealing, where idle processors ask for work <ref> [19] </ref>, and diffusion, where neighbors exchange local load information and then move some jobs from busy to lazy processors [61]. 5.2.1 Models and notation The input comprises a set J of jobs presented to the algorithm in a distributed and on-line fashion. <p> In work stealing, the graph is expanded depth-first locally in each processor, and idle processors steal jobs nearest to the root <ref> [122, 19] </ref>. <p> We studied work sharing, where busy processors forward jobs to random processors. This is similar to Karp and Zhang's model [78]. Work sharing is good at dispersing hot-spots. Another technique that has been designed and analyzed lately is work stealing, where idle processors ask for work <ref> [19] </ref>. The theoretical bounds are only applicable to divide and conquer, where a finite, fixed task tree has to be completely executed.
Reference: [20] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka. </author> <title> A compilation approach for Fortran 90D/HPF compilers on distributed memory MIMD computers. </title> <booktitle> In Proc. Sixth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, Oregon, </address> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation and generating the communication necessary to fetch values of non-local data referenced by a processor <ref> [72, 123, 20, 5, 21, 68] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data, for the following reasons. It is getting increasingly cost-effective to build multiprocessors from commodity hardware components and system software. <p> Particular technical references and their relation with this thesis are given in specific chapters. 6.1 Communication scheduling The results in Chapter 2 follow (and generalize) a large number of preliminary results in communication optimization, such as message vectorizing, coalescing, and redundancy elimination <ref> [72, 123, 20, 5, 21, 68] </ref>. Message vectorization is the technique of hoisting communication of array elements or sections out of loops to produce a single large message. Typically this works locally on single loop-nests [123, 72, 67, 83, 68].
Reference: [21] <author> T. Brandes. </author> <title> ADAPTOR: A compilation system for data-parallel Fortran programs. </title> <editor> In C. W. Kessler, editor, </editor> <title> Automatic parallelization new approaches to code generation, data distribution, and performance prediction. </title> <booktitle> Vieweg Advanced Studies in Computer Science, </booktitle> <publisher> Vieweg, Wiesbaden, </publisher> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation and generating the communication necessary to fetch values of non-local data referenced by a processor <ref> [72, 123, 20, 5, 21, 68] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data, for the following reasons. It is getting increasingly cost-effective to build multiprocessors from commodity hardware components and system software. <p> Particular technical references and their relation with this thesis are given in specific chapters. 6.1 Communication scheduling The results in Chapter 2 follow (and generalize) a large number of preliminary results in communication optimization, such as message vectorizing, coalescing, and redundancy elimination <ref> [72, 123, 20, 5, 21, 68] </ref>. Message vectorization is the technique of hoisting communication of array elements or sections out of loops to produce a single large message. Typically this works locally on single loop-nests [123, 72, 67, 83, 68].
Reference: [22] <author> A. Broder and A. Karlin. </author> <title> Multi-level adaptive hashing. </title> <booktitle> In Symposium on Discrete Algorithms (SODA). ACM-SIAM, </booktitle> <year> 1990. </year>
Reference-contexts: Our application of random allocation to load balancing scenarios is related to results on random hashing and PRAM emulation <ref> [22, 77] </ref>.
Reference: [23] <author> K. Brown et al. </author> <title> Resource allocation and scheduling for mixed database workloads. </title> <type> Technical Report 1095, </type> <institution> University of Wisconsin at Madison, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: In x4.3 and x4.4 we give the makespan lower and upper bounds. In x4.5 we show how to extend the makespan algorithm to a WACT algorithm. In x4.6 we pose some unresolved problems. 4.2 Motivation 4.2.1 Databases Query scheduling in parallel databases is a topic of active research <ref> [23, 90, 119, 66, 59] </ref>. Queries arrive from many users to a front-end manager process.
Reference: [24] <author> M. Carlisle and A. Rogers. </author> <title> Software caching and computation migration in Olden. </title> <booktitle> In Principles and Practice of Parallel Programming (PPoPP), </booktitle> <address> Santa Barbara, CA, </address> <year> 1995. </year> <journal> ACM SIGPLAN. </journal>
Reference-contexts: For popular languages like C or C++, detecting parallelism in presence of aliases, pointers, and linked data structures is very difficult. Several projects such as Suif, Olden, and Jade, as well as commutativity and synchronization analyses have made initial progress in these directions <ref> [24, 118, 97] </ref>. The second problem is for the compiler to explicitly orchestrate communication between different register files or functional units.
Reference: [25] <author> S. Chakrabarti. </author> <title> Random allocation of jobs with weights and precedence. </title> <booktitle> Theoretical Computer Science, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: Although used in practice, the effect of random allocation on dynamic, irregular task graphs was unresolved before this work; the closest known analysis was for unit-time tasks [78]. The resulting bounds are supported by practical experience with irregular applications <ref> [30, 25] </ref>, which show that randomization is an effective tool for load balancing a certain class of applications. It is well-known from the literature on occupancy problems that one round of random allocation leads to roughly a logarithmic smoothing of load.
Reference: [26] <author> S. Chakrabarti, J. Demmel, and K. Yelick. </author> <title> Modeling the benefits of mixed data and task parallelism. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures (SPAA). ACM, </booktitle> <year> 1995. </year>
Reference-contexts: On the other hand, these applications have to be scheduled dynamically, and therefore cannot use the expensive optimization algorithms, such as linear-programming, that are used in compile-time scheduling. In Chapter 3 we will describe a simple and effective heuristic for scheduling divide and conquer applications with mixed parallelism <ref> [26] </ref>. The algorithm classifies the tasks into two types. The large problems near the root are allocated all the processors in turn, while small problems close to the leaves are packed in a task-parallel fashion, each task being assigned exactly one processor. <p> There is also task parallelism between unordered nodes. Recently algorithms have been designed for trading between locality and load balance in this scenario [33]. We will come back to similar problems in Chapter 4. The switching technique has been independently discovered after our paper <ref> [26] </ref> was published in a different context: that of scheduling tasks with penalties. Every task has a running time, and a penalty for rejection; the goal is to minimize the sum of the makespan of accepted tasks and the penalty of rejected tasks [15].
Reference: [27] <author> S. Chakrabarti, M. Gupta, and J.-D. Choi. </author> <title> Global communication analysis and optimization. </title> <booktitle> In Programming Language Design and Implementation (PLDI), Philadelphia, 1996. ACM. </booktitle> <pages> 118 </pages>
Reference-contexts: In Chapter 2 we will describe a new compiler algorithm for communication analysis 4 and optimization that achieves redundancy elimination while maximizing network and cache performance across loops and statements in a global manner <ref> [27] </ref>. This significantly improves on the recently proposed array dataflow analyses which use the "earliest placement" policy: our algorithm takes into account all legal positions for all remote accesses before deciding on the final placement of any access.
Reference: [28] <author> S. Chakrabarti and S. Muthukrishnan. </author> <title> Resource scheduling for parallel database and scientific applications. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <address> Italy, </address> <month> June </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: We obtain an O (log T ) approximation for both WACT and makespan <ref> [28] </ref>, where T is the longest to shortest job time ratio. Our algorithm uses a technique that deliberately introduces delays to improve on a greedy schedule. We also show that the log T blowup is unavoidable for certain instances. <p> type processor processor off-line / WACT Garey et al [57] N/A N/A fi p ; on makespan Feldmann et al [51] m par p fi any on makespan Hall et al [70] 1; m seq fi fi any off both 1; m seq fi fi ; on both This chapter <ref> [29, 28] </ref> m par p fi any on both m par fi ; on both m par any on makespan m par any off WACT m par forest/SPG on both Table 4.1: Comparison of results. N/A=not applicable, seq=sequential, par=parallel, SPG=series parallel graph.
Reference: [29] <author> S. Chakrabarti, C. Phillips, A. Schulz, D. Shmoys, C. Stein, and J. Wein. </author> <title> Improved scheduling algorithms for minsum criteria. </title> <booktitle> In Automata, Languages and Programming (ICALP), LNCS, Paderborn, </booktitle> <address> Germany, July 1996. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: type processor processor off-line / WACT Garey et al [57] N/A N/A fi p ; on makespan Feldmann et al [51] m par p fi any on makespan Hall et al [70] 1; m seq fi fi any off both 1; m seq fi fi ; on both This chapter <ref> [29, 28] </ref> m par p fi any on both m par fi ; on both m par any on makespan m par any off WACT m par forest/SPG on both Table 4.1: Comparison of results. N/A=not applicable, seq=sequential, par=parallel, SPG=series parallel graph. <p> Apart from algorithm design, we believe it is important to point out some differences between existing scheduling literature and features needed by schedulers in parallel computing systems. We also remark that although 71 our problem is different from existing theoretical settings, our solution borrows from various existing techniques <ref> [57, 103, 29, 70] </ref>. In x4.2 we study database and scientific computing scenarios to justify our model. In x4.3 and x4.4 we give the makespan lower and upper bounds. In x4.5 we show how to extend the makespan algorithm to a WACT algorithm. <p> By relaxing the requirement on WACT from optimal to a constant factor, we show in Theorem 4.5.2 that there are schedules optimal for both WACT and makespan to within constant factors <ref> [29] </ref>, even for unrelated machines and jobs with precedence. <p> How can we get constructive polynomial-time computable (a 1 ; a 2 )-schedules for small constants a 1 and a 2 ? Hall et al suggest a framework to achieve small a 2 [70]; their framework is actually also good for makespan <ref> [29] </ref>. First divide time into geometrically increasing intervals. That 80 is, define t 0 = 1, t ` = 2 `1 , ` = 1; : : :. In what follows, consider the `th interval in time, namely, (t `1 ; t ` ]; other intervals are processed similarly. <p> By the design of DualPack, all jobs will be scheduled in iteration K + 1, and will complete within time 2 K+2 . The best expected constants are obtained by scaling the geometric series by a random variable <ref> [29] </ref>. Specifically, instead of t ` = 2 ` , one picks t ` = 2 X+` , where X is a random variable uniformly distributed in (0; 1].
Reference: [30] <author> S. Chakrabarti, A. Ranade, and K. Yelick. </author> <title> Randomized load balancing for tree structured computation. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <pages> pages 666-673, </pages> <address> Knoxville, Tennessee, </address> <month> May </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: Although used in practice, the effect of random allocation on dynamic, irregular task graphs was unresolved before this work; the closest known analysis was for unit-time tasks [78]. The resulting bounds are supported by practical experience with irregular applications <ref> [30, 25] </ref>, which show that randomization is an effective tool for load balancing a certain class of applications. It is well-known from the literature on occupancy problems that one round of random allocation leads to roughly a logarithmic smoothing of load.
Reference: [31] <author> S. Chakrabarti and K. Yelick. </author> <title> Distributed data structures and algorithms for Grobner basis computation. </title> <journal> Journal of Lisp and Symbolic Computation, </journal> <volume> 7 </volume> <pages> 147-172, </pages> <year> 1994. </year>
Reference-contexts: We report on experiments with two applications. The first is a parallel divide and conquer algorithm to solve a symmetric tridiagonal eigenvalue problem [43]. The second is a parallel symbolic multivariate polynomial equation solver which uses a procedure similar to branch and bound <ref> [31] </ref>. Both our applications lead to tree-shaped precedence between jobs, and the job times are diverse. In both cases, most of shared data can be replicated at small communication cost, so random allocation is feasible.
Reference: [32] <author> S. Chatterjee. </author> <title> Compiling data-parallel programs for efficient execution on shared-memory multiprocessors. </title> <type> Technical Report CMU-CS-91-189, </type> <address> CMU, Pittsburgh, PA 15213, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: In the theory area, the best known online scheduling algorithm for mixed parallelism is 2:62-optimal [16, 52], and the best offline algorithm is 2-optimal [113, 87]. But these are worst-case guarantees. In the systems area, the Paradigm compiler [95], iWarp compiler [110], and NESL compiler <ref> [32] </ref> all support forms of mixed task and data parallelism, and there are plans to merge data Fortran D with Fortran M [56] and pC++ with CC++ [84] to support mixed parallelism.
Reference: [33] <author> C. Chekuri, W. Hasan, and R. Motwani. </author> <title> Scheduling problems in parallel query optimization. </title> <booktitle> In ACM Symposium on Principles of Database Systems, </booktitle> <year> 1995. </year>
Reference-contexts: A chain of operators may also show pipelined parallelism, but we can regard those nodes to be collapsed into a single node with data parallelism. There is also task parallelism between unordered nodes. Recently algorithms have been designed for trading between locality and load balance in this scenario <ref> [33] </ref>. We will come back to similar problems in Chapter 4. The switching technique has been independently discovered after our paper [26] was published in a different context: that of scheduling tasks with penalties.
Reference: [34] <author> C. Chekuri, R. Motwani, and B. Natarajan. </author> <title> Scheduling to minimize weighted completion time. </title> <type> Manuscript, </type> <year> 1995. </year>
Reference-contexts: Claim 4.3.1 also implies that a recent elegant constant factor WACT approximation technique due to Chekuri et al, which converts uniprocessor schedules to multiprocessor schedules <ref> [34] </ref>, will not generalize to handle non-malleable resources. Their technique is to start with a uniprocessor schedule, where job j completes at time C 1 j , and derive an m-machine schedule with C m j = O (C 1 j =m + j ). <p> For this model, constant factor makespan approximations are known that handle precedence between jobs exposed on-line. Furthermore, the algorithm is non-clairvoyant; i.e., it does not need to know the running time of a job before completion [51]. These and similar algorithms <ref> [15, 34] </ref> are essentially based on online gambling techniques. 6.3 Minsum metrics Most makespan results are relatively straightforward in that the approximation algorithms are compared against a very simple lower bound: the sum of average work per processor and critical path length [64, 51].
Reference: [35] <author> J.-D. Choi, R. Cytron, and J. Ferrante. </author> <title> On the efficient engineering of ambitious program analysis. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 20(2) </volume> <pages> 105-114, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: For each array expression on the right hand side of a statement that may need communication, identify the earliest (x2.4.3) and latest (x2.4.2) safe position to place that communication. One of our key innovations is to exploit the static single assignment (SSA) information <ref> [39, 35] </ref> already computed in an earlier phase by pHPF, refined by array dependence-testing [121]. In contrast, previous proposals for such analysis typically use a bidirectional dataflow approach with array section descriptors and/or bit-vectors [69]. 2. <p> All regular array defs are considered preserving, meaning that (unless proved otherwise) the original value is not assumed to be killed. See Cytron et al for a detailed treatment of SSA algorithms <ref> [39, 35] </ref>. We refer interchangeably 3 Arrays are regarded as scalars and the index information is ignored during SSA analysis. 19 control flow graph. <p> Given a use u, let d range over the reaching regular defs of u. (Reaching defs are defined in Cytron et al <ref> [39, 35] </ref>.) Consider some d. Observe that it is never necessary to place communication for u deeper than at CNL (d; u). Given d and u, we can compute all possible direction vectors (each is a CNL (d; u)-dimensional vector) [120].
Reference: [36] <author> C. Click. </author> <title> Global code motion global value numbering. </title> <booktitle> In Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 246-257. </pages> <booktitle> ACM SIGPLAN, </booktitle> <year> 1995. </year>
Reference-contexts: In the common message cost model using fixed overhead per message and bandwidth, minimizing the cost is N Phard (also see x2.6). In practice, simple greedy heuristics work quite well; see Figure 2.9 (g). It is similar to Click's global code motion heuristic <ref> [36] </ref>: consider the most constrained communication entry next, and put it where it is compatible in communication pattern (as shown by the test below) with the largest number of other candidate communication.
Reference: [37] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. Schauser, E. Santos, R. Sumbramonian, and T. von Eicken. </author> <title> LogP: Towards a realistic model of parallel computation. </title> <booktitle> In Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 1-12. ACM-SIGPLAN, </pages> <year> 1993. </year>
Reference-contexts: The destination processor is not interrupted. Communication. The machine model consists of processors with individual local memory connected by a communication network. We ignore the topology of the interconnect as in the LogP model <ref> [37] </ref>. Communicating a job takes unit time at the two processors involved in the transfer. Pruning and termination.
Reference: [38] <author> J. Cuppen. </author> <title> A divide and conquer method for the symmetric tridiagonal eigenproblem. </title> <journal> Numer. Math., </journal> <volume> 36 </volume> <pages> 177-195, </pages> <year> 1981. </year> <month> 119 </month>
Reference-contexts: An eigenvalue algorithm of a different flavor, but still from the divide and conquer category, is Cuppen's method for symmetric tridiagonal matrices, where we can actually split the matrix exactly in half all the time <ref> [38, 99] </ref> (although the costs of the children are not so simple). 51 Sparse Cholesky. We consider the regular but important special case of the matrix arising from the 5-point Laplacian on a square grid, ordered using the nested dissection ordering [60]. <p> If Cuppen's eigenvalue algorithm is used, and the effect of "deflation" is small <ref> [38] </ref>, the task tree has the same parameters as the sign function example above, although is different. Comments. From Table 3.1, typical values of are all in the 10 2 to 10 6 range.
Reference: [39] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and F. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: For each array expression on the right hand side of a statement that may need communication, identify the earliest (x2.4.3) and latest (x2.4.2) safe position to place that communication. One of our key innovations is to exploit the static single assignment (SSA) information <ref> [39, 35] </ref> already computed in an earlier phase by pHPF, refined by array dependence-testing [121]. In contrast, previous proposals for such analysis typically use a bidirectional dataflow approach with array section descriptors and/or bit-vectors [69]. 2. <p> All regular array defs are considered preserving, meaning that (unless proved otherwise) the original value is not assumed to be killed. See Cytron et al for a detailed treatment of SSA algorithms <ref> [39, 35] </ref>. We refer interchangeably 3 Arrays are regarded as scalars and the index information is ignored during SSA analysis. 19 control flow graph. <p> Given a use u, let d range over the reaching regular defs of u. (Reaching defs are defined in Cytron et al <ref> [39, 35] </ref>.) Consider some d. Observe that it is never necessary to place communication for u deeper than at CNL (d; u). Given d and u, we can compute all possible direction vectors (each is a CNL (d; u)-dimensional vector) [120].
Reference: [40] <author> J. Demmel and K. Stanley. </author> <title> The performance of finding eigenvalues and eigenvectors of dense symmetric matrices on distributed memory computers. </title> <booktitle> In Proceedings of the Seventh SIAM Conference on Parallel Proceesing for Scientific Computing. </booktitle> <publisher> SIAM, </publisher> <year> 1994. </year>
Reference-contexts: The asymptotic model is a good fit for the actual efficiency profiles: the mean relative error is 6-11%. Estimates of are important for performance analysis as well as runtime scheduling decisions. To this end, we collect values of for some parallel scientific libraries [45], using existing analytical performance models <ref> [40, 42] </ref>. <p> Parameters ff (latency) and fi (inverse bandwidth; transfer time per double) are normalized to a BLAS-3 FLOP, and the model is fit to data generated from analytical models <ref> [40, 42, 107] </ref>. The curves were fit for 2 P 500 and 100 n = N 1=2 10000. An estimate of memory per processor in megabytes is given in the column marked M/P.
Reference: [41] <author> X. Deng, N. Gu, T. Brecht, and K. Lu. </author> <title> Preemptive scheduling of parallel jobs on multiprocessors. </title> <booktitle> In Symposium on Discrete Algorithms (SODA). ACM-SIAM, </booktitle> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Previous work on the WACT metric gave either non-clairvoyant, preemptive solutions for sequential jobs or jobs that used a perfectly malleable processor resource, with job precedence <ref> [41] </ref>; or clairvoyant, non-preemptive solutions for independent jobs with only non-malleable resource, if any [92, 112]. Apart from algorithm design, we believe it is important to point out some differences between existing scheduling literature and features needed by schedulers in parallel computing systems. <p> Two decades later there is significant interest in the Computer Science community on scheduling parallel jobs <ref> [111, 41] </ref>. For independent jobs that need a fixed given number of processors, various rectangle packing algorithms naturally model the problem: every job can be regarded as a rectangle of width equal to the number of processors, and of height representing the running time [14, 13]. <p> As might be suspected, non-clairvoyant schedulers are remarkably handicapped compared to clairvoyant ones [92], even when permitted the (essential) power of preemption. These results are for sequential jobs; some of them have been extended to the perfectly malleable job model <ref> [41] </ref>. A further refinement of the scheduler performance metric is weighted flow time, defined as P j w j (C j a j ), where w j is the priority of job j, and a j and C j are its arrival and completion time respectively.
Reference: [42] <author> F. Desprez, B. Tourancheau, and J. J. Dongarra. </author> <title> Performance complexity of LU factorization with efficient pipelining and overlap on a multiprocessor. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, </institution> <month> Feb </month> <year> 1994. </year> <note> (LAPACK Working Note #67). </note>
Reference-contexts: The asymptotic model is a good fit for the actual efficiency profiles: the mean relative error is 6-11%. Estimates of are important for performance analysis as well as runtime scheduling decisions. To this end, we collect values of for some parallel scientific libraries [45], using existing analytical performance models <ref> [40, 42] </ref>. <p> Parameters ff (latency) and fi (inverse bandwidth; transfer time per double) are normalized to a BLAS-3 FLOP, and the model is fit to data generated from analytical models <ref> [40, 42, 107] </ref>. The curves were fit for 2 P 500 and 100 n = N 1=2 10000. An estimate of memory per processor in megabytes is given in the column marked M/P.
Reference: [43] <author> I. Dhillon and J. Demmel. </author> <title> Private communication., </title> <month> March </month> <year> 1994. </year>
Reference-contexts: It is therefore interesting to evaluate the cost and benefit of decentralization in practical settings. We report on experiments with two applications. The first is a parallel divide and conquer algorithm to solve a symmetric tridiagonal eigenvalue problem <ref> [43] </ref>. The second is a parallel symbolic multivariate polynomial equation solver which uses a procedure similar to branch and bound [31]. Both our applications lead to tree-shaped precedence between jobs, and the job times are diverse.
Reference: [44] <author> J. Dongarra, R. Hempel, A. Hay, and D. Walker. </author> <title> A proposal for a user-level message passing interface in a distributed memory environment. </title> <type> Technical Report ORNL/TM-12231, </type> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: This involves the following runtime support: 55 * It must be possible to create and destroy subsets of the set of physical processor and rename them in a virtual processor space. These are called contexts in the Message Passing Interface (MPI) jargon <ref> [44] </ref>. Note that every message has to be indirectly addressed to a virtual destination processor, and if many contexts are defined hierarchically, many table-lookups are needed for each message. * Contexts need to provide synchronization and collective communication primitives, so that they can be gang-scheduled. These can be potentially expensive.
Reference: [45] <author> J. Dongarra, R. van de Geijn, and D. Walker. </author> <title> A look at scalable dense linear algebra libraries. </title> <booktitle> In Scalable High-Performance Computing Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> April </month> <year> 1992. </year>
Reference-contexts: The gap is small. Data parallelism is the most familiar parallel execution model in scientific computing, as in ScaLAPACK, CMSSL, Fortran D, CM-Fortran, and High Performance Fortran <ref> [45, 76, 72, 49, 55] </ref>. The computational load can usually be balanced statically, and the important performance issue is generating parallel loops with minimal communication overhead (as in Chapter 2). <p> N=P 1 e 1 3.4.1.2 Experimental validation We validated our model using experimental data. In Figure 3.3 on page 49, we consider three ScaLAPACK programs: LU, QR and Cholesky factorizations, and three machines: the Delta, Paragon and iPSC/860 <ref> [45] </ref>. Each graph plots performance in GFLOPS per processor versus N=P , including experimental data (the circles), as well as the prediction of the asymptotic model. The iPSC/860 experiments were run with 128 processors, and the Paragon and Delta experiments were run with both 128 and 512 processors. <p> The asymptotic model is a good fit for the actual efficiency profiles: the mean relative error is 6-11%. Estimates of are important for performance analysis as well as runtime scheduling decisions. To this end, we collect values of for some parallel scientific libraries <ref> [45] </ref>, using existing analytical performance models [40, 42]. <p> exhibit troughs because at the lower end of problem sizes, absolute efficiency of all the strategies are very small but close to each other. 3.5 Experiments Encouraged by the promising simulation and analysis, we implemented the switched parallel scheduler as a new module in the ScaLAPACK scalable scientific software library <ref> [45] </ref>. In particular, it was used to schedule the divide and conquer tree generated by the sign function example in x3.2. The ScaLAPACK library is structured as follows.
Reference: [46] <author> J. Du, J. Y.-T. Leung, and G. H. Young. </author> <title> Minimizing mean flow time with release time constraint. </title> <journal> Theoretical Computer Science, </journal> <volume> 75(3) </volume> <pages> 347-355, </pages> <month> Oct. </month> <year> 1990. </year>
Reference-contexts: For a uniprocessor, the least remaining processing time or LRPT heuristic is easily seen to be optimal for total flow time P j (C j a j ), while for any fixed number of processors m 2, Du et al <ref> [46] </ref> show that obtaining a preemptive schedule to minimize total flow time is N P-hard. In fact, C.
Reference: [47] <author> J. Eckstein. </author> <title> Parallel branch and bound algorithms for general mixed integer programming on the CM-5. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 4, </volume> <year> 1994. </year>
Reference-contexts: In many application such as parallel search or branch and bound, the total work done is very sensitive to the job order, and one wishes to deviate from the best sequential order as little as possible <ref> [47, 8] </ref>. 5.2.3 Weighted occupancy At first we consider the weighted occupancy problem, where there is no precedence among jobs. There are n weighted balls (jobs), ball j having weight t j . These balls are thrown uniformly at random into P bins (processors). <p> In both cases, most of shared data can be replicated at small communication cost, so random allocation is feasible. Random allocation with diverse times have also been use in N body simulation [86] and integer linear programming <ref> [47] </ref>. For each application, we added instrumentation to the sequential program to emit the task tree with task times, and input this tree to a simulator that simulated the parallel execution of the randomized load balancing algorithm, as well as Graham's list schedule.
Reference: [48] <author> K. Ekanadham, J. E. Moreira, and V. K. Naik. </author> <title> Application oriented resource management on large scale parallel systems. </title> <type> Technical Report RC 20151, </type> <institution> IBM Research, Yorktown Heights, </institution> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: Vendors like IBM, SGI and Convex place significant emphasis on load management software, as do academic communities [50]. It is also becoming apparent that applications should dynamically allocate and deallocate resources to avoid under-utilization and long job response times <ref> [48] </ref>. While significant software infrastructure is being built, the algorithms at the heart of these systems are often variants of list-based greedy schedulers, which perform poorly under heavy load. In Chapter 4 we give several simple and near-optimal algorithms for variations of this scenario. <p> To improve utilization, system support has been designed to express jobs at a finer level inside an application and convey the information to the resource manager by annotating the parallel executable <ref> [65, 48, 91] </ref>.
Reference: [49] <author> Fortran 90. </author> <title> ANSI standard X3.198-199x, which is identical to ISO standard ISO/IEC 1539:1991. </title> <type> 120 </type>
Reference-contexts: HPF extends Fortran with two features: one can refer to regular array sections rather than individual elements (this extension is also part of Fortran 90 <ref> [49] </ref>), and give directives to the compiler to distribute and align arrays on parallel machines. The compiler partitions computation to match the data distribution and generates communication for remote arrays. However, compiling HPF code to get performance close to hand-coded message-passing programs is still a major challenge. <p> The gap is small. Data parallelism is the most familiar parallel execution model in scientific computing, as in ScaLAPACK, CMSSL, Fortran D, CM-Fortran, and High Performance Fortran <ref> [45, 76, 72, 49, 55] </ref>. The computational load can usually be balanced statically, and the important performance issue is generating parallel loops with minimal communication overhead (as in Chapter 2).
Reference: [50] <author> D. G. Feitelson and L. Rudolph, </author> <title> editors. Job Scheduling Strategies for Parallel Processing, </title> <booktitle> number 949 in LNCS. </booktitle> <publisher> Springer-Verlag, </publisher> <month> Apr. </month> <year> 1995. </year> <booktitle> Workshop at International Parallel Processing Symposium. </booktitle>
Reference-contexts: Until recently, few machines allowed multiple jobs to execute concurrently. As most machines are going towards a multiprogrammed full-UNIX node model, resource scheduling is becoming more crucial at the system level. Vendors like IBM, SGI and Convex place significant emphasis on load management software, as do academic communities <ref> [50] </ref>. It is also becoming apparent that applications should dynamically allocate and deallocate resources to avoid under-utilization and long job response times [48]. <p> Consequently, algorithms for scheduling have been extensively researched since the 60's, both in theory and in practice. In the past decade, there has been increasing 65 interest in scheduling specifically for parallel systems. There is increased awareness of general resource scheduling problems, rather than just processor scheduling <ref> [50] </ref>. Owing to the vastly different nature and applicability of sequential and parallel computing systems, the scheduling problems of practical interest are rather different in the two settings. <p> First, preemption is not allowed. Time-slicing and preemption of space-shared resources is very expensive because (1) the state has to be evicted to slower layers of the memory hierarchy, (2) processes have to synchronize and switch across protection domains, and (3) in-flight messages have to be flushed and reinjected <ref> [50, page 6] </ref>. Anecdotal evidence suggests that many practitioners switch off time sharing for production runs on parallel machines that do not permit creating dedicated space partitions. <p> We give a simple approximation algorithm that matches the O (V + log T ) makespan bound. In contrast to our algorithm, most known practical solutions use some variant of greedy list- or queue-type scheduling <ref> [50, 59, 90] </ref>. Jobs on arrival are placed in a list ordered by some heuristic (often FIFO). The scheduler dispatches the first ready job on the list when enough resources become available.
Reference: [51] <author> A. Feldmann, M.-Y. Kao, J. Sgall, and S.-H. Teng. </author> <title> Optimal online scheduling of parallel jobs with dependencies. </title> <booktitle> In Symposium on the Theory of Computing (STOC), </booktitle> <pages> pages 642-651. </pages> <publisher> ACM, </publisher> <year> 1993. </year>
Reference-contexts: Feldmann et al show that if t j is not known before a job completes, no on-line algorithm can give a makespan within a factor better than n, the number of jobs, of the off-line optimal <ref> [51] </ref>. Even if t j is known upon arrival, Garey and Graham showed that any greedy list-schedule has worst case makespan performance ratio at least nT =(n + T ), which is roughly T when T t n and n when T n [57, Theorem 1]. <p> we give instances of our problem where no schedule can achieve a makespan smaller than (V + log T ). 69 Reference # Job Malleable Nonmalleable Precedence Online/ Makespan procs type processor processor off-line / WACT Garey et al [57] N/A N/A fi p ; on makespan Feldmann et al <ref> [51] </ref> m par p fi any on makespan Hall et al [70] 1; m seq fi fi any off both 1; m seq fi fi ; on both This chapter [29, 28] m par p fi any on both m par fi ; on both m par any on makespan m <p> Only a malleable resource. If there are no nonmalleable resources but only a malleable resource, then precedence can be handled (in the sense of approximating makespan) even by a scheduler that does not know t j before a job finishes <ref> [51] </ref>. Small resource demand. Another possible restriction is to allow nonmalleable resources, but require each job to reserve no more than fraction of the non-malleable resources, where is small. That is, the maximum fraction requested by a job is at most . In that case naive approaches will work well. <p> remove either precedence or non-malleable resources, then this lower bound can be achieved to a constant factor: + 1 m j t j = + V for sequential jobs [64], and + ( p 3 5 m j t j m j = + O (V ) for malleable jobs <ref> [51] </ref>. These depend on arguments of the form: "if a critical path is being ignored, most of the resources are being utilized." In this section, we show that O (V + ) makespan is not always possible with both nonmalleable resources and precedence constraints. <p> Thus this differentiates our problem from list-scheduling [64, 57] and malleable job scheduling <ref> [51] </ref>. Our formulation is different in flavor because jobs may have to wait even when resource utilization is very low because their specific requirement of the nonmalleable resources are not met. In contrast, jobs can proceed with a smaller amount of a malleable resource with proportionate slowdown. <p> Also let j be the critical path length from a root through job j, and = max j f j g. The makespan algorithm first invokes the malleable scheduling of Feldmann et al <ref> [51] </ref> to allocate processors to jobs in J , and to assign (infeasible) preliminary execution intervals to these jobs that still violate other resource constraints. Then we partition the jobs into a sequence of layers with jobs within a layer being independent of each other. <p> Compute a greedy schedule for J ignoring all non-malleable resource requirements, as follows. Whenever there are more than flm free processors, schedule any job j in J (whose predecessors have all completed) on the minimum of m j and the number of free processors <ref> [51] </ref>. 76 Denote by j the number of processors allocated to job j. After processor allocation let the modified job times be t 0 j = t j m j = j , and modified critical path lengths be 0 j . <p> Picking fl such that (1 fl)(1 + 1 fl ) = 1, the length of the (invalid) schedule is at most + 1 mfl j m j t j , similar to <ref> [51] </ref>. 1 However, this problem is not one where items are solid blocks with volume and the bin is a hollow unit cube. 77 Lemma 4.4.2 Jobs assigned to a particular layer I are independent; the layer ordering is consistent with job precedence and in any layer I, P Proof. <p> This gives a deterministic online (12 + *)- approximation algorithm for scheduling malleable jobs on parallel machines, and a randomized on-line algorithm with expected performance within 8:67 of optimal. 4.5.2.2 Perfectly malleable jobs with precedence We shall consider perfectly malleable jobs, as in Feldmann et al <ref> [51] </ref>, and precedence constraints that are forests or series parallel graphs. Our DualPack routine is as follows. We remove from J any j with PathToRoot (j) &gt; D, set J 0 = Knapsack (J ` ; mD), and list schedule J 0 as in [51]: let = ( p 5 1)=2 <p> jobs, as in Feldmann et al <ref> [51] </ref>, and precedence constraints that are forests or series parallel graphs. Our DualPack routine is as follows. We remove from J any j with PathToRoot (j) &gt; D, set J 0 = Knapsack (J ` ; mD), and list schedule J 0 as in [51]: let = ( p 5 1)=2 be the golden ratio; whenever there is a job j with all of its predecessors completed and the number of busy processors is less than m, schedule the job on the minimum of m j and the number of free processors. <p> For this model, constant factor makespan approximations are known that handle precedence between jobs exposed on-line. Furthermore, the algorithm is non-clairvoyant; i.e., it does not need to know the running time of a job before completion <ref> [51] </ref>. These and similar algorithms [15, 34] are essentially based on online gambling techniques. 6.3 Minsum metrics Most makespan results are relatively straightforward in that the approximation algorithms are compared against a very simple lower bound: the sum of average work per processor and critical path length [64, 51]. <p> These and similar algorithms [15, 34] are essentially based on online gambling techniques. 6.3 Minsum metrics Most makespan results are relatively straightforward in that the approximation algorithms are compared against a very simple lower bound: the sum of average work per processor and critical path length <ref> [64, 51] </ref>. Also, makespan results are relevant for a batch of jobs in isolation; in fact, online arrival of the batch make little difference [105] (also see 110 Chapter 5).
Reference: [52] <author> A. Feldmann, J. Sgall, and S.-H. Teng. </author> <title> Dynamic scheduling on parallel machines. </title> <booktitle> In Foundations of Computer Science (FOCS), </booktitle> <pages> pages 111-120, </pages> <year> 1992. </year>
Reference-contexts: Several researchers have proposed support to take advantage of this mixed parallelism. In the theory area, the best known online scheduling algorithm for mixed parallelism is 2:62-optimal <ref> [16, 52] </ref>, and the best offline algorithm is 2-optimal [113, 87]. But these are worst-case guarantees.
Reference: [53] <author> W. Feller. </author> <title> An Introduction to Probability Theory and Its Applications. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1958. </year>
Reference-contexts: Finally, we apply the Central Limit Theorem <ref> [53] </ref> to observe that lim !1 p = 1 2 . Let fX i g be a sequence of n iid random variables. Suppose the expectation = E [X i ] and variance 2 = V [X i ] exist and let S n = P i X i .
Reference: [54] <author> L. L. Fong and M. S. Squillante. </author> <title> Time-function scheduling: a general approach to controllable resource management. Operating Systems Review, </title> <address> 29(5):230, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: The Operating Systems research literature has many proposals for choosing the priority function to favor starved or short jobs <ref> [54] </ref>.
Reference: [55] <author> H. P. F. Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Rice University, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: However, the absence of a global address space and the resulting need for explicit message passing makes these machines difficult to program. This has motivated the design of languages like High Performance Fortran (HPF) <ref> [55] </ref>, which allow the programmer to write sequential or shared-memory parallel programs that are annotated with directives specifying data decomposition. <p> Recent versions of the HPF proposal includes support for such applications <ref> [55] </ref>. 38 Some parallelizing compilers have been extended to express such a mixed parallel execution model, but the algorithmic details of the runtime scheduler are still under debate. <p> The gap is small. Data parallelism is the most familiar parallel execution model in scientific computing, as in ScaLAPACK, CMSSL, Fortran D, CM-Fortran, and High Performance Fortran <ref> [45, 76, 72, 49, 55] </ref>. The computational load can usually be balanced statically, and the important performance issue is generating parallel loops with minimal communication overhead (as in Chapter 2).
Reference: [56] <author> I. Foster, M. Xu, B. Avalani, and A. Chowdhary. </author> <title> A compilation system that integrates High Performance Fortran and Fortran M. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <pages> pages 293-300. </pages> <publisher> IEEE, </publisher> <year> 1994. </year>
Reference-contexts: But these are worst-case guarantees. In the systems area, the Paradigm compiler [95], iWarp compiler [110], and NESL compiler [32] all support forms of mixed task and data parallelism, and there are plans to merge data Fortran D with Fortran M <ref> [56] </ref> and pC++ with CC++ [84] to support mixed parallelism. The compiler efforts all depend on static task graph and profile data and perform extensive optimizations to allocate processors. We have already noted how dynamic task parallelism is more promising to exploit.
Reference: [57] <author> M. R. Garey and R. L. Graham. </author> <title> Bounds for multiprocessor scheduling with resource constraints. </title> <journal> SIAM Journal on Computing, </journal> <volume> 4(2) </volume> <pages> 187-200, </pages> <month> June </month> <year> 1975. </year>
Reference-contexts: Even if t j is known upon arrival, Garey and Graham showed that any greedy list-schedule has worst case makespan performance ratio at least nT =(n + T ), which is roughly T when T t n and n when T n <ref> [57, Theorem 1] </ref>. No algorithm is needed to achieve these bounds. The status of clairvoyant non-greedy algorithms was previously unknown. Essentially all known job graph scheduling results depend on two bulk parameters of the input graph: the volume and the critical path. <p> Specifically, we give instances of our problem where no schedule can achieve a makespan smaller than (V + log T ). 69 Reference # Job Malleable Nonmalleable Precedence Online/ Makespan procs type processor processor off-line / WACT Garey et al <ref> [57] </ref> N/A N/A fi p ; on makespan Feldmann et al [51] m par p fi any on makespan Hall et al [70] 1; m seq fi fi any off both 1; m seq fi fi ; on both This chapter [29, 28] m par p fi any on both m <p> No precedence. If the precedence graph is empty, (i.e., jobs are independent) then a number of approaches are known to get approximation bounds <ref> [57, 87, 113, 112, 111] </ref>. In the database scenario, it is possible to collapse each query, consisting of several jobs with a precedence relation among them, into one job, i.e., allocate maximum resources over all the jobs in the query, and then apply the results for independent jobs [119]. <p> Apart from algorithm design, we believe it is important to point out some differences between existing scheduling literature and features needed by schedulers in parallel computing systems. We also remark that although 71 our problem is different from existing theoretical settings, our solution borrows from various existing techniques <ref> [57, 103, 29, 70] </ref>. In x4.2 we study database and scientific computing scenarios to justify our model. In x4.3 and x4.4 we give the makespan lower and upper bounds. In x4.5 we show how to extend the makespan algorithm to a WACT algorithm. <p> Thus this differentiates our problem from list-scheduling <ref> [64, 57] </ref> and malleable job scheduling [51]. Our formulation is different in flavor because jobs may have to wait even when resource utilization is very low because their specific requirement of the nonmalleable resources are not met. <p> Step 4. Schedule each layer separately in time order. Consider each layer to be an instance of a generalized s-dimensional bin-packing problem 1 In this problem, studied by Garey and Graham <ref> [57] </ref>, and Garey, Graham, Johnson and Yao [58], there are items to pack into bins; each item e is an s-dimensional vector ~r e with components in (0; 1), and the bin is an s-dimensional with all components set to one. <p> The above method also gives an alternative algorithm and much simpler analysis (weaker only in a constant) for the (s + 1)-approximate resource constrained scheduling result of <ref> [57] </ref>. Their (s + 1)-approximation is for = ;. In this case = T , and we allocate log T layers with t (I) 2 f1; 2; 4; : : : ; T g. Then P I t (I) &lt; 2T , giving an O (s) approximation. <p> We set the weight and size of job j 2 J to be w j and m j p j , respectively, and then call Knapsack, returning J 0 = Knapsack (J; mD). Finally, we use the list scheduling algorithm of Garey and Graham <ref> [57] </ref> to schedule J 0 . Theorem 4.5.5 The above DualPack routine is a (3 + *)-approximation algorithm for the maximum scheduled weight problem. <p> For independent non-malleable jobs in the off-line setting, several WACT results have been known [111]. Recently, Hall etal suggested two important general frameworks for off-line and on-line WACT optimization [70]. Our results in Chapter 4 build upon these results. 6.4 Resource scheduling Garey et al <ref> [57] </ref> study a generalization of nonmalleable jobs where there are s distinct types of resources. At any time a total of at most one unit of each resource can be allocated. <p> Job j specifies an s-dimensional resource vector ~r j = (r jk ), where r jk 1 is the fraction of resource of type k that j keeps allocated for the duration it runs. Their positive (makespan) results are for independent jobs <ref> [57] </ref> and for dependent jobs all of equal duration [58]. <p> j r j and critical path , cannot always be matched to a constant factor by even an optimal makespan algorithm. (Here j is the earliest time job j can finish ignoring resource needs and only considering the total time of its longest predecessor chain.) Not surprisingly, Garey et al <ref> [57] </ref> observe that list scheduling algorithms can perform very poorly (give a makespan that is larger than optimal by a factor equal to the number of jobs) on such problems. 111 6.5 Distributed load balancing The above settings all assumed a centralized scheduler.
Reference: [58] <author> M. R. Garey, R. L. Graham, D. S. Johnson, and A. C. Yao. </author> <title> Resource constrained scheduling as generalized bin packing. </title> <journal> Journal of Combinatorial Theory, Series A, </journal> <volume> 21(3) </volume> <pages> 257-298, </pages> <month> Nov. </month> <year> 1976. </year>
Reference-contexts: Step 4. Schedule each layer separately in time order. Consider each layer to be an instance of a generalized s-dimensional bin-packing problem 1 In this problem, studied by Garey and Graham [57], and Garey, Graham, Johnson and Yao <ref> [58] </ref>, there are items to pack into bins; each item e is an s-dimensional vector ~r e with components in (0; 1), and the bin is an s-dimensional with all components set to one. A set E of items fits in a bin if P where is elementwise. <p> Their positive (makespan) results are for independent jobs [57] and for dependent jobs all of equal duration <ref> [58] </ref>.
Reference: [59] <author> M. Garofalakis and Y. Ioannidis. </author> <title> Multidimensional resource scheduling for parallel queries. </title> <booktitle> In ACM SIGMOD Conference on the Management of Data. ACM, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: We give a simple approximation algorithm that matches the O (V + log T ) makespan bound. In contrast to our algorithm, most known practical solutions use some variant of greedy list- or queue-type scheduling <ref> [50, 59, 90] </ref>. Jobs on arrival are placed in a list ordered by some heuristic (often FIFO). The scheduler dispatches the first ready job on the list when enough resources become available. <p> In x4.3 and x4.4 we give the makespan lower and upper bounds. In x4.5 we show how to extend the makespan algorithm to a WACT algorithm. In x4.6 we pose some unresolved problems. 4.2 Motivation 4.2.1 Databases Query scheduling in parallel databases is a topic of active research <ref> [23, 90, 119, 66, 59] </ref>. Queries arrive from many users to a front-end manager process. <p> It is difficult to find this function <ref> [59] </ref>, and unclear if it is simple enough to be used by the optimizer. We group the resources into two types: malleable and nonmalleable. We handle only one malleable resource that affects the running time. It may be of interest to evaluate more elaborate alternatives.
Reference: [60] <author> A. George and J. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year> <month> 121 </month>
Reference-contexts: We consider the regular but important special case of the matrix arising from the 5-point Laplacian on a square grid, ordered using the nested dissection ordering <ref> [60] </ref>. In this case one may think of dividing the matrix into 4 independent subproblems, corresponding to dividing the square grid into 4 subsquares, each of half the perimeter.
Reference: [61] <author> B. Ghosh and S. Muthukrishnan. </author> <title> Dynamic load balancing on parallel and distributed networks by random matchings. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 226-235, </pages> <address> Cape May, NJ, </address> <month> June </month> <year> 1994. </year> <note> ACM. </note>
Reference-contexts: We study work sharing, where busy processors forward jobs to random processors. Some other techniques are work stealing, where idle processors ask for work [19], and diffusion, where neighbors exchange local load information and then move some jobs from busy to lazy processors <ref> [61] </ref>. 5.2.1 Models and notation The input comprises a set J of jobs presented to the algorithm in a distributed and on-line fashion. Job j has running time t j , also referred to as its "weight" (not to be confused with "priority" as in Chapter 4). <p> Second, although the above result establishes near-optimal load balance, our model does not reflect the gains from avoiding communication bottlenecks. Other results of the diffusion type are based on occasionally matching busy and idle processors and transferring jobs <ref> [61, 98] </ref>. These are not appropriate for relatively fine-grain jobs which is our focus. Notice also that diversity in job execution times makes coordination even harder unless a processor can suspend long jobs and participate in global communication. <p> Both of the above assume an amorphous network. For general network topologies, another technique that has been studied a great deal recently is diffusion, where neighbors exchange local load information and then move some jobs from busy to lazy processors <ref> [98, 61] </ref>. Our application of random allocation to load balancing scenarios is related to results on random hashing and PRAM emulation [22, 77].
Reference: [62] <author> J. Gilbert and R. Tarjan. </author> <title> The analysis of a nested dissection algorithm. </title> <journal> Numerische Mathematik, </journal> <volume> 50 </volume> <pages> 377-404, </pages> <year> 1987. </year>
Reference-contexts: We can also derive some heuristic bounds to the performance gap in some irregular graphs. E.g., Gilbert and Tarjan study nested dissection algorithms to solve sparse systems on planar graphs <ref> [62] </ref>, where a problem of size N is divided into d = 2 subproblems, where each part is no bigger than 2N=3.
Reference: [63] <author> G. Gonnet. </author> <title> Expected length of the longest probe sequence in hash code searching. </title> <journal> Journal of the ACM, </journal> <volume> 28(2) </volume> <pages> 289-304, </pages> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: On the other hand, reducing T limits the maximum load at the expense of many rounds. We will quantify this tradeoff in x5.3.3. Our analysis exploits a basic tool that expresses the relation between occupancy distributions and Poisson distributions <ref> [63, 1] </ref>. Using this tool, we show that after a fixed number of rounds r, the final maximum load is O q log log n with high probability. We also show via lower bounds that no better load balance can be achieved by a large class of load balancing algorithms.
Reference: [64] <author> R. L. Graham. </author> <title> Bounds on multiprocessor timing anomalies. </title> <journal> SIAM Journal of Applied Mathematics, </journal> <volume> 17(2) </volume> <pages> 416-429, </pages> <month> March </month> <year> 1969. </year>
Reference-contexts: The running time of a task may be unknown before completion, and the scheduler has to be non-clairvoyant, i.e., it cannot know and use the running time of a job to schedule it. It is not necessary for a centralized scheduler (e.g. Graham's list algorithm <ref> [64] </ref>) to be clairvoyant, when evaluated using the finish time metric, also called "makespan." But this solution has a communication bottleneck. <p> One, which we call the volume parameter, is the sum of resource-time products over jobs in the graph, denoted V . For example, consider jobs with only one resource type: processors. If each job runs on only one processor, as in list-scheduling <ref> [64] </ref>, then the fractional resource every job occupies is r j = 1=m, where there are m processors in all. <p> If the problem is relaxed to remove either precedence or non-malleable resources, then this lower bound can be achieved to a constant factor: + 1 m j t j = + V for sequential jobs <ref> [64] </ref>, and + ( p 3 5 m j t j m j = + O (V ) for malleable jobs [51]. <p> Thus this differentiates our problem from list-scheduling <ref> [64, 57] </ref> and malleable job scheduling [51]. Our formulation is different in flavor because jobs may have to wait even when resource utilization is very low because their specific requirement of the nonmalleable resources are not met. <p> Assuming that shared access to the central pool takes zero time, Graham analyzed the makespan of the resulting schedule (called a listschedule) and showed that it is less than a factor of two worse than the optimal makespan <ref> [64] </ref>. This solution has a communication bottleneck problem: all processors access the central pool. The overhead can be especially large for fine-grain tasks. One technique to reduce the overhead is to let each processor have a local task pool from which it removes and executes some task, if any. <p> The objective function to minimize is makespan, the maximum completion time of a job. Although the exact problem is N P-hard, good approximations are possible if the algorithm assigning jobs to processors is centralized, and thus has perfect global knowledge. For example, Graham's list-scheduling algorithm <ref> [64] </ref> will result in a finish time at most 2 1=P times optimal, where P is the 88 number of processors. However, this may lead to a severe communication bottleneck at the processor where the pool of jobs resides, especially for fine-grained tasks. <p> These and similar algorithms [15, 34] are essentially based on online gambling techniques. 6.3 Minsum metrics Most makespan results are relatively straightforward in that the approximation algorithms are compared against a very simple lower bound: the sum of average work per processor and critical path length <ref> [64, 51] </ref>. Also, makespan results are relevant for a batch of jobs in isolation; in fact, online arrival of the batch make little difference [105] (also see 110 Chapter 5).
Reference: [65] <author> S. L. Graham, S. Lucco, and O. Sharp. </author> <title> Orchestrating interactions among parallel computations. </title> <booktitle> In Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 100-111, </pages> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: To improve utilization, system support has been designed to express jobs at a finer level inside an application and convey the information to the resource manager by annotating the parallel executable <ref> [65, 48, 91] </ref>.
Reference: [66] <author> J. Gray. </author> <title> A survey of parallel database techniques and systems, September 1995. </title> <booktitle> Tutorial at VLDB. </booktitle>
Reference-contexts: The work in this dissertation is based on the following premises. * Performance engineering is of paramount importance in parallel computing, because, "Parallelism is not cheaper or easier, it is faster" <ref> [66] </ref>. * Scheduling the resources of the multiprocessor (mainly, processors, network, memory, and IO activity) judiciously between the units of work is essential for achieving high utilization of the resources. <p> In x4.3 and x4.4 we give the makespan lower and upper bounds. In x4.5 we show how to extend the makespan algorithm to a WACT algorithm. In x4.6 we pose some unresolved problems. 4.2 Motivation 4.2.1 Databases Query scheduling in parallel databases is a topic of active research <ref> [23, 90, 119, 66, 59] </ref>. Queries arrive from many users to a front-end manager process. <p> This model is best suited to shared memory databases running on symmetric 72 multiprocessors (SMP) with shared access to disk [73]. They currently scale to 30-40 processors. There is growing consensus that SMP's and scalable multiprocessors will converge to networked clusters of SMP nodes <ref> [66] </ref>.
Reference: [67] <author> M. Gupta and P. Banerjee. </author> <title> A methodology for high-level synthesis of communication on multicomputers. </title> <booktitle> In Proc. 6th ACM International Conference on Supercomputing, </booktitle> <address> Washington D.C., </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Message vectorization is the technique of hoisting communication of array elements or sections out of loops to produce a single large message. Typically this works locally on single loop-nests <ref> [123, 72, 67, 83, 68] </ref>. As compilers were applied to more complex code, eliminating redundant communication beyond single loop-nests and even across procedures became essential.
Reference: [68] <author> M. Gupta, S. Midkiff, E. Schonberg, V. Seshadri, K. Wang, D. Shields, W.-M. Ching, and T. Ngo. </author> <title> An HPF compiler for the IBM SP2. </title> <booktitle> In Proc. Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation and generating the communication necessary to fetch values of non-local data referenced by a processor <ref> [72, 123, 20, 5, 21, 68] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data, for the following reasons. It is getting increasingly cost-effective to build multiprocessors from commodity hardware components and system software. <p> The algorithm achieves both redundancy elimination and message combining globally, and is able to reduce the number of messages to an extent that is not achievable with any previous approach. Our algorithm has been implemented in the IBM pHPF prototype compiler <ref> [68] </ref>. We report results from a preliminary study of some well-known HPF programs. The performance gains are impressive. Reduction in static message count can be up to a factor of almost nine. Time spent in communication is reduced in many cases by a factor of two or more. <p> Even if the programmer were to write the code in the first column, intermediate passes of compilation may destroy the loop structure by fission. In fact, the IBM HPF scalarizer <ref> [68] </ref> will translate the F90-style source to the scalarized form in the second column. If loop fusion can be performed before this analysis, as in this case, the problem can be avoided. But this is not always possible [120, x 9.2]. <p> This analysis is done after the compiler has performed transformations like loop distribution and loop interchange to increase opportunities for moving communication outside loops <ref> [68] </ref>. Wolfe provides an excellent overview of the compiler terminology we use [120]. The steps 17 of our algorithm are described below and shown in Figure 2.5. 1. <p> This follows from standard communication analysis: communication is placed just before the outermost loop in which there is no true dependence on u, and is placed just before the statement containing u if no such loop exists <ref> [123, 72, 68] </ref>. Given a use u, let d range over the reaching regular defs of u. (Reaching defs are defined in Cytron et al [39, 35].) Consider some d. Observe that it is never necessary to place communication for u deeper than at CNL (d; u). <p> Particular technical references and their relation with this thesis are given in specific chapters. 6.1 Communication scheduling The results in Chapter 2 follow (and generalize) a large number of preliminary results in communication optimization, such as message vectorizing, coalescing, and redundancy elimination <ref> [72, 123, 20, 5, 21, 68] </ref>. Message vectorization is the technique of hoisting communication of array elements or sections out of loops to produce a single large message. Typically this works locally on single loop-nests [123, 72, 67, 83, 68]. <p> Message vectorization is the technique of hoisting communication of array elements or sections out of loops to produce a single large message. Typically this works locally on single loop-nests <ref> [123, 72, 67, 83, 68] </ref>. As compilers were applied to more complex code, eliminating redundant communication beyond single loop-nests and even across procedures became essential.
Reference: [69] <author> M. Gupta, E. Schonberg, and H. Srinivasan. </author> <title> A unified framework for optimizing communication in data-parallel programs. </title> <type> Technical Report RC 19872(87937) 12/14/94, </type> <institution> IBM Research, </institution> <year> 1994. </year> <note> To appear in IEEE Transactions on Parallel and Distributed Systems. </note>
Reference-contexts: In this chapter we present a novel compiler algorithm that recognizes the global nature of the communication placement problem. Our algorithm derives from static single assignment analysis, array dependence analysis, and data availability analysis <ref> [69] </ref>, which is extended to detect compatibility of communication patterns in addition to redundancy. We differ significantly from previous research, in which the position of 11 communication code for each remote access is decided independent of other remote accesses; instead, we determine the positions in an interdependent and global manner. <p> One of our key innovations is to exploit the static single assignment (SSA) information [39, 35] already computed in an earlier phase by pHPF, refined by array dependence-testing [121]. In contrast, previous proposals for such analysis typically use a bidirectional dataflow approach with array section descriptors and/or bit-vectors <ref> [69] </ref>. 2. For each nonlocal reference, identify a set of candidate positions, any one of which can be potentially chosen as the final point to emit a call to a message-passing runtime routine (x2.4.4). 3. Perform the "array-section" analog of common subexpression elimination: detect and eliminate subsumed communication (x2.4.6). 4. <p> Claim 2.4.1 Earliest (u) returns the earliest single dominating communication point d 1 for use u. In Figure 2.6, Earliest (a 1 ) = Earliest (a 2 ) = 7. Traditional array dataflow analysis, which does not insist on dominating defs <ref> [69] </ref>, would lead to Earliest 0 (a 1 ) = Earliest 0 (a 2 ) = f4; 6g. In both cases, a 2 subsumes a 1 . We prove Claim 2.4.1 using the following three lemmas. Lemma 2.4.2 d 1 dominates u. <p> This test is based on the Available Section Descriptor (ASD) representation of communication <ref> [69] </ref>. <p> Thus, by choosing a placement for b 1 later than Earliest (b 1 ), we are able to eliminate the communication for b 1 completely. In contrast, the solution proposed in <ref> [69] </ref> would move each communication to the earliest point, and reduce the communication 28 for b 2 to ASD (b 2 ) ASD (b 1 ), while the communication for b 1 would remain unchanged. <p> The check for M 1 M 2 is done in the virtual processor space of template positions, as described in <ref> [69] </ref>. However, we have incorporated extensions to check for equality of mappings in physical processor space for nearest-neighbor communication and for mappings to a constant processor position [69]. 2.4.8 Code generation As shown in Figure 2.5, the step after communication analysis and optimization is to insert communication code in the form <p> The check for M 1 M 2 is done in the virtual processor space of template positions, as described in <ref> [69] </ref>. However, we have incorporated extensions to check for equality of mappings in physical processor space for nearest-neighbor communication and for mappings to a constant processor position [69]. 2.4.8 Code generation As shown in Figure 2.5, the step after communication analysis and optimization is to insert communication code in the form of subroutine calls to the pHPF runtime library routines, which in turn invoke MPL/MPI. <p> As compilers were applied to more complex code, eliminating redundant communication beyond single loop-nests and even across procedures became essential. This was typically done by tracing the uses of an array (section) to a defining statement, placing all the communication entries after that def, and canceling subsumed communication entries <ref> [69] </ref>. Communication can also be regarded as invoking a long-lasting asynchronous operation on the network "functional unit." To minimize the latency penalty for fine-grain communication, it helps to detect writes to shared memory that can be reordered or overlapped without violating the programmer's view of consistency of program variables.
Reference: [70] <author> L. Hall, D. Shmoys, and J. Wein. </author> <title> Scheduling to minimize average completion time: Off-line and on-line algorithms. </title> <booktitle> In Symposium on Discrete Algorithms (SODA). ACM-SIAM, </booktitle> <year> 1996. </year>
Reference-contexts: a makespan smaller than (V + log T ). 69 Reference # Job Malleable Nonmalleable Precedence Online/ Makespan procs type processor processor off-line / WACT Garey et al [57] N/A N/A fi p ; on makespan Feldmann et al [51] m par p fi any on makespan Hall et al <ref> [70] </ref> 1; m seq fi fi any off both 1; m seq fi fi ; on both This chapter [29, 28] m par p fi any on both m par fi ; on both m par any on makespan m par any off WACT m par forest/SPG on both Table 4.1: <p> Apart from algorithm design, we believe it is important to point out some differences between existing scheduling literature and features needed by schedulers in parallel computing systems. We also remark that although 71 our problem is different from existing theoretical settings, our solution borrows from various existing techniques <ref> [57, 103, 29, 70] </ref>. In x4.2 we study database and scientific computing scenarios to justify our model. In x4.3 and x4.4 we give the makespan lower and upper bounds. In x4.5 we show how to extend the makespan algorithm to a WACT algorithm. <p> How can we get constructive polynomial-time computable (a 1 ; a 2 )-schedules for small constants a 1 and a 2 ? Hall et al suggest a framework to achieve small a 2 <ref> [70] </ref>; their framework is actually also good for makespan [29]. First divide time into geometrically increasing intervals. That 80 is, define t 0 = 1, t ` = 2 `1 , ` = 1; : : :. <p> We schedule the output of Makespan in the interval [t ` ; t `+1 ). The following analysis of the geometric series framework for WACT was proposed by Hall et al <ref> [70] </ref>. We repeat this for later reference, and give a proof that it is also good for makespan. <p> Recall that throughout we have assumed s = O (1). In general, the algorithm above can be proved to be an O (s + log T ) approximation. If we are only interested in offline schedules, we can compute J 0 ` via rounding an integer program similar to <ref> [70] </ref>, obviating the need for the DualPack routine. We omit the details of the following claim. Theorem 4.5.8 There is an offline algorithm, polynomial in s, T and n, that approxi mates makespan and WACT to an O (s + log T ) multiplicative factor. <p> It is known that precedence-constrained knapsack with general precedence is strongly N Phard [75], unlike forests. We show that settling the approximability issue will be challenging [4]. This shows that the framework of <ref> [70] </ref> may need modification to handle DAG's, not necessarily that the scheduling problem is difficult. <p> For independent non-malleable jobs in the off-line setting, several WACT results have been known [111]. Recently, Hall etal suggested two important general frameworks for off-line and on-line WACT optimization <ref> [70] </ref>. Our results in Chapter 4 build upon these results. 6.4 Resource scheduling Garey et al [57] study a generalization of nonmalleable jobs where there are s distinct types of resources. At any time a total of at most one unit of each resource can be allocated.
Reference: [71] <author> L. A. Hall. </author> <title> Approximation Algorithms for N P-Hard Problems, chapter Approximation Algorithms for Scheduling. </title> <publisher> PWS Publishing Company, </publisher> <address> Boston, MA, </address> <year> 1996. </year> <month> 122 </month>
Reference-contexts: The performance impact of these scheduling algorithms has grown with the advent of superscalar RISC architectures which have multiple functional units permitting out-of-order execution. 6.2 Multiprocessor scheduling Job scheduling in multiprocessors has been extensively researched in both Operations Research and Computer Science <ref> [71] </ref>. Two decades later there is significant interest in the Computer Science community on scheduling parallel jobs [111, 41].
Reference: [72] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation and generating the communication necessary to fetch values of non-local data referenced by a processor <ref> [72, 123, 20, 5, 21, 68] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data, for the following reasons. It is getting increasingly cost-effective to build multiprocessors from commodity hardware components and system software. <p> This follows from standard communication analysis: communication is placed just before the outermost loop in which there is no true dependence on u, and is placed just before the statement containing u if no such loop exists <ref> [123, 72, 68] </ref>. Given a use u, let d range over the reaching regular defs of u. (Reaching defs are defined in Cytron et al [39, 35].) Consider some d. Observe that it is never necessary to place communication for u deeper than at CNL (d; u). <p> The gap is small. Data parallelism is the most familiar parallel execution model in scientific computing, as in ScaLAPACK, CMSSL, Fortran D, CM-Fortran, and High Performance Fortran <ref> [45, 76, 72, 49, 55] </ref>. The computational load can usually be balanced statically, and the important performance issue is generating parallel loops with minimal communication overhead (as in Chapter 2). <p> Particular technical references and their relation with this thesis are given in specific chapters. 6.1 Communication scheduling The results in Chapter 2 follow (and generalize) a large number of preliminary results in communication optimization, such as message vectorizing, coalescing, and redundancy elimination <ref> [72, 123, 20, 5, 21, 68] </ref>. Message vectorization is the technique of hoisting communication of array elements or sections out of loops to produce a single large message. Typically this works locally on single loop-nests [123, 72, 67, 83, 68]. <p> Message vectorization is the technique of hoisting communication of array elements or sections out of loops to produce a single large message. Typically this works locally on single loop-nests <ref> [123, 72, 67, 83, 68] </ref>. As compilers were applied to more complex code, eliminating redundant communication beyond single loop-nests and even across procedures became essential.
Reference: [73] <author> W. Hong and M. Stonebraker. </author> <title> Optimization of parallel query execution plans in XPRS. </title> <booktitle> Distributed and Parallel Databases, </booktitle> <volume> 1(1) </volume> <pages> 9-32, </pages> <month> Jan. </month> <year> 1993. </year> <title> Also see Parallel Query Processing using Shared Memory Multiprocessing and Disk Arrays by W. </title> <type> Hong, PhD thesis, </type> <institution> UCB/ERL M93-28. </institution>
Reference-contexts: This has a serious drawback in that some obvious, critical co-scheduling may be lost. For example, a CPU-bound job from one query and the IO-bound job of another can be co-scheduled and it is highly desirable to do so <ref> [73] </ref>; this cannot be done after collapsing the query. Only a malleable resource. If there are no nonmalleable resources but only a malleable resource, then precedence can be handled (in the sense of approximating makespan) even by a scheduler that does not know t j before a job finishes [51]. <p> The tools are standard in database literature [101]. For parallel databases, one can also estimate for each operation the maximum number of processors that can be employed for near-linear speedup <ref> [73] </ref>. Thus t j and m j can be estimated when a job arrives. Estimates of sizes of intermediate results can be used to estimate the memory and disk bandwidth resource vector ~r j . <p> Once processor and memory allocation are fixed, the disk bandwidth requirement can be estimated from the total IO volume and job running time. This model is best suited to shared memory databases running on symmetric 72 multiprocessors (SMP) with shared access to disk <ref> [73] </ref>. They currently scale to 30-40 processors. There is growing consensus that SMP's and scalable multiprocessors will converge to networked clusters of SMP nodes [66].
Reference: [74] <author> C. A. J. Hurkens and M. J. Coster. </author> <title> On the makespan of a schedule minimizing total completion time for unrelated parallel machines. </title> <type> Unpublished manuscript, </type> <year> 1996. </year>
Reference-contexts: For unrelated machines, there are instances where any optimal WACT schedule for n jobs has makespan (log n= log log n) times the optimal makespan <ref> [74] </ref>. 79 metric. By relaxing the requirement on WACT from optimal to a constant factor, we show in Theorem 4.5.2 that there are schedules optimal for both WACT and makespan to within constant factors [29], even for unrelated machines and jobs with precedence.
Reference: [75] <author> D. S. Johnson and K. A. Niemi. </author> <title> On knapsacks, partitions, and a new dynamic programming technique for trees. </title> <journal> Mathematics of Operations Research, </journal> <volume> 8(1) </volume> <pages> 1-14, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: To achieve this, we round down each s j by units of *S=n and then use dynamic programming as in <ref> [75] </ref>. In each of the following subsections, we will give an implementation of DualPack for a specific problem; throughout, we denote the deadline by D and the set of jobs from which we choose by J . <p> Arbitrary DAGs. While we have handled hierarchical job graphs such as forests or series-parallel graphs, the general DAG case is open. It is known that precedence-constrained knapsack with general precedence is strongly N Phard <ref> [75] </ref>, unlike forests. We show that settling the approximability issue will be challenging [4]. This shows that the framework of [70] may need modification to handle DAG's, not necessarily that the scheduling problem is difficult.
Reference: [76] <author> S. L. Johnsson. </author> <title> Communication efficient basic linear algebra computations on hypercube architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 4(2), </volume> <month> Apr. </month> <year> 1987. </year>
Reference-contexts: The gap is small. Data parallelism is the most familiar parallel execution model in scientific computing, as in ScaLAPACK, CMSSL, Fortran D, CM-Fortran, and High Performance Fortran <ref> [45, 76, 72, 49, 55] </ref>. The computational load can usually be balanced statically, and the important performance issue is generating parallel loops with minimal communication overhead (as in Chapter 2).
Reference: [77] <author> R. Karp, M. Luby, and F. M. auf der Heide. </author> <title> Efficient PRAM simulation on a distributed memory machine. </title> <booktitle> In Symposium on the Theory of Computing (STOC), </booktitle> <pages> pages 318-326. </pages> <publisher> ACM, </publisher> <year> 1992. </year>
Reference-contexts: Our application of random allocation to load balancing scenarios is related to results on random hashing and PRAM emulation <ref> [22, 77] </ref>.
Reference: [78] <author> R. M. Karp and Y. Zhang. </author> <title> A randomized parallel branch-and-bound procedure. </title> <journal> Journal of the ACM, </journal> <volume> 40 </volume> <pages> 765-789, </pages> <year> 1993. </year> <note> Preliminary version in ACM STOC 1988, pp290-300. </note>
Reference-contexts: Although used in practice, the effect of random allocation on dynamic, irregular task graphs was unresolved before this work; the closest known analysis was for unit-time tasks <ref> [78] </ref>. The resulting bounds are supported by practical experience with irregular applications [30, 25], which show that randomization is an effective tool for load balancing a certain class of applications. <p> Processors can negotiate to transfer available jobs among themselves. There is no coordinated global communication for load balancing purposes. We study the setting with a local priority queue of jobs in the memory of each processor as in <ref> [78] </ref>. Thus, priority is preserved within each local queue but not across processors. An idle processor non-preemptively executes the best job from its local queue, if any. Any newly available job with completed predecessors is enqueued into the priority queue of a processor chosen uniformly at random. <p> The bottleneck can be relieved in a variety of ways, each of which reduces communication cost by sacrificing global load information and thus risking some load imbalance. We studied work sharing, where busy processors forward jobs to random processors. This is similar to Karp and Zhang's model <ref> [78] </ref>. Work sharing is good at dispersing hot-spots. Another technique that has been designed and analyzed lately is work stealing, where idle processors ask for work [19]. The theoretical bounds are only applicable to divide and conquer, where a finite, fixed task tree has to be completely executed.
Reference: [79] <author> K. Keeton, T. Anderson, and D. Patterson. </author> <title> LogP quantified: The case for low-overhead local area networks. </title> <booktitle> In Proc. Hot Interconnects III: A Symposium on High Performance Interconnects, </booktitle> <address> Stanford, CA, </address> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: The SP2 uses IBM's message passing library MPL; the NOW uses MPICH, a portable implementation of the MPI standard from Argonne National Labs. Details of the networks can be found in <ref> [109, 106, 79] </ref>. We want to measure the benefits of large messages, while estimating the local block copy (bcopy) cost to collect many small messages into a large one. Figure 2.4 shows the profiling code and results.
Reference: [80] <author> H. Kellerer, T. Tautenhahn, and G. J. Woeginger. </author> <title> Approximability and non-approximability results for minimizing total flow time on a single machine. </title> <booktitle> In Symposium on the Theory of Computing (STOC). ACM, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Unfortunately, even with a single machine, off-line problem instance, and no resource or precedence constraints, it is N Phard to approximate non-preemptive flowtime better than about a factor of ( p n) <ref> [80] </ref>. <p> In sharp contrast, a remarkable recent result shows that for non-preemptive weighted flow time, approximating better than about a factor of p n is N P-hard <ref> [80] </ref> even in the simplest model of sequential jobs on a single processor, with an off-line clairvoyant scheduler. This lower bound holds a fortiori for our generalized job models. It is perhaps unreasonable in practice to demand near-optimal weighted flow time without preemption.
Reference: [81] <author> K. Kennedy and N. Nedeljkovic. </author> <title> Combining dependence and data-flow analyses to optimize communication. </title> <booktitle> In International Parallel Processing Symposium. IEEE, </booktitle> <year> 1995. </year> <month> 123 </month>
Reference-contexts: Kennedy et al estimate this to be typically 5-9% for 2-d stencil problems of size 256 fi 256 on a 16-processor iPSC/860, in the context of the Fortran-D execution model and runtime system <ref> [81] </ref>. The top curve shows the bandwidth of local block copy (bcopy) as a function of buffer size. The bottom curve plots network bandwidth as a function of message length, based on the time that the receiver waits for completion. <p> The x-axis is to a log scale. certain message sizes. Injection bandwidth also declines around the cache limit. Given our execution model and typical machine characteristics, we consider message aggregation and parallelization as the first-order concerns, and overlap as a second-order concern <ref> [81] </ref>. The latter also depends on the co-processor and network software. E.g., the implementors of MPL minimize coprocessor assistance because the i860 coprocessor is much slower than the RS 6000 CPU, and the channel between the CPU and the co-processor is slow [106].
Reference: [82] <author> A. Krishnamurthy and K. Yelick. </author> <title> Optimizing parallel programs with explicit synchronization. </title> <booktitle> In Programming Language Design and Implementation (PLDI), </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year> <note> ACM. </note>
Reference-contexts: An early result in this direction finds cycles of conflicting reads and writes and inserts minimal 109 synchronization to break all cycles [102]. This technique has recently been improved for single program multiple data (SPMD) sources <ref> [82] </ref>. Another related area is instruction scheduling, which has been intensively studied [94].
Reference: [83] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Message vectorization is the technique of hoisting communication of array elements or sections out of loops to produce a single large message. Typically this works locally on single loop-nests <ref> [123, 72, 67, 83, 68] </ref>. As compilers were applied to more complex code, eliminating redundant communication beyond single loop-nests and even across procedures became essential.
Reference: [84] <author> X. Li and H. Huang. </author> <title> On the concurrency of C++. </title> <booktitle> In Proceedings ICCI '93. Fifth International Conference on Computing and Information, </booktitle> <pages> pages 215-19, </pages> <address> Ontario, Canada, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: But these are worst-case guarantees. In the systems area, the Paradigm compiler [95], iWarp compiler [110], and NESL compiler [32] all support forms of mixed task and data parallelism, and there are plans to merge data Fortran D with Fortran M [56] and pC++ with CC++ <ref> [84] </ref> to support mixed parallelism. The compiler efforts all depend on static task graph and profile data and perform extensive optimizations to allocate processors. We have already noted how dynamic task parallelism is more promising to exploit.
Reference: [85] <author> J. W. H. Liu. </author> <title> The multifrontal method for sparse matrix solution: </title> <journal> theory and practice. SIAM Review, </journal> <volume> 34(1) </volume> <pages> 82-109, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In timing-level circuit simulation there is parallelism between separate subcircuits and parallelism within the model evaluation of each subcircuit [116]. In sparse matrix factorization, multi-frontal algorithms expose task parallelism between separate dense sub-matrices and data parallelism within those dense matrices <ref> [85] </ref>. In global climate modeling [89], there are large data parallel computations performed on grids representing the earth's atmosphere and oceans, and task parallelism from the different physical processes 62 being modeled. Several researchers have proposed support to take advantage of this mixed parallelism.
Reference: [86] <author> P. Liu. </author> <title> The Parallel Implementation of N body Algorithms. </title> <type> PhD thesis, </type> <institution> DIMACS Center, Rutgers University, </institution> <address> Piscataway, New Jersey 08855-1179, </address> <month> May </month> <year> 1994. </year> <note> Also available as DIMACS Technical Report 94-27. </note>
Reference-contexts: Both our applications lead to tree-shaped precedence between jobs, and the job times are diverse. In both cases, most of shared data can be replicated at small communication cost, so random allocation is feasible. Random allocation with diverse times have also been use in N body simulation <ref> [86] </ref> and integer linear programming [47]. For each application, we added instrumentation to the sequential program to emit the task tree with task times, and input this tree to a simulator that simulated the parallel execution of the randomized load balancing algorithm, as well as Graham's list schedule. <p> The performance of multiple round strategies [9, 1] for dynamic job graphs remains open, as is their effect on further reducing the load imbalance. Finally, extending the results to capture network contention as in the atomic message model <ref> [86] </ref> seems like a natural goal. 5.3 Multi-round load balancing protocols The goal of this section is to generalize single round random allocations to multiple rounds, aided by spreading load information, to facilitate better load balance.
Reference: [87] <author> W. Ludwig and P. Tiwari. </author> <title> Scheduling malleable and nonmalleable parallel tasks. </title> <booktitle> In Symposium on Discrete Algorithms (SODA), </booktitle> <pages> pages 167-176. ACM-SIAM, </pages> <year> 1994. </year>
Reference-contexts: Several researchers have proposed support to take advantage of this mixed parallelism. In the theory area, the best known online scheduling algorithm for mixed parallelism is 2:62-optimal [16, 52], and the best offline algorithm is 2-optimal <ref> [113, 87] </ref>. But these are worst-case guarantees. <p> No precedence. If the precedence graph is empty, (i.e., jobs are independent) then a number of approaches are known to get approximation bounds <ref> [57, 87, 113, 112, 111] </ref>. In the database scenario, it is possible to collapse each query, consisting of several jobs with a precedence relation among them, into one job, i.e., allocate maximum resources over all the jobs in the query, and then apply the results for independent jobs [119]. <p> The best off-line performance guarantee known for the non-malleable special case, where all jobs arrive at time zero, is 8.53, due to Turek et al [112]. Using an idea of Ludwig et al <ref> [87] </ref>, this can be extended to the malleable case. Our WACT algorithm handles malleable jobs, online job arrival and has a performance guarantee of 12 + *, and if we allow randomization, a nearly identical (expected) guarantee of 8:67. <p> This tradeoff can be specified as an arbitrary function for the running time on any given number of processors. For jobs with this model of trade-off, various constant-factor approximations for makespan are known for the special case where there is no precedence relation; i.e., the jobs are independent <ref> [16, 87] </ref>. Our results in Chapters 3 and 4 extend these results. In a slightly simpler but still reasonably realistic model of malleability, every job specifies a maximum number of processor up to which it will give perfect speedup.
Reference: [88] <author> S. Luna. </author> <title> Implementing an efficient portable global memory layer on distributed memory multiprocessors. </title> <type> Technical Report UCB/CSD-94-810, </type> <institution> University of California, Berkeley, </institution> <address> CA 94720, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Estimates are in part from <ref> [107, 117, 88, 7] </ref>; the network software are described in these references. to be very large on most distributed memory machines, although reasonable bandwidth can be supported for sufficiently large messages [109, 106]. See Table 2.1 for some idea of the CPU and communication speeds of current multiprocessors. <p> The curves were fit for 2 P 500 and 100 n = N 1=2 10000. An estimate of memory per processor in megabytes is given in the column marked M/P. Estimates for ff and fi are in part from <ref> [107, 117, 88, 7] </ref>. of processors, network latency, and network bandwidth. Using these given functions, we first estimate the parallel running time r (N; P ) for a given machine and problem, then fit Equation (3.1) to it using Matlab.
Reference: [89] <author> C. R. Mechoso, C.-C. Ma, J. Farrara, J. A. Spahr, and R. W. Moore. </author> <title> Parallelization and distribution of a coupled atmosphere-ocean general circulation model. </title> <journal> Monthly Weather Review, </journal> <volume> 121(7) </volume> <pages> 2062-2076, </pages> <year> 1993. </year>
Reference-contexts: In timing-level circuit simulation there is parallelism between separate subcircuits and parallelism within the model evaluation of each subcircuit [116]. In sparse matrix factorization, multi-frontal algorithms expose task parallelism between separate dense sub-matrices and data parallelism within those dense matrices [85]. In global climate modeling <ref> [89] </ref>, there are large data parallel computations performed on grids representing the earth's atmosphere and oceans, and task parallelism from the different physical processes 62 being modeled. Several researchers have proposed support to take advantage of this mixed parallelism.
Reference: [90] <author> M. Mehta and D. Dewitt. </author> <title> Dynamic memory allocation for multiple-query workloads. </title> <booktitle> In Very Large Databases (VLDB), </booktitle> <pages> pages 354-367, </pages> <year> 1993. </year>
Reference-contexts: We give a simple approximation algorithm that matches the O (V + log T ) makespan bound. In contrast to our algorithm, most known practical solutions use some variant of greedy list- or queue-type scheduling <ref> [50, 59, 90] </ref>. Jobs on arrival are placed in a list ordered by some heuristic (often FIFO). The scheduler dispatches the first ready job on the list when enough resources become available. <p> In x4.3 and x4.4 we give the makespan lower and upper bounds. In x4.5 we show how to extend the makespan algorithm to a WACT algorithm. In x4.6 we pose some unresolved problems. 4.2 Motivation 4.2.1 Databases Query scheduling in parallel databases is a topic of active research <ref> [23, 90, 119, 66, 59] </ref>. Queries arrive from many users to a front-end manager process. <p> relations R 1 and R 2 with, say, R 1 being smaller, takes time roughly proportional to dlog r jR 1 je, where r is the memory allocated; typically the query planner picks r = jR 1 j or r = jR 1 j 1=2 , independent of other queries <ref> [90] </ref>. Once processor and memory allocation are fixed, the disk bandwidth requirement can be estimated from the total IO volume and job running time. This model is best suited to shared memory databases running on symmetric 72 multiprocessors (SMP) with shared access to disk [73].
Reference: [91] <author> J. E. Moreira, V. K. Naik, and R. B. Konuru. </author> <title> A system for dynamic resource allocation and data distribution. </title> <type> Technical Report RC 20257, </type> <institution> IBM Research, Yorktown Heights, </institution> <month> Oct. </month> <year> 1995. </year> <month> 124 </month>
Reference-contexts: To improve utilization, system support has been designed to express jobs at a finer level inside an application and convey the information to the resource manager by annotating the parallel executable <ref> [65, 48, 91] </ref>.
Reference: [92] <author> R. Motwani, S. Phillips, and E. Torng. </author> <title> Non-clairvoyant scheduling. </title> <journal> Theoretical Computer Science, </journal> <volume> 130 </volume> <pages> 17-47, </pages> <month> August </month> <year> 1994. </year> <note> Preliminary version in SODA 1993, pp 422-431. </note>
Reference-contexts: Previous work on the WACT metric gave either non-clairvoyant, preemptive solutions for sequential jobs or jobs that used a perfectly malleable processor resource, with job precedence [41]; or clairvoyant, non-preemptive solutions for independent jobs with only non-malleable resource, if any <ref> [92, 112] </ref>. Apart from algorithm design, we believe it is important to point out some differences between existing scheduling literature and features needed by schedulers in parallel computing systems. <p> How can jobs with such persistent resource needs be scheduled? Non-clairvoyance and preemption. For the motivating applications, reasonable estimates of job running time are possible. More general purpose schedulers must be non-clairvoyant, i.e., work without knowledge of t j before j completes <ref> [105, 92] </ref>. To handle this, recourse to job preemption or cancellation is needed, whose large cost has to be factored into the algorithm. Arbitrary DAGs. While we have handled hierarchical job graphs such as forests or series-parallel graphs, the general DAG case is open. <p> In the design of schedulers for general-purpose workstations that run a mix of diverse jobs for which no a priori estimates of time and resource can be made, non-clairvoyance is of great importance. As might be suspected, non-clairvoyant schedulers are remarkably handicapped compared to clairvoyant ones <ref> [92] </ref>, even when permitted the (essential) power of preemption. These results are for sequential jobs; some of them have been extended to the perfectly malleable job model [41].
Reference: [93] <author> R. Motwani and P. Raghavan. </author> <title> Randomized Algorithms. </title> <publisher> Cambridge University Press, </publisher> <year> 1995. </year>
Reference-contexts: It is well known that when n balls are thrown independently and uniformly at random into n bins, with high probability (by which we shall mean 1 O ( 1 n )) the maximum number of balls received by any bin is fi ( log n log log n ) <ref> [93] </ref>. Recently, an important extension of this result was proven by Azar et al [9]. Suppose we place the balls sequentially, one at a time; for each ball, we choose two bins independently and uniformly at random, and place the ball in the less full bin.
Reference: [94] <author> K. V. Palem and B. B. Simons. </author> <title> Scheduling time-critical instructions on RISC machines. </title> <booktitle> In Principles of Programming Languages (POPL), </booktitle> <pages> pages 270-280, </pages> <address> San Francisco, CA, </address> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: An early result in this direction finds cycles of conflicting reads and writes and inserts minimal 109 synchronization to break all cycles [102]. This technique has recently been improved for single program multiple data (SPMD) sources [82]. Another related area is instruction scheduling, which has been intensively studied <ref> [94] </ref>. The performance impact of these scheduling algorithms has grown with the advent of superscalar RISC architectures which have multiple functional units permitting out-of-order execution. 6.2 Multiprocessor scheduling Job scheduling in multiprocessors has been extensively researched in both Operations Research and Computer Science [71].
Reference: [95] <author> S. Ramaswamy, S. Sapatnekar, and P. Banerjee. </author> <title> A convex programming approach for exploiting data and functional parallelism on distributed memory multiprocessors. </title> <booktitle> In International Conference on Parallel Processing (ICPP). IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: Static task graphs, such as those generated from control flow graphs by parallelizing compilers, have a fixed small degree of task parallelism. For example, the benchmarks in <ref> [95] </ref> have 4-7 fold effective task parallelism and the signal processing applications in [110] have a 2-5 fold task parallelism. The task parallelism in climate modeling applications is typically no more than 4-6. <p> deal with inter-process communication on the same processor.) The challenge is that often no information about the structure of the task graph or the cost of tasks is available at compile-time, making load balancing a more difficult runtime problem. 3.2 Notation We will model mixed parallel applications as macro-dataflow graphs <ref> [95] </ref>. Each vertex of the (directed acyclic) graph represents a data-parallel task, which could be written using HPF or ScaLAPACK. The edges represent data and/or control dependency. <p> is not optimal, the relative improvement of using mixed parallelism over pure data parallelism for the batch problem is e M = P + 1 P + 1 (3.3) Example 3.4.1 We apply this analysis to complex matrix multiplication, which was reported as a benchmark for the Illinois Paradigm compiler <ref> [95] </ref>. The task is MM, the machine is the CM5 without vector units, and L = 4. From Figure 3.1 we obtain = 53 and e 1 = 1 for this problem. <p> E.g., if P = 64 and * = 0:5, then n &lt; 42, a tiny problem indeed. It is interesting that the experiments reported in <ref> [95] </ref> for the CM5 use P 2 f64; 128g processors and n = 64. <p> While not quite infinite, our problem range p N goes into several thousands, and therefore the performance gains are much more impressive and meaningful than the benefits on static problems typically of size p N = 64 : : : 128, on P = 64 processors <ref> [110, 95] </ref>. We have also measured some typical numbers for the overhead of the data remap step in the switching algorithm. They are quite low; a few percent. See Figure 3.10. <p> In the theory area, the best known online scheduling algorithm for mixed parallelism is 2:62-optimal [16, 52], and the best offline algorithm is 2-optimal [113, 87]. But these are worst-case guarantees. In the systems area, the Paradigm compiler <ref> [95] </ref>, iWarp compiler [110], and NESL compiler [32] all support forms of mixed task and data parallelism, and there are plans to merge data Fortran D with Fortran M [56] and pC++ with CC++ [84] to support mixed parallelism.
Reference: [96] <author> A. Ranade. </author> <title> A simpler analysis of the Karp-Zhang parallel branch-and-bound method. </title> <type> Technical Report UCB/CSD 90/586, </type> <institution> University of California, Berkeley, </institution> <address> CA 94720, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: We handle this using a delay sequence argument. The following lemma is similar to Ranade's construction <ref> [96] </ref>. Lemma 5.2.2 Suppose the execution finishes at time t . Then the following 4-tuple (s; Q; R; ) exists: * s is a job that finished no earlier than t .
Reference: [97] <author> M. C. Rinard, D. J. Scales, and M. S. Lam. </author> <title> Jade: A high-level, machine-independent language for parallel programming. </title> <booktitle> IEEE Computer, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: For popular languages like C or C++, detecting parallelism in presence of aliases, pointers, and linked data structures is very difficult. Several projects such as Suif, Olden, and Jade, as well as commutativity and synchronization analyses have made initial progress in these directions <ref> [24, 118, 97] </ref>. The second problem is for the compiler to explicitly orchestrate communication between different register files or functional units.
Reference: [98] <author> L. Rudolph, M. Slivkin-Allalouf, and E. Upfal. </author> <title> A simple load balancing scheme for task allocation in parallel machines. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 237-245, </pages> <year> 1991. </year>
Reference-contexts: Second, although the above result establishes near-optimal load balance, our model does not reflect the gains from avoiding communication bottlenecks. Other results of the diffusion type are based on occasionally matching busy and idle processors and transferring jobs <ref> [61, 98] </ref>. These are not appropriate for relatively fine-grain jobs which is our focus. Notice also that diversity in job execution times makes coordination even harder unless a processor can suspend long jobs and participate in global communication. <p> Both of the above assume an amorphous network. For general network topologies, another technique that has been studied a great deal recently is diffusion, where neighbors exchange local load information and then move some jobs from busy to lazy processors <ref> [98, 61] </ref>. Our application of random allocation to load balancing scenarios is related to results on random hashing and PRAM emulation [22, 77].
Reference: [99] <author> J. Rutter. </author> <title> A serial implementation of Cuppen's divide and conquer algorithm for the symmetric eigenvalue problem. </title> <institution> Mathematics Dept. </institution> <note> Master's Thesis available by anonymous ftp to tr-ftp.cs.berkeley.edu, directory pub/tech-reports/csd/csd-94-799, file all.ps, </note> <institution> University of California, </institution> <year> 1994. </year>
Reference-contexts: An eigenvalue algorithm of a different flavor, but still from the divide and conquer category, is Cuppen's method for symmetric tridiagonal matrices, where we can actually split the matrix exactly in half all the time <ref> [38, 99] </ref> (although the costs of the children are not so simple). 51 Sparse Cholesky. We consider the regular but important special case of the matrix arising from the 5-point Laplacian on a square grid, ordered using the nested dissection ordering [60].
Reference: [100] <author> V. Sarkar. </author> <title> The PTRAN parallel programming system. </title> <booktitle> Parallel Functional Programming Languages and Compilers, </booktitle> <pages> pages 309-391, </pages> <year> 1991. </year>
Reference-contexts: shown in the last column for comparison. running example to illustrate the operation of the steps of the algorithm. 2.4.1 Representation and notation We represent the program using the augmented control flow graph (CFG), which makes loop structure more explicit than the standard CFG by placing preheader and postexit nodes <ref> [3, 100] </ref> before and after loops. These extra nodes also provide convenient locations for summarizing dataflow information for the loop. The CFG is a directed graph where each node is a basic block, a sequence of statements without jumps. Execution starts at the ENTRY node.
Reference: [101] <author> P. G. Selinger, M. M. Astrahan, D. D. Chamberlin, R. A. Lorie, and T. G. Price. </author> <title> Access path selection in a relational database management system. </title> <booktitle> In ACM SIGMOD Conference on the Management of Data, </booktitle> <pages> pages 23-34, </pages> <year> 1979. </year> <month> 125 </month>
Reference-contexts: Databases keep certain access statistics along with the relations, which are used to predict the size of the result of a join or a select and how many CPU instructions will be required to compute these results. The tools are standard in database literature <ref> [101] </ref>. For parallel databases, one can also estimate for each operation the maximum number of processors that can be employed for near-linear speedup [73]. Thus t j and m j can be estimated when a job arrives.
Reference: [102] <author> D. Shasha and M. Snir. </author> <title> Efficient and correct execution of parallel programs that share memory. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(2) </volume> <pages> 282-312, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: An early result in this direction finds cycles of conflicting reads and writes and inserts minimal 109 synchronization to break all cycles <ref> [102] </ref>. This technique has recently been improved for single program multiple data (SPMD) sources [82]. Another related area is instruction scheduling, which has been intensively studied [94].
Reference: [103] <author> D. Shmoys, C. Stein, and J. Wein. </author> <title> Improved approximation algorithms for shop scheduling problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 23 </volume> <pages> 617-632, </pages> <year> 1994. </year> <note> Preliminary version in SODA 1991. </note>
Reference-contexts: Apart from algorithm design, we believe it is important to point out some differences between existing scheduling literature and features needed by schedulers in parallel computing systems. We also remark that although 71 our problem is different from existing theoretical settings, our solution borrows from various existing techniques <ref> [57, 103, 29, 70] </ref>. In x4.2 we study database and scientific computing scenarios to justify our model. In x4.3 and x4.4 we give the makespan lower and upper bounds. In x4.5 we show how to extend the makespan algorithm to a WACT algorithm. <p> This is the last layer. Divide [t T; (t + 1)T ) into [t T; (t + 1 2 )T ) and [(t + 1 2 )T; (t + 1)T ) and recurse, placing the generated layers in pre-order <ref> [103] </ref>. The total length of the schedule, which may still violate nonmalleable resource limits, will be O ( log T ). Step 4. Schedule each layer separately in time order.
Reference: [104] <author> D. B. Shmoys and D. S. Hochbaum. </author> <title> Using dual approximation algorithms for scheduling problems: theoretical and practical results. </title> <journal> Journal of the ACM, </journal> <volume> 34(1) </volume> <pages> 144-162, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: Let Pack (P; S) be the makespan (length of schedule) generated by packing tasks from set S in task parallel mode into P processors. There are approximation algorithms that return Pack (1 + *) Pack OPT for any fixed * &gt; 0, within time that is polynomial in jSj <ref> [104] </ref>. Here Pack OPT is the optimal makespan. It is easy to see that Pack OPT (P; S) 1 P s2S t s (1) + max s2S t s (1), by an averaging argument. Consider the following heuristic, which we call Prefix-Suffix. 1. <p> We will do binary search for the makespan between a lower and upper bound similar to the classical *-approximate decision procedures <ref> [104] </ref>. Suppose in the current search step the proposed deadline is D. 1. Compute PathToLeaf (j) by a bottom-up traversal. Compute S P = fj : PathToLeaf (j) t j (1) &gt; Dg. If P j2S P t j (P ) &gt; D then deadline D is infeasible. 2.
Reference: [105] <author> D. B. Shmoys, J. Wein, and D. P. Williamson. </author> <title> Scheduling parallel machines online. </title> <booktitle> In Foundations of Computer Science (FOCS), </booktitle> <pages> pages 131-140, </pages> <year> 1991. </year>
Reference-contexts: How can jobs with such persistent resource needs be scheduled? Non-clairvoyance and preemption. For the motivating applications, reasonable estimates of job running time are possible. More general purpose schedulers must be non-clairvoyant, i.e., work without knowledge of t j before j completes <ref> [105, 92] </ref>. To handle this, recourse to job preemption or cancellation is needed, whose large cost has to be factored into the algorithm. Arbitrary DAGs. While we have handled hierarchical job graphs such as forests or series-parallel graphs, the general DAG case is open. <p> Also, makespan results are relevant for a batch of jobs in isolation; in fact, online arrival of the batch make little difference <ref> [105] </ref> (also see 110 Chapter 5). In contrast, for a parallel computer installation, other measures that emphasize notions of fairness or resource utilization are considered more appropriate. In such scenarios, on-line arrivals may make problems harder.
Reference: [106] <author> M. Snir et al. </author> <title> The communication software and parallel environment of the IBM SP2. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 205-221, </pages> <year> 1995. </year>
Reference-contexts: Estimates are in part from [107, 117, 88, 7]; the network software are described in these references. to be very large on most distributed memory machines, although reasonable bandwidth can be supported for sufficiently large messages <ref> [109, 106] </ref>. See Table 2.1 for some idea of the CPU and communication speeds of current multiprocessors. Therefore, parallelizing compilers have to be extremely careful about generating communication code. A single suboptimal choice may waste time equivalent to several thousand CPU operations. <p> The SP2 uses IBM's message passing library MPL; the NOW uses MPICH, a portable implementation of the MPI standard from Argonne National Labs. Details of the networks can be found in <ref> [109, 106, 79] </ref>. We want to measure the benefits of large messages, while estimating the local block copy (bcopy) cost to collect many small messages into a large one. Figure 2.4 shows the profiling code and results. <p> The latter also depends on the co-processor and network software. E.g., the implementors of MPL minimize coprocessor assistance because the i860 coprocessor is much slower than the RS 6000 CPU, and the channel between the CPU and the co-processor is slow <ref> [106] </ref>. However, our algorithm permits additional techniques like Give-n-Take to be used to overlap latency with computation at the sender [114]. 2.4 Compiler algorithms In this section we describe our algorithm for placing communication code.
Reference: [107] <author> K. Stanley and J. Demmel. </author> <title> Modeling the performance of linear systems solvers on distributed memory multiprocessors. </title> <type> Technical report, </type> <institution> University of California, Berkeley, </institution> <address> CA 94720, </address> <year> 1994. </year> <note> In preparation. </note>
Reference-contexts: Estimates are in part from <ref> [107, 117, 88, 7] </ref>; the network software are described in these references. to be very large on most distributed memory machines, although reasonable bandwidth can be supported for sufficiently large messages [109, 106]. See Table 2.1 for some idea of the CPU and communication speeds of current multiprocessors. <p> Parameters ff (latency) and fi (inverse bandwidth; transfer time per double) are normalized to a BLAS-3 FLOP, and the model is fit to data generated from analytical models <ref> [40, 42, 107] </ref>. The curves were fit for 2 P 500 and 100 n = N 1=2 10000. An estimate of memory per processor in megabytes is given in the column marked M/P. <p> The curves were fit for 2 P 500 and 100 n = N 1=2 10000. An estimate of memory per processor in megabytes is given in the column marked M/P. Estimates for ff and fi are in part from <ref> [107, 117, 88, 7] </ref>. of processors, network latency, and network bandwidth. Using these given functions, we first estimate the parallel running time r (N; P ) for a given machine and problem, then fit Equation (3.1) to it using Matlab.
Reference: [108] <author> C. Stein and J. </author> <title> Wein. </title> <type> Personal communication, </type> <month> May </month> <year> 1996. </year>
Reference-contexts: Note that the two optimal schedules are in general different. Theorem 4.5.2 For any scheduling problem, there exists a (2; 2)-schedule. The constants have since been improved to (2; 1:8) <ref> [108] </ref>. However, note that the proof is not constructive, since we need the optimal schedules for makespan and WACT for the construction.
Reference: [109] <author> C. Stunkel et al. </author> <title> The SP2 high performance switch. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 185-204, </pages> <year> 1995. </year>
Reference-contexts: Estimates are in part from [107, 117, 88, 7]; the network software are described in these references. to be very large on most distributed memory machines, although reasonable bandwidth can be supported for sufficiently large messages <ref> [109, 106] </ref>. See Table 2.1 for some idea of the CPU and communication speeds of current multiprocessors. Therefore, parallelizing compilers have to be extremely careful about generating communication code. A single suboptimal choice may waste time equivalent to several thousand CPU operations. <p> The SP2 uses IBM's message passing library MPL; the NOW uses MPICH, a portable implementation of the MPI standard from Argonne National Labs. Details of the networks can be found in <ref> [109, 106, 79] </ref>. We want to measure the benefits of large messages, while estimating the local block copy (bcopy) cost to collect many small messages into a large one. Figure 2.4 shows the profiling code and results.
Reference: [110] <author> J. Subhlok, J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Exploiting task and data parallelism on a multicomputer. </title> <booktitle> In Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 13-22, </pages> <address> San Diego, </address> <month> May </month> <year> 1993. </year> <pages> ACM-SIGPLAN. </pages>
Reference-contexts: Static task graphs, such as those generated from control flow graphs by parallelizing compilers, have a fixed small degree of task parallelism. For example, the benchmarks in [95] have 4-7 fold effective task parallelism and the signal processing applications in <ref> [110] </ref> have a 2-5 fold task parallelism. The task parallelism in climate modeling applications is typically no more than 4-6. <p> While not quite infinite, our problem range p N goes into several thousands, and therefore the performance gains are much more impressive and meaningful than the benefits on static problems typically of size p N = 64 : : : 128, on P = 64 processors <ref> [110, 95] </ref>. We have also measured some typical numbers for the overhead of the data remap step in the switching algorithm. They are quite low; a few percent. See Figure 3.10. <p> In the theory area, the best known online scheduling algorithm for mixed parallelism is 2:62-optimal [16, 52], and the best offline algorithm is 2-optimal [113, 87]. But these are worst-case guarantees. In the systems area, the Paradigm compiler [95], iWarp compiler <ref> [110] </ref>, and NESL compiler [32] all support forms of mixed task and data parallelism, and there are plans to merge data Fortran D with Fortran M [56] and pC++ with CC++ [84] to support mixed parallelism.
Reference: [111] <author> J. Turek, W. Ludwig, J. Wolf, L. Fleischer, P. Tiwari, J. Glasgow, U. Schweigelshohn, and P. S. Yu. </author> <title> Scheduling parallelizable tasks to minimize average response time. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures (SPAA). ACM, </booktitle> <year> 1994. </year>
Reference-contexts: The N P-hardness of the special case of independent tasks a fortiori means that the scheduling problem (in either the switched or mixed model) is also N Phard. The best algorithms known are constant factor approximations, with worst case factors in the range 2-2.6 <ref> [111] </ref>. Unfortunately, a constant factor of this magnitude (which we will call "packing loss") may substantially mask the benefits which would otherwise be obtained from mixed parallelism. <p> No precedence. If the precedence graph is empty, (i.e., jobs are independent) then a number of approaches are known to get approximation bounds <ref> [57, 87, 113, 112, 111] </ref>. In the database scenario, it is possible to collapse each query, consisting of several jobs with a precedence relation among them, into one job, i.e., allocate maximum resources over all the jobs in the query, and then apply the results for independent jobs [119]. <p> Two decades later there is significant interest in the Computer Science community on scheduling parallel jobs <ref> [111, 41] </ref>. For independent jobs that need a fixed given number of processors, various rectangle packing algorithms naturally model the problem: every job can be regarded as a rectangle of width equal to the number of processors, and of height representing the running time [14, 13]. <p> Another dimension of the problem is whether the jobs arrive on-line or are all known off-line; in the former model the scheduler does not know of job j before time a j . For independent non-malleable jobs in the off-line setting, several WACT results have been known <ref> [111] </ref>. Recently, Hall etal suggested two important general frameworks for off-line and on-line WACT optimization [70]. Our results in Chapter 4 build upon these results. 6.4 Resource scheduling Garey et al [57] study a generalization of nonmalleable jobs where there are s distinct types of resources.
Reference: [112] <author> J. Turek, U. Schwiegelshohn, J. Wolf, and P. Yu. </author> <title> Scheduling parallel tasks to minimize average response time. </title> <booktitle> In Symposium on Discrete Algorithms (SODA), </booktitle> <pages> pages 112-121. ACM-SIAM, </pages> <year> 1994. </year> <month> 126 </month>
Reference-contexts: No precedence. If the precedence graph is empty, (i.e., jobs are independent) then a number of approaches are known to get approximation bounds <ref> [57, 87, 113, 112, 111] </ref>. In the database scenario, it is possible to collapse each query, consisting of several jobs with a precedence relation among them, into one job, i.e., allocate maximum resources over all the jobs in the query, and then apply the results for independent jobs [119]. <p> Previous work on the WACT metric gave either non-clairvoyant, preemptive solutions for sequential jobs or jobs that used a perfectly malleable processor resource, with job precedence [41]; or clairvoyant, non-preemptive solutions for independent jobs with only non-malleable resource, if any <ref> [92, 112] </ref>. Apart from algorithm design, we believe it is important to point out some differences between existing scheduling literature and features needed by schedulers in parallel computing systems. <p> The best off-line performance guarantee known for the non-malleable special case, where all jobs arrive at time zero, is 8.53, due to Turek et al <ref> [112] </ref>. Using an idea of Ludwig et al [87], this can be extended to the malleable case. Our WACT algorithm handles malleable jobs, online job arrival and has a performance guarantee of 12 + *, and if we allow randomization, a nearly identical (expected) guarantee of 8:67. <p> If there is no precedence (= ;), we can use Corollary 4.4.5 to obtain the following generalization of Turek et al's result <ref> [112] </ref> to many non-malleable resources. Corollary 4.5.9 There is an offline algorithm, polynomial in s, T and n, that approxi mates makespan and WACT to an O (s) multiplicative factor. 4.6 Extensions Finally, we raise several questions regarding extensions of the model and algorithms in this chapter. Tighter bounds.
Reference: [113] <author> J. Turek, J. L. Wolf, and P. S. Yu. </author> <title> Approximate algorithms for scheduling parallelizable tasks. </title> <booktitle> In Symposium on Parallel Algorithms and Architectures (SPAA), </booktitle> <pages> pages 323-332, </pages> <year> 1992. </year>
Reference-contexts: Several researchers have proposed support to take advantage of this mixed parallelism. In the theory area, the best known online scheduling algorithm for mixed parallelism is 2:62-optimal [16, 52], and the best offline algorithm is 2-optimal <ref> [113, 87] </ref>. But these are worst-case guarantees. <p> No precedence. If the precedence graph is empty, (i.e., jobs are independent) then a number of approaches are known to get approximation bounds <ref> [57, 87, 113, 112, 111] </ref>. In the database scenario, it is possible to collapse each query, consisting of several jobs with a precedence relation among them, into one job, i.e., allocate maximum resources over all the jobs in the query, and then apply the results for independent jobs [119].
Reference: [114] <author> R. v Hanxleden and K. Kennedy. </author> <title> Given-Take|a balanced code placement framework. </title> <booktitle> In Programming Language Design and Implementation (PLDI), </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year> <journal> ACM SIGPLAN. </journal>
Reference-contexts: However, our algorithm permits additional techniques like Give-n-Take to be used to overlap latency with computation at the sender <ref> [114] </ref>. 2.4 Compiler algorithms In this section we describe our algorithm for placing communication code. This analysis is done after the compiler has performed transformations like loop distribution and loop interchange to increase opportunities for moving communication outside loops [68].
Reference: [115] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In International Symposium on Computer Architecture (ISCA), </booktitle> <address> Australia, </address> <month> May </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: Most current generation CPU's are well beyond the 100 MFLOPS mark, and are consistently out-pacing network performance. Most vendors support UNIX-like environments on each processor, in particular with multiprogramming and virtual memory. This means that low-overhead message passing implementations like Active Messages <ref> [115] </ref>, which work best with gang-scheduled time slicing, user-level access to the network interface, and register-based data transfer, are not the best choice. <p> If the CPU-network overlap can be exploited more effectively in future generation machines, the compiler could obtain better performance by considering the trade-offs between enhancing the overlap and reducing the number of 5 Note that we use MPI on both machines, not Active Messages <ref> [115] </ref>. 33 messages and buffer contention. In particular, the simple subset elimination step (x2.4.5) would have to be dropped, as it could easily degrade the quality of the solution. In fact, the general problem becomes intractable when all of these conflicting optimizations are considered.
Reference: [116] <author> C.-P. Wen and K. Yelick. </author> <title> Parallel timing simulation on a distributed memory multiprocessor. </title> <booktitle> In International Conference on CAD, </booktitle> <address> Santa Clara, CA, </address> <month> November </month> <year> 1993. </year> <note> An earlier version appeared as UCB Technical Report CSD-93-723. </note>
Reference-contexts: In computing eigenvalues of nonsymmetric matrices, the sign function algorithm does divide and conquer with matrix factorizations at each division [12]. In timing-level circuit simulation there is parallelism between separate subcircuits and parallelism within the model evaluation of each subcircuit <ref> [116] </ref>. In sparse matrix factorization, multi-frontal algorithms expose task parallelism between separate dense sub-matrices and data parallelism within those dense matrices [85].
Reference: [117] <author> R. C. Whaley. </author> <title> Basic linear algebra communication subprograms: Analysis and implementation across multiple parallel architectures. </title> <note> Technical Report LAPACK working note 73, </note> <institution> University of Tennessee, Knoxville, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Estimates are in part from <ref> [107, 117, 88, 7] </ref>; the network software are described in these references. to be very large on most distributed memory machines, although reasonable bandwidth can be supported for sufficiently large messages [109, 106]. See Table 2.1 for some idea of the CPU and communication speeds of current multiprocessors. <p> The curves were fit for 2 P 500 and 100 n = N 1=2 10000. An estimate of memory per processor in megabytes is given in the column marked M/P. Estimates for ff and fi are in part from <ref> [107, 117, 88, 7] </ref>. of processors, network latency, and network bandwidth. Using these given functions, we first estimate the parallel running time r (N; P ) for a given machine and problem, then fit Equation (3.1) to it using Matlab.
Reference: [118] <author> R. P. Wilson and M. S. Lam. </author> <title> Efficient context-sensitive pointer analysis for C programs. </title> <booktitle> In Programming Language Design and Implementation (PLDI), </booktitle> <address> La Jolla. CA, </address> <month> June </month> <year> 1995. </year> <journal> ACM SIGPLAN. </journal>
Reference-contexts: For popular languages like C or C++, detecting parallelism in presence of aliases, pointers, and linked data structures is very difficult. Several projects such as Suif, Olden, and Jade, as well as commutativity and synchronization analyses have made initial progress in these directions <ref> [24, 118, 97] </ref>. The second problem is for the compiler to explicitly orchestrate communication between different register files or functional units.
Reference: [119] <author> J. Wolf, J. Turek, M. Chen, and P. Yu. </author> <title> The optimal scheduling of multiple queries in a parallel database machine. </title> <type> Technical Report RC 18595 (81362) 12/17/92, </type> <institution> IBM, </institution> <year> 1992. </year>
Reference-contexts: In the database scenario, it is possible to collapse each query, consisting of several jobs with a precedence relation among them, into one job, i.e., allocate maximum resources over all the jobs in the query, and then apply the results for independent jobs <ref> [119] </ref>. This has a serious drawback in that some obvious, critical co-scheduling may be lost. For example, a CPU-bound job from one query and the IO-bound job of another can be co-scheduled and it is highly desirable to do so [73]; this cannot be done after collapsing the query. <p> In x4.3 and x4.4 we give the makespan lower and upper bounds. In x4.5 we show how to extend the makespan algorithm to a WACT algorithm. In x4.6 we pose some unresolved problems. 4.2 Motivation 4.2.1 Databases Query scheduling in parallel databases is a topic of active research <ref> [23, 90, 119, 66, 59] </ref>. Queries arrive from many users to a front-end manager process.
Reference: [120] <author> M. Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year>
Reference-contexts: In fact, the IBM HPF scalarizer [68] will translate the F90-style source to the scalarized form in the second column. If loop fusion can be performed before this analysis, as in this case, the problem can be avoided. But this is not always possible <ref> [120, x 9.2] </ref>. Thus, limited communication analysis at a single loop-nest level or a rigid placement policy may not work well. <p> This analysis is done after the compiler has performed transformations like loop distribution and loop interchange to increase opportunities for moving communication outside loops [68]. Wolfe provides an excellent overview of the compiler terminology we use <ref> [120] </ref>. The steps 17 of our algorithm are described below and shown in Figure 2.5. 1. For each array expression on the right hand side of a statement that may need communication, identify the earliest (x2.4.3) and latest (x2.4.2) safe position to place that communication. <p> Observe that it is never necessary to place communication for u deeper than at CNL (d; u). Given d and u, we can compute all possible direction vectors (each is a CNL (d; u)-dimensional vector) <ref> [120] </ref>. These vectors are used in the routine IsArrayDep, shown in Figure 2.8 (d) on page 21. Let DepLevel (d; u) = maxf` : IsArrayDep (d; u; `)g, the deepest level at which there is a loop-carried dependency between u and d.
Reference: [121] <author> M. Wolfe and U. Banerjee. </author> <title> Data dependence and its application to parallel processing. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(2) </volume> <pages> 137-178, </pages> <month> Apr. </month> <year> 1987. </year>
Reference-contexts: One of our key innovations is to exploit the static single assignment (SSA) information [39, 35] already computed in an earlier phase by pHPF, refined by array dependence-testing <ref> [121] </ref>. In contrast, previous proposals for such analysis typically use a bidirectional dataflow approach with array section descriptors and/or bit-vectors [69]. 2.
Reference: [122] <author> I.-C. Wu and H. T. Kung. </author> <title> Communication complexity for parallel divide-and-conquer. </title> <booktitle> In Foundations of Computer Science (FOCS), </booktitle> <pages> pages 151-162, </pages> <year> 1991. </year>
Reference-contexts: In work stealing, the graph is expanded depth-first locally in each processor, and idle processors steal jobs nearest to the root <ref> [122, 19] </ref>. <p> The basic idea, dating back to Kung and Wu's work, is for each processor to follow depth-first tree expansion order locally, while an idle processor that "steals" work from others gets a shallowest unexpanded node <ref> [122] </ref>. Work stealing is good at reducing communication, since many jobs run on the processors where they were generated. They do not handle more general cases like branch and bound where the total work to find the solution may depend on the schedule.
Reference: [123] <author> H. Zima, H. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semiautomatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation and generating the communication necessary to fetch values of non-local data referenced by a processor <ref> [72, 123, 20, 5, 21, 68] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data, for the following reasons. It is getting increasingly cost-effective to build multiprocessors from commodity hardware components and system software. <p> This follows from standard communication analysis: communication is placed just before the outermost loop in which there is no true dependence on u, and is placed just before the statement containing u if no such loop exists <ref> [123, 72, 68] </ref>. Given a use u, let d range over the reaching regular defs of u. (Reaching defs are defined in Cytron et al [39, 35].) Consider some d. Observe that it is never necessary to place communication for u deeper than at CNL (d; u). <p> The runtime library provides a high-level interface through which the compiler specifies the data being communicated in the form of array sections, and the runtime system takes care of packing and unpacking of data. For NNC, data is received in overlap regions <ref> [123] </ref> surrounding the local portion of the arrays. <p> Particular technical references and their relation with this thesis are given in specific chapters. 6.1 Communication scheduling The results in Chapter 2 follow (and generalize) a large number of preliminary results in communication optimization, such as message vectorizing, coalescing, and redundancy elimination <ref> [72, 123, 20, 5, 21, 68] </ref>. Message vectorization is the technique of hoisting communication of array elements or sections out of loops to produce a single large message. Typically this works locally on single loop-nests [123, 72, 67, 83, 68]. <p> Message vectorization is the technique of hoisting communication of array elements or sections out of loops to produce a single large message. Typically this works locally on single loop-nests <ref> [123, 72, 67, 83, 68] </ref>. As compilers were applied to more complex code, eliminating redundant communication beyond single loop-nests and even across procedures became essential.
References-found: 123

