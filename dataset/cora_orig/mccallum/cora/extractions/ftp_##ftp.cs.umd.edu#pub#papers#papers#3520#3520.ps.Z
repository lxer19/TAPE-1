URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3520/3520.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: ftrishmcc,aporter,harveyg@cs.umd.edu votta@research.att.com  
Title: An Experiment to Assess Cost-Benefits of Inspection Meetings and their Alternatives  
Author: Patricia McCarthy, Adam Porter Harvey Siy Lawrence Votta 
Address: College Park, Maryland 20742 Naperville, Illinois 60566  
Affiliation: Computer Science Department Software Production Research Department University of Maryland AT&T Bell Laboratories  
Abstract: We hypothesize that inspection meetings are far less effective than many people believe and that meet-ingless inspections are equally effective. However, two of our previous industrial case studies contradict each other on this issue. Therefore, we are conducting a multi-trial, controlled experiment to assess the benefits of inspection meetings and to evaluate alternative procedures. The experiment manipulates four independent variables: (1) the inspection method used (two methods involve meetings, one method does not), (2) the requirements specification to be inspected (there are two), (3) the inspection round (each team participates in two inspections), and (4) the presentation order (either specification can be inspected first). For each experiment we measure 3 dependent variables: (1) the individual fault detection rate, (2) the team fault detection rate, and (3) the percentage of faults originally discovered after the initial inspection phase (during which phase reviewers individually analyze the document). So far we have completed one run of the experiment with 21 graduate students in the computer science at the University of Maryland as subjects, but we do not yet have enough data points to draw definite conclusions. Rather than presenting preliminary conclusions, this article (1) describes the experiment's design and the provocative hypotheses we are evaluating, (2) summarizes our observations from the experiment's initial run, and (3) discusses how we are using these observations to verify our data collection instruments and to refine future experimental runs.
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> IEEE Guide to Software Requirements Specifications. Soft. Eng. Tech. Comm. of the IEEE Computer Society, </institution> <year> 1984. </year> <note> IEEE Std 830-1984. </note>
Reference-contexts: The references for these lectures were Fagan [4], Parnas [10], and the IEEE Guide to Software Requirements Specifications <ref> [1] </ref>.
Reference: [2] <author> Mark A. Ardis. </author> <title> Lessons from using basic lotos. </title> <booktitle> In Sixteenth International Conference on Software Engineering, </booktitle> <pages> pages 5-14, </pages> <address> Sorrento, Italy, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Our experimental specifications are atypical of industrial SRS in two ways. First, most of the experimental specification is written in a formal requirements notation (see Section 2.2). Although several groups at AT&T and elsewhere are experimenting with formal notations <ref> [2, 5] </ref>, it is not the industry's standard practice. Second, the specifications used are considerably shorter than industrial specifications. 3. The inspection process in our experimental design may not be representative of software development practice.
Reference: [3] <author> G. E. P. Box, W. G. Hunter, and J. S. Hunter. </author> <title> Statistics for Experimenters. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: The second step is to evaluate the combined effect of the variables shown to be significant in the initial analysis. Both analyses use standard analysis of variance methods (see <ref> [3] </ref>, pp. 165ff and 210ff or [6]).
Reference: [4] <author> M. E. Fagan. </author> <title> Design and code inspections to reduce errors in program development. </title> <journal> IBM Systems Journal, </journal> <volume> 15(3) </volume> <pages> 182-211, </pages> <year> 1976. </year>
Reference-contexts: The references for these lectures were Fagan <ref> [4] </ref>, Parnas [10], and the IEEE Guide to Software Requirements Specifications [1].
Reference: [5] <author> S. Gerhart, D. Craigen, and T. Ralston. </author> <title> Experience with formal methods in critical systems. </title> <journal> IEEE Software, </journal> <volume> 11(1) </volume> <pages> 21-28, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Our experimental specifications are atypical of industrial SRS in two ways. First, most of the experimental specification is written in a formal requirements notation (see Section 2.2). Although several groups at AT&T and elsewhere are experimenting with formal notations <ref> [2, 5] </ref>, it is not the industry's standard practice. Second, the specifications used are considerably shorter than industrial specifications. 3. The inspection process in our experimental design may not be representative of software development practice.
Reference: [6] <author> R. M. </author> <title> Heiberger. Computation for the Analysis of Designed Experiments. </title> <publisher> Wiley & Sons, </publisher> <address> New York, New York, </address> <year> 1989. </year>
Reference-contexts: The second step is to evaluate the combined effect of the variables shown to be significant in the initial analysis. Both analyses use standard analysis of variance methods (see [3], pp. 165ff and 210ff or <ref> [6] </ref>).
Reference: [7] <author> Kathryn L. Heninger. </author> <title> Specifying Software Requirements for Complex Systems: New Techniques and their Application. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-6(1):2-13, </volume> <month> January </month> <year> 1980. </year>
Reference-contexts: Each specification has four sections: Overview, Specific Functional Requirements, External Interfaces, and a Glossary. The overview is written in natural language, while the other three sections are specified using the SCR tabular requirements notation <ref> [7] </ref>. For this experiment, all three documents were adapted to adhere to the IEEE suggested format [8]. All faults present in these SRS appear in the original documents or were generated during adaptation; no faults were intentionally seeded into the document.
Reference: [8] <institution> IEEE Standard for software reviews and audits. Soft. Eng. Tech. Comm. of the IEEE Computer Society, </institution> <year> 1989. </year> <note> IEEE Std 1028-1988. </note>
Reference-contexts: The overview is written in natural language, while the other three sections are specified using the SCR tabular requirements notation [7]. For this experiment, all three documents were adapted to adhere to the IEEE suggested format <ref> [8] </ref>. All faults present in these SRS appear in the original documents or were generated during adaptation; no faults were intentionally seeded into the document. The authors discovered 42 faults in the WLMS SRS and 26 in the CRUISE SRS.
Reference: [9] <author> J. Kirby. </author> <title> Example NRL/SCR software requirements for an automobile cruise control and monitoring system. </title> <type> Technical Report TR-87-07, </type> <institution> Wang Institute of Graduate Studies, </institution> <month> July </month> <year> 1984. </year>
Reference-contexts: Water Level Monitoring System [13] describes the functional and performance requirements of a system for monitoring the operation of a steam generating system (24 pages). Automobile Cruise Control System <ref> [9] </ref> describes the functional and performance requirements for an automobile cruise control system (31 pages). 2.2.2 Fault Reporting Forms We also developed a Fault Report Form. Whenever a potential fault was discovered during either the fault detection or the collection activities an entry was made on this form.
Reference: [10] <author> Dave L. Parnas and David M. Weiss. </author> <title> Active design reviews: </title> <booktitle> principles and practices. In Proceedings of the 8th International Conference on Software Engineering, </booktitle> <pages> pages 215-222, </pages> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: First each member of the review team analyzes the document. Later the team meets to inspect the document as a group. Finally, the author repairs the faults that have been discovered. Some researchers, notably Parnas and Weiss <ref> [10] </ref>, have questioned the effectiveness of this approach. <p> The references for these lectures were Fagan [4], Parnas <ref> [10] </ref>, and the IEEE Guide to Software Requirements Specifications [1].
Reference: [11] <author> Adam Porter, Harvey Siy, Carol A. Toman, and Lawrence G. Votta. </author> <title> An experiment to assess the cost-benefits of code inspections in large scale software development. </title> <booktitle> In ACM SIGSOFT Software Engineering Notes, </booktitle> <volume> volume 20, </volume> <month> October </month> <year> 1995. </year>
Reference-contexts: This would mean that if 20 faults were discovered during the inspection, 19 were already known before the meeting ever started! This result was striking, but a later data seemed to contradict it. Porter, Siy, Toman and Votta <ref> [11] </ref> conducted another study, also at AT&T, involving &gt; 100 code inspections.
Reference: [12] <author> Adam Porter, Lawrence G. Votta, and Victor Basili. </author> <title> Comparing detection methods for software requirement inspections: A replicated experim ent. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 21(6) </volume> <pages> 563-575, </pages> <month> June </month> <year> 1995. </year>
Reference: [13] <author> J. vanSchouwen. </author> <title> The A-7 requirements model: Re-examination for real-time systems and an application to monitoring systems. </title> <type> Technical Report TR-90-276, </type> <institution> Queen's University, Kingston, </institution> <address> Ontario, Canada, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: The authors did not inspect the ELEVATOR SRS since it was used only for training exercises. Elevator Control System [15] describes the functional and performance requirements of a system for monitoring the operation of a bank of elevators (16 pages). Water Level Monitoring System <ref> [13] </ref> describes the functional and performance requirements of a system for monitoring the operation of a steam generating system (24 pages).
Reference: [14] <author> Lawrence G. Votta. </author> <booktitle> Does every inspection need a meeting? In Proceedings of ACM SIGSOFT '93 Symposium on Foundations of Software Engineering. Association for Computing Machinery, </booktitle> <month> December </month> <year> 1993. </year>
Reference-contexts: Thus they believe that a group of reviewers is likely to be more effective working together than working separately. Votta <ref> [14] </ref> evaluated this argument in a case study involving 20 design inspections at AT&T. To quantify the usefulness of inspection meetings, he determined the proportion of faults found during the inspection that were originally discovered at the meeting (the meeting gain rate).
Reference: [15] <author> William G. Wood. </author> <title> Temporal logic case study. </title> <type> Technical Report CMU/SEI-89-TR-24, </type> <institution> Software Engineering Institute, </institution> <address> Pittsburgh, PA, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: The authors discovered 42 faults in the WLMS SRS and 26 in the CRUISE SRS. The authors did not inspect the ELEVATOR SRS since it was used only for training exercises. Elevator Control System <ref> [15] </ref> describes the functional and performance requirements of a system for monitoring the operation of a bank of elevators (16 pages). Water Level Monitoring System [13] describes the functional and performance requirements of a system for monitoring the operation of a steam generating system (24 pages).
References-found: 15

