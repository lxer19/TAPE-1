URL: http://www.cs.cmu.edu/~jab/pubs/boyan.laser.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/user/jab/web/cv/cv100.html
Root-URL: 
Email: fjab,dayne,thorsteng@cs.cmu.edu  
Title: A Machine Learning Architecture for Optimizing Web Search Engines  
Author: Justin Boyan, Dayne Freitag, and Thorsten Joachims 
Date: May 10, 1996  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Indexing systems for the World Wide Web, such as Lycos and Alta Vista, play an essential role in making the Web useful and usable. These systems are based on Information Retrieval methods for indexing plain text documents, but also include heuristics for adjusting their document rankings based on the special HTML structure of Web documents. In this paper, we describe a wide range of such heuristics|including a novel one inspired by reinforcement learning techniques for propagating rewards through a graph|which can be used to affect a search engine's rankings. We then demonstrate a system which learns to combine these heuristics automatically, based on feedback collected unintrusively from users, resulting in much improved rankings. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bartell, B.; Cottrell, G.; and Belew, R. </author> <year> 1994. </year> <title> Optimizing parameters in a ranked retrieval system using multi-query relevance feedback. </title> <booktitle> In Proceedings of Symposium on Document Analysis and Information Retrieval (SDAIR). </booktitle>
Reference-contexts: Other researchers have focused on retrieval using hypertext structure without making use of the internal structure of documents (Savoy 1991; Croft & Turtle 1993). Automatic parameter optimization was previously proposed by (Fuhr et al. 1994) as well as <ref> (Bartell, Cottrell, & Belew 1994) </ref>. Both approaches differ from LASER's in that they use real relevance feedback data.
Reference: <author> Barto, A. G.; Bradtke, S. J.; and Singh, S. P. </author> <year> 1995. </year> <title> Learning to act using real-time dynamic programming. </title> <booktitle> Artificial Intelligence 72(1) </booktitle> <pages> 81-138. </pages>
Reference-contexts: For retrieval it might be useful not only to look at a document in isolation, but also to take its neighboring documents into account. The approach we took is motivated by an analogy to reinforcement learning as studied in artificial intelligence <ref> (Barto, Bradtke, & Singh 1995) </ref>. Imagine that an agent searching for information on the Web can move from page to page only by following hyperlinks. Whenever the agent finds information relevant to its search goal, it gets a certain amount of reward.
Reference: <author> Bellman, R. </author> <year> 1957. </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press. </publisher>
Reference-contexts: In reinforcement-learning terms, these values represent the "immediate reward" associated with each page. Then, LASER propagates the rewards back through the hypertext graph, discounting them at each step, by value iteration <ref> (Bellman 1957) </ref>: rsv t+1 (q; d) = rsv 0 (q; d) + fl d 0 2links (d) rsv t (q; d 0 ) jlinks (d)j - (1) fl is a discount factor that controls the influence of neighboring pages, and links (d) is the set of pages referenced by hyperlinks in
Reference: <author> Croft, B., and Turtle, H. </author> <year> 1993. </year> <title> Retrieval strategies for hypertext. </title> <booktitle> Information Processing and Management 29(3) </booktitle> <pages> 313-324. </pages>
Reference: <author> Fuhr, N.; Pfeifer, U.; Bremkamp, C.; Pollmann, M.; and Buckley, C. </author> <year> 1994. </year> <title> Probabilistic learning approaches for indexing and retrieval with the TREC-2 collection. </title> <booktitle> In The Second Text Retrieval Conference (TREC-2). </booktitle> <institution> National Institute of Standards and Technology. </institution>
Reference-contexts: These retrieval engines make use of the internal structure of documents, but they do not incorporate hyperlinks. Other researchers have focused on retrieval using hypertext structure without making use of the internal structure of documents (Savoy 1991; Croft & Turtle 1993). Automatic parameter optimization was previously proposed by <ref> (Fuhr et al. 1994) </ref> as well as (Bartell, Cottrell, & Belew 1994). Both approaches differ from LASER's in that they use real relevance feedback data.
Reference: <author> Mauldin, M., and Leavitt, J. </author> <year> 1994. </year> <title> Web agent related research at the Center for Machine Translation. </title> <booktitle> In Proceedings of the ACM Special Interest Group on Networked Information Discovery and Retrieval (SIGNIDR-94). </booktitle>
Reference-contexts: 1 Introduction Lycos <ref> (Mauldin & Leavitt 1994) </ref>, Alta Vista, and similar Web search engines have become essential as tools for locating information on the ever-growing World Wide Web. Underlying these systems are statistical methods for indexing plain text documents. <p> If this trend continues, we will be satisfied that we have successfully learned a new and better ranking scheme. 5 Related Work Many retrieval engines have been developed to index World Wide Web pages. Descriptions of some can be found in <ref> (Mauldin & Leavitt 1994) </ref> and (Pinker-ton 1994). These retrieval engines make use of the internal structure of documents, but they do not incorporate hyperlinks. Other researchers have focused on retrieval using hypertext structure without making use of the internal structure of documents (Savoy 1991; Croft & Turtle 1993).
Reference: <author> Moore, A., and Schneider, J. </author> <year> 1996. </year> <title> Memory-based stochastic optimization. </title> <editor> In Touretzky, D. S.; Mozer, M. C.; and Hasselmo, M. E., eds., </editor> <booktitle> Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: These evaluations could be sped up if Q 0 were subsampled randomly on each call to Perf ; however, this adds noise to the evaluation. We are investigating the use of stochastic optimization techniques, which are designed for optimization of just this type of noisy and expensive objective function <ref> (Moore & Schneider 1996) </ref>. 4 Empirical Results LASER has been in operation since February 14, 1996. The system currently indexes a document database consisting of about 30; 000 pages served by the CMU Computer Science Department web server, www.cs.cmu.edu.
Reference: <author> Morgan, N., and Bourlard, H. </author> <year> 1990. </year> <title> Generalization and parameter estimation in feedforward nets: Some experiments. </title> <editor> In Touretsky, D. S., ed., </editor> <booktitle> Neural Information Processing Systems 2, </booktitle> <pages> 630-637. </pages> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: To guard against overfitting, we use early stopping with a holdout set, as is frequently done in neural network optimization <ref> (Morgan & Bourlard 1990) </ref>, as follows: 1. We consider the sequence of parameter vectors which are the "best so far" during the simulated annealing run. These produce a monotonically decreasing learning curve (see, for example, Figure 2). 2.
Reference: <author> Pinkerton, B. </author> <year> 1994. </year> <title> Finding what people want: Experiences with the WebCrawler. </title> <note> In Second International WWW Conference. </note>
Reference: <author> Press, W.; Teukolsky, S.; Vetterling, W.; and Flan-nery, B. </author> <year> 1992. </year> <title> Numerical Recipes in C: </title> <booktitle> The Art of Scientific Computing. </booktitle> <address> Cambridge University Press, </address> <note> second edition. </note>
Reference-contexts: Thus, we chose to apply a global optimization algorithm, simulated annealing. In particular, we applied the "modified downhill simplex" variant of simulated annealing, as described in <ref> (Press et al. 1992) </ref>. Because we calculate Perf from only a fixed sub-sample of queries, aggressive minimization introduces the danger of overfitting; that is, our converged parameter vector ~p may exploit particular idiosyncracies of the subsample at the expense of generalization over the whole space.
Reference: <author> Salton, G. </author> <year> 1991. </year> <title> Developments in automatic text retrieval. </title> <booktitle> Science 253 </booktitle> <pages> 974-979. </pages>
Reference-contexts: From this page of search results, the user can proceed to any of the abstracted pages or enter a new query. LASER's retrieval function is based on the TFIDF vector space retrieval model <ref> (Salton 1991) </ref>. In this model documents and queries are represented as vectors of real numbers, one for each word; documents and queries with similar contents are transformed into similar vectors. LASER uses an inner product similarity metric to compare documents with a query.
Reference: <author> Savoy, J. </author> <year> 1991. </year> <title> Spreading activation in hypertext systems. </title> <type> Technical report, </type> <institution> Universite de Montreal. van Rijsbergen, </institution> <address> C. </address> <year> 1979. </year> <note> Information Retrieval. Lon-don: Butterworths, second edition. 8 </note>
References-found: 12

