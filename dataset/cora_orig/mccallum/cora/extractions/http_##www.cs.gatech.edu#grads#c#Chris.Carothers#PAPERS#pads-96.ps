URL: http://www.cs.gatech.edu/grads/c/Chris.Carothers/PAPERS/pads-96.ps
Refering-URL: http://www.cs.gatech.edu/grads/c/Chris.Carothers/homepage.html
Root-URL: 
Email: chrisc,fujimoto@cc.gatech.edu  
Title: Background Execution of Time Warp Programs 1  
Author: Christopher Carothers and Richard M. Fujimoto 
Note: 1 This work was supported by U.S. Army Contract DASG60-95-C-0103 funded by the Ballistic Missile Defense Organization, and NSF Grant Number CDA-9501637.  
Address: Atlanta, GA 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. Briner, Jr. </author> <title> Fast parallel simulation of digital systems. </title> <booktitle> In Advances in Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 71-77. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> January </month> <year> 1991. </year>
Reference-contexts: Time Warp has demonstrated some success in speeding up a variety of simulation applications, including combat models [17], communication networks [14], wireless networks [4], queuing networks [6], and digital logic circuits <ref> [1] </ref>, among others. We assume that the reader is familiar with the Time Warp mechanism described in [10].
Reference: [2] <author> R. Brown. </author> <title> Calendar queues: A fast 0(1) priority queue implementation for the simulation event set problem. </title> <journal> Communications of the ACM, </journal> <volume> 31(10) </volume> <pages> 1220-1227, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: This information in-conjunction with a lower bound on all transit messages between consistent cuts is used to approximate GVT. The last significant change was adding support for LP clustering and moving those clusters among the different workstations. To support clustering, a separate calendar queue <ref> [2] </ref> is used to store the pending set of events for each cluster of LPs. Before processing an event, the Time Warp kernel selects the cluster with the smallest time stamped event. Then that smallest time stamped event is dequeue from it's calendar queue and processed.
Reference: [3] <author> C. Burdorf and J. Marti. </author> <title> Load Balancing Strategies for Time Warp on Multi-User Workstations. </title> <journal> The Computer Journal, </journal> <volume> 36(2) </volume> <pages> 168-176, </pages> <year> 1993. </year>
Reference-contexts: The approach proposed here does utilizes the ideas of not considering rolled back computation in deriving load balancing metrics, and workload allocation based on the rate of simulated time advance in developing an approach for background execution. Burdorf and Marti <ref> [3] </ref> propose an approach to periodically compute the average and standard deviation of all the LP local clocks in the system.
Reference: [4] <institution> Reference Deleted. </institution>
Reference-contexts: Time Warp is a well known synchronization protocol that detects out-of-order executions of events as they occur, and recovers using a rollback mechanism [10]. Time Warp has demonstrated some success in speeding up a variety of simulation applications, including combat models [17], communication networks [14], wireless networks <ref> [4] </ref>, queuing networks [6], and digital logic circuits [1], among others. We assume that the reader is familiar with the Time Warp mechanism described in [10]. <p> Controlled experiments were performed with artificial external application processes manually introduced before or during the execution of the Time Warp program. The Time Warp application used in these experiments is a simulation of a wireless personal communication services (PCS) network <ref> [4] </ref>. A PCS network [5] provides wireless communication services for nomadic users. The service area of a PCS network is populated with 13 a set of geographically distributed transmitters/receivers called radio ports. <p> If a channel is available, the Call reconnects and continues without interruption. In order to avoid sharing state between LPs, we realized Cells as LPs but Calls/Portables as timestamped messages that travel among the Cell LPs <ref> [4] </ref>. The PCS simulation was configured with 1024 Cells giving the simulation a total of 1024 LPs. These 1024 LPs are grouped into 64 clusters of 16 LPs each, and mapped to the processors such that the amount of remote communications is minimized. Each processor will initially have 8 clusters.
Reference: [5] <author> Cox, D. C. </author> <title> Personal Communications A Viewpoint. </title> <journal> IEEE Communications Magazine, </journal> <volume> 128(11), </volume> <year> 1990. </year>
Reference-contexts: Controlled experiments were performed with artificial external application processes manually introduced before or during the execution of the Time Warp program. The Time Warp application used in these experiments is a simulation of a wireless personal communication services (PCS) network [4]. A PCS network <ref> [5] </ref> provides wireless communication services for nomadic users. The service area of a PCS network is populated with 13 a set of geographically distributed transmitters/receivers called radio ports.
Reference: [6] <institution> Reference Deleted. </institution>
Reference-contexts: Time Warp has demonstrated some success in speeding up a variety of simulation applications, including combat models [17], communication networks [14], wireless networks [4], queuing networks <ref> [6] </ref>, and digital logic circuits [1], among others. We assume that the reader is familiar with the Time Warp mechanism described in [10].
Reference: [7] <institution> Reference Deleted. </institution>
Reference-contexts: Application messages or events are marshalled to the Time Warp kernel (s) that are executing on that local workstation via shared memory. Another change was the piggy-backing of Mattern's GVT algorithm [13] on top of the existing shared memory GVT algorithm <ref> [7] </ref>. In this arrangement, Mattern's algorithm forces the shared memory algorithm to be executed on each workstation to determine its local virtual time. This information in-conjunction with a lower bound on all transit messages between consistent cuts is used to approximate GVT.
Reference: [8] <author> D. W. Glazer and C. Tropper. </author> <title> On process migration and load balancing in Time Warp. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(4) </volume> <pages> 318-327, </pages> <month> March </month> <year> 1993. </year>
Reference-contexts: Glazer and Tropper propose allocating virtual time-slices to processes, based on their observed rate of advancing the local simulation clock <ref> [8] </ref>. They present simulation results illustrating this approach yields better performance than the Reiher/Jefferson scheme for certain workloads. To our knowledge, this scheme has not been implemented on an operational Time Warp system.
Reference: [9] <author> A. Goldberg. </author> <title> Virtual time synchronization of replicated processes. </title> <booktitle> In 6 th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 24, </volume> <pages> pages 107-116. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: They present simulation results illustrating this approach yields better performance than the Reiher/Jefferson scheme for certain workloads. To our knowledge, this scheme has not been implemented on an operational Time Warp system. Goldberg describes an interesting approach to the load distribution that replicates bottleneck processes to enable concurrent execution <ref> [9] </ref>. Time Warp is used to maintain consistency among the replicated copies. None of these approaches addresses the question of background execution.
Reference: [10] <author> D. R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Time Warp is a well known synchronization protocol that detects out-of-order executions of events as they occur, and recovers using a rollback mechanism <ref> [10] </ref>. Time Warp has demonstrated some success in speeding up a variety of simulation applications, including combat models [17], communication networks [14], wireless networks [4], queuing networks [6], and digital logic circuits [1], among others. We assume that the reader is familiar with the Time Warp mechanism described in [10]. <p> mechanism <ref> [10] </ref>. Time Warp has demonstrated some success in speeding up a variety of simulation applications, including combat models [17], communication networks [14], wireless networks [4], queuing networks [6], and digital logic circuits [1], among others. We assume that the reader is familiar with the Time Warp mechanism described in [10]. With few exceptions, most research on distributed simulation to date assumes the simulation program is allocated a fixed number of processors when execution begins, and has exclusive access to these processors throughout the lifetime of the simulation.
Reference: [11] <author> M. Litzkow and M. Livny. </author> <title> Experience with the condor distributed batch system. </title> <booktitle> In IEEE Workshop on Experimental Distributed Systems, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: Further, there is a close relationship between load management and the efficiency (e.g., amount of rolled back computation) of the synchronization mechanism, as discussed earlier. These factors necessitate development of new load management techniques specific to Time Warp. Thus, systems such as Condor <ref> [11] </ref> that distribute jobs onto networked workstations as background processes that "soak up" otherwise unused CPU cycles are not sufficient for Time Warp simulations. Dynamic load management of Time Warp programs has been studied by others.
Reference: [12] <author> V. K. Madisetti, D. A. Hardaker, and R. M. Fujimoto. </author> <title> The mimdix operating system for parallel simulation and supercomputing. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18(4) </volume> <pages> 473-483, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: This approach also has the side effect of "cleaning up" overly optimistic computation. In this sense, this approach is not unlike the mechanism described in <ref> [12] </ref> which found such periodic, global rollbacks to be beneficial. Our initial experiments indicate that this mechanism provides a reasonably simple and efficient mechanism for reducing migration overhead.
Reference: [13] <author> F. Mattern. </author> <title> Efficient distributed snapshots and global virtual time algorithms for non-fifo systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18(4) </volume> <pages> 423-434, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Its tasks include the sending and receiving of all GVT, application defined and dynamic load management messages. Application messages or events are marshalled to the Time Warp kernel (s) that are executing on that local workstation via shared memory. Another change was the piggy-backing of Mattern's GVT algorithm <ref> [13] </ref> on top of the existing shared memory GVT algorithm [7]. In this arrangement, Mattern's algorithm forces the shared memory algorithm to be executed on each workstation to determine its local virtual time.
Reference: [14] <author> M. Presley, M. Ebling, F. Wieland, and D. R. Jefferson. </author> <title> Benchmarking the Time Warp Operating System with a computer network simulation. </title> <booktitle> In Proceedings of the 21 SCS Multiconference on Distributed Simulation, </booktitle> <volume> volume 21, </volume> <pages> pages 8-13. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> March </month> <year> 1989. </year>
Reference-contexts: Time Warp is a well known synchronization protocol that detects out-of-order executions of events as they occur, and recovers using a rollback mechanism [10]. Time Warp has demonstrated some success in speeding up a variety of simulation applications, including combat models [17], communication networks <ref> [14] </ref>, wireless networks [4], queuing networks [6], and digital logic circuits [1], among others. We assume that the reader is familiar with the Time Warp mechanism described in [10].
Reference: [15] <author> P. L. Reiher and D. Jefferson. </author> <title> Dynamic load management in the Time Warp Operating System. </title> <journal> Transactions of the Society for Computer Simulation, </journal> <volume> 7(2) </volume> <pages> 91-120, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Dynamic load management of Time Warp programs has been studied by others. Reiher and Jefferson propose a new metric called effective processor utilization which is defined as the fraction of the time during which a processor is executing computations that are eventually committed <ref> [15] </ref>. Based on this metric, they propose a strategy that migrates processes 4 from processors with high effective utilization to those with low utilization. <p> Specifically, each LP maintains a history of state vectors, in case rollback is later required. While phases could be used to address this problem (see <ref> [15] </ref>), this necessitates implementation of a mechanism for rollbacks to span processor boundaries because a rollback may extend beyond the beginning point of a recently created 11 phase. A simpler, though perhaps more radical, solution is used here.
Reference: [16] <author> R. SchlagenHaft, M. Ruhwandl, C. Sporrer, and H. Bauer. </author> <title> Dynamic Load Balancing of a Multi-Cluster Simulation on a Network of Workstations. </title> <booktitle> In 9 th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 24, </volume> <pages> pages 175-180. </pages> <publisher> IEEE, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: Marti and Burdorf observe that this approach will balance the workload in the presence of external computations competing for the same processors. Schlagenhaft et. al. <ref> [16] </ref> propose an approach to balance the load of a VLSI circuit application on a distributed Time Warp simulator in the presence of external workloads. They define an inverse measure of the load, called Virtual Time Progress, which reflects how fast a simulation process continues in virtual time.
Reference: [17] <author> F. Wieland, L. Hawley, A. Feinberg, M. DiLorento, L. Blume, P. Reiher, B. Beckman, P. Hontalas, S. Bellenot, and D. R. Jefferson. </author> <title> Distributed combat simulation and Time Warp: The model and its performance. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> volume 21, </volume> <pages> pages 14-20. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> March </month> <year> 1989. </year> <month> 22 </month>
Reference-contexts: Time Warp is a well known synchronization protocol that detects out-of-order executions of events as they occur, and recovers using a rollback mechanism [10]. Time Warp has demonstrated some success in speeding up a variety of simulation applications, including combat models <ref> [17] </ref>, communication networks [14], wireless networks [4], queuing networks [6], and digital logic circuits [1], among others. We assume that the reader is familiar with the Time Warp mechanism described in [10].
References-found: 17

