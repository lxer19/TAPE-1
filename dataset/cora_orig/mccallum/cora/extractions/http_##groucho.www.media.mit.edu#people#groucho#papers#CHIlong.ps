URL: http://groucho.www.media.mit.edu/people/groucho/papers/CHIlong.ps
Refering-URL: http://groucho.www.media.mit.edu/people/groucho/
Root-URL: http://www.media.mit.edu
Email: nicole.yankelovich@east.sun.com  
Phone: 508-442-0441  
Title: Designing SpeechActs: Issues in Speech User Interfaces  
Author: Nicole Yankelovich, Gina-Anne Levow, Matt Marx 
Keyword: Speech interface design, speech recognition, auditory I/O, discourse, conversational interaction.  
Address: Two Elizabeth Drive Chelmsford, MA, USA 01824  
Affiliation: Sun Microsystems Laboratories  
Abstract: SpeechActs is an experimental conversational speech system. Experience with redesigning the system based on user feedback indicates the importance of adhering to conversational conventions when designing speech interfaces, particularly in the face of speech recognition errors. Study results also suggest that speech-only interfaces should be designed from scratch rather than directly translated from their graphical counterparts. This paper examines a set of challenging issues facing speech interface designers and describes approaches to address some of these challenges. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Clark, Herbert H. </author> <title> Arenas of Language Use . University of Chicago Press, </title> <address> Chicago, IL, </address> <year> 1992. </year>
Reference-contexts: Below is a description of each challenge along with our approach to addressing the challenge. Challenge: Simulating Conversation Herb Clark says that speaking and listening are two parts of a collective activity <ref> [1] </ref>. A major design challenge in Participants Recog. Rates Tasks Completed Female 52% 17 Male 68.5% 20 Developers 75.3% 22 4 creating speech applications, therefore, is to simulate the role of speaker/listener convincingly enough to produce successful communication with the human collaborator .
Reference: 2. <author> Grice, H. P. </author> <title> Logic and Conversation, Syntax and Semantics: Speech Acts, </title> <publisher> Cole & Morgan, </publisher> <editor> editors, </editor> <volume> Volume 3, </volume> <publisher> Academic Press, </publisher> <year> 1975. </year>
Reference-contexts: Lack of persistence is another factor . This makes speech both easy to miss and easy to forget. To compensate for these various problems, we attempted to follow some of the maxims H.P. Grice states as part of his cooperative principle of conversation <ref> [2] </ref>. Grice counsels that contributions should be informative, but no more so than is required. They should also be relevant, brief, and orderly. Because speech is an inherently slow output medium, much of our dialog redesign ef fort focused on being brief.
Reference: 3. <author> Grosz, Barbara, and Candy Sidner. </author> <title> Attention, Intentions, and the Structure of Discourse, </title> <journal> Computational Linguistics, </journal> <volume> Volume 12, No. 3, </volume> <year> 1986. </year>
Reference-contexts: The completion of a subdialog corresponds to a discourse segment pop in the discourse structure terminology described by Grosz & Sidner <ref> [3] </ref>. When the subdialog is closed, the context returns to that preceding the subdialog. For example, the user might read a string of messages and then come across one that requires a response.
Reference: 4. <author> Kamm, Candace. </author> <title> User Interfaces for Voice Applications, Voice Communication Between Humans and Machines, </title> <publisher> National Academy Press, </publisher> <address> Washington, DC, </address> <year> 1994. </year>
Reference-contexts: In the few rare cases where we wanted to be absolutely sure we were able to understand the users input, we used what Candace Kamm calls directive prompts <ref> [4] </ref> instead of using a more conversational style. For instance, after the user has recorded a new mail message, we prompt 6 them to Say cancel, send, or review. Challenge: Recognition errors Ironically, the bane of speech-driven interfaces is the very tool which makes them possible: the speech recognizer . <p> W e verify the utterance implicitly by echoing back part of the command in the answer: Today, at 10:00, you have a meeting with... As Kamm suggests <ref> [4] </ref>, we want verification commensurate with the cost of the action which would be ef fected by the recognized utterance.
Reference: 5. <author> Kitai, Mikia, A. Imamura, and Y. Suzuki. </author> <title> Voice Activated Interaction System Based on HMM-based Speaker-Independent Word Spotting, Proceedings of the Voice I/O Systems Applications Confer ence, </title> <address> Atlanta, GA, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Instead, many simply proceeded with their planned task (e.g., Read the next message.). Sometimes they added yes or no to the beginning of their phrase to acknowledge the prompt. This phenomenon was also observed by researchers at NTT <ref> [5] </ref>. When considered in the context of spoken dialog, this behavior is actually quite natural. As the classic example Do you have the time? illustrates, yes/no questions rarely require yes/no answers.
Reference: 6. <author> Ly, Eric, and Chris Schmandt. Chatter: </author> <title> A Conversational Learning Speech Interface, </title> <booktitle> AAAI Spring Symposium on Intelligent MultiMedia Multi-Modal Systems, </booktitle> <address> Stanford, CA, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Further , conversational interfaces are young, and transferring design principles from other media such as graphics can lead to unusable systems. Despite these problems, we, along with others <ref> [6, 9, 10, 11] </ref>, believe the technology good enough and the promise exciting enough to make experimentation worthwhile. In the SpeechActs project, we seek to identify principles and challenges of conversational interface design and to pinpoint limitations of current technology.
Reference: 7. <author> Martin, Paul and Andrew Kehler . SpeechActs: </author> <title> A Testbed for Continuous Speech Applications, </title> <booktitle> AAAI-94 Workshop on the Integration of Natural Language and Speech Processing, 12th National Conference on AI, </booktitle> <address> Seattle, WA, July 31-August 1, </address> <year> 1994. </year>
Reference-contexts: If, on the other hand, the user wants to fax a 500 page mail message, the system will check to make sure that s what was really meant. Although not its primary purpose, the SpeechActs natural language component, called Swiftus, helps to compensate for minor substitution errors <ref> [7] </ref>. It does so by allowing the application developer to convert phrases meaning the same thing into a canonical form.
Reference: 8. <author> Nielsen, Jakob. </author> <title> The Usability Engineering Life Cycle, </title> <booktitle> IEEE Computer, </booktitle> <month> March </month> <year> 1992. </year>
Reference-contexts: Once we had a working prototype, we conducted a usability study in which we adhered to Jakob Nielsen s formative evaluation philosophy of changing and retesting the interface as soon as usability problems are uncovered <ref> [8] </ref>. As a result, the formative evaluation study involved small groups of users and a substantial amount of iterative redesign. Formative Evaluation Study Design Fourteen users participated in the study. The first two participants were pilot subjects.
Reference: 9. <author> Roe, David, and Jay Wilpon, </author> <title> editors. Voice Communication Between Humans and Machines , National Academy Press, </title> <address> Washington, DC, </address> <year> 1994. </year>
Reference-contexts: Further , conversational interfaces are young, and transferring design principles from other media such as graphics can lead to unusable systems. Despite these problems, we, along with others <ref> [6, 9, 10, 11] </ref>, believe the technology good enough and the promise exciting enough to make experimentation worthwhile. In the SpeechActs project, we seek to identify principles and challenges of conversational interface design and to pinpoint limitations of current technology.
Reference: 10. <author> Schmandt, Chris. </author> <title> Voice Communication with Computers: Conversational Systems, </title> <publisher> Van Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Further , conversational interfaces are young, and transferring design principles from other media such as graphics can lead to unusable systems. Despite these problems, we, along with others <ref> [6, 9, 10, 11] </ref>, believe the technology good enough and the promise exciting enough to make experimentation worthwhile. In the SpeechActs project, we seek to identify principles and challenges of conversational interface design and to pinpoint limitations of current technology. <p> Background noise, especially words spoken by passersby, can be mistaken for the users voice. Finally, out-of-vocabulary utterancesi.e., the user says something not covered by the grammar or the dictionarynecessarily result in errors. Recognition errors can be divided into three categories: rejection, substitution, and insertion <ref> [10] </ref>. A rejection error is said to occur when the recognizer has no hypothesis about what the user said. A substitution error involves the recognizer mistaking the user s utterance for a dif ferent legal utterance, as when send a message is interpreted as seventh message. <p> This way, important messages and appointments will be called out to the user first, eliminating some of the need to glance at the information. Speed and Persistence . Although speech is easy for humans to produce, it is much harder for us to consume <ref> [10] </ref>. The slowness of the speech output, whether it be synthesized or recorded, is one contributing factor . Almost everyone can absorb written information more quickly than verbal information. Lack of persistence is another factor . This makes speech both easy to miss and easy to forget.
Reference: 11. <author> Stifelman, Lisa, Barry Arons, Chris Schmandt, and Eric Hulteen, VoiceNotes: </author> <title> A Speech Interface for a HandHeld Voice Notetaker, </title> <booktitle> ACM INTERCHI 93 Conference Proceedings, </booktitle> <address> Amsterdam, The Nether-lands, </address> <month> April 24-29, </month> <year> 1993. </year>
Reference-contexts: Further , conversational interfaces are young, and transferring design principles from other media such as graphics can lead to unusable systems. Despite these problems, we, along with others <ref> [6, 9, 10, 11] </ref>, believe the technology good enough and the promise exciting enough to make experimentation worthwhile. In the SpeechActs project, we seek to identify principles and challenges of conversational interface design and to pinpoint limitations of current technology. <p> By initiating the conversation and providing some common 8 ground, it seems natural for users to respond by saying, What do I have tomorrow? or What does Paul have today? Ambiguous Silence. Another speech-related problem, also observed by Stifelman <ref> [11] </ref>, is the difficulty users have in interpreting silence. Sometimes silence means that the speech recognizer is working on what they said, but other times, it means that the recognizer simply did not hear the users input. This last problem is perhaps the easiest to overcome.
Reference: 12. <author> Yankelovich, Nicole. </author> <title> Talking vs. Taking: Speech Access to Remote Computers, </title> <booktitle> ACM CHI 94 Conference Companion, </booktitle> <address> Boston, MA, </address> <month> April 24-28, </month> <year> 1994. </year>
Reference-contexts: For example, the user may always switch to a different application, ask for help, or end a session by saying good bye. USER STUDY / ITERATIVE DESIGN Before the SpeechActs software was written, we conducted a survey and a field study <ref> [12] </ref> which served as the basis for the preliminary speech user interface (SUI) design. Once we had a working prototype, we conducted a usability study in which we adhered to Jakob Nielsen s formative evaluation philosophy of changing and retesting the interface as soon as usability problems are uncovered [8].
Reference: 13. <author> Yankelovich, Nicole and Eric Baatz. SpeechActs: </author> <title> A Framework for Building Speech Applications, </title> <booktitle> AVIOS 94 Conference Proceedings, </booktitle> <address> San Jose, CA, </address> <month> September 20-23, </month> <year> 1994. </year>
Reference-contexts: THE SpeechActs SYSTEM SpeechActs is a research prototype that integrates third-party speech recognition and synthesis with telephony , natural language processing capabilities, and other tools for creating speech applications. For an overview of the SpeechActs architecture, see <ref> [13] </ref>. To date, the system includes speech-only interfaces to a number of applications including electronic mail, calendar, weather, and stock quotes.
References-found: 13

