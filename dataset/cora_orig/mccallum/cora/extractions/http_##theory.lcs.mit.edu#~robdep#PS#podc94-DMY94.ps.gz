URL: http://theory.lcs.mit.edu/~robdep/PS/podc94-DMY94.ps.gz
Refering-URL: http://theory.lcs.mit.edu/~robdep/papers.html
Root-URL: 
Title: Time-Optimal Message-Efficient Work Performance in the Presence of Faults  
Author: Roberto De Prisco Alain Mayer Moti Yung 
Address: New York, NY 10027  84081 Baronissi (SA), Italy.  
Affiliation: Dept. of Computer Science, Columbia University,  and Dipartimento di Informatica ed Applicazioni, Universita di Salerno,  
Note: (extended summary)  
Abstract: Performing work in parallel by a multitude of processes in a distributed environment is currently a fast growing area of computer applications (due to its cost effectiveness). Adaptation of such applications to changes in system's parallelism (i.e., the availability of processes) is essential for improved performance and reliability. In this work we consider one aspect of coping with dynamic processes failures in such a setting, namely the following scenario formulated by Dwork, Halpern and Waarts [DHW92]: a system of n synchronous processes that communicate only by sending messages to one another. These processes must perform m independent units of work. Processes may fail by crashing and wait-freeness is required, i.e. that whenever at least one process survives, all m units of work will be performed. We consider the notion of fast algorithms in this setting, yet we are not willing to trade improved time for a high cost in communication. Thus, we require message efficiency as well. We therefore put forth the notion of lexicographic efficiency, that is we consider the following two complexity measures in order: The parallel processor step (or S for short) as introduced by Kanellakis and Shvartsman [KS89] in the context of robust PRAM and the number of messages sent (denoted M ). y Dept. of Computer Science, Columbia University, New York, NY 10027. The research of this author was partly supported by an IBM Graduate Fellowship. Part of this work was performed while the author was at the IBM T.J. Watson Research Center. We present an algorithm which has S = O(m + (f + 1)n) (where f denotes the actual number of failures) and prove that this is optimal (in absolute terms in all fault scenarios). Furthermore, the algorithm has M = O((f + 1)n) and hence is the first message-efficient algorithm with optimal S. This is a step in understanding lexicographic efficiency, and towards solving the open problem in [DHW92] of simultaneously optimizing time and messages. z IBM Research Division T. J. Watson Research Center York-town Heights, NY 10590.
Abstract-found: 1
Intro-found: 1
Reference: [AHW92] <author> S. Amdur, V. Hadzilacos, and S. Weber, </author> <title> On the Message Complexity of Binary Agreement Under Crash Failures. Distrib. </title> <journal> Comput, </journal> <volume> 5:175 - 186, </volume> <year> 1992. </year>
Reference-contexts: Checkpoint: Clearly, the algorithmic challenge is to design a checkpoint which is both fast and message-efficient. In order to achieve this goal we combine recent techniques of message-efficient agreement-protocols (see <ref> [AHW92] </ref> and [HH93]) with the well-established idea of eventual (or early-stopping) agreement ([DRS90]). A similar approach was used by [CT90] to achieve an efficient reliable broadcast protocol. Note however, that these two techniques are somewhat contradictory; especially in our case where eventual agreement has to be used repeatedly.
Reference: [RSA-129] <author> D. Atkins, M. Graff, A. Lenstra, and P. Ley-land, </author> <title> Internet announcement, </title> <month> April, </month> <year> 1994. </year>
Reference-contexts: The last example has, in fact, been used with adaptive parallelism and produced successful factorizations by "farming out" prime bases and polynomial bases (for various independent trials) around a local network and even around the globe (e.g. <ref> [Si90, LM89, RSA-129] </ref>). This paper concentrates on one aspect of the above general approach. It is motivated by the pioneering work of Dwork, Halpern, and Waarts ([DHW92]) on performing work of independent tasks efficiently and in the presence of crash failures (i.e., the case of "downwards adaptation").
Reference: [CMY94] <author> T.D. Chandra, A. Mayer, and M. Yung, </author> <note> Subquadratic-Message Early-Stopping Fail-Stop Agreement. manuscript in preparation. </note>
Reference-contexts: Indeed, in an ongoing work we have found the first early stopping (fail-stop) agreement protocol with sub-quadratic message-complexity. This protocol requires only O (f p n+ n) messages (<ref> [CMY94] </ref>) and can be generalized for a checkpoint using the setting of this work. Hence combining [CMY94] with this paper will yield a time-optimal algorithm with M = O (f p 1; log n)n).
Reference: [CT90] <author> T.D. Chandra and S. Toueg, </author> <title> Time and Message Efficient Reliable Broadcast. </title> <booktitle> Proc. Int. Workshop on Distributed Algorithms 1990, </booktitle> <publisher> LNCS Springer-Verlag, </publisher> <pages> 289-303. </pages>
Reference-contexts: Checkpoint: Clearly, the algorithmic challenge is to design a checkpoint which is both fast and message-efficient. In order to achieve this goal we combine recent techniques of message-efficient agreement-protocols (see [AHW92] and [HH93]) with the well-established idea of eventual (or early-stopping) agreement ([DRS90]). A similar approach was used by <ref> [CT90] </ref> to achieve an efficient reliable broadcast protocol. Note however, that these two techniques are somewhat contradictory; especially in our case where eventual agreement has to be used repeatedly. That is, if the outcome of a checkpoint is only eventual, it will destroy the synchronization for the next checkpoint. <p> Safety: W ` &lt; T ` 3. Progress: T ` &lt; W ` + f ` 3.2.2 Implicit Coordinators S ` and C ` are divided into phases of 5 rounds each. In our solutions there is an implicit coordinator for every phase (called "rotating coordinator paradigm" in <ref> [CT90] </ref>). This coordinator is defined as follows: At the end of each checkpoint C ` the set E ` is globally known (and initially the set E 0 is globally known). <p> We are interested in the number of stages, the available processor step S and the total number of messages sent M . We note that the analysis below ignores the constants involved. The actual constants of the algorithms can be improved by using the pipelining-technique of <ref> [CT90] </ref>. 5.1 Available Processor Step We first show that S ff is bounded by O (m + (f + 1)n)): Lemma 10 If in every stage the number of available processes is less than the number of outstanding work, then the total fraction of S ff expended during the work-part is <p> Acknowledgements: We thank Joe Halpern for helpful discussions and providing [DHW93], Vassos Hadzilacos for pointing us to <ref> [CT90] </ref> and related discussions, David Kaminsky for sending us [Kam94], and Madhu Sudan for help with Theorems 3 and 4.
Reference: [DRS90] <author> D. Dolev, R. Reischuk, and H. R. </author> <title> Strong, Early Stopping in Byzantine Agreement. </title> <journal> Journal of the ACM, </journal> <volume> 37(4):720 - 741, </volume> <year> 1990. </year>
Reference: [DHW92] <author> C. Dwork, J. Halpern, and O. Waarts, </author> <title> Performing Work Efficiently in the Presence of Faults. </title> <booktitle> Proc. 11th ACM Symp. on Principles of Distributed Computing (1992). </booktitle>
Reference-contexts: Note that for algorithms which optimize time (and thus try to use all available processes at each working step) S is essentially the work-measure (W for short) of <ref> [DHW92] </ref> for the work performing stages plus the whatever is used for checkpointing (i.e., accounting of live processes). Since the notion of fast algorithms requires that every live process is used at each step | fast accounting of live processes (fast checkpointing) becomes the algorithmic heart of any solution. <p> At the same time, by integrating message-efficient agreement techniques into the checkpoint, ff consumes only linear (in n) messages per failure (M = O ((f + 1)n)) (improving on the quadratic per failure in <ref> [DHW92] </ref>). We note that the latter is non-trivial, as message-efficiency of a checkpoint requires synchronization which is destroyed by previous early-stopping checkpoints. <p> The analysis reveals a suboptimal upper bound and a matching lower bound of the algorithm's strategy (expressed as an optimal strategy of an adversary whose goal is to fail the processes in such a way as to maximize S). See As in <ref> [DHW92] </ref> our emphasis is the case m &gt;> n. Although our algorithm works correctly for m n, it should be noted that S might then grow as fast as (m 2 ). This compares unfavorably with the shared memory solution of O (m log 2 m) in [KS89].
Reference: [DHW93] <institution> Full version of DHW92 (preprint), </institution> <type> Personal Communication by J. Halpern. </type>
Reference-contexts: In <ref> [DHW93] </ref>, two message-efficient algorithms which are not fast are first given, then a different strategy for an algorithm which is fast is proposed, namely to divide the outstanding work always evenly among the surviving processes. No time analysis of this later strategy is given. <p> The synchronization part for C ` takes (5 (f `1 + f ` + 1)(n 1)) messages. 5 Performance Analysis In this section we analyze the performance of our algorithm and also show an analysis of the strategy of <ref> [DHW93] </ref>. We are interested in the number of stages, the available processor step S and the total number of messages sent M . We note that the analysis below ignores the constants involved. <p> Hence, an interesting open question is whether it is possible to obtain a lower bound on the number of messages to achieve early stopping agreement which will shed light on message-performance of our problem. Acknowledgements: We thank Joe Halpern for helpful discussions and providing <ref> [DHW93] </ref>, Vassos Hadzilacos for pointing us to [CT90] and related discussions, David Kaminsky for sending us [Kam94], and Madhu Sudan for help with Theorems 3 and 4.
Reference: [FBCL91] <author> M.J. Feeley, B.N. Bershad, J.S. Chase and H.M. Levy, </author> <title> Dynamic Node Reconfiguration in a Parallel-Distributed Environment Proc. </title> <booktitle> 3-d ACM Princ. and Practice of Parallel Prog., </booktitle> <pages> 114-121, </pages> <year> 1991. </year>
Reference-contexts: In practice, distributed systems have been designed for such exploitation, e.g. Piranha [Kam94], Condor [LLM88], Amber <ref> [FBCL91] </ref>, and the V-system [TLC85]. A particular type of applications defined as "a set of independent tasks" is highly desirable in an environment of adaptive parallelism. These tasks were coined "hunters" in the above systems (agents of an application which independently look after resources).
Reference: [HH93] <author> V. Hadzilacos and J. Halpern, </author> <title> Message-Optimal Protocols for Byzantine Agreement. </title> <journal> Math. </journal> <note> Systems Theory 26, 41-102 (1993). (Also PODC'91). </note>
Reference-contexts: Checkpoint: Clearly, the algorithmic challenge is to design a checkpoint which is both fast and message-efficient. In order to achieve this goal we combine recent techniques of message-efficient agreement-protocols (see [AHW92] and <ref> [HH93] </ref>) with the well-established idea of eventual (or early-stopping) agreement ([DRS90]). A similar approach was used by [CT90] to achieve an efficient reliable broadcast protocol. Note however, that these two techniques are somewhat contradictory; especially in our case where eventual agreement has to be used repeatedly.
Reference: [Kam94] <author> D.L. Kaminsky, </author> <title> Adaptive Parallelism in Piranha. </title> <type> Ph.D. Thesis (draft) Yale University, </type> <year> 1994. </year>
Reference-contexts: In practice, distributed systems have been designed for such exploitation, e.g. Piranha <ref> [Kam94] </ref>, Condor [LLM88], Amber [FBCL91], and the V-system [TLC85]. A particular type of applications defined as "a set of independent tasks" is highly desirable in an environment of adaptive parallelism. These tasks were coined "hunters" in the above systems (agents of an application which independently look after resources). <p> Acknowledgements: We thank Joe Halpern for helpful discussions and providing [DHW93], Vassos Hadzilacos for pointing us to [CT90] and related discussions, David Kaminsky for sending us <ref> [Kam94] </ref>, and Madhu Sudan for help with Theorems 3 and 4.
Reference: [KS89] <author> P. Kanellakis and A. Shvartsman, </author> <title> Efficient Parallel Algorithms Can Be Made Robust. Distrib. </title> <journal> Comput, </journal> <volume> 5:201 - 219, </volume> <year> 1992. </year> <note> (Also PODC'89). </note>
Reference-contexts: Given our goal, in order to compare fast algorithms we need a meaningful complexity measure. For accounting of time we revert to the available processor step (or S for short) of Kanellakis and Shvartsman <ref> [KS89] </ref> (used for the faulty shared memory (PRAM) which is the area conceived in that fundamental work). Consider an algorithm with n initial processes that terminates in parallel-time t after completing its task on some input data I of size m and in the presence of fail-stop errors F . <p> Although our algorithm works correctly for m n, it should be noted that S might then grow as fast as (m 2 ). This compares unfavorably with the shared memory solution of O (m log 2 m) in <ref> [KS89] </ref>. Our lower bound arguments show that any algorithm (i) must perform one or many checkpoints which necessarily have to involve all processes and (ii) such a checkpoint is as hard as agreement (shown by reducing agreement to checkpointing). <p> Thus S = (m 2 ) is a lower bound for any algorithm in the message-passing model. This makes explicit a basic difference between the PRAM and the message-passing model (as was alluded to in <ref> [KS89] </ref>). Organization: Section 2 presents an overview of the algorithm and its techniques, and Section 3 gives the algorithm.
Reference: [LM89] <author> A.K. Lenstra and M.S. Manasse, </author> <title> Factoring by electronic mail. </title> <booktitle> Proc. Eurocrypt 89, </booktitle> <publisher> LNCS 434, Springer Verlag, </publisher> <pages> pp. 355-371, </pages> <year> 1990. </year>
Reference-contexts: The last example has, in fact, been used with adaptive parallelism and produced successful factorizations by "farming out" prime bases and polynomial bases (for various independent trials) around a local network and even around the globe (e.g. <ref> [Si90, LM89, RSA-129] </ref>). This paper concentrates on one aspect of the above general approach. It is motivated by the pioneering work of Dwork, Halpern, and Waarts ([DHW92]) on performing work of independent tasks efficiently and in the presence of crash failures (i.e., the case of "downwards adaptation").
Reference: [LLM88] <author> M. Litzkow, M. Livny and M. </author> <title> Mutka, Condor-a Hunter of Idle Workstations. </title> <booktitle> Proc. 8th Int. Symp. on Distributed Computing Systems. </booktitle> <year> 1988 </year>
Reference-contexts: In practice, distributed systems have been designed for such exploitation, e.g. Piranha [Kam94], Condor <ref> [LLM88] </ref>, Amber [FBCL91], and the V-system [TLC85]. A particular type of applications defined as "a set of independent tasks" is highly desirable in an environment of adaptive parallelism. These tasks were coined "hunters" in the above systems (agents of an application which independently look after resources).
Reference: [Si90] <author> R. Silverman, </author> <title> Massively Distributed Computing and Factoring Large Integers. </title> <journal> Comm. of the ACM, </journal> <volume> 34(2):95 - 103, </volume> <year> 1991. </year>
Reference-contexts: The last example has, in fact, been used with adaptive parallelism and produced successful factorizations by "farming out" prime bases and polynomial bases (for various independent trials) around a local network and even around the globe (e.g. <ref> [Si90, LM89, RSA-129] </ref>). This paper concentrates on one aspect of the above general approach. It is motivated by the pioneering work of Dwork, Halpern, and Waarts ([DHW92]) on performing work of independent tasks efficiently and in the presence of crash failures (i.e., the case of "downwards adaptation").
Reference: [TLC85] <author> M. Theimer, K. Lantz and D. Cheriton, </author> <title> Pre-emptable Remote Execution Facilities for the V-System. </title> <booktitle> Proc. 10th ACM Symp. on Operating Systems Principles. </booktitle> <pages> 2-12, </pages> <year> 1985. </year>
Reference-contexts: In practice, distributed systems have been designed for such exploitation, e.g. Piranha [Kam94], Condor [LLM88], Amber [FBCL91], and the V-system <ref> [TLC85] </ref>. A particular type of applications defined as "a set of independent tasks" is highly desirable in an environment of adaptive parallelism. These tasks were coined "hunters" in the above systems (agents of an application which independently look after resources).
References-found: 15

