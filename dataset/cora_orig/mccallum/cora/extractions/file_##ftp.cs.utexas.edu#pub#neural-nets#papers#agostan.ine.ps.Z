URL: file://ftp.cs.utexas.edu/pub/neural-nets/papers/agostan.ine.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/nn/pages/publications/abstracts.html
Root-URL: 
Email: agogino@ece.utexas.edu  kstanley@cs.utexas.edu  risto@cs.utexas.edu  
Title: Real-time Interactive Neuro-evolution  
Author: Adrian Agogino Kenneth Stanley Risto Miikkulainen 
Date: May 1998  
Address: Austin, TX 78712  Austin, TX 78712  Austin, TX 78712  
Affiliation: Dept. of Electrical and Computer Eng. University of Texas at Austin  Department of Computer Sciences The University of Texas at Austin  Department of Computer Sciences The University of Texas at Austin  
Pubnum: Technical Report AI98-266,  
Abstract: In standard neuro-evolution, a population of networks is evolved in the task, and the network that best solves the task is found. This network is then fixed and used to solve future instances of the problem. Networks evolved in this way do not handle real-time interaction very well. It is hard to evolve a solution ahead of time that can cope effectively with all the possible environments that might arise in the future and with all the possible ways someone may interact with it. This paper proposes evolving feedforward neural networks online to create agents that improve their performance through real-time interaction. This approach is demonstrated in a game world where neural-network-controlled individuals play against humans. Through evolution, these individuals learn to react to varying opponents while appropriately taking into account conflicting goals. After initial evaluation offline, the population is allowed to evolve online, and its performance improves considerably. The population not only adapts to novel situations brought about by changing strategies in the opponent and the game layout, but it also improves its performance in situations that it has already seen in offline training. This paper will describe an implementation of online evolution and shows that it is a practical method that exceeds the performance of offline evolution alone. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Gomez, F., and Miikkulainen, R. </author> <year> (1997). </year> <title> Incremental Evolution of Complex General Behavior. </title> <booktitle> Adaptive Behavior, </booktitle> <volume> 5 </volume> <pages> 317-342. </pages>
Reference: <author> Moriarty, D. </author> <year> (1997). </year> <title> Symbiotic Evolution of Neural Networks in Sequential Decision Tasks. </title> <type> PhD thesis. Technical Report, </type> <institution> UT-AI97-257. </institution>
Reference-contexts: This point aside, a number of papers have shown that evolution can be more powerful than reinforcement learning in certain domains <ref> (Moriarty, 1997, Moriarty & Miikkulainen, 1996) </ref>. 4 Online Evolution Algorithm In the interactive real-time environment where numerous peons are continuously being born and killed, evolution is a natural method for learning.
Reference: <author> Moriarty, D., and Miikkulainen, R. </author> <year> (1995). </year> <title> Discovering Complex Othello Strategies through Evolutionary Neural Networks. </title> <journal> Connection Science, </journal> <volume> 7 </volume> <pages> 195-209. </pages>
Reference: <author> Moriarty, D., and Miikkulainen, R. </author> <year> (1996). </year> <title> Efficient reinforcement learning through symbiotic evolution. </title> <journal> Machine Learning, </journal> <note> Nolfi, </note> <author> S., Elman, J., and Parisi, D. </author> <year> (1994). </year> <title> Learning and Evolution in Neural Networks. Adaptive Behavior, </title> <type> 2 </type> <note> 5-28 Pollack, </note> <author> J., Blair A., and Land M. </author> <year> (1996). </year> <title> Coevolution of Backgammon. </title> <booktitle> In Proceeding of the Fifth Artificial Life Conference. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Richards, N., Moriarty, D., and Miikkulainen, R. </author> <title> (in press). Evolving Neural Networks to Play Go. </title> <journal> Applied Intelligence. </journal>
Reference: <author> Werner, G., and Dyer, M. </author> <year> (1991). </year> <title> Evolution of Communication in Artificial Organisms. </title> <booktitle> Artificial Life II, </booktitle> <pages> 659-687. </pages> <address> Reading, MA: </address> <publisher> Addison Wesley. </publisher>
Reference: <author> Werner, G., and Dyer, M. </author> <year> (1993). </year> <booktitle> Evolution of Herding Behavior in Artificial Animals. In From Animals to Animats 2: Proceedings of the 2 nd International Conference on Simulation of Adaptive Behavior, </booktitle> <editor> Langton, C., Taylor, C., Farmer, J., and Rasmussen S., editors, </editor> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Whitley, D., Dominic S., Das, R., and Anderson, C. </author> <year> (1993). </year> <title> Genetic Reinforcement Learning for Neurocontrol Problems. </title> <journal> Machine Learning, </journal> <volume> 13: </volume> <pages> 259-284 </pages>
Reference-contexts: This is an encouraging result since it suggests that many different versions of genetic algorithms should work in online evolution. This is in contrast to previous work on offline evolution in difficult tasks where a delicate balance exists between crossover and mutation rates <ref> (Whitley, Dominic, Das, and Anderson, 1993) </ref>. 7 Discussion and Future Work The results show that online evolution performs significantly better than offline evolution in interactive real-time domains. If environmental factors are predictable, offline evolution is a reasonable strategy, although online evolution can still increase performance even in these cases.
References-found: 8

