URL: http://www.cs.berkeley.edu/~aiken/cs264/papers/gc-faster-than-stack.ps.Z
Refering-URL: http://www.cs.berkeley.edu/~aiken/cs264/papers/node9.html
Root-URL: 
Title: Garbage Collection Can Be Faster Than Stack Allocation  
Author: Andrew W. Appel 
Date: June 30, 1986 Revised Jan 15, 1987  
Address: Princeton, NJ 08544  
Affiliation: Department of Computer Science Princeton University  
Abstract: A very old and simple algorithm for garbage collection gives very good results when the physical memory is much larger than the number of reachable cells. In fact, the overhead associated with allocating and collecting cells from the heap can be reduced to less than one instruction per cell by increasing the size of physical memory. Special hardware, intricate garbage-collection algorithms, and fancy compiler analysis become unnecessary. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> John McCarthy, </author> <title> ``Recursive functions of symbolic expressions and their computation by machine - I,'' </title> <journal> Communications of the ACM, </journal> <volume> vol. 3, no. 1, </volume> <pages> pp. 184-195, </pages> <publisher> ACM, </publisher> <year> 1960. </year>
Reference-contexts: Allocated cells that are not reachable (via a chain of pointers) from the runtime stack are the ``garbage,'' and they are ``collected'' for re-use by a traversal algorithm. This algorithm typically marks all of the reachable cells using a depth-first search, and then collects all the unmarked cells <ref> [1] </ref>; this takes time proportional to total number of (reachable and garbage) cells. In languages without garbage collection (i.e. Pascal, C, etc.), the programmer must write bookkeeping code to keep track of heap-allocated cells, and free them explicitly when they are no longer needed.
Reference: 2. <author> Guy L. Steele, ``Rabbit: </author> <title> a compiler for Scheme,'' </title> <publisher> AI-TR-474, MIT, </publisher> <year> 1978. </year>
Reference-contexts: On the other hand, because the garbage collector is often slow and expensive, these simpler and more straightforward programs are typically less efficient because they use garbage collection. One trend in optimizing compilers for garbage-collected languages is to have the compiler deduce (statically) which cells may be freed <ref> [2] </ref>. This approach is a good compromise some of the cells will be freed automatically, which reduces the load on the garbage collector; but the complexity is in the optimizing compiler, and not in the compiled program. <p> But in a time when 50 megabytes of memory chips can be obtained for under four thousand dollars [7], there is less need for complex garbage collection algorithms, or special garbage collection hardware. Such techniques as reference-counting [8], ephemeral garbage collection [9], closure analysis <ref> [2] </ref>, etc., may not really be necessary now that it is possible to use massive memories.
Reference: 3. <author> Robert R. Fenichel and Jerome C. Yochelson, </author> <title> ``A LISP garbage-collector for virtual-memory computer systems,'' </title> <journal> Communications of the ACM, </journal> <volume> vol. 12, no. 11, </volume> <pages> pp. 611-612, </pages> <publisher> ACM, </publisher> <year> 1969. </year>
Reference-contexts: For the purposes of this paper, however, we need an algorithm with a time complexity independent of the number of garbage cells. In its simplest form, a copying garbage collector <ref> [3] </ref> works in two equal-sized memory spaces, only one of which is in use at a time. When it is time to collect, the garbage collector traverses all of the - 2 - reachable cells in the active space, and copies them into the inactive space.
Reference: 4. <author> Henry Lieberman and Carl Hewitt, </author> <title> ``A real-time garbage collector based on the lifetimes of objects,'' </title> <journal> Communications of the ACM, </journal> <volume> vol. 23, no. 6, </volume> <pages> pp. 419-429, </pages> <publisher> ACM, </publisher> <year> 1983. </year>
Reference-contexts: Actually, M is really one-half of the machine's memory, since it is the size of each of the two spaces. Some enhancements of the copying algorithm use N spaces, with only one of them unused at a time, so there need not be this additional factor of two <ref> [4] </ref>. Furthermore, if the size of the data is truly much less than the size of each space, then the spaces could be made to overlap: the most memory needed at any one time is really M + sA, not 2M. <p> There are other algorithms that also become cheaper in proportion to the amount of memory in the system; the non-concurrent version of the Lieberman-Hewitt algorithm <ref> [4] </ref> is an example. Any system that incorporates such a garbage collector will automatically improve in speed as memories become cheaper and larger.
Reference: 5. <author> Robin Milner, </author> <title> ``A proposal for Standard ML,'' </title> <booktitle> ACM Symposium on LISP and Functional Programming, </booktitle> <pages> pp. 184-197, </pages> <publisher> ACM, </publisher> <year> 1984. </year>
Reference-contexts: The current machine is a 128-megabyte VAX-11/785. To confirm the results predicted in the previous section that garbage collection time goes down dramatically as memory increases a simple experiment was run: the Edinburgh Standard-ML <ref> [5] </ref> system compiling 2277 lines of Standard-ML code. This system is designed for portability rather than optimality, so it's not particularly fast; its garbage collector is written in C, not in assembly language; but even so, it provides a convincing demonstration of the advantages of massive memory.
Reference: 6. <author> H. G. Baker, </author> <title> ``List processing in real time on a serial computer,'' </title> <journal> Communications of the ACM, </journal> <volume> vol. 21, no. 4, </volume> <pages> pp. 280-294, </pages> <publisher> ACM, </publisher> <year> 1978. </year> <title> 7. ``Oki goes to court on Microtech pact,'' </title> <journal> Electronic News, </journal> <volume> vol. 32, no. 1609, </volume> <month> July 7, </month> <year> 1986. </year>
Reference-contexts: Thus, it has no guaranteed upper bound on response time, which is a disadvantage in a real-time environment. However, the total garbage collection overhead of this algorithm can be much less than that of a concurrent algorithm like Baker's <ref> [6] </ref>, which has several instructions of overhead per cell allocated. There are other algorithms that also become cheaper in proportion to the amount of memory in the system; the non-concurrent version of the Lieberman-Hewitt algorithm [4] is an example.
Reference: 8. <author> G. E. Collins, </author> <title> ``A method for overlapping and erasure of lists,'' </title> <journal> Communications of the ACM, </journal> <volume> vol. 3, no. 12, </volume> <pages> pp. 655-657, </pages> <publisher> ACM, </publisher> <year> 1960. </year>
Reference-contexts: But in a time when 50 megabytes of memory chips can be obtained for under four thousand dollars [7], there is less need for complex garbage collection algorithms, or special garbage collection hardware. Such techniques as reference-counting <ref> [8] </ref>, ephemeral garbage collection [9], closure analysis [2], etc., may not really be necessary now that it is possible to use massive memories.
Reference: 9. <author> David A. Moon, </author> <title> ``Garbage collection in a large LISP system,'' </title> <booktitle> ACM Symposium on LISP and Functional Programming, </booktitle> <pages> pp. 235-246, </pages> <publisher> ACM, </publisher> <year> 1984. </year> <month> - 6 </month> - 
Reference-contexts: But in a time when 50 megabytes of memory chips can be obtained for under four thousand dollars [7], there is less need for complex garbage collection algorithms, or special garbage collection hardware. Such techniques as reference-counting [8], ephemeral garbage collection <ref> [9] </ref>, closure analysis [2], etc., may not really be necessary now that it is possible to use massive memories.
References-found: 8

