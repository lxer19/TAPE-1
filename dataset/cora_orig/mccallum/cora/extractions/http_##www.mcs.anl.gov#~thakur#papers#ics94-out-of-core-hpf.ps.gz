URL: http://www.mcs.anl.gov/~thakur/papers/ics94-out-of-core-hpf.ps.gz
Refering-URL: http://www.mcs.anl.gov/~thakur/papers.html
Root-URL: http://www.mcs.anl.gov
Email: @npac.syr.edu  
Title: Compiler and Runtime Support for Out-of-Core HPF Programs  
Author: Rajeev Thakur Rajesh Bordawekar Alok Choudhary thakur, rajesh, choudhar 
Address: Syracuse, NY 13244, USA  
Affiliation: Dept. of Electrical and Computer Eng. and Northeast Parallel Architectures Center Syracuse University,  
Abstract: This paper describes the design of a compiler which can translate out-of-core programs written in a data parallel language like HPF. Such a compiler is required for compiling large scale scientific applications, such as the Grand Challenge applications, which deal with enormous quantities of data. We propose a framework by which a compiler together with appropriate runtime support can translate an out-of-core HPF program to a message passing node program with explicit parallel I/O. We describe the basic model of the compiler and the various transformations made by the compiler. We also discuss the runtime routines used by the compiler for I/O and communication. In order to minimize I/O, the runtime support system can reuse data already fetched into memory. The working of the compiler is illustrated using two out-of-core applications, namely a Laplace equation solver and LU Decomposition, together with performance results on the Intel Touchstone Delta. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ahmad, I., Bordawekar, R., Bozkus, Z., Choud-hary, A., Fox, G., Parasuram, K., Ponnusamy, R., Ranka, S., and Thakur, R. </author> <title> Fortran 90D Intrinsic Functions on Distributed Memory Machines: Implementation and Scalability. </title> <booktitle> In Proceedings of 26 th Hawaii International Conference on System Sciences (January 1993). </booktitle>
Reference-contexts: Runtime support has been used previously as an aide to the compiler. Runtime primitives for the initial reading of data from a file for an in-core program are discussed in [5]. The in-core HPF compiler developed by our group at Syracuse University uses runtime support <ref> [7, 1] </ref>. Pon-nusamy et al [18] describe how runtime support can be integrated with a compiler to solve unstructured problems with irregular communication patterns. These projects only deal with compilation of in-core programs, so the runtime support is mainly limited to communication libraries.
Reference: [2] <author> Bodin, F., Beckman, P., Gannon, D., Narayana, S., and Yang, S. </author> <title> Distributed pC++: Basic Ideas for an Object Parallel Language. </title> <booktitle> In Proceedings of the First Annual Object-Oriented Numerics Conference (April 1993), </booktitle> <pages> pp. 1-24. </pages>
Reference-contexts: A good overview of the various issues in high performance I/O is given in [12] In this paper, we consider the I/O problem from a language and compiler point of view. Data parallel languages like HPF [15] and pC++ <ref> [2] </ref> have recently been developed to provide support for high performance programming on parallel machines. These languages provide a framework for writing portable parallel programs independent of the underlying architecture and other idiosyncrasies of the machine.
Reference: [3] <author> Bordawekar, R., and Choudhary, A. </author> <title> Language and Compiler Support for Parallel I/O. </title> <booktitle> In IFIP Working Conference on Programming Environments for Massively Parallel Distributed Systems (Apr. </booktitle> <year> 1994). </year>
Reference-contexts: In order that these languages can be used for programming Grand Challenge Applications, it is essential that the compiler can automatically translate out-of-core data parallel programs. Language support for out-of-core programs has been proposed in <ref> [3, 8, 20] </ref>. We propose a framework by which a compiler together with appropriate run-time support can translate an out-of-core HPF program to a message passing node program with explicit parallel I/O.
Reference: [4] <author> Bordawekar, R., Choudhary, A., and del Rosario, J. </author> <title> An Experimental Performance Evaluation of Touchstone Delta Concurrent File System. </title> <booktitle> In Proceedings of International Conference on Supercomputing, </booktitle> <address> Tokyo, Japan (July 1993). </address>
Reference-contexts: The need for high performance I/O is so significant that almost all the present generation parallel computers such as the Paragon, iPSC/860, Touchstone Delta, CM-5, SP-1, nCUBE2 etc. provide some kind of hardware and software support for parallel I/O <ref> [10, 17, 4, 11] </ref>. A good overview of the various issues in high performance I/O is given in [12] In this paper, we consider the I/O problem from a language and compiler point of view.
Reference: [5] <author> Bordawekar, R., del Rosario, J., and Choud-hary, A. </author> <title> Design and Evaluation of Primitives for Parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93 (November 1993), </booktitle> <pages> pp. 452-461. </pages>
Reference-contexts: Runtime support has been used previously as an aide to the compiler. Runtime primitives for the initial reading of data from a file for an in-core program are discussed in <ref> [5] </ref>. The in-core HPF compiler developed by our group at Syracuse University uses runtime support [7, 1]. Pon-nusamy et al [18] describe how runtime support can be integrated with a compiler to solve unstructured problems with irregular communication patterns.
Reference: [6] <author> Bozkus, Z., Choudhary, A., Fox, G., Haupt, T., and Ranka, S. </author> <title> Fortran 90D/HPF Compiler for Distributed Memory MIMD Computers: Design, Implementation, and Performance Results. </title> <booktitle> In Proceedings of Supercomputing '93 (November 1993), </booktitle> <pages> pp. 351-360. </pages>
Reference-contexts: Although we use HPF as the source language, the translation technique is applicable to any other data parallel language. There has been considerable research on compiling in-core data parallel programs for distributed memory machines <ref> [6, 22, 21] </ref>. This work, to our knowledge, is one of the first attempts at a methodology for compiling out-of-core data parallel programs. The rest of the paper is organized as follows. The model for out-of-core compilation is explained in Section 2. <p> This results in each processor having a local array associated with it. In an in-core program, the local array resides in the local memory of the processor. Our group at Syracuse University has developed a compiler for in-core HPF programs <ref> [6] </ref>. For large data sets, however, local arrays cannot entirely fit in main memory. Hence, these programs are referred to as out-of-core programs. In such cases, parts of the local array have to be stored on disk. We refer to such a local array as the Out-of-core Local Array (OCLA). <p> In this example, arrays A and B are distributed in (block,block) fashion on 16 processors arranged as a two-dimensional grid of 4 fi 4. 3.1 In-core Compilation This section describes the in-core compilation methodology used in the HPF compiler developed by our group at Syra-cuse University <ref> [6] </ref>. Consider the array assignment statement from Figure 2. The compiler translates this statement using the following steps:- 1. Analyze the distribution pattern of each array used in the array expression. 2. Depending on the distribution, detect the type of com munication required. 3. <p> The compiler analyzes the statement and inserts a call to the appropriate collective communication routine. The assignment statement is translated into corresponding DO loops with a call to a routine which performs overlap shift type communication <ref> [6] </ref>, as shown in Figure 3. 3.2 Out-of-core Compilation For compiling out-of-core programs, in addition to handling all the issues involved in compiling in-core programs, the compiler must also schedule explicit I/O accesses to fetch/store appropriate data from/to disks. <p> Hence, we prefer to use the out-of-core communication method. Detecting the type of communication required in an array assignment statement involves analyzing the relationships among the subscripts of the arrays in the statement <ref> [6, 16, 13] </ref>. I/O pattern detection involves analyzing I/O characteristics of array expressions. There are many factors that influence the I/O access patterns.
Reference: [7] <author> Bozkus, Z., Choudhary, A., Fox, G., Haupt, T., Ranka, S., Thakur, R., and Wang, J. </author> <title> Scalable Libraries for High Performance Fortran. </title> <booktitle> In Proceedings of Scalable Parallel Libraries Conference (October 1993), </booktitle> <institution> Mississippi State University. </institution>
Reference-contexts: Runtime support has been used previously as an aide to the compiler. Runtime primitives for the initial reading of data from a file for an in-core program are discussed in [5]. The in-core HPF compiler developed by our group at Syracuse University uses runtime support <ref> [7, 1] </ref>. Pon-nusamy et al [18] describe how runtime support can be integrated with a compiler to solve unstructured problems with irregular communication patterns. These projects only deal with compilation of in-core programs, so the runtime support is mainly limited to communication libraries.
Reference: [8] <author> Brezany, P., Gerndt, M., Mehrotra, P., and Zima, H. </author> <title> Concurrent File Operations in a High Performance Fortran. </title> <booktitle> In Proceedings of Supercomputing '92 (November 1992), </booktitle> <pages> pp. 230-238. </pages>
Reference-contexts: In order that these languages can be used for programming Grand Challenge Applications, it is essential that the compiler can automatically translate out-of-core data parallel programs. Language support for out-of-core programs has been proposed in <ref> [3, 8, 20] </ref>. We propose a framework by which a compiler together with appropriate run-time support can translate an out-of-core HPF program to a message passing node program with explicit parallel I/O.
Reference: [9] <author> Chen, M., and Cowie, J. </author> <title> Prototyping Fortran-90 Compilers for Massively Parallel Machines. </title> <booktitle> In Proceedings of the Conference on Programming Language Design and Implementation (1992), </booktitle> <pages> pp. 94-105. </pages>
Reference-contexts: Optimizing such communication patterns can be difficult. It requires extensive pre-processing and the translated code looks unreadable. 3.3 Compiling Out-of-core Array Assignment Statements Array assignments involving distributed arrays often result in different communication patterns <ref> [9, 13] </ref>. The compiler must recognize the type of communication in order to generate appropriate runtime calls (communication as well as I/O). It is relatively easier to detect and optimize the communication in the out-of-core communication method than in the in-core communication method.
Reference: [10] <author> Corbett, P., Feitelson, D., Prost, J., and Bay-lor, S. </author> <title> Parallel Access to Files in the Vesta File System. </title> <booktitle> In Proceedings of Supercomputing '93 (November 1993), </booktitle> <pages> pp. 472-481. </pages>
Reference-contexts: The need for high performance I/O is so significant that almost all the present generation parallel computers such as the Paragon, iPSC/860, Touchstone Delta, CM-5, SP-1, nCUBE2 etc. provide some kind of hardware and software support for parallel I/O <ref> [10, 17, 4, 11] </ref>. A good overview of the various issues in high performance I/O is given in [12] In this paper, we consider the I/O problem from a language and compiler point of view.
Reference: [11] <author> DeBenedictis, E., and del Rosario, J. </author> <title> nCUBE Parallel I/O Software. </title> <booktitle> In Proceedings of 11 th International Phoenix Conference on Computers and Communications (April 1992), </booktitle> <pages> pp. 117-124. </pages> <note> [12] del Rosario, </note> <author> J., and Choudhary, A. </author> <title> High Performance I/O for Parallel Computers: Problems and Prospects. </title> <booktitle> IEEE Computer (March 1994), </booktitle> <pages> 59-68. </pages>
Reference-contexts: The need for high performance I/O is so significant that almost all the present generation parallel computers such as the Paragon, iPSC/860, Touchstone Delta, CM-5, SP-1, nCUBE2 etc. provide some kind of hardware and software support for parallel I/O <ref> [10, 17, 4, 11] </ref>. A good overview of the various issues in high performance I/O is given in [12] In this paper, we consider the I/O problem from a language and compiler point of view.
Reference: [13] <author> Gupta, M. </author> <title> Automatic Data Partitioning on Distributed Memory Multicomputers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> September </month> <year> 1992. </year>
Reference-contexts: Optimizing such communication patterns can be difficult. It requires extensive pre-processing and the translated code looks unreadable. 3.3 Compiling Out-of-core Array Assignment Statements Array assignments involving distributed arrays often result in different communication patterns <ref> [9, 13] </ref>. The compiler must recognize the type of communication in order to generate appropriate runtime calls (communication as well as I/O). It is relatively easier to detect and optimize the communication in the out-of-core communication method than in the in-core communication method. <p> Hence, we prefer to use the out-of-core communication method. Detecting the type of communication required in an array assignment statement involves analyzing the relationships among the subscripts of the arrays in the statement <ref> [6, 16, 13] </ref>. I/O pattern detection involves analyzing I/O characteristics of array expressions. There are many factors that influence the I/O access patterns.
Reference: [14] <institution> High Performance Computing and Communications: </institution> <note> Grand Challenges 1993 Report. A Report by the Committee on Physical, </note> <institution> Mathematical and Engineering Sciences, Federal Coordinating Council for Science, Engineering and Technology. </institution>
Reference-contexts: As a result, MPPs are increasingly being used to solve large scale computational problems in physics, chemistry, biology, engineering, medicine and other sciences. These applications, which are also referred to as Grand Challenge Applications <ref> [14] </ref>, are extremely complex and require several Teraflops of computing power to be solved in a reasonable amount of time. In addition to requiring a great deal of computational power, these applications usually deal with large quantities of data.
Reference: [15] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification. </title> <note> Version 1.0, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: A good overview of the various issues in high performance I/O is given in [12] In this paper, we consider the I/O problem from a language and compiler point of view. Data parallel languages like HPF <ref> [15] </ref> and pC++ [2] have recently been developed to provide support for high performance programming on parallel machines. These languages provide a framework for writing portable parallel programs independent of the underlying architecture and other idiosyncrasies of the machine.
Reference: [16] <author> J. Li and M. Chen. </author> <title> Compiling Communication-Efficient Programs for Massively Parallel Machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems (July 1991), </journal> <pages> 361-376. </pages>
Reference-contexts: Hence, we prefer to use the out-of-core communication method. Detecting the type of communication required in an array assignment statement involves analyzing the relationships among the subscripts of the arrays in the statement <ref> [6, 16, 13] </ref>. I/O pattern detection involves analyzing I/O characteristics of array expressions. There are many factors that influence the I/O access patterns.
Reference: [17] <author> Pierce, P. </author> <title> A Concurrent File System for a Highly Parallel Mass Storage Subsystem. </title> <booktitle> In Proceedings of 4 th Conference on Hypercubes, Concurrent Computers and Applications (March 1989), </booktitle> <pages> pp. 155-160. </pages>
Reference-contexts: The need for high performance I/O is so significant that almost all the present generation parallel computers such as the Paragon, iPSC/860, Touchstone Delta, CM-5, SP-1, nCUBE2 etc. provide some kind of hardware and software support for parallel I/O <ref> [10, 17, 4, 11] </ref>. A good overview of the various issues in high performance I/O is given in [12] In this paper, we consider the I/O problem from a language and compiler point of view.
Reference: [18] <author> Ponnusamy, R., Saltz, J., and Choudhary, A. </author> <title> Runtime-Compilation Techniques for Data Partitioning and Communication Schedule Reuse. </title> <booktitle> In Proceedings of Supercomputing '93 (November 1993), </booktitle> <pages> pp. 361-370. </pages>
Reference-contexts: Runtime primitives for the initial reading of data from a file for an in-core program are discussed in [5]. The in-core HPF compiler developed by our group at Syracuse University uses runtime support [7, 1]. Pon-nusamy et al <ref> [18] </ref> describe how runtime support can be integrated with a compiler to solve unstructured problems with irregular communication patterns. These projects only deal with compilation of in-core programs, so the runtime support is mainly limited to communication libraries.
Reference: [19] <author> Saini, S., and Simon, H. </author> <title> Enhancing Applications Performance on Intel Paragon through Dynamic Memory Allocation. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <institution> Mississippi State University (Octo-ber 1993). </institution>
Reference-contexts: Studies of the performance of virtual memory provided by the OSF/1 operating system on the Intel Paragon have shown that the paging in and paging out of data from the nodes drastically degrades the performance of the user code <ref> [19] </ref>. Also, most of the other massively parallel systems at present, such as the CM-5, iPSC/860, Touchstone Delta, nCUBE-2 etc, do not support virtual memory on the nodes. Hence, the HPF compiler must translate into a code which explicitly performs I/O.
Reference: [20] <author> Snir, M. </author> <title> Proposal for IO. </title> <note> Posted to HPFF I/O Forum by Marc Snir, </note> <month> July </month> <year> 1992. </year>
Reference-contexts: In order that these languages can be used for programming Grand Challenge Applications, it is essential that the compiler can automatically translate out-of-core data parallel programs. Language support for out-of-core programs has been proposed in <ref> [3, 8, 20] </ref>. We propose a framework by which a compiler together with appropriate run-time support can translate an out-of-core HPF program to a message passing node program with explicit parallel I/O.
Reference: [21] <author> Su, E., Palermo, D., and Banerjee, P. </author> <title> Automatic Parallelization of Regular Computations For Distributed Memory Multicomputers in the PARADIGM Compiler. </title> <booktitle> In Proceedings of International Conference on Parallel Processing (August 1993), </booktitle> <pages> pp. </pages> <month> II-30|II-38. </month>
Reference-contexts: Although we use HPF as the source language, the translation technique is applicable to any other data parallel language. There has been considerable research on compiling in-core data parallel programs for distributed memory machines <ref> [6, 22, 21] </ref>. This work, to our knowledge, is one of the first attempts at a methodology for compiling out-of-core data parallel programs. The rest of the paper is organized as follows. The model for out-of-core compilation is explained in Section 2.
Reference: [22] <author> Tseng, C. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Although we use HPF as the source language, the translation technique is applicable to any other data parallel language. There has been considerable research on compiling in-core data parallel programs for distributed memory machines <ref> [6, 22, 21] </ref>. This work, to our knowledge, is one of the first attempts at a methodology for compiling out-of-core data parallel programs. The rest of the paper is organized as follows. The model for out-of-core compilation is explained in Section 2.
Reference: [23] <author> Wolfe, M. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Each processor performs computation on the data in the ICLA. Some of the issues in out-of-core compilation are similar to optimizations carried out in in-core compilers to take advantage of caches or pipelines. This optimization, commonly known as stripmining <ref> [23, 24] </ref>, partitions the loop iterations so that data of fixed size (equal to cache size or pipeline stages) can be operated on in each iteration.
Reference: [24] <author> Zima, H., and Chapman, B. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: Each processor performs computation on the data in the ICLA. Some of the issues in out-of-core compilation are similar to optimizations carried out in in-core compilers to take advantage of caches or pipelines. This optimization, commonly known as stripmining <ref> [23, 24] </ref>, partitions the loop iterations so that data of fixed size (equal to cache size or pipeline stages) can be operated on in each iteration.
References-found: 23

