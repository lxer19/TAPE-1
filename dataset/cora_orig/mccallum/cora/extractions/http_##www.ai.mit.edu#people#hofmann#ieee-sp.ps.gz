URL: http://www.ai.mit.edu/people/hofmann/ieee-sp.ps.gz
Refering-URL: http://www.ai.mit.edu/people/hofmann/publications.html
Root-URL: 
Title: Competitive Learning Algorithms for Robust Vector Quantization  
Author: Thomas Hofmann and Joachim M. Buhmann 
Abstract: The efficient representation and encoding of signals with limited resources, e.g., finite storage capacity and restricted transmission bandwidth, is a fundamental problem in technical as well as biological information processing systems. Typically, under realistic circumstances, the encoding and communication of messages has to deal with different sources of noise and disturbances. In this paper, we propose a unifying approach to data compression by robust vector quantization, which explicitly deals with channel noise, bandwidth limitations, and random elimination of prototypes. The resulting algorithm is able to limit the detrimental effect of noise in a very general communication scenario. In addition, the presented model allows us to derive a novel competitive neural networks algorithm, which covers topology preserving feature maps, the so-called neural-gas algorithm, and the maximum entropy soft-max rule as special cases. Furthermore, continuation methods based on these noise models improve the codebook design by reducing the sensitivity to local minima. We show an exemplary application of the novel robust vector quantization algorithm to image compression for a teleconferencing system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. M. Gray, </author> <title> "Vector quantization," </title> <journal> IEEE Acoustics, Speech and Signal Processing Magazine, </journal> <pages> pp. 4-29, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: The stationary equations c iff = P m 1 n X c iff ; (14) are solved by re-calculating c and p in alternation until convergence. Here is a Lagrange parameter to enforce the rate constraint, and the mapping (R) : <ref> [0; 1] </ref> ! [0; R max ] is known to be monotonically decreasing [30]. Further minimization of H drf (Y; X ; R) w.r.t. Y results in the generalized centroid equations, c iff replacing the Kronecker delta in Eq. (3).
Reference: [2] <author> A. Gersho and R. M. Gray, </author> <title> Vector Quantization and Signal Processing, </title> <publisher> Kluwer Academic Publisher, </publisher> <address> Boston, </address> <year> 1992. </year>
Reference: [3] <author> M. Antonini, M. Barlaud, P. Mathieu, and I. Daubechies, </author> <title> "Image coding using wavelet transform," </title> <journal> IEEE Trans Image Processing, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 205-220, </pages> <year> 1992. </year>
Reference-contexts: The channel noise was a bit flip probability of p = 0:01. wavelet coefficients into blocks is performed according to Antonini et al. <ref> [3] </ref>. All image sequences have been preprocessed by a three-level biorthogonal wavelet transformation [42]. Data vectors were generated by grouping neighboring wavelet coefficients in blocks of size 4 fi 4 and 2 fi 2 for each subband separately.
Reference: [4] <author> T. Lookabaugh, E.A. Riskin, P.A. Chou, and R.M. Gray, </author> <title> "Variable rate vector quantization for speech, image, and video compression," </title> <journal> IEEE Trans Communications, </journal> <volume> vol. 41, no. 1, </volume> <pages> pp. 186-199, </pages> <year> 1993. </year>
Reference-contexts: In other words, the alphabet of the utilized code is dynamically reduced by randomly discarding certain indices ff without altering the reconstruction codebook Y. In encoding applications codebook reductions might be necessitated by rapidly varying bandwidth limits, a problem also known as variable-rate vector quantization <ref> [4] </ref>. Usually, variable-rate codebooks are designed to cover a large range of possible bandwidths, e.g., by storing hierarchies of codebooks of different size. This strategy is problematic for adaptive vector quantization, since a large set of codebooks has to be adapted, creating severe performance problems especially in an on-line setting.
Reference: [5] <author> W.P. Li and Y.P. Zhang, </author> <title> "Vector-based signal processing and quantization for image and video compression," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 83, no. 2, </volume> <pages> pp. 317-335, </pages> <year> 1995. </year>
Reference: [6] <author> Y. Linde, A. Buzo, and R. M. Gray, </author> <title> "An algorithm for vector quantizer design," </title> <journal> IEEE Trans Communications, </journal> <volume> vol. 28, </volume> <pages> pp. 84-95, </pages> <year> 1980. </year>
Reference: [7] <author> S.P. Lloyd, </author> <title> "Least squres quantization in PCM," </title> <journal> IEEE Trans Information Theory, </journal> <volume> vol. 28, </volume> <pages> pp. 129-137, </pages> <year> 1982, </year> <note> reprint of 1957 paper. </note>
Reference: [8] <author> G.B. Ball and D.J. Hall, </author> <title> "A clustering technique for summarizing multivariate data," </title> <booktitle> Behavioral Science, </booktitle> <volume> vol. 12, </volume> <pages> pp. 153-155, </pages> <year> 1967. </year>
Reference: [9] <author> J. MacQueen, </author> <title> "Some methods for classification and analysis of multivariate observations," </title> <booktitle> in Proceedings of the 5th Berkeley Symposium on Mathematical Statistics and Probability, </booktitle> <year> 1967, </year> <pages> pp. 281-297. </pages>
Reference: [10] <author> S. Grossberg, </author> <title> "On learning and energy-entropy dependence in recurrent and nonrecurrent signed networks," </title> <journal> J. Statist. Phys., </journal> <volume> vol. 1, </volume> <pages> pp. 319-350, </pages> <year> 1969. </year>
Reference: [11] <author> T. Kohonen, </author> <title> "Self-organizing formation of topologically correct feature maps," </title> <journal> Biological Cybernetics, </journal> <volume> vol. 43, </volume> <pages> pp. 59-69, </pages> <year> 1982. </year>
Reference: [12] <author> T. Martinetz, S.G. Berkovich, and K.J. Schulten, </author> <title> "Neural-gas network for vector quantization and its application to time-series prediction," </title> <journal> IEEE Trans Neural Networks, </journal> <volume> vol. 4, no. 4, </volume> <pages> pp. 558-569, </pages> <year> 1993. </year>
Reference-contexts: In the case of uniform elimination probabilities * ff * the objective function simplifies to the neural-gas model H ng <ref> [12] </ref>, where p ffjr i / * r i (ff) . <p> A conceptual problem of these techniques in the noise-free case is the need to specify a complete noise model including a topology on the index space f1; : : :; mg. (ii) In the original NG algorithm <ref> [12] </ref> the noise level was reduced according to the exponential law * (n+1) = r* (n) , 0 &lt; r &lt; 1, since the authors wanted to accelerate learning and did not search for robust data representations. (iii) A continuation method based on the 5 Typically, H a is convex for
Reference: [13] <author> J. M. Buhmann and H. Kuhnel, </author> <title> "Complexity optimized data clustering by competitive neural networks," </title> <journal> Neural Computation, </journal> <volume> vol. 5, </volume> <pages> pp. 75-88, </pages> <year> 1993. </year>
Reference-contexts: The objective functions H vq generalizes to H svq (c; Y; X ) = n i=1 -=1 It has been noticed that source-channel coding may lead to a topological ordering of prototypes [24], [25], [14], <ref> [13] </ref>. If s -jff is high, it is advantageous to place y ff and y close to each other in order to limit the detrimental effect of a reconstruction from corrupted indices.
Reference: [14] <author> S.P. Luttrell, </author> <title> "Hierarchical vector quantization," </title> <booktitle> IEE Proceedings, </booktitle> <volume> vol. 136, </volume> <pages> pp. 405-413, </pages> <year> 1989. </year>
Reference-contexts: The objective functions H vq generalizes to H svq (c; Y; X ) = n i=1 -=1 It has been noticed that source-channel coding may lead to a topological ordering of prototypes [24], [25], <ref> [14] </ref>, [13]. If s -jff is high, it is advantageous to place y ff and y close to each other in order to limit the detrimental effect of a reconstruction from corrupted indices.
Reference: [15] <author> T. Kohonen, </author> <title> "The self-organizing map," </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> vol. 78, no. 9, </volume> <pages> pp. 1464-1480, </pages> <month> Sept. </month> <year> 1990. </year>
Reference: [16] <author> J. M. Buhmann and H. Kuhnel, </author> <title> "Vector quantization with complexity costs," </title> <journal> IEEE Trans Information Theory, </journal> <volume> vol. 39, no. 4, </volume> <pages> pp. 1133-1145, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: B. Generalized SOFM Learning Rule The relationship to the SOFM can be made more explicit by deriving the on-line learning equations corresponding to H svq . Proceeding along the same lines as in the noise-free case, we obtain a generalization of Eq. (6) <ref> [16] </ref> with (n) s ffjc n =n (n) ff , where c n is now given by Eq. (8) instead of the WTA rule in Eq. (3) and n (n) P n i=1 s ffjc i . <p> The resulting equations are equivalent to the ones obtained by applying the maximum entropy principle to the empirical risk in Eq. (1), following an optimization technique known as deterministic annealing (DA) [23], [35], <ref> [16] </ref>. In the context of DA, the Lagrange parameter is identified with the computa tional temperature T . <p> B. Channel Noise and the Maximum Entropy Principle The generalization of Eq. (15) to noisy channels is achieved without further conceptual difficulties (cf. <ref> [16] </ref>) by e T - s -jff (x i y -) 2 e T - s -j (x i y -) 2 ; (17) P P P P : (18) As can be seen, the weights in the above generalized centroid condition combine the probabilistic effects of the en tropy and <p> the objective function H uni (Y; X ) = ef1;:::;mg F e = i=1 q2P m X D (x i ; y ff ) -2e X q log q - :(23) F e has an interpretation in terms of statistical physics, where it is known as the free energy [35], <ref> [16] </ref>. Hence H uni is an average of free energies F e over elimination events e. The T ! 0 truncated ME approximation is equivalent to restricting the minimization inside of F e to association probabilities with limited support, while the second approximation restricts the sum over events e. <p> DA is a very general heuristic optimization technique which has been applied to many different combinatorial optimization problems, including the problem of adaptive codebook design [23], [35], <ref> [16] </ref>. Continuation methods like DA are optimization heuristics which have empirically shown to result in a satisfactory tradeoff between solution quality and algorithmic efficiency. However, due to the lack of theoretical insight about the geometry of solution curves, performance guarantees can not be given in the general case.
Reference: [17] <author> J. Hertz, A. Krogh, and R. G. Palmer, </author> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison Wesley, </publisher> <address> New York, </address> <year> 1991. </year>
Reference: [18] <author> T. Kohonen, </author> <title> Self-organization and Associative Memory, </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1984. </year>
Reference-contexts: The above soft competitive learning rule modifies the original SOFM learning <ref> [18] </ref> by defining the `winner' as the neuron with minimal expected distortion, a property which also depends on the weights of neighboring neurons. In addition, each neuron has an individual learning rate as in the noise-free case. IV. Robust Vector Quantization A. <p> These procedures primarily try to avoid unfavorable local minima by smoothing H with increasing a. 5 In fact, all of the discussed competitive learning rules have been utilized in this context: (i) In <ref> [18] </ref> it was proposed to let the neighborhood strength shrink in a combined schedule with the learning rate. More recently, this idea has been discussed under the title of noisy channel relaxation in the context of source-channel coding [38].
Reference: [19] <author> D.E. Rumelhart and D. Zipser, </author> <title> "Feature discovery by competitive learning," </title> <journal> Cognitive Science, </journal> <volume> vol. 9, </volume> <pages> pp. 75-112, </pages> <year> 1985. </year>
Reference: [20] <author> G.A. Carpenter and S. Grossberg, </author> <title> "A massively parallel architecture for a self-organizing neural pattern recognition machine," Computer, Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> vol. 37, </volume> <pages> pp. 54-115, </pages> <year> 1987. </year>
Reference: [21] <author> S.C. Ahalt, P.K. Chen, and D.E. Melton, </author> <title> "Competitive learning algorithms for vector quantization," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 3, no. 3, </volume> <pages> pp. 277-290, </pages> <year> 1990. </year>
Reference: [22] <author> B. Kosko, </author> <title> "Unsupervised learning in noise," </title> <journal> IEEE Trans Neural Networks, </journal> <volume> vol. 1, </volume> <pages> pp. 44-57, </pages> <month> Mar. </month> <year> 1990. </year>
Reference: [23] <author> K. Rose, E. Gurewitz, and G. Fox, </author> <title> "Statistical mechanics and phase transitions in clustering," </title> <journal> Physical Review Letters, </journal> <volume> vol. 65, no. 8, </volume> <pages> pp. 945-948, </pages> <year> 1990. </year>
Reference-contexts: The resulting equations are equivalent to the ones obtained by applying the maximum entropy principle to the empirical risk in Eq. (1), following an optimization technique known as deterministic annealing (DA) <ref> [23] </ref>, [35], [16]. In the context of DA, the Lagrange parameter is identified with the computa tional temperature T . <p> (354 fi 288 pixels). (a) PSNR in dB for robust (middle) and LBG (lower) batch codebook design (upper PSNR curve for noise-free DA added as a reference). (b) and (c) example frame encoded with a robust and LBG codebook, respectively. maximum entropy principle is known as deterministic annealing (DA) [39], <ref> [23] </ref>, which is a deterministic variant of simulated annealing (SA) [40]. With SA it shares the intuitive motivation from statistical physics to introduce a temperature scale T and to track solutions from high to low temperatures (T ! 0 limit). <p> DA is a very general heuristic optimization technique which has been applied to many different combinatorial optimization problems, including the problem of adaptive codebook design <ref> [23] </ref>, [35], [16]. Continuation methods like DA are optimization heuristics which have empirically shown to result in a satisfactory tradeoff between solution quality and algorithmic efficiency. However, due to the lack of theoretical insight about the geometry of solution curves, performance guarantees can not be given in the general case.
Reference: [24] <author> H. Kumazawa, M. Kasahara, and T. Namekawa, </author> <title> "A construction of vector quantizers for noisy channels," </title> <journal> Electronics and Engineering in Japan, </journal> <volume> vol. 67B, no. 4, </volume> <pages> pp. 39-47, </pages> <year> 1984. </year>
Reference-contexts: The objective functions H vq generalizes to H svq (c; Y; X ) = n i=1 -=1 It has been noticed that source-channel coding may lead to a topological ordering of prototypes <ref> [24] </ref>, [25], [14], [13]. If s -jff is high, it is advantageous to place y ff and y close to each other in order to limit the detrimental effect of a reconstruction from corrupted indices.
Reference: [25] <author> M. Farvardin, </author> <title> "A study of vector quantization for noisy channels," </title> <journal> IEEE Trans Information Theory, </journal> <volume> vol. 36, </volume> <pages> pp. 799-809, </pages> <year> 1990. </year>
Reference-contexts: The objective functions H vq generalizes to H svq (c; Y; X ) = n i=1 -=1 It has been noticed that source-channel coding may lead to a topological ordering of prototypes [24], <ref> [25] </ref>, [14], [13]. If s -jff is high, it is advantageous to place y ff and y close to each other in order to limit the detrimental effect of a reconstruction from corrupted indices.
Reference: [26] <author> A.K. Krishnamurthy, S. C. Ahalt, D.E. Melton, and P. Chen, </author> <title> "Neural networks for vector quantization of speech and images," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 8, no. 8, </volume> <pages> pp. 1449-1457, </pages> <year> 1990. </year>
Reference: [27] <author> A. Gersho, </author> <title> "On the structure of vector quantizers," </title> <journal> IEEE Trans Information Theory, </journal> <volume> vol. 28, no. 2, </volume> <pages> pp. 157-166, </pages> <year> 1982. </year>
Reference: [28] <author> Ch. Darken and J. Moody, </author> <title> "Note on learning rate schedules for stochastic optimization," </title> <booktitle> in Advances in Neural Information Processing Systems, 1991, </booktitle> <volume> vol. </volume> <pages> 3. </pages>
Reference-contexts: How ever, the convergence rate might be too slow, and it is advantageous to include an additional learning gain to ac celerate the adaptation. In our simulations we have there fore utilized learning rate schedules, based on the `First-Search-Then-Converge' heuristic <ref> [28] </ref>, (n) n ff = 0 ). By choosing 0 &gt; 1, the plasticity of the weights is increased. A similar modification applies to the update rules derived in the sequel. III. Vector Quantization for Noisy Communication Channels A.
Reference: [29] <author> J. M. Buhmann and T. Hofmann, </author> <title> "Robust vector quantization by competitive learning," </title> <booktitle> in Proceedings International Conference on Acoustics, Speech, and Signal Processing (ICASSP), </booktitle> <address> Munich, </address> <year> 1997. </year>
Reference-contexts: In addition, each neuron has an individual learning rate as in the noise-free case. IV. Robust Vector Quantization A. Prototype Elimination Model A second fundamental extension of the basic vector quantization model deals with random eliminations of codebook vectors and is called robust vector quantization <ref> [29] </ref>. The encoding/transmission scheme for robust vector quantization, including channel noise is depicted in Fig. 1. In this communication model the codebook design addresses the problem, that certain prototypes may not be available at encoding time t due to a temporary codebook reduction Y (t) Y.
Reference: [30] <author> T. Berger, </author> <title> Rate Distortion Theory, </title> <publisher> Prentice-Hall, Inc. </publisher> <address> Engle-wood Cliffs, New Jersey, </address> <year> 1971. </year>
Reference-contexts: In Section IV, a code vector elimination model was introduced which is of interest for robust encoding under rapidly varying bandwidth limitations. Here, we consider the problem of codebook design for a fixed rate R. This constraint requires a constructive approximation of the distortion-rate function <ref> [30] </ref>, [31]. Let us introduce association probabilities c iff , which are probabilistic variants of the encoding function, c iff denoting the probability of using y ff in reconstructing x i . <p> Here is a Lagrange parameter to enforce the rate constraint, and the mapping (R) : [0; 1] ! [0; R max ] is known to be monotonically decreasing <ref> [30] </ref>. Further minimization of H drf (Y; X ; R) w.r.t. Y results in the generalized centroid equations, c iff replacing the Kronecker delta in Eq. (3). For ! 0 the rate R achieves its maximum and the encoding probabilities approach the Boolean values of the nearest neighbor rule.
Reference: [31] <author> K. Rose, </author> <title> "A mapping approach to rate-distortion computation and analysis," </title> <journal> IEEE Trans Information Theory, </journal> <volume> vol. 40, no. 6, </volume> <pages> pp. 1939-1952, </pages> <year> 1994. </year>
Reference-contexts: In Section IV, a code vector elimination model was introduced which is of interest for robust encoding under rapidly varying bandwidth limitations. Here, we consider the problem of codebook design for a fixed rate R. This constraint requires a constructive approximation of the distortion-rate function [30], <ref> [31] </ref>. Let us introduce association probabilities c iff , which are probabilistic variants of the encoding function, c iff denoting the probability of using y ff in reconstructing x i . <p> The discretization of the reconstruction alphabet can also be justified in the case of continuous source alphabets, as shown rigorously in <ref> [31] </ref>. Besides non-negativity and normalization, C (R) is restricted to probabilistic encodings for which the mutual information between the optimal codeword prior distribution p and the encoding probabilities c does not to exceed the rate limit R.
Reference: [32] <author> R.E. Blahut, </author> <title> "Computation of channel cpacity and rate distortion function," </title> <journal> IEEE Trans Information Theory, </journal> <volume> vol. 18, </volume> <pages> pp. 460-473, </pages> <year> 1972. </year>
Reference-contexts: An efficient way to compute c (and p) for given Y is known as the Blahut-Arimoto algorithm <ref> [32] </ref>, [33], which is a special case of the alternating minimization of Csiszar and Tusnady [34]. The stationary equations c iff = P m 1 n X c iff ; (14) are solved by re-calculating c and p in alternation until convergence.
Reference: [33] <author> S. Arimoto, </author> <title> "An algorithm for calculating the capacity of an arbitrary discrete memoryless channel," </title> <journal> IEEE Trans Information Theory, </journal> <volume> vol. 18, </volume> <pages> pp. 14-20, </pages> <year> 1972. </year>
Reference-contexts: An efficient way to compute c (and p) for given Y is known as the Blahut-Arimoto algorithm [32], <ref> [33] </ref>, which is a special case of the alternating minimization of Csiszar and Tusnady [34]. The stationary equations c iff = P m 1 n X c iff ; (14) are solved by re-calculating c and p in alternation until convergence.
Reference: [34] <author> I. Csiszar and G. Tusnady, </author> <title> "Information geometry and alternating minimization procedures," </title> <journal> Statistics and Decisions, </journal> <volume> Supplement Issue 1, </volume> <pages> pp. 205-237, </pages> <year> 1984. </year>
Reference-contexts: An efficient way to compute c (and p) for given Y is known as the Blahut-Arimoto algorithm [32], [33], which is a special case of the alternating minimization of Csiszar and Tusnady <ref> [34] </ref>. The stationary equations c iff = P m 1 n X c iff ; (14) are solved by re-calculating c and p in alternation until convergence.
Reference: [35] <author> K. Rose, E. Gurewitz, and G. Fox, </author> <title> "Vector quantization by deterministic annealing," </title> <journal> IEEE Trans Information Theory, </journal> <volume> vol. 38, no. 4, </volume> <pages> pp. 1249-1257, </pages> <year> 1992. </year>
Reference-contexts: The resulting equations are equivalent to the ones obtained by applying the maximum entropy principle to the empirical risk in Eq. (1), following an optimization technique known as deterministic annealing (DA) [23], <ref> [35] </ref>, [16]. In the context of DA, the Lagrange parameter is identified with the computa tional temperature T . <p> by the objective function H uni (Y; X ) = ef1;:::;mg F e = i=1 q2P m X D (x i ; y ff ) -2e X q log q - :(23) F e has an interpretation in terms of statistical physics, where it is known as the free energy <ref> [35] </ref>, [16]. Hence H uni is an average of free energies F e over elimination events e. The T ! 0 truncated ME approximation is equivalent to restricting the minimization inside of F e to association probabilities with limited support, while the second approximation restricts the sum over events e. <p> DA is a very general heuristic optimization technique which has been applied to many different combinatorial optimization problems, including the problem of adaptive codebook design [23], <ref> [35] </ref>, [16]. Continuation methods like DA are optimization heuristics which have empirically shown to result in a satisfactory tradeoff between solution quality and algorithmic efficiency. However, due to the lack of theoretical insight about the geometry of solution curves, performance guarantees can not be given in the general case.
Reference: [36] <author> E.L. Allgower and K. Georg, </author> <title> Numerical Continuation Methods. An Introduction, </title> <booktitle> vol. 13 of Springer Series in Computational Mathematics, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin Heidelberg, </address> <year> 1990. </year>
Reference-contexts: VII. Continuation Methods and Deterministic Annealing There exists another important utilization of robust vector quantization as part of a continuation method <ref> [36] </ref>. The key idea in continuation methods is to optimize an objective function H by tracking solutions of a family of objective functions H a in the limit of a ! 0, where H 0 = H.
Reference: [37] <author> A. Blake and A. Zisserman, </author> <title> Visual Reconstruction, </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Continuation methods with this features are generalizations of a method known as graduated non convexity (GNC) in computer vision <ref> [37] </ref>. 6 Fig. 2. Performance for codebook design based on the first four frames of the `Miss America' sequence (354 fi 288 pixels).
Reference: [38] <author> S. Gadkari and K. Rose, </author> <title> "Noisy channel relaxation for VQ design," </title> <booktitle> in Proceedings International Conference on Acoustics, Speech, and Signal Processing (ICASSP), </booktitle> <year> 1996, </year> <pages> pp. 2048-2051. </pages>
Reference-contexts: More recently, this idea has been discussed under the title of noisy channel relaxation in the context of source-channel coding <ref> [38] </ref>.
Reference: [39] <author> J. Hopfield and D. Tank, </author> <title> "Neural computation of decisions in optimisation problems," </title> <journal> Biological Cybernetics, </journal> <volume> vol. 52, </volume> <pages> pp. 141-152, </pages> <year> 1985. </year>
Reference-contexts: sequence (354 fi 288 pixels). (a) PSNR in dB for robust (middle) and LBG (lower) batch codebook design (upper PSNR curve for noise-free DA added as a reference). (b) and (c) example frame encoded with a robust and LBG codebook, respectively. maximum entropy principle is known as deterministic annealing (DA) <ref> [39] </ref>, [23], which is a deterministic variant of simulated annealing (SA) [40]. With SA it shares the intuitive motivation from statistical physics to introduce a temperature scale T and to track solutions from high to low temperatures (T ! 0 limit).
Reference: [40] <author> S. Kirkpatrick, </author> <title> C.D. Gelatt, and M.P. Vecchi, "Optimization by simulated annealing," </title> <journal> Science, </journal> <volume> vol. 220, no. 4598, </volume> <pages> pp. 671-680, </pages> <year> 1983. </year>
Reference-contexts: (middle) and LBG (lower) batch codebook design (upper PSNR curve for noise-free DA added as a reference). (b) and (c) example frame encoded with a robust and LBG codebook, respectively. maximum entropy principle is known as deterministic annealing (DA) [39], [23], which is a deterministic variant of simulated annealing (SA) <ref> [40] </ref>. With SA it shares the intuitive motivation from statistical physics to introduce a temperature scale T and to track solutions from high to low temperatures (T ! 0 limit).
Reference: [41] <author> H. Klock, A. Polzer, and J. M. Buhmann, </author> <title> "Region-based motion compensated 3d-wavelet transform coding of video," </title> <booktitle> in Proceedings of the International Conference on Image Processing, </booktitle> <address> Santa Barbara, </address> <year> 1997. </year>
Reference-contexts: The wavelet transformation and the grouping scheme of 6 All compression experiments have been performed on single images. The bit rates are state-of-the-art for still compression but can be substantially improved with elaborate motion compensation <ref> [41] </ref>. 7 Fig. 4. Performance of on-line WTA learning on the first 50 frames of the `Salesman' sequence: (a) PSNR curve, (b) - (d) reconstructed example frames.
Reference: [42] <author> S. Mallat, </author> <title> "A theory for multidimensional signal decomposition: the wavelet representation," </title> <journal> IEEE Trans Pattern Analysis and Machine Intelligence, </journal> <volume> vol. 11, no. 7, </volume> <pages> pp. 674-693, </pages> <year> 1989. </year> <month> 10 </month>
Reference-contexts: The channel noise was a bit flip probability of p = 0:01. wavelet coefficients into blocks is performed according to Antonini et al. [3]. All image sequences have been preprocessed by a three-level biorthogonal wavelet transformation <ref> [42] </ref>. Data vectors were generated by grouping neighboring wavelet coefficients in blocks of size 4 fi 4 and 2 fi 2 for each subband separately.
References-found: 42

