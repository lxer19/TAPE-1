URL: http://www.llnl.gov/CASC/people/musick/papers/thesis.ps
Refering-URL: http://www.llnl.gov/CASC/people/musick/papers/thesis.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Belief Network Induction  
Author: by Charles Ronald Musick Jr. Professor Umesh V. Vazirani 
Degree: 1988 A dissertation submitted in partial satisfaction of the requirements for the degree of Doctor of Philosophy in Computer Science in the GRADUATE DIVISION of the UNIVERSITY of CALIFORNIA at BERKELEY Committee in charge: Professor Stuart J. Russell, Chair Professor John  
Date: 1994  
Affiliation: B.S. (University of Illinois at Urbana-Champaign)  Rice  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. Abramowitz and I. </author> <title> Stegun. Handbook of Mathematical Functions. </title> <publisher> Dover Publications, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: According to this, the midpoint of that bucket should be returned as the value for the corresponding discrete case. For example, the discrete values [0,1,2] map to [-2/3, 0, 2/3] with the mid-range approach, while the max-distance mapping gives <ref> [-1, 0, 1] </ref>. We have chosen to use the mid-range mapping. * Nominal: The difficulty with nominal values is that there is no sensible way to order them. <p> This dissertation deals mainly with probabilities as the random variables, thus insuring the intervals to be <ref> [0; 1] </ref>. <p> As the mean moves towards 0 or 1, the distribution looks like a normal that has been somewhat pushed on near the top. Conceptually, this makes sense because the beta is restricted to be within the interval <ref> [0; 1] </ref>, whereas the normal extends to infinity. If the mean is near the boundary, then much of the probability mass must also be near the boundary, but none of it may extend past it.
Reference: [2] <author> R. Alterman and M. Wentworth. </author> <title> Determining the important features of a case. </title> <booktitle> In Proceedings: Case-Based Reasoning Workshop. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1989. </year>
Reference: [3] <author> M. R. Anderberg. </author> <title> Cluster Analysis for Applications. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference: [4] <author> R. E. Bechhofer and D. M. Goldsman. </author> <title> Truncation of the Bechhofer-Kieffer-Sobel sequential procedure for selecting the normal population which has the largest mean. </title> <journal> Communications in Statistics: Simulation and Computation, </journal> <volume> 16(4) </volume> <pages> 1067-1092, </pages> <year> 1987. </year>
Reference: [5] <author> R. E. Bechhofer and D. M. Goldsman. </author> <title> Truncation of the Bechhofer-Kieffer-Sobel sequential procedure for selecting the normal population which has the largest mean (III): Supplementary trucation numbers and resulting performance characteristics. </title> <journal> Communications in Statistics: Simulation and Computation, </journal> <volume> 18(1) </volume> <pages> 63-81, </pages> <year> 1989. </year>
Reference: [6] <author> R. E. Bechhofer, J. Kiefer, and M. Sobel. </author> <title> Sequential Identification and Ranking Procedures. </title> <publisher> The University of Chicago Press, </publisher> <address> Chicago, Illinois, </address> <year> 1968. </year>
Reference: [7] <author> J. S. Breese. </author> <title> Construction of belief and decision networks. </title> <booktitle> Computational Intelligence, </booktitle> <year> 1992. </year>
Reference: [8] <author> J. S. Breese and E. J. Horvitz. </author> <title> Ideal reformulation of belief networks. </title> <booktitle> In Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 64-72, </pages> <year> 1990. </year>
Reference: [9] <author> L. Breiman. </author> <title> The Q method for estimating multivariate functions from noisy data. </title> <journal> Technometrics, </journal> <volume> 33(2) </volume> <pages> 125-160, </pages> <year> 1991. </year>
Reference-contexts: The DT proved to be a fast approach with results generally on par with the neural network. The algorithm used in the implementation can only deal with binary network variables. 132 5.3.2 Nonlinear Regression The non-linear regression algorithm that we used is Breiman's PIMPLE <ref> [9] </ref>, which tries to find clusters of spline functions that describe the input. This method combines forward selection and backward elimination techniques in model selection. Forward selection is a process of determining (or limiting) the maximum degree polynomial that will be used by the regression.
Reference: [10] <author> B. G. Buchanan and E. A. Feigenbaum. </author> <title> DENDRAL and Meta-DENDRAL: their applications dimension. </title> <journal> Artificial Intelligence, </journal> <volume> 11 </volume> <pages> 5-24, </pages> <year> 1978. </year>
Reference: [11] <author> W. Buntine. </author> <title> Theory refinement on Bayesian networks. </title> <booktitle> In Proceedings of the Seventh Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 52-60, </pages> <year> 1991. </year>
Reference: [12] <author> J. Catlett. </author> <title> Megainduction: machine learning on very large databases. </title> <type> PhD thesis, </type> <institution> University of Sydney, </institution> <address> Sydney, Australia, </address> <year> 1991. </year> <month> 173 </month>
Reference-contexts: A brief description of each implementation follows below. 5.3.1 Decision trees A decision tree algorithm <ref> [12, 78, 79] </ref> was implemented and tested. The construction is fairly simple. The variables in i become the attributes that the tree is built from. The values of the node X i are the classes that are being determined.
Reference: [13] <author> J. Catlett. Peepholing: </author> <title> choosing attributes efficiently for megainduction. </title> <booktitle> In Proceedings of the Ninth International Conference in Machine Learning, </booktitle> <pages> pages 49-54, </pages> <year> 1992. </year>
Reference: [14] <author> K. C. Chang and R. Fung. </author> <title> Refinement and coarsening of Bayesian networks. </title> <booktitle> In Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 475-482, </pages> <year> 1990. </year>
Reference: [15] <author> E. Charniak. </author> <title> Bayesian networks without tears. </title> <journal> AI Magazine, </journal> <volume> 12(4) </volume> <pages> 50-63, </pages> <year> 1991. </year>
Reference: [16] <author> R. M. Chavez and G. F. Cooper. </author> <title> An empirical evaluation of a randomized algorithm for probabilistic inference. </title> <editor> In M. Henrion, R. D. Shacter, and J. F. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 5. </booktitle> <publisher> Elsevier Science Publishers B. V., North-Holland, </publisher> <year> 1990. </year>
Reference: [17] <author> P. Cheeseman, J. Kelly, M. Self, J. Stutz, W. Taylor, and D. Freeman. </author> <note> AutoClass: </note>
References-found: 17

