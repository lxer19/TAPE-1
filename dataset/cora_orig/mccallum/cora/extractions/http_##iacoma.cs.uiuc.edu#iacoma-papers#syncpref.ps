URL: http://iacoma.cs.uiuc.edu/iacoma-papers/syncpref.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Email: Email: trancoso,torrella@csrd.uiuc.edu  
Title: The Impact of Speeding up Critical Sections with Data Prefetching and Forwarding 1  
Author: Pedro Trancoso and Josep Torrellas 
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract: While shared-memory multiprocessing offers a simple model for process synchronization, actual synchronization may be expensive. Indeed, processors may have to wait for a long time to acquire the lock of a critical section. In addition, a processor may have to stall for a long time waiting for all of its pending accesses to complete before releasing the lock. To address this problem, we target well-known optimization techniques to specifically speed-up accesses to critical sections. We reduce the time taken by critical sections by applying data prefetching and forwarding to minimize the number of misses inside these sections. In addition, we prefetch and forward data in exclusive mode to reduce the stall time before lock release. Our evaluation shows that a simple prefetching algorithm is able to speed-up parallel applications significantly at a very low cost. With this optimization, five Splash applications run 20% faster on average, while one of them runs 52% faster. We also conclude that more complicated, forward-based optimizations are not justified. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. L. Baer and T. F. Chen. </author> <title> An Effective On-Chip Preload-ing Scheme to Reduce Data Access Penalty. </title> <booktitle> In Proceedings of Supercomputing'91, </booktitle> <pages> pages 179-186, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: For this reason, we choose data prefetching and forwarding. Data prefetching involves loading the data into the cache ahead of the time of its use <ref> [1, 9, 13] </ref>. Forwarding involves sending the data to the cache of the future consumer so that by the time the consumer needs the data it can access it locally from its cache [6, 7, 10].
Reference: [2] <author> S. Goldschmidt. </author> <title> Simulation of Multiprocessors: Accuracy and Performance. </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: sparse matrix 1806 columns Mp3d Simulates a rarefied 10000 particles hypersonic flow 10 time steps Pthor Simulates a digital small RISC circuit at the logic level processor Water Simulates evolution of 343 molecules set of water molecules 2 time steps This architecture is simulated on a simulator based on TangoLite <ref> [2] </ref>, an execution-driven simulator. For this study, the memory simulator was enhanced with some extra functionalities like monitoring all the synchronization variables and introducing prefetching and forwarding directives. The workloads run are some of the Splash [11] applications.
Reference: [3] <author> J. R. Goodman, M. K. Vernon, and P. J. Woest. </author> <title> Efficient Synchronization Primitives for Large-scale Cache-coherent Multiprocessors. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 64-73, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: One possible approach to speed up critical sections is to provide hardware support to make the acquire and release operations very fast <ref> [3] </ref>. This approach, however, may not be the most cost-effective one.
Reference: [4] <author> Alain Kagi, Nagi Aboulenein, Douglas C. Burger, and James R. Goodman. </author> <title> Techniques for Reducing Overheads of Shared-Memory Multiprocessing. </title> <booktitle> In Proceedings of the 1995 International Conference on Supercomputing, </booktitle> <pages> pages 11-20, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Then, when a processor acquires the lock, the protected data is made automatically available to the processor. This general approach was used by the KSR-1 designers [5] and by Kagi et al <ref> [4] </ref>. One difference between these two works is the granularity, which is sub-page (128 bytes) for the KSR-1 and cache line in the second case. <p> The architectural support required by the forwarding operation defined here is a bit more complex. Indeed, there is the need to implement a per-lock queue where the processors waiting on the lock are inserted in FIFO order. This queue can be implemented in hardware <ref> [4] </ref> or software [8]. Then, we need to add a forwarding instruction, buffer some state for each pending forward until the forward transaction is completed, modify the cache coherence protocol to support forwarding transactions, and modify the caches to accept lines that have not been requested by the processor.
Reference: [5] <institution> Kendall Square Research. </institution> <type> KSR1 Technical Summary. </type> <address> Waltham, MA, </address> <year> 1992. </year>
Reference-contexts: A more promising approach to speed up critical sections is to cluster the lock with the data that the lock protects. Then, when a processor acquires the lock, the protected data is made automatically available to the processor. This general approach was used by the KSR-1 designers <ref> [5] </ref> and by Kagi et al [4]. One difference between these two works is the granularity, which is sub-page (128 bytes) for the KSR-1 and cache line in the second case.
Reference: [6] <author> D. A. Koufaty, X. Chen, D. K. Poulsen, and J. Torrellas. </author> <title> Data Forwarding in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1995 International Conference on Supercomputing, </booktitle> <pages> pages 255-264, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Data prefetching involves loading the data into the cache ahead of the time of its use [1, 9, 13]. Forwarding involves sending the data to the cache of the future consumer so that by the time the consumer needs the data it can access it locally from its cache <ref> [6, 7, 10] </ref>. While these techniques have been studied by many researchers, our contribution in this paper is to apply them efficiently and systematically to critical sections. <p> The algorithms presented are very simple and could be refined in many ways. For example, the forwards could be performed at the same time as the last writes to the variables, like it was proposed in <ref> [6, 10] </ref>, thereby saving cycles. Similarly, the prefetch statements could be scheduled better, specially in the context of variables accessed via pointer dereferenc-ing. Finally, our algorithms as they are now prefetch or forward only one iteration of a loop. <p> Many optimizations to this base scheme are possible. For example, we could use Poulsen and Yew's [10] and Koufaty et al's <ref> [6] </ref> combined write-and-forward instruction. 3 Experimental Setup In this work we simulate a cache-coherent shared-memory machine (CC-NUMA) with 32 nodes. Each node includes a 4-Kbyte first-level write-through cache and a 64-Kbyte second-level write-back cache. Both caches have 4-word cache lines and are direct-mapped.
Reference: [7] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford Dash Multiprocessor. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Data prefetching involves loading the data into the cache ahead of the time of its use [1, 9, 13]. Forwarding involves sending the data to the cache of the future consumer so that by the time the consumer needs the data it can access it locally from its cache <ref> [6, 7, 10] </ref>. While these techniques have been studied by many researchers, our contribution in this paper is to apply them efficiently and systematically to critical sections.
Reference: [8] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: The architectural support required by the forwarding operation defined here is a bit more complex. Indeed, there is the need to implement a per-lock queue where the processors waiting on the lock are inserted in FIFO order. This queue can be implemented in hardware [4] or software <ref> [8] </ref>. Then, we need to add a forwarding instruction, buffer some state for each pending forward until the forward transaction is completed, modify the cache coherence protocol to support forwarding transactions, and modify the caches to accept lines that have not been requested by the processor.
Reference: [9] <author> Todd Mowry and Anoop Gupta. </author> <title> Tolerating Latency Through Software-Controlled Prefetching in Shared-Memory Multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 87-106, </pages> <year> 1991. </year>
Reference-contexts: For this reason, we choose data prefetching and forwarding. Data prefetching involves loading the data into the cache ahead of the time of its use <ref> [1, 9, 13] </ref>. Forwarding involves sending the data to the cache of the future consumer so that by the time the consumer needs the data it can access it locally from its cache [6, 7, 10]. <p> When the data is read in, all sharers are invalidated; when the data is finally written, there is no need to send invalidations. This idea can be combined with prefetching (which is then called exclusive prefetching <ref> [9] </ref>) or with forwarding (which we call exclusive forwarding).
Reference: [10] <author> David K. Poulsen and Pen-Chung Yew. </author> <title> Data Prefetch-ing and Data Forwarding in Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1994 International Conference on Parallel Processing, </booktitle> <volume> volume II, </volume> <pages> pages 276-280, </pages> <month> August </month> <year> 1994. </year> <note> Also available as CSRD tech report No. 1330. </note>
Reference-contexts: Data prefetching involves loading the data into the cache ahead of the time of its use [1, 9, 13]. Forwarding involves sending the data to the cache of the future consumer so that by the time the consumer needs the data it can access it locally from its cache <ref> [6, 7, 10] </ref>. While these techniques have been studied by many researchers, our contribution in this paper is to apply them efficiently and systematically to critical sections. <p> The algorithms presented are very simple and could be refined in many ways. For example, the forwards could be performed at the same time as the last writes to the variables, like it was proposed in <ref> [6, 10] </ref>, thereby saving cycles. Similarly, the prefetch statements could be scheduled better, specially in the context of variables accessed via pointer dereferenc-ing. Finally, our algorithms as they are now prefetch or forward only one iteration of a loop. <p> Many optimizations to this base scheme are possible. For example, we could use Poulsen and Yew's <ref> [10] </ref> and Koufaty et al's [6] combined write-and-forward instruction. 3 Experimental Setup In this work we simulate a cache-coherent shared-memory machine (CC-NUMA) with 32 nodes. Each node includes a 4-Kbyte first-level write-through cache and a 64-Kbyte second-level write-back cache. Both caches have 4-word cache lines and are direct-mapped.
Reference: [11] <author> J. P. Singh, W. Weber, and A. Gupta. </author> <title> SPLASH: Stan-ford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: This occurs, for example, when the ordinary accesses suffer many misses. In fact, misses in the ordinary accesses of the critical section are likely: for instance, 40-85% of the total data read misses in the second-level cache for the five Splash applications <ref> [11] </ref> considered in this paper occur in critical sections. A more promising approach to speed up critical sections is to cluster the lock with the data that the lock protects. Then, when a processor acquires the lock, the protected data is made automatically available to the processor. <p> What motivates the interest in this issue is that many Splash <ref> [11] </ref> and other parallel applications have a signifi-cant synchronization overhead. For example, for the five Splash applications used in this study, we measured an average synchronization time of 24% of the total execution time (Section 4.4). Speeding up critical sections may therefore have a large performance impact. <p> For this study, the memory simulator was enhanced with some extra functionalities like monitoring all the synchronization variables and introducing prefetching and forwarding directives. The workloads run are some of the Splash <ref> [11] </ref> applications. A brief description of the applications and the problem sizes used are presented in Table 1. We run the applications with 32 processors.
Reference: [12] <author> Pedro P. M. Trancoso. </author> <title> Performance Optimization Based on Characterizing Synchronization. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: In the following sections, we first describe the optimizations, then present the algorithms, and finally discuss the required architectural support. A more detailed discussion can be found in <ref> [12] </ref>. 2.1 Description of the Optimizations The first optimization attempts to reduce to one the number of accesses to main memory seen by the processor in the critical section. This is done by prefetching right after the acquire all the shared variables that are read in the critical section.
Reference: [13] <author> Zheng Zhang and Josep Torrellas. </author> <title> Speeding up Irregular Applications in Shared-Memory Multiprocessors: Memory Binding and Group Prefetching. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 188-199, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: For this reason, we choose data prefetching and forwarding. Data prefetching involves loading the data into the cache ahead of the time of its use <ref> [1, 9, 13] </ref>. Forwarding involves sending the data to the cache of the future consumer so that by the time the consumer needs the data it can access it locally from its cache [6, 7, 10].
References-found: 13

