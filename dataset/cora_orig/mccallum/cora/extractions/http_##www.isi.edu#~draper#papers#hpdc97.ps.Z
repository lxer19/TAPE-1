URL: http://www.isi.edu/~draper/papers/hpdc97.ps.Z
Refering-URL: http://www.isi.edu/~draper/papers/papers.html
Root-URL: http://www.isi.edu
Title: Packaging-Driven Scalable Systems multicomputer (PDSS) project uses several innovative interconnect and routing techniques to construct
Note: The  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. C. Athas and J. G. Koller, </author> <title> The Architecture and Program ming of the ISI Embeddable Variant Multicomputer, </title> <booktitle> Proceedings of the Scalable High-Performance Computing Conference </booktitle>
Reference: [2] <author> J. G. Koller, et al., </author> <title> Lessons from Three Generations of Embeddable Supercomputers, </title> <booktitle> Proceedings of the Embeddable High-Performance Computing Workshop, IPPS, </booktitle> <month> April </month> <year> 1997. </year>
Reference: [3] <author> J. Draper, </author> <title> The Red Rover algorithm for deadlock-free routing on bidirectional rings, </title> <booktitle> Proceedings of the International Conference on Parallel and Distributed Processing Techniques and Applications </booktitle>
Reference-contexts: It provides 2 virtual channels [4] per physical channel and uses the Red Rover routing algorithm <ref> [3] </ref> so that either a ring or a strip may be constructed. Although the router architecture is optimized for use in 1D networks, routers may be cascaded to build higher-dimension networks.
Reference: [4] <author> W. J. Dally, </author> <title> Virtual-channel ow control, </title> <journal> IEEE Transactions on Parallel and Distributed Systems </journal>
Reference-contexts: The architectural challenge is to make commu nication efficient in such a 1D wiring pattern. 2.1 Network Architecture The PDSS router is optimized to achieve high bandwidth and low latencies in transferring 40B (8B header, 32B data) packets on a 1D network. It provides 2 virtual channels <ref> [4] </ref> per physical channel and uses the Red Rover routing algorithm [3] so that either a ring or a strip may be constructed. Although the router architecture is optimized for use in 1D networks, routers may be cascaded to build higher-dimension networks.
Reference: [5] <author> R. W. Brodersen, et al, </author> <title> Anatomy of a Silicon Compiler wer, </title> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: The design philosophy was to view the chip as a collection of semi-autonomous but interacting logic units, most of which control corresponding datapath elements connected by a core bus. This separation of control units and datapath elements facilitated an efficient use of layout tools, namely, Lager <ref> [5] </ref> for synthesizing control blocks and Magic [6] for constructing datapath elements and interconnect.
Reference: [6] <author> R. N. Mayo et al., </author> <note> 1990 DEC WRL/Livermore Magic Release. WRL Research Report 90/7, </note> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: This separation of control units and datapath elements facilitated an efficient use of layout tools, namely, Lager [5] for synthesizing control blocks and Magic <ref> [6] </ref> for constructing datapath elements and interconnect.
Reference: [7] <author> L. M. Ni and P. K. McKinley, </author> <title> A survey of wormhole routing techniques in direct networks, </title> <booktitle> Computer 1993. </booktitle>
Reference-contexts: to the request before it is overwritten by a competing request, but the protocol outline here provides a natural ow control mechanism for blocking and serializing multiple requests without expansive resource demands on the request target node. 3.8 Deadlock Avoidance In general, deadlock is not a problem for wormhole-routed networks <ref> [7] </ref>, since local ow-control suppresses production of new messages. Congested networks will progress, provided that receiving nodes consume messages. Remote reads allow deadlock because they do not reduce the total number of messages in the network. A remote-read-request packet is converted into a remote-read-reply packet at the remote node.
Reference: [8] <author> B. Nitzberg and V. Lo, </author> <title> Distributed Shared Memory: A Survey of Issues and Algorithms, </title> <note> IEEE Computer , August 1991. </note>
Reference-contexts: The T3E uses addresses to specify an operation, but puts target addresses on the data bus for the initiating bus cycle. Our partially coherent buffer protocol has similarities to uses of release consistency <ref> [8] </ref>.
Reference: [9] <author> Daniel Lenoski et al., </author> <title> The Stanford DASH multiprocessor, </title> <journal> IEEE Computer , pp. </journal> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Our partially coherent buffer protocol has similarities to uses of release consistency [8]. Our use of explicit software initiation of transfers is fundamentally distinct from the implementation of shared-memory systems such as DASH <ref> [9] </ref>, but is an interesting comparison with the FLASH MAGIC [10][11]architecture, which uses a custom RISC coprocessor to provide programmable network protocols, in contrast to our use of some of the resources of a standard superscalar processor.
Reference: [10] <author> J. Heinlein et al., </author> <title> Integration of message passing and shared memory in the Stanford FLASH multiprocessor, </title> <booktitle> Proceedings of the Sixth Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS VI) </booktitle>
Reference: [11] <author> Jeffrey Kuskin, et al., </author> <title> The Stanford FLASH Multiprocessor, </title> <booktitle> Proceedings of the 21st International Symposium on Com puter Architecture , pp. </booktitle> <pages> 302-313, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference: [12] <author> John Heinlein, et al. </author> <title> Coherent Block Data Transfer in the FLASH Multiprocessor. </title> <booktitle> To appear in Proceedings of the 11th International Parallel Processing Symposium , Geneva, </booktitle> <address> Swit zerland, </address> <month> April </month> <year> 1997. </year>
Reference-contexts: Despite the formidable design effort, the programmable coprocessor is not necessarily superior to primary processor performance <ref> [12] </ref> even on memory-to-memory operations. FLASH, being access-triggered, tries to put enough intelligence in the secondary processor to anticipate the primary processors data references. We let the superscalar primary processor do that itself.
Reference: [13] <author> D. W. Walker, </author> <title> The Design of a Standard Message-Passing Interface for Distributed Memory Concurrent Computers, </title> <journal> Parallel Computing , Vol. </journal> <volume> 20, No. 4, </volume> <pages> pp. 657-673, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The bulk of the communications software is in unprivileged user-mode library code. Programs can be written to either this native network programming interface, which offers direct access to the network buffers and lowest latency, or to a version of the MPI message-passing library <ref> [13] </ref>. Not all communications functions can be performed by unprivileged software. In particular, messages destined for processes other than the current one, or requests for initial allocation of node resources, such as SRAM communication buffers or DRAM, need to be executed by privileged code, which composes a minimal communications-oriented kernel.
Reference: [14] <author> Richard C. Metzger, et al., </author> <title> The C3I Parallel Benchmark Suite Introduction And Preliminary Results, </title> <booktitle> Proceedings of Supercomputing 96 </booktitle>
Reference: [15] <author> Matthias A. Blumrich, et al., </author> <title> Virtual Memory Mapped Net work Interface for the SHRIMP Multicomputer, </title> <booktitle> Proceedings of Int'l Symposium on Computer Architecture April 1994. </booktitle>
Reference-contexts: Overall, our much simpler interface architecture is comparable or superior to the T3E for basic operations, but doesnt support the large global memory of the T3E due to our use of a 32b-address-space processor architecture. The general notion of attaching specialized semantic significance to memory regions is not uncommon <ref> [15] </ref>, but we are not aware of prior use of densely encoded address-only cycles as operation triggers. The T3E uses addresses to specify an operation, but puts target addresses on the data bus for the initiating bus cycle.
Reference: [16] <author> Cray Research, </author> <title> Cray T3D System Architecture Overview, </title> <address> HR-04033, </address> <year> 1993. </year>
Reference-contexts: A remote-read-request packet is converted into a remote-read-reply packet at the remote node. This poses a rare but real possibility of deadlocking the network. Rather than dedicating network hardware resources to a distinct reply channel <ref> [16] </ref>, we allow the receiving network interface to drop remote-read requests. The interface retains sufficient information about outstanding incomplete remote-read requests to allow the requesting node OS to reconstruct the request. <p> A comparable published design is the get/put programmed-I/O interface of the Cray Research T3D <ref> [16] </ref>. The underlying philosophy of using powerful microprocessors to directly control relatively fine-grained data transfers is the same, but our buffer set is a more regular and extensible interface feature than the T3Ds single-word FIFOs, and our use of cache-coherence protocols is better integrated.
Reference: [17] <author> S. L. Scott, </author> <title> Synchronization and Communication in the T3E Multiprocessor, </title> <booktitle> Proceedings of ASPLOS VII , Cambridge, </booktitle> <address> MA, </address> <month> October </month> <year> 1996 </year>
Reference-contexts: Our barrier synchronization mechanism can support one 64-process barrier per node, and has no placement constraints. It does not support other global operations, such as reductions, as does the T3D, since we believe that our general-purpose network is adequate for that purpose. The Cray T3E <ref> [17] </ref> is quite directly comparable to PDSS. It appears that Cray Research drew some of the same conclusions as we did in the design of the T3E multiprocessor, in particular the use of two banks (512 user, 128 system) of 64b E-registers as source or destination for all remote communications.
References-found: 17

