URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/tcm/www/tcm_papers/inf_load_TR95.ps.Z
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/user/tcm/www/Papers.html
Root-URL: 
Title: INFORMING LOADS: ENABLING SOFTWARE TO OBSERVE AND REACT TO MEMORY BEHAVIOR  
Author: Mark Horowitz Margaret Martonosi Todd C. Mowry Michael D. Smith 
Date: July 1995  
Note: (also numbered STAN-CS-95-673)  
Pubnum: Technical Report No. CSL-TR-95-673  
Abstract: This research has been supported by ARPA contract DABT63-94-C-0054. In addition, Margaret Martonosi is supported in part by a National Science Foundation Career Award (CCR-9502516). Todd C. Mowry is supported by a Research Grant from the Natural Sciences and Engineering Research Council of Canada. Michael D. Smith is supported by the National Science Foundation under a Young Investigator Grant No. CCR-9457779. 
Abstract-found: 1
Intro-found: 1
Reference: [ASKL79] <author> W. Abu-Sufah, D. J. Kuck, and D. H. Lawrie. </author> <title> Automatic program transformations for virtual memory computers. </title> <booktitle> Proc. of the 1979 National Computer Conference, </booktitle> <pages> pages 969974, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: These techniques include compiler optimization like blocking <ref> [ASKL79, GJMS87, MC69, WL91, GL89] </ref> and software-controlled prefetching [Mow94, Por89, CMCH91] and the operating system optimizations like page coloring [KH92, BLRC94] and page migration [CDV + 94, LE91, CF89, BFS89].
Reference: [BFS89] <author> William J. Bolosky, Robert P. Fitzgerald, and Michael L. Scott. </author> <title> Simple but effective techniques for NUMA memory management. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating System Principles, pages 1931, </booktitle> <year> 1989. </year>
Reference-contexts: These techniques include compiler optimization like blocking [ASKL79, GJMS87, MC69, WL91, GL89] and software-controlled prefetching [Mow94, Por89, CMCH91] and the operating system optimizations like page coloring [KH92, BLRC94] and page migration <ref> [CDV + 94, LE91, CF89, BFS89] </ref>. Without informing loads, the success of these automatic techniques depend heavily on how well the compiler can predict caching behavior ahead of time.
Reference: [BLRC94] <author> B. N. Bershad, D. Lee, T. H. Romer, and J. B. Chen. </author> <title> Avoiding conflict misses dynamically in large direct-mapped caches. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 158170, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: data about memory system behavior [Mar93, LW94]; compiler optimizations are driven by static guesses rather than dynamic observations of caching behavior [Mow94]; and for some page migration and mapping optimizations, operating systems are turning to specialized add-on hardware support such as bus-based hardware monitors [CDV+94] and cache miss lookaside buffers <ref> [BLRC94] </ref>. The fundamental problem is that load instructions were defined when memory hierarchies were at, and memory latency was not a prime concern. The model they present is one of a uniform high-speed memory. <p> For example, on-chip hardware miss counters have been implemented to offer at least minimal support to compilers and performance monitors [DEC92], while off-chip hardware like the cache miss lookaside buffer has been proposed to support operating systems activities like page coloring <ref> [BLRC94] </ref>. Neither of these approaches is general or efficient enough to support the needs of both the compiler and the operating system. <p> These techniques include compiler optimization like blocking [ASKL79, GJMS87, MC69, WL91, GL89] and software-controlled prefetching [Mow94, Por89, CMCH91] and the operating system optimizations like page coloring <ref> [KH92, BLRC94] </ref> and page migration [CDV + 94, LE91, CF89, BFS89]. Without informing loads, the success of these automatic techniques depend heavily on how well the compiler can predict caching behavior ahead of time.
Reference: [BM89] <author> Helmar Burkhart and Roland Millen. </author> <title> Performance-Measurement Tools in a Multiprocessor Environment. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(5):725737, </volume> <month> May </month> <year> 1989. </year> <note> [CDV 94] R. </note> <author> Chandra, S. Devine, B. Verghese, A. Gupta, and M. Rosenblum. </author> <title> Scheduling and page migration for multiprocessor compute servers. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 1224, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: In the following two sections, we examine and evaluate implementations of several of these techniques using informing loads. 3.0 Monitoring Program Performance A number of performance tools have been proposed to monitor program caching behavior <ref> [BM89, GH93, LW94, Mar93] </ref>. One of the major stumbling-blocks in building such tools is gathering appropriately detailed memory statistics with low runtime overheads and minimal perturbations of the monitored program. <p> Loop-level statistics will report this as a problem with the entire loop, rather than pinpointing the bottleneck to a particular data structure or reference point. For finer-grained memory statistics, other tools rely on dedicated hardware to monitor memory references. Burkhart, et al. <ref> [BM89] </ref> and others have implemented tools based on data collected by special hardware bus monitors. These approaches are increasingly difficult due to the levels of integration in modern processors. With first-level and perhaps second-level caches on-chip, cache performance monitoring warrants integrated processor support.
Reference: [CF89] <author> Alan L. Cox and Robert J. Fowler. </author> <title> The implementation of a coherent memory abstraction on a NUMA multiprocessor: Experiences with PLATINUM. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 3244, </pages> <year> 1989. </year>
Reference-contexts: These techniques include compiler optimization like blocking [ASKL79, GJMS87, MC69, WL91, GL89] and software-controlled prefetching [Mow94, Por89, CMCH91] and the operating system optimizations like page coloring [KH92, BLRC94] and page migration <ref> [CDV + 94, LE91, CF89, BFS89] </ref>. Without informing loads, the success of these automatic techniques depend heavily on how well the compiler can predict caching behavior ahead of time.
Reference: [CMCH91] <author> W. Y. Chen, S. A. Mahlke, P. P. Chang, and W. W. Hwu. </author> <title> Data access microarchitectures for superscalar processors with compiler-assisted data prefetching. </title> <booktitle> In Proceedings of Microcomputing 24, </booktitle> <year> 1991. </year>
Reference-contexts: These techniques include compiler optimization like blocking [ASKL79, GJMS87, MC69, WL91, GL89] and software-controlled prefetching <ref> [Mow94, Por89, CMCH91] </ref> and the operating system optimizations like page coloring [KH92, BLRC94] and page migration [CDV + 94, LE91, CF89, BFS89]. Without informing loads, the success of these automatic techniques depend heavily on how well the compiler can predict caching behavior ahead of time.
Reference: [CWN92] <author> Richard Comerford, George F. Watson, and Ray Ng. </author> <title> Special Report: Memory. </title> <journal> IEEE Spectrum, </journal> <volume> 29(10):3457, </volume> <month> October </month> <year> 1992. </year>
Reference-contexts: In current uniprocessor machines, a reference to main memory takes on the order of 50 processor cycles [DEK + 92], while in shared-memory multiprocessors, latencies to remote memory can be three to four times larger [KOH + 94]. In the future, these latencies are expected to increase even further <ref> [CWN92] </ref>. To cope with memory latency, most computer systems today rely on their cache hierarchy to reduce the effective memory access time. While caches are an important step toward addressing this problem, neither they nor other purely hardware-based mechanisms (e.g., stream buffers [Jou90]) are complete solutions.
Reference: [DEC92] <author> DEC. </author> <title> DECChip 21064 RISC Microprocessor Preliminary Data Sheet. </title> <type> Technical report, </type> <year> 1992. </year> <note> [DEK 92] Todd Dutton, </note> <author> Daniel Eiref, Hugh Kurth, James Reisert, and Robin Stewart. </author> <title> The Design of the DEC 3000 AXP Systems, Two High-performance Workstations. </title> <journal> Digital Technical Journal, </journal> <volume> 4(4):6681, </volume> <year> 1992. </year>
Reference-contexts: Overall, a number of disjoint and specialized solutions have been proposed for different parts of the problem. For example, on-chip hardware miss counters have been implemented to offer at least minimal support to compilers and performance monitors <ref> [DEC92] </ref>, while off-chip hardware like the cache miss lookaside buffer has been proposed to support operating systems activities like page coloring [BLRC94]. Neither of these approaches is general or efficient enough to support the needs of both the compiler and the operating system. <p> Another attempt to address this problem is the inclusion of various forms of hardware cache-miss counters. For example, the Alpha 21064 includes a performance counter that can be configured to cause an interrupt after either 256 or 4096 cache misses have occurred <ref> [DEC92] </ref>. However, because of the overhead and cache perturbation of handling a full interrupt, these techniques are least intrusive with the larger count. <p> With first-level and perhaps second-level caches on-chip, cache performance monitoring warrants integrated processor support. Some techniques monitor memory behavior by trapping on particular accesses and simulating them [RHL+93], or by trapping based on values in sampled hardware miss counters <ref> [DEC92] </ref>. Such trap-based techniques incur overheads of 200 cycles or more on each trapped event just to get to monitoring code. In addition, the cache perturbation due to trap handling can be significant. In contrast, informing loads will have trap costs of 0 to 5 cycles, depending on their implementation.
Reference: [Dix92] <author> Kaivalya M. Dixit. </author> <title> New CPU Benchmark Suites from SPEC. </title> <booktitle> In Proc. COMPCON, </booktitle> <month> Spring </month> <year> 1992. </year> <title> [D 92] Dan Dobberpuhl et al. A 100MHz 64b dual-issue CMOS Microprocessor. </title> <booktitle> In International Solid State Circuits Conference Digest of Technical Papers, </booktitle> <pages> pages 106107, </pages> <month> Feb </month> <year> 1992. </year>
Reference: [Fis81] <author> Josh Fisher. </author> <title> Trace scheduling: A technique for global microcode compaction. </title> <journal> IEEE Transactions on Computer, </journal> <volume> C-30(7):478490, </volume> <month> July </month> <year> 1981. </year>
Reference-contexts: Compilers have historically used control-ow feedback (also known as branch profiling) to perform aggressive instruction scheduling across branches <ref> [Fis81, Smi92] </ref>. Given that informing loads make it practical to collect accurate per-reference miss rates across entire applications (as demonstrated earlier in Section 3.0), a similar feedback methodology can be used to enhance aggressive memory optimizations, such as software-controlled prefetching.
Reference: [GH93] <author> Aaron J. Goldberg and John L. Hennessy. </author> <title> Mtool: An Integrated System for Performance Debugging Shared Memory Multiprocessor Applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <pages> pages 2840, </pages> <month> January </month> <year> 1993. </year> <month> 23 </month>
Reference-contexts: While these techniques are successful in many cases, they are handicapped by the fact that software cannot directly observe the behavior of the memory system. Because of this basic limitation, monitoring tools are forced either to measure memory overhead indirectly (often at a coarser granularity than desirable) <ref> [GH93] </ref> or to rely on simulated data about memory system behavior [Mar93, LW94]; compiler optimizations are driven by static guesses rather than dynamic observations of caching behavior [Mow94]; and for some page migration and mapping optimizations, operating systems are turning to specialized add-on hardware support such as bus-based hardware monitors [CDV+94] <p> More recently, machines have offered high-resolution timers to programmers. While these timers support loop-level monitoring of code, as in Mtool <ref> [GH93] </ref>, they still have too high an overhead to be used to time and record individual memory reference latencies. Another attempt to address this problem is the inclusion of various forms of hardware cache-miss counters. <p> In the following two sections, we examine and evaluate implementations of several of these techniques using informing loads. 3.0 Monitoring Program Performance A number of performance tools have been proposed to monitor program caching behavior <ref> [BM89, GH93, LW94, Mar93] </ref>. One of the major stumbling-blocks in building such tools is gathering appropriately detailed memory statistics with low runtime overheads and minimal perturbations of the monitored program. <p> With the N=100 sampling implementation, overheads drop to a range from 0.6% for grep to 25% for compress. 9 For comparison, Mtool produces much less detailed statistics, but in spite of this still reports overheads in the range of 3% to 15% (for a subset of these applications) <ref> [GH93] </ref>.
Reference: [GJMS87] <author> K. Gallivan, W. Jalby, U. Meier, and A. Sameh. </author> <title> The impact of hierarchical memory systems on linear algebra algorithm design. </title> <type> Technical Report UIUCSRD 625, </type> <institution> University of Illinios, </institution> <year> 1987. </year>
Reference-contexts: These techniques include compiler optimization like blocking <ref> [ASKL79, GJMS87, MC69, WL91, GL89] </ref> and software-controlled prefetching [Mow94, Por89, CMCH91] and the operating system optimizations like page coloring [KH92, BLRC94] and page migration [CDV + 94, LE91, CF89, BFS89].
Reference: [GL89] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: These techniques include compiler optimization like blocking <ref> [ASKL79, GJMS87, MC69, WL91, GL89] </ref> and software-controlled prefetching [Mow94, Por89, CMCH91] and the operating system optimizations like page coloring [KH92, BLRC94] and page migration [CDV + 94, LE91, CF89, BFS89].
Reference: [Gwe94] <author> Linley Gwennap. </author> <title> 620 fills out PowerPC product line. </title> <type> Microprocessor Report, </type> <institution> 8(14):1216, </institution> <month> Oct </month> <year> 1994. </year>
Reference-contexts: The other stall model allows the machine to continue to operate until the missing data is referenced. This option, stall on use, is becoming more common <ref> [D + 92, Gwe94] </ref> because of its potential performance benefits. The principle cost of this alternative stall model is that the cache must be able to handle references while the miss is being serviced. This requires a lockup-free cachei.e. a cache that can handle requests while a miss is outstanding.
Reference: [HP90] <author> John Hennessy and David Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: To demonstrate this fact, this section describes how to implement an informing load instruction in a number of different machine pipelines. We begin by describing a simple, single-issue RISC pipeline that is a simplification on the MIPS R3000 pipeline <ref> [HP90] </ref>. After reviewing this basic pipeline and machine organization, we discuss the changes needed to support informing loads. We first look at several implementations of the informing load functionality in this simple machine, and we then move our description to implementations for more complex 6. <p> All instructions complete in the WB phase of the pipeline and can be squashed (or turned into a null operation) by asserting a suitable signal before WB. Squashing of instructions in the pipeline is necessary to support precise exceptions <ref> [HP90] </ref>. In a normal load operation, the pipeline calculates the effective address of the load during EX and accesses the cache using this address during ME.
Reference: [HP92] <author> HP. </author> <title> PA-RISC 1.1 Architecture, Instruction Set Reference Manual. </title> <institution> Hewlett Packard, </institution> <year> 1992. </year>
Reference-contexts: load in the sequential program. (In a single-issue RISC machine, this would be the instruction in the delay slot of the informing load instruction.) In some sense, an informing load is akin to the squashing branches in the SPARC architecture [Pau94] or the nullifying operations in the HP PA-RISC architecture <ref> [HP92] </ref>. An informing load operation squashes its delay slot instruction if the load hits in the cache. Informing loads are non-blocking loads so that the software can invoke an alternate action during the processing of an informing load that misses in the cache.
Reference: [Jou90] <author> Norm Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers. </title> <booktitle> In Proc. 17th Annual Intl. Symposium on Computer Architecture, </booktitle> <pages> pages 364373, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: To cope with memory latency, most computer systems today rely on their cache hierarchy to reduce the effective memory access time. While caches are an important step toward addressing this problem, neither they nor other purely hardware-based mechanisms (e.g., stream buffers <ref> [Jou90] </ref>) are complete solutions. In addition to hardware mechanisms, a number of software techniques have been proposed for avoiding or tolerating memory latency. For example, performance monitoring tools attempt to measure where memory bottlenecks lie and give indications of how programmers could fix them.
Reference: [KH92] <author> R. E. Kessler and Mark D. Hill. </author> <title> Page placement algorithms for large real-index caches. </title> <journal> ACM TOCS, </journal> <note> 10(4):338359, 1992. + 94] Jeff Kuskin, </note> <author> Dave Ofelt, Mark Heinrich, et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proc. 21st Annual Intl. Symposium on Computer Architecture, </booktitle> <pages> pages 302313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: These techniques include compiler optimization like blocking [ASKL79, GJMS87, MC69, WL91, GL89] and software-controlled prefetching [Mow94, Por89, CMCH91] and the operating system optimizations like page coloring <ref> [KH92, BLRC94] </ref> and page migration [CDV + 94, LE91, CF89, BFS89]. Without informing loads, the success of these automatic techniques depend heavily on how well the compiler can predict caching behavior ahead of time.
Reference: [Kro81] <author> David Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> In Proceedings of the 8th Symposium on Computer Architecture, </booktitle> <pages> pages 8187, </pages> <year> 1981. </year>
Reference: [Lau94] <author> James Laudon. </author> <title> Architectural and Implementation Tradeoffs for Multiple-Context Processors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1994. </year>
Reference: [LE91] <author> Richard P. Jr. LaRowe and Carla Schlatter Ellis. </author> <title> Experimental comparison of memory management policies for NUMA multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(4):319363, </volume> <month> November </month> <year> 1991. </year>
Reference-contexts: These techniques include compiler optimization like blocking [ASKL79, GJMS87, MC69, WL91, GL89] and software-controlled prefetching [Mow94, Por89, CMCH91] and the operating system optimizations like page coloring [KH92, BLRC94] and page migration <ref> [CDV + 94, LE91, CF89, BFS89] </ref>. Without informing loads, the success of these automatic techniques depend heavily on how well the compiler can predict caching behavior ahead of time.
Reference: [LGH94] <author> James Laudon, Anoop Gupta, and Mark Horowitz. </author> <title> Interleaving: A Multithreading Technique Targeting Multiprocessors and Workstations. </title> <booktitle> In Sixth Intl. Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 308318, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The low overhead of an informing load instruction is coupled with an instantaneous notification to the currently executing thread. (Other latency-hiding techniques often force a light-weight context switch on a cache miss <ref> [LGH94] </ref>.) Of course, the notification of a cache miss during an informing load could be used to implement a lightweight context switch functionality. In addition to light-weight context switches, informing loads support a wide variety of application-level responses to a cache miss.
Reference: [LRW91] <author> M. S. Lam, E. E. Rothberg and M. E. Wolf. </author> <title> The Cache Performance and Optimizations of Blocked Algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 6374, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: For 7 example, blocked matrix multiply codes access three matrices within their main loop nest. Of these three matrices, it is the blocked matrix that is most susceptible to poor memory performance due to conict misses <ref> [LRW91] </ref>. Loop-level statistics will report this as a problem with the entire loop, rather than pinpointing the bottleneck to a particular data structure or reference point. For finer-grained memory statistics, other tools rely on dedicated hardware to monitor memory references.
Reference: [LW94] <author> Alvin R. Lebeck and David A. Wood. </author> <title> Cache Profiling and the SPEC Benchmarks: A Case Study. </title> <booktitle> IEEE Computer, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Because of this basic limitation, monitoring tools are forced either to measure memory overhead indirectly (often at a coarser granularity than desirable) [GH93] or to rely on simulated data about memory system behavior <ref> [Mar93, LW94] </ref>; compiler optimizations are driven by static guesses rather than dynamic observations of caching behavior [Mow94]; and for some page migration and mapping optimizations, operating systems are turning to specialized add-on hardware support such as bus-based hardware monitors [CDV+94] and cache miss lookaside buffers [BLRC94]. <p> In the following two sections, we examine and evaluate implementations of several of these techniques using informing loads. 3.0 Monitoring Program Performance A number of performance tools have been proposed to monitor program caching behavior <ref> [BM89, GH93, LW94, Mar93] </ref>. One of the major stumbling-blocks in building such tools is gathering appropriately detailed memory statistics with low runtime overheads and minimal perturbations of the monitored program. <p> Furthermore, from both philosophical and efficiency standpoints, there is no clear justification for requiring applications to use operating system services to monitor their own performance. Because of the drawbacks of hardware-based and trap-based monitoring, tools such as MemSpy [Mar93] and CProf <ref> [LW94] </ref> are based on direct-execution simulation. These approaches require no dedicated hardware, but unfortunately even streamlined implementations of simple simulators impose slowdown factors of three to five on application execution time.
Reference: [Mar93] <author> Margaret Martonosi. </author> <title> Analyzing and Tuning Memory Performance in Sequential and Parallel Programs. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Because of this basic limitation, monitoring tools are forced either to measure memory overhead indirectly (often at a coarser granularity than desirable) [GH93] or to rely on simulated data about memory system behavior <ref> [Mar93, LW94] </ref>; compiler optimizations are driven by static guesses rather than dynamic observations of caching behavior [Mow94]; and for some page migration and mapping optimizations, operating systems are turning to specialized add-on hardware support such as bus-based hardware monitors [CDV+94] and cache miss lookaside buffers [BLRC94]. <p> In the following two sections, we examine and evaluate implementations of several of these techniques using informing loads. 3.0 Monitoring Program Performance A number of performance tools have been proposed to monitor program caching behavior <ref> [BM89, GH93, LW94, Mar93] </ref>. One of the major stumbling-blocks in building such tools is gathering appropriately detailed memory statistics with low runtime overheads and minimal perturbations of the monitored program. <p> This low overhead greatly increases exibility in monitoring style. Furthermore, from both philosophical and efficiency standpoints, there is no clear justification for requiring applications to use operating system services to monitor their own performance. Because of the drawbacks of hardware-based and trap-based monitoring, tools such as MemSpy <ref> [Mar93] </ref> and CProf [LW94] are based on direct-execution simulation. These approaches require no dedicated hardware, but unfortunately even streamlined implementations of simple simulators impose slowdown factors of three to five on application execution time.
Reference: [MC69] <author> A. C. McKeller and E. G. Coffman. </author> <title> The organization of matrices and matrix operations in a paged multiprogramming environment. </title> <journal> CACM, </journal> <volume> 12(3):153165, </volume> <year> 1969. </year>
Reference-contexts: These techniques include compiler optimization like blocking <ref> [ASKL79, GJMS87, MC69, WL91, GL89] </ref> and software-controlled prefetching [Mow94, Por89, CMCH91] and the operating system optimizations like page coloring [KH92, BLRC94] and page migration [CDV + 94, LE91, CF89, BFS89].
Reference: [MLG92] <author> T. C. Mowry, M. S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <volume> volume 27, </volume> <pages> pages 6273, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: For these experiments, we focus on regular array-based codes. Informing loads are used to collect the miss rates of all load references (similar to the monitoring code described in Section 3.1), but misses are not correlated with when they occur. We then augment the compiler algorithm presented in <ref> [MLG92] </ref> to use these miss rates as follows. After performing locality analysis, the predicted and observed miss rates are compared for each reference. <p> This allows the compiler to reason about intermediate miss rates and schedule prefetches only for the dynamic instances that are expected to miss. We simulated the same array-based scientific codes presented in an earlier prefetching study <ref> [MLG92] </ref> using the same architectural assumptions. One of the cases improved significantly using memory feedback: OCEAN, which is a uniprocessor version of a SPLASH application [SWG91].
Reference: [Mow94] <author> T. C. Mowry. </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Because of this basic limitation, monitoring tools are forced either to measure memory overhead indirectly (often at a coarser granularity than desirable) [GH93] or to rely on simulated data about memory system behavior [Mar93, LW94]; compiler optimizations are driven by static guesses rather than dynamic observations of caching behavior <ref> [Mow94] </ref>; and for some page migration and mapping optimizations, operating systems are turning to specialized add-on hardware support such as bus-based hardware monitors [CDV+94] and cache miss lookaside buffers [BLRC94]. <p> These techniques include compiler optimization like blocking [ASKL79, GJMS87, MC69, WL91, GL89] and software-controlled prefetching <ref> [Mow94, Por89, CMCH91] </ref> and the operating system optimizations like page coloring [KH92, BLRC94] and page migration [CDV + 94, LE91, CF89, BFS89]. Without informing loads, the success of these automatic techniques depend heavily on how well the compiler can predict caching behavior ahead of time. <p> If they disagree beyond a certain margin, the locality analysis model is adjusted taking factors such as uncertainty and control-ow feedback into account to find an explanation for the miss rate that is consistent with the intrinsic data reuse (for further details, see <ref> [Mow94] </ref>). This allows the compiler to reason about intermediate miss rates and schedule prefetches only for the dynamic instances that are expected to miss. We simulated the same array-based scientific codes presented in an earlier prefetching study [MLG92] using the same architectural assumptions.
Reference: [Pau94] <author> Richard Paul. </author> <title> SPARC Architecture, Assembly Language Programming, </title> & <address> C. </address> <publisher> Prentice Hall, </publisher> <year> 1994. </year>
Reference-contexts: the execution of the instruction that immediately follows the informing load in the sequential program. (In a single-issue RISC machine, this would be the instruction in the delay slot of the informing load instruction.) In some sense, an informing load is akin to the squashing branches in the SPARC architecture <ref> [Pau94] </ref> or the nullifying operations in the HP PA-RISC architecture [HP92]. An informing load operation squashes its delay slot instruction if the load hits in the cache.
Reference: [Por89] <author> A. K. Porterfield. </author> <title> Software Methods for Improvement of Cache Performance on Supercomputer Applications. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rice University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: These techniques include compiler optimization like blocking [ASKL79, GJMS87, MC69, WL91, GL89] and software-controlled prefetching <ref> [Mow94, Por89, CMCH91] </ref> and the operating system optimizations like page coloring [KH92, BLRC94] and page migration [CDV + 94, LE91, CF89, BFS89]. Without informing loads, the success of these automatic techniques depend heavily on how well the compiler can predict caching behavior ahead of time.
Reference: [RHL+93] <author> S. K. Reinhardt, M. D. Hill, J. R. Larus, et al. </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> In Proc. ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems. </booktitle> <pages> pages 48-59, </pages> <month> May, </month> <year> 1993. </year> <month> 24 </month>
Reference-contexts: These approaches are increasingly difficult due to the levels of integration in modern processors. With first-level and perhaps second-level caches on-chip, cache performance monitoring warrants integrated processor support. Some techniques monitor memory behavior by trapping on particular accesses and simulating them <ref> [RHL+93] </ref>, or by trapping based on values in sampled hardware miss counters [DEC92]. Such trap-based techniques incur overheads of 200 cycles or more on each trapped event just to get to monitoring code. In addition, the cache perturbation due to trap handling can be significant.
Reference: [SWG91] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Stanford University, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: We simulated the same array-based scientific codes presented in an earlier prefetching study [MLG92] using the same architectural assumptions. One of the cases improved significantly using memory feedback: OCEAN, which is a uniprocessor version of a SPLASH application <ref> [SWG91] </ref>.
Reference: [Smi91] <author> Michael D. Smith. </author> <title> Tracing with Pixie. </title> <type> Technical Report CSL-TR-91-497, </type> <institution> Stanford University, </institution> <month> November </month> <year> 1991. </year>
Reference-contexts: We also quantify data cache perturbation induced by running monitoring code interspersed with the application code. The results presented here, and in later sections presenting simulation results, were collected using a simulator based on pixie <ref> [Smi91] </ref>. We model a single-issue processor with split, direct-mapped primary instruction and data caches (each 8KB), and a 256KB, direct-mapped, unified secondary cache. A primary miss satisfied by the secondary cache takes 12 cycles, and a primary miss going all the way to memory takes a total of 75 cycles.
Reference: [Smi92] <author> M. D. Smith. </author> <title> Support for Speculative Execution in High-Performance Processors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> November </month> <year> 1992. </year> <note> [TWL 91] S. </note> <author> Tjiang, M. Wolf, M. Lam, et al. </author> <title> Integrating Scalar Optimizations and Parallelization. </title> <publisher> Springer Verlag, </publisher> <pages> pp. 137-151, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Compilers have historically used control-ow feedback (also known as branch profiling) to perform aggressive instruction scheduling across branches <ref> [Fis81, Smi92] </ref>. Given that informing loads make it practical to collect accurate per-reference miss rates across entire applications (as demonstrated earlier in Section 3.0), a similar feedback methodology can be used to enhance aggressive memory optimizations, such as software-controlled prefetching. <p> Therefore if the majority of references turn out to hit in the cache, the code will still execute at maximum speed since the adaptive code will not be invoked. To experiment with this active approach, we modified the compiler developed for TORCH <ref> [Smi92] </ref> to fill informing load delay slots with a prefetch of the next likely load address whenever possible. <p> This means that there exists many free fetch and execute positions for the informing load slot instructions. We used an aggressive superscalar compiler that was built for TORCH <ref> [Smi92] </ref>, a simple dual-issue machine with limited parallel resources. We then compared the cycle counts of the original code to code where we replaced all loads with informing loads. For these cycle counts, we assumed that the caches always hit so that the informing-load slot instructions are pure overhead.
Reference: [WL91] <author> M. E. Wolf and M. S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the SIGPLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 3044, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: These techniques include compiler optimization like blocking <ref> [ASKL79, GJMS87, MC69, WL91, GL89] </ref> and software-controlled prefetching [Mow94, Por89, CMCH91] and the operating system optimizations like page coloring [KH92, BLRC94] and page migration [CDV + 94, LE91, CF89, BFS89].
References-found: 35

