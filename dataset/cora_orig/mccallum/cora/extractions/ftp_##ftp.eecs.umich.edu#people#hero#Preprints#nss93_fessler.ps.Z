URL: ftp://ftp.eecs.umich.edu/people/hero/Preprints/nss93_fessler.ps.Z
Refering-URL: http://www.eecs.umich.edu/~hero/tom_imaging.html
Root-URL: http://www.cs.umich.edu
Title: New Complete-Data Spaces and Faster Algorithms for Penalized-Likelihood Emission Tomography  
Author: Jeffrey A. Fessler and Alfred O. Hero 
Affiliation: University of Michigan  
Abstract: The classical expectation-maximization (EM) algorithm for image reconstruction suffers from particularly slow convergence when additive background effects such as accidental coincidences and scatter are included. In addition, when smoothness penalties are included in the objective function, the M-step of the EM algorithm becomes intractable due to parameter coupling. This paper describes the space-alternating generalized EM (SAGE) algorithm, in which the parameters are updated sequentially using a sequence of small "hidden" data spaces rather than one large complete-data space. The sequential update decou-ples the M-step, so the maximization can typically be performed analytically. By choosing hidden-data spaces with considerably less Fisher information than the conventional complete-data space for Poisson data, we obtain significant improvements in convergence rate. This acceleration is due to statistical considerations, not to numerical overre-laxation methods, so monotonic increases in the objective function and global convergence are guaranteed. Due to the space constraints, we focus on the unpenal-ized case in this summary, and we eliminate derivations that are similar to those in [1]. See [2, 3] for a more comprehensive literature review, the penalized likelihood algorithms, and experimental results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Lange and R. Carson. </author> <title> EM reconstruction algorithms for emission and transmission tomography. </title> <journal> Journal of Computer Assisted Tomography, </journal> <volume> 8(2) </volume> <pages> 306-316, </pages> <month> April </month> <year> 1984. </year>
Reference-contexts: Convergence properties of the generic SAGE algorithm are discussed in [2]. Convergence for emission tomography is shown in [3]. II. MAXIMUM LIKELIHOOD We first review the linear Poisson model for emission tomography and summarize the classical EM algorithm (ML-EM-1) for maximizing the likelihood <ref> [1] </ref>. We then introduce a new complete-data space that leads to a new, faster converging EM algorithm: ML-EM-2. For more dramatic improvements we introduce two SAGE algorithms. <p> Assume the variates N nk have independent Poisson distributions: N nk ~ Poissonfa nk k g; where the a nk are nonnegative constants that characterize the system <ref> [1] </ref>. The detectors record emissions from several source locations, so at best one would observe only the sums P k N nk , rather than each N nk . <p> Thus, Y n ~ Poissonf X a nk k + r n g: (7) Given realizations fy n g of fY n g, the log-likelihood for this problem is given by <ref> [1] </ref>: log f (y; ) = k where ^y n () = k We would like to compute the ML estimate of = [ 1 ; : : : ; p ] 0 from y. <p> this likelihood, one might try to update k by equating the derivative of the likelihood to zero: 0 = a k + n y n (k) ; (8) where a k = P (k) P fortunately, there is no analytical solution to this equation, hence the popularity of EM-type algorithms <ref> [1] </ref>. B. ML-EM Algorithms The complete-data space for the classical EM algorithm [1] for this problem is the set of unobservable random variates X 1 = ffN nk g p n=1 : (9) For this complete-data space, the Q function (3) becomes [1, eqn. (4)]: X X where [1] N nk <p> of the likelihood to zero: 0 = a k + n y n (k) ; (8) where a k = P (k) P fortunately, there is no analytical solution to this equation, hence the popularity of EM-type algorithms <ref> [1] </ref>. B. ML-EM Algorithms The complete-data space for the classical EM algorithm [1] for this problem is the set of unobservable random variates X 1 = ffN nk g p n=1 : (9) For this complete-data space, the Q function (3) becomes [1, eqn. (4)]: X X where [1] N nk = EfN nk jY = y; g = k a nk y <p> B. ML-EM Algorithms The complete-data space for the classical EM algorithm [1] for this problem is the set of unobservable random variates X 1 = ffN nk g p n=1 : (9) For this complete-data space, the Q function (3) becomes <ref> [1, eqn. (4)] </ref>: X X where [1] N nk = EfN nk jY = y; g = k a nk y n =^y n ( ): Maximizing Q 1 (; i ) analytically leads to the following al gorithm: Page 2 ML-EM-1 Algorithm for i = 0; 1; : : : <p> EM-type algorithms <ref> [1] </ref>. B. ML-EM Algorithms The complete-data space for the classical EM algorithm [1] for this problem is the set of unobservable random variates X 1 = ffN nk g p n=1 : (9) For this complete-data space, the Q function (3) becomes [1, eqn. (4)]: X X where [1] N nk = EfN nk jY = y; g = k a nk y n =^y n ( ): Maximizing Q 1 (; i ) analytically leads to the following al gorithm: Page 2 ML-EM-1 Algorithm for i = 0; 1; : : : f X a nk i for <p> This EM algorithm converges globally <ref> [1, 4] </ref> but slowly. The root-convergence factor is very close to 1 (even if p = 1 [4]). The slow convergence is largely explained by considering the Fisher information of the complete-data space X 1 [4]. <p> Whereas the classical complete-data space X 1 has some intuitive relationship with the underlying image formation physics, we developed the new complete-data space X 3 using a statistical perspective on the problem and its Fisher information. Using a similar derivation as in <ref> [1] </ref> one can show: Q 3 (; ) = n k where M nk = EfM nk jY = y; g = ( k + m k )a nk y n =^y n ( ): Maximizing Q 3 (; i ) analytically (subject to the nonneg-ativity constraint), yields the ML-EM-2 algorithm, <p> The Fisher information for X 5 k is k which is much smaller than the kth diagonal entry of F X 3 since z k &gt;> m k . Using a similar derivation as in <ref> [1] </ref> one can show: Q 5 X (a nk ( k + z k ) + Z nk log (a nk ( k + z k )); where Z nk = EfZ nk jY = y; g = ( k + z k )a nk y n =^y n ( ):
Reference: [2] <author> J. A. Fessler and A. O. Hero. </author> <title> Space-alternating generalized EM algorithm, </title> <note> 1993. Submitted to IEEE Trans. Signal Proc. </note>
Reference-contexts: In emission tomography, if we choose index sets and hidden data spaces appropriately, then the E-step and M-step can be combined via an analytical maximization into a recursion of the form i+1 S = g S ( i ). Convergence properties of the generic SAGE algorithm are discussed in <ref> [2] </ref>. Convergence for emission tomography is shown in [3]. II. MAXIMUM LIKELIHOOD We first review the linear Poisson model for emission tomography and summarize the classical EM algorithm (ML-EM-1) for maximizing the likelihood [1]. <p> In PET, since since random coincidences are pervasive, we will have r n &gt; 0 8n, so that m k &gt; 0 8k. Like ML-EM-1, since ML-EM-2 is an EM algorithm it monotonically increases the likelihood every iteration <ref> [2] </ref>. An interesting difference between the iterates generated by ML-EM-1 and ML-EM-2 is that the latter can move on and off the boundary of the nonnegative orthant from iteration to iteration. <p> That method is not guaranteed to be monotonic, and it is somewhat more expensive per iteration since second derivatives must be computed. But when it converges, its asymptotic convergence rate may be somewhat faster than SAGE since it is even greedier <ref> [2] </ref>. Similar (but monotonic) greediness can be obtained by using multiple sub-iterations of the E- and M-steps in the SAGE algorithm. However, for the few cases we have tested, we have not observed any improvement in convergence rates using multiple sub-iterations.
Reference: [3] <author> J. A. Fessler. </author> <title> Penalized-likelihood image reconstruction using space-alternating generalized EM algorithms, </title> <note> 1993. In preparation. </note>
Reference-contexts: Convergence properties of the generic SAGE algorithm are discussed in [2]. Convergence for emission tomography is shown in <ref> [3] </ref>. II. MAXIMUM LIKELIHOOD We first review the linear Poisson model for emission tomography and summarize the classical EM algorithm (ML-EM-1) for maximizing the likelihood [1]. We then introduce a new complete-data space that leads to a new, faster converging EM algorithm: ML-EM-2. <p> Indeed, the implementation differences between ML-EM-1, ML-EM-2, ML-SAGE-1, and ML-SAGE-2 are all remarkably minor, but the differences in convergence rates are not, as illustrated by the results in <ref> [3] </ref>, one of which is shown in Fig. 1. An alternative to SAGE is the coordinate-wise sequential Newton-Raphson updates recently proposed by Bouman and Sauer [8]. That method is not guaranteed to be monotonic, and it is somewhat more expensive per iteration since second derivatives must be computed.
Reference: [4] <author> J. A. Fessler, N. H. Clinthorne, and W. Leslie Rogers. </author> <title> On complete data spaces for PET reconstruction algorithms. </title> <journal> IEEE Transactions on Nuclear Science, </journal> <volume> 40(4) </volume> <pages> 1055-1061, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: This EM algorithm converges globally <ref> [1, 4] </ref> but slowly. The root-convergence factor is very close to 1 (even if p = 1 [4]). The slow convergence is largely explained by considering the Fisher information of the complete-data space X 1 [4]. <p> This EM algorithm converges globally [1, 4] but slowly. The root-convergence factor is very close to 1 (even if p = 1 <ref> [4] </ref>). The slow convergence is largely explained by considering the Fisher information of the complete-data space X 1 [4]. <p> This EM algorithm converges globally [1, 4] but slowly. The root-convergence factor is very close to 1 (even if p = 1 <ref> [4] </ref>). The slow convergence is largely explained by considering the Fisher information of the complete-data space X 1 [4]. One can think of X 1 as data from a hypothetical tomo-graph that knows whether each detected event is a true emission or a background event, and knows in which pixel each event originated. <p> Then clearly Y n = P has the appropriate distribution (7). However, when the 2 ML-EM-1 is essentially the ML-IB algorithm of [5]. Also proposed in [5] was the ML-IA algorithm, which has a more informative complete-data space and slower convergence <ref> [4] </ref>. Q function is formed, one finds that it has no analytical maximum, so one is no better off than with (8). <p> From a numerical perspective, this may partly explain the faster convergence of ML-EM-2, since when ML-EM-1 converges to the boundary, it can do so at sublinear rates <ref> [4] </ref>. C. ML-LINU Algorithms ML-EM-1 is the special case where ff = 1 of the form [6]: i+1 k + ff k @ k : The ML-LINU algorithm [6] uses a line-search to choose an ff i &gt; 1, which accelerates the convergence of ML-EM-1.
Reference: [5] <author> D. G. Politte and D. L. Snyder. </author> <title> Corrections for accidental coincidences and attenuation in maximum-likelihood image reconstruction for positron-emission tomography. </title> <journal> IEEE Transactions on Medical Imaging, </journal> <volume> 10(1) </volume> <pages> 82-89, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Then clearly Y n = P has the appropriate distribution (7). However, when the 2 ML-EM-1 is essentially the ML-IB algorithm of <ref> [5] </ref>. Also proposed in [5] was the ML-IA algorithm, which has a more informative complete-data space and slower convergence [4]. Q function is formed, one finds that it has no analytical maximum, so one is no better off than with (8). <p> Then clearly Y n = P has the appropriate distribution (7). However, when the 2 ML-EM-1 is essentially the ML-IB algorithm of <ref> [5] </ref>. Also proposed in [5] was the ML-IA algorithm, which has a more informative complete-data space and slower convergence [4]. Q function is formed, one finds that it has no analytical maximum, so one is no better off than with (8).
Reference: [6] <author> L. Kaufman. </author> <title> Implementing and accelerating the EM algorithm for positron emission tomography. </title> <journal> IEEE Transactions on Medical Imaging, </journal> <volume> 6(1) </volume> <pages> 37-51, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: From a numerical perspective, this may partly explain the faster convergence of ML-EM-2, since when ML-EM-1 converges to the boundary, it can do so at sublinear rates [4]. C. ML-LINU Algorithms ML-EM-1 is the special case where ff = 1 of the form <ref> [6] </ref>: i+1 k + ff k @ k : The ML-LINU algorithm [6] uses a line-search to choose an ff i &gt; 1, which accelerates the convergence of ML-EM-1. <p> C. ML-LINU Algorithms ML-EM-1 is the special case where ff = 1 of the form <ref> [6] </ref>: i+1 k + ff k @ k : The ML-LINU algorithm [6] uses a line-search to choose an ff i &gt; 1, which accelerates the convergence of ML-EM-1. <p> m k @ k : In the few experiments tried, we found that "accelerating" ML-EM-2 by choosing ff &gt; 1 did not improve convergence much, primarily because of the nonnegativity constraint. (When ff &gt; 1 causes a "bent line" search, then evaluating the likelihood difference requires an expensive forward projection <ref> [6] </ref>.) D. <p> However, for the few cases we have tested, we have not observed any improvement in convergence rates using multiple sub-iterations. Although further investigation of the tradeoffs available is needed, including comparisons with possibly super-linear methods such as preconditioned conjugate gradient <ref> [6] </ref>, it appears that the statistical perspective required by the SAGE method is a useful addition to conventional numerical tools.
Reference: [7] <author> K. Sauer and C. Bouman. </author> <title> A local update strategy for iterative reconstruction from projections. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 41(2) </volume> <pages> 534-548, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: ML-SAGE Algorithms Motivations for the SAGE algorithms include: * As shown by Sauer and Bouman <ref> [7] </ref>, sequential update methods often converge much faster than simultane ous update methods. * Sequential update methods solve the coupling problem introduced by smoothness penalties. * By using an alternating sequence of hidden-data spaces, we can associate a large fraction of the background events with each parameter as it is updated,
Reference: [8] <author> C. Bouman and K. Sauer. </author> <title> Fast numerical methods for emission and transmission tomographic reconstruction. </title> <booktitle> In Proc. Conf. </booktitle> <institution> Info. Sci. Sys., Johns Hopkins, </institution> <year> 1993. </year> <pages> Page 5 </pages>
Reference-contexts: An alternative to SAGE is the coordinate-wise sequential Newton-Raphson updates recently proposed by Bouman and Sauer <ref> [8] </ref>. That method is not guaranteed to be monotonic, and it is somewhat more expensive per iteration since second derivatives must be computed. But when it converges, its asymptotic convergence rate may be somewhat faster than SAGE since it is even greedier [2].
References-found: 8

