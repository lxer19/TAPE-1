URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-93-34.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Increasing Memory Bandwidth for Vector Computations  
Author: Sally A. McKee, Steven A. Moyer, Wm. A. Wulf, Charles Y. Hitchcock 
Note: This work was supported in part by a grant from Intel Supercomputer Division and by NSF grants MIP-9114110 and MIP-9307626.  
Abstract: Computer Science Report No. CS-93-34 August 1, 1993 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Baer, J. L., Chen, T. F., </author> <title> An Effective On-Chip Preloading Scheme To Reduce Data Access Penalty, </title> <address> Supercomputing91, </address> <month> November </month> <year> 1991. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [11], reordering the computation to increase locality (as in blocking [21]) address transformations for conict-free access to interleaved memory [13, 34, 39], software prefetching data to the cache [4, 20, 37], and hardware prefetching vector data to cache <ref> [1, 8, 18, 35] </ref>. <p> Whether or not such a scheme is superior to a system depends on the relative quality of the compile-time and run-time algorithms for stream detection and relative hardware costs. Proposals for vector prefetch units have recently appeared <ref> [1, 35] </ref>, but these do not order accesses to fully exploit the underlying memory architecture. 4 The Stream Memory Controller Based on our analysis and simulations, we believe that the best engineering choice is to detect streams at compile time, but to defer access ordering and issue to run time in
Reference: 2. <author> Baron, R.L., and Higbie, L., </author> <title> Computer Architecture, </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference: 3. <author> Budnik, P., and Kuck, D., </author> <title> The Organization and Use of Parallel Memories, </title> <journal> IEEE Trans. Comput., </journal> <volume> 20, 12, </volume> <year> 1971. </year>
Reference-contexts: Other sophisticated memory interfaces, such as RamLink and the JEDEC synchronous DRAM, provide similar benefits [16]. The more sophisticated such interfaces become, the more important it is to exploit them intelligently with controllers such as the SMC. Alternative Storage Schemes: Skewed storage <ref> [3, 12] </ref> and dynamic address transformations [13, 34] have been proposed as methods for increasing concurrency, and hence bandwidth, in parallel memory systems.
Reference: 4. <author> Callahan, D., et. al., </author> <title> Software Prefetching, </title> <booktitle> Fourth International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [11], reordering the computation to increase locality (as in blocking [21]) address transformations for conict-free access to interleaved memory [13, 34, 39], software prefetching data to the cache <ref> [4, 20, 37] </ref>, and hardware prefetching vector data to cache [1, 8, 18, 35]. <p> The reference to is a constant within the inner loop, and is therefore preloaded into a processor register. Software Prefetching: Some architectures include instructions to prefetch data from main memory into cache. Using these instructions to load data for a future iteration of a loop <ref> [4, 20, 37] </ref> can improve processor performance by overlapping memory latency with computation, but prefetching does nothing to actually improve memory performance. Note that prefetching can be used in conjunction with an SMC to help hide latency in FIFO references.
Reference: 5. <author> Carr, S., Kennedy, K., </author> <title> Blocking Linear Algebra Codes for Memory Hierarchies, </title> <booktitle> Proc. Fourth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1989. </year>
Reference-contexts: For computations in which vectors are reused, iteration space tiling <ref> [5, 21, 41] </ref> can partition the problem into cache-size blocks, but the technique is difficult to automate. Caching non-unit stride vectors may actually reduce a computations effective memory bandwidth by fetching extraneous data. <p> Most of these are complementary to access ordering. Traditional Caching: Traditional caches retain their importance for code and non-vector data in a system equipped with an SMC. Furthermore, if algorithms can be blocked <ref> [5, 41] </ref> and data aligned to eliminate significant conicts [21], the cache and SMC can be used in a complementary fashion for vector access. Under these conditions multiple-visit vector data can be cached, with the SMC used to reference single-visit vectors.
Reference: 6. <author> Davidson, Jack W., and Benitez, Manuel E., </author> <title> Code Generation for Streaming: An Access/Execute Mechanism, </title> <booktitle> Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Since it contains a first-order linear recurrence, it cannot be vectorized. Nonetheless, the compiler can employ the recurrence detection and optimization algorithm of <ref> [6] </ref> to generate streaming code: each computed value is retained in a register so that it will be available for use as on the following iteration. <p> Each of these functions may be addressed at compile time, CT, or by hardware at run time, RT. This taxonomy classifies access ordering systems by a tuple indicating the time at which each function is performed. Davidson <ref> [6] </ref> detects streams at compile time, and Moyer [31] has derived access-ordering algorithms relative to a precise analytic model of memory systems. Moyers approach unrolls loops and orders memory operations to exploit architectural and device features of the target memory system. <p> Another advantage is that this combined hardware/software scheme doesnt require heroic compiler technology the compiler need only detect the presence of streams, and Davidsons streaming algorithm <ref> [6] </ref> can be used to do this. Continuing the tridiag algorithm and memory system example introduced earlier, the performance effect of such an SMC is illustrated by Figure 4.
Reference: 7. <author> Dongarra, et. al., </author> <title> Linpack Users Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: Results reported here are for the four kernels described in Figure 6. Daxpy and swap are from the BLAS (Basic Linear Algebra Subroutines) <ref> [22, 7] </ref>, tridiag is the fifth Livermore Loop from our earlier example [27], and vaxpy is a vector axpy 1 computation that occurs in matrix-vector multiplication by diagonals.
Reference: 8. <author> Fu, J. W. C., and Patel, J. H., </author> <title> Data Prefetching in Multiprocessor Vector Cache Memories, </title> <booktitle> 18th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [11], reordering the computation to increase locality (as in blocking [21]) address transformations for conict-free access to interleaved memory [13, 34, 39], software prefetching data to the cache [4, 20, 37], and hardware prefetching vector data to cache <ref> [1, 8, 18, 35] </ref>.
Reference: 9. <author> Golub, G., and Ortega, J.M., </author> <title> Scientific Computation: An Introduction with Parallel Computing, </title> <publisher> Academic Press, Inc., </publisher> <year> 1993. </year>
Reference-contexts: Figure 1 (a) represents the natural reference sequence for a straightforward translation of the computation: This computation occurs frequently in practice, especially in the solution of partial differential equations by finite difference or finite element methods <ref> [9] </ref>. Since it contains a first-order linear recurrence, it cannot be vectorized. Nonetheless, the compiler can employ the recurrence detection and optimization algorithm of [6] to generate streaming code: each computed value is retained in a register so that it will be available for use as on the following iteration.
Reference: 10. <author> Goodman, J. R., et al, </author> <title> PIPE: A VLSI Decoupled Architecture, </title> <booktitle> Twelfth International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1985. </year>
Reference-contexts: This organization is both simple and practical from an implementation standpoint: similar designs have been built. In fact, the organization is almost identical to the stream units of the WM architecture [42], or may be thought of as a special case of a decoupled access-execute architecture <ref> [10, 36] </ref>. Another advantage is that this combined hardware/software scheme doesnt require heroic compiler technology the compiler need only detect the presence of streams, and Davidsons streaming algorithm [6] can be used to do this.
Reference: 11. <author> Gupta, R., and Soffa, M., </author> <title> Compile-time Techniques for Efficient Utilization of Parallel Memories, </title> <journal> SIGPLAN Not., </journal> <volume> 23, 9, </volume> <year> 1988, </year> <pages> pp. 235-246. </pages>
Reference-contexts: Lecture Notes in Computer Science 966, S. Haridi, et al., Eds., Springer-Verlag, Berlin, 1995, pages 83-99. 5 There are a number of hardware and software techniques that can help manage the imbalance between processor and memory speeds. These include altering the placement of data to exploit concurrency <ref> [11] </ref>, reordering the computation to increase locality (as in blocking [21]) address transformations for conict-free access to interleaved memory [13, 34, 39], software prefetching data to the cache [4, 20, 37], and hardware prefetching vector data to cache [1, 8, 18, 35].
Reference: 12. <author> Harper, D. T., Jump., J., </author> <title> Vector Access Performance in Parallel Memories Using a Skewed Storage Scheme, </title> <journal> IEEE Trans. Comput., </journal> <volume> 36, 12, </volume> <year> 1987. </year>
Reference-contexts: Other sophisticated memory interfaces, such as RamLink and the JEDEC synchronous DRAM, provide similar benefits [16]. The more sophisticated such interfaces become, the more important it is to exploit them intelligently with controllers such as the SMC. Alternative Storage Schemes: Skewed storage <ref> [3, 12] </ref> and dynamic address transformations [13, 34] have been proposed as methods for increasing concurrency, and hence bandwidth, in parallel memory systems.
Reference: 13. <author> Harper, D. T., </author> <title> Address Transformation to Increase Memory Performance, </title> <booktitle> 1989 International Conference on Supercomputing. </booktitle>
Reference-contexts: These include altering the placement of data to exploit concurrency [11], reordering the computation to increase locality (as in blocking [21]) address transformations for conict-free access to interleaved memory <ref> [13, 34, 39] </ref>, software prefetching data to the cache [4, 20, 37], and hardware prefetching vector data to cache [1, 8, 18, 35]. <p> Other sophisticated memory interfaces, such as RamLink and the JEDEC synchronous DRAM, provide similar benefits [16]. The more sophisticated such interfaces become, the more important it is to exploit them intelligently with controllers such as the SMC. Alternative Storage Schemes: Skewed storage [3, 12] and dynamic address transformations <ref> [13, 34] </ref> have been proposed as methods for increasing concurrency, and hence bandwidth, in parallel memory systems.
Reference: 14. <author> Hayes, J.P., </author> <title> Computer Architecture and Organization, </title> <publisher> McGraw-Hill, </publisher> <year> 1988. </year>
Reference: 15. <author> Hwang, K., and Briggs, </author> <title> F.A., </title> <booktitle> Computer Architecture and Parallel Processing, </booktitle> <publisher> McGraw-Hill, Inc., </publisher> <year> 1984. </year>
Reference: 16. <author> High-speed DRAMs, </author> <title> Special Report, </title> <journal> IEEE Spectrum, </journal> <volume> vol. 29, no. 10, </volume> <month> October </month> <year> 1992. </year> <title> 17. i860 XP Microprocessor Data Book, </title> <publisher> Intel Corporation, </publisher> <year> 1991. </year>
Reference-contexts: Other common devices offer similar features (nibble-mode, static column mode, or a small amount of SRAM cache on chip) or exhibit novel organizations (such as Rambus [33], Ramlink, and the new synchronous DRAM designs <ref> [16] </ref>). The order of requests strongly affects the performance of all these components. For instance, Rambus devices provide high bandwidth for large transfers, but offer little performance benefit for single-word accesses. Appeared in Proceedings of Europar95, Stockholm, Sweden, August 1995. Lecture Notes in Computer Science 966, S. <p> New DRAM interfaces: Rambus [33] is a new, high-speed DRAM interface that provides both higher bandwidth for sequential accesses and true caching of two DRAM pages on the chip. Other sophisticated memory interfaces, such as RamLink and the JEDEC synchronous DRAM, provide similar benefits <ref> [16] </ref>. The more sophisticated such interfaces become, the more important it is to exploit them intelligently with controllers such as the SMC. Alternative Storage Schemes: Skewed storage [3, 12] and dynamic address transformations [13, 34] have been proposed as methods for increasing concurrency, and hence bandwidth, in parallel memory systems.
Reference: 18. <author> Jouppi, N., </author> <title> Improving Direct-Mapped Cache Performance by the Addition of a Small Fully Associative Cache and Prefetch Buffers, </title> <booktitle> 17th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [11], reordering the computation to increase locality (as in blocking [21]) address transformations for conict-free access to interleaved memory [13, 34, 39], software prefetching data to the cache [4, 20, 37], and hardware prefetching vector data to cache <ref> [1, 8, 18, 35] </ref>.
Reference: 19. <author> Katz, R., and Hennessy, J., </author> <title> High Performance Microprocessor Architectures, </title> <institution> University of California, Berkeley, </institution> <note> Report No. UCB/CSD 89/529, </note> <month> August, </month> <year> 1989. </year>
Reference: 20. <author> Klaiber, A., et. al., </author> <title> An Architecture for Software-Controlled Data Prefetching, </title> <booktitle> 18th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [11], reordering the computation to increase locality (as in blocking [21]) address transformations for conict-free access to interleaved memory [13, 34, 39], software prefetching data to the cache <ref> [4, 20, 37] </ref>, and hardware prefetching vector data to cache [1, 8, 18, 35]. <p> The reference to is a constant within the inner loop, and is therefore preloaded into a processor register. Software Prefetching: Some architectures include instructions to prefetch data from main memory into cache. Using these instructions to load data for a future iteration of a loop <ref> [4, 20, 37] </ref> can improve processor performance by overlapping memory latency with computation, but prefetching does nothing to actually improve memory performance. Note that prefetching can be used in conjunction with an SMC to help hide latency in FIFO references.
Reference: 21. <author> Lam, Monica, et. al., </author> <title> The Cache Performance and Optimizations of Blocked Algorithms, </title> <booktitle> Fourth International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: For computations in which vectors are reused, iteration space tiling <ref> [5, 21, 41] </ref> can partition the problem into cache-size blocks, but the technique is difficult to automate. Caching non-unit stride vectors may actually reduce a computations effective memory bandwidth by fetching extraneous data. <p> Caching non-unit stride vectors may actually reduce a computations effective memory bandwidth by fetching extraneous data. Thus, as noted by Lam et al <ref> [21] </ref>, while data caches have been demonstrated to be effective for general-purpose applications , their effectiveness for numerical code has not been established. 1. Current address: Department of Mathematics and Computer Science, Emory Uni versity, Atlanta, GA 30322. 2. <p> These include altering the placement of data to exploit concurrency [11], reordering the computation to increase locality (as in blocking <ref> [21] </ref>) address transformations for conict-free access to interleaved memory [13, 34, 39], software prefetching data to the cache [4, 20, 37], and hardware prefetching vector data to cache [1, 8, 18, 35]. <p> Most of these are complementary to access ordering. Traditional Caching: Traditional caches retain their importance for code and non-vector data in a system equipped with an SMC. Furthermore, if algorithms can be blocked [5, 41] and data aligned to eliminate significant conicts <ref> [21] </ref>, the cache and SMC can be used in a complementary fashion for vector access. Under these conditions multiple-visit vector data can be cached, with the SMC used to reference single-visit vectors. <p> Figure 5 (a) depicts code for a straightforward implementation using matrices stored in column-major order; the code in Figure 5 (b) strip-mines the computation to reuse elements of . Partition size depends on cache size and structure <ref> [21] </ref>. Elements of are preloaded into cache memory at the appropriate loop level, and the SMC is then used to access elements of and , since each element is accessed only once. The reference to is a constant within the inner loop, and is therefore preloaded into a processor register.
Reference: 22. <editor> Lawson, et. al., </editor> <title> Basic Linear Algebra Subprograms for Fortran Usage, </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 5, 3, </volume> <year> 1979. </year> <note> Appeared in Proceedings of Europar95, </note> <institution> Stockholm, Sweden, </institution> <month> August </month> <year> 1995. </year> <note> Lecture Notes in Computer Science 966, </note> <editor> S. Haridi, et al., Eds., </editor> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1995, </year> <pages> pages 83-99. 18 </pages>
Reference-contexts: Results reported here are for the four kernels described in Figure 6. Daxpy and swap are from the BLAS (Basic Linear Algebra Subroutines) <ref> [22, 7] </ref>, tridiag is the fifth Livermore Loop from our earlier example [27], and vaxpy is a vector axpy 1 computation that occurs in matrix-vector multiplication by diagonals.
Reference: 23. <author> Lee, K., </author> <title> Achieving High Performance On the i860 Microprocessor Using Naspack Subroutines, </title> <institution> NAS Systems Division, NASA Ames Research Center, </institution> <month> July </month> <year> 1990. </year>
Reference: 24. <author> Lee, K., </author> <title> On the Floating Point Performance of the i860 Microprocessor, </title> <institution> RNR-90-019, NAS Systems Division, NASA Ames Research Center, </institution> <month> October </month> <year> 1990. </year>
Reference-contexts: On some applications, even with painstakingly handcrafted code, inadequate memory bandwidth limited us to 20% of peak processor performance [30]. Our experience is not unique; results similar to ours have been reported by Lee <ref> [24] </ref>, for example. To illustrate one aspect of the bandwidth problem and how it might be addressed at compile time consider the effect of executing the fifth Livermore Loop (tridiagonal elimination) using non-caching accesses to reference a single bank of page-mode DRAMs. <p> Note that prefetching can be used in conjunction with an SMC to help hide latency in FIFO references. Software Access Ordering: Software techniques such as reordering [30] and vectorization via library routines <ref> [24, 29] </ref> can improve bandwidth by reordering vector length Percentage of Peak Bandwidth w/o SMC FIFO depth 8 6 2 4 28 56 25.0 100 78.53 85.71 87.98 80.43 73.53 73.53 y A B+( ) x= y A B x Appeared in Proceedings of Europar95, Stockholm, Sweden, August 1995.
Reference: 25. <author> Maccabe, </author> <title> A.B., Computer Systems: Architecture, Organization, and Programming, </title> <editor> Richard D. Irwin, </editor> <publisher> Inc., </publisher> <year> 1993. </year>
Reference-contexts: Many computer architecture textbooks ([2, 14, 15, and 26] among them) specifically cultivate this view. Others skirt the issue entirely <ref> [25, 38] </ref>. Somewhat ironically, this assumption no longer applies to modern memory devices: most components manufactured in the last ten to fifteen years provide special capabilities that make it possible to perform some access sequences faster than others.
Reference: 26. <author> Mano, </author> <title> M.M., Computer System Architecture, 2nd ed., </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1982 </year>
Reference: 27. <author> McMahon, F.H., </author> <title> The Livermore Fortran Kernels: A Computer Test of the Numerical Performance Range, </title> <institution> Lawrence Livermore National Laboratory, UCRL-53745, </institution> <month> December </month> <year> 1986. </year>
Reference-contexts: Results reported here are for the four kernels described in Figure 6. Daxpy and swap are from the BLAS (Basic Linear Algebra Subroutines) [22, 7], tridiag is the fifth Livermore Loop from our earlier example <ref> [27] </ref>, and vaxpy is a vector axpy 1 computation that occurs in matrix-vector multiplication by diagonals. These benchmarks were selected because they are representative of the access patterns found in real scientific codes, including the inner loops of blocked algorithms.
Reference: 28. <author> McKee, S.A, </author> <title> Hardware Support for Access Ordering: Performance of Some Design Options, </title> <institution> University of Virginia, Department of Computer Science, </institution> <type> Technical Report CS-93-08, </type> <month> July </month> <year> 1993. </year>
Reference-contexts: Only samples of our results are given here; complete results can be found in <ref> [28] </ref>. In particular, the results given below involve the following restrictions. The simulations here use stride-one vectors aligned to have no DRAM pages in common, but starting in the same bank unless otherwise specified.
Reference: 29. <author> Meadows, L., Nakamoto, S., and Schuster, V., </author> <title> A Vectorizing, Software Pipelining Compiler for LIW and Superscalar Architectures, </title> <address> RISC92, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: Note that prefetching can be used in conjunction with an SMC to help hide latency in FIFO references. Software Access Ordering: Software techniques such as reordering [30] and vectorization via library routines <ref> [24, 29] </ref> can improve bandwidth by reordering vector length Percentage of Peak Bandwidth w/o SMC FIFO depth 8 6 2 4 28 56 25.0 100 78.53 85.71 87.98 80.43 73.53 73.53 y A B+( ) x= y A B x Appeared in Proceedings of Europar95, Stockholm, Sweden, August 1995.
Reference: 30. <author> Moyer, S.A., </author> <title> Performance of the iPSC/860 Node Architecture, </title> <institution> University of Virginia, IPC-TR-91-007, </institution> <year> 1991. </year>
Reference-contexts: This was put in sharp focus for the authors while attempting to optimize numerical libraries for the iPSC/860. On some applications, even with painstakingly handcrafted code, inadequate memory bandwidth limited us to 20% of peak processor performance <ref> [30] </ref>. Our experience is not unique; results similar to ours have been reported by Lee [24], for example. <p> Note that prefetching can be used in conjunction with an SMC to help hide latency in FIFO references. Software Access Ordering: Software techniques such as reordering <ref> [30] </ref> and vectorization via library routines [24, 29] can improve bandwidth by reordering vector length Percentage of Peak Bandwidth w/o SMC FIFO depth 8 6 2 4 28 56 25.0 100 78.53 85.71 87.98 80.43 73.53 73.53 y A B+( ) x= y A B x Appeared in Proceedings of Europar95,
Reference: 31. <author> Moyer, S.A., </author> <title> Access Ordering and Effective Memory Bandwidth, </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, University of Virginia, </institution> <type> Technical Report CS-93-18, </type> <month> April </month> <year> 1993. </year>
Reference-contexts: As pointed out above, the extent to which a compiler can perform this optimization is further constrained by such things as the size of the processor register file <ref> [31] </ref>. The beneficial impact of access ordering on effective memory bandwidth along with the limitations inherent in implementing the technique statically motivate us to consider an implementation that reorders accesses dynamically at run time. Appeared in Proceedings of Europar95, Stockholm, Sweden, August 1995. Lecture Notes in Computer Science 966, S. <p> Each of these functions may be addressed at compile time, CT, or by hardware at run time, RT. This taxonomy classifies access ordering systems by a tuple indicating the time at which each function is performed. Davidson [6] detects streams at compile time, and Moyer <ref> [31] </ref> has derived access-ordering algorithms relative to a precise analytic model of memory systems. Moyers approach unrolls loops and orders memory operations to exploit architectural and device features of the target memory system.
Reference: 32. <author> Quinnell, R., </author> <title> High-speed DRAMs, </title> <type> EDN, </type> <month> May 23, </month> <year> 1991. </year>
Reference-contexts: Somewhat ironically, this assumption no longer applies to modern memory devices: most components manufactured in the last ten to fifteen years provide special capabilities that make it possible to perform some access sequences faster than others. For instance, nearly all current DRAMs implement a form of page-mode operation <ref> [32] </ref>. These devices behave as if implemented with a single on-chip cache line, or page (this should not be confused with a virtual memory page). A memory access falling outside the address range of the current DRAM page forces a new page to be accessed.
Reference: 33. <institution> Architectural Overview, Rambus Inc., Mountain View, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: Other common devices offer similar features (nibble-mode, static column mode, or a small amount of SRAM cache on chip) or exhibit novel organizations (such as Rambus <ref> [33] </ref>, Ramlink, and the new synchronous DRAM designs [16]). The order of requests strongly affects the performance of all these components. For instance, Rambus devices provide high bandwidth for large transfers, but offer little performance benefit for single-word accesses. Appeared in Proceedings of Europar95, Stockholm, Sweden, August 1995. <p> An SMC and data placement are complementary; the SMC will perform better given a good placement. New DRAM interfaces: Rambus <ref> [33] </ref> is a new, high-speed DRAM interface that provides both higher bandwidth for sequential accesses and true caching of two DRAM pages on the chip. Other sophisticated memory interfaces, such as RamLink and the JEDEC synchronous DRAM, provide similar benefits [16].
Reference: 34. <author> Rau, B. R., </author> <title> Pseudo-Randomly Interleaved Memory, </title> <booktitle> 18th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [11], reordering the computation to increase locality (as in blocking [21]) address transformations for conict-free access to interleaved memory <ref> [13, 34, 39] </ref>, software prefetching data to the cache [4, 20, 37], and hardware prefetching vector data to cache [1, 8, 18, 35]. <p> Other sophisticated memory interfaces, such as RamLink and the JEDEC synchronous DRAM, provide similar benefits [16]. The more sophisticated such interfaces become, the more important it is to exploit them intelligently with controllers such as the SMC. Alternative Storage Schemes: Skewed storage [3, 12] and dynamic address transformations <ref> [13, 34] </ref> have been proposed as methods for increasing concurrency, and hence bandwidth, in parallel memory systems.
Reference: 35. <author> Sklenar, Ivan, </author> <title> Prefetch Unit for Vector Operation on Scalar Computers, </title> <journal> Computer Architecture News, </journal> <volume> 20, 4, </volume> <month> September </month> <year> 1992. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [11], reordering the computation to increase locality (as in blocking [21]) address transformations for conict-free access to interleaved memory [13, 34, 39], software prefetching data to the cache [4, 20, 37], and hardware prefetching vector data to cache <ref> [1, 8, 18, 35] </ref>. <p> Whether or not such a scheme is superior to a system depends on the relative quality of the compile-time and run-time algorithms for stream detection and relative hardware costs. Proposals for vector prefetch units have recently appeared <ref> [1, 35] </ref>, but these do not order accesses to fully exploit the underlying memory architecture. 4 The Stream Memory Controller Based on our analysis and simulations, we believe that the best engineering choice is to detect streams at compile time, but to defer access ordering and issue to run time in
Reference: 36. <author> Smith, J. E., et al, </author> <title> The ZS-1 Central Processor, </title> <booktitle> The Second International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <month> Oct. </month> <year> 1987 </year>
Reference-contexts: This organization is both simple and practical from an implementation standpoint: similar designs have been built. In fact, the organization is almost identical to the stream units of the WM architecture [42], or may be thought of as a special case of a decoupled access-execute architecture <ref> [10, 36] </ref>. Another advantage is that this combined hardware/software scheme doesnt require heroic compiler technology the compiler need only detect the presence of streams, and Davidsons streaming algorithm [6] can be used to do this.
Reference: 37. <author> Sohi, G. and Franklin, M., </author> <title> High Bandwidth Memory Systems for Superscalar Processors, </title> <booktitle> Fourth International Conference on Architectural Support for Programming Languages and Systems, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [11], reordering the computation to increase locality (as in blocking [21]) address transformations for conict-free access to interleaved memory [13, 34, 39], software prefetching data to the cache <ref> [4, 20, 37] </ref>, and hardware prefetching vector data to cache [1, 8, 18, 35]. <p> The reference to is a constant within the inner loop, and is therefore preloaded into a processor register. Software Prefetching: Some architectures include instructions to prefetch data from main memory into cache. Using these instructions to load data for a future iteration of a loop <ref> [4, 20, 37] </ref> can improve processor performance by overlapping memory latency with computation, but prefetching does nothing to actually improve memory performance. Note that prefetching can be used in conjunction with an SMC to help hide latency in FIFO references.
Reference: 38. <editor> Tomek, I., </editor> <booktitle> The Foundations of Computer Architecture and Organization, </booktitle> <publisher> Computer Science Press, </publisher> <year> 1990. </year>
Reference-contexts: Many computer architecture textbooks ([2, 14, 15, and 26] among them) specifically cultivate this view. Others skirt the issue entirely <ref> [25, 38] </ref>. Somewhat ironically, this assumption no longer applies to modern memory devices: most components manufactured in the last ten to fifteen years provide special capabilities that make it possible to perform some access sequences faster than others.
Reference: 39. <author> Valero, M., et. al., </author> <title> Increasing the Number of Strides for Conict-Free Vector Access, </title> <booktitle> 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: These include altering the placement of data to exploit concurrency [11], reordering the computation to increase locality (as in blocking [21]) address transformations for conict-free access to interleaved memory <ref> [13, 34, 39] </ref>, software prefetching data to the cache [4, 20, 37], and hardware prefetching vector data to cache [1, 8, 18, 35].
Reference: 40. <author> Wallach, S., </author> <title> The CONVEX C-1 64-bit Supercomputer, </title> <booktitle> Compcon Spring 85, </booktitle> <month> February </month> <year> 1985. </year>
Reference: 41. <author> Wolfe, M., </author> <title> Optimizing Supercompilers for Supercomputers, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: For computations in which vectors are reused, iteration space tiling <ref> [5, 21, 41] </ref> can partition the problem into cache-size blocks, but the technique is difficult to automate. Caching non-unit stride vectors may actually reduce a computations effective memory bandwidth by fetching extraneous data. <p> Most of these are complementary to access ordering. Traditional Caching: Traditional caches retain their importance for code and non-vector data in a system equipped with an SMC. Furthermore, if algorithms can be blocked <ref> [5, 41] </ref> and data aligned to eliminate significant conicts [21], the cache and SMC can be used in a complementary fashion for vector access. Under these conditions multiple-visit vector data can be cached, with the SMC used to reference single-visit vectors.
Reference: 42. <author> Wulf, W. A., </author> <title> Evaluation of the WM Architecture, </title> <booktitle> 19th Annual International Symposium on Computer Architecture, </booktitle> <volume> vol 20, no. 2, </volume> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: This organization is both simple and practical from an implementation standpoint: similar designs have been built. In fact, the organization is almost identical to the stream units of the WM architecture <ref> [42] </ref>, or may be thought of as a special case of a decoupled access-execute architecture [10, 36]. Another advantage is that this combined hardware/software scheme doesnt require heroic compiler technology the compiler need only detect the presence of streams, and Davidsons streaming algorithm [6] can be used to do this.
References-found: 41

