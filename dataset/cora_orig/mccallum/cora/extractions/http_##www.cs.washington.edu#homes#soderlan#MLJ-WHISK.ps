URL: http://www.cs.washington.edu/homes/soderlan/MLJ-WHISK.ps
Refering-URL: http://www.cs.washington.edu/homes/soderlan/
Root-URL: 
Email: soderlan@cs.washington.edu  
Title: Learning Information Extraction Rules for Semi-structured and Free Text Extraction As more and more text
Author: Stephen Soderland incl gar, grt N. Hill 
Date: loc $995. (206) 999-9999 &lt;br&gt;  
Note: Information  for  Capitol Hill 1 br twnhme. fplc D/W W/D. Undrgrnd pkg incl $675. 3 BR, upper  &lt;i&gt; &lt;font size=-2&gt; (This ad last ran on 08/03/97.) &lt;/font&gt; &lt;/i&gt; &lt;hr&gt;  
Address: Seattle, WA 98195-2350  
Affiliation: Dept. Computer Science Engineering University of Washington  
Abstract: A wealth of on-line text information can be made available to automatic processing by information extraction (IE) systems. Each IE application needs a separate set of rules tuned to the domain and writing style. WHISK helps to overcome this knowledge-engineering bottleneck by learning text extraction rules automatically. WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences. Such semi-structured text has largely been beyond the scope of previous systems. When used in conjunction with a syntactic analyzer and semantic tagging, WHISK can also handle extraction from free text such as news stories. WHISK minimizes the human effort to hand-tag training examples by interleaving the learning and tagging, using the current rule set to select informative instances for the user to tag. For structured text, the rules specify a fixed order of relevant information and the labels or HTML tags that delimit strings to be extracted. For free text, an IE system needs several steps in addition to text extraction rules. These include syntactic analysis, semantic tagging, recognizers for domain objects such as person and company names, and discourse processing that makes inferences across sentence boundaries. Extraction rules A useful class of text that falls between these extremes has been largely inaccessible to IE systems. Such semi-structured text 1 is ungrammatical and often telegraphic in style, but does not follow any rigid format. Examples are certain medical records, equipment maintenance logs, and much useful information on the World Wide Web such as the apartment rental ad shown in Figure 1. Figure 1 will serve as a running example of semi-structured text. A typical example of extraction from free text is shown in Figure 3. This is from the "Management Succession" domain used in the Sixth Message Understanding Conference (MUC-6 1995), one of a series of ARPA-sponsored conferences that has promoted research in free text IE. The relevant information here 1 The database community uses the term semi-structured for what this paper calls structured text. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Ashish, N. and Knoblock, C. </author> <booktitle> "Semi-automatic Wrapper Generation for Internet Information Sources" in Proceedings of the Second IFCIS Conference on Cooperative Information Systems, </booktitle> <year> 1997. </year>
Reference-contexts: A bad choice of features can lead to overly restrictive rules that do not generalize well to new instances. Another system that induces rules for structured text is that of Ashish and Knoblock <ref> (Ashish and Knoblock 1997) </ref>. This system is designed for documents with a recursive structure, a regular pattern of sections divided into subsections. It learns a parser for such hierarchically structured text, using cues such as font size of headings and indentation.
Reference: <author> Breiman, L., Friedman, J., Olshen, R., Stone, C. </author> <title> Classification and regression trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: This is a key assumption in top-down decision tree induction such as C4.5 (Quinlan 1993) and CART <ref> (Breiman et al. 1984) </ref> and in the RIPPER rule induction system (Cohen 1996). For the IE problem that WHISK addresses, it is not possible to tabulate how often a term is associated with correct or incorrect extractions. <p> Another anomaly that is closely related to WHISK's rule representation is the behavior of empty rules. Top-down rule induction (Cohen 1996) and top-down decision tree induction (Quinlan 1993) <ref> (Breiman et al. 1984) </ref> assume that an empty rule covers all possible instances and that adding a term or test monotonically reduces the coverage of a rule. This would be the case if WHISK's rules were true regular expressions.
Reference: <author> Califf, M. E. and Mooney, R. </author> <title> Relational learning of pattern-match rules for information extraction. </title> <booktitle> In Working Papers of ACL-97 Workshop on Natural Language Learning, </booktitle> <pages> 9-15, </pages> <year> 1997. </year>
Reference-contexts: These systems are described in more detail later. Some operate only on rigidly structured text such as Kushmerick's Wrapper Induction (WI in the list below) (Kushmeric et al. 1997), or on semi-structured text such as Califf's RAPIER <ref> (Califf and Mooney 1997) </ref> or Freitag's SRV (Freitag 1998). Kushmerick's system learns rules to extract multiple slots of a case frame at once, but SRV and RAPIER extract single slots in isolation. <p> This is important where the input may have several case frames and it is critical to associate extracted slots together properly. Dayne Freitag's SRV (Freitag 1998) and Mary Elaine Califf's RAPIER <ref> (Califf and Mooney 1997) </ref> extends information extraction to semi-structured text, but can extract only isolated slots. This is adequate in domains where each instance has only one case frame. Like SRV and RAPIER, WHISK can handle either structured or semi-structured text and, like Wrapper Induction, extracts multiple slots at once. <p> To learn a rule with k slots would require training on all combinations of k phrases of suitable length in the document. Applying a rule to unseen text would also involve considering an exponential number of candidate phrases. RAPIER Califf's RAPIER system <ref> (Califf and Mooney 1997) </ref> also uses a form of logic programming and requires no prior syntactic analysis. The text is considered to have three fields centered around the target phrase: the target phrase itself, tokens before the target phrase, and tokens after it. <p> The next level of difficulty is semi-structured text, often telegraphic in style with much of the relevant information in a fairly small number of stereotyped contexts. On-line rental ads, seminar announcements, and job listings are examples of this genre. Freitag's SRV (Freitag 1998), Califf's RAPIER <ref> (Califf and Mooney 1997) </ref>, and WHISK have been developed specifically to process semi-structured text. The most challenging text style is free text, such as news stories, where there is wide variation in how relevant information is expressed and where judgments about relevancy often require subtle inferences.
Reference: <author> Cohen, W. </author> <title> Learning Trees and Rules with Set-valued Fea--tures. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 709-716, </pages> <year> 1996. </year>
Reference-contexts: This is a key assumption in top-down decision tree induction such as C4.5 (Quinlan 1993) and CART (Breiman et al. 1984) and in the RIPPER rule induction system <ref> (Cohen 1996) </ref>. For the IE problem that WHISK addresses, it is not possible to tabulate how often a term is associated with correct or incorrect extractions. <p> It is not clear whether this is an artifact of WHISK's rule representation or whether this is intrinsic to the information extraction problem. Another anomaly that is closely related to WHISK's rule representation is the behavior of empty rules. Top-down rule induction <ref> (Cohen 1996) </ref> and top-down decision tree induction (Quinlan 1993) (Breiman et al. 1984) assume that an empty rule covers all possible instances and that adding a term or test monotonically reduces the coverage of a rule. This would be the case if WHISK's rules were true regular expressions.
Reference: <author> Cohn, D., Atlas, L., Ladner, R. </author> <title> Improving generalization with active learning. </title> <booktitle> Machine Learning 15, </booktitle> <year> 1994. </year>
Reference-contexts: Neither of these cases will lead to improvement of the coverage or accuracy of the rule set. The amount of hand tagging required can be greatly reduced through selective sampling, a form of active learning <ref> (Cohn et al. 1994) </ref>, (Lewis and Gale 1994), (Dagan and Engelson 1996). When the rule set is empty the best that can be done is to select instances at random from a reservoir of untagged instances. <p> This makes the best use of small training sets. Selection of additional training instances to be hand-tagged is interleaved with the learning, to provide new training instances most likely to help increase precision or recall of the existing rule set. This is akin to the active learning paradigm <ref> (Cohn et al. 1994) </ref>, (Lewis and Gale 1994), (Dagan and Engelson 1996). The bias of WHISK is to find the most general rules that fit the training data.
Reference: <author> Dagan, I. and Engelson, S. </author> <title> Sample selection in natural language learning. </title> <editor> In Wermter, S., Riloff, E., and Scheller, G. (Eds.), </editor> <title> Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing. </title> <publisher> Springer, </publisher> <year> 1996. </year>
Reference-contexts: Neither of these cases will lead to improvement of the coverage or accuracy of the rule set. The amount of hand tagging required can be greatly reduced through selective sampling, a form of active learning (Cohn et al. 1994), (Lewis and Gale 1994), <ref> (Dagan and Engelson 1996) </ref>. When the rule set is empty the best that can be done is to select instances at random from a reservoir of untagged instances. <p> Selection of additional training instances to be hand-tagged is interleaved with the learning, to provide new training instances most likely to help increase precision or recall of the existing rule set. This is akin to the active learning paradigm (Cohn et al. 1994), (Lewis and Gale 1994), <ref> (Dagan and Engelson 1996) </ref>. The bias of WHISK is to find the most general rules that fit the training data. While each aspect of WHISK's general methodology is shared with other machine learning algorithms, there are characteristics of information extraction that make existing machine learning systems difficult to apply directly.
Reference: <author> Domingos, P. </author> <title> The RISE System: Conquering Without Separating. </title> <booktitle> In Proceedings of the Sixth IEEE International Conference on Tools with Artificial Intelligence. </booktitle> <pages> 704-707, </pages> <year> 1994. </year>
Reference-contexts: WHISK does not "divide and conquer" or even "separate and conquer" The entire training set remains available for testing further rules as in RISE <ref> (Domingos 1994) </ref>. This makes the best use of small training sets. Selection of additional training instances to be hand-tagged is interleaved with the learning, to provide new training instances most likely to help increase precision or recall of the existing rule set.
Reference: <author> Fisher, D., Soderland, S., McCarthy, J., Feng, F., Lehnert, W. </author> <title> Description of the UMass System as Used for MUC-6. </title> <booktitle> In Proceedings of the Sixth Message Understanding Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> 221-236, </pages> <year> 1995. </year>
Reference-contexts: Mr. B is out as chairman of Y and in as president of Z. analysis and tagging of person names, company names, and corporate posts. This example was processed by the University of Massachusetts BADGER syntactic analyzer <ref> (Fisher et al. 1995) </ref>. The sentence is segmented into subject, verb, prepositional phrase, and an ad hoc field REL V for the relative clause attached to the verb. Verb roots are inserted after each head verb, marked with the prefix "@".
Reference: <author> Freitag, D. </author> <title> Multistrategy learning for information extraction. </title> <booktitle> In Proceedings of the Fifteenth International Machine Learning Conference, </booktitle> <year> 1998. </year>
Reference-contexts: These systems are described in more detail later. Some operate only on rigidly structured text such as Kushmerick's Wrapper Induction (WI in the list below) (Kushmeric et al. 1997), or on semi-structured text such as Califf's RAPIER (Califf and Mooney 1997) or Freitag's SRV <ref> (Freitag 1998) </ref>. Kushmerick's system learns rules to extract multiple slots of a case frame at once, but SRV and RAPIER extract single slots in isolation. <p> Nick Kushmerick's Wrapper Induction (Kushmeric et al. 1997) require rigidly structured text, but allows multi-slot extraction that bundles together slots into a single case frame. This is important where the input may have several case frames and it is critical to associate extracted slots together properly. Dayne Freitag's SRV <ref> (Freitag 1998) </ref> and Mary Elaine Califf's RAPIER (Califf and Mooney 1997) extends information extraction to semi-structured text, but can extract only isolated slots. This is adequate in domains where each instance has only one case frame. <p> This system is handling a different, but complementary problem than WHISK. Systems for Semi-structured Text Research on systems that learn extraction rules for semi-structured text has been done recently by Dayne Freitag and by Mary Elaine Califf. SRV Freitag adopts a multistrategy approach and combines evidence from three classifiers <ref> (Freitag 1998) </ref>. The IE problem is transformed into a classification problem by limiting the system to single slot extraction. All possible phrases from the text up to a maximum length are considered as instances. <p> The next level of difficulty is semi-structured text, often telegraphic in style with much of the relevant information in a fairly small number of stereotyped contexts. On-line rental ads, seminar announcements, and job listings are examples of this genre. Freitag's SRV <ref> (Freitag 1998) </ref>, Califf's RAPIER (Califf and Mooney 1997), and WHISK have been developed specifically to process semi-structured text. The most challenging text style is free text, such as news stories, where there is wide variation in how relevant information is expressed and where judgments about relevancy often require subtle inferences.
Reference: <author> Huffman, S. </author> <title> Learning Information Extraction Patterns from Examples. Connectionist, Statistical, and Symbolic approaches to Learning for Natural Language Processing. </title> <publisher> Springer, </publisher> <pages> 246-260, </pages> <year> 1996. </year>
Reference-contexts: The next systems shown in Figure 5 can handle grammatical text such as news stories: Riloff's Au-toSlog (Riloff 1993), Soderland's CRYSTAL (Soder-land et al. 1995), (Soderland 1997), and Huffman's LIEP <ref> (Huffman 1996) </ref>. AutoSlog does single-slot extraction, LIEP does only multi-slot extraction, and CRYSTAL handles either. Each of these systems requires syntactic preprocessing of the text and semantic tagging. Given preprocessed input, CRYSTAL can be extended to handle semi-structured text (Soderland 1997a). <p> LIEP, HASTEN, and PALKA Other systems that learn text extraction rules are LIEP, HASTEN, and PALKA. Scott Huffman's LIEP <ref> (Huffman 1996) </ref> uses heuristics in a manner similar to AutoSlog, but learns multi-slot rules. In contrast to systems we have previously seen that cannot handle multi-slot extraction, LIEP cannot handle single slot extraction. It can find context for a slot only in terms of its syntactic relationship to other slots.
Reference: <author> Kim, J. and Moldovan, D. PALKA: </author> <title> A System for Linguistic Knowledge Acquisition. </title> <type> Technical Report PKPL 92-8, </type> <institution> USC Department of Electrical Engineering Systems, </institution> <year> 1992. </year>
Reference-contexts: To apply the HASTEN classifier, a new sentence is compared to each of the exemplars and a goodness-of-fit metric is multiplied by the learned weight to discount unreliable exemplars. HASTEN was used by one of the highest scoring systems in the MUC-6 evaluation. Jun-Tae Kim and Dan Moldovan's PALKA <ref> (Kim and Moldovan 1992) </ref> uses an induction method similar to Mitchell's candidate elimination algorithm. PALKA is computationally intensive and was implemented on a parallel computer.
Reference: <author> Krupka, G. </author> <title> Description of the SRA System as Used for MUC-6. </title> <booktitle> In Proceedings of the Sixth Message Understanding Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <pages> 221-236, </pages> <year> 1995. </year>
Reference-contexts: The same key word filtering is done before applying the rules on new text. There seems to be no mechanism for guarding against rules that overgeneralize, but high precision is reported when the test set has only relevant sentences. George Krupka's HASTEN <ref> (Krupka 1995) </ref> uses a k-nearest neighbor approach with a set of hand-picked instances as exemplars. Each exemplar represents a positive example of a multi-slot extraction.
Reference: <author> Kushmerick, N., Weld, D., Doorenbos, R. </author> <title> Wrapper Induction for Information Extraction. </title> <booktitle> In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, </booktitle> <year> 1997. </year>
Reference: <author> Lewis, D. and Gale, W. </author> <title> Training text classifiers by uncertainty sampling. </title> <booktitle> In Proceedings of ACM-SIGIR Conference on Information Retrieval, 1994. Proceedings of the Sixth Message Understanding Conference, </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1995. </year>
Reference-contexts: Neither of these cases will lead to improvement of the coverage or accuracy of the rule set. The amount of hand tagging required can be greatly reduced through selective sampling, a form of active learning (Cohn et al. 1994), <ref> (Lewis and Gale 1994) </ref>, (Dagan and Engelson 1996). When the rule set is empty the best that can be done is to select instances at random from a reservoir of untagged instances. <p> Selection of additional training instances to be hand-tagged is interleaved with the learning, to provide new training instances most likely to help increase precision or recall of the existing rule set. This is akin to the active learning paradigm (Cohn et al. 1994), <ref> (Lewis and Gale 1994) </ref>, (Dagan and Engelson 1996). The bias of WHISK is to find the most general rules that fit the training data.
Reference: <author> Quinlan, J.R. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: The implementation of many inductive learning systems makes a further assumption that a term (or test) operates on an instance independently of other terms, and that the test partitions the instances into disjoint classes. This is a key assumption in top-down decision tree induction such as C4.5 <ref> (Quinlan 1993) </ref> and CART (Breiman et al. 1984) and in the RIPPER rule induction system (Cohen 1996). For the IE problem that WHISK addresses, it is not possible to tabulate how often a term is associated with correct or incorrect extractions. <p> It is not clear whether this is an artifact of WHISK's rule representation or whether this is intrinsic to the information extraction problem. Another anomaly that is closely related to WHISK's rule representation is the behavior of empty rules. Top-down rule induction (Cohen 1996) and top-down decision tree induction <ref> (Quinlan 1993) </ref> (Breiman et al. 1984) assume that an empty rule covers all possible instances and that adding a term or test monotonically reduces the coverage of a rule. This would be the case if WHISK's rules were true regular expressions.
Reference: <author> Riloff, E. </author> <title> Automatically Constructing a Dictionary for Information Extraction Tasks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 811-816, </pages> <year> 1993. </year>
Reference-contexts: They identify the exact boundaries of the phrase to be extracted, requiring no later processing to trim away extraneous words. The next systems shown in Figure 5 can handle grammatical text such as news stories: Riloff's Au-toSlog <ref> (Riloff 1993) </ref>, Soderland's CRYSTAL (Soder-land et al. 1995), (Soderland 1997), and Huffman's LIEP (Huffman 1996). AutoSlog does single-slot extraction, LIEP does only multi-slot extraction, and CRYSTAL handles either. Each of these systems requires syntactic preprocessing of the text and semantic tagging. <p> Systems that learn text extraction rules for free text are rare. The systems most closely related to WHISK are Riloff's AutoSlog and Soderland's CRYSTAL. These are discussed as well as Huffman's LIEP, Krupka's HASTEN, and Kim and Moldovan's PALKA. AutoSlog AutoSlog <ref> (Riloff 1993) </ref> was the first system to learn text extraction rules from training examples. AutoSlog handles only single slot extraction and uses heuristics to create a rule from relevant examples that extracts the correct information from that example. <p> WHISK and CRYSTAL (Soderland et al. 1995) (Soder-land 1997) have a more expressive representation than the other systems and are more fully automated. WHISK operates at a finer granularity than systems such as CRYSTAL and AutoSlog <ref> (Riloff 1993) </ref>. Those systems identify the syntactic field, such as subject or direct object, that contains the target phrase. WHISK identifies delimiters of the target phrase exactly and requires no later processing to trim away extraneous words.
Reference: <author> Soderland, S., Fisher, D., Aseltine, J., Lehnert, W. </author> <title> CRYSTAL: Inducing a Conceptual Dictionary. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1314-1321, </pages> <year> 1995. </year>
Reference-contexts: This is when AutoSlog was used in conjunction with the same syntactic analyzer, semantic tagger, and discourse processing routines, as the hand-crafted rules. CRYSTAL A second system to learn text extraction rules is CRYSTAL <ref> (Soderland et al. 1995) </ref> (Soder-land 1997), which like AutoSlog, takes input that has been processed by a syntactic analyzer and a semantic tagger. CRYSTAL uses a bottom-up covering algorithm that begins with the most specific rule to cover a seed instance, then generalizes the rule by merging with similar rules. <p> Neither requires syntactic preprocessing for this genre of text. Each of the systems that learns extraction rules for free text has a variety of strengths and limitations. All of them, including WHISK, must operate in conjunction with a syntactic analyzer and a semantic tagger. WHISK and CRYSTAL <ref> (Soderland et al. 1995) </ref> (Soder-land 1997) have a more expressive representation than the other systems and are more fully automated. WHISK operates at a finer granularity than systems such as CRYSTAL and AutoSlog (Riloff 1993).
Reference: <author> Soderland, S. </author> <title> Learning Text Analysis Rules for Domain-specific Natural Language Processing. </title> <type> Ph.D. thesis, </type> <institution> technical report UM-CS-1996-087 University of Mas-sachusetts, Amherst, </institution> <year> 1997. </year>
Reference: <author> Soderland, S. </author> <title> Learning to Extract Text-based Information from the World Wide Web. </title> <booktitle> In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, </booktitle> <year> 1997. </year>
Reference-contexts: They identify the exact boundaries of the phrase to be extracted, requiring no later processing to trim away extraneous words. The next systems shown in Figure 5 can handle grammatical text such as news stories: Riloff's Au-toSlog (Riloff 1993), Soderland's CRYSTAL (Soder-land et al. 1995), <ref> (Soderland 1997) </ref>, and Huffman's LIEP (Huffman 1996). AutoSlog does single-slot extraction, LIEP does only multi-slot extraction, and CRYSTAL handles either. Each of these systems requires syntactic preprocessing of the text and semantic tagging. Given preprocessed input, CRYSTAL can be extended to handle semi-structured text (Soderland 1997a). <p> Details of implementation give each of these systems some advantages in the domains for which they were developed. For extraction from free text, machine learning is still trailing behind the performance of hand-coded rules. WHISK has performance comparable to Soder-land's CRYSTAL <ref> (Soderland 1997) </ref>, but offers an alternate method more than an advancement. Related Work This section first compares WHISK to other systems that learn text extraction rules for structured or semi-structured text, followed by systems that learn rules for free text. WHISK is then contrasted to related work in machine learning.
Reference: <author> Valiant, L. </author> <title> A Theory of the Learnable. </title> <journal> In Communications of the ACM, </journal> <volume> 27(11), </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: When to Stop Tagging How can the user gauge when to stop adding to the training data? An informal analogy to PAC analysis may be used <ref> (Valiant 1984) </ref>. The PAC formalism calculates how many randomly sampled training examples must be examined so that the probability is less than ffi that the error rate of a rule set is greater than *.
References-found: 20

