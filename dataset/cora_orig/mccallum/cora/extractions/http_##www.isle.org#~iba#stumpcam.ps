URL: http://www.isle.org/~iba/stumpcam.ps
Refering-URL: 
Root-URL: 
Title: Induction of One-Level Decision Trees  
Author: Wayne Iba Pat Langley 
Address: Moffett Field, CA 94035  Moffett Field, CA 94035  
Affiliation: AI Research Branch Mail Stop 269-2 NASA Ames Research Center  AI Research Branch Mail Stop 269-2 NASA Ames Research Center  
Abstract: In recent years, researchers have made considerable progress on the worst-case analysis of inductive learning tasks, but for theoretical results to have impact on practice, they must deal with the average case. In this paper we present an average-case analysis of a simple algorithm that induces one-level decision trees for concepts defined by a single relevant attribute. Given knowledge about the number of training instances, the number of irrelevant attributes, the amount of class and attribute noise, and the class and attribute distributions, we derive the expected classification accuracy over the entire instance space. We then examine the predictions of this analysis for different settings of these domain parameters, comparing them to exper imental results to check our reasoning. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Haussler, D. </author> <year> (1990). </year> <title> Probably approximately correct learning. </title> <booktitle> Proceedings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 1101-1108). </pages> <address> Boston, MA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Hirschberg, D. S., & Pazzani, M. J. </author> <year> (1991). </year> <title> Average-case analysis of a k-CNF learning algorithm (Technical Report 91-50). </title> <institution> Irvine: University of California, Department of Information & Computer Science. </institution>
Reference: <author> Holte, R. C. </author> <year> (1991). </year> <title> Very simple classification rules perform well on most data sets (Technical Report). </title> <type> Ottawa, </type> <institution> Canada: University of Ottawa, Computer Science Department. </institution>
Reference: <author> Kearns, M., Li, M., Pitt, L., & Valiant, L. G. </author> <year> (1987). </year> <title> Recent results on Boolean concept learning. </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning (pp. </booktitle> <pages> 337-352). </pages> <address> Irvine, CA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Kibler, D., & Langley, P. </author> <year> (1988). </year> <title> Machine learning as an experimental science. </title> <booktitle> Proceedings of the Third European Working Session on Learning (pp. </booktitle> <pages> 81-92). </pages> <address> Glasgow: </address> <publisher> Pittman. </publisher>
Reference-contexts: 1 INTRODUCTION In recent years, machine learning has made considerable progress in both the theoretical analysis of learning tasks (e.g., Kearns, Li, Pitt, & Valiant, 1987; Haus-sler, 1990) and in the experimental evaluation of specific algorithms <ref> (Kibler & Langley, 1988) </ref>. However, most theoretical work has remained disconnected from practical algorithms, and the worst-case predictions of the PAC learning framework have been strikingly different from results obtained in experiments. Recently, a few researchers have presented average-case formulations of particular algorithms.
Reference: <author> Littlestone, N. </author> <year> (1988). </year> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 285-318. </pages>
Reference: <author> Levine, M. </author> <year> (1966). </year> <title> Hypothesis behavior by humans during discrimination learning. </title> <journal> Journal of Experimental Psychology, </journal> <volume> 71, </volume> <pages> 331-338. </pages>
Reference-contexts: In addition, previous work in theoretical psychology has addressed the learning of "single attribute discriminations" in humans <ref> (Levine, 1966) </ref>. Also, Holte (1991) reports experimental results suggesting that, in many domains, decision stumps are nearly as accurate as full decision trees. Thus, despite its simplicity, the algorithm has some potential as a practical induction method.
Reference: <author> Mingers, J. </author> <year> (1989). </year> <title> An empirical comparison of selection measures for decision-tree induction. </title> <note> Machine Learning , 3 , 319-342. </note>
Reference: <author> Pazzani, M. J., & Sarrett, W. </author> <year> (1990). </year> <title> Average-case analysis of conjunctive learning algorithms. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 339-347). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1986a). </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning, </booktitle> <pages> 1 , 81-106. </pages>
References-found: 10

