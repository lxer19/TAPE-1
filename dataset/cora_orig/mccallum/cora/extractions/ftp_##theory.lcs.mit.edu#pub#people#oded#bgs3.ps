URL: ftp://theory.lcs.mit.edu/pub/people/oded/bgs3.ps
Refering-URL: http://theory.lcs.mit.edu/~oded/bgs.html
Root-URL: 
Email: e-mail: mihir@cs.ucsd.edu.  e-mail: oded@wisdom.weizmann.ac.il.  e-mail: madhu@watson.ibm.com.  
Phone: 2  3  
Title: Free Bits, PCPs and Non-Approximability|  
Author: Mihir Bellare Oded Goldreich Madhu Sudan 
Affiliation: Department of Applied Mathematics, Weizmann Institute of Sciences,  IBM T.J. Watson Research Center,  
Address: 92093, USA.  Rehovot, Israel.  Jerusalem, Israel.  P.O. Box 218, Yorktown Heights, NY 10598, USA.  
Note: In honor of Shimon Even's 60 th birthday.  Partially supported by grant No. 92-00226 from the US-Israel Binational Science Foundation (BSF),  
Date: December 31, 1995  
Abstract: Towards Tight Results 
Abstract-found: 1
Intro-found: 1
Reference: [Ad] <author> L. Adleman. </author> <title> Two theorems on random polynomial time. </title> <booktitle> Proceedings of the Nineteenth Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1978. </year>
Reference-contexts: ) (1 c) c * 1 , s 0 = (1 + * 2 ) s and r 0 = fl + maxf log 2 (* 2 1 (1 c)) ; log 2 (l) log 2 (* 2 Proof: The proof is reminiscent of Adleman's proof that RP P= poly <ref> [Ad] </ref>. Suppose we are given a pcp system for which we want to reduce the randomness complexity. The idea is that it suffices to choose the random pad for the verifier out of a relatively small set of possibilities (instead than from all 2 r possibilities).
Reference: [AKS] <author> M. Ajtai, J. Komlos and E. Szemeredi. </author> <title> Deterministic Simulation in Logspace. </title> <booktitle> Proceedings of the Nineteenth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1987. </year>
Reference-contexts: Specifically, using a randomized reduction we can transform FPCP 1; 1 2 [log; f ] into FPCP 1;2 k [log +k; k f ]. (This transformation is analogous to the well-known transformation of Berman and Schnitger [BeSc].) Alternatively, using a known deterministic amplification method based on <ref> [AKS, LPS] </ref> one can transform FPCP 1; 1 2 [log; f ] into FPCP 1;2 k [log +2k; k f ] (ignoring multiplicative factors of 1 + * for arbitrarily small * &gt; 0). (To the best of our knowledge this transformation has never appeared with a full proof.) Both alternatives <p> Arora and Safra [ArSa] reduced the randomness complexity of a PCP verifier for NP to logarithmic | they showed NP = PCP 1;1=2 [ coins = log ; query = p log N ]. They also observed that random bits can be recycled for error-reduction via the standard techniques <ref> [AKS, CW, ImZu] </ref>. The consequence was the first NP-hardness result for Max Clique approximation. <p> Another alternative description, carried out in the proof system, is presented in Section 5.2. The de-randomized error reduction method consists of applying general, de-randomized, error-reduction techniques to the proof system setting. The best method knows as the "Expander Walk" technique is due to Ajtai, Komlos and Szemeredi <ref> [AKS] </ref> (see also [CW, ImZu]). <p> The use of random walks on expander graphs for error reduction was suggested by Ajtai, Komlos and Szemeredi <ref> [AKS] </ref> (cf., [CW]). The use of random walks on expander graphs for gap amplification in the context of pcp originates in [ArSa]. The value of the constant multiplier of k in the randomness complexity of the resulting pcp system, depends on the expander graph used. <p> The constant d will be determined so that d &gt; 2 4+ 8 * ). It is well-known by now, that a random walk of length t in an expander avoids a set of density with probability at most ( + d ) t (cf., <ref> [AKS, Kah] </ref>). Thus, as a preparation step, we reduce the error probability of the pcp system to p = d 2 d This is done using the trivial reduction of Proposition 5.2.1.
Reference: [ASE] <author> N. Alon, J. Spencer and P. Erdos. </author> <title> The Probabilistic Method. </title> <publisher> John Wiley and Sons, </publisher> <year> 1992. </year>
Reference-contexts: Clearly, q x 3 q i (1 q) 3i . Thus, the expected value of p x (Q) equals p (q) = i=0 2 3i 3 ! 3 (1 q)q 2 + 4 1 (1 q) 3 Using the method of conditional probabilities <ref> [ASE] </ref>, given x, we can construct in (deterministic) polynomial-time a set Q satisfying p x (Q) p (q).
Reference: [AmKa] <author> E. Amaldi and V. Kann. </author> <title> The complexity and approximability of finding maximum feasible subsystems of linear relations. </title> <journal> Theoretical Computer Science, </journal> <volume> vol. 147, </volume> <pages> pages 181-210, </pages> <year> 1995. </year>
Reference-contexts: Also the case of maximum satisfiable linear constraints over larger fields (of size q) has been considered by Amaldi and Kann <ref> [AmKa] </ref>, who show that this problem is hard to approximate to within a factor of q * for some universal * &gt; 0. Theorem 3.7.6 Let GapParity c;s be defined analogously to the above. Then, for c = 6=7 and c s &lt; 8=7, GapParity c;s is NP-hard.
Reference: [Ar] <author> S. Arora. </author> <title> Reductions, Codes, PCPs and Inapproximability. </title> <type> Manuscript, </type> <month> May </month> <year> 1995. </year>
Reference-contexts: algorithms and hardness results, for Max-3-SAT and Max-2-SAT; Section 3.12 for previous work on Max Clique and history of various chromatic number reductions. 18 Bellare, Goldreich, Sudan 1.7 Related work Following the presentation of our results, Arora has also investigated the limitations of proof checking techniques in proving non-approximability results <ref> [Ar] </ref>. Like in our free-bit lower bound result, he tries to assess the limitations of current techniques by making some assumptions about these techniques and then showing a lower bound.
Reference: [ABSS] <author> S. Arora, L. Babai, J. Stern and Z. Sweedyk. </author> <title> The hardness of approximate optima in lattices, codes and linear equations. </title> <booktitle> FOCS, </booktitle> <year> 1993. </year>
Reference-contexts: Here we provide a stronger bound via a direct reduction from the MaxSNP verifier. Before continuing, we remark that the problem of maximizing the number of satisfiable equations should not be confused with the "complementary" problem of minimizing the number of violated constraints, investigated by Arora et. al. <ref> [ABSS] </ref>. Also the case of maximum satisfiable linear constraints over larger fields (of size q) has been considered by Amaldi and Kann [AmKa], who show that this problem is hard to approximate to within a factor of q * for some universal * &gt; 0.
Reference: [ALMSS] <author> S. Arora, C. Lund, R. Motwani, M. Sudan and M. Szegedy. </author> <title> Proof verification and intractability of approximation problems. </title> <booktitle> Proceedings of the Thirty Third Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1992. </year>
Reference-contexts: First, one exhibits an appropriate proof system for NP. Then one applies the FGLSS reduction. The factor indicated hard depends on the proof system parameters. A key factor in getting better results has been the distilling of appropriate pcp-parameters. The sequence of works <ref> [FGLSS, ArSa, ALMSS, BGLR, FeKi, BeSu] </ref> lead us through a sequence of parameters: query complexity, free-bit complexity and, finally, for the best known results, amortized free-bit complexity. <p> (2 m ) 2 3m 2m 3m free-bits [BeSu] Free Bits in PCP 11 Problem Approx Non-Approx Factor Due to New Factor Previous Factor Assumption Max-3-SAT 1:258 [Ya, GoWi2, SSTW] 1:038 1 + 1 72 [BeSu] P 6= NP Max-E3-SAT 1 + 1 7 folklore 1 + 1 26 unspecified <ref> [ALMSS] </ref> P 6= NP Max-2-SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP Max-CUT 1:139 [GoWi2] 1:015 unspecified [ALMSS] P 6= NP Min-VC 2 o (1) [BaEv, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] <p> Factor Assumption Max-3-SAT 1:258 [Ya, GoWi2, SSTW] 1:038 1 + 1 72 [BeSu] P 6= NP Max-E3-SAT 1 + 1 7 folklore 1 + 1 26 unspecified <ref> [ALMSS] </ref> P 6= NP Max-2-SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP Max-CUT 1:139 [GoWi2] 1:015 unspecified [ALMSS] P 6= NP Min-VC 2 o (1) [BaEv, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o <p> NP Max-E3-SAT 1 + 1 7 folklore 1 + 1 26 unspecified <ref> [ALMSS] </ref> P 6= NP Max-2-SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP Max-CUT 1:139 [GoWi2] 1:015 unspecified [ALMSS] P 6= NP Min-VC 2 o (1) [BaEv, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 coR ~ P Number N 1 1 <p> We have the advantage of being able to use, at the outer level, the verifier of Raz [Raz] which appeared only recently and was not available to previous works. The inner level verifier relies on the use of a "good" encoding scheme. Since <ref> [ALMSS] </ref>, constructions of this verifier have used the Hadamard Code for this purpose. In this paper we change this aspect of the protocol and use instead a much more redundant code which we call the long code. <p> After the work of [FGLSS] the field took off in two major directions. One was to extend the interactive proof approach to apply also to other optimization problems. Direct reductions from proofs were used to show hardness of quadratic programming [BeRo, FeLo], Max-3-SAT <ref> [ALMSS] </ref>, set cover [LuYa], and other problems [Be]. Also, reductions from Max Clique lead to hardness results for the chromatic number [LuYa] and other problems [Zu], while previous reductions from Max-3-SAT lead to hardness results for all of Max-SNP [PaYa]. <p> This work has introduced the idea of recursive proof checking which turned out to play a fundamental role in all subsequent developments. Interestingly, the idea of encoding inputs in an error-correcting form (as suggested in [BFLS]) is essential to make "recursion" work. Arora, Lund, Motwani, Sudan and Szegedy <ref> [ALMSS] </ref>, have reduced the query complexity of pcp systems for NP to a constant, while preserving the logarithmic randomness complexity; namely, they have shown that NP = PCP 1;1=2 [log; O (1)]. This immediately implied the NP-hardness of approximating Max Clique within N * , for some * &gt; 0. <p> This immediately implied the NP-hardness of approximating Max Clique within N * , for some * &gt; 0. Furthermore, it also implied that Max-3-Sat is NP-hard to approximate to within some constant factor <ref> [ALMSS] </ref> and so is the entire class Max-SNP [PaYa]. Attempts to improve the constant in the exponent of the Max Clique hardness factor, and also improve the constant values of the hardness factors in the Max-SNP hardness results, begin with Bellare, Goldwasser, Lund and Russell [BGLR]. <p> Free Bits in PCP 31 Employing the FRS-method [FRS] to any PCP (log,O (1))-system for NP (e.g., <ref> [ALMSS] </ref>) one gets a canonical verifier which is ffi-good for some ffi &lt; 1. Using the Parallel Repetition Theorem of Raz, we obtain our starting point - Lemma 3.4.2 (construction of outer verifiers [Raz]): Let L 2 NP. <p> The latter explains why we are interested in outer verifiers which achieve a constant, but arbitrarily small, error *. For completeness we provide a proof, following the ideas of <ref> [ArSa, ALMSS, BGLR] </ref>. Theorem 3.4.5 (the composition theorem): Let V outer be a (l; l 1 )-canonical outer verifier. Suppose it is *-good for L. Let V inner be an (l; l 1 )-canonical inner verifier that is (; ffi 1 ; ffi 2 )-good. <p> However, it is a different aspect of constant prover proofs that is of more direct concern to us. This aspect is the use of constant-prover proof systems as the penultimate step of the recursion, and begins with <ref> [ALMSS] </ref>. It is instrumental in getting PCP systems with only a constant number of queries. Their construction requires that these proof systems have low complexity: error which is any constant, and randomness and answer sizes that are preferably logarithmic. <p> The available constant-prover proof systems appear in Figure 3.1 and are discussed below. The two-prover proofs of Lapidot-Shamir and Feige-Lovasz [LaSh, FeLo] had poly-logarithmic randomness and answer sizes, so <ref> [ALMSS] </ref> used a modification of these, in the process increasing the number of provers to a constant much larger than two. The later constructions of few-prover proofs of [BGLR, Ta, FeKi] lead to better non-approximability results. <p> Free Bits in PCP 35 Due to Provers Coins Answer size Canonical? Can be made canonical? [LaSh, FeLo] 2 polylog polylog No Yes [BeSu] <ref> [ALMSS] </ref> poly (* 1 ) log polylog No ?? [BGLR] 4 log polyloglog No ?? [Ta] 3 log O (1) No ?? [FeKi] 2 log O (1) No At cost of one more prover [BeSu] [Raz] 2 log O (1) Yes (NA) *. <p> This makes an ideal starting point. To simplify the definitions above we insisted on constant answer size and two provers from the start. The inner verifiers used in all previous works are based on the use of the Hadamard code constructions of <ref> [ALMSS] </ref>. (The improvements mentioned above are obtained by checking this same code in more efficient ways). We instead use a new code, namely the long code, as the basis of our inner verifiers. <p> Their analysis was used in the proof system and Max-3-SAT non-approximability result of <ref> [ALMSS] </ref>. Interest in the tightness of the analysis from the point of view of improving the Max-3-SAT non-approximability began with [BGLR]. They showed that ffi A 3x A 6x 2 A , for every A. <p> This will imply that the Max-X-SAT problem is hard to approximate within a factor equal to the reciprocal of the gap, unless P = NP. Free Bits in PCP 49 Due to Assuming Factor Technique <ref> [ALMSS] </ref> P 6= NP some constant NP PCP 1;1=2 [ log; O (1) ]; Reduction of this to Max-3-SAT. [BGLR] e P 6= N e P 94=93 Framework; better analyses; uses proof systems of [LaSh, FeLo]. [BGLR] P 6= NP 113=112 New four-prover proof systems. [FeKi] P 6= NP 94=93 New <p> Specifically, Goemans and Williamson [GoWi2] exhibited a polynomial time algorithm achieving an approximation factor of 1 0:878 1:139, and consequently Feige and Goemans [FeGo] exhibited an algorithm achieving 1 0:931 1:074. Non-approximability. Non-approximability results for Max-SNP problems begin with <ref> [ALMSS] </ref> who proved that there exists a constant * &gt; 0 such that Gap-3-SAT 1;1* is NP-hard. <p> The basic paradigm of their reduction has been maintained in later improvements. factor) begin with [BGLR]. They used Hadamard code based inner verifiers following <ref> [ALMSS] </ref>. They also introduced a framework for better analysis, and improved some previous analyses; we exploit in particular their better analyses of linearity testing (cf. Section 3.5) and of Freivalds's matrix multiplication test (cf. Lemma 3.5.4). <p> Recently, in a breakthrough result, Goemans and Williamson [GoWi2] gave a new algorithm which achieves a ratio of 1 0:878 = 1:139 for this problem. On the other hand, [PaYa] give an approximation preserving reduction from Max-3-SAT to Max-CUT. Combined with <ref> [ALMSS] </ref> this shows that there exists a constant ff &gt; 1 such that approximating Max-CUTwithin a factor of ff is NP-hard. <p> As for c 0 , it equals 3ff 1 +2ff 2 3m 1 +2m 2 &gt; 0:6. 3.9 Free bits and vertex cover It is known that approximating the minimum vertex cover of a graph to within a 1 + * factor is hard, for some * &gt; 0 <ref> [PaYa, ALMSS] </ref>. However, we do not know of any previous attempt to provide a lower bound for *. <p> The version of Min-VC in which one restricts attention to graphs of degree bounded by a constant B, is Max-SNP complete for suitably large B [PaYa]. In particular they provide a reduction from Max-3-SAT. Combined with <ref> [ALMSS] </ref> this implies the existence of a constant ffi &gt; 0 such that approximating Min-VC within a factor of 1 + ffi is hard unless P = NP. No explicit value of ffi has been stated until now. <p> can construct PCPs for NP using q queries in the worst case (and q av on the average) to achieve a soundness 4 This is in contrast to the MaxCUT implementation where the same non-auxiliary vertices were used in all gadgets. 68 Bellare, Goldreich, Sudan Due to q q av <ref> [ALMSS] </ref> some constant some constant [BGLR] 36 29 [FeKi] 32 24 This paper 11 10.9 logarithmic randomness; that is, results of the form of Eq. (3.14). error of 1=2. We allow only logarithmic randomness. <p> Previous work. It was shown by <ref> [ALMSS] </ref> that there are constants q; q av for which (3.14) is achieved. Reductions in the values of these numbers obtained since then are depicted in Figure 3.9. The interest of [BGLR] in these numbers was to improve non-approximability factors for Max Clique. <p> must be at least 2 1000 , which is more than the number of particles in the universe, before the factor N * exceeds 2! 80 Bellare, Goldreich, Sudan Due to Factor Assumption [FGLSS] 2 log 1* N for any * &gt; 0 NP 6 e P [ArSa] 2 p <ref> [ALMSS] </ref> N * for some * &gt; 0 P 6= NP [BGLR] N 1=25 NP 6 coR e P [BGLR] N 1=30 NP 6= coRP [FeKi] N 1=15 NP 6= coRP [BeSu] N 1=4 NP 6 coR e P [BeSu] N 1=6 P 6= NP This paper N 1=4 P 6= <p> To discuss the quality of reductions, let us, following [BeSu], define an (a; b)- reduction to be one that achieves ffi = 1 ab+(b=*) = * The first reduction, namely that of Lund and Yannakakis [LuYa], was a (1; 5)-reduction. Via the Max Clique hardness results of <ref> [ArSa, ALMSS] </ref> this implies the chromatic number is hard to approximate within N ffi for some ffi &gt; 0. But, again, ffi is very, very small. Improvements to ffi are a function both of improvements to * and the values a; b for which (a; b)-reductions are available. <p> We remark that 1 A Levin-reduction is a polynomial-time many-to-one reduction which is augmented by corresponding polynomial-time witness transformations. 98 Bellare, Goldreich, Sudan testing "closeness" to codewords with respect to codes of large distance is essential in all known pcp constructions <ref> [BFLS, FGLSS, ArSa, ALMSS, BGLR, FeKi, BeSu] </ref>. The absolute distance between two words w; u 2 f0; 1g n , denoted (w; u), is the number of bits on which w and u disagree. <p> The following proposition explores the limitations of probabilistically checkable proof systems which use logarithmic randomness and upto three queries. Some of the qualitative assertions are well-known; for example, when considering perfect completeness, 3 queries are the minimum needed (and sufficient <ref> [ALMSS] </ref>) to get above P.
Reference: [ArSa] <author> S. Arora and S. Safra. </author> <title> Probabilistic checking of proofs: a new characterization of NP. </title> <booktitle> Proceedings of the Thirty Third Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1992. </year>
Reference-contexts: First, one exhibits an appropriate proof system for NP. Then one applies the FGLSS reduction. The factor indicated hard depends on the proof system parameters. A key factor in getting better results has been the distilling of appropriate pcp-parameters. The sequence of works <ref> [FGLSS, ArSa, ALMSS, BGLR, FeKi, BeSu] </ref> lead us through a sequence of parameters: query complexity, free-bit complexity and, finally, for the best known results, amortized free-bit complexity. <p> We ignore, as usual, terms of N * where * &gt; 0 is an arbitrary positive constant. 1.3.3 Techniques As in all recent constructions of efficient pcp's our construction also relies on the use of recursive construction of verifiers, introduced by Arora and Safra <ref> [ArSa] </ref>. We have the advantage of being able to use, at the outer level, the verifier of Raz [Raz] which appeared only recently and was not available to previous works. The inner level verifier relies on the use of a "good" encoding scheme. <p> The first stage of this enterprise started with the work of Arora and Safra <ref> [ArSa] </ref> which, showing that NP PCP 1;1=2 [log; o (log)], provided the first strong NP-hardness result for Max Clique (specifically, a hardness factor of 2 p log N ). <p> The reduction of [FGLSS] identified the parameters of query complexity (number of binary queries), randomness complexity and error. The class PCP 1;1=2 [r; q] was made explicit by <ref> [ArSa] </ref>. The parameterization was expanded by [BGLR] to explicitly consider the answer size (the oracle is now allowed to return more than one bit at a time) and query size- their notation included five parameters: randomness, number of queries, size of each query, size of each answer, and error probability. <p> Thus, verification in it amounts to checking that the first answer satisfies some predicate and that the second answer equals the value obtained from the first answer. Following the "proof composition" paradigm of Arora and Safra <ref> [ArSa] </ref>, we will "encode" the answers of the two provers under a suitable code and then, "recursively", check these encodings. As usual, we will check both that these encodings are valid and that they correspond to answer which would have been accepted by the original verifier. <p> It is useful to understand these things before proceeding to the tests. Overview. The constructions of efficient proofs that follow will exploit the notion of recursive verifier construction due to Arora and Safra <ref> [ArSa] </ref>. We will use just one level of recursion. We first define a notion of a canonical outer verifier whose intent is to capture two-prover one-round proof systems [BGKW] having certain special properties; these verifiers will be our starting point. We then define a canonical inner verifier. <p> The latter explains why we are interested in outer verifiers which achieve a constant, but arbitrarily small, error *. For completeness we provide a proof, following the ideas of <ref> [ArSa, ALMSS, BGLR] </ref>. Theorem 3.4.5 (the composition theorem): Let V outer be a (l; l 1 )-canonical outer verifier. Suppose it is *-good for L. Let V inner be an (l; l 1 )-canonical inner verifier that is (; ffi 1 ; ffi 2 )-good. <p> They also initiated work on improving the factors and assumptions via better proof systems. The best result in their paper is indicated in Figure 3.13. Arora and Safra <ref> [ArSa] </ref> reduced the randomness complexity of a PCP verifier for NP to logarithmic | they showed NP = PCP 1;1=2 [ coins = log ; query = p log N ]. They also observed that random bits can be recycled for error-reduction via the standard techniques [AKS, CW, ImZu]. <p> of the graph must be at least 2 1000 , which is more than the number of particles in the universe, before the factor N * exceeds 2! 80 Bellare, Goldreich, Sudan Due to Factor Assumption [FGLSS] 2 log 1* N for any * &gt; 0 NP 6 e P <ref> [ArSa] </ref> 2 p [ALMSS] N * for some * &gt; 0 P 6= NP [BGLR] N 1=25 NP 6 coR e P [BGLR] N 1=30 NP 6= coRP [FeKi] N 1=15 NP 6= coRP [BeSu] N 1=4 NP 6 coR e P [BeSu] N 1=6 P 6= NP This paper N <p> To discuss the quality of reductions, let us, following [BeSu], define an (a; b)- reduction to be one that achieves ffi = 1 ab+(b=*) = * The first reduction, namely that of Lund and Yannakakis [LuYa], was a (1; 5)-reduction. Via the Max Clique hardness results of <ref> [ArSa, ALMSS] </ref> this implies the chromatic number is hard to approximate within N ffi for some ffi &gt; 0. But, again, ffi is very, very small. Improvements to ffi are a function both of improvements to * and the values a; b for which (a; b)-reductions are available. <p> The best method knows as the "Expander Walk" technique is due to Ajtai, Komlos and Szemeredi [AKS] (see also [CW, ImZu]). It is easy to see that this applies in the pcp context. (The usage of these methods in the pcp context begins with <ref> [ArSa] </ref>.) It turns out that the (constant) parameters of the expander, specifically the ratio def log 2 d log 2 , where d is the degree of the expander and is the second eigenvalue (of its adjacency matrix), play an important role here. <p> We remark that 1 A Levin-reduction is a polynomial-time many-to-one reduction which is augmented by corresponding polynomial-time witness transformations. 98 Bellare, Goldreich, Sudan testing "closeness" to codewords with respect to codes of large distance is essential in all known pcp constructions <ref> [BFLS, FGLSS, ArSa, ALMSS, BGLR, FeKi, BeSu] </ref>. The absolute distance between two words w; u 2 f0; 1g n , denoted (w; u), is the number of bits on which w and u disagree. <p> The use of random walks on expander graphs for error reduction was suggested by Ajtai, Komlos and Szemeredi [AKS] (cf., [CW]). The use of random walks on expander graphs for gap amplification in the context of pcp originates in <ref> [ArSa] </ref>. The value of the constant multiplier of k in the randomness complexity of the resulting pcp system, depends on the expander graph used. Specifically, using a degree d expander graph with second eigenvalue yields a factor of log 2 d 1+log 2 .
Reference: [Bab] <author> L. Babai. </author> <title> Trading Group Theory for Randomness. </title> <booktitle> Proceedings of the Seventeenth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1985. </year>
Reference-contexts: The indication of higher factors, and results for other problems, had to wait for the interactive proof approach. Interactive proofs were introduced by Goldwasser, Micali and Rackoff [GMR] and Babai <ref> [Bab] </ref>. Ben-Or, Goldwasser, Kilian and Wigderson [BGKW] extended these ideas to define a notion of multi-prover interactive proofs. <p> PCP 111 (1) (PCP with 1 query is relatively very weak): For all admissible functions s; c : Z + ! [0; 1], so that c (n) s (n) is non-negligible 1 PCP c;s [poly; 1] AM where AM is the class of languages having one-round Arthur-Merlin proof systems (cf., <ref> [Bab] </ref>). (2) (One-sided error pcp with 2 queries is relatively weak): For all admissible functions s : Z + ! [0; 1] strictly less than 1, PCP 1;s [poly; 2] PSPACE. (3) (Two-sided error pcp with 2 queries is not weak): On the other hand, there exists 0 &lt; s &lt; <p> We stress that all the transformations maintain the number of rounds upto a constant and that the constant-round Arthur-Merlin hierarchy collapses to one round <ref> [Bab] </ref>. Proof of Proposition 5.1.6, Parts (3) and (4): For these parts we observe that the proof systems used in the corresponding items of the proof of Proposition 5.1.2, do "scale-up".
Reference: [BFL] <author> L. Babai, L. Fortnow and C. Lund. </author> <title> Non-deterministic Exponential time has two-prover interactive protocols. </title> <booktitle> Proceedings of the Thirty First Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1990. </year> <type> 123 124 Bellare, Goldreich, Sudan </type>
Reference-contexts: Trying to understand the power of pcp systems with low free-bit complexity, we have waived the bound on the randomness complexity. Recall that in this case pcp systems are able to recognize non-deterministic exponential time (i.e., NEXPT = PCP 1;1=2 [poly; poly]) <ref> [BFL] </ref>. <p> These techniques were used by Shamir [Sh] to show that IP = PSPACE. A central result which enabled the approximation connection is that of Babai, Fortnow and Lund <ref> [BFL] </ref> who showed that the class MIP equals the class NEXP (i.e., languages recognizable in non-deterministic exponential time). The latter result has been "scaled-down" to the NP-level by two independent groups of researchers. <p> In particular, these authors were able to "scale-down" the proof system of <ref> [BFL] </ref> to indicate strong non-approximability factors of 2 log * N for some * &gt; 0, assuming NP is not in quasi-polynomial deterministic time. They also initiated work on improving the factors and assumptions via better proof systems. The best result in their paper is indicated in Figure 3.13. <p> The results we obtain are somewhat analogous to those of Proposition 5.1.2. Recall that PCP 1; 1 2 [poly; poly] equals NEXPT (Non-deterministic exponential time) <ref> [BFL] </ref>. Thus, the power of pcp systems with polynomial randomness has to be compared against NEXPT.
Reference: [BFLS] <author> L. Babai, L. Fortnow, L. Levin, and M. Szegedy. </author> <title> Checking computations in poly-logarithmic time. </title> <booktitle> Proceedings of the Twenty Third Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1991. </year>
Reference-contexts: The latter result has been "scaled-down" to the NP-level by two independent groups of researchers. Babai, Fortnow, Lund and Szegedy <ref> [BFLS] </ref> showed that if the input is encoded using a special error-correcting code (for which encoding and decoding Free Bits in PCP 17 can be performed in polynomial-time) then NP has transparent proof systems (i.e., it is possible to verify the correctness of the proof in poly-logarithmic time). <p> This work has introduced the idea of recursive proof checking which turned out to play a fundamental role in all subsequent developments. Interestingly, the idea of encoding inputs in an error-correcting form (as suggested in <ref> [BFLS] </ref>) is essential to make "recursion" work. Arora, Lund, Motwani, Sudan and Szegedy [ALMSS], have reduced the query complexity of pcp systems for NP to a constant, while preserving the logarithmic randomness complexity; namely, they have shown that NP = PCP 1;1=2 [log; O (1)]. <p> Interestingly, as shown by <ref> [BFLS, FGLSS] </ref>, this framework can be applied in a meaningful manner also to languages in NP. <p> These works provide the verifier V with a "written" proof, modeled as an oracle to which V provides the "address" of a bit position in the proof string and is returned the corresponding bit of the proof. Babai et. al. <ref> [BFLS] </ref> suggested a model in which the instances are encoded in a special (polynomial-time computable and decodable) error-correcting code and the verifier works in polylogarithmic time. <p> Free bits are implicit in [FeKi] and formalized in [BeSu]. Amortized free bits are introduced in [BeSu] but formalized a little better here. Proof sizes were considered in <ref> [BFLS, PoSp] </ref>. We consider them here for a different reason-they play an important role in that the randomized FGLSS reduction [BeSc, Zu] depends actually on this rather than the randomness. <p> We remark that 1 A Levin-reduction is a polynomial-time many-to-one reduction which is augmented by corresponding polynomial-time witness transformations. 98 Bellare, Goldreich, Sudan testing "closeness" to codewords with respect to codes of large distance is essential in all known pcp constructions <ref> [BFLS, FGLSS, ArSa, ALMSS, BGLR, FeKi, BeSu] </ref>. The absolute distance between two words w; u 2 f0; 1g n , denoted (w; u), is the number of bits on which w and u disagree.
Reference: [BaEv1] <author> R. Bar-Yehuda and S. </author> <title> Even. A linear time approximation algorithm for the weighted vertex cover problem. </title> <booktitle> In Jour. of Algorithms Vol. </booktitle> <volume> 2, </volume> <year> 1981, </year> <pages> pages 198-201. </pages>
Reference-contexts: There is a simple polynomial time algorithm to approximate Min-VC in unweighted graphs within a factor of 2. The algorithm, due to F. Gavril (cf. [GJ2]), consists of taking all vertices which appear in a maximal matching of the graph. For weighted graphs, Bar-Yehuda and Even <ref> [BaEv1] </ref> and Hochbaum [Hoc], gave algorithms achieving the same approximation factor. The best known algorithm today achieves a factor only slightly better, namely 2 (log log jV j)=(2 log jV j) [BaEv, MoSp].
Reference: [BaEv] <author> R. Bar-Yehuda and S. </author> <title> Even. A local ratio theorem for approximating the weighted vertex cover problem. In Analysis and Design of Algorithms for Combinatorial Problems Vol. 25 of Annals of Discrete Math, </title> <publisher> Elsevier, </publisher> <year> 1985. </year>
Reference-contexts: 1 + 1 72 [BeSu] P 6= NP Max-E3-SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max-2-SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP Max-CUT 1:139 [GoWi2] 1:015 unspecified [ALMSS] P 6= NP Min-VC 2 o (1) <ref> [BaEv, MoSp] </ref> 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 <p> Recall that the latter is approximable within 2-o (1) <ref> [BaEv, MoSp] </ref>. Our results for Max-CUTand Max-2-SAT show that it is not possible to find a solution with value which is only 1% away from being optimal. <p> For weighted graphs, Bar-Yehuda and Even [BaEv1] and Hochbaum [Hoc], gave algorithms achieving the same approximation factor. The best known algorithm today achieves a factor only slightly better, namely 2 (log log jV j)=(2 log jV j) <ref> [BaEv, MoSp] </ref>.
Reference: [BaMo] <author> R. Bar-Yehuda and S. Moran. </author> <title> On approximation problems related to the independent set and vertex cover problems. </title> <journal> Discrete Applied Mathematics Vol. </journal> <volume> 9, </volume> <year> 1984, </year> <pages> pages 1-10. </pages>
Reference-contexts: to the hardness of approximating Min-VC was given by Bar-Yehuda and Moran who showed that, for every k 2 and * &gt; 0, a 1 + 1 k * approximator for (finding) a minimum vertex cover would yield an algorithm for coloring (k +1)-colorable graphs using only logarithmically many colors <ref> [BaMo] </ref>. The version of Min-VC in which one restricts attention to graphs of degree bounded by a constant B, is Max-SNP complete for suitably large B [PaYa]. In particular they provide a reduction from Max-3-SAT.
Reference: [Be] <author> M. Bellare. </author> <title> Interactive proofs and approximation: reductions from two provers in one round. </title> <booktitle> Proceedings of the Second Israel Symposium on Theory and Computing Systems, </booktitle> <year> 1993. </year>
Reference-contexts: One was to extend the interactive proof approach to apply also to other optimization problems. Direct reductions from proofs were used to show hardness of quadratic programming [BeRo, FeLo], Max-3-SAT [ALMSS], set cover [LuYa], and other problems <ref> [Be] </ref>. Also, reductions from Max Clique lead to hardness results for the chromatic number [LuYa] and other problems [Zu], while previous reductions from Max-3-SAT lead to hardness results for all of Max-SNP [PaYa].
Reference: [BCHKS] <author> M. Bellare, D. Coppersmith, J. H -astad, M. Kiwi and M. Sudan. </author> <title> Linearity testing in characteristic two. </title> <type> Manuscript, </type> <month> November </month> <year> 1994. </year>
Reference-contexts: The improvement in the complexities of the proof systems is the main source of our improved non-approximability results. In addition we also use (for the Max-SAT and Max-CUT problems) a recent improvement in the analysis of linearity testing <ref> [BCHKS] </ref> and introduce special (problem specific) gadgets which represent the various tests. Credits and histories pertaining to each topic are discussed alongside the topic. <p> The idea is to exploit the characterization of Proposition 3.3.2. Thus we will perform (on A) a linearity test, and then a "respect of monomial basis" test. Linearity testing is well understood, and we will use the test of [BLR], with the analyses of <ref> [BLR, BGLR, BCHKS] </ref>. The main novelty is the respect of monomial basis test. Circuit and projection. Having established that A is close to some evaluation operator E a , we now want to test two things. The first is that h (a) = 0 for some predetermined function h. <p> We want to lower bound the probability 1 LinPass (A) that the test rejects when its inputs f 1 ; f 2 are chosen at random, as a function of x = Dist (A; Lin). The following lemma, due to Bellare et. al. <ref> [BCHKS] </ref>, gives the best known lower bound today. Detailed description of the history of developments in this area follows. Lemma 3.5.3 [BCHKS] Let A: F l ! and let x = Dist (A; Lin). <p> The following lemma, due to Bellare et. al. <ref> [BCHKS] </ref>, gives the best known lower bound today. Detailed description of the history of developments in this area follows. Lemma 3.5.3 [BCHKS] Let A: F l ! and let x = Dist (A; Lin). <p> It was shown in <ref> [BCHKS] </ref> (see below) that this combined lower bound is close to the best one possible. History. <p> The work of <ref> [BCHKS] </ref> focused on this case and improved the bound on ffi A for the case x A 1 4 where A: GF (2) n ! GF (2). Specifically, they showed that ffi A 45=128 for x A 1 4 which establishes the second segment of lin . <p> They also showed that ffi A x A , for every A: GF (2) n ! GF (2). Combining the three lower bounds, they have derived the three-segment lower bound stated in Lemma 3.5.3. The optimality of the above analysis has been demonstrated as well in <ref> [BCHKS] </ref>. Essentially 1 , for every x 5=16 there are functions A: GF (2) n ! GF (2) witnessing ffi A = lin (x A ) with x A = x. <p> In particular, for x = 5=16 (and n 4), there is a function A with 1=4 &lt; x A &lt; 1=2 and ffi A = 45=128 = lin (x A ). For the interval ( 5 16 ; 1 2 ], no tight results are known. Instead, <ref> [BCHKS] </ref> reports of computer constructed examples of functions A: GF (2) n ! GF (2) with x A in every interval [ k 100 ], for k = 32; 33; :::; 49, and ffi A &lt; lin (x A ) + 1 20 . <p> Certainly, we improve over these works thanks to the use of the new long code based inner verifier, the atomic tests and their analysis in Section 3.5, the new idea of folding, and the improved analysis of linearity testing due to <ref> [BCHKS] </ref>. 3.6.3 Another application: minimizing soundness error in 3-query pcp As a direct corollary to Proposition 3.6.3, we obtain Theorem 3.6.4 For any s &gt; 0:85, NP PCP 1;s [log; 3]. <p> The latter verifier benefits from the use of the new long code based inner verifiers and the atomic tests and their analysis in Section 3.5. We also gain by using the new idea of folding and the improved analysis, due to <ref> [BCHKS] </ref>, of the linearity test. Our Max-2-SAT result is based on the above as well as a new reduction which directly encodes the computation of the verifier in 2-SAT instances.
Reference: [BGG] <author> M. Bellare, O. Goldreich and S. Goldwasser. </author> <title> Randomness in interactive proofs. </title> <booktitle> Proceedings of the Thirty First Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1990. </year>
Reference-contexts: V 0 will then query the oracle as to which random-pad to use, in the simulation of V , and complete its computation by invoking V with the specified random-pad. To generate the "pseudorandom" sequence we use the sampling procedure of <ref> [BGG] </ref>. Specifically, for m 1=c this merely amounts to generating a pairwise independent sequence of uniformly distributed strings in f0; 1g r , which can be done using randomness maxf2r; 2 log 2 mg. Otherwise (i.e., for m &gt; 1=c) the construction of [BGG] amounts to generating fi (cm) such related <p> sequence we use the sampling procedure of <ref> [BGG] </ref>. Specifically, for m 1=c this merely amounts to generating a pairwise independent sequence of uniformly distributed strings in f0; 1g r , which can be done using randomness maxf2r; 2 log 2 mg. Otherwise (i.e., for m &gt; 1=c) the construction of [BGG] amounts to generating fi (cm) such related sequences, where the sequences are related via a random walk on a constant degree expander. Part (2) follows. The following corollary exemplifies the usage of the above proposition.
Reference: [BGS] <author> M. Bellare, O. Goldreich and M. Sudan. </author> <title> Free Bits, PCPs and Non-Approximability | Towards Tight Results (Version 2). August 1995. </title> <booktitle> TR95-024 of ECCC, the Electronic Colloquium on Computational Complexity, </booktitle> <address> http://www.eccc.uni-trier.de/eccc/. </address>
Reference-contexts: Thus our work with the long code was in the right direction. A question is whether the amortized free-bit bound of 2 can be improved. 1.4.2 A lower bounds on amortized free-bits The following text has appeared in the previous version of our work <ref> [BGS] </ref>: We show that, under the framework used within this and previous papers on this subject, amortized free-bit complexity of 2 seems to be a natural barrier: any proof system in this framework must use 2 * amortized free-bits, where * &gt; 0 as usual can be arbitrarily small. <p> The resulting test is A (f 1 ) A (f 2 ) = A (f 1 f 2 + f 3 ) A (f 3 ) (3.3) This test was analyzed in the previous version of this work <ref> [BGS] </ref>; specifically, this test was shown to reject a folded oracle A with ~ A (the linear function closest to A) which does not respect the monomial basis with probability at least (1 2x) ( 3 8 x + x 2 8 7 2 x 2 x 3 , where x
Reference: [BGLR] <author> M. Bellare, S. Goldwasser, C. Lund and A. Russell. </author> <title> Efficient probabilistically checkable proofs and applications to approximation. </title> <booktitle> Proceedings of the Twenty Fifth Annual Symposium on the Theory of Computing, ACM, 1993. (See also Errata sheet in Proceedings of the Twenty Sixth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1994). </year>
Reference-contexts: First, one exhibits an appropriate proof system for NP. Then one applies the FGLSS reduction. The factor indicated hard depends on the proof system parameters. A key factor in getting better results has been the distilling of appropriate pcp-parameters. The sequence of works <ref> [FGLSS, ArSa, ALMSS, BGLR, FeKi, BeSu] </ref> lead us through a sequence of parameters: query complexity, free-bit complexity and, finally, for the best known results, amortized free-bit complexity. <p> This is shown in Section 3, where we show how this code translates into the theorem described above. A second aspect of the improved hardness result is the fact that we use direct reductions from verifiers to the problems of interest. This follows and extends <ref> [BGLR] </ref>, prior to which results had used "generic" reductions, which did not take advantage of the nature of the tests performed by the verifier. <p> Attempts to improve the constant in the exponent of the Max Clique hardness factor, and also improve the constant values of the hardness factors in the Max-SNP hardness results, begin with Bellare, Goldwasser, Lund and Russell <ref> [BGLR] </ref>. They presented new proof systems minimizing query complexity and exploited a slightly improved version of the FGLSS-reduction due to [BeSc, Zu] to get a N 1=30 hardness of approximation factor for Max Clique. <p> Feige and Kilian [FeKi], however, observed that one should work with free-bits, and noted that the free-bit complexity of the system of <ref> [BGLR] </ref> was 14, yielding a N 1=15 hardness factor. Bellare and Sudan then suggested the notion of amortized free-bits and built new proof systems achieving amortized free-bit complexity three, and in particular a N 1=4 hardness for Max Clique assuming NP 6 coR e P. <p> The reduction of [FGLSS] identified the parameters of query complexity (number of binary queries), randomness complexity and error. The class PCP 1;1=2 [r; q] was made explicit by [ArSa]. The parameterization was expanded by <ref> [BGLR] </ref> to explicitly consider the answer size (the oracle is now allowed to return more than one bit at a time) and query size- their notation included five parameters: randomness, number of queries, size of each query, size of each answer, and error probability. <p> We consider them here for a different reason-they play an important role in that the randomized FGLSS reduction [BeSc, Zu] depends actually on this rather than the randomness. To deal with the now huge array of parameters we have generalized the notation of <ref> [BGLR] </ref> to allow specification of parameters by name. We've followed the common tradition regarding the names of polynomial-time reductions: many-to-one reductions are called Karp-reductions whereas (polynomial-time) Turing reductions are called Cook-reductions. <p> We show how the composite verifier hV outer ; V inner i inherits the goodness of the V outer and V inner . To do so we need the following Lemma. It is the counterpart of a claim in <ref> [BGLR, Lemma 3.5] </ref> and will be used in the same way. The lemma can be derived from a coding theory bound which is slight extension of bounds in [MaSl, Ch. 7] and is provided in Section 3.13. Lemma 3.4.4 Suppose 0 ffi 1=2 and A: F l ! . <p> The latter explains why we are interested in outer verifiers which achieve a constant, but arbitrarily small, error *. For completeness we provide a proof, following the ideas of <ref> [ArSa, ALMSS, BGLR] </ref>. Theorem 3.4.5 (the composition theorem): Let V outer be a (l; l 1 )-canonical outer verifier. Suppose it is *-good for L. Let V inner be an (l; l 1 )-canonical inner verifier that is (; ffi 1 ; ffi 2 )-good. <p> The two-prover proofs of Lapidot-Shamir and Feige-Lovasz [LaSh, FeLo] had poly-logarithmic randomness and answer sizes, so [ALMSS] used a modification of these, in the process increasing the number of provers to a constant much larger than two. The later constructions of few-prover proofs of <ref> [BGLR, Ta, FeKi] </ref> lead to better non-approximability results. Bellare and Sudan [BeSu] identified some extra features of constant prover proofs whose presence they showed could be exploited to further increase the non-approximability factors. These features are captured in their definition of canonical verifiers. <p> Free Bits in PCP 35 Due to Provers Coins Answer size Canonical? Can be made canonical? [LaSh, FeLo] 2 polylog polylog No Yes [BeSu] [ALMSS] poly (* 1 ) log polylog No ?? <ref> [BGLR] </ref> 4 log polyloglog No ?? [Ta] 3 log O (1) No ?? [FeKi] 2 log O (1) No At cost of one more prover [BeSu] [Raz] 2 log O (1) Yes (NA) *. <p> The idea is to exploit the characterization of Proposition 3.3.2. Thus we will perform (on A) a linearity test, and then a "respect of monomial basis" test. Linearity testing is well understood, and we will use the test of [BLR], with the analyses of <ref> [BLR, BGLR, BCHKS] </ref>. The main novelty is the respect of monomial basis test. Circuit and projection. Having established that A is close to some evaluation operator E a , we now want to test two things. The first is that h (a) = 0 for some predetermined function h. <p> Their analysis was used in the proof system and Max-3-SAT non-approximability result of [ALMSS]. Interest in the tightness of the analysis from the point of view of improving the Max-3-SAT non-approximability began with <ref> [BGLR] </ref>. They showed that ffi A 3x A 6x 2 A , for every A. This establishes the first segment of the lower bound quoted above (i.e., of the function lin ). Also, it is possible to use [BLR] to show that ffi A 2=9 when x A 1=4. <p> Putting these together implies a two segment lower bound with phase transition at the largest root of the equation 3x 6x 2 = 2 9 (i.e., at 1 p 36 ). This lower bound was used in the Max-3-SAT analyses of <ref> [BGLR] </ref> and [BeSu]. <p> The resulting test is depicted in Figure 3.2. To analyze the performance of this test, we need some technical lemmas. The reader may skip their proofs, in first reading, and proceed below to their usage (in Lemma 3.5.7). Technical lemmas. First we recall the following lemma of <ref> [BGLR] </ref> which provides an improved analysis of Freivalds's matrix multiplication test in the special case when the matrices are symmetric with common diagonal. Lemma 3.5.4 (symmetric matrix multiplication test [BGLR]): Let M 1 ; M 2 be N -by-N symmetric matrices over which agree on their diagonals. <p> Technical lemmas. First we recall the following lemma of <ref> [BGLR] </ref> which provides an improved analysis of Freivalds's matrix multiplication test in the special case when the matrices are symmetric with common diagonal. Lemma 3.5.4 (symmetric matrix multiplication test [BGLR]): Let M 1 ; M 2 be N -by-N symmetric matrices over which agree on their diagonals. Suppose that M 1 6= M 2 . Then Pr 3 : Furthermore, Pr x R N [xM 1 6= xM 2 ] 3=4 . <p> Free Bits in PCP 49 Due to Assuming Factor Technique [ALMSS] P 6= NP some constant NP PCP 1;1=2 [ log; O (1) ]; Reduction of this to Max-3-SAT. <ref> [BGLR] </ref> e P 6= N e P 94=93 Framework; better analyses; uses proof systems of [LaSh, FeLo]. [BGLR] P 6= NP 113=112 New four-prover proof systems. [FeKi] P 6= NP 94=93 New two-prover proof systems. [BeSu] e P 6= N e P 66=65 Canonicity and some optimizations. [BeSu] P 6= NP <p> Free Bits in PCP 49 Due to Assuming Factor Technique [ALMSS] P 6= NP some constant NP PCP 1;1=2 [ log; O (1) ]; Reduction of this to Max-3-SAT. <ref> [BGLR] </ref> e P 6= N e P 94=93 Framework; better analyses; uses proof systems of [LaSh, FeLo]. [BGLR] P 6= NP 113=112 New four-prover proof systems. [FeKi] P 6= NP 94=93 New two-prover proof systems. [BeSu] e P 6= N e P 66=65 Canonicity and some optimizations. [BeSu] P 6= NP 73=72 Canonicity and some optimizations. <p> The basic paradigm of their reduction has been maintained in later improvements. factor) begin with <ref> [BGLR] </ref>. They used Hadamard code based inner verifiers following [ALMSS]. They also introduced a framework for better analysis, and improved some previous analyses; we exploit in particular their better analyses of linearity testing (cf. Section 3.5) and of Freivalds's matrix multiplication test (cf. Lemma 3.5.4). <p> using q queries in the worst case (and q av on the average) to achieve a soundness 4 This is in contrast to the MaxCUT implementation where the same non-auxiliary vertices were used in all gadgets. 68 Bellare, Goldreich, Sudan Due to q q av [ALMSS] some constant some constant <ref> [BGLR] </ref> 36 29 [FeKi] 32 24 This paper 11 10.9 logarithmic randomness; that is, results of the form of Eq. (3.14). error of 1=2. We allow only logarithmic randomness. <p> Previous work. It was shown by [ALMSS] that there are constants q; q av for which (3.14) is achieved. Reductions in the values of these numbers obtained since then are depicted in Figure 3.9. The interest of <ref> [BGLR] </ref> in these numbers was to improve non-approximability factors for Max Clique. But we now know that free-bits are a better measure towards this end [FeKi, BeSu]. Yet we remain interested in query bits for their own sake. <p> By repeating the proof system of Theorem 3.6.4 five times, we obtain that Eq. (3.14) holds for q = 15. (Four repetitions yielding q = 12 do not suffice.) A straightforward implementation of the recycling technique of <ref> [BGLR] </ref> yields q = 12 and q av = 11:74 for which Eq. (3.14) is achieved. <p> the number of particles in the universe, before the factor N * exceeds 2! 80 Bellare, Goldreich, Sudan Due to Factor Assumption [FGLSS] 2 log 1* N for any * &gt; 0 NP 6 e P [ArSa] 2 p [ALMSS] N * for some * &gt; 0 P 6= NP <ref> [BGLR] </ref> N 1=25 NP 6 coR e P [BGLR] N 1=30 NP 6= coRP [FeKi] N 1=15 NP 6= coRP [BeSu] N 1=4 NP 6 coR e P [BeSu] N 1=6 P 6= NP This paper N 1=4 P 6= NP This paper N 1=3 NP 6= coRP Approximation Factor (in <p> the factor N * exceeds 2! 80 Bellare, Goldreich, Sudan Due to Factor Assumption [FGLSS] 2 log 1* N for any * &gt; 0 NP 6 e P [ArSa] 2 p [ALMSS] N * for some * &gt; 0 P 6= NP <ref> [BGLR] </ref> N 1=25 NP 6 coR e P [BGLR] N 1=30 NP 6= coRP [FeKi] N 1=15 NP 6= coRP [BeSu] N 1=4 NP 6 coR e P [BeSu] N 1=6 P 6= NP This paper N 1=4 P 6= NP This paper N 1=3 NP 6= coRP Approximation Factor (in terms of the graph size N ) which <p> In stating results from <ref> [BGLR] </ref> on, we ignore N * terms in which * &gt; 0 can be arbitrary small. <p> Meanwhile the assumption involved the probabilistic, rather than deterministic time complexity of NP- it would be NP 6 coR e P if r = polylog (n) and NP 6= coRP if r = log (n). New proof systems of <ref> [BGLR] </ref> were able to obtain significantly smaller query complexity: they showed NP PCP 1;1=2 [ coins = polylog ; query = 24 ] and NP PCP 1;1=2 [ coins = log ; query = 29 ]. This leads to their hardness results shown in Figure 3.13. <p> Namely, the factor in the above mentioned reduction is N 1=(1+f) where f is the free-bit complexity. They observed that the proof system of <ref> [BGLR] </ref> has free-bit complexity 14, yielding a N 1=15 hardness of approximation factor. The notion of amortized free-bits was introduced in [BeSu]. <p> We remark that 1 A Levin-reduction is a polynomial-time many-to-one reduction which is augmented by corresponding polynomial-time witness transformations. 98 Bellare, Goldreich, Sudan testing "closeness" to codewords with respect to codes of large distance is essential in all known pcp constructions <ref> [BFLS, FGLSS, ArSa, ALMSS, BGLR, FeKi, BeSu] </ref>. The absolute distance between two words w; u 2 f0; 1g n , denoted (w; u), is the number of bits on which w and u disagree. <p> This contrast 110 Bellare, Goldreich, Sudan may perhaps provide a testing ground to separate PCP from MIP, a question raised by <ref> [BGLR] </ref>. The following corollary is obtained by combining Lemma 5.1.1 and Proposition 5.1.2.2. Corollary 5.1.3 For s &lt; 1=2, MIP 1;s [coins = log; provers = 3] = P.
Reference: [BeRo] <author> M. Bellare and P. Rogaway. </author> <title> The complexity of approximating a quadratic program. Complexity of Numerical Optimization, </title> <editor> ed. P. M. Pardalos, </editor> <publisher> World Scientific, </publisher> <year> 1993. </year>
Reference-contexts: After the work of [FGLSS] the field took off in two major directions. One was to extend the interactive proof approach to apply also to other optimization problems. Direct reductions from proofs were used to show hardness of quadratic programming <ref> [BeRo, FeLo] </ref>, Max-3-SAT [ALMSS], set cover [LuYa], and other problems [Be]. Also, reductions from Max Clique lead to hardness results for the chromatic number [LuYa] and other problems [Zu], while previous reductions from Max-3-SAT lead to hardness results for all of Max-SNP [PaYa]. <p> One of these is that they are a good starting point for reductions| examples of such are reductions of two-prover proofs to quadratic programming <ref> [BeRo, FeLo] </ref> and set cover [LuYa]. However, it is a different aspect of constant prover proofs that is of more direct concern to us. This aspect is the use of constant-prover proof systems as the penultimate step of the recursion, and begins with [ALMSS].
Reference: [BeSu] <author> M. Bellare and M. Sudan. </author> <title> Improved non-approximability results. </title> <booktitle> Proceedings of the Twenty Sixth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1994. </year>
Reference-contexts: First, one exhibits an appropriate proof system for NP. Then one applies the FGLSS reduction. The factor indicated hard depends on the proof system parameters. A key factor in getting better results has been the distilling of appropriate pcp-parameters. The sequence of works <ref> [FGLSS, ArSa, ALMSS, BGLR, FeKi, BeSu] </ref> lead us through a sequence of parameters: query complexity, free-bit complexity and, finally, for the best known results, amortized free-bit complexity. <p> The search for proof systems of low amortized free-bit complexity is motivated of course by the FGLSS reduction. Bellare and Sudan <ref> [BeSu] </ref> have shown that NP FPCP [ log; 3 + * ] for every * &gt; 0. The first result above improves upon this, presenting a new proof system with amortized free-bit complexity 2 + *. <p> But in fact one can do better as indicated above. focus error queries free-bits previous related result 3 queries 0:85 3 2 error 72 73 via MaxSAT <ref> [BeSu] </ref> 2 free-bits 0.794 O (1) 2 error 1/2 1 2 11 7 32 queries (24 on average) [FeKi] amortized free-bits O (2 m ) 2 3m 2m 3m free-bits [BeSu] Free Bits in PCP 11 Problem Approx Non-Approx Factor Due to New Factor Previous Factor Assumption Max-3-SAT 1:258 [Ya, GoWi2, <p> as indicated above. focus error queries free-bits previous related result 3 queries 0:85 3 2 error 72 73 via MaxSAT <ref> [BeSu] </ref> 2 free-bits 0.794 O (1) 2 error 1/2 1 2 11 7 32 queries (24 on average) [FeKi] amortized free-bits O (2 m ) 2 3m 2m 3m free-bits [BeSu] Free Bits in PCP 11 Problem Approx Non-Approx Factor Due to New Factor Previous Factor Assumption Max-3-SAT 1:258 [Ya, GoWi2, SSTW] 1:038 1 + 1 72 [BeSu] P 6= NP Max-E3-SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max-2-SAT 1:075 [GoWi2, FeGo] 1:013 <p> error 1/2 1 2 11 7 32 queries (24 on average) [FeKi] amortized free-bits O (2 m ) 2 3m 2m 3m free-bits <ref> [BeSu] </ref> Free Bits in PCP 11 Problem Approx Non-Approx Factor Due to New Factor Previous Factor Assumption Max-3-SAT 1:258 [Ya, GoWi2, SSTW] 1:038 1 + 1 72 [BeSu] P 6= NP Max-E3-SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max-2-SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP Max-CUT 1:139 [GoWi2] 1:015 unspecified [ALMSS] P 6= NP Min-VC 2 o (1) [BaEv, MoSp] 1 + 1 <p> Problem Approx Non-Approx Factor Due to New Factor Previous Factor Assumption Max-3-SAT 1:258 [Ya, GoWi2, SSTW] 1:038 1 + 1 72 <ref> [BeSu] </ref> P 6= NP Max-E3-SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max-2-SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP Max-CUT 1:139 [GoWi2] 1:015 unspecified [ALMSS] P 6= NP Min-VC 2 o (1) [BaEv, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 <p> [ALMSS] P 6= NP Max-2-SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied <ref> [BeSu] </ref>) P 6= NP Max-CUT 1:139 [GoWi2] 1:015 unspecified [ALMSS] P 6= NP Min-VC 2 o (1) [BaEv, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 coR ~ P Number N 1 1 13 coRP 6= NP N 7 N 14 [BeSu] P 6= NP <p> NP Max-CUT 1:139 [GoWi2] 1:015 unspecified [ALMSS] P 6= NP Min-VC 2 o (1) [BaEv, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 <ref> [BeSu] </ref> NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 coR ~ P Number N 1 1 13 coRP 6= NP N 7 N 14 [BeSu] P 6= NP tors we show are hard to achieve (Non-Approx). <p> o (1) [BaEv, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 <ref> [BeSu] </ref> NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 coR ~ P Number N 1 1 13 coRP 6= NP N 7 N 14 [BeSu] P 6= NP tors we show are hard to achieve (Non-Approx). <p> N 1 4 <ref> [BeSu] </ref> NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 coR ~ P Number N 1 1 13 coRP 6= NP N 7 N 14 [BeSu] P 6= NP tors we show are hard to achieve (Non-Approx). In Figure 1.1 we present a table which depicts the parameters of our new proof systems and compares them to previous related result. <p> The conclusion for Max Clique follows, of course, from the FGLSS-reduction and Part (1) of Theorem 1.3.1. The conclusion for the Chromatic Number follows from a recent reduction of Furer [Fu], which in turn builds on reductions in <ref> [LuYa, KLS, BeSu] </ref>. <p> This served to focus attention on the roles of various parameters, both in reductions and in constructions. Also they introduced the consideration of average query complexity, the first in a sequence of parameter changes towards doing better for clique. Free bits are implicit in [FeKi] and formalized in <ref> [BeSu] </ref>. Amortized free bits are introduced in [BeSu] but formalized a little better here. Proof sizes were considered in [BFLS, PoSp]. We consider them here for a different reason-they play an important role in that the randomized FGLSS reduction [BeSc, Zu] depends actually on this rather than the randomness. <p> Also they introduced the consideration of average query complexity, the first in a sequence of parameter changes towards doing better for clique. Free bits are implicit in [FeKi] and formalized in <ref> [BeSu] </ref>. Amortized free bits are introduced in [BeSu] but formalized a little better here. Proof sizes were considered in [BFLS, PoSp]. We consider them here for a different reason-they play an important role in that the randomized FGLSS reduction [BeSc, Zu] depends actually on this rather than the randomness. <p> The later constructions of few-prover proofs of [BGLR, Ta, FeKi] lead to better non-approximability results. Bellare and Sudan <ref> [BeSu] </ref> identified some extra features of constant prover proofs whose presence they showed could be exploited to further increase the non-approximability factors. These features are captured in their definition of canonical verifiers. But the proof systems of [FeKi] that had worked above no longer sufficed| they are not canonical. <p> These features are captured in their definition of canonical verifiers. But the proof systems of [FeKi] that had worked above no longer sufficed| they are not canonical. So instead <ref> [BeSu] </ref> used (a slight modification of) the proofs of [LaSh, FeLo], thereby incurring poly-logarithmic randomness and answer sizes, so that the assumptions in their non-approximability results pertain to quasi-polynomial time classes. (Alternatively they modify the [FeKi] system to a canonical three-prover one, but then incur a decrease in the non-approximability factors <p> Free Bits in PCP 35 Due to Provers Coins Answer size Canonical? Can be made canonical? [LaSh, FeLo] 2 polylog polylog No Yes <ref> [BeSu] </ref> [ALMSS] poly (* 1 ) log polylog No ?? [BGLR] 4 log polyloglog No ?? [Ta] 3 log O (1) No ?? [FeKi] 2 log O (1) No At cost of one more prover [BeSu] [Raz] 2 log O (1) Yes (NA) *. <p> Coins Answer size Canonical? Can be made canonical? [LaSh, FeLo] 2 polylog polylog No Yes <ref> [BeSu] </ref> [ALMSS] poly (* 1 ) log polylog No ?? [BGLR] 4 log polyloglog No ?? [Ta] 3 log O (1) No ?? [FeKi] 2 log O (1) No At cost of one more prover [BeSu] [Raz] 2 log O (1) Yes (NA) *. We indicate the number of provers, the randomness and answer sizes, and whether or not the system is canonical. <p> Putting these together implies a two segment lower bound with phase transition at the largest root of the equation 3x 6x 2 = 2 9 (i.e., at 1 p 36 ). This lower bound was used in the Max-3-SAT analyses of [BGLR] and <ref> [BeSu] </ref>. <p> Here, unlike previous works (for instance <ref> [BeSu] </ref>), may be an arbitrary mapping from l to l 1 rather than being a projection (i.e., satisfying (x) = x (i 1 ) : : : x (i l 1 ) for some sequence 1 i 1 &lt; &lt; i l 1 l and all x 2 l ). <p> some constant NP PCP 1;1=2 [ log; O (1) ]; Reduction of this to Max-3-SAT. [BGLR] e P 6= N e P 94=93 Framework; better analyses; uses proof systems of [LaSh, FeLo]. [BGLR] P 6= NP 113=112 New four-prover proof systems. [FeKi] P 6= NP 94=93 New two-prover proof systems. <ref> [BeSu] </ref> e P 6= N e P 66=65 Canonicity and some optimizations. [BeSu] P 6= NP 73=72 Canonicity and some optimizations. This paper P 6= NP 27=26 Long code and new proof systems. assumption under which this was done. 3.7.2 Previous work Approximation algorithms. <p> this to Max-3-SAT. [BGLR] e P 6= N e P 94=93 Framework; better analyses; uses proof systems of [LaSh, FeLo]. [BGLR] P 6= NP 113=112 New four-prover proof systems. [FeKi] P 6= NP 94=93 New two-prover proof systems. <ref> [BeSu] </ref> e P 6= N e P 66=65 Canonicity and some optimizations. [BeSu] P 6= NP 73=72 Canonicity and some optimizations. This paper P 6= NP 27=26 Long code and new proof systems. assumption under which this was done. 3.7.2 Previous work Approximation algorithms. Max-3-SAT is the canonical Max-SNP complete problem [PaYa]. <p> Section 3.5) and of Freivalds's matrix multiplication test (cf. Lemma 3.5.4). The improvement of Feige and Kilian [FeKi] was obtained via new proof systems; that of <ref> [BeSu] </ref> by use of the canonicity property of constant prover proofs and some optimizations. (See Section 3.4 for a discussion of the role of constant-prover proofs in this context). <p> Reductions in the values of these numbers obtained since then are depicted in Figure 3.9. The interest of [BGLR] in these numbers was to improve non-approximability factors for Max Clique. But we now know that free-bits are a better measure towards this end <ref> [FeKi, BeSu] </ref>. Yet we remain interested in query bits for their own sake. <p> We will be running each of the atomic tests many times, but, to keep the free-bit count low, these will not be independent repetitions. Rather, following <ref> [BeSu] </ref>, we will run about 2 O (m) copies of each test in a way which is pairwise, or "almost" pairwise independent, to lower the error probability to O (2 m ). This will be done using 2m free-bits. <p> Thus we begin with the iterated projection tests. The analysis of the other iterated tests, where the atomic tests are invoked on two/three linear combinations, require slightly more care. The corresponding lemmas could have been proven using the notion of "weak pairwise independence" introduced in <ref> [BeSu] </ref>. However, we present here an alternative approach. 3.11.2 Iterated projection test The iterated projection test described in Figure 3.11 takes as input a vector ~ f 2 F m l and also a linear function L 2 L m . <p> Let a 2 l be such that Dist (E a ; A) 1=4, and let a 1 = (a) 2 l 1 . If ProjPass m c 3 2 m then Dist (E a 1 ; A 1 ) 0:1. Proof: The proof is similar to that of <ref> [BeSu, Lemma 3.5] </ref>. Let * 1 = Dist (A 1 ; E a 1 ) and assume it is at least 0:1. We show that there is a constant c 3 such that ProjPass m h (A) &lt; c 3 2 m . m j = 2 m 1. <p> Here c; s are functions of N such that 0 s (N ) c (N ) 1. Free Bits in PCP 77 3.12.2 Sources of our improvements We adopt the basic framework of the construction of proof systems with low free-bit complexity as presented in <ref> [BeSu] </ref>. Our improvement comes from the use of the new long code instead of the Hadamard code as a basis for the construction of inner verifiers. This allows us to save one bit in the amortized free-bit complexity. <p> Instead, as seen above, the long code allows us to directly handle any function. The fact that we take linear combinations of these functions should not confuse the reader; these are linear combinations of random functions rather than being linear combinations of random linear functions (as in <ref> [BeSu] </ref>). 3.12.3 Construction and results Our construction of a proof systems with amortized free-bit complexity of two bits is obtained by composing the (l; l 1 )-canonical outer verifier of Lemma 3.4.2 with a (l; l 1 )-canonical inner verifier, denoted V free-in , which is depicted in Figure 3.12. <p> Using the FGLSS-construction on this system, the claim of Part (2) follows. Combining the above with a recent reduction of Furer [Fu], which in turn improved the reductions of <ref> [LuYa, KLS, BeSu] </ref>, we get Theorem 3.12.6 For any * &gt; 0 (1) NP K R Gap-ChromNum c;s for s (N )=c (N ) = N 1=5* . (2) Gap-ChromNum c;s is NP-complete for s (N )=c (N ) = N 1=7* . 3.12.4 Previous work Max Clique. <p> Factor Assumption [FGLSS] 2 log 1* N for any * &gt; 0 NP 6 e P [ArSa] 2 p [ALMSS] N * for some * &gt; 0 P 6= NP [BGLR] N 1=25 NP 6 coR e P [BGLR] N 1=30 NP 6= coRP [FeKi] N 1=15 NP 6= coRP <ref> [BeSu] </ref> N 1=4 NP 6 coR e P [BeSu] N 1=6 P 6= NP This paper N 1=4 P 6= NP This paper N 1=3 NP 6= coRP Approximation Factor (in terms of the graph size N ) which is infeasible to achieve under an indicated Assumption. <p> any * &gt; 0 NP 6 e P [ArSa] 2 p [ALMSS] N * for some * &gt; 0 P 6= NP [BGLR] N 1=25 NP 6 coR e P [BGLR] N 1=30 NP 6= coRP [FeKi] N 1=15 NP 6= coRP <ref> [BeSu] </ref> N 1=4 NP 6 coR e P [BeSu] N 1=6 P 6= NP This paper N 1=4 P 6= NP This paper N 1=3 NP 6= coRP Approximation Factor (in terms of the graph size N ) which is infeasible to achieve under an indicated Assumption. <p> Namely, the factor in the above mentioned reduction is N 1=(1+f) where f is the free-bit complexity. They observed that the proof system of [BGLR] has free-bit complexity 14, yielding a N 1=15 hardness of approximation factor. The notion of amortized free-bits was introduced in <ref> [BeSu] </ref>. They observed that the performance of the reduction depended in fact on this quantity, and that the factor was N 1=(1+ f) where f is the amortized free bit complexity. They then showed that NP FPCP [polylog; 3]. <p> A Free Bits in PCP 81 N * factor hardness for Max Clique translates into a N ffi factor hardness for the Chromatic number 6 , with ffi a function of *. To discuss the quality of reductions, let us, following <ref> [BeSu] </ref>, define an (a; b)- reduction to be one that achieves ffi = 1 ab+(b=*) = * The first reduction, namely that of Lund and Yannakakis [LuYa], was a (1; 5)-reduction. <p> A subsequent reduction of Khanna, Linial and Safra [KLS] is simpler but in fact slightly less efficient, being a (6; 5)-reduction. However a more efficient reduction is given by <ref> [BeSu] </ref>- they present a (1; 3)-reduction. Our N 1=3 hardness for Clique would yield, via this, a N 1=7 hardness for the chromatic number. But more recently an even more efficient reduction has become available, namely that of Furer [Fu]. <p> Each of the two parts of Proposition 4.1.7 shows that the well-known method of obtaining clique-approximation results from efficient pcp systems (cf., <ref> [FGLSS, BeSc, Zu, FeKi, BeSu] </ref>) is "complete" in the sense that if clique-approximation can be shown NP-hard then this can be done via this method. The following is a more precise version of Theorem 1.4.1 in that the role of * &gt; 0 is made explicit. <p> However now we know that FPCP and Max Clique are equivalent, so we can go back and rephrase the old statements. Thus results of <ref> [LuYa, KLS, BeSu] </ref> can be summarized as: For every fl &gt; 0, if approximating Max Clique to within N ff is NP-hard then approximating Chromatic Number to within N h (ff)fl is also NP-hard, where: (1) h (ff) = minf 1 6 ; ff 54ff g [LuYa]. (2) h (ff) = <p> within N ff is NP-hard then approximating Chromatic Number to within N h (ff)fl is also NP-hard, where: (1) h (ff) = minf 1 6 ; ff 54ff g [LuYa]. (2) h (ff) = minf 1 11 ; ff (3) h (ff) = minf 1 4 ; ff 32ff g <ref> [BeSu] </ref>. (4) h (ff) = minf 1 3 ; ff (Our discussion of Furer's results [Fu] reflects only the best current understanding we have of them, since it is on-going work.) We note that it is an open problem whether one can get a reduction in which h (ff) ! 1 <p> We remark that 1 A Levin-reduction is a polynomial-time many-to-one reduction which is augmented by corresponding polynomial-time witness transformations. 98 Bellare, Goldreich, Sudan testing "closeness" to codewords with respect to codes of large distance is essential in all known pcp constructions <ref> [BFLS, FGLSS, ArSa, ALMSS, BGLR, FeKi, BeSu] </ref>. The absolute distance between two words w; u 2 f0; 1g n , denoted (w; u), is the number of bits on which w and u disagree.
Reference: [BGKW] <author> M. Ben-Or, S. Goldwasser, J. Kilian and A. Wigderson. </author> <title> Multi-Prover interactive proofs: How to remove intractability assumptions. </title> <booktitle> Proceedings of the Twentieth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1988. </year>
Reference-contexts: The indication of higher factors, and results for other problems, had to wait for the interactive proof approach. Interactive proofs were introduced by Goldwasser, Micali and Rackoff [GMR] and Babai [Bab]. Ben-Or, Goldwasser, Kilian and Wigderson <ref> [BGKW] </ref> extended these ideas to define a notion of multi-prover interactive proofs. Fortnow, Rompel and Sipser [FRS] showed that the class, MIP, of languages possessing multi-prover interactive proofs equals the class of languages which have (using todays terms) probabilistically checkable proofs (of unrestricted, and thus polynomial, randomness and query complexity). <p> value, such as k = 2. 2.4 History The model underlying what are now known as "probabilistically checkable proofs" is the "oracle" model of Fortnow, Rompel and Sipser [FRS], introduced as an equivalent (with respect to language recognition power) version of the multi-prover model of Ben-Or, Goldwasser, Kilian and Wigderson <ref> [BGKW] </ref>. Interestingly, as shown by [BFLS, FGLSS], this framework can be applied in a meaningful manner also to languages in NP. <p> Overview. The constructions of efficient proofs that follow will exploit the notion of recursive verifier construction due to Arora and Safra [ArSa]. We will use just one level of recursion. We first define a notion of a canonical outer verifier whose intent is to capture two-prover one-round proof systems <ref> [BGKW] </ref> having certain special properties; these verifiers will be our starting point. We then define a canonical inner verifier. Recursion is captured by an appropriate definition of a composed verifier whose attributes we relate to those of the original verifiers in Theorem 3.4.5.
Reference: [BeSc] <author> P. Berman and G. Schnitger. </author> <title> On the complexity of approximating the independent set problem. </title> <booktitle> Information and Computation 96, </booktitle> <month> 77-94 </month> <year> (1992). </year>
Reference-contexts: For the best results one typically uses a randomized form of this reduction due to <ref> [BeSc, Zu] </ref> and it is this that we will assume henceforth. A NP-hard gap problem is obtained roughly as follows. First, one exhibits an appropriate proof system for NP. Then one applies the FGLSS reduction. The factor indicated hard depends on the proof system parameters. <p> Specifically, using a randomized reduction we can transform FPCP 1; 1 2 [log; f ] into FPCP 1;2 k [log +k; k f ]. (This transformation is analogous to the well-known transformation of Berman and Schnitger <ref> [BeSc] </ref>.) Alternatively, using a known deterministic amplification method based on [AKS, LPS] one can transform FPCP 1; 1 2 [log; f ] into FPCP 1;2 k [log +2k; k f ] (ignoring multiplicative factors of 1 + * for arbitrarily small * &gt; 0). (To the best of our knowledge this <p> They presented new proof systems minimizing query complexity and exploited a slightly improved version of the FGLSS-reduction due to <ref> [BeSc, Zu] </ref> to get a N 1=30 hardness of approximation factor for Max Clique. Feige and Kilian [FeKi], however, observed that one should work with free-bits, and noted that the free-bit complexity of the system of [BGLR] was 14, yielding a N 1=15 hardness factor. <p> If the reduction is deterministic we omit the subscript of "R," or, sometimes, for emphasis, replace it by a subscript of "D." An example is the randomized FGLSS transformation <ref> [FGLSS, BeSc, Zu] </ref>. Here (A 1 ; B 1 ) is typically an NP-complete language L, and (A 2 ; B 2 ) is Gap-Clique c;s for some c; s which are determined by the transformation. <p> Free bits are implicit in [FeKi] and formalized in [BeSu]. Amortized free bits are introduced in [BeSu] but formalized a little better here. Proof sizes were considered in [BFLS, PoSp]. We consider them here for a different reason-they play an important role in that the randomized FGLSS reduction <ref> [BeSc, Zu] </ref> depends actually on this rather than the randomness. To deal with the now huge array of parameters we have generalized the notation of [BGLR] to allow specification of parameters by name. <p> The number of queries was unspecified, but indicated to be 10 4 , so * 10 4 . Later work has focused on reducing the constant value of * in the exponent. 5 In later work a slightly tighter form of the FGLSS reduction due to <ref> [BeSc, Zu] </ref> has been used. <p> that the initial randomness cost can be ignored (as long as it were logarithmic). (Otherwise, one would have needed to construct proof systems which minimize also this parameter; i.e., the constant factor in the logarithmic randomness complexity.) The randomized error reduction method originates in the work of Berman and Schnitger <ref> [BeSc] </ref> were it is applied to the Clique Gap promise problem. An alternative description is given by Zuckerman [Zu]. Another alternative description, carried out in the proof system, is presented in Section 5.2. The de-randomized error reduction method consists of applying general, de-randomized, error-reduction techniques to the proof system setting. <p> Each of the two parts of Proposition 4.1.7 shows that the well-known method of obtaining clique-approximation results from efficient pcp systems (cf., <ref> [FGLSS, BeSc, Zu, FeKi, BeSu] </ref>) is "complete" in the sense that if clique-approximation can be shown NP-hard then this can be done via this method. The following is a more precise version of Theorem 1.4.1 in that the role of * &gt; 0 is made explicit. <p> This reduction is analogous to the well-known transformation of Berman and Schnitger <ref> [BeSc] </ref>.
Reference: [Bl] <author> A. Blum. </author> <title> Algorithms for approximate graph coloring. </title> <type> Ph. D Thesis, </type> <institution> Dept. of Computer Science, MIT, </institution> <year> 1991. </year> <note> Free Bits in PCP 125 </note>
Reference-contexts: He showed that a polynomial-time N 1* -factor approximation algorithm for Max Clique implies a polynomial time algorithm to color a three colorable graph with O (log N ) colors <ref> [Bl] </ref>, which is much better than currently known [KMS]. But perhaps N 1o (1) is the best possible. Resolving the approximation complexity of this basic problem seems, in any case, to be worth some effort. Gaps in clique size. <p> will do. (Actually, we get Gap-Clique N * 1 ;N k* 1 (N ) K R Gap-Clique 1 2 M * 0 2 ;2M k* 0 2 (M ), 2 * 2 , but this can be corrected by invoking item (1).) The following theorem, was first shown by Blum <ref> [Bl] </ref>, using the technique of randomized graph products. <p> Inequality (4.2) follows from the fact that t = c log 2 N . Free Bits in PCP 97 The following result was derived as a corollary by Blum <ref> [Bl] </ref> and shows the application of the above theorem to coloring graphs with low-chromatic number with relatively small number of colors.
Reference: [BLR] <author> M. Blum, M. Luby and R. Rubinfeld. </author> <title> Self-testing/correcting with applications to numerical problems. </title> <journal> Journal of Computer and System Sciences Vol. </journal> <volume> 47, </volume> <pages> pp. 549-595, </pages> <year> 1993. </year>
Reference-contexts: The idea is to exploit the characterization of Proposition 3.3.2. Thus we will perform (on A) a linearity test, and then a "respect of monomial basis" test. Linearity testing is well understood, and we will use the test of <ref> [BLR] </ref>, with the analyses of [BLR, BGLR, BCHKS]. The main novelty is the respect of monomial basis test. Circuit and projection. Having established that A is close to some evaluation operator E a , we now want to test two things. <p> The idea is to exploit the characterization of Proposition 3.3.2. Thus we will perform (on A) a linearity test, and then a "respect of monomial basis" test. Linearity testing is well understood, and we will use the test of [BLR], with the analyses of <ref> [BLR, BGLR, BCHKS] </ref>. The main novelty is the respect of monomial basis test. Circuit and projection. Having established that A is close to some evaluation operator E a , we now want to test two things. The first is that h (a) = 0 for some predetermined function h. <p> Thus, it is left to test that the two oracles are consistent in the sense that A 1 is not too far from an evaluation operator which corresponds to (a) for some predetermined function . Self-correction. The following self-correction lemma is due to <ref> [BLR] </ref> and will be used throughout. Lemma 3.5.1 (Self Correction Lemma [BLR]): Let A; ~ A: F l ! with ~ A linear, and let x = Dist (A; ~ A). <p> Self-correction. The following self-correction lemma is due to <ref> [BLR] </ref> and will be used throughout. Lemma 3.5.1 (Self Correction Lemma [BLR]): Let A; ~ A: F l ! with ~ A linear, and let x = Dist (A; ~ A). <p> Convention. All our tests output a bit, with 0 standing for accept and 1 for reject. 3.5.1 Atomic linearity test The atomic linearity test shown in Figure 3.2 is the one of Blum, Luby and Rubinfeld <ref> [BLR] </ref>. We want to lower bound the probability 1 LinPass (A) that the test rejects when its inputs f 1 ; f 2 are chosen at random, as a function of x = Dist (A; Lin). <p> It was shown in [BCHKS] (see below) that this combined lower bound is close to the best one possible. History. The general problem of linearity testing as introduced and studied by Blum et. al. <ref> [BLR] </ref> is stated as follows: given a function A: G ! H, where G; H are groups, obtain a lower bound on ffi A as a function of x A , where ffi A = Pr [A (a) + A (b) 6= A (a + b)] x A = Dist (A; <p> They showed that ffi A 3x A 6x 2 A , for every A. This establishes the first segment of the lower bound quoted above (i.e., of the function lin ). Also, it is possible to use <ref> [BLR] </ref> to show that ffi A 2=9 when x A 1=4. Putting these together implies a two segment lower bound with phase transition at the largest root of the equation 3x 6x 2 = 2 9 (i.e., at 1 p 36 ). <p> The test consists of checking whether A (h + f ) = A (f ) and it outputs 0 if equality holds and 1 otherwise. Assuming that A is close to some evaluation operator E a , the atomic circuit test (above) uses self-correction <ref> [BLR] </ref> to test that a given function h has value 0 at a. As explained above, this test is not needed since all our proof systems will use a (h; 0)-folding (of A) and thus will impose h (a) = 0.
Reference: [BrNa] <author> J. Bruck and M. Naor. </author> <title> The hardness of decoding with preprocessing. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> Vol. 36, No. 2, </volume> <pages> pp. 381-385, </pages> <year> 1990. </year>
Reference-contexts: In other words, we are given a system of linear equations over GF (2), and need to determine the maximum number of equations which may be simultaneously satisfied. The maximization problem is known to be Max-SNP complete (see <ref> [BrNa] </ref> or [Pet]). Here we provide a stronger bound via a direct reduction from the MaxSNP verifier.
Reference: [BoHa] <author> R. Boppana and M. Hald orsson. </author> <title> Approximating maximum independent sets by excluding subgraphs. </title> <journal> BIT, </journal> <volume> Vol. 32, No. 2, </volume> <year> 1992. </year>
Reference-contexts: Max Clique approximation. Although we look at many optimization problems there is a particular focus on Max Clique. Recall the best known polynomial time approximation algorithm for Max Clique achieves a factor of only N 1o (1) <ref> [BoHa] </ref>, scarcely better than the trivial factor of N . (Throughout the paper, when discussing the Max Clique problem, N denotes the number of vertices in the graph.) There is not even a heuristic algorithm that is conjectured to do better. (The Lovasz Theta function had been conjectured to approximate the <p> + 1 26 unspecified [ALMSS] P 6= NP Max-2-SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP Max-CUT 1:139 [GoWi2] 1:015 unspecified [ALMSS] P 6= NP Min-VC 2 o (1) [BaEv, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) <ref> [BoHa] </ref> N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 coR ~ P Number N 1 1 13 coRP 6= NP N 7 N 14 <p> 6= NP Min-VC 2 o (1) [BaEv, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) <ref> [BoHa] </ref> N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 coR ~ P Number N 1 1 13 coRP 6= NP N 7 N 14 [BeSu] P 6= NP tors we show are hard to achieve (Non-Approx).
Reference: [CrKa] <author> P. Crescenzi and V. Kann, </author> <title> A compendium of NP optimization problems. </title> <type> Technical Report, </type> <institution> Dipartimento di Scienze dell'Informazione, Universita di Roma "La Sapienza", SI/RR-95/02, </institution> <year> 1995. </year> <note> The list is updated continuously. The latest version is available by anonymous ftp from nada.kth.se as Theory/Viggo-Kann/compendium.ps.Z. </note>
Reference: [CST] <author> P. Crescenzi, R. Silvestri and L. Trevisan. </author> <title> To Weight or not to Weight: Where is the Question? Manuscript, </title> <month> October </month> <year> 1995. </year>
Reference-contexts: This causes a loss of a factor of 3 in the hardness factor; that is, we would get a hardness factor of 194=193 for the Max-CUTproblem restricted to simple graphs. A better reduction which preserves the non-approximation ratio has been recently suggested by Crescenzi et. al. <ref> [CST] </ref>. 3.8.4 Gadgets and the hardness of Max-CUT Gadgets will be used to express the verifier's computation in terms of cuts in graphs.
Reference: [CW] <author> A. Cohen and A. Wigderson. Dispersers, </author> <title> deterministic amplification, and weak random sources. </title> <booktitle> Proceedings of the Thirtieth Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1989. </year>
Reference-contexts: Arora and Safra [ArSa] reduced the randomness complexity of a PCP verifier for NP to logarithmic | they showed NP = PCP 1;1=2 [ coins = log ; query = p log N ]. They also observed that random bits can be recycled for error-reduction via the standard techniques <ref> [AKS, CW, ImZu] </ref>. The consequence was the first NP-hardness result for Max Clique approximation. <p> The de-randomized error reduction method consists of applying general, de-randomized, error-reduction techniques to the proof system setting. The best method knows as the "Expander Walk" technique is due to Ajtai, Komlos and Szemeredi [AKS] (see also <ref> [CW, ImZu] </ref>). <p> The use of random walks on expander graphs for error reduction was suggested by Ajtai, Komlos and Szemeredi [AKS] (cf., <ref> [CW] </ref>). The use of random walks on expander graphs for gap amplification in the context of pcp originates in [ArSa]. The value of the constant multiplier of k in the randomness complexity of the resulting pcp system, depends on the expander graph used.
Reference: [Co] <author> S. A. Cook. </author> <title> The Complexity of Theorem-Proving Procedures. </title> <booktitle> Proceedings of the Third Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1971. </year>
Reference-contexts: We've followed the common tradition regarding the names of polynomial-time reductions: many-to-one reductions are called Karp-reductions whereas (polynomial-time) Turing reductions are called Cook-reductions. This terminology is somewhat unfair towards Levin whose work on NP-completeness [Lev] was independent of those of Cook <ref> [Co] </ref> and Karp [Ka]. Actually, the reductions considered by Levin are more restricted as they also efficiently transform the corresponding NP-witnesses (this is an artifact of Levin's desire to treat search problems rather than decision problem).
Reference: [EIS] <author> S. Even, A. Itai and A. Shamir. </author> <title> On the complexity of timetable and multicommodity flow problems. </title> <journal> SIAM J. on Computing Vol. </journal> <volume> 5, </volume> <pages> 691-703, </pages> <year> 1976. </year>
Reference-contexts: Furthermore, x can be constructed in polynomial-time given x (and V ). Using a (polynomial-time) decision procedure for satisfiability of Horn Formulae, we are done. (Alternatively, we can use the linear-time decision procedure for 2-SAT due to Even et. al. <ref> [EIS] </ref>.) 108 Bellare, Goldreich, Sudan Proof of Proposition 5.1.2, Part (4): To see that PCP 1;s [log; poly] NP, for every s &lt; 1, consider a non-deterministic machine which tries to guess an oracle which makes the verifier (of the above system) always accept. <p> One can easily verify that the formula is not satisfied iff there exists a variable for which every truth assignment yields a contradiction (i.e., "forcing paths" to contradicting values - cf., <ref> [EIS] </ref>). Thus, a non-deterministic logspace machine can guess this variable and check that both possible truth assignments (to it) yield contradictions. The latter checking reduces to guessing the variable for which a conflicting assignment is implied and verifying the conflict via s-t directed connectivity.
Reference: [ESY] <author> S. Even, A. Selman and Y. Yacobi. </author> <title> The complexity of promise problems with applications to public-key cryptography. </title> <journal> Information and Control Vol. </journal> <volume> 2, </volume> <pages> 159-173, </pages> <year> 1984. </year>
Reference-contexts: While the task is typically language recognition, we will, more generally, consider promise problems (A; B) consisting of a set A of "positive" instances and a set B of "negative" instances <ref> [ESY] </ref>. (Languages are a special case of promise problems; a language L is represented by the promise problem (L; L).) Of interest in the applications are various parameters of the system. <p> A function is admissible if it is polynomially bounded and polynomial time computable. We will ask that all functions measuring complexity (e.g. the query complexity q = q (n)) be admissible. In defining complexity classes we will consider promise problems rather than languages. 1 Following Even et. al. <ref> [ESY] </ref>, a promise problem is a pair of disjoint sets (A; B), the first being the set of "positive" instances and the second the set of "negative" instances. A language L is identified with (L; L). (We refer the reader to [ESY] for issues in promise problems.) 2.2 Proof systems A <p> problems rather than languages. 1 Following Even et. al. <ref> [ESY] </ref>, a promise problem is a pair of disjoint sets (A; B), the first being the set of "positive" instances and the second the set of "negative" instances. A language L is identified with (L; L). (We refer the reader to [ESY] for issues in promise problems.) 2.2 Proof systems A verifier is a probabilistic machine V taking one or more inputs and also allowed access to one or more oracles. Let x denote the sequence of all inputs to V and let n denote its length.
Reference: [Fei] <author> U. Feige. </author> <title> Randomized graph products, chromatic numbers, and the Lovasz theta function. </title> <booktitle> Proceedings of the Twenty Seventh Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1995. </year>
Reference-contexts: when discussing the Max Clique problem, N denotes the number of vertices in the graph.) There is not even a heuristic algorithm that is conjectured to do better. (The Lovasz Theta function had been conjectured to approximate the Max Clique size within p but this conjecture was disproved by Feige <ref> [Fei] </ref>.) The question of whether one can do even slightly better is of interest. Namely, can one present an N 1* factor approximation algorithm for Max Clique for some * &lt; 1? An additional motivation for searching for such "weak" approximation algorithms was suggested by Blum. <p> We also note that Furer's reduction is randomized while the rest are deterministic. Reductions among Max Clique Problems Next we present an invariance of the Gap Clique problem with respect to shifting of the gaps. The following result has also been independently observed by Feige <ref> [Fei] </ref>, where he uses a randomized graph product to show the result. Our description uses the properties of fpcp and its equivalence to clique approximation. Theorem 4.1.10 Let k; * 1 ; * 2 be real numbers such that k 1 and 0 * 1 &lt; * 2 1.
Reference: [FeGo] <author> U. Feige and M. Goemans. </author> <title> Approximating the value of two prover proof systems, with application to Max-2SAT and Max-DICUT. </title> <booktitle> Proceedings of the Third Israel Symposium on Theory and Computing Systems, IEEE, </booktitle> <year> 1995. </year>
Reference-contexts: 3m free-bits [BeSu] Free Bits in PCP 11 Problem Approx Non-Approx Factor Due to New Factor Previous Factor Assumption Max-3-SAT 1:258 [Ya, GoWi2, SSTW] 1:038 1 + 1 72 [BeSu] P 6= NP Max-E3-SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max-2-SAT 1:075 <ref> [GoWi2, FeGo] </ref> 1:013 1 + 1 504 (implied [BeSu]) P 6= NP Max-CUT 1:139 [GoWi2] 1:015 unspecified [ALMSS] P 6= NP Min-VC 2 o (1) [BaEv, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ P N <p> Recall that the latter is approximable within 2-o (1) [BaEv, MoSp]. Our results for Max-CUTand Max-2-SAT show that it is not possible to find a solution with value which is only 1% away from being optimal. This may be contrasted with the recent results of <ref> [GoWi2, FeGo] </ref> which shows that solutions which are within 14% and 7.5%, respectively, of the optimum are obtainable in polynomial time. <p> Remark. As this definition indicates, we adopt the convention that the approximation factor is a number at least 1. Sometimes Max-SNP approximation is discussed in terms of factors at most 1 (e.g. <ref> [GoWi2, FeGo] </ref>) but obviously the two are equivalent via an inversion of the factor. We are interested in promise versions of Max-X-SAT which exhibit a gap in the MaxSAT () value between yes and no instances. <p> This problem is particularly interesting because it has been the focus of recent improvements in the approximation factor attainable in polynomial-time. Specifically, Goemans and Williamson [GoWi2] exhibited a polynomial time algorithm achieving an approximation factor of 1 0:878 1:139, and consequently Feige and Goemans <ref> [FeGo] </ref> exhibited an algorithm achieving 1 0:931 1:074. Non-approximability. Non-approximability results for Max-SNP problems begin with [ALMSS] who proved that there exists a constant * &gt; 0 such that Gap-3-SAT 1;1* is NP-hard.
Reference: [FGLSS] <author> U. Feige, S. Goldwasser, L. Lov asz, S. Safra, and M. Szegedy. </author> <title> Approximating clique is almost NP-complete. </title> <booktitle> Proceedings of the Thirty Second Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1991. </year>
Reference-contexts: First, one exhibits an appropriate proof system for NP. Then one applies the FGLSS reduction. The factor indicated hard depends on the proof system parameters. A key factor in getting better results has been the distilling of appropriate pcp-parameters. The sequence of works <ref> [FGLSS, ArSa, ALMSS, BGLR, FeKi, BeSu] </ref> lead us through a sequence of parameters: query complexity, free-bit complexity and, finally, for the best known results, amortized free-bit complexity. <p> Feige, Goldwasser, Lovasz, Safra and Szegedy <ref> [FGLSS] </ref> showed that NP has probabilistically checkable proofs of poly-logarithmic randomness and query complexity; namely, NP PCP 1;1=2 [r; q], where r (n) = q (n) = O (log n log log n). The breakthrough connection to approximation was made by Feige, Goldwasser, Lovasz, Safra and Szegedy [FGLSS]. <p> Safra and Szegedy <ref> [FGLSS] </ref> showed that NP has probabilistically checkable proofs of poly-logarithmic randomness and query complexity; namely, NP PCP 1;1=2 [r; q], where r (n) = q (n) = O (log n log log n). The breakthrough connection to approximation was made by Feige, Goldwasser, Lovasz, Safra and Szegedy [FGLSS]. <p> After the work of <ref> [FGLSS] </ref> the field took off in two major directions. One was to extend the interactive proof approach to apply also to other optimization problems. Direct reductions from proofs were used to show hardness of quadratic programming [BeRo, FeLo], Max-3-SAT [ALMSS], set cover [LuYa], and other problems [Be]. <p> If the reduction is deterministic we omit the subscript of "R," or, sometimes, for emphasis, replace it by a subscript of "D." An example is the randomized FGLSS transformation <ref> [FGLSS, BeSc, Zu] </ref>. Here (A 1 ; B 1 ) is typically an NP-complete language L, and (A 2 ; B 2 ) is Gap-Clique c;s for some c; s which are determined by the transformation. <p> Interestingly, as shown by <ref> [BFLS, FGLSS] </ref>, this framework can be applied in a meaningful manner also to languages in NP. <p> Babai et. al. [BFLS] suggested a model in which the instances are encoded in a special (polynomial-time computable and decodable) error-correcting code and the verifier works in polylogarithmic time. Here we follow the model of Feige et. al. <ref> [FGLSS] </ref> where the verifier is probabilistic polynomial-time (as usual) and one considers finer complexity measures such as the query and randomness complexity. The reduction of [FGLSS] identified the parameters of query complexity (number of binary queries), randomness complexity and error. <p> Here we follow the model of Feige et. al. <ref> [FGLSS] </ref> where the verifier is probabilistic polynomial-time (as usual) and one considers finer complexity measures such as the query and randomness complexity. The reduction of [FGLSS] identified the parameters of query complexity (number of binary queries), randomness complexity and error. The class PCP 1;1=2 [r; q] was made explicit by [ArSa]. <p> Prior to 1991, no non-approximability results on Max Clique were known. In 1991 the connection to proofs was made by Feige et. al. <ref> [FGLSS] </ref>. The FGLSS reduction says that PCP 1;e [ coins = r ; query = q ] Karp reduces to Gap-Clique c;s via a reduction running in time poly (2 r+q ), and with the gap c=s being a function of (r; q and) the error e. <p> reduction, to 5 The value * = 10 4 means that the size N of the graph must be at least 2 1000 , which is more than the number of particles in the universe, before the factor N * exceeds 2! 80 Bellare, Goldreich, Sudan Due to Factor Assumption <ref> [FGLSS] </ref> 2 log 1* N for any * &gt; 0 NP 6 e P [ArSa] 2 p [ALMSS] N * for some * &gt; 0 P 6= NP [BGLR] N 1=25 NP 6 coR e P [BGLR] N 1=30 NP 6= coRP [FeKi] N 1=15 NP 6= coRP [BeSu] N 1=4 <p> The two together indicate that one needs new techniques to prove better than a N 1=3 hardness for Max Clique. 4.1 The reverse connection and its consequences Feige et al. <ref> [FGLSS] </ref> describe a procedure which takes a verifier V , and an input x and constructs a graph, which we denote G V (x), whose vertices correspond to possible accepting transcripts in V 's computation and edges corresponding to consistent/non-conflicting computations. <p> Each of the two parts of Proposition 4.1.7 shows that the well-known method of obtaining clique-approximation results from efficient pcp systems (cf., <ref> [FGLSS, BeSc, Zu, FeKi, BeSu] </ref>) is "complete" in the sense that if clique-approximation can be shown NP-hard then this can be done via this method. The following is a more precise version of Theorem 1.4.1 in that the role of * &gt; 0 is made explicit. <p> In both items the reduction is randomized. Furthermore the equivalence holds both for Karp and for Cook reductions. Proof: The direction (2) ) (1) follows by first amplifying the gap of the verifier for NP (cf., Corollary 5.2.3) and then by applying the FGLSS-reduction <ref> [FGLSS] </ref> to the amplified gap verifier. Specifically, we first obtain NP R FPCP 1;2 t [(1 + *) t; f t], where t (n) = fl log 2 n (with the constant fl determined by the constant * &gt; 0). <p> We remark that 1 A Levin-reduction is a polynomial-time many-to-one reduction which is augmented by corresponding polynomial-time witness transformations. 98 Bellare, Goldreich, Sudan testing "closeness" to codewords with respect to codes of large distance is essential in all known pcp constructions <ref> [BFLS, FGLSS, ArSa, ALMSS, BGLR, FeKi, BeSu] </ref>. The absolute distance between two words w; u 2 f0; 1g n , denoted (w; u), is the number of bits on which w and u disagree.
Reference: [FeKi] <author> U. Feige and J. Kilian. </author> <title> Two prover protocols Low error at affordable rates. </title> <booktitle> Proceedings of the Twenty Sixth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1994. </year>
Reference-contexts: First, one exhibits an appropriate proof system for NP. Then one applies the FGLSS reduction. The factor indicated hard depends on the proof system parameters. A key factor in getting better results has been the distilling of appropriate pcp-parameters. The sequence of works <ref> [FGLSS, ArSa, ALMSS, BGLR, FeKi, BeSu] </ref> lead us through a sequence of parameters: query complexity, free-bit complexity and, finally, for the best known results, amortized free-bit complexity. <p> But in fact one can do better as indicated above. focus error queries free-bits previous related result 3 queries 0:85 3 2 error 72 73 via MaxSAT [BeSu] 2 free-bits 0.794 O (1) 2 error 1/2 1 2 11 7 32 queries (24 on average) <ref> [FeKi] </ref> amortized free-bits O (2 m ) 2 3m 2m 3m free-bits [BeSu] Free Bits in PCP 11 Problem Approx Non-Approx Factor Due to New Factor Previous Factor Assumption Max-3-SAT 1:258 [Ya, GoWi2, SSTW] 1:038 1 + 1 72 [BeSu] P 6= NP Max-E3-SAT 1 + 1 7 folklore 1 + <p> They presented new proof systems minimizing query complexity and exploited a slightly improved version of the FGLSS-reduction due to [BeSc, Zu] to get a N 1=30 hardness of approximation factor for Max Clique. Feige and Kilian <ref> [FeKi] </ref>, however, observed that one should work with free-bits, and noted that the free-bit complexity of the system of [BGLR] was 14, yielding a N 1=15 hardness factor. <p> This served to focus attention on the roles of various parameters, both in reductions and in constructions. Also they introduced the consideration of average query complexity, the first in a sequence of parameter changes towards doing better for clique. Free bits are implicit in <ref> [FeKi] </ref> and formalized in [BeSu]. Amortized free bits are introduced in [BeSu] but formalized a little better here. Proof sizes were considered in [BFLS, PoSp]. <p> The two-prover proofs of Lapidot-Shamir and Feige-Lovasz [LaSh, FeLo] had poly-logarithmic randomness and answer sizes, so [ALMSS] used a modification of these, in the process increasing the number of provers to a constant much larger than two. The later constructions of few-prover proofs of <ref> [BGLR, Ta, FeKi] </ref> lead to better non-approximability results. Bellare and Sudan [BeSu] identified some extra features of constant prover proofs whose presence they showed could be exploited to further increase the non-approximability factors. These features are captured in their definition of canonical verifiers. <p> Bellare and Sudan [BeSu] identified some extra features of constant prover proofs whose presence they showed could be exploited to further increase the non-approximability factors. These features are captured in their definition of canonical verifiers. But the proof systems of <ref> [FeKi] </ref> that had worked above no longer sufficed| they are not canonical. So instead [BeSu] used (a slight modification of) the proofs of [LaSh, FeLo], thereby incurring poly-logarithmic randomness and answer sizes, so that the assumptions in their non-approximability results pertain to quasi-polynomial time classes. (Alternatively they modify the [FeKi] system <p> of <ref> [FeKi] </ref> that had worked above no longer sufficed| they are not canonical. So instead [BeSu] used (a slight modification of) the proofs of [LaSh, FeLo], thereby incurring poly-logarithmic randomness and answer sizes, so that the assumptions in their non-approximability results pertain to quasi-polynomial time classes. (Alternatively they modify the [FeKi] system to a canonical three-prover one, but then incur a decrease in the non-approximability factors due to having more provers). <p> Free Bits in PCP 35 Due to Provers Coins Answer size Canonical? Can be made canonical? [LaSh, FeLo] 2 polylog polylog No Yes [BeSu] [ALMSS] poly (* 1 ) log polylog No ?? [BGLR] 4 log polyloglog No ?? [Ta] 3 log O (1) No ?? <ref> [FeKi] </ref> 2 log O (1) No At cost of one more prover [BeSu] [Raz] 2 log O (1) Yes (NA) *. We indicate the number of provers, the randomness and answer sizes, and whether or not the system is canonical. <p> This means that even the (modified) [LaSh, FeLo] type proofs won't suffice for us. We could use the three-prover modification of <ref> [FeKi] </ref> but the cost would wipe out our gain. Luckily this discussion is moot since we can use the recent result of Raz [Raz] to provide us with a canonical two-prover proof having logarithmic randomness, constant answer size, and any constant error. This makes an ideal starting point. <p> Due to Assuming Factor Technique [ALMSS] P 6= NP some constant NP PCP 1;1=2 [ log; O (1) ]; Reduction of this to Max-3-SAT. [BGLR] e P 6= N e P 94=93 Framework; better analyses; uses proof systems of [LaSh, FeLo]. [BGLR] P 6= NP 113=112 New four-prover proof systems. <ref> [FeKi] </ref> P 6= NP 94=93 New two-prover proof systems. [BeSu] e P 6= N e P 66=65 Canonicity and some optimizations. [BeSu] P 6= NP 73=72 Canonicity and some optimizations. <p> They also introduced a framework for better analysis, and improved some previous analyses; we exploit in particular their better analyses of linearity testing (cf. Section 3.5) and of Freivalds's matrix multiplication test (cf. Lemma 3.5.4). The improvement of Feige and Kilian <ref> [FeKi] </ref> was obtained via new proof systems; that of [BeSu] by use of the canonicity property of constant prover proofs and some optimizations. (See Section 3.4 for a discussion of the role of constant-prover proofs in this context). <p> in the worst case (and q av on the average) to achieve a soundness 4 This is in contrast to the MaxCUT implementation where the same non-auxiliary vertices were used in all gadgets. 68 Bellare, Goldreich, Sudan Due to q q av [ALMSS] some constant some constant [BGLR] 36 29 <ref> [FeKi] </ref> 32 24 This paper 11 10.9 logarithmic randomness; that is, results of the form of Eq. (3.14). error of 1=2. We allow only logarithmic randomness. <p> Reductions in the values of these numbers obtained since then are depicted in Figure 3.9. The interest of [BGLR] in these numbers was to improve non-approximability factors for Max Clique. But we now know that free-bits are a better measure towards this end <ref> [FeKi, BeSu] </ref>. Yet we remain interested in query bits for their own sake. <p> 80 Bellare, Goldreich, Sudan Due to Factor Assumption [FGLSS] 2 log 1* N for any * &gt; 0 NP 6 e P [ArSa] 2 p [ALMSS] N * for some * &gt; 0 P 6= NP [BGLR] N 1=25 NP 6 coR e P [BGLR] N 1=30 NP 6= coRP <ref> [FeKi] </ref> N 1=15 NP 6= coRP [BeSu] N 1=4 NP 6 coR e P [BeSu] N 1=6 P 6= NP This paper N 1=4 P 6= NP This paper N 1=3 NP 6= coRP Approximation Factor (in terms of the graph size N ) which is infeasible to achieve under an <p> This leads to their hardness results shown in Figure 3.13. However, significantly reducing the (average) number of bits queried seemed hard. However, as observed by Feige and Kilian, the performance of the FGLSS reduction actually depends on the free-bit complexity which may be significantly smaller than the query complexity <ref> [FeKi] </ref>. Namely, the factor in the above mentioned reduction is N 1=(1+f) where f is the free-bit complexity. They observed that the proof system of [BGLR] has free-bit complexity 14, yielding a N 1=15 hardness of approximation factor. The notion of amortized free-bits was introduced in [BeSu]. <p> Each of the two parts of Proposition 4.1.7 shows that the well-known method of obtaining clique-approximation results from efficient pcp systems (cf., <ref> [FGLSS, BeSc, Zu, FeKi, BeSu] </ref>) is "complete" in the sense that if clique-approximation can be shown NP-hard then this can be done via this method. The following is a more precise version of Theorem 1.4.1 in that the role of * &gt; 0 is made explicit. <p> We remark that 1 A Levin-reduction is a polynomial-time many-to-one reduction which is augmented by corresponding polynomial-time witness transformations. 98 Bellare, Goldreich, Sudan testing "closeness" to codewords with respect to codes of large distance is essential in all known pcp constructions <ref> [BFLS, FGLSS, ArSa, ALMSS, BGLR, FeKi, BeSu] </ref>. The absolute distance between two words w; u 2 f0; 1g n , denoted (w; u), is the number of bits on which w and u disagree.
Reference: [FeLo] <author> U. Feige and L. Lov asz. </author> <title> Two-prover one round proof systems: Their power and their problems. </title> <booktitle> Proceedings of the Twenty Fourth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1992. </year>
Reference-contexts: After the work of [FGLSS] the field took off in two major directions. One was to extend the interactive proof approach to apply also to other optimization problems. Direct reductions from proofs were used to show hardness of quadratic programming <ref> [BeRo, FeLo] </ref>, Max-3-SAT [ALMSS], set cover [LuYa], and other problems [Be]. Also, reductions from Max Clique lead to hardness results for the chromatic number [LuYa] and other problems [Zu], while previous reductions from Max-3-SAT lead to hardness results for all of Max-SNP [PaYa]. <p> One of these is that they are a good starting point for reductions| examples of such are reductions of two-prover proofs to quadratic programming <ref> [BeRo, FeLo] </ref> and set cover [LuYa]. However, it is a different aspect of constant prover proofs that is of more direct concern to us. This aspect is the use of constant-prover proof systems as the penultimate step of the recursion, and begins with [ALMSS]. <p> The available constant-prover proof systems appear in Figure 3.1 and are discussed below. The two-prover proofs of Lapidot-Shamir and Feige-Lovasz <ref> [LaSh, FeLo] </ref> had poly-logarithmic randomness and answer sizes, so [ALMSS] used a modification of these, in the process increasing the number of provers to a constant much larger than two. The later constructions of few-prover proofs of [BGLR, Ta, FeKi] lead to better non-approximability results. <p> These features are captured in their definition of canonical verifiers. But the proof systems of [FeKi] that had worked above no longer sufficed| they are not canonical. So instead [BeSu] used (a slight modification of) the proofs of <ref> [LaSh, FeLo] </ref>, thereby incurring poly-logarithmic randomness and answer sizes, so that the assumptions in their non-approximability results pertain to quasi-polynomial time classes. (Alternatively they modify the [FeKi] system to a canonical three-prover one, but then incur a decrease in the non-approximability factors due to having more provers). <p> Free Bits in PCP 35 Due to Provers Coins Answer size Canonical? Can be made canonical? <ref> [LaSh, FeLo] </ref> 2 polylog polylog No Yes [BeSu] [ALMSS] poly (* 1 ) log polylog No ?? [BGLR] 4 log polyloglog No ?? [Ta] 3 log O (1) No ?? [FeKi] 2 log O (1) No At cost of one more prover [BeSu] [Raz] 2 log O (1) Yes (NA) *. <p> But we don't take advantage of this fact.) In addition we need answer sizes of O (log log n) as opposed to the O (log n) of previous methods, for reasons explained below. This means that even the (modified) <ref> [LaSh, FeLo] </ref> type proofs won't suffice for us. We could use the three-prover modification of [FeKi] but the cost would wipe out our gain. <p> Free Bits in PCP 49 Due to Assuming Factor Technique [ALMSS] P 6= NP some constant NP PCP 1;1=2 [ log; O (1) ]; Reduction of this to Max-3-SAT. [BGLR] e P 6= N e P 94=93 Framework; better analyses; uses proof systems of <ref> [LaSh, FeLo] </ref>. [BGLR] P 6= NP 113=112 New four-prover proof systems. [FeKi] P 6= NP 94=93 New two-prover proof systems. [BeSu] e P 6= N e P 66=65 Canonicity and some optimizations. [BeSu] P 6= NP 73=72 Canonicity and some optimizations.
Reference: [FRS] <author> L. Fortnow, J. Rompel and M. Sipser. </author> <title> On the power of multiprover interactive protocols. </title> <booktitle> Proceedings of the 3rd Structures, IEEE, </booktitle> <year> 1988. </year> <note> 126 Bellare, Goldreich, Sudan </note>
Reference-contexts: Interactive proofs were introduced by Goldwasser, Micali and Rackoff [GMR] and Babai [Bab]. Ben-Or, Goldwasser, Kilian and Wigderson [BGKW] extended these ideas to define a notion of multi-prover interactive proofs. Fortnow, Rompel and Sipser <ref> [FRS] </ref> showed that the class, MIP, of languages possessing multi-prover interactive proofs equals the class of languages which have (using todays terms) probabilistically checkable proofs (of unrestricted, and thus polynomial, randomness and query complexity). <p> In such a case it is to be understood that the security parameters has been set to some convenient value, such as k = 2. 2.4 History The model underlying what are now known as "probabilistically checkable proofs" is the "oracle" model of Fortnow, Rompel and Sipser <ref> [FRS] </ref>, introduced as an equivalent (with respect to language recognition power) version of the multi-prover model of Ben-Or, Goldwasser, Kilian and Wigderson [BGKW]. Interestingly, as shown by [BFLS, FGLSS], this framework can be applied in a meaningful manner also to languages in NP. <p> Free Bits in PCP 31 Employing the FRS-method <ref> [FRS] </ref> to any PCP (log,O (1))-system for NP (e.g., [ALMSS]) one gets a canonical verifier which is ffi-good for some ffi &lt; 1. Using the Parallel Repetition Theorem of Raz, we obtain our starting point - Lemma 3.4.2 (construction of outer verifiers [Raz]): Let L 2 NP.
Reference: [Fu] <author> M. Furer. </author> <title> Improved hardness results for approximating the chromatic number. </title> <type> Manuscript, </type> <year> 1994. </year>
Reference-contexts: The conclusion for Max Clique follows, of course, from the FGLSS-reduction and Part (1) of Theorem 1.3.1. The conclusion for the Chromatic Number follows from a recent reduction of Furer <ref> [Fu] </ref>, which in turn builds on reductions in [LuYa, KLS, BeSu]. <p> Using the FGLSS-construction on this system, the claim of Part (2) follows. Combining the above with a recent reduction of Furer <ref> [Fu] </ref>, which in turn improved the reductions of [LuYa, KLS, BeSu], we get Theorem 3.12.6 For any * &gt; 0 (1) NP K R Gap-ChromNum c;s for s (N )=c (N ) = N 1=5* . (2) Gap-ChromNum c;s is NP-complete for s (N )=c (N ) = N 1=7* . <p> However a more efficient reduction is given by [BeSu]- they present a (1; 3)-reduction. Our N 1=3 hardness for Clique would yield, via this, a N 1=7 hardness for the chromatic number. But more recently an even more efficient reduction has become available, namely that of Furer <ref> [Fu] </ref>. It is a (1; 2)-reduction, and thereby we get our N 1=5 hardness. Randomized and de-randomized error reduction. As mentioned above, randomized and de-randomized error reduction techniques play an important role in obtaining the best Clique hardness results via the FGLSS method. <p> also NP-hard, where: (1) h (ff) = minf 1 6 ; ff 54ff g [LuYa]. (2) h (ff) = minf 1 11 ; ff (3) h (ff) = minf 1 4 ; ff 32ff g [BeSu]. (4) h (ff) = minf 1 3 ; ff (Our discussion of Furer's results <ref> [Fu] </ref> reflects only the best current understanding we have of them, since it is on-going work.) We note that it is an open problem whether one can get a reduction in which h (ff) ! 1 as ff ! 1.
Reference: [FGMSZ] <author> M. Furer, O. Goldreich, Y. Mansour, M. Sipser, and S. Zachos. </author> <title> On Completeness and Soundness in Interactive Proof Systems. </title> <booktitle> Advances in Computing Research: a research annual, Vol. 5 (Randomness and Computation, </booktitle> <editor> S. Micali, </editor> <publisher> ed.), </publisher> <pages> pp. 429-442, </pages> <year> 1989. </year>
Reference-contexts: Specifically, we first reduce the error of the interactive proof by parallel repetition, next transform it into an Arthur-Merlin interactive proof [GS], and finally transform it into an Arthur-Merlin interactive proof of perfect completeness <ref> [FGMSZ] </ref>. We stress that all the transformations maintain the number of rounds upto a constant and that the constant-round Arthur-Merlin hierarchy collapses to one round [Bab].
Reference: [GJ1] <author> M. Garey and D. Johnson. </author> <title> The complexity of near optimal graph coloring. </title> <journal> Journal of the ACM Vol. </journal> <volume> 23, No. 1, </volume> <pages> 43-49, </pages> <year> 1976. </year>
Reference-contexts: 2 k [log; f k], provided that the original system has at least 2 k accepting configurations per each possible sequence of coin-tosses. (This condition is satisfied in many natural pcp systems, even for k = f .) 1.6 History Early work in non-approximability includes that of Garey and Johnson <ref> [GJ1] </ref> showing that it is NP-hard to approximate the chromatic factor within a factor less than two. The indication of higher factors, and results for other problems, had to wait for the interactive proof approach. Interactive proofs were introduced by Goldwasser, Micali and Rackoff [GMR] and Babai [Bab]. <p> They then showed that NP FPCP [polylog; 3]. This lead to a N 1=4 hardness factor assuming NP 6= coR e P. Chromatic Number. The first hardness result for the chromatic number is due to Garey and Johnson <ref> [GJ1] </ref>. They showed that if P 6= NP then there is no polynomial time algorithm that can achieve a factor less than 2. This remained the best result until the connection to proofs, and the above mentioned results, emerged.
Reference: [GJ2] <author> M. Garey and D. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <year> 1979. </year>
Reference-contexts: The gap of this problem is defined to be s=c. Known upper and lower bounds. There is a simple polynomial time algorithm to approximate Min-VC in unweighted graphs within a factor of 2. The algorithm, due to F. Gavril (cf. <ref> [GJ2] </ref>), consists of taking all vertices which appear in a maximal matching of the graph. For weighted graphs, Bar-Yehuda and Even [BaEv1] and Hochbaum [Hoc], gave algorithms achieving the same approximation factor.
Reference: [GJS] <author> M. Garey, D. Johnson and L. Stockmeyer. </author> <title> Some simplified NP-complete graph problems. </title> <booktitle> Theoretical Computer Science 1, </booktitle> <pages> pp. 237-267, </pages> <year> 1976. </year>
Reference-contexts: For Max-E3-SAT, which is also Max-SNP complete, a very simple algorithm achieves an approximation of 8=7 1:143 (where 7=8 is the expected fraction of clauses satisfied by a uniformly chosen assignment). Max-2-SAT is also Max-SNP complete <ref> [GJS, PaYa] </ref>. This problem is particularly interesting because it has been the focus of recent improvements in the approximation factor attainable in polynomial-time. <p> The improvement of Feige and Kilian [FeKi] was obtained via new proof systems; that of [BeSu] by use of the canonicity property of constant prover proofs and some optimizations. (See Section 3.4 for a discussion of the role of constant-prover proofs in this context). Garey, Johnson and Stockmeyer <ref> [GJS] </ref> had provided, as early as 1976, a reduction of Max-3-SAT 50 Bellare, Goldreich, Sudan to Max-2-SAT which showed that if the former is non-approximable within (k + 1)=k then the latter is non-approximable within (7k + 1)=(7k).
Reference: [GoWi1] <author> M. Goemans and D. Williamson. </author> <title> New 3/4-approximation algorithm for MAX SAT. </title> <booktitle> Proceedings of the 3rd Mathematical Programming Society Conference on Integer Programming and Combinatorial Optimization, </booktitle> <year> 1993. </year>
Reference-contexts: Max-3-SAT is the canonical Max-SNP complete problem [PaYa]. A polynomial-time algorithm due to Yannakakis [Ya] approximates it to within a factor of 4=3 &lt; 1:334 (see Goemans and Williamson <ref> [GoWi1] </ref> for an alternate algorithm). Currently the best known polynomial-time algorithm for Max-3-SAT achieves a factor of 1:258 (and is due to Sorkin et. al. [SSTW] which in turn build on Goemans and Williamson [GoWi2]).
Reference: [GoWi2] <author> M. Goemans and D. Williamson. </author> <title> :878 approximation algorithms for Max-CUT and Max-2SAT. </title> <booktitle> Proceedings of the Twenty Sixth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1994. </year>
Reference-contexts: MaxSAT [BeSu] 2 free-bits 0.794 O (1) 2 error 1/2 1 2 11 7 32 queries (24 on average) [FeKi] amortized free-bits O (2 m ) 2 3m 2m 3m free-bits [BeSu] Free Bits in PCP 11 Problem Approx Non-Approx Factor Due to New Factor Previous Factor Assumption Max-3-SAT 1:258 <ref> [Ya, GoWi2, SSTW] </ref> 1:038 1 + 1 72 [BeSu] P 6= NP Max-E3-SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max-2-SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP Max-CUT 1:139 [GoWi2] 1:015 unspecified [ALMSS] P 6= NP Min-VC <p> 3m free-bits [BeSu] Free Bits in PCP 11 Problem Approx Non-Approx Factor Due to New Factor Previous Factor Assumption Max-3-SAT 1:258 [Ya, GoWi2, SSTW] 1:038 1 + 1 72 [BeSu] P 6= NP Max-E3-SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max-2-SAT 1:075 <ref> [GoWi2, FeGo] </ref> 1:013 1 + 1 504 (implied [BeSu]) P 6= NP Max-CUT 1:139 [GoWi2] 1:015 unspecified [ALMSS] P 6= NP Min-VC 2 o (1) [BaEv, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ P N <p> New Factor Previous Factor Assumption Max-3-SAT 1:258 [Ya, GoWi2, SSTW] 1:038 1 + 1 72 [BeSu] P 6= NP Max-E3-SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max-2-SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP Max-CUT 1:139 <ref> [GoWi2] </ref> 1:015 unspecified [ALMSS] P 6= NP Min-VC 2 o (1) [BaEv, MoSp] 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP <p> Recall that the latter is approximable within 2-o (1) [BaEv, MoSp]. Our results for Max-CUTand Max-2-SAT show that it is not possible to find a solution with value which is only 1% away from being optimal. This may be contrasted with the recent results of <ref> [GoWi2, FeGo] </ref> which shows that solutions which are within 14% and 7.5%, respectively, of the optimum are obtainable in polynomial time. <p> Remark. As this definition indicates, we adopt the convention that the approximation factor is a number at least 1. Sometimes Max-SNP approximation is discussed in terms of factors at most 1 (e.g. <ref> [GoWi2, FeGo] </ref>) but obviously the two are equivalent via an inversion of the factor. We are interested in promise versions of Max-X-SAT which exhibit a gap in the MaxSAT () value between yes and no instances. <p> Currently the best known polynomial-time algorithm for Max-3-SAT achieves a factor of 1:258 (and is due to Sorkin et. al. [SSTW] which in turn build on Goemans and Williamson <ref> [GoWi2] </ref>). For Max-E3-SAT, which is also Max-SNP complete, a very simple algorithm achieves an approximation of 8=7 1:143 (where 7=8 is the expected fraction of clauses satisfied by a uniformly chosen assignment). Max-2-SAT is also Max-SNP complete [GJS, PaYa]. <p> Max-2-SAT is also Max-SNP complete [GJS, PaYa]. This problem is particularly interesting because it has been the focus of recent improvements in the approximation factor attainable in polynomial-time. Specifically, Goemans and Williamson <ref> [GoWi2] </ref> exhibited a polynomial time algorithm achieving an approximation factor of 1 0:878 1:139, and consequently Feige and Goemans [FeGo] exhibited an algorithm achieving 1 0:931 1:074. Non-approximability. <p> The gap of this problem is defined to be c=s. 3.8.2 Previous work In 1976, Sahni and Gonzales [SaGo] gave a simple 2-approximation algorithm for this problem. Recently, in a breakthrough result, Goemans and Williamson <ref> [GoWi2] </ref> gave a new algorithm which achieves a ratio of 1 0:878 = 1:139 for this problem. On the other hand, [PaYa] give an approximation preserving reduction from Max-3-SAT to Max-CUT.
Reference: [GMW] <author> O. Goldreich, S. Micali, and A. Wigderson. </author> <title> Proofs that yield nothing but their validity and a methodology of cryptographic protocol design. </title> <booktitle> Proceedings of the Twenty Seventh Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1986. </year>
Reference-contexts: to indicate that for every s &lt; 1, FPCP 1;s [poly; 0] coNP (1.2) FPCP 1;s [poly; 1] PSPACE (1.3) It seems that FPCP 1;1=2 [log; 0] is not contained in BPP, since Quadratic Non-Residuosity and Graph Non-Isomorphism belong to the former class. (Specifically, the interactive proofs of [GMR] and <ref> [GMW] </ref> can be viewed as a pcp system with polynomial randomness, query complexity 1 and free-bit complexity 0.) Thus, it seems that also the obvious observation PCP 1;s [poly; 1] AM (for every s &lt; 1, where AM stands for one round Arthur-Merlin games), would be hard to improve upon. 16 <p> First indication to the power of interactive proof systems was given in <ref> [GMW] </ref>, where it was shown that interactive proofs exist for Graph Non-Isomorphism (whereas this language is not known to be in NP). <p> Finally, applying Proposition 5.2.8 with k = f = 2, Part 1 follows. Proof of Proposition 5.1.7, Part (6): We merely note that the interactive proof presented in <ref> [GMW] </ref> for Graph Non-Isomorphism 5 constitute a 1-query pcp system with perfect completeness and soundness bound 1 2 . Furthermore, the query made by the verify has a unique acceptable answer and thus the free-bit complexity of this system is zero. <p> Furthermore, the query made by the verify has a unique acceptable answer and thus the free-bit complexity of this system is zero. The same holds for the interactive proof presented in [GMR] for Quadratic Non-Residuosity QNR, which is actually the inspiration to the proof in <ref> [GMW] </ref>. 5.1.3 Query complexity versus free-bit complexity The following proposition quantifies the intuition that not all queries are "free" (i.e., that the free-bit complexity is lower than the query complexity).
Reference: [GMR] <author> S. Goldwasser, S. Micali, and C. Rackoff. </author> <title> The knowledge complexity of interactive proofs. </title> <journal> SIAM J. </journal> <volume> Computing Vol 18, No. 1, </volume> <pages> 186-208, </pages> <year> 1989. </year>
Reference-contexts: of interest to indicate that for every s &lt; 1, FPCP 1;s [poly; 0] coNP (1.2) FPCP 1;s [poly; 1] PSPACE (1.3) It seems that FPCP 1;1=2 [log; 0] is not contained in BPP, since Quadratic Non-Residuosity and Graph Non-Isomorphism belong to the former class. (Specifically, the interactive proofs of <ref> [GMR] </ref> and [GMW] can be viewed as a pcp system with polynomial randomness, query complexity 1 and free-bit complexity 0.) Thus, it seems that also the obvious observation PCP 1;s [poly; 1] AM (for every s &lt; 1, where AM stands for one round Arthur-Merlin games), would be hard to improve <p> The indication of higher factors, and results for other problems, had to wait for the interactive proof approach. Interactive proofs were introduced by Goldwasser, Micali and Rackoff <ref> [GMR] </ref> and Babai [Bab]. Ben-Or, Goldwasser, Kilian and Wigderson [BGKW] extended these ideas to define a notion of multi-prover interactive proofs. <p> Proof of Proposition 5.1.6, Part (1): We first observe that a 1-query pcp system is actually a one-round interactive proof system (cf., <ref> [GMR] </ref>). (The completeness and soundness bounds are as in the pcp system.) Using well-known transformations we obtain the claimed result. <p> Namely, GNI 2 FPCP 1; 1 2 [coins = poly ; free = 0 ; query = 1] The same holds for QNR ("Quadratic Non-Residuosity" (cf., <ref> [GMR] </ref>)) the set of integer pairs (x; N ) so that x is a non-residue modulo N . contains the clause x ! y. <p> Furthermore, the query made by the verify has a unique acceptable answer and thus the free-bit complexity of this system is zero. The same holds for the interactive proof presented in <ref> [GMR] </ref> for Quadratic Non-Residuosity QNR, which is actually the inspiration to the proof in [GMW]. 5.1.3 Query complexity versus free-bit complexity The following proposition quantifies the intuition that not all queries are "free" (i.e., that the free-bit complexity is lower than the query complexity).
Reference: [GS] <author> S. Goldwasser and M. Sipser. </author> <title> Private Coins versus Public Coins in Interactive Proof Systems. </title> <booktitle> Proceedings of the Eighteenth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1986. </year>
Reference-contexts: Specifically, we first reduce the error of the interactive proof by parallel repetition, next transform it into an Arthur-Merlin interactive proof <ref> [GS] </ref>, and finally transform it into an Arthur-Merlin interactive proof of perfect completeness [FGMSZ]. We stress that all the transformations maintain the number of rounds upto a constant and that the constant-round Arthur-Merlin hierarchy collapses to one round [Bab].
Reference: [Has] <author> J. H -astad. </author> <title> private communication, </title> <month> September </month> <year> 1995. </year>
Reference-contexts: Furthermore, in such a case the lower bound of 1 on the free-bit complexity of the projection test still holds. Meanwhile, Hastad <ref> [Has] </ref> has constructed a pcp system of amortized free-bit complexity 1+*, 8* &gt; 0. Hastad's construction builds on the framework presented in the current work but introduces a (different type of a) relaxed codeword test which is conducted within amortized free-bit complexity *. <p> expect to prove non-approximability of Max Clique within N 1=2 . (The assumptions made by us and by Arora do not seem to be comparable: neither implies the other.) 1.8 Directions for further research Following the publication of an early version of this work and building on its techniques, Has-tad <ref> [Has] </ref> has constructed pcp systems for NP with amortized free-bit complexity 1 + *, 8* &gt; 0. This has resolved our previous challenge of reducing the amortized free-bit complexity to below 2, but left open the more difficult challenge of reducing the amortized free-bit complexity to below 1. <p> This requirement is not essential as can be seen in Section 3.4, where we show that also relaxed codeword tests, in which closeness means approximately the distance of the code, suffice. Furthermore, as shown by Hastad in <ref> [Has] </ref>, pcp systems of amortized free-bit complexity 1 (for NP) can be constructed by using a relaxed form of the codeword test, In a sense the lower bound on the complexity of the codeword test (and of the combined test) justify Hastad's relaxation of the requirements from the codeword test. 4.2.1
Reference: [Hoc] <author> D. Hochbaum. </author> <title> Efficient algorithms for the stable set, vertex cover and set packing problems. </title> <journal> Discrete Applied Mathematics, </journal> <volume> Vol 6, </volume> <pages> pages 243-254, </pages> <year> 1983. </year>
Reference-contexts: There is a simple polynomial time algorithm to approximate Min-VC in unweighted graphs within a factor of 2. The algorithm, due to F. Gavril (cf. [GJ2]), consists of taking all vertices which appear in a maximal matching of the graph. For weighted graphs, Bar-Yehuda and Even [BaEv1] and Hochbaum <ref> [Hoc] </ref>, gave algorithms achieving the same approximation factor. The best known algorithm today achieves a factor only slightly better, namely 2 (log log jV j)=(2 log jV j) [BaEv, MoSp].
Reference: [ImZu] <author> R. Impagliazzo and D. Zuckerman. </author> <title> How to Recycle Random Bits. </title> <booktitle> Proceedings of the Thirtieth Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1989. </year>
Reference-contexts: Arora and Safra [ArSa] reduced the randomness complexity of a PCP verifier for NP to logarithmic | they showed NP = PCP 1;1=2 [ coins = log ; query = p log N ]. They also observed that random bits can be recycled for error-reduction via the standard techniques <ref> [AKS, CW, ImZu] </ref>. The consequence was the first NP-hardness result for Max Clique approximation. <p> The de-randomized error reduction method consists of applying general, de-randomized, error-reduction techniques to the proof system setting. The best method knows as the "Expander Walk" technique is due to Ajtai, Komlos and Szemeredi [AKS] (see also <ref> [CW, ImZu] </ref>).
Reference: [JLL] <author> N.D. Jones, Y.E. Lien and W.T. Laaser. </author> <title> New problems complete for nondeterministic log space. </title> <journal> Math. Systems Theory Vol. </journal> <volume> 10, </volume> <year> 1976, </year> <pages> pages 1-17. </pages>
Reference-contexts: The latter checking reduces to guessing the variable for which a conflicting assignment is implied and verifying the conflict via s-t directed connectivity. Since the latter task is in N L, we are done. (Actually, 2SAT is complete for coN L; see <ref> [JLL] </ref>.) 3 The conjecture is stated for systems with perfect completeness. For systems with two-sided error probability, we know that they can recognize N P languages using zero free-bits see below. Free Bits in PCP 113 Proof of Proposition 5.1.7, Part (4): Here we consider proofs with zero free-bits.
Reference: [Kah] <author> N. Kahale. </author> <title> On the second eigenvalue and linear expansion of regular graphs. </title> <booktitle> Proceedings of the Thirty Third Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1992. </year> <note> Free Bits in PCP 127 </note>
Reference-contexts: The constant d will be determined so that d &gt; 2 4+ 8 * ). It is well-known by now, that a random walk of length t in an expander avoids a set of density with probability at most ( + d ) t (cf., <ref> [AKS, Kah] </ref>). Thus, as a preparation step, we reduce the error probability of the pcp system to p = d 2 d This is done using the trivial reduction of Proposition 5.2.1.
Reference: [KKLP] <author> V. Kann, S. Khanna, J. Lagergren and A. Panconesi. </author> <title> On the hardness of approximating MAX k-CUT and its dual. </title> <institution> Technical Report of the Department of Numerical Analysis and Computing Science, Royal Institute of Technology, Stockholm, TRITA-NA-P9505, </institution> <year> 1995. </year>
Reference: [KMS] <author> D. Karger, R. Motwani and M. Sudan. </author> <title> Approximate graph coloring by semidefinite programming. </title> <booktitle> Proceedings of the Thirty Fifth Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: He showed that a polynomial-time N 1* -factor approximation algorithm for Max Clique implies a polynomial time algorithm to color a three colorable graph with O (log N ) colors [Bl], which is much better than currently known <ref> [KMS] </ref>. But perhaps N 1o (1) is the best possible. Resolving the approximation complexity of this basic problem seems, in any case, to be worth some effort. Gaps in clique size.
Reference: [Ka] <author> R. Karp. </author> <title> Reducibility among combinatorial problems. Complexity of Computer Computations, </title> <editor> Miller and Thatcher (eds.), </editor> <publisher> Plenum Press, </publisher> <address> New York (1972). </address>
Reference-contexts: We've followed the common tradition regarding the names of polynomial-time reductions: many-to-one reductions are called Karp-reductions whereas (polynomial-time) Turing reductions are called Cook-reductions. This terminology is somewhat unfair towards Levin whose work on NP-completeness [Lev] was independent of those of Cook [Co] and Karp <ref> [Ka] </ref>. Actually, the reductions considered by Levin are more restricted as they also efficiently transform the corresponding NP-witnesses (this is an artifact of Levin's desire to treat search problems rather than decision problem).
Reference: [KLS] <author> S. Khanna, N. Linial and S. Safra. </author> <title> On the hardness of approximating the chromatic number. </title> <booktitle> Proceedings of the Second Israel Symposium on Theory and Computing Systems, </booktitle> <year> 1993. </year>
Reference-contexts: The conclusion for Max Clique follows, of course, from the FGLSS-reduction and Part (1) of Theorem 1.3.1. The conclusion for the Chromatic Number follows from a recent reduction of Furer [Fu], which in turn builds on reductions in <ref> [LuYa, KLS, BeSu] </ref>. <p> Using the FGLSS-construction on this system, the claim of Part (2) follows. Combining the above with a recent reduction of Furer [Fu], which in turn improved the reductions of <ref> [LuYa, KLS, BeSu] </ref>, we get Theorem 3.12.6 For any * &gt; 0 (1) NP K R Gap-ChromNum c;s for s (N )=c (N ) = N 1=5* . (2) Gap-ChromNum c;s is NP-complete for s (N )=c (N ) = N 1=7* . 3.12.4 Previous work Max Clique. <p> But, again, ffi is very, very small. Improvements to ffi are a function both of improvements to * and the values a; b for which (a; b)-reductions are available. A subsequent reduction of Khanna, Linial and Safra <ref> [KLS] </ref> is simpler but in fact slightly less efficient, being a (6; 5)-reduction. However a more efficient reduction is given by [BeSu]- they present a (1; 3)-reduction. Our N 1=3 hardness for Clique would yield, via this, a N 1=7 hardness for the chromatic number. <p> However now we know that FPCP and Max Clique are equivalent, so we can go back and rephrase the old statements. Thus results of <ref> [LuYa, KLS, BeSu] </ref> can be summarized as: For every fl &gt; 0, if approximating Max Clique to within N ff is NP-hard then approximating Chromatic Number to within N h (ff)fl is also NP-hard, where: (1) h (ff) = minf 1 6 ; ff 54ff g [LuYa]. (2) h (ff) =
Reference: [LaSh] <author> D. Lapidot and A. Shamir. </author> <title> Fully Parallelized Multi-prover protocols for NEXP-time. </title> <booktitle> Proceedings of the Thirty Second Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1991. </year>
Reference-contexts: The available constant-prover proof systems appear in Figure 3.1 and are discussed below. The two-prover proofs of Lapidot-Shamir and Feige-Lovasz <ref> [LaSh, FeLo] </ref> had poly-logarithmic randomness and answer sizes, so [ALMSS] used a modification of these, in the process increasing the number of provers to a constant much larger than two. The later constructions of few-prover proofs of [BGLR, Ta, FeKi] lead to better non-approximability results. <p> These features are captured in their definition of canonical verifiers. But the proof systems of [FeKi] that had worked above no longer sufficed| they are not canonical. So instead [BeSu] used (a slight modification of) the proofs of <ref> [LaSh, FeLo] </ref>, thereby incurring poly-logarithmic randomness and answer sizes, so that the assumptions in their non-approximability results pertain to quasi-polynomial time classes. (Alternatively they modify the [FeKi] system to a canonical three-prover one, but then incur a decrease in the non-approximability factors due to having more provers). <p> Free Bits in PCP 35 Due to Provers Coins Answer size Canonical? Can be made canonical? <ref> [LaSh, FeLo] </ref> 2 polylog polylog No Yes [BeSu] [ALMSS] poly (* 1 ) log polylog No ?? [BGLR] 4 log polyloglog No ?? [Ta] 3 log O (1) No ?? [FeKi] 2 log O (1) No At cost of one more prover [BeSu] [Raz] 2 log O (1) Yes (NA) *. <p> But we don't take advantage of this fact.) In addition we need answer sizes of O (log log n) as opposed to the O (log n) of previous methods, for reasons explained below. This means that even the (modified) <ref> [LaSh, FeLo] </ref> type proofs won't suffice for us. We could use the three-prover modification of [FeKi] but the cost would wipe out our gain. <p> Free Bits in PCP 49 Due to Assuming Factor Technique [ALMSS] P 6= NP some constant NP PCP 1;1=2 [ log; O (1) ]; Reduction of this to Max-3-SAT. [BGLR] e P 6= N e P 94=93 Framework; better analyses; uses proof systems of <ref> [LaSh, FeLo] </ref>. [BGLR] P 6= NP 113=112 New four-prover proof systems. [FeKi] P 6= NP 94=93 New two-prover proof systems. [BeSu] e P 6= N e P 66=65 Canonicity and some optimizations. [BeSu] P 6= NP 73=72 Canonicity and some optimizations.
Reference: [Lau] <author> C. Lautemann. </author> <title> BPP and the Polynomial Hierarchy. </title> <journal> Information Processing Letters, </journal> <volume> Vol. 17 (4), </volume> <pages> pages 215-217, </pages> <year> 1983. </year>
Reference-contexts: Instead, we use two alternative implementations, which yield the two parts of the proposition. In both implementations the free-bit complexity increases by log 2 m and the soundness bound increases by a factor of m. The first implementation employs a technique introduced by Lautemann (in the context of BPP) <ref> [Lau] </ref>. Using a randomized reduction, we supply the new verifier with a sequence of m possible "shifts" that it may effect. The new verifier selects one random-pad for the original verifier and generates m shifts of this pad.
Reference: [Lev] <author> L.A. Levin. </author> <title> Universal'nye perebornye zadachi (universal search problems : in russian). </title> <journal> Problemy Peredachi Informatsii, </journal> <volume> 9 (3), </volume> <pages> pages 265-266, </pages> <year> 1973. </year>
Reference-contexts: We've followed the common tradition regarding the names of polynomial-time reductions: many-to-one reductions are called Karp-reductions whereas (polynomial-time) Turing reductions are called Cook-reductions. This terminology is somewhat unfair towards Levin whose work on NP-completeness <ref> [Lev] </ref> was independent of those of Cook [Co] and Karp [Ka]. Actually, the reductions considered by Levin are more restricted as they also efficiently transform the corresponding NP-witnesses (this is an artifact of Levin's desire to treat search problems rather than decision problem).
Reference: [LPS] <author> A. Lubotzky, R. Phillips and P. Sarnak. </author> <title> Explicit Expanders and the Ramanu-jan Conjectures. </title> <booktitle> Proceedings of the Eighteenth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1986. </year>
Reference-contexts: Specifically, using a randomized reduction we can transform FPCP 1; 1 2 [log; f ] into FPCP 1;2 k [log +k; k f ]. (This transformation is analogous to the well-known transformation of Berman and Schnitger [BeSc].) Alternatively, using a known deterministic amplification method based on <ref> [AKS, LPS] </ref> one can transform FPCP 1; 1 2 [log; f ] into FPCP 1;2 k [log +2k; k f ] (ignoring multiplicative factors of 1 + * for arbitrarily small * &gt; 0). (To the best of our knowledge this transformation has never appeared with a full proof.) Both alternatives <p> Thus the Ramanujan Expander of Lubotzky, Phillips and Sarnak <ref> [LPS] </ref> play an important role yielding 2 (cf. Proposition 5.2.4), which is the best possible. 3.13 The coding theory bound We provide here the coding theory bound used in the proof of Lemma 3.4.4. <p> Specifically, using a degree d expander graph with second eigenvalue yields a factor of log 2 d 1+log 2 . Thus, it is essential to use Ramanujan graphs <ref> [LPS] </ref> in order to obtain the claimed constant 2 (1 + *). Proof of Proposition 5.2.4: For simplicity assume s = 1=2. The idea is to use a "pseudorandom" sequence generated by a random walk on an expander graph in order to get error reduction at moderate randomness cost. <p> The idea is to use a "pseudorandom" sequence generated by a random walk on an expander graph in order to get error reduction at moderate randomness cost. Specifically, we will use a Ramanujan expander graph of constant degree d and second eigenvalue 2 p d (cf., <ref> [LPS] </ref>). The constant d will be determined so that d &gt; 2 4+ 8 * ). It is well-known by now, that a random walk of length t in an expander avoids a set of density with probability at most ( + d ) t (cf., [AKS, Kah]).
Reference: [LuYa] <author> C. Lund and M. Yannakakis. </author> <title> On the hardness of approximating minimization problems. </title> <journal> Journal of the ACM, </journal> <volume> vol. 41, </volume> <pages> pages 960-981, </pages> <year> 1994. </year>
Reference-contexts: The conclusion for Max Clique follows, of course, from the FGLSS-reduction and Part (1) of Theorem 1.3.1. The conclusion for the Chromatic Number follows from a recent reduction of Furer [Fu], which in turn builds on reductions in <ref> [LuYa, KLS, BeSu] </ref>. <p> After the work of [FGLSS] the field took off in two major directions. One was to extend the interactive proof approach to apply also to other optimization problems. Direct reductions from proofs were used to show hardness of quadratic programming [BeRo, FeLo], Max-3-SAT [ALMSS], set cover <ref> [LuYa] </ref>, and other problems [Be]. Also, reductions from Max Clique lead to hardness results for the chromatic number [LuYa] and other problems [Zu], while previous reductions from Max-3-SAT lead to hardness results for all of Max-SNP [PaYa]. <p> Direct reductions from proofs were used to show hardness of quadratic programming [BeRo, FeLo], Max-3-SAT [ALMSS], set cover <ref> [LuYa] </ref>, and other problems [Be]. Also, reductions from Max Clique lead to hardness results for the chromatic number [LuYa] and other problems [Zu], while previous reductions from Max-3-SAT lead to hardness results for all of Max-SNP [PaYa]. <p> One of these is that they are a good starting point for reductions| examples of such are reductions of two-prover proofs to quadratic programming [BeRo, FeLo] and set cover <ref> [LuYa] </ref>. However, it is a different aspect of constant prover proofs that is of more direct concern to us. This aspect is the use of constant-prover proof systems as the penultimate step of the recursion, and begins with [ALMSS]. <p> Using the FGLSS-construction on this system, the claim of Part (2) follows. Combining the above with a recent reduction of Furer [Fu], which in turn improved the reductions of <ref> [LuYa, KLS, BeSu] </ref>, we get Theorem 3.12.6 For any * &gt; 0 (1) NP K R Gap-ChromNum c;s for s (N )=c (N ) = N 1=5* . (2) Gap-ChromNum c;s is NP-complete for s (N )=c (N ) = N 1=7* . 3.12.4 Previous work Max Clique. <p> To discuss the quality of reductions, let us, following [BeSu], define an (a; b)- reduction to be one that achieves ffi = 1 ab+(b=*) = * The first reduction, namely that of Lund and Yannakakis <ref> [LuYa] </ref>, was a (1; 5)-reduction. Via the Max Clique hardness results of [ArSa, ALMSS] this implies the chromatic number is hard to approximate within N ffi for some ffi &gt; 0. But, again, ffi is very, very small. <p> We stress that the ratio c (N) s (N) does not remain invariant. Rephrasing known reductions from Max Clique to Chromatic Number Starting with the work of Lund and Yannakakis <ref> [LuYa] </ref>, there have been several works on showing the hardness of approximating the Chromatic number, which reduce the Max Clique problem to the Chromatic number problem. <p> However now we know that FPCP and Max Clique are equivalent, so we can go back and rephrase the old statements. Thus results of <ref> [LuYa, KLS, BeSu] </ref> can be summarized as: For every fl &gt; 0, if approximating Max Clique to within N ff is NP-hard then approximating Chromatic Number to within N h (ff)fl is also NP-hard, where: (1) h (ff) = minf 1 6 ; ff 54ff g [LuYa]. (2) h (ff) = <p> Thus results of [LuYa, KLS, BeSu] can be summarized as: For every fl &gt; 0, if approximating Max Clique to within N ff is NP-hard then approximating Chromatic Number to within N h (ff)fl is also NP-hard, where: (1) h (ff) = minf 1 6 ; ff 54ff g <ref> [LuYa] </ref>. (2) h (ff) = minf 1 11 ; ff (3) h (ff) = minf 1 4 ; ff 32ff g [BeSu]. (4) h (ff) = minf 1 3 ; ff (Our discussion of Furer's results [Fu] reflects only the best current understanding we have of them, since it is on-going
Reference: [LFKN] <author> C. Lund, L. Fortnow, H. Karloff, and N. Nisan. </author> <title> Algebraic Methods for Interactive Proof Systems. </title> <booktitle> Proceedings of the Thirty First Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1990. </year>
Reference-contexts: First indication to the power of interactive proof systems was given in [GMW], where it was shown that interactive proofs exist for Graph Non-Isomorphism (whereas this language is not known to be in NP). However, the real breakthrough came with the result of Lund, Fortnow, Karloff and Nisan <ref> [LFKN] </ref> who used algebraic methods for showing that all coNP languages (and actually, all languages in P #P ) have interactive proof systems. These techniques were used by Shamir [Sh] to show that IP = PSPACE.
Reference: [MaSl] <author> F. MacWilliams and N. Sloane. </author> <title> The Theory of Error-Correcting Codes. </title> <publisher> North-Holland, </publisher> <year> 1981. </year>
Reference-contexts: To do so we need the following Lemma. It is the counterpart of a claim in [BGLR, Lemma 3.5] and will be used in the same way. The lemma can be derived from a coding theory bound which is slight extension of bounds in <ref> [MaSl, Ch. 7] </ref> and is provided in Section 3.13. Lemma 3.4.4 Suppose 0 ffi 1=2 and A: F l ! . <p> Proposition 5.2.4), which is the best possible. 3.13 The coding theory bound We provide here the coding theory bound used in the proof of Lemma 3.4.4. It is a slight extension of bounds in <ref> [MaSl, Ch. 17] </ref> which consider only vectors of weight exactly w rather than at most w. For sake of completeness, we include a proof of this bound.
Reference: [MoRa] <author> R. Motwani and P. Raghavan. </author> <title> Randomized Algorithms. </title> <publisher> Cambridge University Press, </publisher> <year> 1995. </year>
Reference-contexts: Let t def = C=L. Then, E [ P v2S v ] = t. Using a multiplicative Chernoff bound <ref> [MoRa] </ref>, we get Pr [ 8v 2 S : l (v) 6= i ] = Pr X v = 0 &lt; 2 t Call the i th layer bad if no vertex of S is placed in it. <p> Clearly, the i 's are mutually independent and each equals 1 with probability ffi s. Using a multiplicative Chernoff Bound (cf. <ref> [MoRa, Theorem 4.3] </ref>), the probability that a random R is bad (for x w.r.t. ) is bounded by Pr i=1 # Thus, by the choice of m, the probability that a random R is bad for x, with respect to any fixed oracle, is smaller than 1 2 2 l .
Reference: [MoSp] <author> Monien and Speckenmeyer. </author> <title> Some further approximation algorithms for the vertex cover problem. </title> <booktitle> Proceedings of CAAP 83, Lecure Notes in Computer Science Vol. </booktitle> <volume> 159, </volume> <publisher> Springer-Verlag, </publisher> <year> 1983. </year>
Reference-contexts: 1 + 1 72 [BeSu] P 6= NP Max-E3-SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max-2-SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP Max-CUT 1:139 [GoWi2] 1:015 unspecified [ALMSS] P 6= NP Min-VC 2 o (1) <ref> [BaEv, MoSp] </ref> 1 + 1 15 unspecified [ALMSS] P 6= NP Max-Clique N 1o (1) [BoHa] N 1 4 [BeSu] NP 6 coR ~ P N 3 N 5 coRP 6= NP N 4 N 6 [BeSu] P 6= NP Chromatic N 1o (1) [BoHa] N 1 10 [BeSu] NP 6 <p> Recall that the latter is approximable within 2-o (1) <ref> [BaEv, MoSp] </ref>. Our results for Max-CUTand Max-2-SAT show that it is not possible to find a solution with value which is only 1% away from being optimal. <p> For weighted graphs, Bar-Yehuda and Even [BaEv1] and Hochbaum [Hoc], gave algorithms achieving the same approximation factor. The best known algorithm today achieves a factor only slightly better, namely 2 (log log jV j)=(2 log jV j) <ref> [BaEv, MoSp] </ref>.
Reference: [PaYa] <author> C. Papadimitriou and M. Yannakakis. </author> <title> Optimization, approximation, and complexity classes. </title> <journal> Journal of Computer and System Sciences 43, </journal> <pages> pp. 425-440, </pages> <year> 1991. </year>
Reference-contexts: Also, reductions from Max Clique lead to hardness results for the chromatic number [LuYa] and other problems [Zu], while previous reductions from Max-3-SAT lead to hardness results for all of Max-SNP <ref> [PaYa] </ref>. The other direction was to increase factors and reduce assumptions for problems already shown hard to some factor under some assumption, by improving the efficiency of the underlying proof systems and/or the efficiency of the reductions. <p> This immediately implied the NP-hardness of approximating Max Clique within N * , for some * &gt; 0. Furthermore, it also implied that Max-3-Sat is NP-hard to approximate to within some constant factor [ALMSS] and so is the entire class Max-SNP <ref> [PaYa] </ref>. Attempts to improve the constant in the exponent of the Max Clique hardness factor, and also improve the constant values of the hardness factors in the Max-SNP hardness results, begin with Bellare, Goldwasser, Lund and Russell [BGLR]. <p> This paper P 6= NP 27=26 Long code and new proof systems. assumption under which this was done. 3.7.2 Previous work Approximation algorithms. Max-3-SAT is the canonical Max-SNP complete problem <ref> [PaYa] </ref>. A polynomial-time algorithm due to Yannakakis [Ya] approximates it to within a factor of 4=3 &lt; 1:334 (see Goemans and Williamson [GoWi1] for an alternate algorithm). <p> For Max-E3-SAT, which is also Max-SNP complete, a very simple algorithm achieves an approximation of 8=7 1:143 (where 7=8 is the expected fraction of clauses satisfied by a uniformly chosen assignment). Max-2-SAT is also Max-SNP complete <ref> [GJS, PaYa] </ref>. This problem is particularly interesting because it has been the focus of recent improvements in the approximation factor attainable in polynomial-time. <p> Recently, in a breakthrough result, Goemans and Williamson [GoWi2] gave a new algorithm which achieves a ratio of 1 0:878 = 1:139 for this problem. On the other hand, <ref> [PaYa] </ref> give an approximation preserving reduction from Max-3-SAT to Max-CUT. Combined with [ALMSS] this shows that there exists a constant ff &gt; 1 such that approximating Max-CUTwithin a factor of ff is NP-hard. <p> As for c 0 , it equals 3ff 1 +2ff 2 3m 1 +2m 2 &gt; 0:6. 3.9 Free bits and vertex cover It is known that approximating the minimum vertex cover of a graph to within a 1 + * factor is hard, for some * &gt; 0 <ref> [PaYa, ALMSS] </ref>. However, we do not know of any previous attempt to provide a lower bound for *. <p> The version of Min-VC in which one restricts attention to graphs of degree bounded by a constant B, is Max-SNP complete for suitably large B <ref> [PaYa] </ref>. In particular they provide a reduction from Max-3-SAT. Combined with [ALMSS] this implies the existence of a constant ffi &gt; 0 such that approximating Min-VC within a factor of 1 + ffi is hard unless P = NP. No explicit value of ffi has been stated until now. <p> No explicit value of ffi has been stated until now. Indeed, the value that could be derived, even using the best existing non-approximability results for Max-3-SAT, will be very small, because of the cost of the reduction of <ref> [PaYa] </ref>, which first reduces Max-3-SAT to its bounded version using expanders, and then reduces this to Min-VC-B. Going from Free bits to VC. Rather than reduce from Max-3-SAT, we will first use Theorem 3.9.4 to get gaps in Clique size. Then we apply the standard reduction.
Reference: [Pet] <author> E. Petrank. </author> <title> The Hardness of Approximations: Gap Location. </title> <institution> TR-754, Department of Computer Science, Technion - Israel Institute of Technology, </institution> <year> 1992. </year> <note> 128 Bellare, Goldreich, Sudan </note>
Reference-contexts: In other words, we are given a system of linear equations over GF (2), and need to determine the maximum number of equations which may be simultaneously satisfied. The maximization problem is known to be Max-SNP complete (see [BrNa] or <ref> [Pet] </ref>). Here we provide a stronger bound via a direct reduction from the MaxSNP verifier. Before continuing, we remark that the problem of maximizing the number of satisfiable equations should not be confused with the "complementary" problem of minimizing the number of violated constraints, investigated by Arora et. al. [ABSS].
Reference: [PoSp] <author> A. Polishchuk and D. Spielman. </author> <title> Nearly-linear size holographic proofs. </title> <booktitle> Proceedings of the Twenty Sixth Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1994. </year>
Reference-contexts: Free bits are implicit in [FeKi] and formalized in [BeSu]. Amortized free bits are introduced in [BeSu] but formalized a little better here. Proof sizes were considered in <ref> [BFLS, PoSp] </ref>. We consider them here for a different reason-they play an important role in that the randomized FGLSS reduction [BeSc, Zu] depends actually on this rather than the randomness.
Reference: [Raz] <author> R. Raz. </author> <title> A parallel repetition theorem. </title> <booktitle> Proceedings of the Twenty Seventh Annual Symposium on the Theory of Computing, ACM, </booktitle> <year> 1995. </year>
Reference-contexts: We have the advantage of being able to use, at the outer level, the verifier of Raz <ref> [Raz] </ref> which appeared only recently and was not available to previous works. The inner level verifier relies on the use of a "good" encoding scheme. Since [ALMSS], constructions of this verifier have used the Hadamard Code for this purpose. <p> and it will help to begin with some indication of what we will be doing. 3.1 Overview and guidemap The starting point for all our proof systems is a two-prover proof system achieving arbitrarily small but fixed constant error with logarithmic randomness and constant answer size, as provided by Raz <ref> [Raz] </ref>. This proof system has the property that the answer of the second prover is supposed to be a predetermined function of the answer of the first prover. <p> We then define a canonical inner verifier. Recursion is captured by an appropriate definition of a composed verifier whose attributes we relate to those of the original verifiers in Theorem 3.4.5. The specific outer verifier we will use is one obtained by a recent work of Raz <ref> [Raz] </ref>. We will construct various inner verifiers based on the long code and the tests in Section 3.5 and Section 3.11. Theorem 3.4.5 will be used ubiquitously to combine the two. <p> Theorem 3.4.5 will be used ubiquitously to combine the two. For a better understanding of the role of constant-prover proof systems in this context, and an explanation of what the use of <ref> [Raz] </ref> buys as opposed to the use of other systems, we have provided at the end of this subsection an explanatory history. 3.4.1 Outer verifiers As mentioned above, outer verifiers will model certain special kinds of two-prover, one-round proof systems. <p> Using the Parallel Repetition Theorem of Raz, we obtain our starting point - Lemma 3.4.2 (construction of outer verifiers <ref> [Raz] </ref>): Let L 2 NP. Then for every * &gt; 0 there exist positive integers l; l 1 and c such that there exists an (l; l 1 )-canonical outer verifier which is *-good for L and uses randomness r (n) = c log 2 n. Actually, Raz's Theorem [Raz] enables <p> verifiers <ref> [Raz] </ref>): Let L 2 NP. Then for every * &gt; 0 there exist positive integers l; l 1 and c such that there exists an (l; l 1 )-canonical outer verifier which is *-good for L and uses randomness r (n) = c log 2 n. Actually, Raz's Theorem [Raz] enables one to assert that l; l 1 and c are O (log * 1 ); but we will not need this fact. <p> Answer size Canonical? Can be made canonical? [LaSh, FeLo] 2 polylog polylog No Yes [BeSu] [ALMSS] poly (* 1 ) log polylog No ?? [BGLR] 4 log polyloglog No ?? [Ta] 3 log O (1) No ?? [FeKi] 2 log O (1) No At cost of one more prover [BeSu] <ref> [Raz] </ref> 2 log O (1) Yes (NA) *. We indicate the number of provers, the randomness and answer sizes, and whether or not the system is canonical. <p> This means that even the (modified) [LaSh, FeLo] type proofs won't suffice for us. We could use the three-prover modification of [FeKi] but the cost would wipe out our gain. Luckily this discussion is moot since we can use the recent result of Raz <ref> [Raz] </ref> to provide us with a canonical two-prover proof having logarithmic randomness, constant answer size, and any constant error. This makes an ideal starting point. To simplify the definitions above we insisted on constant answer size and two provers from the start.
Reference: [SaGo] <author> S. Sahni and T. Gonzales. </author> <title> P-complete approximation problems. </title> <journal> J. of the ACM, </journal> <volume> 23 </volume> <pages> 555-565, </pages> <year> 1976. </year>
Reference-contexts: The gap of this problem is defined to be c=s. 3.8.2 Previous work In 1976, Sahni and Gonzales <ref> [SaGo] </ref> gave a simple 2-approximation algorithm for this problem. Recently, in a breakthrough result, Goemans and Williamson [GoWi2] gave a new algorithm which achieves a ratio of 1 0:878 = 1:139 for this problem. On the other hand, [PaYa] give an approximation preserving reduction from Max-3-SAT to Max-CUT.
Reference: [Ta] <author> G. Tardos. </author> <title> Multi-prover encoding schemes and three prover proof systems. </title> <booktitle> Proceedings of the Ninth Annual Conference on Structure in Complexity Theory, IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: The two-prover proofs of Lapidot-Shamir and Feige-Lovasz [LaSh, FeLo] had poly-logarithmic randomness and answer sizes, so [ALMSS] used a modification of these, in the process increasing the number of provers to a constant much larger than two. The later constructions of few-prover proofs of <ref> [BGLR, Ta, FeKi] </ref> lead to better non-approximability results. Bellare and Sudan [BeSu] identified some extra features of constant prover proofs whose presence they showed could be exploited to further increase the non-approximability factors. These features are captured in their definition of canonical verifiers. <p> Free Bits in PCP 35 Due to Provers Coins Answer size Canonical? Can be made canonical? [LaSh, FeLo] 2 polylog polylog No Yes [BeSu] [ALMSS] poly (* 1 ) log polylog No ?? [BGLR] 4 log polyloglog No ?? <ref> [Ta] </ref> 3 log O (1) No ?? [FeKi] 2 log O (1) No At cost of one more prover [BeSu] [Raz] 2 log O (1) Yes (NA) *. We indicate the number of provers, the randomness and answer sizes, and whether or not the system is canonical.
Reference: [Sh] <author> A. Shamir. IP=PSPACE. </author> <booktitle> Proceedings of the Thirty First Annual Symposium on the Foundations of Computer Science, IEEE, </booktitle> <year> 1990. </year>
Reference-contexts: However, the real breakthrough came with the result of Lund, Fortnow, Karloff and Nisan [LFKN] who used algebraic methods for showing that all coNP languages (and actually, all languages in P #P ) have interactive proof systems. These techniques were used by Shamir <ref> [Sh] </ref> to show that IP = PSPACE. A central result which enabled the approximation connection is that of Babai, Fortnow and Lund [BFL] who showed that the class MIP equals the class NEXP (i.e., languages recognizable in non-deterministic exponential time).
Reference: [SSTW] <author> G. Sorkin, M. Sudan, L. Trevisan and D. Williamson. </author> <note> In preparation. </note> <year> 1995. </year>
Reference-contexts: MaxSAT [BeSu] 2 free-bits 0.794 O (1) 2 error 1/2 1 2 11 7 32 queries (24 on average) [FeKi] amortized free-bits O (2 m ) 2 3m 2m 3m free-bits [BeSu] Free Bits in PCP 11 Problem Approx Non-Approx Factor Due to New Factor Previous Factor Assumption Max-3-SAT 1:258 <ref> [Ya, GoWi2, SSTW] </ref> 1:038 1 + 1 72 [BeSu] P 6= NP Max-E3-SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max-2-SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP Max-CUT 1:139 [GoWi2] 1:015 unspecified [ALMSS] P 6= NP Min-VC <p> A polynomial-time algorithm due to Yannakakis [Ya] approximates it to within a factor of 4=3 &lt; 1:334 (see Goemans and Williamson [GoWi1] for an alternate algorithm). Currently the best known polynomial-time algorithm for Max-3-SAT achieves a factor of 1:258 (and is due to Sorkin et. al. <ref> [SSTW] </ref> which in turn build on Goemans and Williamson [GoWi2]). For Max-E3-SAT, which is also Max-SNP complete, a very simple algorithm achieves an approximation of 8=7 1:143 (where 7=8 is the expected fraction of clauses satisfied by a uniformly chosen assignment). Max-2-SAT is also Max-SNP complete [GJS, PaYa]. <p> Proof of Proposition 5.1.2, Part (5): The result for general verifiers follows from Lemma 3.7.5 and the fact that MaxSAT can be approximated to within a 0:795 = 0:75+ 0:18 4 factor in polynomial-time (cf., <ref> [SSTW] </ref>). The rest of the proof is devoted to the non-adaptive case. Let L 2 naPCP 1;s [log; 3], and let V be a (non-adaptive) verifier demonstrating this fact. Without loss of generality, we may assume that V always makes 3 different queries.
Reference: [Ya] <author> M. Yannakakis, </author> <title> On the approximation of maximum satisfiability. </title> <journal> Journal of Algorithms, </journal> <volume> vol. 17, </volume> <pages> pages 475-502, </pages> <year> 1994. </year>
Reference-contexts: MaxSAT [BeSu] 2 free-bits 0.794 O (1) 2 error 1/2 1 2 11 7 32 queries (24 on average) [FeKi] amortized free-bits O (2 m ) 2 3m 2m 3m free-bits [BeSu] Free Bits in PCP 11 Problem Approx Non-Approx Factor Due to New Factor Previous Factor Assumption Max-3-SAT 1:258 <ref> [Ya, GoWi2, SSTW] </ref> 1:038 1 + 1 72 [BeSu] P 6= NP Max-E3-SAT 1 + 1 7 folklore 1 + 1 26 unspecified [ALMSS] P 6= NP Max-2-SAT 1:075 [GoWi2, FeGo] 1:013 1 + 1 504 (implied [BeSu]) P 6= NP Max-CUT 1:139 [GoWi2] 1:015 unspecified [ALMSS] P 6= NP Min-VC <p> This paper P 6= NP 27=26 Long code and new proof systems. assumption under which this was done. 3.7.2 Previous work Approximation algorithms. Max-3-SAT is the canonical Max-SNP complete problem [PaYa]. A polynomial-time algorithm due to Yannakakis <ref> [Ya] </ref> approximates it to within a factor of 4=3 &lt; 1:334 (see Goemans and Williamson [GoWi1] for an alternate algorithm). Currently the best known polynomial-time algorithm for Max-3-SAT achieves a factor of 1:258 (and is due to Sorkin et. al. [SSTW] which in turn build on Goemans and Williamson [GoWi2]).
Reference: [Zu] <author> D. Zuckerman. </author> <title> NP-Complete Problems have a version that is hard to Approximate. </title> <booktitle> Proceedings of the Eighth Annual Conference on Structure in Complexity Theory, IEEE, </booktitle> <year> 1993. </year>
Reference-contexts: For the best results one typically uses a randomized form of this reduction due to <ref> [BeSc, Zu] </ref> and it is this that we will assume henceforth. A NP-hard gap problem is obtained roughly as follows. First, one exhibits an appropriate proof system for NP. Then one applies the FGLSS reduction. The factor indicated hard depends on the proof system parameters. <p> Direct reductions from proofs were used to show hardness of quadratic programming [BeRo, FeLo], Max-3-SAT [ALMSS], set cover [LuYa], and other problems [Be]. Also, reductions from Max Clique lead to hardness results for the chromatic number [LuYa] and other problems <ref> [Zu] </ref>, while previous reductions from Max-3-SAT lead to hardness results for all of Max-SNP [PaYa]. <p> They presented new proof systems minimizing query complexity and exploited a slightly improved version of the FGLSS-reduction due to <ref> [BeSc, Zu] </ref> to get a N 1=30 hardness of approximation factor for Max Clique. Feige and Kilian [FeKi], however, observed that one should work with free-bits, and noted that the free-bit complexity of the system of [BGLR] was 14, yielding a N 1=15 hardness factor. <p> If the reduction is deterministic we omit the subscript of "R," or, sometimes, for emphasis, replace it by a subscript of "D." An example is the randomized FGLSS transformation <ref> [FGLSS, BeSc, Zu] </ref>. Here (A 1 ; B 1 ) is typically an NP-complete language L, and (A 2 ; B 2 ) is Gap-Clique c;s for some c; s which are determined by the transformation. <p> Free bits are implicit in [FeKi] and formalized in [BeSu]. Amortized free bits are introduced in [BeSu] but formalized a little better here. Proof sizes were considered in [BFLS, PoSp]. We consider them here for a different reason-they play an important role in that the randomized FGLSS reduction <ref> [BeSc, Zu] </ref> depends actually on this rather than the randomness. To deal with the now huge array of parameters we have generalized the notation of [BGLR] to allow specification of parameters by name. <p> The number of queries was unspecified, but indicated to be 10 4 , so * 10 4 . Later work has focused on reducing the constant value of * in the exponent. 5 In later work a slightly tighter form of the FGLSS reduction due to <ref> [BeSc, Zu] </ref> has been used. <p> An alternative description is given by Zuckerman <ref> [Zu] </ref>. Another alternative description, carried out in the proof system, is presented in Section 5.2. The de-randomized error reduction method consists of applying general, de-randomized, error-reduction techniques to the proof system setting. <p> Each of the two parts of Proposition 4.1.7 shows that the well-known method of obtaining clique-approximation results from efficient pcp systems (cf., <ref> [FGLSS, BeSc, Zu, FeKi, BeSu] </ref>) is "complete" in the sense that if clique-approximation can be shown NP-hard then this can be done via this method. The following is a more precise version of Theorem 1.4.1 in that the role of * &gt; 0 is made explicit.
References-found: 77

