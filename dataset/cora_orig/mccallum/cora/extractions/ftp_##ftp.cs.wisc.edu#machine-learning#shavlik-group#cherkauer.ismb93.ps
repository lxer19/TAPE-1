URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/cherkauer.ismb93.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/cherkauer.ismb93.ps.abstract.html
Root-URL: 
Email: cherkaue@cs.wisc.edu shavlik@cs.wisc.edu  
Title: Protein Structure Prediction: Selecting Salient Features from Large Candidate Pools  
Author: Kevin J. Cherkauer Jude W. Shavlik 
Address: 1210 W. Dayton St., Madison, WI 53706  
Affiliation: Computer Sciences Department, University of Wisconsin-Madison  
Note: Appears in Proceedings of the First International Conference on Intelligent Systems for Molecular Biology, Bethesda, MD: AAAI Press (1993). c 1993 AAAI  
Abstract: We introduce a parallel approach, "DT-Select," for selecting features used by inductive learning algorithms to predict protein secondary structure. DT-Select is able to rapidly choose small, nonredundant feature sets from pools containing hundreds of thousands of potentially useful features. It does this by building a decision tree, using features from the pool, that classifies a set of training examples. The features included in the tree provide a compact description of the training data and are thus suitable for use as inputs to other inductive learning algorithms. Empirical experiments in the protein secondary-structure task, in which sets of complex features chosen by DT-Select are used to augment a standard artificial neural network representation, yield surprisingly little performance gain, even though features are selected from very large feature pools. We discuss some possible reasons for this result. 1 
Abstract-found: 1
Intro-found: 1
Reference: <author> Almuallim, H., & Ditterich, T.G. </author> <year> (1991). </year> <title> Learning With Many Irrelevant Features. </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <volume> Vol. </volume> <pages> II (pp. 547-552). </pages> <address> Anaheim, CA: </address> <publisher> AAAI Press/The MIT Press. </publisher>
Reference: <author> Chou, P.Y., & Fasman, G.D. </author> <year> (1978). </year> <title> Prediction of the Secondary Structure of Proteins from their Amino Acid Sequence. </title> <booktitle> Advances in Enzymology, </booktitle> <volume> 47, </volume> <pages> 45-148. </pages>
Reference: <author> Craven, M.W., & Shavlik, J.W. </author> <year> (1993). </year> <title> Learning to Predict Reading Frames in E. coli DNA Sequences. </title> <booktitle> Proceedings of the Twenty-sixth Hawaii International Conference on System Science (pp. </booktitle> <pages> 773-782). </pages> <address> Maui, HI: </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: <author> Farber, R., Lapedes, A., & Sirotkin, K. </author> <year> (1992). </year> <title> Determination of Eucaryotic Protein Coding Regions Using Neural Networks and Information Theory. </title> <journal> Journal of Molecular Biology, </journal> <volume> 226, </volume> <pages> 471-479. </pages>
Reference: <author> Fayyad, </author> <title> U.M., & Irani, K.B. (1992). The Attribute Selection Problem in Decision Tree Generation. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> (pp. 104-110). </pages> <address> San Jose, CA: </address> <publisher> AAAI Press/The MIT Press. </publisher>
Reference: <author> Hunter, L. </author> <year> (1991). </year> <title> Representing Amino Acids with Bit-strings. </title> <booktitle> Working Notes, AAAI Workshop: AI Approaches to Classification and Pattern Recognition in Molecular Biology, </booktitle> <pages> (pp. 110-117). </pages> <address> Anaheim, CA. </address>
Reference: <author> Kidera, A., Konishi, Y., Oka, M., Ooi, T., & Scheraga, H.A. </author> <year> (1985). </year> <title> Statistical Analysis of the Physical Properties of the 20 Naturally Occurring Amino Acids. </title> <journal> Journal of Protein Chemistry, </journal> <volume> 4, 1, </volume> <pages> 23-55. </pages>
Reference: <author> King, R.D., & Sternberg, J.E. </author> <year> (1990). </year> <title> Machine Learning Approach for the Prediction of Protein Secondary Structure. </title> <journal> Journal of Molecular Biology, </journal> <volume> 216, </volume> <pages> 441-457. </pages>
Reference: <author> Lim, V.I. </author> <year> (1974a). </year> <title> Algorithms for Prediction of ff-Helical and fi-Structural Regions in Globular Proteins. </title> <journal> Journal of Molecular Biology, </journal> <volume> 88, </volume> <pages> 873-894. </pages>
Reference: <author> Lim, V.I. </author> <year> (1974b). </year> <title> Structural Principles of the Globular Organization of Protein Chains. A Stereochemical Theory of Globular Protein Secondary Structure. </title> <journal> Journal of Molecular Biology, </journal> <volume> 88, </volume> <pages> 857-872. </pages>
Reference: <author> Qian, N., & Sejnowski, T.J. </author> <year> (1988). </year> <title> Predicting the Secondary Structure of Globular Proteins Using Neural Network Models. </title> <journal> Journal of Molecular Biology, </journal> <volume> 202, </volume> <pages> 865-884. </pages>
Reference: <author> Quinlan, J.R. </author> <year> (1986). </year> <title> Induction of Decision Trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: this problem, we limit tree growth by requiring a 2 We also intend to explore the use of Fayyad and Irani's (1992) orthogonality measure, which exhibits better performance on several tasks, as a substitute for information gain. feature to pass a simple 2 test before being added to the tree <ref> (Quinlan, 1986) </ref>. This test, whose strictness may be adjusted as a parameter, ensures that included features discriminate examples with an accuracy better than expected by chance. 3 Feature selection is accomplished efficiently by decision-tree construction, resulting in small sets of discriminatory features that are capable of describing the dataset.
Reference: <author> Quinlan, J.R. </author> <year> (1990). </year> <title> Learning Logical Definitions from Relations. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239-166. </pages>
Reference-contexts: We will soon begin testing the method on the DNA coding-region prediction task as well. A more general issue we intend to explore is the use of selection algorithms other than decision trees. We have performed a few initial experiments with two other algorithms, one which builds Foil-like rules <ref> (Quinlan, 1990) </ref> to describe the individual example classes and another which applies statistical independence tests to select features which are largely orthogonal, but more work is needed to determine the strengths and weaknesses of different feature-selection approaches.
Reference: <author> Quinlan, J.R. </author> <year> (1992). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The biological literature provides some clues to the kinds of features that are important for this problem (Lim, 1974a, 1974b; Chou & Fasman, 3 We intend to replace this criterion with more a sophisticated overfitting prevention technique, such as the tree pruning methodology of C4.5 <ref> (Quinlan, 1992) </ref>, in the near future. Table 1: Partitionings of amino acids according to high-level attributes. Duplicate partitions are given identical numbers. Structural partition Functional partition 1. Ambivalent fA C G P S T W Yg 4. Acidic fD Eg 2. External fD E H K N Q Rg 8.
Reference: <author> Zhang, X., J.P. Mesirov, </author> <title> D.L. Waltz (1992). A Hybrid System for Protein Secondary Structure Prediction. </title> <journal> Journal of Molecular Biology, </journal> <volume> 225, </volume> <pages> 1049-1063. </pages>
References-found: 15

