URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-424.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: (drew, bobick@media.mit.edu)  
Title: Nonlinear Parametric Hidden Markov Models  
Author: Andrew D. Wilson Aaron F. Bobick 
Address: 20 Ames St., Cambridge, MA 02139  
Affiliation: Vision and Modeling Group MIT Media Laboratory  
Abstract: M.I.T. Media Laboratory Perceptual Computing Section Technical Report No. 424 Abstract In previous work [4], we extended the hidden Markov model (HMM) framework to incorporate a global parametric variation in the output probabilities of the states of the HMM. Development of the parametric HMM was motivated by the task of simultaneoiusly recognizing and interpreting gestures that exhibit meaningful variation. With standard HMMs, such global variation confounds the recognition process. In this paper we extend the parametric HMM approach to handle nonlinear (non-analytic) dependencies of the output distributions on the parameter of interest. We show a generalized expectation-maximization (GEM) algorithm for training the parametric HMM and a GEM algorithm to simultaneously recognize the gesture and estimate the value of the parameter. We present results on a pointing gesture, where the nonlinear approach permits the natural azimuth/elevation parame terization of pointing direction.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. M. Bishop. </author> <title> Neural networks for pattern recognition. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1995. </year>
Reference-contexts: Gradient ascent may be used to update the network parameters in each maximization step of the GEM algorithm. When applied to multi-layer neural networks, gradient ascent (or gradient descent when the goal is minimize "error") is often referred to as the backpropagation algorithm <ref> [1] </ref>.
Reference: [2] <author> R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton. </author> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87, </pages> <year> 1991. </year>
Reference-contexts: Intuitively, this weighting steers each network to model the appropriate part of the input, much as the gating function of a mixtures of experts model <ref> [2] </ref> selects its experts. Also, this weighting may be derived from the form of @Q @ (equation 3). In each maximization step of the GEM algorithm, it is not necessary to completely maximize Q.
Reference: [3] <author> L. R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-16, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: Before presenting nonlinear parametric HMMs in full we reiterate the mathematical developement of linear parameteric HMMs. 2 Linear parameteric hidden Markov models 2.1 Model Parametric HMMs model the dependence on the parameter of interest explicitly. We begin with the usual HMM formulation <ref> [3] </ref> and change the form of the output probability distribution (usually a normal distribution or a mixture model) to depend on the parameter , a vector quantity. In the standard continuous HMM model, a sequence is represented by movement through a set of hidden states. <p> get the update equation for Z j : Z j = X fl ktj x kt T # " k;t k (8) Once the means are estimated, the covariance matrices j are updated in the usual way: j = k;t t fl ktj as is the matrix of transition probabilities <ref> [3] </ref>. 2.3 Testing In testing we are given an HMM and an input sequence. We wish to compute the value of and the probability that the HMM produced the sequence.
Reference: [4] <author> A. D. Wilson and A. F. Bobick. </author> <title> Recognition and interpretation of parametric gesture. Proc. Int. Conf. 4 The true positive sequences are labeled by the value of recovered by the EM testing algorithm and the value computed by direct measurement (in parantheses). z), for each of the eight states of the trained parametric HMM. </title> <note> 5 Comp. Vis., 1998. accepted for publication (see MIT Perceptual Computing Group Technical Report 421, http://www-white.media.mit.edu/vismod). 6 </note>
Reference-contexts: 1 Introduction In <ref> [4] </ref> we introduce parametric hidden Markov models (HMMs) as a technique to simultaneously recognize and interpret parametric gesture. By parametric gesture we mean gestures that exhibit a meaningful variation; an example is a point gesture where the important parameter is direction. <p> A point gesture is then paramterized by two values: the Cartesian coordinates that indicate direction. Alternatively, direction can be specified by spherical coordinates. We refer the reader to <ref> [4] </ref> for a detailed motivation of the parameteric HMM approach as it relates to gesture recognition and interpretation. <p> Parametric HMMs extend the standard HMM model to include a global parametric variation in the output of the HMM states. In <ref> [4] </ref> a linear model was used to model the parametric variation at each state of the HMM. Using the linear model, we formulated an expectation-maximization (EM) method for training the parametric HMM. <p> As with the EM training algorithm of the linear para metric case, with the examples we have tried less than ten GEM iterations are required. 4 Discussion In <ref> [4] </ref> we present an example of a pointing gesture parameterized by projection of hand position onto the plane parallel and in front of the user at the moment that the arm is fully extended. The linear parametric HMM approach works well since the projection is a linear operation. <p> but we must be careful that it is possible to learn the mapping from parameters to observation features given a particular observation 3 over the tangent function. feature space. 5 Results To test the performance of the nonlinear parametric HMM, we conducted an experiment similar to the pointing experiment of <ref> [4] </ref> but with a spherical coordinate parameterization rather than the projection onto a plane in front of the user. We used a Polhemus motion capture system to record the position of the user's wrist at a frame rate of 30Hz. <p> This point was transformed to spherical coordinates (azimuth and elevation) via the arctangent function. Note that for pointing gestures that are confined to a small area in front of the user (as in the experiment presented in <ref> [4] </ref>) the linear parameteric HMM approach will work well enough, since for small values the tangent function is approximately linear. <p> Figure 2 shows the log probability as a function of time and the value of recovered for a number of recovered pointing gestures. All of the pointing gestures were recovered. 6 Conclusion The parametric hidden Markov model framework presented in <ref> [4] </ref> has been generalized to handle nonlinear dependencies of the state output distributions on the parameterization . We have shown that where the linear parametric HMM employs the EM algorithm in training and testing, the nonlinear variant similarly uses the GEM algorithm.
References-found: 4

