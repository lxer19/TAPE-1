URL: http://www.cs.princeton.edu/~ristad/papers/icassp97.ps.gz
Refering-URL: http://www.cs.princeton.edu/~ristad/bibliography.html
Root-URL: http://www.cs.princeton.edu
Email: fristad,rgtg@cs.princeton.edu  
Title: NONUNIFORM MARKOV MODELS  
Author: Eric Sven Ristad Robert G. Thomas 
Address: Princeton, NJ 08544-2087  
Affiliation: Department of Computer Science Princeton University  
Abstract: We propose a new way to model conditional independence in Markov models. The central feature of our nonuniform Markov model is that it makes predictions of varying lengths using contexts of varying lengths. Experiments on the Wall Street Journal reveal that the nonuniform model performs slightly better than the classic interpolated Markov model of Jelinek and Mercer (1980). This result is somewhat remarkable because both models contain identical numbers of parameters whose values are estimated in a similar manner. The only difference between the two models is how they combine the statistics of longer and shorter strings. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bahl, L. R., Brown, P. F., de Souza, P. V., Mercer, R. L., and Nahamoo, D. </author> <title> A fast algorithm for deleted interpolation. </title> <booktitle> In Proc. EUROSPEECH '91 (Genoa, </booktitle> <year> 1991), </year> <pages> pp. 1209-1212. </pages>
Reference-contexts: TWO INTERPOLATED MODELS Recall that an interpolated Markov model = hn; A; ffi; i consists of a maximal string length n, a finite alphabet A, a set of string probabilities ffi : A n ! <ref> [0; 1] </ref>, and the state-conditional interpolation parameters : A &lt;n ! [0; 1]. Given a string y l , l n, the string probabilities ffi (y l ) are typically their empirical probabilities in a training corpus. <p> TWO INTERPOLATED MODELS Recall that an interpolated Markov model = hn; A; ffi; i consists of a maximal string length n, a finite alphabet A, a set of string probabilities ffi : A n ! <ref> [0; 1] </ref>, and the state-conditional interpolation parameters : A &lt;n ! [0; 1]. Given a string y l , l n, the string probabilities ffi (y l ) are typically their empirical probabilities in a training corpus. We now consider two generative interpretations of the interpolated Markov model: the classic context model and our nonuniform model. <p> Until t = T 3. Pick context length i in [0; min (t; n 1)] p c (ijx t ) = (x t Q i+1 tl+1 )) ti+1 ; j max := max (n i; T t); 5. Pick prediction y j 1 of length j in <ref> [1; j max ] </ref> p v (y 1 jc) = (1 (cy 1 ))ffi (y j jcy 1 ; i + j) l=1 (cy l 1 ; l + i) where (cy j max : 6. <p> No parameter tying or parameter selection was performed. We report performance as test message perplexity. We set the ffi parameters to be the empirical probabilities in the training data and then optimized the parameters on the training data using deleted estimation <ref> [6, 1] </ref>. We report the best numbers for each model, as though an oracle told us when to stop running deleted estimation. We considered three initial estimates for the parameters: the uniform estimate 0.5, the Jeffreys-Perks rule of succession [5, 9, 7], and the natural law of succession [10].
Reference: [2] <author> Baum, L., and Eagon, J. </author> <title> An inequality with applications to statistical estimation for probabilistic functions of a Markov process and to models for ecology. Bull. </title> <booktitle> AMS 73 (1967), </booktitle> <pages> 360-363. </pages>
Reference-contexts: DELETED ESTIMATION In this section, we formulate an expectation maximization (EM) algorithm for the nonuniform Markov model. Our development follows the traditional lines established for the hidden Markov model <ref> [2, 3] </ref>. We begin by defining our for ward and backward variables.
Reference: [3] <author> Baum, L., Petrie, T., Soules, G., and Weiss, N. </author> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains. </title> <journal> Ann. Math. Stat. </journal> <volume> 41 (1970), </volume> <pages> 164-171. </pages>
Reference-contexts: DELETED ESTIMATION In this section, we formulate an expectation maximization (EM) algorithm for the nonuniform Markov model. Our development follows the traditional lines established for the hidden Markov model <ref> [2, 3] </ref>. We begin by defining our for ward and backward variables.
Reference: [4] <author> Jeanrenaud, P., Eide, E., Chaudhari, U., McDonough, J., Ng, K., Siu, M., and Gish, H. </author> <title> Reducing word error rate on conversational speech from the Switchboard corpus. </title> <booktitle> In ICASSP 95 (1995), </booktitle> <pages> pp. 53-56. </pages>
Reference-contexts: Nonuniform predictions is a principled way to perform alphabet extension, that is, to make a string become a symbol in the alphabet, an ad hoc technique that can improve model performance <ref> [4] </ref>. The remainder of this article consists of four sections. Section 2. considers two generative interpretations of the interpolated Markov model: the context model and our nonuniform model.
Reference: [5] <author> Jeffreys, H. </author> <title> An invariant form for the prior probability in estimation problems. </title> <journal> Proc. Roy. Soc. (London) A 186 (1946), </journal> <pages> 453-461. </pages>
Reference-contexts: We report the best numbers for each model, as though an oracle told us when to stop running deleted estimation. We considered three initial estimates for the parameters: the uniform estimate 0.5, the Jeffreys-Perks rule of succession <ref> [5, 9, 7] </ref>, and the natural law of succession [10]. Jeffreys-Perks assigns relatively low probability to (x i ), while the natural law assigns relatively high probability to (x i ).
Reference: [6] <author> Jelinek, F., and Mercer, R. L. </author> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Pattern Recognition in Practice (Amsterdam, </booktitle> <month> May 21-23 </month> <year> 1980), </year> <editor> E. S. Gelsema and L. N. Kanal, Eds., </editor> <publisher> North Holland, </publisher> <pages> pp. 381-397. </pages>
Reference-contexts: The first contribution is our interpretation of the interpolation parameters as beliefs about conditional independence. Prior work on interpolated Markov models has interpreted the interpolation parameters as smoothing the "specific probabilities" with the "general probabilities" <ref> [6, 8] </ref>. Our interpretation gives rise to the second contribution of our work, namely, a class of nonuniform Markov models that make predictions of varying lengths using contexts of varying lengths. <p> Context Model In the interpolated context model, the interpolation parameters are understood as smoothing the conditional probabilities estimated from longer histories with those estimated from shorter histories <ref> [6, 8] </ref>. Longer histories support stronger predictions, while shorter histories have more accurate statistics. Interpolating the predictions from histories of different lengths results in more accurate predictions than can be obtained from any fixed history length. This interpretation of the interpolation parameters was originally proposed by Jelinek and Mercer [6]. <p> Longer histories support stronger predictions, while shorter histories have more accurate statistics. Interpolating the predictions from histories of different lengths results in more accurate predictions than can be obtained from any fixed history length. This interpretation of the interpolation parameters was originally proposed by Jelinek and Mercer <ref> [6] </ref>. It leads to the following generation algorithm, where the hidden transition from a longer context to a shorter context (line 3) is temporary, used only for the current prediction (line 4). To appear in Proc. ICASSP-97, Apr 21-24, Munich, Germany1 c fl IEEE 1997 context-generate (T ,) 1. <p> Initialize ffi using B; 5. EXPERIMENTAL RESULTS In this section we compare the performance of the interpolated context model and the nonuniform model on the Wall Street Journal. (Recall that the interpolated context model is the classic interpolated Markov model of Jelinek and Mercer <ref> [6] </ref>.) We performed two sets of experiments. The first set of experiments was with the 6.2 million word WSJ 1989 corpus. The goal of these initial experiments was to better understand how initial parameter values affect model performance. <p> No parameter tying or parameter selection was performed. We report performance as test message perplexity. We set the ffi parameters to be the empirical probabilities in the training data and then optimized the parameters on the training data using deleted estimation <ref> [6, 1] </ref>. We report the best numbers for each model, as though an oracle told us when to stop running deleted estimation. We considered three initial estimates for the parameters: the uniform estimate 0.5, the Jeffreys-Perks rule of succession [5, 9, 7], and the natural law of succession [10].
Reference: [7] <author> Krichevskii, R. E., and Trofimov, V. K. </author> <title> The performance of universal coding. </title> <journal> IEEE Trans. Information Theory IT-27, </journal> <volume> 2 (1981), </volume> <pages> 199-207. </pages>
Reference-contexts: We report the best numbers for each model, as though an oracle told us when to stop running deleted estimation. We considered three initial estimates for the parameters: the uniform estimate 0.5, the Jeffreys-Perks rule of succession <ref> [5, 9, 7] </ref>, and the natural law of succession [10]. Jeffreys-Perks assigns relatively low probability to (x i ), while the natural law assigns relatively high probability to (x i ).
Reference: [8] <author> MacKay, D. J., and Peto, L. C. B. </author> <title> A hierarchical Dirich-let language model. </title> <booktitle> Natural Language Engineering 1, </booktitle> <month> 1 </month> <year> (1994). </year>
Reference-contexts: The first contribution is our interpretation of the interpolation parameters as beliefs about conditional independence. Prior work on interpolated Markov models has interpreted the interpolation parameters as smoothing the "specific probabilities" with the "general probabilities" <ref> [6, 8] </ref>. Our interpretation gives rise to the second contribution of our work, namely, a class of nonuniform Markov models that make predictions of varying lengths using contexts of varying lengths. <p> Context Model In the interpolated context model, the interpolation parameters are understood as smoothing the conditional probabilities estimated from longer histories with those estimated from shorter histories <ref> [6, 8] </ref>. Longer histories support stronger predictions, while shorter histories have more accurate statistics. Interpolating the predictions from histories of different lengths results in more accurate predictions than can be obtained from any fixed history length. This interpretation of the interpolation parameters was originally proposed by Jelinek and Mercer [6].
Reference: [9] <author> Perks, W. </author> <title> Some observations on inverse probability, including a new indifference rule. </title> <journal> J. Inst. Actuar. </journal> <volume> 73 (1947), </volume> <pages> 285-312. </pages>
Reference-contexts: We report the best numbers for each model, as though an oracle told us when to stop running deleted estimation. We considered three initial estimates for the parameters: the uniform estimate 0.5, the Jeffreys-Perks rule of succession <ref> [5, 9, 7] </ref>, and the natural law of succession [10]. Jeffreys-Perks assigns relatively low probability to (x i ), while the natural law assigns relatively high probability to (x i ).
Reference: [10] <author> Ristad, E. S. </author> <title> A natural law of succession. </title> <type> Tech. Rep. 495-95, </type> <institution> Department of Computer Science, Princeton University, Princeton, NJ, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: We report the best numbers for each model, as though an oracle told us when to stop running deleted estimation. We considered three initial estimates for the parameters: the uniform estimate 0.5, the Jeffreys-Perks rule of succession [5, 9, 7], and the natural law of succession <ref> [10] </ref>. Jeffreys-Perks assigns relatively low probability to (x i ), while the natural law assigns relatively high probability to (x i ). The best performance for higher model orders was achieved with uniform initialization in all of our experiments, both before and after optimization via deleted estimation.
Reference: [11] <author> Ristad, E. S., and Thomas, R. G. </author> <title> Nonuniform Markov models. </title> <type> Tech. Rep. 536-96, </type> <institution> Department of Computer Science, Princeton University, Princeton, NJ, </institution> <month> November </month> <year> 1996. </year> <month> 4 </month>
Reference-contexts: The resource requirements of the algorithm may be reduced to O (nT ) time and O (n) space at a significant expense in clarity <ref> [11] </ref>. Note that (x t+j max nonuniform-evaluate (x T ,) 1. <p> The following algorithm accumulates all + (y l ) and (y l ) values in O (n 3 T ) time and O (n 2 T ) space. The full paper <ref> [11] </ref> presents an alternate ex pectation step algorithm that accumulates all expectations in O (nT ) time and space. expectation-step (x T ,, + , ) 1. ff = forward (x T ,); 2. fi = backward (x T ,); 3. For t = 1 to T 1 5.
References-found: 11

