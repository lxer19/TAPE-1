URL: http://www.is.cs.cmu.edu/papers/multimodal/AVSP97/AVSP97-uwe.ps.gz
Refering-URL: http://www.is.cs.cmu.edu/ISL.multimodal.publications.html
Root-URL: 
Email: uwe@ira.uka.de, stiefel@ira.uka.de, yang+@cs.cmu.edu  
Title: PREPROCESSING OF VISUAL SPEECH UNDER REAL WORLD CONDITIONS  
Author: Uwe Meier, Rainer Stiefelhagen, Jie Yang 
Address: Pittsburgh, USA  
Affiliation: Interactive Systems Laboratories University of Karlsruhe, Karlsruhe, Germany Carnegie Mellon University,  
Abstract: In this paper we present recent work on integration of visual information (automatic lip-reading) with acoustic speech for better overall speech recognition. We have developed a modular system for flexible human-computer interaction via speech. In order to give the speaker reasonable freedom of movement within a room, the speaker's face is automatically acquired and followed by a face tracker subsystem, which delivers constant size, centered images of the face in real time. The image of the lips is automatically extracted from the camera image of the speaker's face by the lip tracker module, which can track the lips in real time. Furthermore, we show how the system deals with problems in real environments such as different illuminations and image sizes, and how the system adapts automatically to different noise conditions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.J. Goldschen, O.N. Garcia, and E. Petajan. </author> <title> Continu ous optical automatic speech recognition by lipreading. </title> <booktitle> 28th Annual Asimolar conference on Signal speech and Computers. </booktitle>
Reference-contexts: We are interested in emulating some of these capabilities by combining speech recognition with lipreading to improve robustness and flexebiltie by offering complementary information. Publications of other researchers in this area can be found in <ref> [1, 2, 3, 4, 5, 6, 7] </ref> 2. SYSTEM DESCRIPTION Our audio-visual speech recognizer [8, 9, 10, 11] has been developed for a German spelling task mainly in speaker-dependent mode, first multi speaker / speaker independent tests show promising results.
Reference: [2] <author> P.L. Silsbee. </author> <title> Sensory integration in audiovisual auto matic speech recognition. </title> <booktitle> 28th Annual Asimolar conference on Signal speech and Computers, </booktitle> <year> 1994. </year>
Reference-contexts: We are interested in emulating some of these capabilities by combining speech recognition with lipreading to improve robustness and flexebiltie by offering complementary information. Publications of other researchers in this area can be found in <ref> [1, 2, 3, 4, 5, 6, 7] </ref> 2. SYSTEM DESCRIPTION Our audio-visual speech recognizer [8, 9, 10, 11] has been developed for a German spelling task mainly in speaker-dependent mode, first multi speaker / speaker independent tests show promising results.
Reference: [3] <author> M.M. Cohen and D.W. Massaro. </author> <title> What can vi sual speech synthesis tell visual speech recognition. </title> <booktitle> 28th Annual Asimolar conference on Signal speech and Computers. </booktitle>
Reference-contexts: We are interested in emulating some of these capabilities by combining speech recognition with lipreading to improve robustness and flexebiltie by offering complementary information. Publications of other researchers in this area can be found in <ref> [1, 2, 3, 4, 5, 6, 7] </ref> 2. SYSTEM DESCRIPTION Our audio-visual speech recognizer [8, 9, 10, 11] has been developed for a German spelling task mainly in speaker-dependent mode, first multi speaker / speaker independent tests show promising results.
Reference: [4] <author> J. R. Movellan. </author> <title> Visual speech recognition with stochas tic networks. </title> <booktitle> NIPS 94, </booktitle> <year> 1994. </year>
Reference-contexts: We are interested in emulating some of these capabilities by combining speech recognition with lipreading to improve robustness and flexebiltie by offering complementary information. Publications of other researchers in this area can be found in <ref> [1, 2, 3, 4, 5, 6, 7] </ref> 2. SYSTEM DESCRIPTION Our audio-visual speech recognizer [8, 9, 10, 11] has been developed for a German spelling task mainly in speaker-dependent mode, first multi speaker / speaker independent tests show promising results.
Reference: [5] <author> K. Mase and A. Pentland. </author> <title> Automantic lipreading by optical-flow analysis. </title> <journal> Systems and Computers in Japan, </journal> <volume> 22(6) </volume> <pages> 67-76, </pages> <year> 1991. </year>
Reference-contexts: We are interested in emulating some of these capabilities by combining speech recognition with lipreading to improve robustness and flexebiltie by offering complementary information. Publications of other researchers in this area can be found in <ref> [1, 2, 3, 4, 5, 6, 7] </ref> 2. SYSTEM DESCRIPTION Our audio-visual speech recognizer [8, 9, 10, 11] has been developed for a German spelling task mainly in speaker-dependent mode, first multi speaker / speaker independent tests show promising results.
Reference: [6] <author> B.P. Yuhas, M.H. Goldstein, and T.J. Sejnowski. </author> <title> Inte gration of acoustic and visual speech signals using neural networks. </title> <journal> IEEE Communications Magazine, </journal> <pages> pages 65-71, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: We are interested in emulating some of these capabilities by combining speech recognition with lipreading to improve robustness and flexebiltie by offering complementary information. Publications of other researchers in this area can be found in <ref> [1, 2, 3, 4, 5, 6, 7] </ref> 2. SYSTEM DESCRIPTION Our audio-visual speech recognizer [8, 9, 10, 11] has been developed for a German spelling task mainly in speaker-dependent mode, first multi speaker / speaker independent tests show promising results.
Reference: [7] <author> D.G. Stork, G. Wolff, and E. Levine. </author> <title> Neural net work lipreading system for improved speech recognition. </title> <booktitle> IJCNN, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: We are interested in emulating some of these capabilities by combining speech recognition with lipreading to improve robustness and flexebiltie by offering complementary information. Publications of other researchers in this area can be found in <ref> [1, 2, 3, 4, 5, 6, 7] </ref> 2. SYSTEM DESCRIPTION Our audio-visual speech recognizer [8, 9, 10, 11] has been developed for a German spelling task mainly in speaker-dependent mode, first multi speaker / speaker independent tests show promising results.
Reference: [8] <author> P. Duchnowski, U. Meier, and A. Waibel. </author> <title> See me, hear me: Integrating automatic speech recognition and lipreading. </title> <booktitle> International Conference on Spoken Language Processing, ICSLP, </booktitle> <pages> pages 547-550, </pages> <year> 1994. </year>
Reference-contexts: We are interested in emulating some of these capabilities by combining speech recognition with lipreading to improve robustness and flexebiltie by offering complementary information. Publications of other researchers in this area can be found in [1, 2, 3, 4, 5, 6, 7] 2. SYSTEM DESCRIPTION Our audio-visual speech recognizer <ref> [8, 9, 10, 11] </ref> has been developed for a German spelling task mainly in speaker-dependent mode, first multi speaker / speaker independent tests show promising results. Letter sequences of arbitrary length and content are spelled without pauses.
Reference: [9] <author> P. Duchnowski, M. Hunke, D. Busching, U. Meier, and A. Waibel. </author> <title> Toward movement-invariant automatic lipreading and speech recognition. </title> <booktitle> Proc. ICASSP, </booktitle> <year> 1995. </year>
Reference-contexts: We are interested in emulating some of these capabilities by combining speech recognition with lipreading to improve robustness and flexebiltie by offering complementary information. Publications of other researchers in this area can be found in [1, 2, 3, 4, 5, 6, 7] 2. SYSTEM DESCRIPTION Our audio-visual speech recognizer <ref> [8, 9, 10, 11] </ref> has been developed for a German spelling task mainly in speaker-dependent mode, first multi speaker / speaker independent tests show promising results. Letter sequences of arbitrary length and content are spelled without pauses. <p> The color-distribution is initialized so as to find a variety of skin-colors and is gradually adapted to the actual found face. input image (color!) face-colored regions image. The face is marked in the input image 2.3. Lip Tracking Module In our lipreading system as described in <ref> [9] </ref>, we used a neural net based lip-localization module to find the lip-corners. One disadvantage of this module was, that it did not perform in real time the facial region had to be stored and lip-localization and extraction had to be done off-line which slowed down the system significantly.
Reference: [10] <author> U. Meier, W. Hurst, and P. Duchnowski. </author> <title> Adaptive bimodal sensor fusion forautomatic speechreading. </title>
Reference-contexts: We are interested in emulating some of these capabilities by combining speech recognition with lipreading to improve robustness and flexebiltie by offering complementary information. Publications of other researchers in this area can be found in [1, 2, 3, 4, 5, 6, 7] 2. SYSTEM DESCRIPTION Our audio-visual speech recognizer <ref> [8, 9, 10, 11] </ref> has been developed for a German spelling task mainly in speaker-dependent mode, first multi speaker / speaker independent tests show promising results. Letter sequences of arbitrary length and content are spelled without pauses.
Reference: [11] <author> M.T. Vo, R. Houghton, J. Yang, U. Bub, U. Meier, A. Waibel, and P. Duchnowski. </author> <title> Multimodal learning interfaces. </title> <booktitle> ARPA Spoken Language Technology Workshop, </booktitle> <year> 1995. </year>
Reference-contexts: We are interested in emulating some of these capabilities by combining speech recognition with lipreading to improve robustness and flexebiltie by offering complementary information. Publications of other researchers in this area can be found in [1, 2, 3, 4, 5, 6, 7] 2. SYSTEM DESCRIPTION Our audio-visual speech recognizer <ref> [8, 9, 10, 11] </ref> has been developed for a German spelling task mainly in speaker-dependent mode, first multi speaker / speaker independent tests show promising results. Letter sequences of arbitrary length and content are spelled without pauses.
Reference: [12] <author> R. Stiefelhagen, Jie Yang, and Uwe Meier. </author> <title> Real time lip tracking for lipreading. </title> <booktitle> Eurospeech 97. </booktitle>
Reference-contexts: The image of the lips is automatically extracted from the camera picture of the speaker's face by the lip localisa-tion module <ref> [12, 13] </ref>. The visual data is preprocessed to eliminate problems that raise in real world applications like different illuminations, different face/lip size and different positions in the image. <p> The new lip-localization module is described in detail in <ref> [12] </ref>. 2.4. Preprocessing Size and Translation Invariance From the Lip Finder modul we get the coordinates of the mouth corners. Using these corners we can cut the lips out of the face image and rescale to a constant size.
Reference: [13] <author> R. Stiefelhagen and Jie Yang. </author> <title> Gaze tracking for multi modal human-computer interaction. </title> <booktitle> ICASSP 97. </booktitle>
Reference-contexts: The image of the lips is automatically extracted from the camera picture of the speaker's face by the lip localisa-tion module <ref> [12, 13] </ref>. The visual data is preprocessed to eliminate problems that raise in real world applications like different illuminations, different face/lip size and different positions in the image.
Reference: [14] <author> U. </author> <type> Meier. </type> <institution> Robuste Systemarchitekturen fur au tomatisches Lippenlesen. Diplom-Arbeit, Institut fur Logik, Komplexitat und Deduktionssysteme, Univer-sitat Karlsruhe (TH), Germany, </institution> <year> 1995. </year>
Reference-contexts: The visual data is preprocessed to eliminate problems that raise in real world applications like different illuminations, different face/lip size and different positions in the image. It was shown that the recognitions results decrease if those conditions change within a small range <ref> [14] </ref>: Experiments with an 100% word accuracy test set have shown that if you shift the image 3 pixels, the word accuracy decreases down to 68%. Changing the illumination by adding a offset of 15 to the grayvalue, the word accuracy is only 93%.
Reference: [15] <author> J. Yang and A. Waibel. </author> <title> a real-time face tracker. </title> <address> WACV 96. </address>
Reference-contexts: The color images are used for the Face Tracker and Lip Finder, for the Lip Reading modul gray-level images are used. 2.2. Face Tracking Module To find and track the face, a statistical skin color-model consisting of a two-dimensional Gaussian distribution of normalized skin colors is used <ref> [15] </ref>. The input image is searched for pixels with skin colors and the largest connected region of skin-colored pixels in the camera-image is considered as the region of the face.
Reference: [16] <author> W.K. Pratt. </author> <title> Digital Image Processing. </title> <publisher> A Wiley Interscience Publication. </publisher>
Reference-contexts: If you adjust the grayvalues of your given images in that way, that they have the grayvalue distribution of the target images you can eliminate most illuminations differences. modification. We used for the adjustement a method (grayvalue modification) that is described in <ref> [16] </ref> in detail: The grayvalue distribution is computed, using the accumulated grayvalues, it is easy to adjust the grayvalues in a way, that the accumulated function is the same as from the target function: f (p) = T (f (p)) where f (p) is the original grayvalue, T the modification function
References-found: 16

