URL: http://www.cs.helsinki.fi/~tirri/ecml98lazy.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~tirri/publications.html
Root-URL: 
Phone: 26,  
Title: Bayes Optimal Instance-Based Learning  
Author: Petri Kontkanen, Petri Myllymaki, Tomi Silander, and Henry Tirri 
Address: P.O.Box  FIN-00014 University of Helsinki, Finland  
Affiliation: Complex Systems Computation Group (CoSCo)  Department of Computer Science  
Date: April 24-28,1998.  
Note: To appear in the 10th European Conference on Machine Learning (ECML-98), Chemnitz, Germany,  
Abstract: In this paper we present a probabilistic formalization of the instance-based learning approach. In our Bayesian framework, moving from the construction of an explicit hypothesis to a data-driven instance-based learning approach, is equivalent to averaging over all the (possibly infinitely many) individual models. The general Bayesian instance-based learning framework described in this paper can be applied with any set of assumptions defining a parametric model family, and to any discrete prediction task where the number of simultaneously predicted attributes is small, which includes for example all classification tasks prevalent in the machine learning literature. To illustrate the use of the suggested general framework in practice, we show how the approach can be implemented in the special case with the strong independence assumptions underlying the so called Naive Bayes classifier. The resulting Bayesian instance-based classifier is validated empirically with public domain data sets and the results are compared to the performance of the traditional Naive Bayes classifier. The results suggest that the Bayesian instance-based learning approach yields better results than the traditional Naive Bayes classifier, especially in cases where the amount of the training data is small.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Aha. </author> <title> A Study of Instance-Based Algorithms for Supervised Learning Tasks: Mathematical, Empirical, an Psychological Observations. </title> <type> PhD thesis, </type> <institution> University of California, Irvine, </institution> <year> 1990. </year>
Reference-contexts: In contrast to the traditional (eager) approach described above, in the instance-based (also known as the memory-based or the case-based) approach <ref> [29, 24, 1, 4] </ref>, the learning algorithms base their predictions directly on the sample data, without producing any specific models of the problem domain. This type of machine learning is often referred to as lazy learning, since the algorithms defer all the essential computation until the prediction phase [2].
Reference: 2. <author> D. Aha, </author> <title> editor. Lazy Learning. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Dordrecht, </address> <year> 1997. </year> <journal> Reprinted from Artificial Intelligence Review, </journal> <volume> 11 </volume> <pages> 1-5. </pages>
Reference-contexts: This type of machine learning is often referred to as lazy learning, since the algorithms defer all the essential computation until the prediction phase <ref> [2] </ref>. For making predictions, instance-based learning algorithms typically use a distance function (e.g., Euclidean distance) for determining the most relevant data items for the prediction task in question. <p> In [32, 31, 26, 25] we proposed a Bayesian framework for instance-based learning based on probability theory and the finite mixture model family [9, 33]. The approach suggested in those studies can be seen as a "partially lazy" approach <ref> [2] </ref>, i.e., a hybrid between the traditional machine learning and the instance-based learning approach, which is based solely on the given data.
Reference: 3. <author> K. Ali and M. Pazzani. </author> <title> Error reduction through learning multiple descriptions. </title> <journal> Machine Learning, </journal> <volume> 24(3) </volume> <pages> 173-202, </pages> <month> September </month> <year> 1997. </year>
Reference-contexts: Note that formula (4) offers a formal motivation for the idea of model averaging (see, e.g., <ref> [22, 3] </ref> and the references therein), i.e., for combining multiple predictors for increasing the prediction accuracy: the individual predictions P (d j D; M k ; M) produced by different predictors M k (for example, model classes determined by different decision tree structures), are combined by summing the individual predictions weighted
Reference: 4. <author> C. Atkeson. </author> <title> Memory based approaches to approximating continuous functions. </title> <editor> In M. Casdagli and S. Eubank, editors, </editor> <booktitle> Nonlinear Modeling and Forecasting. Proceedings Volume XII in the Santa Fe Institute Studies in the Sciences of Complexity. </booktitle> <publisher> Addison Wesley, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: In contrast to the traditional (eager) approach described above, in the instance-based (also known as the memory-based or the case-based) approach <ref> [29, 24, 1, 4] </ref>, the learning algorithms base their predictions directly on the sample data, without producing any specific models of the problem domain. This type of machine learning is often referred to as lazy learning, since the algorithms defer all the essential computation until the prediction phase [2].
Reference: 5. <author> C. Atkeson, A. Moore, and S. Schaal. </author> <title> Locally weighted learning. </title> <booktitle> In Aha [2], </booktitle> <pages> pages 11-73. </pages>
Reference-contexts: The method suffers, however, from several drawbacks when applied in practice (see, e.g., the discussion in [32]). Most importantly, the performance of instance-based learning algorithms seems to be highly sensitive to the selection of distance function to be used as demonstrated in recent work reported in <ref> [34, 13, 5] </ref>. In [32, 31, 26, 25] we proposed a Bayesian framework for instance-based learning based on probability theory and the finite mixture model family [9, 33].
Reference: 6. <author> J.O. Berger. </author> <title> Statistical Decision Theory and Bayesian Analysis. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: Nevertheless, it turns out that for some simple model families the integral over all the model instantiations, i.e., the so called evidence or the marginal likelihood <ref> [6] </ref>, can in fact be solved analytically, and calculated with modest computational effort. An example of such a model family is the family of Bayesian networks (see e.g. [7, 15]), where the model family is determined by defining a set of independence relations between the problem domain variables.
Reference: 7. <author> G. Cooper and E. Herskovits. </author> <title> A Bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: An example of such a model family is the family of Bayesian networks (see e.g. <ref> [7, 15] </ref>), where the model family is determined by defining a set of independence relations between the problem domain variables. For more complex model families including those with latent variables, there exist several computationally feasible methods for approximating the evidence integral | see e.g., the discussion in [19]. <p> Producing this predictive distribution corresponds to the case where instead of using a single set of parameters for the Naive Bayes classifier, as in the MAP predictive distribution (7), we sum over all the (infinitely many) parameter alternatives for the Naive Bayes classifier. As shown in <ref> [7, 15, 19] </ref>, with the assumptions listed above the marginal likelihood (1) of the data can be computed by P (D j M) = P K P K k=1 ( k ) K Y m1 Y P n i (h k + l=1 kil ) l=1 ( kil ) ; (8)
Reference: 8. <author> M.H. </author> <title> DeGroot. Optimal statistical decisions. </title> <publisher> McGraw-Hill, </publisher> <year> 1970. </year>
Reference-contexts: Since the family of Dirichlet densities is conjugate (see e.g., <ref> [8] </ref>) to the family of multinomials, i.e., the functional form of parameter distribution is invariant in the prior-to-posterior transformation, we assume that the prior distributions of the parameters are from this family.
Reference: 9. <author> B.S. </author> <title> Everitt and D.J. Hand. Finite Mixture Distributions. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: In [32, 31, 26, 25] we proposed a Bayesian framework for instance-based learning based on probability theory and the finite mixture model family <ref> [9, 33] </ref>. The approach suggested in those studies can be seen as a "partially lazy" approach [2], i.e., a hybrid between the traditional machine learning and the instance-based learning approach, which is based solely on the given data.
Reference: 10. <editor> U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors. </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: In the first research area, the goal is to find useful high-level knowledge representations from the data through exploratory data analysis (this descriptive aspect is very related to the research performed in the field of data mining <ref> [10] </ref>). In the second research area, the goal is to predict the outcome of some future event by using the data given. In this paper, we are motivated purely by this latter, predictive aspect of machine learning.
Reference: 11. <author> D. Fisher. </author> <title> Noise-tolerant conceptual clustering. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 825-830, </pages> <address> Detroit, Michigan, </address> <year> 1989. </year>
Reference-contexts: Thus the predictive distributions required for making predictions could then be computed by using the instance-based learning approach in the distribution space, i.e., by introducing a probabilistic "distance metric". Somewhat similar frameworks have been suggested in <ref> [16, 30, 11, 12] </ref>. The goal of this paper is to present a novel alternative probabilistic formalization of the purely lazy learning approach.
Reference: 12. <author> D. Fisher and D. Talbert. </author> <title> Inference using probabilistic concept trees. </title> <booktitle> In Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 191-202, </pages> <address> Ft. Lauderdale, Florida, </address> <month> January </month> <year> 1997. </year>
Reference-contexts: Thus the predictive distributions required for making predictions could then be computed by using the instance-based learning approach in the distribution space, i.e., by introducing a probabilistic "distance metric". Somewhat similar frameworks have been suggested in <ref> [16, 30, 11, 12] </ref>. The goal of this paper is to present a novel alternative probabilistic formalization of the purely lazy learning approach.
Reference: 13. <author> J.H. Friedman. </author> <title> Flexible metric nearest neighbor classification. </title> <type> Unpublished manuscript. </type> <institution> Available by anonymous ftp from Stanford Research Institute (Menlo Park, CA) at playfair.stanford.edu., </institution> <year> 1994. </year>
Reference-contexts: The method suffers, however, from several drawbacks when applied in practice (see, e.g., the discussion in [32]). Most importantly, the performance of instance-based learning algorithms seems to be highly sensitive to the selection of distance function to be used as demonstrated in recent work reported in <ref> [34, 13, 5] </ref>. In [32, 31, 26, 25] we proposed a Bayesian framework for instance-based learning based on probability theory and the finite mixture model family [9, 33].
Reference: 14. <author> A. Gelman, J. Carlin, H. Stern, and D. Rubin. </author> <title> Bayesian Data Analysis. </title> <publisher> Chapman & Hall, </publisher> <year> 1995. </year>
Reference-contexts: Bayesian probability theory provides a unifying theoretically solid framework for choosing a proper model family, model class, and parameter instantiation during all the three phases of the machine learning process, as discussed for example in <ref> [14, 20] </ref>. In contrast to the traditional (eager) approach described above, in the instance-based (also known as the memory-based or the case-based) approach [29, 24, 1, 4], the learning algorithms base their predictions directly on the sample data, without producing any specific models of the problem domain.
Reference: 15. <author> D. Heckerman, D. Geiger, </author> <title> and D.M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20(3) </volume> <pages> 197-243, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: An example of such a model family is the family of Bayesian networks (see e.g. <ref> [7, 15] </ref>), where the model family is determined by defining a set of independence relations between the problem domain variables. For more complex model families including those with latent variables, there exist several computationally feasible methods for approximating the evidence integral | see e.g., the discussion in [19]. <p> Producing this predictive distribution corresponds to the case where instead of using a single set of parameters for the Naive Bayes classifier, as in the MAP predictive distribution (7), we sum over all the (infinitely many) parameter alternatives for the Naive Bayes classifier. As shown in <ref> [7, 15, 19] </ref>, with the assumptions listed above the marginal likelihood (1) of the data can be computed by P (D j M) = P K P K k=1 ( k ) K Y m1 Y P n i (h k + l=1 kil ) l=1 ( kil ) ; (8)
Reference: 16. <author> S. Kasif, S. Salzberg, D. Waltz, J. Rachlin, and D. Aha. </author> <title> Towards a better understanding of memory-based reasoning systems. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference, </booktitle> <pages> pages 242-250, </pages> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufmann Publishers. </publisher>
Reference-contexts: Thus the predictive distributions required for making predictions could then be computed by using the instance-based learning approach in the distribution space, i.e., by introducing a probabilistic "distance metric". Somewhat similar frameworks have been suggested in <ref> [16, 30, 11, 12] </ref>. The goal of this paper is to present a novel alternative probabilistic formalization of the purely lazy learning approach.
Reference: 17. <author> P. Kontkanen, P. Myllymaki, T. Silander, H. Tirri, and P. Grunwald. </author> <title> Comparing predictive inference methods for discrete domains. </title> <booktitle> In Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 311-318, </pages> <address> Ft. Lauderdale, Florida, </address> <month> January </month> <year> 1997. </year> <note> Also: NeuroCOLT Technical Report NC-TR-97-004. </note>
Reference-contexts: As a matter of fact, from the probability theory point of view the Bayesian instance-based learning predictive distribution (4) produces optimally accurate predictions within the chosen model family. In <ref> [17, 18] </ref>, we described how the recently published new coding scheme by Rissanen [28] for representing the stochastic complexity measure [27] offers an alternative definition for an optimal predictive distribution. <p> For small samples it is well known that the traditional MLNB classifier is too dependent on the observed data and does not take into account that future data may turn out to be different. A more detailed discussion on this topic can be found in <ref> [17, 18] </ref>. 5 Conclusion In this paper we proposed a Bayesian framework for defining the instance-based learning approach.
Reference: 18. <author> P. Kontkanen, P. Myllymaki, T. Silander, H. Tirri, and P. Grunwald. </author> <title> On predictive distributions and Bayesian networks. </title> <editor> In W. Daelemans, P. Flach, and A. van den Bosch, editors, </editor> <booktitle> Proceedings of the Seventh Belgian-Dutch Conference on Machine Learning (BeNeLearn'97), </booktitle> <pages> pages 59-68, </pages> <address> Tilburg, the Netherlands, </address> <month> October </month> <year> 1997. </year>
Reference-contexts: As a matter of fact, from the probability theory point of view the Bayesian instance-based learning predictive distribution (4) produces optimally accurate predictions within the chosen model family. In <ref> [17, 18] </ref>, we described how the recently published new coding scheme by Rissanen [28] for representing the stochastic complexity measure [27] offers an alternative definition for an optimal predictive distribution. <p> For small samples it is well known that the traditional MLNB classifier is too dependent on the observed data and does not take into account that future data may turn out to be different. A more detailed discussion on this topic can be found in <ref> [17, 18] </ref>. 5 Conclusion In this paper we proposed a Bayesian framework for defining the instance-based learning approach.
Reference: 19. <author> P. Kontkanen, P. Myllymaki, and H. Tirri. </author> <title> Comparing Bayesian model class selec-tion criteria by discrete finite mixtures. </title> <editor> In D. Dowe, K. Korb, and J. Oliver, editors, </editor> <booktitle> Information, Statistics and Induction in Science, </booktitle> <pages> pages 364-374, </pages> <booktitle> Proceedings of the ISIS'96 Conference, </booktitle> <address> Melbourne, Australia, </address> <month> August </month> <year> 1996. </year> <title> World Scientific, </title> <publisher> Singapore. </publisher>
Reference-contexts: For more complex model families including those with latent variables, there exist several computationally feasible methods for approximating the evidence integral | see e.g., the discussion in <ref> [19] </ref>. It should be emphasized that we make no claims about having invented the idea of making predictions by marginalizing over all the (possibly infinitely many) models, which is a known technique in the Bayesian community. <p> Producing this predictive distribution corresponds to the case where instead of using a single set of parameters for the Naive Bayes classifier, as in the MAP predictive distribution (7), we sum over all the (infinitely many) parameter alternatives for the Naive Bayes classifier. As shown in <ref> [7, 15, 19] </ref>, with the assumptions listed above the marginal likelihood (1) of the data can be computed by P (D j M) = P K P K k=1 ( k ) K Y m1 Y P n i (h k + l=1 kil ) l=1 ( kil ) ; (8)
Reference: 20. <author> P. Kontkanen, P. Myllymaki, and H. Tirri. </author> <title> Experimenting with the Cheeseman-Stutz evidence approximation for predictive modeling and data mining. </title> <editor> In D. Dankel, editor, </editor> <booktitle> Proceedings of the Tenth International FLAIRS Conference, </booktitle> <pages> pages 204-211, </pages> <address> Daytona Beach, Florida, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: Bayesian probability theory provides a unifying theoretically solid framework for choosing a proper model family, model class, and parameter instantiation during all the three phases of the machine learning process, as discussed for example in <ref> [14, 20] </ref>. In contrast to the traditional (eager) approach described above, in the instance-based (also known as the memory-based or the case-based) approach [29, 24, 1, 4], the learning algorithms base their predictions directly on the sample data, without producing any specific models of the problem domain.
Reference: 21. <author> D. Mackay. </author> <title> Bayesian Methods for Adaptive Models. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: In traditional (eager) machine learning, the model family, model class, and the model parameters must all be fixed in order to produce a single model for making predictions. Bayesian probability theory provides a theoretically solid framework for these tasks, as demonstrated, e.g., in <ref> [21] </ref> in the neural network model family case, and in [32] in the finite mixture model family case.
Reference: 22. <author> D. Madigan, A. Raftery, C. Volinsky, and J. Hoeting. </author> <title> Bayesian model averaging. </title> <booktitle> In AAAI Workshop on Integrating Multiple Learned Models, </booktitle> <year> 1996. </year>
Reference-contexts: Note that formula (4) offers a formal motivation for the idea of model averaging (see, e.g., <ref> [22, 3] </ref> and the references therein), i.e., for combining multiple predictors for increasing the prediction accuracy: the individual predictions P (d j D; M k ; M) produced by different predictors M k (for example, model classes determined by different decision tree structures), are combined by summing the individual predictions weighted
Reference: 23. <editor> D. Michie, D.J. Spiegelhalter, and C.C. Taylor, editors. </editor> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <address> London, </address> <year> 1994. </year>
Reference-contexts: Some simple function, such as majority voting in classification problems, is then used for determining the prediction from the most relevant data items. It has been shown in various studies (see e.g., <ref> [23] </ref> for references) that this type of an approach in some cases produces quite accurate predictions, when compared to alternative machine learning methods. The method suffers, however, from several drawbacks when applied in practice (see, e.g., the discussion in [32]). <p> Description of the datasets used, and the results obtained can be found in Table 1. The results are averages over 100 independent crossvalidation runs, and the number of 1 http://www.ics.uci.edu/~mlearn/ folds used was the same as in <ref> [23] </ref>. By the 0/1-score we mean the relative number of the correct classifications made, while the log-score is obtained by computing minus the logarithm of the probability given to the correct class (thus the smaller the score, the better the result). Table 1. <p> that first of all, although the model family used for the experiments was determined by the strong independence assumptions underlying the structurally simple naive Bayes model, the results are quite competitive when compared to the results obtained by using much more elaborate model families (see, e.g., the results collected in <ref> [23] </ref>). Secondly, we can see that when full datasets are used, in the 0/1-score sense the difference between the performance of the BIBL classifier and the MLNB classifier is not very large, whereas in the log-score sense the BIBL approach produces consistently better results.
Reference: 24. <author> A. Moore. </author> <title> Acquisition of dynamic control knowledge for a robotic manipulator. </title> <booktitle> In Seventh International Machine Learning Workshop. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: In contrast to the traditional (eager) approach described above, in the instance-based (also known as the memory-based or the case-based) approach <ref> [29, 24, 1, 4] </ref>, the learning algorithms base their predictions directly on the sample data, without producing any specific models of the problem domain. This type of machine learning is often referred to as lazy learning, since the algorithms defer all the essential computation until the prediction phase [2].
Reference: 25. <author> P. Myllymaki and H. Tirri. </author> <title> Bayesian case-based reasoning with neural networks. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 422-427, </pages> <address> San Francisco, March 1993. </address> <publisher> IEEE, </publisher> <address> Piscataway, NJ. </address>
Reference-contexts: Most importantly, the performance of instance-based learning algorithms seems to be highly sensitive to the selection of distance function to be used as demonstrated in recent work reported in [34, 13, 5]. In <ref> [32, 31, 26, 25] </ref> we proposed a Bayesian framework for instance-based learning based on probability theory and the finite mixture model family [9, 33].
Reference: 26. <author> P. Myllymaki and H. Tirri. </author> <title> Massively parallel case-based reasoning with probabilistic similarity metrics. </title> <editor> In S. Wess, K.-D. Althoff, and M Richter, editors, </editor> <booktitle> Topics in Case-Based Reasoning, volume 837 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 144-154. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Most importantly, the performance of instance-based learning algorithms seems to be highly sensitive to the selection of distance function to be used as demonstrated in recent work reported in [34, 13, 5]. In <ref> [32, 31, 26, 25] </ref> we proposed a Bayesian framework for instance-based learning based on probability theory and the finite mixture model family [9, 33].
Reference: 27. <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publishing Company, </publisher> <address> New Jersey, </address> <year> 1989. </year>
Reference-contexts: As a matter of fact, from the probability theory point of view the Bayesian instance-based learning predictive distribution (4) produces optimally accurate predictions within the chosen model family. In [17, 18], we described how the recently published new coding scheme by Rissanen [28] for representing the stochastic complexity measure <ref> [27] </ref> offers an alternative definition for an optimal predictive distribution.
Reference: 28. <author> J. Rissanen. </author> <title> Fisher information and stochastic complexity. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 42(1) </volume> <pages> 40-47, </pages> <month> January </month> <year> 1996. </year>
Reference-contexts: As a matter of fact, from the probability theory point of view the Bayesian instance-based learning predictive distribution (4) produces optimally accurate predictions within the chosen model family. In [17, 18], we described how the recently published new coding scheme by Rissanen <ref> [28] </ref> for representing the stochastic complexity measure [27] offers an alternative definition for an optimal predictive distribution.
Reference: 29. <author> C. Stanfill and D. Waltz. </author> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1213-1228, </pages> <year> 1986. </year>
Reference-contexts: In contrast to the traditional (eager) approach described above, in the instance-based (also known as the memory-based or the case-based) approach <ref> [29, 24, 1, 4] </ref>, the learning algorithms base their predictions directly on the sample data, without producing any specific models of the problem domain. This type of machine learning is often referred to as lazy learning, since the algorithms defer all the essential computation until the prediction phase [2].
Reference: 30. <author> K. Ting and R. Cameron-Jones. </author> <title> Exploring a framework for instance based learning and Naive Bayes classifiers. </title> <booktitle> In Proceedings of the Seventh Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 100-107, </pages> <year> 1994. </year>
Reference-contexts: Thus the predictive distributions required for making predictions could then be computed by using the instance-based learning approach in the distribution space, i.e., by introducing a probabilistic "distance metric". Somewhat similar frameworks have been suggested in <ref> [16, 30, 11, 12] </ref>. The goal of this paper is to present a novel alternative probabilistic formalization of the purely lazy learning approach.
Reference: 31. <author> H. Tirri, P. Kontkanen, and P. Myllymaki. </author> <title> A Bayesian framework for case-based reasoning. </title> <editor> In I. Smith and B. Faltings, editors, </editor> <booktitle> Advances in Case-Based Reasoning, volume 1168 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 413-427. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin Heidelberg, </address> <month> November </month> <year> 1996. </year>
Reference-contexts: Most importantly, the performance of instance-based learning algorithms seems to be highly sensitive to the selection of distance function to be used as demonstrated in recent work reported in [34, 13, 5]. In <ref> [32, 31, 26, 25] </ref> we proposed a Bayesian framework for instance-based learning based on probability theory and the finite mixture model family [9, 33].
Reference: 32. <author> H. Tirri, P. Kontkanen, and P. Myllymaki. </author> <title> Probabilistic instance-based learning. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 507-515. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1996. </year>
Reference-contexts: It has been shown in various studies (see e.g., [23] for references) that this type of an approach in some cases produces quite accurate predictions, when compared to alternative machine learning methods. The method suffers, however, from several drawbacks when applied in practice (see, e.g., the discussion in <ref> [32] </ref>). Most importantly, the performance of instance-based learning algorithms seems to be highly sensitive to the selection of distance function to be used as demonstrated in recent work reported in [34, 13, 5]. <p> Most importantly, the performance of instance-based learning algorithms seems to be highly sensitive to the selection of distance function to be used as demonstrated in recent work reported in [34, 13, 5]. In <ref> [32, 31, 26, 25] </ref> we proposed a Bayesian framework for instance-based learning based on probability theory and the finite mixture model family [9, 33]. <p> Bayesian probability theory provides a theoretically solid framework for these tasks, as demonstrated, e.g., in [21] in the neural network model family case, and in <ref> [32] </ref> in the finite mixture model family case.
Reference: 33. <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: In [32, 31, 26, 25] we proposed a Bayesian framework for instance-based learning based on probability theory and the finite mixture model family <ref> [9, 33] </ref>. The approach suggested in those studies can be seen as a "partially lazy" approach [2], i.e., a hybrid between the traditional machine learning and the instance-based learning approach, which is based solely on the given data.
Reference: 34. <author> D. Wettschereck, D. Aha, and T. Mohri. </author> <title> A review and empirical evaluation of feature-weighting methods for a class of lazy learning algorithms. </title> <booktitle> In Aha [2], </booktitle> <pages> pages 273-314. </pages>
Reference-contexts: The method suffers, however, from several drawbacks when applied in practice (see, e.g., the discussion in [32]). Most importantly, the performance of instance-based learning algorithms seems to be highly sensitive to the selection of distance function to be used as demonstrated in recent work reported in <ref> [34, 13, 5] </ref>. In [32, 31, 26, 25] we proposed a Bayesian framework for instance-based learning based on probability theory and the finite mixture model family [9, 33].
References-found: 34

