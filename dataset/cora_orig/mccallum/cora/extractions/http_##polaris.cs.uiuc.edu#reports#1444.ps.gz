URL: http://polaris.cs.uiuc.edu/reports/1444.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Email: rwerger@csrd.uiuc.edu amato@cs.tamu.edu padua@csrd.uiuc.edu  
Phone: phone: (217) 244-0070 fax: (217) 244-1351  
Title: A Scalable Method for Run-Time Loop Parallelization  
Author: Lawrence Rauchwerger yx Nancy M. Amato David A. Padua 
Note: Corresponding Author: Lawrence Rauchwerger  Research supported in part by Army contract #DABT63-92-C-0033. This work is not necessarily representative of the positions or policies of the Army or the Government. Research supported in part by an AT&T Bell Laboratories Graduate Fellowship, and by the International  
Address: R&D  1308 W. Main St., Urbana IL 61801  CA.  
Affiliation: Center for Supercomputing R&D Department of Computer Science Center for Supercomputing  University of Illinois Texas A&M University University of Illinois  CSRD, Univ. of Illinois,  Computer Science Institute, Berkeley,  
Abstract: Current parallelizing compilers do a reasonable job of extracting parallelism from programs with regular, well behaved, statically analyzable access patterns. However, they cannot extract a significant fraction of the available parallelism if the program has a complex and/or statically insufficiently defined access pattern, e.g., simulation programs with irregular domains and/or dynamically changing interactions. Since such programs represent a large fraction of all applications, techniques are needed for extracting their inherent parallelism at run-time. In this paper we give a new run-time technique for finding an optimal parallel execution schedule for a partially parallel loop, i.e., a loop whose parallelization requires synchronization to ensure that the iterations are executed in the correct order. Given the original loop, the compiler generates inspector code that performs run-time preprocessing of the loop's access pattern, and scheduler code that schedules (and executes) the loop iterations. The inspector is fully parallel, uses no synchronization, and can be applied to any loop (from which an inspector can be extracted). In addition, it can implement at run-time the two most effective transformations for increasing the amount of parallelism in a loop: array privatization and reduction parallelization (element-wise). The ability to identify privatizable and reduction variables is very powerful since it eliminates the data dependences involving these variables and thereby potentially increases the overall parallelism of the loop. We also describe a new scheme for constructing an optimal parallel execution schedule for the iterations of the loop. The schedule produced is a partition of the set of iterations into subsets called wavefronts so that there are no data dependences between iterations in a wavefront. Although the wavefronts themselves are constructed one after another, the computation of each wavefront is fully parallel and requires no synchronization. This new method has advantages over all previous run-time techniques for analyzing and scheduling partially parallel loops since none of them has all of these desirable properties. y Research supported in part by Intel and NASA Graduate Fellowships.
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Alliant Computer Systems Corporation. FX/Series Architecture Manual, </institution> <year> 1986. </year>
Reference-contexts: If desired, it can be used speculatively (i.e, without an inspector), and can also identify privatizable and reduction variables. 8 Experimental Results In this section we present experimental results obtained on two modestly parallel machines with 8 (Alliant FX/80 <ref> [1] </ref>) and 14 processors (Alliant FX/2800 [2]). However, we remark that the results scale with the number of processors and the data size and thus they may be extrapolated for massively parallel processors (MPPs), the actual target of our run-time methods.
Reference: [2] <institution> Alliant Computers Systems Corporation. Alliant FX/2800 Series System Description, </institution> <year> 1991. </year> <month> 15 </month>
Reference-contexts: If desired, it can be used speculatively (i.e, without an inspector), and can also identify privatizable and reduction variables. 8 Experimental Results In this section we present experimental results obtained on two modestly parallel machines with 8 (Alliant FX/80 [1]) and 14 processors (Alliant FX/2800 <ref> [2] </ref>). However, we remark that the results scale with the number of processors and the data size and thus they may be extrapolated for massively parallel processors (MPPs), the actual target of our run-time methods.
Reference: [3] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer. </publisher> <address> Boston, MA., </address> <year> 1988. </year>
Reference-contexts: Finally, we present some experimental results in Section 8. 2 Preliminaries In order to guarantee the semantics of a loop, the parallel execution schedule for its iterations must respect the data dependence relations between the statements in the loop body <ref> [26, 19, 3, 41, 44] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program. <p> If adjacent references are both reads, then there is no dependence between the elements, but they may have a common parent (child) in D x : the last write preceding (first write following) them in R x . For example, the dependence graph D 3 for A <ref> [3] </ref> is shown in Figure 2 (c). Our goal is to encode the predecessor/successor information of the (conceptual) dependence graph D x in a hierarchy vector H x so that the scheduler can easily look-up the dependence information for the references to A [x]. <p> We now give an example of how the hierarchy vector serves as a look-up table for the predecessors and successors of all the accesses. Consider the read access to A <ref> [3] </ref> in the sixth iteration, which appears as the sixth entry in R 3 . <p> Processor i can find the predecessors (successors) needed for its hierarchy vectors by scanning the arrays of the processors less than (larger than) i. For example, the ? at the end of pH <ref> [3] </ref> for processor 1 in Figure 3 would be filled in with a pointer to the first element in the array pR [3] of processor 2. Hence, the initial and final entries in the hierarchy vectors also need to store the processor number that contains the predecessor and successor. <p> For example, the ? at the end of pH <ref> [3] </ref> for processor 1 in Figure 3 would be filled in with a pointer to the first element in the array pR [3] of processor 2. Hence, the initial and final entries in the hierarchy vectors also need to store the processor number that contains the predecessor and successor.
Reference: [4] <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orzag, F. Seidl, O. Johnson, G. Swanson, R. Goodrum, and J. Martin. </author> <title> The PERFECT club benchmarks: Effective performance evaluation of supercomputers. </title> <type> Technical Report CSRD-827, </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: To demonstrate that the new methods can achieve speedups, we applied them to three loops contained in the PERFECT Benchmarks <ref> [4] </ref> that could not be parallelized by any compiler available to us. <p> The reason that larger speedups were not obtained is that the loop is heavily imbalanced due to the blocked nature of the algorithm used in MA28. 8.2.1 Parallelizing Benchmark Loops We applied the methods to three loops contained in the PERFECT Benchmarks <ref> [4] </ref> that could not be parallelized by any compiler available to us. In the analysis phase of the inspector it was found that one of the loops was fully parallel, and that the other two could be transformed into doalls by privatizing the shared array under test.
Reference: [5] <author> H. Berryman and J. Saltz. </author> <title> A manual for PARTI runtime primitives. </title> <type> Interim Report 90-13, </type> <institution> ICASE, </institution> <year> 1990. </year>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. During the past few years, several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 16, 21, 24, 28, 33, 34, 35, 42, 43] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> Methods for loops without output dependences. The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. <ref> [5, 33, 34, 35, 42] </ref>. Most of their work assumes that there are no output dependences in the source loop. In doacross parallelization [33], an inspector finds the (at most one) iteration in which each variable is written.
Reference: [6] <author> W. Blume and R. Eigenmann. </author> <title> Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks T M Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Thus, since the available parallelism in theses types of applications cannot be determined statically by present parallelizing compilers <ref> [6, 8, 11] </ref>, compile-time analysis must be complemented by new methods capable of automatically extracting parallelism at run-time.
Reference: [7] <author> M. Burke, R. Cytron, J. Ferrante, and W. Hsieh. </author> <title> Automatic generation of nested, fork-join parallelism. </title> <journal> Journal of Supercomputing, </journal> <pages> pages 71-88, </pages> <year> 1989. </year>
Reference-contexts: Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [7, 22, 23, 39, 40] </ref>).
Reference: [8] <author> W. J. Camp, S. J. Plimpton, B. A. Hendrickson, and R. W. Leland. </author> <title> Massively parallel methods for engineering and science problems. </title> <journal> Comm. ACM, </journal> <volume> 37(4) </volume> <pages> 31-41, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: For example, SPICE for circuit simulation, DYNA-3D and PRONTO-3D for structural mechanics modeling, GAUSSIAN and DMOL for quantum mechanical simulation of molecules, CHARMM and DISCOVER for molecular dynamics simulation of organic systems, and FIDAP for modeling complex fluid flows <ref> [8] </ref>. Thus, since the available parallelism in theses types of applications cannot be determined statically by present parallelizing compilers [6, 8, 11], compile-time analysis must be complemented by new methods capable of automatically extracting parallelism at run-time. <p> Thus, since the available parallelism in theses types of applications cannot be determined statically by present parallelizing compilers <ref> [6, 8, 11] </ref>, compile-time analysis must be complemented by new methods capable of automatically extracting parallelism at run-time.
Reference: [9] <author> D. K. Chen, P. C. Yew, and J. Torrellas. </author> <title> An efficient algorithm for the run-time parallelization of doacross loops. </title> <booktitle> In Proceedings of Supercomputing 1994, </booktitle> <pages> pages 518-527, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. During the past few years, several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 16, 21, 24, 28, 33, 34, 35, 42, 43] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> requires restricts privatizes optimal sequential global type of or finds Method schedule portions synchron loop reductions This Paper Yes No No No P,R Zhu/Yew [43] No 1 No Yes 2 No No Midkiff/Padua [24] Yes No Yes 2 No No Krothapalli/Sadayappan [16] No 3 No Yes 2 No P Chen/Yew/Torrellas <ref> [9] </ref> No 1;3 No Yes No No Saltz/Mirchandaney [33] No 3 No Yes Yes 5 No Saltz et al. [35] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [21] Yes No Yes Yes 5 No Polychronopoulous [28] No No No No No Rauchwerger/Padua [30, 31] No 6 No No No P,R <p> The loop is executed in parallel using synchronization (full/empty bits) to enforce flow dependences. To our knowledge, this is the only other run-time privatization technique except [30, 31]. Recently, Chen, Yew, and Torrellas <ref> [9] </ref> proposed an inspector that has a private phase and a merging phase. In the private phase, the loop is chunked and each processor builds a list of all the accesses to each memory location for its assigned iterations.
Reference: [10] <author> I. S. Duff. </author> <title> Ma28- a set of fortran subroutines for sparse unsymmetric linear equations. </title> <type> Technical Report AERE R8730, </type> <address> HMSO, London, </address> <year> 1977. </year>
Reference-contexts: By applying the new methods to such access patterns, we can reconfirm the conclusions reached above using synthetic reference patterns. For this purpose we have chosen a loop out of MA28, a blocked sparse UN-symmetric linear solver <ref> [10] </ref>. Loop MA30cd/DO 120 performs the forward-backward substitution in the final phase of the blocked sparse linear system solver (MA28). We selected this loop because it can generate many diverse access patterns when using the Harwell-Boeing matrices as input.
Reference: [11] <author> R. Eigenmann and W. Blume. </author> <title> An effectiveness study of parallelizing compiler techniques. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages 17-25, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: Thus, since the available parallelism in theses types of applications cannot be determined statically by present parallelizing compilers <ref> [6, 8, 11] </ref>, compile-time analysis must be complemented by new methods capable of automatically extracting parallelism at run-time.
Reference: [12] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the Automatic Parallelization of Four Perfect-Benchmark Programs. </title> <booktitle> Lecture Notes in Computer Science 589. Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <pages> pages 65-83, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: reduction parallelization: recognizing the reduction variable, and parallelizing the reduction operation. (In contrast, privatization needs only to recognize privatizable variables by performing data dependence analysis, i.e., it is contingent only on the access pattern and not on the operations.) Since parallel methods are known for performing reductions operations (see, e.g., <ref> [12, 17, 18, 20, 44] </ref>), the difficulty encountered by compilers arises from recognizing the reduction statements.
Reference: [13] <author> Daniel Gajski, David Kuck, Duncan Lawrie, and Ahmed Sameh. </author> <title> CEDAR ALarge Scale Multiprocessor. </title> <booktitle> Proceedings of the 1983 International Conference on Parallel Processing, </booktitle> <pages> pages 524-529, </pages> <month> Aug., </month> <year> 1983. </year>
Reference-contexts: Various synchronization schemes have been proposed to delay execution until certain conditions are satisfied. For example, the HEP multiprocessor [36] has a full/empty bit associated with each memory location and read (write) accesses are delayed until the bit is full (empty). Similar data-level synchronization schemes have also been proposed <ref> [13, 27] </ref>. Higher-level synchronization primitives such as lock or compare-and-swap can be used in the same manner [15, 24, 43]. When parallelizing do loops, some of today's compilers postpone part of the analysis to run-time by generating two-version loops.
Reference: [14] <author> M. Guzzi, D. Padua, J. Hoeflinger, and D. Lawrie. </author> <title> Cedar fortran and other vector and parallel fortran dialects. </title> <journal> J. Supercomput., </journal> <volume> 4(1) </volume> <pages> 37-62, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: In addition, in order to analyze the overhead incurred by the methods, we applied them to different access patterns taken from loops in the PERFECT Benchmarks and to synthetic access patterns generated to test their behavior in various situations. The methods were implemented in Cedar Fortran <ref> [14] </ref>. The inspector was essentially as described in Section 4. In particular, we implemented the bucket sort version using separate pR and pH data structures for each processor.
Reference: [15] <author> A.K. Jones and P. Schwartz. </author> <title> Using multiprocessor systems a status report. </title> <booktitle> In ACM Computing Surveys 12(2), </booktitle> <pages> pages 121-166, </pages> <year> 1980. </year>
Reference-contexts: Similar data-level synchronization schemes have also been proposed [13, 27]. Higher-level synchronization primitives such as lock or compare-and-swap can be used in the same manner <ref> [15, 24, 43] </ref>. When parallelizing do loops, some of today's compilers postpone part of the analysis to run-time by generating two-version loops. These consist of an if statement that selects either the original serial loop or its parallel version.
Reference: [16] <author> V. Krothapalli and P. Sadayappan. </author> <title> An approach to synchronization of parallel computing. </title> <booktitle> In Proceedings of the 1988 International Conference on Supercomputing, </booktitle> <pages> pages 573-581, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. During the past few years, several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 16, 21, 24, 28, 33, 34, 35, 42, 43] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> be amortized over all loop instantiations. 10 obtains contains requires restricts privatizes optimal sequential global type of or finds Method schedule portions synchron loop reductions This Paper Yes No No No P,R Zhu/Yew [43] No 1 No Yes 2 No No Midkiff/Padua [24] Yes No Yes 2 No No Krothapalli/Sadayappan <ref> [16] </ref> No 3 No Yes 2 No P Chen/Yew/Torrellas [9] No 1;3 No Yes No No Saltz/Mirchandaney [33] No 3 No Yes Yes 5 No Saltz et al. [35] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [21] Yes No Yes Yes 5 No Polychronopoulous [28] No No No No No <p> An advantage of this method is reduced memory requirements: it uses only a shadow version of the shared array under scrutiny whereas all other methods (except [28, 30, 31]) unroll the loop and store all the accesses to the shared array. Krothapalli and Sadayappan <ref> [16] </ref> proposed a run-time scheme for removing anti and output dependences from loops. Their scheme includes a parallel inspector that determines the number of accesses to each memory location using critical sections as in the method of Zhu and Yew (and is thus sensitive to hotspots).
Reference: [17] <author> C. Kruskal. </author> <title> Efficient parallel algorithms for graph problems. </title> <booktitle> In Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1985. </year>
Reference-contexts: reduction parallelization: recognizing the reduction variable, and parallelizing the reduction operation. (In contrast, privatization needs only to recognize privatizable variables by performing data dependence analysis, i.e., it is contingent only on the access pattern and not on the operations.) Since parallel methods are known for performing reductions operations (see, e.g., <ref> [12, 17, 18, 20, 44] </ref>), the difficulty encountered by compilers arises from recognizing the reduction statements.
Reference: [18] <author> C. Kruskal. </author> <title> Efficient parallel algorithms for graph problems. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <pages> pages 869-876, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: reduction parallelization: recognizing the reduction variable, and parallelizing the reduction operation. (In contrast, privatization needs only to recognize privatizable variables by performing data dependence analysis, i.e., it is contingent only on the access pattern and not on the operations.) Since parallel methods are known for performing reductions operations (see, e.g., <ref> [12, 17, 18, 20, 44] </ref>), the difficulty encountered by compilers arises from recognizing the reduction statements.
Reference: [19] <author> D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proceedings of the 8th ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 207-218, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: Finally, we present some experimental results in Section 8. 2 Preliminaries In order to guarantee the semantics of a loop, the parallel execution schedule for its iterations must respect the data dependence relations between the statements in the loop body <ref> [26, 19, 3, 41, 44] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [20] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: reduction parallelization: recognizing the reduction variable, and parallelizing the reduction operation. (In contrast, privatization needs only to recognize privatizable variables by performing data dependence analysis, i.e., it is contingent only on the access pattern and not on the operations.) Since parallel methods are known for performing reductions operations (see, e.g., <ref> [12, 17, 18, 20, 44] </ref>), the difficulty encountered by compilers arises from recognizing the reduction statements. <p> The cost of these searches can be reduced from p to O (log p) using a standard parallel divide-and-conquer pair-wise merging approach <ref> [20] </ref>, where p is the total number of processors. 4.2 Privatization and Reduction Recognition at Run-Time The basic inspector described above can easily be augmented to find the array elements that are independent (i.e., accessed in only one iteration), read-only, privatizable, or reduction variables. <p> In particular, using the bucket sort implementation, each processor spends constant time on each of its O (a) accesses in the marking phase, and the analysis phase takes time O (a log p) using a parallel divide-and-conquer pair-wise merging strategy <ref> [20] </ref>.
Reference: [21] <author> S. Leung and J. Zahorjan. </author> <title> Improving the performance of runtime parallelization. </title> <booktitle> In 4th PPOPP, </booktitle> <pages> pages 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: More powerful analysis techniques could remove this last limitation when the index arrays are computed using only statically-known values. However, nothing can be done at compile-time when the index arrays are a function of the input data <ref> [21, 35, 43] </ref>. Run-time techniques have been used practically from the beginning of parallel computing. During the 1960s, relatively simple run-time techniques, used to detect parallelism between scalar operations, were implemented in the hardware of the CDC 6600 and the IBM 360/91 [37, 38]. <p> Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. During the past few years, several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 16, 21, 24, 28, 33, 34, 35, 42, 43] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> It is imperative that the marking loop be parallel, for otherwise it defeats the purpose of run-time parallelization <ref> [21, 33] </ref>. (Below, we mention some special circumstances in which speedups might still be obtained using a sequential marking loop.) A parallel marking loop can be obtained if the source loop can be distributed into a loop computing the addresses of the array under test and another loop which uses those <p> Yes 2 No No Midkiff/Padua [24] Yes No Yes 2 No No Krothapalli/Sadayappan [16] No 3 No Yes 2 No P Chen/Yew/Torrellas [9] No 1;3 No Yes No No Saltz/Mirchandaney [33] No 3 No Yes Yes 5 No Saltz et al. [35] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan <ref> [21] </ref> Yes No Yes Yes 5 No Polychronopoulous [28] No No No No No Rauchwerger/Padua [30, 31] No 6 No No No P,R Table 1: A comparison of run-time parallelization techniques for do loops. <p> The topological sort can be parallelized somewhat using doacross parallelization. Leung and Zahorjan <ref> [21] </ref> proposed methods of parallelizing the sequential inspector of [35]. In sectioning, the loop is chunked and each each processor computes an optimal parallel for its chunk, and then these schedules are concatenated together, separated by synchronization barriers. In bootstrapping, the inspector is parallelized using sectioning.
Reference: [22] <author> Zhiyuan Li. </author> <title> Array privatization for parallel execution of loops. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 313-322, </pages> <year> 1992. </year>
Reference-contexts: Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [7, 22, 23, 39, 40] </ref>).
Reference: [23] <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data dependence and data-flow analysis of arrays. </title> <booktitle> In Proceedings 5th Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [7, 22, 23, 39, 40] </ref>).
Reference: [24] <author> S. Midkiff and D. Padua. </author> <title> Compiler algorithms for synchronization. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-36(12):1485-1495, </volume> <year> 1987. </year>
Reference-contexts: Similar data-level synchronization schemes have also been proposed [13, 27]. Higher-level synchronization primitives such as lock or compare-and-swap can be used in the same manner <ref> [15, 24, 43] </ref>. When parallelizing do loops, some of today's compilers postpone part of the analysis to run-time by generating two-version loops. These consist of an if statement that selects either the original serial loop or its parallel version. <p> Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. During the past few years, several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 16, 21, 24, 28, 33, 34, 35, 42, 43] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> marking loop since its one sequential execution will be amortized over all loop instantiations. 10 obtains contains requires restricts privatizes optimal sequential global type of or finds Method schedule portions synchron loop reductions This Paper Yes No No No P,R Zhu/Yew [43] No 1 No Yes 2 No No Midkiff/Padua <ref> [24] </ref> Yes No Yes 2 No No Krothapalli/Sadayappan [16] No 3 No Yes 2 No P Chen/Yew/Torrellas [9] No 1;3 No Yes No No Saltz/Mirchandaney [33] No 3 No Yes Yes 5 No Saltz et al. [35] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [21] Yes No Yes Yes 5 <p> Midkiff and Padua <ref> [24] </ref> extended this method to allow concurrent reads from a memory location in multiple iterations. Due to the compare-and-swap synchronizations, this method runs the risk of a severe degradation in performance for access patterns containing hot spots (i.e., many accesses to the same memory location).
Reference: [25] <author> Jose E. Moreira and Constantine D. Polychronopoulos. </author> <title> Autoscheduling in a Distributed Shared-Memory Environment . Technical Report 1373, </title> <institution> Univ of Illinois at Urbana-Champaign, Cntr for Supercomputing Res & Dev, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: This can either be achieved with a dynamic partition of the processors among these two tasks (see Section 5) or with a dynamic ready queue <ref> [25, 29] </ref>. Schedule reuse and decoupling the inspector/scheduler and the executor. Thus far, we have assumed that our methods must be used each time a loop is executed in order to determine a parallel execution schedule for the loop.
Reference: [26] <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29 </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Such hand-coding is difficult, error-prone, and often not portable to different machines. Restructuring, or parallelizing, compilers address these problems by detecting and exploiting parallelism in sequential programs written in conventional languages. Although compiler techniques for the automatic detection of parallelism have been studied extensively over the last two decades <ref> [26, 41] </ref>, current parallelizing compilers cannot extract a significant fraction of the available parallelism in a loop if it has a complex and/or statically insufficiently defined access pattern. <p> Finally, we present some experimental results in Section 8. 2 Preliminaries In order to guarantee the semantics of a loop, the parallel execution schedule for its iterations must respect the data dependence relations between the statements in the loop body <ref> [26, 19, 3, 41, 44] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [27] <author> J.-K. Peir and D. D. Gajski. </author> <title> Data flow execution of fortran loops. </title> <booktitle> In Proc. First Int'l. Conf. on Supercomputing Systems (SCS 85), </booktitle> <pages> pages 129-139, </pages> <month> Dec. </month> <pages> 16-20, </pages> <year> 1985. </year>
Reference-contexts: Various synchronization schemes have been proposed to delay execution until certain conditions are satisfied. For example, the HEP multiprocessor [36] has a full/empty bit associated with each memory location and read (write) accesses are delayed until the bit is full (empty). Similar data-level synchronization schemes have also been proposed <ref> [13, 27] </ref>. Higher-level synchronization primitives such as lock or compare-and-swap can be used in the same manner [15, 24, 43]. When parallelizing do loops, some of today's compilers postpone part of the analysis to run-time by generating two-version loops.
Reference: [28] <author> C. Polychronopoulos. </author> <title> Compiler Optimizations for Enhancing Parallelism and Their Imp act on Architecture Design. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-37(8):991-1004, </volume> <month> August </month> <year> 1988. </year> <month> 16 </month>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. During the past few years, several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 16, 21, 24, 28, 33, 34, 35, 42, 43] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> Yes 2 No No Krothapalli/Sadayappan [16] No 3 No Yes 2 No P Chen/Yew/Torrellas [9] No 1;3 No Yes No No Saltz/Mirchandaney [33] No 3 No Yes Yes 5 No Saltz et al. [35] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [21] Yes No Yes Yes 5 No Polychronopoulous <ref> [28] </ref> No No No No No Rauchwerger/Padua [30, 31] No 6 No No No P,R Table 1: A comparison of run-time parallelization techniques for do loops. In the table entries, P and R show that the method identities privatizable and reduction variables, respectively. <p> However, when there are no hot spots and the critical path length is very small, then this method should perform well. An advantage of this method is reduced memory requirements: it uses only a shadow version of the shared array under scrutiny whereas all other methods (except <ref> [28, 30, 31] </ref>) unroll the loop and store all the accesses to the shared array. Krothapalli and Sadayappan [16] proposed a run-time scheme for removing anti and output dependences from loops. <p> Although bootstrapping might not optimally parallelize the inspector (due to the synchronization barriers introduced for each processor), it will produce the same optimal schedule as the original sequential inspector. Other methods. In contrast to the above methods which place iterations in the lowest possible wavefront, Poly-chronopolous <ref> [28] </ref> gives a method where wavefronts are maximal sets of contiguous iterations with no cross-iteration dependences. Dependences are detected using shadow versions of the variables, either sequentially, or in parallel with the aid of critical sections as in [43].
Reference: [29] <author> Constantine Polychronopoulos, Nawaf Bitar, and Steve Kleiman. nanoThreads: </author> <title> A User-Level Threads Architecture. </title> <booktitle> Proc. of the 1993 Int'l. Conf. on Parallel Computing Technologies, </booktitle> <address> Moscow, Russia, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: This can either be achieved with a dynamic partition of the processors among these two tasks (see Section 5) or with a dynamic ready queue <ref> [25, 29] </ref>. Schedule reuse and decoupling the inspector/scheduler and the executor. Thus far, we have assumed that our methods must be used each time a loop is executed in order to determine a parallel execution schedule for the loop.
Reference: [30] <author> L. Rauchwerger and D. Padua. </author> <title> The privatizing doall test: A run-time technique for doall loop identification and array privatization. </title> <booktitle> In Proceedings of the 1994 International Conference on Supercomputing, </booktitle> <pages> pages 33-43, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: 3 No Yes 2 No P Chen/Yew/Torrellas [9] No 1;3 No Yes No No Saltz/Mirchandaney [33] No 3 No Yes Yes 5 No Saltz et al. [35] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [21] Yes No Yes Yes 5 No Polychronopoulous [28] No No No No No Rauchwerger/Padua <ref> [30, 31] </ref> No 6 No No No P,R Table 1: A comparison of run-time parallelization techniques for do loops. In the table entries, P and R show that the method identities privatizable and reduction variables, respectively. <p> However, when there are no hot spots and the critical path length is very small, then this method should perform well. An advantage of this method is reduced memory requirements: it uses only a shadow version of the shared array under scrutiny whereas all other methods (except <ref> [28, 30, 31] </ref>) unroll the loop and store all the accesses to the shared array. Krothapalli and Sadayappan [16] proposed a run-time scheme for removing anti and output dependences from loops. <p> The loop is executed in parallel using synchronization (full/empty bits) to enforce flow dependences. To our knowledge, this is the only other run-time privatization technique except <ref> [30, 31] </ref>. Recently, Chen, Yew, and Torrellas [9] proposed an inspector that has a private phase and a merging phase. In the private phase, the loop is chunked and each processor builds a list of all the accesses to each memory location for its assigned iterations. <p> Dependences are detected using shadow versions of the variables, either sequentially, or in parallel with the aid of critical sections as in [43]. All of the above mentioned methods attempt to find a valid parallel execution schedule for the source do loop. Recently, we considered a related problem <ref> [30, 31] </ref>: testing at run-time whether the loop is fully parallel, i.e., whether there are any cross-iteration dependences in the loop. Our interest in fully parallel loops is motivated by the observation that they arise frequently in real programs. <p> These graphs show that the speedup scales with the number of processors and is a significant percentage of the ideal speedup. Below, we discuss each loop in more detail. We remark here that these loops could also be identified by the LRPD test <ref> [30, 31] </ref>, a run-time test for identifying fully parallel loops, or loops that can be transformed into doalls using privatization and reduction parallelization. An advantage of the LRPD test is that it has a smaller overhead than the methods we present here. <p> It should be noted that run-time overhead could be significantly reduced through architectural support. We view the methods described in this paper as a building block in an evolving framework of run-time parallelization as a complement to the existing techniques <ref> [30, 31, 32] </ref>. Acknowledgment We would like to thank Paul Petersen for his useful advice, and William Blume and Brett Marsolf for identifying and clarifying applications for our experiments. We are also very grateful to Richard Cole for his suggestions regarding sorting algorithms.
Reference: [31] <author> Lawrence Rauchwerger and David Padua. </author> <title> The LRPD Test: Speculative Run-Time Parallelization of Loops with Privatization and Reduction Parallelization. </title> <type> Technical Report 1390, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res. & Dev., </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: If a marking loop cannot be extracted, then the compiler must choose between sequential execution and a speculative parallel execution <ref> [31] </ref>. At Run-Time. 1. At run-time (and possibly also at compile-time) an evaluation of the storage requirements of the methods is performed. <p> 3 No Yes 2 No P Chen/Yew/Torrellas [9] No 1;3 No Yes No No Saltz/Mirchandaney [33] No 3 No Yes Yes 5 No Saltz et al. [35] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [21] Yes No Yes Yes 5 No Polychronopoulous [28] No No No No No Rauchwerger/Padua <ref> [30, 31] </ref> No 6 No No No P,R Table 1: A comparison of run-time parallelization techniques for do loops. In the table entries, P and R show that the method identities privatizable and reduction variables, respectively. <p> However, when there are no hot spots and the critical path length is very small, then this method should perform well. An advantage of this method is reduced memory requirements: it uses only a shadow version of the shared array under scrutiny whereas all other methods (except <ref> [28, 30, 31] </ref>) unroll the loop and store all the accesses to the shared array. Krothapalli and Sadayappan [16] proposed a run-time scheme for removing anti and output dependences from loops. <p> The loop is executed in parallel using synchronization (full/empty bits) to enforce flow dependences. To our knowledge, this is the only other run-time privatization technique except <ref> [30, 31] </ref>. Recently, Chen, Yew, and Torrellas [9] proposed an inspector that has a private phase and a merging phase. In the private phase, the loop is chunked and each processor builds a list of all the accesses to each memory location for its assigned iterations. <p> Dependences are detected using shadow versions of the variables, either sequentially, or in parallel with the aid of critical sections as in [43]. All of the above mentioned methods attempt to find a valid parallel execution schedule for the source do loop. Recently, we considered a related problem <ref> [30, 31] </ref>: testing at run-time whether the loop is fully parallel, i.e., whether there are any cross-iteration dependences in the loop. Our interest in fully parallel loops is motivated by the observation that they arise frequently in real programs. <p> These graphs show that the speedup scales with the number of processors and is a significant percentage of the ideal speedup. Below, we discuss each loop in more detail. We remark here that these loops could also be identified by the LRPD test <ref> [30, 31] </ref>, a run-time test for identifying fully parallel loops, or loops that can be transformed into doalls using privatization and reduction parallelization. An advantage of the LRPD test is that it has a smaller overhead than the methods we present here. <p> It should be noted that run-time overhead could be significantly reduced through architectural support. We view the methods described in this paper as a building block in an evolving framework of run-time parallelization as a complement to the existing techniques <ref> [30, 31, 32] </ref>. Acknowledgment We would like to thank Paul Petersen for his useful advice, and William Blume and Brett Marsolf for identifying and clarifying applications for our experiments. We are also very grateful to Richard Cole for his suggestions regarding sorting algorithms.
Reference: [32] <author> Lawrence Rauchwerger and David A. Padua. </author> <title> Parallelizing WHILE Loops for Multiprocessor Systems. </title> <type> Technical Report 1349, </type> <institution> Univ. of Illinois at Urbana-Champaign, Cntr. for Supercomputing Res.and Dev., </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: We will limit our discussion below to two input sets: gemat12, which generates 4929 iterations, and bp 1600, which generates 822 iterations. After extracting and precomputing the linear recurrences from the source loop (based on the methods described in <ref> [32] </ref>), we generated a fully parallel inspector and applied our methods to compute an optimal parallel execution schedule for the loop. From the data obtained we constructed the parallelism profiles depicted in Figures 6 and 7. These profiles depict the size of the wavefronts of the optimal parallel execution schedule. <p> It should be noted that run-time overhead could be significantly reduced through architectural support. We view the methods described in this paper as a building block in an evolving framework of run-time parallelization as a complement to the existing techniques <ref> [30, 31, 32] </ref>. Acknowledgment We would like to thank Paul Petersen for his useful advice, and William Blume and Brett Marsolf for identifying and clarifying applications for our experiments. We are also very grateful to Richard Cole for his suggestions regarding sorting algorithms.
Reference: [33] <author> J. Saltz and R. Mirchandaney. </author> <title> The preprocessed doacross loop. In Dr. </title> <editor> H.D. Schwetman, editor, </editor> <booktitle> Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages 174-178. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1991. </year> <title> Vol. </title> <booktitle> II Software. </booktitle>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. During the past few years, several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 16, 21, 24, 28, 33, 34, 35, 42, 43] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> It is imperative that the marking loop be parallel, for otherwise it defeats the purpose of run-time parallelization <ref> [21, 33] </ref>. (Below, we mention some special circumstances in which speedups might still be obtained using a sequential marking loop.) A parallel marking loop can be obtained if the source loop can be distributed into a loop computing the addresses of the array under test and another loop which uses those <p> or finds Method schedule portions synchron loop reductions This Paper Yes No No No P,R Zhu/Yew [43] No 1 No Yes 2 No No Midkiff/Padua [24] Yes No Yes 2 No No Krothapalli/Sadayappan [16] No 3 No Yes 2 No P Chen/Yew/Torrellas [9] No 1;3 No Yes No No Saltz/Mirchandaney <ref> [33] </ref> No 3 No Yes Yes 5 No Saltz et al. [35] Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [21] Yes No Yes Yes 5 No Polychronopoulous [28] No No No No No Rauchwerger/Padua [30, 31] No 6 No No No P,R Table 1: A comparison of run-time parallelization techniques <p> This is similar to the private marking phase of our inspector except that they serialize read accesses (i.e., they have a list instead of the dependence graph). Next, the lists for each memory location are linked across processors using a global Zhu/Yew algorithm [43]. Their scheduler/executor uses doacross parallelization <ref> [33] </ref>, i.e., iterations are started in a wrapped manner and processors busy wait until their operands are ready. Although this scheme potentially has less communication overhead than [43], it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves inferior to [43]. <p> Methods for loops without output dependences. The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. <ref> [5, 33, 34, 35, 42] </ref>. Most of their work assumes that there are no output dependences in the source loop. In doacross parallelization [33], an inspector finds the (at most one) iteration in which each variable is written. <p> Methods for loops without output dependences. The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. [5, 33, 34, 35, 42]. Most of their work assumes that there are no output dependences in the source loop. In doacross parallelization <ref> [33] </ref>, an inspector finds the (at most one) iteration in which each variable is written. The scheduler/executor starts iterations in a wrapped manner and processors busy wait until their operands are available.
Reference: [34] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> The doconsider loop. </title> <booktitle> In Proceedings of the 1989 International Conference on Supercomputing, </booktitle> <pages> pages 29-40, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. During the past few years, several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 16, 21, 24, 28, 33, 34, 35, 42, 43] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> Methods for loops without output dependences. The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. <ref> [5, 33, 34, 35, 42] </ref>. Most of their work assumes that there are no output dependences in the source loop. In doacross parallelization [33], an inspector finds the (at most one) iteration in which each variable is written.
Reference: [35] <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Run-time parallelization and scheduling of loops. </title> <journal> IEEE Trans. Comput., </journal> <volume> 40(5), </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: More powerful analysis techniques could remove this last limitation when the index arrays are computed using only statically-known values. However, nothing can be done at compile-time when the index arrays are a function of the input data <ref> [21, 35, 43] </ref>. Run-time techniques have been used practically from the beginning of parallel computing. During the 1960s, relatively simple run-time techniques, used to detect parallelism between scalar operations, were implemented in the hardware of the CDC 6600 and the IBM 360/91 [37, 38]. <p> Given the original, or source loop, most of these techniques generate inspector code that analyzes, at run-time, the cross-iteration dependences in the loop, and scheduler/executor code that schedules and executes the loop iterations using the dependence information extracted by the inspector <ref> [35] </ref>. 1.1 Our Results In this paper we give a new inspector/scheduler/executor method for finding an optimal parallel execution schedule for a partially parallel loop. Our inspector is fully parallel, uses no synchronization, and can be applied to any loop (from which an inspector can be extracted). <p> Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. During the past few years, several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 16, 21, 24, 28, 33, 34, 35, 42, 43] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> In the previous techniques, the scheduler and the executor are tightly coupled codes which are collectively referred to as the executor, and the inspector and the scheduler/executor codes are usually decoupled <ref> [35] </ref>. Although for efficiency purposes our methods can also interleave the scheduler and the executor, we treat them separately since scheduling and execution are distinct tasks that can be performed independently. <p> This is a simple illustration of the schedule reuse technique, in which a correct execution schedule is determined once, and subsequently reused if all of the defining conditions remain invariant (see, e.g., Saltz et al. <ref> [35] </ref>). If it can be determined at compile time that the data access pattern is invariant across different executions of the same loop, then no additional computation is required. <p> No No No P,R Zhu/Yew [43] No 1 No Yes 2 No No Midkiff/Padua [24] Yes No Yes 2 No No Krothapalli/Sadayappan [16] No 3 No Yes 2 No P Chen/Yew/Torrellas [9] No 1;3 No Yes No No Saltz/Mirchandaney [33] No 3 No Yes Yes 5 No Saltz et al. <ref> [35] </ref> Yes Yes 4 Yes Yes 5 No Leung/Zahorjan [21] Yes No Yes Yes 5 No Polychronopoulous [28] No No No No No Rauchwerger/Padua [30, 31] No 6 No No No P,R Table 1: A comparison of run-time parallelization techniques for do loops. <p> Methods for loops without output dependences. The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. <ref> [5, 33, 34, 35, 42] </ref>. Most of their work assumes that there are no output dependences in the source loop. In doacross parallelization [33], an inspector finds the (at most one) iteration in which each variable is written. <p> In doacross parallelization [33], an inspector finds the (at most one) iteration in which each variable is written. The scheduler/executor starts iterations in a wrapped manner and processors busy wait until their operands are available. In <ref> [35] </ref>, the inspector constructs wavefronts that respect the flow dependences by performing a sequential topological sort of the accesses in the loop, and the scheduler/executor enforces any anti dependences using old and new versions of each variable (possible since each variable in the source loop is written at most once). <p> The topological sort can be parallelized somewhat using doacross parallelization. Leung and Zahorjan [21] proposed methods of parallelizing the sequential inspector of <ref> [35] </ref>. In sectioning, the loop is chunked and each each processor computes an optimal parallel for its chunk, and then these schedules are concatenated together, separated by synchronization barriers. In bootstrapping, the inspector is parallelized using sectioning.
Reference: [36] <author> B. J. Smith. </author> <title> A pipelined, shared resource mimd computer. </title> <booktitle> In Proceedings of the 1978 International Conference on Parallel Processing, </booktitle> <year> 1987. </year>
Reference-contexts: During the 1960s, relatively simple run-time techniques, used to detect parallelism between scalar operations, were implemented in the hardware of the CDC 6600 and the IBM 360/91 [37, 38]. Various synchronization schemes have been proposed to delay execution until certain conditions are satisfied. For example, the HEP multiprocessor <ref> [36] </ref> has a full/empty bit associated with each memory location and read (write) accesses are delayed until the bit is full (empty). Similar data-level synchronization schemes have also been proposed [13, 27]. Higher-level synchronization primitives such as lock or compare-and-swap can be used in the same manner [15, 24, 43].
Reference: [37] <author> J. E. Thornton. </author> <title> Design of a Computer:The Control Data 6600. </title> <type> Scott, </type> <institution> Foresman, Glenview, Illinois, </institution> <year> 1971. </year>
Reference-contexts: Run-time techniques have been used practically from the beginning of parallel computing. During the 1960s, relatively simple run-time techniques, used to detect parallelism between scalar operations, were implemented in the hardware of the CDC 6600 and the IBM 360/91 <ref> [37, 38] </ref>. Various synchronization schemes have been proposed to delay execution until certain conditions are satisfied. For example, the HEP multiprocessor [36] has a full/empty bit associated with each memory location and read (write) accesses are delayed until the bit is full (empty).
Reference: [38] <author> R. M. Tomasulo. </author> <title> An efficient algorithm for exploiting multiple arithmetic units. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 11 </volume> <pages> 25-33, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: Run-time techniques have been used practically from the beginning of parallel computing. During the 1960s, relatively simple run-time techniques, used to detect parallelism between scalar operations, were implemented in the hardware of the CDC 6600 and the IBM 360/91 <ref> [37, 38] </ref>. Various synchronization schemes have been proposed to delay execution until certain conditions are satisfied. For example, the HEP multiprocessor [36] has a full/empty bit associated with each memory location and read (write) accesses are delayed until the bit is full (empty).
Reference: [39] <author> P. Tu and D. Padua. </author> <title> Array privatization for shared and distributed memory machines. </title> <booktitle> In Proceedings 2nd Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Machines, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [7, 22, 23, 39, 40] </ref>).
Reference: [40] <author> P. Tu and D. Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proceedings 6th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: Privatization creates, for each processor cooperating on the execution of the loop, private copies of the program variables that give rise to anti or output dependences (see, e.g., <ref> [7, 22, 23, 39, 40] </ref>).
Reference: [41] <author> M. Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Boston, MA, </address> <year> 1989. </year>
Reference-contexts: Such hand-coding is difficult, error-prone, and often not portable to different machines. Restructuring, or parallelizing, compilers address these problems by detecting and exploiting parallelism in sequential programs written in conventional languages. Although compiler techniques for the automatic detection of parallelism have been studied extensively over the last two decades <ref> [26, 41] </ref>, current parallelizing compilers cannot extract a significant fraction of the available parallelism in a loop if it has a complex and/or statically insufficiently defined access pattern. <p> Finally, we present some experimental results in Section 8. 2 Preliminaries In order to guarantee the semantics of a loop, the parallel execution schedule for its iterations must respect the data dependence relations between the statements in the loop body <ref> [26, 19, 3, 41, 44] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program.
Reference: [42] <author> J. Wu, J. Saltz, S. Hiranandani, and H. Berryman. </author> <title> Runtime compilation methods for multicomputers. In Dr. </title> <editor> H.D. Schwetman, editor, </editor> <booktitle> Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages 26-30. </pages> <publisher> CRC Press, Inc., </publisher> <year> 1991. </year> <title> Vol. </title> <booktitle> II Software. </booktitle>
Reference-contexts: Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. During the past few years, several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 16, 21, 24, 28, 33, 34, 35, 42, 43] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> Methods for loops without output dependences. The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. <ref> [5, 33, 34, 35, 42] </ref>. Most of their work assumes that there are no output dependences in the source loop. In doacross parallelization [33], an inspector finds the (at most one) iteration in which each variable is written.
Reference: [43] <author> C. Zhu and P. C. Yew. </author> <title> A scheme to enforce data dependence on large multiprocessor systems. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> 13(6) </volume> <pages> 726-739, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: More powerful analysis techniques could remove this last limitation when the index arrays are computed using only statically-known values. However, nothing can be done at compile-time when the index arrays are a function of the input data <ref> [21, 35, 43] </ref>. Run-time techniques have been used practically from the beginning of parallel computing. During the 1960s, relatively simple run-time techniques, used to detect parallelism between scalar operations, were implemented in the hardware of the CDC 6600 and the IBM 360/91 [37, 38]. <p> Similar data-level synchronization schemes have also been proposed [13, 27]. Higher-level synchronization primitives such as lock or compare-and-swap can be used in the same manner <ref> [15, 24, 43] </ref>. When parallelizing do loops, some of today's compilers postpone part of the analysis to run-time by generating two-version loops. These consist of an if statement that selects either the original serial loop or its parallel version. <p> Since compile-time data dependence analysis techniques cannot be used on such programs, methods of performing the analysis at run-time are required. During the past few years, several techniques have been developed for the run-time analysis and scheduling of loops with cross-iteration dependences <ref> [5, 9, 16, 21, 24, 28, 33, 34, 35, 42, 43] </ref>. However, for various reasons, such techniques have not achieved wide-spread use in current parallelizing compilers. In the following we describe a new run-time scheme for constructing a parallel execution schedule for the iterations of a loop. <p> still be possible to obtain speedups with a sequential marking loop since its one sequential execution will be amortized over all loop instantiations. 10 obtains contains requires restricts privatizes optimal sequential global type of or finds Method schedule portions synchron loop reductions This Paper Yes No No No P,R Zhu/Yew <ref> [43] </ref> No 1 No Yes 2 No No Midkiff/Padua [24] Yes No Yes 2 No No Krothapalli/Sadayappan [16] No 3 No Yes 2 No P Chen/Yew/Torrellas [9] No 1;3 No Yes No No Saltz/Mirchandaney [33] No 3 No Yes Yes 5 No Saltz et al. [35] Yes Yes 4 Yes Yes <p> A high level comparison of the various methods is given in Table 1. Methods utilizing critical sections. One of the first run-time methods for scheduling partially parallel loops was proposed by Zhu and Yew <ref> [43] </ref>. It computes the wavefronts one after another using a method similar to the simple scheduler described in Section 5.1. <p> This is similar to the private marking phase of our inspector except that they serialize read accesses (i.e., they have a list instead of the dependence graph). Next, the lists for each memory location are linked across processors using a global Zhu/Yew algorithm <ref> [43] </ref>. Their scheduler/executor uses doacross parallelization [33], i.e., iterations are started in a wrapped manner and processors busy wait until their operands are ready. Although this scheme potentially has less communication overhead than [43], it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves <p> Next, the lists for each memory location are linked across processors using a global Zhu/Yew algorithm <ref> [43] </ref>. Their scheduler/executor uses doacross parallelization [33], i.e., iterations are started in a wrapped manner and processors busy wait until their operands are ready. Although this scheme potentially has less communication overhead than [43], it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves inferior to [43]. Methods for loops without output dependences. The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. [5, 33, 34, 35, 42]. <p> Although this scheme potentially has less communication overhead than <ref> [43] </ref>, it is still sensitive to hot spots and there are cases (e.g., doalls) in which it proves inferior to [43]. Methods for loops without output dependences. The problem of analyzing and scheduling loops at run-time has been studied extensively by Saltz et al. [5, 33, 34, 35, 42]. Most of their work assumes that there are no output dependences in the source loop. <p> Dependences are detected using shadow versions of the variables, either sequentially, or in parallel with the aid of critical sections as in <ref> [43] </ref>. All of the above mentioned methods attempt to find a valid parallel execution schedule for the source do loop. Recently, we considered a related problem [30, 31]: testing at run-time whether the loop is fully parallel, i.e., whether there are any cross-iteration dependences in the loop.
Reference: [44] <author> H. Zima. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, New York, </address> <year> 1991. </year> <pages> 17 18 19 20 </pages>
Reference-contexts: Finally, we present some experimental results in Section 8. 2 Preliminaries In order to guarantee the semantics of a loop, the parallel execution schedule for its iterations must respect the data dependence relations between the statements in the loop body <ref> [26, 19, 3, 41, 44] </ref>. There are three possible types of dependences between two statements that access the same memory location: flow (read after write), anti (write after read), and output (write after write). Flow dependences express a fundamental relationship about the data flow in the program. <p> reduction parallelization: recognizing the reduction variable, and parallelizing the reduction operation. (In contrast, privatization needs only to recognize privatizable variables by performing data dependence analysis, i.e., it is contingent only on the access pattern and not on the operations.) Since parallel methods are known for performing reductions operations (see, e.g., <ref> [12, 17, 18, 20, 44] </ref>), the difficulty encountered by compilers arises from recognizing the reduction statements. <p> this problem has been handled at compile-time by syntactically pattern matching the loop statements with a template of a generic reduction, and then performing a data dependence analysis of the variable under scrutiny to guarantee that it is not used anywhere else in the loop except in the reduction statement <ref> [44] </ref>. 3 Analyzing Partially Parallel Loops at Run-Time Given a do loop whose access pattern cannot be statically analyzed, compilers have traditionally generated sequential code.
References-found: 44

