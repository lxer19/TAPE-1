URL: http://www.cfar.umd.edu/ftp/TRs/CVL-Reports-1994/TR3265-Yacoob.ps.Z
Refering-URL: http://www.umiacs.umd.edu/users/lsd/pubs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: RECOGNIZING HUMAN FACIAL EXPRESSION  
Author: Yaser Yacoob Larry S. Davis 
Note: The support of the Advanced Research Projects Agency (ARPA Order No. 8459) and the U.S. Army Topographic Engineering Center under Contract DACA76-92-C-0009 is gratefully acknowledged.  
Address: College Park, MD 20742-3275  
Affiliation: Computer Vision Laboratory Center for Automation Research University of Maryland  
Date: May 1994  
Pubnum: CAR-TR-706 DACA76-92-C-0009  
Abstract: An approach to the analysis and representation of facial dynamics for recognition of facial expressions from image sequences is proposed. The algorithms we develop utilize optical flow computation to identify the directions of rigid and non-rigid motions that are caused by human facial expressions. A mid-level symbolic representation motivated by linguistic and psychological considerations is developed. Recognition of six facial expressions, as well as eye blinking, is demonstrated on a large set of image sequences.
Abstract-found: 1
Intro-found: 1
Reference: [ABD93] <author> M. Abdel-Mottaleb, R. Chellappa, and A. Rosenfeld, </author> <title> "Binocular motion stereo using MAP estimation", </title> <booktitle> IEEE Conference on Computer Vision and Pattern recognition, </booktitle> <pages> 321-327, </pages> <year> 1993. </year>
Reference-contexts: Our motion analysis focuses on the mouth, eyes, eyebrows and nose. composed of the following components: * Optical flow computation: Optical flow is computed at points with high gradient in each frame. Our flow computation algorithm is based on a correlation approach proposed by Abdel-Mottaleb et al. <ref> [ABD93] </ref>. It computes subpixel flow assuming that the motion between two consecutive images is bounded within an n fi n window. <p> In addition, since facial expressions are non-rigid motions, the filter designs needed for detecting and estimating motion are not easily determined. The correlation approaches (e.g., see <ref> [ABD93, ANA89] </ref>) compare the linearly filtered intensity value of a pixel with linearly filtered intensity values arriving, delayed in time, from a neighboring image region. Correlation approaches are generally computation intensive since some form of exhaustive search is carried out to determine the best estimate of motion. <p> None of these approaches have been extensively tested on non-rigid motions. The approach we use for optical flow computation is a correlation approach recently proposed by Abdel-Mottaleb et al. <ref> [ABD93] </ref>. Assume that a pixel's displacement (x; y) between frame i and frame i + 1 is at most n pixels and is expressed in terms of an integer and a fraction part, i.e., (x; y) = (i x + f x ; i y + f y ).
Reference: [ANA89] <author> P. Anandan, </author> <title> "A computational framework and an algorithm for the measurement of visual motion," </title> <journal> International Journal of Computer Vision, </journal> <volume> Vol. 2, </volume> <pages> 283-310, </pages> <year> 1989. </year>
Reference-contexts: In addition, since facial expressions are non-rigid motions, the filter designs needed for detecting and estimating motion are not easily determined. The correlation approaches (e.g., see <ref> [ABD93, ANA89] </ref>) compare the linearly filtered intensity value of a pixel with linearly filtered intensity values arriving, delayed in time, from a neighboring image region. Correlation approaches are generally computation intensive since some form of exhaustive search is carried out to determine the best estimate of motion.
Reference: [BAS79] <author> J.N. Bassili, </author> <title> "Emotion recognition: The role of facial movement and the relative importance of upper and lower areas of the face," </title> <journal> Journal of Personality and Social Psychology, </journal> <volume> Vol. 37, </volume> <pages> 2049-2059, </pages> <year> 1979. </year>
Reference-contexts: These pictures allow one to detect the presence of static cues (such as wrinkles) as well as the positions and shapes of the facial features. Few studies have directly investigated the influence of the motion and deformation of facial features on the recognition of facial expressions. Bassili <ref> [BAS79] </ref> suggested that motion in the image of a face would allow emotions to be identified even with minimal information about the spatial arrangement of features. <p> Whereas all expressions were recognized at above chance levels in dynamic images, only happiness and sadness were recognized at above chance levels in static images. Table 1: Recognition accuracy in moving and static displays from <ref> [BAS79] </ref> (chance score is %16.7). Happiness Sadness Fear Surprise Anger Disgust Moving Face 90.0 44.2 43.3 88.3 39.2 53.3 Static Face 30.8 25.8 12.5 47.5 16.7 21.7 Before proceeding, we introduce some terminology that is needed in the paper. <p> The proposed mid-level representation is based on two sources from psychology for describing facial expressions: the description of the apex of a facial expression from a static picture as suggested by Ekman and Friesen in [EKM75], and the description of motion patterns of the face as proposed by Bassili in <ref> [BAS79] </ref>. We chose not to model or analyze facial muscle actions, setting our work apart from recent approaches [LI93, MAS91, TER93], as well as not to use models for muscle actions [EKM78]. Instead, we focus on the motions associated with the edges of the mouth, eyes, and eyebrows. <p> We have developed a rule-based system that combines these representations to compute high level descriptions of facial actions (e.g., happiness, surprise, blinking). These rules are based on the descriptions given in [EKM75] describing the universal display of emotions and the rules proposed by Bassili <ref> [BAS79] </ref> for describing facial expressions from motion sequences. 8 3 Optical flow computation Optical flow algorithms can be divided into those based on correlation, filtering, and image gradient. The gradient algorithms are usually based on the formulation by Horn and Schunck [HOR81]. Such algorithms face difficulties in highly textured images. <p> The dictionary borrows from the facial cues of universal expression descriptions proposed by Ekman and Friesen [EKM75], and from the motion patterns of expression proposed by Bassili <ref> [BAS79] </ref>. As a result, we arrive at a dictionary that is a motion-based feature description of facial actions. The dictionary we propose is divided into: components, basic actions of these components, and motion cues. <p> The set of all detected facial actions is used in the following section for recognizing facial expressions. 6 Recognizing facial expressions We have designed a rule based system that combines some of the expression descriptions from [EKM75] and <ref> [BAS79] </ref>. We consider a temporal procedure that is employed in recognizing facial expression, as well as the procedure used to resolve conflicts between hypothesized expressions. <p> As a result, we acquired a variety of presumably similar facial expressions; some were consistent with Ekman and Friesen's dictionary [EKM75] for static images and Bassili's <ref> [BAS79] </ref> dictionary for motion images, while others varied considerably. <p> This approach is based on qualitative tracking of principal regions of the face and flow computation at high intensity gradient points. A mid-level representation is computed from the spatial and the 27 temporal motion results. The representation is linguistically motivated, following research in the psychology literature in <ref> [BAS79, EKM75] </ref>. We have carried out experiments on over thirty subjects in a laboratory environment and achieved good classification of facial expressions. Further study of the system's components will be carried out as well as expanding its capability to deal with non-emotion facial messages.
Reference: [BRU88] <author> V. Bruce, </author> <title> Recognizing Faces, </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> London, </address> <year> 1988. </year>
Reference-contexts: Visual communication has been extensively studied in the psychology literature, mainly as a means of describing the emotional, cognitive and physical states of subjects and the role they play in social interactions <ref> [BRU88, DAV75, EKM73, SCH84, YOU89] </ref>. Ekman and Friesen [EKM75] classified facial signals into three types: static (such as skin color), slow (such as permanent wrinkles), and rapid (such as raising the eyebrows). The rapid facial signals can be further classified as conveying emotional, emblematic, manipulator, illustrator, and regulator messages [EKM92]. <p> Furthermore, it builds on a model that is suitable for synthesizing facial expressions but remains untested in analysis of facial expressions (for more details see <ref> [BRU88] </ref>). Mase's bottom-up approach tessellated the area of the face with rectangular regions over which feature vectors derived from an optical flow computation are computed. The feature vectors are defined over a 15-dimensional space that is based on the means and variances of the optical flow.
Reference: [DAR1872] <author> C. Darwin, </author> <title> The Expression of Emotions in Man and Animals, </title> <publisher> John Murray, 1872, reprinted by University of Chicago Press, </publisher> <year> 1965. </year>
Reference-contexts: Manipulator messages include self-manipulative movements such as lip biting. Illustrators include actions accompanying and highlighting speech such as raising the eyebrows. Regulators are non-verbal mediators such as nods and smiles. Our interest focuses here on recognizing emotions conveyed by human faces. This field was pioneered by Darwin's work <ref> [DAR1872] </ref> and has been extensively studied in psychology during the last twenty years [YOU89]. We focus our attention on the appearance of the face from its near frontal image projection, without considering the underlying anatomic and musculature models and actions.
Reference: [DAV75] <editor> M. Davis and H. College, (Eds.), </editor> <title> Recognition of Facial Expressions, </title> <publisher> Arno Press, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: Visual communication has been extensively studied in the psychology literature, mainly as a means of describing the emotional, cognitive and physical states of subjects and the role they play in social interactions <ref> [BRU88, DAV75, EKM73, SCH84, YOU89] </ref>. Ekman and Friesen [EKM75] classified facial signals into three types: static (such as skin color), slow (such as permanent wrinkles), and rapid (such as raising the eyebrows). The rapid facial signals can be further classified as conveying emotional, emblematic, manipulator, illustrator, and regulator messages [EKM92].
Reference: [EKM73] <author> P. Ekman (Ed.), </author> <title> Darwin and Facial Expression, </title> <publisher> Academic Press, </publisher> <year> 1973. </year>
Reference-contexts: Visual communication has been extensively studied in the psychology literature, mainly as a means of describing the emotional, cognitive and physical states of subjects and the role they play in social interactions <ref> [BRU88, DAV75, EKM73, SCH84, YOU89] </ref>. Ekman and Friesen [EKM75] classified facial signals into three types: static (such as skin color), slow (such as permanent wrinkles), and rapid (such as raising the eyebrows). The rapid facial signals can be further classified as conveying emotional, emblematic, manipulator, illustrator, and regulator messages [EKM92].
Reference: [EKM75] <author> P. Ekman and W. Friesen, </author> <title> Unmasking the Face, </title> <publisher> Prentice-Hall, </publisher> <year> 1975. </year>
Reference-contexts: Visual communication has been extensively studied in the psychology literature, mainly as a means of describing the emotional, cognitive and physical states of subjects and the role they play in social interactions [BRU88, DAV75, EKM73, SCH84, YOU89]. Ekman and Friesen <ref> [EKM75] </ref> classified facial signals into three types: static (such as skin color), slow (such as permanent wrinkles), and rapid (such as raising the eyebrows). The rapid facial signals can be further classified as conveying emotional, emblematic, manipulator, illustrator, and regulator messages [EKM92]. <p> Therefore, an expression classifier that employs a representation of facial feature actions is employed. The proposed mid-level representation is based on two sources from psychology for describing facial expressions: the description of the apex of a facial expression from a static picture as suggested by Ekman and Friesen in <ref> [EKM75] </ref>, and the description of motion patterns of the face as proposed by Bassili in [BAS79]. We chose not to model or analyze facial muscle actions, setting our work apart from recent approaches [LI93, MAS91, TER93], as well as not to use models for muscle actions [EKM78]. <p> We have developed a rule-based system that combines these representations to compute high level descriptions of facial actions (e.g., happiness, surprise, blinking). These rules are based on the descriptions given in <ref> [EKM75] </ref> describing the universal display of emotions and the rules proposed by Bassili [BAS79] for describing facial expressions from motion sequences. 8 3 Optical flow computation Optical flow algorithms can be divided into those based on correlation, filtering, and image gradient. <p> We are studying ways to overcome this in our current research. In spite of these shortcomings, the algorithm was quite robust when applied to thousands of images. 5 Computing local motion representations 5.1 Psychological basis for recognizing facial expressions Table 2 summarizes the results of Ekman and Friesen <ref> [EKM75] </ref> on the universal cues for recognizing the six principal emotions. These cues describe the peak of each expression and provide a human interpretation of the static appearance of the facial feature. <p> The dictionary borrows from the facial cues of universal expression descriptions proposed by Ekman and Friesen <ref> [EKM75] </ref>, and from the motion patterns of expression proposed by Bassili [BAS79]. As a result, we arrive at a dictionary that is a motion-based feature description of facial actions. The dictionary we propose is divided into: components, basic actions of these components, and motion cues. <p> The set of all detected facial actions is used in the following section for recognizing facial expressions. 6 Recognizing facial expressions We have designed a rule based system that combines some of the expression descriptions from <ref> [EKM75] </ref> and [BAS79]. We consider a temporal procedure that is employed in recognizing facial expression, as well as the procedure used to resolve conflicts between hypothesized expressions. <p> As a result, we acquired a variety of presumably similar facial expressions; some were consistent with Ekman and Friesen's dictionary <ref> [EKM75] </ref> for static images and Bassili's [BAS79] dictionary for motion images, while others varied considerably. <p> This approach is based on qualitative tracking of principal regions of the face and flow computation at high intensity gradient points. A mid-level representation is computed from the spatial and the 27 temporal motion results. The representation is linguistically motivated, following research in the psychology literature in <ref> [BAS79, EKM75] </ref>. We have carried out experiments on over thirty subjects in a laboratory environment and achieved good classification of facial expressions. Further study of the system's components will be carried out as well as expanding its capability to deal with non-emotion facial messages.
Reference: [EKM78] <author> P. Ekman and W. Friesen, </author> <title> The Facial Action Coding System, </title> <publisher> Consulting Psychologists Press, </publisher> <address> San Francisco, CA, </address> <year> 1978. </year>
Reference-contexts: Several other emotions, and many combinations of emotions, have been studied but remain unconfirmed as universally distinguishable. The six principal emotions are: happiness, sadness, surprise, fear, anger, and disgust (see Figure 1). In this paper we focus on these emotions. Ekman and Friesen <ref> [EKM78] </ref> developed the most comprehensive system for synthesizing facial expressions based on what they call Action Units (the Facial Actions Coding System|FACS). Each 1 2 AU may correspond to several muscles that together bring about a certain facial action. <p> Four facial expressions were studied: surprise, anger, happiness, and disgust. The top-down approach assumes that the face image is divided into muscle units that correspond to the AUs suggested by Ekman and Friesen <ref> [EKM78] </ref>. Optical flow is computed within rectangles that include these muscle units, which in turn can be related to facial expression. However, no results on recognition of facial expressions based on the motion were reported. <p> We chose not to model or analyze facial muscle actions, setting our work apart from recent approaches [LI93, MAS91, TER93], as well as not to use models for muscle actions <ref> [EKM78] </ref>. Instead, we focus on the motions associated with the edges of the mouth, eyes, and eyebrows. These edges allow us to refer to the face features using natural linguistic terminology.
Reference: [EKM92] <author> P. Ekman, T.S. Huang, T.J. Sejnowski, and J.C. Hager, </author> <title> NSF Planning Workshop on Facial Expression Understanding, </title> <booktitle> 1992, </booktitle> <address> Arlington, VA. </address>
Reference-contexts: Ekman and Friesen [EKM75] classified facial signals into three types: static (such as skin color), slow (such as permanent wrinkles), and rapid (such as raising the eyebrows). The rapid facial signals can be further classified as conveying emotional, emblematic, manipulator, illustrator, and regulator messages <ref> [EKM92] </ref>. Emotional messages include such feelings as sadness, happiness, fear, etc. Emblematic messages describe facial signals as specific nonverbal equivalents of common words or phrases (e.g., an eye wink). Manipulator messages include self-manipulative movements such as lip biting. Illustrators include actions accompanying and highlighting speech such as raising the eyebrows.
Reference: [HEE88] <author> D.J. Heeger, </author> <title> "Optical flow using spatiotemporal filters," </title> <journal> International Journal of Computer Vision, </journal> <volume> Vol. 1, </volume> <pages> 279-302, </pages> <year> 1988. </year>
Reference-contexts: Such algorithms face difficulties in highly textured images. In the context of facial deformations, one has to assume that the deformations of the skin are locally smooth in order to use the gradient approach (see [MAS91]). The filtering approaches (e.g., Heeger <ref> [HEE88] </ref>) use an extended number of images to compute the motion field based on analysis of the spatial and temporal frequencies of the transformed images. Filtering approaches require the use of many frames for motion estimation, which for current sensors limits their utilization to measure the motions associated with expressions.
Reference: [HOR81] <author> B.K.P. Horn, and B.G. Schunk, </author> <title> "Determining optical flow," </title> <journal> Artificial Intelligence, </journal> <volume> Vol. 17, </volume> <pages> 185-203, </pages> <year> 1981. </year>
Reference-contexts: The gradient algorithms are usually based on the formulation by Horn and Schunck <ref> [HOR81] </ref>. Such algorithms face difficulties in highly textured images. In the context of facial deformations, one has to assume that the deformations of the skin are locally smooth in order to use the gradient approach (see [MAS91]).
Reference: [LI93] <author> H. Li, P. Roivainen, and R. Forcheimer, </author> <title> "3-D motion estimation in model-based facial image coding," </title> <journal> IEEE Transactions on PAMI, </journal> <volume> Vol. 15, </volume> <pages> 545-555, </pages> <year> 1993. </year>
Reference-contexts: We focus our attention on the appearance of the face from its near frontal image projection, without considering the underlying anatomic and musculature models and actions. In doing so we depart from the current trend in which facial expression analysis is based on recovering muscle actions <ref> [LI93, MAS91, TER93] </ref>. Research in psychology has indicated that at least six emotions are universally associated with distinct facial expressions. Several other emotions, and many combinations of emotions, have been studied but remain unconfirmed as universally distinguishable. <p> In their model, 46 AUs are responsible for expression control and 12 for gaze direction and orientation. The FACS model has been used to synthesize images of facial expressions, but only limited exploration of its use in analysis has been performed <ref> [LI93, MAS91, TER93] </ref>. Most psychological research on facial expressions has been conducted on "mug-shot" pictures that capture the subject's expression at its "apex" or peak [YOU89]. <p> The goal is to develop computational methods that use face region motions as cues for action recovery. 1.2 Related research in computer vision The problem of recognizing facial expressions has recently attracted attention in the computer vision community <ref> [LI93, MAS91, TER93] </ref>. <p> It remains to be determined whether the computation of muscle contractions would be useful for facial expression recognition (the authors did not explore this in [TER93]). Li et al. <ref> [LI93] </ref> proposed an approach that analyzes facial images for resynthesis. They do not attempt to classify or reason about facial expressions and actions, although some aspects of their work might potentially contribute to that. <p> We chose not to model or analyze facial muscle actions, setting our work apart from recent approaches <ref> [LI93, MAS91, TER93] </ref>, as well as not to use models for muscle actions [EKM78]. Instead, we focus on the motions associated with the edges of the mouth, eyes, and eyebrows. These edges allow us to refer to the face features using natural linguistic terminology.
Reference: [MAS91] <author> K. Mase, </author> <title> "Recognition of facial expression from optical flow," </title> <journal> IEICE Transactions, </journal> <volume> Vol. E 74, </volume> <pages> 3474-3483, </pages> <year> 1991. </year>
Reference-contexts: We focus our attention on the appearance of the face from its near frontal image projection, without considering the underlying anatomic and musculature models and actions. In doing so we depart from the current trend in which facial expression analysis is based on recovering muscle actions <ref> [LI93, MAS91, TER93] </ref>. Research in psychology has indicated that at least six emotions are universally associated with distinct facial expressions. Several other emotions, and many combinations of emotions, have been studied but remain unconfirmed as universally distinguishable. <p> In their model, 46 AUs are responsible for expression control and 12 for gaze direction and orientation. The FACS model has been used to synthesize images of facial expressions, but only limited exploration of its use in analysis has been performed <ref> [LI93, MAS91, TER93] </ref>. Most psychological research on facial expressions has been conducted on "mug-shot" pictures that capture the subject's expression at its "apex" or peak [YOU89]. <p> The goal is to develop computational methods that use face region motions as cues for action recovery. 1.2 Related research in computer vision The problem of recognizing facial expressions has recently attracted attention in the computer vision community <ref> [LI93, MAS91, TER93] </ref>. <p> The following considerations also influenced the proposed approach. Mase <ref> [MAS91] </ref> developed a pattern classifier for four emotions without a mid-level representation of face actions. Mid-level linguistic representations of facial actions can be useful for a variety of tasks beyond recognition of emotions (e.g., emblematic expressions). Therefore, an expression classifier that employs a representation of facial feature actions is employed. <p> We chose not to model or analyze facial muscle actions, setting our work apart from recent approaches <ref> [LI93, MAS91, TER93] </ref>, as well as not to use models for muscle actions [EKM78]. Instead, we focus on the motions associated with the edges of the mouth, eyes, and eyebrows. These edges allow us to refer to the face features using natural linguistic terminology. <p> It computes subpixel flow assuming that the motion between two consecutive images is bounded within an n fi n window. Correlation is particularly effective when flow is computed at discontinuities (in comparison, a derivative based approach was used by Mase <ref> [MAS91] </ref> since the flow was computed at non-edge areas). * Region tracking: Accurate localization of facial features is both difficult and computationally expensive if performed for each frame. We assume that, for each feature, we can initially compute a rectangular region that encloses it. <p> The gradient algorithms are usually based on the formulation by Horn and Schunck [HOR81]. Such algorithms face difficulties in highly textured images. In the context of facial deformations, one has to assume that the deformations of the skin are locally smooth in order to use the gradient approach (see <ref> [MAS91] </ref>). The filtering approaches (e.g., Heeger [HEE88]) use an extended number of images to compute the motion field based on analysis of the spatial and temporal frequencies of the transformed images.
Reference: [SCH84] <author> K.R. Scherer and P. Ekman (Eds.), </author> <title> Approaches to Emotion, </title> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1984. </year> <month> 30 </month>
Reference-contexts: Visual communication has been extensively studied in the psychology literature, mainly as a means of describing the emotional, cognitive and physical states of subjects and the role they play in social interactions <ref> [BRU88, DAV75, EKM73, SCH84, YOU89] </ref>. Ekman and Friesen [EKM75] classified facial signals into three types: static (such as skin color), slow (such as permanent wrinkles), and rapid (such as raising the eyebrows). The rapid facial signals can be further classified as conveying emotional, emblematic, manipulator, illustrator, and regulator messages [EKM92]. <p> In contrast, analyzing muscle actions requires the use of anatomic musculature models. Furthermore, the mapping of expressions into muscle actions is still not well developed; most of the available knowledge is applicable to the synthesis of facial expressions. 6 In <ref> [SCH84] </ref> Ekman argues that a human emotion lasts between 1/2-4 seconds and that a longer display of emotion may signify a false emotion (which may, of course, also be of interest). Our experiments use dense sequences (30 frames per second) to allow us to capture natural expressions over several frames.
Reference: [TER93] <author> D. Terzopoulos, and K. Waters, </author> <title> "Analysis and synthesis of facial image sequences using physical and anatomical models," </title> <journal> IEEE Transactions on PAMI, </journal> <volume> Vol. 15, </volume> <pages> 569-579, </pages> <year> 1993. </year>
Reference-contexts: We focus our attention on the appearance of the face from its near frontal image projection, without considering the underlying anatomic and musculature models and actions. In doing so we depart from the current trend in which facial expression analysis is based on recovering muscle actions <ref> [LI93, MAS91, TER93] </ref>. Research in psychology has indicated that at least six emotions are universally associated with distinct facial expressions. Several other emotions, and many combinations of emotions, have been studied but remain unconfirmed as universally distinguishable. <p> In their model, 46 AUs are responsible for expression control and 12 for gaze direction and orientation. The FACS model has been used to synthesize images of facial expressions, but only limited exploration of its use in analysis has been performed <ref> [LI93, MAS91, TER93] </ref>. Most psychological research on facial expressions has been conducted on "mug-shot" pictures that capture the subject's expression at its "apex" or peak [YOU89]. <p> The goal is to develop computational methods that use face region motions as cues for action recovery. 1.2 Related research in computer vision The problem of recognizing facial expressions has recently attracted attention in the computer vision community <ref> [LI93, MAS91, TER93] </ref>. <p> Furthermore, the optical flow was treated on a per-frame basis without considering the time-sequence of frames. The experiments considered the expressions of just one face and the results were compared with the performance of human subjects that were asked to classify the displayed emotions. Terzopoulos and Waters <ref> [TER93] </ref> proposed an approach to synthesis and analysis of facial expressions based on physical modeling of the muscles of the face. <p> Once the muscle contractions have been estimated they are resynthesized onto the 3D range data model of the subject to recreate the muscle contractions. It remains to be determined whether the computation of muscle contractions would be useful for facial expression recognition (the authors did not explore this in <ref> [TER93] </ref>). Li et al. [LI93] proposed an approach that analyzes facial images for resynthesis. They do not attempt to classify or reason about facial expressions and actions, although some aspects of their work might potentially contribute to that. <p> We chose not to model or analyze facial muscle actions, setting our work apart from recent approaches <ref> [LI93, MAS91, TER93] </ref>, as well as not to use models for muscle actions [EKM78]. Instead, we focus on the motions associated with the edges of the mouth, eyes, and eyebrows. These edges allow us to refer to the face features using natural linguistic terminology. <p> Their approach does not delineate the facial features but it designates rectangles that enclose the features. The only dynamic tracking algorithm for facial features we are aware of is that of Terzopoulos and Waters <ref> [TER93] </ref>. Eleven contours were manually located in the first image of the sequence and then tracked using deformable contour models that are pulled by the local minima of the intensity image.
Reference: [YAC93] <author> Y. Yacoob, and L.S. Davis, </author> <title> "Labeling of human face components from range data," </title> <booktitle> IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> 592-593, </pages> <year> 1993. </year>
Reference-contexts: We assume that, for each feature, we can initially compute a rectangular region that encloses it. Such an algorithm has been recently proposed for range data by Yacoob and Davis <ref> [YAC93] </ref> and a similar algorithm could be developed for intensity images (or stereo images), a problem we are currently working on. Our algorithm tracks these regions through the remainder of the sequence. The tracking is based on the localization of points with high gradient. <p> Their approach requires an accurate initial estimate of the location of the features, and might not be easily applied to facial features 11 such as the eye when it is being closed or opened, since the deformations of the template could become quite complex. Yacoob and Davis <ref> [YAC93] </ref> presented an approach to qualitatively localizing and labeling natural facial components from range data. They used a multi-stage diffusion process that classifies range points into relative convexities and concavities.
Reference: [YOU89] <author> A.W. Young and H.D. Ellis (Eds.), </author> <title> Handbook of Research on Face Processing, </title> <publisher> Elsevier Science Publishers, </publisher> <year> 1989. </year>
Reference-contexts: Visual communication has been extensively studied in the psychology literature, mainly as a means of describing the emotional, cognitive and physical states of subjects and the role they play in social interactions <ref> [BRU88, DAV75, EKM73, SCH84, YOU89] </ref>. Ekman and Friesen [EKM75] classified facial signals into three types: static (such as skin color), slow (such as permanent wrinkles), and rapid (such as raising the eyebrows). The rapid facial signals can be further classified as conveying emotional, emblematic, manipulator, illustrator, and regulator messages [EKM92]. <p> Regulators are non-verbal mediators such as nods and smiles. Our interest focuses here on recognizing emotions conveyed by human faces. This field was pioneered by Darwin's work [DAR1872] and has been extensively studied in psychology during the last twenty years <ref> [YOU89] </ref>. We focus our attention on the appearance of the face from its near frontal image projection, without considering the underlying anatomic and musculature models and actions. In doing so we depart from the current trend in which facial expression analysis is based on recovering muscle actions [LI93, MAS91, TER93]. <p> The FACS model has been used to synthesize images of facial expressions, but only limited exploration of its use in analysis has been performed [LI93, MAS91, TER93]. Most psychological research on facial expressions has been conducted on "mug-shot" pictures that capture the subject's expression at its "apex" or peak <ref> [YOU89] </ref>. These pictures allow one to detect the presence of static cues (such as wrinkles) as well as the positions and shapes of the facial features. Few studies have directly investigated the influence of the motion and deformation of facial features on the recognition of facial expressions.
Reference: [YUI89] <author> A.L. Yuille, D.S. Cohen, and P.W. Hallinan, </author> <title> "Feature extraction from faces using de-formable templates," </title> <booktitle> IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> 104-109, </pages> <year> 1989. </year>
Reference-contexts: Iden tifying and computing motion of AUs is a difficult task in most situations. * Features can be identified from the 2-D image without recovering a 3-D face model. 4.2 Spatio-temporal face feature tracking Yuille et al. <ref> [YUI89] </ref> used deformable templates and an energy minimization approach to localize and approximate the eyes and mouth in intensity face images.
References-found: 19

