URL: http://www.cse.ogi.edu/DISC/projects/ereq/papers/bennet-diss.ps.gz
Refering-URL: http://www.cse.ogi.edu/~bennet/
Root-URL: http://www.cse.ogi.edu
Title: Join-order Optimization with Cartesian Products  
Author: Bennet Vance 
Degree: 1976 M.S., Stanford University, 1981 A dissertation submitted to the faculty of the Oregon Graduate Institute of Science and Technology in partial fulfillment of the requirements for the degree Doctor of Philosophy in Computer Science and Engineering  
Date: January 1998  
Affiliation: B.A., Yale University,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Gennady Antoshenkov. </author> <title> Query processing in DEC Rdb: Major issues and future challenges. </title> <journal> Database Engineering, </journal> <volume> 16(4) </volume> <pages> 42-52, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: However, there are certainly situations in which inaccurate cost estimates have adverse effects, and research since the time of System R has sought to improve the quality of car-dinality estimates (and hence cost estimates) through a variety of sophisticated techniques [5, 14, 31, 35, 47]. Antoshenkov <ref> [1] </ref> goes further, and cites instability in cardinality computations as grounds for rejecting point-valued estimates altogether.
Reference: [2] <author> Andrea Asperti and Giuseppe Longo. </author> <title> Categories, Types, and Structures. </title> <publisher> The MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: in the topology of this space.) It is difficult to construct a genuinely object-oriented analogue of the relational join operator without giving up commutativity and associativity. 1 But it is trivial to construct an analogue that is commutative and associative up to isomorphism, in the manner of a categorical product <ref> [2, 48] </ref>. That is, one can construct an object join ffi 1 such that if A, B, 1 Analogues have been proposed that retain these properties; see, for example, the work of Shaw and Zdonik [52].
Reference: [3] <institution> Roberto Bayardo, IBM Almaden Research Center, </institution> <address> San Jose, </address> <institution> California. </institution> <type> Personal communication, </type> <month> January </month> <year> 1996. </year>
Reference-contexts: The benefits of these thresholds diminish when the thresholds fail to give a fairly tight bound on the cost of the optimal plan. One way to obtain a fairly tight plan-cost threshold, suggested by Roberto Bayardo <ref> [3] </ref>, would be to precede the exhaustive-search optimization with a brief run of a stochastic join-optimization method such as iterative improvement. An alternative, more brute-force strategy is to use multiple optimization passes with successively larger thresholds, as described in Section 7.2 above, but with much finer spacing between the thresholds.
Reference: [4] <author> Keith Billings. </author> <title> A TPC-D model for query optimization in Cascades. </title> <type> Master's thesis, </type> <institution> Portland State University. </institution> <note> In preparation. </note>
Reference-contexts: For example, Kabra and DeWitt [28] present join-optimization timings that show that when emulating Volcano's search strategy, OPT++ performs nearly identically to Volcano itself. No such direct comparisons are available for Cascades and EROC, but both have performed well when applied to the TPC/D benchmark queries <ref> [4, 41] </ref>. 2.7.6 Summary In this section we have discussed the approaches to join-order optimization that appear in the literature: dynamic programming, rule-based optimization, heuristic and sequencing techniques, stochastic techniques, and hybrids.
Reference: [5] <author> Stavros Christodoulakis. </author> <title> Estimating block transfers and join sizes. </title> <booktitle> In SIGMOD '83, Proceedings of Annual Meeting, Database Week, </booktitle> <address> San Jose, </address> <month> May 23-26, </month> <year> 1983, </year> <pages> pages 40-54, </pages> <year> 1983. </year>
Reference-contexts: However, there are certainly situations in which inaccurate cost estimates have adverse effects, and research since the time of System R has sought to improve the quality of car-dinality estimates (and hence cost estimates) through a variety of sophisticated techniques <ref> [5, 14, 31, 35, 47] </ref>. Antoshenkov [1] goes further, and cites instability in cardinality computations as grounds for rejecting point-valued estimates altogether.
Reference: [6] <author> Sophie Cluet and Guido Moerkotte. </author> <title> On the complexity of generating optimal left-deep processing trees with cross products. </title> <booktitle> In Database Theory|ICDT '95, 5th International Conference, </booktitle> <address> Prague, Czech Republic, </address> <month> January 11-13, </month> <year> 1995, </year> <booktitle> Proceedings, volume 893 of Lecture Notes in Computer Science, </booktitle> <pages> pages 54-67. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: But there are special cases where join-order optimization without Cartesian products has merely polynomial complexity [45], whereas the complexity of join-order optimization with Cartesian products is always exponential <ref> [6, 49] </ref>. In these special cases, our exhaustive-search method becomes uncompetitive. Yet we cling tenaciously to our claim that Cartesian products need not be excluded.
Reference: [7] <author> Richard Cole, Mark J. Anderson, and Robert J. Bestgen. </author> <title> Query processing in the IBM Application System 400. </title> <journal> Database Engineering, </journal> <volume> 16(4) </volume> <pages> 19-28, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: of (2.54) should turn out to be optimal, the redundant predicate will have played a vital role|for without this predicate, the optimizer would have produced an inferior "optimum." The potential benefits of redundant predicates are significant enough that many query processing systems, far from eliminating redundancies, actively seek them out <ref> [7, 16] </ref>. These systems analyze the predicates that appear explicitly in a user query, and then construct additional predicates that can be inferred from the explicit ones. Because of such policies, it is essential that an optimizer be prepared to make adjustments for predicate redundancies when carrying out cardinality estimation.
Reference: [8] <author> George Copeland and David Maier. </author> <title> Making Smalltalk a database system. </title> <booktitle> In SIGMOD '84, Proceedings of Annual Meeting, </booktitle> <address> Boston, Massachusetts, </address> <month> June 18-21, </month> <year> 1984, </year> <pages> pages 316-325, </pages> <year> 1984. </year>
Reference-contexts: For example, queries that make use of views [11, 32] often generate hidden joins, as do queries with path expressions <ref> [8, 53, 62] </ref>. As databases become more complex, and as they incorporate additional facilities that automatically generate joins "underneath the covers," queries with large values of n are likely to become more and more common.
Reference: [9] <author> Patrick Cousot and Radhia Cousot. </author> <title> Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints. </title> <booktitle> In Conference Record of the Fourth ACM Symposium on Principles of Programming Languages, Papers Presented at the Symposium, </booktitle> <address> Los Angeles, California, </address> <month> January 17-19, </month> <year> 1977, </year> <pages> pages 238-252, </pages> <year> 1977. </year> <pages> 262 263 </pages>
Reference-contexts: It is possible to interpret (2.60) in a way that does not necessitate full evaluation of the relations. We may regard the denotation brackets [[]] as designating an abstract interpretation <ref> [9] </ref> of the expression they surround, and we may take the arguments of the cost function to be not relations, but partial representations of relations.
Reference: [10] <author> Scott Daniels, Goetz Graefe, Thomas Keller, David Maier, Duri Schmidt, and Ben-net Vance. </author> <title> Query optimization in Revelation, an overview. </title> <journal> Database Engineering, </journal> <volume> 14(2) </volume> <pages> 58-62, </pages> <month> June </month> <year> 1991. </year>
Reference: [11] <author> Ramez Elmasri and Shamkant B. Navathe. </author> <title> Fundamentals of Database Systems. </title> <address> Ben-jamin/Cummings, </address> <year> 1989. </year>
Reference-contexts: Recall that to retrieve information from a relational database, one ordinarily poses a query expressed in some variant of the language SQL (Structured Query Language) <ref> [11, 32] </ref>. <p> Values of n in this range can arise when users submit SQL queries with large numbers of relations in the FROM clause, but they can also arise in other ways that users may not even be aware of. For example, queries that make use of views <ref> [11, 32] </ref> often generate hidden joins, as do queries with path expressions [8, 53, 62]. As databases become more complex, and as they incorporate additional facilities that automatically generate joins "underneath the covers," queries with large values of n are likely to become more and more common. <p> As the Source relation shows, the spark plugs can be obtained from any of four suppliers: Acme, Jolt, Nuke, or Zap. 2.2 The Relational Algebra The relational algebra <ref> [11, 32, 38] </ref> is an algebraic language for manipulating relations to obtain new relations. There are six fundamental operators in the relational algebra: select (), project (), Cartesian product (fi), set union ([), set difference (n or ), and rename ( or ffi).
Reference: [12] <author> Cesar Galindo-Legaria, Arjan Pellenkoft, and Martin Kersten. </author> <title> Fast, </title> <booktitle> randomized join-order selection|why use transformations? In Proceedings of the 20th International Conference on Very Large Data Bases, </booktitle> <month> September 12-15, </month> <year> 1994, </year> <title> Santiago, </title> <booktitle> Chile, </booktitle> <pages> pages 85-95, </pages> <year> 1994. </year>
Reference-contexts: The lack of a single, commonly accepted, and well-defined benchmark for join-order optimization makes it difficult to compare different algorithms on the basis of speed and effectiveness. Presentations of new algorithms in the literature generally do include such comparisons with earlier algorithms <ref> [12, 59] </ref>; producing these comparisons necessitates reimplementing the earlier algorithms [12] unless pre-existing implementations are readily available. This reimplementation effort is unfortunate both because of the labor it entails, and because of the danger that it could introduce performance bugs (or indeed other bugs) in some of the algorithms involved. <p> Presentations of new algorithms in the literature generally do include such comparisons with earlier algorithms [12, 59]; producing these comparisons necessitates reimplementing the earlier algorithms <ref> [12] </ref> unless pre-existing implementations are readily available. This reimplementation effort is unfortunate both because of the labor it entails, and because of the danger that it could introduce performance bugs (or indeed other bugs) in some of the algorithms involved. Such bugs could conceivably go undetected. <p> Transformationless Random Probing The transformationless random probing technique of Galindo-Legaria et al. <ref> [12] </ref> picks each point to probe entirely at random. In other words, unlike most techniques, it is not constrained to probe in the neighborhood of the last point it has probed. <p> In particular, the continuous functions in their model may differ in important ways from the corresponding functions on actual plan spaces, which are discrete domains [37]. In this connection it is also worth noting an observation made by Swami [57] and by Galindo-Legaria et al. <ref> [12] </ref> regarding the proportion of points in query-plan space whose cost is close to that of the global minimum. This proportion tends to decline as the number of relations n increases. This effect suggests that at larger n, the plan space is indeed pockmarked with crevasses of some sort.
Reference: [13] <author> Cesar A. </author> <type> Galindo-Legaria, </type> <institution> Microsoft Corporation, Redmond, Washington. </institution> <type> Personal communication, </type> <month> June </month> <year> 1996. </year>
Reference-contexts: However, path 3 differs more fundamentally from paths 1 and 2, and entails duplication of effort. To circumvent this kind of duplication of effort, the present author [60] and Galindo-Legaria and his colleagues <ref> [13, 46] </ref> have independently devised formulations of join commutativity and associativity under which transformation paths obey the following property: The path from an expression E to another expression E 0 , if one exists, is unique up to reordering of independent transformation steps.
Reference: [14] <author> Sumit Ganguly, Phillip B. Gibbons, Yossi Matias, and Avi Silberschatz. </author> <title> Bifocal sampling for skew-resistant join size estimation. </title> <booktitle> In 1996 Proceedings, ACM SIGMOD International Conference on Management of Data, </booktitle> <address> June 4 to 6, Montreal, Quebec, Canada, </address> <pages> pages 271-281, </pages> <year> 1996. </year>
Reference-contexts: However, there are certainly situations in which inaccurate cost estimates have adverse effects, and research since the time of System R has sought to improve the quality of car-dinality estimates (and hence cost estimates) through a variety of sophisticated techniques <ref> [5, 14, 31, 35, 47] </ref>. Antoshenkov [1] goes further, and cites instability in cardinality computations as grounds for rejecting point-valued estimates altogether.
Reference: [15] <author> Sumit Ganguly, Waqar Hasan, and Ravi Krishnamurthy. </author> <title> Query optimization for parallel execution. </title> <booktitle> In Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data, </booktitle> <address> San Diego, California, </address> <month> June 2-5, </month> <year> 1992, </year> <pages> pages 9-18, </pages> <year> 1992. </year>
Reference-contexts: In the time since the work of Ono and Lohman, there may well have been variations on their implementation that actually achieved O (3 n ) time complexity. The literature is inconclusive. Ganguly et al. <ref> [15] </ref> give O (3 n ) as the complexity of bushy join-order optimization, but again this complexity figure is based on the number of join expressions considered, not on an algorithmic analysis. No evidence is presented of an implementation that in fact achieves the stated complexity. <p> Thus, in supposing B 1 (A 1 C) to be suboptimal, we have arrived at a contradiction; it follows that B 1 (A 1 C) is optimal after all. The necessity of optimality in all subplans of an optimal plan is referred to as the principle of optimality <ref> [15, 24] </ref>. Unfortunately, the principle of optimality breaks down when costs are influenced by physical properties.
Reference: [16] <author> Peter Gassner, Guy Lohman, and K. Bernhard Schiefer. </author> <title> Query optimization in the IBM DB2 family. </title> <journal> Database Engineering, </journal> <volume> 16(4) </volume> <pages> 4-18, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: of (2.54) should turn out to be optimal, the redundant predicate will have played a vital role|for without this predicate, the optimizer would have produced an inferior "optimum." The potential benefits of redundant predicates are significant enough that many query processing systems, far from eliminating redundancies, actively seek them out <ref> [7, 16] </ref>. These systems analyze the predicates that appear explicitly in a user query, and then construct additional predicates that can be inferred from the explicit ones. Because of such policies, it is essential that an optimizer be prepared to make adjustments for predicate redundancies when carrying out cardinality estimation.
Reference: [17] <author> Goetz Graefe. </author> <title> Query evaluation techniques for large databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(2) </volume> <pages> 73-170, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Query execution is often pipelined <ref> [17] </ref>, meaning that successive operators run concurrently, and not in the strictly serial fashion suggested above. The primary motivation for pipelining is to avoid storing intermediate results. Without pipelining, an intermediate result is first produced as the output of one operator, and subsequently consumed as the input of another operator. <p> It is therefore interesting to look at actual join-optimization timings for systems that use the Volcano and Starburst algorithms as reported in some of the recent literature. The measurements for Volcano <ref> [17] </ref> and OPT++ [28] show optimization timings in the range of seconds for a join of eight relations. <p> Optimization time for a ten-way join runs to tens of seconds, as reported by Kabra and DeWitt [28], and as can be inferred by extrapolation from the performance graph given by Graefe and McKenna <ref> [17] </ref>. Kabra and DeWitt also graph timings for bushy join optimization allowing Cartesian products; from the standpoint of complexity, these timings do reflect the worst case, and run to hundreds of seconds for a ten-way join.
Reference: [18] <author> Goetz Graefe. </author> <title> The Cascades framework for query optimization. </title> <journal> Database Engineering, </journal> <volume> 18(3) </volume> <pages> 19-29, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: At the same time, optimizer generators in the style of Volcano lack flexibility inasmuch as they impose a fixed, transformational search strategy. Several newer optimizer frameworks seek to overcome some of the limitations of optimizer generators in the style of Volcano. Cascades <ref> [18] </ref>, OPT++ [28], and EROC [41] are three such frameworks that have several characteristics in common, as well as many individual differences. Among the characteristics they share are the following: * All are constructed out of C++ classes.
Reference: [19] <author> Goetz Graefe and David J. DeWitt. </author> <title> The Exodus optimizer generator. </title> <booktitle> In Proceedings of Association for Computing Machinery Special Interest Group on Management of Data 1987, Annual Conference, </booktitle> <address> San Francisco, </address> <month> May 27-29, </month> <year> 1987, </year> <pages> pages 160-172, </pages> <year> 1987. </year>
Reference-contexts: In the worst case, it discards many more joins 60 than it retains. Appendix A gives the details of the author's analysis. 2.7.2 Rule-based Optimization Principle of Rule-based Optimization and Application to Join Optimization Rule-based optimizers such as Exodus <ref> [19] </ref> and Volcano [20, 40] strive for extensibility through a general-purpose search mechanism that can easily be reconfigured to accommodate new operators and new transformation rules.
Reference: [20] <author> Goetz Graefe and William J. McKenna. </author> <title> The Volcano optimizer generator: Extensibility and efficient search. </title> <booktitle> In Proceedings of the Ninth International Conference on Data Engineering, </booktitle> <address> April 19-23, 1993, Vienna, Austria, </address> <pages> pages 209-218, </pages> <year> 1993. </year> <month> 264 </month>
Reference-contexts: In the worst case, it discards many more joins 60 than it retains. Appendix A gives the details of the author's analysis. 2.7.2 Rule-based Optimization Principle of Rule-based Optimization and Application to Join Optimization Rule-based optimizers such as Exodus [19] and Volcano <ref> [20, 40] </ref> strive for extensibility through a general-purpose search mechanism that can easily be reconfigured to accommodate new operators and new transformation rules. Configuration is accomplished through a special rule-definition file, or through a collection of functions that define the operators and transformation rules of the query algebra. <p> In effect, Volcano's memo structures store representations of the "feasible" joins in the sense of Ono and Lohman, and each such join is represented exactly once. It follows that space complexity for bushy join-order optimization in Volcano is O (3 n ) in the worst case. McKenna <ref> [20, 40] </ref> carried out extensive empirical studies of Volcano performance, which proved to be roughly comparable to that of Starburst. <p> Reproducing the experimental conditions of these various approaches may involve guesswork; in some instances, the published 148 description of the measurement methodology is evidently intended only to give a sense of the approach, not to permit duplication of the experiments <ref> [20, 28] </ref>. <p> The locality properties of the Blitzsplit algorithm make it especially well-suited to taking advantage of shared representations of plan information. But the details of the pertinent data structures, and the corresponding cost-analysis code, remain to be worked out. 10.2 Top-down vs. Bottom-up McKenna and Graefe <ref> [20, 40] </ref> have argued that the top-down, memoizing style of dynamic programming used in Volcano is more efficient than bottom-up dynamic programming. 259 (Volcano's transformation-based plan generation may also offer an advantage in flexibility (e.g., in accommodating new query operators), but here we shall address only the efficiency issue.) The core
Reference: [21] <author> Ronald L. Graham, Donald E. Knuth, and Oren Patashnik. </author> <title> Concrete Mathematics. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference: [22] <author> Joseph M. Hellerstein and Michael Stonebraker. </author> <title> Predicate migration: Optimizing queries with expensive predicates. </title> <booktitle> In Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data, </booktitle> <address> Washington, DC, </address> <month> May 26-28, </month> <year> 1993, </year> <pages> pages 267-276, </pages> <year> 1993. </year>
Reference-contexts: However, those alternatives could be preferable to the original expression if p or q involve expensive computations. Hellerstein and Stonebraker <ref> [22] </ref> describe a cost-based predicate-placement technique that achieves huge gains when expensive predicates are deferred. We have not investigated the applicability of their technique in the context of the Blitzsplit algorithm.
Reference: [23] <author> Robert V. Hogg and Allen T. Craig. </author> <title> Introduction to Mathematical Statistics. </title> <journal> Macmil-lan, </journal> <note> 3rd edition, </note> <year> 1970. </year>
Reference-contexts: A second feature is the spread of the cardinalities|the ratio between the maximum and minimum among them. Let us see what we can learn by varying just these two features. (Note that what we call the spread of the cardinalities is closely related to the statistical concept of variance <ref> [23] </ref>.
Reference: [24] <author> Ellis Horowitz and Sartaj Sahni. </author> <title> Fundamentals of Computer Algorithms. </title> <publisher> Computer Science Press, </publisher> <year> 1978. </year>
Reference-contexts: Thus, in supposing B 1 (A 1 C) to be suboptimal, we have arrived at a contradiction; it follows that B 1 (A 1 C) is optimal after all. The necessity of optimality in all subplans of an optimal plan is referred to as the principle of optimality <ref> [15, 24] </ref>. Unfortunately, the principle of optimality breaks down when costs are influenced by physical properties.
Reference: [25] <author> Toshihide Ibaraki and Tiko Kameda. </author> <title> On the optimal nesting order for computing N - relational joins. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 9(3) </volume> <pages> 482-502, </pages> <month> September </month> <year> 1984. </year>
Reference-contexts: Even so, the time complexity of join-order optimization remains exponential. Ibaraki and Kameda have shown the problem to be NP-complete <ref> [25] </ref>. 3 1.2 Join-order Optimization in Practice The intractability of join-order optimization leads to a dilemma: To optimize, or not to optimize? On the one hand, if one does not optimize, query evaluation may take an inordinately long time; in this case, we shall refer to the query evaluation as having <p> Remarkably, however, there is an optimization technique that, with essentially no search, can directly construct optimal join plans|though only under special conditions. We shall refer to this technique and its derivatives as sequencing techniques. The original application of sequencing to join-order optimization, which is due to Ibaraki and Kameda <ref> [25] </ref>, imposes the following restrictions: * The join graph must be acyclic. * Only left-deep plans are considered. * Cartesian products are excluded. * The cost function must be expressible as (R out ; R lhs ; R rhs ) = jR lhs j g p (R rhs ) for some
Reference: [26] <author> Yannis E. Ioannidis and Younkyung Cha Kang. </author> <title> Left-deep vs. bushy trees: An analysis of strategy spaces and its implications for query optimization. </title> <booktitle> In Proceedings of the 1991 ACM SIGMOD International Conference on Management of Data, </booktitle> <address> Denver, Colorado, </address> <month> May 29-31, </month> <year> 1991, </year> <pages> pages 168-177, </pages> <year> 1991. </year>
Reference-contexts: Much work on stochastic join-order optimization has focused on the space of left-deep join expressions <ref> [26, 56] </ref>; and regardless of any other tactics they employ, nearly all join-order optimizers exclude Cartesian products. <p> There may be other deep local minima that are far better. (Theoretically, simulated annealing can be parameterized so that it finds a global minimum with probability 1; but the time required may be astronomical.) Two-phase Optimization Two-phase optimization or 2PO <ref> [26, 54] </ref> attempts to combine the advantages of iterative improvement and simulated annealing. In the first phase, a number of iterations of iterative improvement are performed so as to obtain a low point among the shallow minima. <p> Ioannidis and Kang <ref> [26] </ref> argue that large classes of query plan spaces are essentially bowl-shaped. But this observation does not apply universally, and does not rule out pathologies of the sort we have discussed. <p> Other hybrids are also imaginable. For example, the simulated-annealing phase of two-phase optimization <ref> [26] </ref> could be replaced by iterated tightening; genetic algorithms [54] could be used to recombine plans obtained in successive bushwhack iterations; and so on.
Reference: [27] <author> Yannis E. Ioannidis and Eugene Wong. </author> <title> Query optimization by simulated annealing. </title> <booktitle> In Proceedings of Association for Computing Machinery Special Interest Group on Management of Data 1987, Annual Conference, </booktitle> <address> San Francisco, </address> <month> May 27-29, </month> <year> 1987, </year> <pages> pages 9-22, </pages> <year> 1987. </year>
Reference-contexts: Stochastic optimization of join orders was first investigated by Ioannidis and Wong <ref> [27] </ref>, and many variations on the theme have been proposed since. Stochastic techniques cannot guarantee optimality, but often they can generate high-quality plans for moderate effort, and with few restrictions. <p> The algorithm therefore requires a very large number of repetitions to stand a good chance of finding a deep minimum. Simulated Annealing Simulated annealing <ref> [27, 54, 58] </ref> partially overcomes the difficulty encountered by iterative improvement in the problem at hand. The simulated-annealing algorithm is somewhat similar to iterative improvement, but rather than methodically climbing downhill at each step, this algorithm permits both downhill and uphill moves.
Reference: [28] <author> Navin Kabra and David J. DeWitt. </author> <title> Opt++: An object-oriented implementation for extensible database query optimization, </title> <note> 1995. Available: http://www.cs.wisc.edu/~navin/research/opt++.ps [September 26, </note> <year> 1997]. </year>
Reference-contexts: At the same time, optimizer generators in the style of Volcano lack flexibility inasmuch as they impose a fixed, transformational search strategy. Several newer optimizer frameworks seek to overcome some of the limitations of optimizer generators in the style of Volcano. Cascades [18], OPT++ <ref> [28] </ref>, and EROC [41] are three such frameworks that have several characteristics in common, as well as many individual differences. Among the characteristics they share are the following: * All are constructed out of C++ classes. <p> However, these frameworks have been designed so as to provide competitive performance in the optimization of joins of moderate numbers of relations. For example, Kabra and DeWitt <ref> [28] </ref> present join-optimization timings that show that when emulating Volcano's search strategy, OPT++ performs nearly identically to Volcano itself. <p> It is therefore interesting to look at actual join-optimization timings for systems that use the Volcano and Starburst algorithms as reported in some of the recent literature. The measurements for Volcano [17] and OPT++ <ref> [28] </ref> show optimization timings in the range of seconds for a join of eight relations. Optimization time for a ten-way join runs to tens of seconds, as reported by Kabra and DeWitt [28], and as can be inferred by extrapolation from the performance graph given by Graefe and McKenna [17]. <p> The measurements for Volcano [17] and OPT++ <ref> [28] </ref> show optimization timings in the range of seconds for a join of eight relations. Optimization time for a ten-way join runs to tens of seconds, as reported by Kabra and DeWitt [28], and as can be inferred by extrapolation from the performance graph given by Graefe and McKenna [17]. <p> Reproducing the experimental conditions of these various approaches may involve guesswork; in some instances, the published 148 description of the measurement methodology is evidently intended only to give a sense of the approach, not to permit duplication of the experiments <ref> [20, 28] </ref>.
Reference: [29] <author> Donald E. Knuth. </author> <title> Fundamental Algorithms, </title> <booktitle> volume 1 of The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley, </publisher> <address> 2nd edition, </address> <year> 1973. </year>
Reference-contexts: The process of choosing from among the available alternatives is called join-order optimization (or simply join optimization). There are, as noted, 12 alternatives in the case of a three-way join (i.e., a join involving three relations). In general, for an n-way join <ref> [29, 31, 54] </ref>, the number of alternatives is (2n 2)! : (1.4) This quantity grows at an explosive, faster-than-exponential rate. <p> is cond count = n X m H 2 m : (3.6) Once again, to simplify we change the bounds of the summation, giving cond count n X m H 2 m : (3.7) Using the fact H k ln k + fl, where fl = 0:57721 : : : <ref> [29] </ref>, we obtain cond count n X m (ln 2 m + fl); (3.8) which expands to n X m m ln 2 + m=0 n or n X m m + fl m=0 n Now recall that P n n = 2 n , and that P n n m
Reference: [30] <author> Robert Kooi and Derek Frankforth. </author> <title> Query optimization in Ingres. </title> <journal> Database Engineering, </journal> <volume> 5(3) </volume> <pages> 2-5, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: We reason as follows: If a query is expected to execute quickly, then it is imperative that it also be optimized quickly, for in general one would like optimization time to be smaller than (preferably, much smaller than) execution time <ref> [30] </ref>. By the same token, if the query execution will be extremely long, then it is probably acceptable to allow more time for optimization, as long as the optimization time is still small compared to the execution time.
Reference: [31] <author> Robert Philip Kooi. </author> <title> The Optimization of Queries in Relational Databases. </title> <type> PhD thesis, </type> <institution> Case Western Reserve University, </institution> <year> 1980. </year>
Reference-contexts: The process of choosing from among the available alternatives is called join-order optimization (or simply join optimization). There are, as noted, 12 alternatives in the case of a three-way join (i.e., a join involving three relations). In general, for an n-way join <ref> [29, 31, 54] </ref>, the number of alternatives is (2n 2)! : (1.4) This quantity grows at an explosive, faster-than-exponential rate. <p> However, there are certainly situations in which inaccurate cost estimates have adverse effects, and research since the time of System R has sought to improve the quality of car-dinality estimates (and hence cost estimates) through a variety of sophisticated techniques <ref> [5, 14, 31, 35, 47] </ref>. Antoshenkov [1] goes further, and cites instability in cardinality computations as grounds for rejecting point-valued estimates altogether.
Reference: [32] <author> Henry F. Korth and Abraham Silberschatz. </author> <title> Database System Concepts. </title> <publisher> McGraw-Hill, </publisher> <address> 2nd edition, </address> <year> 1991. </year> <month> 265 </month>
Reference-contexts: Recall that to retrieve information from a relational database, one ordinarily poses a query expressed in some variant of the language SQL (Structured Query Language) <ref> [11, 32] </ref>. <p> Values of n in this range can arise when users submit SQL queries with large numbers of relations in the FROM clause, but they can also arise in other ways that users may not even be aware of. For example, queries that make use of views <ref> [11, 32] </ref> often generate hidden joins, as do queries with path expressions [8, 53, 62]. As databases become more complex, and as they incorporate additional facilities that automatically generate joins "underneath the covers," queries with large values of n are likely to become more and more common. <p> As the Source relation shows, the spark plugs can be obtained from any of four suppliers: Acme, Jolt, Nuke, or Zap. 2.2 The Relational Algebra The relational algebra <ref> [11, 32, 38] </ref> is an algebraic language for manipulating relations to obtain new relations. There are six fundamental operators in the relational algebra: select (), project (), Cartesian product (fi), set union ([), set difference (n or ), and rename ( or ffi).
Reference: [33] <author> Ravi Krishnamurthy, Haran Boral, and Carlo Zaniolo. </author> <title> Optimization of nonrecursive queries. </title> <booktitle> In Proceedings of the Twelfth International Conference on Very Large Data Bases, </booktitle> <address> Kyoto, Japan, </address> <month> August 25-28, </month> <year> 1986, </year> <pages> pages 128-137, </pages> <year> 1986. </year>
Reference-contexts: Thus, the sequencing algorithm is applied n times, and so the net time complexity of join-order optimization by this technique is O (n 2 log n). Krishnamurthy, Boral, and Zaniolo <ref> [33] </ref> subsequently noticed that a portion of the computation in each of the n applications of the sequencing algorithm was redundant. By 65 eliminating this redundancy, they obtained an O (n 2 ) algorithm for join-order optimization, given the same restrictions as in the method of Ibaraki and Kameda.
Reference: [34] <author> Rosana S. G. Lanzelotte, Patrick Valduriez, Mohamed Zat, and Mikal Ziane. </author> <title> Industrial-strength parallel query optimization: Issues and lessons. </title> <journal> Information Systems, </journal> <volume> 19(4) </volume> <pages> 311-330, </pages> <year> 1994. </year>
Reference-contexts: A join at processing unit t will be denoted t k inputs to a given join operation need not be computed (or reside) at the location where 1 This example is adapted from Lanzelotte et al. <ref> [34] </ref>. 57 the join will take place, but remote inputs will incur a communication penalty that will drive up the estimated cost of the join.
Reference: [35] <author> Richard J. Lipton, Jeffrey F. Naughton, and Donovan A. Schneider. </author> <title> Practical selectivity estimation through adaptive sampling. </title> <booktitle> In Proceedings of the 1990 ACM SIGMOD International Conference on Management of Data, </booktitle> <address> May 23-25, 1990, Atlantic City, New Jersey, </address> <pages> pages 1-11, </pages> <year> 1990. </year>
Reference-contexts: However, there are certainly situations in which inaccurate cost estimates have adverse effects, and research since the time of System R has sought to improve the quality of car-dinality estimates (and hence cost estimates) through a variety of sophisticated techniques <ref> [5, 14, 31, 35, 47] </ref>. Antoshenkov [1] goes further, and cites instability in cardinality computations as grounds for rejecting point-valued estimates altogether.
Reference: [36] <author> C. L. Liu. </author> <title> Introduction to Combinatorial Mathematics. </title> <publisher> McGraw-Hill, </publisher> <year> 1968. </year>
Reference-contexts: m m ln 2 + m=0 n or n X m m + fl m=0 n Now recall that P n n = 2 n , and that P n n m = (n=2)2 n . (For an explanation and interpretation of the former identity, see the text by Liu <ref> [36] </ref>; the latter identity can be derived from the former through straightforward manipulation.) Applying these identities to (3.10) finally yields cond count (ln 2=2)n2 n + fl2 n : (3.11) Let us disregard the fl2 n term; it is relatively small, and in any event, its effect on execution time can
Reference: [37] <author> Guy M. </author> <type> Lohman, </type> <institution> IBM Almaden Research Center, </institution> <address> San Jose, </address> <institution> California. </institution> <type> Personal communication, </type> <month> April </month> <year> 1996. </year>
Reference-contexts: Moreover, the results of Ioannidis and Kang depend on a mathematical model whose conformity to actual query-plan spaces is not well-established. In particular, the continuous functions in their model may differ in important ways from the corresponding functions on actual plan spaces, which are discrete domains <ref> [37] </ref>. In this connection it is also worth noting an observation made by Swami [57] and by Galindo-Legaria et al. [12] regarding the proportion of points in query-plan space whose cost is close to that of the global minimum.
Reference: [38] <author> David Maier. </author> <title> The Theory of Relational Databases. </title> <publisher> Computer Science Press, </publisher> <year> 1983. </year>
Reference-contexts: As the Source relation shows, the spark plugs can be obtained from any of four suppliers: Acme, Jolt, Nuke, or Zap. 2.2 The Relational Algebra The relational algebra <ref> [11, 32, 38] </ref> is an algebraic language for manipulating relations to obtain new relations. There are six fundamental operators in the relational algebra: select (), project (), Cartesian product (fi), set union ([), set difference (n or ), and rename ( or ffi). <p> The empty predicate may be thought of as a conjunction with zero conjuncts, and hence as vacuously true. By a literal reading, therefore, the expression A 1 B must be taken as synonymous with A fi B <ref> [38] </ref>. <p> In the framework of join graphs, ternary predicates correspond to hyperedges|edges with possibly more than two "endpoints"; graphs containing such edges are called hypergraphs <ref> [38] </ref>. So to support ternary, and more generally, n-ary predicates, an optimizer would have to accept join hypergraphs as its input. Hypergraphs have a variety of applications in query processing besides the representation of n-ary predicates.
Reference: [39] <author> O. Martin and S. Otto. </author> <title> Combining simulated annealing with local search heuristics. </title> <journal> Annals of Operations Research, </journal> <volume> 63 </volume> <pages> 57-75, </pages> <year> 1996. </year>
Reference-contexts: Second, methods analogous to local improvement have proved effective in problem domains unrelated to query processing, and may have something to teach us. The present work on stochastic optimization takes its inspiration largely from the combinatorial-optimization work of Martin and Otto <ref> [39] </ref>, who have devised a stochastic technique they call Chained Local Optimization. They have applied this technique with astonishing success to a variety of well-known intractable problems such as the Traveling Salesman Problem. <p> In effect, the recursive calls provide the "kick" that is called for in the Chained Local Optimization technique of Martin and Otto <ref> [39] </ref> (cf. Section 8.1). Later in this chapter we shall present experimental results on the Recursive Bushwhack algorithm's performance. 9.8 Varying the Join Graph Up to this point we have been examining the behavior of the Stochastic Bushwhack algorithm only for queries of the cycle + 3 topology.
Reference: [40] <author> William J. McKenna. </author> <title> Efficient Search in Extensible Database Query Optimization: The Volcano Optimizer Generator. </title> <type> PhD thesis, </type> <institution> University of Colorado, Boulder, </institution> <year> 1993. </year>
Reference-contexts: In the worst case, it discards many more joins 60 than it retains. Appendix A gives the details of the author's analysis. 2.7.2 Rule-based Optimization Principle of Rule-based Optimization and Application to Join Optimization Rule-based optimizers such as Exodus [19] and Volcano <ref> [20, 40] </ref> strive for extensibility through a general-purpose search mechanism that can easily be reconfigured to accommodate new operators and new transformation rules. Configuration is accomplished through a special rule-definition file, or through a collection of functions that define the operators and transformation rules of the query algebra. <p> In effect, Volcano's memo structures store representations of the "feasible" joins in the sense of Ono and Lohman, and each such join is represented exactly once. It follows that space complexity for bushy join-order optimization in Volcano is O (3 n ) in the worst case. McKenna <ref> [20, 40] </ref> carried out extensive empirical studies of Volcano performance, which proved to be roughly comparable to that of Starburst. <p> But one must be cautious in interpreting these timings. Earlier studies of join-optimization performance suggest that when heavier-weight join-enumeration strategies are used, the effort of join enumeration is the limiting factor in join-optimization performance <ref> [40, 45] </ref>. Under those conditions, one can equally well measure performance using a simple cost computation or a more complicated one|the effort involved in the cost computations is dominated by the join-enumeration effort in either case. <p> The locality properties of the Blitzsplit algorithm make it especially well-suited to taking advantage of shared representations of plan information. But the details of the pertinent data structures, and the corresponding cost-analysis code, remain to be worked out. 10.2 Top-down vs. Bottom-up McKenna and Graefe <ref> [20, 40] </ref> have argued that the top-down, memoizing style of dynamic programming used in Volcano is more efficient than bottom-up dynamic programming. 259 (Volcano's transformation-based plan generation may also offer an advantage in flexibility (e.g., in accommodating new query operators), but here we shall address only the efficiency issue.) The core
Reference: [41] <author> William J. McKenna, Louis Burger, Chi Hoang, and Melissa Truong. Eroc: </author> <title> A toolkit for building Neato query optimizers. </title> <booktitle> In Proceedings of the Twenty-second International Conference on Very Large Data Bases, </booktitle> <month> September 3-6, </month> <year> 1996, </year> <title> Mumbai (Bom-bay), </title> <booktitle> India, </booktitle> <pages> pages 111-121, </pages> <year> 1996. </year>
Reference-contexts: At the same time, optimizer generators in the style of Volcano lack flexibility inasmuch as they impose a fixed, transformational search strategy. Several newer optimizer frameworks seek to overcome some of the limitations of optimizer generators in the style of Volcano. Cascades [18], OPT++ [28], and EROC <ref> [41] </ref> are three such frameworks that have several characteristics in common, as well as many individual differences. Among the characteristics they share are the following: * All are constructed out of C++ classes. <p> For example, Kabra and DeWitt [28] present join-optimization timings that show that when emulating Volcano's search strategy, OPT++ performs nearly identically to Volcano itself. No such direct comparisons are available for Cascades and EROC, but both have performed well when applied to the TPC/D benchmark queries <ref> [4, 41] </ref>. 2.7.6 Summary In this section we have discussed the approaches to join-order optimization that appear in the literature: dynamic programming, rule-based optimization, heuristic and sequencing techniques, stochastic techniques, and hybrids. <p> Kabra and DeWitt also graph timings for bushy join optimization allowing Cartesian products; from the standpoint of complexity, these timings do reflect the worst case, and run to hundreds of seconds for a ten-way join. The EROC timings <ref> [41] </ref> appear to be consistent with the Volcano and OPT++ timings, though direct comparison is problematical because of differences in the kinds of queries that were tested.
Reference: [42] <author> R. Milner, M. Tofte, and R. Harper. </author> <title> The Definition of Standard ML. </title> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: In the present section, we give an algorithm that carries out an individual tightening operation. In the next section, we present an algorithm that implements a particular policy of iterated tightening. In both instances, the presented code is in pseudo-ML, and is abstracted from our implementation in Standard ML <ref> [42] </ref>. 210 Earlier, in presenting the Blitzsplit algorithm, we used imperative pseudo-code and discussed an imperative implementation, because updates of the dynamic programming table were an essential ingredient of the algorithm, and because low-level features of C enabled us to fine-tune the implementation. Here we face somewhat different considerations.
Reference: [43] <author> Priti Mishra and Margaret H. Eich. </author> <title> Join processing in relational databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(1) </volume> <pages> 63-113, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Join-processing algorithms provide an efficient alternative. Where applicable, such algorithms can compute A 1 p B directly, bypassing the Cartesian product computation. (Join-processing algorithms are almost always applicable when A 1 p B is an equijoin; in other cases, they may not be.) There are numerous join-processing algorithms <ref> [43] </ref>; among the most widely used are hash join, merge join, and index-nested-loops join. <p> The best plan also depends on what cost model is used to estimate the expected CPU and disk time consumption of alternative plans. As we have noted, there are many join-processing algorithms <ref> [43] </ref>, and as many models (or more) for estimating their costs.
Reference: [44] <author> Kiyoshi Ono and Guy M. Lohman. </author> <title> Extensible enumeration of feasible joins for relational query optimization. </title> <type> Technical Report RJ6625, </type> <institution> IBM Almaden Research Center, </institution> <month> December </month> <year> 1988. </year> <month> 266 </month>
Reference-contexts: the relevant principles are sketched in the concluding commentary. 1.4 Contributions In the course of defending the claim discussed above, this dissertation makes the following contributions to the understanding of join-order optimization: * It demonstrates that the worst-case time complexity of the Starburst optimizer, as described by Ono and Lohman <ref> [44, 45] </ref>, is O (4 n ). * It gives the first detailed account of an exhaustive-search, bushy join-order optimization algorithm with a worst-case time complexity of O (3 n ), and with a worst-case space complexity of O (2 n ). * It shows how this algorithm can be implemented <p> A few words of comment will help to clarify what is new, and what is not new, about our complexity results. 9 Ono and Lohman <ref> [44, 45] </ref> were the first to observe that join-order optimization has time complexity O (3 n ) in the worst case. <p> When requested to do so, the Starburst optimizer can produce bushy plans, as well as plans that contain arbitrary Cartesian products. But the mechanisms used in the Starburst optimizer are conceptually no different from those used in the System R optimizer. Ono and Lohman <ref> [44, 45] </ref> analyze the time complexity of optimization in Starburst under a variety of circumstances. <p> Hence the time required for optimization should be proportional to the number of feasible joins. However, before the feasible joins can be examined, they must be constructed, or enumerated. Ono and Lohman present pseudo-code for enumerating the feasible joins <ref> [44] </ref>, but disregard the time contribution of this code. Analysis by the present author reveals that in general, enumerating the feasible joins can take more time than examining them; the worst-case time complexity of feasible-join enumeration in Starburst is O (4 n ).
Reference: [45] <author> Kiyoshi Ono and Guy M. Lohman. </author> <title> Measuring the complexity of join enumeration in query optimization. </title> <booktitle> In Proceedings of the 16th International Conference on Very Large Data Bases, </booktitle> <month> August 13-16, </month> <year> 1990, </year> <institution> Brisbane, </institution> <address> Australia, </address> <pages> pages 314-325, </pages> <year> 1990. </year>
Reference-contexts: By excluding such expressions, a query optimizer reduces the size of its search space, and reduces optimization effort accordingly. But at the same time, it risks yielding a suboptimal solution in those cases where the true optimum contains a Cartesian product <ref> [45] </ref>. 2. Restriction of the search to left-deep join expressions. Left-deep expressions have the form illustrated in Figure 1.1 (a), with nesting of join operators occurring only in the left-hand inputs. The more general space of bushy expressions, of which inputs. <p> On the other hand, the optimal bushy expression is sometimes far superior to the best left-deep one <ref> [45] </ref>, and never inferior, since the bushy expressions subsume the left-deep ones. 5 (a) Left-deep (b) Bushy 3. Use of stochastic search. This tactic has a rather different character from the previous two. <p> When the number of relations n becomes "large"|meaning somewhere in the teens, as noted above|all exhaustive-search optimizers become overwhelmed by the exponential complexity of join-order optimization. But there are special cases where join-order optimization without Cartesian products has merely polynomial complexity <ref> [45] </ref>, whereas the complexity of join-order optimization with Cartesian products is always exponential [6, 49]. In these special cases, our exhaustive-search method becomes uncompetitive. Yet we cling tenaciously to our claim that Cartesian products need not be excluded. <p> the relevant principles are sketched in the concluding commentary. 1.4 Contributions In the course of defending the claim discussed above, this dissertation makes the following contributions to the understanding of join-order optimization: * It demonstrates that the worst-case time complexity of the Starburst optimizer, as described by Ono and Lohman <ref> [44, 45] </ref>, is O (4 n ). * It gives the first detailed account of an exhaustive-search, bushy join-order optimization algorithm with a worst-case time complexity of O (3 n ), and with a worst-case space complexity of O (2 n ). * It shows how this algorithm can be implemented <p> A few words of comment will help to clarify what is new, and what is not new, about our complexity results. 9 Ono and Lohman <ref> [44, 45] </ref> were the first to observe that join-order optimization has time complexity O (3 n ) in the worst case. <p> In Phase 4, it would find an optimal plan for the join of A, B, C, and D. This final plan would contain subplans that had been constructed in the earlier phases of optimization. Starburst and the Complexity of Dynamic Programming The Starburst optimizer <ref> [45] </ref> extended the techniques used in System R and provided greater generality and flexibility. When requested to do so, the Starburst optimizer can produce bushy plans, as well as plans that contain arbitrary Cartesian products. <p> When requested to do so, the Starburst optimizer can produce bushy plans, as well as plans that contain arbitrary Cartesian products. But the mechanisms used in the Starburst optimizer are conceptually no different from those used in the System R optimizer. Ono and Lohman <ref> [44, 45] </ref> analyze the time complexity of optimization in Starburst under a variety of circumstances. <p> But one must be cautious in interpreting these timings. Earlier studies of join-optimization performance suggest that when heavier-weight join-enumeration strategies are used, the effort of join enumeration is the limiting factor in join-optimization performance <ref> [40, 45] </ref>. Under those conditions, one can equally well measure performance using a simple cost computation or a more complicated one|the effort involved in the cost computations is dominated by the join-enumeration effort in either case. <p> But for star queries as well, we obtain split -execution counts consistent with the feasible-join counts set forth by Ono and Lohman <ref> [45] </ref>. In effect, when equipped with plan-cost thresholds, our optimizer excludes Cartesian products after all. But the exclusion is cost-based and not topology-based. Since the more conventional topology-based exclusions only approximate the effects that are actually desired, we obtain two advantages by directly applying a cost-based criterion.
Reference: [46] <author> Arjan Pellenkoft, Cesar A. Galindo-Legaria, and Martin Kersten. </author> <title> The complexity of transformation-based join enumeration. </title> <booktitle> In Proceedings of the Twenty-third International Conference on Very Large Data Bases, </booktitle> <address> Athens, Greece, </address> <month> 26-29 August, </month> <year> 1997, </year> <pages> pages 306-315, </pages> <year> 1997. </year>
Reference-contexts: The pseudo-code in the present work addresses these matters with care, leaving no doubt as to its O (3 n ) time complexity; in empirical trials we also verify that the claimed complexity is actually achieved. Very recent work by Pellenkoft, Galindo-Legaria, and Kersten <ref> [46] </ref> presents an alternative approach to join-order optimization with worst-case time complexity O (3 n )|though their approach has a higher space complexity than ours. <p> McKenna [20, 40] carried out extensive empirical studies of Volcano performance, which proved to be roughly comparable to that of Starburst. But the first analytical treatment of the subject was given by Pellenkoft, Galindo-Legaria, and Kersten <ref> [46] </ref>, who have shown Volcano's worst-case time complexity in bushy join-order optimization to be O (4 n ). (The present author [61] had previously conjectured, incorrectly, that Volcano's time complexity was just a constant multiple of its space complexity|hence O (3 n ).) We thus see that Starburst and Volcano yield <p> However, path 3 differs more fundamentally from paths 1 and 2, and entails duplication of effort. To circumvent this kind of duplication of effort, the present author [60] and Galindo-Legaria and his colleagues <ref> [13, 46] </ref> have independently devised formulations of join commutativity and associativity under which transformation paths obey the following property: The path from an expression E to another expression E 0 , if one exists, is unique up to reordering of independent transformation steps.
Reference: [47] <author> Gregory Piatetsky-Shapiro and Charles Connell. </author> <title> Accurate estimation of the number of tuples satisfying a condition. </title> <booktitle> In SIGMOD '84, Proceedings of Annual Meeting, </booktitle> <address> Boston, Massachusetts, </address> <month> June 18-21, </month> <year> 1984, </year> <pages> pages 256-276, </pages> <year> 1984. </year>
Reference-contexts: However, there are certainly situations in which inaccurate cost estimates have adverse effects, and research since the time of System R has sought to improve the quality of car-dinality estimates (and hence cost estimates) through a variety of sophisticated techniques <ref> [5, 14, 31, 35, 47] </ref>. Antoshenkov [1] goes further, and cites instability in cardinality computations as grounds for rejecting point-valued estimates altogether.
Reference: [48] <author> Benjamin C. Pierce. </author> <title> Basic Category Theory for Computer Scientists. </title> <publisher> The MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: in the topology of this space.) It is difficult to construct a genuinely object-oriented analogue of the relational join operator without giving up commutativity and associativity. 1 But it is trivial to construct an analogue that is commutative and associative up to isomorphism, in the manner of a categorical product <ref> [2, 48] </ref>. That is, one can construct an object join ffi 1 such that if A, B, 1 Analogues have been proposed that retain these properties; see, for example, the work of Shaw and Zdonik [52].
Reference: [49] <author> Wolfgang Scheufele and Guido Moerkotte. </author> <title> On the complexity of generating optimal plans with cross products. </title> <booktitle> In Proceedings of the Sixteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems, </booktitle> <address> Tucson, Arizona, </address> <month> May 12-14, </month> <year> 1997, </year> <pages> pages 238-248, </pages> <year> 1997. </year>
Reference-contexts: But there are special cases where join-order optimization without Cartesian products has merely polynomial complexity [45], whereas the complexity of join-order optimization with Cartesian products is always exponential <ref> [6, 49] </ref>. In these special cases, our exhaustive-search method becomes uncompetitive. Yet we cling tenaciously to our claim that Cartesian products need not be excluded.
Reference: [50] <author> P. Griffiths Selinger, M. M. Astrahan, D. D. Chamberlin, R. A. Lorie, and T. G. Price. </author> <title> Access path selection in a relational database management system. </title> <booktitle> In ACM-SIGMOD 1979 International Conference on Management of Data, </booktitle> <address> May 30-June 1, The 57 Park Plaza Hotel, Boston, Massachusetts, </address> <booktitle> Proceedings, </booktitle> <pages> pages 23-34, </pages> <year> 1979. </year>
Reference-contexts: The concept of selectivity attempts to grapple with the estimation problem by treating selection as quasi-stochastic; unfortunately, the semantic content of selection predicates often makes their behavior highly non-stochastic. 46 2.4.3 Discussion and Resolution In their classic paper on the System R optimizer, Selinger et al. <ref> [50] </ref> make an intriguing observation regarding errors in cost estimation. Although the System R optimizer proved to be rather poor at estimating the costs of plans, Selinger et al. found that the rankings of plans by their estimated costs tended to coincide with rankings based on true costs. <p> However, as join-order optimization remains a central concern for all query optimizers, it makes sense to include general-purpose query-optimization techniques in our discussion. 2.7.1 Dynamic Programming System R The System R optimizer <ref> [50] </ref> was the first to apply dynamic programming to join-order optimization. This optimizer constructed only left-deep plans, and excluded 58 Cartesian products except where they could not be avoided. In Chapter 3, we shall discuss in detail our own approach to optimization by way of dynamic programming.
Reference: [51] <author> Leonard Shapiro, David Maier, Keith Billings, Yubo Fan, Bennet Vance, Quan Wang, and Hsiao-min Wu. </author> <title> Safe pruning in the Columbia query optimizer. </title> <note> Submitted for publication, 1997. For more information, see the web site http://www.cs.pdx.edu/~len [November 4, </note> <year> 1997]. </year>
Reference-contexts: In doing so, Cascades opens up the possibility of pruning away some of the logical transformations that can be applied to a query|a possibility that Volcano did not offer. Shapiro et al. <ref> [51] </ref> present a modified version of Cascades called Columbia that uses a variety of pruning techniques to improve on the performance of previous rule-based optimizers|but does so without sacrificing the optimality of the generated plans. The design of OPT++ emphasizes an optimizer's adaptability to changing requirements.
Reference: [52] <author> Gail M. Shaw and Stanley B. Zdonik. </author> <title> An object-oriented query algebra. </title> <booktitle> In Proceedings of the Second International Workshop on Database Programming Languages, </booktitle> <address> 4-8 June 1989, Salishan Lodge, Gleneden Beach, Oregon, </address> <pages> pages 103-112. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: That is, one can construct an object join ffi 1 such that if A, B, 1 Analogues have been proposed that retain these properties; see, for example, the work of Shaw and Zdonik <ref> [52] </ref>. However, such analogues tend to exhibit anomalous algebraic characteristics. 261 and C are unordered object collections, then A 1 B ~ = B 1 A (10.1) ffi ffi ffi ffi Actual equality between the left- and right-hand sides is unnecessary for join reordering| it turns out that isomorphism suffices.
Reference: [53] <author> David W. Shipman. </author> <title> The functional data model and the data language Daplex. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 6(1) </volume> <pages> 140-173, </pages> <month> March </month> <year> 1981. </year>
Reference-contexts: For example, queries that make use of views [11, 32] often generate hidden joins, as do queries with path expressions <ref> [8, 53, 62] </ref>. As databases become more complex, and as they incorporate additional facilities that automatically generate joins "underneath the covers," queries with large values of n are likely to become more and more common.
Reference: [54] <author> M. Steinbrunn. </author> <title> Heuristic and Randomised Optimisation Techniques in Object-Oriented Database Systems. </title> <address> Infix-Verlag, Germany, </address> <year> 1996. </year> <note> Also published as a PhD thesis, </note> <institution> Universitat Passau. </institution> <month> 267 </month>
Reference-contexts: The process of choosing from among the available alternatives is called join-order optimization (or simply join optimization). There are, as noted, 12 alternatives in the case of a three-way join (i.e., a join involving three relations). In general, for an n-way join <ref> [29, 31, 54] </ref>, the number of alternatives is (2n 2)! : (1.4) This quantity grows at an explosive, faster-than-exponential rate. <p> First, a labeled join graph concisely captures much of the information needed to specify a given join-optimization problem. In fact, Steinbrunn <ref> [54] </ref> characterizes the input to a join optimizer as being exactly the join graph. In most of what follows, that characterization works well for us, too, provided that we think of our graphs as being labeled with relation cardinalities in addition to relation names. <p> Hypergraphs have a variety of applications in query processing besides the representation of n-ary predicates. For this reason, one does encounter mention of hypergraphs in the query-optimization research literature|but rarely in connection with n-ary predicates. Research on join-order optimization techniques appears to make no provisions for hypergraphs or hyperedges <ref> [54] </ref>. The present work will likewise focus on conventional join graphs with binary edges. 2.6 Cost Models and Physical Properties 2.6.1 Cost Models It was observed above that one may think of the input for a join-optimization problem as being a labeled join graph. <p> Join-optimization techniques permit flexibility in the cost model to varying degrees. We shall touch further on this matter in Section 2.7 below. For now, we introduce the approach to cost modeling that we will rely on in the present work. 2.6.2 A Generic Cost Model Following Steinbrunn <ref> [54, 55] </ref>, the present work will allow for flexibility in the cost model by making use of a single generic cost model. This generic model is parameterized by a cost function, as described below. <p> A consequence of the lack of cost estimation is that heuristic join optimizers are somewhat inflexible: they cannot be adapted to different cost models, and hence cannot take into account the performance of new join algorithms that might become available. Steinbrunn <ref> [54, 55] </ref> surveys a variety of heuristic techniques, explaining their mechanisms and examining their performance. As Steinbrunn shows, these techniques are extremely fast|which one would expect, since they have no searching to do|but generally deliver query plans of very poor quality. <p> But in the process, they give up the guarantee of optimality, and thus their technique becomes a heuristic one. Steinbrunn's measurements <ref> [54] </ref> show that this heuristic extension generates plans of mediocre quality. More recently, Swami and Iyer [59] have proposed another technique based on sequencing. Their approach begins by applying the technique of Krishnamurthy et al., and then seeks to improve the resulting plan by perturbing it in small ways. <p> Reproducing the experimental conditions of these various approaches may involve guesswork; in some instances, the published 148 description of the measurement methodology is evidently intended only to give a sense of the approach, not to permit duplication of the experiments [20, 28]. Even when the descriptions are more thorough <ref> [54, 55] </ref>, experimental conditions may be difficult to duplicate because of their dependence on the use of a particular pseudorandom number generator and seed. (Understandably, published accounts of the experiments rarely (if ever) specify such details as random number generation.) Random number generation arises when a benchmark seeks to report average <p> This reimplementation effort is unfortunate both because of the labor it entails, and because of the danger that it could introduce performance bugs (or indeed other bugs) in some of the algorithms involved. Such bugs could conceivably go undetected. Some benchmarks yield misleading results. Steinbrunn <ref> [54] </ref> reports an instance of unintentional benchmark bias in measurements he and his collaborators had made in an earlier study. <p> Using cost models that have appeared in previous studies is preferable to inventing new cost models. To satisfy the last two requirements, we borrow from the comprehensive survey by Steinbrunn et al. <ref> [54, 55] </ref>. Steinbrunn's measurements were run using several different join-graph topologies and cost models; here we use a subset of those topologies and cost models. But we depart from Steinbrunn in the assignment of cardinalities and selectivities. <p> There is precedent for these cost models, as all three are drawn from the performance analysis of Steinbrunn et al. [55]; moreover, they are appealing choices for benchmarking purposes because of their simplicity, and because they are very different from one another. But they lack realism. Steinbrunn's revised survey <ref> [54] </ref> abandons the cost models of the earlier version, and instead adopts a single, more complicated and more realistic cost model that combines features of the earlier cost models, as well as adding new features. <p> Subsequently we will also consider how Swami's local-improvement technique and the Chained Local Optimization technique of Martin and Otto would cope with this problem. 196 8.1.1 Characteristics of Various Approaches Iterative Improvement The algorithm known as iterative improvement <ref> [54, 58] </ref> is very simple, but quite effective. Given the problem at hand it might proceed as follows. First, it probes f at a randomly chosen value x; let us say that it probes the point labeled A in Figure 8.1. <p> The algorithm therefore requires a very large number of repetitions to stand a good chance of finding a deep minimum. Simulated Annealing Simulated annealing <ref> [27, 54, 58] </ref> partially overcomes the difficulty encountered by iterative improvement in the problem at hand. The simulated-annealing algorithm is somewhat similar to iterative improvement, but rather than methodically climbing downhill at each step, this algorithm permits both downhill and uphill moves. <p> There may be other deep local minima that are far better. (Theoretically, simulated annealing can be parameterized so that it finds a global minimum with probability 1; but the time required may be astronomical.) Two-phase Optimization Two-phase optimization or 2PO <ref> [26, 54] </ref> attempts to combine the advantages of iterative improvement and simulated annealing. In the first phase, a number of iterations of iterative improvement are performed so as to obtain a low point among the shallow minima. <p> Thus, for the chain and star queries, k-pct = 32; for the cycle + 3 queries, k-pct = 44; and the for the clique queries, k-pct = 60. The timings shown are hundreds to thousands of times lower than the timings reported by Steinbrunn <ref> [54] </ref> for stochastic join-optimization techniques applied to joins of up to 245 queries) 30 relations. <p> The time required to obtain these plans varies with the join graph, but generally runs to seconds, not minutes, of CPU time. As such, our optimization times appear to be at least two orders of magnitude lower than those reported in Steinbrunn's survey of stochastic join-optimization techniques <ref> [54] </ref>. Our results must be treated with caution because of the simplicity of our cost models, and because of our omission of consideration of physical properties. <p> Other hybrids are also imaginable. For example, the simulated-annealing phase of two-phase optimization [26] could be replaced by iterated tightening; genetic algorithms <ref> [54] </ref> could be used to recombine plans obtained in successive bushwhack iterations; and so on. Despite the apparent similarity of the Stochastic Bushwhack algorithm to Swami's local-improvement technique [56], performance of the Stochastic Bushwhack algorithm| and for larger queries, the Recursive Bushwhack algorithm|seems to be far more satisfactory.
Reference: [55] <author> Michael Steinbrunn, Guido Moerkotte, and Alfons Kemper. </author> <title> Optimizing join orders. </title> <type> Technical Report MIP-9307, </type> <institution> Universitat Passau, </institution> <year> 1993. </year>
Reference-contexts: Join-optimization techniques permit flexibility in the cost model to varying degrees. We shall touch further on this matter in Section 2.7 below. For now, we introduce the approach to cost modeling that we will rely on in the present work. 2.6.2 A Generic Cost Model Following Steinbrunn <ref> [54, 55] </ref>, the present work will allow for flexibility in the cost model by making use of a single generic cost model. This generic model is parameterized by a cost function, as described below. <p> A consequence of the lack of cost estimation is that heuristic join optimizers are somewhat inflexible: they cannot be adapted to different cost models, and hence cannot take into account the performance of new join algorithms that might become available. Steinbrunn <ref> [54, 55] </ref> surveys a variety of heuristic techniques, explaining their mechanisms and examining their performance. As Steinbrunn shows, these techniques are extremely fast|which one would expect, since they have no searching to do|but generally deliver query plans of very poor quality. <p> Reproducing the experimental conditions of these various approaches may involve guesswork; in some instances, the published 148 description of the measurement methodology is evidently intended only to give a sense of the approach, not to permit duplication of the experiments [20, 28]. Even when the descriptions are more thorough <ref> [54, 55] </ref>, experimental conditions may be difficult to duplicate because of their dependence on the use of a particular pseudorandom number generator and seed. (Understandably, published accounts of the experiments rarely (if ever) specify such details as random number generation.) Random number generation arises when a benchmark seeks to report average <p> Such bugs could conceivably go undetected. Some benchmarks yield misleading results. Steinbrunn [54] reports an instance of unintentional benchmark bias in measurements he and his collaborators had made in an earlier study. In the earlier study <ref> [55] </ref>, they had found that a join-optimization heuristic known as RDC performed competitively with other heuristics; however, subsequent experiments using a more sophisticated cost model revealed that the RDC heuristic was actually rather fragile, and worked well only under special conditions. 1 Graphical presentations of averages at each of various parameter <p> Using cost models that have appeared in previous studies is preferable to inventing new cost models. To satisfy the last two requirements, we borrow from the comprehensive survey by Steinbrunn et al. <ref> [54, 55] </ref>. Steinbrunn's measurements were run using several different join-graph topologies and cost models; here we use a subset of those topologies and cost models. But we depart from Steinbrunn in the assignment of cardinalities and selectivities. <p> There is precedent for these cost models, as all three are drawn from the performance analysis of Steinbrunn et al. <ref> [55] </ref>; moreover, they are appealing choices for benchmarking purposes because of their simplicity, and because they are very different from one another. But they lack realism.
Reference: [56] <author> Arun Swami. </author> <title> Optimization of large join queries: Combining heuristics and combinatorial techniques. </title> <booktitle> In Proceedings of the 1989 ACM SIGMOD International Conference on the Management of Data, Portland, Oregon, </booktitle> <pages> pages 367-376, </pages> <year> 1989. </year>
Reference-contexts: Much work on stochastic join-order optimization has focused on the space of left-deep join expressions <ref> [26, 56] </ref>; and regardless of any other tactics they employ, nearly all join-order optimizers exclude Cartesian products. <p> We shall discuss several stochastic techniques in more 66 detail in Chapter 8, in the context of presenting our own stochastic technique. 2.7.5 Hybrids and Frameworks Not all join-optimization techniques fit neatly into one of the classifications listed above. Swami <ref> [56] </ref> investigated a variety of hybrids built from combinations of optimization techniques; Swami's hybrids all incorporated a stochastic component. <p> In this chapter we consider a stochastic extension of the Blitzsplit algorithm that can handle larger numbers of relations, and that has the potential to be more resilient in the presence of complicating considerations such as physical properties. The ideas described in this chapter are not fundamentally new. Swami <ref> [56] </ref> explored a variety of approaches to join optimization that combined heuristic and combinatorial techniques. One approach he considered was a hybrid technique that he referred to as local improvement ; this technique improved randomly generated plans by applying dynamic programming to subproblems of the join-optimization problem. <p> For example, the simulated-annealing phase of two-phase optimization [26] could be replaced by iterated tightening; genetic algorithms [54] could be used to recombine plans obtained in successive bushwhack iterations; and so on. Despite the apparent similarity of the Stochastic Bushwhack algorithm to Swami's local-improvement technique <ref> [56] </ref>, performance of the Stochastic Bushwhack algorithm| and for larger queries, the Recursive Bushwhack algorithm|seems to be far more satisfactory.
Reference: [57] <author> Arun Swami. </author> <title> Distributions of query plan costs for large join queries. </title> <type> Technical Report RJ7908, </type> <institution> IBM Almaden Research Center, </institution> <month> January </month> <year> 1991. </year>
Reference-contexts: In particular, the continuous functions in their model may differ in important ways from the corresponding functions on actual plan spaces, which are discrete domains [37]. In this connection it is also worth noting an observation made by Swami <ref> [57] </ref> and by Galindo-Legaria et al. [12] regarding the proportion of points in query-plan space whose cost is close to that of the global minimum. This proportion tends to decline as the number of relations n increases. <p> Performance of a stochastic algorithm has two aspects: the quality of the solutions obtained, and the amount of time it takes to find them. Here we study both of these aspects of performance in the Stochastic Bushwhack algorithm, and the relationship between them. Swami <ref> [57] </ref> has broadly characterized the quality of plans obtained from stochastic join optimizers as follows: * A low-cost or good plan has an estimated cost within a factor of 2 of the optimum. * An acceptable plan has an estimated cost that is more than twice the optimum, but that does
Reference: [58] <author> Arun Swami and Anoop Gupta. </author> <title> Optimization of large join queries. </title> <booktitle> In 1988 Proceedings, SIGMOD International Conference on Management of Data, </booktitle> <address> Chicago, Illinois, </address> <month> June 1-3, </month> <pages> pages 8-17, </pages> <year> 1988. </year>
Reference-contexts: Subsequently we will also consider how Swami's local-improvement technique and the Chained Local Optimization technique of Martin and Otto would cope with this problem. 196 8.1.1 Characteristics of Various Approaches Iterative Improvement The algorithm known as iterative improvement <ref> [54, 58] </ref> is very simple, but quite effective. Given the problem at hand it might proceed as follows. First, it probes f at a randomly chosen value x; let us say that it probes the point labeled A in Figure 8.1. <p> The algorithm therefore requires a very large number of repetitions to stand a good chance of finding a deep minimum. Simulated Annealing Simulated annealing <ref> [27, 54, 58] </ref> partially overcomes the difficulty encountered by iterative improvement in the problem at hand. The simulated-annealing algorithm is somewhat similar to iterative improvement, but rather than methodically climbing downhill at each step, this algorithm permits both downhill and uphill moves.
Reference: [59] <author> Arun N. Swami and Balakrishna R. Iyer. </author> <title> A polynomial time algorithm for optimizing join queries. </title> <booktitle> In Proceedings of the Ninth International Conference on Data Engineering, </booktitle> <address> April 19-23, 1993, Vienna, Austria, </address> <pages> pages 345-354, </pages> <year> 1993. </year>
Reference-contexts: But in the process, they give up the guarantee of optimality, and thus their technique becomes a heuristic one. Steinbrunn's measurements [54] show that this heuristic extension generates plans of mediocre quality. More recently, Swami and Iyer <ref> [59] </ref> have proposed another technique based on sequencing. Their approach begins by applying the technique of Krishnamurthy et al., and then seeks to improve the resulting plan by perturbing it in small ways. <p> The lack of a single, commonly accepted, and well-defined benchmark for join-order optimization makes it difficult to compare different algorithms on the basis of speed and effectiveness. Presentations of new algorithms in the literature generally do include such comparisons with earlier algorithms <ref> [12, 59] </ref>; producing these comparisons necessitates reimplementing the earlier algorithms [12] unless pre-existing implementations are readily available. This reimplementation effort is unfortunate both because of the labor it entails, and because of the danger that it could introduce performance bugs (or indeed other bugs) in some of the algorithms involved. <p> One could first optimize using a simple cost model and without regard to physical properties, and then iteratively tighten the resultant plan, using the sophisticated cost model and taking physical properties into account. Such a two-step approach would be reminiscent of a proposal by Swami and Iyer <ref> [59] </ref>, in which a tentative plan is first obtained under one set of assumptions, and then locally improved (if possible) by considering changes to those assumptions. Other hybrids are also imaginable.
Reference: [60] <author> Bennet Vance. </author> <title> Presentation to group meeting of the Revelation project [10], led by Prof. </title> <institution> David Maier at the Oregon Graduate Institute of Science & Technology, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: However, path 3 differs more fundamentally from paths 1 and 2, and entails duplication of effort. To circumvent this kind of duplication of effort, the present author <ref> [60] </ref> and Galindo-Legaria and his colleagues [13, 46] have independently devised formulations of join commutativity and associativity under which transformation paths obey the following property: The path from an expression E to another expression E 0 , if one exists, is unique up to reordering of independent transformation steps.
Reference: [61] <author> Bennet Vance and David Maier. </author> <title> Rapid bushy join-order optimization with Cartesian products. </title> <booktitle> In 1996 Proceedings, ACM SIGMOD International Conference on Management of Data, </booktitle> <address> June 4 to 6, Montreal, Quebec, Canada, </address> <pages> pages 35-46, </pages> <year> 1996. </year>
Reference-contexts: But the first analytical treatment of the subject was given by Pellenkoft, Galindo-Legaria, and Kersten [46], who have shown Volcano's worst-case time complexity in bushy join-order optimization to be O (4 n ). (The present author <ref> [61] </ref> had previously conjectured, incorrectly, that Volcano's time complexity was just a constant multiple of its space complexity|hence O (3 n ).) We thus see that Starburst and Volcano yield a worst-case time complexity of the same order| O (4 n )|despite their use of unrelated search algorithms.
Reference: [62] <author> Carlo Zaniolo. </author> <title> The database language Gem. </title> <booktitle> In SIGMOD '83, Proceedings of Annual Meeting, Database Week, </booktitle> <address> San Jose, </address> <month> May 23-26, </month> <year> 1983, </year> <pages> pages 207-218, </pages> <year> 1983. </year>
Reference-contexts: For example, queries that make use of views [11, 32] often generate hidden joins, as do queries with path expressions <ref> [8, 53, 62] </ref>. As databases become more complex, and as they incorporate additional facilities that automatically generate joins "underneath the covers," queries with large values of n are likely to become more and more common.
References-found: 62

