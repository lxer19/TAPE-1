URL: http://www.cs.umn.edu/Users/dept/users/kumar/irregular94.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: CONTENTS 1 SCALABLE PARALLEL ALGORITHMS FOR UNSTRUCTURED PROBLEMS  
Author: Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis 
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Arvindam, Vipin Kumar, and V. Nageshwara Rao. </author> <title> Floorplan optimization on multiprocessors. </title> <booktitle> In Proceedings of the 1989 International Conference on Computer Design, </booktitle> <year> 1989. </year> <note> Also published as Technical Report ACT-OODS-241-89, </note> <institution> Microelectornics and Computer Corporation, Austin, TX. </institution>
Reference-contexts: For these applications, parallel processing is perhaps the only way to obtain acceptable performance. For some problems, optimal solutions are highly desirable, and can be obtained for moderate sized instances in a reasonable amount of time using parallel search techniques (e.g. VLSI floor-plan optimization <ref> [1] </ref>). In this section, we provide an overview of our research on parallel algorithms for solving discrete optimization problems. 2.1 Parallel Depth First Search Depth-first search (DFS), also referred to as Backtracking, is a general technique for solving a variety of discrete optimization problems [17, 3, 1]. <p> VLSI floor-plan optimization [1]). In this section, we provide an overview of our research on parallel algorithms for solving discrete optimization problems. 2.1 Parallel Depth First Search Depth-first search (DFS), also referred to as Backtracking, is a general technique for solving a variety of discrete optimization problems <ref> [17, 3, 1] </ref>. Since many of the problems solved by DFS are computationally intensive, there has been a great interest in developing parallel versions of DFS. <p> Parallel depth-first branch-and-bound for floor-plan optimization for VLSI circuits yielded linear speedups on a 1024 processor Ncube T M , a 128 processor Symult T M and a network of 16 SUN workstations <ref> [1] </ref>. <p> For a more detailed discussion see [11, 19]. From our scalability analysis of a number of architecture-algorithm combinations, we have been able to gain valuable insights into the relative performance of parallel formulations for a given architecture. For instance, in <ref> [1] </ref> an implementation of parallel depth first branch and bound for VLSI floorplan optimization is presented, and speedups obtained on a network of 16 workstations.
Reference: [2] <author> S. Arvindam, Vipin Kumar, and V. Nageshwara Rao. </author> <title> Efficient parallel algorithms for search problems: Applications in VLSI CAD. </title> <booktitle> In Proceedings of the Frontiers 90 Conference on Massively Parallel Computation, </booktitle> <year> 1990. </year>
Reference-contexts: Linear speedups were also obtained for parallel DFS for the tautology verification problem for up to 1024 processors on the Ncube/10 T M and the Ncube/2 T M <ref> [2, 11, 19] </ref>. 1 In [14], we have presented new methods for load balancing of unstructured tree computations on large-scale SIMD machines such as CM-2 TM 2 . The analysis and experiments show that our new load balancing methods provide good speedups for parallel DFS on SIMD architectures.
Reference: [3] <author> S. Arvindam, Vipin Kumar, V. Nageshwara Rao, and Vineet Singh. </author> <title> Automatic test pattern generation on multiprocessors. </title> <journal> Parallel Computing, </journal> <volume> 17(12) </volume> <pages> 1323-1342, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: VLSI floor-plan optimization [1]). In this section, we provide an overview of our research on parallel algorithms for solving discrete optimization problems. 2.1 Parallel Depth First Search Depth-first search (DFS), also referred to as Backtracking, is a general technique for solving a variety of discrete optimization problems <ref> [17, 3, 1] </ref>. Since many of the problems solved by DFS are computationally intensive, there has been a great interest in developing parallel versions of DFS. <p> Parallel formulation of PODEM, which is the best known sequential algorithm for solving the test pattern generation problem, provided linear speedup on a 128 processor Symult T M <ref> [3] </ref>. Parallel depth-first branch-and-bound for floor-plan optimization for VLSI circuits yielded linear speedups on a 1024 processor Ncube T M , a 128 processor Symult T M and a network of 16 SUN workstations [1].
Reference: [4] <author> Cleve Ashcraft, S. C. Eisenstat, J. W.-H. Liu, and A. H. Sherman. </author> <title> A comparison of three column based distributed sparse factorization schemes. </title> <type> Technical Report YALEU/DCS/RR-810, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <year> 1990. </year> <booktitle> Also appears in Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1991. </year>
Reference-contexts: The communication volume of the column-based schemes represented in box A has been improved using smarter ways of mapping the matrix columns onto processors, such as, the subtree-to-subcube mapping [8] (box B). A number of column-based parallel factorization algorithms <ref> [27, 4, 8, 7, 13, 34] </ref> have a lower bound of O (N p) on the total communication volume. Since the overall computation is only O (N 1:5 ) [9], the ratio of communication to computation of column-based schemes is quite high.
Reference: [5] <author> Iain S. Duff and J. K. Reid. </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 9 </volume> <pages> 302-325, </pages> <year> 1983. </year>
Reference-contexts: This can make the difference between the feasibility and non-feasibility of parallel sparse factorization on highly parallel (p 256) computers. 12 Chapter 1 The algorithm presented in [12] is based on the multifrontal principle <ref> [5, 26] </ref> and the computation is guided by an elimination tree. Independent subtrees of the elimination tree are initially assigned to individual processors.
Reference: [6] <author> Iain S. Duff and J. K. Reid. </author> <title> The multifrontal solution of unsymmetric sets of linear equations. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 5(3) </volume> <pages> 633-641, </pages> <year> 1984. </year>
Reference-contexts: Our work so far has focussed on Cholesky factorization of symmetric positive definite matrices; however, the same ideas can be adapted for performing Gaussian elimination on 14 Chapter 1 diagonally dominant matrices that are almost symmetric in structure <ref> [6] </ref> and for solving sparse linear least squares problems [28].
Reference: [7] <author> G. A. Geist and E. G.-Y. Ng. </author> <title> Task scheduling for parallel sparse Cholesky factorization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18(4) </volume> <pages> 291-314, </pages> <year> 1989. </year>
Reference-contexts: The communication volume of the column-based schemes represented in box A has been improved using smarter ways of mapping the matrix columns onto processors, such as, the subtree-to-subcube mapping [8] (box B). A number of column-based parallel factorization algorithms <ref> [27, 4, 8, 7, 13, 34] </ref> have a lower bound of O (N p) on the total communication volume. Since the overall computation is only O (N 1:5 ) [9], the ratio of communication to computation of column-based schemes is quite high.
Reference: [8] <author> A. George, M. T. Heath, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Sparse Cholesky factorization on a local memory multiprocessor. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9 </volume> <pages> 327-340, </pages> <year> 1988. </year>
Reference-contexts: A simple fan-out algorithm <ref> [8] </ref> with column-wise partitioning of an N fi N matrix of this type on p processors results in an O (N p log N ) total communication volume [10] (box A). <p> The communication volume of the column-based schemes represented in box A has been improved using smarter ways of mapping the matrix columns onto processors, such as, the subtree-to-subcube mapping <ref> [8] </ref> (box B). A number of column-based parallel factorization algorithms [27, 4, 8, 7, 13, 34] have a lower bound of O (N p) on the total communication volume. <p> The communication volume of the column-based schemes represented in box A has been improved using smarter ways of mapping the matrix columns onto processors, such as, the subtree-to-subcube mapping [8] (box B). A number of column-based parallel factorization algorithms <ref> [27, 4, 8, 7, 13, 34] </ref> have a lower bound of O (N p) on the total communication volume. Since the overall computation is only O (N 1:5 ) [9], the ratio of communication to computation of column-based schemes is quite high.
Reference: [9] <author> A. George and J. W.-H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year> <title> Parallel Algorithms for Unstructured Problems 15 </title>
Reference-contexts: A number of column-based parallel factorization algorithms [27, 4, 8, 7, 13, 34] have a lower bound of O (N p) on the total communication volume. Since the overall computation is only O (N 1:5 ) <ref> [9] </ref>, the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased [34, 33].
Reference: [10] <author> A. George, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Communication results for parallel sparse Cholesky factorization on a hypercube. </title> <journal> Parallel Computing, </journal> <volume> 10(3) </volume> <pages> 287-298, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: A simple fan-out algorithm [8] with column-wise partitioning of an N fi N matrix of this type on p processors results in an O (N p log N ) total communication volume <ref> [10] </ref> (box A). The communication volume of the column-based schemes represented in box A has been improved using smarter ways of mapping the matrix columns onto processors, such as, the subtree-to-subcube mapping [8] (box B).
Reference: [11] <author> Ananth Grama, Vipin Kumar, and V. Nageshwara Rao. </author> <title> Experimental evaluation of load balancing techniques for the hypercube. </title> <booktitle> In Proceedings of the Parallel Computing '91 Conference, </booktitle> <pages> pages 497-514, </pages> <year> 1991. </year>
Reference-contexts: For most applications, state space trees searched by DFS tend to be highly irregular, and any static allocation of subtrees to processors is bound to result in significant load imbalance among processors. In <ref> [11, 19, 24, 31] </ref>, we presented and analyzed a number of parallel formulations of DFS which retain the storage efficiency of DFS, and can be implemented Parallel Algorithms for Unstructured Problems 5 on any MIMD multiprocessor. <p> Linear speedups were also obtained for parallel DFS for the tautology verification problem for up to 1024 processors on the Ncube/10 T M and the Ncube/2 T M <ref> [2, 11, 19] </ref>. 1 In [14], we have presented new methods for load balancing of unstructured tree computations on large-scale SIMD machines such as CM-2 TM 2 . The analysis and experiments show that our new load balancing methods provide good speedups for parallel DFS on SIMD architectures. <p> By modeling these, we were able to determine the isoefficiency functions (and thus scalability) of a variety of work-distribution schemes <ref> [11, 19, 24] </ref> for the ring, cube, mesh, network of workstations, and shared memory architectures. Some of the existing parallel formulations were found to have poor scalability. This motivated the design of substantially improved schemes for various architectures. <p> We established lower bounds on the scalability of any possible parallel formulation for various architectures. For each of these architectures, we determined near optimal load balancing schemes. The performance characteristics of various schemes have been proved and experimentally verified in <ref> [11, 19, 24, 31] </ref>. Schemes with better isoefficiency functions performed better than those with poorer isoefficiency functions for a wide range of problem sizes and processors. Table 1 presents isoefficiency functions of various parallel formulations on various architectures. For a more detailed discussion see [11, 19]. <p> Schemes with better isoefficiency functions performed better than those with poorer isoefficiency functions for a wide range of problem sizes and processors. Table 1 presents isoefficiency functions of various parallel formulations on various architectures. For a more detailed discussion see <ref> [11, 19] </ref>. From our scalability analysis of a number of architecture-algorithm combinations, we have been able to gain valuable insights into the relative performance of parallel formulations for a given architecture. <p> The various scemes such as ARR, NN, GRR, GRR-M, and RP are described in detail in <ref> [11, 19] </ref>. ing technique. Our scalability analysis can be used to investigate the viability of using a much larger number of workstations for solving this problem. Note from Table 1 that GRR has an overall isoefficiency of O (P 2 log P ) for this platform.
Reference: [12] <author> Anshul Gupta and Vipin Kumar. </author> <title> A scalable parallel algorithm for sparse matrix factorization. </title> <type> Technical Report 94-19, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year> <note> A short version appears in Supercomputing '94 Proceedings. TR available in users/kumar at anonymous FTP site ftp.cs.umn.edu. </note>
Reference-contexts: Although direct methods require more memory and are often costlier than iterative methods, they are important because of their generality 10 Chapter 1 and robustness. For solving sparse linear systems arising in certain applications, they are the only feasible methods known. In <ref> [12] </ref>, we describe an optimally scalable parallel algorithm for factorization of sparse matrices. This algorithm incurs strictly less communication overhead than any known parallel formulation of sparse matrix factorization, and hence, can utilize a higher number of processors effectively. <p> However, despite inherent parallelism in sparse sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations [13, 34]. We have shown that the parallel Cholesky factorization algorithm described in <ref> [12] </ref> is as scalable as the best parallel formulation of dense matrix factorization on both mesh and hypercube architectures. We also show that our algorithm is equally scalable for sparse matrices arising from two- and three-dimensional finite element problems. <p> Box D represents our algorithm, which is a significant im provement over other known classes of algorithms for this problem. the mapping of matrix columns onto processors (box B) and the other by The algorithm in <ref> [12] </ref> combines the benefits of improvements along both these lines. The total communication overhead of our algorithm is only O (N p factoring an N fi N matrix on p processors if it corresponds to a graph that satisfies the separator criterion. <p> This can make the difference between the feasibility and non-feasibility of parallel sparse factorization on highly parallel (p 256) computers. 12 Chapter 1 The algorithm presented in <ref> [12] </ref> is based on the multifrontal principle [5, 26] and the computation is guided by an elimination tree. Independent subtrees of the elimination tree are initially assigned to individual processors.
Reference: [13] <author> M. T. Heath, E. G.-Y. Ng, and Barry W. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <journal> SIAM Review, </journal> <volume> 33 </volume> <pages> 420-460, </pages> <year> 1991. </year> <note> Also appears in K. </note> <author> A. Gallivan et al. </author> <title> Parallel Algorithms for Matrix Computations. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference-contexts: It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers [29, 18]. However, despite inherent parallelism in sparse sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations <ref> [13, 34] </ref>. We have shown that the parallel Cholesky factorization algorithm described in [12] is as scalable as the best parallel formulation of dense matrix factorization on both mesh and hypercube architectures. <p> The communication volume of the column-based schemes represented in box A has been improved using smarter ways of mapping the matrix columns onto processors, such as, the subtree-to-subcube mapping [8] (box B). A number of column-based parallel factorization algorithms <ref> [27, 4, 8, 7, 13, 34] </ref> have a lower bound of O (N p) on the total communication volume. Since the overall computation is only O (N 1:5 ) [9], the ratio of communication to computation of column-based schemes is quite high.
Reference: [14] <author> George Karypis and Vipin Kumar. </author> <title> Unstructured Tree Search on SIMD Parallel Computers. </title> <type> Technical Report 92-21, </type> <institution> Computer Science Department, University of Minnesota, </institution> <year> 1992. </year> <journal> Appears in IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> Volume 5, Number 10, </volume> <pages> pp. 1057-1072, </pages> <month> October </month> <year> 1994. </year> <note> A short version appears in Supercomputing '92 Proceedings, pages 453-462, 1992. Available via anonymous ftp from ftp.cs.umn.edu at users/kumar/lb-SIMD.ps. </note>
Reference-contexts: Linear speedups were also obtained for parallel DFS for the tautology verification problem for up to 1024 processors on the Ncube/10 T M and the Ncube/2 T M [2, 11, 19]. 1 In <ref> [14] </ref>, we have presented new methods for load balancing of unstructured tree computations on large-scale SIMD machines such as CM-2 TM 2 . The analysis and experiments show that our new load balancing methods provide good speedups for parallel DFS on SIMD architectures.
Reference: [15] <author> George Karypis and Vipin Kumar. </author> <title> A high performance sparse Cholesky factorization algorithm for scalable parallel computers. </title> <type> Technical Report TR 94-41, </type> <institution> Department of Computer Science, University of Minnesota, Min-neapolis, MN, </institution> <year> 1994. </year> <booktitle> Submitted to the Eighth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <year> 1995. </year>
Reference-contexts: In the later part, the increase in communication is balanced by a corresponding increase in computation per column, and at the same time the processors do not starve due to narrowing of the elimination tree. Although, asymptotically scalable, subtree-to-subcube mapping often suffers from significant load-imbalance. In <ref> [15] </ref>, we describe a new elimination tree mapping scheme that minimizes this problem. We assign many subtrees (sub-forest) of the elimination tree to each processor subcube. These trees are chosen in such a way that the total amount of work assigned to each subcube is as equal as possible. <p> Node v is deleted from Q, and its children are inserted into Q. The algorithm then continues by repeating the whole process. Since it assigns subforests of the elimination tree to processor subcubes, we refer to it as subforest-to-subcube mapping scheme. The analysis presented in <ref> [15] </ref> shows that this new mapping scheme does not increase the overall communication overhead, and its scalability is similar to the earlier algorithm. We implemented our new parallel sparse multifrontal algorithm on a 1024-processor Cray T3D parallel computer. <p> On large enough problems, one of our algorithms <ref> [15] </ref> delivers up to 20 GFLOPS for medium-size structures and linear programming problems on a Cray T3D. To the best of our knowledge, this is the first parallel implementation of sparse Cholesky factorization that has delivered speedups of this magnitude and has been able to benefit from several hundred processors.
Reference: [16] <author> V. Kumar and L. N. Kanal. </author> <title> Parallel branch-and-bound formulations for and/or tree search. </title> <journal> IEEE Transactions Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-6:768-778, </volume> <year> 1984. </year>
Reference-contexts: A number of researchers have investigated parallel formulations of A*/B&B algorithms <ref> [16, 22] </ref>. An important component of A*/B&B algorithms is the priority queue which is used to maintain the "frontier" (i:e:, unexpanded) nodes of the search graph in a heuristic order.
Reference: [17] <author> Vipin Kumar. </author> <title> Depth-first search. </title> <editor> In Stuart C. Shapiro, editor, </editor> <booktitle> Ency-clopaedia of Artificial Intelligence: </booktitle> <volume> Vol 2, </volume> <pages> pages 1004-1005. </pages> <publisher> John Wiley and Sons, </publisher> <address> New York, NY, </address> <year> 1987. </year> <note> Revised version appears in the second edition of the encyclopedia to be published in 1992. </note>
Reference-contexts: VLSI floor-plan optimization [1]). In this section, we provide an overview of our research on parallel algorithms for solving discrete optimization problems. 2.1 Parallel Depth First Search Depth-first search (DFS), also referred to as Backtracking, is a general technique for solving a variety of discrete optimization problems <ref> [17, 3, 1] </ref>. Since many of the problems solved by DFS are computationally intensive, there has been a great interest in developing parallel versions of DFS. <p> Table 2 shows average speedup obtained in parallel algorithm for solving instances of the satisfiability problem using different load balancing techniques. 2.2 Parallel Best First Search The A* algorithm is a well known search algorithm that can use problem-specific heuristic information to prune search space. As discussed in <ref> [17] </ref>, A* 8 Chapter 1 Scheme! ARR GRR-M RP NN GRR P# 16 14.936 14.356 15.000 14.945 14.734 64 57.721 56.310 58.857 58.535 57.729 256 178.92 197.011 218.255 217.127 184.828 1024 284.425 644.383 660.582 671.202 Table 2 Average speedups for various parallel formulations. is essentially a "best-first" branch-and-bound (B&B) algorithm.
Reference: [18] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <address> Ben-jamin/Cummings, Redwood City, CA, </address> <year> 1994. </year> <note> 16 Chapter 1 </note>
Reference-contexts: This algorithm incurs strictly less communication overhead than any known parallel formulation of sparse matrix factorization, and hence, can utilize a higher number of processors effectively. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [29, 18] </ref>. However, despite inherent parallelism in sparse sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations [13, 34].
Reference: [19] <author> Vipin Kumar, Ananth Grama, and V. Nageshwara Rao. </author> <title> Scalable load balancing techniques for parallel computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(1) </volume> <pages> 60-79, </pages> <month> July </month> <year> 1994. </year> <note> Also available as Technical Report 91-55 (November 1991), </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution> <note> Available via anonymous ftp from ftp.cs.umn.edu at users/kumar/lb-MIMD.ps. </note>
Reference-contexts: For most applications, state space trees searched by DFS tend to be highly irregular, and any static allocation of subtrees to processors is bound to result in significant load imbalance among processors. In <ref> [11, 19, 24, 31] </ref>, we presented and analyzed a number of parallel formulations of DFS which retain the storage efficiency of DFS, and can be implemented Parallel Algorithms for Unstructured Problems 5 on any MIMD multiprocessor. <p> Linear speedups were also obtained for parallel DFS for the tautology verification problem for up to 1024 processors on the Ncube/10 T M and the Ncube/2 T M <ref> [2, 11, 19] </ref>. 1 In [14], we have presented new methods for load balancing of unstructured tree computations on large-scale SIMD machines such as CM-2 TM 2 . The analysis and experiments show that our new load balancing methods provide good speedups for parallel DFS on SIMD architectures. <p> By modeling these, we were able to determine the isoefficiency functions (and thus scalability) of a variety of work-distribution schemes <ref> [11, 19, 24] </ref> for the ring, cube, mesh, network of workstations, and shared memory architectures. Some of the existing parallel formulations were found to have poor scalability. This motivated the design of substantially improved schemes for various architectures. <p> We established lower bounds on the scalability of any possible parallel formulation for various architectures. For each of these architectures, we determined near optimal load balancing schemes. The performance characteristics of various schemes have been proved and experimentally verified in <ref> [11, 19, 24, 31] </ref>. Schemes with better isoefficiency functions performed better than those with poorer isoefficiency functions for a wide range of problem sizes and processors. Table 1 presents isoefficiency functions of various parallel formulations on various architectures. For a more detailed discussion see [11, 19]. <p> Schemes with better isoefficiency functions performed better than those with poorer isoefficiency functions for a wide range of problem sizes and processors. Table 1 presents isoefficiency functions of various parallel formulations on various architectures. For a more detailed discussion see <ref> [11, 19] </ref>. From our scalability analysis of a number of architecture-algorithm combinations, we have been able to gain valuable insights into the relative performance of parallel formulations for a given architecture. <p> The various scemes such as ARR, NN, GRR, GRR-M, and RP are described in detail in <ref> [11, 19] </ref>. ing technique. Our scalability analysis can be used to investigate the viability of using a much larger number of workstations for solving this problem. Note from Table 1 that GRR has an overall isoefficiency of O (P 2 log P ) for this platform.
Reference: [20] <author> Vipin Kumar and Anshul Gupta. </author> <title> Analyzing the scalability of parallel algorithms and architectures: A survey. </title> <booktitle> In Proceedings of the 1991 International Conference on Supercomputing, </booktitle> <year> 1991. </year> <note> Also appears in September 1994 issue of JPDC. Available via anonymous ftp from ftp.cs.umn.edu at users/kumar/survey-scalability.ps. </note>
Reference-contexts: Hence any conclusions drawn from experimental results on a specific parallel computer and problem instance are rendered invalid by changes in any one of the above parameters. Scalability analysis of a parallel algorithm and architecture combination has been shown to be useful in extrapolating these conclusions <ref> [20, 24] </ref>. 1 Our work on tautology verification, test pattern generation and floorplan optimization received honorable mention for the Gordon Bell Award for outstanding research in practical parallel computing. <p> Parallel systems that can maintain a fixed efficiency level while increasing both W and P are defined as scalable <ref> [20] </ref>. If W needs to grow as f (P ) to maintain an efficiency E, then f (P ) is defined to be the isoefficiency function for efficiency E and the plot of f (P ) with respect to P is defined to be the isoefficiency curve for efficiency E.
Reference: [21] <author> Vipin Kumar and Anshul Gupta. </author> <title> Analyzing scalability of parallel algorithms and architectures. </title> <journal> Journal of Parallel and Distributed Computing (special issue on scalability), </journal> <volume> 22(3) </volume> <pages> 379-391, </pages> <month> September </month> <year> 1994. </year> <note> A short version of the paper appears in the Proceedings of the 1991 International Conference on Supercomputing. Available via anonymous ftp from ftp.cs.umn.edu at users/kumar/survey-scalability.ps. </note>
Reference-contexts: This is because the performance of different schemes may be altered in different ways by changes in hardware characteristics (such as interconnection network, CPU speed, speed of communication channels etc.), number of processors, and the size of the problem instance being solved <ref> [21] </ref>. Hence any conclusions drawn from experimental results on a specific parallel computer and problem instance are rendered invalid by changes in any one of the above parameters.
Reference: [22] <author> Vipin Kumar, K. Ramesh, and V. Nageshwara Rao. </author> <title> Parallel best-first search of state-space graphs: A summary of results. </title> <booktitle> In Proceedings of the 1988 National Conference on Artificial Intelligence, </booktitle> <pages> pages 122-126, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: A number of researchers have investigated parallel formulations of A*/B&B algorithms <ref> [16, 22] </ref>. An important component of A*/B&B algorithms is the priority queue which is used to maintain the "frontier" (i:e:, unexpanded) nodes of the search graph in a heuristic order. <p> The reason is that the TSP and VCP generate search spaces that are qualitatively different from each other, even though both problems are NP-hard problems. We have also performed a preliminary analysis of the relationship between the characteristics of the search spaces and their suitability to various parallel formulations <ref> [22] </ref>. Parallel Algorithms for Unstructured Problems 9 2.3 Speedup Anomalies in Parallel Search In parallel DFS, the speedup can differ greatly from one execution to another, as the actual parts of the search space examined by different processors are determined dynamically, and can be different for different executions.
Reference: [23] <author> Vipin Kumar and V. N. Rao. </author> <title> Scalable parallel formulations of depth-first search. </title> <editor> In Vipin Kumar, P. S. Gopalakrishnan, and L. N. Kanal, editors, </editor> <booktitle> Parallel Algorithms for Machine Intelligence and Vision. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference-contexts: Parallel formulations of IDA* for solving the 15 puzzle problem yielded near linear speedup on Sequent Balance up to 30 processors and on the Intel Hypercube T M and BBN Butterfly T M up to 128 processors <ref> [23, 24, 31] </ref>. Parallel formulation of PODEM, which is the best known sequential algorithm for solving the test pattern generation problem, provided linear speedup on a 128 processor Symult T M [3].
Reference: [24] <author> Vipin Kumar and V. Nageshwara Rao. </author> <title> Parallel depth-first search, part II: Analysis. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16 </volume> (6):501-519, December 1987. 
Reference-contexts: For most applications, state space trees searched by DFS tend to be highly irregular, and any static allocation of subtrees to processors is bound to result in significant load imbalance among processors. In <ref> [11, 19, 24, 31] </ref>, we presented and analyzed a number of parallel formulations of DFS which retain the storage efficiency of DFS, and can be implemented Parallel Algorithms for Unstructured Problems 5 on any MIMD multiprocessor. <p> Parallel formulations of IDA* for solving the 15 puzzle problem yielded near linear speedup on Sequent Balance up to 30 processors and on the Intel Hypercube T M and BBN Butterfly T M up to 128 processors <ref> [23, 24, 31] </ref>. Parallel formulation of PODEM, which is the best known sequential algorithm for solving the test pattern generation problem, provided linear speedup on a 128 processor Symult T M [3]. <p> Hence any conclusions drawn from experimental results on a specific parallel computer and problem instance are rendered invalid by changes in any one of the above parameters. Scalability analysis of a parallel algorithm and architecture combination has been shown to be useful in extrapolating these conclusions <ref> [20, 24] </ref>. 1 Our work on tautology verification, test pattern generation and floorplan optimization received honorable mention for the Gordon Bell Award for outstanding research in practical parallel computing. <p> on numerical problems. 2 CM-2 is a registered trademark of the Thinking Machines Corporation. 6 Chapter 1 We have developed a scalability metric, called isoefficiency , which relates the problem size to the number of processors necessary for an increase in speedup in proportion to the number of processors used <ref> [24] </ref>. In general, for a fixed problem size W , increasing the number of processors P causes a decrease in efficiency because parallel processing overhead will increase while the sum of time spent by all processors in meaningful computation will remain the same. <p> By modeling these, we were able to determine the isoefficiency functions (and thus scalability) of a variety of work-distribution schemes <ref> [11, 19, 24] </ref> for the ring, cube, mesh, network of workstations, and shared memory architectures. Some of the existing parallel formulations were found to have poor scalability. This motivated the design of substantially improved schemes for various architectures. <p> We established lower bounds on the scalability of any possible parallel formulation for various architectures. For each of these architectures, we determined near optimal load balancing schemes. The performance characteristics of various schemes have been proved and experimentally verified in <ref> [11, 19, 24, 31] </ref>. Schemes with better isoefficiency functions performed better than those with poorer isoefficiency functions for a wide range of problem sizes and processors. Table 1 presents isoefficiency functions of various parallel formulations on various architectures. For a more detailed discussion see [11, 19].
Reference: [25] <author> Vipin Kumar and Vineet Singh. </author> <title> Scalability of Parallel Algorithms for the All-Pairs Shortest Path Problem: A Summary of Results. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <year> 1990. </year> <note> An extended version appears in Journal of Parallel and Distributed Processing, 13 124-138, 1991. Available via anonymous ftp from ftp.cs.umn.edu at users/kumar/shortest-path.ps. </note>
Reference-contexts: The isoefficiency metric has been found to be very useful in characterizing scalability of a number of algorithms <ref> [25] </ref>. Scalability Analysis of Parallel Depth-First Search The primary reason for loss of efficiency in our parallel formulations of DFS is the communication overhead incurred by different processors in finding work and in contention over shared resources.
Reference: [26] <author> J. W.-H. Liu. </author> <title> The multifrontal method for sparse matrix solution: Theory and practice. </title> <type> Technical Report CS-90-04, </type> <institution> York University, </institution> <address> Ontario, Canada, </address> <year> 1990. </year> <note> Also appears in SIAM Review, </note> <month> 34 </month> <pages> 82-109, </pages> <year> 1992. </year> <title> Parallel Algorithms for Unstructured Problems 17 </title>
Reference-contexts: This can make the difference between the feasibility and non-feasibility of parallel sparse factorization on highly parallel (p 256) computers. 12 Chapter 1 The algorithm presented in [12] is based on the multifrontal principle <ref> [5, 26] </ref> and the computation is guided by an elimination tree. Independent subtrees of the elimination tree are initially assigned to individual processors.
Reference: [27] <author> Robert F. Lucas, Tom Blank, and Jerome J. Tiemann. </author> <title> A parallel solution method for large sparse systems of equations. </title> <journal> IEEE Transactions on Computer Aided Design, </journal> <volume> CAD-6(6):981-991, </volume> <month> November </month> <year> 1987. </year>
Reference-contexts: The communication volume of the column-based schemes represented in box A has been improved using smarter ways of mapping the matrix columns onto processors, such as, the subtree-to-subcube mapping [8] (box B). A number of column-based parallel factorization algorithms <ref> [27, 4, 8, 7, 13, 34] </ref> have a lower bound of O (N p) on the total communication volume. Since the overall computation is only O (N 1:5 ) [9], the ratio of communication to computation of column-based schemes is quite high.
Reference: [28] <author> Pontus Matstoms. </author> <title> The multifrontal solution of sparse linear least squares problems. </title> <type> PhD thesis, </type> <institution> Department of Mathematics, Linkoping University, S-581 83 Linkoping, Sweden, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: Our work so far has focussed on Cholesky factorization of symmetric positive definite matrices; however, the same ideas can be adapted for performing Gaussian elimination on 14 Chapter 1 diagonally dominant matrices that are almost symmetric in structure [6] and for solving sparse linear least squares problems <ref> [28] </ref>.
Reference: [29] <author> Dianne P. O'Leary and G. W. Stewart. </author> <title> Assignment and scheduling in parallel matrix factorization. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 77 </volume> <pages> 275-299, </pages> <year> 1986. </year>
Reference-contexts: This algorithm incurs strictly less communication overhead than any known parallel formulation of sparse matrix factorization, and hence, can utilize a higher number of processors effectively. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [29, 18] </ref>. However, despite inherent parallelism in sparse sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations [13, 34].
Reference: [30] <author> Alex Pothen, H. D. Simon, and K.-P. Liou. </author> <title> Partioning sparce matrices with eigenvectors of graphs. </title> <journal> SIAM Journal of Mathematical Analysis and Applications, </journal> <volume> 11(3) </volume> <pages> 430-452, </pages> <year> 1990. </year>
Reference-contexts: COPTER2 comes from a model of a helicopter rotor. CUBE35 is a 35 fi 35 fi 35 regular three-dimensional grid. NUG15 is from a linear programming problem derived from a quadratic assignment problem obtained from AT&T. In all of our experiments, we used spectral nested dissection <ref> [30] </ref> to order the matrices. The performance obtained by our multifrontal algorithm in some of these matrices is shown in 3. The operation count shows only the number of operations required to factor the nodes of the elimination tree.
Reference: [31] <author> V. Nageshwara Rao and V. Kumar. </author> <title> Parallel depth-first search, part I: Implementation. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16 </volume> (6):479-499, December 1987. 
Reference-contexts: For most applications, state space trees searched by DFS tend to be highly irregular, and any static allocation of subtrees to processors is bound to result in significant load imbalance among processors. In <ref> [11, 19, 24, 31] </ref>, we presented and analyzed a number of parallel formulations of DFS which retain the storage efficiency of DFS, and can be implemented Parallel Algorithms for Unstructured Problems 5 on any MIMD multiprocessor. <p> Parallel formulations of IDA* for solving the 15 puzzle problem yielded near linear speedup on Sequent Balance up to 30 processors and on the Intel Hypercube T M and BBN Butterfly T M up to 128 processors <ref> [23, 24, 31] </ref>. Parallel formulation of PODEM, which is the best known sequential algorithm for solving the test pattern generation problem, provided linear speedup on a 128 processor Symult T M [3]. <p> We established lower bounds on the scalability of any possible parallel formulation for various architectures. For each of these architectures, we determined near optimal load balancing schemes. The performance characteristics of various schemes have been proved and experimentally verified in <ref> [11, 19, 24, 31] </ref>. Schemes with better isoefficiency functions performed better than those with poorer isoefficiency functions for a wide range of problem sizes and processors. Table 1 presents isoefficiency functions of various parallel formulations on various architectures. For a more detailed discussion see [11, 19].
Reference: [32] <author> V. Nageshwara Rao and Vipin Kumar. </author> <title> On the efficicency of parallel backtracking. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(4) </volume> <pages> 427-437, </pages> <month> April </month> <year> 1993. </year> <note> Also available as Technical Report TR 90-55, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution> <note> Available via anonymous ftp from ftp.cs.umn.edu at users/kumar/suplin.ps. </note>
Reference-contexts: This phenomenon of speedup greater than P on P processors in isolated executions of parallel DFS has been reported by many researchers. for a variety of problems and is referred to by the term speedup anomaly. In <ref> [32] </ref>, we present analytical models and experimental results on the average case behavior of parallel backtracking. We consider two types of backtrack search algorithms: (i) simple backtracking (which does not use any heuristic information); (ii) heuristic backtracking (which uses heuristics to order and prune search).
Reference: [33] <author> Edward Rothberg. </author> <title> Performance of panel and block approaches to sparse Cholesky factorization on the iPSC/860 and Paragon multicomputers. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: Since the overall computation is only O (N 1:5 ) [9], the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased <ref> [34, 33] </ref>. A few schemes with two-dimensional partitioning of the matrix have been proposed [33], and the total communication volume in the best of these schemes [33] is O (N p log p) (box C). <p> As a result, these column-cased schemes scale very poorly as the number of processors is increased [34, 33]. A few schemes with two-dimensional partitioning of the matrix have been proposed <ref> [33] </ref>, and the total communication volume in the best of these schemes [33] is O (N p log p) (box C). <p> As a result, these column-cased schemes scale very poorly as the number of processors is increased [34, 33]. A few schemes with two-dimensional partitioning of the matrix have been proposed <ref> [33] </ref>, and the total communication volume in the best of these schemes [33] is O (N p log p) (box C). In summary, the simple parallel algorithm with O (N p log p) communication volume (box A) has been improved along two directions|one by improving Parallel Algorithms for Unstructured Problems 11 node grid graphs.
Reference: [34] <author> Robert Schreiber. </author> <title> Scalability of sparse direct solvers. </title> <type> Technical Report RIACS TR 92.13, </type> <institution> NASA Ames Research Center, Moffet Field, </institution> <address> CA, </address> <month> May </month> <year> 1992. </year> <note> Also appears in A. </note> <editor> George, John R. Gilbert, and J. W.-H. Liu, editors, </editor> <title> Sparse Matrix Computations: Graph Theory Issues and Algorithms (An IMA Workshop Volume). </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers [29, 18]. However, despite inherent parallelism in sparse sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations <ref> [13, 34] </ref>. We have shown that the parallel Cholesky factorization algorithm described in [12] is as scalable as the best parallel formulation of dense matrix factorization on both mesh and hypercube architectures. <p> The communication volume of the column-based schemes represented in box A has been improved using smarter ways of mapping the matrix columns onto processors, such as, the subtree-to-subcube mapping [8] (box B). A number of column-based parallel factorization algorithms <ref> [27, 4, 8, 7, 13, 34] </ref> have a lower bound of O (N p) on the total communication volume. Since the overall computation is only O (N 1:5 ) [9], the ratio of communication to computation of column-based schemes is quite high. <p> Since the overall computation is only O (N 1:5 ) [9], the ratio of communication to computation of column-based schemes is quite high. As a result, these column-cased schemes scale very poorly as the number of processors is increased <ref> [34, 33] </ref>. A few schemes with two-dimensional partitioning of the matrix have been proposed [33], and the total communication volume in the best of these schemes [33] is O (N p log p) (box C).
References-found: 34

