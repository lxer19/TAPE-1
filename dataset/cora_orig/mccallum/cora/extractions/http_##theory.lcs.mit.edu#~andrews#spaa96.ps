URL: http://theory.lcs.mit.edu/~andrews/spaa96.ps
Refering-URL: http://theory.lcs.mit.edu/~andrews/
Root-URL: 
Email: Email: andrews@math.mit.edu  Email: ftl@math.mit.edu.  Email: pmetaxas@wellesley.edu.  Email: ylz@math.mit.edu.  
Title: Improved Methods for Hiding Latency in High Bandwidth Networks (Extended Abstract) d ave on any
Author: Matthew Andrews Tom Leighton P. Takis Metaxas Lisa Zhang 
Keyword: dimensional array with unit delays, using slowdown s  
Note: Supported by NSF contract 9302476-CCR, ARMY grant DAAH04-95-1-0607 and ARPA contract N00014-95-1-1246.  Supported by ARMY grant DAAH04-95-1-0607 and ARPA contract N00014-95-1-1246.  Supported by NSF contract 9504421-CCR, ARMY grant DAAH04-95-1-0607 and ARPA contract N00014-95-1-1246.  Supported by an NSF graduate fellowship, ARMY grant DAAH04-95-1-0607 and ARPA contract N00014-95-1-1246.  d ave This is still far superior to a slowdown of fi(d max which can occur without redundant computation.  O( N log 3 N N 1=4 log 3 N  than in  
Affiliation: Department of Mathematics and Laboratory for Computer Science, MIT.  Department of Mathematics and Laboratory for Computer Science, MIT.  Department of Computer Science, Wellesley College.  Department of Mathematics and Laboratory for Computer Science, MIT.  
Abstract: In this paper we describe methods for mitigating the degradation in performance caused by high latencies in parallel and distributed networks. Our approach is similar in spirit to the "complementary slackness" method of latency hiding, but has the advantage that the slackness does not need to be provided by the programmer, and that large slowdowns are not needed in order to hide the latency. Our approach is also similar in spirit to the latency hiding methods of [2], but is not restricted to memoryless dataflow types of programs. Most of our analysis is centered on the simulation of unit-delay rings on networks of workstations (NOWs) with arbitrary delays on the links. For example, given any collection of operations (including updates of large local memories or databases) that runs in t steps on a ring of n workstations with unit link delays, we show how to perform the same collection of operations in O(t log 3 n) steps on any connected, bounded-degree network of n= log 3 n workstations for which the average link delay is constant. (Here we assume that the bandwidth available on the NOW links is O(log n) times the bandwidth available on the ring links. An extra factor of log n appears in the slowdown without this assumption.) The result makes non-trivial use of redundant compu tation, which is required to avoid a slowdown that is proportional to the maximum link delay. The increase in memory and computational load on each workstation needed for the redundant computation is at most O(1). In the case where the average latency in the network of workstations (d ave ) is not constant, then the slowdown needed for the simulation degrades by an additional factor of O( As a consequence of our work on rings, we can also derive emulations of a wide variety of other unit-delay network architectures on a NOW with high-latency links. For example, we show how to emulate an N -node 2 We also prove lower bounds that establish limits on the degree to which the high latency links can be mitigated. These bounds demonstrate that it is easier to overcome latencies in dataflow types of computations
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> The Connection Machine CM-5 Technical Summary. Thinking Machines Corporation, </institution> <address> 245 First Street, Cambridge, MA 02154-1264, </address> <year> 1991. </year>
Reference-contexts: Such an approach is clearly less than desirable in the context of a NOW with high-latency links. An alternative approach is to organize the network in a hierarchical fashion so that the latencies are consistent with the hierarchy. For example, in the CM-5 <ref> [10, 1] </ref> the highest latency links are segregated into the top levels of the network hierarchy. This type of architecture works well for applications in which most of the computation is local since local computation can proceed using the low-level low-latency links.
Reference: [2] <author> M. Andrews, T. Leighton, P. T. Metaxas, and L. Zhang. </author> <title> Automatic methods for hiding latency in high bandwidth networks. </title> <booktitle> In Proceedings of the 28th Annual ACM Symposium on Theory of Computing (to appear), </booktitle> <year> 1996. </year>
Reference-contexts: In a previous paper <ref> [2] </ref>, we devised automatic methods for hiding latency in a dataflow model of computation. In the dataflow model, the computation performed by a processor p at step t depends only on the results of the computations performed by p and its neighbors at the preceding step. <p> Our method makes substantial use of redundant computation. In fact, we prove that redundant computation is necessary to hide latency for generic computations in the database model. This represents a substantial difference from the work in <ref> [2] </ref> for the dataflow model, for which redundant computation is apparently not useful in hiding latency. As a consequence of our work on rings, we can also derive efficient emulations of a wide variety of other unit-delay network architectures on a NOW with high-latency links. <p> Hence, it takes at most 5d steps in total for processor p j to compute every pebble in P j . Once this is done the next p d time steps can be simulated in a similar fashion. 2 In <ref> [2] </ref>, ( d) is proved to lower bound the slowdown of simulating G on H 0 . Combining Theorem 2 and 4 we can improve the slowdown to O ( p d ave log 3 n) while preserving efficiency and minimum load. <p> is needed to efficiently simulate a guest network G with unit delay links on a host NOW H with arbitrary link delays? In the case that G is a ring, we have proved upper and lower bounds that are existentially tight to within a polylog-arithmic factor. (For lower bounds, see <ref> [2] </ref>.) It would be nice to close the gap between the upper and lower bounds. More importantly, it would be nice to devise an approximation algorithm that was always within a polylogarithmic factor of optimal for any NOW.
Reference: [3] <author> Y. Aumann and M. Ben-Or. </author> <title> Computing with faulty arrays. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 162-169, </pages> <year> 1992. </year>
Reference-contexts: The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing [15, 16] and it has been employed in several algorithms papers <ref> [11, 7, 3, 12, 6] </ref>. Unfortunately, in all of the preceding examples, it is incumbent on the programmer to provide the slackness or pipelining needed or to determine what part of the computation must be redundantly duplicated and by which processors to overcome the latencies in the network.
Reference: [4] <author> G. E. Blelloch, S. Chatterjee, J. C. Hardwick, J. Sipelstein, and M. Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming PPoPP, </booktitle> <address> San Diego, CA, </address> <pages> pages 102-112. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: Processors on the HEP machine [13] swapped between unrelated threads while waiting for the data. The CM-1 and CM-2 were designed to emulate much larger virtual machines so that a single processor would perform the computation of many virtual processors <ref> [14, 4] </ref>. The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing [15, 16] and it has been employed in several algorithms papers [11, 7, 3, 12, 6].
Reference: [5] <author> R. Cole, B. Maggs, and R. Sitaraman. </author> <title> Multi-scale self-simulation: A technique for reconfiguring arrays with faults. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 561-572, </pages> <year> 1993. </year>
Reference-contexts: Unfortunately, this approach is not suitable for scenarios where the network is unstructured (which is often the case for a NOW) or when the underlying application requires frequent communications through the high-level links. Redundant computation is another approach that has been used in the past <ref> [9, 5, 7] </ref> to hide the effects of latency. Here the idea is to avoid latency by recomputing data locally instead of waiting to receive it through a high-latency link. Probably the most generally applicable method of hiding latency is the approach known as complementary slackness.
Reference: [6] <author> C. Kaklamanis, A. R. Karlin, F. T. Leighton, V. Milenkovic, P. Raghavan, S. Rao, C. Thom-borson, and A. Tsantilas. </author> <title> Asymptotically tight bounds for computing with faulty arrays of processors. </title> <booktitle> In Proceedings of the 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 285-296, </pages> <year> 1990. </year>
Reference-contexts: The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing [15, 16] and it has been employed in several algorithms papers <ref> [11, 7, 3, 12, 6] </ref>. Unfortunately, in all of the preceding examples, it is incumbent on the programmer to provide the slackness or pipelining needed or to determine what part of the computation must be redundantly duplicated and by which processors to overcome the latencies in the network.
Reference: [7] <author> R. Koch, T. Leighton, B. Maggs, S. Rao, and A. Rosenberg. </author> <title> Work-preserving emulations of fixed-connection networks. </title> <booktitle> In Proceedings of the 21st Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 227-240, </pages> <year> 1989. </year>
Reference-contexts: Unfortunately, this approach is not suitable for scenarios where the network is unstructured (which is often the case for a NOW) or when the underlying application requires frequent communications through the high-level links. Redundant computation is another approach that has been used in the past <ref> [9, 5, 7] </ref> to hide the effects of latency. Here the idea is to avoid latency by recomputing data locally instead of waiting to receive it through a high-latency link. Probably the most generally applicable method of hiding latency is the approach known as complementary slackness. <p> The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing [15, 16] and it has been employed in several algorithms papers <ref> [11, 7, 3, 12, 6] </ref>. Unfortunately, in all of the preceding examples, it is incumbent on the programmer to provide the slackness or pipelining needed or to determine what part of the computation must be redundantly duplicated and by which processors to overcome the latencies in the network.
Reference: [8] <author> T. Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays * Trees * Hypercubes. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: In particular, we show how to simulate a guest ring G with unit-delay links on an n-processor host ring H with arbitrary delays on the links. (Actually, our results are described in terms of linear arrays, but since a linear array can simulate a ring with slowdown 2 <ref> [8] </ref>, the distinction is not important.) A 1 Our algorithm assumes that the bandwidth available on the host links is log n times larger than the bandwidth on the guest links. <p> The simulation is work efficient and has minimum load. 4 Simulating Linear Arrays on General Networks Algorithm overlap can be generalized to the simulation of a guest linear array by an arbitrary bounded-degree fixed-connection host network. The following fact <ref> [8] </ref> is used in our argument. * Fact 3 An n-node linear array can be one-to-one embedded with dilation 3 in any connected n-node network. Let H be the n-node linear array embedded in H by Fact 3. <p> The following fact [8] is used in our argument. * Fact 3 An n-node linear array can be one-to-one embedded with dilation 3 in any connected n-node network. Let H be the n-node linear array embedded in H by Fact 3. The proof of Fact 3 <ref> [8, page 470] </ref> implies that if H has bounded degree ffi then H has average delay at most ffid ave . By Theorem 5, H can simulate G with a slowdown of O ( p d ave log 3 n).
Reference: [9] <author> T. Leighton, B. Maggs, and R. Sitaraman. </author> <title> On the fault tolerance of some popular bounded-degree networks. </title> <booktitle> In Proceedings of the 33rd Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 542-552, </pages> <year> 1992. </year>
Reference-contexts: Unfortunately, this approach is not suitable for scenarios where the network is unstructured (which is often the case for a NOW) or when the underlying application requires frequent communications through the high-level links. Redundant computation is another approach that has been used in the past <ref> [9, 5, 7] </ref> to hide the effects of latency. Here the idea is to avoid latency by recomputing data locally instead of waiting to receive it through a high-latency link. Probably the most generally applicable method of hiding latency is the approach known as complementary slackness.
Reference: [10] <author> C. E. Leiserson, Z. Abuhamdeh, D. Douglas, C. Feynman, M. Ganmukhi, J. Hill, D. Hillis, B. Kuszmaul, M. St. Pierre, D. Wells, M. Wong, S. Yang, and R. Zak. </author> <title> The network architecture of the connection machine CM-5. </title> <booktitle> In Proceedings of the 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-285, </pages> <year> 1992. </year>
Reference-contexts: Such an approach is clearly less than desirable in the context of a NOW with high-latency links. An alternative approach is to organize the network in a hierarchical fashion so that the latencies are consistent with the hierarchy. For example, in the CM-5 <ref> [10, 1] </ref> the highest latency links are segregated into the top levels of the network hierarchy. This type of architecture works well for applications in which most of the computation is local since local computation can proceed using the low-level low-latency links.
Reference: [11] <author> C. E. Leiserson, S. Rao, and S. Toledo. </author> <title> Efficient out-of-core algorithms for linear relaxation using blocking covers. </title> <booktitle> In Proceedings of the 34th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 704-713, </pages> <year> 1993. </year>
Reference-contexts: The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing [15, 16] and it has been employed in several algorithms papers <ref> [11, 7, 3, 12, 6] </ref>. Unfortunately, in all of the preceding examples, it is incumbent on the programmer to provide the slackness or pipelining needed or to determine what part of the computation must be redundantly duplicated and by which processors to overcome the latencies in the network.
Reference: [12] <author> M. O. Rabin. </author> <title> Efficient dispersal of information for security, load balancing and fault tolerance. </title> <journal> Journal of the ACM, </journal> <volume> 36(2) </volume> <pages> 335-348, </pages> <year> 1989. </year>
Reference-contexts: The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing [15, 16] and it has been employed in several algorithms papers <ref> [11, 7, 3, 12, 6] </ref>. Unfortunately, in all of the preceding examples, it is incumbent on the programmer to provide the slackness or pipelining needed or to determine what part of the computation must be redundantly duplicated and by which processors to overcome the latencies in the network.
Reference: [13] <author> B. J. Smith. </author> <title> Architecture and applications of the HEP multiprocessor computer system. </title> <booktitle> In SPIE, </booktitle> <pages> pages 298 241-248, </pages> <year> 1981. </year>
Reference-contexts: There are many implementations and incarnations of this method. For example, each processor in the CRAY YMP C-90 keeps busy by operating on a pipeline of 128 64-bit words. Processors on the HEP machine <ref> [13] </ref> swapped between unrelated threads while waiting for the data. The CM-1 and CM-2 were designed to emulate much larger virtual machines so that a single processor would perform the computation of many virtual processors [14, 4].
Reference: [14] <author> Lewis W. Tucker and George G. Robertson. </author> <title> Architecture and applications of the connection machine. </title> <journal> Computer, </journal> <volume> 21(8) </volume> <pages> 26-38, </pages> <year> 1988. </year>
Reference-contexts: Processors on the HEP machine [13] swapped between unrelated threads while waiting for the data. The CM-1 and CM-2 were designed to emulate much larger virtual machines so that a single processor would perform the computation of many virtual processors <ref> [14, 4] </ref>. The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing [15, 16] and it has been employed in several algorithms papers [11, 7, 3, 12, 6].
Reference: [15] <author> L. G. Valiant. </author> <title> Bulk-synchronous parallel computers. </title> <type> Technical report TR-08-89, </type> <institution> Center for Research in Computing Technology, Harvard University, </institution> <year> 1989. </year>
Reference-contexts: The CM-1 and CM-2 were designed to emulate much larger virtual machines so that a single processor would perform the computation of many virtual processors [14, 4]. The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing <ref> [15, 16] </ref> and it has been employed in several algorithms papers [11, 7, 3, 12, 6].
Reference: [16] <author> L. G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <year> 1990. </year>
Reference-contexts: The CM-1 and CM-2 were designed to emulate much larger virtual machines so that a single processor would perform the computation of many virtual processors [14, 4]. The technique also forms a critical component of Valiant's bulk synchronous model of parallel computing <ref> [15, 16] </ref> and it has been employed in several algorithms papers [11, 7, 3, 12, 6].
References-found: 16

