URL: http://www.cs.princeton.edu/prism/papers-ps/nbody-arch.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Title: Implications of Hierarchical N-body Methods for Multiprocessor Architecture  
Author: Jaswinder Pal Singh, John L. Hennessy and Anoop Gupta 
Note: To appear in ACM Transactions on Computer Systems. Prelim. version available as  
Address: Stanford, CA 94305  
Affiliation: Computer Systems Laboratory Stanford University  Stanford University  
Pubnum: Technical Report No. CSL-TR-92-506.  
Abstract: To design effective large-scale multiprocessors, designers need to understand the characteristics of the applications that will use the machines. Application characteristics of particular interest include the amount of communication relative to computation, the structure of the communication, and the local cache and memory requirements, as well as how these characteristics scale with larger problems and machines. One important class of applications is based on hierarchical N-body methods, which are used to efficiently solve a wide range of scientific and engineering problems. Important characteristics of these methods include the fact that the physical domains they are applied to are often nonuniform and dynamically changing, and their use of long-range, irregular communication. Applications that use these methods may therefore present challenges for parallel performance. This paper examines the key architectural implications of applications that represent the use of the two dominant hierarchical N-body methods: the Barnes-Hut Method and the Fast Multipole Method. We first show that exploiting temporal locality on accesses to communicated data is critical to obtaining good performance on these applications, and argue that coherent caches on shared address space machines exploit this locality both automatically and very effectively. Next, we examine the implications of scaling the applications to run on larger machines. We use scaling methods that reflect the concerns of the application scientist, and find that this leads to different conclusions about how communication traffic and local memory usage scale than do naive methods of scaling. In particular, we show that under the most realistic form of scaling, both the communication to computation ratio as well as the working set size (and hence the ideal cache size per processor) grow slowly as larger problems are run on larger machines. Lastly, we examine the effects of using two different abstractions for interprocessor communication: a shared address space and explicit message passing between private address spaces. We show that the lack of an efficiently supported shared address space substantially increases the programming complexity and performance overhead of message-passing implementations.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S.J. Aarseth, M. Henon, and R. Wielen. </author> <title> Astronomy and Astrophysics, </title> <type> 37, </type> <year> 1974. </year>
Reference-contexts: The initial distribution in both cases is a pair of nonuniform, Plummer model <ref> [1] </ref> galaxies that interact with each other. In these and all results we present, we measure five time-steps of the galactic simulation, ignoring the first two time-steps to factor out cold-start effects that would be negligible in real runs that execute for many hundreds of time-steps.
Reference: [2] <author> Andrew A. Appel. </author> <title> An efficient program for many body simulation. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 6 </volume> <pages> 85-93, </pages> <year> 1985. </year>
Reference-contexts: Hierarchical, tree-based methods have recently been developed that reduce the complexity to O (n log n) [4] for general distributions, or even O (n) for uniform distributions <ref> [2, 15] </ref>. <p> The hierarchical application of this insight|first used by Appel <ref> [2] </ref> in 1985|implies that the farther away the particles, the larger the group that can be approximated by a single particle. Although Newton arrived at his powerful insight in the context of gravitation, hierarchical N-body techniques based on it have found increasing applicability in various problem domains. <p> The first of these methods was devised by Appel <ref> [2] </ref>. However, his method is quite unstructured, which makes it difficult to program and to analyze for accuracy. The Barnes-Hut and Fast Multipole methods we study are better structured and hence more popular.
Reference: [3] <author> Joshua E. Barnes. </author> <type> Personal communication. </type> <month> May </month> <year> 1991. </year>
Reference-contexts: In gravitational N-body simulations, however, errors do not combine very predictably. We therefore use a scaling principle that is considered most realistic in practice for this as well as many other classes of scientific applications <ref> [3, 33] </ref>: * All sources of error should be scaled so that their error contributions are about equal. This scaling principle addresses the question of how an application's parameters should be scaled relative to one another. <p> However, when the appropriate partitioning and scheduling techniques are used [34], the ratio remains very small for realistic problems on today's machines. For example, a typical simulation today uses 64K particles with = 1.0 and quadrupole moments <ref> [3] </ref>. When run for 512 time-steps, this problem takes about three days to run on a single processor of an SGI 4D/240. When run on a 64-processor machine, this problem has a communication to computation ratio of less than 1 double precision word per 2000 instructions.
Reference: [4] <author> Joshua E. Barnes and Piet Hut. </author> <title> A hierarchical O(N log N) force calculation algorithm. </title> <journal> Nature, </journal> <volume> 324(4) </volume> <pages> 446-449, </pages> <year> 1986. </year>
Reference-contexts: Hierarchical N-body algorithms are a class of methods that satisfy all these criteria. In this paper, we study the above key architectural implications of gravitational simulations that employ the two most important hierarchical N-body algorithms: the Barnes-Hut method <ref> [4] </ref> and the Fast Multipole Method or FMM [15]. We first show that exploiting temporal locality on accesses to communicated data is critical to obtaining good performance on hierarchical N-body applications, and argue that coherent caches in shared address space machines provide this locality both automatically and very effectively. <p> If all pairwise forces are computed directly, this calculation has a time complexity that is O (n 2 ) in the number of particles. Hierarchical, tree-based methods have recently been developed that reduce the complexity to O (n log n) <ref> [4] </ref> for general distributions, or even O (n) for uniform distributions [2, 15].
Reference: [5] <author> Joshua E. Barnes and Piet Hut. </author> <title> Error analysis of a tree code. </title> <journal> Astrophysics Journal Supplement, </journal> <volume> 70 </volume> <pages> 389-417, </pages> <year> 1989. </year>
Reference-contexts: The last source is a static machine characteristic, and we ignore it. Studies in the astrophysics community have investigated the impact of the other parameters on simulation accuracy <ref> [5, 20] </ref>. <p> factor of p s (to match that due to a s-fold increase in n) requires a decrease in t by a factor of 4 p s, and hence that many more time-steps to simulate a fixed amount of physical time, which is usually held constant. * : The results in <ref> [5, 20] </ref> demonstrate a scaling of the force-calculation error proportional to 2 in the range of practical interest, for common distributions. (This is for the original Barnes-Hut algorithm|the one we use|which does not incorporate quadrupole corrections to the force approximation.
Reference: [6] <author> Tony Chan. </author> <title> Hierarchical algorithms and architectures for parallel scientific computing. </title> <booktitle> In Proceedings of ACM Conference on Supercomputing, </booktitle> <month> May </month> <year> 1990. </year> <month> 37 </month>
Reference-contexts: Because these algorithms use fundamental physical insights to solve large-scale problems efficiently, and because they are naturally amenable to parallelization, it has been urged that parallel architectures be designed to especially support the communication needs of these algorithms <ref> [6] </ref>. The class of applications we examine in this paper, classical N-body simulations, study the evolution of a system of particles (bodies) under the influences exerted on each particle by the whole ensemble. 1 The most time-consuming part of these simulations is the inter-particle force calculation.
Reference: [7] <author> A. J. Chorin. </author> <title> Numerical study of slightly viscous flow. </title> <journal> Journal of Fluid Mechanics, </journal> <volume> 57 </volume> <pages> 785-796, </pages> <year> 1973. </year>
Reference-contexts: Classical domains include astrophysics (gravitational force law), plasma physics (Coulombic) and molecular dynamics (Coulombic and others). Other domains include the vortex blob method in fluid dynamics <ref> [7] </ref>, integral equations in boundary value problems [25], Cauchy integrals in numerical complex analysis, and|most recently|radiosity calculations in computer graphics [19]. Particularly given the continued discovery of new domains of application, applications that use hierarchical N-body techniques will clearly continue to be among the dominant users of high-performance computers.
Reference: [8] <author> Peter J. Denning. </author> <title> The working set model for program behavior. </title> <journal> Communications of the ACM, </journal> <volume> 11(5) </volume> <pages> 323-333, </pages> <month> May </month> <year> 1968. </year>
Reference-contexts: In this section, we address this question for our hierarchical N-body applications by examining the scaling of the different working sets in the applications. 14 The working set model of program behavior <ref> [8] </ref> is based on the temporal locality exhibited by the data referencing patterns of programs. Under this model, a program has a set of data that it reuses substantially for a period of time, before moving on to other data.
Reference: [9] <author> S. Eggers and R. Katz. </author> <title> The Effect of Sharing on the Cache and Bus Performance of Para llel Programs. </title> <booktitle> In Proceedings of the 3rd International Conference on Architec tural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 257-270, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: For example, detailed studies of coherence traffic in some parallel applications have found the ideal granularity for replication/coherence in SAS machines to be only about 32 to 64 bytes, for the applications studied <ref> [37, 21, 9] </ref>. We examine message-passing machines on one hand and SAS machines with efficient, system-supported, fine-grained cache coherence on the other (we assume hardware coherence, although it could be an efficient combination of hardware and system software).
Reference: [10] <author> Anant Agarwal et al. </author> <title> LimitLESS directories: A scalable cache coherence scheme. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The support may be provided in hardware or software, depending on the particular architecture and the level of the memory hierarchy at which data are being replicated. For example, some SAS architectures (such as the Stanford DASH [23] and the MIT Alewife <ref> [10] </ref>) automatically replicate communicated data in the processor caches under hardware control, even without replication in main memory. Replication is managed at the fixed, usually small granularity of a cache line in these cases. Other systems provide automatic replication in main memory under system software control. <p> The primary advantage of explicit message-passing is the ease and efficiency of building scalable machines, since processing nodes require minimal hardware/software support for communication management. On the other hand, several research projects (for example, <ref> [10, 18, 23] </ref>) are demonstrating that the overheads of providing cache-coherence are relatively small, and that cost-effective, scalable SAS-CC machines can indeed be built.
Reference: [11] <author> Geoffrey C. Fox. </author> <title> Numerical Algorithms for Modern Parallel Computer Architectures, chapter A Graphical Approach to Load Balancing and Sparse Matrix Vector Multiplication on the Hypercube, </title> <address> pages 37-62. </address> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: nonuniform particle distributions, however (see Figure 15), which has implications for message-passing implementations as we shall see. 1 4 11 19 30 28 252421 33 63 62 434238 ORB 8 7 1213 16 20 29 36 4546 49 5253 5758 39 Costzones 9.4.2 Orthogonal Recursive Bisection (ORB) The ORB technique <ref> [11] </ref> obtains more regular partitions (at least in Barnes-Hut, see Section 9.7) by directly partitioning space rather than the tree. The tree is not used in the partitioning process at all.
Reference: [12] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Performance evaluation of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 245-257, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Cache hits cost a single cycle, read misses that are satisfied in the local memory unit stall the processor for 15 cycles, and read misses satisfied in some other memory unit stall the processor for 60 cycles. Write miss latencies can be hidden very effectively by hardware techniques <ref> [12] </ref>, so that they rarely stall the processor in most applications; we therefore assume that local write misses cost a single cycle and remote write misses 3 cycles. 6 The Importance of Temporal Locality on Communicated Data In this section, we show that the reuse obtained by caching shared data (from
Reference: [13] <author> Stephen R. Goldschmidt and Helen Davis. </author> <title> Tango introduction and tutorial. </title> <type> Technical Report CSL-TR-90-410, </type> <institution> Stanford University, </institution> <year> 1990. </year>
Reference-contexts: We therefore use a multiprocessor simulator, which is composed of two parts: the Tango event-driven reference generator (described in <ref> [13] </ref>), and a memory system simulator that feeds back into Tango. We simulate a very general shared address space architecture, consisting of a number of processing nodes connected together by some general interconnection network (see Figure 4).
Reference: [14] <author> Leslie Greengard. </author> <title> The Rapid Evaluation of Potential Fields in Particle Systems. </title> <publisher> ACM Press, </publisher> <year> 1987. </year>
Reference-contexts: In fact, the use of cell-cell interactions makes the complexity of this algorithm O (n), rather than O (n log n) as in Barnes-Hut, at least for uniform distributions <ref> [14, 31] </ref>. The other key difference is that force-calculation accuracy in the FMM is not determined by controlling which cells are considered far enough away with respect to a given cell. <p> Once the interactions for internal cells are computed, their effects are propagated down the tree until they reach the leaves, where they are evaluated at individual particles. Details of the different types of interactions, which are not very important for our purposes, can be found in <ref> [14] </ref>. The efficiency of the FMM is improved by allowing cells at the lowest level of the tree to contain more than a single particle (we allow a maximum of 40 particles per leaf cell, as Greengard suggests [14] and as works best in our implementations, unless otherwise mentioned). <p> which are not very important for our purposes, can be found in <ref> [14] </ref>. The efficiency of the FMM is improved by allowing cells at the lowest level of the tree to contain more than a single particle (we allow a maximum of 40 particles per leaf cell, as Greengard suggests [14] and as works best in our implementations, unless otherwise mentioned). Thus, both the leaves and the internal nodes of the tree represent space cells in the FMM. <p> In the FMM, force-calculation accuracy is controlled by the number of terms, m, retained in the multipole expansions used to represent cells, rather than by a parameter like the Barnes-Hut that determines which cells are far enough away from a given point. In <ref> [14] </ref>, Greengard shows that an error bound of * in force calculation can be guaranteed by using a number of terms m equal to log *. <p> Time Complexity: The serial complexity scales as O ( m 2 fl n t ). (As discussed in [31], at least one phase of the FMM has a complexity that is not quite O (n) as claimed in <ref> [14] </ref>.
Reference: [15] <author> Leslie Greengard and Vladimir Rokhlin. </author> <title> A fast algorithm for particle simulation. </title> <journal> Journal of Computational Physics, </journal> <volume> 73(325), </volume> <year> 1987. </year>
Reference-contexts: Hierarchical N-body algorithms are a class of methods that satisfy all these criteria. In this paper, we study the above key architectural implications of gravitational simulations that employ the two most important hierarchical N-body algorithms: the Barnes-Hut method [4] and the Fast Multipole Method or FMM <ref> [15] </ref>. We first show that exploiting temporal locality on accesses to communicated data is critical to obtaining good performance on hierarchical N-body applications, and argue that coherent caches in shared address space machines provide this locality both automatically and very effectively. Then, we examine the scaling of important execution characteristics. <p> Hierarchical, tree-based methods have recently been developed that reduce the complexity to O (n log n) [4] for general distributions, or even O (n) for uniform distributions <ref> [2, 15] </ref>.
Reference: [16] <author> Anoop Gupta, Wolf-Dietrich Weber, and Todd Mowry. </author> <title> Reducing memory and traffic requirements for scalable directory-based cache coherence schemes. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, </booktitle> <pages> pages I: 312-321, </pages> <year> 1990. </year>
Reference-contexts: We call the latter machines SAS-CC (for "cache-coherent"), and assume that the mechanisms they use to maintain coherence are scalable <ref> [16, 29, 22, 30] </ref>. Overhead/Granularity of Communication: Finally, the structure and granularity of communication are usually very different in SAS-CC and message-passing machines.
Reference: [17] <author> John L. Gustafson, Gary R. Montry, and Robert E. Brenner. </author> <title> Development of parallel methods for a 1024-processor hypercube. </title> <journal> SIAM Journal of Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 532-533, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: Since memory-constrained scaling keeps the data set per processor constant, it keeps the communication to computation ratio constant, promising constant per-processor efficiency as larger machines run "proportionally larger" problems. These were the results shown in the Sandia experiments <ref> [17] </ref>. However, we showed in [33] that primarily because the data set size does not grow linearly with the number of processors under the most appropriate scaling model|time-constrained scaling|the communication to computation ratio becomes worse under this model. <p> The communication to computation ratio therefore increases quickly. We have already seen that naive MC scaling (i.e. scaling p while keeping n g , and t fixed) is not enough to keep the ratio constant in this application, but results in a slow increase (unlike in the Sandia applications <ref> [17] </ref>). Keeping the ratio in check in fact takes realistic MC scaling, in which case and t also decrease while n g still stays fixed, and the ratio actually falls a little with scaling.
Reference: [18] <author> Erik Hagarsten, Seif Haridi, and David H.D. Warren. </author> <title> Cache and Interconnect Architectures in Multiprocessors, chapter The Cache Coherence Protocol of the Data Diffusion Machine. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1990. </year>
Reference-contexts: These next levels may include software-managed main memory|as in DASH- or Alewife-like machines|in which case replication is done at the level of pages with the attendant disadvantages, or they may themselves be hardware-managed caches, as in cache-only memory architectures <ref> [18] </ref>. As we have seen in Section 7.5, the working sets of our hierarchical N-body applications fit comfortably in even relatively small processor caches. <p> The primary advantage of explicit message-passing is the ease and efficiency of building scalable machines, since processing nodes require minimal hardware/software support for communication management. On the other hand, several research projects (for example, <ref> [10, 18, 23] </ref>) are demonstrating that the overheads of providing cache-coherence are relatively small, and that cost-effective, scalable SAS-CC machines can indeed be built.
Reference: [19] <author> P. Hanrahan, D. Salzman, and L. Aupperle. </author> <title> A rapid hierarchical radiosity algorithm. </title> <booktitle> In Proceedings of SIGGRAPH, </booktitle> <year> 1991. </year>
Reference-contexts: Classical domains include astrophysics (gravitational force law), plasma physics (Coulombic) and molecular dynamics (Coulombic and others). Other domains include the vortex blob method in fluid dynamics [7], integral equations in boundary value problems [25], Cauchy integrals in numerical complex analysis, and|most recently|radiosity calculations in computer graphics <ref> [19] </ref>. Particularly given the continued discovery of new domains of application, applications that use hierarchical N-body techniques will clearly continue to be among the dominant users of high-performance computers. The two applications that we study in this paper are galactic simulations from astrophysics.
Reference: [20] <author> Lars Hernquist. </author> <title> Performance characteristics of tree codes. </title> <journal> Astrophysics Journal Supplement, </journal> <volume> 64 </volume> <pages> 715-734, </pages> <year> 1987. </year>
Reference-contexts: The last source is a static machine characteristic, and we ignore it. Studies in the astrophysics community have investigated the impact of the other parameters on simulation accuracy <ref> [5, 20] </ref>. <p> factor of p s (to match that due to a s-fold increase in n) requires a decrease in t by a factor of 4 p s, and hence that many more time-steps to simulate a fixed amount of physical time, which is usually held constant. * : The results in <ref> [5, 20] </ref> demonstrate a scaling of the force-calculation error proportional to 2 in the range of practical interest, for common distributions. (This is for the original Barnes-Hut algorithm|the one we use|which does not incorporate quadrupole corrections to the force approximation. <p> The former require memory proportional to n. The latter depend on the spatial distribution as well as n, but are also found to be proportional to n for distributions of interest <ref> [20] </ref>. The other two parameters we scale ( and t) do not affect the data requirements of the application in a shared address space. Time Complexity: The serial time complexity depends on n in an O (n log n) fashion for realistic ranges of . <p> Time Complexity: The serial time complexity depends on n in an O (n log n) fashion for realistic ranges of . The dependence on is found to be roughly proportional to 1 2 for fixed n <ref> [20] </ref>. Finally, the complexity can be assumed to be proportional to the number of time-steps; that is, inversely proportional to t when a fixed amount of physical time is being simulated (as is usually the case).
Reference: [21] <author> J. L. Hennessy J. Torrellas, M. S. Lam. </author> <title> Measurement, analysis, and improvement of the cache behavior of shared data in cache coherent multiprocessors. </title> <type> Technical Report CSL-TR-90-412, </type> <institution> Stanford University, </institution> <month> February </month> <year> 1990. </year>
Reference-contexts: For example, detailed studies of coherence traffic in some parallel applications have found the ideal granularity for replication/coherence in SAS machines to be only about 32 to 64 bytes, for the applications studied <ref> [37, 21, 9] </ref>. We examine message-passing machines on one hand and SAS machines with efficient, system-supported, fine-grained cache coherence on the other (we assume hardware coherence, although it could be an efficient combination of hardware and system software).
Reference: [22] <author> David V. James. </author> <title> P1596 - SCI coherence overview. </title> <type> Technical Report 27, </type> <institution> SCI Committee, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: We call the latter machines SAS-CC (for "cache-coherent"), and assume that the mechanisms they use to maintain coherence are scalable <ref> [16, 29, 22, 30] </ref>. Overhead/Granularity of Communication: Finally, the structure and granularity of communication are usually very different in SAS-CC and message-passing machines.
Reference: [23] <author> Dan Lenoski, James Laudon, Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: To demonstrate that the scheme achieves the goals of load balance and data locality, Figure 3 shows the speedups obtained with both the Barnes-Hut and FMM applications on a state-of-the-art experimental multiprocessor, the Stanford DASH multiprocessor, which has a multilevel memory hierarchy with highly nonuniform access costs <ref> [23] </ref>. The initial distribution in both cases is a pair of nonuniform, Plummer model [1] galaxies that interact with each other. <p> | | | | | | | | | | (a) FMM: 32K particles, 24 terms Number of Processors Speedup fi fi fi fi Processor Cache Memory Processor Cache Memory Interconnection Network and shared data that are cached are kept coherent by a hardware mechanism such as a distributed directory <ref> [23] </ref>. This architecture affords locality at three levels of its memory hierarchy: * Cache Locality: This is the reuse of data that a processor brings into its cache (whether from its own local memory unit or from across the network) before they are replaced or invalidated. <p> The support may be provided in hardware or software, depending on the particular architecture and the level of the memory hierarchy at which data are being replicated. For example, some SAS architectures (such as the Stanford DASH <ref> [23] </ref> and the MIT Alewife [10]) automatically replicate communicated data in the processor caches under hardware control, even without replication in main memory. Replication is managed at the fixed, usually small granularity of a cache line in these cases. <p> The primary advantage of explicit message-passing is the ease and efficiency of building scalable machines, since processing nodes require minimal hardware/software support for communication management. On the other hand, several research projects (for example, <ref> [10, 18, 23] </ref>) are demonstrating that the overheads of providing cache-coherence are relatively small, and that cost-effective, scalable SAS-CC machines can indeed be built.
Reference: [24] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <year> 1989. </year>
Reference-contexts: Naming: When a process needs a datum that is logically shared, how does it reference that datum or find it in the machine? In the shared address space (henceforth abbreviated SAS) paradigm, the burden of finding data is placed on the hardware or system software <ref> [24] </ref>, so that any application process can directly reference any variable that is declared in the shared address space (we assume hardware support for this address translation or naming, and present some results for software versus hardware address translation in Section 9.10). <p> Replication is managed at the fixed, usually small granularity of a cache line in these cases. Other systems provide automatic replication in main memory under system software control. For example, IVY <ref> [24] </ref> provides replication at the fixed, relatively large granularity of physical memory pages.
Reference: [25] <author> Vladimir Rokhlin. </author> <title> Rapid solution of integral equations of classical potential theory. </title> <journal> Journal of Computational Physics, </journal> <volume> 60 </volume> <pages> 187-207, </pages> <year> 1985. </year>
Reference-contexts: Classical domains include astrophysics (gravitational force law), plasma physics (Coulombic) and molecular dynamics (Coulombic and others). Other domains include the vortex blob method in fluid dynamics [7], integral equations in boundary value problems <ref> [25] </ref>, Cauchy integrals in numerical complex analysis, and|most recently|radiosity calculations in computer graphics [19]. Particularly given the continued discovery of new domains of application, applications that use hierarchical N-body techniques will clearly continue to be among the dominant users of high-performance computers.
Reference: [26] <author> John K. Salmon. </author> <title> Parallel Hierarchical N-body Methods. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, </institution> <month> December </month> <year> 1990. </year> <month> 38 </month>
Reference-contexts: We describe two partitioning methods, initially in the context of the Barnes-Hut application: One (called costzones) is a simple technique that works best in our shared address space implementation, and the second (called Orthogonal Recursive Bisection or ORB) is a more complex technique used by Salmon <ref> [26] </ref> in a message-passing Barnes-Hut implementation (which we shall describe), and which we have also implemented for shared address space machines. 9.4.1 Costzones The Barnes-Hut algorithm already has a representation of the spatial distribution encoded in its tree data structure. <p> The result is a set of regularly shaped partitions that are each contiguous in space. Details of implementing ORB are omitted for reasons of space; a complete description of its application to this problem can be found in <ref> [26, 34] </ref>. ORB introduces new data structures, including a separate binary ORB tree whose nodes represent the recursively subdivided subspaces and the processors associated with them, and whose leaves are the final spatial partitions. <p> The only prior parallel implementation of a nonuniform hierarchical N-body method is a message-passing, galactic Barnes-Hut application by Salmon <ref> [26] </ref>. His implementation uses ORB partitioning, and yielded good parallel performance on a 512-processor NCUBE system. <p> The algorithm to build locally essential trees is outlined in Figure 17. Details can be found in <ref> [26] </ref>. Locally essential trees built in this way are conservatively large. <p> Some of the complications introduced by merging data into these trees are listed in Appendix B, and make the programmer's task very difficult. Details of merging can be found in Salmon's thesis <ref> [26] </ref>, as can descriptions of several more subtle complications in building locally essential trees that we have glossed over. Finally, the structure of communication is clearly large-grained and sender initiated in Salmon's approach, rather than demand-driven and fine-grained as in our SAS implementations. <p> Merging received data into a locally essential tree is also a high-overhead operation. In fact, Salmon finds that the vast majority of the runtime overhead in his parallel message-passing implementation is due to building the locally essential trees <ref> [26] </ref>. Salmon breaks down the overheads in his parallel implementation by their source. He finds that the largest source of overhead in the entire application is the "complexity overhead" or extra work done in the parallel implementation. <p> He finds that the largest source of overhead in the entire application is the "complexity overhead" or extra work done in the parallel implementation. By far the dominant source of this extra work overhead is associated with building the locally essential trees, not with doing the partitioning <ref> [26] </ref>. The next highest source of overhead is the load imbalance or waiting time at synchronization events, which is also found primarily in the phase of building locally essential trees. <p> A representative example is a run with 100,000 particles on 512 processors, in which the complexity overhead of building locally essential trees is about five times the waiting time overhead and a hundred times the communication overhead (a complete set of data for various problem configurations can be found in <ref> [26] </ref>). In an SAS implementation, locally essential trees do not have to be built at all. And the synchronization and communication overheads involved in collaboratively building the shared Barnes-Hut tree are not very significant relative to total execution time for typical ratios of problem size to number of processors. <p> the other hand, although the amount of data in a processor's partition stays about the same if both the number of particles and number of processors are doubled, Table 2 shows thatthe size of a processor's locally essential tree increases (this can also be gleaned from the data presented in <ref> [26] </ref>). Figure 19 shows the resulting dramatic difference in the number of particles that can be run, using data from Salmon's thesis [26] for the locally essential trees approach. In fact, these results assume that only the number of particles is scaled with the number of processors. <p> and number of processors are doubled, Table 2 shows thatthe size of a processor's locally essential tree increases (this can also be gleaned from the data presented in <ref> [26] </ref>). Figure 19 shows the resulting dramatic difference in the number of particles that can be run, using data from Salmon's thesis [26] for the locally essential trees approach. In fact, these results assume that only the number of particles is scaled with the number of processors. <p> First, a processor receives a lot of data in each iteration of the Buildtree algorithm (from its partner at that bisection) that it does not itself need but must pass on to another processor in a later iteration (see the earlier description of the Buildtree algorithm and <ref> [26] </ref>). Our measurements over a range of problem parameters show that about 40% of the communication volume falls into this nonessential category.
Reference: [27] <author> John K. Salmon. </author> <type> Personal communication. </type> <year> 1991. </year>
Reference-contexts: This is a problem not only for the original Barnes-Hut criterion, but for any of the very useful opening criteria that require knowledge of the distribution of particles within a cell <ref> [27] </ref>. 9.5.3 Summary of Communication Management Communication in Salmon's locally essential trees method is initiated and managed by the sender of data rather than by the receiver. <p> The fact that Salmon's message-passing version took several times longer to implement than our shared address space version <ref> [27] </ref> demonstrates the difference in complexity. The difference is also demonstrated by our experience with having groups of graduate students implement the application on SAS-CC and message-passing machines, in a ten-week parallel programming project course at Stanford University.
Reference: [28] <author> Daniel Scales and Monica Lam. </author> <title> A comparison of shared and distributed memory multiprocessors for performance and programmability. </title> <note> To appear as a Technical Report of the Computer Systems Laboratory, </note> <institution> Stanford University, </institution> <year> 1994. </year>
Reference-contexts: Several people have built such optimized software layers. Scales and Lam have implemented the Barnes-Hut application on one such optimized system (called SAM) that they have designed. Table 3 shows some of their results on three current message-passing machines for a 25K particle problem <ref> [28] </ref>. Thirty two processors are used in all cases. For comparison, our SAS implementation yields a speedup of about 28 on 32 processors on the DASH machine for this problem.
Reference: [29] <author> Steven Scott. </author> <title> A cache coherence mechanism for scalable, shared-memory multiprocessors. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <pages> pages 49-59, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: We call the latter machines SAS-CC (for "cache-coherent"), and assume that the mechanisms they use to maintain coherence are scalable <ref> [16, 29, 22, 30] </ref>. Overhead/Granularity of Communication: Finally, the structure and granularity of communication are usually very different in SAS-CC and message-passing machines.
Reference: [30] <author> Richard Simoni and Mark Horowitz. </author> <title> Dynamic pointer allocation for scalable cache coherence directories. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <pages> pages 72-81, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: We call the latter machines SAS-CC (for "cache-coherent"), and assume that the mechanisms they use to maintain coherence are scalable <ref> [16, 29, 22, 30] </ref>. Overhead/Granularity of Communication: Finally, the structure and granularity of communication are usually very different in SAS-CC and message-passing machines.
Reference: [31] <author> Jaswinder Pal Singh. </author> <title> Parallel Hierarchical N-body Methods and their Implications for Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: In fact, the use of cell-cell interactions makes the complexity of this algorithm O (n), rather than O (n log n) as in Barnes-Hut, at least for uniform distributions <ref> [14, 31] </ref>. The other key difference is that force-calculation accuracy in the FMM is not determined by controlling which cells are considered far enough away with respect to a given cell. <p> The contribution of increasing the number of terms, m, in the multipole expansions is negligible for practical purposes, particularly given how slowly m scales relative to n. Time Complexity: The serial complexity scales as O ( m 2 fl n t ). (As discussed in <ref> [31] </ref>, at least one phase of the FMM has a complexity that is not quite O (n) as claimed in [14].
Reference: [32] <author> Jaswinder Pal Singh and John L. Hennessy. </author> <title> Finding and exploiting parallelism in an ocean simulation program: Experiences, results, implications. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(1) </volume> <pages> 27-48, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In the Barnes-Hut application, the parallelism exploited in all phases is across particles, except in computing the cell centers of mass, where it is across cells. Unlike many scientific applications that operate on uniform problem domains and use algorithms that require only localized communication 2 (see, for example, <ref> [32] </ref>), our N-body applications have several characteristics that make it challenging to obtain scalable parallel performance. Some of these characteristics have direct implications for architectural support, as we shall see. <p> The primary disadvantage of the message-passing paradigm is programming complexity. In many scientific applications, the interprocessor communication patterns are naturally well-structured and predictable, so that the complexity of managing communication in the user program is not very severe. We have ourselves studied applications for which this is the case <ref> [32] </ref>. For the hierarchical N-body applications considered here, however, we will show that communication management adds substantial conceptual, programming and runtime overheads in message-passing implementations.
Reference: [33] <author> Jaswinder Pal Singh, John L. Hennessy, and Anoop Gupta. </author> <title> Scaling parallel programs for multiprocessors: Methodology and examples. </title> <journal> IEEE Computer, </journal> <volume> 26(7) </volume> <pages> 42-50, </pages> <month> july </month> <year> 1993. </year>
Reference-contexts: In this section, we perform such an analysis for the Barnes-Hut hierarchical N-body application. We assume that a multiprocessor is scaled by adding identical processors to it, each added processor having the same amount of main memory as the others already in the ensemble. In another paper <ref> [33] </ref>, we discussed methodological issues in studying the scaling of parallel applications and architectures. <p> In gravitational N-body simulations, however, errors do not combine very predictably. We therefore use a scaling principle that is considered most realistic in practice for this as well as many other classes of scientific applications <ref> [3, 33] </ref>: * All sources of error should be scaled so that their error contributions are about equal. This scaling principle addresses the question of how an application's parameters should be scaled relative to one another. <p> Let us now examine how each of the parameters separately impacts the serial computational complexity and storage requirements of the applications. Since we used the Barnes-Hut application as our primary example in the scaling methodology paper <ref> [33] </ref>, some of what follows can also be found in that paper. However, we repeat it here for completeness. 10 7.2 Scaling of Memory Requirements and Computational Complexity Memory: The main sources of memory requirement in the Barnes-Hut application are the data for particles and tree cells. <p> Since memory-constrained scaling keeps the data set per processor constant, it keeps the communication to computation ratio constant, promising constant per-processor efficiency as larger machines run "proportionally larger" problems. These were the results shown in the Sandia experiments [17]. However, we showed in <ref> [33] </ref> that primarily because the data set size does not grow linearly with the number of processors under the most appropriate scaling model|time-constrained scaling|the communication to computation ratio becomes worse under this model.
Reference: [34] <author> Jaswinder Pal Singh, Chris Holt, Takashi Totsuka, Anoop Gupta, and John L. Hennessy. </author> <title> Load balancing and data locality in hierarchical N-body methods. </title> <journal> Journal of Parallel and Distributed Computing. </journal> <note> To appear. Preliminary version available as Stanford Univeristy Tech. Report no. CSL-TR-92-505, </note> <month> January </month> <year> 1992. </year>
Reference-contexts: We have developed a successful partitioning scheme, called costzones, a description of which is deferred to Section 9. (Partitioning and scheduling issues for hierarchical N-body applications are discussed in detail in <ref> [34] </ref>). The scheme, which is inexpensive and hence invoked every time-step, assigns every processor a set of particles and cells which that processor is then responsible for in that timestep. <p> Under the most appropriate scaling model, then, the communication to computation ratio increases slowly in the Barnes-Hut application as larger problems are run on larger machines. However, when the appropriate partitioning and scheduling techniques are used <ref> [34] </ref>, the ratio remains very small for realistic problems on today's machines. For example, a typical simulation today uses 64K particles with = 1.0 and quadrupole moments [3]. When run for 512 time-steps, this problem takes about three days to run on a single processor of an SGI 4D/240. <p> The problem likely to limit speedup first is the performance of the tree-building phase, as discussed in <ref> [34] </ref>. <p> Other phases, however, require finer-grained coherence within them. In building a globally shared tree as in our SAS implementation, for example, different processors read and modify the same parts of the logically shared tree in an unstructured way (see <ref> [34] </ref>). The patterns of this read/write sharing are unpredictable, so that maintaining the required coherence in the application program is not straightforward. Communication Granularity: The natural data referencing patterns in these applications lead to fine-grained communication. <p> Processors partially traverse the tree in parallel, picking up the bodies (or usually entire large 27 cells) that belong in their cost zone. The partitioning algorithm requires only a few lines of code, has negligible runtime overhead and yields very good load balance <ref> [34] </ref>. <p> The result is a set of regularly shaped partitions that are each contiguous in space. Details of implementing ORB are omitted for reasons of space; a complete description of its application to this problem can be found in <ref> [26, 34] </ref>. ORB introduces new data structures, including a separate binary ORB tree whose nodes represent the recursively subdivided subspaces and the processors associated with them, and whose leaves are the final spatial partitions. <p> It incurs substantially more runtime overhead, and is more complex to implement and debug than the tree-partitioning scheme we described above <ref> [34] </ref>. In our SAS-CC implementations, we find the costzones scheme to perform better than ORB, particularly as the number of processors increases. <p> performance of the force-calculation phase is almost equally good in the two schemes (costzones being a little better since it has slightly better load balancing and since it automatically orders the particles within a partition to maximize temporal locality), but the cost of partitioning quickly becomes much larger in ORB <ref> [34] </ref>. The only prior parallel implementation of a nonuniform hierarchical N-body method is a message-passing, galactic Barnes-Hut application by Salmon [26]. His implementation uses ORB partitioning, and yielded good parallel performance on a 512-processor NCUBE system. <p> Finally, we note that the consequences of a phase-structured replacement policy in message-passing are even greater for partitioning schemes that do not incorporate physical locality but focus only on load balancing. In <ref> [34] </ref>, we 10 In fact, it is an overestimate, since the amount of data per cell needs to be a little larger in the locally essential trees case than in our SAS approach, and since many cells that exist in different processors' local trees in Salmon's approach are in fact shared <p> The additional complications arise from two sources, both discussed in the context of partitioning in <ref> [34] </ref>. First, it is not only leaf cells that are partitioned in the FMM, but internal cells as well. Second, many cells will straddle the bisectors that ORB partitioning produces (unlike particles, which fall on one or the other side of a bisector).
Reference: [35] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <year> 1992. </year> <note> Also Stanford University Technical Report No. CSL-TR-92-526, </note> <month> June </month> <year> 1992. </year>
Reference-contexts: Coherence: Two levels of coherence need to be provided in these applications. First, as in many scientific programs <ref> [35] </ref>, some phases of computation have the property that the shared data read in those phases are not modified in them.
Reference: [36] <author> M.S. Warren and J.K. Salmon. </author> <title> A parallel hashed oct-tree n-body algorithm. </title> <booktitle> In Proceedings of Supercomputing'93, </booktitle> <pages> pages 12-21, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: The fact that individual communications in our shared address space implementations are fine-grained is more than compensated for by two facts: that caching shared data 12 Such an approach has been used in <ref> [36] </ref> since this writing. 35 is very effective at keeping communication to computation ratios low for N-body problems, and that the hardware mechanisms that provide address translation and cache coherence support fine-grained communication efficiently. 9.10 Is Hardware Support Necessary? An interesting architectural question that remains is whether the cache-coherent shared address
Reference: [37] <author> Wolf-Dietrich Weber and Anoop Gupta. </author> <title> Analysis of cache invalidation patterns in multiprocessors. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 243-256, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: For example, detailed studies of coherence traffic in some parallel applications have found the ideal granularity for replication/coherence in SAS machines to be only about 32 to 64 bytes, for the applications studied <ref> [37, 21, 9] </ref>. We examine message-passing machines on one hand and SAS machines with efficient, system-supported, fine-grained cache coherence on the other (we assume hardware coherence, although it could be an efficient combination of hardware and system software).
References-found: 37

