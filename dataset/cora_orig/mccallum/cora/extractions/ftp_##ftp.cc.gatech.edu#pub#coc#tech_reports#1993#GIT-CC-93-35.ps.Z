URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1993/GIT-CC-93-35.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.93.html
Root-URL: 
Title: LDA Scalable, Off-Line Multiprocessor Scheduling for Real-Time Systems  
Author: Hongyi Zhou Karsten Schwan 
Date: Oct. 26, 1994  
Address: NVC 1C-222, 331 Newman Springs Road  Red Bank, NJ 07701-7020 Atlanta, GA 30332  
Affiliation: Bellcore College of Computing  Georgia Institute of Technology  
Abstract: Off-line task scheduling remains important for real-time systems since such algorithms may be used to schedule tasks known at the time of system initialization. The LDA algorithm described in this paper uses both task laxities and deadlines when deciding processor allocation and task scheduling for a multiprocessor system. The algorithm is different from existing algorithms (1) in its use of distributed scheduling information and (2) in its ability to co-exist with an on-line multiprocessor scheduling algorithm able to use the same data structures. The LDA algorithm is shown superior to the EDF (earliest-deadline-first) algorithm and at least as good as the LLF (least-laxity-first) algorithm by formal argument and by simulation.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Albus, H. McCain, and R. Lumia. </author> <title> Nasa/nbs standard reference model for telerobot control system architecture (nasrem). </title> <institution> National Bureau of Standards Technical Note 1235, </institution> <month> July </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Applications like autonomous robotics, the U.S. space station, multi-media systems, and real-time simulations give rise to multiprocessor computations that must satisfy certain timing constraints determined by the external physical environment <ref> [1, 2] </ref>. Such real-time applications and the scheduling of their parallel time-constrained tasks have been the subject of significant recent interest [3, 19, 21, 5, 16], in part because it is difficult to develop efficient scheduling algorithms that can provide guarantees concerning tasks' timing constraints.
Reference: [2] <author> T. Bihari, D. Pugh, T. Walliser, and E. Ribble. </author> <title> Timing analysis of a robot motion-planning algorithm. </title> <booktitle> In Seventh IEEE Workshop on Real-Time Operating Systems and Software, </booktitle> <institution> Univ. of Virginia, </institution> <address> Charlottesville, </address> <pages> pages 104-107, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Applications like autonomous robotics, the U.S. space station, multi-media systems, and real-time simulations give rise to multiprocessor computations that must satisfy certain timing constraints determined by the external physical environment <ref> [1, 2] </ref>. Such real-time applications and the scheduling of their parallel time-constrained tasks have been the subject of significant recent interest [3, 19, 21, 5, 16], in part because it is difficult to develop efficient scheduling algorithms that can provide guarantees concerning tasks' timing constraints.
Reference: [3] <author> Ben A. Blake and Karsten Schwan. </author> <title> Experimental evaluation of a real-time scheduler for a multiprocessor system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1), </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: Such real-time applications and the scheduling of their parallel time-constrained tasks have been the subject of significant recent interest <ref> [3, 19, 21, 5, 16] </ref>, in part because it is difficult to develop efficient scheduling algorithms that can provide guarantees concerning tasks' timing constraints. Furthermore, such algorithms are quite different from scheduling algorithms for multiprogrammed shared-memory multiprocessors that minimize average response times [12, 4] by providing desirable levels of parallelism. <p> First, all S i 's and D i 's, i = 1; 2; ; n, are sorted together and placed in increasing order without repetition into an array E [0::k] 1 The use of 5% task migration cost derives from measurements of the ASV application described in <ref> [3] </ref>. 2 Note that in this case the SL is searched only to find the laxities of tasks but not to schedule the tasks.
Reference: [4] <author> S.H. Bokhari. </author> <title> Dual processor scheduling with dynamic reassignment. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-5(4):341-349, </volume> <month> July </month> <year> 1979. </year>
Reference-contexts: Furthermore, such algorithms are quite different from scheduling algorithms for multiprogrammed shared-memory multiprocessors that minimize average response times <ref> [12, 4] </ref> by providing desirable levels of parallelism.
Reference: [5] <author> Houssine Chetto and Maryline Chetto. </author> <title> Some results of the earliest deadline scheduling algorithm. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(10) </volume> <pages> 1261-1269, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Such real-time applications and the scheduling of their parallel time-constrained tasks have been the subject of significant recent interest <ref> [3, 19, 21, 5, 16] </ref>, in part because it is difficult to develop efficient scheduling algorithms that can provide guarantees concerning tasks' timing constraints. Furthermore, such algorithms are quite different from scheduling algorithms for multiprogrammed shared-memory multiprocessors that minimize average response times [12, 4] by providing desirable levels of parallelism.
Reference: [6] <author> M. L. Dertouzos and A. K. Mok. </author> <title> Multiprocessor on-line scheduling of hard-real-time tasks. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(12) </volume> <pages> 1497-1506, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: In [8], Horn presents an O (n 3 ) static optimal scheduling algorithm for a shared-memory multiprocessor system. The solution approach is based on the network flow method and assumes zero cost in performing task preemption and migration. Mok and Dertouzos in <ref> [6] </ref> present the following results regarding real-time multiprocessor scheduling.
Reference: [7] <author> Michael L. Dertouzos. </author> <title> Control robotics: the procedural control of physical processes. </title> <booktitle> In Proc. of the IFIP Congress, </booktitle> <year> 1974. </year>
Reference-contexts: Similarly, a smaller migration cost (e.g., 3% of the mean task computation time) should result in less cases with equal to or greater than 1.0 RSR values. 5 Related Research Past research in the area of deadline scheduling for uniprocessor systems <ref> [8, 10, 7] </ref> has not been concerned with the representation of scheduling information. Similarly, many current results for multiprocessor systems do not focus on schedule representations or on the effects of task migration.
Reference: [8] <author> W. A. Horn. </author> <title> Some simple scheduling algorithms. </title> <journal> Naval Res. Logist. Quart., </journal> <volume> 21 </volume> <pages> 177-185, </pages> <year> 1974. </year>
Reference-contexts: Task migration. The LDA algorithm presented in this paper does not permit the task migration permitted in the algorithms described in <ref> [8] </ref> and [21]. In their work, a schedule S is defined as a sequence of slices, where each slice is a vector of length p (the number of processors). <p> (i.e., a feasible schedule for the task set may not exist), the scheduling performance of the LDA algorithm is compared with that of the LLF algorithm according to the number of task sets, out 3 An optimal polynomial-time algorithm exists only for migratable scheduling with zero preemption and migration costs <ref> [8] </ref>. 17 SC 2 SC 1 T 2 T 2 (a) 34100T 5 3410 2640 Initial Max LaxityDCS 0T 1 0 T 3 A T 4 T 2 T 2 P 1 : (c) T 1 T 1 T 5 T 3 P 0 : of the 100 generated sets, that <p> Similarly, a smaller migration cost (e.g., 3% of the mean task computation time) should result in less cases with equal to or greater than 1.0 RSR values. 5 Related Research Past research in the area of deadline scheduling for uniprocessor systems <ref> [8, 10, 7] </ref> has not been concerned with the representation of scheduling information. Similarly, many current results for multiprocessor systems do not focus on schedule representations or on the effects of task migration. <p> Similarly, many current results for multiprocessor systems do not focus on schedule representations or on the effects of task migration. In <ref> [8] </ref>, Horn presents an O (n 3 ) static optimal scheduling algorithm for a shared-memory multiprocessor system. The solution approach is based on the network flow method and assumes zero cost in performing task preemption and migration. Mok and Dertouzos in [6] present the following results regarding real-time multiprocessor scheduling. <p> In [11], Locke, Tokuda, and Jensen compared a number of simple scheduling policies and found that EDF and LLF are two good heuristic policies. Schedule and task ready queue representations are addressed for small-scale parallel machines in <ref> [8] </ref> and [21]. In these designs, a single dedicated processor performs all scheduling, maintains all scheduling information, and offers a task ready queue accessible by all dispatchers resident on each processor.
Reference: [9] <author> A.K. Jones, R.J. Chansler, I. Durham, J. Mohan, K. Schwan, and S. Vegdahl. Staros, </author> <title> a multiprocessor operating system. </title> <booktitle> In Proceedings of the 7th Symposium on Operating System Principles, Asilomar, </booktitle> <address> CA, </address> <pages> pages 117-127. </pages> <publisher> SIGOPS, Assoc. Comput. Mach., </publisher> <month> Dec. </month> <year> 1979. </year>
Reference-contexts: Data distribution multiprocessor scheduling consists of both (1) schedulability analysis (also called `scheduling' [17]) and (2) task scheduling (also called `low-level scheduling' or `dispatching' <ref> [12, 9] </ref>). Schedulers performing steps (1) or (2) should be able to access and manipulate the required information (i.e., task queues and scheduling information) concurrently and locally so that the resulting scheduling support can be made reliable and scalable.
Reference: [10] <author> C. L. Liu and James W. Layland. </author> <title> Scheduling algorithms for multiprogramming in hard real-time environment. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 20(1) </volume> <pages> 46-61, </pages> <month> January </month> <year> 1973. </year>
Reference-contexts: Similarly, a smaller migration cost (e.g., 3% of the mean task computation time) should result in less cases with equal to or greater than 1.0 RSR values. 5 Related Research Past research in the area of deadline scheduling for uniprocessor systems <ref> [8, 10, 7] </ref> has not been concerned with the representation of scheduling information. Similarly, many current results for multiprocessor systems do not focus on schedule representations or on the effects of task migration.
Reference: [11] <author> C. D. Locke, H. Tokuda, and E. D. Jensen. </author> <title> A time-driven scheduling model for real-time operating systems. </title> <type> Technical report, </type> <institution> Carnegie-Mellon University, </institution> <year> 1985. </year>
Reference-contexts: First, neither the on-line earliest-deadline-first (EDF) nor the on-line least-laxity-first (LLF) algorithm is optimal. 20 Second, no scheduling algorithm can be optimal without a priori knowledge of (i) the deadlines, (ii) the computation times, and (iii) the start times of all tasks. In <ref> [11] </ref>, Locke, Tokuda, and Jensen compared a number of simple scheduling policies and found that EDF and LLF are two good heuristic policies. Schedule and task ready queue representations are addressed for small-scale parallel machines in [8] and [21].
Reference: [12] <author> Cathy McCann, Raj Vaswani, and John Zahorjan. </author> <title> A dynamic processor allocation policy for multi-programmed, shared memory multiprocessors. </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, University of Washington, TR90-03-02, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: Furthermore, such algorithms are quite different from scheduling algorithms for multiprogrammed shared-memory multiprocessors that minimize average response times <ref> [12, 4] </ref> by providing desirable levels of parallelism. <p> Data distribution multiprocessor scheduling consists of both (1) schedulability analysis (also called `scheduling' [17]) and (2) task scheduling (also called `low-level scheduling' or `dispatching' <ref> [12, 9] </ref>). Schedulers performing steps (1) or (2) should be able to access and manipulate the required information (i.e., task queues and scheduling information) concurrently and locally so that the resulting scheduling support can be made reliable and scalable.
Reference: [13] <author> Karsten Schwan, Ben Blake, Win Bo, and John Gawkowski. </author> <title> Global data and control in multicom-puters: Operating system primitives and experimentation with a parallel branch-and-bound algorithm. Concurrency: Practice and Experience, </title> <publisher> Wiley and Sons, </publisher> <pages> pages 191-218, </pages> <month> Dec. </month> <year> 1989. </year> <month> 22 </month>
Reference-contexts: LDA does not permit such task migration (1) in order to avoid the implementation overheads arising from dynamic task migration and from the subsequently required updates to scheduling information <ref> [20, 13] </ref> and (2) to avoid two specific real-time scheduling problems that may result from migration. First, the number of task migrations in a schedule may be large, leading to excessive overhead due to migration. <p> This holds for Uniform Memory Access (UMA) machines where task migration may cause increased cache misses [18] and more so for NUMA or distributed memory machines due to increased costs of state transfers <ref> [13] </ref>. Second, task migration may lead to low CPU utilization when tasks' actual computation times are less than their estimated maximal computation times. This is demonstrated with a sample schedule shown in Figure 4. In the example, task T 2 is scheduled on processor P 1 .
Reference: [14] <author> Karsten Schwan and Hongyi Zhou. </author> <title> Dynamic scheduling of hard real-time tasks and real-time threads. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(8) </volume> <pages> 736-748, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: As a result, the scheduling algorithm's data structures should be distributed across the processors' memory units. The LDA algorithm described in this paper uses two lists resident on each processor: (1) the SL list (slot list) <ref> [14] </ref> recording the processor's scheduling information, and (2) the EL list [14] containing all of the tasks that are assigned to the processor. See [14] for detailed information on the SL and EL lists. <p> As a result, the scheduling algorithm's data structures should be distributed across the processors' memory units. The LDA algorithm described in this paper uses two lists resident on each processor: (1) the SL list (slot list) <ref> [14] </ref> recording the processor's scheduling information, and (2) the EL list [14] containing all of the tasks that are assigned to the processor. See [14] for detailed information on the SL and EL lists. <p> The LDA algorithm described in this paper uses two lists resident on each processor: (1) the SL list (slot list) <ref> [14] </ref> recording the processor's scheduling information, and (2) the EL list [14] containing all of the tasks that are assigned to the processor. See [14] for detailed information on the SL and EL lists. These p SLs and ELs shown in Figure 1 are the major data structures used by our LDA algorithm, and they are distributed across the memory units of the parallel machine. <p> In both cases, the selected task is scheduled on processor Processor on which it attains the maximal laxity (ties are broken arbitrarily). Once a task is assigned to a processor, the dynamic uniprocessor scheduling algorithm described in <ref> [14] </ref> is used to determine the task's scheduling on the chosen processor. Such uniprocessor scheduling implies both the update of the chosen processor's SL and the insertion of the task into the processor's EL. <p> if there is a task with 0 Max Laxity value in S, then T = the task with 0 Max Laxity value in S; else T = the task with the earliest deadline in S; P = T.Processor; back: schedule T on processor P, calling the dynamic uniprocessor scheduling algorithm <ref> [14] </ref>; insert T into EL P ; update SL P ; remove T from S; count += 1; TT = the first task in S; while ( TT != NULL ) do f update TT's laxity on P; 6 update TT.Max Laxity and TT.Processor; if ( TT.Max Laxity &lt; 0) f <p> The LDA algorithm presented in this paper is interoperable with that dynamic scheduling algorithm in that (1) both algorithms use the same data structures (the EL and SL lists) for maintaining scheduling information, (2) they employ the same dynamic uniprocessor scheduling algorithm described in <ref> [14] </ref> to perform task scheduling for individual processors in the parallel machine, and (3) they both assume the existence of local dispatchers on each processor being used for real-time task execution. Such local dispatchers can run concurrently. <p> Note that because the tasks are ordered by their deadlines before being scheduled, there is no need to perform the schedule reorganization as described in the dynamic uniprocessor scheduling algorithm <ref> [14] </ref>. <p> The argument on O ( P n i=1 r i ) = O (n) can be found in <ref> [14] </ref>. 4.2 Comparison with the EDF and LLF Algorithms In this section, we compare the performance of the LDA multiprocessor scheduling algorithm with that of the EDF and LLF algorithms. The EDF algorithm is a variation of the LDA algorithm.
Reference: [15] <author> Karsten Schwan, Hongyi Zhou, and Ahmed Gheith. </author> <title> Multiprocessor real-time threads. </title> <journal> ACM Operating Systems Reviews, </journal> <volume> 25(4), </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: This implies that the static LDA algorithm and its dynamic counterpart described in [23] are good choices for use in the multiprocessor real-time threads packages being developed by our group <ref> [15] </ref>. 4.1 LDA Algorithm Time Complexity Analysis Much of the LDA algorithm's computation concerns updating the laxities of all remaining tasks whenever a task is scheduled on a processor.
Reference: [16] <author> T. Shepard and J. A. M. Gagne. </author> <title> A pre-run-time scheduling algorithm for hard real-time systems. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> 17(7), </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: Such real-time applications and the scheduling of their parallel time-constrained tasks have been the subject of significant recent interest <ref> [3, 19, 21, 5, 16] </ref>, in part because it is difficult to develop efficient scheduling algorithms that can provide guarantees concerning tasks' timing constraints. Furthermore, such algorithms are quite different from scheduling algorithms for multiprogrammed shared-memory multiprocessors that minimize average response times [12, 4] by providing desirable levels of parallelism.
Reference: [17] <author> A. Tucker and A. Gupta. </author> <title> Process control and scheduling issues on a network of multiprocessors. </title> <booktitle> In Twelth ACM Symposium on Operating System Principles, </booktitle> <address> Litchfielf Park, AZ, </address> <pages> pages 159-166, </pages> <month> Dec. </month> <year> 1989. </year>
Reference-contexts: It also implies limitations on user-transparent task migration due to the resulting additional scheduling overheads and due to issues in processor utilization discussed in Section 3.3. 2. Data distribution multiprocessor scheduling consists of both (1) schedulability analysis (also called `scheduling' <ref> [17] </ref>) and (2) task scheduling (also called `low-level scheduling' or `dispatching' [12, 9]). Schedulers performing steps (1) or (2) should be able to access and manipulate the required information (i.e., task queues and scheduling information) concurrently and locally so that the resulting scheduling support can be made reliable and scalable.
Reference: [18] <author> Raj Vaswani and John Zahorjan. </author> <title> The implications of cache affinity on processor scheduling for multi-programmed, shared memory multiprocessor. </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, University of Washington, TR91-03-03, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: First, the number of task migrations in a schedule may be large, leading to excessive overhead due to migration. This holds for Uniform Memory Access (UMA) machines where task migration may cause increased cache misses <ref> [18] </ref> and more so for NUMA or distributed memory machines due to increased costs of state transfers [13]. Second, task migration may lead to low CPU utilization when tasks' actual computation times are less than their estimated maximal computation times.
Reference: [19] <author> Jia Xu and David Lorge Parnas. </author> <title> Scheduling processes with release times, deadlines, precedence, and exclusion relations. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> 16(3), </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: Such real-time applications and the scheduling of their parallel time-constrained tasks have been the subject of significant recent interest <ref> [3, 19, 21, 5, 16] </ref>, in part because it is difficult to develop efficient scheduling algorithms that can provide guarantees concerning tasks' timing constraints. Furthermore, such algorithms are quite different from scheduling algorithms for multiprogrammed shared-memory multiprocessors that minimize average response times [12, 4] by providing desirable levels of parallelism.
Reference: [20] <author> Edward R. Zayas. </author> <title> Attacking the process migration bottleneck. </title> <booktitle> In Proceedings of the eleventh ACM symposium on Operating System Principles, ACM SIGOPS, </booktitle> <pages> pages 13-24, </pages> <month> Nov </month> <year> 1987. </year>
Reference-contexts: LDA does not permit such task migration (1) in order to avoid the implementation overheads arising from dynamic task migration and from the subsequently required updates to scheduling information <ref> [20, 13] </ref> and (2) to avoid two specific real-time scheduling problems that may result from migration. First, the number of task migrations in a schedule may be large, leading to excessive overhead due to migration.
Reference: [21] <author> Wei Zhao, Krithi Ramamritham, and J. A. Stankovic. </author> <title> Preemptive scheduling under time and resource constraints. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(8):949-960, </volume> <month> August </month> <year> 1987. </year>
Reference-contexts: Such real-time applications and the scheduling of their parallel time-constrained tasks have been the subject of significant recent interest <ref> [3, 19, 21, 5, 16] </ref>, in part because it is difficult to develop efficient scheduling algorithms that can provide guarantees concerning tasks' timing constraints. Furthermore, such algorithms are quite different from scheduling algorithms for multiprogrammed shared-memory multiprocessors that minimize average response times [12, 4] by providing desirable levels of parallelism. <p> Task migration. The LDA algorithm presented in this paper does not permit the task migration permitted in the algorithms described in [8] and <ref> [21] </ref>. In their work, a schedule S is defined as a sequence of slices, where each slice is a vector of length p (the number of processors). <p> In [11], Locke, Tokuda, and Jensen compared a number of simple scheduling policies and found that EDF and LLF are two good heuristic policies. Schedule and task ready queue representations are addressed for small-scale parallel machines in [8] and <ref> [21] </ref>. In these designs, a single dedicated processor performs all scheduling, maintains all scheduling information, and offers a task ready queue accessible by all dispatchers resident on each processor.
Reference: [22] <author> Hongyi Zhou, Karsten Schwan, and Ian Akyildiz. </author> <title> Performance effects of information sharing in a distributed multi processor real-time scheduler. </title> <type> Technical report, </type> <institution> College of Computing, Georgia Institute of Technology, GIT- CC-91/40, </institution> <month> Sept. </month> <year> 1991. </year>
Reference-contexts: Furthermore, by simulation, the LDA algorithm is shown as good or better than the LLF algorithm. Our current work concerns experimental studies of on-line real-time multiprocessor scheduling algorithms, as well as the development of realistic multiprocessor real-time applications. The dynamic multiprocessor scheduling algorithm described in <ref> [22] </ref> uses incomplete scheduling information in order to decrease scheduling latency.
Reference: [23] <author> Hongyi Zhou, Karsten Schwan, and Ian Akyildiz. </author> <title> Performance effects of information sharing in a distributed multiprocessor real-time scheduler. </title> <booktitle> In Proceedings of the Real-Time Systems Symposium, Phoenix, Arizona, </booktitle> <pages> pages 46-56. </pages> <publisher> IEEE, </publisher> <month> Dec. </month> <year> 1992. </year> <month> 23 </month>
Reference-contexts: In <ref> [23] </ref>, we described a dynamic multiprocessor scheduling algorithm for hard real-time systems. <p> By simulation, we show that the LDA algorithm performs as well as the LLF algorithm when the cost of task migration is assumed 5% of the average task computation time 1 . This implies that the static LDA algorithm and its dynamic counterpart described in <ref> [23] </ref> are good choices for use in the multiprocessor real-time threads packages being developed by our group [15]. 4.1 LDA Algorithm Time Complexity Analysis Much of the LDA algorithm's computation concerns updating the laxities of all remaining tasks whenever a task is scheduled on a processor. <p> Furthermore, this distribution permits local memory accesses to scheduling information whenever possible, thereby increasing the dynamic algorithm's scalability to large-scale parallel machines <ref> [23] </ref>. In contrast to pure EDF algorithms, the LDA scheduling algorithm uses tasks' laxities to discover those tasks that must be scheduled immediately. This is shown useful in this paper when there are several tasks with relatively large computation times but small laxities.
References-found: 23

