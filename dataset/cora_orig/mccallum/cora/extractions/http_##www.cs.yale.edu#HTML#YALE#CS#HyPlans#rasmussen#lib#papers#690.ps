URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/rasmussen/lib/papers/690.ps
Refering-URL: http://www.cs.yale.edu/HTML/YALE/CS/AI/VisionRobotics/YaleMobile.html
Root-URL: http://www.cs.yale.edu
Title: Visual Servoing Mobile Robot Navigation  
Author: Christopher Rasmussen 
Date: June 15, 1995  
Address: 690  
Affiliation: Computer Science  
Abstract: This paper describes a vision-based approach to autonomous robot navigation. No a priori model of the environment is used, nor is an attempt made to derive its three-dimensional structure. Rather, an annotated topological map is built based on correspondences between images of scenes stored during exploration. As a navigational tool, the map is referred to both for path-planning and as an indicator of visible subgoals than can be reached immediately via visual servoing. Advantages of the proposed system include resistance to positional error, relatively low storage requirements and faster image matching, modularity, and flexibility in task definition.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> I. Bachelder, A. Waxman. </author> <title> "Mobile Robot Visual Mapping and Localization: A View-Based Neurocomputational Architecture That Emulates Hip-pocampal Place Learning." </title> <booktitle> In Neural Networks, </booktitle> <volume> Volume 7, </volume> <pages> pp. 1083-1099, </pages> <year> 1994. </year>
Reference-contexts: Two variations on feature-matching are given in [21] and [7]. Image signatures [3], neural networks <ref> [1] </ref>, and other whole image techniques offer the prospect of being more robust and speedier. Image signatures have the property that the measured similarity between two views of a scene falls off smoothly as the angle or distance between them increases.
Reference: [2] <author> F. Chaumette, P. Rives, and B. Espian. </author> <title> "Positioning of a Robot with respect to an Object, Tracking it, and Estimating its Velocity by Visual Servoing." </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <address> Sacramento, CA, </address> <month> April, </month> <year> 1991, </year> <pages> pp. 2248-2253. </pages>
Reference-contexts: By adding the ability to maintain to our system, the robot might be able to negotiate curves and some corners in a closed-loop manner. 6 Visual Servoing Our approach to visual servoing is based on the techniques described in <ref> [2] </ref>. The idea is to identify a marker in the initial and target images and, using its image location and shape and knowledge of how three-dimensional motions give rise to two-dimensional image changes, move in such a way as to transform the initial marker into the target one. <p> In other words, the user was responsible for locating the target in the robot field of view and appropriately indicating it. 6.2 Square We start with the example in <ref> [2] </ref>: a vertically-oriented square of side 2a, with s fl = (a; a; a; a; a; a; a; a) T . This specifies the corners in clockwise order from the upper left, centering the square at the center of the image.
Reference: [3] <author> S. P. Engelson. </author> <title> "Passive Map Learning and Visual Place Recognition." </title> <type> Ph.D. Thesis, </type> <institution> Yale University Department of Computer Science, </institution> <year> 1994. </year>
Reference-contexts: We do not concern ourselves as they do about the completeness of the robot's exploration of its environment, but we adopt their notion of human-specified landmarks (albeit in the form of pretaken images or stock recognition procedures) within it as a useful guide. Similarly, in [9], [11], and <ref> [3] </ref>, the robot learns a model of the environment by building a graph based on topological connectivity of interesting locations, but it attempts to decide what constitute landmarks on its own. <p> The robot in [9] looks for gateways, or portals between spaces, with sonar, while remembering visual scenes along the way for place recognition. The system in <ref> [3] </ref> also looks for gateways (waypoints), using vision and odometry; matching is accomplished by comparing image signatures for (possibly) different locations. The robot in [11] uses sonar exclusively to detect places that maximize a distinctiveness function and to match or add them to its map. <p> Images must be compared to one another for similarity constantly. This is the least-developed component of the current system, but two main approaches seem to present themselves. One is matching images on the level of features or markers [4]; the other is matching whole images to each another <ref> [3] </ref>. Feature matching is vulnerable to the absence of features between images being compared. <p> S = start node, C = convergence node, and D = divergence node. locate low-level features in a scene when the robot's position cannot be precisely pinpointed; markers lessen this problem in proportion to their complexity. Two variations on feature-matching are given in [21] and [7]. Image signatures <ref> [3] </ref>, neural networks [1], and other whole image techniques offer the prospect of being more robust and speedier. Image signatures have the property that the measured similarity between two views of a scene falls off smoothly as the angle or distance between them increases.
Reference: [4] <author> C. Fennema, A. Hanson, E. Riseman, J. Beveridge, and R. Kumar. </author> <title> "Model-Directed Mobile Robot Navigation." </title> <journal> In IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> Volume 20, Number 6, </volume> <pages> pp. 1352-1369, </pages> <address> Novem-ber/December, </address> <year> 1990. </year>
Reference-contexts: We make use of this idea, but by employing vision instead of sonar we expect a better ability to distinguish between places. Closer to the work of this paper is <ref> [4] </ref>. They use a servoing approach to navigation with reference to an accurate geometric model of the environment (which we do not have). The geometric data for individual locales and features are embedded in a graph capturing the topological relationships between them. <p> Images must be compared to one another for similarity constantly. This is the least-developed component of the current system, but two main approaches seem to present themselves. One is matching images on the level of features or markers <ref> [4] </ref>; the other is matching whole images to each another [3]. Feature matching is vulnerable to the absence of features between images being compared.
Reference: [5] <author> G. Hager. </author> <title> "The `X-Vision' System: A General-Purpose Substrate for Vision-Based Robotics." </title> <booktitle> Submitted to Workshop on Vision for Robotics, </booktitle> <year> 1995. </year>
Reference-contexts: Each incoming image is processed using automatic feature selection [16] to pick out corners, edges, and other interesting subimages. The best features are tracked from frame to frame using the SSD (sum of squared differences) tracking package of the X-Vision system <ref> [5] </ref>. Groups of features with certain common characteristics are identified and designated markers. Images are stored when markers appear or disappear. To be precise, the first and last images in a sequence that a marker is completely contained within are stored.
Reference: [6] <author> G. Hager, S. Puri, and K. Toyama. </author> <title> "A Framework for Real-Time Vision-Based Tracking Using Off-the-Shelf Hardware." </title> <institution> Yale University Tech Report DCS RR-988, </institution> <year> 1993. </year> <month> 15 </month>
Reference-contexts: First, the task of tracking features from frame to frame is easier and more robust when locational constraints between features may be enforced <ref> [6] </ref>, [18]. Momentary occlusions or distracting backgrounds, ordinarily problematic when tracking an individual feature, are less so when that feature has a known spatial relationship to others not suffering from the same problem at the same time.
Reference: [7] <author> J. Hong, X. Tan, B. Pinette, R. Weiss, and E. Riseman. </author> <title> "Image-Based Homing." </title> <booktitle> In IEEE Control Systems, </booktitle> <pages> pp. 38-44, </pages> <year> 1992. </year>
Reference-contexts: Navigation proceeds by identifying landmarks along a path through the graph that are visible from their predecessors, and servoing on them in succession via visual feedback. Recognizing landmarks is a matching problem between 2-D image data and the 3-D world model. A route-based approach to navigation is described in <ref> [7] </ref> and [21]. The robots in these papers are guided in a learning stage through the environment, periodically taking visual snapshots as they go. A spherical reflection of the local environment alone is stored in [7]; a composite panoramic representation and 3-D geometry computed from it are used in [21]. <p> A route-based approach to navigation is described in <ref> [7] </ref> and [21]. The robots in these papers are guided in a learning stage through the environment, periodically taking visual snapshots as they go. A spherical reflection of the local environment alone is stored in [7]; a composite panoramic representation and 3-D geometry computed from it are used in [21]. A graph is built up by comparing newly taken snapshots with the known corpus by a matching process; paths can be planned in the completed graph with a standard algorithm. <p> They argue that taking snapshots at regular intervals leads to the storage of much unnecessary information. Moreover, new scenes must be matched against all remembered ones as they are taken in, an inefficient process. Our system departs from the work in <ref> [7] </ref> and [21] in attempting to address these problems. In the next section we give an overview of the navigation system's major 3 components and how they fit together. Following it are sections devoted to each of those components: map building, matching, navigation, and visual ser-voing. <p> A different arc in the graph will be created. This problem is avoided in <ref> [7] </ref> and [21] by taking care to capture views in all directions anywhere that matching might be essayed. As far as we can tell, though, the extra arcs and nodes created by our method do not affect its correctness. <p> S = start node, C = convergence node, and D = divergence node. locate low-level features in a scene when the robot's position cannot be precisely pinpointed; markers lessen this problem in proportion to their complexity. Two variations on feature-matching are given in [21] and <ref> [7] </ref>. Image signatures [3], neural networks [1], and other whole image techniques offer the prospect of being more robust and speedier. Image signatures have the property that the measured similarity between two views of a scene falls off smoothly as the angle or distance between them increases.
Reference: [8] <author> B. K. Horn. </author> <title> Robot Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: We assume perspective projection (x = X=Z, y = Y =Z, where x and y are image coordinates and X, Y , and Z are global coordinates), leading to the familiar equation from optical flow described in <ref> [8] </ref>: _y = 1=Z 0 x=Z xy 1 x 2 y 11 v = (U; V; W; A; B; C) T , where U , V , and W are the three translational components of motion along the X, Y , and Z axes, respectively, and A, B, and C are
Reference: [9] <author> D. Kortenkamp, T. Weymouth, E. Chown, and S. Kaplan. </author> <title> "A scene-based, multi-level representation for mobile robot spatial mapping and navigation." </title> <institution> University of Michigan Tech Report CSE-TR-119-92, </institution> <year> 1992. </year>
Reference-contexts: We do not concern ourselves as they do about the completeness of the robot's exploration of its environment, but we adopt their notion of human-specified landmarks (albeit in the form of pretaken images or stock recognition procedures) within it as a useful guide. Similarly, in <ref> [9] </ref>, [11], and [3], the robot learns a model of the environment by building a graph based on topological connectivity of interesting locations, but it attempts to decide what constitute landmarks on its own. The robot in [9] looks for gateways, or portals between spaces, with sonar, while remembering visual scenes <p> Similarly, in <ref> [9] </ref>, [11], and [3], the robot learns a model of the environment by building a graph based on topological connectivity of interesting locations, but it attempts to decide what constitute landmarks on its own. The robot in [9] looks for gateways, or portals between spaces, with sonar, while remembering visual scenes along the way for place recognition. The system in [3] also looks for gateways (waypoints), using vision and odometry; matching is accomplished by comparing image signatures for (possibly) different locations. <p> Our system stores directed images rather than panoramic ones. Though place recognition becomes more difficult with the introduction of directionality, there is less data to match and the direction of movement is implicitly stored. Some criticisms of the route-based approach are brought up in <ref> [9] </ref>. They argue that taking snapshots at regular intervals leads to the storage of much unnecessary information. Moreover, new scenes must be matched against all remembered ones as they are taken in, an inefficient process.
Reference: [10] <author> A. Kosaka, A. C. Kak. </author> <title> "Fast Vision-Guided Mobile Robot Navigation Using Model-Based Reasoning and Prediction of Uncertainties." </title> <booktitle> In CVGIP: Image Understanding, </booktitle> <volume> 56(3), </volume> <pages> pp. 271-329, </pages> <month> November, </month> <year> 1992. </year>
Reference-contexts: As a result we hope to make the process of map-learning less error-prone and perhaps faster. For context it is useful to review some previous approaches to mobile robot navigation, particularly vision-based ones. The system in <ref> [10] </ref> requires knowledge of the environment's three-dimensional geometry. The robot is supplied with its initial position and orientation; its navigation task is to find a path to a goal position and orientation. Using Kalman filtering, a model of uncertainty for the location of objects is maintained during movement.
Reference: [11] <author> B. Kuipers, Y. T. Byun. </author> <title> "A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations." </title> <booktitle> In Robotics and Autonomous Systems, </booktitle> <volume> Volume 8, </volume> <pages> pp. 47-63, </pages> <year> 1981. </year>
Reference-contexts: We do not concern ourselves as they do about the completeness of the robot's exploration of its environment, but we adopt their notion of human-specified landmarks (albeit in the form of pretaken images or stock recognition procedures) within it as a useful guide. Similarly, in [9], <ref> [11] </ref>, and [3], the robot learns a model of the environment by building a graph based on topological connectivity of interesting locations, but it attempts to decide what constitute landmarks on its own. <p> The system in [3] also looks for gateways (waypoints), using vision and odometry; matching is accomplished by comparing image signatures for (possibly) different locations. The robot in <ref> [11] </ref> uses sonar exclusively to detect places that maximize a distinctiveness function and to match or add them to its map. We make use of this idea, but by employing vision instead of sonar we expect a better ability to distinguish between places.
Reference: [12] <author> J. C. Latombe. </author> <title> Robot Motion Planning. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1991. </year>
Reference-contexts: The approach outlined above moves the robot in a direct path toward the target image without regard for objects in the way. We would like the robot to avoid such objects while still progressing toward its goal. Taking inspiration from the potential field approach to path planning <ref> [12] </ref>, we can reformulate the robot's control law by combining repulsion from sonar-detected obstacles and visual attraction to the target to get a final movement.
Reference: [13] <author> J. Leonard, H. Durrant-Whyte, and I. Cox. </author> <title> "Dynamic Map Building for an Autonomous Mobile Robot." </title> <booktitle> In IEEE International Workshop on Intelligent Robots and Systems, </booktitle> <pages> pp. 89-95, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction For a definition of navigation, we second the dictionary quote given in <ref> [13] </ref>: "the science of getting [something] from place to place; especially the method of determining position [and] course." Measuring success in this context depends, therefore, on the answer to the question "Where is the robot?" For if the robot is where we want it to be, it must have navigated there <p> To minimize the disparity between model and reality, it would seem desirable for the robot to at least make its own map and update it during use. The sonar-based algorithm of <ref> [13] </ref> does not furnish the robot with any a priori information about the environment. Rather, the robot builds a positional map of beacons using Kalman filtering to factor in uncertainty about their and its own positions. <p> Note that more than one disappearance node may be associated with a marker because of intervening branch points. When an arc is retraveled, either during exploration or navigation, the map is updated. Observed markers are matched to stored ones, modifying a confidence measure associated with each of them <ref> [13] </ref>, as well as their expected image location (leading, eventually, to an expectation associated with the average robot location along that path segment). If new markers have appeared, they are added to the stored data. If old markers cannot be found, confidence in them is downgraded.
Reference: [14] <author> M. Meng, A. C. Kak. </author> <title> "Mobile Robot Navigation Using Neural Networks and Nonmetrical Environment Models." </title> <booktitle> In IEEE Control Systems, </booktitle> <pages> pp. 30-39, </pages> <year> 1993. </year>
Reference-contexts: The task of correlating image features and remembered landmarks is made simpler by the bounds established by the uncertainty model. Path-planning follows Dijkstra's algorithm for graph traversal; the graph is constructed by skeletonizing a bitmap of configuration space. The requirements for an environmental model are loosened in <ref> [14] </ref>. Their model of the environment is a weighted graph with the weights corresponding to distances between nodes. Nodes are assigned to locales such as corridors, dead-ends, and junctions; each node is associated with a number of distinctive features visible when the robot is at that place.
Reference: [15] <editor> J. Mundy, A. Zisserman, and D. Forsyth, eds. </editor> <booktitle> Applications of Invariance in Computer Vision Springer-Verlag, </booktitle> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: Also, if the robot has come back to the same place, but somewhat off-center, it is better able to find the marker by searching the image starting from the expected image location. We have experimented with identifying coplanar groups of features as markers. Following an approach outlined in <ref> [15] </ref>, for any five points in an image one can compute a cross-ratio based on their image coordinates that is invariant if they are coplanar in three dimensions. That is, no matter how the view of those five points changes subsequently, the cross-ratio remains unchanged.
Reference: [16] <author> J. Shi, C. Tomasi. </author> <title> "Good Features to Track." </title> <booktitle> In CVPR, </booktitle> <pages> pp. 593-600, </pages> <year> 1994. </year>
Reference-contexts: Each incoming image is processed using automatic feature selection <ref> [16] </ref> to pick out corners, edges, and other interesting subimages. The best features are tracked from frame to frame using the SSD (sum of squared differences) tracking package of the X-Vision system [5]. Groups of features with certain common characteristics are identified and designated markers.
Reference: [17] <author> C. Taylor, D. Kriegman. </author> <title> "Vision-Based Motion Planning and Exploration Algorithms for Mobile Robots." </title> <booktitle> In Workshop on the Algorithmic Foundations of Robotics, </booktitle> <year> 1994. </year>
Reference-contexts: Prediction allows directed sensing: "knowing where to look" can improve 2 the speed and reliability of the mapping process. A somewhat different approach is described in <ref> [17] </ref>. Their model of the environment is a graph that is built up by the robot itself as it explores. The nodes of the graph are in a sense predetermined, though, by special landmarks (barcodes) placed around the environment. <p> A large, mostly empty space can be criss-crossed by any number of paths, making it harder to represent its essence by stitching them together. Furthermore, we sidestep the problem of the robot identifying interesting locations (discussed in <ref> [17] </ref>) by stipulating that a human identify what places might be possible destinations or starting points. This is done by furnishing a set of images, or key scenes, of interesting places, each one representative of what the robot would see if it were there.
Reference: [18] <author> K. Toyama, G. Hager. </author> <title> "Distraction-Proof Tracking: Keeping One's Eye on the Ball." </title> <note> Submitted to IROS `95, </note> <year> 1995. </year>
Reference-contexts: First, the task of tracking features from frame to frame is easier and more robust when locational constraints between features may be enforced [6], <ref> [18] </ref>. Momentary occlusions or distracting backgrounds, ordinarily problematic when tracking an individual feature, are less so when that feature has a known spatial relationship to others not suffering from the same problem at the same time.
Reference: [19] <author> M. Turk, D. Morgenthaler, K. Gremban, and M. Marra. </author> <title> "VITS|A Vision System for Autonomous Land Vehicle Navigation." </title> <journal> In IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Volume 10, Number 3, </volume> <pages> pp. 342-361, </pages> <year> 1988. </year>
Reference-contexts: forms of visual servoing: homing, in which a robot can move without constraint in order to transform one image into another (e.g., a far-away house into a close-up house); and maintaining, 10 in which a robot attempts to move in a certain way while obeying a visual constraint (e.g., road-following <ref> [19] </ref>). The system in [20] uses visual servoing not to home as we do, but rather to align the robot toward the vanishing point of two parallel lines.
Reference: [20] <author> Z. Zhang, R. Weiss, and A. Hanson. </author> <title> "Visual Servoing Control of Autonomous Robot Navigation." </title> <journal> Submitted to Journal of Robotics and Automation, </journal> <year> 1995. </year>
Reference-contexts: Scanning over the entire range for each feature and computing the error at each spot is computationally expensive, however. This resort to odometry might be eliminated in some cases by modifying an idea in <ref> [20] </ref>. <p> The system in <ref> [20] </ref> uses visual servoing not to home as we do, but rather to align the robot toward the vanishing point of two parallel lines.
Reference: [21] <author> J. Zheng, S. Tsuji. </author> <title> "Panoramic Representation for Route Recognition by a Mobile Robot." </title> <journal> In International Journal of Computer Vision, </journal> <volume> 9(1), </volume> <pages> pp. 55-76, </pages> <year> 1992. </year>
Reference-contexts: Recognizing landmarks is a matching problem between 2-D image data and the 3-D world model. A route-based approach to navigation is described in [7] and <ref> [21] </ref>. The robots in these papers are guided in a learning stage through the environment, periodically taking visual snapshots as they go. A spherical reflection of the local environment alone is stored in [7]; a composite panoramic representation and 3-D geometry computed from it are used in [21]. <p> in [7] and <ref> [21] </ref>. The robots in these papers are guided in a learning stage through the environment, periodically taking visual snapshots as they go. A spherical reflection of the local environment alone is stored in [7]; a composite panoramic representation and 3-D geometry computed from it are used in [21]. A graph is built up by comparing newly taken snapshots with the known corpus by a matching process; paths can be planned in the completed graph with a standard algorithm. When executing a path, image-based servoing is used to traverse individual edge segments. <p> They argue that taking snapshots at regular intervals leads to the storage of much unnecessary information. Moreover, new scenes must be matched against all remembered ones as they are taken in, an inefficient process. Our system departs from the work in [7] and <ref> [21] </ref> in attempting to address these problems. In the next section we give an overview of the navigation system's major 3 components and how they fit together. Following it are sections devoted to each of those components: map building, matching, navigation, and visual ser-voing. <p> A different arc in the graph will be created. This problem is avoided in [7] and <ref> [21] </ref> by taking care to capture views in all directions anywhere that matching might be essayed. As far as we can tell, though, the extra arcs and nodes created by our method do not affect its correctness. <p> S = start node, C = convergence node, and D = divergence node. locate low-level features in a scene when the robot's position cannot be precisely pinpointed; markers lessen this problem in proportion to their complexity. Two variations on feature-matching are given in <ref> [21] </ref> and [7]. Image signatures [3], neural networks [1], and other whole image techniques offer the prospect of being more robust and speedier. Image signatures have the property that the measured similarity between two views of a scene falls off smoothly as the angle or distance between them increases.
References-found: 21

