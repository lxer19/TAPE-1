URL: ftp://ftp.cs.rochester.edu/pub/u/rosca/gp/96.aaai.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/rosca/research.html
Root-URL: 
Email: E-mail: frosca,danag@cs.rochester.edu  
Title: Evolution-based Discovery of Hierarchical Behaviors  
Author: Justinian P. Rosca and Dana H. Ballard 
Address: Rochester, NY 14627  
Affiliation: Computer Science Department University of Rochester  
Date: 1996  
Note: In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96) AAAI Press The MIT Press,  
Abstract: Procedural representations of control policies have two advantages when facing the scale-up problem in learning tasks. First they are implicit, with potential for inductive generalization over a very large set of situations. Second they facilitate modularization. In this paper we compare several randomized algorithms for learning modular procedural representations. The main algorithm, called Adaptive Representation through Learning (ARL) is a genetic programming extension that relies on the discovery of subroutines. ARL is suitable for learning hierarchies of subroutines and for constructing policies to complex tasks. ARL was successfully tested on a typical reinforcement learning problem of controlling an agent in a dynamic and nondeterministic environment where the discovered subroutines correspond to agent behaviors. 
Abstract-found: 1
Intro-found: 1
Reference: <editor> Gordon, D. F., and DesJardins, M. </editor> <year> 1995. </year> <title> Evaluation and selection of biases in machine learning. </title> <booktitle> Machine Learning 20 </booktitle> <pages> 5-22. </pages>
Reference-contexts: In GP, the use of subroutines biases the search for good programs besides offering the possibility to reuse code. An adaptive learning system selects its bias automatically. An overview of current efforts in this active research area appeared recently in <ref> (Gordon & DesJardins 1995) </ref>. Conclusions Although the Pac-Man is a typical reinforcement learning task it was successfully approached using GP. GP worked well for the task because it used an implicit representation of the agent state space. Therefore, GP solutions acquire generality and can be modularized. <p> Additionally, a comparison with memoryless and state-maintaining reinforcement learning algorithms is also worth further investigation. ARL can be studied as an example of a system where procedural bias interacts with representational bias <ref> (Gordon & DesJardins 1995) </ref>. This may shed additional light on how GP exploits structures and constructs solutions.
Reference: <author> Holland, J. H. </author> <year> 1992. </year> <title> Adaptation in Natural and Artificial Systems, An Introductory Analysis with Applications to Biology, </title> <booktitle> Control and Artificial Intelligence. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher> <address> Second edition (First edition, </address> <year> 1975). </year>
Reference: <author> Kirkpatrick, S.; Gelatt, C.; and Vecchi, M. </author> <year> 1983. </year> <title> Optimization by simulated annealing. </title> <booktitle> Science 220 </booktitle> <pages> 671-680. </pages>
Reference-contexts: In order to test the Pac-Man representations we generated both simple and modular random programs. programs were obtained with problem representation B, followed by the modular versions M-A and M-B. Simulated Annealing A simple technique for iterative improvement is simulated annealing (SA) <ref> (Kirkpatrick, Gelatt, & Vec-chi 1983) </ref>. Although SA performs well in continuous spaces, it has also been applied to combinatorial optimization problems in search for optima of functions of discrete variables. For example, a similar search technique, GSAT (Selman & Kautz 1993), offers the best known performance for hard satisfiability problems.
Reference: <author> Koza, J. R. </author> <year> 1992. </year> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press. </publisher>
Reference-contexts: This work is placed into a broader perspective in the related work section, before concluding remarks. The Application Task We consider the problem of controlling an agent in a dynamic environment, similar to the well known Pac-Man game described in more detail in <ref> (Koza 1992) </ref>. An agent, called Pac-Man, can be controlled to act in a maze of corridors. Up to four monsters chase Pac-Man most of the time. Food pellets, energizers and fruit objects result in rewards of 10, 50 and 2000 points respectively when reached by Pac-Man. <p> First Solutions Random Search The parse trees of random programs are created recursively in a top-down manner. First a function is chosen as the label of the program root node and then for each formal argument of the function new subprograms are generated recursively. <ref> (Koza 1992) </ref> describes a method called ramped-half-and-half for generating very diverse random tree structures. No particular structure is favored due to both randomly choosing node labels and randomly varying the tree depth and balance.
Reference: <author> Koza, J. R. </author> <year> 1994. </year> <title> Genetic Programming II. </title> <publisher> MIT Press. </publisher>
Reference-contexts: Thus machine learning, or machine discovery approaches that attempt to cope with non-trivial problems should provide some hierarchical mechanisms for creating and exploiting such modularity. An approach that incorporates modularization mechanisms is genetic programming (GP) with automatically defined functions (ADF) <ref> (Koza 1994) </ref>. In ADF-GP computer programs are modularized through the explicit use of subroutines. One shortcoming of this approach is the need to design an appropriate architecture for programs, i.e. set in advance the number of subroutines and arguments, as well as the nature of references among subroutines. <p> Building blocks are relevant pieces of partial solutions that can be assembled together in order to generate better partial solutions. Our modular representations are modeled after the automatically defined functions (ADF) approach <ref> (Koza 1994) </ref>. ADF is an extension of GP where individuals are represented by a fixed number of components or branches to be evolved: a predefined number of function branches and a main program branch.
Reference: <author> Koza, J. R. </author> <year> 1995. </year> <title> Gene duplication to enable genetic programming to concurrently evolve both the architecture and work-performing steps of a computer program. </title> <editor> In Mellish, C. S., ed., </editor> <booktitle> IJCAI, </booktitle> <volume> volume 1, </volume> <pages> 734-740. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: One shortcoming of this approach is the need to design an appropriate architecture for programs, i.e. set in advance the number of subroutines and arguments, as well as the nature of references among subroutines. A biologically inspired approach to architecture discovery introduced in <ref> (Koza 1995) </ref> is based on new operations for duplicating parts of the genome. Code duplication transformations seem to work well in combination with crossover, as duplication protects code against the destructive effects of crossover. Duplication operations are performed such that they preserve the semantics of the resulting programs.
Reference: <author> Laarhoven, v. P. J. M. </author> <year> 1988. </year> <title> Theoretical and computational aspects of simulated annealing. </title> <address> Netherlands: </address> <publisher> Centrum voor Wiskunde en Informatica. </publisher>
Reference-contexts: The cooling schedule is defined by the following parameters: the initial temperature T 0 , the final temperature T f , the length of the Markov chain at a fixed temperature L and the number of iterations G <ref> (Laarhoven 1988) </ref>.
Reference: <author> Maes, P. </author> <year> 1993. </year> <booktitle> Behavior-based artificial intelligence. In SAB-2. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: Representation modularity is important from a scale-up perspective. Ideally, a modular representation organizes the knowledge and competences of the agent such that local changes, improvements or tuning do not affect the functioning of most other components. Researchers in "behavior-based" artificial intelligence <ref> (Maes 1993) </ref> talk about integrated competences or behaviors as given decompositions of the problem. We are interested in discovering decompositions that naturally emerge from the interaction agent-environment. 1 For the same reason, parameterized function approxi-mators have been used to replace table lookup in reinforcement learning.
Reference: <author> Olsson, R. </author> <year> 1995. </year> <title> Inductive functional programming using incremental program transformation. </title> <booktitle> Artificial Intelligence 74 </booktitle> <pages> 55-81. </pages>
Reference-contexts: More generally, pure functional languages such as the ml language treat functions and values according to a formal set of rules. As a consequence, the process of formal reasoning applied to program control structures can be automated. One recent example of such an attempt is ADATE <ref> (Olsson 1995) </ref>. ADATE iteratively transforms programs in a top-down manner, searching the space of programs written in a subset of ml for a program that explains a set of 7 initial training cases. ADATE creates new predicates by abstraction transformations. Algorithms that use predicate invention are called constructive induction algorithms.
Reference: <author> O'Reilly, U.-M. </author> <year> 1995. </year> <title> An Analysis of Genetic Programming. </title> <type> Ph.D. Dissertation, </type> <institution> Ottawa-Carleton Institute for Computer Science. </institution>
Reference: <author> Rosca, J. P., and Ballard, D. H. </author> <year> 1994. </year> <title> Hierarchical self-organization in genetic programming. </title> <booktitle> In 11th ICML, </booktitle> <pages> 251-258. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Subroutines have small sizes due to the explicit bias towards small blocks of code. The hierarchy of evolved subroutines allows a program to grow in effective size (i.e. in expanded structural complexity, see <ref> (Rosca & Ballard 1994) </ref>) if this offers an evolutionary advantage. For instance, the best-of-generation program evolved by ARL in one run with problem representation B is extremely modular. ARL discovered 86 subroutines during the 50 generations while it ran.
Reference: <author> Rosca, J. P., and Ballard, D. H. </author> <year> 1995. </year> <title> Causality in genetic programming. </title> <editor> In Eshelman, L., ed., </editor> <booktitle> ICGA95, </booktitle> <pages> 256-263. </pages> <address> San Francisco, CA., USA: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: The architecture imposes the possible hierarchical references between branches. For instance, if we order the branches in the sequence ADF 0 ; ADF 1 , ADF 2 , P rogram-Body then a branch may invoke any component to its left. <ref> (Rosca 1995) </ref> analyzed how this preimposed hierarchical ordering biases the way ADF searches the space of programs. In the "bottom-up evolution hypothesis" he conjectured that ADF representations become stable in a bottom-up fashion. Early in the process changes are focused towards the evolution of low level functions. <p> In the "bottom-up evolution hypothesis" he conjectured that ADF representations become stable in a bottom-up fashion. Early in the process changes are focused towards the evolution of low level functions. Later, changes are focused towards higher levels in the hierarchy of functions (see also <ref> (Rosca & Ballard 1995) </ref>). ARL will consider a bottom-up approach to subroutine discovery as the default. The ARL Algorithm The nature of GP is that programs that contain useful code tend to have a higher fitness and thus their offspring tend to dominate the population.
Reference: <author> Rosca, J. P. </author> <year> 1995. </year> <title> Genetic programming exploratory power and the discovery of functions. </title> <editor> In McDonnell, J. R.; Reynolds, R. G.; and Fogel, D. B., eds., </editor> <booktitle> Evolutionary Programming IV Proceedings of the Fourth Annual Conference on Evolutionary Programming, </booktitle> <pages> 719-736. </pages> <address> San Diego, CA, USA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The architecture imposes the possible hierarchical references between branches. For instance, if we order the branches in the sequence ADF 0 ; ADF 1 , ADF 2 , P rogram-Body then a branch may invoke any component to its left. <ref> (Rosca 1995) </ref> analyzed how this preimposed hierarchical ordering biases the way ADF searches the space of programs. In the "bottom-up evolution hypothesis" he conjectured that ADF representations become stable in a bottom-up fashion. Early in the process changes are focused towards the evolution of low level functions. <p> In the "bottom-up evolution hypothesis" he conjectured that ADF representations become stable in a bottom-up fashion. Early in the process changes are focused towards the evolution of low level functions. Later, changes are focused towards higher levels in the hierarchy of functions (see also <ref> (Rosca & Ballard 1995) </ref>). ARL will consider a bottom-up approach to subroutine discovery as the default. The ARL Algorithm The nature of GP is that programs that contain useful code tend to have a higher fitness and thus their offspring tend to dominate the population.
Reference: <author> Russell, S. J., and Norvig, P. </author> <year> 1995. </year> <title> Artificial Intelligence: A Modern Approach. </title> <address> Englewood Cliffs, New Jersey: </address> <publisher> Prentice Hall. </publisher>
Reference: <author> Selman, B., and Kautz, H. A. </author> <year> 1993. </year> <title> An empirical study of greedy local search for satisfiability testing. </title> <booktitle> In AAAI. </booktitle> <publisher> AAAI Press/The MIT Press. </publisher> <pages> 46-51. </pages>
Reference-contexts: Simulated Annealing A simple technique for iterative improvement is simulated annealing (SA) (Kirkpatrick, Gelatt, & Vec-chi 1983). Although SA performs well in continuous spaces, it has also been applied to combinatorial optimization problems in search for optima of functions of discrete variables. For example, a similar search technique, GSAT <ref> (Selman & Kautz 1993) </ref>, offers the best known performance for hard satisfiability problems. The space of programs is also non-continuous. SA has been previously tested on program discovery problems (O'Reilly 1995).
Reference: <author> Simon, H. A. </author> <year> 1973. </year> <title> The organization of complex systems. </title> <editor> In Howard H. Pattee, G. B., ed., </editor> <title> Hierarchy Theory; The Challenge of Complex Systems. </title> <address> New York. </address> <pages> 3-27. </pages>
Reference-contexts: This discovery of modularity while learning or solving a problem can considerably speed up the task, as the time needed for the system to "evolve" based on its modular subsystems is much shorter than if the system evolves from its elementary parts <ref> (Simon 1973) </ref>. Thus machine learning, or machine discovery approaches that attempt to cope with non-trivial problems should provide some hierarchical mechanisms for creating and exploiting such modularity. An approach that incorporates modularization mechanisms is genetic programming (GP) with automatically defined functions (ADF) (Koza 1994).
Reference: <author> Stahl, I. </author> <year> 1993. </year> <title> Predicate invention in ILP an overview. </title> <editor> In Brazdil, P. B., ed., </editor> <booktitle> ECML, </booktitle> <pages> 313-322. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: More importantly, invented predicates may generalize over the search space thus compensating for missing background knowledge. A difficult problem is evaluating the quality of new predicates <ref> (Stahl 1993) </ref>. The predicate invention problem is related to the more general problem of bias in machine learning. In GP, the use of subroutines biases the search for good programs besides offering the possibility to reuse code. An adaptive learning system selects its bias automatically.
Reference: <author> Tackett, W. A. </author> <year> 1994. </year> <title> Recombination, Selection and the Genetic Construction of Computer Programs. </title> <type> Ph.D. Dissertation, </type> <institution> University of Southern California. </institution>
Reference-contexts: Blocks are generalized by replacing some random subset of terminals in the block with variables (see Step 5a in Figure 3). Variables become formal arguments of the created subroutine. 5 4 This condition is imposed in order to eliminate from consideration blocks containing introns and hitch-hiking phenomena <ref> (Tackett 1994) </ref>. It is represented by the pruning step (4) in Figure 3. 5 In the typed implementation block generalization additionally assigns a signature to each subroutine created. <p> ARL modularity emerges during a run as subroutines are created or deleted. SGP solutions are not explicitly modular. Other Related Work Tackett studied, under the name "gene banking," ways in which programs constructed by genetic search can be mined off-line for subexpressions that represent salient problem traits <ref> (Tackett 1994) </ref>. He hypothesized that traits which display the same fitness and frequency characteristics are salient. Unfortunately, many subex-pressions are in a hierarchical "part-of" relation. Thus it may be hard to distinguish "hitchhikers" from true salient expressions.
References-found: 18

