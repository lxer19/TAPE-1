URL: http://www.cs.colorado.edu/~grunwald/Papers/ISCA97-MarkovPrefetch/paper.ps
Refering-URL: http://www.cs.colorado.edu/~grunwald/Papers/ISCA97-MarkovPrefetch/
Root-URL: http://www.cs.colorado.edu
Email: djoseph@watson.ibm.com  grunwald@cs.colorado.edu  
Title: Prefetching using Markov Predictors  
Author: Doug Joseph Dirk Grunwald 
Address: P.O. Box 218  Yorktown Heights, NY 10598  Boulder, Colorado, 80309-0430  
Affiliation: IBM T. J. Watson Research Lab  IBM. T. J. Watson Research  Department of Computer Science University of Colorado  
Abstract: Prefetching is one approach to reducing the latency of memory operations in modern computer systems. In this paper, we describe the Markov prefetcher. This prefetcher acts as an interface between the on-chip and off-chip cache, and can be added to existing computer designs. The Markov prefetcher is distinguished by prefetch-ing multiple reference predictions from the memory subsystem, and then prioritizing the delivery of those references to the processor. This design results in a prefetching system that provides good coverage, is accurate and produces timely results that can be effectively used by the processor. In our cycle-level simulations, the Markov Prefetcher reduces the overall execution stalls due to instruction and data memory operations by an average of 54% for various commercial benchmarks while only using two thirds the memory of a demand-fetch cache organization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Alexander and G. Kedem. </author> <title> Distributed predictive cache design for high performance memory system. </title> <booktitle> In Second International Symposium on High-Performance Computer Architecture, </booktitle> <month> Feb </month> <year> 1996. </year>
Reference-contexts: For the work-loads used in this paper, we find that there are insufficient stride references in the applications we examined for this scheme to offer much improvement in prefetch coverage. However, in the case of very stridy workloads, it seems clear this approach is advantageous. Alexander and Kedem <ref> [1] </ref> proposed a mechanism similar to correlation-based prefetching but used a distributed prediction table.
Reference: [2] <author> J.L. Baer. </author> <title> Dynamic improvements of locality in virtual memory systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 2, </volume> <month> March </month> <year> 1976. </year>
Reference-contexts: We also simulated this design, but do not present the performance since it was uniformly worse than correlation-based prefetching. Correlation-Based Prefetching Markov prefetching is a continuing evolution of what has been called correlation-based prefetch-ing [3]. The basic concept of correlation-based prefetching was introduced by Baer <ref> [2] </ref> in the context of paged virtual memory systems. Baer associated a single prefetch address with each memory address referenced and developed algorithms for updating the prefetch address based upon observed reference patterns. When a reference occurs, the associated prefetch address is checked for residence in physical memory.
Reference: [3] <author> M.J. Charney and A.P. Reeves. </author> <title> Generalized correlation based hardware prefetching. </title> <type> Technical Report EE-CEG-95-1, </type> <institution> Cor-nell University, </institution> <month> Feb </month> <year> 1995. </year>
Reference-contexts: However research on prefetching for unstructured workloads is not nearly as common and only recently have results in this area begun to appear <ref> [17, 3, 9, 13] </ref>. The section on correlation-based prefetching is especially relevant since Markov prefetching is an evolution of correlation based prefetching. <p> We also simulated this design, but do not present the performance since it was uniformly worse than correlation-based prefetching. Correlation-Based Prefetching Markov prefetching is a continuing evolution of what has been called correlation-based prefetch-ing <ref> [3] </ref>. The basic concept of correlation-based prefetching was introduced by Baer [2] in the context of paged virtual memory systems. Baer associated a single prefetch address with each memory address referenced and developed algorithms for updating the prefetch address based upon observed reference patterns. <p> This mechanism is very much like the allocation filters introduced by Palacharla et.al. [14] to improve the accuracy of stream buffers and serves a similar purpose here. Charney and Reeves <ref> [3] </ref> extend the Pomerene and Puzak mechanism and apply it to the L1 miss reference stream rather than directly to the load/store stream. Besides being the first to publish results on the Pomerene and Puzak scheme, this work improved upon the mechanism in two significant ways. <p> The third bar shows the bandwidth reduction that occurs when a simple noise rejection filter is used. This filter was proposed by Pomerene and Puzak [15] and also examined by Charney and Reeves <ref> [3] </ref>. It is similar to the filter used by Kessler [14] to improve the accuracy of stream buffers a prefetch request is not dispatched until the prefetch pattern has been seen twice. The fourth column shows the performance for accuracy based adaptivity. <p> Although the Markov prefetcher can be characterized as an extension to the correlation prefetcher previously described by Pomerene and Puzak [15] and Charney and Reeves <ref> [3] </ref>, there are a number of design issues presented by the Markov prefetcher. The prefetcher must be able to launch multiple prefetch requests and prioritize them, and the prefetcher must consider some mechanism to limit the bandwidth devoted to prefetching.
Reference: [4] <author> T.F. Chen and J.L. Baer. </author> <title> Reducing memory latency via non-blocking and prefetching caches. </title> <booktitle> In ASPLOS-V, </booktitle> <pages> pages 5161, </pages> <month> Oct </month> <year> 1992. </year>
Reference-contexts: We then show the effect of the various parameters in the Markov prefetcher implementation. 3 Prior Work Hardware and software prefetching schemes have been devised that are effective on structured workloads <ref> [14, 4, 12, 8] </ref>. However research on prefetching for unstructured workloads is not nearly as common and only recently have results in this area begun to appear [17, 3, 9, 13]. The section on correlation-based prefetching is especially relevant since Markov prefetching is an evolution of correlation based prefetching. <p> Stride Prefetchers Chen and Baer investigate a mechanism for prefetching data references characterized by regular strides <ref> [4] </ref>. Their scheme is based on a reference prediction table (RPT) and look-ahead program counter (LPC). The RPT is a cache, tagged with the instruction address of load instructions. <p> When this matches the offset stored in the table, a prefetch is launched for the data address one offset ahead of the current data address. In <ref> [4] </ref>, the reference address stream was used to index the reference prediction table. In practice, we found little performance difference between using the reference addresses or the miss address stream. Our later simulations of stride prefetchers use the miss address stream.
Reference: [5] <author> Z. Cvetanovic and D. Bhandakar. </author> <title> Characterization of alpha axp using tp and spec workloads. </title> <booktitle> In 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 6070, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Past research has indicated that operating system activity and multi-programming can significantly effect cache performance <ref> [10, 5] </ref>. However, little has been reported on the impact these factors have on prefetching strategies. At the same time, the cache performance on such work-loads tends to be significantly worse than other workloads, making the need for latency reducing methods such as prefetching even more important.
Reference: [6] <author> K.I. </author> <title> Farkas and N.P. Jouppi. Complexity/performance tradeoffs with non-blocking loads. </title> <booktitle> In 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 211222, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: If there are no duplicates when adding an entry, an empty slot is filled, or if there are no empty slots the last slot (the least recently used entry) is replaced. This design is similar to the stream buffer design of Farkas et.al. <ref> [6] </ref>. The primary difference is that in [6], the entire buffer is shifted, discarding all the entries above the matching one. There are many other parameters that affect the performance of this hardware configuration. <p> This design is similar to the stream buffer design of Farkas et.al. <ref> [6] </ref>. The primary difference is that in [6], the entire buffer is shifted, discarding all the entries above the matching one. There are many other parameters that affect the performance of this hardware configuration. <p> Palacharla and Kessler [14] extended the stream buffer mechanism to also detect non-unit strides without having direct access to the program context. They also introduced a noise rejection scheme for improving the accuracy of stream buffers. Farkas et.al. <ref> [6] </ref> further enhanced stream buffers by providing them with an associative lookup capability and a mechanism for detecting and eliminating the allocation of stream buffers to duplicate streams. Later, we compare the performance of the design of Farkas et. al. to Markov prefetchers. <p> Stream buffers generally exhibit greater prefetch coverage than stride prefetchers, but also are much more inaccurate, even when using allocation filters. However, while detecting non-unit strides is natural for stride prefetchers, providing non-unit stride detection to stream buffers is more difficult <ref> [6] </ref>. Stream buffers tend to be more efficient in use of resources than stride prefetchers. <p> There was no advantage to tables larger than 2KBytes for the traces we considered, and there was little difference between a 4-way associative table larger than 2-KBytes or the 16-entry fully-associative table. In earlier work on stream buffers, Farkas et. al. <ref> [6] </ref> used four three-entry stream buffers. We used eight because there was a small, but noticable improvement in coverage up to that point. We also varied the number of entries in the stream buffer and reached the same conclusions stated in [6]: below three entries, prefetches have insufficient lead time and <p> In earlier work on stream buffers, Farkas et. al. <ref> [6] </ref> used four three-entry stream buffers. We used eight because there was a small, but noticable improvement in coverage up to that point. We also varied the number of entries in the stream buffer and reached the same conclusions stated in [6]: below three entries, prefetches have insufficient lead time and above three entries, accuracy begins to fall off rapidly.. The Correlation and Markov prefetchers are combined with the second-level cache.
Reference: [7] <author> N. Jouppi. </author> <title> Improving direct-mapped cache performance by the addition of a small fully associative cache and prefetch buffers. </title> <booktitle> In 17th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: Prefetch mechanisms usually assume that programs access A, B, C, D, C, E, A, C, F, F, E, A, A, B, C, D, E, A, B, C, D, C via transition probabilities. memory using a particular pattern or access model. For example, stream buffers <ref> [7] </ref> assume that memory is accessed as a linear stream, possibly with a non-unit stride. Once an access model has been determined, architects design a hardware mechanism to capture or approximate that reference stream. <p> In practice, we found little performance difference between using the reference addresses or the miss address stream. Our later simulations of stride prefetchers use the miss address stream. Stream Buffers Jouppi introduced stream buffers as one of two significant methods to improved direct mapped cache performance <ref> [7] </ref>. In contrast to stride prefetchers, stream buffers are designed to prefetch sequential streams of cache lines, independent of program context. The design presented by Jouppi is unable to detect streams containing non-unit strides.
Reference: [8] <author> A.C. Klaiber and H.M. Levy. </author> <title> An architecture for software controlled data prefetching. </title> <booktitle> In 18th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: We then show the effect of the various parameters in the Markov prefetcher implementation. 3 Prior Work Hardware and software prefetching schemes have been devised that are effective on structured workloads <ref> [14, 4, 12, 8] </ref>. However research on prefetching for unstructured workloads is not nearly as common and only recently have results in this area begun to appear [17, 3, 9, 13]. The section on correlation-based prefetching is especially relevant since Markov prefetching is an evolution of correlation based prefetching.
Reference: [9] <author> M.H. Lipasti and et.al. Spaid: </author> <title> Software prefetching in pointer and call intensive enviornments. </title> <booktitle> In Proceedings of 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 231236, </pages> <month> Nov </month> <year> 1995. </year>
Reference-contexts: However research on prefetching for unstructured workloads is not nearly as common and only recently have results in this area begun to appear <ref> [17, 3, 9, 13] </ref>. The section on correlation-based prefetching is especially relevant since Markov prefetching is an evolution of correlation based prefetching. <p> However, since prediction information is embedded in the program at compile time, compiler based schemes lack the flexibility to account for the dynamic behavior of a workload. Compiler based techniques have been proposed which insert prefetch instructions at sites where pointer dereferences are anticipated. Lipasti et. al. <ref> [9] </ref> developed heuristics that consider pointers passed as arguments on procedure calls and insert prefetches at the call sites for the data referenced by the pointers.
Reference: [10] <author> A.M. Maynard, C.M. Donnelly, and B.R. Olszewski. </author> <title> Contrasting characteristics and cache performance of technical and multi-user commercial workloads. </title> <booktitle> In ASPOS-VI, </booktitle> <month> Apr </month> <year> 1994. </year>
Reference-contexts: Past research has indicated that operating system activity and multi-programming can significantly effect cache performance <ref> [10, 5] </ref>. However, little has been reported on the impact these factors have on prefetching strategies. At the same time, the cache performance on such work-loads tends to be significantly worse than other workloads, making the need for latency reducing methods such as prefetching even more important. <p> Commercial environments tend to be unstructured because of high process switch rates, high random I/O rates, and they typically involve a large number of user processes <ref> [10] </ref>. Transaction processing also utilize searching and sorting algorithms that give rise to unstructured access patterns. Examples of commercial workloads include: transaction processing, multi-user software development environments, network and file server kernels, desktop publishing tools, and compilers. <p> The simulations of this work are based on address traces of technical and commercial industry-standard benchmarks. They were captured on an IBM RS/6000 running AIX using a proprietary tracing tool developed at IBM. Cache performance characteristics on most of the traces we used were presented by Maynard et.al. <ref> [10] </ref>. The traces include both instruction and data references obtained throughout the execution of multiple processes containing kernel, user and shared library activity. Four of the traces used were generated from unstructured technical codes, and four were from commercially oriented workloads. Table 1 provides a brief summary of all eight. <p> Four of the traces used were generated from unstructured technical codes, and four were from commercially oriented workloads. Table 1 provides a brief summary of all eight. More information on the benchmarks can be found in <ref> [10] </ref> and the appendix. Table 2 shows statistics indicative of the impact of these differences. The first two columns show the percentage of the instructions that are branches and the percentage of these that are taken branches. <p> In contrast, the percentage of taken branches in commercial workloads is relatively low, indicating that these work-loads execute relatively few iterations per loop. The lack of dominant loops is why these commercial workloads have a lower probability of re-executing recent instructions, leading to higher miss rates <ref> [10] </ref>. We note that Spice is anomalous in this trend, yet still has a very low I-miss rate. As with all the SPEC92 and SPEC95 benchmarks, the instruction working set for Spice is very small and fits comfortably in the I-cache.
Reference: [11] <author> S. Mehrota and H. Luddy. </author> <title> Examination of a memory access classification scheme for pointer-intensive and numeric programs. </title> <type> Technical Report CRSD tech report 1351, </type> <institution> CRSD University of Illinois, </institution> <month> Dec </month> <year> 1995. </year>
Reference-contexts: The combination provides better coverage than either mechanism alone, and is generally more accurate than stream buffers alone (although less accurate than a stride prefetcher alone). Indirect Stream Detectors Mehrota <ref> [11] </ref> describes a hardware data prefetching scheme based on the recursion that occurs in linked list traversals. We also simulated this design, but do not present the performance since it was uniformly worse than correlation-based prefetching.
Reference: [12] <author> T.C. Mowry, M.S. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In ASPLOS-V, </booktitle> <pages> pages 6273, </pages> <month> Oct </month> <year> 1992. </year>
Reference-contexts: We then show the effect of the various parameters in the Markov prefetcher implementation. 3 Prior Work Hardware and software prefetching schemes have been devised that are effective on structured workloads <ref> [14, 4, 12, 8] </ref>. However research on prefetching for unstructured workloads is not nearly as common and only recently have results in this area begun to appear [17, 3, 9, 13]. The section on correlation-based prefetching is especially relevant since Markov prefetching is an evolution of correlation based prefetching. <p> The section on correlation-based prefetching is especially relevant since Markov prefetching is an evolution of correlation based prefetching. Static predictors Almost all static predictors rely on the compiler to determine possible L1 cache misses and embed the information into the code in the form of prefetch instructions. Mowry et.al. <ref> [12] </ref> show that structured scientific codes are very amenable to this approach. However, they also show that their techniques failed to improve performance of the pointer intensive applications used in their study. In terms of hardware resources, compiler based schemes are inexpensive to implement.
Reference: [13] <author> T. Ozawa and et.al. </author> <title> Cache miss heuristics an preloading techniques for general-purpose programs. </title> <booktitle> In Proceedings of 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 243248, </pages> <month> Nov </month> <year> 1995. </year>
Reference-contexts: However research on prefetching for unstructured workloads is not nearly as common and only recently have results in this area begun to appear <ref> [17, 3, 9, 13] </ref>. The section on correlation-based prefetching is especially relevant since Markov prefetching is an evolution of correlation based prefetching. <p> Compiler based techniques have been proposed which insert prefetch instructions at sites where pointer dereferences are anticipated. Lipasti et. al. [9] developed heuristics that consider pointers passed as arguments on procedure calls and insert prefetches at the call sites for the data referenced by the pointers. Ozawa et.al. <ref> [13] </ref> classify loads whose data address comes from a previous load as list accesses, and perform code motions to separate them from the instructions that use the data fetched by list accesses. Stride Prefetchers Chen and Baer investigate a mechanism for prefetching data references characterized by regular strides [4].
Reference: [14] <author> S. Palacharla and R.E. Kessler. </author> <title> Evaluating stream buffers as a secondary cache replacement. </title> <booktitle> In 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2433, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: We then show the effect of the various parameters in the Markov prefetcher implementation. 3 Prior Work Hardware and software prefetching schemes have been devised that are effective on structured workloads <ref> [14, 4, 12, 8] </ref>. However research on prefetching for unstructured workloads is not nearly as common and only recently have results in this area begun to appear [17, 3, 9, 13]. The section on correlation-based prefetching is especially relevant since Markov prefetching is an evolution of correlation based prefetching. <p> In contrast to stride prefetchers, stream buffers are designed to prefetch sequential streams of cache lines, independent of program context. The design presented by Jouppi is unable to detect streams containing non-unit strides. Palacharla and Kessler <ref> [14] </ref> extended the stream buffer mechanism to also detect non-unit strides without having direct access to the program context. They also introduced a noise rejection scheme for improving the accuracy of stream buffers. <p> If there is no match in any stream buffer, a new stream buffer is allocated to the new stream. In the model employed in this paper, an empty or least recently used buffer is selected for replacement. The noise rejection scheme introduced by Palacharla <ref> [14] </ref> is also employed in the allocation of stream buffers used in this research. It is a simple filtering mechanism that waits for two consecutive L1 misses to sequential cache line addresses before allocating a stream buffer to the stream. <p> They also introduce a confirmation mechanism that only activates new pairs when data that would have been prefetched would also have been used. This mechanism is very much like the allocation filters introduced by Palacharla et.al. <ref> [14] </ref> to improve the accuracy of stream buffers and serves a similar purpose here. Charney and Reeves [3] extend the Pomerene and Puzak mechanism and apply it to the L1 miss reference stream rather than directly to the load/store stream. <p> The third bar shows the bandwidth reduction that occurs when a simple noise rejection filter is used. This filter was proposed by Pomerene and Puzak [15] and also examined by Charney and Reeves [3]. It is similar to the filter used by Kessler <ref> [14] </ref> to improve the accuracy of stream buffers a prefetch request is not dispatched until the prefetch pattern has been seen twice. The fourth column shows the performance for accuracy based adaptivity.
Reference: [15] <author> J. Pomerene and et.al. </author> <title> Prefetching system for a cache having a second directory for sequentially accessed blocks. </title> <type> Technical Report 4807110, U.S. Patent Office, </type> <month> Feb </month> <year> 1989. </year>
Reference-contexts: The first address of the pair is referred to as the parent or key that is used to select a child prefetch address. The first instance of correlation-based prefetching being applied to data prefetching is presented in a patent application by Pomerene and Puzak <ref> [15] </ref>. A hardware cache is used to hold the parent-child information. A further innovation they introduce is to incorporate other information into the parent key. They suggest the use of bits from the instruction causing the miss, and also bits from the last data address referenced. <p> The second bar shows the same information for Markov prefetch-ing, using four addresses predictors and an LRU prioritization policy. The third bar shows the bandwidth reduction that occurs when a simple noise rejection filter is used. This filter was proposed by Pomerene and Puzak <ref> [15] </ref> and also examined by Charney and Reeves [3]. It is similar to the filter used by Kessler [14] to improve the accuracy of stream buffers a prefetch request is not dispatched until the prefetch pattern has been seen twice. The fourth column shows the performance for accuracy based adaptivity. <p> Although the Markov prefetcher can be characterized as an extension to the correlation prefetcher previously described by Pomerene and Puzak <ref> [15] </ref> and Charney and Reeves [3], there are a number of design issues presented by the Markov prefetcher. The prefetcher must be able to launch multiple prefetch requests and prioritize them, and the prefetcher must consider some mechanism to limit the bandwidth devoted to prefetching.
Reference: [16] <author> Gary Tyson, Matt Farrens, and Andrew Pleszkun. </author> <title> A modified approach to data cache management. </title> <booktitle> In Proceedings of 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 93105, </pages> <month> Nov </month> <year> 1995. </year>
Reference-contexts: The fifth column shows the effect of using an automatic cache-no-allocate (CNA) policy to reduce the portion of the cache line that is actually placed in the prefetch buffer. This design is based on the CNA mechanism of Tyson et. al. <ref> [16] </ref>. As in [16], we use a table of two bit counters to determine if a specific memory instruction should use a CNA policy. The prefetcher records information both about the predicted address and the instruction predicted to issue the memory reference. <p> The fifth column shows the effect of using an automatic cache-no-allocate (CNA) policy to reduce the portion of the cache line that is actually placed in the prefetch buffer. This design is based on the CNA mechanism of Tyson et. al. <ref> [16] </ref>. As in [16], we use a table of two bit counters to determine if a specific memory instruction should use a CNA policy. The prefetcher records information both about the predicted address and the instruction predicted to issue the memory reference. <p> The CNA mechanism improves the basic cache performance for MM4, but decreases the performance for G92 and SPICE. This is an artifact of the CNA bandwidth reduction technique <ref> [16] </ref>. 5.2 Comparison Using a Memory-system Simulator We have seen that the Markov predictor provides better prefetch coverage than the other prefetchers we examined. We used a memory-level simulator to determine if that improved coverage resulted in better performance for the memory subsystem.

References-found: 16

