URL: http://www.cs.nyu.edu/cs/projects/proteus/sekine/papers/iwpt95.ps
Refering-URL: http://www.cs.nyu.edu/cs/projects/proteus/sekine/index.html
Root-URL: http://www.cs.nyu.edu
Email: sekine,grishman@cs.nyu.edu  
Title: A Corpus-based Probabilistic Grammar with Only Two  
Author: Non-terminals Satoshi SEKINE Ralph GRISHMAN 
Address: New York University 715 Broadway, 7th floor New York, NY 10003, USA  
Affiliation: Computer Science Department  
Abstract: The availability of large, syntactically-bracketed corpora such as the Penn Tree Bank affords us the opportunity to automatically build or train broad-coverage grammars, and in particular to train probabilistic grammars. A number of recent parsing experiments have also indicated that grammars whose production probabilities are dependent on the context can be more effective than context-free grammars in selecting a correct parse. To make maximal use of context, we have automatically constructed, from the Penn Tree Bank version 2, a grammar in which the symbols S and NP are the only real non-terminals, and the other non-terminals or grammatical nodes are in effect embedded into the right-hand-sides of the S and NP rules. For example, one of the rules extracted from the tree bank would be S -&gt; NP VBX JJ CC VBX NP [1] (where NP is a non-terminal and the other symbols are terminals - part-of-speech tags of the Tree Bank). The most common structure in the Tree Bank associated with this expansion is (S NP (VP (VP VBX (ADJ JJ) CC (VP VBX NP)))) [2]. So if our parser uses rule [1] in parsing a sentence, it will generate structure [2] for the corresponding part of the sentence. Using 94% of the Penn Tree Bank for training, we extracted 32,296 distinct rules (23,386 for S, and 8,910 for NP). We also built a smaller version of the grammar based on higher frequency patterns for use as a back-up when the larger grammar is unable to produce a parse due to memory limitation. We applied this parser to 1,989 Wall Street Journal sentences (separate from the training set and with no limit on sentence length). Of the parsed sentences (1,899), the percentage of no-crossing sentences is 33.9%, and Parseval recall and precision are 73.43% and 72.61%.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E.Black, et.al. </author> <title> "A procedure for Quantitatively Comparing the Syntactic Coverage of English Grammars" Proc. </title> <booktitle> of Fourth DARPA Speech and Natural Language Workshop (1991) </booktitle>
Reference-contexts: Next, the evaluation using parseval method on parsed sentences is shown in Table 4. "Parse-val" is the measurement of parsed result proposed by Black et.al. <ref> [1] </ref>. The result in the table is the result achieved by the G-0 grammar, supplemented by the result using the G-2 grammar, if the larger grammar can't generate a parse tree.
Reference: [2] <author> E.Black, F.Jelinek, J.Lafferty, D.Magerman, R.Mercer, </author> <title> S.Roukos "Towards History-based Grammars: Using Richer Models for Probabilistic Parsing" ACL-93 (1993) </title>
Reference-contexts: Furthermore, experiments over the past few years have shown the benefits of using probabilistic information in parsing, and the large corpus allows us to train the probabilities of a grammar [8] [7] [11] <ref> [2] </ref> [4] [12]. A number of recent parsing experiments have also indicated that grammars whose production probabilities are dependent on the context can be more effective than context-free grammars in selecting a correct parse. <p> This context sensitivity can be acquired easily using a large corpus, whereas human ability to compute such information is obviously limited. There have been several attempts to build context-dependent grammars based on large corpora. [14] [11] [13] <ref> [2] </ref> [4] [12]. As is evident from the two lists of citations, there has been considerable research involving both probabilistic grammar based on syntactically-bracketed corpora and context-sensitivity. For example, Black proposed `History based parsing', a context-dependent grammar trained using a large corpus [2]. <p> grammars based on large corpora. [14] [11] [13] <ref> [2] </ref> [4] [12]. As is evident from the two lists of citations, there has been considerable research involving both probabilistic grammar based on syntactically-bracketed corpora and context-sensitivity. For example, Black proposed `History based parsing', a context-dependent grammar trained using a large corpus [2]. History-based parsing uses decision-tree methods to identify the most useful information in the prior context for selecting the next production to try. This approach, however, requires a hand-constructed grammar as a starting point. Bod [4] has described how to parse directly from a tree bank.
Reference: [3] <editor> E.Black "Parsing English By Computer: </editor> <booktitle> The State Of the Art" ATR International Workshop on Speech Translation (1993) </booktitle>
Reference-contexts: It is not easy to compare these numbers between systems, because some conditions of the test sentences, length, complexity, etc., affect the result. However, roughly speaking, these numbers are comparable to or better than the score of so-called traditional grammar, or handmade grammars. For example, Black <ref> [3] </ref> cited the best non-crossing score using a traditional grammars as 41% and the average of several systems as 22%. 6 Future work Analyzing the errors made by the parser, we found that a considerable number of unparsed sentences (including no-parse and memory-exhausted sentences) and wrongly parsed sentences contain special symbols,
Reference: [4] <institution> R.Bod "Using an Annotated Corpus as a Stochastic Grammar" EACL-93 (1993) </institution>
Reference-contexts: Furthermore, experiments over the past few years have shown the benefits of using probabilistic information in parsing, and the large corpus allows us to train the probabilities of a grammar [8] [7] [11] [2] <ref> [4] </ref> [12]. A number of recent parsing experiments have also indicated that grammars whose production probabilities are dependent on the context can be more effective than context-free grammars in selecting a correct parse. <p> This context sensitivity can be acquired easily using a large corpus, whereas human ability to compute such information is obviously limited. There have been several attempts to build context-dependent grammars based on large corpora. [14] [11] [13] [2] <ref> [4] </ref> [12]. As is evident from the two lists of citations, there has been considerable research involving both probabilistic grammar based on syntactically-bracketed corpora and context-sensitivity. For example, Black proposed `History based parsing', a context-dependent grammar trained using a large corpus [2]. <p> History-based parsing uses decision-tree methods to identify the most useful information in the prior context for selecting the next production to try. This approach, however, requires a hand-constructed grammar as a starting point. Bod <ref> [4] </ref> has described how to parse directly from a tree bank. A new parse tree can be assembled from arbitrary subtrees drawn from the tree bank; the parser searches for the combination with highest probability. This can, in principle, take advantage of arbitrary context information.
Reference: [5] <institution> E.Brill "Automatic Grammar Induction and Parsing Free Text: </institution> <note> A Transformation-Based Approach" ACL-93 (1993) </note>
Reference: [6] <author> T.Briscoe, </author> <title> J.Carroll "Generalized Probabilistic LR Parsing of Natural Language (corpora) with Unification-Based Grammars" Computational Linguistics Vol.19,No.1 (1993) </title>
Reference: [7] <editor> M.Chitaro, </editor> <booktitle> R.Grishman "Statistical parsing of messages" In proceedings of the Speech and Natural Language Workshop (1990) </booktitle>
Reference-contexts: Furthermore, experiments over the past few years have shown the benefits of using probabilistic information in parsing, and the large corpus allows us to train the probabilities of a grammar [8] <ref> [7] </ref> [11] [2] [4] [12]. A number of recent parsing experiments have also indicated that grammars whose production probabilities are dependent on the context can be more effective than context-free grammars in selecting a correct parse.
Reference: [8] <author> R.Garside, </author> <title> F.Leech "A Probabilistic Parser" EACL-85 (1985) </title>
Reference-contexts: Furthermore, experiments over the past few years have shown the benefits of using probabilistic information in parsing, and the large corpus allows us to train the probabilities of a grammar <ref> [8] </ref> [7] [11] [2] [4] [12]. A number of recent parsing experiments have also indicated that grammars whose production probabilities are dependent on the context can be more effective than context-free grammars in selecting a correct parse.
Reference: [9] <author> R.Grishman, </author> <title> J.Sterling "Generalizing Automatically Generated Selectional Patterns" COLING-94 (1994) </title>
Reference-contexts: We have been conducting research on automatically acquiring these selectional constraints, and we are planning to incorporate this semantic knowledge into the parser <ref> [9] </ref>, [15]. Introducing lexical information in the parser is also useful for other kinds of the structural disambiguation. We showed this in minor modification 4, by introducing new categories based on lexicon. However, the method we used to choose the candidates depended on human intuition.
Reference: [10] <author> M.Marcus, B.Santorini, </author> <title> M.Marcinkiewicz "Building a large annotated corpus of English: the Penn Tree bank" in the distributed Penn Tree Bank Project CD-ROM, Linguistic Data Consortium, </title> <institution> University of Pennsylvania. </institution>
Reference-contexts: This modification also works to reduce the number of grammar rules. Finally, we know that the Penn Tree Bank project tried to reduce the number of part-of-speech categories in order to ease the tagging effort. The Penn Tree Bank manual <ref> [10] </ref> says that they combine categories, in cases where finer distinctions can be recovered based on lexical information.
Reference: [11] <author> D.Magerman, </author> <title> C.Weir "Efficiency, Robustness and Accuracy in Picky Chart Parsing" ACL-92 (1992) </title>
Reference-contexts: Furthermore, experiments over the past few years have shown the benefits of using probabilistic information in parsing, and the large corpus allows us to train the probabilities of a grammar [8] [7] <ref> [11] </ref> [2] [4] [12]. A number of recent parsing experiments have also indicated that grammars whose production probabilities are dependent on the context can be more effective than context-free grammars in selecting a correct parse. <p> This context sensitivity can be acquired easily using a large corpus, whereas human ability to compute such information is obviously limited. There have been several attempts to build context-dependent grammars based on large corpora. [14] <ref> [11] </ref> [13] [2] [4] [12]. As is evident from the two lists of citations, there has been considerable research involving both probabilistic grammar based on syntactically-bracketed corpora and context-sensitivity. For example, Black proposed `History based parsing', a context-dependent grammar trained using a large corpus [2].
Reference: [12] <institution> D.Magerman "Statistical Decision-Tree Models for Parsing" ACL-95 (1995) </institution>
Reference-contexts: Furthermore, experiments over the past few years have shown the benefits of using probabilistic information in parsing, and the large corpus allows us to train the probabilities of a grammar [8] [7] [11] [2] [4] <ref> [12] </ref>. A number of recent parsing experiments have also indicated that grammars whose production probabilities are dependent on the context can be more effective than context-free grammars in selecting a correct parse. <p> This context sensitivity can be acquired easily using a large corpus, whereas human ability to compute such information is obviously limited. There have been several attempts to build context-dependent grammars based on large corpora. [14] [11] [13] [2] [4] <ref> [12] </ref>. As is evident from the two lists of citations, there has been considerable research involving both probabilistic grammar based on syntactically-bracketed corpora and context-sensitivity. For example, Black proposed `History based parsing', a context-dependent grammar trained using a large corpus [2].
Reference: [13] <author> Y.Shabes, </author> <title> R.Waters "Lexicalized Context-Free Grammars" ACL-93 (1993) </title>
Reference-contexts: This context sensitivity can be acquired easily using a large corpus, whereas human ability to compute such information is obviously limited. There have been several attempts to build context-dependent grammars based on large corpora. [14] [11] <ref> [13] </ref> [2] [4] [12]. As is evident from the two lists of citations, there has been considerable research involving both probabilistic grammar based on syntactically-bracketed corpora and context-sensitivity. For example, Black proposed `History based parsing', a context-dependent grammar trained using a large corpus [2].
Reference: [14] <author> R.Simmons, </author> <title> Y.Yu "The Acquisition and Application of Context Sensitive Grammar for English" ACL-91 (1991) </title>
Reference-contexts: This context sensitivity can be acquired easily using a large corpus, whereas human ability to compute such information is obviously limited. There have been several attempts to build context-dependent grammars based on large corpora. <ref> [14] </ref> [11] [13] [2] [4] [12]. As is evident from the two lists of citations, there has been considerable research involving both probabilistic grammar based on syntactically-bracketed corpora and context-sensitivity. For example, Black proposed `History based parsing', a context-dependent grammar trained using a large corpus [2].
Reference: [15] <author> S.Sekine, S.Ananiadou, J.J.Carroll, J.Tsujii: </author> <title> "Linguistic Knowledge Generator" COLING-92 (1992) </title>
Reference-contexts: We have been conducting research on automatically acquiring these selectional constraints, and we are planning to incorporate this semantic knowledge into the parser [9], <ref> [15] </ref>. Introducing lexical information in the parser is also useful for other kinds of the structural disambiguation. We showed this in minor modification 4, by introducing new categories based on lexicon. However, the method we used to choose the candidates depended on human intuition.
References-found: 15

