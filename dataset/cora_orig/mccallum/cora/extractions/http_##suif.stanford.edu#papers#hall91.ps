URL: http://suif.stanford.edu/papers/hall91.ps
Refering-URL: http://suif.stanford.edu/papers/papers.html
Root-URL: 
Title: Managing Interprocedural Optimization  
Author: by Mary Wolcott Hall Keith D. Cooper Linda Torczon Robert Bixby 
Degree: A Thesis Submitted in Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy Approved, Thesis Committee: Ken Kennedy, Chairman Professor of Computer Science  Associate Professor of Computer Science  Professor  
Date: April, 1991  
Address: Houston, Texas  
Affiliation: RICE UNIVERSITY  Research Associate  of Mathematical Sciences  
Abstract-found: 0
Intro-found: 0
Reference: [AC72] <author> F. E. Allen and J. Cocke. </author> <title> A catalogue of optimizing transformations. </title> <editor> In J. Rustin, editor, </editor> <booktitle> Design and Optimization of Compilers. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1972. </year>
Reference-contexts: To eliminate dependences and improve parallelism, compound statements such as loops and conditionals must be located and moved outside of the loop. Such extensions to the traditional algorithm have been described [CLZ86] [FOW87]. 141 Loop unswitching. A similar technique, loop unswitching <ref> [AC72] </ref>, can be applied when a condition in the loop is loop-invariant, but the code guarded by the condition is not. For an if-then-else clause within a loop, two copies of the loop are created.
Reference: [AC76] <author> F.E. Allen and J. Cocke. </author> <title> A program data flow analysis procedure. </title> <journal> Communications of the ACM, </journal> <volume> 19(3) </volume> <pages> 137-147, </pages> <month> March </month> <year> 1976. </year>
Reference-contexts: For this reason, Cooper was not concerned about restricting cloning to ensure efficiency. 5.3.2 Similar Techniques Cloning bears some relationship to node splitting as used in interval-based data-flow analysis techniques <ref> [AC76] </ref>. Two copies of a control flow node are made, and edges into the node are reassociated with the copies, to break up unstructured program portions. In this case, the copies are made to enable the analysis, not to improve optimization.
Reference: [ACF + 80] <author> F.E. Allen, J.L. Carter, J. Fabri, J. Ferrante, W.H. Harrison, P.G. Loewner, </author> <title> and L.H. Trevillyan. The experimental compiling system. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 24(6) </volume> <pages> 695-715, </pages> <month> November </month> <year> 1980. </year>
Reference-contexts: A very restricted inlining strategy eliminated about 20 percent of the calls, with about a 2 percent decrease in the program intermediate representation. The general-purpose optimizer in the Experimental Compiling System at IBM used inline substitution as a key component in the implementation of an optimizing compiler [Har77b] <ref> [ACF + 80] </ref>. For these two compilers, the kinds of improvements anticipated by inlining are not given.
Reference: [AK84] <author> R. Allen and K. Kennedy. </author> <title> Automatic loop interchange. </title> <booktitle> In Proceedings of the SIGPLAN 84 Symposium on Compiler Construction. ACM, </booktitle> <month> June </month> <year> 1984. </year>
Reference-contexts: Even if the outermost loop cannot be parallelized, it may be possible to move an inner parallel loop to the outermost position. This transformation is called loop permutation [Ban90], a generalization of loop interchange <ref> [AK84] </ref> [AK87] [Wol86] [Wol89]. To perform loop permutation, the dependence analysis phase looks for loops that do not carry any dependences. These loops will be candidates for moving to the outermost position. This section presents two versions of loop permutation. First, we address the problem of perfectly nested loops.
Reference: [AK87] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic translation of Fortran programs to vector form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4) </volume> <pages> 491-542, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: A large amount of code can then be executed in parallel without having to synchronize parallel processes. However, an outermost loop cannot be parallelized if it carries a dependence <ref> [AK87] </ref>. A particular loop carries a dependence if the references of the source and sink of the dependence occur in different iterations of the loop. Even if the outermost loop cannot be parallelized, it may be possible to move an inner parallel loop to the outermost position. <p> Even if the outermost loop cannot be parallelized, it may be possible to move an inner parallel loop to the outermost position. This transformation is called loop permutation [Ban90], a generalization of loop interchange [AK84] <ref> [AK87] </ref> [Wol86] [Wol89]. To perform loop permutation, the dependence analysis phase looks for loops that do not carry any dependences. These loops will be candidates for moving to the outermost position. This section presents two versions of loop permutation. First, we address the problem of perfectly nested loops.
Reference: [All90] <author> R. Allen. </author> <title> Private communication, </title> <month> March </month> <year> 1990. </year>
Reference-contexts: Unfortunately, loops containing conditionals are not fused with any other loops <ref> [All90] </ref>. On cedeta, the optimized loop was distributed into four separate loops, two of which consisted of only a single statement. Parallelizing the short loops caused execution time of the loop to increase by 25 percent.
Reference: [ASU86] <author> A.V. Aho, R. Sethi, and J.D. Ullman. </author> <booktitle> Compilers, Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: Further improvements are possible with other techniques. For example, in loops that are not do loops, we can look for induction variables <ref> [ASU86] </ref>. Then, we locate exit branches from the loop and try to determine how many iterations of the loop are required to make the exit condition evaluate to true. <p> Locating variables whose values are based on the induction variable. Some variables that are functions of the induction variable are auxiliary induction variables and can be located using a variant of induction variable elimination <ref> [ASU86] </ref>. Once 142 these variables are located, their possible ranges must be determined.
Reference: [Bal79] <author> J. E. Ball. </author> <title> Predicting the effects of optimization on a procedure body. </title> <booktitle> In Proceedings of the SIGPLAN 79 Symposium on Compiler Construction. ACM, </booktitle> <month> August </month> <year> 1979. </year>
Reference-contexts: However, other work suggests that improvements will arise from (1) propagating constant-valued parameters through the body of the called procedure, (2) enabling code motion across the former call site, and (3) exposing more information to the register allocator <ref> [Bal79] </ref> [RG89a] [WZ85]. Unfortunately, very little experimental evidence has been published to prove these assumptions. Prior to this study, only two others have analyzed through experimentation the impact of inlining on optimization. Richardson and Ganapathi's study on 5 Pascal programs demonstrated an average of 20 percent improvement [RG89a]. <p> We were interested in determining which of these was the case. If secondary effects were masking improvements, then other interprocedural techniques might still be effective. Improved constant propagation has been suggested as the most important effect of inlining <ref> [Bal79] </ref> [WZ89]. To test this, we performed an experiment to isolate the effects of interprocedural constant propagation. We also studied improvements when constants information is further refined by cloning. <p> This information is required by the propagation phase, which maps variables in CloningV ars (p) to variables that determine their values at call sites invoking p. 4.4.1 Phase 1: Local Analysis The approach for local analysis is inspired by Ball's work <ref> [Bal79] </ref>. To determine the benefits of constant propagation on a procedure (as a result of inlining), he calculates the strong dependency set for each statement. For a given statement, the strong dependency set consists of those variables that, if constant, would determine the value of the statement. <p> However, when compilers perform optimization and register allocation, the interaction with inlining can reduce the importance of call overhead. What is really significant is the amount of optimization enabled by inlining. Only Ball and Cooper select call sites for inlining based on the optimizations that will result <ref> [Bal79] </ref> [Coo83]. Ball formulates improvement estimates resulting from inline substitution as a data-flow problem [Bal79]. The strong dependency sets, as described in section 4.4, provide the set of formal parameters whose values determine a statement's value. <p> What is really significant is the amount of optimization enabled by inlining. Only Ball and Cooper select call sites for inlining based on the optimizations that will result <ref> [Bal79] </ref> [Coo83]. Ball formulates improvement estimates resulting from inline substitution as a data-flow problem [Bal79]. The strong dependency sets, as described in section 4.4, provide the set of formal parameters whose values determine a statement's value. <p> Previous research, particularly Ball's work, used inlining to accomplish the same result <ref> [Bal79] </ref>. Cooper observed that cloning had an advantage over inlining in refining constants 86 information, since multiple calls to a procedure can share a cloned version. A further advantage of cloning is that it does not introduce as many compilation dependences among procedures as compared to inlining.
Reference: [Bal89] <author> V. Balasundaram. </author> <title> Interactive Parallelization of Numerical Scientific Programs. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, TX, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: This technique locates the convex hull surrounding two regions representing accesses. Dependence testing requires an assymp totically exponential linear inequality solver. * Balasundaram proposed an approach to summarizing array accesses designed to locate opportunities for task-level parallelism <ref> [Bal89] </ref>. It can also be used to provide programmers with a visual description of affected regions of an array. For this purpose, Data Access Descriptors also include a traversal order and a reference template. The implementation of rsd analysis described in [HK91] gives up some precision in favor of efficiency.
Reference: [Ban79] <author> J. P. Banning. </author> <title> An efficient way to find the side effects of procedure calls and the aliases of variables. </title> <booktitle> In Proceedings of the Sixth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> January </month> <year> 1979. </year> <month> 160 </month>
Reference-contexts: Once aliasing information is known, mod and ref sets are updated to reflect this knowledge. This is because a variable is modified (referenced) if any of its aliases are modified (referenced). Side-effect analysis and alias analysis are performed separately, with alias information added to the side-effect information <ref> [Ban79] </ref>. Recent algorithms have been proposed to calculate aliasing and interprocedural side-effect information in time effectively linear in the size of the call multigraph [CK88b] [CK89]. However, currently in ParaScope, we use an iterative technique based on Banning's equations [Ban79]. <p> are performed separately, with alias information added to the side-effect information <ref> [Ban79] </ref>. Recent algorithms have been proposed to calculate aliasing and interprocedural side-effect information in time effectively linear in the size of the call multigraph [CK88b] [CK89]. However, currently in ParaScope, we use an iterative technique based on Banning's equations [Ban79]. In addition to making side-effect information correct, alias information is useful to the register allocator. Compilers cannot place variables in registers that might be aliased, so the absence of a variable in the alias set enables allocating it to a register.
Reference: [Ban90] <author> U. Banerjee. </author> <title> A theory of loop permutations. </title> <editor> In D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, chapter 4. </booktitle> <publisher> The MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Even if the outermost loop cannot be parallelized, it may be possible to move an inner parallel loop to the outermost position. This transformation is called loop permutation <ref> [Ban90] </ref>, a generalization of loop interchange [AK84] [AK87] [Wol86] [Wol89]. To perform loop permutation, the dependence analysis phase looks for loops that do not carry any dependences. These loops will be candidates for moving to the outermost position. This section presents two versions of loop permutation.
Reference: [BC86] <author> M. Burke and R. Cytron. </author> <title> Interprocedural dependence analysis and par-allelization. </title> <booktitle> In Proceedings of the SIGPLAN 86 Symposium on Compiler Construction. ACM, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: Just as with inlining, this can be difficult if the programmer has reshaped the array at the call so that dimension sizes are different. We could linearize the subscript expressions <ref> [BC86] </ref>, reducing all arrays to a single dimension. However, this yields very complicated subscript expressions that may greatly hamper the ability of the dependence analyzer in disproving dependences, especially since dependence analysis techniques are more successful on simple subscript expressions. <p> The first of these, by Burke and Cytron, linearizes all subscript expressions, resulting in a list of accesses to 1-dimensional objects <ref> [BC86] </ref>. The effect of linearization can be a significant loss of information for dependence testing. Li and Yew developed a more precise representation of array accesses, called atom images [LY88a] [LY88b]. These precisely represent linear subscript 147 expressions in rectangular and triangular iteration spaces.
Reference: [BCKT90] <author> M. Burke, K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural optimization: Eliminating unnecessary recompilation. </title> <type> Technical Report TR90-126, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: Recompilation analysis determines procedures requiring recompilation, attempting to minimize the recompilation requirements. Interprocedural transformations such as inlining and cloning require further consideration. 6.5.1 Recompilation Analysis for Interprocedural Information Torczon presents three different approaches for minimizing the need for recompilation after interprocedural optimization [Tor85] [CKT86b] <ref> [BCKT90] </ref>. The method currently used in the program compiler compares interprocedural information from the last compilation of a module with information obtained in the current compilation. <p> The Cray fortran compiler manages dependences when it performs automatic inlining. The recompilation algorithm in this chapter extends the previous algorithms, accommodating incremental inlining and cloning [Tor85] [CKT86b] <ref> [BCKT90] </ref>. Torczon suggests that inlining and cloning be part of the program compiler design, but does not provide an algorithm for determining if inlined or cloned versions require recompilation. A representation of inlining and cloning similar to the one in this chapter is described in [BCKT90]. <p> inlining and cloning [Tor85] [CKT86b] <ref> [BCKT90] </ref>. Torczon suggests that inlining and cloning be part of the program compiler design, but does not provide an algorithm for determining if inlined or cloned versions require recompilation. A representation of inlining and cloning similar to the one in this chapter is described in [BCKT90].
Reference: [Bur87] <author> M. Burke. </author> <title> An interval-based approach to exhaustive and incremental interprocedural analysis. </title> <type> Research Report RC 12702, </type> <institution> IBM Yorktown Heights, </institution> <month> September </month> <year> 1987. </year>
Reference: [CCH + 87] <author> A. Carle, K. D. Cooper, R. T. Hood, K. Kennedy, L. Torczon, and S. K. Warren. </author> <title> A practical environment for scientific programming. </title> <journal> IEEE Computer, </journal> 20(11) 75-89, November 1987. 
Reference: [CCH + 88] <author> D. Callahan, K. Cooper, R. T. Hood, K. Kennedy, and L. M. Torczon. </author> <title> ParaScope: a parallel programming environment. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 2(4) </volume> <pages> 84-89, </pages> <month> December </month> <year> 1988. </year>
Reference: [CCHK90] <author> D. Callahan, A. Carle, M. W. Hall, and K. Kennedy. </author> <title> Constructing the procedure call multigraph. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(4) </volume> <pages> 483-487, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: It also describes transformations to apply following inlining to further enhance parallelization. 152 8.2 Implementation Status 8.2.1 Program Compiler Large portions of the program compiler have already been implemented, and further implementation is planned. The current program compiler first builds the call multi-graph, using the precise algorithm <ref> [CCHK90] </ref>. Then interprocedural information is calculated on the call multigraph. Currently, the program compiler builds scalar mod, ref, alias and constant sets. rsd information is also available in the environment, although it is calculated outside of ParaScope.
Reference: [CCK88] <author> D. Callahan, J. Cocke, and K. Kennedy. </author> <title> Estimating interlock and improving balance for pipelined architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(4) </volume> <pages> 334-358, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: loop nest for dgemv3: do j = 1, n Y (1, i) = Y (1, i) + X (1, j) * A (j, i) enddo enddo For dgemv1, the results are similar. 70 At this point we are finally in a position to make use of the transformations described in <ref> [CCK88] </ref>. We perform loop interchange, loop fusion, 14 unroll and jam, and scalar replacement. <p> The goal for inlining is simple | we want to inline call sites in a way that enables application of the memory-management optimizations. Memory-management optimizations are used to adjust balance <ref> [CCK88] </ref>. A machine's balance fi M is the number of floating point operations that can be performed in the time it takes for a single memory access. The balance of a loop fi l is roughly the ratio of memory accesses to floating point operations in the loop.
Reference: [CCK90] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the SIGPLAN 90 Conference on Programming Language Design and Implementation. ACM, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: 0.3 1.6 wanal1 0.0 0.0 wave 5.9 6.0 avg 4.8 5.3 to constant propagation and cloning. what constitutes an important constant later in the chapter, we will return to this experiment. 4.2 Optimizing matrix300 Colleagues at Rice have been demonstrating impressive results with optimizations designed to improve memory hierarchy performance <ref> [CCK90] </ref>. In particular, strip mining is used to improve cache blocking, scalar replacement is used to place array values in registers, and unroll and jam is used to adjust register pressure and loop balance. <p> Further, the Blas routines were renamed to emphasize that all floating-point computations are double-precision. 68 We would like to apply optimizations to reduce the number of memory accesses in the loop. The key to avoiding memory accesses is recognizing reuse <ref> [CCK90] </ref>. When values are used multiple times, they can be kept in registers (or cache). By exposing reuse, accesses to memory are converted to register or cache accesses, which are much faster. Techniques for exposing reuse include scalar replacement and unroll and jam. <p> However, if the measure indicates a loop is compute-bound, it would definitely be compute-bound when variables were reused in registers or cache. Thus, it allows us to ignore the loops that it considers compute-bound. To take reuse into account in measuring loop balance requires dependence information <ref> [CCK90] </ref>. A dependence suggests the occurrence of two accesses to the same memory location. Certain types of dependences indicate that two accesses refer to the same value two reads, or a write followed by a read.
Reference: [CCKT86] <author> D. Callahan, K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural constant propagation. </title> <booktitle> In Proceedings of the SIGPLAN 86 Symposium on Compiler Construction. ACM, </booktitle> <month> June </month> <year> 1986. </year> <month> 161 </month>
Reference-contexts: By hand, we applied interprocedural constant propagation to the 8 programs from the in-lining study. The constants were located using the "Pass Through" method improved by side-effect information, as described in <ref> [CCKT86] </ref>. After applying constant propagation, we looked for procedures invoked from multiple call sites in the program. We examined values of global variables and actual parameters at each call site. <p> This would require that enough information be recorded during local analysis of a procedure to be able to determine values of expressions dependent upon incoming constants. Recorded functions that are based on unknown parameter values are called jump functions <ref> [CCKT86] </ref>. <p> For each one of these, we construct a jump function that describes the expression value as a function of potential interprocedural constants <ref> [CCKT86] </ref>. To reduce the number of jump functions, they are only provided for program points whose values could become known through interprocedural constants, as determined during the calculation of CloningVars. With this information and a CloningVector describing constant interprocedural values, the value of a StateVector can be determined. <p> Since the problem of finding all variables that are constant at run-time is undecidable [KU77], the set calculated is an approximation. The algorithm used for interprocedural constant propagation is given in <ref> [CCKT86] </ref>. It relies on a jump function J c at the call site c which gives the values of the actual parameters of c as functions of the formal parameters of the called procedure. These jump functions are determined by the module editor on completion of an editing session. <p> There are several alternative methods for generating jump functions in the editor, with each improvement to the information requiring additional analysis. In the current implementation in ParaScope, we use a modification of the pass through scheme described in <ref> [CCKT86] </ref>. This detects constants when they are directly passed at a call, or when they are passed as parameters through a chain of calls without modification before being passed. <p> In fact, changes to backward interprocedural solutions may also affect solutions to forward problems. In particular, mod results can be used to improve constant information, by formulating jump functions to include values of a variable conditional on whether it is modified at a call site <ref> [CCKT86] </ref>. Although taking advantage of improved backward solutions to refine forward solutions may seem desirable, this prevents a two phase algorithm on subsequent compilations.
Reference: [CFR + 89] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and K. Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In Conference Record of the Sixteenth Annual Symposium on Principles of Programming Languages, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Variables determining the value of the guards on the assignments will also be discovered since all guards are considered important program points. Local analysis is based on a static single assignment (SSA) representation of the source <ref> [CFR + 89] </ref>. To represent a program in SSA form, it is transformed so that only one definition reaches a use. Definitions are given unique names, and each use of a variable is renamed to correspond to the definition that reaches it. <p> SSA form is used in constant propagation because the space requirement is much less than traditional data-flow techniques on the control flow graph, and it has a better expected time bound [WZ89] <ref> [CFR + 89] </ref>. We use the SSA representation for this reason, but also because SSA form is already constructed in ParaScope to perform local analysis for constant propagation. Algorithm. The primary goal of the local analysis algorithm is to compute the portion of CloningVars used directly in this procedure.
Reference: [Cho88] <author> F. Chow. </author> <title> Minimizing register usage penalty at procedure calls. </title> <booktitle> In Proceedings of the SIGPLAN 88 Conference on Programming Language Design and Implementation. ACM, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: The callee saved registers are saved in a procedure before it uses them and are restored before exit from the procedure. Callee-saved registers are used for values that span a long range within a procedure <ref> [Cho88] </ref>. Upon inlining the last call in a procedure, the caller is able to freely use the caller-saved registers without ever having to save and restore them. Thus, inlining the final call in a procedure allows all registers to be used freely throughout the entire procedure body. <p> we explain cloning + constants constants ffi cedeta -4 5 9 euler -52 -51 11 linpackd -24 -5 3 wanal1 -4 6 8 wave -29 -23 24 avg -23 -14 11 constant propagation, and after cloning. 11 The mips compiler performs interprocedural register allocation at its highest level of optimization <ref> [Cho88] </ref>. The result of increased constants information appeared to interact poorly with interproce-dural register allocation. We speculate that simplification of inner loop bodies may have permitted loop unrolling in a few cases.
Reference: [CHT90a] <author> K. D. Cooper, M. Hall, and L. Torczon. </author> <title> The perils of interprocedural knowledge. </title> <type> Technical Report TR90-132, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Without complex analysis, the compiler must assume that the references can overlap. This introduces the data interlocks that we observed. A discussion of the analysis required to detect the changes brought about by inlining is found in <ref> [CHT90a] </ref>. Finally, design decisions that are justifiable for code written by a programmer may have unforeseen consequences when applied to code generated by the inliner.
Reference: [CHT90b] <author> K.D. Cooper, M.W. Hall, and L. Torczon. </author> <title> An experiment with inline substitution. </title> <type> Technical Report TR90-128, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> July </month> <year> 1990. </year> <note> To appear in Software|Practice and Experience. </note>
Reference: [CK84] <author> K. Cooper and K. Kennedy. </author> <title> Efficient computation of flow insensitive interprocedural summary information. </title> <booktitle> In Proceedings of the SIGPLAN 84 Sym. on Compiler Construction. ACM, </booktitle> <month> June </month> <year> 1984. </year>
Reference: [CK88a] <author> D. Callahan and K. Kennedy. </author> <title> Analysis of interprocedural side effects in a parallel programming environment. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 517-550, </pages> <year> 1988. </year>
Reference-contexts: We suggest some optimizations to further reduce dependences in inlined loops, and describe an experiment to determine the effectiveness of the optimizations in increasing parallelism. The chapter also describes a form of array side-effect analysis called regular section analysis <ref> [CK88a] </ref> [HK91]. This chapter provides an overview of interprocedural optimization for enhancing parallelization. It is organized into five sections. The next section describes inter-procedural information required to perform dependence analysis of loops containing procedure calls. <p> At the end of this chapter, we describe a number of techniques designed to provide information about array side-effects. We base our transformations on regular section descriptors (rsds) <ref> [CK88a] </ref> [HK91]. Regular sections describe side-effects to a few important substructures of arrays: single elements, rows, columns, grids and their higher dimensional analogs. The restriction to a few shapes makes the implementation efficient as compared to other techniques.
Reference: [CK88b] <author> K. Cooper and K. Kennedy. </author> <title> Interprocedural side-effect analysis in linear time. </title> <booktitle> In Proceedings of the SIGPLAN 88 Conference on Programming Languages Design and Implementation. ACM, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: Thus, the set values can change once for every possible element in the set. We expect this number to be small, and to not grow with the procedure size. This is consistent with assumptions made in other interprocedural analysis algorithms <ref> [CK88b] </ref> [CK89]. 81 4.4.2 Phase 2: Propagation The propagation phase adds to CloningVars (p) any variables of p that can be used at important program points in p's successors in the call multigraph. <p> Side-effect analysis and alias analysis are performed separately, with alias information added to the side-effect information [Ban79]. Recent algorithms have been proposed to calculate aliasing and interprocedural side-effect information in time effectively linear in the size of the call multigraph <ref> [CK88b] </ref> [CK89]. However, currently in ParaScope, we use an iterative technique based on Banning's equations [Ban79]. In addition to making side-effect information correct, alias information is useful to the register allocator.
Reference: [CK89] <author> K. Cooper and K. Kennedy. </author> <title> Fast interprocedural alias analysis. </title> <booktitle> In Conference Record of the Sixteenth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> January </month> <year> 1989. </year>
Reference-contexts: Thus, the set values can change once for every possible element in the set. We expect this number to be small, and to not grow with the procedure size. This is consistent with assumptions made in other interprocedural analysis algorithms [CK88b] <ref> [CK89] </ref>. 81 4.4.2 Phase 2: Propagation The propagation phase adds to CloningVars (p) any variables of p that can be used at important program points in p's successors in the call multigraph. <p> Side-effect analysis and alias analysis are performed separately, with alias information added to the side-effect information [Ban79]. Recent algorithms have been proposed to calculate aliasing and interprocedural side-effect information in time effectively linear in the size of the call multigraph [CK88b] <ref> [CK89] </ref>. However, currently in ParaScope, we use an iterative technique based on Banning's equations [Ban79]. In addition to making side-effect information correct, alias information is useful to the register allocator.
Reference: [CKT85] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization on the design of a software development environment. </title> <booktitle> In Proceedings of the SIGPLAN 85 Symposium on Compiler Construction, </booktitle> <month> June </month> <year> 1985. </year> <month> 162 </month>
Reference: [CKT86a] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> The impact of interprocedural analysis and optimization in the R n environment. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(4) </volume> <pages> 491-523, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: At that time, the technique was referred to as node splitting; it was renamed cloning in a later publication <ref> [CKT86a] </ref>. Cooper performed cloning based on interprocedural constants, and did not propagate improvement in information due to cloning down the call multigraph. Thus, a cloning decision was based on the direct effects it would have on a procedure.
Reference: [CKT86b] <author> K. Cooper, K. Kennedy, and L. Torczon. </author> <title> Interprocedural optimization: Eliminating unnecessary recompilation. </title> <booktitle> In Proceedings of the SIGPLAN 86 Symposium on Compiler Construction. ACM, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: Recompilation analysis determines procedures requiring recompilation, attempting to minimize the recompilation requirements. Interprocedural transformations such as inlining and cloning require further consideration. 6.5.1 Recompilation Analysis for Interprocedural Information Torczon presents three different approaches for minimizing the need for recompilation after interprocedural optimization [Tor85] <ref> [CKT86b] </ref> [BCKT90]. The method currently used in the program compiler compares interprocedural information from the last compilation of a module with information obtained in the current compilation. <p> The Cray fortran compiler manages dependences when it performs automatic inlining. The recompilation algorithm in this chapter extends the previous algorithms, accommodating incremental inlining and cloning [Tor85] <ref> [CKT86b] </ref> [BCKT90]. Torczon suggests that inlining and cloning be part of the program compiler design, but does not provide an algorithm for determining if inlined or cloned versions require recompilation. A representation of inlining and cloning similar to the one in this chapter is described in [BCKT90].
Reference: [CKT + 86c] <author> K. Cooper, K. Kennedy, L. Torczon, A. Weingarten, and M. Wolcott. </author> <title> Editing and compiling whole programs. </title> <booktitle> In Proceedings of the SIG-SOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, </booktitle> <month> December </month> <year> 1986. </year>
Reference-contexts: It is very similar to an algorithm used in ParaScope to minimize the number of implementations of a procedure required when multiple definitions of the procedure occur in the program composition <ref> [CKT + 86c] </ref>. The partitioning algorithm is presented in Figure 5.3. Initially, all clones of a procedure are placed in the same partition. The algorithm distinguishes between cloned versions, based on their StateVector and the partitioning of procedures they invoke.
Reference: [CLZ86] <author> R. Cytron, A. Lowry, and K. Zadeck. </author> <title> Code motion of control structures in high-level languages. </title> <booktitle> In Conference Record of the Thirteenth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> Jan-uary </month> <year> 1986. </year>
Reference-contexts: The traditional loop-invariant code motion algorithm is used to eliminate a single expression at a time. To eliminate dependences and improve parallelism, compound statements such as loops and conditionals must be located and moved outside of the loop. Such extensions to the traditional algorithm have been described <ref> [CLZ86] </ref> [FOW87]. 141 Loop unswitching. A similar technique, loop unswitching [AC72], can be applied when a condition in the loop is loop-invariant, but the code guarded by the condition is not. For an if-then-else clause within a loop, two copies of the loop are created.
Reference: [Con83] <author> R. Conradi. </author> <title> Inter-procedural optimization of object code. </title> <type> Technical Report 25/83, </type> <institution> University of Trondheim, Norway, </institution> <year> 1983. </year>
Reference-contexts: Studies for other programming languages have shown mixed results. Conradi estimated that a 5 to 20 percent improvement in execution time was possible with a combination of inlining and inter-procedural information, based on empirical results on program characteristics for the PQCC multi-language compiler backend <ref> [Con83] </ref>. Richardson and Ganapathi observed an average of 1.5 percent using only mod and ref information to optimize Pascal programs [RG89b]. Richardson and Ganapathi's results may not reflect what can be expected from scientific fortran code.
Reference: [Coo83] <author> K. D. Cooper. </author> <title> Interprocedural Data Flow Analysis in a Programming Environment. </title> <type> PhD thesis, </type> <institution> Rice University, Houston,TX, </institution> <month> April </month> <year> 1983. </year>
Reference-contexts: However, when compilers perform optimization and register allocation, the interaction with inlining can reduce the importance of call overhead. What is really significant is the amount of optimization enabled by inlining. Only Ball and Cooper select call sites for inlining based on the optimizations that will result [Bal79] <ref> [Coo83] </ref>. Ball formulates improvement estimates resulting from inline substitution as a data-flow problem [Bal79]. The strong dependency sets, as described in section 4.4, provide the set of formal parameters whose values determine a statement's value. <p> This method estimates decreases in code size and execution time as a result of constant propagation and test elision. Building on Ball's work, Cooper presents an algorithm for linkage tailoring, assigning linkages to call sites <ref> [Coo83] </ref>. The possible linkage styles include a default linkage where separate compilation is improved by interprocedural information, in-lining, cloning and some variants. <p> Section 5.3 presents related work on procedure cloning, and section 5.4 concludes the chapter. 5.1 Motivation Procedure cloning was introduced by Cooper as a method of partitioning calls to a procedure based on its interprocedural constants information <ref> [Coo83] </ref>. Previous research, particularly Ball's work, used inlining to accomplish the same result [Bal79]. Cooper observed that cloning had an advantage over inlining in refining constants 86 information, since multiple calls to a procedure can share a cloned version. <p> This requires that the compiler have some control over program linking. It would be impossible to do this strictly at the source level. 99 5.3 Related Work 5.3.1 Procedure Cloning Procedure cloning was introduced in Cooper's dissertation as part of his linkage tailoring algorithm <ref> [Coo83] </ref>. At that time, the technique was referred to as node splitting; it was renamed cloning in a later publication [CKT86a]. Cooper performed cloning based on interprocedural constants, and did not propagate improvement in information due to cloning down the call multigraph. <p> All other edges are given the annotation 1. The first rule requires further explanation. Accurately estimating the number of iterations of a loop can be difficult. As a simplification, Cooper suggests that we assume that all loops perform 8 iterations <ref> [Coo83] </ref>. Then, the number of times a call site is executed is 8 d , where d is the nesting depth of the loop in the procedure. This estimate at least takes into account the multiplicative benefits of optimizing a deeply nested procedure call.
Reference: [Coo85] <author> K. Cooper. </author> <title> Analyzing aliases of reference formal parameters. </title> <booktitle> In Conference Record of the Twelfth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> January </month> <year> 1985. </year>
Reference: [CS85] <author> R. Conradi and D. Svanaes. </author> <title> FORTVER | a tool for documentation and error diagnosis of FORTRAN-77 programs. </title> <type> Technical Report 1/85, </type> <institution> University of Trondheim, Norway, </institution> <month> January </month> <year> 1985. </year>
Reference-contexts: A program that contains such a call site does not conform to the fortran standard. Nonetheless, few fortran compilers enforce this restriction. We found such call sites in our study; others have reported similar results <ref> [CS85] </ref>. Some, quite obviously, had been carefully crafted to achieve specific behavior. For example, in euler, one call site passes an array of reals to a formal that is a complex array, relying on the standard's requirement that complex numbers must be implemented as pairs of reals. 2. <p> In this case, there are no constants, but a significant number of alias pairs. The call sites invoking fehl and procedure-valued formal f are annotated with their mod and ref sets. The display of interprocedural information to the programmer can be useful in debugging <ref> [CS85] </ref>. For example, from the main procedure it is possible to locate variables that are modified but never referenced. The buttons on the title bar of this pane are callers, calls and linkage.
Reference: [DH88] <author> J. W. Davidson and A. M. Holler. </author> <title> A study of a C function inliner. </title> <journal> Software|Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 775-790, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Davidson and Holler also implemented source-to-source inlining for C programs, giving results for 4 different target compilers <ref> [DH88] </ref>. They report an average execution time improvement of 12 percent on 13 programs. However, the improvements range from a 60 percent improvement to a 9 percent degradation. They attribute the cases of increased execution time to be caused primarily by register pressure. <p> input data. 83 * Hwu and Chang use a similar approach, considering not only the increased code size as the cost of inlining, but also the size of the control stack during execution [HC89]. * Davidson and Holler substitute called procedures when their caller appears in the same source file <ref> [DH88] </ref>. They avoid inlining for recursion and when register declarations would exceed the number available to the compiler. In her dissertation, a model for predicting the improvement to a call site due to inlin-ing is presented [Hol91]. <p> The model considers the following costs: saving and restoring registers, passing parameters, local stack adjustment, parameter stack adjustment and the call/return sequence. Heuristics focusing on eliminating call overhead work reasonably well for non-optimizing compilers [Sch77] <ref> [DH88] </ref>. However, when compilers perform optimization and register allocation, the interaction with inlining can reduce the importance of call overhead. What is really significant is the amount of optimization enabled by inlining. Only Ball and Cooper select call sites for inlining based on the optimizations that will result [Bal79] [Coo83].
Reference: [Fel79] <author> S. Feldman. </author> <title> Make a program for maintaining computer programs. </title> <journal> Software|Practice and Experience, </journal> <volume> 9 </volume> <pages> 255-265, </pages> <year> 1979. </year> <month> 163 </month>
Reference-contexts: The Unix make facility by Feldman was perhaps the first tool that managed the relationship between procedures in a program <ref> [Fel79] </ref>. The make facility allows a programmer to specify the components of a program, the actions required to build the program, and the dependences among the components. Modification to a component is enough to force rebuilding of components that depend upon it.
Reference: [FMM77] <author> G. E. Forsythe, M. A. Malcolm, and C. B. Moler. </author> <title> Computer Methods for Mathematical Computations. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1977. </year>
Reference-contexts: The compiler also provides the list of procedures requiring recompilation and explains why recompilation was necessary. To illustrate the utility of such a display, we use the example program rkf45, part of the Forsythe, Malcolm and Moler library package <ref> [FMM77] </ref>. In the entry display from Figure 8.1, the procedure definition of rkfs is annotated with its interprocedural constants and aliases. In this case, there are no constants, but a significant number of alias pairs.
Reference: [FOW87] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: The traditional loop-invariant code motion algorithm is used to eliminate a single expression at a time. To eliminate dependences and improve parallelism, compound statements such as loops and conditionals must be located and moved outside of the loop. Such extensions to the traditional algorithm have been described [CLZ86] <ref> [FOW87] </ref>. 141 Loop unswitching. A similar technique, loop unswitching [AC72], can be applied when a condition in the loop is loop-invariant, but the code guarded by the condition is not. For an if-then-else clause within a loop, two copies of the loop are created.
Reference: [Ger89] <author> M. Gerndt. </author> <title> Array distribution in superb. </title> <booktitle> In Proceedings of the 1989 International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: The compiler also adds the necessary synchronization to guarantee that data dependences are preserved. Distributed memory compilers typically assume that procedure calls do not occur in parallel loops, with the exception of the superb compiler <ref> [Ger89] </ref>. If this assumption is relaxed, the compiler must have knowledge of decompositions across procedure boundaries. This adds a host of problems requiring interprocedural solutions [HKT91].
Reference: [Har77a] <author> W. Harrison. </author> <title> Compiler analysis for the value ranges of variables. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-3(5):243-250, </volume> <month> May </month> <year> 1977. </year>
Reference-contexts: We can also improve the results of constant propagation using range analysis and range propagation, which give ranges of variable values rather than a single value <ref> [Har77a] </ref>. However, since the analysis described in this paragraph must occur in the module editor at the end of an editing session, it is doubtful that the increased precision of the execution frequency estimates will be worth the added cost. <p> Note that if i appears in the expression for j's value, substituting lb for i instead gives the upper bound of j, and substituting ub gives the lower bound of j. This is a simplification of the techniques range analysis and range propagation <ref> [Har77a] </ref>. These techniques track the range of values for all variables in a program. Although much more precise, tracking ranges of all variables is too expensive for most practical compilers.
Reference: [Har77b] <author> W. Harrison. </author> <title> A new strategy for code generation-the general purpose optimizing compiler. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-5(7):367-373, </volume> <month> July </month> <year> 1977. </year>
Reference-contexts: A very restricted inlining strategy eliminated about 20 percent of the calls, with about a 2 percent decrease in the program intermediate representation. The general-purpose optimizer in the Experimental Compiling System at IBM used inline substitution as a key component in the implementation of an optimizing compiler <ref> [Har77b] </ref> [ACF + 80]. For these two compilers, the kinds of improvements anticipated by inlining are not given.
Reference: [HC89] <author> W. W. Hwu and P. P. Chang. </author> <title> Inline function expansion for inlining C programs. </title> <booktitle> In Proceedings of the SIGPLAN 89 Conference on Programming Language Design and Implementation. ACM, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Hwu and Chang used a similar inlining strategy on C programs, considering not only the increased code size as the cost of inline substitution of a call site, but also the size of the control stack during execution <ref> [HC89] </ref>. Davidson and Holler also implemented source-to-source inlining for C programs, giving results for 4 different target compilers [DH88]. They report an average execution time improvement of 12 percent on 13 programs. However, the improvements range from a 60 percent improvement to a 9 percent degradation. <p> Execution frequency estimates are obtained from run-time measurements of the program on sample input data. 83 * Hwu and Chang use a similar approach, considering not only the increased code size as the cost of inlining, but also the size of the control stack during execution <ref> [HC89] </ref>. * Davidson and Holler substitute called procedures when their caller appears in the same source file [DH88]. They avoid inlining for recursion and when register declarations would exceed the number available to the compiler.
Reference: [Hec77] <author> M. Hecht. </author> <title> Flow Analysis of Computer Programs. </title> <publisher> American Elsevier, North Holland, </publisher> <year> 1977. </year>
Reference-contexts: Hecht included inline substitution in an optimizing compiler for a structured programming language <ref> [Hec77] </ref>. A very restricted inlining strategy eliminated about 20 percent of the calls, with about a 2 percent decrease in the program intermediate representation. <p> In general, these inlining strategies are driven by heuristics weighted toward frequently executed procedures, while attempting to control program growth: * Hecht's SIMPL-T compiler only inlines procedures called once with a single entry and exit <ref> [Hec77] </ref>. * Scheifler substitutes any procedure not resulting in an increase in code size, and then procedures with the highest ratio of expected number of executions to net increase in code size [Sch77]. He restricts the final program size to twice its original size.
Reference: [HK83] <author> R. T. Hood and K. Kennedy. </author> <title> A programming environment for Fortran. </title> <type> Technical Report MASC TR 83-22, </type> <institution> Dept. of Mathematical Sciences, Rice University, </institution> <year> 1983. </year>
Reference: [HK91] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3), </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: Additional cloning and inlining is only performed when recompilation is needed. 120 Chapter 7 Interprocedural Optimization for Parallelization Previous work on interprocedural optimization for parallelism has focused on inline substitution and interprocedural analysis of array side-effects. Even though array side-effect analysis and inlining are frequently successful [Hus82] <ref> [HK91] </ref>, each of these methods has its limitations. Considerations of compilation time and space require that array side-effect analysis summarize information about accesses. In general, summary information is less precise than the analysis of inlined code. <p> We suggest some optimizations to further reduce dependences in inlined loops, and describe an experiment to determine the effectiveness of the optimizations in increasing parallelism. The chapter also describes a form of array side-effect analysis called regular section analysis [CK88a] <ref> [HK91] </ref>. This chapter provides an overview of interprocedural optimization for enhancing parallelization. It is organized into five sections. The next section describes inter-procedural information required to perform dependence analysis of loops containing procedure calls. <p> At the end of this chapter, we describe a number of techniques designed to provide information about array side-effects. We base our transformations on regular section descriptors (rsds) [CK88a] <ref> [HK91] </ref>. Regular sections describe side-effects to a few important substructures of arrays: single elements, rows, columns, grids and their higher dimensional analogs. The restriction to a few shapes makes the implementation efficient as compared to other techniques. <p> A code generator that incorporates scalar expansion must consider these effects. 7.4 Related Work 7.4.1 Approaches to Summarizing Array Side Effects A number of approaches to array side effects have been suggested in the literature. A comparison of these techniques can be found in <ref> [HK91] </ref>. In this comparison, the precision of the techniques is weighed against their costs. The costs are broken down into a number of categories. <p> It can also be used to provide programmers with a visual description of affected regions of an array. For this purpose, Data Access Descriptors also include a traversal order and a reference template. The implementation of rsd analysis described in <ref> [HK91] </ref> gives up some precision in favor of efficiency. By describing only simple access patterns and by summarizing multiple accesses to an array, all aspects of the technique are efficient.
Reference: [HKMC90] <author> R. Hood, K. Kennedy, and J. Mellor-Crummey. </author> <title> Parallel program debugging with on-the-fly anomaly detection. </title> <booktitle> In Proceedings of Supercomputing 90, </booktitle> <month> November </month> <year> 1990. </year>
Reference-contexts: When all concurrent threads that may access the same variable have completed execution, trace information about the variable is discarded. To further reduce the overhead of access anomaly detection, researchers at Rice are taking advantage of dependence analysis <ref> [HKMC90] </ref>. By assuming that concurrent threads are only created by parallel loops, dependence analysis at the loop-level can eliminate memory locations from consideration. If there are no dependences on a variable in a parallel loop, there is no need to trace accesses to it.
Reference: [HKT91] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <type> Technical Report TR90-149, </type> <institution> Dept. of Computer Science, Rice University, </institution> <note> February 164 1991. To appear in J. </note> <editor> Saltz and P. Mehrotra, editors, </editor> <title> Compilers and Runtime Software for Scalable Multiprocessors, </title> <publisher> Elsevier, </publisher> <year> 1991. </year>
Reference-contexts: Distributed memory compilers typically assume that procedure calls do not occur in parallel loops, with the exception of the superb compiler [Ger89]. If this assumption is relaxed, the compiler must have knowledge of decompositions across procedure boundaries. This adds a host of problems requiring interprocedural solutions <ref> [HKT91] </ref>. The compiler must propagate decompositions on the call multigraph, with formal parameters inheriting the decomposition of the actual parameters passed at the call, and globals retaining their decomposition across the call.
Reference: [Hol91] <author> A. M. Holler. </author> <title> A Study of the Effects of Subprogram Inlining. </title> <type> PhD thesis, </type> <institution> Univ. of Virginia, </institution> <address> Charlottesville, VA, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: After inlining, the number of register declarations in a procedure may increase, in some cases exceeding the available number of registers. In Holler's dissertation, she further investigates the effects of inlining on execution-time performance <ref> [Hol91] </ref>. By avoiding inlining when the number of register declarations will become too large, she still observes adverse effects of register allocation. Many compilers divide the burden of saving registers around procedure calls between the caller and the callee. Inlining moves the location of these saves and restores. <p> They avoid inlining for recursion and when register declarations would exceed the number available to the compiler. In her dissertation, a model for predicting the improvement to a call site due to inlin-ing is presented <ref> [Hol91] </ref>. The model considers the following costs: saving and restoring registers, passing parameters, local stack adjustment, parameter stack adjustment and the call/return sequence. Heuristics focusing on eliminating call overhead work reasonably well for non-optimizing compilers [Sch77] [DH88].
Reference: [Hop71] <author> J. Hopcroft. </author> <title> An nlogn algorithm for minimizing states in a finite automaton. </title> <editor> In Z. Kohavi and A. Paz, editors, </editor> <booktitle> Theory of Machines and Computations, </booktitle> <pages> pages 189-196. </pages> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1971. </year>
Reference-contexts: Partitioning Algorithm The algorithm for merging equivalent CloningVectors is related to the algorithm for minimizing the number of states in a Deterministic Finite Automaton (DFA) <ref> [Hop71] </ref>. It is very similar to an algorithm used in ParaScope to minimize the number of implementations of a procedure required when multiple definitions of the procedure occur in the program composition [CKT + 86c]. The partitioning algorithm is presented in Figure 5.3. <p> With an appropriate representation for StateVector and for CloningVectors resulting from call sites, the expected time required for partitioning can be done in time linear in the number of elements being partitioned. (A different representation would yield O (nlogn) time, even for worst-case performance <ref> [Hop71] </ref>.) As a possibility, the set representations can be treated as strings, with some canonical order imposed on the set elements. Then, partitioning can be performed by hashing to a location matching the string representation of the set.
Reference: [Hus82] <author> C. A. Huson. </author> <title> An inline subroutine expander for Parafrase. </title> <type> Masters Thesis UIUCDCS-R-82-1118, </type> <institution> Dept. of Computer Science, University of Illinois, Urbana-Champaign, </institution> <year> 1982. </year>
Reference-contexts: However, the compile times grew on average by a factor of five. They provide evidence to suggest that the performance improvement was primarily due to eliminating call overhead. Huson investigated inlining in Parafrase, an automatic parallelization system for fortran <ref> [Hus82] </ref>. His results were mixed, with only a third of the procedures demonstrating any improvements. However, a single example yielded a speedup of 4 due to inlining. Huson's study was performed on the linpack library of subroutines. <p> Additional cloning and inlining is only performed when recompilation is needed. 120 Chapter 7 Interprocedural Optimization for Parallelization Previous work on interprocedural optimization for parallelism has focused on inline substitution and interprocedural analysis of array side-effects. Even though array side-effect analysis and inlining are frequently successful <ref> [Hus82] </ref> [HK91], each of these methods has its limitations. Considerations of compilation time and space require that array side-effect analysis summarize information about accesses. In general, summary information is less precise than the analysis of inlined code. <p> In contrast, this chapter has described how array side-effect information can also be utilized in guiding interproce-dural transformations. In fact, we know of only one other author who has addressed the issue of interprocedural transformations. Huson's implementation of inline substitution actually performs loop embedding <ref> [Hus82] </ref>, as described in Section 7.2.1. No interprocedural information is required to determine the safety of this transformation. The Convex Applications Compiler analyzes array side effects using a technique similar to regular section analysis [Met91]. This information is combined with inlin-ing and cloning to parallelize loops with procedure calls.
Reference: [KKL + 81] <author> D. J. Kuck, R. Kuhn, B. Leasure, D. Padua, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> Jan-uary </month> <year> 1981. </year>
Reference-contexts: This transformation is called loop distribution <ref> [KKL + 81] </ref>. 130 Safety. This version of loop embeddings is safe if loop distribution around the call is legal. Loop distribution must preserve the dependences in the loop. This is true as long as statements involved in a dependence cycle remain within the same loop. <p> We observed that many of the dependences remaining in the loops after inlining were on scalar variables. The technique commonly used in vectorization to eliminate dependences on scalar variables is scalar expansion <ref> [KKL + 81] </ref>. A similar technique is used in parallelization, where such variables are made private to each processor.
Reference: [KMV90] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory machines. </title> <booktitle> In Proceedings of the Second SIGPLAN Symposium on Principles and Practice of Parallel Programming. ACM, </booktitle> <month> March </month> <year> 1990. </year>
Reference-contexts: Most compilers for distributed memory machines require the programmer to determine how the data will be assigned to the processors [ZBG88] [RP89] <ref> [KMV90] </ref>. The compiler is responsible for adding the message passing to move the data to processors that use it and from processors that define it. The compiler also adds the necessary synchronization to guarantee that data dependences are preserved.
Reference: [KU77] <author> J. Kam and J. D. Ullman. </author> <title> Monotone data flow analysis frameworks. </title> <journal> Acta Informatica, </journal> <volume> 7(3) </volume> <pages> 305-318, </pages> <year> 1977. </year>
Reference-contexts: Interprocedural constants. The set constant (p) gives variables and their constant values only when the constant values exist for all calls to p. Since the problem of finding all variables that are constant at run-time is undecidable <ref> [KU77] </ref>, the set calculated is an approximation. The algorithm used for interprocedural constant propagation is given in [CCKT86].
Reference: [Kuc78] <author> D. J. Kuck. </author> <title> The Structure of Computers and Computations, volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: If the compiler were to automatically perform the translation and substitute for k, the resulting reference to A (1,i) in daxpy would be A (1+(j-1)*ij+(i-1)*ii,1). This subscript expression is complex enough to defy the dependence analysis <ref> [Kuc78] </ref> on which scalar replacement and unroll and jam rely. The value of ii depends on the parameter job so we trace backwards through the program to locate the value of job.
Reference: [LY88a] <author> Z. Li and P.-C. Yew. </author> <title> Efficient interprocedural analysis for program restructuring for parallel programs. </title> <booktitle> In Proceedings of the SIGPLAN Symposium on Parallel Programs: Experience with Applications, Languages and Systems, </booktitle> <month> July </month> <year> 1988. </year>
Reference-contexts: The effect of linearization can be a significant loss of information for dependence testing. Li and Yew developed a more precise representation of array accesses, called atom images <ref> [LY88a] </ref> [LY88b]. These precisely represent linear subscript 147 expressions in rectangular and triangular iteration spaces. In both cases, testing whether two lists have a non-empty intersection requires a traversal of the lists of accesses. <p> Experiments with regular section analysis on the linpack library demonstrated a 33 percent re-dution in parallelism-inhibiting dependences, allowing 31 loops containing calls to be parallelized. Comparing these numbers against published results of more precise techniques, <ref> [LY88a] </ref> [LY88b] [TIF86], there was no benefit to the increased precision of the other techniques. 7.4.2 Interprocedural Transformations Prior to this research, array side-effect information has been employed only in deciding whether loops containing calls can be parallelized.
Reference: [LY88b] <author> Z. Li and P.-C. Yew. </author> <title> Interprocedural analysis and program restructuring for parallel programs. </title> <type> Technical Report CSRD-720, </type> <institution> University of Illinois, Urbana-Champaign, </institution> <month> January </month> <year> 1988. </year> <month> 165 </month>
Reference-contexts: The effect of linearization can be a significant loss of information for dependence testing. Li and Yew developed a more precise representation of array accesses, called atom images [LY88a] <ref> [LY88b] </ref>. These precisely represent linear subscript 147 expressions in rectangular and triangular iteration spaces. In both cases, testing whether two lists have a non-empty intersection requires a traversal of the lists of accesses. <p> Experiments with regular section analysis on the linpack library demonstrated a 33 percent re-dution in parallelism-inhibiting dependences, allowing 31 loops containing calls to be parallelized. Comparing these numbers against published results of more precise techniques, [LY88a] <ref> [LY88b] </ref> [TIF86], there was no benefit to the increased precision of the other techniques. 7.4.2 Interprocedural Transformations Prior to this research, array side-effect information has been employed only in deciding whether loops containing calls can be parallelized.
Reference: [Mar89] <author> T. J. Marlowe. </author> <title> Incremental Iteration and Data Flow. </title> <type> PhD thesis, </type> <institution> Rutgers University, </institution> <address> New Brunswick, NJ, </address> <month> October </month> <year> 1989. </year>
Reference-contexts: To do this efficiently requires an incremental algorithm for performing the analysis. The design of the two-phase algorithm is amenable to efficient incremental updates to the interprocedural information. The incremental updates are based on Marlowe's algorithm for incremental data-flow analysis [MR90] <ref> [Mar89] </ref>. This algorithm requires that the graph be maintained in topological order, and that the strongly connected components, or loops, be located. Topological ordering guarantees that during a single update, the information is not 117 propagated to a particular node more than once.
Reference: [Met91] <author> R. Metzger. </author> <title> Private communication, </title> <month> January </month> <year> 1991. </year>
Reference-contexts: Huson's implementation of inline substitution actually performs loop embedding [Hus82], as described in Section 7.2.1. No interprocedural information is required to determine the safety of this transformation. The Convex Applications Compiler analyzes array side effects using a technique similar to regular section analysis <ref> [Met91] </ref>. This information is combined with inlin-ing and cloning to parallelize loops with procedure calls. Inlining is used to eliminate call overhead. Cloning is used to expose constants in order to improve dependence 148 analysis.
Reference: [MHK86] <author> H. Muller, R. Hood, and K. Kennedy. </author> <title> Efficient recompilation of module interfaces in a software development environment. </title> <booktitle> In Proceedings of the SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, </booktitle> <month> December </month> <year> 1986. </year>
Reference-contexts: Recompilation for a component C is indicated if the intersection of the reference set for C and the change set for a component upon which C depends is non-empty. Muller developed an algorithm similar to Tichy and Baker's for the Rigi software development environment [Mul86] <ref> [MHK86] </ref>. Rigi is designed to support code sharing across programmers and projects. The global interface analysis algorithms are designed to detect recompilation requirements across a system of programs based on imported and exported interfaces of the components.
Reference: [MR90] <author> T. J. Marlowe and B. G. Ryder. </author> <title> An efficient hybrid algorithm for incremental data flow analysis. </title> <booktitle> In Conference Record of the Seventeenth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> Jan-uary </month> <year> 1990. </year>
Reference-contexts: To do this efficiently requires an incremental algorithm for performing the analysis. The design of the two-phase algorithm is amenable to efficient incremental updates to the interprocedural information. The incremental updates are based on Marlowe's algorithm for incremental data-flow analysis <ref> [MR90] </ref> [Mar89]. This algorithm requires that the graph be maintained in topological order, and that the strongly connected components, or loops, be located. Topological ordering guarantees that during a single update, the information is not 117 propagated to a particular node more than once.
Reference: [MS91] <author> R. Metzger and P. Smith. </author> <title> The CONVEX application compiler. </title> <journal> Fortran Journal, </journal> <volume> 3(1) </volume> <pages> 8-10, </pages> <year> 1991. </year>
Reference-contexts: Experience using this compiler has demonstrated that interprocedural analysis and optimization typically execution-time improvement of 10 to 25 percent over standard compilation. In a few cases, the compiler compiler yields a speedup of 5 over separate compilation <ref> [MS91] </ref>. 7.5 Chapter Summary The ideas in this chapter provide a framework for interprocedural optimization supporting parallelization. The chapter makes two main contributions: how to use array subsections to guide interprocedural transformations, and how to eliminate dependences in loops after inlining. Transformation framework.
Reference: [Mul86] <author> H. Muller. </author> <title> Rigi A Model for Software System Construction, Integration, and Evolution Based on Module Interface Specifications. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, TX, </institution> <month> August </month> <year> 1986. </year>
Reference-contexts: Recompilation for a component C is indicated if the intersection of the reference set for C and the change set for a component upon which C depends is non-empty. Muller developed an algorithm similar to Tichy and Baker's for the Rigi software development environment <ref> [Mul86] </ref> [MHK86]. Rigi is designed to support code sharing across programmers and projects. The global interface analysis algorithms are designed to detect recompilation requirements across a system of programs based on imported and exported interfaces of the components.
Reference: [Mye81] <author> E. Myers. </author> <title> A precise inter-procedural data flow algorithm. </title> <booktitle> In Conference of the Eighth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> January </month> <year> 1981. </year>
Reference-contexts: The precision of dependence analysis makes it intractable across procedure boundaries <ref> [Mye81] </ref>. As a compromise, reuse is only considered within a procedure. Memory accesses crossing procedure boundaries are considered to be independent. This approach is reasonable since reuse across procedure boundaries, even if detected, would be difficult for a compiler to exploit. <p> As a result, mod and ref sets may contain some variables that are not actually modified or used (i.e., the sets are conservative). A flow-insensitive approach is necessary in the presence of aliasing to guarantee a polynomial time bound <ref> [Mye81] </ref>. With side-effect information, a compiler can avoid reading in a variable from memory after a procedure call if the variable is not modified by the call.
Reference: [RG89a] <author> S. Richardson and M. Ganapathi. </author> <title> Interprocedural analysis versus procedure integration. </title> <journal> Information Processing Letters, </journal> <volume> 32(3) </volume> <pages> 137-142, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: However, other work suggests that improvements will arise from (1) propagating constant-valued parameters through the body of the called procedure, (2) enabling code motion across the former call site, and (3) exposing more information to the register allocator [Bal79] <ref> [RG89a] </ref> [WZ85]. Unfortunately, very little experimental evidence has been published to prove these assumptions. Prior to this study, only two others have analyzed through experimentation the impact of inlining on optimization. Richardson and Ganapathi's study on 5 Pascal programs demonstrated an average of 20 percent improvement [RG89a]. <p> the register allocator [Bal79] <ref> [RG89a] </ref> [WZ85]. Unfortunately, very little experimental evidence has been published to prove these assumptions. Prior to this study, only two others have analyzed through experimentation the impact of inlining on optimization. Richardson and Ganapathi's study on 5 Pascal programs demonstrated an average of 20 percent improvement [RG89a]. However, the compile times grew on average by a factor of five. They provide evidence to suggest that the performance improvement was primarily due to eliminating call overhead. Huson investigated inlining in Parafrase, an automatic parallelization system for fortran [Hus82].
Reference: [RG89b] <author> S. Richardson and M. Ganapathi. </author> <title> Interprocedural optimization: Experimental results. </title> <journal> Software|Practice and Experience, </journal> <volume> 19(2) </volume> <pages> 149-169, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Richardson and Ganapathi observed an average of 1.5 percent using only mod and ref information to optimize Pascal programs <ref> [RG89b] </ref>. Richardson and Ganapathi's results may not reflect what can be expected from scientific fortran code. Since Pascal allows the programmer to declare a parameter as call-by-value or call-by-reference, the programmer can convey a certain amount of the interprocedural information to the compiler that is not possible with fortran.
Reference: [Ros79] <author> B. Rosen. </author> <title> Data flow analysis for procedural languages. </title> <journal> Journal of the ACM, </journal> <volume> 26(2) </volume> <pages> 322-344, </pages> <year> 1979. </year>
Reference: [RP89] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN 89 Conference on Program Language Design and Implementation, </booktitle> <month> June </month> <year> 1989. </year> <month> 166 </month>
Reference-contexts: Most compilers for distributed memory machines require the programmer to determine how the data will be assigned to the processors [ZBG88] <ref> [RP89] </ref> [KMV90]. The compiler is responsible for adding the message passing to move the data to processors that use it and from processors that define it. The compiler also adds the necessary synchronization to guarantee that data dependences are preserved.
Reference: [Ryd79] <author> B. Ryder. </author> <title> Constructing the call graph of a program. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-5:216-225, </volume> <month> May </month> <year> 1979. </year>
Reference: [Sch77] <author> R. Scheifler. </author> <title> An analysis of inline substitution for a structured programming language. </title> <journal> Communications of the ACM, </journal> <volume> 20(9) </volume> <pages> 647-654, </pages> <month> September </month> <year> 1977. </year>
Reference-contexts: The first study undertaken to understand the impact of inlining on call overhead was Scheifler's <ref> [Sch77] </ref>. He implemented inlining in a compiler for Clu, selecting call sites for inlining based on execution profiles, and restricting overall program growth to twice its original size. He observes that in data abstraction languages such as Clu, programs consist of many very short procedures. <p> control program growth: * Hecht's SIMPL-T compiler only inlines procedures called once with a single entry and exit [Hec77]. * Scheifler substitutes any procedure not resulting in an increase in code size, and then procedures with the highest ratio of expected number of executions to net increase in code size <ref> [Sch77] </ref>. He restricts the final program size to twice its original size. <p> The model considers the following costs: saving and restoring registers, passing parameters, local stack adjustment, parameter stack adjustment and the call/return sequence. Heuristics focusing on eliminating call overhead work reasonably well for non-optimizing compilers <ref> [Sch77] </ref> [DH88]. However, when compilers perform optimization and register allocation, the interaction with inlining can reduce the importance of call overhead. What is really significant is the amount of optimization enabled by inlining.
Reference: [Sch89] <author> E. Schonberg. </author> <title> On-the-fly detection of access anomalies. </title> <booktitle> In Proceedings of the SIGPLAN 89 Conference on Programming Language Design and Implementation. ACM, </booktitle> <month> June </month> <year> 1989. </year>
Reference-contexts: Thus, they may be difficult to repeat with subsequent executions of the program. Access anomaly detection is a technique used in debugging on shared memory multiprocessors to locate potential race conditions by tracing memory accesses <ref> [Sch89] </ref>. To avoid significant overhead, Schonberg only traces memory locations in executing concurrent threads. When all concurrent threads that may access the same variable have completed execution, trace information about the variable is discarded.
Reference: [Shi88] <author> O. Shivers. </author> <title> Control flow analysis in Scheme. </title> <booktitle> In Proceedings of the SIG-PLAN 88 Conference on Programming Language Design and Implementation. ACM, </booktitle> <month> June </month> <year> 1988. </year>
Reference: [Spi71] <author> T. C. Spillman. </author> <title> Exposing side-effects in a PL/I optimizing compiler. </title> <booktitle> In Proceedings of the IFIP Congress 1971, </booktitle> <pages> pages 376-381, </pages> <address> Amsterdam, 1971. </address> <publisher> North Holland. </publisher>
Reference: [TB85] <author> W. F. Tichy and M. C. Baker. </author> <title> Smart recompilation. </title> <booktitle> In Conference Record of the Twelfth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> January </month> <year> 1985. </year>
Reference-contexts: Modification to a component is enough to force rebuilding of components that depend upon it. Tichy and Baker considered a finer granularity for establishing dependences among components <ref> [TB85] </ref>. Their method determines the portions of a component that are shared by other components: definitions, declarations and constants. This is basically the interface to the outside. Then modification of a component can only force recompilation of other components if the interface changes.
Reference: [TIF86] <author> R. Triolet, F. Irigoin, and P. Feautrier. </author> <title> Direct parallelization of call statements. </title> <booktitle> In Proceedings of the SIGPLAN 86 Symposium on Compiler Construction. ACM, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: In both cases, testing whether two lists have a non-empty intersection requires a traversal of the lists of accesses. Thus, the cost of these techniques can be significant. * Triolet summarizes array accesses using a more precise representation of the summary than rsds <ref> [TIF86] </ref>. This technique locates the convex hull surrounding two regions representing accesses. Dependence testing requires an assymp totically exponential linear inequality solver. * Balasundaram proposed an approach to summarizing array accesses designed to locate opportunities for task-level parallelism [Bal89]. <p> Experiments with regular section analysis on the linpack library demonstrated a 33 percent re-dution in parallelism-inhibiting dependences, allowing 31 loops containing calls to be parallelized. Comparing these numbers against published results of more precise techniques, [LY88a] [LY88b] <ref> [TIF86] </ref>, there was no benefit to the increased precision of the other techniques. 7.4.2 Interprocedural Transformations Prior to this research, array side-effect information has been employed only in deciding whether loops containing calls can be parallelized.
Reference: [Tor85] <author> L. Torczon. </author> <title> Compilation Dependencies in an Ambitious Optimizing Compiler. </title> <type> PhD thesis, </type> <institution> Rice University, Houston, TX, </institution> <month> May </month> <year> 1985. </year>
Reference-contexts: Recompilation analysis determines procedures requiring recompilation, attempting to minimize the recompilation requirements. Interprocedural transformations such as inlining and cloning require further consideration. 6.5.1 Recompilation Analysis for Interprocedural Information Torczon presents three different approaches for minimizing the need for recompilation after interprocedural optimization <ref> [Tor85] </ref> [CKT86b] [BCKT90]. The method currently used in the program compiler compares interprocedural information from the last compilation of a module with information obtained in the current compilation. <p> The Cray fortran compiler manages dependences when it performs automatic inlining. The recompilation algorithm in this chapter extends the previous algorithms, accommodating incremental inlining and cloning <ref> [Tor85] </ref> [CKT86b] [BCKT90]. Torczon suggests that inlining and cloning be part of the program compiler design, but does not provide an algorithm for determining if inlined or cloned versions require recompilation. A representation of inlining and cloning similar to the one in this chapter is described in [BCKT90].
Reference: [Wal76] <author> K. Walter. </author> <title> Recursion analysis for compiler optimization. </title> <journal> Communications of the ACM, </journal> <volume> 19(9) </volume> <pages> 514-516, </pages> <month> September </month> <year> 1976. </year>
Reference: [Weg81] <author> M. Wegman. </author> <title> General and Efficient Methods for Global Code Improvement. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <month> December </month> <year> 1981. </year>
Reference-contexts: In this case, the copies are made to enable the analysis, not to improve optimization. In his dissertation, Wegman describes a technique called node distinction, which uses node splitting intraprocedurally on the control flow graph to improve optimization <ref> [Weg81] </ref>. His technique makes multiple copies of a control flow node if its data-flow information differs on incoming edges. Thus, based on the value of some forward data-flow set, he constructs the maximal node distinction control flow graph.
Reference: [Wei80] <author> W. E. Weihl. </author> <title> Interprocedural data flow analysis in the presence of pointers, procedure variables, and label variables. </title> <booktitle> In Conference Record of the 167 Seventh Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1980. </year>
Reference: [Wol86] <author> M. Wolfe. </author> <title> Advanced loop interchanging. </title> <booktitle> In Proceedings of the 1986 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1986. </year>
Reference-contexts: Even if the outermost loop cannot be parallelized, it may be possible to move an inner parallel loop to the outermost position. This transformation is called loop permutation [Ban90], a generalization of loop interchange [AK84] [AK87] <ref> [Wol86] </ref> [Wol89]. To perform loop permutation, the dependence analysis phase looks for loops that do not carry any dependences. These loops will be candidates for moving to the outermost position. This section presents two versions of loop permutation. First, we address the problem of perfectly nested loops. <p> Possible exceptions include loop bounds based on induction variables of an outer loop. Permutation of these triangular and trapezoidal loops requires a slight extension to the transformation described here <ref> [Wol86] </ref>. Mechanics. During dependence analysis, loops that do not carry any dependences are marked. To determine whether a loop containing a procedure call carries a dependence, rsds represent the called procedure. When selecting loops for loop permutation, the compiler examines a chain of procedures making up a loop nest. <p> The class of loop nests for which interchange is applicable is different than the one for this version <ref> [Wol86] </ref>.) This transformation is not performed along arbitrary chains in the call multigraph, but only across a single call. Following the presentation of this transformation, we explain why these restrictions are necessary. Safety.
Reference: [Wol89] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Even if the outermost loop cannot be parallelized, it may be possible to move an inner parallel loop to the outermost position. This transformation is called loop permutation [Ban90], a generalization of loop interchange [AK84] [AK87] [Wol86] <ref> [Wol89] </ref>. To perform loop permutation, the dependence analysis phase looks for loops that do not carry any dependences. These loops will be candidates for moving to the outermost position. This section presents two versions of loop permutation. First, we address the problem of perfectly nested loops.
Reference: [WZ85] <author> M. Wegman and F. K. Zadeck. </author> <title> Constant propagation with conditional branches. </title> <booktitle> In Conference Record of the Twelfth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> January </month> <year> 1985. </year>
Reference-contexts: However, other work suggests that improvements will arise from (1) propagating constant-valued parameters through the body of the called procedure, (2) enabling code motion across the former call site, and (3) exposing more information to the register allocator [Bal79] [RG89a] <ref> [WZ85] </ref>. Unfortunately, very little experimental evidence has been published to prove these assumptions. Prior to this study, only two others have analyzed through experimentation the impact of inlining on optimization. Richardson and Ganapathi's study on 5 Pascal programs demonstrated an average of 20 percent improvement [RG89a].
Reference: [WZ89] <author> M. Wegman and K. Zadeck. </author> <title> Constant propagation with conditional branches. </title> <type> Technical Report CS-89-36, </type> <institution> Dept. of Computer Science, Brown University, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: We were interested in determining which of these was the case. If secondary effects were masking improvements, then other interprocedural techniques might still be effective. Improved constant propagation has been suggested as the most important effect of inlining [Bal79] <ref> [WZ89] </ref>. To test this, we performed an experiment to isolate the effects of interprocedural constant propagation. We also studied improvements when constants information is further refined by cloning. The experiment, described in the next section, proved that interprocedural constants are reasonably important | yielding greater improvements than occurred with inlining. <p> SSA form is used in constant propagation because the space requirement is much less than traditional data-flow techniques on the control flow graph, and it has a better expected time bound <ref> [WZ89] </ref> [CFR + 89]. We use the SSA representation for this reason, but also because SSA form is already constructed in ParaScope to perform local analysis for constant propagation. Algorithm. The primary goal of the local analysis algorithm is to compute the portion of CloningVars used directly in this procedure. <p> However, the expected time bound is linear in the size of the SSA graph <ref> [WZ89] </ref>. The algorithm for calculating initial CloningVars is more expensive than this because the set values associated with a node can change more than twice. The number of elements in a set V s is bounded by the number of scalar formal parameters and global variables appearing in the procedure.
Reference: [Yer66] <author> A. P. Yershov. </author> <title> Alpha an automatic programming system of high efficiency. </title> <journal> Journal of the ACM, </journal> <volume> 13(1) </volume> <pages> 17-24, </pages> <month> January </month> <year> 1966. </year>
Reference: [Zad84] <author> F. K. Zadeck. </author> <title> Incremental data flow analysis in a structured program editor. </title> <booktitle> In Proceedings of the SIGPLAN 84 Symposium on Compiler Construction, </booktitle> <month> June </month> <year> 1984. </year>
Reference-contexts: Potentially, the algorithm would not terminate. This was our concern when disallowing cloning within recursive cycles. We now understand how to support recursion. First, strongly-connected regions are located in the call multigraph, and each cycle is replaced with a representative node <ref> [Zad84] </ref>. When the algorithm reaches a node representing a cycle, it must take each incoming CloningVector and propagate it within nodes in the recursive cycle until the CloningVector information stabilizes. <p> First, assume that the cycles in the call multigraph have been located, and nodes appearing in a cycle are collapsed into a single node <ref> [Zad84] </ref>. Secondly, assume that a topological ordering of the nodes in the reduced graph is available. NodeFrequency (n) is initialized to 1 for the main procedure. To deal with the possibility of cycles in the call multigraph (denoting recursion), we locate cycles and eliminate back edges from our consideration.
Reference: [ZBG88] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> Superb: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: Most compilers for distributed memory machines require the programmer to determine how the data will be assigned to the processors <ref> [ZBG88] </ref> [RP89] [KMV90]. The compiler is responsible for adding the message passing to move the data to processors that use it and from processors that define it. The compiler also adds the necessary synchronization to guarantee that data dependences are preserved.
References-found: 88

