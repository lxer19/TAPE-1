URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95616.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: gcf@npac.syr.edu  
Title: Software and Hardware Requirements for Some Applications of Parallel Computing to Industrial Problems  
Author: Geoffrey C. Fox 
Address: 111 College Place  Syracuse, New York 13244-4100  
Affiliation: Northeast Parallel Architectures Center  Syracuse University  
Web: http://www.npac.syr.edu  
Abstract: We discuss the hardware and software requirements that appear relevant for a set of industrial applications of parallel computing. these are divided into 33 separate categories, and come from a recent survey of industry in New York State. The software discussions includes data parallel languages, message passing, databases, and high-level integration systems. The analysis is based on a general classification of problem architectures originally developed for academic applications of parallel computing. Suitable hardware architectures are suggested for each application. The general discussion is crystalized with three case studies: computational chemistry, computational fluid dynamics, including manufacturing, and Monte Carlo Methods.
Abstract-found: 1
Intro-found: 1
Reference: [Angus:90a] <author> Angus, I. G., Fox, G. C., Kim, J. S., and Walker, D. W. </author> <title> Solving Problems on Concurrent Processors: Software for Concurrent Processors, volume 2. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: This classification <ref> [Angus:90a] </ref>, [Denning:90a], [Fox:88b;90p;91g;94a], was deduced from our experience at Caltech combined with a literature survey that was reasonably complete up to the middle of 1989.
Reference: [Barnes:86a] <author> Barnes, J., and Hut, P. </author> <title> "A hierarchical O(N log N ) force calculation algorithm," </title> <journal> Nature, </journal> <volume> 324 </volume> <pages> 446-449, </pages> <year> 1986. </year>
Reference-contexts: The latter can either use the synchronous O (N 2 particle ) algorithm or the faster, but complex loosely synchronous fast multiple O (N particle ) or O (N particle log N particle ) approaches <ref> [Barnes:86a] </ref>, [Edelsohn:91b], [Goil:94a], [Goil:95a], [Greengard:87b], [Salmon:90a], [Singh:93a], [Sunderam:93a], [Warren:92b], [Warren:93a]. 5.2 Computational Fluid Dynamics and Manufacturing (Ap plications 1, 2, 3, 4, and 32) CFD (Computational Fluid Dynamics) has been a major motivator for much algorithm and software work in HPCC, and indeed extensions of HPF have largely been based on
Reference: [Berger:84a] <author> Berger, M. J., and Oliger, J. </author> <title> "Adaptive mesh refinement for hyperbolic partial differential equations," </title> <journal> Journal of Computational Physics, </journal> <volume> 53:484, </volume> <year> 1984. </year>
Reference-contexts: The latter are also seen in fast multipole particle dynamics problems, as well as fully adaptive PDE's [Edelsohn:91b]. Some excellent methods, such as the Berger-Oliger adaptive mesh refinement <ref> [Berger:84a] </ref> require modest 31 HPF extensions as we have shown in our Grand Challenge work on col-liding black holes [Haupt:95a].
Reference: [Birman:87a] <author> Birman, K. P., and Joseph, T. </author> <title> "Reliable communication in the presence of failures," </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 5 </volume> <pages> 47-76, </pages> <month> February </month> <year> 1987. </year>
Reference: [Birman:87b] <author> Birman, K. P., and Joseph, T. </author> <title> "Exploiting virtual synchrony in distributed systems," </title> <booktitle> in Proceedings of the Eleventh Symposium on Operating Systems Principles, </booktitle> <pages> pages 123-138. </pages> <publisher> ACM, </publisher> <month> November </month> <year> 1987. </year>
Reference: [Birman:91a] <author> Birman, K., and Cooper, R. </author> <title> "The ISIS project: Real experience with a fault tolerant programming system," </title> <booktitle> Operating Systems Review, </booktitle> <pages> pages 103-107, </pages> <month> April </month> <year> 1991. </year> <booktitle> ACM/SIGOPS European Workshop on Fault-Tolerance Techniques in Operating Systems, held in Bologna, </booktitle> <address> Italy (1990). </address>
Reference: [Bodin:91a] <author> Bodin, F., Beckman, P., Gannon, D., Narayana, S., and Shelby, Y. </author> <title> "Distributed pC++: Basic ideas for an object parallel language," </title> <booktitle> in Proceedings of Supercomputing '91, </booktitle> <pages> pages 273-282. </pages> <publisher> (IEEE) Computer Society and (ACM) (SIGARCH), </publisher> <month> November </month> <year> 1991. </year>
Reference-contexts: 6: Candidate Software Paradigms for Each Problem Architectures * Synchronous: High Performance Fortran (HPF) [Foster:95a], [HPFCSep:95a], [Koelbel:94a]; Fortran 77D [Bozkus:93a], [Fox:91e], [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ <ref> [Bodin:91a] </ref>, [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN [Chandy:90a]; WebWork [Fox:95a] * Compound Metaproblems: AVS [Mills:92a;92b], [Cheng:93a]; PCN, Linda (or Trellis built on Linda); Web-work; Fortran-M [Foster:95a].
Reference: [Bogucz:94a] <author> Bogucz, E., Fox, G., Haupt, T., Hawick, K., and Ranka, S. </author> <title> "Preliminary evaluation of high-performance Fortran as a language for computational fluid dynamics." </title> <type> Technical Report SCCS-625, </type> <institution> Syra-cuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> June </month> <year> 1994. </year> <booktitle> Proc. AIAA 25th Computational Fluid Dynamics Conference, Colorado Springs, </booktitle> <pages> AIAA 94-2262. </pages>
Reference-contexts: We, and others, have discussed this at length, both in general [Choudhary:92d;92e], [Fox:90p], [Goil:94a;95a], , and in case of High Performance Fortran <ref> [Bogucz:94a] </ref>, [Chapman:94b], [Cheng:94e], [Choudhary:92g;94c], [Fox:94g], [Hawick:95a;95c], [HPF:94a], [HPFapp:95a], [Joubert:95a], [Muller:95a], [Robinson:95a], [Sturler:95a]. Note that Figure 9 refers to "HPF+"|this is some extension, called officially HPF2 (and later 3 perhaps) of HPF [HPF:93a], [HPFF:95a] to fill gaps in the original language. <p> Fluid Dynamics and Manufacturing (Ap plications 1, 2, 3, 4, and 32) CFD (Computational Fluid Dynamics) has been a major motivator for much algorithm and software work in HPCC, and indeed extensions of HPF have largely been based on CFD (or similar partial differential equation based applications) and molecular dynamics <ref> [Bogucz:94a] </ref>, [Choudhary:92d;94c], [Dincer:95b], [Goil:94a;95a], [Hawick:95a;95b], , [HPF:94a]. Partial differential equations can be quite straightforward on parallel machines if one uses regular grids, such as those coming from the simplest finite difference equations.
Reference: [Bozkus:93a] <author> Bozkus, Z., Choudhary, A., Fox, G. C., Haupt, T., and Ranka, S. </author> <title> "Fortran 90D/HPF compiler for distributed memory MIMD computers: Design, implementation, and performance results." </title> <type> Technical 35 Report SCCS-498, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <year> 1993. </year> <booktitle> Proceedings of Supercomputing '93, </booktitle> <address> Portland, OR, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Software should not be designed 24 for a particular machine model|it expresses problem and not machine char- acteristics. Table 6: Candidate Software Paradigms for Each Problem Architectures * Synchronous: High Performance Fortran (HPF) [Foster:95a], [HPFCSep:95a], [Koelbel:94a]; Fortran 77D <ref> [Bozkus:93a] </ref>, [Fox:91e], [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time
Reference: [Chandy:90a] <author> Chandy, K., and Taylor, S. </author> <title> "A primer for program composition notation." </title> <type> Technical Report CRPC-TR90056, </type> <institution> California Institute of Technology, Pasadena, </institution> <address> CA, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN <ref> [Chandy:90a] </ref>; WebWork [Fox:95a] * Compound Metaproblems: AVS [Mills:92a;92b], [Cheng:93a]; PCN, Linda (or Trellis built on Linda); Web-work; Fortran-M [Foster:95a].
Reference: [Chandy:93a] <author> Chandy, K. M., and Kesselman, C. </author> <title> CC++: A Declarative Concurrent Object-Oriented Programming Notation. Research Directions in Concurrent Object-Oriented Programming. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: Candidate Software Paradigms for Each Problem Architectures * Synchronous: High Performance Fortran (HPF) [Foster:95a], [HPFCSep:95a], [Koelbel:94a]; Fortran 77D [Bozkus:93a], [Fox:91e], [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], <ref> [Chandy:93a] </ref>, [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN [Chandy:90a]; WebWork [Fox:95a] * Compound Metaproblems: AVS [Mills:92a;92b], [Cheng:93a]; PCN, Linda (or Trellis built on Linda); Web-work; Fortran-M [Foster:95a]. <p> 77D [Bozkus:93a], [Fox:91e], [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], <ref> [Chandy:93a] </ref>, [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN [Chandy:90a]; WebWork [Fox:95a] * Compound Metaproblems: AVS [Mills:92a;92b], [Cheng:93a]; PCN, Linda (or Trellis built on Linda); Web-work; Fortran-M [Foster:95a].
Reference: [Chapman:92b] <author> Chapman, B., Mehrotra, P., and Zima, H. </author> <title> "Programming in Vienna Fortran," </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <year> 1992. </year>
Reference-contexts: Software should not be designed 24 for a particular machine model|it expresses problem and not machine char- acteristics. Table 6: Candidate Software Paradigms for Each Problem Architectures * Synchronous: High Performance Fortran (HPF) [Foster:95a], [HPFCSep:95a], [Koelbel:94a]; Fortran 77D [Bozkus:93a], [Fox:91e], [Hiranandani:92c]; Vi-enna Fortran <ref> [Chapman:92b] </ref>; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN [Chandy:90a]; WebWork
Reference: [Chapman:94b] <author> Chapman, B., Mehrotra, P., and Zima, H. </author> <title> "Extending HPF for advanced data-parallel applications," </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(3) </volume> <pages> 15-27, </pages> <year> 1994. </year>
Reference-contexts: Table 6: Candidate Software Paradigms for Each Problem Architectures * Synchronous: High Performance Fortran (HPF) [Foster:95a], [HPFCSep:95a], [Koelbel:94a]; Fortran 77D [Bozkus:93a], [Fox:91e], [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF <ref> [Chapman:94b] </ref>, [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN [Chandy:90a]; WebWork [Fox:95a] * Compound Metaproblems: AVS [Mills:92a;92b], [Cheng:93a]; PCN, Linda (or Trellis built on Linda); Web-work; Fortran-M [Foster:95a]. <p> We, and others, have discussed this at length, both in general [Choudhary:92d;92e], [Fox:90p], [Goil:94a;95a], , and in case of High Performance Fortran [Bogucz:94a], <ref> [Chapman:94b] </ref>, [Cheng:94e], [Choudhary:92g;94c], [Fox:94g], [Hawick:95a;95c], [HPF:94a], [HPFapp:95a], [Joubert:95a], [Muller:95a], [Robinson:95a], [Sturler:95a]. Note that Figure 9 refers to "HPF+"|this is some extension, called officially HPF2 (and later 3 perhaps) of HPF [HPF:93a], [HPFF:95a] to fill gaps in the original language.
Reference: [Chen:88b] <author> Chen, M., Li, J., and Choo, Y. </author> <title> "Compiling parallel programs by optimizing performance," </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 171-207, </pages> <year> 1988. </year>
Reference-contexts: Software should not be designed 24 for a particular machine model|it expresses problem and not machine char- acteristics. Table 6: Candidate Software Paradigms for Each Problem Architectures * Synchronous: High Performance Fortran (HPF) [Foster:95a], [HPFCSep:95a], [Koelbel:94a]; Fortran 77D [Bozkus:93a], [Fox:91e], [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal <ref> [Chen:88b] </ref>; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN [Chandy:90a]; WebWork [Fox:95a] * Compound Metaproblems:
Reference: [Cheng:92a] <author> Cheng, G., Faigle, C., Fox, G. C., Furmanski, W., Li, B., and Mills, K. </author> <title> "Exploring AVS for HPDC software integration: Case studies towards parallel support for GIS." </title> <type> Technical Report SCCS-473, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> March </month> <year> 1992. </year> <booktitle> Paper presented at the 2nd Annual International AVS Conference The Magic of Science: </booktitle> <address> AVS '93, Lake Buena Vista, Florida, </address> <month> May 24-26, </month> <year> 1993. </year>
Reference: [Cheng:93a] <author> Cheng, G., Lu, Y., Fox, G. C., Mills, K., and Haupt, T. </author> <title> "An interactive remote visualization environment for an electromagnetic scattering simulation on a high performance computing system." </title> <type> Technical Report SCCS-467, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> March </month> <year> 1993. </year> <booktitle> Proceedings of Supercomputing '93, </booktitle> <address> Portland, Oregon, </address> <month> November 15-19. </month>
Reference-contexts: for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN [Chandy:90a]; WebWork [Fox:95a] * Compound Metaproblems: AVS [Mills:92a;92b], <ref> [Cheng:93a] </ref>; PCN, Linda (or Trellis built on Linda); Web-work; Fortran-M [Foster:95a].
Reference: [Cheng:94a] <author> Cheng, G., Fox, G., Mills, K., and Podgorny, M. </author> <title> "Developing interactive PVM-based parallel programs on distributed computing systems within AVS framework." </title> <type> Technical Report SCCS-611, </type> <institution> Syra-cuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> January </month> <year> 1994. </year> <booktitle> Proceedings of the 3rd Annual International AVS Conference, JOIN THE REVOLUTION: </booktitle> <address> AVS'94, Boston, MA, </address> <month> May 2-4. 36 </month>
Reference: [Cheng:94c] <author> Cheng, G., Hawick, K., Mortensen, G., and Fox, G. </author> <title> "Dis--tributed computational electromagnetics systems." </title> <type> Technical Report SCCS-635, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> August </month> <year> 1994. </year> <booktitle> Proceedings of the 7th SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <month> February 15-17, </month> <year> 1995. </year>
Reference-contexts: Note that in both cases, the matrix M is treated as full <ref> [Cheng:94c] </ref>, and is quite different from the familiar sparse matrices gotten from discretizing a partial differential equation. We note in passing that such spatial discretization is a quite viable approach to CEM and leads to a totally different computational problem architecture from the spectral moment formulation. <p> The matrix solution stage exploits fully the Fortran 90 array manipulation and clearly requires good compiler support for matrix and vector manipulation primitives. NPAC's experience with a production CEM code PARAMOM from the Syracuse Research Corporation is illuminating <ref> [Cheng:94c] </ref>. Both stages could be implemented on IBM SP-2 with specialized Fortran code for the matrix element generation joined to SCALAPACK based matrix solution [Choi:92c]. However, the CM-5 implementation was not so simple. The CMSSL library provided exceptional matrix solution with good use being made of the CM-5's vector nodes.
Reference: [Cheng:94d] <author> Cheng, G., Fox, G., and Mills, K. </author> <title> "Integrating multiple programming paradigms on Connection Machine CM5 in a dataflow-based software environment (draft)." </title> <type> Technical Report SCCS-548, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> October </month> <year> 1994. </year>
Reference: [Cheng:94e] <author> Cheng, G., Fox, G. C., and Hawick, K. </author> <title> A Scalable Parallel Paradigm for Effectively-Dense Matrix Formulated Applications, </title> <booktitle> volume 797 of Lecture Notes in Computer Science, </booktitle> <pages> pages 202-210. </pages> <publisher> Springer-Verlag, </publisher> <month> April </month> <year> 1994. </year> <booktitle> Proceedings of the European Conference and Exhibition on High-Performance Computing and Networking (HPCN Eu-rope) 1994, </booktitle> <institution> Munich, Germany; Syracuse University Technical Report SCCS-580. </institution>
Reference-contexts: We, and others, have discussed this at length, both in general [Choudhary:92d;92e], [Fox:90p], [Goil:94a;95a], , and in case of High Performance Fortran [Bogucz:94a], [Chapman:94b], <ref> [Cheng:94e] </ref>, [Choudhary:92g;94c], [Fox:94g], [Hawick:95a;95c], [HPF:94a], [HPFapp:95a], [Joubert:95a], [Muller:95a], [Robinson:95a], [Sturler:95a]. Note that Figure 9 refers to "HPF+"|this is some extension, called officially HPF2 (and later 3 perhaps) of HPF [HPF:93a], [HPFF:95a] to fill gaps in the original language.
Reference: [Choi:92c] <author> Choi, J., Dongarra, J. J., Pozo, R., and Walker, D. W. "Scala-pack: </author> <title> A scalable linear algebra library for distributed memory concurrent computers," </title> <booktitle> in Proceedings of the Fourth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 120-127. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1992. </year>
Reference-contexts: NPAC's experience with a production CEM code PARAMOM from the Syracuse Research Corporation is illuminating [Cheng:94c]. Both stages could be implemented on IBM SP-2 with specialized Fortran code for the matrix element generation joined to SCALAPACK based matrix solution <ref> [Choi:92c] </ref>. However, the CM-5 implementation was not so simple. The CMSSL library provided exceptional matrix solution with good use being made of the CM-5's vector nodes. However, the matrix element computation was not so straightforward.
Reference: [Choudhary:92d] <author> Choudhary, A., Fox, G., Hiranandani, S., Kennedy, K., Koelbel, C., Ranka, S., and Saltz, J. </author> <title> "A classification of irregular loosely synchronous problems and their support in scalable parallel software systems," </title> <booktitle> in DARPA Software Technology Conference 1992 Proceedings, </booktitle> <pages> pages 138-149, </pages> <month> April </month> <year> 1992. </year> <note> Syracuse Technical Report SCCS-255. </note>
Reference-contexts: Problems handled by pC++, HPF extensions, Message Passing (Table 6) Asyncsoft Parallel Software System for (particular) class of asyn chronous problems (Table 6) CFD Computational Fluid Dynamics ED Event Driven Simulation FD Finite Difference Method FEM Finite Element Method HPF High Performance Fortran [HPF:93a], [HPFF:95a] HPF+ Natural Extensions of HPF <ref> [Choudhary:92d] </ref>, [HPF:94a], [HPFapp:95a] Integration Software to integrate components of metaproblems (Ta ble 6) MPF Fortran plus message passing for loosely synchronous pro gramming support PDE Partial Differential Equation TS Time Stepped Simulation VR Virtual Reality Note on Language: HPF, MPF use Fortran for illustration, one can use parallel C, C++ or <p> Table 6: Candidate Software Paradigms for Each Problem Architectures * Synchronous: High Performance Fortran (HPF) [Foster:95a], [HPFCSep:95a], [Koelbel:94a]; Fortran 77D [Bozkus:93a], [Fox:91e], [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], <ref> [Choudhary:92d] </ref>, [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN [Chandy:90a]; WebWork [Fox:95a] * Compound Metaproblems: AVS [Mills:92a;92b], [Cheng:93a]; PCN, Linda (or Trellis built on Linda); Web-work; Fortran-M [Foster:95a].
Reference: [Choudhary:92e] <author> Choudhary, A., Fox, G., Ranka, S., Hiranandani, S., Kennedy, K., Koelbel, C., and Saltz, J. </author> <title> "Software support for irregular and loosely synchronous problems," </title> <booktitle> Computing Systems in Engineering, </booktitle> <address> 3(1-4):43-52, </address> <year> 1992. </year> <note> CSE-MS 118, CRPC-TR92258. </note>
Reference: [Choudhary:92g] <author> Choudhary, A., Fox, G., Haupt, T., and Ranka, S. </author> <title> "Which applications can use high performance Fortran and FortranD| industry standard data parallel languages?," </title> <booktitle> in Proceedings of Fifth 37 Australian Supercomputing Conference, </booktitle> <month> December </month> <year> 1992. </year> <month> CRPC--TR92264. </month>
Reference: [Choudhary:94c] <author> Choudhary, A., Dincer, K., Fox, G., and Hawick, K. </author> <title> "Conjugate gradient algorithms implemented in high performance Fortran." </title> <type> Technical Report SCCS-639, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> October </month> <year> 1994. </year>
Reference: [Copty:93a] <author> Copty, N., Ranka, S., Fox, G., and Shankar, R. </author> <title> "Solving the region growing problem on the Connection Machine," </title> <booktitle> in Proceedings of the 22nd International Conference on Parallel Processing, </booktitle> <volume> volume 3, </volume> <pages> pages 102-105, </pages> <year> 1993. </year> <institution> Syracuse University, NPAC Technical Report SCCS-397b. </institution>
Reference: [Copty:94a] <author> Copty, N., ranka, S., Fox, G., and Shankar, R. </author> <title> "A data parallel algorithm for solving the region growing problem on the Connection Machine," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1), </volume> <year> 1994. </year> <institution> Syracuse University, NPAC Technical Report SCCS-596. </institution>
Reference: [Copty:95a] <author> Copty, N. </author> <title> Language and Runtime Support for the Execution of Clustering Applications on Distributed Memory Machines. </title> <type> PhD thesis, </type> <institution> Syracuse University, </institution> <year> 1995. </year>
Reference: [Denning:90a] <author> Denning, P. J., and Tichy, W. F. </author> <title> "Highly parallel computation," </title> <journal> Science, </journal> <volume> 250 </volume> <pages> 1217-1222, </pages> <year> 1990. </year>
Reference-contexts: This classification [Angus:90a], <ref> [Denning:90a] </ref>, [Fox:88b;90p;91g;94a], was deduced from our experience at Caltech combined with a literature survey that was reasonably complete up to the middle of 1989.
Reference: [Dincer:95b] <author> Dincer, K., Hawick, K., Choudhary, A., and Fox, G. </author> <title> "High performance Fortran and possible extensions to support conjugate gradient algorithms." </title> <type> Technical Report SCCS-703, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> March </month> <year> 1995. </year> <note> To appear in Proc. Supercomputing '95, </note> <month> December </month> <year> 1995. </year>
Reference-contexts: and Manufacturing (Ap plications 1, 2, 3, 4, and 32) CFD (Computational Fluid Dynamics) has been a major motivator for much algorithm and software work in HPCC, and indeed extensions of HPF have largely been based on CFD (or similar partial differential equation based applications) and molecular dynamics [Bogucz:94a], [Choudhary:92d;94c], <ref> [Dincer:95b] </ref>, [Goil:94a;95a], [Hawick:95a;95b], , [HPF:94a]. Partial differential equations can be quite straightforward on parallel machines if one uses regular grids, such as those coming from the simplest finite difference equations.
Reference: [Edelsohn:91b] <author> Edelsohn, D., and Fox, G. C. </author> <title> "Hierarchical tree-structures as adaptive meshes." </title> <type> Technical Report SCCS-193, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> November </month> <year> 1991. </year> <journal> Published in the International Journal of Modern Physics C, </journal> <volume> Vol. 4, No. 5, </volume> <pages> pp. 909-917; CRPC-TR91186. </pages>
Reference-contexts: The latter can either use the synchronous O (N 2 particle ) algorithm or the faster, but complex loosely synchronous fast multiple O (N particle ) or O (N particle log N particle ) approaches [Barnes:86a], <ref> [Edelsohn:91b] </ref>, [Goil:94a], [Goil:95a], [Greengard:87b], [Salmon:90a], [Singh:93a], [Sunderam:93a], [Warren:92b], [Warren:93a]. 5.2 Computational Fluid Dynamics and Manufacturing (Ap plications 1, 2, 3, 4, and 32) CFD (Computational Fluid Dynamics) has been a major motivator for much algorithm and software work in HPCC, and indeed extensions of HPF have largely been based on CFD <p> All the more advanced partial differential equation schemes naturally need somewhat more sophisticated (than simple arrays) data structures, including arrays of pointers, linked lists, nested arrays, and complex trees. The latter are also seen in fast multipole particle dynamics problems, as well as fully adaptive PDE's <ref> [Edelsohn:91b] </ref>. Some excellent methods, such as the Berger-Oliger adaptive mesh refinement [Berger:84a] require modest 31 HPF extensions as we have shown in our Grand Challenge work on col-liding black holes [Haupt:95a].
Reference: [Edjali:95a] <author> Edjali, G., Agrawal, G., Sussman, A., and Saltz, J. </author> <title> "Data parallel programming in an adaptive environment," </title> <booktitle> in IPPS '95, </booktitle> <pages> pages 827-832, </pages> <year> 1995. </year> <note> An extended version also available as University of Maryland Technical Report CS-TR-3350 and UMIACS-TR-94-109. 38 </note>
Reference-contexts: However, as Saltz's group has shown in a set of pioneering projects [HPF:94a], many important PDE methods require nontrivial HPF language extensions, as well as sophisticated runtime support, such as the PARTI [Saltz:91b] and CHAOS systems <ref> [Edjali:95a] </ref>, [Hwang:94a], [Ponnusamy:93c;94b]. The needed language support can be thought of as expressing the problem architecture (computational graph as in Figure 3 (a), which is only implicitly defined by the standard (Fortran) code. Correctly written, this vanilla Fortran implies all needed information for efficient parallelism.
Reference: [Factor:90a] <author> Factor, M. </author> <title> "The process Trellis architecture for real time monitors," </title> <booktitle> in Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPOP), </booktitle> <month> March </month> <year> 1990. </year> <title> Held in Seattle, </title> <address> Washington. </address>
Reference: [Factor:90b] <author> Factor, M., and Gelertner, D. G. </author> <title> "Experience with Trellis architecture." </title> <type> Technical Report YALEU/DCS/RR-818, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <month> August </month> <year> 1990. </year>
Reference: [Felten:88i] <author> Felten, E. W., and Otto, S. W. </author> <title> "A highly parallel chess program," </title> <booktitle> in Proceedings of International Conference on Fifth Generation Computer Systems 1988, </booktitle> <pages> pages 1001-1009. </pages> <publisher> ICOT, </publisher> <month> November </month> <year> 1988. </year> <institution> Tokyo, </institution> <address> Japan, </address> <month> November 28 - December 2. </month> <note> Caltech Report C3P-579c. </note>
Reference-contexts: Other examples include computer chess <ref> [Felten:88i] </ref> and transaction analysis. Asynchronous problems are hard to parallelize and some may not run well on massively parallel machines. They require sophisticated software and hardware support to properly synchronize the nodes of the parallel machine, as is illustrated by time warp mechanism [Wieland:89a]. <p> In the above cases, the asynchronous components of the problems were large grain modules with modest parallelism. This can be contrasted with Otto and Felten's MIMD computer chess algorithm, where the asynchornous evaluation of the pruned tree is "massively parallel" <ref> [Felten:88i] </ref>. Here, one can break the problem up into many loosely coupled but asynchronous parallel components, which give excellent and scalable parallel performance. Each asynchronous task is now a synchronous or loosely synchronous modestly parallel evaluation of a given chess position.
Reference: [Foster:95a] <author> Foster, I. </author> <title> Designing and Building Parallel Programs. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year> <note> http://www.mcs.acl.gov/dbpp/. </note>
Reference-contexts: Software should not be designed 24 for a particular machine model|it expresses problem and not machine char- acteristics. Table 6: Candidate Software Paradigms for Each Problem Architectures * Synchronous: High Performance Fortran (HPF) <ref> [Foster:95a] </ref>, [HPFCSep:95a], [Koelbel:94a]; Fortran 77D [Bozkus:93a], [Fox:91e], [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda <p> above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN [Chandy:90a]; WebWork [Fox:95a] * Compound Metaproblems: AVS [Mills:92a;92b], [Cheng:93a]; PCN, Linda (or Trellis built on Linda); Web-work; Fortran-M <ref> [Foster:95a] </ref>.
Reference: [Fox:87d] <author> Fox, G. C. </author> <title> "Questions and unexpected answers in concurrent computation," </title> <editor> in J. J. Dongarra, editor, </editor> <booktitle> Experimental Parallel Computing Architectures, </booktitle> <pages> pages 97-121. </pages> <publisher> Elsevier Science Publishers B.V., North-Holland, </publisher> <address> Amsterdam, </address> <year> 1987. </year> <note> Caltech Report C3P-288. </note>
Reference-contexts: At Caltech, we developed some 50 applications on parallel machines, 25 of which led to publications in the scientific literature, describing the results of simulations performed on our parallel computers <ref> [Fox:87d] </ref> [Fox:88a], [Fox:88oo], [Fox:89n]. Our Caltech work was mainly on the hypercube, but the total of 300 references used in original classification covered work on the Butterfly, transputers, the SIMD Connection Machine, and DAP.
Reference: [Fox:88a] <author> Fox, G. C., Johnson, M. A., Lyzenga, G. A., Otto, S. W., Salmon, J. K., and Walker, D. W. </author> <title> Solving Problems on Concurrent Processors, volume 1. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: At Caltech, we developed some 50 applications on parallel machines, 25 of which led to publications in the scientific literature, describing the results of simulations performed on our parallel computers [Fox:87d] <ref> [Fox:88a] </ref>, [Fox:88oo], [Fox:89n]. Our Caltech work was mainly on the hypercube, but the total of 300 references used in original classification covered work on the Butterfly, transputers, the SIMD Connection Machine, and DAP.
Reference: [Fox:88b] <author> Fox, G. C. </author> <title> "What have we learnt from using real parallel machines to solve real problems?," </title> <editor> in G. C. Fox, editor, </editor> <booktitle> The Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <volume> Volume 2, </volume> <pages> pages 897-955. </pages> <publisher> ACM Press, </publisher> <address> New York, </address> <month> January </month> <year> 1988. </year> <note> Caltech Report C3P-522. </note>
Reference-contexts: Here, we summarize relevant features of it in Section 2. Section 3 reviews and extends a classification of problem architectures originally developed in 1988 from a rather complete survey of parallel applications at the time <ref> [Fox:88b] </ref>, [Fox:88tt], [Fox:91g], [Fox:94a]. 1 In Section 4, we show how the different problem categories or architec-tures are addressed by parallel software systems with different capabilities. We give illustrative examples, but not an exhaustive list of existing software systems with these characteristics. <p> It is interesting that massively parallel distributed memory MIMD machines that have an asynchronous hardware architecture are perhaps most relevant for loosely synchronous scientific problems. We have looked at many more applications since the detailed survey in <ref> [Fox:88b] </ref>, and the general picture described above remains valid. Industrial applications have less synchronous and more loosely synchronous problems than academic problems. We have recently recognized that many complicated problems are mixtures of the basic classifications.
Reference: [Fox:88oo] <author> Fox, G. C. </author> <title> "The hypercube and the Caltech Concurrent Computation Program: A microcosm of parallel computing," </title> <editor> in B. J. Alder, editor, </editor> <booktitle> Special Purpose Computers, </booktitle> <pages> pages 1-40. </pages> <publisher> Academic Press, Inc., </publisher> <address> Boston, </address> <year> 1988. </year> <note> Caltech Report C3P-422. </note>
Reference-contexts: At Caltech, we developed some 50 applications on parallel machines, 25 of which led to publications in the scientific literature, describing the results of simulations performed on our parallel computers [Fox:87d] [Fox:88a], <ref> [Fox:88oo] </ref>, [Fox:89n]. Our Caltech work was mainly on the hypercube, but the total of 300 references used in original classification covered work on the Butterfly, transputers, the SIMD Connection Machine, and DAP.
Reference: [Fox:88tt] <author> Fox, G. C., and Furmanski, W. </author> <title> "The physical structure of concurrent problems and concurrent computers," </title> <journal> Phil. Trans. R. Soc. Lond. A, </journal> <volume> 326 </volume> <pages> 411-444, </pages> <year> 1988. </year> <note> Caltech Report C3P-493. 39 </note>
Reference-contexts: Here, we summarize relevant features of it in Section 2. Section 3 reviews and extends a classification of problem architectures originally developed in 1988 from a rather complete survey of parallel applications at the time [Fox:88b], <ref> [Fox:88tt] </ref>, [Fox:91g], [Fox:94a]. 1 In Section 4, we show how the different problem categories or architec-tures are addressed by parallel software systems with different capabilities. We give illustrative examples, but not an exhaustive list of existing software systems with these characteristics. <p> The temporal structure of a problem is analogous to the hardware classification into SIMD and MIMD. Further detail is contained in the spatial structure or computational graph of Figure 3a describing the problem at a given instant of simulation time <ref> [Fox:88tt] </ref>. This is important in determing the performance, as shown in Chapter 3 of [Fox:94a] of an implementation, but it does not affect the broad software issues discussed here.
Reference: [Fox:89n] <author> Fox, G. C. </author> <title> "Parallel computing comes of age: Supercomputer level parallel computations at Caltech," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 1(1) </volume> <pages> 63-103, </pages> <month> September </month> <year> 1989. </year> <note> Caltech Report C3P-795. </note>
Reference-contexts: At Caltech, we developed some 50 applications on parallel machines, 25 of which led to publications in the scientific literature, describing the results of simulations performed on our parallel computers [Fox:87d] [Fox:88a], [Fox:88oo], <ref> [Fox:89n] </ref>. Our Caltech work was mainly on the hypercube, but the total of 300 references used in original classification covered work on the Butterfly, transputers, the SIMD Connection Machine, and DAP.
Reference: [Fox:90p] <author> Fox, G. C. </author> <title> "Hardware and software architectures for irregular problem architectures," </title> <editor> in P. Mehrotra, J. Saltz, and R. Voigt, editors, </editor> <booktitle> Unstructured Scientific Computation on Scalable Microprocessors, </booktitle> <pages> pages 125-160. </pages> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year> <title> Scientific and Engineering Computation Series. Held by ICASE in Nags Head, </title> <publisher> North Carolina. SCCS-111; CRPC-TR91164. </publisher>
Reference-contexts: We, and others, have discussed this at length, both in general [Choudhary:92d;92e], <ref> [Fox:90p] </ref>, [Goil:94a;95a], , and in case of High Performance Fortran [Bogucz:94a], [Chapman:94b], [Cheng:94e], [Choudhary:92g;94c], [Fox:94g], [Hawick:95a;95c], [HPF:94a], [HPFapp:95a], [Joubert:95a], [Muller:95a], [Robinson:95a], [Sturler:95a]. Note that Figure 9 refers to "HPF+"|this is some extension, called officially HPF2 (and later 3 perhaps) of HPF [HPF:93a], [HPFF:95a] to fill gaps in the original language.
Reference: [Fox:91e] <author> Fox, G. C., Hiranandani, S., Kennedy, K., Koelbel, C., Kremer, U., Tseng, C.-W., and Wu, M.-Y. </author> <title> "Fortran D language specification." </title> <type> Technical Report SCCS-42c, </type> <institution> Syracuse University, Syracuse, </institution> <address> NY, </address> <month> April </month> <year> 1991. </year> <note> Rice Center for Research in Parallel Computation; CRPC-TR90079. </note>
Reference-contexts: Software should not be designed 24 for a particular machine model|it expresses problem and not machine char- acteristics. Table 6: Candidate Software Paradigms for Each Problem Architectures * Synchronous: High Performance Fortran (HPF) [Foster:95a], [HPFCSep:95a], [Koelbel:94a]; Fortran 77D [Bozkus:93a], <ref> [Fox:91e] </ref>, [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp
Reference: [Fox:91g] <author> Fox, G. C. </author> <title> "The architecture of problems and portable parallel software systems." </title> <type> Technical Report SCCS-134, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> July </month> <year> 1991. </year> <note> Revised SCCS-78b. </note>
Reference-contexts: Here, we summarize relevant features of it in Section 2. Section 3 reviews and extends a classification of problem architectures originally developed in 1988 from a rather complete survey of parallel applications at the time [Fox:88b], [Fox:88tt], <ref> [Fox:91g] </ref>, [Fox:94a]. 1 In Section 4, we show how the different problem categories or architec-tures are addressed by parallel software systems with different capabilities. We give illustrative examples, but not an exhaustive list of existing software systems with these characteristics.
Reference: [Fox:91m] <author> Fox, G. C. </author> <title> "Lessons from massively parallel architectures on message passing computers," </title> <booktitle> in The 37th Annual IEEE International Computer Conference, COMPCON '92. </booktitle> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <month> December </month> <year> 1991. </year> <pages> Held February 24-28, </pages> <address> 1992 San Francisco, California. CRPC-TR91192; SCCS-214. </address>
Reference-contexts: High Performance Fortran (HPF) [Foster:95a], [HPFCSep:95a], [Koelbel:94a]; Fortran 77D [Bozkus:93a], [Fox:91e], [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing <ref> [Fox:91m] </ref>, [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN [Chandy:90a]; WebWork [Fox:95a] * Compound Metaproblems: AVS [Mills:92a;92b], [Cheng:93a]; PCN, Linda (or Trellis built on Linda); Web-work; Fortran-M [Foster:95a].
Reference: [Fox:92e] <author> Fox, G. C. </author> <title> "Parallel computing in industry|an initial survey," </title> <booktitle> in Proceedings of Fifth Australian Supercomputing Conference (supplement), </booktitle> <pages> pages 1-10. </pages> <publisher> Communications Services, </publisher> <address> Melbourne, </address> <month> December </month> <year> 1992. </year> <title> Held at World Congress Centre, </title> <institution> Melbourne, Australia. Syracuse University Technical Report SCCS-302b. CRPC-TR92219. </institution>
Reference-contexts: We show how this allows one to isolate the parallel computing hardware and software characteristics needed for each problem. The industrial applications come from a survey undertaken of New York State industry in 1991 and 1992. Further details of the survey will be found in <ref> [Fox:92e] </ref>, [Fox:94b], [Fox:94h], [Fox:94c], [Fox:94i], [Mills:93a]. Here, we summarize relevant features of it in Section 2. <p> In this paper, we concentrate on the software capabilities needed for these applications, and also the appropriate parallel machine architectures. Further discussion of the survey technique, particular companies interviewed, and the detailed nature of the applications will be found in <ref> [Fox:92e] </ref>, [Fox:94a], and papers cited earlier. The survey has inevitable limitations. There are many important applications|oil exploration is a good example|that are not well represented in New York State.
Reference: [Fox:93c] <author> Fox, G., Bogucz, E., Jones, D., Mills, K., and Podgorny, M. "In-foMall: </author> <title> a scalable organization for the development of HPCC software and systems." </title> <type> Technical Report SCCS-531, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> September </month> <year> 1993. </year> <note> Unpublished. </note>
Reference-contexts: It is not directly relevant to this paper, but the results of this survey caused the ACTION program to refocus its efforts and evolve into InfoMall <ref> [Fox:93c] </ref>, [Fox:94f], [Fox:94h], [Fox:95b], [Infourl:95a], [Mills:94a]. Here, "Info" refers to the information based application focus and "Mall" to the use of a virtual corporation (groups of "storeholders") to produce the complex integrated applications enabled by HPCC. The first column of Table 4 contains the area label and some sample applications.
Reference: [Fox:94a] <editor> Fox, G. C., Messina, P. C., and Williams, R. D., editors. </editor> <publisher> Parallel Computing Works! Morgan Kaufmann Publishers, </publisher> <address> San Francisco, CA, </address> <year> 1994. </year> <note> http://www.infomall.org/npac/pcw/. 40 </note>
Reference-contexts: Here, we summarize relevant features of it in Section 2. Section 3 reviews and extends a classification of problem architectures originally developed in 1988 from a rather complete survey of parallel applications at the time [Fox:88b], [Fox:88tt], [Fox:91g], <ref> [Fox:94a] </ref>. 1 In Section 4, we show how the different problem categories or architec-tures are addressed by parallel software systems with different capabilities. We give illustrative examples, but not an exhaustive list of existing software systems with these characteristics. <p> In this paper, we concentrate on the software capabilities needed for these applications, and also the appropriate parallel machine architectures. Further discussion of the survey technique, particular companies interviewed, and the detailed nature of the applications will be found in [Fox:92e], <ref> [Fox:94a] </ref>, and papers cited earlier. The survey has inevitable limitations. There are many important applications|oil exploration is a good example|that are not well represented in New York State. <p> We originally identified three temporal structures and one especially important (as it was so simple) spatial structure, which are the first four entries in Table 4. Chapter 3 of <ref> [Fox:94a] </ref> describes a "complex systems" approach to computation and introduces the spatial and temporal structure of problems and computers. <p> Further detail is contained in the spatial structure or computational graph of Figure 3a describing the problem at a given instant of simulation time [Fox:88tt]. This is important in determing the performance, as shown in Chapter 3 of <ref> [Fox:94a] </ref> of an implementation, but it does not affect the broad software issues discussed here. In Table 4, we only single out one special spatial structure, "embarrassingly parallel," where there is little or no connection between the individual parallel program components. <p> paradigms and machine architectures are appropriate. 5.1 Computational Chemistry and Electromagnetics (Applications 3, 7, and 8) Many chemistry problems are formulated in terms of states of a chemical system, which can be labelled by an index corresponding to species, choice of wave function, or internal excitation (see Chapter 8 of <ref> [Fox:94a] </ref>). The 28 calculation of energy levels, potential or transition probability can often be related to a matrix M ij whose rows and columns are just the possible system states. M is often an approximation to the Hamiltonian of the system or it could represent overlap between the states. <p> Not all chemistry computations have this structure. For instance, there is a set of applications such as AMBER and CHARMM that are based on molecular dynamics simulations, as described in Chapter 16 of <ref> [Fox:94a] </ref>, [Ranka:92a]. These are typically loosely synchronous problems with each particle linked to a dynamic set of "nearest neighbors" combined with long-range nonbonded force computations. <p> As described in Chapter 4 of <ref> [Fox:94a] </ref>, this application is straightforward to parallelize and very suitable for HPF as the basic data structure is an array. The array represents a regular structure in space time as seen in the simplest finite different problems. <p> QCD is typical of simulations of crystalline substances with a regular array of atoms. However, many substances|in particular gases and liquids|have irregular particle distributions and many of issues discussed briefly in Section 5.2 for finite element methods. As described in Chapter 14 of <ref> [Fox:94a] </ref>, there is a subtle point that distinguishes Monte Carlo and PDE algorithms as one cannot simultaneously update in Monte Carlo, sites with overlapping neighbors. <p> This complicates the loosely synchronous structure and can make problem architecture look like that of a synchronous event driven simulations|here events are individual Monte Carlo updates. "Detailed balance" requires that such events be sequentially (if arbitrarily) ordered. In the example of [Johnson:86c] described in <ref> [Fox:94a] </ref>, a clever implementation gave good parallel performance. Monte Carlo methods can be implemented quite differently|above we decomposed the underlying physical data. One can also use "data parallelism" on the random number set used in the simulation. This is not possi 33 ble for QCD for two reasons. <p> Thermalization is very time consuming for QCD and makes multiple starting points of limited value. However, there are many cases where this is not true, and as show in Chapter 7 of <ref> [Fox:94a] </ref>, one can get an embarrassing parallel architecture for Monte Carlo problems. Each instance of the problem has the full physical dataset, but can be run independently with different random number streams. <p> A simple example comes from ferromagnetic materials where domains form where spins are locked in the same direction over large regions., Clustering algorithms are quite hard to find for sequential systems, and their paral-lelization is challenging and very different from the earlier examples. As discussed in Section 12.6 of <ref> [Fox:94a] </ref>, the algorithm is similar to that used in region finding in image processing [Copty:93a;94a;95a].
Reference: [Fox:94b] <author> Fox, G., and Mills, K. </author> <booktitle> Information Processing and Opportunities for HPCN Use in Industry, </booktitle> <pages> pages 1-14. </pages> <booktitle> Number 796 in Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <month> April </month> <year> 1994. </year> <booktitle> Proceedings of HPCN Europe 1994, "High Performance Computing and Networking. </booktitle>
Reference-contexts: We show how this allows one to isolate the parallel computing hardware and software characteristics needed for each problem. The industrial applications come from a survey undertaken of New York State industry in 1991 and 1992. Further details of the survey will be found in [Fox:92e], <ref> [Fox:94b] </ref>, [Fox:94h], [Fox:94c], [Fox:94i], [Mills:93a]. Here, we summarize relevant features of it in Section 2.
Reference: [Fox:94c] <author> Fox, G., and Mills, K. </author> <title> "Information processing and HPCC applications in industry," </title> <booktitle> in Proceedings of Annual 1994 Dual-use Conference, </booktitle> <address> Utica, NY, </address> <month> May </month> <year> 1994. </year> <note> IEEE Mohawk Valley. </note>
Reference-contexts: We show how this allows one to isolate the parallel computing hardware and software characteristics needed for each problem. The industrial applications come from a survey undertaken of New York State industry in 1991 and 1992. Further details of the survey will be found in [Fox:92e], [Fox:94b], [Fox:94h], <ref> [Fox:94c] </ref>, [Fox:94i], [Mills:93a]. Here, we summarize relevant features of it in Section 2.
Reference: [Fox:94f] <author> Fox, G., Furmanski, W., Hawick, K., and Leskiw, D. </author> <title> "Exploration of the InfoMall concept." </title> <type> Technical Report SCCS-634, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: It is not directly relevant to this paper, but the results of this survey caused the ACTION program to refocus its efforts and evolve into InfoMall [Fox:93c], <ref> [Fox:94f] </ref>, [Fox:94h], [Fox:95b], [Infourl:95a], [Mills:94a]. Here, "Info" refers to the information based application focus and "Mall" to the use of a virtual corporation (groups of "storeholders") to produce the complex integrated applications enabled by HPCC. The first column of Table 4 contains the area label and some sample applications.
Reference: [Fox:94g] <author> Fox, G., and Hawick, K. </author> <title> "An applications perspective on high performance Fortran." </title> <type> Technical Report SCCS-641, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: We, and others, have discussed this at length, both in general [Choudhary:92d;92e], [Fox:90p], [Goil:94a;95a], , and in case of High Performance Fortran [Bogucz:94a], [Chapman:94b], [Cheng:94e], [Choudhary:92g;94c], <ref> [Fox:94g] </ref>, [Hawick:95a;95c], [HPF:94a], [HPFapp:95a], [Joubert:95a], [Muller:95a], [Robinson:95a], [Sturler:95a]. Note that Figure 9 refers to "HPF+"|this is some extension, called officially HPF2 (and later 3 perhaps) of HPF [HPF:93a], [HPFF:95a] to fill gaps in the original language.
Reference: [Fox:94h] <author> Fox, G., Hawick, K., Podgorny, M., and Mills, K. </author> <title> The Electronic InfoMall|HPCN Enabling Industry and Commerce, </title> <booktitle> volume 919 of Lecture Notes in Computer Science, </booktitle> <pages> pages 360-365. </pages> <publisher> Springer-Verlag, </publisher> <month> November </month> <year> 1994. </year> <institution> Syracuse University Technical Report SCCS-665. </institution>
Reference-contexts: We show how this allows one to isolate the parallel computing hardware and software characteristics needed for each problem. The industrial applications come from a survey undertaken of New York State industry in 1991 and 1992. Further details of the survey will be found in [Fox:92e], [Fox:94b], <ref> [Fox:94h] </ref>, [Fox:94c], [Fox:94i], [Mills:93a]. Here, we summarize relevant features of it in Section 2. <p> It is not directly relevant to this paper, but the results of this survey caused the ACTION program to refocus its efforts and evolve into InfoMall [Fox:93c], [Fox:94f], <ref> [Fox:94h] </ref>, [Fox:95b], [Infourl:95a], [Mills:94a]. Here, "Info" refers to the information based application focus and "Mall" to the use of a virtual corporation (groups of "storeholders") to produce the complex integrated applications enabled by HPCC. The first column of Table 4 contains the area label and some sample applications.
Reference: [Fox:94i] <author> Fox, G. C. </author> <title> "Involvement of industry in the national high performance computing and communication enterprise." </title> <type> Technical Report SCCS-716, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> May </month> <year> 1994. </year> <title> Developing a Computer Science Agenda for High Performance Computing, edited by U. Vishkin, </title> <publisher> ACM Press. </publisher>
Reference-contexts: We show how this allows one to isolate the parallel computing hardware and software characteristics needed for each problem. The industrial applications come from a survey undertaken of New York State industry in 1991 and 1992. Further details of the survey will be found in [Fox:92e], [Fox:94b], [Fox:94h], [Fox:94c], <ref> [Fox:94i] </ref>, [Mills:93a]. Here, we summarize relevant features of it in Section 2.
Reference: [Fox:95a] <author> Fox, G. C., Furmanski, W., Chen, M., Rebbi, C., and Cowie, J. H. "WebWork: </author> <title> integrated programming environment tools for national and grand challenges." </title> <type> Technical Report SCCS-715, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> June </month> <year> 1995. </year> <note> Joint Boston-CSC-NPAC Project Plan to Develop WebWork. </note>
Reference-contexts: C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN [Chandy:90a]; WebWork <ref> [Fox:95a] </ref> * Compound Metaproblems: AVS [Mills:92a;92b], [Cheng:93a]; PCN, Linda (or Trellis built on Linda); Web-work; Fortran-M [Foster:95a]. <p> The linkage of these stages used AVS, but one could alternatively use many other coordination software approaches. We expect to test our use of World Wide Web technology WebWork <ref> [Fox:95a] </ref> on this example. This simple example illustrates three problem classes: embarrassingly parallel, synchronous and metaproblems, and associated machine and software architecture. There is an interesting software engineering issue. Typically, one would develop a single Fortran program for such a computational chemistry or electromagnetics problem. <p> The integration of conceptual (initial) and detailed design with the manufacturing and life cycle support phases naturally requires the integration of information and computing in the support sys 32 tem. WebWork <ref> [Fox:95a] </ref> has been designed for this. Further, we see this problem is a heterogeneous metaproblem with perhaps up to 10,000 (Fortran) programs linked together in the full optimization process. This is basically an embarrassingly parallel meta-architecture with only a few of the programs linked together at each stage.
Reference: [Fox:95b] <author> Fox, G. C., Furmanski, W., Hawick, K., and Leskiw, D. </author> <title> "Exploration of the InfoMall concept|building on the electronic InfoMall." </title> <type> Technical Report SCCS-711, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> May </month> <year> 1995. </year> <month> 41 </month>
Reference-contexts: It is not directly relevant to this paper, but the results of this survey caused the ACTION program to refocus its efforts and evolve into InfoMall [Fox:93c], [Fox:94f], [Fox:94h], <ref> [Fox:95b] </ref>, [Infourl:95a], [Mills:94a]. Here, "Info" refers to the information based application focus and "Mall" to the use of a virtual corporation (groups of "storeholders") to produce the complex integrated applications enabled by HPCC. The first column of Table 4 contains the area label and some sample applications.
Reference: [Gelertner:89a] <author> Gelertner, D. </author> <title> Multiple Tuple Spaces in Linda, </title> <booktitle> volume 366 of Lecture Notes in Computer Science, Proceedings of Parallel Architectures and Languages, Europe, </booktitle> <volume> Volume 2, </volume> <pages> pages 20-27. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin/New York, </address> <month> June </month> <year> 1989. </year>
Reference-contexts: [Koelbel:94a]; Fortran 77D [Bozkus:93a], [Fox:91e], [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], <ref> [Gelertner:89a] </ref>; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN [Chandy:90a]; WebWork [Fox:95a] * Compound Metaproblems: AVS [Mills:92a;92b], [Cheng:93a]; PCN, Linda (or Trellis built on Linda); Web-work; Fortran-M [Foster:95a].
Reference: [Goil:94a] <author> Goil, S. </author> <title> "Primitives for problems using hierarchical algorithms on distributed memory machines." </title> <type> Technical Report SCCS-687, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> December </month> <year> 1994. </year> <booktitle> Proceedings of the First International Workshop in Parallel Processing, </booktitle> <address> Bangalore, India. </address>
Reference-contexts: The latter can either use the synchronous O (N 2 particle ) algorithm or the faster, but complex loosely synchronous fast multiple O (N particle ) or O (N particle log N particle ) approaches [Barnes:86a], [Edelsohn:91b], <ref> [Goil:94a] </ref>, [Goil:95a], [Greengard:87b], [Salmon:90a], [Singh:93a], [Sunderam:93a], [Warren:92b], [Warren:93a]. 5.2 Computational Fluid Dynamics and Manufacturing (Ap plications 1, 2, 3, 4, and 32) CFD (Computational Fluid Dynamics) has been a major motivator for much algorithm and software work in HPCC, and indeed extensions of HPF have largely been based on CFD (or
Reference: [Goil:95a] <author> Goil, S., and Ranka, S. </author> <title> "Software support for parallelization of hierarchically structured applications on distributed memory machines." </title> <type> Technical Report SCCS-688, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: The latter can either use the synchronous O (N 2 particle ) algorithm or the faster, but complex loosely synchronous fast multiple O (N particle ) or O (N particle log N particle ) approaches [Barnes:86a], [Edelsohn:91b], [Goil:94a], <ref> [Goil:95a] </ref>, [Greengard:87b], [Salmon:90a], [Singh:93a], [Sunderam:93a], [Warren:92b], [Warren:93a]. 5.2 Computational Fluid Dynamics and Manufacturing (Ap plications 1, 2, 3, 4, and 32) CFD (Computational Fluid Dynamics) has been a major motivator for much algorithm and software work in HPCC, and indeed extensions of HPF have largely been based on CFD (or similar
Reference: [Gottschalk:90b] <author> Gottschalk, T. D. </author> <title> "Concurrent multi-target tracking," </title> <editor> in D. W. Walker and Q. F. Stout, editors, </editor> <booktitle> The Fifth Distributed Memory Computing Conference, </booktitle> <volume> Volume I, </volume> <pages> pages 85-88. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, 1990. Held April 9-12, Charleston, SC. </address> <note> Caltech Report C3P-908. </note>
Reference-contexts: This is formally asynchronous with temporally and spatially irregular interconnections between various modules, such as sensors for control platforms and input/output tasks. However, each module uses a loosely synchronous algorithm, such as the multi-target Kalman filter <ref> [Gottschalk:90b] </ref> or the target-weapon pairing system. Thus, the whole metaproblem consists of a few (~ 10-50) large grain asynchronous objects, each of which is a data parallel synchronous or loosely synchronous algorithm. This type of asynchronous problem can be implemented in a scaling fashion on massively parallel machines.
Reference: [Greengard:87b] <author> Greengard, L., and Rokhlin, V. </author> <title> "A fast algorithm for particle simulations," </title> <journal> Journal of Computational Physics, </journal> <volume> 73 </volume> <pages> 325-348, </pages> <year> 1987. </year> <institution> Yale University Computer Science Research Report YALEU/DCS/RR-459. </institution>
Reference-contexts: The latter can either use the synchronous O (N 2 particle ) algorithm or the faster, but complex loosely synchronous fast multiple O (N particle ) or O (N particle log N particle ) approaches [Barnes:86a], [Edelsohn:91b], [Goil:94a], [Goil:95a], <ref> [Greengard:87b] </ref>, [Salmon:90a], [Singh:93a], [Sunderam:93a], [Warren:92b], [Warren:93a]. 5.2 Computational Fluid Dynamics and Manufacturing (Ap plications 1, 2, 3, 4, and 32) CFD (Computational Fluid Dynamics) has been a major motivator for much algorithm and software work in HPCC, and indeed extensions of HPF have largely been based on CFD (or similar partial
Reference: [Grimshaw:93b] <author> Grimshaw, A. S. </author> <title> "Easy to use object-oriented parallel programming with Mentat," </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 39-51, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Software Paradigms for Each Problem Architectures * Synchronous: High Performance Fortran (HPF) [Foster:95a], [HPFCSep:95a], [Koelbel:94a]; Fortran 77D [Bozkus:93a], [Fox:91e], [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], <ref> [Grimshaw:93b] </ref>, [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN [Chandy:90a]; WebWork [Fox:95a] * Compound Metaproblems: AVS [Mills:92a;92b], [Cheng:93a]; PCN, Linda (or Trellis built on Linda); Web-work; Fortran-M [Foster:95a].
Reference: [Harrington:61a] <author> Harrington, R. F. </author> <title> Time-Harmonic Electromagnetic Fields. </title> <publisher> McGraw Hill Book Company, </publisher> <address> New York, </address> <year> 1961. </year>
Reference: [Harrington:67a] <author> Harrington, R. F. </author> <title> "Matrix methods for field problems," </title> <booktitle> in Proc. IEEE, </booktitle> <volume> volume 55(2), </volume> <pages> pages 136-149, </pages> <month> February </month> <year> 1967. </year>
Reference: [Harrington:68a] <author> Harrington, R. F. </author> <title> Field Computation by Moment Methods. </title> <publisher> The Macmillan Company, </publisher> <address> New York, 1968. </address> <publisher> Reprinted by Krieger Publishing Co., </publisher> <address> Malabar, FT. </address> <year> (1982). </year>
Reference: [Hatcher:91a] <author> Hatcher, P. J., and Quinn, M. J. </author> <title> Data-Parallel Programming on MIMD Computers. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year> <month> 42 </month>
Reference: [Hatcher:91b] <author> Hatcher, P., Lapadula, A., Jones, R., Quinn, M., and Ander--son, R. </author> <title> "A production-quality C* compiler for hypercube multicom-puters," </title> <booktitle> in Third ACM SIGPLAN Symposium on PPOPP, </booktitle> <volume> volume 26, </volume> <pages> pages 73-82, </pages> <month> July </month> <year> 1991. </year>
Reference: [Haupt:95a] <author> Haupt, T. </author> <title> http://www.npac.syr.edu/projects/bbh describes use of High Performance Fortran for solving Einstein's equations for the collision of two black holes. </title>
Reference-contexts: The latter are also seen in fast multipole particle dynamics problems, as well as fully adaptive PDE's [Edelsohn:91b]. Some excellent methods, such as the Berger-Oliger adaptive mesh refinement [Berger:84a] require modest 31 HPF extensions as we have shown in our Grand Challenge work on col-liding black holes <ref> [Haupt:95a] </ref>. However, as Saltz's group has shown in a set of pioneering projects [HPF:94a], many important PDE methods require nontrivial HPF language extensions, as well as sophisticated runtime support, such as the PARTI [Saltz:91b] and CHAOS systems [Edjali:95a], [Hwang:94a], [Ponnusamy:93c;94b].
Reference: [Hawick:95a] <author> Hawick, K., Dincer, K., Robinson, G., and Fox, G. </author> <title> "Conjugate gradient algorithms in Fortran 90 and high performance Fortran." </title> <type> Technical Report SCCS-691, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> February </month> <year> 1995. </year>
Reference: [Hawick:95b] <author> Hawick, K., and Fox, G. </author> <title> Exploiting High Performance Fortran for Computational Fluid Dynamics, </title> <booktitle> volume 919 of Lecture Notes in Computer Science, </booktitle> <pages> pages 413-419. </pages> <publisher> Springer-Verlag, </publisher> <month> May </month> <year> 1995. </year> <booktitle> International Conference on High Performance Computing and Networking, HPCN Europe 1995, </booktitle> <institution> Milan; Syracuse University Technical Report SCCS-661. </institution>
Reference: [Hawick:95c] <author> Hawick, K., Bogucz, E. A., Degani, A. T., Fox, G. C., and Robinson, G. </author> <title> "Computational fluid dynamics algorithms in high performance Fortran," </title> <booktitle> in Proc. AIAA 26th Computational Fluid Dynamics Conference, </booktitle> <month> June </month> <year> 1995. </year>
Reference: [Hillis:87a] <author> Hillis, W. D. </author> <title> "The Connection Machine," </title> <journal> Scientific American, </journal> <volume> 256 </volume> <pages> 108-115, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: For embarrassingly parallel problems, illustrated in Figure 4, the synchronization (both software and hardware) issues are greatly simplified. Synchronous problems are data parallel in the language of Hillis <ref> [Hillis:87a] </ref> with the restriction that the time dependence of each data point is governed by the same algorithm.
Reference: [Hiranandani:92c] <author> Hiranandani, S., Kennedy, K., and Tseng, C. </author> <title> "Compiling Fortran D for MIMD distributed-memory machines," </title> <journal> Comm. ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <year> 1992. </year>
Reference-contexts: Software should not be designed 24 for a particular machine model|it expresses problem and not machine char- acteristics. Table 6: Candidate Software Paradigms for Each Problem Architectures * Synchronous: High Performance Fortran (HPF) [Foster:95a], [HPFCSep:95a], [Koelbel:94a]; Fortran 77D [Bozkus:93a], [Fox:91e], <ref> [Hiranandani:92c] </ref>; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a];
Reference: [HPF:93a] <author> High Performance Fortran Forum. </author> <title> "High performance Fortran language specification." </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, Texas, </institution> <year> 1993. </year>
Reference-contexts: HPCC Adaptive Software for Irregular Loosely Synchronous Problems handled by pC++, HPF extensions, Message Passing (Table 6) Asyncsoft Parallel Software System for (particular) class of asyn chronous problems (Table 6) CFD Computational Fluid Dynamics ED Event Driven Simulation FD Finite Difference Method FEM Finite Element Method HPF High Performance Fortran <ref> [HPF:93a] </ref>, [HPFF:95a] HPF+ Natural Extensions of HPF [Choudhary:92d], [HPF:94a], [HPFapp:95a] Integration Software to integrate components of metaproblems (Ta ble 6) MPF Fortran plus message passing for loosely synchronous pro gramming support PDE Partial Differential Equation TS Time Stepped Simulation VR Virtual Reality Note on Language: HPF, MPF use Fortran for illustration, <p> Note that Figure 9 refers to "HPF+"|this is some extension, called officially HPF2 (and later 3 perhaps) of HPF <ref> [HPF:93a] </ref>, [HPFF:95a] to fill gaps in the original language.
Reference: [HPF:94a] <institution> HPF-2 Scope of Activities and Motivating Applications. </institution> <month> Novem-ber </month> <year> 1994. </year> <title> ftp://hpsl.cs.umd.edu/pub/hpfbench/index.html. [HPFapp:95a] http//www.npac.syr.edu/hpfa/algorithms.html. A collection of applications designed to test HPF, which is online at NPAC. </title> <note> 43 [HPFCSep:95a] "Fortran 90 and computational science." Online Computa--tional Science Educational Project; http://csep1.phys.ornl/csep.html. </note>
Reference-contexts: handled by pC++, HPF extensions, Message Passing (Table 6) Asyncsoft Parallel Software System for (particular) class of asyn chronous problems (Table 6) CFD Computational Fluid Dynamics ED Event Driven Simulation FD Finite Difference Method FEM Finite Element Method HPF High Performance Fortran [HPF:93a], [HPFF:95a] HPF+ Natural Extensions of HPF [Choudhary:92d], <ref> [HPF:94a] </ref>, [HPFapp:95a] Integration Software to integrate components of metaproblems (Ta ble 6) MPF Fortran plus message passing for loosely synchronous pro gramming support PDE Partial Differential Equation TS Time Stepped Simulation VR Virtual Reality Note on Language: HPF, MPF use Fortran for illustration, one can use parallel C, C++ or any <p> Table 6: Candidate Software Paradigms for Each Problem Architectures * Synchronous: High Performance Fortran (HPF) [Foster:95a], [HPFCSep:95a], [Koelbel:94a]; Fortran 77D [Bozkus:93a], [Fox:91e], [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], <ref> [HPF:94a] </ref>; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN [Chandy:90a]; WebWork [Fox:95a] * Compound Metaproblems: AVS [Mills:92a;92b], [Cheng:93a]; PCN, Linda (or Trellis built on Linda); Web-work; Fortran-M [Foster:95a]. <p> We, and others, have discussed this at length, both in general [Choudhary:92d;92e], [Fox:90p], [Goil:94a;95a], , and in case of High Performance Fortran [Bogucz:94a], [Chapman:94b], [Cheng:94e], [Choudhary:92g;94c], [Fox:94g], [Hawick:95a;95c], <ref> [HPF:94a] </ref>, [HPFapp:95a], [Joubert:95a], [Muller:95a], [Robinson:95a], [Sturler:95a]. Note that Figure 9 refers to "HPF+"|this is some extension, called officially HPF2 (and later 3 perhaps) of HPF [HPF:93a], [HPFF:95a] to fill gaps in the original language. <p> 1, 2, 3, 4, and 32) CFD (Computational Fluid Dynamics) has been a major motivator for much algorithm and software work in HPCC, and indeed extensions of HPF have largely been based on CFD (or similar partial differential equation based applications) and molecular dynamics [Bogucz:94a], [Choudhary:92d;94c], [Dincer:95b], [Goil:94a;95a], [Hawick:95a;95b], , <ref> [HPF:94a] </ref>. Partial differential equations can be quite straightforward on parallel machines if one uses regular grids, such as those coming from the simplest finite difference equations. However, modern numerical methods use either finite elements or a refinement strategy for finite elements, which gives rise to irregular meshes. <p> Some excellent methods, such as the Berger-Oliger adaptive mesh refinement [Berger:84a] require modest 31 HPF extensions as we have shown in our Grand Challenge work on col-liding black holes [Haupt:95a]. However, as Saltz's group has shown in a set of pioneering projects <ref> [HPF:94a] </ref>, many important PDE methods require nontrivial HPF language extensions, as well as sophisticated runtime support, such as the PARTI [Saltz:91b] and CHAOS systems [Edjali:95a], [Hwang:94a], [Ponnusamy:93c;94b].
Reference: [HPFF:95a] <institution> High Performance Fortran Forum. </institution> <note> http://www.erc.msstate.edu/hpff/home.html. </note>
Reference-contexts: Adaptive Software for Irregular Loosely Synchronous Problems handled by pC++, HPF extensions, Message Passing (Table 6) Asyncsoft Parallel Software System for (particular) class of asyn chronous problems (Table 6) CFD Computational Fluid Dynamics ED Event Driven Simulation FD Finite Difference Method FEM Finite Element Method HPF High Performance Fortran [HPF:93a], <ref> [HPFF:95a] </ref> HPF+ Natural Extensions of HPF [Choudhary:92d], [HPF:94a], [HPFapp:95a] Integration Software to integrate components of metaproblems (Ta ble 6) MPF Fortran plus message passing for loosely synchronous pro gramming support PDE Partial Differential Equation TS Time Stepped Simulation VR Virtual Reality Note on Language: HPF, MPF use Fortran for illustration, one <p> Note that Figure 9 refers to "HPF+"|this is some extension, called officially HPF2 (and later 3 perhaps) of HPF [HPF:93a], <ref> [HPFF:95a] </ref> to fill gaps in the original language.
Reference: [Hwang:94a] <author> Hwang, Y.-S., Moon, B., Sharma, S., Das, R., and Saltz, J. </author> <title> "Runtime support to parallelize adaptive irregular programs," </title> <booktitle> in Proceedings of the Workshop on Environments and Tools for Parallel Scientific Computing, </booktitle> <year> 1994. </year>
Reference-contexts: However, as Saltz's group has shown in a set of pioneering projects [HPF:94a], many important PDE methods require nontrivial HPF language extensions, as well as sophisticated runtime support, such as the PARTI [Saltz:91b] and CHAOS systems [Edjali:95a], <ref> [Hwang:94a] </ref>, [Ponnusamy:93c;94b]. The needed language support can be thought of as expressing the problem architecture (computational graph as in Figure 3 (a), which is only implicitly defined by the standard (Fortran) code. Correctly written, this vanilla Fortran implies all needed information for efficient parallelism.
Reference: [Infourl:95a] <institution> The InfoMall Home Page http://www.infomall.org. </institution>
Reference-contexts: It is not directly relevant to this paper, but the results of this survey caused the ACTION program to refocus its efforts and evolve into InfoMall [Fox:93c], [Fox:94f], [Fox:94h], [Fox:95b], <ref> [Infourl:95a] </ref>, [Mills:94a]. Here, "Info" refers to the information based application focus and "Mall" to the use of a virtual corporation (groups of "storeholders") to produce the complex integrated applications enabled by HPCC. The first column of Table 4 contains the area label and some sample applications.
Reference: [Johnson:86c] <author> Johnson, M. A. </author> <title> "The specification of CrOS III." </title> <type> Technical Report C3P-253, </type> <institution> California Institute of Technology, Pasadena, </institution> <address> CA, </address> <month> February </month> <year> 1986. </year>
Reference-contexts: This complicates the loosely synchronous structure and can make problem architecture look like that of a synchronous event driven simulations|here events are individual Monte Carlo updates. "Detailed balance" requires that such events be sequentially (if arbitrarily) ordered. In the example of <ref> [Johnson:86c] </ref> described in [Fox:94a], a clever implementation gave good parallel performance. Monte Carlo methods can be implemented quite differently|above we decomposed the underlying physical data. One can also use "data parallelism" on the random number set used in the simulation.
Reference: [Jordon:69a] <author> Jordon, E. C., and Balmain. </author> <title> Electromagnetic Waves and Radiating Systems. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1969. </year> <note> Second Edition. </note>
Reference-contexts: Matrix multiplication could be exception as it is insensitive to latency and communication bandwidth for large matrices and so suitable for workstation clusters. One of the standard approaches to computational electromagnetics (CEM) is the method of moments [Harrington:61a;67a;68a], <ref> [Jordon:69a] </ref>.
Reference: [Joubert:95a] <author> Joubert, A. </author> <title> "Financial applications and HPF." </title> <type> Technical report, </type> <institution> The London Parallel Applications Centre, </institution> <address> London, UK, </address> <year> 1995. </year>
Reference-contexts: We, and others, have discussed this at length, both in general [Choudhary:92d;92e], [Fox:90p], [Goil:94a;95a], , and in case of High Performance Fortran [Bogucz:94a], [Chapman:94b], [Cheng:94e], [Choudhary:92g;94c], [Fox:94g], [Hawick:95a;95c], [HPF:94a], [HPFapp:95a], <ref> [Joubert:95a] </ref>, [Muller:95a], [Robinson:95a], [Sturler:95a]. Note that Figure 9 refers to "HPF+"|this is some extension, called officially HPF2 (and later 3 perhaps) of HPF [HPF:93a], [HPFF:95a] to fill gaps in the original language.
Reference: [Kalos:85a] <author> Kalos, M. </author> <title> The Basics of Monte Carlo Methods. </title> <publisher> John Wiley and Sons, </publisher> <year> 1985. </year>
Reference-contexts: Like many such embarrassingly parallel cases, the different instances do need to accumulate their data|in this case, Monte Carlo averages. One important examples of this class of application is Quantum Monte Carlo used in many ab initio chemistry problems <ref> [Kalos:85a] </ref>. Yet, a different set of issues comes with a class of Monte Carlo problems which are termed "clustered." In most physical system Monte Carlos, one updates a single "entity" (grid point or particle) at a time. This is very ineffective when there is substantial correlation between neighboring points.
Reference: [Koelbel:94a] <author> Koelbel, C., Loveman, D., Schreiber, R., Steele, G., and Zosel, M. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Software should not be designed 24 for a particular machine model|it expresses problem and not machine char- acteristics. Table 6: Candidate Software Paradigms for Each Problem Architectures * Synchronous: High Performance Fortran (HPF) [Foster:95a], [HPFCSep:95a], <ref> [Koelbel:94a] </ref>; Fortran 77D [Bozkus:93a], [Fox:91e], [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a];
Reference: [Lemke:92a] <author> Lemke, M., and Quinland, D. </author> <title> "P++, a parallel C++ array class library for architecture-independent development of structured grid applications," </title> <booktitle> in Proc. Workshop on Languages, Compilers, and Runtime Environments for Distributed Memory Computers. ACM, </booktitle> <year> 1992. </year>
Reference-contexts: Paradigms for Each Problem Architectures * Synchronous: High Performance Fortran (HPF) [Foster:95a], [HPFCSep:95a], [Koelbel:94a]; Fortran 77D [Bozkus:93a], [Fox:91e], [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], <ref> [Lemke:92a] </ref>; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN [Chandy:90a]; WebWork [Fox:95a] * Compound Metaproblems: AVS [Mills:92a;92b], [Cheng:93a]; PCN, Linda (or Trellis built on Linda); Web-work; Fortran-M [Foster:95a].
Reference: [McBryan:94a] <author> McBryan, O. </author> <title> "An overview of message passing environments," </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 417-444, </pages> <year> 1994. </year>
Reference-contexts: Performance Fortran (HPF) [Foster:95a], [HPFCSep:95a], [Koelbel:94a]; Fortran 77D [Bozkus:93a], [Fox:91e], [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], <ref> [McBryan:94a] </ref> * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp [Wieland:89a]; PCN [Chandy:90a]; WebWork [Fox:95a] * Compound Metaproblems: AVS [Mills:92a;92b], [Cheng:93a]; PCN, Linda (or Trellis built on Linda); Web-work; Fortran-M [Foster:95a].
Reference: [Meier:89a] <author> Meier, D. L., Cloud, K. C., Horvath, J. C., Allan, L. D., Ham-mond, W. H., and Maxfield, H. A. </author> <title> "A general framework for complex time-driven simulations on hypercubes." </title> <type> Technical Report C3P-761, </type> <institution> 44 California Institute of Technology, Pasadena, </institution> <address> CA, </address> <month> March </month> <year> 1989. </year> <booktitle> Pa--per presented at the Fourth Conference on Hypercubes, Concurrent Computers and Applications. </booktitle>
Reference-contexts: Industrial applications have less synchronous and more loosely synchronous problems than academic problems. We have recently recognized that many complicated problems are mixtures of the basic classifications. The first major example with which I was involved was a battle management simulation implemented by my collaborators at JPL <ref> [Meier:89a] </ref>. This is formally asynchronous with temporally and spatially irregular interconnections between various modules, such as sensors for control platforms and input/output tasks. However, each module uses a loosely synchronous algorithm, such as the multi-target Kalman filter [Gottschalk:90b] or the target-weapon pairing system.
Reference: [Mills:92a] <author> Mills, K., Vinson, M., Cheng, G., and Thomas, F. </author> <title> "A large scale comparison of option pricing models with historical market data," </title> <booktitle> in Proceedings of The 4th Symposium on the Frontiers of Massively Parallel Computing. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1992. </year> <title> Held in McLean, </title> <address> VA. SCCS-260. </address>
Reference: [Mills:92b] <author> Mills, K., Cheng, G., Vinson, M., Ranka, S., and Fox, G. </author> <title> "Software issues and performance of a parallel model for stock option pricing," </title> <booktitle> in Proceedings of the Fifth Australian Supercomputing Conference, </booktitle> <pages> pages 125-134, </pages> <month> December </month> <year> 1992. </year> <title> Held in Melbourne, </title> <address> Australia. SCCS-273b. </address>
Reference: [Mills:93a] <author> Mills, K., and Fox, G. C. </author> <title> "HPCC applications development and technology transfer to industry," </title> <editor> in I. D. Scherson, editor, </editor> <booktitle> The New Frontiers: A Workshop on Future Directions of Massively Parallel Processing, </booktitle> <pages> pages 58-65, </pages> <address> Los Alamitos, CA, October 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The industrial applications come from a survey undertaken of New York State industry in 1991 and 1992. Further details of the survey will be found in [Fox:92e], [Fox:94b], [Fox:94h], [Fox:94c], [Fox:94i], <ref> [Mills:93a] </ref>. Here, we summarize relevant features of it in Section 2.
Reference: [Mills:94a] <author> Mills, K., and Fox, G. "InfoMall: </author> <title> an innovative strategy for high-performance computing and communications applications development," </title> <journal> Internet Research, </journal> <volume> 4 </volume> <pages> 31-45, </pages> <year> 1994. </year>
Reference-contexts: It is not directly relevant to this paper, but the results of this survey caused the ACTION program to refocus its efforts and evolve into InfoMall [Fox:93c], [Fox:94f], [Fox:94h], [Fox:95b], [Infourl:95a], <ref> [Mills:94a] </ref>. Here, "Info" refers to the information based application focus and "Mall" to the use of a virtual corporation (groups of "storeholders") to produce the complex integrated applications enabled by HPCC. The first column of Table 4 contains the area label and some sample applications.
Reference: [Mills:95a] <author> Mills, K., Fox, G., Coddington, P., Mihalas, B., Podgorny, M., Shelly, B., and Bossert, S., </author> <title> "The living textbook and the K-12 classroom of the future." </title> <address> http://www.npac.syr.edu/projects/ltb/SC95/index.html. </address>
Reference-contexts: Electronic Cash, etc. * 31: Electronic Shopping * 32: Agile Manufacturing|Multidisciplinary Design and Concurrent Engineering * Combines CAD with Applications 1 to 3 * Requires major changes to Manufacturing Infrastructure and Approach * 33: Education * InfoMall Living Textbook|6 Schools on ATM network linked to HPCC InfoVision Servers at NPAC <ref> [Mills:95a] </ref> Table 1 describes the general guidelines used in organizing Table 4. Note that we did not directly cover academic areas, and a more complete list (which included our industrial table) was produced by the Petaflops meeting [Peta:94a].
Reference: [MOPAC:95a] <institution> See electronic description of NPAC's activities in this area at http://www.npac.syr.edu/projects/mopac/mopac.html. </institution>
Reference-contexts: In this way, we see the breakup of metaproblems into components, and use of systems such as AVS as helpful software engineering strategies [Cheng:92a;94d]. We have successfully used such an approach to produce an effective parallel version of the public domain molecular orbital chemistry code MOPAC <ref> [MOPAC:95a] </ref>. Not all chemistry computations have this structure. For instance, there is a set of applications such as AMBER and CHARMM that are based on molecular dynamics simulations, as described in Chapter 16 of [Fox:94a], [Ranka:92a].
Reference: [Muller:95a] <author> Muller, A., and Ruhl, R. </author> <title> "Extending high performance Fortran for the support of unstructured computations," </title> <booktitle> in International Conference on Supercomputing, </booktitle> <address> July 1995. Barcelona, Spain. </address>
Reference-contexts: We, and others, have discussed this at length, both in general [Choudhary:92d;92e], [Fox:90p], [Goil:94a;95a], , and in case of High Performance Fortran [Bogucz:94a], [Chapman:94b], [Cheng:94e], [Choudhary:92g;94c], [Fox:94g], [Hawick:95a;95c], [HPF:94a], [HPFapp:95a], [Joubert:95a], <ref> [Muller:95a] </ref>, [Robinson:95a], [Sturler:95a]. Note that Figure 9 refers to "HPF+"|this is some extension, called officially HPF2 (and later 3 perhaps) of HPF [HPF:93a], [HPFF:95a] to fill gaps in the original language.
Reference: [Nicplocha:94a] <author> Nicplocha, J., Harrison, R. J., and Littlefield, R. J. </author> <title> "Global Arrays: a portable `shared-memory' programming model for distributed memory computers," </title> <booktitle> in Supercomputing '94, </booktitle> <year> 1994. </year> <institution> Pacific Northwest Laboratory, </institution> <note> http://www.emsl.pnl.gov.2080. 45 </note>
Reference-contexts: one or more of a set of matrix operations Matrix Multiplication as in change of basis Matrix Eigenvalue determination as in energy level computations Matrix Equation solution as in solving multichannel scattering problems This structure has been elegantly exploited within the "Global Array" programming model built at Pacific Northwest Laboratory <ref> [Nicplocha:94a] </ref> with a set of tools (libraries) designed for this class of computational chemistry problem. These two steps have very different characteristics.
Reference: [Parasoft:88a] <author> ParaSoft. </author> <title> EXPRESS: A Communication Environment for Parallel Computers. ParaSoft, </title> <publisher> Inc., </publisher> <address> Pasadena, CA, </address> <year> 1988. </year> <note> [Peta:94a] http://www.npac.syr.edu/roadmap/petaapps.html is HTML version of application table. The full published proceedings is T. </note> <author> Sterling, P. Messina, and P. H. Smith, </author> <title> Enabling Technologies for Petaflops Computing, </title> <publisher> MIT press, </publisher> <year> 1995. </year>
Reference-contexts: Generally, extensions of ADA, Fortran, C, or C++ controlling modules written in synchro nous or loosely synchronous approach * Embarrassingly Parallel: Several approaches work? * PCN, Linda, WebWork, PVM [Sunderam:90a], Network Express <ref> [Parasoft:88a] </ref>, ISIS [Birman:87a;87b;91a] We have described those issues at length in [Fox:90p;91g;94a], and here we will just present a simple table (Table 6) mapping the five problem architectures into possible software environments.
Reference: [Ponnusamy:93c] <author> Ponnusamy, R., Saltz, J., Choudhary, A., Hwang, Y.- S., and Fox, G. </author> <title> "Runtime support and compilation methods for user-specified data distributions." </title> <institution> Technical Report CS-TR-3194 and UMIACS-TR-93-135, University of Maryland, Department of Computer Science, </institution> <year> 1993. </year> <note> To appear in IEEE Transaction on Parallel and Distributed Memory Systems. </note>
Reference: [Ponnusamy:94b] <author> Ponnusamy, R., Hwang, Y.-S., Saltz, J., Choudhary, A., and Fox, G. </author> <title> "Supporting irregular distributions in FORTRAN 90D/HPF compilers." </title> <institution> Technical Report CR-TR-3268 and UMIACS-TR-94-57, University of Maryland, Department of Computer Science, </institution> <year> 1994. </year> <note> Also available in IEEE Parallel and Distributed Technology, Spring 1995. </note>
Reference: [Ranka:92a] <author> Ranka, S., Fox, G. C., Saltz, J., and Das, R. </author> <title> "Parallelization of CHARMM molecular dynamics code on multicomputers." </title> <type> Technical Report SCCS-236, </type> <institution> Syracuse University, NPAC, Syracuse, </institution> <address> NY, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: Not all chemistry computations have this structure. For instance, there is a set of applications such as AMBER and CHARMM that are based on molecular dynamics simulations, as described in Chapter 16 of [Fox:94a], <ref> [Ranka:92a] </ref>. These are typically loosely synchronous problems with each particle linked to a dynamic set of "nearest neighbors" combined with long-range nonbonded force computations.
Reference: [Robinson:95a] <author> Robinson, G., Hawick, K. A., and Fox, G. C. </author> <title> "Fortran 90 and high performance Fortran for dense matrix-formulated applications." </title> <type> Technical Report SCCS-709, </type> <institution> Syracuse University, NPAC, </institution> <address> Syra-cuse, NY, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: We, and others, have discussed this at length, both in general [Choudhary:92d;92e], [Fox:90p], [Goil:94a;95a], , and in case of High Performance Fortran [Bogucz:94a], [Chapman:94b], [Cheng:94e], [Choudhary:92g;94c], [Fox:94g], [Hawick:95a;95c], [HPF:94a], [HPFapp:95a], [Joubert:95a], [Muller:95a], <ref> [Robinson:95a] </ref>, [Sturler:95a]. Note that Figure 9 refers to "HPF+"|this is some extension, called officially HPF2 (and later 3 perhaps) of HPF [HPF:93a], [HPFF:95a] to fill gaps in the original language. <p> We note in passing that such spatial discretization is a quite viable approach to CEM and leads to a totally different computational problem architecture from the spectral moment formulation. HPF can handle both stages of the matrix based CEM or chemistry problems <ref> [Robinson:95a] </ref>. The matrix solution stage exploits fully the Fortran 90 array manipulation and clearly requires good compiler support for matrix and vector manipulation primitives. NPAC's experience with a production CEM code PARAMOM from the Syracuse Research Corporation is illuminating [Cheng:94c].
Reference: [Salmon:90a] <author> Salmon, J. </author> <title> Parallel Hierarchical N-Body Methods. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, </institution> <month> December </month> <year> 1990. </year> <note> SCCS-52, CRPC-TR90115. Caltech Report C3P-966. </note>
Reference-contexts: The latter can either use the synchronous O (N 2 particle ) algorithm or the faster, but complex loosely synchronous fast multiple O (N particle ) or O (N particle log N particle ) approaches [Barnes:86a], [Edelsohn:91b], [Goil:94a], [Goil:95a], [Greengard:87b], <ref> [Salmon:90a] </ref>, [Singh:93a], [Sunderam:93a], [Warren:92b], [Warren:93a]. 5.2 Computational Fluid Dynamics and Manufacturing (Ap plications 1, 2, 3, 4, and 32) CFD (Computational Fluid Dynamics) has been a major motivator for much algorithm and software work in HPCC, and indeed extensions of HPF have largely been based on CFD (or similar partial differential
Reference: [Saltz:91b] <author> Saltz, J., Berryman, H., and Wu, J. </author> <title> "Multiprocessor and runtime compilation," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3(6) </volume> <pages> 573-592, </pages> <month> December </month> <year> 1991. </year> <title> Special Issue: Practical Parallel Computing: Status and Prospects. </title> <editor> Guest Editors: Paul Messina and Almerico Murli. </editor> <volume> 46 </volume>
Reference-contexts: However, as Saltz's group has shown in a set of pioneering projects [HPF:94a], many important PDE methods require nontrivial HPF language extensions, as well as sophisticated runtime support, such as the PARTI <ref> [Saltz:91b] </ref> and CHAOS systems [Edjali:95a], [Hwang:94a], [Ponnusamy:93c;94b]. The needed language support can be thought of as expressing the problem architecture (computational graph as in Figure 3 (a), which is only implicitly defined by the standard (Fortran) code. Correctly written, this vanilla Fortran implies all needed information for efficient parallelism.
Reference: [Singh:93a] <author> Singh, J. P. </author> <title> Parallel Hierarchical N-body Methods and Their Im--plications for Multiprocessors. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1993. </year>
Reference-contexts: The latter can either use the synchronous O (N 2 particle ) algorithm or the faster, but complex loosely synchronous fast multiple O (N particle ) or O (N particle log N particle ) approaches [Barnes:86a], [Edelsohn:91b], [Goil:94a], [Goil:95a], [Greengard:87b], [Salmon:90a], <ref> [Singh:93a] </ref>, [Sunderam:93a], [Warren:92b], [Warren:93a]. 5.2 Computational Fluid Dynamics and Manufacturing (Ap plications 1, 2, 3, 4, and 32) CFD (Computational Fluid Dynamics) has been a major motivator for much algorithm and software work in HPCC, and indeed extensions of HPF have largely been based on CFD (or similar partial differential equation
Reference: [Sturler:95a] <author> De Sturler, E., and Strumpen, V. </author> <title> "First experiences with high performance Fortran on the Intel Paragon." </title> <type> Technical Report 95-10, </type> <institution> Interdisciplinary Project Center for Supercomputing, Swiss Federal Institute of Technology Zurich, </institution> <year> 1995. </year>
Reference-contexts: We, and others, have discussed this at length, both in general [Choudhary:92d;92e], [Fox:90p], [Goil:94a;95a], , and in case of High Performance Fortran [Bogucz:94a], [Chapman:94b], [Cheng:94e], [Choudhary:92g;94c], [Fox:94g], [Hawick:95a;95c], [HPF:94a], [HPFapp:95a], [Joubert:95a], [Muller:95a], [Robinson:95a], <ref> [Sturler:95a] </ref>. Note that Figure 9 refers to "HPF+"|this is some extension, called officially HPF2 (and later 3 perhaps) of HPF [HPF:93a], [HPFF:95a] to fill gaps in the original language.
Reference: [Sunderam:90a] <author> Sunderam, V. S. </author> <title> "PVM: a framework for parallel distributed computing," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4) </volume> <pages> 315-340, </pages> <year> 1990. </year>
Reference-contexts: Generally, extensions of ADA, Fortran, C, or C++ controlling modules written in synchro nous or loosely synchronous approach * Embarrassingly Parallel: Several approaches work? * PCN, Linda, WebWork, PVM <ref> [Sunderam:90a] </ref>, Network Express [Parasoft:88a], ISIS [Birman:87a;87b;91a] We have described those issues at length in [Fox:90p;91g;94a], and here we will just present a simple table (Table 6) mapping the five problem architectures into possible software environments.
Reference: [Sunderam:93a] <author> Sunderam, S. </author> <title> Fast Algorithms for N-body Simulations. </title> <type> PhD thesis, </type> <institution> Cornell University, </institution> <year> 1993. </year>
Reference-contexts: The latter can either use the synchronous O (N 2 particle ) algorithm or the faster, but complex loosely synchronous fast multiple O (N particle ) or O (N particle log N particle ) approaches [Barnes:86a], [Edelsohn:91b], [Goil:94a], [Goil:95a], [Greengard:87b], [Salmon:90a], [Singh:93a], <ref> [Sunderam:93a] </ref>, [Warren:92b], [Warren:93a]. 5.2 Computational Fluid Dynamics and Manufacturing (Ap plications 1, 2, 3, 4, and 32) CFD (Computational Fluid Dynamics) has been a major motivator for much algorithm and software work in HPCC, and indeed extensions of HPF have largely been based on CFD (or similar partial differential equation based
Reference: [Warren:92b] <author> Warren, M. S., and Salmon, J. K. </author> <title> "Astrophysical N-Body simulations using hierarchical tree data structures," </title> <booktitle> in Supercomputing '92. IEEE Comp. </booktitle> <publisher> Soc., Los Alamitos, </publisher> <address> CA, </address> <year> 1992. </year>
Reference-contexts: The latter can either use the synchronous O (N 2 particle ) algorithm or the faster, but complex loosely synchronous fast multiple O (N particle ) or O (N particle log N particle ) approaches [Barnes:86a], [Edelsohn:91b], [Goil:94a], [Goil:95a], [Greengard:87b], [Salmon:90a], [Singh:93a], [Sunderam:93a], <ref> [Warren:92b] </ref>, [Warren:93a]. 5.2 Computational Fluid Dynamics and Manufacturing (Ap plications 1, 2, 3, 4, and 32) CFD (Computational Fluid Dynamics) has been a major motivator for much algorithm and software work in HPCC, and indeed extensions of HPF have largely been based on CFD (or similar partial differential equation based applications)
Reference: [Warren:93a] <author> Warren, M. S., and Salmon, J. K. </author> <title> "A parallel hashed oct-tree N-Body algorithm," </title> <booktitle> in Supercomputing `93. IEEE Comp. </booktitle> <publisher> Soc., Los Alamitos, </publisher> <address> CA, </address> <year> 1993. </year>
Reference-contexts: The latter can either use the synchronous O (N 2 particle ) algorithm or the faster, but complex loosely synchronous fast multiple O (N particle ) or O (N particle log N particle ) approaches [Barnes:86a], [Edelsohn:91b], [Goil:94a], [Goil:95a], [Greengard:87b], [Salmon:90a], [Singh:93a], [Sunderam:93a], [Warren:92b], <ref> [Warren:93a] </ref>. 5.2 Computational Fluid Dynamics and Manufacturing (Ap plications 1, 2, 3, 4, and 32) CFD (Computational Fluid Dynamics) has been a major motivator for much algorithm and software work in HPCC, and indeed extensions of HPF have largely been based on CFD (or similar partial differential equation based applications) and
Reference: [Wieland:89a] <author> Wieland, F., Hawley, L., Feinberg, A., DiLoreto, M., Blume, L., Ru*es, J., Reiher, P., Beckman, B., Hontalas, P., Bellenot, S., and Jefferson, D. </author> <title> "The performance of a distributed combat simulation with the time warp operating system," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 1(1) </volume> <pages> 35-50, </pages> <year> 1989. </year> <note> Caltech Report C3P-798. 47 </note>
Reference-contexts: Other examples include computer chess [Felten:88i] and transaction analysis. Asynchronous problems are hard to parallelize and some may not run well on massively parallel machines. They require sophisticated software and hardware support to properly synchronize the nodes of the parallel machine, as is illustrated by time warp mechanism <ref> [Wieland:89a] </ref>. Both synchronous and loosely synchronous problems parallelize on systems with many nodes. The algorithm naturally synchronizes the parallel components of the problem without any of the complex software or hardware synchronization mentioned above for event driven simulations. <p> [Hiranandani:92c]; Vi-enna Fortran [Chapman:92b]; C* [Hatcher:91a;91b]; Crystal [Chen:88b]; APL; Fortran for SIMD parallel computers * Loosely Synchronous: Extensions of the above, especially HPF [Chapman:94b], [Choudhary:92d], [HPF:94a]; and parallel C++ [Bodin:91a], [Chandy:93a], [Grimshaw:93b], [Lemke:92a]; Fortran or C plus message passing [Fox:91m], [McBryan:94a] * Asynchronous: Linda [Factor:90a;90b], [Gelertner:89a]; CC++ [Chandy:93a]; Time Warp <ref> [Wieland:89a] </ref>; PCN [Chandy:90a]; WebWork [Fox:95a] * Compound Metaproblems: AVS [Mills:92a;92b], [Cheng:93a]; PCN, Linda (or Trellis built on Linda); Web-work; Fortran-M [Foster:95a].
References-found: 109

