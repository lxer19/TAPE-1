URL: ftp://ftp.cs.washington.edu/tr/1995/10/UW-CSE-95-10-02.PS.gz
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: Improving Performance of Bus-Based Multiprocessors  
Author: by Craig S. Anderson 
Degree: A dissertation submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy  Approved by (Chairperson of Supervisory Committee)  
Note: Program Authorized to Offer Degree Date  
Date: 1995  
Affiliation: University of Washington  
Abstract-found: 0
Intro-found: 1
Reference: [AB86] <author> James Archibald and Jean-Loup Baer. </author> <title> Cache coherence protocols: Evaluation using a multiprocessor simulation model. </title> <journal> ACM TOCS, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: The protocol is snoopy-based <ref> [AB86] </ref>. It incorporates features from the Illinois protocol [PP84] and from protocols or write policies with subblock (in)validations [CD93, Jou93]. The basic philosophy behind the protocol is as follows. As much as possible, we favor cache to cache transfers.
Reference: [AB94] <author> Craig Anderson and Jean-Loup Baer. </author> <title> Design and evaluation of a subblock cache coherence protocol for bus-based multiprocessors. </title> <type> Technical Report 94-05-02, </type> <institution> University of Washington, </institution> <year> 1994. </year>
Reference-contexts: Our experiments simulated both usual and sector caches. In all experiments the subblock size b was 8 bytes and caches were two-way set-associative (k = 2). In order to minimize simulation time, we only simulated relatively large caches (C = 128K). Previous work <ref> [AB94] </ref> indicates that using a smaller cache size does not significantly change the results. The block size was either 8 or 64 bytes. For the usual caches we used the Illinois protocol.
Reference: [AB95] <author> Craig Anderson and Jean-Loup Baer. </author> <title> Two techniques for improving the performance of bus-based multiprocessors. </title> <booktitle> In International Symposium on High-Performance Computer Architecture, </booktitle> <pages> pages 264-275, </pages> <year> 1995. </year>
Reference-contexts: These results are not surprising, given the poor behavior of Pverify when using large block sizes <ref> [AB95] </ref>. While false sharing hurts performance under both WI and WU, it much worse under WI, since the entire cache block is "ping-ponging" between the false sharing caches, rather than just the values actually being written. The adaptive protocols performed quite well, nearly matching WU's performance.
Reference: [ABC + 95] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk Johnson, and David Krantz et. al. </author> <title> The MIT Alewife machine: Architecture and performance. </title> <booktitle> In Proc. of 22nd Int. Symp. on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <year> 1995. </year>
Reference: [And90] <author> Thomas Anderson. </author> <title> The performance of spin lock alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <year> 1990. </year>
Reference-contexts: One explanation for this phenomenon is that increasing the bus cache size increased contention for the Test and Test and Set lock guarding the barrier code. It is well known that TTS locks perform badly when many processors contend for the lock <ref> [And90] </ref>. The problem is especially bad when the critical section is short, as it is in the barrier implementation we used. Therefore, it is quite likely that bad synchronization behavior caused the slowdown in the 4X8 configuration.
Reference: [Arc88] <author> James Archibald. </author> <title> A cache coherence approach for large multiprocessor systems. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 337-345, </pages> <year> 1988. </year>
Reference-contexts: Unfortunately, this requires some hardware mechanism to determine when writes originate from different processors. Archibald introduced an improved protocol in which a block is not automatically invalidated when its count reaches 0 <ref> [Arc88] </ref>. Instead, a cache simply doesn't raise the shared line if the relevant block's count is 0. If some cache whose block count is not 0 indicates that it is keeping the block by raising the shared line, then no cache invalidates its block.
Reference: [BAD89] <author> Eugene D. Brooks III, Tim S. Axelrod, and Gregory A. Darmohray. </author> <title> The Cerberus multiprocessor simulator. </title> <editor> In G. Rodrigue, editor, </editor> <booktitle> Parallel Processing for Scientific Computing, </booktitle> <pages> pages 384-390. </pages> <publisher> SIAM, </publisher> <year> 1989. </year> <month> 214 </month>
Reference: [BBW92] <author> Jonathan Bertoni, Jean-Loup Baer, and Wen-Hann Wang. </author> <title> Scaling shared-bus multiprocessors with multiple buses and shared caches: a performance study. </title> <journal> Microprocessors and Microsystems, </journal> <volume> 16(7) </volume> <pages> 339-50, </pages> <year> 1992. </year>
Reference-contexts: Since it is unlikely that non-cluster busses will be shorter than cluster busses, clocking the non-cluster busses faster than the cluster busses will probably not be an option. Instead, wider busses (to improve bandwidth) or multiple (possibly interleaved) busses <ref> [BBW92] </ref> would allow more clusters (hence more processors) to be effectively connected in a hierarchical system. The hierarchical subblock protocol performed relatively well on the six benchmarks.
Reference: [BDCW91] <author> Eric Brewer, Chrysanthos Dellarocas, Adrian Colbrook, and William Weihl. PROTEUS: </author> <title> A high performance parallel-architecture simulator. </title> <type> Technical Report MIT/LCS/TR-516, </type> <institution> MIT, </institution> <month> September </month> <year> 1991. </year>
Reference: [BDNS93] <author> Mats Brorsson, Fredrik Dahlgren, Hakan Nilsson, and Per Stenstrom. </author> <title> The CacheMire test bench | a flexible and effective approache for simulation of multiprocessors. </title> <booktitle> In Proceedings of the 26th Annual Simulation Symposium, </booktitle> <pages> pages 41-49, </pages> <year> 1993. </year>
Reference: [Bed94] <author> Robert Bedichek. </author> <title> The Meerkat multicomputer: Tradeoffs in multicom-puter architecture. </title> <type> Technical Report 94-06-06, </type> <institution> University of Washington, </institution> <year> 1994. </year>
Reference-contexts: By their nature, busses provide low-cost broadcasting capabilities. The major drawback of busses is limited bandwidth | busses normally allow only one module to transmit at a time, and bus length limits how fast the bus can cycle. Chapter 2 of Robert Bedichek's thesis <ref> [Bed94] </ref> has a good discussion of the trade-offs in designing high-performance busses. Some high performance busses used in multiprocessors include the XDBus [SFG + 93] (used in the SPARCcenter 2000) and the POWERpath-2 (used in the SGI Challenge).
Reference: [Bed95] <author> Robert Bedichek. Talisman: </author> <title> Fast and accurate multicomputer simulation. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 14-24, </pages> <year> 1995. </year>
Reference: [BLRC94] <author> Brian Bershad, Dennis Lee, Theodore Romer, and J. Bradley Chen. </author> <title> Avoiding conflict misses dynamically in large direct-mapped caches. </title> <booktitle> In Proc. of ASPLOS VI, </booktitle> <pages> pages 158-170, </pages> <year> 1994. </year>
Reference: [BW88] <author> Jean-Loup Baer and Wen-Han Wang. </author> <title> On the inclusion properties for multi-level cache hierarchies. </title> <booktitle> In Proc. of 15th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 73-80, </pages> <year> 1988. </year>
Reference-contexts: The primary reason for doing this is to limit unnecessary propagation of bus traffic from upper (i.e. towards the root of the tree) levels of the hierarchy to lower (cluster) levels <ref> [BW88] </ref>. Because all caches obey inclusion, an upper level cache will not forward requests on its upper bus to its lower bus if the block in question is not present in its cache, because it is known not to be present in the caches below it. <p> A request must be sent to the cluster below, where the subblock might be present. Dirty Owned The subblock is valid, and the bus cache has an up-to-date copy. The subblock is not valid anywhere else in the system. A bus caches always maintains the inclusion property <ref> [BW88] </ref> with regard to the caches beneath it in the hierarchy. Table 6.1 shows the allowable cluster subblock states, given the bus subblock state. Lower level protocol The lower level protocol is very similar to that of the single bus subblock protocol as detailed in Chapter 3.
Reference: [BW92] <author> Jean-Loup Baer and Wen-Han Wang. </author> <title> Architectural choices for multilevel hierarchies. </title> <type> Technical Report 87-01-04, </type> <institution> University of Washington, </institution> <year> 1992. </year>
Reference-contexts: Another design feature is that of a shared, or cluster cache. Such a cache can serve two purposes. First, if the cluster cache maintains inclusion <ref> [BW92] </ref> with regard to the caches below it, then the cluster cache can filter requests from the upper level (s) of the system to the lower (cluster level), since the cluster cache contains directory information for all blocks cached below it.
Reference: [CD93] <author> Yung-Syau Chen and Michel Dubois. </author> <title> Cache protocols with partial block invalidations. </title> <booktitle> In 7th International Parallel Processing Symposium, </booktitle> <pages> pages 16-24, </pages> <year> 1993. </year> <month> 215 </month>
Reference-contexts: The protocol is snoopy-based [AB86]. It incorporates features from the Illinois protocol [PP84] and from protocols or write policies with subblock (in)validations <ref> [CD93, Jou93] </ref>. The basic philosophy behind the protocol is as follows. As much as possible, we favor cache to cache transfers. On read misses, we transfer as many valid subblocks 25 in the block as possible. <p> However, at some point increased traffic from false sharing misses will cause an overall increase in memory latency and bus traffic. Other authors have proposed directory based schemes in which the unit of coher ence is smaller than an address block. Chen and Dubois <ref> [CD93] </ref> describe an extension to a full-map directory protocol in which a valid bit is associated with each invalidation block. They show a substantial decrease in both miss rates and memory traffic when partial invalidations are used.
Reference: [CF78] <author> L. M. Censier and P. Feautrier. </author> <title> A new solution to coherence problems in multicache systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27:1112-1118, </volume> <month> December </month> <year> 1978. </year>
Reference-contexts: When a write is done to a block, the directory is consulted, and update or invalidate messages are sent to the appropriate caches. The simplest directory scheme is the full-map directory <ref> [CF78] </ref>, in which each directory entry consists of one bit for each element which might cache the block, plus one additional bit to indicate if the block has been modified with respect to memory. The DASH system uses a full-map directory.
Reference: [CF93] <author> Alan Cox and Robert Fowler. </author> <title> Adaptive cache coherency for detecting migratory shared data. </title> <booktitle> In Proc. of 20th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 98-108, </pages> <year> 1993. </year>
Reference-contexts: It is well known that depending on the application, either write-invalidate (WI) or write-update (WU) per forms best [EK88]. Indeed, even within applications, various forms of sharing behavior exist: for example, widely write-shared data (like locks), largely read-only data, and migratory data <ref> [CF93, SBS93] </ref>. Because of these different sharing patterns, varying the protocol as the application executes has the potential to increase application performance by decreasing both the number of bus transactions and the number of bytes transferred over the bus, leading to decreased bus contention and reduced latency for memory operations.
Reference: [CK94] <author> Bob Cmelik and David Keppel. Shade: </author> <title> A fast instruction-set simulator for execution profiling. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 128-137, </pages> <year> 1994. </year>
Reference: [CKA91] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS directories: A scalable cache coherence scheme. </title> <booktitle> In Proc. of ASPLOS IV, </booktitle> <pages> pages 224-234, </pages> <year> 1991. </year>
Reference-contexts: The DASH system uses a full-map directory. To save directory bits, limited directory protocols can be used. An example of such a protocol is the LimitLESS protocol <ref> [CKA91] </ref>, which uses a small number of hardware pointers for each memory block. If the number of pointers is insufficient to maintain the block's sharing list, then a full-map directory protocol is emulated in software to keep the block coherent.
Reference: [Con94] <author> Convex Computer Corporation. </author> <title> Convex Exemplar Scalable Parallel Processing System, </title> <year> 1994. </year>
Reference-contexts: Cache Coherence Top Snoopy Snoopy Snoopy Direc- Hybrid SCI None Cache tory Coherence Cluster Yes Dir.y Dir.y Yes Yesz Yes No Cache? Inclusion? Yes Yes Yes No N/A Yes N/A Memory UMA COMA COMA NUMA NUMA NUMA NUMA Organi zation Reference [Wil87] [HLH92] [FIR93] [LLJ + 92] [WTP + 92] <ref> [Con94] </ref> [VSLW91] yDir.: Only directory entries are cached, not data. zEach cluster shares a portion of main memory, which can be used as a cache of other clusters' pages.
Reference: [CYS + 93] <author> Michel Cekleov, David Yen, Pradeep Sindhu, Jean-Marc Frailong, and Jean Gastinel et. al. </author> <booktitle> SPARCcenter 2000: Multiprocessing for the 90's! In COMPCON Spring 1993, </booktitle> <pages> pages 345-353, </pages> <year> 1993. </year>
Reference-contexts: Introduction Because of their simplicity and low cost, single bus shared-memory multiprocessors have enjoyed a great deal of commercial success. Machines like the Sequent Symmetry [LT88], SGI Challenge [GW94], and Sun's SPARCcenter 2000 <ref> [CYS + 93] </ref> all use commodity processors and bus technology to reduce design complexity while still achieving good performance. Such designs leverage the investment made in unipro-cessors to keep costs down. Like uniprocessors, single-bus multiprocessors generally use per-processor (or private) caches in order to reduce memory latency and bus traffic.
Reference: [Dar88] <author> Gregory A. Darmohray. </author> <title> Gaussian techniques on shared-memory multiprocessors. </title> <type> Master's thesis, </type> <institution> University of California, Davis, </institution> <month> April </month> <year> 1988. </year>
Reference: [DDHY92] <author> David L. Dill, Andreas J. Drexler, Alan J. Hu, and C. Han Yang. </author> <title> Protocol verification as a hardware design aid. </title> <booktitle> In Proc. of Int. Conference on Computer Design, </booktitle> <pages> pages 522-525, </pages> <year> 1992. </year>
Reference: [DL92] <author> Czarek Dubnicki and Thomas LeBlanc. </author> <title> Adjustable block size coherent caches. </title> <booktitle> In Proc. of 19th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 170-180, </pages> <year> 1992. </year> <month> 216 </month>
Reference-contexts: Chen and Dubois [CD93] describe an extension to a full-map directory protocol in which a valid bit is associated with each invalidation block. They show a substantial decrease in both miss rates and memory traffic when partial invalidations are used. Dubnicki and LeBlanc <ref> [DL92] </ref> propose adjustable block size caches where the size of an address block dynamically grows or shrinks, in a buddy-system like fashion, in response to various patterns of write sharing. Unlike Chen's protocol, Dubnicki's protocol is not easily adaptable to non-directory based coherence protocols. <p> As our results showed, no single block size works well for all applications. One possible solution, that of varying the block size dynamically <ref> [DL92] </ref>, was not easily adaptable to snoopy-based coherence protocols. Instead, we adapted a static policy of using a large transfer unit (block), while using a small coherence unit (subblock). We found that the subblock protocol which implemented this policy worked well on our benchmark applications.
Reference: [DN87] <author> Srinivas Devadas and A. Richard Newton. </author> <title> Topological optimization of multiple level array logic. </title> <journal> IEEE Transactions on Computer-Aided Design, </journal> <month> November </month> <year> 1987. </year>
Reference: [Egg89] <author> Susan Eggers. </author> <title> Simulation Analysis of Data Sharing in Shared Memory Multiprocessor. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1989. </year>
Reference: [EJ91] <author> Susan Eggers and Tor Jeremiassen. </author> <title> Eliminating false-sharing. </title> <booktitle> In Proc. of Int. Conf. on Parallel Processing, </booktitle> <pages> pages I-377-381, </pages> <year> 1991. </year>
Reference-contexts: Snarfing had very little effect on either the number of bus transactions or bytes transferred for either the 8I or subblock protocols. Pverify Pverify has great deal of false sharing, even for moderate blocks sizes <ref> [EJ91] </ref>. It is not surprising, therefore, that using a large block size substantially degrades performance, as can be seen in Figure 3.12 (L). In contrast, using a small block size leads to near-linear speedup.
Reference: [EK88] <author> Susan Eggers and Randy Katz. </author> <title> A characterization of sharing in parallel programs and its application to coherency protocol evaluation. </title> <booktitle> In Proc. of 15th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 373-382, </pages> <year> 1988. </year>
Reference-contexts: Chapter 4 ADAPTIVE HYBRID CACHE PROTOCOLS 4.1 Introduction Applications exhibit a wide variety of reference patterns. It is well known that depending on the application, either write-invalidate (WI) or write-update (WU) per forms best <ref> [EK88] </ref>. Indeed, even within applications, various forms of sharing behavior exist: for example, widely write-shared data (like locks), largely read-only data, and migratory data [CF93, SBS93]. <p> A key concept in choosing between WI and WU is that of the write run <ref> [EK88] </ref>. Eggers defines a write run as a sequence of write references by one processor to a shared address, uninterrupted by any accesses by other processors. We will use a slightly different definition of a write run in our discussions.
Reference: [EK89] <author> Susan Eggers and Randy Katz. </author> <title> Evaluating the performance of four snooping cache coherence protocols. </title> <booktitle> In Proc. of 16th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 2-15, </pages> <year> 1989. </year>
Reference-contexts: Read snarfing was discussed in previous work by Rudolph [SR84], Goodman [GW88], and Eggers <ref> [EK89] </ref>. Of these, only Eggers evaluated the performance benefits of read snarfing. <p> This results in the writer's block becoming exclusive faster, since all caches sharing the block decrement their counts on each write, rather than just one <ref> [EK89] </ref>. <p> This scheme also has the advantage that a succession of single writes from different processors will not cause other copies to be deleted. It does this without any additional hardware required. Eggers and Katz evaluated the "snoopy reading" scheme against a pure update scheme <ref> [EK89] </ref>. Using a value of 3 for the threshold, they found that while the hybrid scheme reduced the number of updates for all 4 benchmarks used, two of the four programs executed slower than WU while using the adaptive scheme.
Reference: [FIR93] <author> S. Frank, H. Burkhardt III, and J. Rothnie. </author> <title> The KSR-1: bridging the gap between shared memory and MPPs. </title> <booktitle> In Proc. of Spring 1993 COMPCON, </booktitle> <pages> pages 285-294, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Snoopy Snoopy Snoopy Snoopy Snoopy Directory Snoopy Cache Coherence Top Snoopy Snoopy Snoopy Direc- Hybrid SCI None Cache tory Coherence Cluster Yes Dir.y Dir.y Yes Yesz Yes No Cache? Inclusion? Yes Yes Yes No N/A Yes N/A Memory UMA COMA COMA NUMA NUMA NUMA NUMA Organi zation Reference [Wil87] [HLH92] <ref> [FIR93] </ref> [LLJ + 92] [WTP + 92] [Con94] [VSLW91] yDir.: Only directory entries are cached, not data. zEach cluster shares a portion of main memory, which can be used as a cache of other clusters' pages.
Reference: [Fuj90] <author> Richard Fujimoto. </author> <title> Parallel discrete event simulation. </title> <journal> Communications of the ACM, </journal> <volume> 33(10) </volume> <pages> 30-53, </pages> <year> 1990. </year>
Reference: [Goo87] <author> James Goodman. </author> <title> Coherency for multiprocessor virtual caches. </title> <booktitle> In Proc. of 2nd Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 72-81, </pages> <year> 1987. </year>
Reference-contexts: in P2. 3.3.2 Related work Goodman introduced the concept of a coherence block which can be different in size from either an address block (the amount of storage associated with a cache address tag) or a transfer block, which is the amount of data transferred from memory on a miss <ref> [Goo87] </ref>.
Reference: [Gus92] <author> David Gustavson. </author> <title> The Scalable Coherence Interface and related standards projects. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 10-22, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: If the number of pointers is insufficient to maintain the block's sharing list, then a full-map directory protocol is emulated in software to keep the block coherent. SCI The Scalable Coherence Interface includes not only a cache-coherence protocol, but also specifies a standard for physical and logical connections <ref> [Gus92] </ref>. The SCI cache coherence protocol consists of a doubly-linked sharing list of caches 98 which have the block. With each cache block is stored pointers to the previous and next cache on the sharing list.
Reference: [GW88] <author> James Goodman and Phillip Woest. </author> <title> The Wisconsin Multicube: A new large-scale cache coherent multiprocessor. </title> <booktitle> In Proc. of 15th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 422-431, </pages> <year> 1988. </year> <month> 217 </month>
Reference-contexts: Read snarfing was discussed in previous work by Rudolph [SR84], Goodman <ref> [GW88] </ref>, and Eggers [EK89]. Of these, only Eggers evaluated the performance benefits of read snarfing.
Reference: [GW92] <author> Anoop Gupta and Wolf-Dietrich Weber. </author> <title> Cache invalidation patterns in shared memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(7) </volume> <pages> 794-810, </pages> <year> 1992. </year>
Reference: [GW94] <author> Mike Galles and Eric Williams. </author> <title> Performance optimizations, implementation, and verification of the SGI Challenge multiprocessor. </title> <booktitle> In Proceedings of the 27th Hawaii International Conference on System Sciences, </booktitle> <volume> volume I, </volume> <pages> pages 134-143, </pages> <year> 1994. </year>
Reference-contexts: Introduction Because of their simplicity and low cost, single bus shared-memory multiprocessors have enjoyed a great deal of commercial success. Machines like the Sequent Symmetry [LT88], SGI Challenge <ref> [GW94] </ref>, and Sun's SPARCcenter 2000 [CYS + 93] all use commodity processors and bus technology to reduce design complexity while still achieving good performance. Such designs leverage the investment made in unipro-cessors to keep costs down.
Reference: [GWM90] <author> Anoop Gupta, Wolf-Dietrich Weber, and Todd Mowry. </author> <title> Reducing memory and traffic requirements for scalable directory-based cache coherence schemes. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages (I) 312-321, </pages> <year> 1990. </year>
Reference-contexts: In the absence of read-snarfing, we must estimate the number of re-reads done. Previous work by Gupta et al. has shown that most blocks are not widely shared <ref> [GWM90] </ref>. Therefore, in our experiments where we did not used read snarfing, we estimated a small number of re-reads for every line. It is important to know the cost of an invalidation because it is used to calculate the invalidation ratio.
Reference: [Her93] <author> Stephen Herrod. </author> <title> Tango Lite: A Multiprocessor Simulation Environment. </title> <institution> Stanford University Computer Systems Laboratory, </institution> <month> November </month> <year> 1993. </year>
Reference: [HLH92] <author> Erik Hagersten, Anders Landin, and Seif Haridi. </author> <title> DDM a cache-only memory architecture. </title> <journal> IEEE Computer, </journal> <volume> 25(9) </volume> <pages> 44-54, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Cache-Only Memory Architecture (COMA) A COMA machine has no main memory; instead, large Main Caches (called Attraction Memories in <ref> [HLH92] </ref>) take the role of main memory. <p> Cluster Snoopy Snoopy Snoopy Snoopy Snoopy Directory Snoopy Cache Coherence Top Snoopy Snoopy Snoopy Direc- Hybrid SCI None Cache tory Coherence Cluster Yes Dir.y Dir.y Yes Yesz Yes No Cache? Inclusion? Yes Yes Yes No N/A Yes N/A Memory UMA COMA COMA NUMA NUMA NUMA NUMA Organi zation Reference [Wil87] <ref> [HLH92] </ref> [FIR93] [LLJ + 92] [WTP + 92] [Con94] [VSLW91] yDir.: Only directory entries are cached, not data. zEach cluster shares a portion of main memory, which can be used as a cache of other clusters' pages. <p> One way to save space is to cache tags only above the processor level, which the Data Diffusion Machine <ref> [HLH92] </ref> does. The drawback to this approach is that read requests which miss in the local cluster must be transmitted down to another cluster, if the data is dirty with respect to memory.
Reference: [HP90] <author> John Hennessy and David Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference: [Jer95] <author> Tor E. Jeremiassen. </author> <title> Using Compile-Time Analysis and Transformations to Reduce False Sharing on Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <year> 1995. </year>
Reference: [Jou93] <author> Norm Jouppi. </author> <title> Cache write policies and performance. </title> <booktitle> In Proc. of 20th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 191-201, </pages> <year> 1993. </year>
Reference-contexts: The protocol is snoopy-based [AB86]. It incorporates features from the Illinois protocol [PP84] and from protocols or write policies with subblock (in)validations <ref> [CD93, Jou93] </ref>. The basic philosophy behind the protocol is as follows. As much as possible, we favor cache to cache transfers. On read misses, we transfer as many valid subblocks 25 in the block as possible.
Reference: [KEL91] <author> Eric Koldinger, Susan Eggers, and Henry Levy. </author> <title> On the validity of trace-driven simulation for mulitprocessors. </title> <booktitle> In Proc. 18st Int. Symp. on Computer Architecture, </booktitle> <pages> pages 244-253, </pages> <year> 1991. </year> <month> 218 </month>
Reference: [KEW + 85] <author> R. Katz, S. Eggers, D. Wood, C. Perkins, and R. G. Sheldon. </author> <title> Implementing a cache consistency protocol. </title> <booktitle> In Proc. 12th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 276-283, </pages> <year> 1985. </year>
Reference-contexts: This optimization conserves bus bandwidth and decreases write latency. Two examples of invalidate protocols are the Illinois protocol [PP84] and the Berkeley protocol <ref> [KEW + 85] </ref>. Both protocols feature cache-to-cache transfers on read misses and three common cache block states (Invalid, Clean-Shared, and Dirty).
Reference: [KKEL90] <author> David Keppel, Eric Koldinger, Susan Eggers, and Henry Levy. </author> <title> Techniques for efficient inline tracing on a shared-memory multiprocessor. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 37-47, </pages> <year> 1990. </year>
Reference: [KLMO91] <author> Anna Karlin, Kai Li, Mark Manasse, and Susan Owicki. </author> <title> Empirical studies of competitive spinning for a shared-memory multiprocessor. </title> <booktitle> In Proc. of 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 41-55, </pages> <year> 1991. </year>
Reference-contexts: This is a problem with using a static value for the invalidate threshold. 4.2 Two dynamic hybrid protocols In this section, we describe two dynamic hybrid protocols. Both protocols are adapted from competitive thread-spinning algorithms found in the paper by Karlin et al. <ref> [KLMO91] </ref>. In the paper, the algorithms decide when to switch between spinning a thread while waiting for a lock | an inexpensive operation that can be repeated many times | and blocking a thread, which is an expensive operation that is done 51 only once.
Reference: [KMRS86] <author> Anna Karlin, Mark Manasse, Larry Rudolf, and Daniel Sleator. </author> <title> Competitive snoopy caching. </title> <booktitle> In Proc. of 27th Symposium on Foundations of Computer Science, </booktitle> <pages> pages 244-254, </pages> <year> 1986. </year>
Reference-contexts: In the following three schemes, the receivers of the block decide when to change to a WI strategy. Karlin et al. proposed an algorithm which was proved to be within a factor of 2 of the optimal off line strategy <ref> [KMRS86] </ref>. The algorithm works by associating a counter with each cache block. On an update transaction, a receiver of the update (a cache which currently shares the block) is chosen at random, and its associated block counter is decremented. When a block's counter reaches 0, the block is invalidated. <p> If a static hybrid protocol uses the invalidate ratio for the invalidate threshold, it will do no worse than twice the cost of the optimal algorithm <ref> [KMRS86] </ref>; however it is possible to do better if the value of the threshold is varied as the program runs. The following two algorithms vary the threshold from 0 up to (and including) the value of the invalidate ratio in order to improve performance.
Reference: [Lil93] <author> David Lilja. </author> <title> Cache coherence in large-scale multiprocessors: Issues and comparisons. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(3) </volume> <pages> 303-338, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: In general, the mechanism can be provided in software, hardware, or some combination of both. One hardware strategy is to maintain coherence information with every memory block in structures called directories <ref> [Lil93] </ref>. When a cache miss occurs, the directory is consulted and the appropriate action taken to maintain coherence. Directory-based coherence protocols don't rely on inexpensive broadcast mechanisms, so they are often a good protocol choice for non-bus based systems.
Reference: [LLJ + 92] <author> Daniel Lenoski, James Laudon, Truman Joe, David Nakahira, Luis Stevens, Anoop Gupta, and John Hennessy. </author> <title> The DASH prototype: Implementation and performance. </title> <booktitle> In Proc. 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 92-103, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Snoopy Snoopy Snoopy Snoopy Directory Snoopy Cache Coherence Top Snoopy Snoopy Snoopy Direc- Hybrid SCI None Cache tory Coherence Cluster Yes Dir.y Dir.y Yes Yesz Yes No Cache? Inclusion? Yes Yes Yes No N/A Yes N/A Memory UMA COMA COMA NUMA NUMA NUMA NUMA Organi zation Reference [Wil87] [HLH92] [FIR93] <ref> [LLJ + 92] </ref> [WTP + 92] [Con94] [VSLW91] yDir.: Only directory entries are cached, not data. zEach cluster shares a portion of main memory, which can be used as a cache of other clusters' pages.
Reference: [LT88] <author> Tom Lovett and Shreekant Thakkar. </author> <title> The Symmetry multiprocessor system. </title> <booktitle> In Proc. of Int. Conf. on Parallel Processing, </booktitle> <pages> pages 303-310, </pages> <year> 1988. </year>
Reference-contexts: Introduction Because of their simplicity and low cost, single bus shared-memory multiprocessors have enjoyed a great deal of commercial success. Machines like the Sequent Symmetry <ref> [LT88] </ref>, SGI Challenge [GW94], and Sun's SPARCcenter 2000 [CYS + 93] all use commodity processors and bus technology to reduce design complexity while still achieving good performance. Such designs leverage the investment made in unipro-cessors to keep costs down.
Reference: [LZGS84] <author> Edward Lazowska, John Zahorjan, G. Scott Graham, and Kenneth C. Sevick. </author> <title> Quanititative System Performance. </title> <publisher> Prentice-Hall, Inc., </publisher> <year> 1984. </year>
Reference: [McC84] <author> E. McCreight. </author> <title> The Dragon computer system: An early overview. </title> <type> Technical report, </type> <institution> Xerox Corp., </institution> <year> 1984. </year> <month> 219 </month>
Reference-contexts: Several examples of both types of protocol exist. A good example of a write-update (WU) protocol is the Dragon protocol <ref> [McC84] </ref>. A feature of this protocol is the addition of a sharing line. When a processor performs an update transaction on the bus, other caches signal that they read the data by raising the sharing line. <p> The WI protocol we used is the Illinois protocol [PP84], while the WU protocol we used is the Dragon protocol <ref> [McC84] </ref>. An important aspect to note about the Dragon protocol is that it uses a shared line to detect if a block being updated is being actively shared.
Reference: [MDWS87] <author> H-K. T. Ma, S. Devadas, R. Wei, and A. Sangiovanni-Vincentelli. </author> <title> Logic verification algorithms and their parallel implementation. </title> <booktitle> In Proceedings of the 24th Design Automation Conference, </booktitle> <pages> pages 283-290, </pages> <year> 1987. </year>
Reference: [NO94] <author> Basem A. Nayfeh and Kunle Olukotun. </author> <title> Exploring the design space for a shared-cache multiprocessor. </title> <booktitle> In Proc. 21st Int. Symp. on Computer Architecture, </booktitle> <pages> pages 166-175, </pages> <year> 1994. </year>
Reference-contexts: This will become technically feasible as circuit density increases. It is less likely that the additional 100 space will be used for caches, since increasing the size of a cache usually slows down access to it. A study done by Nayfeh and Olukotun <ref> [NO94] </ref> concluded that with current technology, putting two processors per chip leads to better performance than one processor per chip with a larger cache.
Reference: [PP84] <author> Mark Papamarcos and Janak Patel. </author> <title> A low overhead coherence solution for multiprocessors with private cache memories. </title> <booktitle> In Proc. of 11th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 348-354, </pages> <year> 1984. </year>
Reference-contexts: If no other processor currently shares the data, the writing processor will observe that the sharing line is not asserted and will cease sending updates until the block becomes shared again. This optimization conserves bus bandwidth and decreases write latency. Two examples of invalidate protocols are the Illinois protocol <ref> [PP84] </ref> and the Berkeley protocol [KEW + 85]. Both protocols feature cache-to-cache transfers on read misses and three common cache block states (Invalid, Clean-Shared, and Dirty). <p> The protocol is snoopy-based [AB86]. It incorporates features from the Illinois protocol <ref> [PP84] </ref> and from protocols or write policies with subblock (in)validations [CD93, Jou93]. The basic philosophy behind the protocol is as follows. As much as possible, we favor cache to cache transfers. On read misses, we transfer as many valid subblocks 25 in the block as possible. <p> The additional storage needed makes LTS a less practical algorithm than Random Walk. 54 4.2.3 Methodology Using the Cerberus simulator, we simulated both adaptive strategies, as well as a WI and WU protocol. The WI protocol we used is the Illinois protocol <ref> [PP84] </ref>, while the WU protocol we used is the Dragon protocol [McC84]. An important aspect to note about the Dragon protocol is that it uses a shared line to detect if a block being updated is being actively shared. <p> Only when a cache at the top the hierarchy writes a block back does the block migrate to memory. 5.4.1 Protocol states Processor level The processor level protocol most closely resembles the Illinois protocol <ref> [PP84] </ref>. The non-transitional states are as follows: INVALID The block is not present in the cache. VALID EXCLUSIVE The block is present in no other processor cache except this one, and the data has not been written by the processor.
Reference: [RHL + 93] <author> Steven Reinhardt, Mark Hill, James Larus, Alvin Lebeck, James Lewis, and David Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual prototyping of parallel computers. </title> <booktitle> In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <year> 1993. </year>
Reference: [Rid] <institution> Ridge Computers. </institution> <note> Ridge 32 User's Guide. </note>
Reference: [SBS93] <author> Per Stenstrom, Mats Brorsson, and Lars Sandberg. </author> <title> An adaptive cache coherence protocol optimized for migratory sharing. </title> <booktitle> In Proc. of 20th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 109-118, </pages> <year> 1993. </year>
Reference-contexts: It is well known that depending on the application, either write-invalidate (WI) or write-update (WU) per forms best [EK88]. Indeed, even within applications, various forms of sharing behavior exist: for example, widely write-shared data (like locks), largely read-only data, and migratory data <ref> [CF93, SBS93] </ref>. Because of these different sharing patterns, varying the protocol as the application executes has the potential to increase application performance by decreasing both the number of bus transactions and the number of bytes transferred over the bus, leading to decreased bus contention and reduced latency for memory operations.
Reference: [SD87] <author> Christoph Scheurich and Michel Dubois. </author> <title> Correct memory operation of cache-based multiprocessors. </title> <booktitle> In Proc. of 45th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 234-233, </pages> <year> 1987. </year>
Reference-contexts: This is consistent with the view that it is the programmer's responsibility to avoid data races. The workings of the system meet Scheurich and Dubois's three conditions that are sufficient for making a system sequentially consistent <ref> [SD87] </ref>. 113 Request Here-&gt; Shared Read Invalidate Down Cache (N,0,z) Invalid Invalid InvalidDirty InvalidInvalid Valid Ex Valid Ex Trans Inv Pinned Invalid Invalid Super Cluster 0 Super Cluster N InvalidInvalid Cache (N,1,X)Cache (N,1,0)Cache (0,1,X) Cache (N,2,0) Bus (N,1,X) Bus (N,1,0) Bus 3 Bus (0, 2, 0) Memory Cache (0,0,0) Cache (0,0,x)
Reference: [Sez94] <author> Andre Seznec. </author> <title> Decoupled sector caches: Conciliating low tag implementation cost and low miss ratio. </title> <booktitle> In Proc. of 21th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 384-393, </pages> <year> 1994. </year>
Reference-contexts: Note also that a usual cache with L = b requires more memory than a sector cache with L and b as above <ref> [Sez94] </ref>. Systems with sector caches also require more bus lines to transmit bitmasks corresponding to the status of the subblocks in a particular block.
Reference: [SFG + 93] <author> Pradeep Sindhu, Jean-Marc Frailong, Jean Gastinel, Michel Cekleov, Leo Yuan, Bill Gunning, and Don Curry. XDBus: </author> <title> A high-performance, consistent, packet-switched VLSI bus. </title> <booktitle> In COMPCON Spring 1993, </booktitle> <pages> pages 338-344, </pages> <year> 1993. </year> <month> 220 </month>
Reference-contexts: Chapter 2 of Robert Bedichek's thesis [Bed94] has a good discussion of the trade-offs in designing high-performance busses. Some high performance busses used in multiprocessors include the XDBus <ref> [SFG + 93] </ref> (used in the SPARCcenter 2000) and the POWERpath-2 (used in the SGI Challenge). Rings Rings address the limited bandwidth problem of busses by using only point-to-point links between caches/clusters.
Reference: [SJF91] <author> Craig Stunkel, Bob Janssens, and W. Kent Fuchs. </author> <title> Address tracing for parallel machines. </title> <journal> IEEE Computer, </journal> <volume> 24(1) </volume> <pages> 31-8, </pages> <month> January </month> <year> 1991. </year>
Reference: [SJG92] <author> Per Stenstrom, Truman Joe, and Anoop Gupta. </author> <title> Comparative performance evaluation of cache-coherent NUMA and COMA architectures. </title> <booktitle> In Proc. 19th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 80-91, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Singh et al. examined the effects of data placement on the DASH, where the granularity of data distribution to home nodes is the page [SJGH93]. A general 1 The flat COMA as proposed by Stenstrom et al. <ref> [SJG92] </ref> does not require broadcast to find a block. However, this is at the expense of keeping a sharing list for every block. 172 technique they tried was the first touch, in which data was allocated in the home cluster which first accessed it.
Reference: [SJGH93] <author> Jaswinder Pal Singh, Truman Joe, Anoop Gupta, and John L. Hennessy. </author> <title> An emprical comparison of the Kendall Square Research KSR-1 and Stan-ford DASH multiprocessors. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 214-225, </pages> <year> 1993. </year>
Reference-contexts: Singh et al. examined the effects of data placement on the DASH, where the granularity of data distribution to home nodes is the page <ref> [SJGH93] </ref>. A general 1 The flat COMA as proposed by Stenstrom et al. [SJG92] does not require broadcast to find a block.
Reference: [SR84] <author> Z. Segall and L. Rudolph. </author> <title> Dynamic decentralized cache schemes for an MIMD parallel processor. </title> <booktitle> In Proc. of 11th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 340-347, </pages> <year> 1984. </year>
Reference-contexts: Read snarfing was discussed in previous work by Rudolph <ref> [SR84] </ref>, Goodman [GW88], and Eggers [EK89]. Of these, only Eggers evaluated the performance benefits of read snarfing.
Reference: [SWG92] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <booktitle> Computer Architecture News, </booktitle> <pages> pages 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Cholesky Hierarchical architecture Like Gauss, Cholesky performs best when using a large block size. Unlike Gauss, Cholesky did not achieve near linear speedup in our runs. The major reason for this is the size of the input matrix | it has limited available concurrency <ref> [SWG92] </ref>. Maximum speedup of about 17.5 was obtained using the 64 byte block size and 32 processors (in the 8X4 or 4X8 organizations) and 128K bus cache size. The advantage of using 64I over 8I was largest when the small bus cache size was used.
Reference: [TCS92] <author> Charles Thacker, David Conroy, and Lawrence Stewart. </author> <title> The Alpha demonstration unit: A high-performance multiprocessor for software and chip development. </title> <journal> Digital Technical Journal, </journal> <volume> 4(4) </volume> <pages> 51-65, </pages> <year> 1992. </year>
Reference-contexts: The benchmarks ran slower because the time and bus traffic saved by doing fewer updates were outweighed by the traffic caused by the invalidations. Veenstra and Fowler evaluated the cache coherence protocol used by the DEC Alpha <ref> [TCS92] </ref>, which uses the Snoopy Reading scheme with an invalidate threshold of 2 [VF94b]. They reported that using a threshold of 2 resulted in shared blocks being invalidated too quickly, thus applications did not gain from selective updating.
Reference: [TM91] <author> Milo Tomasevic and Veljko Milutinovic. </author> <title> A simulation study of snoopy cache coherence protocols. </title> <booktitle> In Proc. of the 25th Hawaii Int. Conference on System Sciences, </booktitle> <pages> pages 427-436, </pages> <year> 1991. </year>
Reference-contexts: Unlike Chen's protocol, Dubnicki's protocol is not easily adaptable to non-directory based coherence protocols. Tomasevic and Milutinovic <ref> [TM91] </ref> advocate a Word Invalidate Protocol (WIP) that uses per-word valid bits and additional block states to reduce the "pollution" of valid blocks with invalid words.
Reference: [VF94a] <author> Jack Veenstra and Robert Fowler. MINT: </author> <title> A front end for efficient simulation of shared-memory multiprocessors. </title> <booktitle> In Proceedings of the Second International Workshop on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS), </booktitle> <pages> pages 201-207, </pages> <month> Jan-uary </month> <year> 1994. </year>
Reference: [VF94b] <author> Jack Veenstra and Robert Fowler. </author> <title> The prospects for on-line hybrid coherency protocols on bus-based multiprocessors. </title> <type> Technical Report 490, </type> <institution> University of Rochester, </institution> <year> 1994. </year> <month> 221 </month>
Reference-contexts: Veenstra and Fowler evaluated the cache coherence protocol used by the DEC Alpha [TCS92], which uses the Snoopy Reading scheme with an invalidate threshold of 2 <ref> [VF94b] </ref>. They reported that using a threshold of 2 resulted in shared blocks being invalidated too quickly, thus applications did not gain from selective updating. Increasing the invalidate threshold in order to get more updating behavior does have drawbacks, however.
Reference: [VSLW91] <author> Zvonko Vranesic, Micahel Stumm, David Lewis, and Ron White. Hector: </author> <title> A hierarchically structured shared-memory multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 24(1) </volume> <pages> 72-79, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Coherence Top Snoopy Snoopy Snoopy Direc- Hybrid SCI None Cache tory Coherence Cluster Yes Dir.y Dir.y Yes Yesz Yes No Cache? Inclusion? Yes Yes Yes No N/A Yes N/A Memory UMA COMA COMA NUMA NUMA NUMA NUMA Organi zation Reference [Wil87] [HLH92] [FIR93] [LLJ + 92] [WTP + 92] [Con94] <ref> [VSLW91] </ref> yDir.: Only directory entries are cached, not data. zEach cluster shares a portion of main memory, which can be used as a cache of other clusters' pages.
Reference: [WB72] <author> W. A. Wulf and C. G. Bell. </author> <title> C.mmp amulti-mini processor. </title> <booktitle> In Proc. Fall Joint Computer Conference, </booktitle> <pages> pages 765-777, </pages> <month> December </month> <year> 1972. </year>
Reference-contexts: The problem of providing all processors a consistent view of memory is called the cache consistency problem. There are many solutions to this problem. The simplest solution is to disallow caching of all write-shared data in a program <ref> [WB72] </ref>. A disadvantage of this solution is that the compiler must conservatively determine which blocks might be write shared. A bigger drawback is that the additional bus traffic generated by accesses to non-cacheable data substantially reduces system performance.
Reference: [Wil87] <author> Andrew Wilson Jr. </author> <title> Hierarchical cache/bus architecture for shared memory multiprocessors. </title> <booktitle> In Proc. of 14th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 244-252, </pages> <year> 1987. </year>
Reference-contexts: Ring Cluster Snoopy Snoopy Snoopy Snoopy Snoopy Directory Snoopy Cache Coherence Top Snoopy Snoopy Snoopy Direc- Hybrid SCI None Cache tory Coherence Cluster Yes Dir.y Dir.y Yes Yesz Yes No Cache? Inclusion? Yes Yes Yes No N/A Yes N/A Memory UMA COMA COMA NUMA NUMA NUMA NUMA Organi zation Reference <ref> [Wil87] </ref> [HLH92] [FIR93] [LLJ + 92] [WTP + 92] [Con94] [VSLW91] yDir.: Only directory entries are cached, not data. zEach cluster shares a portion of main memory, which can be used as a cache of other clusters' pages. <p> Note that inclusion is guaranteed not through associativity, but through the actions of the protocol, as in <ref> [Wil87] </ref>. When an upper cache replaces a block in its cache, it sends a message down that forces all caches below it in the hierarchy to invalidate the block. <p> DIRTY OWNED The block is present in the cache and is dirty with respect to memory. The block does not exist lower in the hierarchy. 5.4.2 Bus request types Having different states for non-processor caches is usual for hierarchical architectures <ref> [Wil87] </ref>, [YTB92]. Depending on cache state and processor actions, the following bus requests can be present (see Appendix C for a complete specification of state transitions and bus request types). READ A request to load a cache block.
Reference: [WTP + 92] <author> Andrew Wilson, Marc Teller, Thoams Probert, Dyung Le, and Richard LaRowe. </author> <title> Lynx/Galactica Net: A distributed, cache coherent multiprocessor system. </title> <booktitle> In Proc. of the 25th Hawaii International Conference on System Sciences, </booktitle> <volume> volume 1, </volume> <pages> pages 416-426, </pages> <year> 1992. </year>
Reference-contexts: Snoopy Directory Snoopy Cache Coherence Top Snoopy Snoopy Snoopy Direc- Hybrid SCI None Cache tory Coherence Cluster Yes Dir.y Dir.y Yes Yesz Yes No Cache? Inclusion? Yes Yes Yes No N/A Yes N/A Memory UMA COMA COMA NUMA NUMA NUMA NUMA Organi zation Reference [Wil87] [HLH92] [FIR93] [LLJ + 92] <ref> [WTP + 92] </ref> [Con94] [VSLW91] yDir.: Only directory entries are cached, not data. zEach cluster shares a portion of main memory, which can be used as a cache of other clusters' pages.
Reference: [YTB92] <author> Q. Yang, G. Thangadurai, and L. Bhuyan. </author> <title> Design of an adaptive cache coherence protocol for large scale multiprocessors. </title> <journal> IEEE TPDS, </journal> <volume> 3(3) </volume> <pages> 281-293, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: DIRTY OWNED The block is present in the cache and is dirty with respect to memory. The block does not exist lower in the hierarchy. 5.4.2 Bus request types Having different states for non-processor caches is usual for hierarchical architectures [Wil87], <ref> [YTB92] </ref>. Depending on cache state and processor actions, the following bus requests can be present (see Appendix C for a complete specification of state transitions and bus request types). READ A request to load a cache block.
Reference: [ZB92] <author> Richard Zucker and Jean-Loup Baer. </author> <title> A performance study of memory consistency models. </title> <booktitle> In Proc. of 19th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 2-12, </pages> <year> 1992. </year>
References-found: 77

