URL: http://www.research.att.com/library/trs/TRs/96/96.10/96.10.1.body.ps.Z
Refering-URL: http://www.research.att.com/library/trs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fhiyan,albg@research.att.com  
Title: Head Automata and Bilingual Tiling: Translation with Minimal Representations  
Author: Hiyan Alshawi Adam L. Buchsbaum 
Keyword: L  
Note: AT&T Labs|Research TR 96.10.1  
Date: September 5, 1996  
Address: 600 Mountain Ave. Murray Hill NJ, 07974  
Affiliation: AT&T Labs|Research  
Abstract: We present a language model consisting of a collection of costed bidirectional finite state automata associated with the head words of phrases. The model is suitable for incremental application of lexical associations in a dynamic programming search for optimal dependency tree derivations. We also present a model and algorithm for machine translation involving optimal "tiling" of a dependency tree with entries of a costed bilingual lexicon. Experimental results are reported comparing methods for assigning cost functions to these models. We conclude with a discussion of the adequacy of annotated linguistic strings as representations for machine translation. (This material was presented by H. Alshawi in an invited lecture at the 34th Annual Meeting of the Association for Computational Linguistics, Santa Cruz, California, 1996.) 
Abstract-found: 1
Intro-found: 1
Reference: <author> Alshawi, H. </author> <year> 1996a. </year> <title> "Qualitative and Quantitative Models of Speech Translation". In The Balancing Act: Combining Symbolic and Statistical Approaches to Language, edited by P. </title> <editor> Resnik and J. Klavans, </editor> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference: <author> Alshawi, H. </author> <year> 1996b. </year> <title> "Head Automata for Speech Translation". </title> <booktitle> In Proceedings of ICSLP 96, the Fourth International Conference on Spoken Language Processing, </booktitle> <address> Philadelphia, Pennsylvania. </address>
Reference: <author> Alshawi, H. </author> <year> 1995. </year> <title> "Underspecified First Order Logics". In Semantic Ambiguity and Underspecifi-cation, edited by K. </title> <editor> van Deemter and S. Peters, </editor> <publisher> CSLI Publications, Stanford, </publisher> <address> California. </address>
Reference: <author> Alshawi, H. </author> <year> 1992. </year> <title> The Core Language Engine. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference-contexts: This way we can keep the benefits of monolingual/bilingual modularity (Isabelle and Macklovitch 1986) without the computational overhead of transfer-and-filter <ref> (Alshawi et al. 1992) </ref>. It is possible to apply the subtree search directly to the whole graph starting with the initial runtime entries from lexical matching. <p> In our experimental system, these relation symbols are themselves natural language words, although this is not a necessary property of our models. Information coded explicitly in sentence representations by word senses and feature constraints in our previous work <ref> (Alshawi 1992) </ref> is implicit in the models used to derive the dependency trees and translations. In particular, dependency parameters and context-dependent transfer parameters give rise to an implicit, graded notion of word sense. <p> The fact that translation is insensitive to many ambiguities motivated the use of unresolved quasi-logical form for transfer <ref> (Alshawi et al. 1992) </ref>. To the extent that contextual resolution is necessary, context may be provided by the state of the language processor rather than complex semantic representations.
Reference: <author> Alshawi, H. </author> <year> 1987. </year> <title> Memory and Context for Language Interpretation. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, England. </address>
Reference-contexts: Local context may include the state of local processing components (such as our head automata) for capturing grammatical constraints, or the identity of other words in a phrase for capturing sense distinctions. For larger scale context, we have argued elsewhere <ref> (Alshawi 1987) </ref> that memory activation patterns resulting from the process of carrying out an understanding task can act as global context without explicit representations of discourse.
Reference: <author> Alshawi, H., D. Carter, B. Gamback and M. Rayner. </author> <year> 1992. </year> <title> "Swedish-English QLF Translation". In The Core Language Engine, edited by H. Alshawi, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference-contexts: This way we can keep the benefits of monolingual/bilingual modularity (Isabelle and Macklovitch 1986) without the computational overhead of transfer-and-filter <ref> (Alshawi et al. 1992) </ref>. It is possible to apply the subtree search directly to the whole graph starting with the initial runtime entries from lexical matching. <p> In our experimental system, these relation symbols are themselves natural language words, although this is not a necessary property of our models. Information coded explicitly in sentence representations by word senses and feature constraints in our previous work <ref> (Alshawi 1992) </ref> is implicit in the models used to derive the dependency trees and translations. In particular, dependency parameters and context-dependent transfer parameters give rise to an implicit, graded notion of word sense. <p> The fact that translation is insensitive to many ambiguities motivated the use of unresolved quasi-logical form for transfer <ref> (Alshawi et al. 1992) </ref>. To the extent that contextual resolution is necessary, context may be provided by the state of the language processor rather than complex semantic representations.
Reference: <author> Booth, T. </author> <year> 1969. </year> <title> "Probabilistic Representation of Formal Languages". </title> <booktitle> Tenth Annual IEEE Symposium on Switching and Automata Theory. </booktitle>
Reference-contexts: The model is intended to combine the lexical sensitivity of N-gram models (Jelinek et al. 1992) and the structural properties of statistical context free grammars <ref> (Booth 1969) </ref> without the computational overhead of statistical lexicalized tree-adjoining grammars (Schabes 1992, Resnik 1992). The quantitative dependency model described here grew out of the model presented in Alshawi 1996a. An alternative model based on transducer versions of the automata is described in Alshawi 1996b.
Reference: <author> Brew, C. </author> <year> 1992. </year> <title> "Letting the Cat out of the Bag: Generation for Shake-and-Bake MT". </title> <booktitle> Proceedings of COLING92, the International Conference on Computational Linguistics, </booktitle> <address> Nantes, France. </address>
Reference-contexts: The transfer algorithm described in Section 4 searches for the lowest cost `tiling' of the target dependency graph with entries from the bilingual lexicon. Dynamic programming is again used to make exhaustive search tractable, avoiding the combinatoric explosion of shake-and-bake translation <ref> (Whitelock 1992, Brew 1992) </ref>. In Section 5 we present a general framework for associating costs with the solutions of search processes, pointing out some benefits of cost functions other than log likelihood, including an error-minimization cost function for unsupervised training of the parameters in our translation application.
Reference: <author> Brown, P., J. Cocke, S. Della Pietra, V. Della Pietra, F. Jelinek, J. Lafferty, R. Mercer and P. Rossin. </author> <year> 1990. </year> <title> "A Statistical Approach to Machine Translation". </title> <booktitle> Computational Linguistics 16 </booktitle> <pages> 79-85. </pages>
Reference-contexts: The merging of target fragment nodes in the last condition has the effect of joining the target fragments in a consistent fashion. The node mapping function f for the entire tree thus has a different role from the alignment function in the IBM statistical translation model <ref> (Brown et al. 1990, 1993) </ref>; the role of the latter includes the linear ordering of words in the target string. In our approach, target word order is handled exclusively by the target monolingual model. 8 4.3 Transfer Algorithm The main transfer search is preceded by a bilingual lexicon matching phase.
Reference: <author> Brown, P.F., S.A. Della Pietra, V.J. Della Pietra, and R.L. Mercer. </author> <year> 1993. </year> <title> "The Mathematics of Statistical Machine Translation: Parameter Estimation". </title> <booktitle> Computational Linguistics 19 </booktitle> <pages> 263-312. </pages>
Reference: <author> Chen, K.H. and H. H. Chen. </author> <year> 1992. </year> <title> "Attachment and Transfer of Prepositional Phrases with Constraint Propagation". </title> <journal> Computer Processing of Chinese and Oriental Languages, </journal> <volume> Vol. 6, No. 2, </volume> <pages> 123-142. </pages>
Reference: <author> Church K. and R. Patil. </author> <year> 1982. </year> <title> "Coping with Syntactic Ambiguity or How to Put the Block in the Box on the Table". </title> <booktitle> Computational Linguistics 8 </booktitle> <pages> 139-149. </pages>
Reference-contexts: In Section 2 we present reversible mono-lingual models consisting of collections of simple automata associated with the heads of phrases. These head automata are applied by an algorithm with admissible incremental pruning based on semantic association costs, providing a practical solution to the problem of combinatoric disambiguation <ref> (Church and Patil 1982) </ref>. The model is intended to combine the lexical sensitivity of N-gram models (Jelinek et al. 1992) and the structural properties of statistical context free grammars (Booth 1969) without the computational overhead of statistical lexicalized tree-adjoining grammars (Schabes 1992, Resnik 1992).
Reference: <author> Collins, M. and J. Brooks. </author> <year> 1995. </year> <title> "Prepositional Phrase Attachment through a Backed-Off Model." </title> <booktitle> Proceedings of the Third Workshop on Very Large Corpora, </booktitle> <address> Cambridge, Massachusetts, </address> <booktitle> ACL, </booktitle> <pages> 27-38. </pages>
Reference: <author> Dorr, B.J. </author> <year> 1994. </year> <title> "Machine Translation Divergences: A Formal Description and Proposed Solution". </title> <booktitle> Computational Linguistics 20 </booktitle> <pages> 597-634. </pages>
Reference: <author> Dunning, T. </author> <year> 1993. </year> <title> "Accurate Methods for Statistics of Surprise and Coincidence." </title> <journal> Computational Linguistics. </journal> <volume> 19 </volume> <pages> 61-74. </pages>
Reference: <author> Early, J. </author> <year> 1970. </year> <title> "An Efficient Context-Free Parsing Algorithm". </title> <journal> Communications of the ACM 14: </journal> <pages> 453-60. </pages>
Reference: <author> Gazdar, G., E. Klein, G.K. Pullum, and I.A.Sag. </author> <year> 1985. </year> <title> Generalised Phrase Structure Grammar. </title> <publisher> Blackwell, Oxford. </publisher>
Reference-contexts: The case of zero transitions will yield empty sequences, corresponding to a leaf node of the dependency tree. From a linguistic perspective, head automata allow for a compact, graded, notion of lexical subcategorization <ref> (Gazdar et al. 1985) </ref> and the linear order of a head and its dependent phrases. Lexical parameters can control the saturation of a lexical item (for example a verb that is both transitive and intransitive) by starting the same automaton in different states.
Reference: <author> Hinton, G.E., P. Dayan, B.J. Frey and R.M. Neal. </author> <year> 1995. </year> <title> "The `Wake-Sleep' Algorithm for Unsupervised Neural Networks". </title> <booktitle> Science 268 </booktitle> <pages> 1158-1161. </pages>
Reference: <author> Hudson, R.A. </author> <year> 1984. </year> <title> Word Grammar. </title> <publisher> Blackwell, Oxford. </publisher>
Reference-contexts: We first describe the model in terms of the familiar paradigm of a generative statistical model, presenting the parameters as conditional probabilities. This gives us a stochastic version of dependency grammar <ref> (Hudson 1984) </ref>.
Reference: <author> Hirschman, L., M. Bates, D. Dahl, W. Fisher, J. Garofolo, D. Pallett, K. Hunicke-Smith, P. </author> <title> Price, </title> <publisher> A. </publisher>
Reference: <author> Rudnicky, and E. Tzoukermann. </author> <year> 1993. </year> <title> "Multi-Site Data Collection and Evaluation in Spoken Language Understanding". </title> <booktitle> In Proceedings of the Human Language Technology Workshop, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, </address> <pages> 19-24. </pages>
Reference: <author> Isabelle, P. and E. Macklovitch. </author> <year> 1986. </year> <title> "Transfer and MT Modularity", </title> <booktitle> Eleventh International Conference on Computational Linguistics, </booktitle> <address> Bonn, Germany, </address> <pages> 115-117. </pages>
Reference-contexts: Keeping the arcs P separate in the configuration allows efficient incremental application of target dependency costs c G 0 during the search, so these costs are taken into account in the pruning step of the overall search control. This way we can keep the benefits of monolingual/bilingual modularity <ref> (Isabelle and Macklovitch 1986) </ref> without the computational overhead of transfer-and-filter (Alshawi et al. 1992). It is possible to apply the subtree search directly to the whole graph starting with the initial runtime entries from lexical matching.
Reference: <author> Jackendoff, R.S. </author> <year> 1977. </year> <title> X-bar Syntax: A Study of Phrase Structure. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mas-sachusetts. </address>
Reference-contexts: Head automata can also be used to code a grammar in which states of an automaton for word w corresponds to X-bar levels <ref> (Jackendoff 1977) </ref> for phrases headed by w. Head automata are formally more powerful than finite state automata that accept regular languages in the following sense.
Reference: <author> Jelinek, F., R.L. Mercer and S. Roukos. </author> <year> 1992. </year> <title> "Principles of Lexical Language Modeling for Speech Recognition". In Advances in Speech Signal Processing, edited by S. Furui and M.M. Sondhi. </title> <publisher> Marcel Dekker, </publisher> <address> New York. </address>
Reference-contexts: These head automata are applied by an algorithm with admissible incremental pruning based on semantic association costs, providing a practical solution to the problem of combinatoric disambiguation (Church and Patil 1982). The model is intended to combine the lexical sensitivity of N-gram models <ref> (Jelinek et al. 1992) </ref> and the structural properties of statistical context free grammars (Booth 1969) without the computational overhead of statistical lexicalized tree-adjoining grammars (Schabes 1992, Resnik 1992). The quantitative dependency model described here grew out of the model presented in Alshawi 1996a.
Reference: <author> Lafferty, J., D. Sleator and D. Temperley. </author> <year> 1992. </year> <title> "Grammatical Trigrams: A Probabilistic Model of Link Grammar". </title> <booktitle> In Proceedings of the 1992 AAAI Fall Symposium on Probabilistic Approaches to Natural Language, </booktitle> <pages> 89-97. </pages>
Reference-contexts: In practice, the number of parameters in a head automaton language model is dominated by the dependency parameters, that is, O (jV j 2 jRj) parameters. This puts the size of the model somewhere in between 2-gram and 3-gram model. The similarly motivated link grammar model <ref> (Lafferty, Sleator and Temperley 1992) </ref> has O (jV j 3 ) parameters. Unlike simple N-gram models, head automata models yield an interesting distribution of sentence lengths.
Reference: <author> Kay, M. </author> <year> 1989. </year> <title> "Head Driven Parsing". </title> <booktitle> In Proceedings of the Workshop on Parsing Technologies, </booktitle> <address> Pittsburg, </address> <year> 1989. </year>
Reference-contexts: In our experimental system we use a more general version of the algorithm to allow input in the form of word lattices. 4 The algorithm is a bottom-up tabular parser (Younger 1967, Early 1970) in which constituents are constructed "head-outwards" <ref> (Kay 1989, Sata and Stock 1989) </ref>. Since we are analyzing bottom-up with generative model automata, the algorithm `runs' the automata backwards.
Reference: <author> Lindop, J. and J. Tsujii. </author> <year> 1991. </year> <title> "Complex Transfer in MT: A Survey of Examples". </title> <type> Technical Report 91/5, </type> <institution> Centre for Computational Linguistics, UMIST, Manchester, UK. </institution>
Reference-contexts: Unlike the head automata monolingual models, the transfer model operates with unordered dependency trees, that is, it treats the dependents of a word as an unordered bag. The model is general enough to cover 6 the common translation problems discussed in the literature <ref> (e.g. Lindop and Tsujii 1991 and Dorr 1994) </ref> including many-to-many word mapping, argument switching, and head switching. A transfer model consists of a bilingual lexicon and a transfer parameter table.
Reference: <author> Resnik, P. </author> <year> 1992. </year> <title> "Probabilistic Tree-Adjoining Grammar as a Framework for Statistical Natural Language Processing". </title> <booktitle> In Proceedings of COLING-92, </booktitle> <address> Nantes, France, </address> <pages> 418-424. </pages>
Reference-contexts: The model is intended to combine the lexical sensitivity of N-gram models (Jelinek et al. 1992) and the structural properties of statistical context free grammars (Booth 1969) without the computational overhead of statistical lexicalized tree-adjoining grammars <ref> (Schabes 1992, Resnik 1992) </ref>. The quantitative dependency model described here grew out of the model presented in Alshawi 1996a. An alternative model based on transducer versions of the automata is described in Alshawi 1996b.
Reference: <author> Sata, G. and O. Stock. </author> <year> 1989. </year> <title> "Head-Driven Bidirectional Parsing". </title> <booktitle> In Proceedings of the Workshop on Parsing Technologies, </booktitle> <address> Pittsburg, </address> <year> 1989. </year>
Reference-contexts: In our experimental system we use a more general version of the algorithm to allow input in the form of word lattices. 4 The algorithm is a bottom-up tabular parser (Younger 1967, Early 1970) in which constituents are constructed "head-outwards" <ref> (Kay 1989, Sata and Stock 1989) </ref>. Since we are analyzing bottom-up with generative model automata, the algorithm `runs' the automata backwards.
Reference: <author> Schabes, Y. </author> <year> 1992. </year> <title> "Stochastic Lexicalized Tree-Adjoining Grammars". </title> <booktitle> In Proceedings of COLING-92, </booktitle> <address> Nantes, France, </address> <pages> 426-432. </pages>
Reference-contexts: The model is intended to combine the lexical sensitivity of N-gram models (Jelinek et al. 1992) and the structural properties of statistical context free grammars (Booth 1969) without the computational overhead of statistical lexicalized tree-adjoining grammars <ref> (Schabes 1992, Resnik 1992) </ref>. The quantitative dependency model described here grew out of the model presented in Alshawi 1996a. An alternative model based on transducer versions of the automata is described in Alshawi 1996b.
Reference: <author> Whitelock, P.J. </author> <year> 1992. </year> <title> "Shake-and-Bake Translation". </title> <booktitle> Proceedings of COLING92, the International Conference on Computational Linguistics, </booktitle> <address> Nantes, France. </address>
Reference-contexts: The transfer algorithm described in Section 4 searches for the lowest cost `tiling' of the target dependency graph with entries from the bilingual lexicon. Dynamic programming is again used to make exhaustive search tractable, avoiding the combinatoric explosion of shake-and-bake translation <ref> (Whitelock 1992, Brew 1992) </ref>. In Section 5 we present a general framework for associating costs with the solutions of search processes, pointing out some benefits of cost functions other than log likelihood, including an error-minimization cost function for unsupervised training of the parameters in our translation application.
Reference: <author> Younger, D. </author> <year> 1967. </year> <title> Recognition and Parsing of Context-Free Languages in Time n 3 . Information and Control, </title> <booktitle> 10, </booktitle> <pages> 189-208. 17 </pages>
Reference-contexts: In our experimental system we use a more general version of the algorithm to allow input in the form of word lattices. 4 The algorithm is a bottom-up tabular parser <ref> (Younger 1967, Early 1970) </ref> in which constituents are constructed "head-outwards" (Kay 1989, Sata and Stock 1989). Since we are analyzing bottom-up with generative model automata, the algorithm `runs' the automata backwards. <p> This limit can be derived in a similar way to cubic time tabular recognition algorithms for context free grammars <ref> (Younger 1967) </ref> with the grammar related term being replaced by the term min (n 2 ; jV j 2 ) since the words of the input sentence also act as categories in the head automata model.
References-found: 32

