URL: http://www.demo.cs.brandeis.edu/postscript.dump/darwenthesis.ps.gz
Refering-URL: http://www.cs.brandeis.edu/~darwen/publications.html
Root-URL: http://www.cs.brandeis.edu
Title: CO-EVOLUTIONARY LEARNING BY AUTOMATIC MODULARISATION WITH SPECIATION  
Author: Paul James Darwen 
Degree: a thesis submitted to  for the degree of doctor of philosophy By  
Date: 15 November 1996  
Affiliation: the school of computer science University College University of New South Wales Australian Defence Force Academy  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> David H. Ackley. </author> <title> A Connectionist Machine for Genetic Hillclimbing. </title> <booktitle> The Kluwer International Series in Engineering and Computer Science. </booktitle> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <year> 1987. </year>
Reference-contexts: In any realistic problem, n is very large. For example, in Section 3.1.3 the number of possible strategies is n = 2 70 10 21 . * Each information pattern is denoted I i , where i 2 <ref> [1; n] </ref>. We assume that each information pattern breeds true, i.e., an I i only produces perfect copies of itself, ignoring mutation and recombination [3, page 20] [188, page 473]. * The number of copies of an information pattern is x i , where x i 0. <p> We describe two games that are popular and useful machine learning problems in Sections 2.3.3 and 2.3.4. Evolutionary learning is especially good for knowledge-lean [174] or black box <ref> [1] </ref> problems, i.e., problems for which little or no prior knowledge exists. Prior knowledge would allow the search to be limited to regions known to be promising. 2.3.2 The Sequential Decision Problem Sequential decision tasks include many control and management tasks, including many games. <p> That is, we have added fine texture at local scales without changing the large-scale structure of the landscape 1 . Such fine texture makes this space difficult for simple hill-climbing <ref> [1, pages 13-16] </ref>. Search algorithms suitable for this type of space include: Local search The local search algorithm samples all the points in a neighbourhood around the current point. If the neighbourhood is larger than the small hills, this has the effect of smoothing the search space. <p> High temperatures act like a bandpass filter to smooth out the local texture, to reveal only the large-scale structure of the space. After converging to the region of the global optimum, lower temperatures then allow the search of the local texture close to the global optimum <ref> [1, page 166] </ref>. Figure B.3 [1, pages 16-17] resembles Figure B.2, but now the local hills and valleys are the dominant features. This type of landscape is good for evolutionary search. <p> After converging to the region of the global optimum, lower temperatures then allow the search of the local texture close to the global optimum [1, page 166]. Figure B.3 <ref> [1, pages 16-17] </ref> resembles Figure B.2, but now the local hills and valleys are the dominant features. This type of landscape is good for evolutionary search. <p> This depends on the representation: representing the x y plane of Figure B.3 in Cartesian coordinates would make the x- and y-coordinates easily crossed over, 1 The function in Figure B.2 is f 9 in Thomas Back's GENEsYs genetic algorithm software <ref> [1, page 13] </ref>. Page 152 University College, The University of New South Wales Figure B.3: Coarse texture at local scales, added to a unimodal function. because the global optimum shares coordinates with other local optima under that representation. <p> The mathematical models of Equations A.5 and 2.13 generalise to two populations [85, pages 137, 273] [3, page 21], and similar results can be derived. Consider two populations. Population A has m possible information patterns, where the number of each pattern is given by x i 0; i 2 <ref> [1; m] </ref>, and the i th pattern's share of the total population is p i = x i = P k x k . <p> Similarly, population B has n possible patterns, counted by y j 0; j 2 <ref> [1; n] </ref>, and the j th pattern's share of the population is q j = y j = k y k .
Reference: [2] <author> Nobue Adachi and Kazuhiro Matsuo. </author> <title> Ecological dynamics under different selection rules in distributed and iterated prisoner's dilemma game. </title> <editor> In Hans-Paul Schwefel and Reinhard Manner, editors, </editor> <title> Parallel Problem Solving from Nature: </title> <booktitle> first workshop, volume 496 of Lecture Notes in Computer Science, </booktitle> <pages> pages 388-394, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: A similar study [131] found that if there were only two possible strategies, they could both survive. But another found that more than two would cause convergence to a single optimum and the extinction of the others <ref> [2] </ref>. A more detailed version of this model allowed strategies of increasing sophistication [109]. It found that, for particular ranges of the payoffs in IPD, stable patches of different strategies emerge [109, page 309]. Why diversity emerges only for some variants of the game remains a mystery.
Reference: [3] <author> Ethan Akin. </author> <title> The differential geometry of population genetics and evolutionary games. </title> <editor> In Sabin Lessard, editor, </editor> <booktitle> Mathematical and Statistical Developments of Evolutionary Theory: Proceedings of the NATO Advanced Study Institute and Seminaire de Mathematiques Superieures on Mathematical and Statistical Developments of Evolutionary Theory, </booktitle> <pages> pages 1-93. </pages> <publisher> Kluwer Academic, </publisher> <month> August </month> <year> 1987. </year>
Reference-contexts: Replicator equations describe the populations of entities that reproduce. As well as evolution, they explain epidemics [25, page 33], ecosystems [157], compound interest <ref> [3, page 20] </ref>, population genetics, prebiotic evolution (chemical "hypercycles"), animal behaviour, population ecology, and evolutionary game theory [157]. There are two approaches to replicator equations, continuous and discrete. These are closely related [3, page 81]. <p> As well as evolution, they explain epidemics [25, page 33], ecosystems [157], compound interest [3, page 20], population genetics, prebiotic evolution (chemical "hypercycles"), animal behaviour, population ecology, and evolutionary game theory [157]. There are two approaches to replicator equations, continuous and discrete. These are closely related <ref> [3, page 81] </ref>. Genetic algorithms have discrete generations, so we Page 14 University College, The University of New South Wales concentrate on the discrete case. Appendix A discusses some analogous results in the continuous case. <p> We assume that each information pattern breeds true, i.e., an I i only produces perfect copies of itself, ignoring mutation and recombination <ref> [3, page 20] </ref> [188, page 473]. * The number of copies of an information pattern is x i , where x i 0. In the discrete case, x i is a non-negative integer. <p> In the discrete case, x i is a non-negative integer. In the continuous case, we assume x i 2 IR + , even though this allows unrealistic values such as 5:5 copies of I i . To avoid this, we assume that populations are large <ref> [3, page 20] </ref> [32, page 5]. * The proportion of the whole population that is type I i is p i , where 0 p i 1: p i = P n (2.2) * The population vector p = (p 1 ; p 2 ; : : : ; p n <p> S n = p 2 IR n : p i 0 and i ) Vector p represents a distribution of individuals. These individuals might be strategies for playing a game. In classical game theory, individuals may be mixed strategies, which are themselves represented by vectors on a simplex <ref> [3, pages 21, 64-65] </ref>. Although we use the language of classical game theory, individuals are not necessarily mixed strategies. Readers familiar with classical game theory should bear this in mind to avoid confusion. * The relative growth rate (or "fitness") of a type i is i . <p> For the discrete case: i = x i (t) In many biological and economic fields, the relative growth rate is of interest. Examples of relative growth rates include bank interest rates, the half-life of an isotope, and the doubling time of a broth of bacteria <ref> [3, page 20] </ref>. 1 This is not to be confused with capital N , the number of players in an N -player game. Australian Defence Force Academy Page 15 All the complexities of fitness and selection are simplified into the parameters i . We will relax this simplification later. <p> The continuous version of Equation 2.6 is Equation A.3 on page 147, and uses the difference between the same terms. There is actually a close relationship between the two <ref> [3, page 81] </ref>. <p> The continuous version of Equation 2.6 is Equation A.3 on page 147, and uses the difference between the same terms. There is actually a close relationship between the two [3, page 81]. These equations appear in related fields, such as natural selection in a diploid population <ref> [3, pages 21, 64] </ref>, and in the n-dimensional continuous Logistic Equation of predator-prey models [20, page 28]. 2.2.4 A Special Replicator Equation: the Schema Theorem A "fundamental theorem" for an evolutionary system is one of several similar theorems that share the prediction that the population tends to become more fit with <p> We call this matrix A the interaction matrix 5 . We assume this interaction between pairs is random: the probability of an information pattern I i meeting I j is merely the proportion of the population, p j <ref> [3, page 21] </ref>. <p> If this is not the case, there are finite-population effects. We ignore these effects by assuming the population is very large, i.e., effectively infinite [32, page 5]. Substituting Equation 2.12 into the replicator Equation 2.6 means that p i changes according to <ref> [3, page 21] </ref> [157, page 92]: p i (t + 1) = p i (t) j a ij p j (t) (2.13) The term p Ap = P ij p i p j a ij is the average payoff of the entire population [3, page 21] [157, Equation 4.13]. <p> means that p i changes according to <ref> [3, page 21] </ref> [157, page 92]: p i (t + 1) = p i (t) j a ij p j (t) (2.13) The term p Ap = P ij p i p j a ij is the average payoff of the entire population [3, page 21] [157, Equation 4.13]. <p> Applying Definition 2.1 to Equation 2.13 says that a population vector p is an equilibrium point (i.e., a fixed point) if it satisfies <ref> [3, page 65] </ref> [85, pages 15, 126] [157, page 94]: n X a ij p j = p Ap for all i; 1 i n (2.16) If no information pattern is extinct at the equilibrium point (i.e. p i &gt; 0 for all i) then Equation 2.16 gives a system of <p> a ij p j = p Ap for all i; 1 i n (2.16) If no information pattern is extinct at the equilibrium point (i.e. p i &gt; 0 for all i) then Equation 2.16 gives a system of linear equations for the n variables p 1 to p n <ref> [3, page 66] </ref> [85, pages 16, 126] [145, page 272]: X a 1j p j = j X a nj p j (2.17) i p i = p 1 + p 2 + : : : + p n = 1 (From Equation 2.2) (2.18) If one or more of the <p> (From Equation 2.2) (2.18) If one or more of the p i are zero (i.e., one or more information patterns are extinct) at equilibrium, then we obtain a linear system like Equations 2.17 and 2.18, but of lower dimension, by excluding the indices i for which p i = 0 <ref> [3, page 69] </ref> [85, page 16]. The common value of the expressions in Equation 2.17 is the population's average payoff 8 , p Ap [85, page 16]. <p> The common value of the expressions in Equation 2.17 is the population's average payoff 8 , p Ap [85, page 16]. These terms will aid discussion: * The patterns I i that aren't extinct (for which p i &gt; 0) are called active strategies <ref> [3, page 65] </ref>. * An equilibrium point p 2 S n is called an interior equilibrium if all strategies are active, p i &gt; 0 8 i [3, page 65]. * If one or more of the p i are zero, an equilibrium point is said to be in the interior <p> These terms will aid discussion: * The patterns I i that aren't extinct (for which p i &gt; 0) are called active strategies <ref> [3, page 65] </ref>. * An equilibrium point p 2 S n is called an interior equilibrium if all strategies are active, p i &gt; 0 8 i [3, page 65]. * If one or more of the p i are zero, an equilibrium point is said to be in the interior of the face of the simplex, i.e., in the interior of the lower-dimensional simplex formed by excluding the indices i for which p i = 0 [85, <p> However, a solution where only one information pattern survives is always an equilibrium solution <ref> [3, page 66] </ref>, because this simple model ignores mutation, crossover, or any other way to restore diversity. We are more interested in interior solutions, when more than one species survives. <p> The proof for the discrete case of Equation 2.13 is straightforward when using the quotient rule of Equation 2.15. For the continuous case of Equation A.5, proofs include the version from classical game theory when individuals are mixed strategies, represented as points on the simplex <ref> [3, page 67] </ref> [157, page 102]. Theorem 2.1 lets us know from A whether there is an interior equilibrium point. <p> That our model ignores mutation and crossover does not stop us examining stability. Some different uses of the word "stability" are discussed in Section D.1. How can we find the stability of equilibria? Linearisation <ref> [3, page 32] </ref> [145, page 273] is one way. Consider the discrete system of Equation 2.14 on page 30. Linearisation involves adding a perturbation vector q to the equilibrium point in question p eq . <p> Australian Defence Force Academy Page 33 2.4.5 Evolutionary Stability The concept of "evolutionary stability" uses pairwise comparisons of populations. Although popular, this definition of stability is not consistent with our intuitive meaning of stability. The original definition came from Maynard Smith's ESS condition <ref> [3, page 73] </ref>, where ESS stood for "evolutionarily stable strategy", and the vector p on the simplex S n was a single mixed strategy. In our formulation, we do not use mixed strategies, and a vector p represents a population of information patterns which are not necessarily strategies. <p> Readers more familiar with classical game theory should avoid confusion. Definition 2.3 (Evolutionarily stable) A point p es 2 S n , is evolutionarily stable if it satisfies these conditions <ref> [3, page 72] </ref> [13, page 27] [157, page 101] [188, page 473]: 1. Equilibrium condition:- n X a ij p es 2. <p> Equilibrium condition:- n X a ij p es 2. Stability condition:- If p es Ap es = q Ap es for some q 6= p es (2.21) then p es Aq &gt; q Ap es (2.22) Equation 2.20 is the definition of a Nash equilibrium <ref> [3, page 73] </ref> [85, page 121], a natural concept for classical game theory, where p is a mixed strategy. However, in our situation where p represents a population instead of a strategy, the Nash condition is exactly the equilibrium condition [3, page 72] [85, page 127] from Equation 2.16. <p> However, in our situation where p represents a population instead of a strategy, the Nash condition is exactly the equilibrium condition <ref> [3, page 72] </ref> [85, page 127] from Equation 2.16. Of more interest is the meaning of the stability condition in Definition 2.3. Imagine that we could somehow take population p es and find its average payoff not against itself, but against a rival population q. <p> And even for the continuous case, the question "Are there other dynamically stable equilibria?" has no clear answer [32, page 16]. In this section, we describe this and other problems with evolutionary "stability". The continuous case is closely related to the discrete Equation 2.13 <ref> [3, page 81] </ref>. The discrete system often, but not always, behaves similarly to the continuous system [3, page 82]. We first consider the behaviour of the continuous system of Equation A.5. <p> In this section, we describe this and other problems with evolutionary "stability". The continuous case is closely related to the discrete Equation 2.13 [3, page 81]. The discrete system often, but not always, behaves similarly to the continuous system <ref> [3, page 82] </ref>. We first consider the behaviour of the continuous system of Equation A.5. <p> As described in Section D.2, if the linearisation matrix B at an equilibrium point p eq has one or more eigenvalues equal to zero, then it is not an isolated equilibrium point <ref> [3, page 36] </ref> [85, page 126]. These equilibrium points form a linear manifold in the interior of S n [85, pages 16, 126, 228]. <p> The first, a stable ending characterised by the shark, was considered in Sec tions 2.4.4 through 2.4.6. 2. The second, where co-evolution never stabilises, was considered in Section 2.4.7. Since Section 2.4.1, we have used simple models that ignored mutation, crossover, the effects of finite population size <ref> [3, page 20] </ref>, and many other complications of a GA. Nonetheless, even for these simple models, mathematical analysis does not tell us all we want to know. <p> x i changes according to: d n X x i = i dt X i x i By equation A.1 = i X x j By equation 2.2 (A.2) Equation A.2 says that the growth rate of the whole population is P weighted average of each type's relative growth rate <ref> [3, page 20] </ref>. <p> Information Australian Defence Force Academy Page 147 patterns with a higher-than-average growth rate will become more plentiful, and lower-than-average patterns will become less plentiful and eventually die out. Equation A.3 appears in related fields, such as natural selection in a diploid population (minus mutation and recombination) <ref> [3, pages 21, 64] </ref>, and in the n dimensional continuous Logistic Equation of predator-prey models [20, page 28]. <p> j A A.2 Co-Evolution in the Continuous Case Recall Equation 2.12, which says that the relative growth rate i depends on its interactions with the other members of its population: i = j Substituting Equation 2.12 into the continuous replicator Equation A.3 means that p i changes according to 1 <ref> [3, page 21] </ref>: dp i = p i @ j=1 1 That last term p Ap = P ij p i p j a ij is the average payoff of the entire population [3, page 21] [157, Equation 4.13]. <p> Substituting Equation 2.12 into the continuous replicator Equation A.3 means that p i changes according to 1 <ref> [3, page 21] </ref>: dp i = p i @ j=1 1 That last term p Ap = P ij p i p j a ij is the average payoff of the entire population [3, page 21] [157, Equation 4.13]. Incidentally, Equation A.5 is equivalent to the Lotka-Volterra equation [85, page 134] [172, pages 23-25]. <p> The two populations co-evolved, and the first created a good sorting algorithm. The mathematical models of Equations A.5 and 2.13 generalise to two populations [85, pages 137, 273] <ref> [3, page 21] </ref>, and similar results can be derived. Consider two populations. <p> Solution paths need not begin close to the equilibrium point, but can begin anywhere in the space. * Asymptotic stability: If solution paths that begin near enough to an equilibrium point move towards it asymptotically, approaching it in the limit as time tends to infinity, then it has asymptotic stability <ref> [3, page 39] </ref>. One can have local asymptotic stability or global asymptotic stability. Zeeman [188] considers a different type of stability, of perturbations of the interaction matrix A instead of the current population p. <p> This is how we tell if the equilibrium point p eq is stable. Doing this for the discrete system of Equation 2.14 tells us that the perturbation variable q changes (approximated to first order) <ref> [3, page 32] </ref> [145, page 283]: q (t + 1) = Bq (t) (D.1) The n fi n matrix B is the first-order approximation of how the perturbation changes. If the first-order approximation is zero, then we must go to second-order, but this is rarely necessary. <p> However, what happens if the population vector p is near that equilibrium but not in that face of the simplex? Equivalently, what happens to the population if we introduce some extinct individuals? It could converge somewhere else entirely, having been lifted away from its face of the simplex <ref> [3, pages 69-70] </ref>. This situation is analogous to introducing some new type of individual with a great advantage. B is a full n fi n matrix to cover this possibility. Finding the eigenvalues of matrix B tells you if the equilibrium point p eq is stable or not. <p> Linearisation allows an accurate insight into the nature of each equilibrium point. Of particular interest to co-evolutionary GA learning, if the linearisation B at an equilibrium point p eq has an eigenvalue equal to zero, then p eq is a degenerate equilibrium <ref> [3, page 35] </ref>. Degenerate equilibrium points come in large groups 1 , while non-degenerate equilibrium points are isolated [3, page 36]. In Section 2.4.7, we discuss the implications this has for co-evolutionary GA learning. <p> Of particular interest to co-evolutionary GA learning, if the linearisation B at an equilibrium point p eq has an eigenvalue equal to zero, then p eq is a degenerate equilibrium [3, page 35]. Degenerate equilibrium points come in large groups 1 , while non-degenerate equilibrium points are isolated <ref> [3, page 36] </ref>. In Section 2.4.7, we discuss the implications this has for co-evolutionary GA learning.
Reference: [4] <author> Peter J. Angeline and Jordan B. Pollack. </author> <title> Evolutionary module acquisition. </title> <booktitle> In Proceedings of the Second Annual Conference on Evolutionary Programming, </booktitle> <month> February </month> <year> 1993. </year>
Reference-contexts: In Section 2.7, we describe some studies of modularisation in evolutionary learning. Some do not automatically decompose the solution into relevant parts | human decisions do that [92] [136] [137]. Another study uses GA speciation, but does not utilise the diverse expertise thus formed [144]. Another approach <ref> [4] </ref> [100] [104] randomly selects and stores parts of individuals off-line. This ties in loosely with what are called "cultural algorithms" [141] [140]. This approach evaluates the randomly-selected parts (directly or indirectly) that are stored off-line, in the hope that this will produce useful specialisation. <p> two similar approaches to the first version of modularisation mentioned in Section 2.7.1 | instead of having different versions of the same task (e.g., diverse strategies for a game), these attempt to create modules that serve very different functions, and not different versions of the same function. * One approach <ref> [4] </ref> randomly selected parts of evolving individuals (which were represented as finite state machines 20 ) and stored those parts off-line. These parts would then be re-introduced to the population later.
Reference: [5] <author> Jaroslaw Arabas, Zbigniew Michaelwicz, and Jan Mulawka. </author> <title> Gavaps | a genetic algorithm with varying population size. </title> <booktitle> In Proceedings of the First IEEE Conference on Evolutionary Computation, </booktitle> <volume> volume 1, </volume> <pages> pages 73-78. </pages> <publisher> IEEE Press, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: We use the Hamming metric because our particular representation is well-suited. Page 126 University College, The University of New South Wales population size <ref> [5] </ref>. An interesting future paper could improve implicit sharing by dynamically changing the sample size in tune with the diversity of the current GA population. In any case, finding a good sample selection scheme is not our first interest. <p> This improvement, plus the flaws of speciation methods described in Sections 4.2 and 5.2, shows the need (and the opportunity) for improved GA speciation methods. For example, just as there are GA systems which dynamically change the mutation rate [29] and the population size <ref> [5] </ref>, a future study might improve implicit sharing by dynamically changing the sample size in tune with the diversity of the current GA population. 5.3.5 Future Work on Automatic Modularisation The method presented here is a very simple implementation of a modular approach. Further refinement should give improved results.
Reference: [6] <author> Robert M. Axelrod. </author> <title> The Evolution of Cooperation. </title> <publisher> Basic Books, </publisher> <year> 1984. </year>
Reference-contexts: The game being learned in Chapter 3 is the Iterated Prisoner's Dilemma (IPD), both in 2- and N -player versions. Although we use it merely as a test problem, IPD is a simplified version of many significant real-world problems, from recycling programs [63, page 62] to trench warfare <ref> [6, pages 73-87] </ref> and superpower confrontation [18]. Previous Work Axelrod [7] and others [27] [54] [108] have studied co-evolutionary learning of IPD, and co-evolution has been used to learn other games [46] [144] [160] and other tasks [81] [82] [99]. <p> The 2-player Prisoner's Dilemma has been widely studied in such diverse fields as economics, mathematical game theory, political science, and artificial intelligence. The Prisoner's Dilemma is popular because it is a simple game, yet it captures the basics of many real-world situations <ref> [6] </ref>. These include such diverse situations as the Watergate scandal of 1972-1974 [128], the Cold War of 1945-1990 [18], and life in the trenches during the First World War of 1914-1918 [6, pages 73-87]. T R P S Cooperate Defect Cooperate Defect game. <p> These include such diverse situations as the Watergate scandal of 1972-1974 [128], the Cold War of 1945-1990 [18], and life in the trenches during the First World War of 1914-1918 <ref> [6, pages 73-87] </ref>. T R P S Cooperate Defect Cooperate Defect game. In 2-player Iterated Prisoner's Dilemma (2IPD), the above interaction is repeated many times, and both players can remember previous outcomes. <p> In the Prisoner's Dilemma, each player has a choice of two operations: either cooperate with the other player, or defect. Payoff to both players is calculated according to Figure 2.4. The payoff parameters S; P; R; T in Figure 2.4 must satisfy the following conditions <ref> [6] </ref>: T &gt; R &gt; P &gt; S (2.9) Page 24 University College, The University of New South Wales The Prisoner's Dilemma is a dilemma because of Equation 2.9. <p> So there is also an incentive to not defect. Hence the dilemma. In the Iterated Prisoner's Dilemma (IPD), the game is repeated many times, and each player can remember previous steps. The iterated game has many real-life applications <ref> [6] </ref>, yet is a simple and non-trivial game, with few inputs (only the memory of recent iterations) and a single binary output (cooperate or defect) [27, page 665]. Equation 2.10 prevents IPD from degenerating into an alternating pattern where the two players take turns at cooperating and defecting. <p> It is the long-term, accumulated payoff by which an IPD strategy is judged. Exactly how many iterations does a game of IPD last? The number of iterations casts the so-called "shadow of the future" <ref> [6, page 13] </ref>. * If the two players will never meet again, then the sensible action is to defect, because there are no later iterations that let the other player retaliate. * So if both players know it is the last round, and that both should defect on the last round, <p> second-last round as well. * So if both will defect on the second-last round, it makes sense to defect on the third-last, and so on back to the start. * So if the number of iterations is known in advance, then the best action is to defect all the time <ref> [6, page 13] </ref>. In the real-life problems that resemble Iterated Prisoner's Dilemma, nobody knows how long the game will last, providing an incentive for mutual cooperation. In this thesis, however, the shadow of the future is not a problem. <p> More sophisticated strategies may learn this, and most human-designed strategies will utilise this knowledge if available. There exist more sophisticated ways to take into account the shadow of the future <ref> [6, page 13] </ref>. Section 2.4.5 will discuss "evolutionary stability". Iterated Prisoner's Dilemma does not allow evolutionary stability [17], unless mistakes can occur [16]. These two results also apply to the N -player game [184]. <p> We will study a co-evolving GA learning IPD in more detail in Chapter 3. There has been much recent research on evolutionary learning in the 2-player Iterated Prisoner's Dilemma (2IPD) [7] [27] [37] [52] [54] [108]. One of the more effective strategies for IPD is "Tit-for-Tat" <ref> [6] </ref>. This simple strategy cooperates at the first iteration, and after that does whatever the opponent did in the previous iteration. That is, if the opponent defected on the previous iteration, then Tit-for-tat will defect. If the opponent cooperated on the previous iteration, then Tit-for-tat will cooperate. <p> We use a co-evolutionary GA to learn to play the iterated Prisoner's Dilemma (IPD) <ref> [6] </ref>. As described in Section 2.3.3, this simple non-cooperative non-zero-sum game is widely studied in political science, machine learning, economics, and mathematical game theory. Axelrod [6] investigated evolution and the IPD. <p> We use a co-evolutionary GA to learn to play the iterated Prisoner's Dilemma (IPD) <ref> [6] </ref>. As described in Section 2.3.3, this simple non-cooperative non-zero-sum game is widely studied in political science, machine learning, economics, and mathematical game theory. Axelrod [6] investigated evolution and the IPD. He used a GA population of strategies, in which each plays IPD with every other strategy in the population [7, page 38]. He found that this dynamic environment produced high scorers. Similar studies give similar results [27] [37] [52] [54] [108]. <p> We will see in Section 3.1.4 that strategies used here are represented as simple rule sets, which are too simple to learn that the number of rounds of IPD is finite and fixed. This means we can ignore the " of the future" <ref> [6] </ref> that normally must be addressed when learning IPD. 3.1.4 Software Implementation for 2IPD We used Grefenstette's Genesis 5.0 software package, and modified the evaluation procedure to evaluate individuals by how they score in IPD against members of the GA population. <p> This makes it impossible for them to learn to defect because the game is finite, as described in Section 2.3.3. Therefore, we need not consider ways that keep the length of the game effectively unknown <ref> [6] </ref>. An individual strategy is evaluated by playing IPD against all the other members of the population. The average of the scores that an individual achieves in all these games is its fitness. This is straightforward co-evolutionary evaluation. <p> We simulate the most mercenary, dog-eat-dog form of evolution. Despite this, cooperative strategies proliferate and drive their non-cooperative competitors to extinction. Axelrod <ref> [6] </ref> [7] describes this in detail. We briefly review the evolution of cooperation, and go on to generalisation in Section 3.2.2. of random strategies, the population's average payoff plummets as uncooperative strategies exploit the many random gullible strategies who do not retaliate. <p> For l = 3, this comes to 70 bits and 2 70 10 21 possible strategies. Again, these strategies are too simple to learn that the game is finite, so we may ignore the "shadow of the future" of IPD <ref> [6, page 13] </ref>. All games of IPD last for 100 iterations. Genetic Algorithm Parameters The parameters below were selected after a modest amount of searching. The different parameters for l = 4 are to keep running times reasonable.
Reference: [7] <author> Robert M. Axelrod. </author> <title> The evolution of strategies in the iterated prisoner's dilemma. </title> <editor> In Lawrence Davis, editor, </editor> <title> Genetic Algorithms and Simulated Annealing, </title> <booktitle> chapter 3, </booktitle> <pages> pages 32-41. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1987. </year>
Reference-contexts: However, a fixed fitness function is not appropriate for certain problems. Consider the problem of learning to play a game of conflict. Individual strategies could be evaluated by how well they perform against a set of fixed strategies of known high quality <ref> [7] </ref>. This approach has disadvantages: * If the fixed evaluation strategies are not a representative sample, then evolution will over-specialise to those strategies. <p> We cover some relevant mathematical theory of co-evolution, and previous work on co-evolution and speciation with genetic algorithms. In Chapter 3, we implement a simple co-evolutionary learning system, similar to Axelrod's <ref> [7] </ref>, and set it to acquire expertise in Iterated Prisoner's Dilemma. Unfortunately, it generalises poorly. We explain why, and demonstrate that some plausible improvements do not greatly change this poor generalisation ability. Speciation could maintain diversity, avoid over-specialisation, and so improve generalisation ability. <p> Although we use it merely as a test problem, IPD is a simplified version of many significant real-world problems, from recycling programs [63, page 62] to trench warfare [6, pages 73-87] and superpower confrontation [18]. Previous Work Axelrod <ref> [7] </ref> and others [27] [54] [108] have studied co-evolutionary learning of IPD, and co-evolution has been used to learn other games [46] [144] [160] and other tasks [81] [82] [99]. <p> So these analytical results do not give definite answers about the behaviour of a co-evolving GA learning IPD. We will study a co-evolving GA learning IPD in more detail in Chapter 3. There has been much recent research on evolutionary learning in the 2-player Iterated Prisoner's Dilemma (2IPD) <ref> [7] </ref> [27] [37] [52] [54] [108]. One of the more effective strategies for IPD is "Tit-for-Tat" [6]. This simple strategy cooperates at the first iteration, and after that does whatever the opponent did in the previous iteration. <p> Axelrod <ref> [7, page 38] </ref> used an otherwise ordinary GA, but with a co-evolutionary evaluation function, to search the space of strategies for the Iterated Prisoner's Dilemma game. That is, each individual strategy was evaluated by its average score against all the other individuals. <p> However, some strategies evolve a pattern of reciprocating what cooperation they find, and retaliating against defection. These reciprocating players tend to do well because they score highly with others who cooperate, without being exploited for very long by strategies which only defect <ref> [7, page 38] </ref>. As these reciprocators multiply, eventually the entire population cooperates with each other. In Figure 2.8, average payoff eventually stays close to the 100% cooperation level 16 of 2 (the payoff matrix used is Figure 5.1 on page 121). <p> In Figure 2.8, average payoff eventually stays close to the 100% cooperation level 16 of 2 (the payoff matrix used is Figure 5.1 on page 121). This "evolution of cooperation" [119, section 16.2] demonstrates how a coevolutionary GA could evolve high-scoring (in this case, cooperative) strategies for a game <ref> [7, page 38] </ref>. Fogel [52] [54] did a similar study, but represented the strategies in a completely different way (as finite state machines), and emulated evolution in a manner different from a GA. <p> As described in Section 2.3.3, this simple non-cooperative non-zero-sum game is widely studied in political science, machine learning, economics, and mathematical game theory. Axelrod [6] investigated evolution and the IPD. He used a GA population of strategies, in which each plays IPD with every other strategy in the population <ref> [7, page 38] </ref>. He found that this dynamic environment produced high scorers. Similar studies give similar results [27] [37] [52] [54] [108]. <p> That distinguishes this chapter from previous studies of co-evolutionary learning, especially those that looked at IPD from the perspective of game playing. 3.1.1 Mass Extinctions in Previous Research Lindgren [108] studied a system similar to Axelrod's <ref> [7, page 38] </ref>. He showed that when a population of strategies plays IPD against its own members, high-quality Australian Defence Force Academy Page 63 strategies dominate the population for long periods of time, but occasionally they are suddenly wiped out and replaced. <p> In particular, our suspicion that co-evolution produces strategies that are not robust is not obviously explained in terms of a fixed evaluation function. 3.1.2 Causes and Possible Solutions of Poor Generalisation In this chapter, we study a co-evolutionary learning system similar to Axelrod's <ref> [7] </ref>, and find a novel reason for Lindgren's catastrophic collapses [108]. <p> Our interest has a wider significance | we study the generalisation ability of co-evolutionary learning, using IPD as a convenient test problem. This game is particular suitable, as over-specialisation to one particular strategy is easy to observe. We use Axelrod's payoffs for prisoner's dilemma <ref> [7] </ref>, as shown in Figure 3.1. <p> We represent individual IPD strategies as rule sets, following Axelrod <ref> [7] </ref> and Lindgren [108]. In Section 3.4.1, we use an improved representation which has shorter length and is applicable to the N -player IPD. However, we could use any convenient representation, and our results are relevant to co-evolutionary learning in general. <p> However, we could use any convenient representation, and our results are relevant to co-evolutionary learning in general. Here, a strategy remembers only the three most recent iterations of IPD, following Axelrod <ref> [7] </ref>. Each iteration has 4 possible outcomes (from Figure 3.1), so if we remember the 3 most recent iterations, there are 4 fi 4 fi 4 = 64 possible histories of 3 steps. <p> We simulate the most mercenary, dog-eat-dog form of evolution. Despite this, cooperative strategies proliferate and drive their non-cooperative competitors to extinction. Axelrod [6] <ref> [7] </ref> describes this in detail. We briefly review the evolution of cooperation, and go on to generalisation in Section 3.2.2. of random strategies, the population's average payoff plummets as uncooperative strategies exploit the many random gullible strategies who do not retaliate. <p> This graph shows the mean and standard deviation of the bias for 30 runs of a GA population playing repeated prisoner's dilemma in a round robin. The system has converged by 250 generations. reaches 100%, rest assured that cooperation effectively dominates the population in Axelrod <ref> [7] </ref> and others were impressed that co-evolutionary learning could create a population of cooperators, even though this simulates the most mercenary, dog-eat-dog kind of evolution. We are more interested in what this means for the generalisation ability of co-evolutionary learning. <p> Page 78 University College, The University of New South Wales There has been much research on the 2IPD with evolutionary learning <ref> [7] </ref> [27] [37] [52] [54] [108]. However, few experimental studies have been carried out on the NIPD in spite of its importance and qualitative difference from the 2IPD, as described in Section 2.3.4. We merely use NIPD as another learning problem while we study the generalisation ability of co-evolutionary learning. <p> We look at different co-evolutionary learning environments, and see what difference this makes in different testing environments. 3.4.1 Genotypic Representation of NIPD Strategies Earlier in this chapter, we represented strategies the same way as Axelrod <ref> [7] </ref>, as described in Section 3.1.4. The representation scheme used in this section is different, and has these useful properties: * It scales well as the number of players N increases; * With more than 2 players it provides no more information than is necessary. <p> These three use different opponents to evaluate an individual's fitness. The three methods are: 1. Choosing from among the individuals in the GA population, i.e., normal co evolution of a single population like Axelrod's implementation <ref> [7] </ref>; 2. <p> This demonstrates again that adding to the round-robin does not improve generalisation ability, verifying Section 3.3.2. 3.5 Conclusions on Simple Co-Evolution In Section 3.2, in order to study generalisation in simple co-evolutionary learning, we implemented a co-evolutionary GA similar to Axelrod's <ref> [7, page 38] </ref> and Lindgren's [108]. We found that the strategies created can have such poor generalisation ability that spectacular collapses occur, as in Figure 3.6. This is caused when a random mutation (much less than a clever human) exploited the navety created in a coevolutionary population without diversity. <p> This chapter's work has a wider significance than it may first seem. The test problem used in this chapter is again the game of Iterated Prisoner's Dilemma (IPD), which we described in Section 2.3.3. IPD has been widely used as a test for evolutionary learning <ref> [7] </ref> [27] [37] [54] [108]. Although this test problem is merely a game, the results of this chapter have much wider relevance. Games of conflict are examples of a large body of important problems where each player's outcome depends on the actions of other similar players. <p> On the other hand, it has been argued [42, page 187] that using two populations encourages speedier progress. We would like intra-population competition, and we would also like to keep things simple. So we follow Axelrod <ref> [7] </ref> and use only a single GA: an individual's fitness is found by its performance against its own population. We also use one population because our test problem is symmetric. <p> We also use one population because our test problem is symmetric. One successful two-population approach to co-evolution had an asymmetric problem, with one population of sorting problems and another of sorting algorithms [82]. Genotype for Strategies Our representation scheme is similar to Axelrod's <ref> [7] </ref>, and the same as in Section 3.4.1. Each individual is a set of rules stored in a look-up table that covers every possible history, and is represented as a binary string. <p> The results are averaged over 30: from each of the 30 co-evolutionary GA runs with and without speciation, we use each of the following ways to extract expertise from a GA final population: * Best single individual from the final generation of a co-evolutionary GA with out speciation, like Axelrod <ref> [7] </ref> (best.ns). * A co-evolutionary GA using implicit fitness sharing with random sample selection, using the best individual from the final generation (best.sr), and the gating algorithm on the entire final generation (gate.sr). <p> This is the same as used by Axelrod <ref> [7] </ref>. Note that the modular approach works better than the best individual. <p> But here we take a broader view, returning to the questions originally asked in Section 1.3. 6.1 Generalisation Chapter 3 takes a canonical GA and uses a co-evolutionary evaluation function, a recipe used in many previous studies <ref> [7, page 38] </ref> [27] [37] [52] [54] [108]. Also, some studies of co-evolutionary learning have observed mass extinctions [58] [108] [132]. An explanation for mass extinctions in co-evolution is currently incomplete.
Reference: [8] <editor> Thomas Back, Frank Hoffmeister, and Hans-Paul Schwefel. </editor> <title> A survey of evolution strategies. </title> <editor> In Richard K. Belew and Lashon B. Booker, editors, </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pages 2-9. </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1991. </year>
Reference-contexts: After early attempts to emulate the learning process of evolution [53, pages 70-80], today evolutionary computation includes a range of approaches. The labels they go by include genetic algorithms (GA) [65] [86], evolutionary strategies (ES) <ref> [8] </ref>, evolutionary programming (EP) [182], and genetic programming (GP) [101]. This thesis concentrates on genetic algorithms, but is broadly applicable to similar algorithms. Some critics assert that artificial evolution cannot be intelligent, because natural evolution is a slow process [123, page 71].
Reference: [9] <author> David Beasley, David R. Bull, and Ralph R. Martin. </author> <title> A sequential niche technique for multimodal function optimization. </title> <journal> Evolutionary Computation, </journal> <volume> 1(2) </volume> <pages> 101-125, </pages> <year> 1993. </year>
Reference-contexts: In Sections 2.6.6, 2.6.7, and 2.6.8, we review GA speciation schemes that restrict competition (emulating allopatric speciation). 2.6.3 Re-Initialisation of a Canonical GA Iteration is the simplest technique that can be used with any optimisation method to produce multiple solutions <ref> [9, page 102] </ref>. If a canonical GA can only find one optimum, a simple solution is to run that GA several times (each from a different initial population) to find several different peaks. <p> However, their work was in the context of tracking a changing environment, and not finding multiple optima. The best study of re-initialising a GA to find multiple optima is that of Beasley et al. <ref> [9] </ref>. Each time the GA finds an optimum 17 , this optimum is stored off-line. The GA is then re-initialised to a new random population, and begins searching again. <p> Each time the GA finds an optimum 17 , this optimum is stored off-line. The GA is then re-initialised to a new random population, and begins searching again. To encourage it to find a different optimum each run, Beasley et al. <ref> [9, page 105] </ref> modify the fitness function by reducing fitness around each new optimum, after the GA has found that optimum. The intent is to encourage the GA to find a different optimum each time, to avoid wasteful repetition. <p> For example, if we were searching the space in Figure 2.9 and the GA found the left-hand peak, then we would reduce fitness around this optimum and re-start the GA on the modified space, shown in This approach of Beasley et al. <ref> [9] </ref> is interesting for co-evolutionary genetic algorithms, because of Theorem 2.3 on page 35: evolutionarily stable points cannot share a dimension. So if a co-evolutionary GA finds one evolutionarily stable point, a later run that finds a different evolutionarily stable point finds completely different strategies. <p> Australian Defence Force Academy Page 43 this space, and first found the left-hand peak, it would reduce the fitness around this optimum before re-starting the GA from a different random initial population. The modified space is shown in Figure 2.10. the iterated GA of Beasley et al. <ref> [9] </ref> reduces the fitness around this optimum. giving the modified space shown here. It then re-starts the GA from a different random initial population. Page 44 University College, The University of New South Wales Sikora and Shaw [158] use a similar scheme for generating classification rules. <p> However, the iterated method suffers a number of limitations. 1. Exactly how should we modify the fitness function, without prior knowledge of where nearby optima are? Beasley et al. provide a function to reduce fitness. It contains various parameters <ref> [9, page 107] </ref>. These include a niche radius which determines how large an area around each optimum gets modified. To correctly set this parameter requires prior knowledge of the inter-peak distance of the search space. This knowledge that can only be obtained by searching the space. <p> This makes the GA less likely to find those nearby optima. 5. Mahfoud [117, chapter 10] [116] compared Beasley et al.'s method <ref> [9] </ref> with other methods of finding multiple optima with a GA, discussed later in this section. Mahfoud found that on the test problems used, the other methods found all optima faster. <p> Therefore, this thesis will not use the re-initialisation approach of GA speciation. 2.6.4 Island Models and Landscape Models Beasley et al. <ref> [9] </ref> run a GA many times, one after the other, so that the previous run can communicate with the next run by reducing the fitness of known peaks. Australian Defence Force Academy Page 45 runs might mistake for true optima. <p> Reiko Tanese [163, page 438] A distributed GA with very low migration is equivalent to multiple runs of a canonical GA, so this method will give results similar to iterated schemes like Page 46 University College, The University of New South Wales Beasley et al. <ref> [9] </ref>, which are not as good as the parallel methods reviewed later in this section [116] [117, chapter 10]. A landscape or cellular GA is where each individual only interacts with a fraction of the entire GA population, instead of with all of them. <p> There is even an extension to this, known as "adaptive restricted tournament selection" [146], which attempts to improve on the previous scheme [78]. 2.6.7 Restricted Competition: Fitness Sharing Like Beasley et al. <ref> [9] </ref>, fitness sharing modifies a search landscape by reducing payoff in well-explored regions of the search space. It does this dynamically, during the course of a single run. This encourages search in unexplored regions, and causes subpopulations to form. <p> To set s requires a priori knowledge of how far apart optima are, and their (unshared) fitness. Without searching the space, this information is unknown. 18 In historical order, Beasley et al. <ref> [9] </ref> built on the work of Goldberg et al. [45] [67]. We present them in the opposite order to that in which they appeared, for ease of exposition. <p> There are some methods which at this time could be improved further to be used in this way. Sequential runs of a normal GA (like the scheme of Beasley et al. <ref> [9] </ref>) seems promising, as it should be able to find all possible high-quality strategies. However, Mahfoud [116] [117, chapter 10] has shown that certain parallel methods (such as fitness sharing) perform better. This suggests that the sequential scheme needs further refinement. <p> Island models can maintain different sub-populations only at very low migration rates [117, page 41]. Higher migration rates cause island models to behave like a normal GA. With very low migration, this method is equivalent to running completely different GA runs, equivalent to iterated schemes like Beasley et al. <ref> [9] </ref>. So although island models are useful at finding a single optimum faster [126] [163] [178], they have not yet been developed for finding multiple peaks.
Reference: [10] <author> Richard K. Belew. </author> <title> Evolution, learning and culture: Computational metaphors for adaptive search. </title> <journal> Complex Systems, </journal> <volume> 4(1) </volume> <pages> 11-49, </pages> <year> 1990. </year>
Reference-contexts: In these two studies, the "modularisation" is not driven by a process like GA speciation: it is done by cutting off pieces of individuals in a more ad hoc manner, and storing them off-line. This very similar to what are loosely called cultural algorithms <ref> [10] </ref> [60] [61] [94] [140] [141]: attributes of individuals are copied into an off-line cache, called a library or "belief space", which influences newer individuals. The belief space itself can undergo variation and selection. This implements some observations that culture is varied and transmitted in ways similar to genetic evolution [10] <p> <ref> [10] </ref> [60] [61] [94] [140] [141]: attributes of individuals are copied into an off-line cache, called a library or "belief space", which influences newer individuals. The belief space itself can undergo variation and selection. This implements some observations that culture is varied and transmitted in ways similar to genetic evolution [10] [15] [25] [43, chapter 11] [156] [119, chapter 16]. This applies not only for human culture, but for non-human cultural objects such as bird song [43, pages 189-190]. Cultural algorithms are very interesting in that they emulate the evolution of societies. <p> Each line is the average of 30 runs. Page 106 University College, The University of New South Wales Culture could play a critical role here in allowing adaptive specialisation without the genetic speciation that irreversibly partitions the population. Richard K. Belew <ref> [10] </ref> (His italics) A few studies have been done on cultural evolution [10] [60] [61] [94] [140] [141] and this innovative approach has performed well on certain problems. Nonetheless, more comparison studies are needed. New GA speciation methods may overcome the flaws of fitness sharing, including the one discovered here. <p> Page 106 University College, The University of New South Wales Culture could play a critical role here in allowing adaptive specialisation without the genetic speciation that irreversibly partitions the population. Richard K. Belew <ref> [10] </ref> (His italics) A few studies have been done on cultural evolution [10] [60] [61] [94] [140] [141] and this innovative approach has performed well on certain problems. Nonetheless, more comparison studies are needed. New GA speciation methods may overcome the flaws of fitness sharing, including the one discovered here.
Reference: [11] <author> Donald A. Berry and Bert Fristedt. </author> <title> Bandit Problems: Sequential Allocation of Experiments. </title> <publisher> Chapman and Hall, </publisher> <year> 1985. </year>
Reference-contexts: This phenomenon is called implicit parallelism. To work well, a search algorithm must balance between exploring new regions of the search space, and refining search in regions known to contain good solutions this balance between exploration and exploitation is studied with so-called 2-arm bandit models <ref> [11] </ref>. How well a GA balances these is an interesting topic [65, pages 36-38] [86, page 66] [111]. Section 4.3.2 will look more closely at this issue. A disadvantage: while a GA can rapidly find near-optimal solutions, other methods are better at going from near-optimal to optimal [71] [87]. <p> As mentioned in Section 2.2.2, it is desirable for search algorithms to balance the priorities of exploration and exploitation in a near-optimal manner [65, pages 36-38] [86, page 66]. The so-called "2-armed bandit problem" is a convenient way to analyse different algorithms <ref> [11] </ref> [111].
Reference: [12] <author> Uwe Beyer and Frank J. Smieja. </author> <title> Learning from examples, agent teams and the concept of reflection. </title> <type> Technical Report REFLEX report 1993/1, GMD report 776, </type> <institution> Institute for System Design Technology (GMD), Adaptive Systems group, </institution> <address> Sankt Augustin, Germany, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: There are many ways to adjudicate among a repertoire of different strategies. A gating algorithm is a generic term for an algorithm that adjudicates among different experts. Methods for choosing between different experts include: * Voting [76]; * Committees [110] [167] [168]; * Agent teams <ref> [12] </ref>; * Stacked generalisation [47] [180]; * Model averaging [23]; * Monte Carlo methods [129]; Page 58 University College, The University of New South Wales Later in this thesis, in Section 5.3.1, we will use the simple approach of voting among the repertoire of strategies.
Reference: [13] <author> Immanuel M. Bomze and Benedikt M. Potscher. </author> <title> Game Theoretical Foundations of Evolutionary Stability, </title> <booktitle> volume 324 of Lecture Notes in Economics and Methematical Systems. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: Readers more familiar with classical game theory should avoid confusion. Definition 2.3 (Evolutionarily stable) A point p es 2 S n , is evolutionarily stable if it satisfies these conditions [3, page 72] <ref> [13, page 27] </ref> [157, page 101] [188, page 473]: 1. Equilibrium condition:- n X a ij p es 2.
Reference: [14] <author> Justin A. Boyan. </author> <title> Modular neural networks for learning context-dependent game strategies. </title> <type> Master's thesis, </type> <institution> University of Cambridge, </institution> <month> August </month> <year> 1991. </year> <institution> Australian Defence Force Academy Page 171 </institution>
Reference-contexts: If we only consider neural networks, modular solutions have attempted such diverse problems as stock market prediction [98], vision [55] [91], speech recognition [75], learning backgammon <ref> [14] </ref>, and a six-legged walking robot [33]. Designing a modular system has relied on human expertise (often a committee) to manually divide a system into specialised parts. This relies on their opinions about the inherent structure of the problem, in an ad hoc manner.
Reference: [15] <author> Richard Boyd and Peter J. Richerson. </author> <title> Culture and the Evolutionary Process. </title> <publisher> The University of Chicago Press, </publisher> <year> 1985. </year>
Reference-contexts: These can then be separately adapted 1 . * When Orville and Wilbur Wright designed a flying machine, they did not attempt an all-inclusive approach, but instead decomposed the solution into separate parts to solve particular aspects of the problem: lift and drag, lateral stability, and propulsion [66] <ref> [117, pages 5, 15] </ref>. * The success of modular artificial neural networks at image processing [55] [91] and speech recognition [75] are further examples of how the modular approach can solve complicated problems. <p> The belief space itself can undergo variation and selection. This implements some observations that culture is varied and transmitted in ways similar to genetic evolution [10] <ref> [15] </ref> [25] [43, chapter 11] [156] [119, chapter 16]. This applies not only for human culture, but for non-human cultural objects such as bird song [43, pages 189-190]. Cultural algorithms are very interesting in that they emulate the evolution of societies.
Reference: [16] <author> Robert Boyd. </author> <title> Mistakes allow evolutionary stability in the repeated prisoner's dilemma game. </title> <journal> Journal of Theoretical Biology, </journal> <volume> 136 </volume> <pages> 47-56, </pages> <year> 1989. </year>
Reference-contexts: There exist more sophisticated ways to take into account the shadow of the future [6, page 13]. Section 2.4.5 will discuss "evolutionary stability". Iterated Prisoner's Dilemma does not allow evolutionary stability [17], unless mistakes can occur <ref> [16] </ref>. These two results also apply to the N -player game [184]. Unfortunately, as described in Australian Defence Force Academy Page 25 Section 2.4.6, evolutionary stability is not an accurate characterisation of stability (despite the name).
Reference: [17] <author> Robert Boyd and Jeffrey P. Lorberbaum. </author> <title> No pure strategy is evolutionarily stable in the repeated prisoner's dilemma game. </title> <journal> Nature, </journal> <volume> 327 </volume> <pages> 58-59, </pages> <month> 7 May </month> <year> 1987. </year>
Reference-contexts: More sophisticated strategies may learn this, and most human-designed strategies will utilise this knowledge if available. There exist more sophisticated ways to take into account the shadow of the future [6, page 13]. Section 2.4.5 will discuss "evolutionary stability". Iterated Prisoner's Dilemma does not allow evolutionary stability <ref> [17] </ref>, unless mistakes can occur [16]. These two results also apply to the N -player game [184]. Unfortunately, as described in Australian Defence Force Academy Page 25 Section 2.4.6, evolutionary stability is not an accurate characterisation of stability (despite the name). <p> So in a population of strategies resembling Tit-for-Tat, sometimes not retaliating can pay off. Tit-for-Two-Tats can invade a population of Tit-for-Tats <ref> [17] </ref> because it is sometimes desirable to cooperate after the first defection. This may be what happens in Figure 3.8. In any case, by generation 200 in Figure 3.8, most individuals had a 0 (cooperate) in bit 32.
Reference: [18] <author> Steven J. Brams. </author> <title> Superpower Games. </title> <publisher> Yale University Press, </publisher> <year> 1985. </year>
Reference-contexts: Although we use it merely as a test problem, IPD is a simplified version of many significant real-world problems, from recycling programs [63, page 62] to trench warfare [6, pages 73-87] and superpower confrontation <ref> [18] </ref>. Previous Work Axelrod [7] and others [27] [54] [108] have studied co-evolutionary learning of IPD, and co-evolution has been used to learn other games [46] [144] [160] and other tasks [81] [82] [99]. <p> The Prisoner's Dilemma is popular because it is a simple game, yet it captures the basics of many real-world situations [6]. These include such diverse situations as the Watergate scandal of 1972-1974 [128], the Cold War of 1945-1990 <ref> [18] </ref>, and life in the trenches during the First World War of 1914-1918 [6, pages 73-87]. T R P S Cooperate Defect Cooperate Defect game. In 2-player Iterated Prisoner's Dilemma (2IPD), the above interaction is repeated many times, and both players can remember previous outcomes.
Reference: [19] <author> Fred Brauer and John A. Nohel. </author> <title> Introduction to Differential Equations with Applications. </title> <publisher> Harper and Row, </publisher> <year> 1986. </year>
Reference-contexts: Some common phrases are: * Local stability: Roughly speaking, stability means that solutions starting close to an equilibrium point remain close to it in future <ref> [19, page 427] </ref>. * Global stability: Solution paths need not begin close to the equilibrium point, but can begin anywhere in the space. * Asymptotic stability: If solution paths that begin near enough to an equilibrium point move towards it asymptotically, approaching it in the limit as time tends to infinity,
Reference: [20] <author> Martin Braun. </author> <title> Differential Equations and Their Applications. </title> <publisher> Springer-Verlag, </publisher> <address> fourth edition, </address> <year> 1993. </year>
Reference-contexts: There is actually a close relationship between the two [3, page 81]. These equations appear in related fields, such as natural selection in a diploid population [3, pages 21, 64], and in the n-dimensional continuous Logistic Equation of predator-prey models <ref> [20, page 28] </ref>. 2.2.4 A Special Replicator Equation: the Schema Theorem A "fundamental theorem" for an evolutionary system is one of several similar theorems that share the prediction that the population tends to become more fit with passing time | or, more strictly speaking, that on average fitness improves or stays <p> Allopatric : Speciation by specialisation, among populations with overlapping ranges. Specialising to different food sources restricts competition. For example, on the Black Sea island of Jorilgatch, there are four similar species of tern, which obtain food in different ways <ref> [20, page 451] </ref>. Originally, there was only one species of tern, but specialisation to different food sources has restricted inter-species competition, and gradually created four different species. <p> Under allopatric speciation, mating restrictions are a consequence of speciation and not a primary cause [117, page 47]. Once started, allopatric speciation is accentuated by the Principle of Competitive Exclusion <ref> [20, Section 4.11] </ref> which asserts that no two species that extract resources in the same manner can both survive indefinitely. <p> Equation A.3 appears in related fields, such as natural selection in a diploid population (minus mutation and recombination) [3, pages 21, 64], and in the n dimensional continuous Logistic Equation of predator-prey models <ref> [20, page 28] </ref>.
Reference: [21] <author> Rodney A. Brooks. </author> <title> A robot that walks: Emergent behaviours from a carefully evolved network. </title> <editor> In Randall D. Beer, Roy E. Ritzmann, and Thomas McKenna, editors, </editor> <booktitle> Biological Neural Networks in Invertebrate Neuroethology and Robotics, Neural Networks: Foundations to Applications. </booktitle> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1993. </year>
Reference-contexts: It also relies on the availability of prior knowledge. In some problems, the inherent structure seems clear. For example, consider a six-legged robot learning to walk. One module per leg seems a sensible way to divide the solution, and it gives good performance <ref> [21] </ref> [33]. However, a system for predicting electricity demand uses modules to separately process the hourly, daily, and weekly demand [97].
Reference: [22] <author> Seth G. Bullock. </author> <title> Co-evolutionary design: Impliciations for evolutionary robotics. </title> <type> Technical Report CSRP-384, </type> <institution> School of Cognitive and Computing Sciences, The University of Sussex, </institution> <address> Brighton, </address> <year> 1995. </year>
Reference-contexts: is where each individual is evaluated by how well it performs against the current members of the continually evolving population (or perhaps a separate population): rather than work towards a solution of some fixed problem, the population adapts (over evolutionary time) to their surroundings, i.e., themselves, as they continually improve <ref> [22] </ref>. Emulating co-evolution is an attractive approach for learning a number of tasks, such as strategy acquisition for games of conflict. Section 2.4 covers some relevant mathematical analysis of co-evolution.
Reference: [23] <author> Wray L. Buntine and Andreas S. Weigend. </author> <title> Bayesian back-propagation. </title> <journal> Complex Systems, </journal> <volume> 5(6) </volume> <pages> 603-643, </pages> <year> 1991. </year>
Reference-contexts: A gating algorithm is a generic term for an algorithm that adjudicates among different experts. Methods for choosing between different experts include: * Voting [76]; * Committees [110] [167] [168]; * Agent teams [12]; * Stacked generalisation [47] [180]; * Model averaging <ref> [23] </ref>; * Monte Carlo methods [129]; Page 58 University College, The University of New South Wales Later in this thesis, in Section 5.3.1, we will use the simple approach of voting among the repertoire of strategies.
Reference: [24] <author> C. Cannings. </author> <title> Topics in the theory of evolutionarily stable strategies. </title> <editor> In Sabin Lessard, editor, </editor> <booktitle> Mathematical and Statistical Developments of Evolutionary Theory: Proceedings of the NATO Advanced Study Institute and Seminaire de Mathematiques Superieures on Mathematical and Statistical Developments of Evolutionary Theory, </booktitle> <pages> pages 95-119. </pages> <publisher> Kluwer Academic, </publisher> <month> August </month> <year> 1987. </year>
Reference-contexts: Equation 2.13 can be written in an alternative form that includes C, a positive constant 7 corresponding to the fitness without interaction <ref> [24, page 104] </ref> [85, page 133] [145, page 271] [157, page 92]: p i (t + 1) = p i (t) j a ij p j (t) + C (2.14) Equation 2.13 and its continuous counterpart, Equation A.5, are closed in the simplex S n : if a point starts on <p> Page 30 University College, The University of New South Wales Incidentally, the constant C in Equation 2.14 does not affect the position of the equilibrium points [145, page 272], although it can affect their stability <ref> [24, page 105] </ref> [145, page 271]. <p> The term "locally asymptotically stable" is defined in Section D.1 on page 159. No similar result holds for the discrete system of Equation 2.14 | an ES point is not necessarily an attractor for the discrete system <ref> [24, page 104] </ref> [32, page 18]. And even for the continuous case, the question "Are there other dynamically stable equilibria?" has no clear answer [32, page 16]. In this section, we describe this and other problems with evolutionary "stability". <p> The GA is a discrete system, not a continuous one. The discrete system (Equation 2.14) is even less well-behaved. In contrast to Theorem 2.4, we can't even say that an evolutionarily stable point is an attractor for the discrete Equation 2.13 <ref> [24, page 104] </ref> [32, page 18]. <p> Also, there are examples where changing the value of the constant C in Equation 2.14 11 changes an equilibrium point from an attractor to a an unstable equilibrium, while satisfying the definition of "evolutionarily stable" for both values of C <ref> [24, page 105] </ref> [145, page 271]. Some researchers suspect that evolutionary stability cannot be modified to accurately characterise stability for the discrete system, and that one should only use linearisation to test each and every equilibrium [32, page 19] [145]. Linearisation is cumbersome, but accurate. <p> It gets closer, then further away, but on the whole it gets closer. D.2 Finding Stability by Linearisation Recall the discrete replicator equation from Equation 2.14 on page 30 <ref> [24, page 104] </ref> [85, page 133] [145, page 271] [157, page 92]: p i (t + 1) = p i (t) j a ij p j (t) + C Linearisation adds a perturbation vector q to the equilibrium point in question p eq .
Reference: [25] <author> Luigi L. Cavalli-Sforza and Marcus W. Feldman. </author> <title> Cultural Transmission and Evolution: A Quantitative Approach, volume 16 of Monographs in Population Biology. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1981. </year>
Reference-contexts: Replicator equations describe the populations of entities that reproduce. As well as evolution, they explain epidemics <ref> [25, page 33] </ref>, ecosystems [157], compound interest [3, page 20], population genetics, prebiotic evolution (chemical "hypercycles"), animal behaviour, population ecology, and evolutionary game theory [157]. There are two approaches to replicator equations, continuous and discrete. These are closely related [3, page 81]. <p> The belief space itself can undergo variation and selection. This implements some observations that culture is varied and transmitted in ways similar to genetic evolution [10] [15] <ref> [25] </ref> [43, chapter 11] [156] [119, chapter 16]. This applies not only for human culture, but for non-human cultural objects such as bird song [43, pages 189-190]. Cultural algorithms are very interesting in that they emulate the evolution of societies.
Reference: [26] <author> Uday Kumar Chakraborty and D. Ghosh Dastidar. </author> <title> Using reliability analysis to estimate the number of generations to convergence in genetic algorithms. </title> <journal> Information Processing Letters, </journal> <volume> 46 </volume> <pages> 199-209, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: For a canonical GA that represents individuals as fixed-length binary strings, the time taken for convergence can be estimated <ref> [26] </ref>. Equation 2.8 only applies to the canonical GA, representing individuals as binary strings of fixed length.
Reference: [27] <author> David M. </author> <title> Chess. Simulating the evolution of behavior: the iterated prisoners' dilemma problem. </title> <journal> Complex Systems, </journal> <volume> 2 </volume> <pages> 663-670, </pages> <year> 1988. </year>
Reference-contexts: Although we use it merely as a test problem, IPD is a simplified version of many significant real-world problems, from recycling programs [63, page 62] to trench warfare [6, pages 73-87] and superpower confrontation [18]. Previous Work Axelrod [7] and others <ref> [27] </ref> [54] [108] have studied co-evolutionary learning of IPD, and co-evolution has been used to learn other games [46] [144] [160] and other tasks [81] [82] [99]. <p> Different search problems are best handled by different search algorithms. As a result, the only way to justify using a particular search algorithm is to argue that it suits the features of the space to be searched <ref> [181, pages 25, 27] </ref>. If, as is often the case, one lacks prior knowledge of the features of the search space, then the next best solution is to use a search algorithm good for problems of that general type [181, page 28]. <p> The iterated game has many real-life applications [6], yet is a simple and non-trivial game, with few inputs (only the memory of recent iterations) and a single binary output (cooperate or defect) <ref> [27, page 665] </ref>. Equation 2.10 prevents IPD from degenerating into an alternating pattern where the two players take turns at cooperating and defecting. Creating strategies for IPD is a sequential decision problem with delayed payoff, as described in Section 2.3.2. <p> So these analytical results do not give definite answers about the behaviour of a co-evolving GA learning IPD. We will study a co-evolving GA learning IPD in more detail in Chapter 3. There has been much recent research on evolutionary learning in the 2-player Iterated Prisoner's Dilemma (2IPD) [7] <ref> [27] </ref> [37] [52] [54] [108]. One of the more effective strategies for IPD is "Tit-for-Tat" [6]. This simple strategy cooperates at the first iteration, and after that does whatever the opponent did in the previous iteration. That is, if the opponent defected on the previous iteration, then Tit-for-tat will defect. <p> This study also demonstrated the evolution of cooperation, showing it was not some artifact of the system, but that co-evolution really can create good strategies. A study by Chess <ref> [27] </ref> also saw similar behaviour when an individual plays against a single random opponent, instead of the entire population. Thus, co-evolution appears to be a robust creator of high-payoff strategies. Lindgren [108] also did an Axelrod-style study on IPD. <p> Axelrod [6] investigated evolution and the IPD. He used a GA population of strategies, in which each plays IPD with every other strategy in the population [7, page 38]. He found that this dynamic environment produced high scorers. Similar studies give similar results <ref> [27] </ref> [37] [52] [54] [108]. This raises a serious question: can a strategy produced by co-evolving in its own population succeed against strategies not in its own population? That is, how well does co-evolutionary learning generalise? Generalisation is an important aspect of machine learning. <p> Page 78 University College, The University of New South Wales There has been much research on the 2IPD with evolutionary learning [7] <ref> [27] </ref> [37] [52] [54] [108]. However, few experimental studies have been carried out on the NIPD in spite of its importance and qualitative difference from the 2IPD, as described in Section 2.3.4. We merely use NIPD as another learning problem while we study the generalisation ability of co-evolutionary learning. <p> This chapter's work has a wider significance than it may first seem. The test problem used in this chapter is again the game of Iterated Prisoner's Dilemma (IPD), which we described in Section 2.3.3. IPD has been widely used as a test for evolutionary learning [7] <ref> [27] </ref> [37] [54] [108]. Although this test problem is merely a game, the results of this chapter have much wider relevance. Games of conflict are examples of a large body of important problems where each player's outcome depends on the actions of other similar players. <p> But here we take a broader view, returning to the questions originally asked in Section 1.3. 6.1 Generalisation Chapter 3 takes a canonical GA and uses a co-evolutionary evaluation function, a recipe used in many previous studies [7, page 38] <ref> [27] </ref> [37] [52] [54] [108]. Also, some studies of co-evolutionary learning have observed mass extinctions [58] [108] [132]. An explanation for mass extinctions in co-evolution is currently incomplete. Section 3.2 found that in a canonical GA with co-evolution, genetic drift to a single strategy caused over-specialisation to that single strategy.
Reference: [28] <author> Sung-Bae Cho and Katsunori Shimohara. </author> <title> Modular neural networks evolved by genetic programming. </title> <booktitle> In Proceedings of the 1996 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pages 681-686. </pages> <publisher> IEEE Press, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: This approach is known as "genetic programming" [101] [102] [103]. Sub-branches of these individual parse trees can be crossed over, to emulate genetic recombination. Australian Defence Force Academy Page 57 There have been variations on this theme [89], including representing neural networks as tree-like structures <ref> [28] </ref>. In these two studies, the "modularisation" is not driven by a process like GA speciation: it is done by cutting off pieces of individuals in a more ad hoc manner, and storing them off-line.
Reference: [29] <author> Helen G. Cobb and John J. Grefenstette. </author> <title> Genetic algorithms for tracking changing environments. </title> <booktitle> In Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <pages> pages 523-530. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Several schemes re-initialise the GA population with the aim of refining the current best solution, not to find multiple optima [49] [118] [177]. Cobb and Grefenstette <ref> [29] </ref> did something similar: instead of completely re-initialising the whole population with random individuals, they re-initialised only a part of it. However, their work was in the context of tracking a changing environment, and not finding multiple optima. <p> There are GA systems which dynamically change the mutation rate <ref> [29] </ref> and the 2 The Hamming distance between binary strings is not always the best way to quantify the distance between GA individuals, or even for many uses of binary strings. We use the Hamming metric because our particular representation is well-suited. <p> This improvement, plus the flaws of speciation methods described in Sections 4.2 and 5.2, shows the need (and the opportunity) for improved GA speciation methods. For example, just as there are GA systems which dynamically change the mutation rate <ref> [29] </ref> and the population size [5], a future study might improve implicit sharing by dynamically changing the sample size in tune with the diversity of the current GA population. 5.3.5 Future Work on Automatic Modularisation The method presented here is a very simple implementation of a modular approach.
Reference: [30] <author> Edwin H. Colbert. </author> <title> Evolution of the Vertebrates. </title> <publisher> John Wiley and Sons, </publisher> <address> third edition, </address> <year> 1980. </year>
Reference-contexts: There were sharks in late Devonian times [345 million years ago <ref> [30, page 10] </ref>, a full 120 million years before the age of the dinosaurs began] and there are sharks today, and through the intervening geologic periods, ..., sharks have lived in the oceans of the world. <p> In spite of competition from the bony fishes, from aquatic reptiles such as icthyosaurs, and from aquatic mammals such as the whales, the sharks have carried on in a most successful way. Edwin H. Colbert <ref> [30, page 50] </ref> Unfortunately, shark evolution is not co-evolution. Almost everything that sharks ate when they first appeared has since become extinct | they are optimised for speed and victim detection. Page 28 University College, The University of New South Wales where the rules do change from time to time. <p> The biological definition of a species is a distinct population of individuals which can breed freely with each other, but cannot successfully interbreed with another group <ref> [30, page 12] </ref>. Natural species emerge in at least two ways, allopatric speciation [85, page 30] and sympatric speciation [85, page 31]: Sympatric : Speciation by restricted mating, caused by geographic barriers, migration, or similar causes.
Reference: [31] <author> Andrew M. Colman. </author> <title> Game Theory and Experimental Games: the study of strategic interaction, volume 4 of International series in experimental social psychology. </title> <publisher> Pergamon Press, Oxford, </publisher> <year> 1982. </year>
Reference-contexts: A non-cooperative game is one where no preplay communication is permitted between the players <ref> [31] </ref> [139]. The Prisoner's Dilemma game is a 2 fi 2 non-zerosum non-cooperative game. The 2-player Prisoner's Dilemma has been widely studied in such diverse fields as economics, mathematical game theory, political science, and artificial intelligence. <p> This encourages cooperation from the other player, and thus a high score. 2.3.4 N -player Iterated Prisoner's Dilemma While the 2-player Iterated Prisoner's Dilemma (2IPD) has been studied extensively for several decades, there are many real-world problems, especially many social and economic ones, which cannot be modelled by the 2IPD <ref> [31, pages 156 - 159] </ref>. The classic example is the so-called "tragedy of the commons" [77]. Imagine six farmers with access to a common pasture where they graze their cows. Each farmer owns a cow weighing 1000 pounds. <p> Imagine six farmers with access to a common pasture where they graze their cows. Each farmer owns a cow weighing 1000 pounds. This pasture can support six cows, but each additional cow causes overgrazing and reduces the weight of every cow by 100 pounds. The N -player dilemma is <ref> [31, page 159] </ref>: * If one farmer decides to add an extra cow, then he gets a pair of 900 pound cows, instead of just one 1000 pound cow. <p> Other N -player dilemmas include whether or not to conserve water (or energy, or any other scarce resource) during a shortage <ref> [31, page 158] </ref>, whether or not to join a trade union, or alternatively whether or not to go on strike for a wage increase that exceeds the rate of inflation, whether or not to rush for a fire escape in a crowded theatre, and many others [31, page 158]. <p> resource) during a shortage <ref> [31, page 158] </ref>, whether or not to join a trade union, or alternatively whether or not to go on strike for a wage increase that exceeds the rate of inflation, whether or not to rush for a fire escape in a crowded theatre, and many others [31, page 158]. The N -player Iterated Prisoner's Dilemma (NIPD) is a more realistic and general game which can model those problems. In comparison with the 2IPD: The N -player case has greater generality and applicability to real-life situations. <p> In comparison with the 2IPD: The N -player case has greater generality and applicability to real-life situations. In addition to the problems of energy conservation, ecology, and overpopulation, many other real-life problems can be represented by the (NIPD) paradigm. Davis et al. [40, page 520] Colman <ref> [31, page 142] </ref> and Glance and Huberman [62] [63] have also indicated that the NIPD is "qualitatively different" from the 2IPD and that "... certain strategies that work well for individuals in the Prisoner's Dilemma fail in large groups." Page 26 University College, The University of New South Wales The N <p> the NIPD is "qualitatively different" from the 2IPD and that "... certain strategies that work well for individuals in the Prisoner's Dilemma fail in large groups." Page 26 University College, The University of New South Wales The N -player Prisoner's Dilemma game can be defined by the following three properties <ref> [31, page 159] </ref>: 1. Each player faces two choices between cooperation (C) and defection (D); 2. The D option is dominant for each player, i.e., each is better off choosing D than C no matter how many of the other players choose C; 3.
Reference: [32] <author> Ross Cressman. </author> <title> The Stability Concept of Evolutionary Game Theory, </title> <booktitle> volume 94 of Lecture Notes in Biomathematics. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1992. </year>
Reference-contexts: In the discrete case, x i is a non-negative integer. In the continuous case, we assume x i 2 IR + , even though this allows unrealistic values such as 5:5 copies of I i . To avoid this, we assume that populations are large [3, page 20] <ref> [32, page 5] </ref>. * The proportion of the whole population that is type I i is p i , where 0 p i 1: p i = P n (2.2) * The population vector p = (p 1 ; p 2 ; : : : ; p n ) is a <p> the whole population that is type I i is p i , where 0 p i 1: p i = P n (2.2) * The population vector p = (p 1 ; p 2 ; : : : ; p n ) is a point in the space or simplex <ref> [32, page 4] </ref> [83, page 232] [85, page 13] denoted by S n , defined in Equation 2.3 below. Figure 2.2 shows the three-dimensional simplex S 3 . S n = p 2 IR n : p i 0 and i ) Vector p represents a distribution of individuals. <p> This section reviews the mathematical analysis of co-evolution. Unfortunately, current knowledge lacks many answers which would be of interest to co-evolutionary learning. Even the central concept of evolutionary stability is Australian Defence Force Academy Page 27 controversial <ref> [32] </ref> [145]. As a result, the main part of this thesis does not attempt an all-inclusive mathematical framework. <p> Australian Defence Force Academy Page 29 Equation 2.12 says an individual interacts with all individuals, including itself. If this is not the case, there are finite-population effects. We ignore these effects by assuming the population is very large, i.e., effectively infinite <ref> [32, page 5] </ref>. <p> Equation 2.25 simply extends Equation 2.21 to a set of points, instead of just a single point <ref> [32, page 98] </ref>. An interesting result of evolutionary stability is: Theorem 2.3 ([32, page 14] [188, page 476]) No ES equilibrium shares dimensions with any other ES equilibrium. A similar result applies to ES sets [32, page 99]. <p> An interesting result of evolutionary stability is: Theorem 2.3 ([32, page 14] [188, page 476]) No ES equilibrium shares dimensions with any other ES equilibrium. A similar result applies to ES sets <ref> [32, page 99] </ref>. For example, out of the n possible information patterns, imagine that an ES equilibrium exists with p 1 and p 2 &gt; 0, with all the others extinct. Then every other ES equilibrium must have p 1 = p 2 = 0. <p> Australian Defence Force Academy Page 35 2.4.6 Critique of Evolutionary Stability Since the appearance of the definition of evolutionary stability (Definition 2.3), its validity as a predictor of stability in evolution has been questioned <ref> [32, page 16] </ref>. The motivation for evolutionary stability (in Definition 2.3) is that an interior ES point asymptotically attracts the motion of the continuous Equation A.5 [32, pages 14-15]. <p> The motivation for evolutionary stability (in Definition 2.3) is that an interior ES point asymptotically attracts the motion of the continuous Equation A.5 <ref> [32, pages 14-15] </ref>. That is, small perturbations at such a point will not cause evolution to move elsewhere, but it will spiral back to the evolutionarily stable point. So it gives us a handle on when a co-evolving GA will stop, having found a "best" solution in some sense. <p> The term "locally asymptotically stable" is defined in Section D.1 on page 159. No similar result holds for the discrete system of Equation 2.14 | an ES point is not necessarily an attractor for the discrete system [24, page 104] <ref> [32, page 18] </ref>. And even for the continuous case, the question "Are there other dynamically stable equilibria?" has no clear answer [32, page 16]. In this section, we describe this and other problems with evolutionary "stability". The continuous case is closely related to the discrete Equation 2.13 [3, page 81]. <p> No similar result holds for the discrete system of Equation 2.14 | an ES point is not necessarily an attractor for the discrete system [24, page 104] [32, page 18]. And even for the continuous case, the question "Are there other dynamically stable equilibria?" has no clear answer <ref> [32, page 16] </ref>. In this section, we describe this and other problems with evolutionary "stability". The continuous case is closely related to the discrete Equation 2.13 [3, page 81]. The discrete system often, but not always, behaves similarly to the continuous system [3, page 82]. <p> The discrete system often, but not always, behaves similarly to the continuous system [3, page 82]. We first consider the behaviour of the continuous system of Equation A.5. For the continuous case of Equation A.5, examples abound of non-ES equilibria that are still local attractors <ref> [32, page 16] </ref> | an evolutionarily stable point is an attractor, but not every attractor is an evolutionarily stable point [172, page 29] [188, page 474]. <p> The GA is a discrete system, not a continuous one. The discrete system (Equation 2.14) is even less well-behaved. In contrast to Theorem 2.4, we can't even say that an evolutionarily stable point is an attractor for the discrete Equation 2.13 [24, page 104] <ref> [32, page 18] </ref>. Also, there are examples where changing the value of the constant C in Equation 2.14 11 changes an equilibrium point from an attractor to a an unstable equilibrium, while satisfying the definition of "evolutionarily stable" for both values of C [24, page 105] [145, page 271]. <p> Some researchers suspect that evolutionary stability cannot be modified to accurately characterise stability for the discrete system, and that one should only use linearisation to test each and every equilibrium <ref> [32, page 19] </ref> [145]. Linearisation is cumbersome, but accurate. Although evolutionary stability is an intuitive concept, with interesting results 11 Or, equivalently, changing all the values of the a ij by the same amount. <p> In Figure 3.8, imagine "horn size" on the y-axis to see a similar drama in IPD. With hindsight, this sort of event has been predicted by theorists. For example, Cressman <ref> [32] </ref> shows that if a population consists of more than 50% Tit-for-Tat strategies, then all individuals will eventually use Tit-for-Tat or all-cooperate, and remain there as long as a sufficient number of Tit-for-Tats are maintained [32, pages 121-122]. <p> With hindsight, this sort of event has been predicted by theorists. For example, Cressman [32] shows that if a population consists of more than 50% Tit-for-Tat strategies, then all individuals will eventually use Tit-for-Tat or all-cooperate, and remain there as long as a sufficient number of Tit-for-Tats are maintained <ref> [32, pages 121-122] </ref>. He did not specifically state the obverse | if the Tit-for-Tats are not maintained, then cooperation need not dominate forever. In a co-evolutionary GA, there is no selection for Tit-for-Tats if everyone cooperates. This is what lets collapses happen in Figures 3.6 and 3.7. <p> eigenvalues (including this one) must have modulus less than one for the equilibrium to be locally asymptotically stable [145, page 284]. * If some eigenvalues of B have zero real part, then one must examine higher-order terms than B to determine if the equilibrium point is locally asymptot ically stable <ref> [32, page 20] </ref>. There can be more complicated situations than local asymptotic stability, which can be interpreted from the eigenvalues from normal qualitative equilibrium analysis. Australian Defence Force Academy Page 161 These include non-point attracting sets. Linearisation allows an accurate insight into the nature of each equilibrium point.
Reference: [33] <author> H. Cruse, C. Bartling, G. Cymbalyuk, J. Dean, and M. Dreifert. </author> <title> A modular artificial neural net for controlling a six-legged walking system. </title> <journal> Biological Cybernetics, </journal> <volume> 72(5) </volume> <pages> 421-430, </pages> <year> 1995. </year> <institution> Page 172 University College, The University of New South Wales </institution>
Reference-contexts: It also relies on the availability of prior knowledge. In some problems, the inherent structure seems clear. For example, consider a six-legged robot learning to walk. One module per leg seems a sensible way to divide the solution, and it gives good performance [21] <ref> [33] </ref>. However, a system for predicting electricity demand uses modules to separately process the hourly, daily, and weekly demand [97]. Is this the best division? If not, what is better? Also, manual modularisation becomes even more ad hoc on "knowledge-lean" or "black box" problems, for which existing expertise is unavailable. <p> If we only consider neural networks, modular solutions have attempted such diverse problems as stock market prediction [98], vision [55] [91], speech recognition [75], learning backgammon [14], and a six-legged walking robot <ref> [33] </ref>. Designing a modular system has relied on human expertise (often a committee) to manually divide a system into specialised parts. This relies on their opinions about the inherent structure of the problem, in an ad hoc manner.
Reference: [34] <author> Paul J. Darwen. </author> <title> Metapopulation persistence in cellular automata population models. </title> <type> Hon--ours thesis, </type> <institution> Australian National University, Canberra, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Only geographic neighbours interact: they compete with and cross over with each other. their geographic neighbours. This approach is motivated by a conjecture in landscape biology known as the Metapopulation Hypothesis <ref> [34] </ref> [164]. This says that migration between local populations promotes the persistence of a regional "metapopulation" despite short-term fluctuations, or even temporary extinctions, on the local scale. This hypothesis forms the basis of certain population models [35] [80].
Reference: [35] <author> Paul J. Darwen and David G. Green. </author> <title> Viability of populations in a landscape. Ecological Modelling, </title> <address> 85(2-3):165-171, </address> <year> 1996. </year>
Reference-contexts: This says that migration between local populations promotes the persistence of a regional "metapopulation" despite short-term fluctuations, or even temporary extinctions, on the local scale. This hypothesis forms the basis of certain population models <ref> [35] </ref> [80]. Since spatial distribution maintains diversity, emulating this may allow a GA to find multiple optima. Another motivation is that since only near neighbours cross over, this reduces crossover disruption [39, page 261].
Reference: [36] <author> Paul J. Darwen and Xin Yao. </author> <title> A dilemma for fitness sharing with a scaling function. </title> <booktitle> In Proceedings of the 1995 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pages 166-171. </pages> <publisher> IEEE Press, </publisher> <month> 29 November - 1 December </month> <year> 1995. </year>
Reference-contexts: It is this process that we hope to emulate. Although the two most popular GA speciation methods have been successful on various problems [67] [159], this thesis contributes to the knowledge of their flaws, and demonstrates some practical improvements <ref> [36] </ref> [38]. Using the most suitable of these GA speciation methods, in Chapter 5 we present a co-evolutionary learning system that uses speciation as automatic modularisation. 1.3 Thesis Overview This thesis will explore the following questions. <p> methods, but conspicuously forbids the fitness sharing method from using a scaling function [117, pages 206-207] | this may not be a fair comparison, as fitness sharing worked well with scaling [67] (although Section 4.2 shows how scaling can cause problems for fitness Australian Defence Force Academy Page 7 sharing <ref> [36] </ref>). <p> This is illustrated in Figure 2.11. Later GA runs may find these artifacts of the method, even though they are not real optima. Our own work has found similar effects for the speciation method of fitness sharing <ref> [36] </ref>. We describe these effects in Section 4.2 of this thesis. 3. <p> This suggests we cannot yet use the restricted-mating approach to find diverse high-payoff solutions. However, mating restrictions often improve the performance of other GA spe-ciation methods [45, page 49] <ref> [36, page 168] </ref> [121, Section 5.3]. This happens by avoiding the creation of lethal individuals by crossing over individuals with very different attributes [117, page 83], although this may be desirable in some search problems [113, section 5] [117, page 50]. <p> comparing deterministic crowding with fitness sharing (to be described in Section 2.6.7), that study conspicuously forbids fitness sharing from using a scaling function [117, pages 206-207] | this may not be a fair comparison, as fitness sharing worked well with scaling [67], although scaling can cause problems for that method <ref> [36] </ref>. Another objection is that deterministic crowding was seen to have "failed to maintain certain sought-out optima when those could recombine to form more fit optima in the search space" [78, page 25]. A speciation method similar to Mahfoud's deterministic crowding is "restricted tournament selection" [78]. <p> The payoffs in Figure 5.1 give mass extinctions just as in Section 3.2.2. As described in Section 2.6.7, fitness sharing [45] is a popular GA speciation method which has achieved good results in difficult search problems [67]. However, fitness sharing is not without flaws <ref> [36] </ref> [159], including the one described in Section 4.2. Because of these flaws, as well as the results in Section 4.4, this chapter uses an extension to the original fitness sharing, known as implicit sharing [56] [159], which was described in Section 2.6.8.
Reference: [37] <author> Paul J. Darwen and Xin Yao. </author> <title> On evolving robust strategies for iterated prisoner's dilemma. </title> <editor> In Xin Yao, editor, </editor> <booktitle> Progress in Evolutionary Computation, volume 956 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 276-292. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: We will study a co-evolving GA learning IPD in more detail in Chapter 3. There has been much recent research on evolutionary learning in the 2-player Iterated Prisoner's Dilemma (2IPD) [7] [27] <ref> [37] </ref> [52] [54] [108]. One of the more effective strategies for IPD is "Tit-for-Tat" [6]. This simple strategy cooperates at the first iteration, and after that does whatever the opponent did in the previous iteration. That is, if the opponent defected on the previous iteration, then Tit-for-tat will defect. <p> Another study, of co-evolving stock traders on an artificial stock market [132], also showed mass extinctions, where high-volume trading is interrupted by bubbles and crashes. We will examine mass extinctions in Section 3.2.2 of this thesis, building on previous work <ref> [37] </ref>. 2.5.3 Two Populations Instead of evaluating an individual by its performance against individuals in the same population, some studies have evaluated an individual by its performance against the individuals in a different population. We briefly describe the mathematics of 2 co-evolving populations in Appendix C. <p> Axelrod [6] investigated evolution and the IPD. He used a GA population of strategies, in which each plays IPD with every other strategy in the population [7, page 38]. He found that this dynamic environment produced high scorers. Similar studies give similar results [27] <ref> [37] </ref> [52] [54] [108]. This raises a serious question: can a strategy produced by co-evolving in its own population succeed against strategies not in its own population? That is, how well does co-evolutionary learning generalise? Generalisation is an important aspect of machine learning. <p> Page 78 University College, The University of New South Wales There has been much research on the 2IPD with evolutionary learning [7] [27] <ref> [37] </ref> [52] [54] [108]. However, few experimental studies have been carried out on the NIPD in spite of its importance and qualitative difference from the 2IPD, as described in Section 2.3.4. We merely use NIPD as another learning problem while we study the generalisation ability of co-evolutionary learning. <p> This chapter's work has a wider significance than it may first seem. The test problem used in this chapter is again the game of Iterated Prisoner's Dilemma (IPD), which we described in Section 2.3.3. IPD has been widely used as a test for evolutionary learning [7] [27] <ref> [37] </ref> [54] [108]. Although this test problem is merely a game, the results of this chapter have much wider relevance. Games of conflict are examples of a large body of important problems where each player's outcome depends on the actions of other similar players. <p> But here we take a broader view, returning to the questions originally asked in Section 1.3. 6.1 Generalisation Chapter 3 takes a canonical GA and uses a co-evolutionary evaluation function, a recipe used in many previous studies [7, page 38] [27] <ref> [37] </ref> [52] [54] [108]. Also, some studies of co-evolutionary learning have observed mass extinctions [58] [108] [132]. An explanation for mass extinctions in co-evolution is currently incomplete. Section 3.2 found that in a canonical GA with co-evolution, genetic drift to a single strategy caused over-specialisation to that single strategy.
Reference: [38] <author> Paul J. Darwen and Xin Yao. </author> <title> Automatic modularization with speciation. </title> <booktitle> In Proceedings of the 1996 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pages 88-93. </pages> <publisher> IEEE Press, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: It is this process that we hope to emulate. Although the two most popular GA speciation methods have been successful on various problems [67] [159], this thesis contributes to the knowledge of their flaws, and demonstrates some practical improvements [36] <ref> [38] </ref>. Using the most suitable of these GA speciation methods, in Chapter 5 we present a co-evolutionary learning system that uses speciation as automatic modularisation. 1.3 Thesis Overview This thesis will explore the following questions. We will conduct a series of experimental studies, supported by mathematical analysis where tractable. 1. <p> Again, much of the motivation for improved GA speciation comes from engineering applications [134] [143] [175]. Previous Work Implicit sharing [56] [159] originally modeled the immune system. It has also been used for learning game strategies with co-evolution <ref> [38] </ref> [144]. One previous study [144] examined sampling in implicit sharing. This Thesis Extends Knowledge In Sections 5.2.1 through 5.2.4 we discuss certain pitfalls that may trap an unsuspecting user, which occur when using implicit sharing in a co-evolutionary GA. <p> can greatly delay genetic drift [114] [117, Section 8.2], and prevent the convergence and overspecialisation that can otherwise happen in co-evolutionary learning (found in Chapter 3) However, if the speciated GA misses some good strategies, then we get poor generalisation | as speciation finds more high-quality strategies, generalisation ability improves <ref> [38, page 92] </ref>. So knowing what GA speciation method gives a good coverage of the interesting optima is important for strategy acquisition in co-evolutionary learning, not only for playing games but for a wide range of problems in management and control. <p> Originally, in an immune system simulation, antibodies which best matched an invading antigen received the payoff for that antigen [56] [159]. Another use is in learning a game: a strategy receives payoff when it achieves the best score against a sample test strategy <ref> [38] </ref> [144]. So in Table 2.1, "strategy" could be replaced by "antigen" or "sorting problem" or whatever. At first glance, Table 2.1 looks completely different to the original fitness sharing in Section 2.6.7. Surprisingly, they share the same theoretical basis [159]. <p> So a co-evolutionary GA with speciation is especially useful to acquire strategies for a game of conflict: not only can speciation prevent poor generalisation, but diverse species embody a repertoire of high-quality strategies <ref> [38] </ref>. Having a full repertoire lets us combine the diverse expertise embodied in the different species, by using a gating algorithm to choose which high-quality strategy to use when. We will see in Chapter 5 that this modular approach gives improved generalisation ability from co-evolutionary learning. <p> Another use is in learning a game, where the discrete objects are test strategies: in Table 2.1, a strategy would receive payoff when it achieves the best-in-sample score against a test strategy <ref> [38] </ref> [144]. This makes it suitable for co-evolutionary learning of game strategies, if each new test strategy comes from the GA population itself. At first glance, Table 2.1 looks completely different to fitness sharing. Surprisingly, they share the same theoretical basis [159]. <p> Consider our original problem of using a co-evolutionary GA with speciation to find all good strategies for a game. Our aim is to obtain a full repertoire of strategies, to generalise well against any expert opponent <ref> [38] </ref>. With a large enough population, implicit sharing would be best. In fact, some previous attempts to learn game strategies with a co-evolutionary GA used implicit sharing [38] [144]. <p> Our aim is to obtain a full repertoire of strategies, to generalise well against any expert opponent <ref> [38] </ref>. With a large enough population, implicit sharing would be best. In fact, some previous attempts to learn game strategies with a co-evolutionary GA used implicit sharing [38] [144]. However, from Figure 4.18, if the game has more strategies than we expected or our population is too small, then implicit sharing could get side-tracked and miss strategies it should have found, and we will generalise poorly.
Reference: [39] <editor> Yuval Davidor and Hans-Paul Schwefel. </editor> <title> An introduction to adaptive optimisation algorithms based on principles of natural evolution. </title> <editor> In Branco Soucek, editor, </editor> <booktitle> Dynamic, Genetic, and Chaotic Programming, chapter 9, </booktitle> <pages> pages 183-202. </pages> <publisher> Wiley, </publisher> <year> 1992. </year>
Reference-contexts: This hypothesis forms the basis of certain population models [35] [80]. Since spatial distribution maintains diversity, emulating this may allow a GA to find multiple optima. Another motivation is that since only near neighbours cross over, this reduces crossover disruption <ref> [39, page 261] </ref>. Australian Defence Force Academy Page 47 We now review several studies where neighbouring individuals crossed over only with their geographic neighbours, and whose fitness was determined by their score against those same neighbours while playing IPD | co-evolution on a landscape. <p> Why diversity emerges only for some variants of the game remains a mystery. But at least it demonstrates that it is feasible for a GA over a landscape to find multiple optima | but more seems to be needed to ensure reliability in finding multiple optima. Davidor <ref> [39, page 262] </ref> tentatively agrees, saying that his landscape GA "does not claim to maintain niche and species ad infinitum. [...] To achieve a complete niche and species equilibrium, additional operators are needed." As promising as these observations are, the landscape GA approach has not yet been followed up with an <p> Australian Defence Force Academy Page 53 Landscape models can allow different sub-populations to form in different regions of the landscape [109, page 309], but not reliably <ref> [39, page 262] </ref>. This suggests that this method may in future provide a working method for finding multiple solutions.
Reference: [40] <author> J. H. Davis, P. R. Laughlin, and S. S. Komorita. </author> <title> The psychology of small groups. </title> <journal> Annual Review of Psychology, </journal> <volume> 27 </volume> <pages> 501-542, </pages> <year> 1976. </year>
Reference-contexts: In comparison with the 2IPD: The N -player case has greater generality and applicability to real-life situations. In addition to the problems of energy conservation, ecology, and overpopulation, many other real-life problems can be represented by the (NIPD) paradigm. Davis et al. <ref> [40, page 520] </ref> Colman [31, page 142] and Glance and Huberman [62] [63] have also indicated that the NIPD is "qualitatively different" from the 2IPD and that "... certain strategies that work well for individuals in the Prisoner's Dilemma fail in large groups." Page 26 University College, The University of New
Reference: [41] <author> Richard Dawkins. </author> <title> The Extended Phenotype. </title> <publisher> Oxford University Press, </publisher> <year> 1982. </year>
Reference-contexts: In reality, this nerve runs from the brain all the way down the giraffe's long neck, loops around a major blood vessel, then runs back up the neck to the larynx <ref> [41, page 39] </ref> [142, pages 343-344]. One of the myths of evolution is that fitness is a tautology: supposedly, the "fit" are precisely those who have many offspring, and those who have many offspring are by definition "fit" [65, page 76].
Reference: [42] <author> Richard Dawkins. </author> <title> The Blind Watchmaker. Longman, </title> <booktitle> first edition, </booktitle> <year> 1986. </year>
Reference-contexts: This creates a dynamic environment in which the algorithm is optimising to a moving target, instead of over-specialising to some fixed criterion. The hope is that this will stimulate an "arms race" of escalating innovation <ref> [42, pages 179-193] </ref>, and create innovation and creativity. This co-evolutionary approach is a suitable learning method for a range of problems. The simplest example of these is strategy acquisition for games of conflict. <p> Imagine a population of trees in a forest, where the amount of sunlight a tree receives depends on how much sunlight nearby trees block. Evolution would select for ever-taller trees, as extra height gives more sunlight <ref> [42, page 184] </ref>. So what was a tall (and thus a fit) tree at one time, would be a relatively short (and unfit) tree many generations later, as the fitness criterion escalates. <p> This raises a question: how does such a co-evolutionary "arms race" end? One possibility is that co-evolution can stabilise when no further improvement gives an economical advantage <ref> [42, pages 190-193] </ref>. In nature, examples exist of evolutionary "arms races" ending in a design so good that further change is not economical. <p> Australian Defence Force Academy Page 39 2.5 Genetic Algorithms with Co-evolution 2.5.1 Introduction Nature has many examples of co-evolutionary "arms races" producing innovative solutions to the problem of survival <ref> [42, pages 179-193] </ref>. In this section, we review previous attempts to emulate co-evolution to create innovative solutions to various problems. <p> Kristian Lindgren [108, page 310]. Similar collapses have been observed in other co-evolutionary simulations [58] [132]. These bear a resemblance to the "punctuated equilibria" of natural evolution <ref> [42] </ref>. Although that term is not a catch-all phrase for any infrequently-changing system, we will it as a convenient shorthand for collapses like those observed by Lindgren [108]. An explanation for these collapses is not yet complete. <p> No intra-population competition means that similar offspring do not compete with each other, which may discourage improvements. On the other hand, it has been argued <ref> [42, page 187] </ref> that using two populations encourages speedier progress. We would like intra-population competition, and we would also like to keep things simple. So we follow Axelrod [7] and use only a single GA: an individual's fitness is found by its performance against its own population.
Reference: [43] <author> Richard Dawkins. </author> <title> The Selfish Gene. </title> <publisher> Oxford University Press, </publisher> <address> second edition, </address> <year> 1989. </year>
Reference-contexts: The belief space itself can undergo variation and selection. This implements some observations that culture is varied and transmitted in ways similar to genetic evolution [10] [15] [25] <ref> [43, chapter 11] </ref> [156] [119, chapter 16]. This applies not only for human culture, but for non-human cultural objects such as bird song [43, pages 189-190]. Cultural algorithms are very interesting in that they emulate the evolution of societies. <p> This implements some observations that culture is varied and transmitted in ways similar to genetic evolution [10] [15] [25] [43, chapter 11] [156] [119, chapter 16]. This applies not only for human culture, but for non-human cultural objects such as bird song <ref> [43, pages 189-190] </ref>. Cultural algorithms are very interesting in that they emulate the evolution of societies. They have also produced impressive results compared to the canonical GA [140, page 99]. However, randomly-selected pieces stored in the library may not be the best driving force for creating a modular solution.
Reference: [44] <author> Kenneth A. De Jong. </author> <title> Genetic-algorithm-based learning. </title> <editor> In Y. Kodratoff and R. Michalski, editors, </editor> <booktitle> Machine Learning, chapter 21, </booktitle> <pages> pages 611-638. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: We need this to compare learning methods. The most practical method is to simply use a suitable test set. Australian Defence Force Academy Page 11 2.2 The Genetic Algorithm 2.2.1 The Canonical Genetic Algorithm Machine learning often involves searching a space of legal alternatives for near-optimal solutions <ref> [44, page 614] </ref>. The genetic algorithm (GA) has been found useful for searching certain types of spaces, discussed in more detail in Section 2.3. Games whose strategies involve identifying and exploiting an opponent are search spaces of this character [44]. <p> The genetic algorithm (GA) has been found useful for searching certain types of spaces, discussed in more detail in Section 2.3. Games whose strategies involve identifying and exploiting an opponent are search spaces of this character <ref> [44] </ref>. <p> In general, if a binary string is of length l, then the schemae it contains cover: C l l! (2.1) distinct n-th order partitions of the search space <ref> [44, page 619] </ref>. We call the number of fixed positions in a schema its order. An n-th order partition divides the space into 2 n parts. Thus, each individual binary string actually samples a surprisingly large number of partitions of the search space. This phenomenon is called implicit parallelism. <p> In Section 2.6, we review extensions to the GA that attempt to find multiple peaks in a search space with more than one global optimum. 2.2.7 Pitt and Michigan Approaches to Classifier Systems There are two different ways to learn with a GA. The Pitt approach <ref> [44, page 626] </ref> is the usual way of using a GA. Each individual in the population is an entire solution to the problem. For example, if each solution is represented as a set of rules, then the GA uses selection and genetic operators on entire rule sets. <p> So if a good player causes an otherwise poor team to win some games, the GA will make copies of the entire team, giving the poor players on the team a free ride. The Michigan approach <ref> [44, page 627] </ref> attempts to learn faster by only copying the good players instead of entire teams. Continuing our analogy, it does this by maintaining a population of football players instead of teams, so that the GA selects and manipulates individual players instead of whole teams. <p> One of these is the bucket brigade algorithm [86, page 177]. This algorithm is not without flaws, and later attempts have tried to improve upon it [72]. The Michigan approach has achieved some interesting results <ref> [44, page 628] </ref> [72] [74]. However, the unresolved credit assignment problem, and the unwieldy bucket brigade algorithm, are points in favour of the Pitt approach. <p> In Appendix B, we briefly discuss the general attributes of problems suited to evolutionary learning. It turns out that searching the space of game strategies is suited to evolutionary learning <ref> [44] </ref>, as we describe in Section 2.3.2. We describe two games that are popular and useful machine learning problems in Sections 2.3.3 and 2.3.4. Evolutionary learning is especially good for knowledge-lean [174] or black box [1] problems, i.e., problems for which little or no prior knowledge exists. <p> Evolutionary search is good for problems with little prior knowledge, and which have combinatorially large spaces that contain many local irregularities and dis-continuities <ref> [44] </ref> [73, page 188]. One way to learn sequential decision tasks with evolutionary search, each individual is a strategy. <p> However, if there is prior knowledge available, this can be utilised when designing the representation or genetic operators, to help make the GA more efficient. To summarise, the GA is best for spaces which are <ref> [44] </ref> combinatorially large, and the main features of the space are fine-grained discontinuities, local optima, and other irregularities. B.2 Multimodal Spaces for Evolutionary Search We are especially interested in spaces with more than one global optimum.
Reference: [45] <editor> Kalyanmoy Deb and David E. Goldberg. </editor> <title> An investigation of niche and species formation in genetic function optimization. </title> <editor> In J. David Schaffer, editor, </editor> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> pages 42-50. </pages> <publisher> Morgan Kaufmann, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: Unfortunately, it generalises poorly. We explain why, and demonstrate that some plausible improvements do not greatly change this poor generalisation ability. Speciation could maintain diversity, avoid over-specialisation, and so improve generalisation ability. In Chapter 4, we investigate the most popular GA speciation method, fitness sharing <ref> [45] </ref> [67], and discover an interesting flaw which can cause it to fail to find all optima. We also compare fitness sharing with a similar GA speciation method, implicit sharing [56] [159]. <p> Much of the motivation for GA speciation is to find diverse high-quality solutions for engineering problems [134] [143] [175]. The most popular and widely-used GA speciation method is fitness sharing <ref> [45] </ref>. Previous Work Section 2.6 describes several existing GA speciation methods, including fitness sharing. Fitness sharing works effectively on many difficult problems, but has some known flaws [159], to be described in Section 2.6.7. <p> This suggests we cannot yet use the restricted-mating approach to find diverse high-payoff solutions. However, mating restrictions often improve the performance of other GA spe-ciation methods <ref> [45, page 49] </ref> [36, page 168] [121, Section 5.3]. This happens by avoiding the creation of lethal individuals by crossing over individuals with very different attributes [117, page 83], although this may be desirable in some search problems [113, section 5] [117, page 50]. <p> Each offspring thus created replaces the parent that is most similar to it. Crowding does not model the method by which a population arrives at a stable ensemble of species, but it maintains the diversity of the original population [117, page 78]. Also, empirical results <ref> [45] </ref> indicate that crowding is not as effective at finding multiple optima as another GA speciation method, fitness sharing (to be described in Section 2.6.7). Mahfoud refined crowding to create deterministic crowding [112] [113] [117, page 79], which works as follows: 1. <p> To set s requires a priori knowledge of how far apart optima are, and their (unshared) fitness. Without searching the space, this information is unknown. 18 In historical order, Beasley et al. [9] built on the work of Goldberg et al. <ref> [45] </ref> [67]. We present them in the opposite order to that in which they appeared, for ease of exposition. <p> This suggests that this method may in future provide a working method for finding multiple solutions. The speciation methods suitable for finding multiple high-quality solutions are: * Fitness sharing <ref> [45] </ref> [67] and its variations [162]; * Implicit sharing [56] [159]; * Mahfoud's deterministic crowding [116] [117]; * More recent efforts, including Ronald's multiple solution technique [143]. Exactly which method we use is not our main focus: we hope to demonstrate that co-evolutionary learning with speciation is effective. <p> Australian Defence Force Academy Page 85 So which GA speciation method should we use? Section 2.6 reviewed various methods, and at the end (Section 2.6.10) we decided to look more closely at fitness sharing <ref> [45] </ref> [67], and its close relative implicit sharing [56] [159]. 4.1.1 Organisation of this Chapter The first part of this chapter, Sections 4.2 and 4.3, scrutinises fitness sharing. We discover a new flaw, adding to those described in Section 2.6.7 on page 50. <p> Page 92 University College, The University of New South Wales The results are shown in Table 4.3. Assortative crossover is the best in both spaces: closer examination of succeeding generations reveals that random crossover between very different sub-populations disrupts the search. Table 4.3 agrees with some previous studies <ref> [45, page 49] </ref> [121] [122] in that assortative crossover helps fitness sharing find more peaks. <p> But if the population is too small, the GA will miss many good strategies. Such an approach relies heavily on the speciation method to cover all the good strategies. 4.5 Conclusions about Speciation In this chapter, we have examined the most widley-used GA speciation method, fitness sharing <ref> [45] </ref> [67], and a newer method based on it, implicit sharing [56] [159]. In addition to the flaws of fitness sharing described in in Section 2.6.7 on page 50, we discovered in Section 4.2 a new flaw when using a scaling function with fitness sharing. <p> With a non-speciated GA, these payoffs also cause the occasional mass extinction, as in Dilemma. The payoffs in Figure 5.1 give mass extinctions just as in Section 3.2.2. As described in Section 2.6.7, fitness sharing <ref> [45] </ref> is a popular GA speciation method which has achieved good results in difficult search problems [67]. However, fitness sharing is not without flaws [36] [159], including the one described in Section 4.2. <p> Australian Defence Force Academy Page 143 We have described in detail a problem whose effects have been previously observed, and presented a solution that solves the problem. 6.2 Flaws of Speciation Methods GA speciation is an active area of research. Fitness sharing <ref> [45] </ref> has given impressive performance on difficult problems [67], and was for a long time the only practical GA method for multi-optima optimisation [117, page 84]. It is natural for such a revolutionary advance to be imperfect [159].
Reference: [46] <author> Paul Donnelly, Patrick Corr, and Danny Crookes. </author> <title> Evolving Go playing strategy in neural networks. </title> <booktitle> In Proceedings of the AISB Workshop on Evolutionary Computation, </booktitle> <pages> pages 67-75, </pages> <year> 1994. </year>
Reference-contexts: Previous Work Axelrod [7] and others [27] [54] [108] have studied co-evolutionary learning of IPD, and co-evolution has been used to learn other games <ref> [46] </ref> [144] [160] and other tasks [81] [82] [99]. However, some co-evolutionary learning systems [108] [132] have displayed sudden "mass extinctions" | a high-quality strategy dominates the population for a long period of time, and suddenly dies out.
Reference: [47] <author> Thomas M. </author> <title> English. Stacked generalization and simulated evolution. </title> <journal> BioSystems, </journal> <volume> 39(1):3, </volume> <year> 1996. </year>
Reference-contexts: There are many ways to adjudicate among a repertoire of different strategies. A gating algorithm is a generic term for an algorithm that adjudicates among different experts. Methods for choosing between different experts include: * Voting [76]; * Committees [110] [167] [168]; * Agent teams [12]; * Stacked generalisation <ref> [47] </ref> [180]; * Model averaging [23]; * Monte Carlo methods [129]; Page 58 University College, The University of New South Wales Later in this thesis, in Section 5.3.1, we will use the simple approach of voting among the repertoire of strategies.
Reference: [48] <author> Susan L. Epstein. </author> <title> Toward an ideal trainer. </title> <journal> Machine Learning, </journal> <volume> 15 </volume> <pages> 251-277, </pages> <year> 1994. </year>
Reference-contexts: Genetic diversity aids performance, which agrees with Grefenstette [71]. But in a co-evolving system, seeding the initial population does not by itself improve diversity. This agrees with the theoretical work of Epstein <ref> [48] </ref>: she found that being exposed to only good instructors (without diversity in quality) does not improve generalisation ability. In a similar way, seeding a GA population with too many good Tit-for-Tat strategies also gives poor generalisation, as on the right-hand side of Figure 3.9. <p> This is because seeding the initial population does not change diversity in the long run. Also, Figure 3.9 shows the optimal level of seeding is low | genetic diversity is better than the small advantage that seeding brings. This agrees with theoretical work <ref> [48] </ref> which finds that learning only from high-quality opponents gives poor generalisation ability. It seems that in co-evolutionary learning, diversity determines generalisation. The second attempt to improve generalisation ability, in Sections 3.3.2 and 3.4, added extra strategies to the round robin, to increase diversity (and hopefully gen-eralisation) by adding strategies.
Reference: [49] <editor> Larry J. Eshelman and J. David Schaffer. </editor> <title> Preventing premature confergence in genetic algorithms by preventing incest. </title> <editor> In Richard K. Belew and Lashon B. Booker, editors, </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pages 115-122. </pages> <publisher> Morgan Kauf-mann, </publisher> <month> July </month> <year> 1991. </year>
Reference-contexts: Several schemes re-initialise the GA population with the aim of refining the current best solution, not to find multiple optima <ref> [49] </ref> [118] [177]. Cobb and Grefenstette [29] did something similar: instead of completely re-initialising the whole population with random individuals, they re-initialised only a part of it. However, their work was in the context of tracking a changing environment, and not finding multiple optima. <p> But we allow ourselves a glance at Tables 4.4 and 4.5. Page 112 University College, The University of New South Wales 2. We used assortative crossover, so only similar individuals mate. This can improve a speciated GA, as discussed in Section 4.2.4. Also, incest prevention <ref> [49] </ref> was used, i.e., individuals mate with the most similar non-identical partner left in the population. 3. The mutation rate was 0.01, so a 64-bit genotype has a 53% chance of escaping unmutated. This is high, but assortative crossover reduces disruption. 4. The crossover rate was 1.
Reference: [50] <author> Timothy Fridtjof Flannery. </author> <title> The Future Eaters: an ecological history of the Australasian lands and people. </title> <publisher> Reed Books, </publisher> <address> Port Melbourne, </address> <year> 1994. </year>
Reference-contexts: There are analogous situations from nature. For example: * The native birds of New Zealand evolved without mammals, and were exterminated when mammals (such as rats) first arrived with humans less than a thousand years ago <ref> [50] </ref>. It seems that without diversity, the birds generalised poorly to novel mammalian competitors. * In contrast, diverse competitors can improve generalisation to a newcomer. One unusual example: the Tasmanian Devil is a carnivorous marsupial 2 , found only on the Australian island state of Tasmania.
Reference: [51] <author> Terence C. Fogarty. </author> <title> First nearest neighbor classification problem on Frey and Slate's letter recognition problem. </title> <journal> Machine Learning, </journal> <volume> 9(4) </volume> <pages> 387-388, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: We know such clusters exist, because nearest neighbour Australian Defence Force Academy Page 109 D 4 11 6 8 6 10 6 2 6 10 3 7 3 7 3 9 classification works well on this data set <ref> [51] </ref>. The test phase will classify points according to the species that covers it. near a certain point are of the same letter. Consider a subset of the data, by randomly taking only 2 letters out of 26. Table 4.4 shows how scattered this data is. <p> Incidentally, how well GA methods compare with non-GA methods as covering or clustering algorithms remains an open question. For example, Frey and Slate [59] obtained worse accuracy with a Holland-style adaptive classifier (82.7%) than with a simple nearest neighbour classifier (95.4%) <ref> [51] </ref> or several other non-GA classification schemes [120, pages 140-142]. This suggests the evolutionary approach may need further development for this application. We represent GA individuals as points in the 16-dimensional integer space.
Reference: [52] <author> David B. Fogel. </author> <title> The evolution of intelligent decision making in gaming. </title> <journal> Cybernetics and Systems: An International Journal, </journal> <volume> 22 </volume> <pages> 223-236, </pages> <year> 1991. </year>
Reference-contexts: We will study a co-evolving GA learning IPD in more detail in Chapter 3. There has been much recent research on evolutionary learning in the 2-player Iterated Prisoner's Dilemma (2IPD) [7] [27] [37] <ref> [52] </ref> [54] [108]. One of the more effective strategies for IPD is "Tit-for-Tat" [6]. This simple strategy cooperates at the first iteration, and after that does whatever the opponent did in the previous iteration. That is, if the opponent defected on the previous iteration, then Tit-for-tat will defect. <p> This "evolution of cooperation" [119, section 16.2] demonstrates how a coevolutionary GA could evolve high-scoring (in this case, cooperative) strategies for a game [7, page 38]. Fogel <ref> [52] </ref> [54] did a similar study, but represented the strategies in a completely different way (as finite state machines), and emulated evolution in a manner different from a GA. <p> Axelrod [6] investigated evolution and the IPD. He used a GA population of strategies, in which each plays IPD with every other strategy in the population [7, page 38]. He found that this dynamic environment produced high scorers. Similar studies give similar results [27] [37] <ref> [52] </ref> [54] [108]. This raises a serious question: can a strategy produced by co-evolving in its own population succeed against strategies not in its own population? That is, how well does co-evolutionary learning generalise? Generalisation is an important aspect of machine learning. <p> Page 78 University College, The University of New South Wales There has been much research on the 2IPD with evolutionary learning [7] [27] [37] <ref> [52] </ref> [54] [108]. However, few experimental studies have been carried out on the NIPD in spite of its importance and qualitative difference from the 2IPD, as described in Section 2.3.4. We merely use NIPD as another learning problem while we study the generalisation ability of co-evolutionary learning. <p> But here we take a broader view, returning to the questions originally asked in Section 1.3. 6.1 Generalisation Chapter 3 takes a canonical GA and uses a co-evolutionary evaluation function, a recipe used in many previous studies [7, page 38] [27] [37] <ref> [52] </ref> [54] [108]. Also, some studies of co-evolutionary learning have observed mass extinctions [58] [108] [132]. An explanation for mass extinctions in co-evolution is currently incomplete. Section 3.2 found that in a canonical GA with co-evolution, genetic drift to a single strategy caused over-specialisation to that single strategy.
Reference: [53] <author> David B. Fogel. </author> <title> Evolving Artificial Intelligence. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> San Diego, </address> <year> 1992. </year> <institution> Australian Defence Force Academy Page 173 </institution>
Reference-contexts: In evolutionary computation, we attempt to create search, optimisation, and learning algorithms that emulate the creativity which abounds in natural evolution. Evolution as a learning process has interesting similarities to the formal scientific method <ref> [53, Section 1.6] </ref>. In nature, individuals serve as so many hypotheses about their environment. Validation is demonstrated by their survival. As generations pass, organisms tend to become better solutions to the problem of survival [53, page 39]. <p> Evolution as a learning process has interesting similarities to the formal scientific method [53, Section 1.6]. In nature, individuals serve as so many hypotheses about their environment. Validation is demonstrated by their survival. As generations pass, organisms tend to become better solutions to the problem of survival <ref> [53, page 39] </ref>. Several early Artificial Intelligence (AI) researchers observed that evolution is an intelligent learning process [53, page 38] | for example, Alan Turing recognised "an obvious connection between [machine learning] and evolution" in 1950 [170]. <p> In nature, individuals serve as so many hypotheses about their environment. Validation is demonstrated by their survival. As generations pass, organisms tend to become better solutions to the problem of survival [53, page 39]. Several early Artificial Intelligence (AI) researchers observed that evolution is an intelligent learning process <ref> [53, page 38] </ref> | for example, Alan Turing recognised "an obvious connection between [machine learning] and evolution" in 1950 [170]. After early attempts to emulate the learning process of evolution [53, pages 70-80], today evolutionary computation includes a range of approaches. <p> Several early Artificial Intelligence (AI) researchers observed that evolution is an intelligent learning process [53, page 38] | for example, Alan Turing recognised "an obvious connection between [machine learning] and evolution" in 1950 [170]. After early attempts to emulate the learning process of evolution <ref> [53, pages 70-80] </ref>, today evolutionary computation includes a range of approaches. The labels they go by include genetic algorithms (GA) [65] [86], evolutionary strategies (ES) [8], evolutionary programming (EP) [182], and genetic programming (GP) [101]. This thesis concentrates on genetic algorithms, but is broadly applicable to similar algorithms. <p> The genetic algorithm (GA) has been found useful for searching certain types of spaces, discussed in more detail in Section 2.3. Games whose strategies involve identifying and exploiting an opponent are search spaces of this character [44]. Evolution has interesting similarities to the formal scientific method <ref> [53, Section 1.6] </ref>, an iterative process for learning about the underlying processes of an observable phenomenon: * Observations about the phenomenon are collected; * A variety of models (or hypotheses) for the underlying processes are generated; * The models are tested for accuracy by comparing their predictions with the data, possibly <p> But these individuals were represented as parse trees in a computer language 21 20 This approach is known as "evolutionary programming" (EP) <ref> [53] </ref>. EP is not merely a variant of the GA, but both emulate evolution. 21 The language is usually LISP, as LISP programs are closely related to their parse trees, but any computer language will work as it is the parse tree (and not the unparsed program) that is manipulated.
Reference: [54] <author> David B. Fogel. </author> <title> Evolving behaviours in the iterated prisoner's dilemma. </title> <journal> Evolutionary Com--putation, </journal> <volume> 1(1) </volume> <pages> 77-97, </pages> <year> 1993. </year>
Reference-contexts: Although we use it merely as a test problem, IPD is a simplified version of many significant real-world problems, from recycling programs [63, page 62] to trench warfare [6, pages 73-87] and superpower confrontation [18]. Previous Work Axelrod [7] and others [27] <ref> [54] </ref> [108] have studied co-evolutionary learning of IPD, and co-evolution has been used to learn other games [46] [144] [160] and other tasks [81] [82] [99]. <p> We will study a co-evolving GA learning IPD in more detail in Chapter 3. There has been much recent research on evolutionary learning in the 2-player Iterated Prisoner's Dilemma (2IPD) [7] [27] [37] [52] <ref> [54] </ref> [108]. One of the more effective strategies for IPD is "Tit-for-Tat" [6]. This simple strategy cooperates at the first iteration, and after that does whatever the opponent did in the previous iteration. That is, if the opponent defected on the previous iteration, then Tit-for-tat will defect. <p> This "evolution of cooperation" [119, section 16.2] demonstrates how a coevolutionary GA could evolve high-scoring (in this case, cooperative) strategies for a game [7, page 38]. Fogel [52] <ref> [54] </ref> did a similar study, but represented the strategies in a completely different way (as finite state machines), and emulated evolution in a manner different from a GA. <p> Axelrod [6] investigated evolution and the IPD. He used a GA population of strategies, in which each plays IPD with every other strategy in the population [7, page 38]. He found that this dynamic environment produced high scorers. Similar studies give similar results [27] [37] [52] <ref> [54] </ref> [108]. This raises a serious question: can a strategy produced by co-evolving in its own population succeed against strategies not in its own population? That is, how well does co-evolutionary learning generalise? Generalisation is an important aspect of machine learning. <p> Although that term is not a catch-all phrase for any infrequently-changing system, we will it as a convenient shorthand for collapses like those observed by Lindgren [108]. An explanation for these collapses is not yet complete. However, Fogel <ref> [54] </ref> observed that the highest scorer (in a co-evolutionary population) sometimes contained significant flaws. <p> Page 78 University College, The University of New South Wales There has been much research on the 2IPD with evolutionary learning [7] [27] [37] [52] <ref> [54] </ref> [108]. However, few experimental studies have been carried out on the NIPD in spite of its importance and qualitative difference from the 2IPD, as described in Section 2.3.4. We merely use NIPD as another learning problem while we study the generalisation ability of co-evolutionary learning. <p> This is caused when a random mutation (much less than a clever human) exploited the navety created in a coevolutionary population without diversity. Lindgren [108] observed similar collapses, and Fogel <ref> [54] </ref> noticed that strategies produced by co-evolution can do well in their closed population, but can fail against outsiders. <p> This chapter's work has a wider significance than it may first seem. The test problem used in this chapter is again the game of Iterated Prisoner's Dilemma (IPD), which we described in Section 2.3.3. IPD has been widely used as a test for evolutionary learning [7] [27] [37] <ref> [54] </ref> [108]. Although this test problem is merely a game, the results of this chapter have much wider relevance. Games of conflict are examples of a large body of important problems where each player's outcome depends on the actions of other similar players. <p> But here we take a broader view, returning to the questions originally asked in Section 1.3. 6.1 Generalisation Chapter 3 takes a canonical GA and uses a co-evolutionary evaluation function, a recipe used in many previous studies [7, page 38] [27] [37] [52] <ref> [54] </ref> [108]. Also, some studies of co-evolutionary learning have observed mass extinctions [58] [108] [132]. An explanation for mass extinctions in co-evolution is currently incomplete. Section 3.2 found that in a canonical GA with co-evolution, genetic drift to a single strategy caused over-specialisation to that single strategy.
Reference: [55] <author> Tyler C. Folsom. </author> <title> A modular hierarchical neural network for machine vision. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 897-902. </pages> <publisher> IEEE Press, </publisher> <year> 1990. </year>
Reference-contexts: Wright designed a flying machine, they did not attempt an all-inclusive approach, but instead decomposed the solution into separate parts to solve particular aspects of the problem: lift and drag, lateral stability, and propulsion [66] [117, pages 5, 15]. * The success of modular artificial neural networks at image processing <ref> [55] </ref> [91] and speech recognition [75] are further examples of how the modular approach can solve complicated problems. A closely related study found combining several high-quality neural networks gave better results than only one high quality neural network [187]. <p> The approach we present is particularly suited to many control and management problems, the simplest of which is strategy acquisition for a game. Previous Work There are many examples of manual modularisation, including artificial neural networks for speech [75] and vision <ref> [55] </ref> [91] recognition. However, these depend on human expertise to modularise. In Section 2.7, we describe some studies of modularisation in evolutionary learning. Some do not automatically decompose the solution into relevant parts | human decisions do that [92] [136] [137]. <p> Many studies use the word "modular" to describe systems that manage a complicated engineering task by separately dealing with different aspects of the problem. If we only consider neural networks, modular solutions have attempted such diverse problems as stock market prediction [98], vision <ref> [55] </ref> [91], speech recognition [75], learning backgammon [14], and a six-legged walking robot [33]. Designing a modular system has relied on human expertise (often a committee) to manually divide a system into specialised parts. <p> The system described here is specific to learning game strategies, to validate the modular approach. But speciation as automatic modularisation can solve more realistic and complicated problems. Modular neural networks have been effective in many problems <ref> [55] </ref> [91]. Evolving neural networks with genetic algorithms has become an increasingly mature field [57] [125] [183]. This indicates that a modular approach to evolving neural networks should be a fertile field for improved learning.
Reference: [56] <author> Stephanie Forrest, Brenda Javornik, Robert E. Smith, and Alan S. Perelson. </author> <title> Using genetic algorithms to explore pattern recognition in the immune system. </title> <journal> Evolutionary Computation, </journal> <volume> 1(3) </volume> <pages> 191-211, </pages> <year> 1993. </year>
Reference-contexts: In Chapter 4, we investigate the most popular GA speciation method, fitness sharing [45] [67], and discover an interesting flaw which can cause it to fail to find all optima. We also compare fitness sharing with a similar GA speciation method, implicit sharing <ref> [56] </ref> [159]. We find that implicit sharing is better at comprehensively covering all peaks when the population is large enough for a species to form at each optimum, but worse when the population is not large enough Page 4 University College, The University of New South Wales to do this. <p> Page 6 University College, The University of New South Wales 1.4.3 New Flaws in Implicit Sharing Significance An extension of the original fitness sharing is known as implicit fitness sharing or emergent fitness sharing <ref> [56] </ref> [159] (hereafter termed simply implicit sharing). This was created to try to improve upon fitness sharing. As a part of the continuing progress of GA speciation methods, it is important to locate and describe any flaws with current methods. <p> Knowing that these flaws exist helps users to avoid problems, as well as helping designers of future GA speciation methods. Again, much of the motivation for improved GA speciation comes from engineering applications [134] [143] [175]. Previous Work Implicit sharing <ref> [56] </ref> [159] originally modeled the immune system. It has also been used for learning game strategies with co-evolution [38] [144]. One previous study [144] examined sampling in implicit sharing. <p> The most important aspect of the SSS extension is that it gets around the fixed sharing radius, and thus the requirement that peaks be equidistant and their number be known in advance. 2.6.8 Implicit Fitness Sharing Implicit fitness sharing <ref> [56] </ref> [159] is a modified version of fitness sharing, which we described in Section 2.6.7. Implicit fitness sharing 19 modifies the fitness function according to Table 2.1. Individuals bid for the payoff of discrete objects. <p> Implicit fitness sharing 19 modifies the fitness function according to Table 2.1. Individuals bid for the payoff of discrete objects. Originally, in an immune system simulation, antibodies which best matched an invading antigen received the payoff for that antigen <ref> [56] </ref> [159]. Another use is in learning a game: a strategy receives payoff when it achieves the best score against a sample test strategy [38] [144]. So in Table 2.1, "strategy" could be replaced by "antigen" or "sorting problem" or whatever. <p> This suggests that this method may in future provide a working method for finding multiple solutions. The speciation methods suitable for finding multiple high-quality solutions are: * Fitness sharing [45] [67] and its variations [162]; * Implicit sharing <ref> [56] </ref> [159]; * Mahfoud's deterministic crowding [116] [117]; * More recent efforts, including Ronald's multiple solution technique [143]. Exactly which method we use is not our main focus: we hope to demonstrate that co-evolutionary learning with speciation is effective. <p> Australian Defence Force Academy Page 85 So which GA speciation method should we use? Section 2.6 reviewed various methods, and at the end (Section 2.6.10) we decided to look more closely at fitness sharing [45] [67], and its close relative implicit sharing <ref> [56] </ref> [159]. 4.1.1 Organisation of this Chapter The first part of this chapter, Sections 4.2 and 4.3, scrutinises fitness sharing. We discover a new flaw, adding to those described in Section 2.6.7 on page 50. <p> In Section 4.4.4, we describe the experimental setup, whose results are shown in Section 4.4.5, and discussed in Section 4.4.6. Section 4.4.7 concludes Section 4.4. 4.4.3 Sharing Methods to be Compared An extension to the original fitness sharing is implicit sharing <ref> [56] </ref> [159], which we described in Section 2.6.8. Like the original, implicit sharing also modifies the fitness function. It does this according to Table 2.1 on page 52. In implicit sharing, individuals bid for the payoff of discrete objects. <p> It does this according to Table 2.1 on page 52. In implicit sharing, individuals bid for the payoff of discrete objects. Originally, in an immune system simulation, antigens were those objects: the antibody that best matched an invading antigen received the payoff for that antigen <ref> [56] </ref> [159]. Another use is in learning a game, where the discrete objects are test strategies: in Table 2.1, a strategy would receive payoff when it achieves the best-in-sample score against a test strategy [38] [144]. <p> Such an approach relies heavily on the speciation method to cover all the good strategies. 4.5 Conclusions about Speciation In this chapter, we have examined the most widley-used GA speciation method, fitness sharing [45] [67], and a newer method based on it, implicit sharing <ref> [56] </ref> [159]. In addition to the flaws of fitness sharing described in in Section 2.6.7 on page 50, we discovered in Section 4.2 a new flaw when using a scaling function with fitness sharing. <p> However, fitness sharing is not without flaws [36] [159], including the one described in Section 4.2. Because of these flaws, as well as the results in Section 4.4, this chapter uses an extension to the original fitness sharing, known as implicit sharing <ref> [56] </ref> [159], which was described in Section 2.6.8. Implicit sharing modifies the fitness function according to Table 5.1. Incidentally, Rosin and Belew [144] also used implicit sharing in a co-evolutionary, speciated GA to learn games. <p> We discuss this next. 5.2.3 Sample Selection Schemes In Table 5.1, we select samples of size . How should we select the individuals? Australian Defence Force Academy Page 125 Forrest et al. <ref> [56, page 200] </ref> select randomly, without replacement. Small samples may be unrepresentative, containing only strategies that do poorly against the single test strategy and missing those that do well against the single test strategy.
Reference: [57] <author> Bernd Freisleben. </author> <title> Teaching a neural network to play Go-Moku. </title> <editor> In Igor Aleksander, editor, </editor> <booktitle> Proceedings of the 1992 International Conference on Artificial Neural Networks, </booktitle> <pages> pages 1659-1662, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: But speciation as automatic modularisation can solve more realistic and complicated problems. Modular neural networks have been effective in many problems [55] [91]. Evolving neural networks with genetic algorithms has become an increasingly mature field <ref> [57] </ref> [125] [183]. This indicates that a modular approach to evolving neural networks should be a fertile field for improved learning. For a different game, an earlier speciated co-evolutionary system [160] sometimes displayed a never-ending turnover of species.
Reference: [58] <author> Harald Freund and Robert Wolter. </author> <title> Evolution of bit strings II: A simple model of co-evolution. </title> <journal> Complex Systems, </journal> <volume> 7 </volume> <pages> 25-42, </pages> <year> 1993. </year>
Reference-contexts: This stalls the co-evolutionary "arms race", and makes those over-specialised individuals vulnerable to novel opponents. So poor is their generalisation ability, that even random mutation can produce an effective opponent, causing sudden mass extinctions that have been previously observed <ref> [58] </ref> [108]. This thesis explains this problem in detail in Section 3.2. The rest of the thesis sets out to solve this problem. In Chapter 5 we present a practical and effective solution that brings co-evolutionary learning closer to its potential, after evaluating suitable tools in Chapter 4. <p> However, some co-evolutionary learning systems [108] [132] have displayed sudden "mass extinctions" | a high-quality strategy dominates the population for a long period of time, and suddenly dies out. An explanation for this behaviour is not yet complete, and is an active area of research <ref> [58] </ref> [96] [173]. Australian Defence Force Academy Page 5 This Thesis Extends Knowledge Chapter 3 examines the generalisation ability of expertise produced by a simple coevolutionary system. <p> In particular, the large extinctions that appear in these simulations should be studied in more detail, since these collapses are triggered by the dynamical system itself and do not need external catastrophes for their explanation. Kristian Lindgren [108, page 310]. Similar collapses have been observed in other co-evolutionary simulations <ref> [58] </ref> [132]. These bear a resemblance to the "punctuated equilibria" of natural evolution [42]. Although that term is not a catch-all phrase for any infrequently-changing system, we will it as a convenient shorthand for collapses like those observed by Lindgren [108]. An explanation for these collapses is not yet complete. <p> Also, some studies of co-evolutionary learning have observed mass extinctions <ref> [58] </ref> [108] [132]. An explanation for mass extinctions in co-evolution is currently incomplete. Section 3.2 found that in a canonical GA with co-evolution, genetic drift to a single strategy caused over-specialisation to that single strategy.
Reference: [59] <author> P. W. Frey and David J. </author> <title> Slate. Letter recognition using Holland-style adaptive classifiers. </title> <journal> Machine Learning, </journal> <volume> 6(2) </volume> <pages> 161-182, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: This reduces the relative selection pressure in fitness sharing. So it's reasonable to suspect implicit sharing will find more peaks. 4.4.4 Experimental Setup: Data, Payoff, Parameters We use Frey and Slate's <ref> [59] </ref> letter recognition data set. It has about 770 examples each of all 26 capital letters. Each data point consists of one class (out of 26 letters), plus 16 integer values between 0 and 15 inclusive. These values measure certain attributes of the characters. <p> These values measure certain attributes of the characters. What these attributes are is not important for these results. We choose this set because it is a hard classification task, and because it was used with a Holland-style adaptive classifier <ref> [59] </ref>. Accurate classification is not our primary interest | our main focus is to compare two GA speciation methods. We want the speciated GA to form a species at each cluster of same-letter data points. <p> We will learn on the same data that we test with. However, generalisation has been considered in our definition of the payoff function. Incidentally, how well GA methods compare with non-GA methods as covering or clustering algorithms remains an open question. For example, Frey and Slate <ref> [59] </ref> obtained worse accuracy with a Holland-style adaptive classifier (82.7%) than with a simple nearest neighbour classifier (95.4%) [51] or several other non-GA classification schemes [120, pages 140-142]. This suggests the evolutionary approach may need further development for this application.
Reference: [60] <author> Liane M. Gabora. </author> <title> Meme and variations: A computational model of cultural evolution. </title> <editor> In Lynn Nadel and Daniel L. Stein, editors, </editor> <booktitle> 1993 Lectures in Complex Systems, volume Lecture Volume 6 of Santa Fe Institute Studies in the Sciences of Complexity. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: In these two studies, the "modularisation" is not driven by a process like GA speciation: it is done by cutting off pieces of individuals in a more ad hoc manner, and storing them off-line. This very similar to what are loosely called cultural algorithms [10] <ref> [60] </ref> [61] [94] [140] [141]: attributes of individuals are copied into an off-line cache, called a library or "belief space", which influences newer individuals. The belief space itself can undergo variation and selection. <p> Page 106 University College, The University of New South Wales Culture could play a critical role here in allowing adaptive specialisation without the genetic speciation that irreversibly partitions the population. Richard K. Belew [10] (His italics) A few studies have been done on cultural evolution [10] <ref> [60] </ref> [61] [94] [140] [141] and this innovative approach has performed well on certain problems. Nonetheless, more comparison studies are needed. New GA speciation methods may overcome the flaws of fitness sharing, including the one discovered here.
Reference: [61] <author> Nigel Gilbert and Jim Doran. </author> <title> Simulating Societies: The computer simulation of social phenomena. </title> <publisher> UCL Press, </publisher> <year> 1994. </year>
Reference-contexts: In these two studies, the "modularisation" is not driven by a process like GA speciation: it is done by cutting off pieces of individuals in a more ad hoc manner, and storing them off-line. This very similar to what are loosely called cultural algorithms [10] [60] <ref> [61] </ref> [94] [140] [141]: attributes of individuals are copied into an off-line cache, called a library or "belief space", which influences newer individuals. The belief space itself can undergo variation and selection. <p> Page 106 University College, The University of New South Wales Culture could play a critical role here in allowing adaptive specialisation without the genetic speciation that irreversibly partitions the population. Richard K. Belew [10] (His italics) A few studies have been done on cultural evolution [10] [60] <ref> [61] </ref> [94] [140] [141] and this innovative approach has performed well on certain problems. Nonetheless, more comparison studies are needed. New GA speciation methods may overcome the flaws of fitness sharing, including the one discovered here.
Reference: [62] <author> Natalie S. Glance and Bernardo A. Huberman. </author> <title> The outbreak of cooperation. </title> <journal> Journal of Mathematical Sociology, </journal> <volume> 17(4) </volume> <pages> 281-302, </pages> <year> 1993. </year>
Reference-contexts: In addition to the problems of energy conservation, ecology, and overpopulation, many other real-life problems can be represented by the (NIPD) paradigm. Davis et al. [40, page 520] Colman [31, page 142] and Glance and Huberman <ref> [62] </ref> [63] have also indicated that the NIPD is "qualitatively different" from the 2IPD and that "... certain strategies that work well for individuals in the Prisoner's Dilemma fail in large groups." Page 26 University College, The University of New South Wales The N -player Prisoner's Dilemma game can be defined <p> player. satisfy the conditions of Equation 2.11: D i &gt; C i f or 0 i n 1 C i+1 &gt; C i f or 0 i &lt; n 1 (2.11) Although there has been much research on the 2IPD using evolutionary learning in recent years, NIPD is less studied <ref> [62] </ref> [63] [184] [186] despite its importance and its qualitative difference from the 2IPD. 2.4 Mathematical Analysis of Co-Evolution Emulating co-evolution can induce an "arms race" of innovation, and create expertise for games of conflict and other management and control jobs. This section reviews the mathematical analysis of co-evolution. <p> Rank-based selection was used, with the worst performer assigned an average of 0.75 offspring, the best 1.25 offspring 5 Each NIPD games lasts for 100 iterations. Unlike the 2-player IPD, cooperation is less likely as the number of players N increases <ref> [62] </ref> [63] [185] [186]. This means that as N increases, a co-evolutionary GA learning NIPD is less likely to converge to a population where cooperation dominates, as we saw in 2IPD in Figure 3.3 on page 68.
Reference: [63] <author> Natalie S. Glance and Bernardo A. Huberman. </author> <title> The dynamics of social dilemmas. </title> <publisher> Scientific American, </publisher> <pages> pages 58-63, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: The game being learned in Chapter 3 is the Iterated Prisoner's Dilemma (IPD), both in 2- and N -player versions. Although we use it merely as a test problem, IPD is a simplified version of many significant real-world problems, from recycling programs <ref> [63, page 62] </ref> to trench warfare [6, pages 73-87] and superpower confrontation [18]. Previous Work Axelrod [7] and others [27] [54] [108] have studied co-evolutionary learning of IPD, and co-evolution has been used to learn other games [46] [144] [160] and other tasks [81] [82] [99]. <p> In addition to the problems of energy conservation, ecology, and overpopulation, many other real-life problems can be represented by the (NIPD) paradigm. Davis et al. [40, page 520] Colman [31, page 142] and Glance and Huberman [62] <ref> [63] </ref> have also indicated that the NIPD is "qualitatively different" from the 2IPD and that "... certain strategies that work well for individuals in the Prisoner's Dilemma fail in large groups." Page 26 University College, The University of New South Wales The N -player Prisoner's Dilemma game can be defined by <p> satisfy the conditions of Equation 2.11: D i &gt; C i f or 0 i n 1 C i+1 &gt; C i f or 0 i &lt; n 1 (2.11) Although there has been much research on the 2IPD using evolutionary learning in recent years, NIPD is less studied [62] <ref> [63] </ref> [184] [186] despite its importance and its qualitative difference from the 2IPD. 2.4 Mathematical Analysis of Co-Evolution Emulating co-evolution can induce an "arms race" of innovation, and create expertise for games of conflict and other management and control jobs. This section reviews the mathematical analysis of co-evolution. <p> Rank-based selection was used, with the worst performer assigned an average of 0.75 offspring, the best 1.25 offspring 5 Each NIPD games lasts for 100 iterations. Unlike the 2-player IPD, cooperation is less likely as the number of players N increases [62] <ref> [63] </ref> [185] [186]. This means that as N increases, a co-evolutionary GA learning NIPD is less likely to converge to a population where cooperation dominates, as we saw in 2IPD in Figure 3.3 on page 68.
Reference: [64] <author> David E. Goldberg. </author> <title> Genetic algorithms and Walsh functions: Part I, a gentle introduction. </title> <journal> Complex Systems, </journal> <volume> 3 </volume> <pages> 129-152, </pages> <year> 1989. </year>
Reference-contexts: The continuous version of Equation 2.6 is Equation A.3 on page 147, and uses the difference between the same terms. There is actually a close relationship between the two [3, page 81]. These equations appear in related fields, such as natural selection in a diploid population <ref> [3, pages 21, 64] </ref>, and in the n-dimensional continuous Logistic Equation of predator-prey models [20, page 28]. 2.2.4 A Special Replicator Equation: the Schema Theorem A "fundamental theorem" for an evolutionary system is one of several similar theorems that share the prediction that the population tends to become more fit with <p> For the two-bit schemae to lead away from the optimum, we require: f (fl01) &lt; f (fl00) f (fl11) &lt; f (fl00) Such a search space would be deceptive to a GA: high-fitness building-blocks would multiply, but they do not combine to form the optimum 111 <ref> [64, page 143] </ref>. If a search space is deceptive, the operation of inversion can be added to the usual operators of mutation and crossover. Inversion is where all the bits of an individual are reversed. <p> Information Australian Defence Force Academy Page 147 patterns with a higher-than-average growth rate will become more plentiful, and lower-than-average patterns will become less plentiful and eventually die out. Equation A.3 appears in related fields, such as natural selection in a diploid population (minus mutation and recombination) <ref> [3, pages 21, 64] </ref>, and in the n dimensional continuous Logistic Equation of predator-prey models [20, page 28].
Reference: [65] <author> David E. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: After early attempts to emulate the learning process of evolution [53, pages 70-80], today evolutionary computation includes a range of approaches. The labels they go by include genetic algorithms (GA) <ref> [65] </ref> [86], evolutionary strategies (ES) [8], evolutionary programming (EP) [182], and genetic programming (GP) [101]. This thesis concentrates on genetic algorithms, but is broadly applicable to similar algorithms. Some critics assert that artificial evolution cannot be intelligent, because natural evolution is a slow process [123, page 71]. <p> However, there are different ways to evaluate individuals. When using a GA for function optimisation, individuals' fitness is based on the unchanging function we want to optimise <ref> [65, pages 75-79] </ref>. This is a fine evaluation method for many learning tasks. For example, if you already have a computer simulation of the task to be learned, then you can evaluate trial solutions (represented by GA individuals) by how well they perform the task. <p> Various operators to do this include crossover (which emulates genetic recombination), mutation, and many others. Mutation makes a random change to an individual. The mutation rate is usually low; 1 mutation per 1000 bits is typical <ref> [65, page 14] </ref>. <p> How well a GA balances these is an interesting topic <ref> [65, pages 36-38] </ref> [86, page 66] [111]. Section 4.3.2 will look more closely at this issue. A disadvantage: while a GA can rapidly find near-optimal solutions, other methods are better at going from near-optimal to optimal [71] [87]. <p> One of the myths of evolution is that fitness is a tautology: supposedly, the "fit" are precisely those who have many offspring, and those who have many offspring are by definition "fit" <ref> [65, page 76] </ref>. In the real world, if individuals survive better or have more offspring (or run faster, or have bigger teeth, or whatever), then their traits will not necessarily be more frequent in the next generation. <p> This theorem is the starting point for more specialised "fundamental theorems" for other evolutionary systems, such as genetic algorithms. A fundamental theorem of genetic algorithms is the Schema Theorem <ref> [65, pages 28-33, 49-50] </ref>. Like the fundamental theorem of natural selection [85, page 15], the Schema Theorem predicts that population's expected average fitness tends to rise (or more precisely, it tends to either rise or stay the same, but to not decrease). <p> Let f i be the average fitness of the fixed-length binary strings containing schema I i (where and let f be the average fitness of the entire population): f = i x i f i j x j We also denote the following <ref> [65, pages 28-33] </ref>: * l is the length of the bit string; * o (I i ) is the number of defined bits in schema I i , i.e., the number of non-wildcard (*) characters; * ffi (I i ) is the defining length of schema I i , i.e., the <p> The point is, a GA population will eventually cluster around only one peak, even if there are several global optima in the search space that are equally attractive. The reason is genetic drift <ref> [65, page 185] </ref>. Genetic drift is the convergence of a finite population in the absence of selection pressure due to noisy selection. <p> By emulating the tag-template matching of DNA, some researchers have concatenated a tag and template onto a conventional GA bitstring genotype, so that individuals only cross over when their tags match each other's templates <ref> [65, pages 195-197] </ref> [117, pages 47-48]. To illustrate, individuals' genotypes contain both the main, functional part plus a template [65, page 195]: Template : Functional *10* : 1010 *00* : 0000 The asterisk (*) character can match either 0 or 1. <p> To illustrate, individuals' genotypes contain both the main, functional part plus a template <ref> [65, page 195] </ref>: Template : Functional *10* : 1010 *00* : 0000 The asterisk (*) character can match either 0 or 1. <p> Dissimilar individuals (of very different species) do not compete. A GA speciation method based on this style of restricted competition is De Jong's crowding <ref> [65, page 190] </ref> and its derivatives. At each generation, crowding selects a fraction of the population (according to fitness) to undergo crossover and mutation. Each offspring thus created replaces the parent that is most similar to it. <p> This encourages search in unexplored regions, and causes subpopulations to form. Fitness sharing is, in effect, a parallelised version of Beasley et al.'s method 18 . Consider an individual i with fitness f i . Its niche count m i <ref> [65, page 191] </ref> [114] measures how many other individuals with which i shares fitness. The shared fitness f s f s f i (2.28) The niche count m i is calculated with a distance metric d ij that describes the difference between individuals i and j. <p> That is, the fitness of every individual is adjusted according to some function, and it is that adjusted fitness that the GA works with. However, choosing a scaling function is a procedure lacking theoretical conviction <ref> [65, page 124] </ref> [105]. Choosing an appropriate scaling is very important ... (T)he existing pro cedures are somewhat ad hoc; i.e., they lack deep theoretical motivation. Kreinovich et al. [105, page 13] We investigate fitness sharing's performance at multimodal optimisation, using a simple non-deceptive search space. <p> This is shown conceptually in Figure 4.7. To encourage subpopulations to go to an optimum, instead of staying around an optimum, one attractive method is to use a scaling function [116, page 143]. A scaling function is often a power function <ref> [65, page 124] </ref> [67] [105, page 12]. <p> For higher scaling powers, the optimum is indeed more attractive than the surrounding region, as desired. This demonstrates why we need scaling. 4.2.6 Optimal Scaling Function Why a power function? Choosing a scaling function is an ad hoc procedure that lacks a solid theoretical foundation <ref> [65, page 124] </ref>. Kreinovich et al. [105] give the form of the optimal scaling function, given the attributes of certain properties of the search problem and presuming the scaling function's dimension. <p> We present a simple implementation merely to demonstrate how practical improvements follow from our understanding the problem. As mentioned in Section 2.2.2, it is desirable for search algorithms to balance the priorities of exploration and exploitation in a near-optimal manner <ref> [65, pages 36-38] </ref> [86, page 66]. The so-called "2-armed bandit problem" is a convenient way to analyse different algorithms [11] [111].
Reference: [66] <author> David E. Goldberg. </author> <title> First flights at genetic-algorithm Kitty Hawk. </title> <type> Technical Report 94008, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: These can then be separately adapted 1 . * When Orville and Wilbur Wright designed a flying machine, they did not attempt an all-inclusive approach, but instead decomposed the solution into separate parts to solve particular aspects of the problem: lift and drag, lateral stability, and propulsion <ref> [66] </ref> [117, pages 5, 15]. * The success of modular artificial neural networks at image processing [55] [91] and speech recognition [75] are further examples of how the modular approach can solve complicated problems.
Reference: [67] <author> David E. Goldberg, Kalyanmoy Deb, and Jeffrey Horn. </author> <title> Massive multimodality, deception, and genetic algorithms. </title> <editor> In Reinhard Manner and Bernard Manderick, editors, </editor> <booktitle> Parallel Problem Solving from Nature 2, </booktitle> <pages> pages 37-46. </pages> <publisher> North-Holland, </publisher> <month> September </month> <year> 1992. </year>
Reference-contexts: Natural speciation can create diverse and specialised solutions to surprisingly narrow aspects of the problem of survival. It is this process that we hope to emulate. Although the two most popular GA speciation methods have been successful on various problems <ref> [67] </ref> [159], this thesis contributes to the knowledge of their flaws, and demonstrates some practical improvements [36] [38]. <p> Unfortunately, it generalises poorly. We explain why, and demonstrate that some plausible improvements do not greatly change this poor generalisation ability. Speciation could maintain diversity, avoid over-specialisation, and so improve generalisation ability. In Chapter 4, we investigate the most popular GA speciation method, fitness sharing [45] <ref> [67] </ref>, and discover an interesting flaw which can cause it to fail to find all optima. We also compare fitness sharing with a similar GA speciation method, implicit sharing [56] [159]. <p> Another study compares some speciation methods, but conspicuously forbids the fitness sharing method from using a scaling function [117, pages 206-207] | this may not be a fair comparison, as fitness sharing worked well with scaling <ref> [67] </ref> (although Section 4.2 shows how scaling can cause problems for fitness Australian Defence Force Academy Page 7 sharing [36]). <p> Unfortunately, when comparing deterministic crowding with fitness sharing (to be described in Section 2.6.7), that study conspicuously forbids fitness sharing from using a scaling function [117, pages 206-207] | this may not be a fair comparison, as fitness sharing worked well with scaling <ref> [67] </ref>, although scaling can cause problems for that method [36]. Another objection is that deterministic crowding was seen to have "failed to maintain certain sought-out optima when those could recombine to form more fit optima in the search space" [78, page 25]. <p> To set s requires a priori knowledge of how far apart optima are, and their (unshared) fitness. Without searching the space, this information is unknown. 18 In historical order, Beasley et al. [9] built on the work of Goldberg et al. [45] <ref> [67] </ref>. We present them in the opposite order to that in which they appeared, for ease of exposition. <p> This suggests that this method may in future provide a working method for finding multiple solutions. The speciation methods suitable for finding multiple high-quality solutions are: * Fitness sharing [45] <ref> [67] </ref> and its variations [162]; * Implicit sharing [56] [159]; * Mahfoud's deterministic crowding [116] [117]; * More recent efforts, including Ronald's multiple solution technique [143]. Exactly which method we use is not our main focus: we hope to demonstrate that co-evolutionary learning with speciation is effective. <p> Australian Defence Force Academy Page 85 So which GA speciation method should we use? Section 2.6 reviewed various methods, and at the end (Section 2.6.10) we decided to look more closely at fitness sharing [45] <ref> [67] </ref>, and its close relative implicit sharing [56] [159]. 4.1.1 Organisation of this Chapter The first part of this chapter, Sections 4.2 and 4.3, scrutinises fitness sharing. We discover a new flaw, adding to those described in Section 2.6.7 on page 50. <p> Another study compares GA speciation methods, but conspicuously forbids fitness sharing from using a scaling function [117, pages 206-207] | this may not be a fair comparison, as fitness sharing worked well with scaling <ref> [67] </ref>. Section 4.4 helps fill the shortage of comparison studies. We show that implicit sharing finds more peaks when the population is large enough to form a species at each peak. <p> We explain both theoretically and empirically the dilemma that this causes. 4.2.1 Introduction to Fitness Sharing Fitness sharing is a speciation method for genetic algorithms, allowing a GA population to find multiple solutions. We described it in Section 2.6.7. Fitness sharing often works better with a scaling function <ref> [67] </ref>, as does a canonical GA. That is, the fitness of every individual is adjusted according to some function, and it is that adjusted fitness that the GA works with. However, choosing a scaling function is a procedure lacking theoretical conviction [65, page 124] [105]. <p> The search space consists of all binary strings of length 30 bits, the same size used by Goldberg et al. <ref> [67] </ref>. Of the 2 30 10 9 strings, only 10 are global optima. These are listed in Table 4.1. A conceptual diagram of part of the space is shown in Figure 4.1. <p> This subpopulation consists of individuals which are mostly zero with some erroneous ones littered at random. It looks as if there were deceptive optima around the global optimum, as in the search space of Goldberg and Deb <ref> [67] </ref>. But the space has no deception or epistasis (which we described in Section 2.2.5). Why is this subpopulation error-prone? Shared payoff depends on nearby individuals. Recall that unshared payoff is 1 for a perfect match of an optimum, decreasing linearly to zero payoff for 10 incorrect bits. <p> This is shown conceptually in Figure 4.7. To encourage subpopulations to go to an optimum, instead of staying around an optimum, one attractive method is to use a scaling function [116, page 143]. A scaling function is often a power function [65, page 124] <ref> [67] </ref> [105, page 12]. <p> This would make the effect of deception appear worse than it really is. As shown in effect. For a deceptive search space, users have justified high scaling powers only to "increase the capacity of the niches centered on the global optima, relative to the niches centered on deceptive attractors" <ref> [67, page 44] </ref>, unaware of the additional effect described in here. Deception is only partly to blame. Mahfoud [116, page 143] also found that fitness sharing is easily attracted to non-optimal peaks around the optimum, without questioning why. <p> Unfortunately, high scaling powers cause some peaks to be missed completely, as shown in Figure 4.11. To compensate for this, a larger population can achieve satisfactory results, by causing more "super-individuals". For example, Goldberg et al. <ref> [67] </ref> used fitness sharing on a deceptive space, and found that the algorithm worked well using a large population (of 5000 individuals) and a very high scaling power (of 15). <p> Also, they did not elaborate on the reason why they "... achieved substantial stable subpopulations at each global optima by using [a scaling power of] fi = 15 and a population size of 5000" <ref> [67, section 4.2.3] </ref>, suggesting a gradual increase of population and scaling power until global optima were found. This paper explains another reason why high scaling powers with large populations were needed. It should be kept in mind that large populations usually require long computation time. <p> Something more general could be devised, e.g., the annealing rate could be connected to the degree of sharing. A future study could pursue this approach with more sophistication, on a realistic problem, e.g., the massively multimodal space used by Goldberg et al. <ref> [67] </ref>. the predicament in Section 4.2, gives better results. Practical improvements follow theoretical understanding. 4.3.3 Further Work on GA Speciation Other extensions to the GA may be effective at finding multiple peaks. <p> Another study compares GA speciation methods, but conspicuously forbids the fitness sharing method from using a scaling function [117, pages 206-207] | this may not be a fair comparison, as fitness sharing worked well with scaling <ref> [67] </ref> (although scaling can cause problems for fitness sharing as we saw in Section 4.2). New GA speciation methods proliferate [122] [138] [143] [151] without a proper comparison of previous GA speciation methods. This section attempts to help fill that gap. <p> Fitness sharing is not so easily sidetracked when there are too many peaks for the population size, and gives a higher classification rate for 6 and 7 letters in Table 4.8. In a previous study on a massively multimodal space with many suboptimal peaks, fitness sharing worked well <ref> [67] </ref> (although it relied on scaling, which can cause problems for fitness sharing, as seen in Section 4.2). We conjecture that implicit sharing would not work as well in such a search space. <p> But if the population is too small, the GA will miss many good strategies. Such an approach relies heavily on the speciation method to cover all the good strategies. 4.5 Conclusions about Speciation In this chapter, we have examined the most widley-used GA speciation method, fitness sharing [45] <ref> [67] </ref>, and a newer method based on it, implicit sharing [56] [159]. In addition to the flaws of fitness sharing described in in Section 2.6.7 on page 50, we discovered in Section 4.2 a new flaw when using a scaling function with fitness sharing. <p> The payoffs in Figure 5.1 give mass extinctions just as in Section 3.2.2. As described in Section 2.6.7, fitness sharing [45] is a popular GA speciation method which has achieved good results in difficult search problems <ref> [67] </ref>. However, fitness sharing is not without flaws [36] [159], including the one described in Section 4.2. <p> Fitness sharing [45] has given impressive performance on difficult problems <ref> [67] </ref>, and was for a long time the only practical GA method for multi-optima optimisation [117, page 84]. It is natural for such a revolutionary advance to be imperfect [159]. In Section 4.2, we find a new flaw of fitness sharing, whose effects are apparent in some previous studies.
Reference: [68] <author> David E. Goldberg, Kalyanmoy Deb, and Bradley Korb. </author> <title> Messy genetic algorithms revisited: Studies in mixed size and scale. </title> <journal> Complex Systems, </journal> <volume> 4 </volume> <pages> 415-444, </pages> <year> 1990. </year>
Reference-contexts: Inversion is where all the bits of an individual are reversed. However, inversion and other re-ordering operators do not by themselves make deceptive search spaces easy for a GA <ref> [68, page 415] </ref>. The problem of GA search in deceptive spaces is a continuing area of research. The word epistasis is sometimes used.
Reference: [69] <author> David E. Goldberg and P. Segrest. </author> <title> Finite Markov chain analysis of genetic algorithms. </title> <editor> In John J. Grefenstette, editor, </editor> <booktitle> Proceedings of the Second International Conference on Genetic Algorithms, </booktitle> <pages> pages 1-8. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <month> July </month> <year> 1987. </year>
Reference-contexts: Then the entire GA population will be at only one optimum, missing the other optimum. Genetic drift is a consequence of these random fluctuations in a small population [79]. A description of the expected time to extinction can be derived using Markov chains <ref> [69] </ref>. But the important point is that a canonical GA will eventually cluster around only one solution.
Reference: [70] <author> Denise Grady. </author> <title> The vision thing: Mainly in the brain. </title> <journal> Discover, </journal> <volume> 14 </volume> <pages> 57-66, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: There are many examples of natural and artificial systems which show that a modular approach can reduce the total complexity of the system, while solving a difficult problem satisfactorily. For example: * Grady <ref> [70] </ref> considers the modularisation of our brains: she points out that separate parts of the human brain deal with different aspects of vision: colour, depth, movement, detail, and shape. * In another example, it has been argued [130] that humans learn by developing rules that are applicable only to a specific
Reference: [71] <author> John J. Grefenstette. </author> <title> Incorporating problem specific knowledge into genetic algorithms. </title> <editor> In Lawrence Davis, editor, </editor> <title> Genetic Algorithms and Simulated Annealing, </title> <booktitle> chapter 4, </booktitle> <pages> pages 42-60. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1987. </year>
Reference-contexts: How well a GA balances these is an interesting topic [65, pages 36-38] [86, page 66] [111]. Section 4.3.2 will look more closely at this issue. A disadvantage: while a GA can rapidly find near-optimal solutions, other methods are better at going from near-optimal to optimal <ref> [71] </ref> [87]. An example from natural evolution is the giraffe: the giraffe's larynx (or voice box) is near its brain, and an optimal giraffe would have a short brain-to-larynx nerve. <p> In Section 3.4, we use different test environments. 3.3.1 Seeding the Initial Population One possible way to improve generalisation in co-evolutionary learning is to seed the initial population with strategies known to be of high quality. Schultz and Grefenstette <ref> [71] </ref> [154] used seeding, and found it gave a slight improvement. But their GA was heavily modified, and was not co-evolutionary, so this conclusion might not apply here. Seeding is easy to implement and check. <p> When more about than 10% of the population is Tit-for-Tat, generalisation to the random strategies in Figure 3.9 is actually worse. This indicates that beyond a certain level, genetic diversity outweighs any small improvement from seeding the initial population. Genetic diversity aids performance, which agrees with Grefenstette <ref> [71] </ref>. But in a co-evolving system, seeding the initial population does not by itself improve diversity. This agrees with the theoretical work of Epstein [48]: she found that being exposed to only good instructors (without diversity in quality) does not improve generalisation ability. <p> This nave vulnerability to novelty is a serious problem for simple co-evolutionary learning. We have examined two plausible attempts to improve this generalisation ability. The first, in Section 3.3.1, seeded the initial population with known high-quality strategies. Seeding gave improved results in a GA without co-evolution <ref> [71] </ref> [154]. Unfortunately, Figure 3.9 shows little difference in generalisation ability between no seeding and the optimal level of seeding. This is because seeding the initial population does not change diversity in the long run.
Reference: [72] <author> John J. Grefenstette. </author> <title> Credit assignment in rule discovery systems based on genetic algorithms. </title> <booktitle> Machine Learning, </booktitle> <pages> pages 225-245, </pages> <year> 1988. </year>
Reference-contexts: One of these is the bucket brigade algorithm [86, page 177]. This algorithm is not without flaws, and later attempts have tried to improve upon it <ref> [72] </ref>. The Michigan approach has achieved some interesting results [44, page 628] [72] [74]. However, the unresolved credit assignment problem, and the unwieldy bucket brigade algorithm, are points in favour of the Pitt approach. <p> One of these is the bucket brigade algorithm [86, page 177]. This algorithm is not without flaws, and later attempts have tried to improve upon it <ref> [72] </ref>. The Michigan approach has achieved some interesting results [44, page 628] [72] [74]. However, the unresolved credit assignment problem, and the unwieldy bucket brigade algorithm, are points in favour of the Pitt approach.
Reference: [73] <author> John J. Grefenstette. </author> <title> Strategy acquisition with genetic algorithms. </title> <editor> In Lawrence Davis, editor, </editor> <booktitle> Handbook of Genetic Algorithms, chapter 14, </booktitle> <pages> pages 186-201. </pages> <publisher> Von Nostrand Reinhold, </publisher> <address> New York, </address> <year> 1991. </year> <institution> Page 174 University College, The University of New South Wales </institution>
Reference-contexts: This points out which individual rule that made that action, so payoff can be accurately assigned. But in many interesting problems, payoff is received only after several decision steps, made by many different rules <ref> [73, page 187] </ref>. In such delayed-payoff problems, the question of which rule receives how much credit is unclear [86, pages 176-179] [174, page 456], and elaborate credit assignment algorithms must be used. One of these is the bucket brigade algorithm [86, page 177]. <p> Prior knowledge would allow the search to be limited to regions known to be promising. 2.3.2 The Sequential Decision Problem Sequential decision tasks include many control and management tasks, including many games. A sequential decision task may be characterised by the following scenario <ref> [73, page 187] </ref>: 1. A decision making agent interacts with a discrete-time dynamical system in an iterative fashion; 2. At each iteration, the agent observes a representation of the current state of the system; 3. The agent then selects one of a finite set of actions; 4. <p> This cycle repeats, and the agent does not know when it will terminate. The objective of sequential decision tasks is to maximise the expected asymptotic payoff <ref> [73] </ref>. <p> Another example of a sequential decision problem is playing a game with alternating moves against some opponent. Sequential decision tasks with delayed payoff are those where instead of receiving payoff at every iteration, payoff is delayed for several iterations <ref> [73, page 187] </ref>. This makes it difficult to assign credit, because it is unclear what decisions contributed most to the final lump-sum payoff. <p> Some possible methods for learning sequential decision tasks include: * The field of adaptive control theory has developed sophisticated techniques for sequential decision problems, but these only work if enough prior knowledge exists to create a tractable mathematical model <ref> [73, page 187] </ref>. * For problems which lack prior knowledge to build a convenient model, dynamic programming can produce optimal decision rules. However, as dynamic programming is essentially an enumerative method, it only works well if the number of states is fairly small [73, page 187]. * Temporal Difference learning [153] <p> exists to create a tractable mathematical model <ref> [73, page 187] </ref>. * For problems which lack prior knowledge to build a convenient model, dynamic programming can produce optimal decision rules. However, as dynamic programming is essentially an enumerative method, it only works well if the number of states is fairly small [73, page 187]. * Temporal Difference learning [153] [165] can learn control rules or artificial neural networks. Like dynamic programming, it requires a large memory for problems with large state spaces, and these memory constraints may require the space to be partitioned, which again requires prior knowledge [73, page 188]. <p> Like dynamic programming, it requires a large memory for problems with large state spaces, and these memory constraints may require the space to be partitioned, which again requires prior knowledge <ref> [73, page 188] </ref>. Evolutionary search is good for problems with little prior knowledge, and which have combinatorially large spaces that contain many local irregularities and dis-continuities [44] [73, page 188]. One way to learn sequential decision tasks with evolutionary search, each individual is a strategy. <p> memory for problems with large state spaces, and these memory constraints may require the space to be partitioned, which again requires prior knowledge <ref> [73, page 188] </ref>. Evolutionary search is good for problems with little prior knowledge, and which have combinatorially large spaces that contain many local irregularities and dis-continuities [44] [73, page 188]. One way to learn sequential decision tasks with evolutionary search, each individual is a strategy. <p> One way to learn sequential decision tasks with evolutionary search, each individual is a strategy. If a strategy is represented as a set of rules, the number of rules can be constrained to be much less than the number of possible states, and still be a high-quality strategy <ref> [73] </ref>. This indicates that searching the space of strategies for a sequential decision problem (including many games) is suitable for evolutionary search.
Reference: [74] <author> John J. Grefenstette, Connie L. Ramsey, and Alan C. Schultz. </author> <title> Learning sequential decision rules using simulation models and competition. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 355-381, </pages> <year> 1990. </year>
Reference-contexts: One of these is the bucket brigade algorithm [86, page 177]. This algorithm is not without flaws, and later attempts have tried to improve upon it [72]. The Michigan approach has achieved some interesting results [44, page 628] [72] <ref> [74] </ref>. However, the unresolved credit assignment problem, and the unwieldy bucket brigade algorithm, are points in favour of the Pitt approach. <p> This agrees with Grefenstette <ref> [74] </ref>, who found that training under particular conditions created strategies that performed well under those conditions. * But the question we are most interested in | will the strategies generalise to a situation not directly included | is answered with a resounding "no".
Reference: [75] <author> F. S. Gurgen, J. M. Song, and R. W. King. </author> <title> A continuous HMM based preprocessor for modular speech recognition neural networks. </title> <booktitle> In Proceedings of 1994 International Conference on Spoken Language Processing, </booktitle> <volume> volume 3, </volume> <pages> pages 1507-1510, </pages> <year> 1994. </year>
Reference-contexts: they did not attempt an all-inclusive approach, but instead decomposed the solution into separate parts to solve particular aspects of the problem: lift and drag, lateral stability, and propulsion [66] [117, pages 5, 15]. * The success of modular artificial neural networks at image processing [55] [91] and speech recognition <ref> [75] </ref> are further examples of how the modular approach can solve complicated problems. A closely related study found combining several high-quality neural networks gave better results than only one high quality neural network [187]. <p> Automatically decomposing a solution would be useful for machine learning. The approach we present is particularly suited to many control and management problems, the simplest of which is strategy acquisition for a game. Previous Work There are many examples of manual modularisation, including artificial neural networks for speech <ref> [75] </ref> and vision [55] [91] recognition. However, these depend on human expertise to modularise. In Section 2.7, we describe some studies of modularisation in evolutionary learning. Some do not automatically decompose the solution into relevant parts | human decisions do that [92] [136] [137]. <p> Many studies use the word "modular" to describe systems that manage a complicated engineering task by separately dealing with different aspects of the problem. If we only consider neural networks, modular solutions have attempted such diverse problems as stock market prediction [98], vision [55] [91], speech recognition <ref> [75] </ref>, learning backgammon [14], and a six-legged walking robot [33]. Designing a modular system has relied on human expertise (often a committee) to manually divide a system into specialised parts. This relies on their opinions about the inherent structure of the problem, in an ad hoc manner.
Reference: [76] <author> Lars Kai Hansen and Peter Salamon. </author> <title> Neural network ensembles. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(10) </volume> <pages> 993-1001, </pages> <year> 1990. </year>
Reference-contexts: Species form and specialise to different strategies for the game, without human help or prior knowledge of what strategies would be the best to optimise. Their diverse expertise thus created is utilised in a unified way. Using an ensemble of separate experts is widely used with neural networks <ref> [76] </ref> [155]. There are many ways to adjudicate among a repertoire of different strategies. A gating algorithm is a generic term for an algorithm that adjudicates among different experts. Methods for choosing between different experts include: * Voting [76]; * Committees [110] [167] [168]; * Agent teams [12]; * Stacked generalisation <p> Using an ensemble of separate experts is widely used with neural networks <ref> [76] </ref> [155]. There are many ways to adjudicate among a repertoire of different strategies. A gating algorithm is a generic term for an algorithm that adjudicates among different experts. Methods for choosing between different experts include: * Voting [76]; * Committees [110] [167] [168]; * Agent teams [12]; * Stacked generalisation [47] [180]; * Model averaging [23]; * Monte Carlo methods [129]; Page 58 University College, The University of New South Wales Later in this thesis, in Section 5.3.1, we will use the simple approach of voting among the
Reference: [77] <author> Garrett Hardin. </author> <title> The tragedy of the commons. </title> <journal> Science, </journal> <volume> 162 </volume> <pages> 1243-1248, </pages> <year> 1968. </year>
Reference-contexts: The classic example is the so-called "tragedy of the commons" <ref> [77] </ref>. Imagine six farmers with access to a common pasture where they graze their cows. Each farmer owns a cow weighing 1000 pounds. This pasture can support six cows, but each additional cow causes overgrazing and reduces the weight of every cow by 100 pounds.
Reference: [78] <author> Georges R. Harik. </author> <title> Finding multimodal solutions using restricted tournament selection. </title> <editor> In Larry J. Eshelman, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 24-31. </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: Another objection is that deterministic crowding was seen to have "failed to maintain certain sought-out optima when those could recombine to form more fit optima in the search space" <ref> [78, page 25] </ref>. A speciation method similar to Mahfoud's deterministic crowding is "restricted tournament selection" [78]. <p> Another objection is that deterministic crowding was seen to have "failed to maintain certain sought-out optima when those could recombine to form more fit optima in the search space" [78, page 25]. A speciation method similar to Mahfoud's deterministic crowding is "restricted tournament selection" <ref> [78] </ref>. This works similarly, except that instead of only the parents, a larger sample is used: the sampled individual most like the offspring is Australian Defence Force Academy Page 49 compared with that offspring, and the less fit is erased. <p> That is, instead of competing with a parent, each offspring competes with the most similar individual in a sample. There is even an extension to this, known as "adaptive restricted tournament selection" [146], which attempts to improve on the previous scheme <ref> [78] </ref>. 2.6.7 Restricted Competition: Fitness Sharing Like Beasley et al. [9], fitness sharing modifies a search landscape by reducing payoff in well-explored regions of the search space. It does this dynamically, during the course of a single run. This encourages search in unexplored regions, and causes subpopulations to form. <p> In Section 4.2 of this thesis, we demonstrate that even without the above limitations to fitness sharing, in a non-deceptive space where the peaks are equidistant and equal-valued, fitness sharing can still miss peaks. To get around these problems, some schemes do not use a fixed sharing radius <ref> [78] </ref> [113] [159] [162]. Another study applies the restricted mating approach of Section 2.6.5 to fitness sharing, giving the Simple Subpopulation Scheme (SSS) [162].
Reference: [79] <author> Inman F. Harvey. </author> <title> The puzzle of the persistent question marks: A case study of genetic drift. </title> <type> Technical Report CSRP-278, </type> <institution> School of Cognitive and Computing Sciences, The University of Sussex, </institution> <address> Brighton, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Then the entire GA population will be at only one optimum, missing the other optimum. Genetic drift is a consequence of these random fluctuations in a small population <ref> [79] </ref>. A description of the expected time to extinction can be derived using Markov chains [69]. But the important point is that a canonical GA will eventually cluster around only one solution.
Reference: [80] <author> Alan Hastings and Susan Harrison. </author> <title> Metapopulation dynamics and genetics. </title> <editor> In Daphne G. Fautin, Douglas J. Futuyma, and Frances C. James, editors, </editor> <booktitle> Annual Review of Ecology and Systematics, </booktitle> <volume> volume 25, </volume> <pages> pages 167-188. </pages> <publisher> Annual Reviews Inc., </publisher> <address> Palo Alto, </address> <year> 1994. </year>
Reference-contexts: This says that migration between local populations promotes the persistence of a regional "metapopulation" despite short-term fluctuations, or even temporary extinctions, on the local scale. This hypothesis forms the basis of certain population models [35] <ref> [80] </ref>. Since spatial distribution maintains diversity, emulating this may allow a GA to find multiple optima. Another motivation is that since only near neighbours cross over, this reduces crossover disruption [39, page 261].
Reference: [81] <author> W. Daniel Hillis. </author> <title> Co-evolving parasites improve simulated evolution as an optimization procedure. </title> <journal> Physica D, </journal> <volume> 42 </volume> <pages> 228-234, </pages> <year> 1990. </year>
Reference-contexts: Previous Work Axelrod [7] and others [27] [54] [108] have studied co-evolutionary learning of IPD, and co-evolution has been used to learn other games [46] [144] [160] and other tasks <ref> [81] </ref> [82] [99]. However, some co-evolutionary learning systems [108] [132] have displayed sudden "mass extinctions" | a high-quality strategy dominates the population for a long period of time, and suddenly dies out. <p> Similar offspring in the same population do not play against each other directly, and competition among one population is indirect. This may reduce competition. Nonetheless, this two-population approach appears to work very well for problems with a natural symmetry. For example, Hillis <ref> [81] </ref> [82] used two GA populations: one of sorting problems, and another of sorting algorithms. The aim was to maintain the difficulty of the sorting problems, to encourage the evolution of better sorting algorithms. <p> The aim was to maintain the difficulty of the sorting problems, to encourage the evolution of better sorting algorithms. This produced high-quality sorting algorithms, including one Australian Defence Force Academy Page 41 that would have been a world first in 1969 <ref> [81, page 233] </ref>. However, this sorting problem and sorting algorithm domain has a natural symmetry to it, whereas a normal two-player game has any number of potential factions of strategies. <p> However, this sorting problem and sorting algorithm domain has a natural symmetry to it, whereas a normal two-player game has any number of potential factions of strategies. Hillis' study <ref> [81] </ref> [82] worked well by decomposing the problem-solving into specialised parts, with a separate sub-population dealing with its own part. This thesis extends the idea of decomposing a solution into specialised parts in Chapter 5, after reviewing relevant previous work (including Hillis [81] [82]) in Section 2.7. 2.6 Genetic Algorithms with <p> Hillis' study <ref> [81] </ref> [82] worked well by decomposing the problem-solving into specialised parts, with a separate sub-population dealing with its own part. This thesis extends the idea of decomposing a solution into specialised parts in Chapter 5, after reviewing relevant previous work (including Hillis [81] [82]) in Section 2.7. 2.6 Genetic Algorithms with Speciation 2.6.1 Introduction to Speciation To find more than one high-quality solution at a time requires changes to the canonical GA, due to genetic drift as described in Section 2.2.6. <p> This is an example of simple manual modularisation: there are precisely two sides to the problem, and each population deals with their own aspect. For example, Hillis <ref> [81] </ref> [82] used one GA population of sorting algorithms, and another population of sorting problems which tried to be difficult for the sorting algorithms to solve. The two populations co-evolved, and the first created a good sorting algorithm.
Reference: [82] <author> W. Daniel Hillis. </author> <title> Co-evolving parasites improve simulated evolution as an optimization procedure. </title> <editor> In Christopher G. Langton, Charles Taylor, J. Doyne Farmer, and Steen Ras-mussen, editors, </editor> <booktitle> Artificial Life 2, volume 10 of Santa Fe Institute Studies in the Sciences of Complexity, </booktitle> <pages> pages 313-323. </pages> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: This co-evolutionary approach is a suitable learning method for a range of problems. The simplest example of these is strategy acquisition for games of conflict. More serious examples include stock trading [132], creating a sorting algorithm <ref> [82] </ref>, scheduling a manufacturing process [92], and many other control, management, and military problems where outcomes are partly determined by the actions of others. This thesis demonstrates that, unfortunately, the most straightforward implementation of co-evolutionary learning can give poor generalisation ability. <p> previous work has coped with the problem, and; * How this thesis extends current knowledge of the problem. 1.4.1 Explaining Mass Extinctions and Generalisation in Co evolutionary Learning Significance Co-evolutionary genetic algorithms can find solutions that rival human-created solutions | in tasks as diverse as finding a good sorting algorithm <ref> [82] </ref> or learning to play the game of Othello [125] | without prior knowledge from human experts in the field to be learned. Generalisation ability is an important aspect of machine learning, so knowing how well co-evolutionary learning can generalise is significant. <p> Previous Work Axelrod [7] and others [27] [54] [108] have studied co-evolutionary learning of IPD, and co-evolution has been used to learn other games [46] [144] [160] and other tasks [81] <ref> [82] </ref> [99]. However, some co-evolutionary learning systems [108] [132] have displayed sudden "mass extinctions" | a high-quality strategy dominates the population for a long period of time, and suddenly dies out. An explanation for this behaviour is not yet complete, and is an active area of research [58] [96] [173]. <p> Some studies use separate GA populations, where each member of one population is evaluated by its performance against members of the other population <ref> [82] </ref> [144]. We consider this in Appendix C on page 157, but it gives similar results to the one-population case we consider here. Imagine that every member of the population plays every other member of the same population (one at a time) at a two-player game. <p> Similar offspring in the same population do not play against each other directly, and competition among one population is indirect. This may reduce competition. Nonetheless, this two-population approach appears to work very well for problems with a natural symmetry. For example, Hillis [81] <ref> [82] </ref> used two GA populations: one of sorting problems, and another of sorting algorithms. The aim was to maintain the difficulty of the sorting problems, to encourage the evolution of better sorting algorithms. <p> However, this sorting problem and sorting algorithm domain has a natural symmetry to it, whereas a normal two-player game has any number of potential factions of strategies. Hillis' study [81] <ref> [82] </ref> worked well by decomposing the problem-solving into specialised parts, with a separate sub-population dealing with its own part. This thesis extends the idea of decomposing a solution into specialised parts in Chapter 5, after reviewing relevant previous work (including Hillis [81] [82]) in Section 2.7. 2.6 Genetic Algorithms with Speciation <p> Hillis' study [81] <ref> [82] </ref> worked well by decomposing the problem-solving into specialised parts, with a separate sub-population dealing with its own part. This thesis extends the idea of decomposing a solution into specialised parts in Chapter 5, after reviewing relevant previous work (including Hillis [81] [82]) in Section 2.7. 2.6 Genetic Algorithms with Speciation 2.6.1 Introduction to Speciation To find more than one high-quality solution at a time requires changes to the canonical GA, due to genetic drift as described in Section 2.2.6. <p> We also use one population because our test problem is symmetric. One successful two-population approach to co-evolution had an asymmetric problem, with one population of sorting problems and another of sorting algorithms <ref> [82] </ref>. Genotype for Strategies Our representation scheme is similar to Axelrod's [7], and the same as in Section 3.4.1. Each individual is a set of rules stored in a look-up table that covers every possible history, and is represented as a binary string. <p> Australian Defence Force Academy Page 155 Appendix C Evolutionary Game Theory: Two Populations Some co-evolutionary GA systems run two different populations: the fitness of each individual is determined by how well it performs against all the individuals in the other population <ref> [82] </ref> [144]. This is an example of simple manual modularisation: there are precisely two sides to the problem, and each population deals with their own aspect. For example, Hillis [81] [82] used one GA population of sorting algorithms, and another population of sorting problems which tried to be difficult for the <p> the fitness of each individual is determined by how well it performs against all the individuals in the other population <ref> [82] </ref> [144]. This is an example of simple manual modularisation: there are precisely two sides to the problem, and each population deals with their own aspect. For example, Hillis [81] [82] used one GA population of sorting algorithms, and another population of sorting problems which tried to be difficult for the sorting algorithms to solve. The two populations co-evolved, and the first created a good sorting algorithm.
Reference: [83] <author> W. Hines and D. Anfossi. </author> <title> A discussion of evolutionarily stable strategies. </title> <editor> In Sabin Lessard, editor, </editor> <booktitle> Mathematical and Statistical Developments of Evolutionary Theory: Proceedings of the NATO Advanced Study Institute and Seminaire de Mathematiques Superieures on Mathematical and Statistical Developments of Evolutionary Theory, </booktitle> <pages> pages 229-267. </pages> <publisher> Kluwer Academic, </publisher> <month> August </month> <year> 1987. </year>
Reference-contexts: that is type I i is p i , where 0 p i 1: p i = P n (2.2) * The population vector p = (p 1 ; p 2 ; : : : ; p n ) is a point in the space or simplex [32, page 4] <ref> [83, page 232] </ref> [85, page 13] denoted by S n , defined in Equation 2.3 below. Figure 2.2 shows the three-dimensional simplex S 3 . S n = p 2 IR n : p i 0 and i ) Vector p represents a distribution of individuals.
Reference: [84] <author> Cem Hocaoglu and Arthur C. Sanderson. </author> <title> Planning multi-paths using speciation in genetic algorithms. </title> <booktitle> In Proceedings of the 1996 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pages 378-383. </pages> <publisher> IEEE Press, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: This which may not scale up well when searching a combinatorially large space. * One that repeatedly divides the population into sub-populations (the number of which varies dynamically) and re-combines them <ref> [84] </ref>. * An attempt to improve fitness sharing has led to "clearing" [138]. Instead of evenly sharing resources among individuals in the same part of the search space, this method only rewards the best individuals in that part of the space and erases the others.
Reference: [85] <author> Josef Hofbauer and Karl Sigmund. </author> <title> The Theory of Evolution and Dynamical Systems, volume 7 of London Mathematical Society Student Texts. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: I i is p i , where 0 p i 1: p i = P n (2.2) * The population vector p = (p 1 ; p 2 ; : : : ; p n ) is a point in the space or simplex [32, page 4] [83, page 232] <ref> [85, page 13] </ref> denoted by S n , defined in Equation 2.3 below. Figure 2.2 shows the three-dimensional simplex S 3 . S n = p 2 IR n : p i 0 and i ) Vector p represents a distribution of individuals. <p> The original example is the fundamental theorem of natural selection <ref> [85, page 15] </ref>, which says that the population's average fitness can only increase (or stay the same) with passing time 3 . This theorem is the starting point for more specialised "fundamental theorems" for other evolutionary systems, such as genetic algorithms. <p> This theorem is the starting point for more specialised "fundamental theorems" for other evolutionary systems, such as genetic algorithms. A fundamental theorem of genetic algorithms is the Schema Theorem [65, pages 28-33, 49-50]. Like the fundamental theorem of natural selection <ref> [85, page 15] </ref>, the Schema Theorem predicts that population's expected average fitness tends to rise (or more precisely, it tends to either rise or stay the same, but to not decrease). <p> The simple fitness parameters i are replaced by terms representing selection, mutation, 3 This is true both for the discrete case in Equation 2.13 <ref> [85, pages 17-19] </ref> [172, pages 38-39] and the continuous case in Equation A.5 [85, pages 226-227] [172, pages 31-36]. The fundamental theorem of natural selection applies to diploid genetics, where the interaction matrix A (to be described in Section 2.4.2) is symmetric, a ij = a ji . <p> The simple fitness parameters i are replaced by terms representing selection, mutation, 3 This is true both for the discrete case in Equation 2.13 [85, pages 17-19] [172, pages 38-39] and the continuous case in Equation A.5 <ref> [85, pages 226-227] </ref> [172, pages 31-36]. The fundamental theorem of natural selection applies to diploid genetics, where the interaction matrix A (to be described in Section 2.4.2) is symmetric, a ij = a ji . <p> The fundamental theorem of natural selection applies to diploid genetics, where the interaction matrix A (to be described in Section 2.4.2) is symmetric, a ij = a ji . In the context of game theory, a symmetric interaction matrix means that both players share their payoff equally <ref> [85, page 227] </ref>, in a partnership game in which there is no conflict. Games of conflict lack this property. Hence the need for other "fundamental" theorems. Australian Defence Force Academy Page 17 and recombination. Again, we only consider a GA with binary fixed-length strings. <p> Equation 2.13 can be written in an alternative form that includes C, a positive constant 7 corresponding to the fitness without interaction [24, page 104] <ref> [85, page 133] </ref> [145, page 271] [157, page 92]: p i (t + 1) = p i (t) j a ij p j (t) + C (2.14) Equation 2.13 and its continuous counterpart, Equation A.5, are closed in the simplex S n : if a point starts on the simplex, p <p> The replicator equations for co-evolution (Equation 2.14) have a useful property in the quotient rule <ref> [85, page 125] </ref> [157, page 94]. For any information patterns I i and I j : p j (t + 1) p i (t) P P (discrete case) (2.15) 2.4.3 Equilibrium Points An equilibrium point is not necessarily stable. <p> Applying Definition 2.1 to Equation 2.13 says that a population vector p is an equilibrium point (i.e., a fixed point) if it satisfies [3, page 65] <ref> [85, pages 15, 126] </ref> [157, page 94]: n X a ij p j = p Ap for all i; 1 i n (2.16) If no information pattern is extinct at the equilibrium point (i.e. p i &gt; 0 for all i) then Equation 2.16 gives a system of linear equations for <p> j = p Ap for all i; 1 i n (2.16) If no information pattern is extinct at the equilibrium point (i.e. p i &gt; 0 for all i) then Equation 2.16 gives a system of linear equations for the n variables p 1 to p n [3, page 66] <ref> [85, pages 16, 126] </ref> [145, page 272]: X a 1j p j = j X a nj p j (2.17) i p i = p 1 + p 2 + : : : + p n = 1 (From Equation 2.2) (2.18) If one or more of the p i are <p> (2.18) If one or more of the p i are zero (i.e., one or more information patterns are extinct) at equilibrium, then we obtain a linear system like Equations 2.17 and 2.18, but of lower dimension, by excluding the indices i for which p i = 0 [3, page 69] <ref> [85, page 16] </ref>. The common value of the expressions in Equation 2.17 is the population's average payoff 8 , p Ap [85, page 16]. <p> then we obtain a linear system like Equations 2.17 and 2.18, but of lower dimension, by excluding the indices i for which p i = 0 [3, page 69] <ref> [85, page 16] </ref>. The common value of the expressions in Equation 2.17 is the population's average payoff 8 , p Ap [85, page 16]. <p> [3, page 65]. * If one or more of the p i are zero, an equilibrium point is said to be in the interior of the face of the simplex, i.e., in the interior of the lower-dimensional simplex formed by excluding the indices i for which p i = 0 <ref> [85, page 16] </ref>. This is illustrated in Figure 2.6. <p> Equilibrium condition:- n X a ij p es 2. Stability condition:- If p es Ap es = q Ap es for some q 6= p es (2.21) then p es Aq &gt; q Ap es (2.22) Equation 2.20 is the definition of a Nash equilibrium [3, page 73] <ref> [85, page 121] </ref>, a natural concept for classical game theory, where p is a mixed strategy. However, in our situation where p represents a population instead of a strategy, the Nash condition is exactly the equilibrium condition [3, page 72] [85, page 127] from Equation 2.16. <p> However, in our situation where p represents a population instead of a strategy, the Nash condition is exactly the equilibrium condition [3, page 72] <ref> [85, page 127] </ref> from Equation 2.16. Of more interest is the meaning of the stability condition in Definition 2.3. Imagine that we could somehow take population p es and find its average payoff not against itself, but against a rival population q. <p> There are several convenient ways of telling if a given equilibrium point p es is evolutionarily stable (ES). 10 The classical game theory approach interprets the points p on the simplex as mixed strategies <ref> [85, pages 121, 127] </ref> [172, page 28]. Page 34 University College, The University of New South Wales Theorem 2.2 ([157, page 102]) The following conditions are equivalent: 1. Point p eq 2 S n is evolutionarily stable; 2. <p> Point p eq 2 S n is evolutionarily stable; 2. For all p 6= p eq in some neighbourhood of p eq , p eq Ap p Ap (2.23) 3. For all q 6= p eq , for sufficiently small * we have <ref> [85, page 121] </ref>:- p eq A [(1 *)p eq + *q] &gt; q A [(1 *)p eq + *q] (2.24) We can extend Definition 2.3 from an ES point to an ES set: Definition 2.4 ([32, page 97]) An evolutionarily stable set L is a set of points p l 2 <p> The second of these was that co-evolution may never settle down to a stable ending, but instead keeps changing. We consider this second possibility in this section. Depending on the interaction matrix A, there may be zero, or one, or many equilibrium points <ref> [85, page 16] </ref>. This applies for all possible lower-dimensional cases (i.e., in the interior of each face of the simplex S n ). <p> As described in Section D.2, if the linearisation matrix B at an equilibrium point p eq has one or more eigenvalues equal to zero, then it is not an isolated equilibrium point [3, page 36] <ref> [85, page 126] </ref>. These equilibrium points form a linear manifold in the interior of S n [85, pages 16, 126, 228]. <p> These equilibrium points form a linear manifold in the interior of S n <ref> [85, pages 16, 126, 228] </ref>. In such a situation, a co-evolving GA (even with some way to prevent genetic drift) would not converge to a stable ensemble of species, and never find a "best" solution. <p> There is currently no way to check, other than finding the eigenvalues of the linearisation matrix B and seeing if any are zero 14 . For the combinatorially large spaces, it is infeasible to check directly. 12 Except when the interaction matrix A is symmetric, corresponding to diploid genetics <ref> [85, page 229] </ref>. 13 If you can't, then that negates the conjecture that you always can [188, page 478]. 14 Actually, it is more complicated than that, as the constant C in Equation 2.14 can be zero. See Section D.2 for more details. <p> The biological definition of a species is a distinct population of individuals which can breed freely with each other, but cannot successfully interbreed with another group [30, page 12]. Natural species emerge in at least two ways, allopatric speciation <ref> [85, page 30] </ref> and sympatric speciation [85, page 31]: Sympatric : Speciation by restricted mating, caused by geographic barriers, migration, or similar causes. <p> The biological definition of a species is a distinct population of individuals which can breed freely with each other, but cannot successfully interbreed with another group [30, page 12]. Natural species emerge in at least two ways, allopatric speciation [85, page 30] and sympatric speciation <ref> [85, page 31] </ref>: Sympatric : Speciation by restricted mating, caused by geographic barriers, migration, or similar causes. For example, each isolated island of the Galapagos archipelago has its own species of finch, all of which are descended from the same continental finch species [85, page 31]. <p> speciation [85, page 30] and sympatric speciation <ref> [85, page 31] </ref>: Sympatric : Speciation by restricted mating, caused by geographic barriers, migration, or similar causes. For example, each isolated island of the Galapagos archipelago has its own species of finch, all of which are descended from the same continental finch species [85, page 31]. Allopatric : Speciation by specialisation, among populations with overlapping ranges. Specialising to different food sources restricts competition. For example, on the Black Sea island of Jorilgatch, there are four similar species of tern, which obtain food in different ways [20, page 451]. <p> Incidentally, Equation A.5 is equivalent to the Lotka-Volterra equation <ref> [85, page 134] </ref> [172, pages 23-25]. <p> Incidentally, Equation A.5 is equivalent to the Lotka-Volterra equation [85, page 134] [172, pages 23-25]. Like Equation 2.14 on page 30, the continuous replicator equations for co-evolution also obey a quotient rule <ref> [85, page 125] </ref> [157, page 94]: 1 Maynard Smith suggests that the continuous Equation A.5 might be sometimes more biolog ically appropriate if its form was a quotient, as in Equation 2.13 [188, page 474]. <p> The two populations co-evolved, and the first created a good sorting algorithm. The mathematical models of Equations A.5 and 2.13 generalise to two populations <ref> [85, pages 137, 273] </ref> [3, page 21], and similar results can be derived. Consider two populations. <p> The m fi n interaction matrix A, describes the rate-of-increase of x i in A from y j in B, and similarly for the n fi m interaction matrix B. The continuous replicator equation for this case <ref> [85, pages 141, 273] </ref> is very similar to the single-population case in Equation A.5 on page 148: dp i = p i @ j=1 1 dq i = q j i=1 ! Here, B T is the transpose of matrix B. <p> The discrete version <ref> [85, page 273] </ref> is again similar to the single-population case in Equation 2.14:- p i (t + 1) = p i j=1 a ij q j + C A (C.3) Australian Defence Force Academy Page 157 q j (t + 1) = q j i=1 b ij p i + C <p> 1) = p i j=1 a ij q j + C A (C.3) Australian Defence Force Academy Page 157 q j (t + 1) = q j i=1 b ij p i + C B (C.4) As for the single-population case, the two-population case has a definition for evolutionary stability <ref> [85, page 138, 283] </ref>, and a way of finding equilibrium points [85, page 142], or if an equilibrium is stable [85, page 284]. One can even find the system's Hamiltonian of motion [85, page 277]. In Section 2.5.3 we critique this two-population approach to co-evolving genetic algorithms. <p> A (C.3) Australian Defence Force Academy Page 157 q j (t + 1) = q j i=1 b ij p i + C B (C.4) As for the single-population case, the two-population case has a definition for evolutionary stability [85, page 138, 283], and a way of finding equilibrium points <ref> [85, page 142] </ref>, or if an equilibrium is stable [85, page 284]. One can even find the system's Hamiltonian of motion [85, page 277]. In Section 2.5.3 we critique this two-population approach to co-evolving genetic algorithms. <p> j (t + 1) = q j i=1 b ij p i + C B (C.4) As for the single-population case, the two-population case has a definition for evolutionary stability [85, page 138, 283], and a way of finding equilibrium points [85, page 142], or if an equilibrium is stable <ref> [85, page 284] </ref>. One can even find the system's Hamiltonian of motion [85, page 277]. In Section 2.5.3 we critique this two-population approach to co-evolving genetic algorithms. <p> One can even find the system's Hamiltonian of motion <ref> [85, page 277] </ref>. In Section 2.5.3 we critique this two-population approach to co-evolving genetic algorithms. Page 158 University College, The University of New South Wales Appendix D Stability D.1 Meanings of "Stability" The different uses of the word "stable" can confuse. <p> It gets closer, then further away, but on the whole it gets closer. D.2 Finding Stability by Linearisation Recall the discrete replicator equation from Equation 2.14 on page 30 [24, page 104] <ref> [85, page 133] </ref> [145, page 271] [157, page 92]: p i (t + 1) = p i (t) j a ij p j (t) + C Linearisation adds a perturbation vector q to the equilibrium point in question p eq . <p> So once you've found an equilibrium point by solving the linear system in Equation 2.17, you can determine the linearisation B and find its eigenvalues to determine if that equilibrium point is stable or not. 1 Actually they form a linear manifold <ref> [85, pages 16, 126, 228] </ref>, but suffice it to say that they are not isolated.
Reference: [86] <author> John H. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> MIT Press, </publisher> <address> second edition, </address> <year> 1992. </year>
Reference-contexts: After early attempts to emulate the learning process of evolution [53, pages 70-80], today evolutionary computation includes a range of approaches. The labels they go by include genetic algorithms (GA) [65] <ref> [86] </ref>, evolutionary strategies (ES) [8], evolutionary programming (EP) [182], and genetic programming (GP) [101]. This thesis concentrates on genetic algorithms, but is broadly applicable to similar algorithms. Some critics assert that artificial evolution cannot be intelligent, because natural evolution is a slow process [123, page 71]. <p> How well a GA balances these is an interesting topic [65, pages 36-38] <ref> [86, page 66] </ref> [111]. Section 4.3.2 will look more closely at this issue. A disadvantage: while a GA can rapidly find near-optimal solutions, other methods are better at going from near-optimal to optimal [71] [87]. <p> Continuing our analogy, it does this by maintaining a population of football players instead of teams, so that the GA selects and manipulates individual players instead of whole teams. The most common example of the Michigan approach is Holland's classifier system <ref> [86, pages 171-181] </ref>. The rules are the individuals, and the entire population is a single rule set. That is, the level of the individual has moved down. However, the added complexity brings with it some problems. <p> But in many interesting problems, payoff is received only after several decision steps, made by many different rules [73, page 187]. In such delayed-payoff problems, the question of which rule receives how much credit is unclear <ref> [86, pages 176-179] </ref> [174, page 456], and elaborate credit assignment algorithms must be used. One of these is the bucket brigade algorithm [86, page 177]. This algorithm is not without flaws, and later attempts have tried to improve upon it [72]. <p> In such delayed-payoff problems, the question of which rule receives how much credit is unclear [86, pages 176-179] [174, page 456], and elaborate credit assignment algorithms must be used. One of these is the bucket brigade algorithm <ref> [86, page 177] </ref>. This algorithm is not without flaws, and later attempts have tried to improve upon it [72]. The Michigan approach has achieved some interesting results [44, page 628] [72] [74]. <p> We present a simple implementation merely to demonstrate how practical improvements follow from our understanding the problem. As mentioned in Section 2.2.2, it is desirable for search algorithms to balance the priorities of exploration and exploitation in a near-optimal manner [65, pages 36-38] <ref> [86, page 66] </ref>. The so-called "2-armed bandit problem" is a convenient way to analyse different algorithms [11] [111].
Reference: [87] <author> John H. Holland. </author> <title> Genetic algorithms. </title> <journal> Scientific American, </journal> <volume> 267(4) </volume> <pages> 44-50, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Crossover is the other commonly-used mechanism of genetic rearrangement in genetic algorithms <ref> [87] </ref>. The simplest implementation of crossover is when individuals are fixed-length binary strings: pick two individuals, and cut and paste to create two new strings, as in Figure 2.1. Genetic recombination is the dominant mechanism for genetic rearrangement in real organisms [87]. <p> other commonly-used mechanism of genetic rearrangement in genetic algorithms <ref> [87] </ref>. The simplest implementation of crossover is when individuals are fixed-length binary strings: pick two individuals, and cut and paste to create two new strings, as in Figure 2.1. Genetic recombination is the dominant mechanism for genetic rearrangement in real organisms [87]. But crossover is not the same as sex | in nature, crossover as described in Figure 2.1 only happens in eukaryotes (a primitive single-celled organism) [150]. <p> How well a GA balances these is an interesting topic [65, pages 36-38] [86, page 66] [111]. Section 4.3.2 will look more closely at this issue. A disadvantage: while a GA can rapidly find near-optimal solutions, other methods are better at going from near-optimal to optimal [71] <ref> [87] </ref>. An example from natural evolution is the giraffe: the giraffe's larynx (or voice box) is near its brain, and an optimal giraffe would have a short brain-to-larynx nerve. <p> Our result also sheds light on another aspect of evolutionary computation. GA search is good at finding a solution that is almost optimal, but not quite. So it is common to augment a GA with another method <ref> [87, page 50] </ref>, such as hill-climbing. Is this necessary for a GA with fitness sharing? As shown in Figure 4.8, no scaling causes subpopulations to form around an optimum, instead of at it. Hill-climbing is one way to cover that gap.
Reference: [88] <author> John H. Holland, Keith J. Holyoak, Richard E. Nisbett, and Paul R. Thagard. </author> <title> Induction: Processes of Inference, Learning, and Discovery. Computational Models of Cognition and Perception. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This is the case 1 This very human approach to thinking can cause superstitions, due to the specialisation of different (and possibly contradictory) rules for different situations <ref> [88, pages 204-211] </ref>. Australian Defence Force Academy Page 3 for new problems created by technological progress, problems so new that there is no committee of experts to interview.
Reference: [89] <author> Naohiro Hondo, Hitoshi Iba, and Yukinori Kakazu. </author> <title> Sharing and refinement for reusable subroutines of genetic programming. </title> <booktitle> In Proceedings of the 1996 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pages 565-570. </pages> <publisher> IEEE Press, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: This approach is known as "genetic programming" [101] [102] [103]. Sub-branches of these individual parse trees can be crossed over, to emulate genetic recombination. Australian Defence Force Academy Page 57 There have been variations on this theme <ref> [89] </ref>, including representing neural networks as tree-like structures [28]. In these two studies, the "modularisation" is not driven by a process like GA speciation: it is done by cutting off pieces of individuals in a more ad hoc manner, and storing them off-line.
Reference: [90] <author> Norman F. Hughes. </author> <title> The Enigma of Angiosperm Origins, </title> <booktitle> volume 1 of Cambridge Paleobiology Series. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1994. </year>
Reference-contexts: In nature, examples exist of evolutionary "arms races" ending in a design so good that further change is not economical. Insects provide numerous examples: the Orthoptera (grasshoppers and locusts) have been the stable end of an arms race which grasses having been fighting since before the dinosaurs <ref> [90, page 45] </ref>, while the Isoptera (termites) have been causing trees to fall over since the Cretaceous period that ended the age of the dinosaurs 4 This is the sort of end-product that we want co-evolutionary learning to create | no further improvement is possible.
Reference: [91] <author> M. M. Van Hulle and G. A. Orban. </author> <title> The EDANN concept: a modular artificial neural network model for biological vision and image processing. </title> <booktitle> In World Congress on Neural Networks-San Diego, </booktitle> <volume> volume 4, </volume> <pages> page 320. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1994. </year> <institution> Australian Defence Force Academy Page 175 </institution>
Reference-contexts: designed a flying machine, they did not attempt an all-inclusive approach, but instead decomposed the solution into separate parts to solve particular aspects of the problem: lift and drag, lateral stability, and propulsion [66] [117, pages 5, 15]. * The success of modular artificial neural networks at image processing [55] <ref> [91] </ref> and speech recognition [75] are further examples of how the modular approach can solve complicated problems. A closely related study found combining several high-quality neural networks gave better results than only one high quality neural network [187]. <p> The approach we present is particularly suited to many control and management problems, the simplest of which is strategy acquisition for a game. Previous Work There are many examples of manual modularisation, including artificial neural networks for speech [75] and vision [55] <ref> [91] </ref> recognition. However, these depend on human expertise to modularise. In Section 2.7, we describe some studies of modularisation in evolutionary learning. Some do not automatically decompose the solution into relevant parts | human decisions do that [92] [136] [137]. <p> Many studies use the word "modular" to describe systems that manage a complicated engineering task by separately dealing with different aspects of the problem. If we only consider neural networks, modular solutions have attempted such diverse problems as stock market prediction [98], vision [55] <ref> [91] </ref>, speech recognition [75], learning backgammon [14], and a six-legged walking robot [33]. Designing a modular system has relied on human expertise (often a committee) to manually divide a system into specialised parts. This relies on their opinions about the inherent structure of the problem, in an ad hoc manner. <p> The system described here is specific to learning game strategies, to validate the modular approach. But speciation as automatic modularisation can solve more realistic and complicated problems. Modular neural networks have been effective in many problems [55] <ref> [91] </ref>. Evolving neural networks with genetic algorithms has become an increasingly mature field [57] [125] [183]. This indicates that a modular approach to evolving neural networks should be a fertile field for improved learning.
Reference: [92] <author> Philip Husbands and Frank Mill. </author> <title> Simulated co-evolution as the mechanism for emergent planning and scheduling. </title> <editor> In Richard K. Belew and Lashon B. Booker, editors, </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pages 264-270. </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1991. </year>
Reference-contexts: This co-evolutionary approach is a suitable learning method for a range of problems. The simplest example of these is strategy acquisition for games of conflict. More serious examples include stock trading [132], creating a sorting algorithm [82], scheduling a manufacturing process <ref> [92] </ref>, and many other control, management, and military problems where outcomes are partly determined by the actions of others. This thesis demonstrates that, unfortunately, the most straightforward implementation of co-evolutionary learning can give poor generalisation ability. <p> However, these depend on human expertise to modularise. In Section 2.7, we describe some studies of modularisation in evolutionary learning. Some do not automatically decompose the solution into relevant parts | human decisions do that <ref> [92] </ref> [136] [137]. Another study uses GA speciation, but does not utilise the diverse expertise thus formed [144]. Another approach [4] [100] [104] randomly selects and stores parts of individuals off-line. This ties in loosely with what are called "cultural algorithms" [141] [140]. <p> It is this approach that this thesis will follow. In this section, we review previous attempts to use genetic algorithms for modular design. 2.7.2 Manual Modularisation Husbands et al. <ref> [92] </ref> [93] use several separate but co-evolving GA populations, each of which solves a different aspect of the problem. <p> Each separate GA population represents a component to be manufactured (actually, a process plan to manufacture a component). The different populations co-evolve in that their fitness functions take into account the use of shared resources in their common world, namely a model of a machine shop <ref> [92, page 266] </ref>. There is another GA population of "arbitrators", which resolve conflicts between members of different populations: their fitness depends on how well they achieve this. This study is a good example of the practical benefits of co-evolutionary learning. <p> This is similar to Husbands et al. <ref> [92] </ref> in that a human decides (in advance) what each GA population will do. Potter et al. [137] did a similar study, to evolve rule sets for a sequential decision problem. Separate GA populations develop their own rule sets. <p> Can a GA speciation method from Section 2.6 dynamically cause separate sub-populations to form, so that human designers don't have to allocate modules in advance? This would relieve humans of the chore of modularising the solution, and improve upon the work of Husbands et al. <ref> [92] </ref> [93] and Potter et al. [136] [137]. Returning to the baseball example: if you knew nothing about baseball except the rules, then manually decomposing a solution may not give a good decomposition. <p> Knowledge of these flaws allows users to avoid the pitfalls, as well as adding to the body of knowledge that allows the future design of improved GA speciation methods. 6.3 Modularisation Previous studies have manually created a modular system, deciding in advance which co-evolving GA sub-population would do what <ref> [92] </ref> [136] [137]. Also, some previous studies have used speciation with co-evolution, to create specialised solutions [144] [160], without considering the problem of how to re-combine that specialised expertise.
Reference: [93] <author> Philip Husbands, Frank Mill, and Stephen Warrington. </author> <title> Genetic algorithms, production plan optimisation and scheduling. </title> <editor> In Hans-Paul Schwefel and Reinhard Manner, editors, </editor> <title> Parallel Problem Solving from Nature: </title> <booktitle> first workshop, volume 496 of Lecture Notes in Computer Science, </booktitle> <pages> pages 80-84. </pages> <publisher> Springer-Verlag, </publisher> <month> October </month> <year> 1990. </year>
Reference-contexts: It is this approach that this thesis will follow. In this section, we review previous attempts to use genetic algorithms for modular design. 2.7.2 Manual Modularisation Husbands et al. [92] <ref> [93] </ref> use several separate but co-evolving GA populations, each of which solves a different aspect of the problem. <p> Can a GA speciation method from Section 2.6 dynamically cause separate sub-populations to form, so that human designers don't have to allocate modules in advance? This would relieve humans of the chore of modularising the solution, and improve upon the work of Husbands et al. [92] <ref> [93] </ref> and Potter et al. [136] [137]. Returning to the baseball example: if you knew nothing about baseball except the rules, then manually decomposing a solution may not give a good decomposition. Instead of pitching, batting, fielding, a committee of inexperienced humans might manually decompose into throwers, receivers, and guards.
Reference: [94] <author> Edwin Hutchins and Brian Hazlehurst. </author> <title> Learning in the cultural process. </title> <editor> In Christopher G. Langton, Charles Taylor, J. Doyne Farmer, and Steen Rasmussen, editors, </editor> <booktitle> Artificial Life 2, </booktitle> <pages> pages 689-706. </pages> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: In these two studies, the "modularisation" is not driven by a process like GA speciation: it is done by cutting off pieces of individuals in a more ad hoc manner, and storing them off-line. This very similar to what are loosely called cultural algorithms [10] [60] [61] <ref> [94] </ref> [140] [141]: attributes of individuals are copied into an off-line cache, called a library or "belief space", which influences newer individuals. The belief space itself can undergo variation and selection. <p> Page 106 University College, The University of New South Wales Culture could play a critical role here in allowing adaptive specialisation without the genetic speciation that irreversibly partitions the population. Richard K. Belew [10] (His italics) A few studies have been done on cultural evolution [10] [60] [61] <ref> [94] </ref> [140] [141] and this innovative approach has performed well on certain problems. Nonetheless, more comparison studies are needed. New GA speciation methods may overcome the flaws of fitness sharing, including the one discovered here.
Reference: [95] <author> Richard L. Ingraham. </author> <title> A Survey of Nonlinear Dynamics: Chaos Theory. </title> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1992. </year>
Reference-contexts: A useful concept is an attracting set: Definition D.1 (<ref> [95, page 101] </ref>) An attracting set fl is a closed and invariant set which has a neighbourhood V such that, from the continuous Equation A.5, if p (0) 2 V then p (t) ! fl as t ! 1 [95, page 101]. That is, the motion of the system is attracted to the set fl. Definition D.1 can be strengthened by requiring extra properties, to define strange attractors. <p> Definition D.1 can be strengthened by requiring extra properties, to define strange attractors. We shall not try to be too precise, because there is not yet agreement in the literature on what these extra properties should be <ref> [95, page 20] </ref>. Loosely speaking, an attractor need not be a single point, but can be a complicated set. An attractor need not be asymptotically stable.
Reference: [96] <author> Stuart A. Kauffman and Sonke Johnsen. </author> <title> Co-evolution at the edge of chaos: Coupled fitness landscapes, </title> <editor> poised states, and co-evolutionary avalanches. In Christopher G. Langton, Charles Taylor, J. Doyne Farmer, and Steen Rasmussen, editors, </editor> <booktitle> Artificial Life 2, volume 10 of Santa Fe Institute Studies in the Sciences of Complexity, </booktitle> <pages> pages 325-369. </pages> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: However, some co-evolutionary learning systems [108] [132] have displayed sudden "mass extinctions" | a high-quality strategy dominates the population for a long period of time, and suddenly dies out. An explanation for this behaviour is not yet complete, and is an active area of research [58] <ref> [96] </ref> [173]. Australian Defence Force Academy Page 5 This Thesis Extends Knowledge Chapter 3 examines the generalisation ability of expertise produced by a simple coevolutionary system.
Reference: [97] <author> A. Khotanzad, Rey-Chue Hwang, A. Abaye, and D. Maratukulam. </author> <title> An adaptive modular artificial neural network hourly load forecaster and its implementation at electric utilities. </title> <journal> IEEE Transactions on Power Systems, </journal> <volume> 10(3) </volume> <pages> 1716-1722, </pages> <year> 1995. </year>
Reference-contexts: For example, consider a six-legged robot learning to walk. One module per leg seems a sensible way to divide the solution, and it gives good performance [21] [33]. However, a system for predicting electricity demand uses modules to separately process the hourly, daily, and weekly demand <ref> [97] </ref>. Is this the best division? If not, what is better? Also, manual modularisation becomes even more ad hoc on "knowledge-lean" or "black box" problems, for which existing expertise is unavailable.
Reference: [98] <author> T. Kimoto, K. Asakawa, M. Yoda, and M. Takeoka. </author> <title> Stock market prediction system with modular neural networks. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <volume> volume 1. </volume> <publisher> IEEE Press, </publisher> <year> 1990. </year>
Reference-contexts: Many studies use the word "modular" to describe systems that manage a complicated engineering task by separately dealing with different aspects of the problem. If we only consider neural networks, modular solutions have attempted such diverse problems as stock market prediction <ref> [98] </ref>, vision [55] [91], speech recognition [75], learning backgammon [14], and a six-legged walking robot [33]. Designing a modular system has relied on human expertise (often a committee) to manually divide a system into specialised parts.
Reference: [99] <author> Kenneth E. Kinnear. </author> <title> Generality and difficulty in genetic programming: Evolving a sort. </title> <editor> In Lawrence Davis, editor, </editor> <booktitle> Proceedings of the Fifth International Conference on Genetic Algorithms. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Previous Work Axelrod [7] and others [27] [54] [108] have studied co-evolutionary learning of IPD, and co-evolution has been used to learn other games [46] [144] [160] and other tasks [81] [82] <ref> [99] </ref>. However, some co-evolutionary learning systems [108] [132] have displayed sudden "mass extinctions" | a high-quality strategy dominates the population for a long period of time, and suddenly dies out. An explanation for this behaviour is not yet complete, and is an active area of research [58] [96] [173].
Reference: [100] <author> Kenneth E. Kinnear. </author> <title> Alternatives in automatic function definition. </title> <editor> In Kenneth E. Kinnear, editor, </editor> <booktitle> Advances in Genetic Programming, </booktitle> <pages> pages 119-142. </pages> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: In Section 2.7, we describe some studies of modularisation in evolutionary learning. Some do not automatically decompose the solution into relevant parts | human decisions do that [92] [136] [137]. Another study uses GA speciation, but does not utilise the diverse expertise thus formed [144]. Another approach [4] <ref> [100] </ref> [104] randomly selects and stores parts of individuals off-line. This ties in loosely with what are called "cultural algorithms" [141] [140]. This approach evaluates the randomly-selected parts (directly or indirectly) that are stored off-line, in the hope that this will produce useful specialisation. <p> These parts would then be re-introduced to the population later. This made learning faster, apparently because useful components could be protected from disrup tion. * The other approach <ref> [100] </ref> [104] also randomly selected parts of evolving individuals, stored them off-line, and re-introduced them into the population. But these individuals were represented as parse trees in a computer language 21 20 This approach is known as "evolutionary programming" (EP) [53].
Reference: [101] <author> John R. Koza. </author> <title> Genetic programming: A paradigm for genetically breeding populations of computer programs to solve problems. </title> <type> Technical Report STAN-CS-90-1314, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1990. </year>
Reference-contexts: After early attempts to emulate the learning process of evolution [53, pages 70-80], today evolutionary computation includes a range of approaches. The labels they go by include genetic algorithms (GA) [65] [86], evolutionary strategies (ES) [8], evolutionary programming (EP) [182], and genetic programming (GP) <ref> [101] </ref>. This thesis concentrates on genetic algorithms, but is broadly applicable to similar algorithms. Some critics assert that artificial evolution cannot be intelligent, because natural evolution is a slow process [123, page 71]. <p> This approach is known as "genetic programming" <ref> [101] </ref> [102] [103]. Sub-branches of these individual parse trees can be crossed over, to emulate genetic recombination. Australian Defence Force Academy Page 57 There have been variations on this theme [89], including representing neural networks as tree-like structures [28].
Reference: [102] <author> John R. Koza. </author> <title> Genetic evolution and co-evolution of computer programs. </title> <editor> In Christopher G. Langton, Charles Taylor, J. Doyne Farmer, and Steen Rasmussen, editors, </editor> <booktitle> Artificial Life 2, </booktitle> <volume> volume 10, </volume> <pages> pages 603-629. </pages> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: This approach is known as "genetic programming" [101] <ref> [102] </ref> [103]. Sub-branches of these individual parse trees can be crossed over, to emulate genetic recombination. Australian Defence Force Academy Page 57 There have been variations on this theme [89], including representing neural networks as tree-like structures [28].
Reference: [103] <author> John R. Koza. </author> <title> The genetic programming paradigm: Genetically breeding populations of computer programs to solve problems. </title> <editor> In Branco Soucek, editor, </editor> <booktitle> Dynamic, Genetic, and Chaotic Programming, chapter 10, </booktitle> <pages> pages 203-321. </pages> <publisher> Wiley, </publisher> <year> 1992. </year>
Reference-contexts: This approach is known as "genetic programming" [101] [102] <ref> [103] </ref>. Sub-branches of these individual parse trees can be crossed over, to emulate genetic recombination. Australian Defence Force Academy Page 57 There have been variations on this theme [89], including representing neural networks as tree-like structures [28].
Reference: [104] <author> John R. Koza. </author> <title> Genetic Programming 2: Automatic Discovery of Reusable Programs. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: In Section 2.7, we describe some studies of modularisation in evolutionary learning. Some do not automatically decompose the solution into relevant parts | human decisions do that [92] [136] [137]. Another study uses GA speciation, but does not utilise the diverse expertise thus formed [144]. Another approach [4] [100] <ref> [104] </ref> randomly selects and stores parts of individuals off-line. This ties in loosely with what are called "cultural algorithms" [141] [140]. This approach evaluates the randomly-selected parts (directly or indirectly) that are stored off-line, in the hope that this will produce useful specialisation. <p> These parts would then be re-introduced to the population later. This made learning faster, apparently because useful components could be protected from disrup tion. * The other approach [100] <ref> [104] </ref> also randomly selected parts of evolving individuals, stored them off-line, and re-introduced them into the population. But these individuals were represented as parse trees in a computer language 21 20 This approach is known as "evolutionary programming" (EP) [53].
Reference: [105] <author> Vladik Kreinovich, Chris Quintana, and Olac Fuentes. </author> <title> Genetic algorithms: What fitness scaling is optimal? Cybernetics and Systems, </title> <booktitle> 24(1) </booktitle> <pages> 9-26, </pages> <year> 1993. </year>
Reference-contexts: We briefly review why scaling is needed in Section 4.2: it is known that for many problems, the best scaling function is a power function, (f ) = f fi for some fi <ref> [105] </ref>. The dilemma is that: * High values of fi cause premature convergence and missed peaks; * Low values cause subpopulations to form around peaks, but not at them. This resembles the effect of deception. <p> That is, the fitness of every individual is adjusted according to some function, and it is that adjusted fitness that the GA works with. However, choosing a scaling function is a procedure lacking theoretical conviction [65, page 124] <ref> [105] </ref>. Choosing an appropriate scaling is very important ... (T)he existing pro cedures are somewhat ad hoc; i.e., they lack deep theoretical motivation. Kreinovich et al. [105, page 13] We investigate fitness sharing's performance at multimodal optimisation, using a simple non-deceptive search space. <p> However, choosing a scaling function is a procedure lacking theoretical conviction [65, page 124] [105]. Choosing an appropriate scaling is very important ... (T)he existing pro cedures are somewhat ad hoc; i.e., they lack deep theoretical motivation. Kreinovich et al. <ref> [105, page 13] </ref> We investigate fitness sharing's performance at multimodal optimisation, using a simple non-deceptive search space. We demonstrate the need for a scaling function, and review the optimal form of a scaling function. We discover a dilemma whose effects resemble deception (we described deception in Section 2.2.5). <p> This is shown conceptually in Figure 4.7. To encourage subpopulations to go to an optimum, instead of staying around an optimum, one attractive method is to use a scaling function [116, page 143]. A scaling function is often a power function [65, page 124] [67] <ref> [105, page 12] </ref>. <p> This demonstrates why we need scaling. 4.2.6 Optimal Scaling Function Why a power function? Choosing a scaling function is an ad hoc procedure that lacks a solid theoretical foundation [65, page 124]. Kreinovich et al. <ref> [105] </ref> give the form of the optimal scaling function, given the attributes of certain properties of the search problem and presuming the scaling function's dimension. In this section, we will merely quote the appropriate parts of Kreinovich et al. [105], which gives a rigorous proof. <p> Kreinovich et al. <ref> [105] </ref> give the form of the optimal scaling function, given the attributes of certain properties of the search problem and presuming the scaling function's dimension. In this section, we will merely quote the appropriate parts of Kreinovich et al. [105], which gives a rigorous proof. A scaling function's optimal form depends on the attributes of the optimality Page 94 University College, The University of New South Wales the payoff radius, and zero outside. criterion that judges which scaling function gives a better result. <p> So Definition 4.3 is not satisfied. Our criterion is not shift-invariant. Different search problems have different criteria, which may have different in-variances. Kreinovich et al. <ref> [105, page 18] </ref> consider each combination of invariances (or lack of them), and find that each combination gives a different optimal scaling Australian Defence Force Academy Page 97 function. Commonly-used scaling functions from earlier studies are derived, which explains why they worked so well in those earlier studies. <p> A one-dimensional basis is often called scalar. For our optimality criterion, which is unit-invariant but not shift-invariant, Kreinovich et al. <ref> [105, page 18] </ref> have shown that: 1. If we assume the scaling function is scalar, i.e., one-dimensional, then the optimal family of scaling functions is (z) = z fi and its multiples, for some fi. <p> Kreinovich et al. <ref> [105] </ref> do not consider the possibility of a scaling function with an infinite basis. There is however no way of knowing if a particular problem's optimal family of scaling functions has a scalar, finite, or infinite basis. <p> For practicality's sake, we will consider a single-dimensional scaling function of the form (z) = z fi . It is worth noting that Kreinovich et al. <ref> [105] </ref> do not tell us what power fi works best for a particular problem. <p> However, we cannot tune the scaling power to achieve this without one of the above problems, and the best we can do is about 8 peaks found instead of the expected 9.77 peaks. Since Kreinovich et al. <ref> [105] </ref> show that the best single-dimensional scaling function for our problem is a power function, and no power meets our expectations, then this indicates no suitable single-dimensional scaling function exists.
Reference: [106] <author> Wei-Po Lee, John Hallam, and Henrik H. Lund. </author> <title> A hybrid gp/ga approach to co-evolving controllers and robot bodies to achieve fitness-specific tasks. </title> <booktitle> In Proceedings of the 1996 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pages 384-389. </pages> <publisher> IEEE Press, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: There are several other uses of co-evolutionary genetic algorithms besides learning game strategies: for example, they can be used for constrained optimisation [133], or for designing a robot body and its software controller <ref> [106] </ref>. 2.5.2 Single Population: The Evolution of Cooperation We discuss co-evolution with a single population, where every individual's fitness is determined by its interaction with the other individuals in the same population.
Reference: [107] <author> R. J. Lincoln, G. A. Boxshall, and P. F. Clark. </author> <title> A Dictionary of Ecology, Evolution and Systematics. </title> <publisher> Cambridge University Press, </publisher> <year> 1982. </year> <institution> Page 176 University College, The University of New South Wales </institution>
Reference-contexts: The problem of GA search in deceptive spaces is a continuing area of research. The word epistasis is sometimes used. In biology, a gene is said to be epistatic if its presence or absence affects another gene, i.e., if a gene masks the effect of another gene <ref> [107] </ref>. In evolutionary computation, it usually expresses how the contribution to fitness of a schema depends on the presence or absence of other schema. Problems with little epistasis can be solved one bit at a time, perhaps by hill-climbing.
Reference: [108] <author> Kristian Lindgren. </author> <title> Evolutionary phenomena in simple dynamics. </title> <editor> In Christopher G. Langton, Charles Taylor, J. Doyne Farmer, and Steen Rasmussen, editors, </editor> <booktitle> Artificial Life 2, volume 10 of Santa Fe Institute Studies in the Sciences of Complexity, </booktitle> <pages> pages 295-312. </pages> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: This stalls the co-evolutionary "arms race", and makes those over-specialised individuals vulnerable to novel opponents. So poor is their generalisation ability, that even random mutation can produce an effective opponent, causing sudden mass extinctions that have been previously observed [58] <ref> [108] </ref>. This thesis explains this problem in detail in Section 3.2. The rest of the thesis sets out to solve this problem. In Chapter 5 we present a practical and effective solution that brings co-evolutionary learning closer to its potential, after evaluating suitable tools in Chapter 4. <p> Although we use it merely as a test problem, IPD is a simplified version of many significant real-world problems, from recycling programs [63, page 62] to trench warfare [6, pages 73-87] and superpower confrontation [18]. Previous Work Axelrod [7] and others [27] [54] <ref> [108] </ref> have studied co-evolutionary learning of IPD, and co-evolution has been used to learn other games [46] [144] [160] and other tasks [81] [82] [99]. However, some co-evolutionary learning systems [108] [132] have displayed sudden "mass extinctions" | a high-quality strategy dominates the population for a long period of time, and <p> Previous Work Axelrod [7] and others [27] [54] <ref> [108] </ref> have studied co-evolutionary learning of IPD, and co-evolution has been used to learn other games [46] [144] [160] and other tasks [81] [82] [99]. However, some co-evolutionary learning systems [108] [132] have displayed sudden "mass extinctions" | a high-quality strategy dominates the population for a long period of time, and suddenly dies out. An explanation for this behaviour is not yet complete, and is an active area of research [58] [96] [173]. <p> We will study a co-evolving GA learning IPD in more detail in Chapter 3. There has been much recent research on evolutionary learning in the 2-player Iterated Prisoner's Dilemma (2IPD) [7] [27] [37] [52] [54] <ref> [108] </ref>. One of the more effective strategies for IPD is "Tit-for-Tat" [6]. This simple strategy cooperates at the first iteration, and after that does whatever the opponent did in the previous iteration. That is, if the opponent defected on the previous iteration, then Tit-for-tat will defect. <p> A study by Chess [27] also saw similar behaviour when an individual plays against a single random opponent, instead of the entire population. Thus, co-evolution appears to be a robust creator of high-payoff strategies. Lindgren <ref> [108] </ref> also did an Axelrod-style study on IPD. <p> Axelrod [6] investigated evolution and the IPD. He used a GA population of strategies, in which each plays IPD with every other strategy in the population [7, page 38]. He found that this dynamic environment produced high scorers. Similar studies give similar results [27] [37] [52] [54] <ref> [108] </ref>. This raises a serious question: can a strategy produced by co-evolving in its own population succeed against strategies not in its own population? That is, how well does co-evolutionary learning generalise? Generalisation is an important aspect of machine learning. <p> That distinguishes this chapter from previous studies of co-evolutionary learning, especially those that looked at IPD from the perspective of game playing. 3.1.1 Mass Extinctions in Previous Research Lindgren <ref> [108] </ref> studied a system similar to Axelrod's [7, page 38]. He showed that when a population of strategies plays IPD against its own members, high-quality Australian Defence Force Academy Page 63 strategies dominate the population for long periods of time, but occasionally they are suddenly wiped out and replaced. <p> In particular, the large extinctions that appear in these simulations should be studied in more detail, since these collapses are triggered by the dynamical system itself and do not need external catastrophes for their explanation. Kristian Lindgren <ref> [108, page 310] </ref>. Similar collapses have been observed in other co-evolutionary simulations [58] [132]. These bear a resemblance to the "punctuated equilibria" of natural evolution [42]. <p> Similar collapses have been observed in other co-evolutionary simulations [58] [132]. These bear a resemblance to the "punctuated equilibria" of natural evolution [42]. Although that term is not a catch-all phrase for any infrequently-changing system, we will it as a convenient shorthand for collapses like those observed by Lindgren <ref> [108] </ref>. An explanation for these collapses is not yet complete. However, Fogel [54] observed that the highest scorer (in a co-evolutionary population) sometimes contained significant flaws. We suspect that Lindgren's mass extinctions [108] are caused by co-evolution producing strategies that are not robust, i.e., the strategies did well against the local <p> for any infrequently-changing system, we will it as a convenient shorthand for collapses like those observed by Lindgren <ref> [108] </ref>. An explanation for these collapses is not yet complete. However, Fogel [54] observed that the highest scorer (in a co-evolutionary population) sometimes contained significant flaws. We suspect that Lindgren's mass extinctions [108] are caused by co-evolution producing strategies that are not robust, i.e., the strategies did well against the local population, but when mutation produced something new, they generalised poorly and went extinct. <p> suspicion that co-evolution produces strategies that are not robust is not obviously explained in terms of a fixed evaluation function. 3.1.2 Causes and Possible Solutions of Poor Generalisation In this chapter, we study a co-evolutionary learning system similar to Axelrod's [7], and find a novel reason for Lindgren's catastrophic collapses <ref> [108] </ref>. It turns out that when the whole population has substantially converged to the same high-quality strategy (due to genetic drift, as described in Section 2.2.6), the homogeneity in their behaviour causes the "arms race" to stall, and certain desirable features atrophy. <p> We represent individual IPD strategies as rule sets, following Axelrod [7] and Lindgren <ref> [108] </ref>. In Section 3.4.1, we use an improved representation which has shorter length and is applicable to the N -player IPD. However, we could use any convenient representation, and our results are relevant to co-evolutionary learning in general. <p> In a similar simulation, Page 70 University College, The University of New South Wales Average Best Generations (time steps) P erformance of est a erage 500450400350300250200150100500 3.4 3 2.6 2.2 cooperated for so long that they can be suckered. Lindgren <ref> [108] </ref> found that collapses like this can happen repeatedly if the simulation runs for long enough. It should be pointed out that collapses like Figure 3.6 are relatively rare | or equivalently, the periods of stability are long, as Lindgren observed [108, page 310]. <p> Lindgren [108] found that collapses like this can happen repeatedly if the simulation runs for long enough. It should be pointed out that collapses like Figure 3.6 are relatively rare | or equivalently, the periods of stability are long, as Lindgren observed <ref> [108, page 310] </ref>. Most of our runs resemble Figure 3.3. We only observed one collapse like collapse (shown in Figure 3.7), plus a few borderline cases which were not nearly as dramatic. We emphasise the relative rarity of these collapses. <p> Page 78 University College, The University of New South Wales There has been much research on the 2IPD with evolutionary learning [7] [27] [37] [52] [54] <ref> [108] </ref>. However, few experimental studies have been carried out on the NIPD in spite of its importance and qualitative difference from the 2IPD, as described in Section 2.3.4. We merely use NIPD as another learning problem while we study the generalisation ability of co-evolutionary learning. <p> This demonstrates again that adding to the round-robin does not improve generalisation ability, verifying Section 3.3.2. 3.5 Conclusions on Simple Co-Evolution In Section 3.2, in order to study generalisation in simple co-evolutionary learning, we implemented a co-evolutionary GA similar to Axelrod's [7, page 38] and Lindgren's <ref> [108] </ref>. We found that the strategies created can have such poor generalisation ability that spectacular collapses occur, as in Figure 3.6. This is caused when a random mutation (much less than a clever human) exploited the navety created in a coevolutionary population without diversity. Lindgren [108] observed similar collapses, and Fogel <p> [7, page 38] and Lindgren's <ref> [108] </ref>. We found that the strategies created can have such poor generalisation ability that spectacular collapses occur, as in Figure 3.6. This is caused when a random mutation (much less than a clever human) exploited the navety created in a coevolutionary population without diversity. Lindgren [108] observed similar collapses, and Fogel [54] noticed that strategies produced by co-evolution can do well in their closed population, but can fail against outsiders. <p> This chapter's work has a wider significance than it may first seem. The test problem used in this chapter is again the game of Iterated Prisoner's Dilemma (IPD), which we described in Section 2.3.3. IPD has been widely used as a test for evolutionary learning [7] [27] [37] [54] <ref> [108] </ref>. Although this test problem is merely a game, the results of this chapter have much wider relevance. Games of conflict are examples of a large body of important problems where each player's outcome depends on the actions of other similar players. <p> As a canonical GA finds only one solution at a time, it over-specialises to one particular strategy, becoming vulnerable to novel opponents. This can cause mass extinctions as in Figure 5.3, when mutation eventually creates a sufficiently novel strategy. Such collapses have been observed before <ref> [108] </ref> [132], and Chapter 3 sheds new light on why they happen. Unaware of this reason, some studies have nonetheless used both speciation and co-evolution [144] [160]. This prevents over-specialisation and improves generalisa-tion ability. <p> But here we take a broader view, returning to the questions originally asked in Section 1.3. 6.1 Generalisation Chapter 3 takes a canonical GA and uses a co-evolutionary evaluation function, a recipe used in many previous studies [7, page 38] [27] [37] [52] [54] <ref> [108] </ref>. Also, some studies of co-evolutionary learning have observed mass extinctions [58] [108] [132]. An explanation for mass extinctions in co-evolution is currently incomplete. Section 3.2 found that in a canonical GA with co-evolution, genetic drift to a single strategy caused over-specialisation to that single strategy. <p> view, returning to the questions originally asked in Section 1.3. 6.1 Generalisation Chapter 3 takes a canonical GA and uses a co-evolutionary evaluation function, a recipe used in many previous studies [7, page 38] [27] [37] [52] [54] <ref> [108] </ref>. Also, some studies of co-evolutionary learning have observed mass extinctions [58] [108] [132]. An explanation for mass extinctions in co-evolution is currently incomplete. Section 3.2 found that in a canonical GA with co-evolution, genetic drift to a single strategy caused over-specialisation to that single strategy.
Reference: [109] <author> Kristian Lindgren. </author> <title> Evolutionary dynamics of spatial games. </title> <journal> Physica D, </journal> <volume> 75 </volume> <pages> 292-309, </pages> <year> 1994. </year>
Reference-contexts: But another found that more than two would cause convergence to a single optimum and the extinction of the others [2]. A more detailed version of this model allowed strategies of increasing sophistication <ref> [109] </ref>. It found that, for particular ranges of the payoffs in IPD, stable patches of different strategies emerge [109, page 309]. Why diversity emerges only for some variants of the game remains a mystery. <p> A more detailed version of this model allowed strategies of increasing sophistication [109]. It found that, for particular ranges of the payoffs in IPD, stable patches of different strategies emerge <ref> [109, page 309] </ref>. Why diversity emerges only for some variants of the game remains a mystery. But at least it demonstrates that it is feasible for a GA over a landscape to find multiple optima | but more seems to be needed to ensure reliability in finding multiple optima. <p> So although island models are useful at finding a single optimum faster [126] [163] [178], they have not yet been developed for finding multiple peaks. Australian Defence Force Academy Page 53 Landscape models can allow different sub-populations to form in different regions of the landscape <ref> [109, page 309] </ref>, but not reliably [39, page 262]. This suggests that this method may in future provide a working method for finding multiple solutions.
Reference: [110] <author> David J. C. Mackay. </author> <title> Bayesian methods for supervised neural networks. </title> <editor> In Michael A. Arbib, editor, </editor> <booktitle> The Handbook of Brain Theory and Neural Networks, </booktitle> <pages> pages 144-148. </pages> <publisher> MIT Press, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: There are many ways to adjudicate among a repertoire of different strategies. A gating algorithm is a generic term for an algorithm that adjudicates among different experts. Methods for choosing between different experts include: * Voting [76]; * Committees <ref> [110] </ref> [167] [168]; * Agent teams [12]; * Stacked generalisation [47] [180]; * Model averaging [23]; * Monte Carlo methods [129]; Page 58 University College, The University of New South Wales Later in this thesis, in Section 5.3.1, we will use the simple approach of voting among the repertoire of strategies.
Reference: [111] <author> William G. Macready and David H. Wolpert. </author> <title> On 2-armed gaussian bandits and optimization. </title> <type> Technical Report SFI TR 96-03-009, </type> <institution> The Santa Fe Institute, </institution> <address> Santa Fe NM 87501, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: How well a GA balances these is an interesting topic [65, pages 36-38] [86, page 66] <ref> [111] </ref>. Section 4.3.2 will look more closely at this issue. A disadvantage: while a GA can rapidly find near-optimal solutions, other methods are better at going from near-optimal to optimal [71] [87]. <p> As mentioned in Section 2.2.2, it is desirable for search algorithms to balance the priorities of exploration and exploitation in a near-optimal manner [65, pages 36-38] [86, page 66]. The so-called "2-armed bandit problem" is a convenient way to analyse different algorithms [11] <ref> [111] </ref>.
Reference: [112] <author> Samir W. Mahfoud. </author> <title> Crowding and preselection revisited. </title> <editor> In R. Manner and B. Manderick, editors, </editor> <booktitle> Parallel Problem Solving from Nature 2, </booktitle> <pages> pages 27-36. </pages> <publisher> North-Holland, </publisher> <year> 1992. </year>
Reference-contexts: Also, empirical results [45] indicate that crowding is not as effective at finding multiple optima as another GA speciation method, fitness sharing (to be described in Section 2.6.7). Mahfoud refined crowding to create deterministic crowding <ref> [112] </ref> [113] [117, page 79], which works as follows: 1. First, it randomly groups the population in pairs (no mating restrictions); 2. Each pair crosses over to produce two offspring; 3. Each offspring is compared with the parent it most resembles; 4.
Reference: [113] <author> Samir W. Mahfoud. </author> <title> Crossover interactions among niches. </title> <booktitle> In Proceedings of the First IEEE Conference on Evolutionary Computation, </booktitle> <volume> volume 1, </volume> <pages> pages 188-193. </pages> <publisher> IEEE Press, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: This happens by avoiding the creation of lethal individuals by crossing over individuals with very different attributes [117, page 83], although this may be desirable in some search problems <ref> [113, section 5] </ref> [117, page 50]. Mating restrictions are not the primary cause of speciation in these cases. <p> Also, empirical results [45] indicate that crowding is not as effective at finding multiple optima as another GA speciation method, fitness sharing (to be described in Section 2.6.7). Mahfoud refined crowding to create deterministic crowding [112] <ref> [113] </ref> [117, page 79], which works as follows: 1. First, it randomly groups the population in pairs (no mating restrictions); 2. Each pair crosses over to produce two offspring; 3. Each offspring is compared with the parent it most resembles; 4. <p> In Section 4.2 of this thesis, we demonstrate that even without the above limitations to fitness sharing, in a non-deceptive space where the peaks are equidistant and equal-valued, fitness sharing can still miss peaks. To get around these problems, some schemes do not use a fixed sharing radius [78] <ref> [113] </ref> [159] [162]. Another study applies the restricted mating approach of Section 2.6.5 to fitness sharing, giving the Simple Subpopulation Scheme (SSS) [162]. <p> But again, this parameter does not have a critical effect on performance. 4.2.4 Random or Assortative Crossover As mentioned in Section 2.6.5, assortative crossover can improve the performance of a GA speciation method. Using a different GA speciation method, Mahfoud <ref> [113, section 5] </ref> found that hybrids of radically different individuals helped the search. He conjectured that random crossover was better. This was for a complex deceptive search space. <p> Practical improvements follow theoretical understanding. 4.3.3 Further Work on GA Speciation Other extensions to the GA may be effective at finding multiple peaks. Implicit sharing [159], Mahfoud's deterministic crowding <ref> [113] </ref> [117, page 79], and Ronald's multiple solution technique [143], are examples of recent approaches to speciation that do away with the fixed niche radius of fitness sharing (Equation 2.30 on page 50).
Reference: [114] <author> Samir W. Mahfoud. </author> <title> Genetic drift in sharing methods. </title> <booktitle> In Proceedings of the First IEEE Conference on Evolutionary Computation, </booktitle> <volume> volume 1, </volume> <pages> pages 67-72. </pages> <publisher> IEEE Press, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: Although the issue of sample selection in Section 5.2.3 has been previously addressed [144], we believe the others may be novel. 1.4.4 Comparing Two GA Speciation Methods Significance There exist several GA speciation methods, which we describe in Section 2.6. Speci-ation can greatly delay genetic drift <ref> [114] </ref> [117, Section 8.2], and prevent the convergence and overspecialisation that can otherwise happen in co-evolutionary learning (found in Chapter 3) However, if the speciated GA misses some good strategies, then we get poor generalisation | as speciation finds more high-quality strategies, generalisation ability improves [38, page 92]. <p> This encourages search in unexplored regions, and causes subpopulations to form. Fitness sharing is, in effect, a parallelised version of Beasley et al.'s method 18 . Consider an individual i with fitness f i . Its niche count m i [65, page 191] <ref> [114] </ref> measures how many other individuals with which i shares fitness. The shared fitness f s f s f i (2.28) The niche count m i is calculated with a distance metric d ij that describes the difference between individuals i and j. <p> We present them in the opposite order to that in which they appeared, for ease of exposition. Page 50 University College, The University of New South Wales Specifically, to maintain the less fit of peaks i and j, we require <ref> [114, page 69] </ref>: s 1 min ( f i f j (2.31) This important flaw is often overlooked by simply assuming that we have perfect discrimination between different peaks [117, pages 106, 158]. <p> To overcome this problem, a speciated GA can automatically create different species. Speciation avoids genetic drift <ref> [114] </ref> [117, Section 8.2], and prevents the convergence and overspecialisation that otherwise happens. So a co-evolutionary GA with speciation is especially useful to acquire strategies for a game of conflict: not only can speciation prevent poor generalisation, but diverse species embody a repertoire of high-quality strategies [38].
Reference: [115] <author> Samir W. Mahfoud. </author> <title> Population sizing for sharing methods. </title> <type> Technical Report 94005, </type> <institution> Illinois Genetic Algorithms Laboratory, University of Illinois at Urbana-Champaign, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Assortative mating produces few hybrids between sub-populations, so we may neglect crossover when sizing the population. The minimum population size n for maintaining c sub-populations of equal fitness for G generations with probability fl is then <ref> [115, equation 13] </ref>: n = 1 c ) c ) Equation 4.3 says for c = 10 optima, maintained for G = 100 generations with probability fl = 0:999, the required population size is n = 131 individuals, so a Australian Defence Force Academy Page 89 0% 50% 100% Non paying
Reference: [116] <author> Samir W. Mahfoud. </author> <title> A comparison of parallel and sequential niching methods. </title> <editor> In Larry J. Eshelman, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 136-143. </pages> <booktitle> International Society for Genetic Algorithms, </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: If the niche radius is too large, modifying the search space can reduce the fitness of other optima that are close to the optimum just found <ref> [116, page143] </ref>. This makes the GA less likely to find those nearby optima. 5. Mahfoud [117, chapter 10] [116] compared Beasley et al.'s method [9] with other methods of finding multiple optima with a GA, discussed later in this section. <p> If the niche radius is too large, modifying the search space can reduce the fitness of other optima that are close to the optimum just found [116, page143]. This makes the GA less likely to find those nearby optima. 5. Mahfoud [117, chapter 10] <ref> [116] </ref> compared Beasley et al.'s method [9] with other methods of finding multiple optima with a GA, discussed later in this section. Mahfoud found that on the test problems used, the other methods found all optima faster. However, as promising as the iterated approach seems, Mahfoud [116] [117, chapter 10] has <p> Mahfoud [117, chapter 10] <ref> [116] </ref> compared Beasley et al.'s method [9] with other methods of finding multiple optima with a GA, discussed later in this section. Mahfoud found that on the test problems used, the other methods found all optima faster. However, as promising as the iterated approach seems, Mahfoud [116] [117, chapter 10] has shown that it is not as good as the parallel schemes we describe later in this section. <p> low migration is equivalent to multiple runs of a canonical GA, so this method will give results similar to iterated schemes like Page 46 University College, The University of New South Wales Beasley et al. [9], which are not as good as the parallel methods reviewed later in this section <ref> [116] </ref> [117, chapter 10]. A landscape or cellular GA is where each individual only interacts with a fraction of the entire GA population, instead of with all of them. There are many ways to decide which individuals interact with which. <p> Each pair crosses over to produce two offspring; 3. Each offspring is compared with the parent it most resembles; 4. If the offspring is better than that parent, it replaces it. Deterministic crowding has performed well on a variety of problems <ref> [116] </ref> [117, chapter 10]. <p> More important is s , the sharing radius or cutoff distance: two strings at a distance s or further apart do not share fitness. Mahfoud <ref> [116] </ref> [117, chapter 10] finds that the parallel approach of fitness sharing works better than the sequential scheme. <p> There are some methods which at this time could be improved further to be used in this way. Sequential runs of a normal GA (like the scheme of Beasley et al. [9]) seems promising, as it should be able to find all possible high-quality strategies. However, Mahfoud <ref> [116] </ref> [117, chapter 10] has shown that certain parallel methods (such as fitness sharing) perform better. This suggests that the sequential scheme needs further refinement. Island models can maintain different sub-populations only at very low migration rates [117, page 41]. <p> This suggests that this method may in future provide a working method for finding multiple solutions. The speciation methods suitable for finding multiple high-quality solutions are: * Fitness sharing [45] [67] and its variations [162]; * Implicit sharing [56] [159]; * Mahfoud's deterministic crowding <ref> [116] </ref> [117]; * More recent efforts, including Ronald's multiple solution technique [143]. Exactly which method we use is not our main focus: we hope to demonstrate that co-evolutionary learning with speciation is effective. Fitness sharing is the most widely-used of these, so we will study it first, in Section 4.2. <p> This is shown conceptually in Figure 4.7. To encourage subpopulations to go to an optimum, instead of staying around an optimum, one attractive method is to use a scaling function <ref> [116, page 143] </ref>. A scaling function is often a power function [65, page 124] [67] [105, page 12]. <p> Deception is only partly to blame. Mahfoud <ref> [116, page 143] </ref> also found that fitness sharing is easily attracted to non-optimal peaks around the optimum, without questioning why. He suggested scaling could fix this problem, a suggestion we have found to lead to the dilemma described here.
Reference: [117] <author> Samir W. Mahfoud. </author> <title> Niching Methods for Genetic Algorithms. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1995. </year>
Reference-contexts: These can then be separately adapted 1 . * When Orville and Wilbur Wright designed a flying machine, they did not attempt an all-inclusive approach, but instead decomposed the solution into separate parts to solve particular aspects of the problem: lift and drag, lateral stability, and propulsion [66] <ref> [117, pages 5, 15] </ref>. * The success of modular artificial neural networks at image processing [55] [91] and speech recognition [75] are further examples of how the modular approach can solve complicated problems. <p> Although the issue of sample selection in Section 5.2.3 has been previously addressed [144], we believe the others may be novel. 1.4.4 Comparing Two GA Speciation Methods Significance There exist several GA speciation methods, which we describe in Section 2.6. Speci-ation can greatly delay genetic drift [114] <ref> [117, Section 8.2] </ref>, and prevent the convergence and overspecialisation that can otherwise happen in co-evolutionary learning (found in Chapter 3) However, if the speciated GA misses some good strategies, then we get poor generalisation | as speciation finds more high-quality strategies, generalisation ability improves [38, page 92]. <p> Previous Work Some studies evaluate GA speciation methods only on simple test problems [151] [162], even though parallel hill-climbing is usually better on such easy test functions <ref> [117, page 202] </ref>. <p> Previous Work Some studies evaluate GA speciation methods only on simple test problems [151] [162], even though parallel hill-climbing is usually better on such easy test functions [117, page 202]. Another study compares some speciation methods, but conspicuously forbids the fitness sharing method from using a scaling function <ref> [117, pages 206-207] </ref> | this may not be a fair comparison, as fitness sharing worked well with scaling [67] (although Section 4.2 shows how scaling can cause problems for fitness Australian Defence Force Academy Page 7 sharing [36]). <p> Page 42 University College, The University of New South Wales Natural speciation is a complex process, and these two mechanisms are by no means the whole story. Under allopatric speciation, mating restrictions are a consequence of speciation and not a primary cause <ref> [117, page 47] </ref>. Once started, allopatric speciation is accentuated by the Principle of Competitive Exclusion [20, Section 4.11] which asserts that no two species that extract resources in the same manner can both survive indefinitely. <p> Even if you can get around the problem of "ghost peaks" in Figure 2.11, sequential niching could fail to find the last remaining peaks: once it has squashed most peaks, those remaining would be isolated and difficult to find <ref> [117, page 204] </ref>. 4. If the niche radius is too large, modifying the search space can reduce the fitness of other optima that are close to the optimum just found [116, page143]. This makes the GA less likely to find those nearby optima. 5. <p> If the niche radius is too large, modifying the search space can reduce the fitness of other optima that are close to the optimum just found [116, page143]. This makes the GA less likely to find those nearby optima. 5. Mahfoud <ref> [117, chapter 10] </ref> [116] compared Beasley et al.'s method [9] with other methods of finding multiple optima with a GA, discussed later in this section. Mahfoud found that on the test problems used, the other methods found all optima faster. <p> <ref> [117, chapter 10] </ref> [116] compared Beasley et al.'s method [9] with other methods of finding multiple optima with a GA, discussed later in this section. Mahfoud found that on the test problems used, the other methods found all optima faster. However, as promising as the iterated approach seems, Mahfoud [116] [117, chapter 10] has shown that it is not as good as the parallel schemes we describe later in this section. <p> to a single optimum as happens in a canonical GA? Empirical results find this is only possible at extremely low migration rates, of the order of 1% every 5 to 500 generations, and biological studies indicate that even lower migration rates are necessary to prevent convergence to a single optimum <ref> [117, page 41] </ref>. <p> migration is equivalent to multiple runs of a canonical GA, so this method will give results similar to iterated schemes like Page 46 University College, The University of New South Wales Beasley et al. [9], which are not as good as the parallel methods reviewed later in this section [116] <ref> [117, chapter 10] </ref>. A landscape or cellular GA is where each individual only interacts with a fraction of the entire GA population, instead of with all of them. There are many ways to decide which individuals interact with which. <p> By emulating the tag-template matching of DNA, some researchers have concatenated a tag and template onto a conventional GA bitstring genotype, so that individuals only cross over when their tags match each other's templates [65, pages 195-197] <ref> [117, pages 47-48] </ref>. To illustrate, individuals' genotypes contain both the main, functional part plus a template [65, page 195]: Template : Functional *10* : 1010 *00* : 0000 The asterisk (*) character can match either 0 or 1. <p> Unfortunately, tag-template GA approaches have not yet demonstrated that they can maintain diverse sub-populations, except in flat search spaces, i.e., no selection <ref> [117, page 47] </ref> [169, page 553]. <p> This is because mating restrictions do not prevent different species competing, resulting in the elimination of all but one identically-fit Page 48 University College, The University of New South Wales species due to competition, noisy selection, and all the other causes of convergence in a non-speciated GA <ref> [117, pages 49-50] </ref>. This suggests we cannot yet use the restricted-mating approach to find diverse high-payoff solutions. However, mating restrictions often improve the performance of other GA spe-ciation methods [45, page 49] [36, page 168] [121, Section 5.3]. <p> However, mating restrictions often improve the performance of other GA spe-ciation methods [45, page 49] [36, page 168] [121, Section 5.3]. This happens by avoiding the creation of lethal individuals by crossing over individuals with very different attributes <ref> [117, page 83] </ref>, although this may be desirable in some search problems [113, section 5] [117, page 50]. Mating restrictions are not the primary cause of speciation in these cases. <p> This happens by avoiding the creation of lethal individuals by crossing over individuals with very different attributes [117, page 83], although this may be desirable in some search problems [113, section 5] <ref> [117, page 50] </ref>. Mating restrictions are not the primary cause of speciation in these cases. <p> Each offspring thus created replaces the parent that is most similar to it. Crowding does not model the method by which a population arrives at a stable ensemble of species, but it maintains the diversity of the original population <ref> [117, page 78] </ref>. Also, empirical results [45] indicate that crowding is not as effective at finding multiple optima as another GA speciation method, fitness sharing (to be described in Section 2.6.7). Mahfoud refined crowding to create deterministic crowding [112] [113] [117, page 79], which works as follows: 1. <p> Also, empirical results [45] indicate that crowding is not as effective at finding multiple optima as another GA speciation method, fitness sharing (to be described in Section 2.6.7). Mahfoud refined crowding to create deterministic crowding [112] [113] <ref> [117, page 79] </ref>, which works as follows: 1. First, it randomly groups the population in pairs (no mating restrictions); 2. Each pair crosses over to produce two offspring; 3. Each offspring is compared with the parent it most resembles; 4. <p> Each pair crosses over to produce two offspring; 3. Each offspring is compared with the parent it most resembles; 4. If the offspring is better than that parent, it replaces it. Deterministic crowding has performed well on a variety of problems [116] <ref> [117, chapter 10] </ref>. <p> Deterministic crowding has performed well on a variety of problems [116] [117, chapter 10]. Unfortunately, when comparing deterministic crowding with fitness sharing (to be described in Section 2.6.7), that study conspicuously forbids fitness sharing from using a scaling function <ref> [117, pages 206-207] </ref> | this may not be a fair comparison, as fitness sharing worked well with scaling [67], although scaling can cause problems for that method [36]. <p> More important is s , the sharing radius or cutoff distance: two strings at a distance s or further apart do not share fitness. Mahfoud [116] <ref> [117, chapter 10] </ref> finds that the parallel approach of fitness sharing works better than the sequential scheme. However, fitness sharing has its own limitations, including the following: 1. s is the same for all individuals, so all optima in the search space must be equidistant or nearly so [159]. 2. <p> University College, The University of New South Wales Specifically, to maintain the less fit of peaks i and j, we require [114, page 69]: s 1 min ( f i f j (2.31) This important flaw is often overlooked by simply assuming that we have perfect discrimination between different peaks <ref> [117, pages 106, 158] </ref>. These limitations can cause fitness sharing to fail to find all optima if they are not equidistant and equal-valued, or if the estimated distance between optima is incorrect. <p> There are some methods which at this time could be improved further to be used in this way. Sequential runs of a normal GA (like the scheme of Beasley et al. [9]) seems promising, as it should be able to find all possible high-quality strategies. However, Mahfoud [116] <ref> [117, chapter 10] </ref> has shown that certain parallel methods (such as fitness sharing) perform better. This suggests that the sequential scheme needs further refinement. Island models can maintain different sub-populations only at very low migration rates [117, page 41]. <p> However, Mahfoud [116] [117, chapter 10] has shown that certain parallel methods (such as fitness sharing) perform better. This suggests that the sequential scheme needs further refinement. Island models can maintain different sub-populations only at very low migration rates <ref> [117, page 41] </ref>. Higher migration rates cause island models to behave like a normal GA. With very low migration, this method is equivalent to running completely different GA runs, equivalent to iterated schemes like Beasley et al. [9]. <p> This suggests that this method may in future provide a working method for finding multiple solutions. The speciation methods suitable for finding multiple high-quality solutions are: * Fitness sharing [45] [67] and its variations [162]; * Implicit sharing [56] [159]; * Mahfoud's deterministic crowding [116] <ref> [117] </ref>; * More recent efforts, including Ronald's multiple solution technique [143]. Exactly which method we use is not our main focus: we hope to demonstrate that co-evolutionary learning with speciation is effective. Fitness sharing is the most widely-used of these, so we will study it first, in Section 4.2. <p> Introduced foxes thrive on the Australian mainland, but they do not survive on the island of Tasmania. This is partly because the Tasmanian Devil is better-adapted to its habitat, helping it generalise well against the novel competitor 3 . 1 Mahfoud's thesis <ref> [117] </ref> considers genetic drift for several GA speciation methods. 2 A marsupial is a mammal that gives birth to partially-developed offspring, which continue to grow in a pouch. The kangaroo is the most well-known marsupial. 3 Another reason is that Tasmanian Devils are known to catch and eat young foxes. <p> More comparative results are needed. Some studies compare GA speciation methods only on simple test problems [151] [162], even though parallel hill-climbing is usually better on easy test functions <ref> [117, page 202] </ref>. Another study compares GA speciation methods, but conspicuously forbids fitness sharing from using a scaling function [117, pages 206-207] | this may not be a fair comparison, as fitness sharing worked well with scaling [67]. Section 4.4 helps fill the shortage of comparison studies. <p> More comparative results are needed. Some studies compare GA speciation methods only on simple test problems [151] [162], even though parallel hill-climbing is usually better on easy test functions [117, page 202]. Another study compares GA speciation methods, but conspicuously forbids fitness sharing from using a scaling function <ref> [117, pages 206-207] </ref> | this may not be a fair comparison, as fitness sharing worked well with scaling [67]. Section 4.4 helps fill the shortage of comparison studies. We show that implicit sharing finds more peaks when the population is large enough to form a species at each peak. <p> Practical improvements follow theoretical understanding. 4.3.3 Further Work on GA Speciation Other extensions to the GA may be effective at finding multiple peaks. Implicit sharing [159], Mahfoud's deterministic crowding [113] <ref> [117, page 79] </ref>, and Ronald's multiple solution technique [143], are examples of recent approaches to speciation that do away with the fixed niche radius of fitness sharing (Equation 2.30 on page 50). <p> To overcome this problem, a speciated GA can automatically create different species. Speciation avoids genetic drift [114] <ref> [117, Section 8.2] </ref>, and prevents the convergence and overspecialisation that otherwise happens. So a co-evolutionary GA with speciation is especially useful to acquire strategies for a game of conflict: not only can speciation prevent poor generalisation, but diverse species embody a repertoire of high-quality strategies [38]. <p> More comparative results are needed. Some studies evaluate GA speciation only on simple test problems [151] [162], even though parallel hill-climbing is usually better on easy test functions <ref> [117, page 202] </ref>. <p> More comparative results are needed. Some studies evaluate GA speciation only on simple test problems [151] [162], even though parallel hill-climbing is usually better on easy test functions [117, page 202]. Another study compares GA speciation methods, but conspicuously forbids the fitness sharing method from using a scaling function <ref> [117, pages 206-207] </ref> | this may not be a fair comparison, as fitness sharing worked well with scaling [67] (although scaling can cause problems for fitness sharing as we saw in Section 4.2). <p> New GA speciation methods proliferate [122] [138] [143] [151] without a proper comparison of previous GA speciation methods. This section attempts to help fill that gap. A complete mathematical analysis could make use of previous work for the two speciation methods we study, fitness sharing <ref> [117] </ref> and implicit sharing [159]. However, this would require a number of simplifying assumptions in order to be tractable. These assumptions may not be satisfied in practice. Here, we present empirical results, and find that when the population is large enough to form a species at each peak. <p> Fitness sharing [45] has given impressive performance on difficult problems [67], and was for a long time the only practical GA method for multi-optima optimisation <ref> [117, page 84] </ref>. It is natural for such a revolutionary advance to be imperfect [159]. In Section 4.2, we find a new flaw of fitness sharing, whose effects are apparent in some previous studies.
Reference: [118] <author> Keith Mathias and Darrell Whitley. </author> <title> Remapping hyperspace during genetic search: canonical delta folding. </title> <editor> In Gregory J. E. Rawlins, editor, </editor> <booktitle> Foundations of Genetic Algorithms, </booktitle> <pages> pages 167-186. </pages> <publisher> Morgan Kauffman, </publisher> <year> 1991. </year>
Reference-contexts: Several schemes re-initialise the GA population with the aim of refining the current best solution, not to find multiple optima [49] <ref> [118] </ref> [177]. Cobb and Grefenstette [29] did something similar: instead of completely re-initialising the whole population with random individuals, they re-initialised only a part of it. However, their work was in the context of tracking a changing environment, and not finding multiple optima.
Reference: [119] <author> John Maynard Smith and Eors Szathmary. </author> <title> The Major Transitions in Evolution. </title> <publisher> Freeman, Oxford, </publisher> <year> 1995. </year>
Reference-contexts: As these reciprocators multiply, eventually the entire population cooperates with each other. In Figure 2.8, average payoff eventually stays close to the 100% cooperation level 16 of 2 (the payoff matrix used is Figure 5.1 on page 121). This "evolution of cooperation" <ref> [119, section 16.2] </ref> demonstrates how a coevolutionary GA could evolve high-scoring (in this case, cooperative) strategies for a game [7, page 38]. <p> The belief space itself can undergo variation and selection. This implements some observations that culture is varied and transmitted in ways similar to genetic evolution [10] [15] [25] [43, chapter 11] [156] <ref> [119, chapter 16] </ref>. This applies not only for human culture, but for non-human cultural objects such as bird song [43, pages 189-190]. Cultural algorithms are very interesting in that they emulate the evolution of societies. They have also produced impressive results compared to the canonical GA [140, page 99].
Reference: [120] <author> Donald Michie, D. J. Spiegelhalter, and C. C. Taylor. </author> <title> Machine Learning, Neural and Statistical Classification. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: There are many statistical and neural network approaches to "clustering" data, which can give high classification accuracy <ref> [120] </ref>. Again, we are not primarily interested in classification accuracy itself, but in comparing two GA speciation methods. For this reason, we will not follow the usual approach to classification problems of having separate data sets for the learning and testing phases. <p> Incidentally, how well GA methods compare with non-GA methods as covering or clustering algorithms remains an open question. For example, Frey and Slate [59] obtained worse accuracy with a Holland-style adaptive classifier (82.7%) than with a simple nearest neighbour classifier (95.4%) [51] or several other non-GA classification schemes <ref> [120, pages 140-142] </ref>. This suggests the evolutionary approach may need further development for this application. We represent GA individuals as points in the 16-dimensional integer space. We need 4 bits to represent the 16 values of each of the 16 ranges, making a GA genotype of 64 bits.
Reference: [121] <author> Brad L. Miller and Michael J. Shaw. </author> <title> Genetic algorithms with dynamic niche sharing for multimodal function optimization. </title> <type> Technical Report 95010, </type> <institution> Illinois Genetic Algorithms Laboratory, University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: This suggests we cannot yet use the restricted-mating approach to find diverse high-payoff solutions. However, mating restrictions often improve the performance of other GA spe-ciation methods [45, page 49] [36, page 168] <ref> [121, Section 5.3] </ref>. This happens by avoiding the creation of lethal individuals by crossing over individuals with very different attributes [117, page 83], although this may be desirable in some search problems [113, section 5] [117, page 50]. Mating restrictions are not the primary cause of speciation in these cases. <p> He conjectured that random crossover was better. This was for a complex deceptive search space. But in simple 1-dimensional functions, Deb and Goldberg [45, page Page 90 University College, The University of New South Wales 49] and Miller and Shaw <ref> [121] </ref> [122] found that assortative crossover worked bet-ter with fitness sharing: it prevents hybrids of individuals near different peaks, as these hybrids are usually far from any optimum. Thus, it is not clear if random or assortative crossover is better. <p> Assortative crossover is the best in both spaces: closer examination of succeeding generations reveals that random crossover between very different sub-populations disrupts the search. Table 4.3 agrees with some previous studies [45, page 49] <ref> [121] </ref> [122] in that assortative crossover helps fitness sharing find more peaks.
Reference: [122] <author> Brad L. Miller and Michael J. Shaw. </author> <title> Genetic algorithms with dynamic niche sharing for multimodal function optimization. </title> <booktitle> In Proceedings of the 1996 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pages 786-791. </pages> <publisher> IEEE Press, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: This is significant because new GA speciation methods proliferate <ref> [122] </ref> [138] [143] [151] without a detailed understanding of why older GA speciation methods don't work perfectly. Section 4.2 attempts to help fill that gap, and provide a foundation for less ad hoc improvements to GA speciation. <p> New GA speciation methods <ref> [122] </ref> [138] [143] [151] are being presented without a proper comparison of previous This Thesis Extends Knowledge In Section 4.4, we compare two GA speciation methods, fitness sharing and its extension implicit sharing, in how comprehensively they cover the high-quality optima in a difficult search space. <p> He conjectured that random crossover was better. This was for a complex deceptive search space. But in simple 1-dimensional functions, Deb and Goldberg [45, page Page 90 University College, The University of New South Wales 49] and Miller and Shaw [121] <ref> [122] </ref> found that assortative crossover worked bet-ter with fitness sharing: it prevents hybrids of individuals near different peaks, as these hybrids are usually far from any optimum. Thus, it is not clear if random or assortative crossover is better. In this section, we see which is best for this space. <p> Assortative crossover is the best in both spaces: closer examination of succeeding generations reveals that random crossover between very different sub-populations disrupts the search. Table 4.3 agrees with some previous studies [45, page 49] [121] <ref> [122] </ref> in that assortative crossover helps fitness sharing find more peaks. <p> New GA speciation methods proliferate <ref> [122] </ref> [138] [143] [151] without a proper comparison of previous GA speciation methods. This section attempts to help fill that gap. A complete mathematical analysis could make use of previous work for the two speciation methods we study, fitness sharing [117] and implicit sharing [159].
Reference: [123] <author> Marvin L. Minsky. </author> <title> The Society of Mind. </title> <publisher> Simon and Schuster, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: This thesis concentrates on genetic algorithms, but is broadly applicable to similar algorithms. Some critics assert that artificial evolution cannot be intelligent, because natural evolution is a slow process <ref> [123, page 71] </ref>. While some examples of natural evolution are slow, not only is natural evolution sometimes very fast (as for viruses), but the emulation of evolution on a computer can be very fast indeed. The evolutionary process evaluates individuals according to some criteria.
Reference: [124] <author> John E. Moody. </author> <title> The effective number of parameters: An analysis of generalization and regularization in nonlinear learning systems. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lipp-mann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, </address> <year> 1992. </year>
Reference-contexts: It does not tell us how to measure the imperfect generalisation ability of different learning methods, on problems which lack prior knowledge. The prediction risk or expected test set error is a statistics-oriented measure, having its origins in curve-fitting with splines. Moody <ref> [124] </ref> derives an expression for the estimated test set error as a function of the training set error. This extends earlier estimates of risk prediction, and works well on randomly distributed solutions.
Reference: [125] <author> David Moriarty and Risto Miikkulainen. </author> <title> Discovering complex Othello strategies through evolutionary neural networks. </title> <journal> Connection Science, </journal> <volume> 7(3) </volume> <pages> 195-209, </pages> <year> 1995. </year>
Reference-contexts: How this thesis extends current knowledge of the problem. 1.4.1 Explaining Mass Extinctions and Generalisation in Co evolutionary Learning Significance Co-evolutionary genetic algorithms can find solutions that rival human-created solutions | in tasks as diverse as finding a good sorting algorithm [82] or learning to play the game of Othello <ref> [125] </ref> | without prior knowledge from human experts in the field to be learned. Generalisation ability is an important aspect of machine learning, so knowing how well co-evolutionary learning can generalise is significant. <p> method over another? We consider this question in each of the later sections that use sets of test strategies to compare the generalisation ability of the strategies produced by GA variants. 23 This happened in the game of Othello in the early 1970s, with the discovery of the mobility strategy <ref> [125] </ref>. Page 60 University College, The University of New South Wales Australian Defence Force Academy Page 61 Chapter 3 Co-Evolutionary Learning and Generalisation 3.1 Introduction In this chapter, we evaluate the most simple and straightforward approach to coevolutionary learning, and find its ability to learn game strategies has serious limitations. <p> But speciation as automatic modularisation can solve more realistic and complicated problems. Modular neural networks have been effective in many problems [55] [91]. Evolving neural networks with genetic algorithms has become an increasingly mature field [57] <ref> [125] </ref> [183]. This indicates that a modular approach to evolving neural networks should be a fertile field for improved learning. For a different game, an earlier speciated co-evolutionary system [160] sometimes displayed a never-ending turnover of species.
Reference: [126] <author> H. Muhlenbein, M. Schomisch, and J. Born. </author> <title> The parallel genetic algorithm as a function optimiser. </title> <editor> In Richard K. Belew and Lashon B. Booker, editors, </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms. </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1991. </year> <institution> Australian Defence Force Academy Page 177 </institution>
Reference-contexts: Applying Definition 2.1 to Equation 2.13 says that a population vector p is an equilibrium point (i.e., a fixed point) if it satisfies [3, page 65] <ref> [85, pages 15, 126] </ref> [157, page 94]: n X a ij p j = p Ap for all i; 1 i n (2.16) If no information pattern is extinct at the equilibrium point (i.e. p i &gt; 0 for all i) then Equation 2.16 gives a system of linear equations for <p> j = p Ap for all i; 1 i n (2.16) If no information pattern is extinct at the equilibrium point (i.e. p i &gt; 0 for all i) then Equation 2.16 gives a system of linear equations for the n variables p 1 to p n [3, page 66] <ref> [85, pages 16, 126] </ref> [145, page 272]: X a 1j p j = j X a nj p j (2.17) i p i = p 1 + p 2 + : : : + p n = 1 (From Equation 2.2) (2.18) If one or more of the p i are <p> These equilibrium points form a linear manifold in the interior of S n <ref> [85, pages 16, 126, 228] </ref>. In such a situation, a co-evolving GA (even with some way to prevent genetic drift) would not converge to a stable ensemble of species, and never find a "best" solution. <p> This is analogous to having evolution proceed on isolated islands, with some inter-island migration. This is known as an island model or distributed GA. Such studies use distributed GAs <ref> [126] </ref> [163] [178] to improve the canonical GA, without attempting to find multiple optima. <p> Higher migration rates cause island models to behave like a normal GA. With very low migration, this method is equivalent to running completely different GA runs, equivalent to iterated schemes like Beasley et al. [9]. So although island models are useful at finding a single optimum faster <ref> [126] </ref> [163] [178], they have not yet been developed for finding multiple peaks. Australian Defence Force Academy Page 53 Landscape models can allow different sub-populations to form in different regions of the landscape [109, page 309], but not reliably [39, page 262]. <p> So once you've found an equilibrium point by solving the linear system in Equation 2.17, you can determine the linearisation B and find its eigenvalues to determine if that equilibrium point is stable or not. 1 Actually they form a linear manifold <ref> [85, pages 16, 126, 228] </ref>, but suffice it to say that they are not isolated.
Reference: [127] <author> Heinz Muhlenbein. </author> <title> Darwin's continent cycle theory and its simulation by the prisoner's dilemma. </title> <journal> Complex Systems, </journal> <volume> 5 </volume> <pages> 459-478, </pages> <year> 1991. </year>
Reference-contexts: There are several convenient ways of telling if a given equilibrium point p es is evolutionarily stable (ES). 10 The classical game theory approach interprets the points p on the simplex as mixed strategies <ref> [85, pages 121, 127] </ref> [172, page 28]. Page 34 University College, The University of New South Wales Theorem 2.2 ([157, page 102]) The following conditions are equivalent: 1. Point p eq 2 S n is evolutionarily stable; 2. <p> Australian Defence Force Academy Page 47 We now review several studies where neighbouring individuals crossed over only with their geographic neighbours, and whose fitness was determined by their score against those same neighbours while playing IPD | co-evolution on a landscape. One study <ref> [127] </ref> investigates the various ways of having individuals interact. Among other interesting results, they found that the most diversity is caused by a ring population, i.e., the population is distributed in a one-dimensional line, connected at either end, with only adjacent neighbours interacting.
Reference: [128] <author> Douglas Muzzio. </author> <title> Watergate Games: strategies, choices, outcomes. </title> <publisher> New York University Press, </publisher> <year> 1982. </year>
Reference-contexts: The Prisoner's Dilemma is popular because it is a simple game, yet it captures the basics of many real-world situations [6]. These include such diverse situations as the Watergate scandal of 1972-1974 <ref> [128] </ref>, the Cold War of 1945-1990 [18], and life in the trenches during the First World War of 1914-1918 [6, pages 73-87]. T R P S Cooperate Defect Cooperate Defect game.
Reference: [129] <author> Radford M. Neal. </author> <title> Probabilistic inference using Markov chain Monte Carlo methods. </title> <type> Technical Report CRG-TR-93-1, </type> <institution> Department of Computer Science, University of Toronto, </institution> <year> 1993. </year>
Reference-contexts: A gating algorithm is a generic term for an algorithm that adjudicates among different experts. Methods for choosing between different experts include: * Voting [76]; * Committees [110] [167] [168]; * Agent teams [12]; * Stacked generalisation [47] [180]; * Model averaging [23]; * Monte Carlo methods <ref> [129] </ref>; Page 58 University College, The University of New South Wales Later in this thesis, in Section 5.3.1, we will use the simple approach of voting among the repertoire of strategies.
Reference: [130] <author> Allen Newell and Herbert A. Simon. </author> <title> Human Problem Solving. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1972. </year>
Reference-contexts: For example: * Grady [70] considers the modularisation of our brains: she points out that separate parts of the human brain deal with different aspects of vision: colour, depth, movement, detail, and shape. * In another example, it has been argued <ref> [130] </ref> that humans learn by developing rules that are applicable only to a specific situation.
Reference: [131] <author> Martin A. Nowak and Robert M. </author> <month> May. </month> <title> Evolutionary games and spatial chaos. </title> <journal> Nature, </journal> <volume> 359 </volume> <pages> 826-829, </pages> <month> 29 October </month> <year> 1992. </year>
Reference-contexts: One study [127] investigates the various ways of having individuals interact. Among other interesting results, they found that the most diversity is caused by a ring population, i.e., the population is distributed in a one-dimensional line, connected at either end, with only adjacent neighbours interacting. A similar study <ref> [131] </ref> found that if there were only two possible strategies, they could both survive. But another found that more than two would cause convergence to a single optimum and the extinction of the others [2]. A more detailed version of this model allowed strategies of increasing sophistication [109].
Reference: [132] <author> Rorbert G. Palmer, W. Brian Arthur, John H. Holland, Blake LeBaron, and Paul Tayler. </author> <title> Artificial economic life: a simple model of a stockmarket. </title> <journal> Physica D, </journal> <volume> 75 </volume> <pages> 264-274, </pages> <year> 1994. </year>
Reference-contexts: This co-evolutionary approach is a suitable learning method for a range of problems. The simplest example of these is strategy acquisition for games of conflict. More serious examples include stock trading <ref> [132] </ref>, creating a sorting algorithm [82], scheduling a manufacturing process [92], and many other control, management, and military problems where outcomes are partly determined by the actions of others. This thesis demonstrates that, unfortunately, the most straightforward implementation of co-evolutionary learning can give poor generalisation ability. <p> Previous Work Axelrod [7] and others [27] [54] [108] have studied co-evolutionary learning of IPD, and co-evolution has been used to learn other games [46] [144] [160] and other tasks [81] [82] [99]. However, some co-evolutionary learning systems [108] <ref> [132] </ref> have displayed sudden "mass extinctions" | a high-quality strategy dominates the population for a long period of time, and suddenly dies out. An explanation for this behaviour is not yet complete, and is an active area of research [58] [96] [173]. <p> And this is for a co-evolving system that ignores mutation, crossover, and all the other destabilising disruptions of a typical GA. It is worth pointing out that at least one co-evolutionary GA behaved in this manner <ref> [132] </ref>, but the possibility of infinitely many equilibrium solutions was not considered. Fortunately, this problem can be avoided. There are examples of how an interaction matrix A which yields infinitely many equilibria, can be perturbed to one that does not, and vice-versa [157, page 101]. <p> Page 40 University College, The University of New South Wales tial population, converges to cooperative strategies. extinctions. Another study, of co-evolving stock traders on an artificial stock market <ref> [132] </ref>, also showed mass extinctions, where high-volume trading is interrupted by bubbles and crashes. <p> Kristian Lindgren [108, page 310]. Similar collapses have been observed in other co-evolutionary simulations [58] <ref> [132] </ref>. These bear a resemblance to the "punctuated equilibria" of natural evolution [42]. Although that term is not a catch-all phrase for any infrequently-changing system, we will it as a convenient shorthand for collapses like those observed by Lindgren [108]. An explanation for these collapses is not yet complete. <p> Although this test problem is merely a game, the results of this chapter have much wider relevance. Games of conflict are examples of a large body of important problems where each player's outcome depends on the actions of other similar players. Stock trading <ref> [132] </ref> is one obvious application similar to games of conflict. Similarly, although the approach presented here uses rule sets to represent strategies, it could easily use other representations such as artificial neural networks or decision trees. <p> This agrees with the theoretical possibility (described in Section 2.4.7) that instead of equilibrium points being isolated, they can exist in a continuum of infinitely many equilibrium points. This would cause co-evolution to keeping moving through this continuum. Another co-evolutionary system <ref> [132] </ref> displayed a similar turnover. This means that the final population does not represent the repertoire of good strategies when escalation has peaked and come to equilibrium | but merely one possible repertoire in a continuum. <p> As a canonical GA finds only one solution at a time, it over-specialises to one particular strategy, becoming vulnerable to novel opponents. This can cause mass extinctions as in Figure 5.3, when mutation eventually creates a sufficiently novel strategy. Such collapses have been observed before [108] <ref> [132] </ref>, and Chapter 3 sheds new light on why they happen. Unaware of this reason, some studies have nonetheless used both speciation and co-evolution [144] [160]. This prevents over-specialisation and improves generalisa-tion ability. <p> Also, some studies of co-evolutionary learning have observed mass extinctions [58] [108] <ref> [132] </ref>. An explanation for mass extinctions in co-evolution is currently incomplete. Section 3.2 found that in a canonical GA with co-evolution, genetic drift to a single strategy caused over-specialisation to that single strategy.
Reference: [133] <author> Jan Paredis. </author> <title> Coevolutionary constraint satisfaction. </title> <editor> In Yuval Davidor, Hans-Paul Schwe-fel, and Richard Manner, editors, </editor> <booktitle> Proceedings of the Third Conference on Parallel Problem Solving from Nature, volume 866 of Lecture Notes in Computer Science, </booktitle> <pages> pages 46-55. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: In this section, we review previous attempts to emulate co-evolution to create innovative solutions to various problems. There are several other uses of co-evolutionary genetic algorithms besides learning game strategies: for example, they can be used for constrained optimisation <ref> [133] </ref>, or for designing a robot body and its software controller [106]. 2.5.2 Single Population: The Evolution of Cooperation We discuss co-evolution with a single population, where every individual's fitness is determined by its interaction with the other individuals in the same population.
Reference: [134] <author> D. T. Pham and Y. Yang. </author> <title> Optimization of multi-modal discrete functions using genetic algorithms. </title> <journal> Journal of Automobile Design, </journal> 207(1) 53-59, 1993. 
Reference-contexts: Much of the motivation for GA speciation is to find diverse high-quality solutions for engineering problems <ref> [134] </ref> [143] [175]. The most popular and widely-used GA speciation method is fitness sharing [45]. Previous Work Section 2.6 describes several existing GA speciation methods, including fitness sharing. Fitness sharing works effectively on many difficult problems, but has some known flaws [159], to be described in Section 2.6.7. <p> Knowing that these flaws exist helps users to avoid problems, as well as helping designers of future GA speciation methods. Again, much of the motivation for improved GA speciation comes from engineering applications <ref> [134] </ref> [143] [175]. Previous Work Implicit sharing [56] [159] originally modeled the immune system. It has also been used for learning game strategies with co-evolution [38] [144]. One previous study [144] examined sampling in implicit sharing. <p> The motivation for this is that a human designer usually wants a few high-quality and varied solutions, and then utilises those ideas in the final solution. This follows a previous attempt to evolve a design for a gearbox <ref> [134] </ref>. There is a limitation in that this method does not scale up well for problems where a large number of peaks are required.
Reference: [135] <author> Robert M. Pirsig. </author> <title> Zen and the art of motorcycle maintenance: an inquiry into values. </title> <address> Bodley Head, London, </address> <year> 1975. </year>
Reference-contexts: A closely related study found combining several high-quality neural networks gave better results than only one high quality neural network [187]. Despite the success of the modular approach, it may not be the best solution for all possible problems, and has been criticised on philosophical grounds <ref> [135] </ref>. While more sophisticated problem-solving approaches may be possible, the modular approach remains an effective way to solve many real-world problems. Until now, designing a modular system has relied on human expertise (often a committee) to manually divide a system into specialised parts.
Reference: [136] <author> Mitchell A. Potter and Kenneth A. De Jong. </author> <title> Evolving neural networks with collaborative species. </title> <editor> In cer Oren Tun and Louis G. Birta, editors, </editor> <booktitle> Proceedings of the 1995 Summer Computer Simulation Conference. Society for Computer Simulation International, </booktitle> <address> 24-26 July 1995. Ottawa, Canada. </address>
Reference-contexts: However, these depend on human expertise to modularise. In Section 2.7, we describe some studies of modularisation in evolutionary learning. Some do not automatically decompose the solution into relevant parts | human decisions do that [92] <ref> [136] </ref> [137]. Another study uses GA speciation, but does not utilise the diverse expertise thus formed [144]. Another approach [4] [100] [104] randomly selects and stores parts of individuals off-line. This ties in loosely with what are called "cultural algorithms" [141] [140]. <p> They call each separate population a "species", but do not use a speciation method like those in Section 2.6. 2.7.3 Less Strict Manual Modularisation The work of Potter et al. <ref> [136] </ref> [137] has less direct control over what GA population does what. Potter and De Jong [136] evolved a multi-layer neural network. Each layer was optimised by a different GA population. <p> They call each separate population a "species", but do not use a speciation method like those in Section 2.6. 2.7.3 Less Strict Manual Modularisation The work of Potter et al. <ref> [136] </ref> [137] has less direct control over what GA population does what. Potter and De Jong [136] evolved a multi-layer neural network. Each layer was optimised by a different GA population. <p> a GA speciation method from Section 2.6 dynamically cause separate sub-populations to form, so that human designers don't have to allocate modules in advance? This would relieve humans of the chore of modularising the solution, and improve upon the work of Husbands et al. [92] [93] and Potter et al. <ref> [136] </ref> [137]. Returning to the baseball example: if you knew nothing about baseball except the rules, then manually decomposing a solution may not give a good decomposition. Instead of pitching, batting, fielding, a committee of inexperienced humans might manually decompose into throwers, receivers, and guards. <p> Knowledge of these flaws allows users to avoid the pitfalls, as well as adding to the body of knowledge that allows the future design of improved GA speciation methods. 6.3 Modularisation Previous studies have manually created a modular system, deciding in advance which co-evolving GA sub-population would do what [92] <ref> [136] </ref> [137]. Also, some previous studies have used speciation with co-evolution, to create specialised solutions [144] [160], without considering the problem of how to re-combine that specialised expertise.
Reference: [137] <author> Mitchell A. Potter, Kenneth A. De Jong, and John J. Grefenstette. </author> <title> A coevolutionary approach to learning sequential decision rules. </title> <editor> In Larry J. Eshelman, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 366-372. </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: However, these depend on human expertise to modularise. In Section 2.7, we describe some studies of modularisation in evolutionary learning. Some do not automatically decompose the solution into relevant parts | human decisions do that [92] [136] <ref> [137] </ref>. Another study uses GA speciation, but does not utilise the diverse expertise thus formed [144]. Another approach [4] [100] [104] randomly selects and stores parts of individuals off-line. This ties in loosely with what are called "cultural algorithms" [141] [140]. <p> They call each separate population a "species", but do not use a speciation method like those in Section 2.6. 2.7.3 Less Strict Manual Modularisation The work of Potter et al. [136] <ref> [137] </ref> has less direct control over what GA population does what. Potter and De Jong [136] evolved a multi-layer neural network. Each layer was optimised by a different GA population. <p> This is similar to Husbands et al. [92] in that a human decides (in advance) what each GA population will do. Potter et al. <ref> [137] </ref> did a similar study, to evolve rule sets for a sequential decision problem. Separate GA populations develop their own rule sets. Payoff is allocated by taking the best rule set from each GA population, and seeing how well the combined rule set performs. That is, teamwork is rewarded. <p> That is, teamwork is rewarded. They hope that: Australian Defence Force Academy Page 55 Each ... module evolves its own subpopulation of rule sets that, in prac- tice, is dominated by ... expertise in a particular area. Potter et al. <ref> [137, page 368] </ref> It is unclear what motivates the separate GA populations to specialise to different areas. <p> Potter et al. [137, page 368] It is unclear what motivates the separate GA populations to specialise to different areas. They are aware of this: It could happen, for example, that sub-populations end up containing complete, monolithic solutions rather than subcomponents Potter et al. <ref> [137, page 370] </ref> In order to encourage specialisation to separate areas, they do not use a GA speciation method. Instead, they seed the different GA populations with rule sets that reflected their human prejudices about how to modularise the problem. The coevolutionary system consisted of two ... instances [GA populations/modules]. <p> The coevolutionary system consisted of two ... instances [GA populations/modules]. The instances were seeded with rules intended to give them initial biases corresponding to a fairly natural decomposition of the task. Potter et al. <ref> [137, page 370] </ref> This "natural decomposition" (into only two modules) turns out to be a very human judgement about which GA population will do what, and how many there are to be. <p> This is still manual modularisation, although it has the least human input of the previous studies | seeding the initial population, so that some change might occur. However, in this study, the separate modules maintain the speciality that the human choice gave them <ref> [137, page 371] </ref>, displaying no creativity in that regard. That study did not consider the possibility that the GA populations might specialise in ways other than decided in advance by a human designer. <p> GA speciation method from Section 2.6 dynamically cause separate sub-populations to form, so that human designers don't have to allocate modules in advance? This would relieve humans of the chore of modularising the solution, and improve upon the work of Husbands et al. [92] [93] and Potter et al. [136] <ref> [137] </ref>. Returning to the baseball example: if you knew nothing about baseball except the rules, then manually decomposing a solution may not give a good decomposition. Instead of pitching, batting, fielding, a committee of inexperienced humans might manually decompose into throwers, receivers, and guards. <p> of these flaws allows users to avoid the pitfalls, as well as adding to the body of knowledge that allows the future design of improved GA speciation methods. 6.3 Modularisation Previous studies have manually created a modular system, deciding in advance which co-evolving GA sub-population would do what [92] [136] <ref> [137] </ref>. Also, some previous studies have used speciation with co-evolution, to create specialised solutions [144] [160], without considering the problem of how to re-combine that specialised expertise.
Reference: [138] <author> Alain Petrowski. </author> <title> A clearing procedure as a niching method for genetic algorithms. </title> <booktitle> In Proceedings of the 1996 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pages 798-803. </pages> <publisher> IEEE Press, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: This is significant because new GA speciation methods proliferate [122] <ref> [138] </ref> [143] [151] without a detailed understanding of why older GA speciation methods don't work perfectly. Section 4.2 attempts to help fill that gap, and provide a foundation for less ad hoc improvements to GA speciation. <p> New GA speciation methods [122] <ref> [138] </ref> [143] [151] are being presented without a proper comparison of previous This Thesis Extends Knowledge In Section 4.4, we compare two GA speciation methods, fitness sharing and its extension implicit sharing, in how comprehensively they cover the high-quality optima in a difficult search space. <p> This which may not scale up well when searching a combinatorially large space. * One that repeatedly divides the population into sub-populations (the number of which varies dynamically) and re-combines them [84]. * An attempt to improve fitness sharing has led to "clearing" <ref> [138] </ref>. Instead of evenly sharing resources among individuals in the same part of the search space, this method only rewards the best individuals in that part of the space and erases the others. <p> Though interesting, this approach nonetheless still has the problems of a fixed sharing radius (called the clearing radius <ref> [138, page 798] </ref>). Some parts of this thesis contribute to the understanding of existing GA speci-ation methods, helping future work in this active area of research. 2.6.10 Discussion of GA Speciation We have reviewed various GA speciation method. <p> New GA speciation methods proliferate [122] <ref> [138] </ref> [143] [151] without a proper comparison of previous GA speciation methods. This section attempts to help fill that gap. A complete mathematical analysis could make use of previous work for the two speciation methods we study, fitness sharing [117] and implicit sharing [159].
Reference: [139] <author> Anatol Rapoport. </author> <title> Optimal policies for the prisoner's dilemma. </title> <type> Technical Report TR 50, </type> <institution> The Psychometric Laboratory, University of North Carolina, Chapel Hill, </institution> <month> July </month> <year> 1966. </year>
Reference-contexts: A non-cooperative game is one where no preplay communication is permitted between the players [31] <ref> [139] </ref>. The Prisoner's Dilemma game is a 2 fi 2 non-zerosum non-cooperative game. The 2-player Prisoner's Dilemma has been widely studied in such diverse fields as economics, mathematical game theory, political science, and artificial intelligence.
Reference: [140] <author> Robert G. Reynolds and ChanJin Chung. </author> <title> A self-adaptive approach to representation shifts in cultural algorithms. </title> <booktitle> In Proceedings of the 1996 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pages 94-99. </pages> <publisher> IEEE Press, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: Another study uses GA speciation, but does not utilise the diverse expertise thus formed [144]. Another approach [4] [100] [104] randomly selects and stores parts of individuals off-line. This ties in loosely with what are called "cultural algorithms" [141] <ref> [140] </ref>. This approach evaluates the randomly-selected parts (directly or indirectly) that are stored off-line, in the hope that this will produce useful specialisation. This is an interesting idea, but the driving force for modularisation is random selection. Our approach emulates natural evolution by allowing speciation to modularise. <p> In these two studies, the "modularisation" is not driven by a process like GA speciation: it is done by cutting off pieces of individuals in a more ad hoc manner, and storing them off-line. This very similar to what are loosely called cultural algorithms [10] [60] [61] [94] <ref> [140] </ref> [141]: attributes of individuals are copied into an off-line cache, called a library or "belief space", which influences newer individuals. The belief space itself can undergo variation and selection. <p> This applies not only for human culture, but for non-human cultural objects such as bird song [43, pages 189-190]. Cultural algorithms are very interesting in that they emulate the evolution of societies. They have also produced impressive results compared to the canonical GA <ref> [140, page 99] </ref>. However, randomly-selected pieces stored in the library may not be the best driving force for creating a modular solution. This modularisation aspect is a continuing area of research. <p> Page 106 University College, The University of New South Wales Culture could play a critical role here in allowing adaptive specialisation without the genetic speciation that irreversibly partitions the population. Richard K. Belew [10] (His italics) A few studies have been done on cultural evolution [10] [60] [61] [94] <ref> [140] </ref> [141] and this innovative approach has performed well on certain problems. Nonetheless, more comparison studies are needed. New GA speciation methods may overcome the flaws of fitness sharing, including the one discovered here.
Reference: [141] <author> Robert G. Reynolds and Stefan R. Rolnick. </author> <title> Cultural algorithms. </title> <booktitle> In Proceedings of the 1995 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pages 819-824. </pages> <publisher> IEEE Press, </publisher> <month> 29 November - 1 December </month> <year> 1995. </year>
Reference-contexts: Another study uses GA speciation, but does not utilise the diverse expertise thus formed [144]. Another approach [4] [100] [104] randomly selects and stores parts of individuals off-line. This ties in loosely with what are called "cultural algorithms" <ref> [141] </ref> [140]. This approach evaluates the randomly-selected parts (directly or indirectly) that are stored off-line, in the hope that this will produce useful specialisation. This is an interesting idea, but the driving force for modularisation is random selection. Our approach emulates natural evolution by allowing speciation to modularise. <p> In these two studies, the "modularisation" is not driven by a process like GA speciation: it is done by cutting off pieces of individuals in a more ad hoc manner, and storing them off-line. This very similar to what are loosely called cultural algorithms [10] [60] [61] [94] [140] <ref> [141] </ref>: attributes of individuals are copied into an off-line cache, called a library or "belief space", which influences newer individuals. The belief space itself can undergo variation and selection. <p> Richard K. Belew [10] (His italics) A few studies have been done on cultural evolution [10] [60] [61] [94] [140] <ref> [141] </ref> and this innovative approach has performed well on certain problems. Nonetheless, more comparison studies are needed. New GA speciation methods may overcome the flaws of fitness sharing, including the one discovered here.
Reference: [142] <author> Mark Ridley. </author> <title> Evolution. </title> <publisher> Blackwell Scientific, </publisher> <address> Boston, </address> <year> 1993. </year>
Reference-contexts: In reality, this nerve runs from the brain all the way down the giraffe's long neck, loops around a major blood vessel, then runs back up the neck to the larynx [41, page 39] <ref> [142, pages 343-344] </ref>. One of the myths of evolution is that fitness is a tautology: supposedly, the "fit" are precisely those who have many offspring, and those who have many offspring are by definition "fit" [65, page 76].
Reference: [143] <author> Simon Ronald. </author> <title> Finding multiple solutions with an evolutionary algorithm. </title> <booktitle> In Proceedings of the 1995 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pages 641-646. </pages> <publisher> IEEE Press, </publisher> <month> 29 November - 1 December </month> <year> 1995. </year>
Reference-contexts: Much of the motivation for GA speciation is to find diverse high-quality solutions for engineering problems [134] <ref> [143] </ref> [175]. The most popular and widely-used GA speciation method is fitness sharing [45]. Previous Work Section 2.6 describes several existing GA speciation methods, including fitness sharing. Fitness sharing works effectively on many difficult problems, but has some known flaws [159], to be described in Section 2.6.7. <p> This is significant because new GA speciation methods proliferate [122] [138] <ref> [143] </ref> [151] without a detailed understanding of why older GA speciation methods don't work perfectly. Section 4.2 attempts to help fill that gap, and provide a foundation for less ad hoc improvements to GA speciation. <p> Knowing that these flaws exist helps users to avoid problems, as well as helping designers of future GA speciation methods. Again, much of the motivation for improved GA speciation comes from engineering applications [134] <ref> [143] </ref> [175]. Previous Work Implicit sharing [56] [159] originally modeled the immune system. It has also been used for learning game strategies with co-evolution [38] [144]. One previous study [144] examined sampling in implicit sharing. <p> New GA speciation methods [122] [138] <ref> [143] </ref> [151] are being presented without a proper comparison of previous This Thesis Extends Knowledge In Section 4.4, we compare two GA speciation methods, fitness sharing and its extension implicit sharing, in how comprehensively they cover the high-quality optima in a difficult search space. <p> This knowledge that can only be obtained by searching the space. This disadvantage is shared by fitness sharing, described in Section 2.6.7 below. 2. This scheme modifies the search space by reducing fitness around a known optimum. This can cause "ghost peaks", resembling the cone of a volcano <ref> [143, page 642] </ref>. This is illustrated in Figure 2.11. Later GA runs may find these artifacts of the method, even though they are not real optima. Our own work has found similar effects for the speciation method of fitness sharing [36]. <p> Many new algorithms are presented each year. This thesis cannot hope to describe every one, but in this section we briefly sketch a few. A novel approach to finding multiple peaks, Roland's Multiple Solution Technique (MST) <ref> [143] </ref>, is a simple add-on to a canonical GA. Unlike the methods previously described, MST does not seek to partition the population into distinct sub-populations, each targeting a different peak. <p> This follows a previous attempt to evolve a design for a gearbox [134]. There is a limitation in that this method does not scale up well for problems where a large number of peaks are required. Ronald <ref> [143] </ref> intended this for engineering applications, and a human designer only wants a few good but varied solutions: one can use this method to automatically invent some high-quality solutions that are all different from each other, and then choose from among these ideas in making the final design. <p> The MST approach <ref> [143] </ref> has much to offer. The only objection is that, unlike an engineer who wants a limited number of high-quality solutions, we would like all the high-quality strategies for a game of conflict, without knowing in advance how many the game offers. <p> The speciation methods suitable for finding multiple high-quality solutions are: * Fitness sharing [45] [67] and its variations [162]; * Implicit sharing [56] [159]; * Mahfoud's deterministic crowding [116] [117]; * More recent efforts, including Ronald's multiple solution technique <ref> [143] </ref>. Exactly which method we use is not our main focus: we hope to demonstrate that co-evolutionary learning with speciation is effective. Fitness sharing is the most widely-used of these, so we will study it first, in Section 4.2. <p> Practical improvements follow theoretical understanding. 4.3.3 Further Work on GA Speciation Other extensions to the GA may be effective at finding multiple peaks. Implicit sharing [159], Mahfoud's deterministic crowding [113] [117, page 79], and Ronald's multiple solution technique <ref> [143] </ref>, are examples of recent approaches to speciation that do away with the fixed niche radius of fitness sharing (Equation 2.30 on page 50). Methods like these do not require prior knowledge of how far apart peaks are, or that peaks be nearly equidistant. <p> New GA speciation methods proliferate [122] [138] <ref> [143] </ref> [151] without a proper comparison of previous GA speciation methods. This section attempts to help fill that gap. A complete mathematical analysis could make use of previous work for the two speciation methods we study, fitness sharing [117] and implicit sharing [159].
Reference: [144] <author> Christopher D. Rosin and Richard K. Belew. </author> <title> Methods for competitive co-evolution: Finding opponents worth beating. </title> <editor> In Larry J. Eshelman, editor, </editor> <booktitle> Proceedings of the Sixth International Conference on Genetic Algorithms, </booktitle> <pages> pages 373-380. </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1995. </year> <institution> Page 178 University College, The University of New South Wales </institution>
Reference-contexts: Previous Work Axelrod [7] and others [27] [54] [108] have studied co-evolutionary learning of IPD, and co-evolution has been used to learn other games [46] <ref> [144] </ref> [160] and other tasks [81] [82] [99]. However, some co-evolutionary learning systems [108] [132] have displayed sudden "mass extinctions" | a high-quality strategy dominates the population for a long period of time, and suddenly dies out. <p> Again, much of the motivation for improved GA speciation comes from engineering applications [134] [143] [175]. Previous Work Implicit sharing [56] [159] originally modeled the immune system. It has also been used for learning game strategies with co-evolution [38] <ref> [144] </ref>. One previous study [144] examined sampling in implicit sharing. This Thesis Extends Knowledge In Sections 5.2.1 through 5.2.4 we discuss certain pitfalls that may trap an unsuspecting user, which occur when using implicit sharing in a co-evolutionary GA. <p> Again, much of the motivation for improved GA speciation comes from engineering applications [134] [143] [175]. Previous Work Implicit sharing [56] [159] originally modeled the immune system. It has also been used for learning game strategies with co-evolution [38] <ref> [144] </ref>. One previous study [144] examined sampling in implicit sharing. This Thesis Extends Knowledge In Sections 5.2.1 through 5.2.4 we discuss certain pitfalls that may trap an unsuspecting user, which occur when using implicit sharing in a co-evolutionary GA. Although the issue of sample selection in Section 5.2.3 has been previously addressed [144], we believe <p> previous study <ref> [144] </ref> examined sampling in implicit sharing. This Thesis Extends Knowledge In Sections 5.2.1 through 5.2.4 we discuss certain pitfalls that may trap an unsuspecting user, which occur when using implicit sharing in a co-evolutionary GA. Although the issue of sample selection in Section 5.2.3 has been previously addressed [144], we believe the others may be novel. 1.4.4 Comparing Two GA Speciation Methods Significance There exist several GA speciation methods, which we describe in Section 2.6. <p> In Section 2.7, we describe some studies of modularisation in evolutionary learning. Some do not automatically decompose the solution into relevant parts | human decisions do that [92] [136] [137]. Another study uses GA speciation, but does not utilise the diverse expertise thus formed <ref> [144] </ref>. Another approach [4] [100] [104] randomly selects and stores parts of individuals off-line. This ties in loosely with what are called "cultural algorithms" [141] [140]. This approach evaluates the randomly-selected parts (directly or indirectly) that are stored off-line, in the hope that this will produce useful specialisation. <p> Some studies use separate GA populations, where each member of one population is evaluated by its performance against members of the other population [82] <ref> [144] </ref>. We consider this in Appendix C on page 157, but it gives similar results to the one-population case we consider here. Imagine that every member of the population plays every other member of the same population (one at a time) at a two-player game. <p> Originally, in an immune system simulation, antibodies which best matched an invading antigen received the payoff for that antigen [56] [159]. Another use is in learning a game: a strategy receives payoff when it achieves the best score against a sample test strategy [38] <ref> [144] </ref>. So in Table 2.1, "strategy" could be replaced by "antigen" or "sorting problem" or whatever. At first glance, Table 2.1 looks completely different to the original fitness sharing in Section 2.6.7. Surprisingly, they share the same theoretical basis [159]. <p> Can GA speciation decompose a solution for us, and relieve humans of that burden? Some previous studies <ref> [144] </ref> [160] have used a co-evolutionary GA with specia-tion to learn game strategies. However, these did not consider the question of how to make best use of the diverse expertise embodied in the various species. They merely considered individuals in the final GA generation one at a time. <p> This thesis extends the work of Smith and Gray [160] [161] by using a gating algorithm to utilise the diverse expertise. Rosin and Belew <ref> [144] </ref> also used a speciated, co-evolutionary GA to learn games. However, they hoped that an individual in the GA would be well-rounded enough to play against all possible opponents. It is worth noting that in Rosin and Belew [144], the test games were simple enough for a perfect strategy to exist, <p> Rosin and Belew <ref> [144] </ref> also used a speciated, co-evolutionary GA to learn games. However, they hoped that an individual in the GA would be well-rounded enough to play against all possible opponents. It is worth noting that in Rosin and Belew [144], the test games were simple enough for a perfect strategy to exist, except for the more difficult game of Go. 2.7.5 Modularisation in Evolving Programs A few previous studies have considered automatic modularisation by evolution. <p> Another use is in learning a game, where the discrete objects are test strategies: in Table 2.1, a strategy would receive payoff when it achieves the best-in-sample score against a test strategy [38] <ref> [144] </ref>. This makes it suitable for co-evolutionary learning of game strategies, if each new test strategy comes from the GA population itself. At first glance, Table 2.1 looks completely different to fitness sharing. Surprisingly, they share the same theoretical basis [159]. <p> Our aim is to obtain a full repertoire of strategies, to generalise well against any expert opponent [38]. With a large enough population, implicit sharing would be best. In fact, some previous attempts to learn game strategies with a co-evolutionary GA used implicit sharing [38] <ref> [144] </ref>. However, from Figure 4.18, if the game has more strategies than we expected or our population is too small, then implicit sharing could get side-tracked and miss strategies it should have found, and we will generalise poorly. <p> This thesis extends the work of Smith and Gray [160] [161] by using a gating algorithm to utilise this diverse expertise. * Rosin and Belew <ref> [144] </ref> also used a speciated, co-evolutionary GA to learn games. However, they hoped that an individual in the GA would be well-rounded enough to play against all possible opponents. It is worth noting that in Rosin and Belew [144], the test games were simple enough for a perfect strategy to exist, <p> a gating algorithm to utilise this diverse expertise. * Rosin and Belew <ref> [144] </ref> also used a speciated, co-evolutionary GA to learn games. However, they hoped that an individual in the GA would be well-rounded enough to play against all possible opponents. It is worth noting that in Rosin and Belew [144], the test games were simple enough for a perfect strategy to exist, except for the more difficult game of Go. The diverse strategies thus found are most effective against certain opponents, Australian Defence Force Academy Page 119 and not necessarily against all possible opponents. <p> Because of these flaws, as well as the results in Section 4.4, this chapter uses an extension to the original fitness sharing, known as implicit sharing [56] [159], which was described in Section 2.6.8. Implicit sharing modifies the fitness function according to Table 5.1. Incidentally, Rosin and Belew <ref> [144] </ref> also used implicit sharing in a co-evolutionary, speciated GA to learn games. Although we use implicit sharing, we could use any suitable GA speciation method | implicit sharing merely seems appropriate after Section 2.6 and Chapter 4. <p> However, we will discuss some issues of using this particular method of speciation. In Section 5.2, we study some complications that arise when implicit sharing is used with a co-evolutionary fitness function. Although some have been considered by Rosin and Belew <ref> [144] </ref>, we describe some novel problems. Section 5.3 is the climax of this thesis. We use speciation to form separate specialists, without human interference. <p> To avoid this, the sample size must be large enough to sample a member of every different species of high-quality strategies. A large sample reduces this noise, but uses more computation time. Instead of random sampling, Rosin and Belew <ref> [144] </ref> use "shared sampling" | each new member of the sample is one that scores well against individuals whom the other sample members did poorly against [144, page 375]. That is, the sample contains diverse talent. Knowing who scores well against whom is found from the previous generation. <p> A large sample reduces this noise, but uses more computation time. Instead of random sampling, Rosin and Belew [144] use "shared sampling" | each new member of the sample is one that scores well against individuals whom the other sample members did poorly against <ref> [144, page 375] </ref>. That is, the sample contains diverse talent. Knowing who scores well against whom is found from the previous generation. <p> The A strategies will then recover, due to their higher fitness. Both types will persist in this stable equilibrium. This is desirable. This is a more detailed version of the justification that Rosin and Belew <ref> [144, page 374] </ref> gave for using fixed payoff: The effect of this method is to reward hosts that defeat parasites few others can, even though the rewarded host might not defeat as many parasites as others can Christopher D. Rosin and Richard K. Belew [144, page 374] Note that the justification <p> the justification that Rosin and Belew <ref> [144, page 374] </ref> gave for using fixed payoff: The effect of this method is to reward hosts that defeat parasites few others can, even though the rewarded host might not defeat as many parasites as others can Christopher D. Rosin and Richard K. Belew [144, page 374] Note that the justification for fixed payoff given in Section 5.2.2 is different: Rosin and Belew [144, page 374] anticipated Equations 5.1 and 5.2, but not Equations 5.10 and 5.11. We now consider what we label "parasite" species. Consider three species, A, P , and Z. <p> Rosin and Richard K. Belew <ref> [144, page 374] </ref> Note that the justification for fixed payoff given in Section 5.2.2 is different: Rosin and Belew [144, page 374] anticipated Equations 5.1 and 5.2, but not Equations 5.10 and 5.11. We now consider what we label "parasite" species. Consider three species, A, P , and Z. <p> This satisfies Equation 5.17. With fixed payoff, each individual test strategy in Table 5.1 is a resource won by those strategies that score highest against it <ref> [144, page 374] </ref>. The resources gained by being best-in-sample against A are shared between both A and P (both of whom cooperate with P ). But the resources of both P and Z all go exclusively to Z (who exploits P ). <p> Australian Defence Force Academy Page 133 5.3.2 Genetic Algorithm Details One or Two Populations Some attempts at co-evolution use two GA populations: an individual's fitness is determined by how well it performs against the other population, with no intra-population competition <ref> [144] </ref>. No intra-population competition means that similar offspring do not compete with each other, which may discourage improvements. On the other hand, it has been argued [42, page 187] that using two populations encourages speedier progress. We would like intra-population competition, and we would also like to keep things simple. <p> Australian Defence Force Academy Page 135 * A co-evolutionary GA using implicit fitness sharing with reverse assortative sample selection, as described in Section 5.2.3. We use the best individual from the final generation (best.sp), like Rosin and Belew <ref> [144] </ref> who also tried best-in-population with a special sample selection scheme. We also use the gating algorithm on the final generation (gate.sp). With the larger population used for l = 4, this sample selection scheme was so slow we did not use it. <p> This is a flaw of our test set selection. In simple games which offer a single perfect strategy, our modular approach is not necessary. Speciation can help a co-evolutionary GA find this perfect individual, by preventing over-specialisation 5 . 5 As noted previously, a study by Rosin and Belew <ref> [144] </ref> used test games that were simple enough for a perfect strategy to exist, and a co-evolutionary speciated GA worked well even without a gating algorithm to manage the different species. <p> This indicates that our modular approach relies heavily on the speciated GA to maintain diversity: it works better as the speciation method maintains greater diversity. This improvement agrees with the work of Rosin and Belew <ref> [144] </ref> who also improved sample diversity. Remember that in Chapter 3, we tried to make the learning environment more Australian Defence Force Academy Page 137 diverse by seeding or adding extra strategies to the co-evolutionary round robin. <p> This can cause mass extinctions as in Figure 5.3, when mutation eventually creates a sufficiently novel strategy. Such collapses have been observed before [108] [132], and Chapter 3 sheds new light on why they happen. Unaware of this reason, some studies have nonetheless used both speciation and co-evolution <ref> [144] </ref> [160]. This prevents over-specialisation and improves generalisa-tion ability. However, although these studies learned diverse and specialised strategies, they did not address the question of when to use which species. This wastes much of the specialised expertise found by speciation. <p> This wastes much of the specialised expertise found by speciation. Also, certain complexities arise when using implicit sharing with co-evolution, and we have described some of these in Section 5.2. Some of these are new. Sections 5.2.1 and 5.2.2 show the desirability of fixed payoff (as used before <ref> [144] </ref>), but show that when a species is its own best opponent, fixed payoff allows desirable species to become extinct through genetic drift. This genetic drift occurs in spite of the speciation method. Also, Section 5.2.4 shows another way that desirable species may be driven extinct. <p> Also, some previous studies have used speciation with co-evolution, to create specialised solutions <ref> [144] </ref> [160], without considering the problem of how to re-combine that specialised expertise. Chapter 5 combines these two currents of research, presenting a system that creates expertise with no human help, using only a "black-box" simulation of the problem. <p> Chapter 5 combines these two currents of research, presenting a system that creates expertise with no human help, using only a "black-box" simulation of the problem. As discussed in Section 5.3.4, the modular approach performs better against test strategies than the best individual, as used previously <ref> [144] </ref>. This justifies our approach | a speciated, co-evolutionary GA can automatically modularise. 6.4 Conclusion of the Thesis Co-evolutionary learning is one of the most promising areas of artificial intelligence. <p> Australian Defence Force Academy Page 155 Appendix C Evolutionary Game Theory: Two Populations Some co-evolutionary GA systems run two different populations: the fitness of each individual is determined by how well it performs against all the individuals in the other population [82] <ref> [144] </ref>. This is an example of simple manual modularisation: there are precisely two sides to the problem, and each population deals with their own aspect.
Reference: [145] <author> G. W. Rowe, I. F. Harvey, and S. F. Hubbard. </author> <title> The essential properties of evolutionary stability. </title> <journal> Journal of Theoretical Biology, </journal> <volume> 115 </volume> <pages> 269-285, </pages> <year> 1985. </year>
Reference-contexts: This section reviews the mathematical analysis of co-evolution. Unfortunately, current knowledge lacks many answers which would be of interest to co-evolutionary learning. Even the central concept of evolutionary stability is Australian Defence Force Academy Page 27 controversial [32] <ref> [145] </ref>. As a result, the main part of this thesis does not attempt an all-inclusive mathematical framework. <p> Equation 2.13 can be written in an alternative form that includes C, a positive constant 7 corresponding to the fitness without interaction [24, page 104] [85, page 133] <ref> [145, page 271] </ref> [157, page 92]: p i (t + 1) = p i (t) j a ij p j (t) + C (2.14) Equation 2.13 and its continuous counterpart, Equation A.5, are closed in the simplex S n : if a point starts on the simplex, p (0) 2 S <p> We consider various attempts to define stability in the next sections. 7 When modelling a real biological system, the constant C must be chosen for biological reasons, and not merely so that fitnesses never become negative <ref> [145, page 281] </ref>. Page 30 University College, The University of New South Wales Incidentally, the constant C in Equation 2.14 does not affect the position of the equilibrium points [145, page 272], although it can affect their stability [24, page 105] [145, page 271]. <p> Page 30 University College, The University of New South Wales Incidentally, the constant C in Equation 2.14 does not affect the position of the equilibrium points <ref> [145, page 272] </ref>, although it can affect their stability [24, page 105] [145, page 271]. <p> Page 30 University College, The University of New South Wales Incidentally, the constant C in Equation 2.14 does not affect the position of the equilibrium points [145, page 272], although it can affect their stability [24, page 105] <ref> [145, page 271] </ref>. <p> for all i; 1 i n (2.16) If no information pattern is extinct at the equilibrium point (i.e. p i &gt; 0 for all i) then Equation 2.16 gives a system of linear equations for the n variables p 1 to p n [3, page 66] [85, pages 16, 126] <ref> [145, page 272] </ref>: X a 1j p j = j X a nj p j (2.17) i p i = p 1 + p 2 + : : : + p n = 1 (From Equation 2.2) (2.18) If one or more of the p i are zero (i.e., one or <p> To gain a full list of equilibria, we must solve the system of equations in Equations 2.17 and 2.18 for all possible lower dimensional cases <ref> [145, page 273] </ref>. For example, if evolution has only four choices, n = 4, so that the interaction matrix A is a 4 fi 4 matrix, then we must solve Equations 2.17 and 2.18 for each of [145, page 273]: 8 Except for those p i which are zero, for which <p> of equations in Equations 2.17 and 2.18 for all possible lower dimensional cases <ref> [145, page 273] </ref>. For example, if evolution has only four choices, n = 4, so that the interaction matrix A is a 4 fi 4 matrix, then we must solve Equations 2.17 and 2.18 for each of [145, page 273]: 8 Except for those p i which are zero, for which the value of P j a ij p j need not equal the average fitness [172, page 27]. <p> That our model ignores mutation and crossover does not stop us examining stability. Some different uses of the word "stability" are discussed in Section D.1. How can we find the stability of equilibria? Linearisation [3, page 32] <ref> [145, page 273] </ref> is one way. Consider the discrete system of Equation 2.14 on page 30. Linearisation involves adding a perturbation vector q to the equilibrium point in question p eq . <p> Also, there are examples where changing the value of the constant C in Equation 2.14 11 changes an equilibrium point from an attractor to a an unstable equilibrium, while satisfying the definition of "evolutionarily stable" for both values of C [24, page 105] <ref> [145, page 271] </ref>. Some researchers suspect that evolutionary stability cannot be modified to accurately characterise stability for the discrete system, and that one should only use linearisation to test each and every equilibrium [32, page 19] [145]. Linearisation is cumbersome, but accurate. <p> Some researchers suspect that evolutionary stability cannot be modified to accurately characterise stability for the discrete system, and that one should only use linearisation to test each and every equilibrium [32, page 19] <ref> [145] </ref>. Linearisation is cumbersome, but accurate. Although evolutionary stability is an intuitive concept, with interesting results 11 Or, equivalently, changing all the values of the a ij by the same amount. Australian Defence Force Academy Page 37 such as Theorem 2.3, it does not accurately characterise stability in co-evolution. <p> Zeeman [188] considers a different type of stability, of perturbations of the interaction matrix A instead of the current population p. This is analogous to checking the robustness of a population under changes to its environment, rather than internal interaction <ref> [145, page 279] </ref>. We will only consider stability of the population, with constant A. <p> It gets closer, then further away, but on the whole it gets closer. D.2 Finding Stability by Linearisation Recall the discrete replicator equation from Equation 2.14 on page 30 [24, page 104] [85, page 133] <ref> [145, page 271] </ref> [157, page 92]: p i (t + 1) = p i (t) j a ij p j (t) + C Linearisation adds a perturbation vector q to the equilibrium point in question p eq . <p> This is how we tell if the equilibrium point p eq is stable. Doing this for the discrete system of Equation 2.14 tells us that the perturbation variable q changes (approximated to first order) [3, page 32] <ref> [145, page 283] </ref>: q (t + 1) = Bq (t) (D.1) The n fi n matrix B is the first-order approximation of how the perturbation changes. If the first-order approximation is zero, then we must go to second-order, but this is rarely necessary. <p> If the first-order approximation is zero, then we must go to second-order, but this is rarely necessary. Matrix B is given by <ref> [145, page 273] </ref>: Page 160 University College, The University of New South Wales b ij = &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; ffi ij C + j a ij p j C + p eq Ap eq ; for i for which p eq ffi ij + p i <p> p eq (D.2) In Equation D.2, constant C is from Equation 2.14, and ffi ij is the Kronecker delta: ffi ij = 1; if i = j (D.3) Note that B is always a full n fi n matrix, even if the equilibrium point being checked contains some zero components <ref> [145, page 274] </ref>. The reason for this is natural. Imagine an equilibrium point in a face of the simplex S n , that is also asymptotically stable if the system is constrained to that face. <p> One small complication: the system of equations for finding an equilibrium point (Equation 2.17) has a redundancy, so there are only n 1 independent variables. This means that one of the eigenvalues of B should equal <ref> [145, page 274] </ref>: C * If this eigenvalue is not zero (i.e., C 6= 0) and occurs only once, and if all the other eigenvalues have modulus less than one, then the equilibrium point is locally asymptotically stable [145, page 284]. * If this eigenvalue is not zero (i.e., C 6= <p> This means that one of the eigenvalues of B should equal [145, page 274]: C * If this eigenvalue is not zero (i.e., C 6= 0) and occurs only once, and if all the other eigenvalues have modulus less than one, then the equilibrium point is locally asymptotically stable <ref> [145, page 284] </ref>. * If this eigenvalue is not zero (i.e., C 6= 0) and occurs more than once, then all eigenvalues (including this one) must have modulus less than one for the equilibrium to be locally asymptotically stable [145, page 284]. * If some eigenvalues of B have zero real <p> less than one, then the equilibrium point is locally asymptotically stable <ref> [145, page 284] </ref>. * If this eigenvalue is not zero (i.e., C 6= 0) and occurs more than once, then all eigenvalues (including this one) must have modulus less than one for the equilibrium to be locally asymptotically stable [145, page 284]. * If some eigenvalues of B have zero real part, then one must examine higher-order terms than B to determine if the equilibrium point is locally asymptot ically stable [32, page 20].
Reference: [146] <author> Rajkumar Roy and Ian C. Parmee. </author> <title> Adaptive restricted tournament selection and a local hill climbing hybrid for the identification of multiple "good" solutions. </title> <booktitle> In AISB workshop on Evolutionary Computation, </booktitle> <address> Brighton, </address> <month> 1-2 April </month> <year> 1996. </year>
Reference-contexts: That is, instead of competing with a parent, each offspring competes with the most similar individual in a sample. There is even an extension to this, known as "adaptive restricted tournament selection" <ref> [146] </ref>, which attempts to improve on the previous scheme [78]. 2.6.7 Restricted Competition: Fitness Sharing Like Beasley et al. [9], fitness sharing modifies a search landscape by reducing payoff in well-explored regions of the search space. It does this dynamically, during the course of a single run.
Reference: [147] <author> Gunter Rudolph. </author> <title> Convergence analysis of canonical genetic algorithms. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(1) </volume> <pages> 96-101, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: However, a canonical GA uses proportional selection: x i (t + 1) / f i =f . With proportional selection, a GA will not necessarily converge, to a global optimum or to anywhere else <ref> [147] </ref>, although it can happen under certain conditions [148]. A selection scheme that always maintains the best individual of the population, elitism, can be shown to converge to a global optimum [147] [149], and a GA using elitist selection is less sensitive to undersized populations [166]. <p> With proportional selection, a GA will not necessarily converge, to a global optimum or to anywhere else <ref> [147] </ref>, although it can happen under certain conditions [148]. A selection scheme that always maintains the best individual of the population, elitism, can be shown to converge to a global optimum [147] [149], and a GA using elitist selection is less sensitive to undersized populations [166]. We will use elitism in this thesis.
Reference: [148] <author> Gunter Rudolph. </author> <title> Convergence of non-elitist strategies. </title> <booktitle> In Proceedings of the First IEEE Conference on Evolutionary Computation, </booktitle> <volume> volume 1, </volume> <pages> pages 63-66. </pages> <publisher> IEEE Press, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: However, a canonical GA uses proportional selection: x i (t + 1) / f i =f . With proportional selection, a GA will not necessarily converge, to a global optimum or to anywhere else [147], although it can happen under certain conditions <ref> [148] </ref>. A selection scheme that always maintains the best individual of the population, elitism, can be shown to converge to a global optimum [147] [149], and a GA using elitist selection is less sensitive to undersized populations [166]. We will use elitism in this thesis.
Reference: [149] <author> Gunter Rudolph. </author> <title> Convergence of evolutionary algorithms in general search spaces. </title> <booktitle> In Proceedings of the 1996 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pages 50-54. </pages> <publisher> IEEE Press, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: With proportional selection, a GA will not necessarily converge, to a global optimum or to anywhere else [147], although it can happen under certain conditions [148]. A selection scheme that always maintains the best individual of the population, elitism, can be shown to converge to a global optimum [147] <ref> [149] </ref>, and a GA using elitist selection is less sensitive to undersized populations [166]. We will use elitism in this thesis.
Reference: [150] <author> Peter J. Russell. Genetics. Harper Collins, </author> <note> fourth edition, </note> <year> 1996. </year>
Reference-contexts: Genetic recombination is the dominant mechanism for genetic rearrangement in real organisms [87]. But crossover is not the same as sex | in nature, crossover as described in Figure 2.1 only happens in eukaryotes (a primitive single-celled organism) <ref> [150] </ref>. However, there are numerous sophisticated versions of crossover, and people can choose whichever works best for their problem. 2.2.2 Why It Works The GA quickly locates high-payoff regions of the search space because each single string samples many regions.
Reference: [151] <author> Conner Ryan. </author> <title> Racial harmony and function optimization in genetic algorithms | the races genetic algorithm. </title> <booktitle> In Proceedings of the 1995 Evolutionary Programming Conference, </booktitle> <pages> pages 296-307. </pages> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: This is significant because new GA speciation methods proliferate [122] [138] [143] <ref> [151] </ref> without a detailed understanding of why older GA speciation methods don't work perfectly. Section 4.2 attempts to help fill that gap, and provide a foundation for less ad hoc improvements to GA speciation. <p> Previous Work Some studies evaluate GA speciation methods only on simple test problems <ref> [151] </ref> [162], even though parallel hill-climbing is usually better on such easy test functions [117, page 202]. <p> New GA speciation methods [122] [138] [143] <ref> [151] </ref> are being presented without a proper comparison of previous This Thesis Extends Knowledge In Section 4.4, we compare two GA speciation methods, fitness sharing and its extension implicit sharing, in how comprehensively they cover the high-quality optima in a difficult search space. <p> This approach resembles several completely different genetic algorithms, the total population of which remains a constant. Although this increases the efficiency of fitness sharing [162], it brings new problems. In particular, it is possible that sub-populations with different tags could search the same peak <ref> [151, page 298] </ref>, which wastes computation time. <p> Other recent GA speciation methods include: * One that partitions the search space, with separate sub-populations searching their partition <ref> [151] </ref>. This which may not scale up well when searching a combinatorially large space. * One that repeatedly divides the population into sub-populations (the number of which varies dynamically) and re-combines them [84]. * An attempt to improve fitness sharing has led to "clearing" [138]. <p> We compare their ability to form species at as many near-optimal peaks as possible, when those peaks have similar fitness, but varying basins of attraction and inter-peak distances. More comparative results are needed. Some studies compare GA speciation methods only on simple test problems <ref> [151] </ref> [162], even though parallel hill-climbing is usually better on easy test functions [117, page 202]. <p> More comparative results are needed. Some studies evaluate GA speciation only on simple test problems <ref> [151] </ref> [162], even though parallel hill-climbing is usually better on easy test functions [117, page 202]. <p> New GA speciation methods proliferate [122] [138] [143] <ref> [151] </ref> without a proper comparison of previous GA speciation methods. This section attempts to help fill that gap. A complete mathematical analysis could make use of previous work for the two speciation methods we study, fitness sharing [117] and implicit sharing [159].
Reference: [152] <author> Steven L. Salzberg. </author> <title> On comparing classifiers: A critique of current research and methods. </title> <type> Technical Report CS-1995-06, </type> <institution> John Hopkins University, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: It may turn out that ... there is no way to measure generalisation more rigorously than via test problems. David Wolpert [179, page 246] This immediately raises the question of what goes into the test set, and how representative are its contents <ref> [152] </ref>. <p> Using their best cutoff values, Table 4.8 shows which of the two methods had the better classification rate. Is there a significant difference in the number of wins in Table 4.8? We use a binomial test, as each set of letters are drawn randomly from the alphabet <ref> [152] </ref>. The binomial test needs the number of times one method is better, and ignores the ties. We try two different definitions of a tie in Table 4.8: 1. The first is the usual one, where a method is better even if it classifies only one extra letter correctly. 2. <p> If there is actually no differences between the two methods, then p = q = 0:5 in Equation 4.8, and the probability that A and B are equally good is <ref> [152] </ref>: Chance they're the same = n! p s q ns (4.8) When p = q = 0:5, Equation 4.8 tells us the probability that both methods are equally good.
Reference: [153] <author> Nicol N. Schraudolph, Peter Dayan, and Terrence J. Sejnowski. </author> <title> Temporal difference learning of position evaluation in the game of go. </title> <editor> In J. D. Cowan, G. Tesauro, and J. Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing 6, </booktitle> <pages> pages 817-824. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: However, as dynamic programming is essentially an enumerative method, it only works well if the number of states is fairly small [73, page 187]. * Temporal Difference learning <ref> [153] </ref> [165] can learn control rules or artificial neural networks. Like dynamic programming, it requires a large memory for problems with large state spaces, and these memory constraints may require the space to be partitioned, which again requires prior knowledge [73, page 188].
Reference: [154] <author> Alan C. Schultz and John J. Grefenstette. </author> <title> Improving tactical plans with genetic algorithms. </title> <booktitle> In Proceedings of IEEE Conference on Tools for Artificial Intelligence, </booktitle> <pages> pages 328-334. </pages> <publisher> IEEE Press, </publisher> <month> November </month> <year> 1990. </year>
Reference-contexts: Later in this chapter, we study two plausible methods to improve the generali-sation ability of co-evolutionary learning: Page 64 University College, The University of New South Wales 1. Section 3.3.1: Seeding the initial GA population with known high-quality strategies, as examined elsewhere for a GA without co-evolution <ref> [154] </ref>; 2. Sections 3.3.2 and 3.4: Adding known high-quality strategies to the co-evolutionary GA's round robin to increase the diversity in the learning environment | i.e., each individual plays every other individual, plus some extra strategies. <p> In Section 3.4, we use different test environments. 3.3.1 Seeding the Initial Population One possible way to improve generalisation in co-evolutionary learning is to seed the initial population with strategies known to be of high quality. Schultz and Grefenstette [71] <ref> [154] </ref> used seeding, and found it gave a slight improvement. But their GA was heavily modified, and was not co-evolutionary, so this conclusion might not apply here. Seeding is easy to implement and check. <p> This nave vulnerability to novelty is a serious problem for simple co-evolutionary learning. We have examined two plausible attempts to improve this generalisation ability. The first, in Section 3.3.1, seeded the initial population with known high-quality strategies. Seeding gave improved results in a GA without co-evolution [71] <ref> [154] </ref>. Unfortunately, Figure 3.9 shows little difference in generalisation ability between no seeding and the optimal level of seeding. This is because seeding the initial population does not change diversity in the long run.
Reference: [155] <author> Christopher L. Scofield, Lannie Kenton, and Jung-Chou Chang. </author> <title> Multiple neural net architectures for character recognition. </title> <booktitle> In Compcon 91: Thirty-Sixth IEEE Computer Society International Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> Spring </month> <year> 1991. </year>
Reference-contexts: Species form and specialise to different strategies for the game, without human help or prior knowledge of what strategies would be the best to optimise. Their diverse expertise thus created is utilised in a unified way. Using an ensemble of separate experts is widely used with neural networks [76] <ref> [155] </ref>. There are many ways to adjudicate among a repertoire of different strategies. A gating algorithm is a generic term for an algorithm that adjudicates among different experts.
Reference: [156] <author> Martin I. Sereno. </author> <title> Four analogies between biological and cultural/linguistic evolution. </title> <journal> Journal of Theoretical Biology, </journal> <volume> 151(4) </volume> <pages> 467-508, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: That is, we create the evolutionary environment, to make evolution solve (hopefully) the problem we set. 2.2.3 Simple Replicator Equations The analogy between evolutionary biology and other forms of evolution (such as the GA) is fundamentally superficial <ref> [156] </ref>, yet the same quantitative dynamics can describe evolution in a wide range of information structures [157]. Replicator equations describe the populations of entities that reproduce. <p> The belief space itself can undergo variation and selection. This implements some observations that culture is varied and transmitted in ways similar to genetic evolution [10] [15] [25] [43, chapter 11] <ref> [156] </ref> [119, chapter 16]. This applies not only for human culture, but for non-human cultural objects such as bird song [43, pages 189-190]. Cultural algorithms are very interesting in that they emulate the evolution of societies. They have also produced impressive results compared to the canonical GA [140, page 99].
Reference: [157] <author> Karl Sigmund. </author> <title> A survey of replicator equations. </title> <editor> In John L. Casti and Anders Karlqvist, editors, </editor> <booktitle> Complexity, Language, and Life: Mathematical Approaches, volume 16 of Biomath-ematics, chapter 4, </booktitle> <pages> pages 88-104. </pages> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: create the evolutionary environment, to make evolution solve (hopefully) the problem we set. 2.2.3 Simple Replicator Equations The analogy between evolutionary biology and other forms of evolution (such as the GA) is fundamentally superficial [156], yet the same quantitative dynamics can describe evolution in a wide range of information structures <ref> [157] </ref>. Replicator equations describe the populations of entities that reproduce. As well as evolution, they explain epidemics [25, page 33], ecosystems [157], compound interest [3, page 20], population genetics, prebiotic evolution (chemical "hypercycles"), animal behaviour, population ecology, and evolutionary game theory [157]. <p> biology and other forms of evolution (such as the GA) is fundamentally superficial [156], yet the same quantitative dynamics can describe evolution in a wide range of information structures <ref> [157] </ref>. Replicator equations describe the populations of entities that reproduce. As well as evolution, they explain epidemics [25, page 33], ecosystems [157], compound interest [3, page 20], population genetics, prebiotic evolution (chemical "hypercycles"), animal behaviour, population ecology, and evolutionary game theory [157]. There are two approaches to replicator equations, continuous and discrete. These are closely related [3, page 81]. <p> describe evolution in a wide range of information structures <ref> [157] </ref>. Replicator equations describe the populations of entities that reproduce. As well as evolution, they explain epidemics [25, page 33], ecosystems [157], compound interest [3, page 20], population genetics, prebiotic evolution (chemical "hypercycles"), animal behaviour, population ecology, and evolutionary game theory [157]. There are two approaches to replicator equations, continuous and discrete. These are closely related [3, page 81]. Genetic algorithms have discrete generations, so we Page 14 University College, The University of New South Wales concentrate on the discrete case. Appendix A discusses some analogous results in the continuous case. <p> We also assume that an individual plays the game with (or has an equal chance of playing with) all other individuals 6 Adding all these probabilities gives the relative growth rate i of information pattern I i <ref> [157, page 89] </ref>: i = j 5 Some authors call matrix A the payoff matrix, but this can be confusing as the term usually applies to the game's payoff matrix. For example, in Figure 2.4 on page 24, the payoff matrix for Prisoner's Dilemma is a 2 fi 2 matrix. <p> If this is not the case, there are finite-population effects. We ignore these effects by assuming the population is very large, i.e., effectively infinite [32, page 5]. Substituting Equation 2.12 into the replicator Equation 2.6 means that p i changes according to [3, page 21] <ref> [157, page 92] </ref>: p i (t + 1) = p i (t) j a ij p j (t) (2.13) The term p Ap = P ij p i p j a ij is the average payoff of the entire population [3, page 21] [157, Equation 4.13]. <p> i changes according to [3, page 21] [157, page 92]: p i (t + 1) = p i (t) j a ij p j (t) (2.13) The term p Ap = P ij p i p j a ij is the average payoff of the entire population [3, page 21] <ref> [157, Equation 4.13] </ref>. <p> Equation 2.13 can be written in an alternative form that includes C, a positive constant 7 corresponding to the fitness without interaction [24, page 104] [85, page 133] [145, page 271] <ref> [157, page 92] </ref>: p i (t + 1) = p i (t) j a ij p j (t) + C (2.14) Equation 2.13 and its continuous counterpart, Equation A.5, are closed in the simplex S n : if a point starts on the simplex, p (0) 2 S n , then <p> The replicator equations for co-evolution (Equation 2.14) have a useful property in the quotient rule [85, page 125] <ref> [157, page 94] </ref>. For any information patterns I i and I j : p j (t + 1) p i (t) P P (discrete case) (2.15) 2.4.3 Equilibrium Points An equilibrium point is not necessarily stable. In this section, we use "equilibrium" to mean a fixed point of the equation. <p> Applying Definition 2.1 to Equation 2.13 says that a population vector p is an equilibrium point (i.e., a fixed point) if it satisfies [3, page 65] [85, pages 15, 126] <ref> [157, page 94] </ref>: n X a ij p j = p Ap for all i; 1 i n (2.16) If no information pattern is extinct at the equilibrium point (i.e. p i &gt; 0 for all i) then Equation 2.16 gives a system of linear equations for the n variables p <p> The proof for the discrete case of Equation 2.13 is straightforward when using the quotient rule of Equation 2.15. For the continuous case of Equation A.5, proofs include the version from classical game theory when individuals are mixed strategies, represented as points on the simplex [3, page 67] <ref> [157, page 102] </ref>. Theorem 2.1 lets us know from A whether there is an interior equilibrium point. <p> There is a more convenient test for stability (called "evolutionary stability"), which unfortunately does not accurately characterise stability. We discuss this next in Section 2.4.5. 9 However, extinction does not always happen: an example allows the population to move around the edges of the simplex <ref> [157, page 95] </ref>. Australian Defence Force Academy Page 33 2.4.5 Evolutionary Stability The concept of "evolutionary stability" uses pairwise comparisons of populations. Although popular, this definition of stability is not consistent with our intuitive meaning of stability. <p> Readers more familiar with classical game theory should avoid confusion. Definition 2.3 (Evolutionarily stable) A point p es 2 S n , is evolutionarily stable if it satisfies these conditions [3, page 72] [13, page 27] <ref> [157, page 101] </ref> [188, page 473]: 1. Equilibrium condition:- n X a ij p es 2. <p> Fortunately, this problem can be avoided. There are examples of how an interaction matrix A which yields infinitely many equilibria, can be perturbed to one that does not, and vice-versa <ref> [157, page 101] </ref>. In fact, it is conjectured that this can always be done i.e., that well-behaved interaction matrices, with only isolated equilibria, are dense in the space of matrices [188, page 478]. <p> We say the continuous replicator equations are permanent if there is a set in the interior of the simplex S n such that from any initial population, evolution eventually enters and stays inside that set <ref> [157, page 96] </ref>. Such a system is thus bounded away from the edge of S n , so that extinctions cannot occur | hence the term "permanent". <p> A system is permanent if there exists a function P defined on S n , with P (p) &gt; 0 on the interior of S n , and zero on the boundary, such that dP dt = P , where is a continuous function with the property that <ref> [157, Equation 4.24] </ref>: 1 Z T (p (t))dt &gt; 0 (2.27) This rather cumbersome test does not eliminate the possibility of an infinite number of equilibrium points, but if they are all in the same tiny region of the interior then it doesn't matter. <p> into the continuous replicator Equation A.3 means that p i changes according to 1 [3, page 21]: dp i = p i @ j=1 1 That last term p Ap = P ij p i p j a ij is the average payoff of the entire population [3, page 21] <ref> [157, Equation 4.13] </ref>. Incidentally, Equation A.5 is equivalent to the Lotka-Volterra equation [85, page 134] [172, pages 23-25]. <p> Incidentally, Equation A.5 is equivalent to the Lotka-Volterra equation [85, page 134] [172, pages 23-25]. Like Equation 2.14 on page 30, the continuous replicator equations for co-evolution also obey a quotient rule [85, page 125] <ref> [157, page 94] </ref>: 1 Maynard Smith suggests that the continuous Equation A.5 might be sometimes more biolog ically appropriate if its form was a quotient, as in Equation 2.13 [188, page 474]. <p> It gets closer, then further away, but on the whole it gets closer. D.2 Finding Stability by Linearisation Recall the discrete replicator equation from Equation 2.14 on page 30 [24, page 104] [85, page 133] [145, page 271] <ref> [157, page 92] </ref>: p i (t + 1) = p i (t) j a ij p j (t) + C Linearisation adds a perturbation vector q to the equilibrium point in question p eq .
Reference: [158] <author> Riyaz Sikora and Michael J. Shaw. </author> <title> A double-layered learning approach to acquiring rules for classification: Integrating genetic algorithms with similarity-based learning. </title> <journal> ORSA Journal on Computing, </journal> <volume> 6(2) </volume> <pages> 174-187, </pages> <year> 1994. </year>
Reference-contexts: It then re-starts the GA from a different random initial population. Page 44 University College, The University of New South Wales Sikora and Shaw <ref> [158] </ref> use a similar scheme for generating classification rules. When the GA population stops improving, they retain the best rule off-line and remove (from the examples set) all examples which this best rule covers. From a new random population, the GA restarts on the reduced problem. <p> University College, The University of New South Wales Specifically, to maintain the less fit of peaks i and j, we require [114, page 69]: s 1 min ( f i f j (2.31) This important flaw is often overlooked by simply assuming that we have perfect discrimination between different peaks <ref> [117, pages 106, 158] </ref>. These limitations can cause fitness sharing to fail to find all optima if they are not equidistant and equal-valued, or if the estimated distance between optima is incorrect.
Reference: [159] <author> Robert E. Smith, Stephanie Forrest, and Alan S. Perelson. </author> <title> Searching for diverse, cooperative populations with genetic algorithms. </title> <journal> Evolutionary Computation, </journal> <volume> 1(2) </volume> <pages> 127-149, </pages> <year> 1992. </year>
Reference-contexts: Natural speciation can create diverse and specialised solutions to surprisingly narrow aspects of the problem of survival. It is this process that we hope to emulate. Although the two most popular GA speciation methods have been successful on various problems [67] <ref> [159] </ref>, this thesis contributes to the knowledge of their flaws, and demonstrates some practical improvements [36] [38]. <p> In Chapter 4, we investigate the most popular GA speciation method, fitness sharing [45] [67], and discover an interesting flaw which can cause it to fail to find all optima. We also compare fitness sharing with a similar GA speciation method, implicit sharing [56] <ref> [159] </ref>. We find that implicit sharing is better at comprehensively covering all peaks when the population is large enough for a species to form at each optimum, but worse when the population is not large enough Page 4 University College, The University of New South Wales to do this. <p> The most popular and widely-used GA speciation method is fitness sharing [45]. Previous Work Section 2.6 describes several existing GA speciation methods, including fitness sharing. Fitness sharing works effectively on many difficult problems, but has some known flaws <ref> [159] </ref>, to be described in Section 2.6.7. <p> Page 6 University College, The University of New South Wales 1.4.3 New Flaws in Implicit Sharing Significance An extension of the original fitness sharing is known as implicit fitness sharing or emergent fitness sharing [56] <ref> [159] </ref> (hereafter termed simply implicit sharing). This was created to try to improve upon fitness sharing. As a part of the continuing progress of GA speciation methods, it is important to locate and describe any flaws with current methods. <p> Knowing that these flaws exist helps users to avoid problems, as well as helping designers of future GA speciation methods. Again, much of the motivation for improved GA speciation comes from engineering applications [134] [143] [175]. Previous Work Implicit sharing [56] <ref> [159] </ref> originally modeled the immune system. It has also been used for learning game strategies with co-evolution [38] [144]. One previous study [144] examined sampling in implicit sharing. <p> However, fitness sharing has its own limitations, including the following: 1. s is the same for all individuals, so all optima in the search space must be equidistant or nearly so <ref> [159] </ref>. 2. To set s requires a priori knowledge of how far apart optima are, and their (unshared) fitness. Without searching the space, this information is unknown. 18 In historical order, Beasley et al. [9] built on the work of Goldberg et al. [45] [67]. <p> To get around these problems, some schemes do not use a fixed sharing radius [78] [113] <ref> [159] </ref> [162]. Another study applies the restricted mating approach of Section 2.6.5 to fitness sharing, giving the Simple Subpopulation Scheme (SSS) [162]. <p> The most important aspect of the SSS extension is that it gets around the fixed sharing radius, and thus the requirement that peaks be equidistant and their number be known in advance. 2.6.8 Implicit Fitness Sharing Implicit fitness sharing [56] <ref> [159] </ref> is a modified version of fitness sharing, which we described in Section 2.6.7. Implicit fitness sharing 19 modifies the fitness function according to Table 2.1. Individuals bid for the payoff of discrete objects. <p> Implicit fitness sharing 19 modifies the fitness function according to Table 2.1. Individuals bid for the payoff of discrete objects. Originally, in an immune system simulation, antibodies which best matched an invading antigen received the payoff for that antigen [56] <ref> [159] </ref>. Another use is in learning a game: a strategy receives payoff when it achieves the best score against a sample test strategy [38] [144]. So in Table 2.1, "strategy" could be replaced by "antigen" or "sorting problem" or whatever. <p> So in Table 2.1, "strategy" could be replaced by "antigen" or "sorting problem" or whatever. At first glance, Table 2.1 looks completely different to the original fitness sharing in Section 2.6.7. Surprisingly, they share the same theoretical basis <ref> [159] </ref>. The sample size corresponds to the sharing radius s of the original: both control species 19 This is also known as emergent fitness sharing, but for the rest of this thesis we will call it simply implicit sharing. <p> The best in the sample receives payoff. In the case of a tie, payoff is shared equally among the tie-breakers. a Or the largest winning margin, if you prefer. Table 2.1: Payoff function for implicit fitness sharing <ref> [159] </ref>. size, and regulate the number of individuals who can obtain payoff by specialising to the same part of the search space. For example: * If the sample size is the population size, then only the best individual will get any payoff. <p> This suggests that this method may in future provide a working method for finding multiple solutions. The speciation methods suitable for finding multiple high-quality solutions are: * Fitness sharing [45] [67] and its variations [162]; * Implicit sharing [56] <ref> [159] </ref>; * Mahfoud's deterministic crowding [116] [117]; * More recent efforts, including Ronald's multiple solution technique [143]. Exactly which method we use is not our main focus: we hope to demonstrate that co-evolutionary learning with speciation is effective. <p> Australian Defence Force Academy Page 85 So which GA speciation method should we use? Section 2.6 reviewed various methods, and at the end (Section 2.6.10) we decided to look more closely at fitness sharing [45] [67], and its close relative implicit sharing [56] <ref> [159] </ref>. 4.1.1 Organisation of this Chapter The first part of this chapter, Sections 4.2 and 4.3, scrutinises fitness sharing. We discover a new flaw, adding to those described in Section 2.6.7 on page 50. <p> Practical improvements follow theoretical understanding. 4.3.3 Further Work on GA Speciation Other extensions to the GA may be effective at finding multiple peaks. Implicit sharing <ref> [159] </ref>, Mahfoud's deterministic crowding [113] [117, page 79], and Ronald's multiple solution technique [143], are examples of recent approaches to speciation that do away with the fixed niche radius of fitness sharing (Equation 2.30 on page 50). <p> New GA speciation methods proliferate [122] [138] [143] [151] without a proper comparison of previous GA speciation methods. This section attempts to help fill that gap. A complete mathematical analysis could make use of previous work for the two speciation methods we study, fitness sharing [117] and implicit sharing <ref> [159] </ref>. However, this would require a number of simplifying assumptions in order to be tractable. These assumptions may not be satisfied in practice. Here, we present empirical results, and find that when the population is large enough to form a species at each peak. <p> In Section 4.4.4, we describe the experimental setup, whose results are shown in Section 4.4.5, and discussed in Section 4.4.6. Section 4.4.7 concludes Section 4.4. 4.4.3 Sharing Methods to be Compared An extension to the original fitness sharing is implicit sharing [56] <ref> [159] </ref>, which we described in Section 2.6.8. Like the original, implicit sharing also modifies the fitness function. It does this according to Table 2.1 on page 52. In implicit sharing, individuals bid for the payoff of discrete objects. <p> It does this according to Table 2.1 on page 52. In implicit sharing, individuals bid for the payoff of discrete objects. Originally, in an immune system simulation, antigens were those objects: the antibody that best matched an invading antigen received the payoff for that antigen [56] <ref> [159] </ref>. Another use is in learning a game, where the discrete objects are test strategies: in Table 2.1, a strategy would receive payoff when it achieves the best-in-sample score against a test strategy [38] [144]. <p> This makes it suitable for co-evolutionary learning of game strategies, if each new test strategy comes from the GA population itself. At first glance, Table 2.1 looks completely different to fitness sharing. Surprisingly, they share the same theoretical basis <ref> [159] </ref>. The sample size of implicit sharing resembles the sharing radius s of fitness sharing: both control species size, and regulate the number of individuals who obtain payoff by specialising to the same part of the search space. <p> Thus there is less relative selection pressure. Australian Defence Force Academy Page 115 increase. Recall Table 2.1: under implicit sharing, the probability that w individuals from this subset (of 2) are in a sample of size taken without replacement, from a population of size N , is given by <ref> [159] </ref>: p (w; ; N; 2) = w C N2 C N (4.9) In these experiments, N = 50 and = 38. <p> Such an approach relies heavily on the speciation method to cover all the good strategies. 4.5 Conclusions about Speciation In this chapter, we have examined the most widley-used GA speciation method, fitness sharing [45] [67], and a newer method based on it, implicit sharing [56] <ref> [159] </ref>. In addition to the flaws of fitness sharing described in in Section 2.6.7 on page 50, we discovered in Section 4.2 a new flaw when using a scaling function with fitness sharing. <p> The payoffs in Figure 5.1 give mass extinctions just as in Section 3.2.2. As described in Section 2.6.7, fitness sharing [45] is a popular GA speciation method which has achieved good results in difficult search problems [67]. However, fitness sharing is not without flaws [36] <ref> [159] </ref>, including the one described in Section 4.2. Because of these flaws, as well as the results in Section 4.4, this chapter uses an extension to the original fitness sharing, known as implicit sharing [56] [159], which was described in Section 2.6.8. <p> However, fitness sharing is not without flaws [36] <ref> [159] </ref>, including the one described in Section 4.2. Because of these flaws, as well as the results in Section 4.4, this chapter uses an extension to the original fitness sharing, known as implicit sharing [56] [159], which was described in Section 2.6.8. Implicit sharing modifies the fitness function according to Table 5.1. Incidentally, Rosin and Belew [144] also used implicit sharing in a co-evolutionary, speciated GA to learn games. <p> Find the strategy in that sample which achieves the highest score against the single test strategy i. 3. The best in the sample receives payoff. In the case of a tie, payoff is shared equally among the tie-breakers. Table 5.1: Payoff function for implicit fitness sharing <ref> [159] </ref>. Simple questions about payoff and sampling raise complex issues in Section 5.2. these, and the gating algorithm that manages the diverse expertise thus found. <p> In this section, we will discuss: * Sections 5.2.1 and 5.2.2: How much payoff should the best-in-sample receive: a fixed credit, or should we follow Smith et al. <ref> [159] </ref> and credit them their score? This simple question raises quite complicated answers. <p> However, they can cause problems for users who do not know these flaws exist. 5.2.1 Extinction Despite Speciation We start with a simple question: in Table 5.1, how much payoff do we assign to the best strategy in the sample? In implicit sharing applied to the immune system <ref> [159] </ref>, the best individual in the sample was given payoff proportional to its "match score", or how well it matched the Australian Defence Force Academy Page 123 test antigen. <p> is more ruthless in some sense: even though strategy B is the highest scorer against another strategy B, that highest score is still low in absolute terms, so that: p AA &gt; p BB (5.3) So if we navely replace the "match score" in the original version of implicit sharing <ref> [159] </ref> with the game score achieved by the best strategy in the sample, then type A individuals will get more payoff than the ruthless type B individuals. <p> This loss will cause the remaining species to specialise to the surviving strategies, producing nave expertise that is vulnerable to the ruthless strategy B. 5.2.2 Genetic Drift Despite Speciation So, unlike the original implicit sharing <ref> [159] </ref>, we assign a fixed payoff to the best-in-sample, instead of its actual score. <p> Fitness sharing [45] has given impressive performance on difficult problems [67], and was for a long time the only practical GA method for multi-optima optimisation [117, page 84]. It is natural for such a revolutionary advance to be imperfect <ref> [159] </ref>. In Section 4.2, we find a new flaw of fitness sharing, whose effects are apparent in some previous studies. Knowing this problem suggests its solution, and in Section 4.3.2 we implement a simple change that partially overcomes this flaw. Implicit sharing is a modification of fitness sharing. <p> Knowing this problem suggests its solution, and in Section 4.3.2 we implement a simple change that partially overcomes this flaw. Implicit sharing is a modification of fitness sharing. Both share the same theoretical basis <ref> [159] </ref>. In Section 5.2, we discuss a number of problems that can occur when using implicit sharing with co-evolution. These are particularly insidious, as they cause extinction for only some species, giving the impression that diversity is maintained. But in fact it is precisely the ruthless strategies that become extinct.
Reference: [160] <author> Robert E. Smith and Brian Gray. </author> <title> Co-adaptive genetic algorithms: An example in Othello strategy. </title> <editor> In D. D. Dankel, editor, </editor> <booktitle> Proceedings of the 1994 Florida Artificial Intelligence Research Symposium, </booktitle> <pages> pages 259-264, </pages> <address> Saint Petersburg, Florida, </address> <year> 1994. </year> <institution> Florida AI Research Society. </institution>
Reference-contexts: Previous Work Axelrod [7] and others [27] [54] [108] have studied co-evolutionary learning of IPD, and co-evolution has been used to learn other games [46] [144] <ref> [160] </ref> and other tasks [81] [82] [99]. However, some co-evolutionary learning systems [108] [132] have displayed sudden "mass extinctions" | a high-quality strategy dominates the population for a long period of time, and suddenly dies out. <p> Can GA speciation decompose a solution for us, and relieve humans of that burden? Some previous studies [144] <ref> [160] </ref> have used a co-evolutionary GA with specia-tion to learn game strategies. However, these did not consider the question of how to make best use of the diverse expertise embodied in the various species. They merely considered individuals in the final GA generation one at a time. <p> In the baseball example, such a speciated GA might sensibly produce separate species of pitchers, batters, and fielders: but to only use the best individual instead of the whole repertoire of strategies is a waste of the diverse expertise created by the speciated GA. Smith and Gray <ref> [160] </ref> [161] found diverse high-quality strategies in Othello using a co-evolutionary GA with speciation, but did not address the question of when to use which strategy | they did, however, discuss the various separate species which had formed, finding some features of interest to a human player. <p> This thesis extends the work of Smith and Gray <ref> [160] </ref> [161] by using a gating algorithm to utilise the diverse expertise. Rosin and Belew [144] also used a speciated, co-evolutionary GA to learn games. However, they hoped that an individual in the GA would be well-rounded enough to play against all possible opponents. <p> In Chapter 4, we reviewed two promising GA specia-tion methods based on sharing. GA speciation is not a new idea. Co-evolution with speciation is rarer, but has been studied before. As discussed in Section 2.7: * Smith and Gray <ref> [160] </ref> [161] found diverse high-quality strategies in Othello using a co-evolutionary GA with speciation, but did not address the question of when to use which strategy | they did, however, discuss the various separate species which had formed, finding features of interest to a human player. <p> This thesis extends the work of Smith and Gray <ref> [160] </ref> [161] by using a gating algorithm to utilise this diverse expertise. * Rosin and Belew [144] also used a speciated, co-evolutionary GA to learn games. However, they hoped that an individual in the GA would be well-rounded enough to play against all possible opponents. <p> Evolving neural networks with genetic algorithms has become an increasingly mature field [57] [125] [183]. This indicates that a modular approach to evolving neural networks should be a fertile field for improved learning. For a different game, an earlier speciated co-evolutionary system <ref> [160] </ref> sometimes displayed a never-ending turnover of species. This agrees with the theoretical possibility (described in Section 2.4.7) that instead of equilibrium points being isolated, they can exist in a continuum of infinitely many equilibrium points. This would cause co-evolution to keeping moving through this continuum. <p> This can cause mass extinctions as in Figure 5.3, when mutation eventually creates a sufficiently novel strategy. Such collapses have been observed before [108] [132], and Chapter 3 sheds new light on why they happen. Unaware of this reason, some studies have nonetheless used both speciation and co-evolution [144] <ref> [160] </ref>. This prevents over-specialisation and improves generalisa-tion ability. However, although these studies learned diverse and specialised strategies, they did not address the question of when to use which species. This wastes much of the specialised expertise found by speciation. <p> Also, some previous studies have used speciation with co-evolution, to create specialised solutions [144] <ref> [160] </ref>, without considering the problem of how to re-combine that specialised expertise. Chapter 5 combines these two currents of research, presenting a system that creates expertise with no human help, using only a "black-box" simulation of the problem.
Reference: [161] <author> Robert E. Smith and Brian Gray. </author> <title> Co-adaptive genetic algorithms: An example in Othello strategy. </title> <type> Technical Report TCGA-94002, </type> <institution> The University of Alamaba, </institution> <year> 1994. </year>
Reference-contexts: In the baseball example, such a speciated GA might sensibly produce separate species of pitchers, batters, and fielders: but to only use the best individual instead of the whole repertoire of strategies is a waste of the diverse expertise created by the speciated GA. Smith and Gray [160] <ref> [161] </ref> found diverse high-quality strategies in Othello using a co-evolutionary GA with speciation, but did not address the question of when to use which strategy | they did, however, discuss the various separate species which had formed, finding some features of interest to a human player. <p> This thesis extends the work of Smith and Gray [160] <ref> [161] </ref> by using a gating algorithm to utilise the diverse expertise. Rosin and Belew [144] also used a speciated, co-evolutionary GA to learn games. However, they hoped that an individual in the GA would be well-rounded enough to play against all possible opponents. <p> In Chapter 4, we reviewed two promising GA specia-tion methods based on sharing. GA speciation is not a new idea. Co-evolution with speciation is rarer, but has been studied before. As discussed in Section 2.7: * Smith and Gray [160] <ref> [161] </ref> found diverse high-quality strategies in Othello using a co-evolutionary GA with speciation, but did not address the question of when to use which strategy | they did, however, discuss the various separate species which had formed, finding features of interest to a human player. <p> This thesis extends the work of Smith and Gray [160] <ref> [161] </ref> by using a gating algorithm to utilise this diverse expertise. * Rosin and Belew [144] also used a speciated, co-evolutionary GA to learn games. However, they hoped that an individual in the GA would be well-rounded enough to play against all possible opponents.
Reference: [162] <author> William M. Spears. </author> <title> Simple subpopulation schemes. </title> <booktitle> In Proceedings of the 1994 Evolutionary Programming Conference, </booktitle> <pages> pages 296-307. </pages> <publisher> World Scientific, </publisher> <year> 1994. </year> <institution> Australian Defence Force Academy Page 179 </institution>
Reference-contexts: Previous Work Some studies evaluate GA speciation methods only on simple test problems [151] <ref> [162] </ref>, even though parallel hill-climbing is usually better on such easy test functions [117, page 202]. <p> To get around these problems, some schemes do not use a fixed sharing radius [78] [113] [159] <ref> [162] </ref>. Another study applies the restricted mating approach of Section 2.6.5 to fitness sharing, giving the Simple Subpopulation Scheme (SSS) [162]. <p> To get around these problems, some schemes do not use a fixed sharing radius [78] [113] [159] <ref> [162] </ref>. Another study applies the restricted mating approach of Section 2.6.5 to fitness sharing, giving the Simple Subpopulation Scheme (SSS) [162]. <p> This approach resembles several completely different genetic algorithms, the total population of which remains a constant. Although this increases the efficiency of fitness sharing <ref> [162] </ref>, it brings new problems. In particular, it is possible that sub-populations with different tags could search the same peak [151, page 298], which wastes computation time. <p> This suggests that this method may in future provide a working method for finding multiple solutions. The speciation methods suitable for finding multiple high-quality solutions are: * Fitness sharing [45] [67] and its variations <ref> [162] </ref>; * Implicit sharing [56] [159]; * Mahfoud's deterministic crowding [116] [117]; * More recent efforts, including Ronald's multiple solution technique [143]. Exactly which method we use is not our main focus: we hope to demonstrate that co-evolutionary learning with speciation is effective. <p> We compare their ability to form species at as many near-optimal peaks as possible, when those peaks have similar fitness, but varying basins of attraction and inter-peak distances. More comparative results are needed. Some studies compare GA speciation methods only on simple test problems [151] <ref> [162] </ref>, even though parallel hill-climbing is usually better on easy test functions [117, page 202]. Another study compares GA speciation methods, but conspicuously forbids fitness sharing from using a scaling function [117, pages 206-207] | this may not be a fair comparison, as fitness sharing worked well with scaling [67]. <p> More comparative results are needed. Some studies evaluate GA speciation only on simple test problems [151] <ref> [162] </ref>, even though parallel hill-climbing is usually better on easy test functions [117, page 202].
Reference: [163] <author> Reiko Tanese. </author> <title> Distributed genetic algorithms. </title> <editor> In J. David Schaffer, editor, </editor> <booktitle> Proceedings of the Third International Conference on Genetic Algorithms, </booktitle> <pages> pages 434-439. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1989. </year>
Reference-contexts: This is analogous to having evolution proceed on isolated islands, with some inter-island migration. This is known as an island model or distributed GA. Such studies use distributed GAs [126] <ref> [163] </ref> [178] to improve the canonical GA, without attempting to find multiple optima. <p> Tanese <ref> [163] </ref> compared migration with zero migration | she observed: Some of the [results] indicate that the partitioned algorithm [no migration] consistently found fitter individuals than the distributed algorithm [some migration], which seemed to occur either when too many migrants are sent at one time, or when migrants are sent too frequently. <p> Reiko Tanese <ref> [163, page 438] </ref> A distributed GA with very low migration is equivalent to multiple runs of a canonical GA, so this method will give results similar to iterated schemes like Page 46 University College, The University of New South Wales Beasley et al. [9], which are not as good as the <p> Higher migration rates cause island models to behave like a normal GA. With very low migration, this method is equivalent to running completely different GA runs, equivalent to iterated schemes like Beasley et al. [9]. So although island models are useful at finding a single optimum faster [126] <ref> [163] </ref> [178], they have not yet been developed for finding multiple peaks. Australian Defence Force Academy Page 53 Landscape models can allow different sub-populations to form in different regions of the landscape [109, page 309], but not reliably [39, page 262].
Reference: [164] <author> A. D. Taylor. Metapopulations, </author> <title> dispersal, and predator-prey dynamics: an overview. </title> <journal> Ecology, </journal> <volume> 71 </volume> <pages> 429-433, </pages> <year> 1990. </year>
Reference-contexts: Only geographic neighbours interact: they compete with and cross over with each other. their geographic neighbours. This approach is motivated by a conjecture in landscape biology known as the Metapopulation Hypothesis [34] <ref> [164] </ref>. This says that migration between local populations promotes the persistence of a regional "metapopulation" despite short-term fluctuations, or even temporary extinctions, on the local scale. This hypothesis forms the basis of certain population models [35] [80].
Reference: [165] <author> Gerald Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 257-277, </pages> <year> 1992. </year>
Reference-contexts: However, as dynamic programming is essentially an enumerative method, it only works well if the number of states is fairly small [73, page 187]. * Temporal Difference learning [153] <ref> [165] </ref> can learn control rules or artificial neural networks. Like dynamic programming, it requires a large memory for problems with large state spaces, and these memory constraints may require the space to be partitioned, which again requires prior knowledge [73, page 188].
Reference: [166] <author> Dirk Thierens and David Goldberg. </author> <title> Elitist recombination: an integrated selection recombination GA. </title> <booktitle> In Proceedings of the First IEEE Conference on Evolutionary Computation, </booktitle> <volume> volume 1, </volume> <pages> pages 508-512. </pages> <publisher> IEEE Press, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: A selection scheme that always maintains the best individual of the population, elitism, can be shown to converge to a global optimum [147] [149], and a GA using elitist selection is less sensitive to undersized populations <ref> [166] </ref>. We will use elitism in this thesis. Even a GA with proportional selection will only find one solution, in the sense that its Australian Defence Force Academy Page 19 population will end up all very similar to each other, if not necessarily identical to each other.
Reference: [167] <author> Hans Henrik Thodberg. </author> <title> Ace of Bayes: Application of neural networks with pruning. </title> <type> Technical Report 1132E, </type> <institution> Danish Meat Research Institute, Roskilde, Denmark, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: There are many ways to adjudicate among a repertoire of different strategies. A gating algorithm is a generic term for an algorithm that adjudicates among different experts. Methods for choosing between different experts include: * Voting [76]; * Committees [110] <ref> [167] </ref> [168]; * Agent teams [12]; * Stacked generalisation [47] [180]; * Model averaging [23]; * Monte Carlo methods [129]; Page 58 University College, The University of New South Wales Later in this thesis, in Section 5.3.1, we will use the simple approach of voting among the repertoire of strategies.
Reference: [168] <author> Hans Henrik Thodberg. </author> <title> Review of Bayesian neural networks with an application to near infrared spectroscopy. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 7(1) </volume> <pages> 56-72, </pages> <year> 1996. </year>
Reference-contexts: There are many ways to adjudicate among a repertoire of different strategies. A gating algorithm is a generic term for an algorithm that adjudicates among different experts. Methods for choosing between different experts include: * Voting [76]; * Committees [110] [167] <ref> [168] </ref>; * Agent teams [12]; * Stacked generalisation [47] [180]; * Model averaging [23]; * Monte Carlo methods [129]; Page 58 University College, The University of New South Wales Later in this thesis, in Section 5.3.1, we will use the simple approach of voting among the repertoire of strategies.
Reference: [169] <author> Peter M. Todd and Geoffrey F. Miller. </author> <title> On the sympatric origin of species: Mercurial mating in the quicksilver model. </title> <editor> In Richard K. Belew and Lashon B. Booker, editors, </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pages 547-554. </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1991. </year>
Reference-contexts: Unfortunately, tag-template GA approaches have not yet demonstrated that they can maintain diverse sub-populations, except in flat search spaces, i.e., no selection [117, page 47] <ref> [169, page 553] </ref>.
Reference: [170] <author> Alan M. </author> <title> Turing. </title> <journal> Computing machinery and intelligence. Mind, </journal> <volume> 59(236), </volume> <year> 1950. </year>
Reference-contexts: Several early Artificial Intelligence (AI) researchers observed that evolution is an intelligent learning process [53, page 38] | for example, Alan Turing recognised "an obvious connection between [machine learning] and evolution" in 1950 <ref> [170] </ref>. After early attempts to emulate the learning process of evolution [53, pages 70-80], today evolutionary computation includes a range of approaches. The labels they go by include genetic algorithms (GA) [65] [86], evolutionary strategies (ES) [8], evolutionary programming (EP) [182], and genetic programming (GP) [101].
Reference: [171] <author> Geerat J. Vermeij. </author> <title> The evolutionary interaction among species: selection, </title> <editor> escalation, and coevolution. In Daphne G. Fautin, Douglas J. Futuyma, and Frances C. James, editors, </editor> <booktitle> Annual Review of Ecology and Systematics, </booktitle> <volume> volume 25, </volume> <pages> pages 219-236. </pages> <publisher> Annual Reviews Inc., </publisher> <address> Palo Alto, </address> <year> 1994. </year>
Reference-contexts: It does not yet offer useful tools for a deeper understanding of this stable end to co-evolution. Incidentally, in natural evolution the rules sometimes do change (due to things like climate changes) which can cause specialised species to become extinct <ref> [171, page 222] </ref>. <p> Over-specialisation and collapse also happens in natural co-evolution, where: Those species that are most functionally specialised, most highly esca lated, ... are especially prone to extinction. Geerat J. Vermeij <ref> [171, page 222] </ref> A fanciful analogy may help explain this: imagine a species of big-horned buffalo competing with other species of buffalo. Australian Defence Force Academy Page 73 1. Our species' huge horns make lesser-horned competitors extinct. 2.
Reference: [172] <author> Hans-Michael Voigt. </author> <title> Evolution and Optimization. </title> <publisher> Akademie-Verlag, </publisher> <address> Berlin, </address> <year> 1989. </year>
Reference-contexts: The simple fitness parameters i are replaced by terms representing selection, mutation, 3 This is true both for the discrete case in Equation 2.13 [85, pages 17-19] <ref> [172, pages 38-39] </ref> and the continuous case in Equation A.5 [85, pages 226-227] [172, pages 31-36]. The fundamental theorem of natural selection applies to diploid genetics, where the interaction matrix A (to be described in Section 2.4.2) is symmetric, a ij = a ji . <p> The simple fitness parameters i are replaced by terms representing selection, mutation, 3 This is true both for the discrete case in Equation 2.13 [85, pages 17-19] [172, pages 38-39] and the continuous case in Equation A.5 [85, pages 226-227] <ref> [172, pages 31-36] </ref>. The fundamental theorem of natural selection applies to diploid genetics, where the interaction matrix A (to be described in Section 2.4.2) is symmetric, a ij = a ji . <p> the interaction matrix A is a 4 fi 4 matrix, then we must solve Equations 2.17 and 2.18 for each of [145, page 273]: 8 Except for those p i which are zero, for which the value of P j a ij p j need not equal the average fitness <ref> [172, page 27] </ref>. Australian Defence Force Academy Page 31 * the full system of 4 dimensions; * the four 3-dimensional subsystems, formed by constraining one dimension at a time to be zero; * the six 2-dimensional subsystems, and; * the four 1-dimensional subsystems. <p> There are several convenient ways of telling if a given equilibrium point p es is evolutionarily stable (ES). 10 The classical game theory approach interprets the points p on the simplex as mixed strategies [85, pages 121, 127] <ref> [172, page 28] </ref>. Page 34 University College, The University of New South Wales Theorem 2.2 ([157, page 102]) The following conditions are equivalent: 1. Point p eq 2 S n is evolutionarily stable; 2. <p> We first consider the behaviour of the continuous system of Equation A.5. For the continuous case of Equation A.5, examples abound of non-ES equilibria that are still local attractors [32, page 16] | an evolutionarily stable point is an attractor, but not every attractor is an evolutionarily stable point <ref> [172, page 29] </ref> [188, page 474]. That is: from the point of view of smooth dynamics [Equation A.5] an attractor is a more general notion than [evolutionary stability], and a better characterisation of the resistance to mutation. E. C. Zeeman [188, page 474]. <p> Incidentally, Equation A.5 is equivalent to the Lotka-Volterra equation [85, page 134] <ref> [172, pages 23-25] </ref>.
Reference: [173] <author> Michael D. Vose and Gunar E. Liepins. </author> <title> Punctuated equilibria in genetic search. </title> <journal> Complex Systems, </journal> <volume> 5 </volume> <pages> 31-44, </pages> <year> 1991. </year>
Reference-contexts: However, some co-evolutionary learning systems [108] [132] have displayed sudden "mass extinctions" | a high-quality strategy dominates the population for a long period of time, and suddenly dies out. An explanation for this behaviour is not yet complete, and is an active area of research [58] [96] <ref> [173] </ref>. Australian Defence Force Academy Page 5 This Thesis Extends Knowledge Chapter 3 examines the generalisation ability of expertise produced by a simple coevolutionary system. We also observe mass extinctions, and discover a novel reason for why they occur: convergence to a single solution causes over-specialisation and poor generalisation ability. <p> Why did these high-scoring strategies generalise poorly? One study analysed punctuated equilibria in a GA that uses a fixed evaluation function <ref> [173] </ref>. In short, this found that the (fixed) fitness function creates attractors in the search space, some of which are unstable. <p> Attraction to an unstable attractor gives the impression of convergence, as the population changes less as it gets closer to that attractor: but as it's unstable, attraction to a different attractor causes a major change <ref> [173, page 40] </ref>. The evaluation function is fixed in that study, but a co-evolutionary evaluation function changes as the population evolves. Extending that work [173] to coevolution may give interesting insights. But considering a fixed evaluation function does not cover some issues particular to co-evolution. <p> The evaluation function is fixed in that study, but a co-evolutionary evaluation function changes as the population evolves. Extending that work <ref> [173] </ref> to coevolution may give interesting insights. But considering a fixed evaluation function does not cover some issues particular to co-evolution.
Reference: [174] <author> Benjamin W. Wah. </author> <title> Population-based learning: A method for learning from examples under resource constraints. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 4(5) </volume> <pages> 454-474, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: But in many interesting problems, payoff is received only after several decision steps, made by many different rules [73, page 187]. In such delayed-payoff problems, the question of which rule receives how much credit is unclear [86, pages 176-179] <ref> [174, page 456] </ref>, and elaborate credit assignment algorithms must be used. One of these is the bucket brigade algorithm [86, page 177]. This algorithm is not without flaws, and later attempts have tried to improve upon it [72]. <p> It turns out that searching the space of game strategies is suited to evolutionary learning [44], as we describe in Section 2.3.2. We describe two games that are popular and useful machine learning problems in Sections 2.3.3 and 2.3.4. Evolutionary learning is especially good for knowledge-lean <ref> [174] </ref> or black box [1] problems, i.e., problems for which little or no prior knowledge exists. Prior knowledge would allow the search to be limited to regions known to be promising. 2.3.2 The Sequential Decision Problem Sequential decision tasks include many control and management tasks, including many games.
Reference: [175] <author> Hirokaza Watabe and Norio Okino. </author> <title> Structural shape optimization by multi-species genetic algorithm. </title> <editor> In Chris Rowles, Huan Liu, and Norman Foo, editors, </editor> <booktitle> Proceedings of the Sixth Aus-tralian Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 109-116. </pages> <publisher> World Scientific, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: Much of the motivation for GA speciation is to find diverse high-quality solutions for engineering problems [134] [143] <ref> [175] </ref>. The most popular and widely-used GA speciation method is fitness sharing [45]. Previous Work Section 2.6 describes several existing GA speciation methods, including fitness sharing. Fitness sharing works effectively on many difficult problems, but has some known flaws [159], to be described in Section 2.6.7. <p> Knowing that these flaws exist helps users to avoid problems, as well as helping designers of future GA speciation methods. Again, much of the motivation for improved GA speciation comes from engineering applications [134] [143] <ref> [175] </ref>. Previous Work Implicit sharing [56] [159] originally modeled the immune system. It has also been used for learning game strategies with co-evolution [38] [144]. One previous study [144] examined sampling in implicit sharing.
Reference: [176] <author> Peter A. Whigham. </author> <title> A schema theorem for context-free grammars. </title> <booktitle> In Proceedings of the 1995 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pages 178-181. </pages> <publisher> IEEE Press, </publisher> <month> 29 November - 1 December </month> <year> 1995. </year>
Reference-contexts: Games of conflict lack this property. Hence the need for other "fundamental" theorems. Australian Defence Force Academy Page 17 and recombination. Again, we only consider a GA with binary fixed-length strings. Other schema theorems exist for other representations of individuals, e.g., for tree structures <ref> [176] </ref>. Let x i (t) be the number of copies of schema I i at generation t (we described schemae in Section 2.2.2).
Reference: [177] <author> D. Whitley, K. Mathias, and P. Fitzhorn. </author> <title> Delta coding: An iterative search strategy for genetic algorithms. </title> <editor> In Richard K. Belew and Lashon B. Booker, editors, </editor> <booktitle> Proceedings of the Fourth International Conference on Genetic Algorithms, </booktitle> <pages> pages 77-84. </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1991. </year>
Reference-contexts: Several schemes re-initialise the GA population with the aim of refining the current best solution, not to find multiple optima [49] [118] <ref> [177] </ref>. Cobb and Grefenstette [29] did something similar: instead of completely re-initialising the whole population with random individuals, they re-initialised only a part of it. However, their work was in the context of tracking a changing environment, and not finding multiple optima.
Reference: [178] <author> Darrell Whitley and Timothy Starkweather. </author> <title> Genitor ii: a distributed genetic algorithm. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 189-214, </pages> <year> 1990. </year>
Reference-contexts: This is analogous to having evolution proceed on isolated islands, with some inter-island migration. This is known as an island model or distributed GA. Such studies use distributed GAs [126] [163] <ref> [178] </ref> to improve the canonical GA, without attempting to find multiple optima. <p> Higher migration rates cause island models to behave like a normal GA. With very low migration, this method is equivalent to running completely different GA runs, equivalent to iterated schemes like Beasley et al. [9]. So although island models are useful at finding a single optimum faster [126] [163] <ref> [178] </ref>, they have not yet been developed for finding multiple peaks. Australian Defence Force Academy Page 53 Landscape models can allow different sub-populations to form in different regions of the landscape [109, page 309], but not reliably [39, page 262].
Reference: [179] <author> David H. Wolpert. </author> <title> A mathematical theory of generalization. </title> <journal> Complex Systems, </journal> <volume> 4 </volume> <pages> 151-249, </pages> <year> 1990. </year>
Reference-contexts: Part of the popularity of neural networks is due to their perceived ability to generalise, although this reputation is controversial <ref> [179] </ref>. <p> Part of the popularity of neural networks is due to their perceived ability to generalise, although this reputation is controversial [179]. Wolpert gives a description of generalisation <ref> [179, page 156] </ref>: "Now define gen-eralisation as the ability of a system to take a learning set ... and based only on that learning set, make a `good' guess ... corresponding to an input not contained in the learning set." In learning a game of conflict, a strategy generalises well if <p> How can we measure the generalisation ability of a strategy for a game, when we do not know a priori the high-quality strategies for that game? 2.8.2 Measures of Generalisation Ability There is no clear-cut way to measure generalisation ability. Wolpert <ref> [179] </ref> derives the attributes of a perfect generaliser, which he defines in terms of "self-guessing": when exposed to a subset of a learning set, a perfect generaliser correctly generates the unseen remainder of that learning set 22 . <p> The training set could easily miss a high-quality strategy, causing poor generalisation against an opponent who follows that strategy. So this measure of generalisation ability is not appropriate here. The usual and ad hoc method of measuring generalisation ability is applying the algorithm to a test set. Wolpert <ref> [179, page 246] </ref> observes: When saying that these systems are good generalisers, all that is really meant is that they have tested well when presented with problems whose "correct" generalisations are known beforehand. <p> Clearly, this is an ad hoc 22 Incidentally, Wolpert's self-guessing criterion is essentially equivalent to the information compactification criterion <ref> [179, footnote on page 222] </ref>, which is another way to characterise generalisation. Australian Defence Force Academy Page 59 way of dealing with the highly complex and vital issue of generalisation. It may turn out that ... there is no way to measure generalisation more rigorously than via test problems. <p> Australian Defence Force Academy Page 59 way of dealing with the highly complex and vital issue of generalisation. It may turn out that ... there is no way to measure generalisation more rigorously than via test problems. David Wolpert <ref> [179, page 246] </ref> This immediately raises the question of what goes into the test set, and how representative are its contents [152]. <p> If an algorithm cannot generalise from the seen to the unseen, then it is no more intelligent than a tape recorder that blindly memorises and regurgitates information. Generalisation is so important, that part of the reason for today's interest in neural networks is their reputation for generalising <ref> [179] </ref>. Although there has been much interest in co-evolutionary learning, we believe our study is novel in examining the generalisation ability of a simple co-evolutionary GA. <p> We do this by seeing how well they perform against a set of test opponents, as justified in Section 2.8. As Wolpert <ref> [179, page 246] </ref> observes, "It may turn out that ... there is no way to measure generalisation more rigorously than via test problems." To obtain a set of test strategies, we performed a large random search of all strategy genotypes, for each value of remembered history l.
Reference: [180] <author> David H. Wolpert. </author> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5(2) </volume> <pages> 241-259, </pages> <year> 1992. </year>
Reference-contexts: A gating algorithm is a generic term for an algorithm that adjudicates among different experts. Methods for choosing between different experts include: * Voting [76]; * Committees [110] [167] [168]; * Agent teams [12]; * Stacked generalisation [47] <ref> [180] </ref>; * Model averaging [23]; * Monte Carlo methods [129]; Page 58 University College, The University of New South Wales Later in this thesis, in Section 5.3.1, we will use the simple approach of voting among the repertoire of strategies.
Reference: [181] <author> David H. Wolpert and William G. Macready. </author> <title> No free lunch theorems for search. </title> <type> Technical Report SFI TR 95-02-010, </type> <institution> The Santa Fe Institute, </institution> <address> Santa Fe NM 87501, </address> <month> February </month> <year> 1996. </year> <institution> Page 180 University College, The University of New South Wales </institution>
Reference-contexts: not the same as the Michigan approach, which uses parts of a solution, but is a novel approach, as we discuss in Section 5.3.1. 2.3 Problems of Interest 2.3.1 Introduction: No Free Lunch The No Free Lunch theorem states that no search algorithm is best for all possible search problems <ref> [181] </ref>. That is, "if algorithm A outperforms algorithm B on some cost [i.e., fitness/payoff] functions, then loosely speaking there must exist exactly as many other functions where B outperforms A" [181]. Different search problems are best handled by different search algorithms. <p> No Free Lunch The No Free Lunch theorem states that no search algorithm is best for all possible search problems <ref> [181] </ref>. That is, "if algorithm A outperforms algorithm B on some cost [i.e., fitness/payoff] functions, then loosely speaking there must exist exactly as many other functions where B outperforms A" [181]. Different search problems are best handled by different search algorithms. As a result, the only way to justify using a particular search algorithm is to argue that it suits the features of the space to be searched [181, pages 25, 27]. <p> Different search problems are best handled by different search algorithms. As a result, the only way to justify using a particular search algorithm is to argue that it suits the features of the space to be searched <ref> [181, pages 25, 27] </ref>. If, as is often the case, one lacks prior knowledge of the features of the search space, then the next best solution is to use a search algorithm good for problems of that general type [181, page 28]. <p> If, as is often the case, one lacks prior knowledge of the features of the search space, then the next best solution is to use a search algorithm good for problems of that general type <ref> [181, page 28] </ref>. In Appendix B, we briefly discuss the general attributes of problems suited to evolutionary learning. It turns out that searching the space of game strategies is suited to evolutionary learning [44], as we describe in Section 2.3.2.
Reference: [182] <author> Xin Yao. </author> <title> An empirical study of genetic operators in genetic algorithms. </title> <journal> Microprocessing and Microprogramming, </journal> <volume> 38 </volume> <pages> 707-714, </pages> <year> 1993. </year>
Reference-contexts: After early attempts to emulate the learning process of evolution [53, pages 70-80], today evolutionary computation includes a range of approaches. The labels they go by include genetic algorithms (GA) [65] [86], evolutionary strategies (ES) [8], evolutionary programming (EP) <ref> [182] </ref>, and genetic programming (GP) [101]. This thesis concentrates on genetic algorithms, but is broadly applicable to similar algorithms. Some critics assert that artificial evolution cannot be intelligent, because natural evolution is a slow process [123, page 71].
Reference: [183] <author> Xin Yao. </author> <title> A review of evolutionary artificial neural networks. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 8 </volume> <pages> 539-567, </pages> <year> 1993. </year>
Reference-contexts: But speciation as automatic modularisation can solve more realistic and complicated problems. Modular neural networks have been effective in many problems [55] [91]. Evolving neural networks with genetic algorithms has become an increasingly mature field [57] [125] <ref> [183] </ref>. This indicates that a modular approach to evolving neural networks should be a fertile field for improved learning. For a different game, an earlier speciated co-evolutionary system [160] sometimes displayed a never-ending turnover of species.
Reference: [184] <author> Xin Yao. </author> <title> Evolutionary stability in the n-person iterated prisoner's dilemma. </title> <journal> BioSystems, </journal> <volume> 37(3) </volume> <pages> 189-197, </pages> <year> 1996. </year>
Reference-contexts: There exist more sophisticated ways to take into account the shadow of the future [6, page 13]. Section 2.4.5 will discuss "evolutionary stability". Iterated Prisoner's Dilemma does not allow evolutionary stability [17], unless mistakes can occur [16]. These two results also apply to the N -player game <ref> [184] </ref>. Unfortunately, as described in Australian Defence Force Academy Page 25 Section 2.4.6, evolutionary stability is not an accurate characterisation of stability (despite the name). So these analytical results do not give definite answers about the behaviour of a co-evolving GA learning IPD. <p> the conditions of Equation 2.11: D i &gt; C i f or 0 i n 1 C i+1 &gt; C i f or 0 i &lt; n 1 (2.11) Although there has been much research on the 2IPD using evolutionary learning in recent years, NIPD is less studied [62] [63] <ref> [184] </ref> [186] despite its importance and its qualitative difference from the 2IPD. 2.4 Mathematical Analysis of Co-Evolution Emulating co-evolution can induce an "arms race" of innovation, and create expertise for games of conflict and other management and control jobs. This section reviews the mathematical analysis of co-evolution.
Reference: [185] <author> Xin Yao and Paul J. Darwen. </author> <title> An experimental study of n-person iterated prisoner's dilemma games. </title> <journal> Informatica, </journal> <volume> 18 </volume> <pages> 435-450, </pages> <year> 1994. </year>
Reference-contexts: Rank-based selection was used, with the worst performer assigned an average of 0.75 offspring, the best 1.25 offspring 5 Each NIPD games lasts for 100 iterations. Unlike the 2-player IPD, cooperation is less likely as the number of players N increases [62] [63] <ref> [185] </ref> [186]. This means that as N increases, a co-evolutionary GA learning NIPD is less likely to converge to a population where cooperation dominates, as we saw in 2IPD in Figure 3.3 on page 68.
Reference: [186] <author> Xin Yao and Paul J. Darwen. </author> <title> An experimental study of N-person iterated prisoner's dilemma games. </title> <editor> In Xin Yao, editor, </editor> <booktitle> Progress in Evolutionary Computation, volume 956 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 90-108. </pages> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: conditions of Equation 2.11: D i &gt; C i f or 0 i n 1 C i+1 &gt; C i f or 0 i &lt; n 1 (2.11) Although there has been much research on the 2IPD using evolutionary learning in recent years, NIPD is less studied [62] [63] [184] <ref> [186] </ref> despite its importance and its qualitative difference from the 2IPD. 2.4 Mathematical Analysis of Co-Evolution Emulating co-evolution can induce an "arms race" of innovation, and create expertise for games of conflict and other management and control jobs. This section reviews the mathematical analysis of co-evolution. <p> Rank-based selection was used, with the worst performer assigned an average of 0.75 offspring, the best 1.25 offspring 5 Each NIPD games lasts for 100 iterations. Unlike the 2-player IPD, cooperation is less likely as the number of players N increases [62] [63] [185] <ref> [186] </ref>. This means that as N increases, a co-evolutionary GA learning NIPD is less likely to converge to a population where cooperation dominates, as we saw in 2IPD in Figure 3.3 on page 68. <p> Figure 5.1 shows the variant of the payoff matrix of the two-player prisoner's dilemma used in this chapter. The general definition of prisoner's dilemma <ref> [186, page 91] </ref> is in Section 2.3.3. Page 120 University College, The University of New South Wales Player A D C D Payoff to player A Player B 3 1 defect (D). Payoff is symmetric for both players. ure 5.1 cause the evolution of cooperation, just like in Section 3.2.2.
Reference: [187] <author> Xin Yao and Yong Liu. </author> <title> Ensemble structure of evolutionary artificial neural networks. </title> <booktitle> In Proceedings of the 1996 IEEE Conference on Evolutionary Computation. </booktitle> <publisher> IEEE Press, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: A closely related study found combining several high-quality neural networks gave better results than only one high quality neural network <ref> [187] </ref>. Despite the success of the modular approach, it may not be the best solution for all possible problems, and has been criticised on philosophical grounds [135]. While more sophisticated problem-solving approaches may be possible, the modular approach remains an effective way to solve many real-world problems.

References-found: 187

