URL: http://www.research.microsoft.com/~philtorr/Papers/Sic/sic.ps
Refering-URL: http://www.research.microsoft.com/~philtorr/paper.html
Root-URL: http://www.research.microsoft.com
Email: philtorr@microsoft.com,  
Title: Model Selection for Two View Geometry: A Review  
Author: P. H. S. Torr 
Note: home page:  
Web: WWW  http://www.research.microsoft.com/research/vision/  
Address: One Microsoft Way, Redmond, WA 98052, USA,  
Affiliation: Microsoft Research,  
Abstract: Computer vision often concerns the estimation of models of the world from visual input. Sometimes it is possible to fit several different models or hypotheses to a set of data, the choice of which is usually left to the vision practitioner. This paper explores ways of automating the model selection process, with specific emphasis on the least squares problem. The statistical literature is reviewed and it will become apparent that although no one method has yet been developed that will be generally useful for all computer vision problems, there do exist some useful partial solutions. Thus this paper is intended as a beginner's guide to model selection, highlighting the pertinent problem areas in model selection and illustrating them by the example of estimating two view geometry. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M.A. Aitkin. </author> <title> Posterior Bayes Factors. J.R. </title> <journal> Statist. Soc. B, </journal> <volume> 53(1) </volume> <pages> 111-142, </pages> <year> 1991. </year>
Reference-contexts: Other Ways to Approximate Bayes Factors The crux of the problem with Bayes factors is the choice of prior Pr ( k jR k ) as this may influence the result, ideally the Bayes factors should be evaluated over a range of priors to check their stability. Aitkin <ref> [1] </ref> suggests using the posterior pdf of to compute what he calls posterior Bayes factors.
Reference: 2. <author> H. Akaike. </author> <title> A new look at the statistical model identification. </title> <journal> IEEE Trans. on Automatic Control, </journal> <volume> Vol. AC-19(6):716-723, </volume> <year> 1974. </year>
Reference: 3. <author> P. Beardsley, P. H. S. Torr, and A. Zisserman. </author> <title> 3D model aquisition from extended image sequences. </title> <editor> In B. Buxton and Cipolla R., editors, </editor> <booktitle> Proc. 4th European Conference on Computer Vision, </booktitle> <publisher> LNCS 1065, Cambridge, </publisher> <pages> pages 683-695. </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: This paper reviews current statistical methods in model selection with respect to determining the two view geometric relations from the point matches between two images of a scene, e.g. the fundamental matrix [11, 17]. These relations can be used to guide matching [47, 53] and thence structure <ref> [3] </ref> or segmentation [46]. There are several two view relations that could describe an image pair. Hence it is necessary to estimate the type of model as well as the parameters of the model. The paper is laid out as follows.
Reference: 4. <author> P. Beardsley, P. H. S. Torr, and A. Zisserman. </author> <title> 3D model aquisition from extended image sequences. </title> <editor> In B. Buxton and Cipolla R., editors, </editor> <booktitle> Proc. 4th European Conference on Computer Vision, </booktitle> <publisher> LNCS 1065, Cambridge, </publisher> <pages> pages 683-695. </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: If the type of relation R is known then, observing the data, we can estimate the parameters of R to minimize this log likelihood. This inference is called `Maximum Likelihood Estimation' (Fisher 1936 [12]). Numerical methods for finding these two and three view relations are given in <ref> [4] </ref>. Robust Estimation The above derivation assumes that the errors are Gaussian, often however features are mismatched and the error on m is not Gaussian.
Reference: 5. <author> H. Bozdogan. </author> <title> Model selection and Akaike's information criterion (aic): The general theory and its analytical extensions. </title> <journal> Psychometrika, </journal> <volume> 52(3) </volume> <pages> 345-370, </pages> <year> 1987. </year>
Reference-contexts: Generally 1 should be set to 2, a useful value for 2 is 4.0 so that for (p 1 p 2 ) = 1 ff = 0:0456. The fact that the AIC tends to overfit is generally recognized in the literature. Bozdogan <ref> [5] </ref> attempts to derive measures that are asymptotically consistent CAIC = L + 1 p (log (N ) + 1) CAICF = L + 1 p (log (N ) + 2) + log jJj where J is the information matrix of the estimated parameters.
Reference: 6. <author> C. Chatfield. </author> <title> Model uncertainty, data mining and statistical inference. </title> <journal> J. R. Statist Soc A., </journal> <volume> 158 </volume> <pages> 419-466, </pages> <year> 1995. </year>
Reference-contexts: So far I have described methods in which all the potential models are fitted and a model selection procedure is used to select which is best and it is adopted rather than from other defensible models. This is somewhat arbitrary ("a quiet scandal" <ref> [6] </ref>), first admitting model uncertainty by searching for a "best" model and then ignoring this uncertainty by making inferences and forecasts as if it were certain that his chosen model is actually correct.
Reference: 7. <author> G. Csurka, C. Zeller, Z. Zhang, and O. Faugeras. </author> <title> Characterizing the uncertainty of the fundamental matrix. </title> <journal> CVIU, </journal> <volume> 68(1) </volume> <pages> 18-36, </pages> <year> 1996. </year>
Reference: 8. <author> M. </author> <title> DeGroot. Optimal Statistical Decisions. </title> <publisher> McGraw-Hill, </publisher> <year> 1970. </year>
Reference-contexts: In data contaminated with outliers this procedure is thrawt with peril, e.g. all the outliers lie in one of the sets. One can use uniform or diffuse priors but great care must be taken when doing this or Lindley's paradox may occur <ref> [8] </ref> in which one model is arbitrarily favoured over another.
Reference: 9. <author> D. Draper. </author> <title> Assessment and propagation of model uncertainty (with discussion). </title> <journal> Journal of the Royal Statistical Society series B, </journal> <volume> 57 </volume> <pages> 45-97, </pages> <year> 1995. </year>
Reference-contexts: If more than one theory is consistent with the observations, keep all theories" [29]. Thus as pointed out by his follower Lucretius <ref> (95-55 B.C.) </ref> if there are several explanations as to why a man died, but one does not definitively know which one is true, then Lucretius (and later Solomonoff [29]) advocates maintaining them all for the purpose of prediction (somewhat the opposite of Ockham's point of view). <p> Fig. 2. Two cases, X where the uncertainty in the model is unimportant, Y where the uncertainty in the model becomes important Bayesian model averaging takes into account explicitly model uncertainty by representing the data|each set of matches by a combination of models <ref> [9, 20, 24, 27, 33] </ref>. Rather than using one motion model a standard Bayesian formalism is adopted which averages the posterior distributions of the prediction under each model, weighted by their posterior model probabilities.
Reference: 10. <author> B. Efron and R.J. Tibshirani. </author> <title> An Introduction to the Bootstrap. </title> <publisher> Chapman and Hall, </publisher> <address> London, UK, </address> <year> 1993. </year>
Reference-contexts: Model averaging produced little improvement together with increasing the amount of computation necessary to estimate likelihoods. 10 Discussion Another popular class of methods for model selection include cross validation, (In the asymptotic case Stone [43] demonstrates that AIC and cross validation are equivalent.) jackknifing, bootstrap, and data splitting <ref> [10, 19] </ref> in which the data is divided into two parts one is used to fit the relation and the other is used to evaluate the goodness of fit. However such procedures are very computationally intensive and highly sensitive to outliers, and results on real images are poor.
Reference: 11. <author> O.D. Faugeras. </author> <title> What can be seen in three dimensions with an uncalibrated stereo rig? In G. </title> <editor> Sandini, editor, </editor> <booktitle> Proc. 2nd European Conference on Computer Vision, LNCS 588, </booktitle> <address> Santa Margherita Ligure, </address> <pages> pages 563-578. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: This paper reviews current statistical methods in model selection with respect to determining the two view geometric relations from the point matches between two images of a scene, e.g. the fundamental matrix <ref> [11, 17] </ref>. These relations can be used to guide matching [47, 53] and thence structure [3] or segmentation [46]. There are several two view relations that could describe an image pair. Hence it is necessary to estimate the type of model as well as the parameters of the model. <p> The two sets of features are related by an implicit functional relationship x 0&gt; i Fx i = 0 where F is the rank 2, 3fi3 <ref> [11, 17] </ref> fundamental matrix, this is relation R 1 . The fundamental matrix encapsulates the epipolar geometry. It contains all the information on camera motion and internal camera parameters available from image feature correspondences alone.
Reference: 12. <author> R. A. Fisher. </author> <title> Uncertain inference. </title> <journal> Proc. Amer. Acad. Arts and Sciences, </journal> <volume> 71 </volume> <pages> 245-258, </pages> <year> 1936. </year>
Reference-contexts: If the type of relation R is known then, observing the data, we can estimate the parameters of R to minimize this log likelihood. This inference is called `Maximum Likelihood Estimation' (Fisher 1936 <ref> [12] </ref>). Numerical methods for finding these two and three view relations are given in [4]. Robust Estimation The above derivation assumes that the errors are Gaussian, often however features are mismatched and the error on m is not Gaussian. <p> Fisher was aware of the limitations of maximum likelihood estimation and admits the possibility of a wider form of inductive argument that would determine the functional form of the data (Fisher 1936 p. 250 <ref> [12] </ref>); but then goes on to state `At present it is only important to make clear that no such theory has been established'. Estimated Point Motion General Orthographic Rotation Fundamental 114 92 92 Affine 399 96 143 Homography 493 252 195 Table 2.
Reference: 13. <author> A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. </author> <title> Bayesian Data Analysis. </title> <publisher> Chapman & Hall, </publisher> <address> New York, </address> <year> 1995. </year>
Reference-contexts: By assigning Gaussians probability distributions to these with meaningful parameters monte carlo sampling methods <ref> [13] </ref> can be used to generate the prior for each R, and hence the Bayes factor. It is then possible to use (23) directly to calculate the Bayes factor. Kass and Raftery [24] review a large number of such Monte Carlo style techniques for estimating Bayes factors.
Reference: 14. <author> J. P. Hampel, E. M. Ronchetti, P. J. Rousseeuw, and W. A. Stahel. </author> <title> Robust Statistics: An Approach Based on Influence Functions. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: Usually the model selection methods are derived under Gaussian assumptions, but this assumption is not necessary, and the methods are equally valid using the robust function for probability given above (e.g. see Ronchetti in <ref> [14] </ref> or [36]). Thus in all that follows the robust likelihood is used.
Reference: 15. <author> C. Harris. </author> <title> Resolution of the bas-relief ambiguity in structure-from-motion under orthographic projection. </title> <address> pages 67-72. BMVA, </address> <year> 1990. </year>
Reference-contexts: Buggy. The left two images Figures 3 two views of a buggy rotating on a turn table, good orthographic but not perspective structure can be generated for this scene (attempts to generate full perspective become dogged by the bas-relief ambiguity <ref> [15, 44] </ref>). Hence the correct (or ground truth model) should be F A . Model house data. The right two images Figure 3 show a scene in which a camera rotates and translates whilst fixating on a model house.
Reference: 16. <author> C. Harris. </author> <title> Structure-from-motion under orthographic projection. </title> <editor> In O. Faugeras, editor, </editor> <booktitle> Proc. 1st European Conference on Computer Vision, </booktitle> <volume> LNCS 427, </volume> <pages> pages 118-128. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: Real Data. The algorithms have been tested on many images, here two typical image pairs are shown. In all the examples tested for this paper, the corners are obtained by using the detector described in <ref> [16] </ref>, the matching procedure uses cross correlation in a square search window. The standard deviation of the error of the point correspondences was estimated robustly [50].
Reference: 17. <author> R. I. </author> <title> Hartley. Estimation of relative camera positions for uncalibrated cameras. </title> <editor> In G. Sandini, editor, </editor> <booktitle> Proc. 2nd European Conference on Computer Vision, LNCS 588, </booktitle> <address> Santa Margherita Ligure, </address> <pages> pages 579-87. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: This paper reviews current statistical methods in model selection with respect to determining the two view geometric relations from the point matches between two images of a scene, e.g. the fundamental matrix <ref> [11, 17] </ref>. These relations can be used to guide matching [47, 53] and thence structure [3] or segmentation [46]. There are several two view relations that could describe an image pair. Hence it is necessary to estimate the type of model as well as the parameters of the model. <p> The two sets of features are related by an implicit functional relationship x 0&gt; i Fx i = 0 where F is the rank 2, 3fi3 <ref> [11, 17] </ref> fundamental matrix, this is relation R 1 . The fundamental matrix encapsulates the epipolar geometry. It contains all the information on camera motion and internal camera parameters available from image feature correspondences alone.
Reference: 18. <author> R. I. Hartley and P. Sturm. </author> <title> Triangulation. </title> <booktitle> In American Image Understanding Workshop, </booktitle> <pages> pages 957-966, </pages> <year> 1994. </year>
Reference-contexts: ^x i x i = j ^ j = i X e 2 where the log likelihood of a given match is l (m; R) = e 2 i = j ^x i x i discounting the constant terms (which is equivalent to the reprojection error of Hartley and Sturm <ref> [18] </ref>). If the type of relation R is known then, observing the data, we can estimate the parameters of R to minimize this log likelihood. This inference is called `Maximum Likelihood Estimation' (Fisher 1936 [12]). Numerical methods for finding these two and three view relations are given in [4].
Reference: 19. <author> U. Hjorth. </author> <title> On model selection in the computer age. </title> <journal> J. Statist Planng Inf, </journal> <volume> 23 </volume> <pages> 101-115, </pages> <year> 1989. </year>
Reference-contexts: Selection of just one model means that the uncertainty on some parameters is underestimated <ref> [19, 32] </ref> as will now be demonstrated. Hjorth distinguishes between global parameters which are defined for all models and local parameters which are not. Consider figure 2 depicting a two dimensional slice of parameter space for two global parameters. <p> Furthermore note that there is no chance for model 1 to take values of the parameters within the regions that AIC allocates to model 2 or 3 for this data set; even if that is the correct answer. Hjorth <ref> [19] </ref> gives a rather neglected bias theorem which is intuitively obvious: that the expected value of the MAIC will be less than the minimum of the expected value of the AIC for all the models under consideration. <p> Model averaging produced little improvement together with increasing the amount of computation necessary to estimate likelihoods. 10 Discussion Another popular class of methods for model selection include cross validation, (In the asymptotic case Stone [43] demonstrates that AIC and cross validation are equivalent.) jackknifing, bootstrap, and data splitting <ref> [10, 19] </ref> in which the data is divided into two parts one is used to fit the relation and the other is used to evaluate the goodness of fit. However such procedures are very computationally intensive and highly sensitive to outliers, and results on real images are poor.
Reference: 20. <author> J. S. Hodges. </author> <title> Uncertainty, policy analysis and statistics (with discussion). </title> <journal> Statistical Science, </journal> <volume> 2 </volume> <pages> 259-291, </pages> <year> 1987. </year>
Reference-contexts: Fig. 2. Two cases, X where the uncertainty in the model is unimportant, Y where the uncertainty in the model becomes important Bayesian model averaging takes into account explicitly model uncertainty by representing the data|each set of matches by a combination of models <ref> [9, 20, 24, 27, 33] </ref>. Rather than using one motion model a standard Bayesian formalism is adopted which averages the posterior distributions of the prediction under each model, weighted by their posterior model probabilities.
Reference: 21. <author> E.T. Jaynes. </author> <title> Where do we stand on maximum entropy inference. </title> <editor> In R.D. Levine and M. Tribus, editors, </editor> <booktitle> The Maximum Entropy Formalism. </booktitle> <year> 1978. </year>
Reference-contexts: observations tends to infinity 4 He later demonstrated that AIC was an estimate of the expected entropy (Kullback-Leibler information), showing that the model with the minimum AIC score also minimized the expected entropy, thus providing one way of generalizing MLE; which is linked to the Bayesian method of maximum entropy <ref> [21] </ref> from here on.
Reference: 22. <author> H. Jeffreys. </author> <title> Theory of Probability. </title> <publisher> Clarendon Press, Oxford, </publisher> <address> third edition, </address> <year> 1961. </year>
Reference-contexts: data supports R i over R j is measured by the pos terior odds, B ij = (R j j) Pr (jR i ; i ) Pr (R i ) (18) which is called a Bayes Factor, the term originated by Good the method he attributes to Turing and Jeffreys <ref> [22] </ref>. It is similar to the likelihood ratio for model comparison but involves integration rather than comparing maxima. The first term on the right hand side of (18) is the ratio of two integrals given in (17).
Reference: 23. <author> K. Kanatani. </author> <title> Statistical Optimization for Geometric Computation: Theory and Practice. </title> <publisher> Elsevier Science, </publisher> <address> Amsterdam, </address> <year> 1996. </year>
Reference-contexts: this paper it is assumed that the noise in the location of features in all the images is Gaussian on each image coordinate with zero mean and uniform standard deviation , thus fl = I 2 (extension to the more general case is not difficult and is described by Kanatani <ref> [23] </ref>). <p> In Table 2 the average sum of square of this error (SSE) (MLE is described in <ref> [23] </ref>) are shown for 100 sets of 100 synthetic matches. The matches were generated to be consistent with random F, F A or H constraints, i.e. with a general motion, orthographic projection or a camera rotation.
Reference: 24. <author> R. E. Kass and A. E. Raftery. </author> <title> Bayes factors. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 90 </volume> <pages> 733-795, </pages> <year> 1995. </year>
Reference-contexts: This would only occur with a very large baseline and very large perspective effects. The problem comes by the approximation of the prior distribution .e. the approximation of 1 2 jHj by p 2 log n, which is generally a bad approximation if N &lt; 5p <ref> [24] </ref>. Other Ways to Approximate Bayes Factors The crux of the problem with Bayes factors is the choice of prior Pr ( k jR k ) as this may influence the result, ideally the Bayes factors should be evaluated over a range of priors to check their stability. <p> Fig. 2. Two cases, X where the uncertainty in the model is unimportant, Y where the uncertainty in the model becomes important Bayesian model averaging takes into account explicitly model uncertainty by representing the data|each set of matches by a combination of models <ref> [9, 20, 24, 27, 33] </ref>. Rather than using one motion model a standard Bayesian formalism is adopted which averages the posterior distributions of the prediction under each model, weighted by their posterior model probabilities. <p> By assigning Gaussians probability distributions to these with meaningful parameters monte carlo sampling methods [13] can be used to generate the prior for each R, and hence the Bayes factor. It is then possible to use (23) directly to calculate the Bayes factor. Kass and Raftery <ref> [24] </ref> review a large number of such Monte Carlo style techniques for estimating Bayes factors. Another interesting question is whether it is necessary to calculate all the models prior to model selection? All the approaches advocated involve estimation of all putative models which is costly in computation time.
Reference: 25. <author> M. Kendall and A. Stuart. </author> <title> The Advanced Theory of Statistics. </title> <publisher> Charles Griffin and Company, </publisher> <address> London, </address> <year> 1983. </year>
Reference-contexts: To accomplish this Lagrange multipliers are used, the derivatives of L = nI log ^ 2 + 2^ 2 ( i ^ i ) &gt; fl 1 with respect to 2 ^ ; ^ ; ^ 2 <ref> [25] </ref> are equated to zero. The solution of this set of equations are the maximum likelihood estimates of the noise , the parameters for the relation, and the best fitting correspondences .
Reference: 26. <author> A.N. </author> <title> Kolmogorov. Three approaches to the quantitative definition of information. </title> <journal> Problems of Information Transmission, </journal> <volume> 1 </volume> <pages> 4-7, </pages> <year> 1965. </year>
Reference-contexts: II,p.398) Rissanen [37] (1978) developed a criterion with a similar form to the BIC from a totally different standpoint. He derived the minimum-bit representation of the data, termed SSD|shortest description length, and MDL|minimum description length|an approach suggested by the idea of Algorithmic Complexity (Solomonoff [42] and Kolmogorov <ref> [26] </ref>). Wallace and Boulton [51] developed a very similar idea to MDL called the minimum message length (MML) approach.
Reference: 27. <author> E. E. Leamer. </author> <title> Specification searches: ad hoc inference with nonexperimental data. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: Fig. 2. Two cases, X where the uncertainty in the model is unimportant, Y where the uncertainty in the model becomes important Bayesian model averaging takes into account explicitly model uncertainty by representing the data|each set of matches by a combination of models <ref> [9, 20, 24, 27, 33] </ref>. Rather than using one motion model a standard Bayesian formalism is adopted which averages the posterior distributions of the prediction under each model, weighted by their posterior model probabilities. <p> Then Bayesian inference about m i is based its posterior distribution, which is Pr (m i j) = i=1 by the law of total probability <ref> [27] </ref>. Thus the full posterior probability of m i is a weighted average of its posterior distribution under each of the models, where the weights are the posterior model probabilities, Pr (R k j) derived in (26).
Reference: 28. <author> I. J. Leontaritis and S. A. Billings. </author> <title> Model selection and validation methods for non-linear systems. </title> <journal> INT J CONTROL, </journal> <volume> 45(1) </volume> <pages> 311-341, </pages> <year> 1987. </year>
Reference-contexts: A common way to do this is by the likelihood ratio test (e.g. see <ref> [28, 31] </ref>). To do this the MLE of the parameters ^ 1 and ^ 2 for both relations must be recovered.
Reference: 29. <author> M. Li and P. Vitanyi. </author> <title> An introduction to Kolmogorov complexity and its applications. </title> <publisher> Springer-Verlag, </publisher> <year> 1997. </year>
Reference-contexts: If more than one theory is consistent with the observations, keep all theories" <ref> [29] </ref>. Thus as pointed out by his follower Lucretius (95-55 B.C.) if there are several explanations as to why a man died, but one does not definitively know which one is true, then Lucretius (and later Solomonoff [29]) advocates maintaining them all for the purpose of prediction (somewhat the opposite of <p> If more than one theory is consistent with the observations, keep all theories" <ref> [29] </ref>. Thus as pointed out by his follower Lucretius (95-55 B.C.) if there are several explanations as to why a man died, but one does not definitively know which one is true, then Lucretius (and later Solomonoff [29]) advocates maintaining them all for the purpose of prediction (somewhat the opposite of Ockham's point of view).
Reference: 30. <author> D. Madigan and A. E. Raftery. </author> <title> Model selection and accounting for model uncertainty in graphical models using Occam's window. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 89 </volume> <pages> 1535-1546, </pages> <year> 1994. </year>
Reference-contexts: Then its likelihood is computed as a weighted average over all the models. Indeed it has been shown that model averaging can produce superior results in predictive performance than commitment to a single model <ref> [30] </ref>. Suppose that there are K competing motion models with relations R 1 : : : R K with parameter vectors 1 : : : K , that could describe C j .
Reference: 31. <author> G.I. McLachlan and K. Basford. </author> <title> Mixture models: inference and applications to clustering. </title> <publisher> Marcel Dekker. </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: A common way to do this is by the likelihood ratio test (e.g. see <ref> [28, 31] </ref>). To do this the MLE of the parameters ^ 1 and ^ 2 for both relations must be recovered.
Reference: 32. <author> A. J. Miller. </author> <title> Selection of subsets of regression variables (with discussion). </title> <journal> Journal of the Royal Statistical Society (Series A), </journal> <volume> 147 </volume> <pages> 389-425, </pages> <year> 1984. </year>
Reference-contexts: Selection of just one model means that the uncertainty on some parameters is underestimated <ref> [19, 32] </ref> as will now be demonstrated. Hjorth distinguishes between global parameters which are defined for all models and local parameters which are not. Consider figure 2 depicting a two dimensional slice of parameter space for two global parameters.
Reference: 33. <author> B. R. </author> <title> Moulton. A Bayesian-approach to regression selection and estimation with application to a price-index for radio services. </title> <journal> Journal of Econometrics, </journal> <volume> 49 </volume> <pages> 169-193, </pages> <year> 1991. </year>
Reference-contexts: Fig. 2. Two cases, X where the uncertainty in the model is unimportant, Y where the uncertainty in the model becomes important Bayesian model averaging takes into account explicitly model uncertainty by representing the data|each set of matches by a combination of models <ref> [9, 20, 24, 27, 33] </ref>. Rather than using one motion model a standard Bayesian formalism is adopted which averages the posterior distributions of the prediction under each model, weighted by their posterior model probabilities.
Reference: 34. <author> J. Mundy and A. Zisserman. </author> <title> Geometric Invariance in Computer Vision. </title> <publisher> MIT press, </publisher> <year> 1992. </year>
Reference-contexts: For instance when the camera is only rotating about its optic centre. Three other models are con-sidered: R 2 , which is the affine camera model of Mundy & Zisserman <ref> [34] </ref> with linear fundamental matrix F A ; R 3 , which is image projectivities; and R 4 , which is affinities as induced by planar homographies.
Reference: 35. <author> J. Pearl. </author> <title> On the connection between the complexity and credibility of inferred models. </title> <journal> Int. J. General Systems, </journal> <volume> 4 </volume> <pages> 255-264, </pages> <year> 1978. </year>
Reference-contexts: Generally model selection biases are hard to quantify, but are characterized by deflation of covariance estimates. The type of bias will very much depend on the algorithm used to perform the model selection (as well as the criterion of model selection). To quote Pearl <ref> [35] </ref> "It would, therefore, be more appropriate to connect credibility with the nature of the selection procedure rather than the properties of the final product. When the former is not explicitly known...simplicity merely serves as a rough indicator for the type of processing that took place prior to the discovery".
Reference: 36. <author> B. D. Ripley. </author> <title> Pattern recognition and neural networks. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1996. </year>
Reference-contexts: Usually the model selection methods are derived under Gaussian assumptions, but this assumption is not necessary, and the methods are equally valid using the robust function for probability given above (e.g. see Ronchetti in [14] or <ref> [36] </ref>). Thus in all that follows the robust likelihood is used. Problems with MLE: If the type of relation R is unknown then we cannot use maximum likelihood estimation to decide the form of R as the most general model will always be most likely i.e. have lowest L.
Reference: 37. <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: To this purpose the philosophers say that Nature does nothing in vain, and more is in vain when less will serve; for Nature is pleased with simplicity; and affects not the pomp of superfluous causes." (Newton Principia 1726,vol. II,p.398) Rissanen <ref> [37] </ref> (1978) developed a criterion with a similar form to the BIC from a totally different standpoint. He derived the minimum-bit representation of the data, termed SSD|shortest description length, and MDL|minimum description length|an approach suggested by the idea of Algorithmic Complexity (Solomonoff [42] and Kolmogorov [26]).
Reference: 38. <author> P. J. Rousseeuw. </author> <title> Robust Regression and Outlier Detection. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Another interesting question is whether it is necessary to calculate all the models prior to model selection? All the approaches advocated involve estimation of all putative models which is costly in computation time. I have experimented with the use of covariance matrices (non-robust and robust <ref> [38] </ref>) in the hope that by fitting the most general model the covariance matrix might reveal if there are degeneracies. This works reasonably well if there are few outliers in the data but poorly otherwise.
Reference: 39. <author> B. Russell. </author> <title> History of Western Philosophy. </title> <publisher> Routledge, </publisher> <year> 1961. </year>
Reference-contexts: That is to say, if everything in some science (here computer vision) can be interpreted without assuming this or that entity, there is no ground for assuming it <ref> [39] </ref>. In addition to the degrees of freedom in the parameters we shall see that the complexity of a model is also determined by its dimension, which is defined now.
Reference: 40. <author> G. Schwarz. </author> <title> Estimating dimension of a model. </title> <journal> Ann. Stat., </journal> <volume> 6 </volume> <pages> 461-464, </pages> <year> 1978. </year>
Reference-contexts: covariance ^ V leading to Pr ( ^ k ) = (2) p=2 j ^ V 2 j exp ( 2 1 thus log (Pr (R k j)) L 2 1 1 log j ^ k j + 2 If the prior is assumed to be very diffuse then Schwarz <ref> [40] </ref> suggests discounting the second term and approximating the Hessian by 1 2 jHj p log (R k j) log (j ^ k ) 2 where p = dn+ k is the total number of parameters in the system, and N = 4n is the total number of observations, n the
Reference: 41. <author> G.A.F. Wild C. J. Seber. </author> <title> Non-Linear Regression. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: Although this is a standard result the derivation will reveal that there are more parameters to be considered in the model formulation than just the explicit parameters of given in the last section and Table 1. These additional parameters are sometimes referred to as nuisance parameters <ref> [41] </ref>. This is important as later it will be seen that the prior distribution of R is related to the number of parameters that need to be estimated.
Reference: 42. <author> R. Solomonoff. </author> <title> A formal theory of inductive inference i. </title> <journal> Information and Control, </journal> <volume> 7 </volume> <pages> 1-22, </pages> <year> 1964. </year>
Reference-contexts: II,p.398) Rissanen [37] (1978) developed a criterion with a similar form to the BIC from a totally different standpoint. He derived the minimum-bit representation of the data, termed SSD|shortest description length, and MDL|minimum description length|an approach suggested by the idea of Algorithmic Complexity (Solomonoff <ref> [42] </ref> and Kolmogorov [26]). Wallace and Boulton [51] developed a very similar idea to MDL called the minimum message length (MML) approach.
Reference: 43. <author> M. Stone. </author> <title> An asymptotic equivalence of choice of model by cross-validation and Akaike's criterion. </title> <journal> J. Roy. Statist. Soc. B, </journal> <volume> 39 </volume> <pages> 44-47, </pages> <year> 1977. </year>
Reference-contexts: Model averaging produced little improvement together with increasing the amount of computation necessary to estimate likelihoods. 10 Discussion Another popular class of methods for model selection include cross validation, (In the asymptotic case Stone <ref> [43] </ref> demonstrates that AIC and cross validation are equivalent.) jackknifing, bootstrap, and data splitting [10, 19] in which the data is divided into two parts one is used to fit the relation and the other is used to evaluate the goodness of fit.
Reference: 44. <author> R. Szeliski and S. B. Kang. </author> <title> Shape ambiguities in structure from motion. </title> <editor> In B. Buxton and R. Cipolla, editors, </editor> <booktitle> Proc. 4th European Conference on Computer Vision, </booktitle> <publisher> LNCS 1064, Cambridge, </publisher> <pages> pages 709-721. </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: Buggy. The left two images Figures 3 two views of a buggy rotating on a turn table, good orthographic but not perspective structure can be generated for this scene (attempts to generate full perspective become dogged by the bas-relief ambiguity <ref> [15, 44] </ref>). Hence the correct (or ground truth model) should be F A . Model house data. The right two images Figure 3 show a scene in which a camera rotates and translates whilst fixating on a model house.
Reference: 45. <author> P. H. S. Torr. </author> <title> Outlier Detection and Motion Segmentation. </title> <type> PhD thesis, </type> <institution> Dept. of Engineering Science, University of Oxford, </institution> <year> 1995. </year>
Reference-contexts: Note however that just because the set of image correspondences conforms to F A it does not necessarily follow that the imaging conditions are affine (see <ref> [45] </ref>). Determination of when this is the case is left to future work.
Reference: 46. <author> P. H. S. Torr. </author> <title> Geometric motion segmentation and model selection. </title> <editor> In J. Lasenby, A. Zisserman, R. Cipolla, and H. Longuet-Higgins, </editor> <title> editors, </title> <journal> Philosophical Transactions of the Royal Society A, </journal> <pages> pages 1321-1340. </pages> <publisher> Roy Soc, </publisher> <year> 1998. </year>
Reference-contexts: These relations can be used to guide matching [47, 53] and thence structure [3] or segmentation <ref> [46] </ref>. There are several two view relations that could describe an image pair. Hence it is necessary to estimate the type of model as well as the parameters of the model. The paper is laid out as follows.
Reference: 47. <author> P. H. S. Torr, A. FitzGibbon, and A. Zisserman. </author> <title> Maintaining multiple motion model hypotheses through many views to recover matching and structure. </title> <editor> In U Desai, editor, </editor> <booktitle> ICCV6, </booktitle> <pages> pages 485-492. </pages> <publisher> Narosa Publishing House, </publisher> <year> 1998. </year>
Reference-contexts: This paper reviews current statistical methods in model selection with respect to determining the two view geometric relations from the point matches between two images of a scene, e.g. the fundamental matrix [11, 17]. These relations can be used to guide matching <ref> [47, 53] </ref> and thence structure [3] or segmentation [46]. There are several two view relations that could describe an image pair. Hence it is necessary to estimate the type of model as well as the parameters of the model. The paper is laid out as follows.
Reference: 48. <author> P. H. S. Torr and D. W. Murray. </author> <title> The development and comparison of robust methods for estimating the fundamental matrix. </title> <journal> IJCV, </journal> <volume> 24(3) </volume> <pages> 271-300, </pages> <year> 1997. </year>
Reference: 49. <author> P. H. S. Torr and A. Zisserman. </author> <title> Concerning bayesian motion segmentation, model averaging, matching and the trifocal tensor. </title> <editor> In H. Burkharddt and B. Neumann, editors, </editor> <volume> ECCV98 Vol 1, </volume> <pages> pages 511-528. </pages> <publisher> Springer, </publisher> <year> 1998. </year>
Reference-contexts: As there is no model selection procedure a slightly different test criterion had to be used. The results for model averaging were assessed by examining how points were correctly classified as inlying or outlying, using the combination of models for the classification. (elsewhere model averaging is assessed for segmentation <ref> [49] </ref>) Overall the results were disappointing with there being very little difference between the model averaging approach and using the model suggested by model selection procedure used to generate the posterior distribution over the models (AIC and BIC were tried). 9 Results All of the model selection procedures have been implemented
Reference: 50. <author> P. H. S. Torr, A Zisserman, and S. Maybank. </author> <title> Robust detection of degenerate configurations for the fundamental matrix. </title> <note> To Appear CVIU, </note> <year> 1998. </year>
Reference-contexts: In all the examples tested for this paper, the corners are obtained by using the detector described in [16], the matching procedure uses cross correlation in a square search window. The standard deviation of the error of the point correspondences was estimated robustly <ref> [50] </ref>.
Reference: 51. <author> C.S. Wallace and D.M. Boulton. </author> <title> An information measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11(2) </volume> <pages> 195-209, </pages> <year> 1968. </year>
Reference-contexts: He derived the minimum-bit representation of the data, termed SSD|shortest description length, and MDL|minimum description length|an approach suggested by the idea of Algorithmic Complexity (Solomonoff [42] and Kolmogorov [26]). Wallace and Boulton <ref> [51] </ref> developed a very similar idea to MDL called the minimum message length (MML) approach. The criteria MDL and MML are based on minimum code lengths|given the data represented up to a finite precision, one picks the parameters so that the model they define permits the shortest possible code length.
Reference: 52. <author> C.S. Wallace and P.R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> J. R. Statist. Soc B, </journal> <volume> 49(3) </volume> <pages> 240-265, </pages> <year> 1987. </year>
Reference-contexts: However the term derived MDL = log (j ^ k ) 2 has the same form and deficiencies as the BIC. Yet this is only a first order approximation to the optimal code length. Wallace & Freeman <ref> [52] </ref> (1987) further develop/expand MML leading to a very similar criterion to Bozdogan's CAICF. Test results using MDL.
Reference: 53. <author> Z. Zhang. </author> <title> Determining the epipolar geometry and its uncertainty: A review. </title> <journal> IJCV, </journal> <volume> 27(2) </volume> <pages> 161-195, </pages> <year> 1997. </year>
Reference-contexts: This paper reviews current statistical methods in model selection with respect to determining the two view geometric relations from the point matches between two images of a scene, e.g. the fundamental matrix [11, 17]. These relations can be used to guide matching <ref> [47, 53] </ref> and thence structure [3] or segmentation [46]. There are several two view relations that could describe an image pair. Hence it is necessary to estimate the type of model as well as the parameters of the model. The paper is laid out as follows.
References-found: 53

