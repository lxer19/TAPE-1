URL: ftp://ftp.cs.umass.edu/pub/anw/pub/sutton/sutton-97.ps
Refering-URL: http://www-anw.cs.umass.edu/~rich/publications.html
Root-URL: 
Title: On the Significance of Markov Decision Processes  
Author: Richard S. Sutton 
Web: http://www.cs.umass.edu/~rich  
Address: Amherst, MA USA  
Affiliation: Department of Computer Science University of Massachusetts,  
Abstract: Formulating the problem facing an intelligent agent as a Markov decision process (MDP) is increasingly common in artificial intelligence, reinforcement learning, artificial life, and artificial neural networks. In this short paper we examine some of the reasons for the appeal of this framework. Foremost among these are its generality, simplicity, and emphasis on goal-directed interaction between the agent and its environment. MDPs may be becoming a common focal point for different approaches to understanding the mind. Finally, we speculate that this focus may be an enduring one insofar as many of the efforts to extend the MDP framework end up bringing a wider class of problems back within it. Sometimes the establishment of a problem is a major step in the development of a field, more important than discovery of solution methods. For example, the problem of supervised learning has played a central role as it has developed through pattern recognition, statistics, machine learning and artificial neural networks. Regulation of linear systems has practically defined the field of control theory for decades. To understand what has happened in these and other fields it is essential to track the origins, development, and range of acceptance of particular problem classes. Major points of change are marked sometimes by a new solution to an existing problem, but just as often by the promulgation and recognition of the significance of a new problem. Now may be one such time of transition in the study of mental processes, with Markov decision processes being the newly accepted problem. Markov decision processes (MDPs) originated in the study of stochastic optimal control (Bellman, 1957) and have remained the key problem in that area ever since. In the 1980s and 1990s, incompletely known MDPs were gradually recognized as a natural problem formulation for reinforcement learning (e.g., Witten, 1977; Watkins, 1989; Sutton and Barto, 1998). Recognizing the common problem led to the discovery of a wealth of common algorithmic ideas and theoretical analyses. MDPs have also come to be widely studied within AI as a new, particularly suitable kind of planning problem, e.g., as in decision-theoretic planning (e.g., Dean et al., 1995), and in conjunction with structured Bayes nets (e.g., Boutilier et al., 1995). In robotics, artificial life, and evolutionary methods it is less common to use the language and mathematics of MDPs, but again the problems considered are well expressed in MDP terms. Recognition of this 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Bradtke, S. J., and Singh, S. P. </author> <year> (1995). </year> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72 </volume> <pages> 81-138. </pages>
Reference: <author> Barto, A. G., Sutton, R. S., and Watkins, C. J. C. H. </author> <year> (1990). </year> <title> Learning and sequential decision making. </title> <editor> In Gabriel, M. and Moore, J., editors, </editor> <booktitle> Learning and Computational Neuroscience: Foundations of Adaptive Networks, </booktitle> <pages> pages 539-602. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Bellman, R. E. </author> <year> (1957). </year> <title> A Markov decision process. </title> <journal> Journal of Mathematical Mech., </journal> <volume> 6 </volume> <pages> 679-684. </pages>
Reference: <author> Boutilier, C., Dearden, R., and Goldszmidt, M. </author> <year> (1995). </year> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence. </booktitle>
Reference: <author> Crites, R. H. and Barto, A. G. </author> <year> (1996). </year> <title> Improving elevator performance using reinforcement learning. </title> <editor> In D. S. Touretzky, M. C. Mozer, M. E. H., editor, </editor> <booktitle> Advances in Neural Information Processing Systems: Proceedings of the 1995 Conference, </booktitle> <pages> pages 1017-1023, </pages> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Dean, T. L., Kaelbling, L. P., Kirman, J., and Nicholson, A. </author> <year> (1995). </year> <title> Planning under time constraints in stochastic domains. </title> <journal> Artificial Intelligence, 76(1-2):35-74. </journal>
Reference-contexts: For example, AI planning research using MDPs may consider much larger and more richly structured state spaces. Other important extensions are approximation methods, for example, anytime planning issues <ref> (Dean et al., 1995) </ref>, incomplete dynamics knowledge (as in reinforcement learning), and sampling methods (e.g., Tesauro and Galperin, 1997).
Reference: <author> Houk, J. C., Adams, J. L., and Barto, A. G. </author> <year> (1995). </year> <title> A model of how the basal ganglia generates and uses neural signals that predict reinforcement. </title> <editor> In Houk, J. C., Davis, J. L., and Beiser, D. G., editors, </editor> <booktitle> Models of Information Processing in the Basal Ganglia, </booktitle> <pages> pages 249-270. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> McCallum, A. K. </author> <year> (1995). </year> <title> Reinforcement Learning with Selective Perception and Hidden State. </title> <type> PhD thesis, </type> <institution> University of Rochester, Rochester. </institution>
Reference-contexts: Others have proposed simply adding memories of earlier observations to the state representation <ref> (e.g., McCallum, 1995) </ref> and then proceeding with the same methods as approximations. In some cases, convergence results can still be obtained for such ad hoc approaches (e.g., Singh et al., 1994, 1995).
Reference: <author> Precup, D. and Sutton, R. S. </author> <title> (in preparation). Multi-time models for temporally abstract planning. </title>
Reference: <author> Santamaria, J. C., Sutton, R. S., and Ram, A. </author> <year> (1996). </year> <title> Experiments with reinforcement learning in problems with continuous state and action spaces. </title> <type> Technical Report UM-CS-1996-088, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA 01003. </address>
Reference: <author> Schultz, W., Dayan, P., and Montague, P. R. </author> <year> (1997). </year> <title> A neural substrate of prediction and reward. </title> <journal> Science, </journal> <volume> 275 </volume> <pages> 1593-1598. </pages>
Reference: <author> Singh, S. P. </author> <year> (1992). </year> <title> Transfer of learning by composing solutions of elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 323-339. </pages>
Reference: <author> Singh, S. P., Jaakkola, T., and Jordan, M. I. </author> <year> (1994). </year> <title> Learning without state-estimation in partially observable Markovian decision problems. </title> <editor> In Cohen, W. W. and Hirsch, H., editors, </editor> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 284-292, </pages> <address> San Francisco, CA. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: Others have proposed simply adding memories of earlier observations to the state representation (e.g., McCallum, 1995) and then proceeding with the same methods as approximations. In some cases, convergence results can still be obtained for such ad hoc approaches <ref> (e.g., Singh et al., 1994, 1995) </ref>. Another important but simple extension is that from finite MDPs to general MDPs with continuous state and action variables. In this case, the state space can be arbitrarily large, even infinite.
Reference: <author> Singh, S. P., Jaakkola, T., and Jordan, M. I. </author> <year> (1995). </year> <title> Reinforcement learing with soft state aggregation. </title> <editor> In G. Tesauro, D. Touretzky, T. L., editor, </editor> <booktitle> Advances in Neural Information Processing Systems: Proceedings of the 1994 Conference, </booktitle> <pages> pages 359-368, </pages> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1995). </year> <title> TD models: Modeling the world at a mixture of time scales. </title> <editor> In Prieditis, A. and Russell, S., editors, </editor> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 531-539, </pages> <address> San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1996). </year> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems: Proceedings of the 1995 Conference, </booktitle> <pages> pages 1038-1044, </pages> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Sutton, R. S. and Barto, A. G. </author> <year> (1990). </year> <title> Time-derivative models of Pavlovian reinforcement. </title> <editor> In Gabriel, M. and Moore, J., editors, </editor> <booktitle> Learning and Computational Neuroscience: Foundations of Adaptive Networks, </booktitle> <pages> pages 497-537. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Sutton, R. S. and Barto, A. G. </author> <year> (1998). </year> <title> Introduction to Reinforcement Learning. </title> <publisher> MIT Press/Bradford Books, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The results of planning accumulate in the improving policy function and, even more so, in the approximate value function. Fig. 2. In generalized policy iteration, a general schema for solving MDPs, approximate policy and value functions continually interact until settling at their optimal values. In our recent book <ref> (Sutton and Barto, 1998) </ref>, Andy Barto and I suggest that a wide range of methods for solving MDPs can be understood as a recursive ratcheting interplay between the approximate policy and value functions.
Reference: <author> Tesauro, G. J. </author> <year> (1995). </year> <title> Temporal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <volume> 38 </volume> <pages> 58-68. </pages>
Reference: <author> Tesauro, G. J. and Galperin, G. R. </author> <year> (1997). </year> <title> On-line policy improvement using monte-carlo search. </title> <booktitle> In Advances in Neural Information Processing Systems: Proceedings of the 1996 Conference, </booktitle> <address> Cambridge, MA. </address> <publisher> MIT Press. </publisher>
Reference-contexts: For example, AI planning research using MDPs may consider much larger and more richly structured state spaces. Other important extensions are approximation methods, for example, anytime planning issues (Dean et al., 1995), incomplete dynamics knowledge (as in reinforcement learning), and sampling methods <ref> (e.g., Tesauro and Galperin, 1997) </ref>. The most important implication of MDPs for planning methods is the importance that it suggests should be placed on the concepts of policies and value functions as long-term memory structures that accumulate planning results.
Reference: <author> Van Roy, B., Bertsekas, D. P., Lee, Y., and Tsitsiklis, J. N. </author> <year> (1996). </year> <title> A neuro-dynamic programming approach to retailer inventory management. </title> <type> Technical Report LIDS-P-?, </type> <institution> Laboratory for Information and Decision Systems, Massachusetts Institute of Technology. </institution>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge University, </institution> <address> Cambridge, England. </address>
Reference: <author> Witten, I. H. </author> <year> (1977). </year> <title> Exploring, modelling and controlling discrete sequential environments. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 9 </volume> <pages> 715-735. </pages>
References-found: 23

