URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/rapier-aaaisymp-98.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: fmecaliff,mooneyg@cs.utexas.edu  
Title: on Applying Machine Learning to Discourse Processing Relational Learning of Pattern-Match Rules for Information Extraction  
Author: Mary Elaine Califf and Raymond J. Mooney 
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas at Austin  
Note: Appears in the Proceedings of AAAI Spring Symposium  
Abstract: Information extraction is a form of shallow text processing which locates a specified set of relevant items in natural language documents. Such systems can be useful, but require domain-specific knowledge and rules, and are time-consuming and difficult to build by hand, making infomation extraction a good testbed for the application of machine learning techniques to natural language processing. This paper presents a system, Rapier, that takes pairs of documents and filled templates and induces pattern-match rules that directly extract fillers for the slots in the template. The learning algorithm incorporates techniques from several inductive logic programming systems and learns unbounded patterns that include constraints on the words and part-of-speech tags surrounding the filler. Encouraging results are presented on learning to extract information from computer job post-ings from the newsgroup misc.jobs.offered. 
Abstract-found: 1
Intro-found: 1
Reference: <editor> Birnbaum, L. A., and Collins, G. C., eds. </editor> <booktitle> 1991. Proceedings of the Eighth International Workshop on Machine Learning: Part VI Learning Relations. </booktitle>
Reference-contexts: Relational Learning Most empirical natural-language research has employed statistical techniques that base decisions on very limited contexts, or symbolic techniques such as decision trees that require the developer to specify a manageable, finite set of features for use in making decisions. Inductive logic programming and other relational learning methods <ref> (Birnbaum & Collins 1991) </ref> allow induction over structured examples that can include first-order logical predicates and functions and unbounded data structures such as lists, strings, and trees.
Reference: <author> Brill, E. </author> <year> 1994. </year> <title> Some advances in rule-based part of speech tagging. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> 722-727. </pages>
Reference-contexts: Figure 2 shows a rule created by hand that extracts the area filler from the example document in figure reftemplate. This rule assumes that the document has been tagged with the POS tagger of <ref> (Brill 1994) </ref>. The Learning Algorithm As noted above, Rapier is inspired by ILP methods, and primarily consists of a specific to general (bottom-up) search. First, for each slot, most-specific patterns are created for each example, specifying word and tag for the filler and its complete context.
Reference: <author> Cohen, W. W. </author> <year> 1995. </year> <title> Text categorization and relational learning. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> 124-132. </pages> <address> San Francisco, CA: </address> <publisher> Morgan Kaufman. DARPA., </publisher> <editor> ed. </editor> <booktitle> 1992. Proceedings of the Fourth DARPA Message Understanding Evaluation and Conference. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman. DARPA., </publisher> <editor> ed. </editor> <booktitle> 1993. Proceedings of the Fifth DARPA Message Understanding Evaluation and Conference. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Detailed experimental comparisons of ILP and feature-based induction have demonstrated the advantages of relational representations in two language related tasks, text categorization <ref> (Cohen 1995) </ref> and generating the past tense of an English verb (Mooney & Califf 1995). While Rapier is not strictly an ILP system, its relational learning algorithm was inspired by ideas from the following ILP systems.
Reference: <author> Huffman, S. B. </author> <year> 1996. </year> <title> Learning information extraction patterns from examples. </title> <editor> In Wermter, S.; Riloff, E.; and Scheler, G., eds., </editor> <title> Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing. </title> <publisher> Berlin: Springer. </publisher> <pages> 246-260. </pages>
Reference: <author> Kim, J.-T., and Moldovan, D. I. </author> <year> 1995. </year> <title> Acquisition of linguistic patterns for knowledge-based information extraction. </title> <journal> IEEE Transactions on Knowledge and DataEngineering 7(5) </journal> <pages> 713-724. </pages>
Reference-contexts: AutoSlog creates a dictionary of extraction patterns by specializing a set of general syntactic patterns (Riloff 1993; 1996). It assumes that an expert will later examine the patterns it produces. Palka learns extraction patterns relying on a concept hierarchy to guide generalization and specialization <ref> (Kim & Moldovan 1995) </ref>. AutoSlog, Crystal, and Palka all rely on prior sentence analysis to identify syntactic elements and their relationships, and their output requires further processing to produce the final filled templates. Liep also learns IE patterns (Huff-man 1996).
Reference: <author> Lehnert, W., and Sundheim, B. </author> <year> 1991. </year> <title> A performance evaluation of text-analysis technologies. </title> <journal> AI Magazine 12(3) </journal> <pages> 81-94. </pages>
Reference-contexts: In recognition of their significance, IE systems have been the focus of DARPA's MUC program <ref> (Lehnert & Sund-heim 1991) </ref>. Unfortunately, IE systems, although they don't attempt full text understanding, are still difficult and time-consuming to build and the resulting systems generally contain highly domain-specific components, making them difficult to port to new domains.
Reference: <author> McCarthy, J., and Lehnert, W. </author> <year> 1995. </year> <title> Using decision trees for coreference resolution. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1050-1055. </pages>
Reference-contexts: Related Work Previous researchers have generally applied machine learning only to parts of the IE task and their systems have typically required more human interaction than just providing texts with filled templates. Resolve uses decision trees to handle coref-erence decisions for an IE system and requires annotated coreference examples <ref> (McCarthy & Lehn-ert 1995) </ref>. Crystal uses a form of clustering to create a dictionary of extraction patterns by generalizing patterns identified in the text by an expert (Soderland et al. 1995; 1996). AutoSlog creates a dictionary of extraction patterns by specializing a set of general syntactic patterns (Riloff 1993; 1996).
Reference: <author> Miller, G.; Beckwith, R.; Fellbaum, C.; Gross, D.; and Miller, K. </author> <year> 1993. </year> <title> Introduction to WordNet: An on-line lexical database. </title> <note> Available by ftp to clarity.princeton.edu. </note>
Reference-contexts: Using only a corpus of documents paired with filled templates, Rapier learns unbounded Eliza-like patterns (Weizenbaum 1966) that utilize limited syntactic information, such as the output of a part-of-speech tagger. Induced patterns can also easily incorporate semantic class information, such as that provided by WordNet <ref> (Miller et al. 1993) </ref>. The learning algorithm was inspired by several Inductive Logic Programming (ILP) systems and primarily consists of a specific-to-general (bottom-up) search for patterns that characterize slot-fillers and their surrounding context. The remainder of the paper is organized as follows. <p> The Rapier System Rule Representation Rapier's rule representation uses patterns that make use of limited syntactic and semantic information, using freely available, robust knowledge sources such as a part-of-speech tagger and a lexicon with semantic classes, such as the hypernym links in WordNet <ref> (Miller et al. 1993) </ref>. The initial implementation does not use a parser, primarily because of the difficulty of producing a robust parser for unrestricted text and because simpler patterns of the type we propose can represent useful extraction rules for at least some domains.
Reference: <author> Mooney, R. J., and Califf, M. E. </author> <year> 1995. </year> <title> Induction of first-order decision lists: Results on learning the past tense of English verbs. </title> <journal> Journal of Artificial Intelligence Research 3 </journal> <pages> 1-24. </pages>
Reference-contexts: Detailed experimental comparisons of ILP and feature-based induction have demonstrated the advantages of relational representations in two language related tasks, text categorization (Cohen 1995) and generating the past tense of an English verb <ref> (Mooney & Califf 1995) </ref>. While Rapier is not strictly an ILP system, its relational learning algorithm was inspired by ideas from the following ILP systems. Golem (Muggleton & Feng 1992) employs a bottom-up algorithm based on the construction of relative least-general generalizations, rlggs (Plotkin 1970).
Reference: <author> Muggleton, S., and Feng, C. </author> <year> 1992. </year> <title> Efficient induction of logic programs. </title> <editor> In Muggleton, S., ed., </editor> <booktitle> Inductive Logic Programming. </booktitle> <address> New York: </address> <publisher> Academic Press. </publisher> <pages> 281-297. </pages>
Reference-contexts: While Rapier is not strictly an ILP system, its relational learning algorithm was inspired by ideas from the following ILP systems. Golem <ref> (Muggleton & Feng 1992) </ref> employs a bottom-up algorithm based on the construction of relative least-general generalizations, rlggs (Plotkin 1970). The algorithm operates by randomly selecting pairs of positive examples, computing the determinate rlggs of each pair, and selecting the resulting consistent clauses with the greatest coverage of positive examples.
Reference: <author> Muggleton, S. </author> <year> 1995. </year> <title> Inverse entailment and Pro-gol. </title> <journal> New Generation Computing Journal 13 </journal> <pages> 245-286. </pages>
Reference-contexts: The search for consistent generalizations combines bottom-up methods from Golem with top-down methods from Foil (Quinlan 1990). At each step, a number of possible generalizations are considered; the one producing the greatest compaction of the theory is implemented, and the process repeats. The third system, Progol <ref> (Muggleton 1995) </ref> also combines bottom-up and top-down search. Using mode declarations provided for both the background predicates and the predicate being learned, it constructs a most specific clause for a random seed example.
Reference: <author> Plotkin, G. D. </author> <year> 1970. </year> <title> A note on inductive generalization. </title> <editor> In Meltzer, B., and Michie, D., eds., </editor> <booktitle> Machine Intelligence (Vol. 5). </booktitle> <address> New York: </address> <publisher> Elsevier North-Holland. </publisher>
Reference-contexts: While Rapier is not strictly an ILP system, its relational learning algorithm was inspired by ideas from the following ILP systems. Golem (Muggleton & Feng 1992) employs a bottom-up algorithm based on the construction of relative least-general generalizations, rlggs <ref> (Plotkin 1970) </ref>. The algorithm operates by randomly selecting pairs of positive examples, computing the determinate rlggs of each pair, and selecting the resulting consistent clauses with the greatest coverage of positive examples.
Reference: <author> Quinlan, J. </author> <year> 1990. </year> <title> Learning logical definitions from relations. </title> <booktitle> Machine Learning 5(3) </booktitle> <pages> 239-266. </pages>
Reference-contexts: Chillin (Zelle & Mooney 1994) combines bottom-up and top-down techniques. The algorithm starts with a most specific definition (the complete set of positive examples) and introduces consistent generalizations that make the definition more compact. The search for consistent generalizations combines bottom-up methods from Golem with top-down methods from Foil <ref> (Quinlan 1990) </ref>. At each step, a number of possible generalizations are considered; the one producing the greatest compaction of the theory is implemented, and the process repeats. The third system, Progol (Muggleton 1995) also combines bottom-up and top-down search. <p> We maintain a list of the best n rules created and specialize the rules under consideration by adding pieces of the generalizations of the pre- and post-filler patterns of the seed rules, working outward from the fillers. The rules are ordered using an information value metric <ref> (Quinlan 1990) </ref> weighted by the size of the rule (preferring smaller rules). When the best rule under consideration produces no negative examples, specialization ceases; that rule is added to the rule base, and all rules empirically subsumed by it are removed.
Reference: <author> Riloff, E. </author> <year> 1993. </year> <title> Automatically constructing a dictionary for information extraction tasks. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> 811-816. </pages>
Reference: <author> Riloff, E. </author> <year> 1996. </year> <title> Automatically generating extraction patterns from untagged text. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> 1044-1049. </pages>
Reference-contexts: This performance is comparable to that of Crystal on a medical domain task (Soderland et al. 1996), and better than that of AutoSlog and AutoSlog-TS on part of the MUC4 terrorism task <ref> (Riloff 1996) </ref>. It also compares favorably with the typical system performance on the MUC tasks (DARPA 1992; 1993). All of these comparisons are only general, since the tasks are different, but they do indicate that Rapier is doing relatively well.
Reference: <author> Soderland, S.; Fisher, D.; Aseltine, J.; and Lehn-ert, W. </author> <year> 1995. </year> <title> Crystal: Inducing a conceptual dictionary. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1314-1319. </pages>
Reference-contexts: IE can be useful in a variety of domains. The various MUC's have focused on domains such as Latin American terrorism, joint ventures, microelectronics, and company management changes. Others have used IE to track medical patient records <ref> (Soderland et al. 1995) </ref> or company mergers (Huff-man 1996). A general task considered in this paper is extracting information from postings to USENET newsgroups, such as job announcements.
Reference: <author> Soderland, S.; Fisher, D.; Aseltine, J.; and Lehn-ert, W. </author> <year> 1996. </year> <title> Issues in inductive learning of domain-specific text extraction rules. </title> <editor> In Wermter, S.; Riloff, E.; and Scheller, G., eds., </editor> <title> Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing, </title> <booktitle> Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer. </publisher> <pages> 290-301. </pages>
Reference-contexts: At 90 training examples, the average precision was 87.1% and the average recall was 58.8%. These numbers look quite promising when compared to the measured performance of other information extraction systems on various domains. This performance is comparable to that of Crystal on a medical domain task <ref> (Soderland et al. 1996) </ref>, and better than that of AutoSlog and AutoSlog-TS on part of the MUC4 terrorism task (Riloff 1996). It also compares favorably with the typical system performance on the MUC tasks (DARPA 1992; 1993).
Reference: <author> Weizenbaum, J. </author> <year> 1966. </year> <title> ELIZA A computer program for the study of natural language communications between men and machines. </title> <journal> Communications of the Association for Computing Machinery 9 </journal> <pages> 36-45. </pages>
Reference-contexts: Our system, Rapier (Robust Automated Production of Information Extraction Rules), learns rules for the complete IE task. The resulting rules extract the desired items directly from documents without prior parsing or subsequent processing. Using only a corpus of documents paired with filled templates, Rapier learns unbounded Eliza-like patterns <ref> (Weizenbaum 1966) </ref> that utilize limited syntactic information, such as the output of a part-of-speech tagger. Induced patterns can also easily incorporate semantic class information, such as that provided by WordNet (Miller et al. 1993).
Reference: <author> Zelle, J. M., and Mooney, R. J. </author> <year> 1994. </year> <title> Combining top-down and bottom-up methods in inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> 343-351. </pages>
Reference-contexts: That clause is further generalized by computing the rlggs of the clause with new randomly selected positive examples, and generalization terminates when the coverage of the best consistent clause stops improving. Chillin <ref> (Zelle & Mooney 1994) </ref> combines bottom-up and top-down techniques. The algorithm starts with a most specific definition (the complete set of positive examples) and introduces consistent generalizations that make the definition more compact. The search for consistent generalizations combines bottom-up methods from Golem with top-down methods from Foil (Quinlan 1990).
References-found: 19

