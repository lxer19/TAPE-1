URL: http://robotics.stanford.edu/~ronnyk/impSBC.ps.Z
Refering-URL: http://robotics.stanford.edu/users/ronnyk/ronnyk-bib.html
Root-URL: 
Email: fbecker,ronnyk,sommdag@engr.sgi.com  
Title: Improving Simple Bayes  
Author: Ron Kohavi Barry Becker Dan Sommerfield 
Address: 2011 N. Shoreline Blvd. Mountain View, CA 94043  
Affiliation: Data Mining and Visualization Group Silicon Graphics, Inc.  
Abstract: The simple Bayesian classifier (SBC), sometimes called Naive-Bayes, is built based on a conditional independence model of each attribute given the class. The model was previously shown to be surprisingly robust to obvious violations of this independence assumption, yielding accurate classification models even when there are clear conditional dependencies. We examine different approaches for handling unknowns and zero counts when estimating probabilities. Large scale experiments on 37 datasets were conducted to determine the effects of these approaches and several interesting insights are given, including a new variant of the Laplace estimator that outperforms other methods for dealing with zero counts. Using the bias-variance decomposition [15, 10], we show that while the SBC has performed well on common benchmark datasets, its accuracy will not scale up as the dataset sizes grow. Even with these limitations in mind, the SBC can serve as an excellent tool for initial exploratory data analysis, especially when coupled with a visualizer that makes its structure comprehensible.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Leo Breiman. </author> <title> Heuristics of instability in model selection. </title> <institution> Technical Report Statis tics Department, University of California at Berkeley, </institution> <year> 1994. </year>
Reference-contexts: The SBC usually has low variance as perturbations of the training sets will rarely cause large changes in its predictions, which are based on probabilities. Contrast this with decision tree inducers that are unstable <ref> [2, 1] </ref> because if two attributes are ranked very closely at the root of a subtree, their order might change when the training set is perturbed, and cause the whole subtree to differ. However, the SBC usually has high bias because of its inability to locally fit the data.
Reference: 2. <author> Leo Breiman. </author> <title> Bias, variance, and arcing classifiers. </title> <type> Technical report, </type> <institution> Statistics Department, University of California, Berkeley, </institution> <year> 1996. </year> <note> Available at: http://www.stat.Berkeley.EDU/users/breiman/. </note>
Reference-contexts: The SBC usually has low variance as perturbations of the training sets will rarely cause large changes in its predictions, which are based on probabilities. Contrast this with decision tree inducers that are unstable <ref> [2, 1] </ref> because if two attributes are ranked very closely at the root of a subtree, their order might change when the training set is perturbed, and cause the whole subtree to differ. However, the SBC usually has high bias because of its inability to locally fit the data.
Reference: 3. <author> Bojan Cestnik. </author> <title> Estimating probabilities: A crucial task in machine learning. </title> <editor> In Luigia Carlucci Aiello, editor, </editor> <booktitle> Proceedings of the ninth European Conference on Artificial Intelligence, </booktitle> <pages> pages 147-149, </pages> <year> 1990. </year>
Reference-contexts: The numbers after the error indicate the standard deviation of the mean error. The SBC model discretizes using entropy, estimates probabilities using Laplace-m, and ignores unknown values during classification. Some versions of the SBC, most notably the version described by Cestnik <ref> [3] </ref>, have used an alternative formulation that is mathematically equivalent, but requires estimating P (CjA) instead of P (AjC). Comparisons (not reported here) showed insignificant differences in accuracy between the two methods.
Reference: 4. <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3(4) </volume> <pages> 261-283, </pages> <year> 1989. </year>
Reference-contexts: The different approaches use a different numerator, but the idea is the same. Clark and Niblett <ref> [4] </ref> and Domingos and Pazzani [5] used P (C)=m. In MLC ++ [14], the default was 0:5=m. Laplace approaches Given a predefined factor f, if there are N matches out of n instances for a k value problem, estimate the probability as (N +f )=(n+ kf). <p> Comparisons (not reported here) showed insignificant differences in accuracy between the two methods. Many researchers have noted the good performance of SBC, including Clark and Niblett <ref> [4] </ref>, Kononenko [17], Langley and Sage [19], and Domingos and Pazzani [5]. Proposed extensions generally resulted in little improvements [16, 18, 22], although some recent proposals seem promising [9, 13]. 5 Summary We studied different options for handling unknowns, estimating probabilities, and discretizing.
Reference: 5. <author> Pedro Domingos and Michael Pazzani. </author> <title> Beyond independence: conditions for the optimality of the simple bayesian classifier. </title> <editor> In Lorenza Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 105-112. </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1996. </year>
Reference-contexts: We also can explain in some cases why these differences arise and when one option is preferable. Before describing the different options, we describe the methodology used throughout the paper. 2.1 Experimental Methodology We chose all the datasets reported in Domingos and Pazzani <ref> [5] </ref>, except lung-cancer, labor-negotiations, and soybean (small), which had fewer than 100 instances. We added more datasets, especially larger ones, such as segment, mushroom, letter, and adult for a total of 37. The specific datasets are shown below in Table 2. <p> One can either consider unknowns to be a separate value, as was done by Domingos and Pazzani <ref> [5] </ref>, or they can be ignored for a given instance by not including the matching term in the overall product. The optimal treatment of unknowns depend on their meaning in the domain. <p> The different approaches use a different numerator, but the idea is the same. Clark and Niblett [4] and Domingos and Pazzani <ref> [5] </ref> used P (C)=m. In MLC ++ [14], the default was 0:5=m. Laplace approaches Given a predefined factor f, if there are N matches out of n instances for a k value problem, estimate the probability as (N +f )=(n+ kf). <p> Comparisons (not reported here) showed insignificant differences in accuracy between the two methods. Many researchers have noted the good performance of SBC, including Clark and Niblett [4], Kononenko [17], Langley and Sage [19], and Domingos and Pazzani <ref> [5] </ref>. Proposed extensions generally resulted in little improvements [16, 18, 22], although some recent proposals seem promising [9, 13]. 5 Summary We studied different options for handling unknowns, estimating probabilities, and discretizing.
Reference: 6. <author> James Dougherty, Ron Kohavi, and Mehran Sahami. </author> <title> Supervised and unsupervised discretization of continuous features. </title> <editor> In Armand Prieditis and Stuart Russell, editors, </editor> <booktitle> Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 194-202. </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: Both methods for correcting frequency counts seem to work best when very small correction values are used, which to our knowledge has not been previously reported. If, in addition to unknown handling and zero counts, we also discretize the data using entropy minimization <ref> [6] </ref>, the average absolute error for all datasets decreases from 18.58% to 18.13% with an average relative error ratio of 0.94. 2.5 Limitations of the SBC While the SBC shows good performance on many of the datasets from UCI, it is still a very limited classifier.
Reference: 7. <author> Richard Duda and Peter Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: Simple models are especially useful if the model is to be understood by non-experts in machine learning. The simple Bayes classifier (SBC), sometimes called Naive-Bayes, is built based on a conditional independence model of each attribute given the class <ref> [11, 7] </ref>.
Reference: 8. <author> E. Fix and J.L. Hodges. </author> <title> Discriminatory analysis|nonparametric discrimination: Consistency properties. </title> <type> Technical Report 21-49-004, report no. 04, </type> <institution> USAF School of Aviation Medicine, Randolph Field, Tex, </institution> <month> February </month> <year> 1951. </year>
Reference-contexts: Bayesian inducer cannot be consistent in the statistical sense without additional strong assumptions (an inducer is consistent if the classifiers it produces approach the Bayes optimal error as the dataset size grows to infinity). Proofs have been given for decision tree inducers [12] and for nearest-neighbor inducers <ref> [8] </ref> under mild assumptions. In the bias-variance decomposition of error [15, 10], the error is the sum of two terms: the squared bias and the variance.
Reference: 9. <author> Nir Friedman and Moises Goldszmidt. </author> <title> Building classifiers using bayesian networks. </title> <editor> In Lorenza Saitta, editor, </editor> <booktitle> Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 157-165. </pages> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1996. </year>
Reference-contexts: Many researchers have noted the good performance of SBC, including Clark and Niblett [4], Kononenko [17], Langley and Sage [19], and Domingos and Pazzani [5]. Proposed extensions generally resulted in little improvements [16, 18, 22], although some recent proposals seem promising <ref> [9, 13] </ref>. 5 Summary We studied different options for handling unknowns, estimating probabilities, and discretizing. Through a large scale comparison of 37 datasets, we were able to pinpoint interesting datasets where error differences were significant and explained many of the reasons for different error results.
Reference: 10. <author> Stuart Geman, Eli Bienenstock, and Renee Doursat. </author> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 1-48, </pages> <year> 1992. </year>
Reference-contexts: Proofs have been given for decision tree inducers [12] and for nearest-neighbor inducers [8] under mild assumptions. In the bias-variance decomposition of error <ref> [15, 10] </ref>, the error is the sum of two terms: the squared bias and the variance. The bias measures how well the induced classifiers fit the data (low values are good), and the variance measures the stability (low values indicate stability).
Reference: 11. <author> Irving John Good. </author> <title> The Estimation of Probabilities: An Essay on Modern Bayesian Methods. </title> <publisher> M.I.T. Press, </publisher> <year> 1965. </year>
Reference-contexts: Simple models are especially useful if the model is to be understood by non-experts in machine learning. The simple Bayes classifier (SBC), sometimes called Naive-Bayes, is built based on a conditional independence model of each attribute given the class <ref> [11, 7] </ref>. <p> Laplace approaches Given a predefined factor f, if there are N matches out of n instances for a k value problem, estimate the probability as (N +f )=(n+ kf). For a two valued problems with f = 1, we get the well-known Laplace's law of succession <ref> [11] </ref> (N + 1)=(N + 2). Table 1 summarizes the average errors and the average error ratios relative to No-matches-PC (the No-match approach with the numerator factor set to P (C)) for all the datasets.
Reference: 12. <author> Louis Gordon and Richard A Olshen. </author> <title> Almost sure consistent nonparametric re gression from recursive partitioning schemes. </title> <journal> Journal of Multivariate Analysis, </journal> <volume> 15 </volume> <pages> 147-163, </pages> <year> 1984. </year>
Reference-contexts: Bayesian inducer cannot be consistent in the statistical sense without additional strong assumptions (an inducer is consistent if the classifiers it produces approach the Bayes optimal error as the dataset size grows to infinity). Proofs have been given for decision tree inducers <ref> [12] </ref> and for nearest-neighbor inducers [8] under mild assumptions. In the bias-variance decomposition of error [15, 10], the error is the sum of two terms: the squared bias and the variance.
Reference: 13. <author> Ron Kohavi. </author> <title> Scaling up the accuracy of naive-bayes classifiers: a decision-tree hybrid. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 114-119, </pages> <year> 1996. </year>
Reference-contexts: Many researchers have noted the good performance of SBC, including Clark and Niblett [4], Kononenko [17], Langley and Sage [19], and Domingos and Pazzani [5]. Proposed extensions generally resulted in little improvements [16, 18, 22], although some recent proposals seem promising <ref> [9, 13] </ref>. 5 Summary We studied different options for handling unknowns, estimating probabilities, and discretizing. Through a large scale comparison of 37 datasets, we were able to pinpoint interesting datasets where error differences were significant and explained many of the reasons for different error results.
Reference: 14. <author> Ron Kohavi, Dan Sommerfield, and James Dougherty. </author> <title> Data mining using MLC++: A machine learning library in C++. </title> <booktitle> In Tools with Artificial Intelligence, </booktitle> <pages> pages 234-245. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year> <note> Received the best paper award. http://www.sgi.com/Technology/mlc. </note>
Reference-contexts: The different approaches use a different numerator, but the idea is the same. Clark and Niblett [4] and Domingos and Pazzani [5] used P (C)=m. In MLC ++ <ref> [14] </ref>, the default was 0:5=m. Laplace approaches Given a predefined factor f, if there are N matches out of n instances for a k value problem, estimate the probability as (N +f )=(n+ kf). <p> Acknowledgments We would like to thank Pedro Domingos, Jim Kelly, Mehran Sahami and Joel Tesler for their excellent comments and suggestions. We would like to thank Eric Bauer and Clay Kunz for their work on MLC ++ <ref> [14] </ref>. The experiments described here were all done using MLC ++ .
Reference: 15. <author> Ron Kohavi and David H. Wolpert. </author> <title> Bias plus variance decomposition for zero-one loss functions. </title> <editor> In Lorenza Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference. </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> July </month> <year> 1996. </year> <note> Available at http://robotics.stanford.edu/users/ronnyk. </note>
Reference-contexts: Proofs have been given for decision tree inducers [12] and for nearest-neighbor inducers [8] under mild assumptions. In the bias-variance decomposition of error <ref> [15, 10] </ref>, the error is the sum of two terms: the squared bias and the variance. The bias measures how well the induced classifiers fit the data (low values are good), and the variance measures the stability (low values indicate stability).
Reference: 16. <author> Igor Kononenko. </author> <title> Semi-naive bayesian classifiers. </title> <booktitle> In Proceedings of the sixth Euro pean Working Session on Learning, </booktitle> <pages> pages 206-219, </pages> <year> 1991. </year>
Reference-contexts: Comparisons (not reported here) showed insignificant differences in accuracy between the two methods. Many researchers have noted the good performance of SBC, including Clark and Niblett [4], Kononenko [17], Langley and Sage [19], and Domingos and Pazzani [5]. Proposed extensions generally resulted in little improvements <ref> [16, 18, 22] </ref>, although some recent proposals seem promising [9, 13]. 5 Summary We studied different options for handling unknowns, estimating probabilities, and discretizing.
Reference: 17. <author> Igor Kononenko. </author> <title> Inductive and bayesian learning in medical diagnosis. </title> <journal> Applied Artificial Intelligence, </journal> <volume> 7 </volume> <pages> 317-337, </pages> <year> 1993. </year>
Reference-contexts: Its accuracy is very good on small datasets but it may asymptote to a high error rate, making it less useful as a classifier for very large databases. 4 Related Work The SBC model is very simple and its explanatory power was previously noted by Kononenko <ref> [17] </ref>, who wrote that "Physicians found such explanations [using conditional probabilities] as natural and similar to their classification. <p> Comparisons (not reported here) showed insignificant differences in accuracy between the two methods. Many researchers have noted the good performance of SBC, including Clark and Niblett [4], Kononenko <ref> [17] </ref>, Langley and Sage [19], and Domingos and Pazzani [5]. Proposed extensions generally resulted in little improvements [16, 18, 22], although some recent proposals seem promising [9, 13]. 5 Summary We studied different options for handling unknowns, estimating probabilities, and discretizing.
Reference: 18. <author> Pat Langley. </author> <title> Induction of recursive bayesian classifiers. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 153-164, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Comparisons (not reported here) showed insignificant differences in accuracy between the two methods. Many researchers have noted the good performance of SBC, including Clark and Niblett [4], Kononenko [17], Langley and Sage [19], and Domingos and Pazzani [5]. Proposed extensions generally resulted in little improvements <ref> [16, 18, 22] </ref>, although some recent proposals seem promising [9, 13]. 5 Summary We studied different options for handling unknowns, estimating probabilities, and discretizing.
Reference: 19. <author> Pat Langley and Stephanie Sage. </author> <title> Scaling to domains with many irrelevant features. </title> <editor> In Russel Greiner, editor, </editor> <booktitle> Computational Learning Theory and Natural Learning Systems. </booktitle> <publisher> MIT Press, to appear. </publisher>
Reference-contexts: Comparisons (not reported here) showed insignificant differences in accuracy between the two methods. Many researchers have noted the good performance of SBC, including Clark and Niblett [4], Kononenko [17], Langley and Sage <ref> [19] </ref>, and Domingos and Pazzani [5]. Proposed extensions generally resulted in little improvements [16, 18, 22], although some recent proposals seem promising [9, 13]. 5 Summary We studied different options for handling unknowns, estimating probabilities, and discretizing.
Reference: 20. <author> Christopher J. Merz and Patrick M. Murphy. </author> <title> UCI repository of machine learning databases. </title> <note> http://www.ics.uci.edu/~mlearn/MLRepository.html, 1996. </note>
Reference-contexts: We address two separate issues related to the SBC: how to treat unknown values and how to estimate the probabilities (especially when some of the counts are zero). A large scale comparison of these variants on 37 datasets from the UCI Repository <ref> [20] </ref> was done. We emphasize the extreme cases that led to interesting insights.
Reference: 21. <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference-contexts: We reduced the overall error from 20.30% for the basic SBC to 18.13%, which is a relative improvement of 10.7%. Table 2 shows the dataset characteristics and absolute errors for C4.5, C4.5-rules <ref> [21] </ref>, and our SBC. The average error for C4.5 is 17.85%, for C4.5-rules it is 17.90%, and for SBC it is 18.19%. If we ignore the big datasets (datasets DNA through adult in the table), C4.5's error is 20.83%, C4.5-rules's error is 20.93%, and SBC's error is 20.10%.
Reference: 22. <author> Moninder Singh and Gregory M. Provan. </author> <title> A comparison of induction algorithms for selective and non-selective bayesian classifiers. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 497-505, </pages> <month> July </month> <year> 1995. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Comparisons (not reported here) showed insignificant differences in accuracy between the two methods. Many researchers have noted the good performance of SBC, including Clark and Niblett [4], Kononenko [17], Langley and Sage [19], and Domingos and Pazzani [5]. Proposed extensions generally resulted in little improvements <ref> [16, 18, 22] </ref>, although some recent proposals seem promising [9, 13]. 5 Summary We studied different options for handling unknowns, estimating probabilities, and discretizing.
References-found: 22

