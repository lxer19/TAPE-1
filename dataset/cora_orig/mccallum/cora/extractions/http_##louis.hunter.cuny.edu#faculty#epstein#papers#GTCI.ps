URL: http://louis.hunter.cuny.edu/faculty/epstein/papers/GTCI.ps
Refering-URL: http://wwwcs.hunter.cuny.edu/faculty/epstein/papers/se_web/html/graph_theory.html
Root-URL: 
Title: One Systems Search for Mathematical Knowledge  
Author: Susan L. Epstein 
Keyword: discovery, machine learning, knowledge representation, concept definition, mathematics, graph theory  
Address: New York New York, NY 10021  
Affiliation: Department of Computer Science Hunter College and The Graduate School The City University of  
Note: Learning and Discovery:  
Abstract: This paper appeared in 1988 in Computational Intelligence 4 (1): 42-53. Abstract The Graph Theorist, GT, is a system that performs mathematical research in graph theory. From the definitions in its input knowledge base, GT constructs examples of mathematical concepts, conjectures and proves mathematical theorems about concepts, and discovers new concepts. Discovery is driven both by examples and by definitional form. The discovery processes construct a semantic net that links all of GTs concepts together. Each definition is an algebraic expression whose semantic interpretation is a stylized algorithm to generate a class of graphs correctly and completely. From a knowledge base of these concept definitions, GT is able to conjecture and prove such theorems as The set of acyclic, connected graphs is precisely the set of trees and There is no odd-regular graph on an odd number of vertices. GT explores new concepts either to develop an area of knowledge or to link a newly-acquired concept into a preexisting knowledge base. New concepts arise from the specialization of an existing concept, the generalization of an existing concept, and the merger of two or more existing concepts. From an initial knowledge base containing only the definition of graph, GT discovers such concepts as acyclic graphs, connected graphs and bipartite graphs. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bondy, J. and Murty, U. </author> <year> 1976. </year> <title> Graph Theory with Applications . New York: </title> <publisher> North-Holland. </publisher>
Reference-contexts: graph graph with n vertices k-chromatic graph graph with m edges graph with vertex covering number k pinwheel graph with circumference k non-planar graph graph with edge covering number k k-factorable graph with a k-factor even-regular graph Hamiltonian graph odd-regular graph planar graph topics presented as definitions and theorems, and <ref> (Bondy and Murty 1976) </ref> , an algorithmic approach. There is some evidence that p-generators may exist for every P in U, or at least for every interesting P in graph theory.
Reference: <author> Buchanan, B. G. and Mitchell, T. M. </author> <year> 1978. </year> <title> Model-Directed Learning of Production Rules. </title> <address> In D. </address>
Reference-contexts: Although the space of all possible classes of graphs is large, the representation language is able to control and focus exploration through it in a variety of ways. When it begins with an extremely general definition and explores specialized versions of it fairly exhaustively, GT is similar to META-DENDRAL <ref> (Buchanan and Mitchell 1978) </ref>. Both programs survive a generate-and-test strategy because their representations encourage what Michalski (Michalski 1983) calls conceptual data analysis. 2.
Reference: <editor> A. Waterman, & F. Hayes-Roth (Ed.), </editor> <booktitle> Pattern-Directed Inference Systems, </booktitle> <pages> 297-312. </pages> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Carbonell, J. G., Michalski, R. S. and Mitchell, T. M. </author> <year> 1983. </year> <title> An Overview of Machine Learning. </title>
Reference-contexts: 1. Context and Background 1.1 The Representation of Objects and Classes Given a set of objects in a domain, the task of inductively inferring their natural classes and the relations among those classes may be categorized as learning by discovery <ref> (Carbonell, Michalski, and Mitchell 1983) </ref>. When the objects are many and highly detailed, search over the domain usually requires heuristic guidance.
Reference: <editor> In R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Ed.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> 3-23. </pages> <address> Palo Alto: </address> <publisher> Tioga Publishing. </publisher>
Reference: <author> Dietterich, T. G. and Michalski, R. S. </author> <year> 1983. </year> <title> A Comparative Review of Selected Methods for Learning from Examples. </title> <editor> In R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Ed.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> 41-81. </pages> <address> Palo Alto: </address> <publisher> Tioga Publishing. </publisher>
Reference: <author> Emde, W., Habel, C. U. and Rollinger, C.-R. </author> <year> 1983. </year> <title> The Discovery of the Equator or Concept Driven Learning. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 455-458. </pages> <address> Karlsruhe: </address> <publisher> William Kaufmann, Inc. </publisher>
Reference-contexts: STABB (Utgoff 1986) creates disjunctions of existing attributes to expand LEXs solution space. CLUSTER/S (Stepp and Michalski 1986) uses either numerical combinations or logical operators to create new attributes. PI (Thagard and Holyoak 1985) combines frames containing production rules. Other efforts <ref> (Emde, Habel, and Rollinger 1983) </ref> use mathematical properties, such as transitivity, to combine relational attributes. GT takes an alternative approach to the detection of relations among object classes.
Reference: <author> Epstein, S. L. </author> <year> 1983. </year> <title> Knowledge Representation in Mathematics: A Case Study in Graph Theory. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science, Rutgers University. </institution>
Reference-contexts: Both programs survive a generate-and-test strategy because their representations encourage what Michalski (Michalski 1983) calls conceptual data analysis. 2. Concept Description in GT GT derives much of its power from a set of representation languages whose theoretical formulation is detailed rigorously in <ref> (Epstein 1983) </ref> and summarized in (Epstein 1987). The treatment of the representation here is informal and describes only selected, implemented segments of the theory. <p> There is some evidence that p-generators may exist for every P in U, or at least for every interesting P in graph theory. At this writing, more than 40 properties of varying difficulty have been selected from the three benchmark texts and described correctly and completely as p-generators <ref> (Epstein 1983) </ref>. They appear in Table 3. Three graph properties (self-complementary, uniquely k-colorable, and kedge-colorable) have been expressed and proved correct but lack completeness proofs, although no counterexamples are known. Current research suggests that a more extensive knowledge of graph theory would resolve such difficulties. <p> ,s 2 &gt;, and a conjecture that p 1 subsumes p 2 , GT attempts to show that: Learning and Discovery Page 12 f 2 is subsumed by f 1 , that is, f 2 is a special case of f 1 . (Extended definitions for operator subsumption appear in <ref> (Epstein 1983) </ref>.) Every graph in S 2 has property p 1 . s 2 is subsumed by s 1 , that is, s 2 is more restrictive than s 1 . (Extended definitions for selector subsumption appear in (Epstein 1983).) Because there are usually only a few known seeds, GT checks <p> case of f 1 . (Extended definitions for operator subsumption appear in <ref> (Epstein 1983) </ref>.) Every graph in S 2 has property p 1 . s 2 is subsumed by s 1 , that is, s 2 is more restrictive than s 1 . (Extended definitions for selector subsumption appear in (Epstein 1983).) Because there are usually only a few known seeds, GT checks the list of examples for p 1 against S 2 . <p> GTs descriptive ability lies in the number and nature of the primitive operators permitted in f and of the selector descriptions permitted in s . As the set of such operators and descriptions is extended, a lattice of descriptive languages (detailed in <ref> (Epstein 1983) </ref>) can be constructed. Such an extended language offers additional alternatives, and ordinarily has greater expressive power (as measured by the number of graph properties it defines) than GTs current representation. In turn, operations with an extended language are likely to require more computer resources.
Reference: <author> Epstein, S. L. </author> <year> 1987. </year> <title> Languages for Problem Solving in Graph Theory. </title> <editor> In J. C. Boudreaux, B. </editor> <publisher> W. </publisher>
Reference-contexts: Both programs survive a generate-and-test strategy because their representations encourage what Michalski (Michalski 1983) calls conceptual data analysis. 2. Concept Description in GT GT derives much of its power from a set of representation languages whose theoretical formulation is detailed rigorously in (Epstein 1983) and summarized in <ref> (Epstein 1987) </ref>. The treatment of the representation here is informal and describes only selected, implemented segments of the theory.
Reference: <editor> Hamill, & R. N. Jernigan (Ed.), </editor> <booktitle> The Role of Language in Problem Solving 2, </booktitle> <pages> 261-300. </pages> <address> New York: </address> <publisher> North-Holland. </publisher>
Reference: <author> Harary, F. </author> <year> 1972. </year> <title> Graph Theory . Reading: Addison-Wesley. </title> <note> Learning and Discovery Page 26 Laird, </note> <author> P. G. </author> <year> 1986. </year> <title> Inductive Inference by Refinement. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> 472-476. </pages> <address> Philadelphia, PA. </address>
Reference-contexts: The definition generates the infinite class of acyclic graphs; it will never halt because bindings for the variables in s can be found on each iteration. The content of the following three general texts is taken as graph theory: (Ore 1962) , a classical development in elegant mathematical fashion, <ref> (Harary 1972) </ref> , a broad overview of Learning and Discovery Page 10 Table 3: Graph Properties Representable as P-Generators graph graph on even number of vertices edgeless graph graph on odd number of vertices connected graph graph with even number of edges biconnected graph graph with odd number of edges acyclic
Reference: <author> Langley, P., Bradshaw, G. L. and Simon, H. A. </author> <year> 1983. </year> <title> Rediscovering Chemistry with the BACON System. </title> <editor> In R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Ed.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> 307-329. </pages> <address> Palo Alto: </address> <publisher> Tioga Publishing. </publisher>
Reference: <author> Langley, P., Zytkow, J. M., Simon, H. A. and Bradshaw, G. L. </author> <year> 1986. </year> <title> The Search for Regularity: Four Aspects of Scientific Discovery. </title> <editor> In R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Ed.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> 425-469. </pages> <address> Palo Alto: </address> <publisher> Tioga Publishing. </publisher>
Reference-contexts: The BACON programs (Langley, Bradshaw, and Simon 1983; Langley, Zytkow, Simon, and Bradshaw 1986) postulate and calculate new numerical attributes from the original ones. OPUS (Nordhausen 1986) postulates and calculates new attributes as relational combinations of the original ones. Other expansions are formed in STAHL and GLAUBER <ref> (Langley, Zytkow, Simon, and Bradshaw 1986) </ref>, in (Lee and Ray 1986), and in (Laird 1986). Even when the representation of a concept is not explicitly a feature vector, inadequacies in the description language present research challenges. STABB (Utgoff 1986) creates disjunctions of existing attributes to expand LEXs solution space.
Reference: <author> Lee, W. D. and Ray, S. R. </author> <year> 1986. </year> <title> Rule Refinement Using the Probabilistic Generator. </title> <booktitle> In Proceedings of the Fifth National Joint Conference on Artificial Intelligence, </booktitle> <pages> 442-447. </pages> <address> Philadelphia, PA: Lenat, D. </address> <publisher> B. </publisher> <year> 1976. </year> <title> AM: An Artificial Intelligence Approach to Discovery in Mathematics . Ph.D. </title> <type> thesis, </type> <institution> Department of Computer Science, Stanford University. </institution>
Reference-contexts: OPUS (Nordhausen 1986) postulates and calculates new attributes as relational combinations of the original ones. Other expansions are formed in STAHL and GLAUBER (Langley, Zytkow, Simon, and Bradshaw 1986), in <ref> (Lee and Ray 1986) </ref>, and in (Laird 1986). Even when the representation of a concept is not explicitly a feature vector, inadequacies in the description language present research challenges. STABB (Utgoff 1986) creates disjunctions of existing attributes to expand LEXs solution space.
Reference: <author> Lenat, D. B. </author> <year> 1983. </year> <title> The Role of Heuristics in Learning by Discovery: Three Case Studies. </title> <booktitle> In R. </booktitle>
Reference-contexts: Learning and Discovery Page 24 7. Future Work Lenats work with AM convinced him that, as the research area within mathematics changed (from, say, set theory to number theory) new discovery heuristics were required <ref> (Lenat 1983) </ref> . GT is designed to work within a single area of mathematics; no need for new heuristics is anticipated. Instead, plans for GTs future development are based upon the power and flexibility of the p-generator representation.
Reference: <editor> S. Michalski, J. G. Carbonell, & T. M. Mitchell (Ed.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> 243-306. </pages> <address> Palo Alto: </address> <publisher> Tioga Publishing. </publisher>
Reference: <author> Lenat, D. B. and Brown, J. S. </author> <year> 1984. </year> <title> Why AM and EURISKO Appear to Work. </title> <journal> Artificial Intelligence, </journal> <volume> 23 (3): </volume> <pages> 249-268. </pages>
Reference: <author> Michalski, R. S. </author> <year> 1983. </year> <title> A Theory and Methodology of Inductive Learning. </title> <editor> In R. S. Michalski, </editor> <publisher> J. </publisher>
Reference-contexts: 1. Context and Background 1.1 The Representation of Objects and Classes Given a set of objects in a domain, the task of inductively inferring their natural classes and the relations among those classes may be categorized as learning by discovery <ref> (Carbonell, Michalski, and Mitchell 1983) </ref>. When the objects are many and highly detailed, search over the domain usually requires heuristic guidance. <p> The loss of detail incurred by a feature vector representation is expected to be justified by an ability to support the efficient computation of simple descriptions while maximizing inter-class differences <ref> (Michalski and Stepp 1983) </ref>. Once such rules for classes have been inferred, however, the next task may be to infer the relations among the classes. There is no guarantee that the features that were relevant to class identification will be equally useful in reasoning about the classes. <p> When it begins with an extremely general definition and explores specialized versions of it fairly exhaustively, GT is similar to META-DENDRAL (Buchanan and Mitchell 1978). Both programs survive a generate-and-test strategy because their representations encourage what Michalski <ref> (Michalski 1983) </ref> calls conceptual data analysis. 2. Concept Description in GT GT derives much of its power from a set of representation languages whose theoretical formulation is detailed rigorously in (Epstein 1983) and summarized in (Epstein 1987). <p> Relations between Concepts in GT As Michalski has observed <ref> (Michalski 1983) </ref>, inductive inference from examples does not preserve truth but only falsity. Although research mathematicians devote much time to example generation, and infer conjectures about relations among mathematical ideas based on these examples, inductive inference is only a tool.
Reference: <editor> G. Carbonell, & T. M. Mitchell (Ed.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> 83-134. </pages> <address> Palo Alto: </address> <publisher> Tioga Publishing. </publisher>
Reference: <author> Michalski, R. S. </author> <year> 1986. </year> <booktitle> Understanding the Nature of Learning: Issues and Research Directions. </booktitle>
Reference-contexts: Thus an approximation must be discovered in place of the discarded details about structural similarities. Constraining the solution space to the vocabulary of the original attributes has been repeatedly identified as a severe limitation (for example, <ref> (Stepp and Michalski 1986) </ref> ), and there has been much recent interest in expanding the expressive power of feature-based descriptions. The BACON programs (Langley, Bradshaw, and Simon 1983; Langley, Zytkow, Simon, and Bradshaw 1986) postulate and calculate new numerical attributes from the original ones. <p> Even when the representation of a concept is not explicitly a feature vector, inadequacies in the description language present research challenges. STABB (Utgoff 1986) creates disjunctions of existing attributes to expand LEXs solution space. CLUSTER/S <ref> (Stepp and Michalski 1986) </ref> uses either numerical combinations or logical operators to create new attributes. PI (Thagard and Holyoak 1985) combines frames containing production rules. Other efforts (Emde, Habel, and Rollinger 1983) use mathematical properties, such as transitivity, to combine relational attributes. <p> The theoretical sciences concern themselves with highly detailed objects, objects already bound into classes by strong structural similarities. Mathematics, in particular, has many classes of objects already clearly delineated by definitions. In modeling such domains, the origin of these classes (what <ref> (Michalski 1986) </ref> calls constructive induction) and the relations among them should be the focus of attention. In theory-driven discovery, search heuristics postulate statements and then run experiments to validate those statements. <p> Thus knowledge acquisition consists of the construction of intermediate concepts linked to both the focus concept and the knowledge base. 6. Results and Significance According to Michalskis characterization of learning systems <ref> (Michalski 1986) </ref>, GT learns both by observation (of its input examples and definitions) and by discovery (upon construction of new examples and properties). GT expands its knowledge about a concept by generating examples of it and by determining its relation to other concepts. <p> No specific tasks were input, only the general directive to explore the knowledge base. GT formulates its own conjectures and then attempts to construct proofs for them based on the structure of the definitions. The modifications to the representation for ACYCLIC constitute learning as defined in <ref> (Michalski 1986) </ref>. Clearly GT learns how ACYCLIC relates to other concepts, and constructs and stores additional examples of acyclic graphs.
Reference: <editor> In R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Ed.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> 3-25. </pages> <address> Palo Alto: </address> <publisher> Tioga Publishing. </publisher>
Reference: <author> Michalski, R. S. and Stepp, R. E. </author> <year> 1981. </year> <title> Concept-based Clustering versus Numerical Taxonomy, </title> <type> 1073, </type> <institution> Department of Computer Science, University of Illinois. </institution>
Reference: <author> Michalski, R. S. and Stepp, R. E. </author> <year> 1983. </year> <title> Learning from Observation: Conceptual Clustering. </title> <editor> In R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Ed.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> 331-363. </pages> <address> Palo Alto: </address> <publisher> Tioga Publishing. </publisher>
Reference-contexts: 1. Context and Background 1.1 The Representation of Objects and Classes Given a set of objects in a domain, the task of inductively inferring their natural classes and the relations among those classes may be categorized as learning by discovery <ref> (Carbonell, Michalski, and Mitchell 1983) </ref>. When the objects are many and highly detailed, search over the domain usually requires heuristic guidance. <p> The loss of detail incurred by a feature vector representation is expected to be justified by an ability to support the efficient computation of simple descriptions while maximizing inter-class differences <ref> (Michalski and Stepp 1983) </ref>. Once such rules for classes have been inferred, however, the next task may be to infer the relations among the classes. There is no guarantee that the features that were relevant to class identification will be equally useful in reasoning about the classes. <p> When it begins with an extremely general definition and explores specialized versions of it fairly exhaustively, GT is similar to META-DENDRAL (Buchanan and Mitchell 1978). Both programs survive a generate-and-test strategy because their representations encourage what Michalski <ref> (Michalski 1983) </ref> calls conceptual data analysis. 2. Concept Description in GT GT derives much of its power from a set of representation languages whose theoretical formulation is detailed rigorously in (Epstein 1983) and summarized in (Epstein 1987). <p> Relations between Concepts in GT As Michalski has observed <ref> (Michalski 1983) </ref>, inductive inference from examples does not preserve truth but only falsity. Although research mathematicians devote much time to example generation, and infer conjectures about relations among mathematical ideas based on these examples, inductive inference is only a tool.
Reference: <author> Michener, E. R. </author> <year> 1978. </year> <title> Understanding Understanding Mathematics, AI MEMO-488, LOGO MEMO-50, </title> <publisher> MIT. </publisher>
Reference-contexts: In GT, a concept is a frame representing a graph property and knowledge associated with it. (This representation was inspired by Micheners work <ref> (Michener 1978) </ref>.) A slightly-edited example of an initial GT frame for the concept ACYCLIC appears in Figure 2.
Reference: <author> Nordhausen, B. </author> <year> 1986. </year> <title> Conceptual Clustering Using Relational Information. </title> <booktitle> In Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> 508-512. </pages> <address> Philadelphia. </address>
Reference-contexts: There is no guarantee that the features that were relevant to class identification will be equally useful in reasoning about the classes. For example, the food chain domain of <ref> (Nordhausen 1986) </ref> contains facts about a variety of birds; within it, reasoning about hierarchical relations requires the definition of new attributes, such as the size of what a bird eats, to infer a classification tree. <p> The BACON programs (Langley, Bradshaw, and Simon 1983; Langley, Zytkow, Simon, and Bradshaw 1986) postulate and calculate new numerical attributes from the original ones. OPUS <ref> (Nordhausen 1986) </ref> postulates and calculates new attributes as relational combinations of the original ones. Other expansions are formed in STAHL and GLAUBER (Langley, Zytkow, Simon, and Bradshaw 1986), in (Lee and Ray 1986), and in (Laird 1986).
Reference: <author> Ore, O. </author> <year> 1962. </year> <title> Theory of Graphs . Providence: </title> <publisher> American Mathematical Society. </publisher>
Reference-contexts: The definition generates the infinite class of acyclic graphs; it will never halt because bindings for the variables in s can be found on each iteration. The content of the following three general texts is taken as graph theory: <ref> (Ore 1962) </ref> , a classical development in elegant mathematical fashion, (Harary 1972) , a broad overview of Learning and Discovery Page 10 Table 3: Graph Properties Representable as P-Generators graph graph on even number of vertices edgeless graph graph on odd number of vertices connected graph graph with even number of
Reference: <author> Ritchie, G. D. and Hanna, F. K. </author> <year> 1984. </year> <title> AM: A Case Study in AI Methodology. </title> <booktitle> Artificial Learning and Discovery Page 27 Intelligence, </booktitle> <volume> 23 (3): </volume> <pages> 269-294. </pages>
Reference: <author> Shapiro, E. Y. </author> <year> 1981. </year> <title> An Algorithm that Infers Theories from Facts. </title> <booktitle> In Proceedings of the Seventh International Joint Conference on Artificial Intelligence, </booktitle> <pages> 446-451. </pages> <address> Vancouver, B.C. </address>
Reference: <author> Stepp, R. E. and Michalski, R. S. </author> <year> 1986. </year> <title> Conceptual Clustering: Inventing Goal-Oriented Classifications of Structured Objects. </title> <editor> In R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Ed.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach - Volume II, </booktitle> <pages> 471-498. </pages> <address> Palo Alto: </address> <publisher> Tioga Publishing. </publisher>
Reference-contexts: Thus an approximation must be discovered in place of the discarded details about structural similarities. Constraining the solution space to the vocabulary of the original attributes has been repeatedly identified as a severe limitation (for example, <ref> (Stepp and Michalski 1986) </ref> ), and there has been much recent interest in expanding the expressive power of feature-based descriptions. The BACON programs (Langley, Bradshaw, and Simon 1983; Langley, Zytkow, Simon, and Bradshaw 1986) postulate and calculate new numerical attributes from the original ones. <p> Even when the representation of a concept is not explicitly a feature vector, inadequacies in the description language present research challenges. STABB (Utgoff 1986) creates disjunctions of existing attributes to expand LEXs solution space. CLUSTER/S <ref> (Stepp and Michalski 1986) </ref> uses either numerical combinations or logical operators to create new attributes. PI (Thagard and Holyoak 1985) combines frames containing production rules. Other efforts (Emde, Habel, and Rollinger 1983) use mathematical properties, such as transitivity, to combine relational attributes.
Reference: <author> Thagard, P. and Holyoak, K. </author> <year> 1985. </year> <title> Discovering the Wave Theory of Sound: Inductive Inference in the Context of Problem Solving. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <pages> 610-612. </pages> <address> Los Angeles: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: STABB (Utgoff 1986) creates disjunctions of existing attributes to expand LEXs solution space. CLUSTER/S (Stepp and Michalski 1986) uses either numerical combinations or logical operators to create new attributes. PI <ref> (Thagard and Holyoak 1985) </ref> combines frames containing production rules. Other efforts (Emde, Habel, and Rollinger 1983) use mathematical properties, such as transitivity, to combine relational attributes. GT takes an alternative approach to the detection of relations among object classes. <p> Unlike AM whose cumbersome LISP code limited it to conjecture based solely upon examples, GT is capable of reasoning both from p - generator definitions and from specific graphs, either seeds or generated examples. (The latter is an example of what Thagard and Holyoak <ref> (Thagard and Holyoak 1985) </ref> call instance-based generalization.) These two sources support the formulation of projects in a variety of ways. When presented with a definition for a concept, most mathematicians immediately construct examples.
Reference: <author> Utgoff, P. E. </author> <year> 1986. </year> <title> Shift of Bias for Inductive Concept Learning. </title> <editor> In R. S. Michalski, J. </editor> <publisher> G. </publisher>
Reference-contexts: Other expansions are formed in STAHL and GLAUBER (Langley, Zytkow, Simon, and Bradshaw 1986), in (Lee and Ray 1986), and in (Laird 1986). Even when the representation of a concept is not explicitly a feature vector, inadequacies in the description language present research challenges. STABB <ref> (Utgoff 1986) </ref> creates disjunctions of existing attributes to expand LEXs solution space. CLUSTER/S (Stepp and Michalski 1986) uses either numerical combinations or logical operators to create new attributes. PI (Thagard and Holyoak 1985) combines frames containing production rules. <p> These extensions will also provide a testbed for the study of performance under representational shifts. The key to the most interesting specializations, those involving additional descriptions in s, is the language in which those descriptions may be written. Utgoff <ref> (Utgoff 1986) </ref> warns that, unless the [s-]language is extensible, GT may not be able to access many interesting ideas. Ways to have GT extend the s language itself are currently being studied.
Reference: <editor> Carbonell, & T. M. Mitchell (Ed.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach - Volume II, </booktitle> <pages> 107-148. </pages> <address> Palo Alto: </address> <publisher> Tioga Publishing. </publisher>
References-found: 32

