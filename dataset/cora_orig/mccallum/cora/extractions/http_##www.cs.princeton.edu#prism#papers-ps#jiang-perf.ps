URL: http://www.cs.princeton.edu/prism/papers-ps/jiang-perf.ps
Refering-URL: http://www.cs.princeton.edu/prism/html/all-papers.html
Root-URL: http://www.cs.princeton.edu
Email: fdj, shz, jpsg@cs.princeton.edu  
Title: Application Restructuring and Performance Portability on Shared Virtual Memory and Hardware-Coherent Multiprocessors  
Author: Dongming Jiang, Hongzhang Shan and Jaswinder Pal Singh 
Address: 35 Olden Street  Princeton, NJ 08544  
Affiliation: Department of Computer Science  Princeton University  
Abstract: The performance portability of parallel programs across a wide range of emerging coherent shared address space systems is not well understood. Programs that run well on efficient, hardware cache-coherent systems often do not perform well on less optimal or more commodity-based communication architectures. This paper studies this issue of performance portability, with the commodity communication architecture of interest being page-grained shared virtual memory. We begin with applications that perform well on moderate-scale hardware cache-coherent systems, and find that they do not do so well on SVM systems. Then, we examine whether and how the applications can be improved for SVM systems |through data structuring or algorithmic enhancements|and the nature and difficulty of the optimizations. Finally, we examine the impact of the successful optimizations on hardware-coherent platforms themselves, to see whether they are helpful, harmful or neutral on those platforms. We develop a systematic methodology to explore optimizations in different structured classes. The results, and the difficulty of the optimizations, lead insight not only into performance portability but also into the viability of SVM as a platform for these types of applications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. Myrinet: </author> <title> A Gigabit-per-Second Local Area Network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: HLRC has recently been shown to equal or outperform non home-based LRC protocols as well, at least on the platform studied [21]. The simulator models an architecture of processing nodes connected by a commodity interconnect that is modeled on Myrinet <ref> [1] </ref>. In our experiments, each node can be considered to have a 200Mhz x86 processor running at 1 CPI (without memory effects).
Reference: [2] <author> Holt C., Singh J. P., and Hennessy J. </author> <title> Application and Architectural Bottlenecks in Large Scale Distributed Shared Memory Machines. </title> <booktitle> In Proceedings of the 23th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 134-145, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Nevertheless, software communication and synchronization costs can be high as can protocol overhead, and the performance potential of this approach across a wide range of applications is not well understood. Previous research has studied parallel application performance on particular shared memory systems <ref> [11, 12, 7, 5, 17, 2, 14, 20] </ref>. Studies on shared virtual memory have largely used applications as they were written for hardware cache-coherent machines. <p> While classifying programming improvements is difficult, since they can be quite ad hoc, we use structured classes of optimizations starting from the simplest to the most challenging, improved by the structure used in <ref> [2] </ref>. Specifically, we divide optimizations into three classes: * Padding and Alignment is the simplest optimization. <p> When we use real systems, we plan to investigate the issues with larger numbers of processors. For example, it would be interesting to see if the optimizations that are useful at smaller scale for SVM but not CC-NUMA become more useful for CC-NUMA machines at larger scale <ref> [2] </ref>, and how problem size affects these results.
Reference: [3] <author> Jiang D and Singh J.P. </author> <title> Parallel Shear-Warp Volume Rendering on Shared Address Space Multiprocessors. </title> <booktitle> In Proceedings of the 1997 ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Our application suite contains 7 applications, each with several versions. Six are originally from the SPLASH-2 [18] suite, and one is a recently published parallel shear-warp volume rendering program [9, 15] that is also described in another paper in these proceedings <ref> [3] </ref>. Let us briefly describe the access patterns in the original applications. Details of the applications themselves can be found in [18] and the references there. 2.2.1 Regular Applications LU performs the blocked LU factorization of a dense matrix. <p> How this is done is beyond the scope of this paper but it relies on application insight. It is described in another paper in these proceedings <ref> [3] </ref>. <p> Summary: Shear-Warp is an example in which restructuring the application can improve performance tremendously due to communication and memory system interactions, but it requires major changes to this parallel algorithm as well as insight into the application and not just the algorithms (for profile-based load balancing) <ref> [3] </ref>. It also demonstrates how contention invoked by expensive communication and synchronization causes load imbalance. Understanding the performance bottlenecks in Shear-Warp was difficult| particularly since the warp phase is relatively insignificant in sequential execution|as was conceptualizing and implementing the optimizations [3]. 4.2.3 Raytrace Padding and Alignment: Like Volrend, padding and aligning <p> application and not just the algorithms (for profile-based load balancing) <ref> [3] </ref>. It also demonstrates how contention invoked by expensive communication and synchronization causes load imbalance. Understanding the performance bottlenecks in Shear-Warp was difficult| particularly since the warp phase is relatively insignificant in sequential execution|as was conceptualizing and implementing the optimizations [3]. 4.2.3 Raytrace Padding and Alignment: Like Volrend, padding and aligning the write-shared task queues does not help much in Ray-trace. Data Structure: Like Volrend, the final image plane is decomposed into small square tiles of pixels.
Reference: [4] <author> Lenoski D., Laudon J., Joe T., Nakahira D., Stevens L., Gupta A., and Hennessy J. </author> <title> The DASH Prototype: Implementation and Performance. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 92-103, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Every processor has separate direct-mapped 16KB first-level instruction and data caches and a unified 4-way set associative 1MB second-level cache. Caches are kept coherent across nodes by a distributed directory protocol <ref> [4] </ref>. The second-level cache line size is 64 bytes. Peak node-to-network communication bandwidth is 400MB/sec. Buffering and contention are modeled everywhere except the network links and routers. We choose speedup (over the best sequential version) as our performance metric.
Reference: [5] <author> Agarwal A. et al. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22th International Symposium on Computer Architecuture, </booktitle> <pages> pages 2-13, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Nevertheless, software communication and synchronization costs can be high as can protocol overhead, and the performance potential of this approach across a wide range of applications is not well understood. Previous research has studied parallel application performance on particular shared memory systems <ref> [11, 12, 7, 5, 17, 2, 14, 20] </ref>. Studies on shared virtual memory have largely used applications as they were written for hardware cache-coherent machines.
Reference: [6] <author> Babak Falsafi et al. </author> <title> Application-specific Protocols for Shared Memory Multiprocessors. </title> <booktitle> In Proceedings of Su-percomputing95, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: Finally, it may be interesting to examine how optimizations performed on the applications compare with the use of custom protocols to improve the performance of the original applications on commodity-based protocols <ref> [6] </ref>. Acknowledgment We would like to thank Angelos Bilas and Liviu Iftode for their help with the SVM simulators.
Reference: [7] <author> Heinrich M. et al. </author> <title> The Performance Impact of Flexibility in the Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 6th International Conference on Architecu-tural Support for Programming Language and Operating Systems, </booktitle> <pages> pages 274-285, </pages> <month> October </month> <year> 1995. </year>
Reference-contexts: Nevertheless, software communication and synchronization costs can be high as can protocol overhead, and the performance potential of this approach across a wide range of applications is not well understood. Previous research has studied parallel application performance on particular shared memory systems <ref> [11, 12, 7, 5, 17, 2, 14, 20] </ref>. Studies on shared virtual memory have largely used applications as they were written for hardware cache-coherent machines.
Reference: [8] <author> Kuskin J. et al. </author> <title> The Stanford Flash Multiprocessor. </title> <booktitle> In Proceedings of the 21th International Symposium on Computer Architecuture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Hardware cache-coherent machines have been shown to deliver good parallel performance, at least at moderate scale, but they are expensive to design and purchase. Many efforts have therefore been made to support a coherent shared address space using commodity-oriented parts for the communication architecture|both the controller and the network <ref> [8, 17] </ref>. One extreme in the spectrum is to support the abstraction entirely in software on networks of commodity workstations (or personal computers) with no additional hardware. This approach, called shared virtual memory (SVM), provides the coherent shared address space at page granularity through virtual memory management.
Reference: [9] <author> Lacroute P. G. </author> <title> Fast Volume Rendering Using a Share-Warp Factorization of the Viewing Transformation. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1995. </year>
Reference-contexts: Third, we choose applications from different domains of computation. Our application suite contains 7 applications, each with several versions. Six are originally from the SPLASH-2 [18] suite, and one is a recently published parallel shear-warp volume rendering program <ref> [9, 15] </ref> that is also described in another paper in these proceedings [3]. Let us briefly describe the access patterns in the original applications. <p> Shear-Warp renders three-dimensional volume data into an image using a shear-warp factorization algorithm. There are two phases in the rendering (see Figure 1) <ref> [15, 9] </ref>. First, the run-length encoded volume (not shown) is composited into an intermediate image, by traversing the volume in scanline order slice by slice and writing the image. <p> Read accesses are coarse grained but write accesses are fine grained and scattered. It suffers substantial false sharing at page granularity. 3 Methodology We begin with the applications as they appear in the SPLASH-2 suite (and for Shear-Warp as it appears in <ref> [9] </ref>). They are all quite well tuned for hardware cache-coherence. We perform data distribution on the SVM and DSM platforms as suggested in SPLASH-2. For LU and Ocean, we start from the "non-contiguous" versions that use 2-dimensional arrays to represent the 2-d matrix and grids.
Reference: [10] <author> Schoinas I., Falsafi B., Hill M., Larus J., Lucas C., Mukherjee S., Reinhardt S., Schnarr E., and Wood D. </author> <title> Implementing Fine-Grain Distributed Shared Memory On Commodity SMP Workstations. </title> <type> Technical Report 1307, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> March </month> <year> 1996. </year>
Reference-contexts: We would also like to look at the impact of these optimizations on systems that support fine-grained coherence with either more commodity-oriented controllers [16] or in software <ref> [10, 19] </ref>, thus completing the performance portability picture, and to enlarge our coverage by including more applications in our suite.
Reference: [11] <author> Singh J.P., Joe T., Hennessy J., and Anoop Gupta. </author> <title> An Empirical Comparison of the KSR-1 ALLCACHE and Stanford DASH Multiprocessors. </title> <booktitle> In Supercomputing '93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: Nevertheless, software communication and synchronization costs can be high as can protocol overhead, and the performance potential of this approach across a wide range of applications is not well understood. Previous research has studied parallel application performance on particular shared memory systems <ref> [11, 12, 7, 5, 17, 2, 14, 20] </ref>. Studies on shared virtual memory have largely used applications as they were written for hardware cache-coherent machines. <p> This is harder to implement than just padding/alignment. Examples include moving from two-dimensional to four-dimensional arrays to represent two-dimensional grids <ref> [11] </ref>, organizing records of particles by field rather than by particle, etc. The changes are performed to increase spatial locality, and thus to decrease fragmen tation and false sharing. * Algorithm Redesign is usually the most challenging optimization.
Reference: [12] <author> P. Keleher, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Nevertheless, software communication and synchronization costs can be high as can protocol overhead, and the performance potential of this approach across a wide range of applications is not well understood. Previous research has studied parallel application performance on particular shared memory systems <ref> [11, 12, 7, 5, 17, 2, 14, 20] </ref>. Studies on shared virtual memory have largely used applications as they were written for hardware cache-coherent machines. <p> We use 16-processor systems in each case. 2.1.1 Shared Virtual Memory Platform The shared virtual memory (SVM) platform we use simulates an all-software home-based lazy release consistency (HLRC) protocol [13], which has memory overhead and scalability advantages over non home-based protocols such as that in TreadMarks <ref> [12] </ref>. HLRC has recently been shown to equal or outperform non home-based LRC protocols as well, at least on the platform studied [21]. The simulator models an architecture of processing nodes connected by a commodity interconnect that is modeled on Myrinet [1].
Reference: [13] <author> Iftode L., Singh J. P., and Li K. </author> <title> Scope Consistency: a Bridge Between Release Consistency and Entry Consistency. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: We use 16-processor systems in each case. 2.1.1 Shared Virtual Memory Platform The shared virtual memory (SVM) platform we use simulates an all-software home-based lazy release consistency (HLRC) protocol <ref> [13] </ref>, which has memory overhead and scalability advantages over non home-based protocols such as that in TreadMarks [12]. HLRC has recently been shown to equal or outperform non home-based LRC protocols as well, at least on the platform studied [21].
Reference: [14] <author> Iftode L., Singh J. P., and Li K. </author> <title> Understanding Application Performance on Shared Virtual Memory Systems. </title> <booktitle> In Proceedings of the 23th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 122-133, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Nevertheless, software communication and synchronization costs can be high as can protocol overhead, and the performance potential of this approach across a wide range of applications is not well understood. Previous research has studied parallel application performance on particular shared memory systems <ref> [11, 12, 7, 5, 17, 2, 14, 20] </ref>. Studies on shared virtual memory have largely used applications as they were written for hardware cache-coherent machines. <p> First we use both regular and irregular applications. Second, we explore applications with a range of behaviors: different inherent communication and data referencing patterns, and different access granularities to data that interact with SVM page granularity to produce different "induced" sharing patterns <ref> [14] </ref>. By fine-grained access we mean that the accesses to data by a process are not highly spatially contiguous, which usually implies that accesses from different processes (at least compared to page size) are interleaved at quite fine granularity in the address space. As per the classification in [14], we are <p> sharing patterns <ref> [14] </ref>. By fine-grained access we mean that the accesses to data by a process are not highly spatially contiguous, which usually implies that accesses from different processes (at least compared to page size) are interleaved at quite fine granularity in the address space. As per the classification in [14], we are also concerned with how many processors write (produce) and read (consume) a unit of communication or coherence. Third, we choose applications from different domains of computation. Our application suite contains 7 applications, each with several versions.
Reference: [15] <author> Singh J. P., Gupta A., and Levoy M. </author> <title> Paralle Visualization Algorithms: Performance and Architectural Implications. </title> <journal> Computer, </journal> <volume> 27 </volume> <pages> 45-55, </pages> <year> 1994. </year>
Reference-contexts: Third, we choose applications from different domains of computation. Our application suite contains 7 applications, each with several versions. Six are originally from the SPLASH-2 [18] suite, and one is a recently published parallel shear-warp volume rendering program <ref> [9, 15] </ref> that is also described in another paper in these proceedings [3]. Let us briefly describe the access patterns in the original applications. <p> Shear-Warp renders three-dimensional volume data into an image using a shear-warp factorization algorithm. There are two phases in the rendering (see Figure 1) <ref> [15, 9] </ref>. First, the run-length encoded volume (not shown) is composited into an intermediate image, by traversing the volume in scanline order slice by slice and writing the image.
Reference: [16] <author> Pfile R. </author> <title> Typhoon-Zero Implementation: The Vortex Module. </title> <type> Technical Report CS-TR-95-1290, </type> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <month> October </month> <year> 1995. </year>
Reference-contexts: We would also like to look at the impact of these optimizations on systems that support fine-grained coherence with either more commodity-oriented controllers <ref> [16] </ref> or in software [10, 19], thus completing the performance portability picture, and to enlarge our coverage by including more applications in our suite.
Reference: [17] <author> Reinhardt S., Larus J., and Wood D. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21th International Symposium on Computer Ar-chitecuture, </booktitle> <pages> pages 325-336, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Hardware cache-coherent machines have been shown to deliver good parallel performance, at least at moderate scale, but they are expensive to design and purchase. Many efforts have therefore been made to support a coherent shared address space using commodity-oriented parts for the communication architecture|both the controller and the network <ref> [8, 17] </ref>. One extreme in the spectrum is to support the abstraction entirely in software on networks of commodity workstations (or personal computers) with no additional hardware. This approach, called shared virtual memory (SVM), provides the coherent shared address space at page granularity through virtual memory management. <p> Nevertheless, software communication and synchronization costs can be high as can protocol overhead, and the performance potential of this approach across a wide range of applications is not well understood. Previous research has studied parallel application performance on particular shared memory systems <ref> [11, 12, 7, 5, 17, 2, 14, 20] </ref>. Studies on shared virtual memory have largely used applications as they were written for hardware cache-coherent machines.
Reference: [18] <author> Woo S.C., Ohara M., Torrie E., Singh J. P., and Gupta A. </author> <title> The SPLASH-2 Programs: Characterization and Methodological Considerations. </title> <booktitle> In Proceedings of the 22th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Third, we choose applications from different domains of computation. Our application suite contains 7 applications, each with several versions. Six are originally from the SPLASH-2 <ref> [18] </ref> suite, and one is a recently published parallel shear-warp volume rendering program [9, 15] that is also described in another paper in these proceedings [3]. Let us briefly describe the access patterns in the original applications. Details of the applications themselves can be found in [18] and the references there. <p> originally from the SPLASH-2 <ref> [18] </ref> suite, and one is a recently published parallel shear-warp volume rendering program [9, 15] that is also described in another paper in these proceedings [3]. Let us briefly describe the access patterns in the original applications. Details of the applications themselves can be found in [18] and the references there. 2.2.1 Regular Applications LU performs the blocked LU factorization of a dense matrix. We begins with the non-contiguous version of LU, which uses the natural 2-d arrays to represent the 2-d matrix. <p> To reduce scattered remote writes, an alternative algorithm is to have a processor first write the output of its permutation into a local buffer, thus gathering the changes into consecutive subsequences of keys locally before writing them to the global output array in a less scattered way <ref> [18] </ref>. This improves the speedup from 1.4 to 2.24 on 6 processors, but it is still terrible. <p> Improving the performance of sorting seems to require a completely different algorithm that can use more contiguous version. patterns of access (e.g. sample sorting) or a very much larger number of keys to reduce false sharing and page granularity in the permutation <ref> [18] </ref>. As we will see later, Radix turns out to be a challenge for hardware cache-coherent machines as well. Summary: Radix is a difficult application on SVM due to the high communication traffic and contention.
Reference: [19] <author> D.J. Scales, K. Gharachorloo, and C.A. Thekkath. </author> <title> Shasta: A Low Overhead, SOftware-Only Approach for Supporting Fine-Grain Shared Memory. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Octo-ber </month> <year> 1996. </year>
Reference-contexts: We would also like to look at the impact of these optimizations on systems that support fine-grained coherence with either more commodity-oriented controllers [16] or in software <ref> [10, 19] </ref>, thus completing the performance portability picture, and to enlarge our coverage by including more applications in our suite.
Reference: [20] <author> Radhika Thekkath, Amit Pal Singh, Jaswinder Pal Singh, John L. Hennessy, and Susan John. </author> <title> An Evaluation of the Convex Exemplar SP-1200. </title> <booktitle> In Proc. Intl. Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: Nevertheless, software communication and synchronization costs can be high as can protocol overhead, and the performance potential of this approach across a wide range of applications is not well understood. Previous research has studied parallel application performance on particular shared memory systems <ref> [11, 12, 7, 5, 17, 2, 14, 20] </ref>. Studies on shared virtual memory have largely used applications as they were written for hardware cache-coherent machines.
Reference: [21] <author> Y. Zhou, L. Iftode, and K. Li. </author> <title> Performance Evaluation of Two Home-Based Lazy Release Consistency Protocols for Shared Virtual Memory Systems. </title> <booktitle> In Proceedings of the Operating Systems Design and Implementation Symposium, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: HLRC has recently been shown to equal or outperform non home-based LRC protocols as well, at least on the platform studied <ref> [21] </ref>. The simulator models an architecture of processing nodes connected by a commodity interconnect that is modeled on Myrinet [1]. In our experiments, each node can be considered to have a 200Mhz x86 processor running at 1 CPI (without memory effects). <p> At a release operation, diffs are propagated to the designated home of the page (not to the other sharers). The home copy is thus kept up to date. Upon a page fault following a causally related acquire operation, the entire page is fetched from the home <ref> [21] </ref>. 2.1.2 SGI Challenge The SGI Challenge is a bus-based, symmetric shared-memory multiprocessor with centralized main memory. The machine we use has sixteen 150Mhz processors, each with separate 16KB first-level instruction and data caches and a unified 1MB second-level cache.
References-found: 21

