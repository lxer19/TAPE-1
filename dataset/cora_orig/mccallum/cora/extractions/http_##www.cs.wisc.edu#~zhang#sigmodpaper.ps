URL: http://www.cs.wisc.edu/~zhang/sigmodpaper.ps
Refering-URL: http://www.cs.wisc.edu/~zhang/
Root-URL: 
Email: zhang@cs.wisc.edu  raghu@cs.wisc.edu  miron@cs.wisc.edu  
Title: BIRCH: An Efficient Data Clustering Method for Very Large Databases  
Author: Tian Zhang Raghu Ramakrishnan Miron Livny 
Affiliation: Computer Sciences Dept. Univ. of Wisconsin-Madison  Computer Sciences Dept. Univ. of Wisconsin-Madison  Computer Sciences Dept. Univ. of Wisconsin-Madison  
Abstract: This paper presents a data clustering method named BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), and demonstrates that it is especially suitable for very large databases. BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle "noise" (data points that are not part of the underlying pattern) effectively. We evaluate BIRCH's time/space efficiency, data input order sensitivity, and clustering quality through several experiments. We also present a performance comparisons of BIRCH versus CLARANS, a clustering method proposed recently for large datasets, and show that BIRCH is consistently superior. 
Abstract-found: 1
Intro-found: 1
Reference: [CKS88] <author> Peter Cheeseman, James Kelly, Matthew Self, et al., </author> <title> AutoClass : A Bayesian Classification System, </title> <booktitle> Proc. of the 5th Int'l Conf. on Machine Learning, </booktitle> <publisher> Morgan Kaufman, </publisher> <month> Jun. </month> <year> 1988. </year>
Reference-contexts: Finally our conclusions and directions for future research are presented in Sec. 7. 2 Summary of Relevant Research Data clustering has been studied in the Statistics [DH73, DJ80, Lee81, Mur83], Machine Learning <ref> [CKS88, Fis87, Fis95, Leb87] </ref> and Database [NH94, EKX95a, EKX95b] communities with different methods and different emphases. <p> Probability-based approaches: They typically <ref> [Fis87, CKS88] </ref> make the assumption that probability distributions on separate attributes are statistically independent of each other. In reality, this is far from true. Correlation between attributes exists, and sometimes this kind of correlation is exactly what we are looking for.
Reference: [DH73] <author> Richard Duda, and Peter E. Hart, </author> <title> Pattern Classification and Scene Analysis, </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: The details of BIRCH algorithm is described in Sec. 5, and a preliminary performance study of BIRCH is presented in Sec. 6. Finally our conclusions and directions for future research are presented in Sec. 7. 2 Summary of Relevant Research Data clustering has been studied in the Statistics <ref> [DH73, DJ80, Lee81, Mur83] </ref>, Machine Learning [CKS88, Fis87, Fis95, Leb87] and Database [NH94, EKX95a, EKX95b] communities with different methods and different emphases. <p> Hence none of them have linear time scalability with stable quality. For example, using exhaustive enumeration (EE), there are approximately K N =K! <ref> [DH73] </ref> ways of partitioning a set of N data points into K subsets. So in practice, though it can find the global minimum, it is infeasible except when N and K are extremely small. <p> For example, using exhaustive enumeration (EE), there are approximately K N =K! [DH73] ways of partitioning a set of N data points into K subsets. So in practice, though it can find the global minimum, it is infeasible except when N and K are extremely small. Iterative optimization (IO) <ref> [DH73, KR90] </ref> starts with an initial partition, then tries all possible moving or swapping of data points from one group to another to see if such a moving or swapping improves the value of the measurement function. <p> It can find a local minimum, but the quality of the local minimum is very sensitive to the initially selected partition, and the worst case time complexity is still exponential. Hierarchical clustering (HC) <ref> [DH73, KR90, Mur83] </ref> does not try to find "best" clusters, but keeps merging the closest pair (or splitting the farthest pair) of objects to form clusters. With a reasonable distance measurement, the best time complexity of a practical HC algorithm is O (N 2 ).
Reference: [DJ80] <author> R. Dubes, and A.K. Jain, </author> <title> Clustering Methodologies in Exploratory Data Analysis Advances in Computers, Edited by M.C. </title> <journal> Yovits, </journal> <volume> Vol. 19, </volume> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: Data clustering identifies the sparse and the crowded places, and hence discovers the overall distribution patterns of the dataset. Besides, the derived clusters can be visualized more efficiently and effectively than the original dataset <ref> [Lee81, DJ80] </ref>. fl This research has been supported by NSF Grant IRI-9057562 and NASA Grant 144-EC78. Generally, there are two types of attributes involved in the data to be clustered: metric and nonmetric 1 . <p> The details of BIRCH algorithm is described in Sec. 5, and a preliminary performance study of BIRCH is presented in Sec. 6. Finally our conclusions and directions for future research are presented in Sec. 7. 2 Summary of Relevant Research Data clustering has been studied in the Statistics <ref> [DH73, DJ80, Lee81, Mur83] </ref>, Machine Learning [CKS88, Fis87, Fis95, Leb87] and Database [NH94, EKX95a, EKX95b] communities with different methods and different emphases.
Reference: [EKX95a] <author> Martin Ester, Hans-Peter Kriegel, </author> <title> and Xiaowei Xu, </title>
Reference-contexts: Finally our conclusions and directions for future research are presented in Sec. 7. 2 Summary of Relevant Research Data clustering has been studied in the Statistics [DH73, DJ80, Lee81, Mur83], Machine Learning [CKS88, Fis87, Fis95, Leb87] and Database <ref> [NH94, EKX95a, EKX95b] </ref> communities with different methods and different emphases. Previous approaches, probability-based (like most approaches in Machine Learning) or distance-based (like most work in Statistics) , do not adequately consider the case that the dataset can be too large to fit in main memory. <p> CLARANS suffers from the same drawbacks as the above IO method wrt. efficiency. In addition, it may not find a real local minimum due to the searching trimming controlled by maxneighbor. Later <ref> [EKX95a] </ref> and [EKX95b] propose focusing techniques (based on R fl -trees) to improve CLARANS's ability to deal with data objects that may reside on disks by (1) clustering a sample of the dataset that is drawn from each R fl -tree data page; and (2) focusing on relevant data points for
References-found: 4

