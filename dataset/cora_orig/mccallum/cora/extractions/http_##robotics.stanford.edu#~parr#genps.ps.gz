URL: http://robotics.stanford.edu/~parr/genps.ps.gz
Refering-URL: http://robotics.stanford.edu/~parr/
Root-URL: http://www.cs.stanford.edu
Email: fdandre,nir,parrg@cs.berkeley.edu  
Title: Generalized Prioritized Sweeping  
Author: David Andre Nir Friedman Ronald Parr 
Address: 387 Soda Hall  Berkeley, CA 94720  
Affiliation: Computer Science Division,  University of California,  
Abstract: Prioritized sweeping is a model-based reinforcement learning method that attempts to focus an agent's limited computational resources to achieve a good estimate of the value of environment states. To choose effectively where to spend a costly planning step, classic prioritized sweeping uses a simple heuristic to focus computation on the states that are likely to have the largest errors. In this paper, we introduce generalized prioritized sweeping, a principled method for generating such estimates in a representation-specific manner. This allows us to extend prioritized sweeping beyond an explicit, state-based representation to deal with compact representations that are necessary for dealing with large state spaces. We apply this method for generalized model approximators (such as Bayesian networks), and describe preliminary experiments that compare our approach with classical prioritized sweeping. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Davies. </author> <title> Multidimensional triangulation and interpolation for reinforcement learning. </title> <booktitle> In Advances in Neural Information Processing Systems 9. </booktitle> <year> 1996. </year>
Reference-contexts: A crucial desideratum for reinforcement learning is the ability to scale-up to complex domains. For this, we need to use compact (or generalizing) representations of the model and the value function. While it is possible to apply PS in the presence of such representations (e.g., see <ref> [1] </ref>), we claim that classic PS is ill-suited in this case. With a generalizing model, a single experience may affect our estimation of the dynamics of many other states.
Reference: [2] <author> T. Dean and K. </author> <title> Kanazawa. A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5 </volume> <pages> 142-150, </pages> <year> 1989. </year>
Reference-contexts: If GenPS is used with an explicit state-space model and value function representation, an algorithm similar to the original (classic) PS results. When a model approximator (such as a dynamic Bayesian network <ref> [2] </ref>) is used, the resulting algorithm prioritizes the states of the environment using the generalizations inherent in the model representation. 2 The Basic Principle We assume the reader is familiar with the basic concepts of Markov Decision Processes (MDPs); see, for example, [5]. <p> Thus, we claim that classic PS is desirable primarily when explicit representations are used. 4 Factored Representation We now examine a compact representation of p (s 0 j s; a) that is based on dynamic Bayesian networks (DBNs) <ref> [2] </ref>. DBNs have been combined with reinforcement learning before in [8], where they were used primarily as a means getting better generalization while learning.
Reference: [3] <author> G. J. Gordon. </author> <title> Stable function approximation in dynamic programming. </title> <booktitle> In Proc. 12th Int. Conf. on Machine Learning, </booktitle> <year> 1995. </year>
Reference-contexts: The generalized procedure can then be applied not only in the explicit, state-based case, but in cases where approximators are used for the model. The generalized procedure also extends to cases where a function approximator (such as that discussed in <ref> [3] </ref>) is used for the value function, and future work will empirically test this application of GenPS. We are currently working on applying GenPS to other types of model and function approximators.
Reference: [4] <author> D. Heckerman. </author> <title> A tutorial on learning with Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research, </institution> <year> 1995. </year> <month> Revised November </month> <year> 1996. </year>
Reference-contexts: To simplify the discussion, we denote by Y 1 ; : : : ; Y n the agent's state after 2 Formally, we are using multinomial Dirichlet priors. See, for example, <ref> [4] </ref> for an introduction to these Bayesian methods. 3 Although @ Q (s;a) @N s;a;t involves a summation over all states, it can be computed efficiently. <p> It is often easy to assess structure information from experts even when precise probabilities are not available. As in the state-based representation, we learn the parameters using Dirichlet priors for each multinomial distribution <ref> [4] </ref>. In this method, we assess the conditional probability i;y;z using prior knowledge and the frequency of transitions observed in the past where Y i = y among those transitions where Pa i = z.
Reference: [5] <author> L. P. Kaelbling, M. L. Littman and A. W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: model approximator (such as a dynamic Bayesian network [2]) is used, the resulting algorithm prioritizes the states of the environment using the generalizations inherent in the model representation. 2 The Basic Principle We assume the reader is familiar with the basic concepts of Markov Decision Processes (MDPs); see, for example, <ref> [5] </ref>. <p> This issue, which involves the problem of exploration, is orthogonal to the our main topic. Standard approaches, such as those described in <ref> [5, 6, 7] </ref>, can be used with our procedure. This abstract description specifies neither how to update the model, nor how to update the value function in the value-propagation steps. Both of these depend on the choices made in the corresponding representation of the model and the value function. <p> The second procedure uses a factored model of the environment for learning the model parameters, but uses the same prioritization strategy as the first one. The third procedure uses the GenPS prioritization strategy we describe in Section 4. All three procedures use the Boltzman exploration strategy (see for example <ref> [5] </ref>). Finally, in each iteration these procedures process at most 10 states from the priority queue. The results are shown in Figure 1 (c). As we can see, the GenPS procedure converged faster than the procedures that used classic PS.
Reference: [6] <author> A. W. Moore and C. G. Atkeson. </author> <title> Prioritized sweepingreinforcement learning with less data and less time. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 103-130, </pages> <year> 1993. </year>
Reference-contexts: Model-free methods take one extreme on this question the agent updates only the state most recently visited. On the other end of the spectrum lie classical dynamic programming methods that reevaluate the utility of every state in the environment after every experiment. Prioritized sweeping (PS) <ref> [6] </ref> provides a middle ground in that only the most important states are updated, according to a priority metric that attempts to measure the anticipated size of the update for each state. Roughly speaking, PS interleaves performing actions in the environment with propagating the values of states. <p> This issue, which involves the problem of exploration, is orthogonal to the our main topic. Standard approaches, such as those described in <ref> [5, 6, 7] </ref>, can be used with our procedure. This abstract description specifies neither how to update the model, nor how to update the value function in the value-propagation steps. Both of these depend on the choices made in the corresponding representation of the model and the value function. <p> We used a maze domain similar to the one described in <ref> [6] </ref>. The maze, shown in Figure 1 (a), contains 59 cells, and 3 binary flags, resulting in 59 fi 2 3 = 472 possible states. Initially the agent is at the start cell (marked by S) and the flags are reset. <p> The i'th flag is set when the agent leaves the cell marked by i. The agent receives a reward when it arrives at the goal cell (marked by G) and all of the flags are set. In this situation, any action resets the game. As noted in <ref> [6] </ref>, this environment exhibits independencies. Namely, the probability of transition from one cell to another does not depend on the flag settings.
Reference: [7] <author> R. S. Sutton. </author> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Machine Learning: Proc. 7th Int. Conf., </booktitle> <year> 1990. </year>
Reference-contexts: Fortunately, we can approximate this scheme if we notice that the approximate model changes only slightly at each step. Thus, we can assume that the value function from the previous model can be easily repaired to reflect these changes. This approach was pursued in the DYNA <ref> [7] </ref> framework, where after the execution of an action, the agent updates its model of the environment, and then performs some bounded number of value propagation steps to update its approximation of the value function. <p> This issue, which involves the problem of exploration, is orthogonal to the our main topic. Standard approaches, such as those described in <ref> [5, 6, 7] </ref>, can be used with our procedure. This abstract description specifies neither how to update the model, nor how to update the value function in the value-propagation steps. Both of these depend on the choices made in the corresponding representation of the model and the value function.
Reference: [8] <author> P. Tadepalli and D. </author> <title> Ok. Scaling up average reward reinforcement learning by approximating the domain models and the value function. </title> <booktitle> In Proc. 13th Int. Conf. on Machine Learning, </booktitle> <year> 1996. </year>
Reference-contexts: Thus, we claim that classic PS is desirable primarily when explicit representations are used. 4 Factored Representation We now examine a compact representation of p (s 0 j s; a) that is based on dynamic Bayesian networks (DBNs) [2]. DBNs have been combined with reinforcement learning before in <ref> [8] </ref>, where they were used primarily as a means getting better generalization while learning. We will show that they also can be used with prioritized sweeping to focus the agent's attention on groups of states that are affected as the agent refines its environment model.
Reference: [9] <author> R. J. Williams and L. C. III Baird. </author> <title> Tight performance bounds on greedy policies based on imperfect value functions. </title> <type> Technical report, </type> <institution> Computer Science, Northeastern University. </institution> <year> 1993. </year>
Reference-contexts: The motivation for this principle is straightforward. The maximum Bellman error can be used to bound the maximum difference between the current value function, V (s) and the optimal value function, V fl (s) <ref> [9] </ref>. This difference bounds the policy loss, the difference between the expected discounted reward received under the agent's current policy and the expected discounted reward received under the optimal policy. To carry out this principle we have to recognize when the Bellman error at a state changes.
References-found: 9

