URL: http://polaris.cs.uiuc.edu/reports/1301.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Title: COMPILE-TIME ANALYSIS OF EXPLICITLY PARALLEL PROGRAMS  
Author: BY JYH-HERNG CHOW 
Degree: 1990 THESIS Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in Computer Science in the Graduate College of the  
Address: 1985  1993 Urbana, Illinois  
Affiliation: B.S., National Taiwan University,  M.S., University of Illinois at Urbana-Champaign,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [ABC + 88] <author> F. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante. </author> <title> An Overview of the PTRAN Analysis System for Multiprocessing. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 617-640, </pages> <year> 1988. </year>
Reference-contexts: Chapter 12 Related Work 12.1 Sequential Program Optimization and Parallelization Research on automatic program optimization and parallelization for sequential programs has been extensive, covering the programming languages Fortran <ref> [AK87, ABC + 88, PGH + 90] </ref>, C [Gua88, AJ88], Lisp [LH88], Scheme [Har89], and many others. To enable automatic optimization or parallelization, compilers have to capture dynamic program properties from static analyses.
Reference: [AdBKR86] <author> Pierre America, J. de Bakker, J.N. Kok, and J. Rutten. </author> <title> Operational Semantics of a Parallel Object-Oriented Language. </title> <booktitle> In ACM 13th Symposium on Principles of Programming Languages, </booktitle> <pages> pages 194-208, </pages> <year> 1986. </year>
Reference-contexts: Note that our definition of Process is much more complicated than those used in most proposed transition systems, because we are dealing with a more complex language and atomicity of a transition is modeled at a finer level. By comparison, some proposed systems (e.g., <ref> [AdBKR86] </ref>) treat every assignment statement x:=e as an atomic action, where e may be a very complex expression. They will not be acceptable if we want to analyze a realistic concurrent program. The complete semantic domains are shown in Figure 5.2.
Reference: [Agh86] <author> Gul A. Agha. </author> <title> ACTOR: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: mathematical object a process is, and this makes it difficult to justify a preference for one semantics or proof system over another. [page v] Examples include labeled transition systems [Plo83], metric space [dBZ82], synchronization tree [Win83], Petri-net [Pet77], event structures [Win89], pomset [Pra86], logics [Pnu81, Lam84], powerdomains [Plo76], Actor models <ref> [Agh86] </ref>, and process algebra [Mil80, Hen88]. The models for concurrency are often classified into interleaving or non-interleaving models.
Reference: [AH87] <editor> Samson Abramsky and Chris Hankin. </editor> <title> Abstract Interpretation of Declarative Languages. </title> <publisher> Ellis Horwood Limited, </publisher> <year> 1987. </year>
Reference-contexts: We give only a brief introduction in this section. A good source of further information on this topic can be found in <ref> [AH87] </ref> or [Bur91]. Abstract interpretation works as follows: first, we define a formal semantics, called the standard semantics, for the given language. The standard semantics defines the meaning 2.3 Abstract Interpretation 13 of a program, which all the later semantics must preserve. <p> Powerdomains are also used in relating an abstract semantics to the concrete semantics for the purpose of showing the correctness of an abstract interpretation. Most of material in this section is borrowed from <ref> [GMS89, Bur87, AH87, Bur91] </ref>. In particular, we will explain the the Hoare powerdomain, which is used in this work for both the purposes of correctness and to express concurrency. Then the Galois connection [GHK + 80] is described, which is used to establish the correctness of an abstract interpretation.
Reference: [AH90] <author> Zahira Ammarguellat and W. Ludwell Harrison III. </author> <title> Automatic Recognition of Induction Variables and Recurrence Relations by Abstract Interpretation. </title> <booktitle> In ACM Sigplan Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 283-295, </pages> <year> 1990. </year>
Reference-contexts: some of the classic optimizations [ASU86]: constant propagation common subexpression elimination code motion constant folding dead-code elimination induction-variable elimination copy propagation redundant store elimination strength reduction In general, we can design an individual abstract interpretation for each optimization (e.g., an abstract interpreter for recognition of constant expressions or induction variables <ref> [AH90] </ref>). Constant propagation and constant folding are achieved automatically during our analysis. 10.1 Existing Representation Graphs for Sequential Programs 123 The result of our abstract interpretation records the value (in an abstract domain) of every expression; thus, any expression with constant value can be replaced by the constant.
Reference: [AJ88] <author> Randy Allen and Steve Johnson. </author> <title> Compiling C for Vectorization, Parallelization, and Inline Expansion. </title> <booktitle> In ACM Sigplan Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 241-249, </pages> <year> 1988. </year>
Reference-contexts: Chapter 12 Related Work 12.1 Sequential Program Optimization and Parallelization Research on automatic program optimization and parallelization for sequential programs has been extensive, covering the programming languages Fortran [AK87, ABC + 88, PGH + 90], C <ref> [Gua88, AJ88] </ref>, Lisp [LH88], Scheme [Har89], and many others. To enable automatic optimization or parallelization, compilers have to capture dynamic program properties from static analyses.
Reference: [AK87] <author> R. Allen and K. Kennedy. </author> <title> Automatic Translation of Fortran Programs to Vector Form. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(4), </volume> <year> 1987. </year>
Reference-contexts: Chapter 12 Related Work 12.1 Sequential Program Optimization and Parallelization Research on automatic program optimization and parallelization for sequential programs has been extensive, covering the programming languages Fortran <ref> [AK87, ABC + 88, PGH + 90] </ref>, C [Gua88, AJ88], Lisp [LH88], Scheme [Har89], and many others. To enable automatic optimization or parallelization, compilers have to capture dynamic program properties from static analyses.
Reference: [AKPW83] <author> J.R. Allen, Ken Kennedy, Carrie Porterfield, and Joe Warren. </author> <title> Conversion of Control Dependence to Data Dependence. </title> <booktitle> In ACM Annual Symposium on Principles of Programming Languages, </booktitle> <pages> pages 177-189, </pages> <year> 1983. </year>
Reference-contexts: If v 2 is not control dependent on v 1 , then v 1 and v 2 can be executed in any order if there is no data dependence between them. Control dependences can be converted to data dependences <ref> [AKPW83] </ref>. Data Dependence Graphs Given the control flow graph of a program, data flow analysis is performed to compute the reaching definitions of variables for basic blocks, and from this, global def-use information.
Reference: [ASU86] <author> A.V. Aho, R. Sethi, and J.D. Ullman. </author> <booktitle> Compilers Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year> <note> 166 BIBLIOGRAPHY 167 </note>
Reference-contexts: The other cases are similar. 2 That is, a sequence of consecutive statements in which flow of control enters at the beginning and leaves at the end without halt or possibility of branching except at the end <ref> [ASU86] </ref>. 7.2 Virtual Coarsening 67 Suppose 1 = hP; i, and 1 a a 0 2 . <p> Before that, let us first consider what kinds of optimization we may want to do on programs. The following lists some of the classic optimizations <ref> [ASU86] </ref>: constant propagation common subexpression elimination code motion constant folding dead-code elimination induction-variable elimination copy propagation redundant store elimination strength reduction In general, we can design an individual abstract interpretation for each optimization (e.g., an abstract interpreter for recognition of constant expressions or induction variables [AH90]). <p> There are several program representation graphs for sequential programs, including the control flow graph <ref> [ASU86] </ref>, the data dependence graph [KKP + 81], or the program dependence graph [FOW87] which combines both graphs into a single one. To reduce the size of a representation graph, a node in the graph usually represents a basic block, i.e., a sequence of consecutive statements without branching or halt.
Reference: [BC86] <author> M. Burke and R. Cytron. </author> <title> Interprocedural Dependence Analysis and Paralleliza-tion. </title> <booktitle> In ACM SIGPLAN '86 Symp. on Compiler Construction, </booktitle> <pages> pages 162-175, </pages> <year> 1986. </year>
Reference-contexts: The difficulty occurs when we try to adapt this graph to deal with more complex languages, such as those with dynamic allocation, pointers, and first class functions. An evidence of this difficulty is the distinction of intra-procedural analysis, inter-procedure analysis <ref> [BC86] </ref>, and inter-module analysis (in the presence of concurrency) [DS91]. Some entities, such as heap objects or environments, do not even have syntactic correspondences in 3 It is like performing in-line procedure expansion first and then analyzing the results as a whole. <p> In the presence of procedure calls, interprocedural analysis <ref> [BC86] </ref> is necessary for finding possible side effects of the procedure calls, to obtain an accurate dependence analysis.
Reference: [BHA86] <author> G.L. Burn, C.L. Hankin, and S. Abramsky. </author> <title> The Theory and Practice of Strictness Analysis for Higher Order Functions. In Programs as Data Objects, </title> <booktitle> volume 217 of Lecture Notes in Computer Science, </booktitle> <pages> pages 43-62. </pages> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: Quoting from the seminal paper [CC77]: abstract interpretation provides a fundamental unity among all apparently unrelated program analysis techniques, and most program analysis techniques may be understood as abstract interpretations of programs. Since then, abstract interpretation has been used widely in analyzing functional programs, for example, strictness analysis <ref> [BHA86, Nie87] </ref>, parallel execution [Bur87], sharing and lifetime analysis [Hud87, JM89] and execution-time estimation [Ros89]. Denotational semantics is often used to define the semantics of a functional language, and this in turn has a great influence on the development of abstract interpretation, such as the use of domain theory.
Reference: [BK89] <author> Vasanth Balasundaram and Ken Kennedy. </author> <title> Compile-time Detection of Race Conditions in a Parallel Program. </title> <booktitle> In Proceedings of the 1989 International Conference on Supercomputing. Association for Computing Machinery, </booktitle> <year> 1989. </year>
Reference-contexts: This will result in a total of 25 different configurations, if we do not mind finishing the graph 3 It is interesting to compare this to the work of [CS89] or <ref> [BK89] </ref> for parallel program debugging, where a program graph is constructed in such a way that wait events and signal events are required to be the first and the last events of the nodes, respectively. <p> Several kinds of representation graphs for parallel programs have been proposed, such as module interaction graphs [DS91], task graphs [EGP89], co-graphs <ref> [BK89] </ref>, concurrency history graphs [Tay83], task interaction graphs [LC89], synchronized control flow graphs [CS89], or parallel control flow graphs [SW91, GS92]. Most of them are proposed to detect race conditions or to formulate data flow equations. <p> One approach to the static methods is based on either the formulation of data flow equations [TO80, CS89], or graph manipulation <ref> [EGP89, BK89] </ref>, from a given program flow graph which may contain some fork/join nodes and synchronization edges constraining their execution orders. One of the earliest works to take this approach is by Taylor and Osterweil [TO80]. <p> The data flow formulation has been adapted in the work of Duesterwald and Soffa [DS91], and Grunwald and Srinivasan [GS92]. Instead of using data flow analysis for determining concurrent statements, explicit con-currency graphs are constructed in Emrath, Ghosh, and Padua [EGP89], and Balasundaram and Kennedy <ref> [BK89] </ref>. One concern here is the pairing of signal and wait events as synchronization edges. In [EGP89], the task graph is constructed where synchronization edges from the nearest common ancestors of the signals that might trigger a wait to the wait are added iteratively. In [BK89], the co-graph is constructed where <p> [EGP89], and Balasundaram and Kennedy <ref> [BK89] </ref>. One concern here is the pairing of signal and wait events as synchronization edges. In [EGP89], the task graph is constructed where synchronization edges from the nearest common ancestors of the signals that might trigger a wait to the wait are added iteratively. In [BK89], the co-graph is constructed where two blocks that might be concurrent are connected. They also discuss subscripted event variables within loops. Another major approach to static methods is based on state space generation, where all possible concurrent states of a program are examined [Tay83, McD89].
Reference: [Bou90] <author> F. Bourdoncle. </author> <title> Interprocedural Abstract Interpretation of Block Structured Languages with Nested Procedures, Aliasing and Recursivity. </title> <booktitle> In Programming Language Implementation and Logic Programming, volume 456 of Lecture Notes in Computer Science, </booktitle> <pages> pages 307-323. </pages> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: abstract interpretation, Like the original work by Cousot and Cousot [CC77, CC79], abstract interpretation has also been used in analyzing non-functional programs for obtaining store-related properties, for example, sharing and lifetime analysis [Har89, Deu90, Deu92], determining data dependence [Har89, HPR89], symbolic evaluation [HP92], and eliminating run-time tests on array bounds <ref> [Bou90] </ref>. In this imperative world, operational semantics is often used to define the language semantics; in particular, transition systems are used in the work of Cousot [CC77, CC79, CC92] and Deutsch [Deu90, Deu92], where sets are used in their development, instead of the more complicated domain.
Reference: [BRW84] <editor> S. Brookes, A.W. Roscoe, and G. Winskel, editors. </editor> <booktitle> Semantics on Concurrency, volume 197 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Although many mathematical models for describing concurrent systems have been proposed, each model seems to have arisen in an attempt to capture precisely a particular type of behavior. The following statement from the introduction in <ref> [BRW84] </ref> illustrates this situation: There is a lack of agreement at the basic level of what kind of mathematical object a process is, and this makes it difficult to justify a preference for one semantics or proof system over another. [page v] Examples include labeled transition systems [Plo83], metric space [dBZ82],
Reference: [Bur87] <author> Geoffery L. Burn. </author> <title> Abstract Interpretation and the Parallel Evaluation of Functional Languages. </title> <type> PhD thesis, </type> <institution> University of London, </institution> <year> 1987. </year>
Reference-contexts: Third, any optimization should preserve the semantics of source programs. Because abstract interpretation is based on a formal mathematic/semantic model, the correctness of analysis (i.e., its consistency with the language semantics) can be proved formally and easily if we follow some existing framework, for example <ref> [Bur87] </ref>. 2.4 Abstract Interpretation vs. <p> Powerdomains are also used in relating an abstract semantics to the concrete semantics for the purpose of showing the correctness of an abstract interpretation. Most of material in this section is borrowed from <ref> [GMS89, Bur87, AH87, Bur91] </ref>. In particular, we will explain the the Hoare powerdomain, which is used in this work for both the purposes of correctness and to express concurrency. Then the Galois connection [GHK + 80] is described, which is used to establish the correctness of an abstract interpretation. <p> 2 X; X 2 Qg = S 2 Define F 22 Domains Another operator we will use is: F F F Proposition 3.21 (1) fj:jg is continuous. (2) is continuous, and for f : D!E, S E ffi IPIPf = IPf ffi S (3) is continuous, and F Proof: See <ref> [Bur87] </ref>. 2 Moreover, F F X for finite complete lattices. 3.3 Galois Connections The material in this section is from [GHK + 80]. Definition 3.22 (Galois connection) Let S and T be two posets. <p> The Hoare powerdomain naturally captures the idea of sets of elements with a certain level of definedness which is what is needed for our application <ref> [Bur87] </ref>. <p> It is interesting to notice that the design of abstraction for powerdomains describing concurrency semantics is exactly the same problem as that of abstracting a collecting semantics, often used for analyzing sequential programs (e.g. <ref> [Bur87] </ref>). Suppose E = IP (D) is a Hoare powerdomain, and D is the abstract domain of D with adjoined maps Abs D : D!D and Conc D : D!IP (D). Following the work of Burn [Bur87], define the following abstraction map for the abstract domain E (recall Section 3.2: X <p> as that of abstracting a collecting semantics, often used for analyzing sequential programs (e.g. <ref> [Bur87] </ref>). Suppose E = IP (D) is a Hoare powerdomain, and D is the abstract domain of D with adjoined maps Abs D : D!D and Conc D : D!IP (D). Following the work of Burn [Bur87], define the following abstraction map for the abstract domain E (recall Section 3.2: X #= fyjyvx; x 2 Xg, and S S F F in the Hoare powerdomain): Abs E : IP (D)!D F fAbs D djd 2 Xg # = X: fAbs D djd 2 Xg ( F F <p> Proposition 8.3 Abs E and Conc E are continuous. And, (1) Abs E (Conc E (e)) = e, 8e 2 E (2) Conc E (Abs E (e)) w e, 8e 2 E Proof: See the proof in Burn <ref> [Bur87] </ref>. The map Abs E is onto if Abs D is onto and D is a complete lattice. 2 For these abstraction and concretization maps, equation (8.2) is also true. The 2 in Proposition 8.2, equation 8.3, and equation 8.4, should be replaced by . <p> The 2 in Proposition 8.2, equation 8.3, and equation 8.4, should be replaced by . Now, these forms are exactly those appeared in <ref> [CC77, Bur87] </ref> for relating the abstract semantics to the collecting semantics. Note, however, that the abstraction Abs E : IP (D)!D may cause significant information loss. Sometimes a more accurate mapping is preferred. <p> Proposition 8.12 The above abstraction map for PossConf and the concretization map in equation (8.8) are adjoined. Proof: See a similar proof in <ref> [Bur87] </ref> (Proposition 2.5.1). 2 The effect of this abstraction on the state space is the following: Observation 8.13 The effect of this abstraction is that the number of configurations in evaluating a cobegin with n branches, each with m i atomic actions, is bounded by m 1 fi m 2 fi <p> Since then, abstract interpretation has been used widely in analyzing functional programs, for example, strictness analysis [BHA86, Nie87], parallel execution <ref> [Bur87] </ref>, sharing and lifetime analysis [Hud87, JM89] and execution-time estimation [Ros89]. Denotational semantics is often used to define the semantics of a functional language, and this in turn has a great influence on the development of abstract interpretation, such as the use of domain theory. <p> Denotational semantics is often used to define the semantics of a functional language, and this in turn has a great influence on the development of abstract interpretation, such as the use of domain theory. Burn's thesis <ref> [Bur87, Bur91] </ref> proposes a framework where the user provides only the lattices and maps for basic domains, and the framework guarantees the correctness of an abstract interpretation, Like the original work by Cousot and Cousot [CC77, CC79], abstract interpretation has also been used in analyzing non-functional programs for obtaining store-related properties,
Reference: [Bur91] <author> Geoffery Burn. </author> <title> Lazy Functional Languages: Abstract Interpretation and Compilation. </title> <booktitle> Research Monographs in Parallel and Distributed Computing. </booktitle> <publisher> The MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: We give only a brief introduction in this section. A good source of further information on this topic can be found in [AH87] or <ref> [Bur91] </ref>. Abstract interpretation works as follows: first, we define a formal semantics, called the standard semantics, for the given language. The standard semantics defines the meaning 2.3 Abstract Interpretation 13 of a program, which all the later semantics must preserve. <p> Powerdomains are also used in relating an abstract semantics to the concrete semantics for the purpose of showing the correctness of an abstract interpretation. Most of material in this section is borrowed from <ref> [GMS89, Bur87, AH87, Bur91] </ref>. In particular, we will explain the the Hoare powerdomain, which is used in this work for both the purposes of correctness and to express concurrency. Then the Galois connection [GHK + 80] is described, which is used to establish the correctness of an abstract interpretation. <p> Denotational semantics is often used to define the semantics of a functional language, and this in turn has a great influence on the development of abstract interpretation, such as the use of domain theory. Burn's thesis <ref> [Bur87, Bur91] </ref> proposes a framework where the user provides only the lattices and maps for basic domains, and the framework guarantees the correctness of an abstract interpretation, Like the original work by Cousot and Cousot [CC77, CC79], abstract interpretation has also been used in analyzing non-functional programs for obtaining store-related properties,
Reference: [CC77] <author> Patrick Cousot and Radhia Cousot. </author> <title> Abstract Interpretation: A Unified Lattice Model for Static Analysis of Program by Construction or Approximation of Fixpoints. </title> <booktitle> In ACM 4th Symposium on Principles of Programming Languages, </booktitle> <pages> pages 238-252, </pages> <year> 1977. </year>
Reference-contexts: Our framework is based on abstract interpretation <ref> [CC77] </ref>, which is a semantics-based technique for static program analysis. To study the effects of concurrency mixed with other language features, the language of this work supports dynamic allocations, pointers, first-class functions, and cobegin parallelism which can be nested. <p> Compiler techniques that guarantee program transformations preserve all possible results will be discussed in Chapter 10. 2.3 Abstract Interpretation This work takes a semantic-based approach to the program analysis problem by using abstract interpretation <ref> [CC77] </ref>. We give only a brief introduction in this section. A good source of further information on this topic can be found in [AH87] or [Bur91]. Abstract interpretation works as follows: first, we define a formal semantics, called the standard semantics, for the given language. <p> Both maps are required to be monotonic and to assure the correctness of an abstract interpretation. The correspondence between these two are established by showing the adjoined functions property (that is, the Galois connection [GHK + 80] described in Section 3.3) <ref> [CC77] </ref>: Property 8.1 (adjoined functions) Let D and D be posets. <p> The 2 in Proposition 8.2, equation 8.3, and equation 8.4, should be replaced by . Now, these forms are exactly those appeared in <ref> [CC77, Bur87] </ref> for relating the abstract semantics to the collecting semantics. Note, however, that the abstraction Abs E : IP (D)!D may cause significant information loss. Sometimes a more accurate mapping is preferred. <p> In this approach we collect program properties by simulating the program execution through the semantics in an abstract domain. The abstract interpretation technique opens a rich dimension for generalizing program analyses and establishes a sound foundation for proving the correctness of analyses. Quoting from the seminal paper <ref> [CC77] </ref>: abstract interpretation provides a fundamental unity among all apparently unrelated program analysis techniques, and most program analysis techniques may be understood as abstract interpretations of programs. <p> Burn's thesis [Bur87, Bur91] proposes a framework where the user provides only the lattices and maps for basic domains, and the framework guarantees the correctness of an abstract interpretation, Like the original work by Cousot and Cousot <ref> [CC77, CC79] </ref>, abstract interpretation has also been used in analyzing non-functional programs for obtaining store-related properties, for example, sharing and lifetime analysis [Har89, Deu90, Deu92], determining data dependence [Har89, HPR89], symbolic evaluation [HP92], and eliminating run-time tests on array bounds [Bou90]. <p> In this imperative world, operational semantics is often used to define the language semantics; in particular, transition systems are used in the work of Cousot <ref> [CC77, CC79, CC92] </ref> and Deutsch [Deu90, Deu92], where sets are used in their development, instead of the more complicated domain. Unlike the traditional approaches, since abstract interpretation simulates a program execution, there is no distinction between intraprocedural and interprocedural analysis. <p> His work addresses only the constant propagation problem. The work of Cousot and Cousot [CC84] presents an invariance proof system for shared memory parallel programs. It discusses the extension of their abstract interpretation framework <ref> [CC77, CC79] </ref> to parallel program analysis and shows that the same framework applies equally well to parallel programs as to sequential programs. The semantics of a parallel program is based also on transition systems. However, they do not mention the practical issue about how to design an effective abstraction.
Reference: [CC79] <author> Patrick Cousot and Radhia Cousot. </author> <title> Systematic Design of Program Analysis Frameworks. </title> <booktitle> In ACM 6th Symposium on Principles of Programming Languages, </booktitle> <pages> pages 269-282, </pages> <year> 1979. </year>
Reference-contexts: We shall keep in mind what information we want, and how much of the information will be lost when an abstraction is taken. Some useful abstraction methods can be found in Cousot and Cousot <ref> [CC79] </ref>, and Deutsch [Deu90, Deu92]. There are two important issues in designing an abstract semantics: correctness and termination of the abstract interpretation. <p> Burn's thesis [Bur87, Bur91] proposes a framework where the user provides only the lattices and maps for basic domains, and the framework guarantees the correctness of an abstract interpretation, Like the original work by Cousot and Cousot <ref> [CC77, CC79] </ref>, abstract interpretation has also been used in analyzing non-functional programs for obtaining store-related properties, for example, sharing and lifetime analysis [Har89, Deu90, Deu92], determining data dependence [Har89, HPR89], symbolic evaluation [HP92], and eliminating run-time tests on array bounds [Bou90]. <p> In this imperative world, operational semantics is often used to define the language semantics; in particular, transition systems are used in the work of Cousot <ref> [CC77, CC79, CC92] </ref> and Deutsch [Deu90, Deu92], where sets are used in their development, instead of the more complicated domain. Unlike the traditional approaches, since abstract interpretation simulates a program execution, there is no distinction between intraprocedural and interprocedural analysis. <p> His work addresses only the constant propagation problem. The work of Cousot and Cousot [CC84] presents an invariance proof system for shared memory parallel programs. It discusses the extension of their abstract interpretation framework <ref> [CC77, CC79] </ref> to parallel program analysis and shows that the same framework applies equally well to parallel programs as to sequential programs. The semantics of a parallel program is based also on transition systems. However, they do not mention the practical issue about how to design an effective abstraction.
Reference: [CC80] <author> Patrick Cousot and Radhia Cousot. </author> <title> Semantic Analysis of Communicating Sequential Processes. </title> <booktitle> In International Conference on Automata, Languages and Programming, volume 85 of Lecture Notes in Computer Science, </booktitle> <pages> pages 119-133, </pages> <year> 1980. </year>
Reference-contexts: Moreover, it is the first application of the abstract interpretation technique to parallel 8 Introduction programs with complex constructs. Although abstract interpretation has been applied to analyzing parallel programs before (e.g., <ref> [CC80, CC84, Mer91] </ref>), their languages either do not allow task interaction through shared variables, or do not consider the above constructs. Neither do they address the state space explosion problem associated with the abstract interpretation technique. <p> Abstract interpretation has been applied to parallel programs as well. Cousot and Cousot present an invariance proof system for communicating sequential processes <ref> [CC80] </ref> and shared memory parallel programs [CC84]. Both can be used for program optimization. However, the language (CSP) used in [CC80] does not consider task interaction through shared variables, and the language used in [CC84] considers only simple constructs. <p> Abstract interpretation has been applied to parallel programs as well. Cousot and Cousot present an invariance proof system for communicating sequential processes <ref> [CC80] </ref> and shared memory parallel programs [CC84]. Both can be used for program optimization. However, the language (CSP) used in [CC80] does not consider task interaction through shared variables, and the language used in [CC84] considers only simple constructs. Neither of them addresses the state space explosion problem assoicated with the abstract interpretation technique. <p> Each message channel is associated with a variable that records all the values transmitted over the channel during the lifetime of the program. Cousot and Cousot <ref> [CC80] </ref> present an invariance proof method for communicating processes, which also can be used for automatic program optimization by abstract interpretation.
Reference: [CC84] <author> Patrick Cousot and Radhia Cousot. </author> <title> Invariance Proof Methods and Analysis Techniques for Parallel Programs. </title> <editor> In A.W. Biermann, G. Guiho, and 168 BIBLIOGRAPHY Y. Kodratoff, editors, </editor> <title> Automatic Program Construction Techniques, </title> <booktitle> chapter 12, </booktitle> <pages> pages 243-271. </pages> <publisher> Macmillan Publishing Company, </publisher> <year> 1984. </year>
Reference-contexts: Moreover, it is the first application of the abstract interpretation technique to parallel 8 Introduction programs with complex constructs. Although abstract interpretation has been applied to analyzing parallel programs before (e.g., <ref> [CC80, CC84, Mer91] </ref>), their languages either do not allow task interaction through shared variables, or do not consider the above constructs. Neither do they address the state space explosion problem associated with the abstract interpretation technique. <p> Abstract interpretation has been applied to parallel programs as well. Cousot and Cousot present an invariance proof system for communicating sequential processes [CC80] and shared memory parallel programs <ref> [CC84] </ref>. Both can be used for program optimization. However, the language (CSP) used in [CC80] does not consider task interaction through shared variables, and the language used in [CC84] considers only simple constructs. Neither of them addresses the state space explosion problem assoicated with the abstract interpretation technique. <p> Cousot and Cousot present an invariance proof system for communicating sequential processes [CC80] and shared memory parallel programs <ref> [CC84] </ref>. Both can be used for program optimization. However, the language (CSP) used in [CC80] does not consider task interaction through shared variables, and the language used in [CC84] considers only simple constructs. Neither of them addresses the state space explosion problem assoicated with the abstract interpretation technique. <p> Ghosh [Gho92], based on earlier work on detecting race conditions [EGP89], develops data flow equations for scalar optimization of parallel programs, by constructing a concurrent control flow graph for programs with simple constructs. His work addresses only the constant propagation problem. The work of Cousot and Cousot <ref> [CC84] </ref> presents an invariance proof system for shared memory parallel programs. It discusses the extension of their abstract interpretation framework [CC77, CC79] to parallel program analysis and shows that the same framework applies equally well to parallel programs as to sequential programs.
Reference: [CC92] <author> Patrick Cousot and Radhia Cousot. </author> <title> Abstract Interpretation Frameworks. </title> <journal> Journal of Logic and Computation, </journal> <note> 1992. Also Ecole Polytechnique report LIX/RR/92/10. </note>
Reference-contexts: Again, the abstract semantics must be consistent with the previous semantics; namely, it must be safe in that any conclusion we draw must be weaker (vaguer) than the conclusion that could be made from the previous semantics. The design of this phase usually takes place in the following phases <ref> [CC92] </ref>: choosing abstract semantic domains that are approximate versions of the concrete semantic domains, defining the abstract semantics, specifying the soundness correspondence between the concrete and abstract properties, ensuring the termination of abstract interpreter, and finally choosing a solution that leads to rapid termination. <p> a pair of adjoined functions 3 iff Abs and Conc are monotonic, and (1) Abs (Conc (d)) = d, 8d 2 D (2) Conc (Abs (d)) w fdg, 8d 2 D Several other formulation for connecting abstract semantics and concrete semantics in abstract interpretation frameworks have been extensively discussed in <ref> [CC92] </ref>. The following proposition is useful. <p> In this imperative world, operational semantics is often used to define the language semantics; in particular, transition systems are used in the work of Cousot <ref> [CC77, CC79, CC92] </ref> and Deutsch [Deu90, Deu92], where sets are used in their development, instead of the more complicated domain. Unlike the traditional approaches, since abstract interpretation simulates a program execution, there is no distinction between intraprocedural and interprocedural analysis.
Reference: [CCF91] <author> Jong-Deok Choi, Ron Cytron, and Jeanne Ferrante. </author> <title> Automatic Construction of Sparse Data Flow Evaluation Graphs. </title> <booktitle> In ACM 18th Symposium on Principles of Programming Languages, </booktitle> <pages> pages 55-66, </pages> <year> 1991. </year>
Reference-contexts: To accelerate the fix-point computation, some form of flow graph (e.g., the entailment graph in [CH92a]), may be constructed internally during semantic evaluation for fast propagation, similar to the role played by def-use chains (or the sparse data flow evaluation graph in <ref> [CCF91] </ref>) in traditional techniques. Difficult language constructs cause no fundamental changes to the framework; once the language semantics is defined, existing abstract interpretation techniques will allow us to develop correct program analyses.
Reference: [CGL92] <author> Edmund M. Clarke, Orna Grumberg, and David E. </author> <title> Long. Model Checking and Abstraction. </title> <booktitle> In ACM 19th Symposium on Principles of Programming Languages, </booktitle> <pages> pages 343-354, </pages> <year> 1992. </year>
Reference-contexts: only if processes are loosely-coupled. 12.6 State Space Reduction The state space explosion problem is inevitable in almost any analysis and verification method based on the examination of all possible states that a program can reach, for example, parallel program verification [Lip75, Ove81, Tay83], Petri-net reachability analysis [Val88], model checking <ref> [CGL92] </ref> (whether specific properties are satisfied by a system), circuit verification [GGS88], and communication protocol verification [CPS90, LL92], some of which focus on finite state models, and use process algebra or temporal logic to specify a system. <p> A tutorial on this theory and its comparison to other recent related works can be found in [Val92]. In particular, a similar idea was presented by Godefroid [God90], which is based on Mazurkiewicz's trace theory [Maz86]. The work by Clarke, Grumberg, and Long <ref> [CGL92] </ref> on model checking makes use of boolean decision diagrams (BDD), a concise representation of states allowing efficient manipulation, and state abstraction, very similar to our abstract interpretation, to reduce the state space complexity.
Reference: [CH92a] <author> Li-Ling Chen and W. Ludwell Harrison III. </author> <title> Efficient Computation of the Fixpoints That Arise in Complex Program Analysis. </title> <type> Technical Report 1245, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: These basic domains replace the control-flow graph as the underlying `bases' of a program execution. To accelerate the fix-point computation, some form of flow graph (e.g., the entailment graph in <ref> [CH92a] </ref>), may be constructed internally during semantic evaluation for fast propagation, similar to the role played by def-use chains (or the sparse data flow evaluation graph in [CCF91]) in traditional techniques. <p> Effective fixpoint computation: The current algorithm uses a simple LIFO schedule to evaluate the elements in the worklist. By utilizing the dependency information among expressions, an intelligent schedule can be more effective <ref> [CH92a] </ref>. Also, as there appears to be plenty of parallelism in collecting results from interleavings, the fixpoint computation itself may be parallelized to run effectively in a parallel environment. 4.
Reference: [CH92b] <author> Jyh-Herng Chow and Williams Ludwell Harrison III. </author> <title> A General Framework for Analyzing Shared-Memory Parallel Programs. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing - Vol II Software, </booktitle> <pages> pages 192-199, </pages> <year> 1992. </year>
Reference-contexts: For cobegin-coend process synchronization, a program edge is added from the fork node to the beginning of each thread, and also from the end of each thread to the join node. We assume the relation P and C are known for a given program (by the analyses of <ref> [CH92b, CH92c] </ref>, for example). In an architecture where sequential consistency is not enforced, the order specified by P may be violated in an actual execution. <p> The result of abstract interpretation also indicates which statements will never be executed (those having ? as their states after an abstract interpretation <ref> [CH92c, CH92b] </ref>). Thus, dead code can be eliminated. The constraints in a PPDG indicate whether two statements can be executed in a particular order: the compiler can apply code motion optimization to statements that are not related in the PPDG.
Reference: [CH92c] <author> Jyh-Herng Chow and Williams Ludwell Harrison III. </author> <title> Compile-Time Analysis of Parallel Programs that Share Memory. </title> <booktitle> In ACM 19th Symposium on Principles of Programming Languages, </booktitle> <pages> pages 130-141, </pages> <year> 1992. </year>
Reference-contexts: For cobegin-coend process synchronization, a program edge is added from the fork node to the beginning of each thread, and also from the end of each thread to the join node. We assume the relation P and C are known for a given program (by the analyses of <ref> [CH92b, CH92c] </ref>, for example). In an architecture where sequential consistency is not enforced, the order specified by P may be violated in an actual execution. <p> The result of abstract interpretation also indicates which statements will never be executed (those having ? as their states after an abstract interpretation <ref> [CH92c, CH92b] </ref>). Thus, dead code can be eliminated. The constraints in a PPDG indicate whether two statements can be executed in a particular order: the compiler can apply code motion optimization to statements that are not related in the PPDG.
Reference: [CKS90] <author> David Callahan, Ken Kennedy, and Jaspal Subhlok. </author> <title> Analysis of Event Synchronization in A Parallel Programming Tool. </title> <booktitle> In ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming, </booktitle> <pages> pages 21-30, </pages> <year> 1990. </year>
Reference-contexts: However, they assume the existence of an algorithm to determine whether two statements can be concurrent, and the problem of building a correct graph is not addressed. The work of Callahan and Subhlok <ref> [CS89, CKS90] </ref> on parallel program debugging considers only serializable programs, where a parallel program has a corresponding sequential semantics (parallelism is present only for performance reasons). Thus, the correctness of a parallel program is defined with respect to its sequential execution.
Reference: [CM89] <author> K. Mani Chandy and Jayadev Misra. </author> <title> Parallel Program Design A Foundation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1989. </year>
Reference-contexts: Overman's thesis [Ove81] proposes an algorithm which is essentially the stubborn set method applied to the shared memory programming model. The stubborn set method 3 We should also mention the UNITY model by Chandy and Misra <ref> [CM89] </ref>.
Reference: [CPS90] <author> Rance Cleaveland, Joachim Parrow, and Bernhard Steffen. </author> <title> A Semantics-Based Verification Tool for Finite-State Systems. In Protocol Specification, Testing, and Verification, </title> <booktitle> IX, </booktitle> <pages> pages 287-302. </pages> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference-contexts: in almost any analysis and verification method based on the examination of all possible states that a program can reach, for example, parallel program verification [Lip75, Ove81, Tay83], Petri-net reachability analysis [Val88], model checking [CGL92] (whether specific properties are satisfied by a system), circuit verification [GGS88], and communication protocol verification <ref> [CPS90, LL92] </ref>, some of which focus on finite state models, and use process algebra or temporal logic to specify a system. Virtual coarsening, which has long been used in many analysis tools, attempts to create coarser granularity in order to reduce the number of interleavings [Pnu86].
Reference: [CS89] <author> David Callahan and Jaspal Subhlok. </author> <title> Static Analysis of Low-level Synchronization. </title> <booktitle> In Proc. of the ACM SIGPLAN/SIGOPS Workshop on Parallel and Distributed Debugging, </booktitle> <pages> pages 100-111, </pages> <year> 1989. </year> <note> BIBLIOGRAPHY 169 </note>
Reference-contexts: The idea of virtual coarsening has been widely used, disguised behind different faces, for simplifying program analysis and program verification. For instance, to construct the 66 State Space Reduction concurrency graph for parallel program debugging, one usually defines nodes to contain at most one explicit synchronization (e.g., <ref> [CS89] </ref>). A similar idea in most compilers is the use of basic blocks, 2 which have only one straight control flow, as units in building a program control flow graph. <p> This will result in a total of 25 different configurations, if we do not mind finishing the graph 3 It is interesting to compare this to the work of <ref> [CS89] </ref> or [BK89] for parallel program debugging, where a program graph is constructed in such a way that wait events and signal events are required to be the first and the last events of the nodes, respectively. <p> Several kinds of representation graphs for parallel programs have been proposed, such as module interaction graphs [DS91], task graphs [EGP89], co-graphs [BK89], concurrency history graphs [Tay83], task interaction graphs [LC89], synchronized control flow graphs <ref> [CS89] </ref>, or parallel control flow graphs [SW91, GS92]. Most of them are proposed to detect race conditions or to formulate data flow equations. <p> One approach to the static methods is based on either the formulation of data flow equations <ref> [TO80, CS89] </ref>, or graph manipulation [EGP89, BK89], from a given program flow graph which may contain some fork/join nodes and synchronization edges constraining their execution orders. One of the earliest works to take this approach is by Taylor and Osterweil [TO80]. <p> However, they assume the existence of an algorithm to determine whether two statements can be concurrent, and the problem of building a correct graph is not addressed. The work of Callahan and Subhlok <ref> [CS89, CKS90] </ref> on parallel program debugging considers only serializable programs, where a parallel program has a corresponding sequential semantics (parallelism is present only for performance reasons). Thus, the correctness of a parallel program is defined with respect to its sequential execution. <p> The same semantics is assumed in the work of Grunwald and Srinivasan [GS92], where traditional data flow equations for computing reaching definitions are defined for parallel programs with post and wait synchronization. It makes use of the formulation of preserved sets defined in <ref> [CS89] </ref>. The work by Duesterwald and Soffa [DS91] presents a data flow framework for computing the happen-before and happen-after relations, in the presence of procedures. Their framework first constructs a module interaction graph; the work considers only rendezvous synchronization.
Reference: [Cyt86] <author> Ron Cytron. </author> <title> On the Implications of Parallel Languages for Compilers. </title> <type> Technical Report RC-11723, </type> <institution> IBM T.J. Watson Research Center, </institution> <month> April </month> <year> 1986. </year>
Reference-contexts: The work by Long and Clarke [LC89] proposes the task interaction graph for building a concurrency graph, which tries to reduce the number of state nodes in Taylor's algorithm. It considers only rendezvous synchronization and assumes no shared data among tasks. 12.5 Analysis and Optimization of Parallel Programs Cytron <ref> [Cyt86] </ref> advocates that new compiler techniques must be developed to accommodate features found in parallel languages. The paper gives some examples that illustrate the need for compile-time analysis techniques for parallel programs. However, no methods are proposed.
Reference: [dBKM + 86] <author> J.W. de Bakker, J.N. Kok, J.-J.Ch. Meyer, E.-R. Olderog, and J.I. Zucker. </author> <title> Contrasting Themes in the Semantics of Imperative Concurrency. </title> <booktitle> In Current Thrends in Concurrency, volume 224 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: Another extensive survey of concurrency semantics is given by de Bakker et al., <ref> [dBKM + 86] </ref>, which focuses on models based on interleaving semantics. 12.4 Static Analysis for Debugging Parallel Programs There are several program properties often considered as bugs in a parallel program, for example, deadlock or nondeterminism, which do not exist in sequential programs.
Reference: [dBZ82] <author> J.W. de Bakker and J.I. Zucker. </author> <title> Processes and the Denotational Semantics of Concurrency. </title> <journal> Information and Control, </journal> <volume> 54 </volume> <pages> 70-120, </pages> <year> 1982. </year>
Reference-contexts: [BRW84] illustrates this situation: There is a lack of agreement at the basic level of what kind of mathematical object a process is, and this makes it difficult to justify a preference for one semantics or proof system over another. [page v] Examples include labeled transition systems [Plo83], metric space <ref> [dBZ82] </ref>, synchronization tree [Win83], Petri-net [Pet77], event structures [Win89], pomset [Pra86], logics [Pnu81, Lam84], powerdomains [Plo76], Actor models [Agh86], and process algebra [Mil80, Hen88]. The models for concurrency are often classified into interleaving or non-interleaving models.
Reference: [Deu90] <author> Alain Deutsch. </author> <title> On Determining Lifetime and Aliasing of Dynamically Allocated Data in Higher-order Functional Specifications. </title> <booktitle> In ACM 17th Symposium on Principles of Programming Languages, </booktitle> <pages> pages 157-168, </pages> <year> 1990. </year>
Reference-contexts: We shall keep in mind what information we want, and how much of the information will be lost when an abstraction is taken. Some useful abstraction methods can be found in Cousot and Cousot [CC79], and Deutsch <ref> [Deu90, Deu92] </ref>. There are two important issues in designing an abstract semantics: correctness and termination of the abstract interpretation. <p> user provides only the lattices and maps for basic domains, and the framework guarantees the correctness of an abstract interpretation, Like the original work by Cousot and Cousot [CC77, CC79], abstract interpretation has also been used in analyzing non-functional programs for obtaining store-related properties, for example, sharing and lifetime analysis <ref> [Har89, Deu90, Deu92] </ref>, determining data dependence [Har89, HPR89], symbolic evaluation [HP92], and eliminating run-time tests on array bounds [Bou90]. <p> In this imperative world, operational semantics is often used to define the language semantics; in particular, transition systems are used in the work of Cousot [CC77, CC79, CC92] and Deutsch <ref> [Deu90, Deu92] </ref>, where sets are used in their development, instead of the more complicated domain. Unlike the traditional approaches, since abstract interpretation simulates a program execution, there is no distinction between intraprocedural and interprocedural analysis.
Reference: [Deu92] <author> Alain Deutsch. </author> <title> Operational Models of Programming Languages and Representations of Relations on Regular Languages with Application to the Static Determination of Dynamic Aliasing Properties of Data. </title> <type> PhD thesis, </type> <institution> Universite Paris VI, </institution> <year> 1992. </year>
Reference-contexts: We shall keep in mind what information we want, and how much of the information will be lost when an abstraction is taken. Some useful abstraction methods can be found in Cousot and Cousot [CC79], and Deutsch <ref> [Deu90, Deu92] </ref>. There are two important issues in designing an abstract semantics: correctness and termination of the abstract interpretation. <p> user provides only the lattices and maps for basic domains, and the framework guarantees the correctness of an abstract interpretation, Like the original work by Cousot and Cousot [CC77, CC79], abstract interpretation has also been used in analyzing non-functional programs for obtaining store-related properties, for example, sharing and lifetime analysis <ref> [Har89, Deu90, Deu92] </ref>, determining data dependence [Har89, HPR89], symbolic evaluation [HP92], and eliminating run-time tests on array bounds [Bou90]. <p> In this imperative world, operational semantics is often used to define the language semantics; in particular, transition systems are used in the work of Cousot [CC77, CC79, CC92] and Deutsch <ref> [Deu90, Deu92] </ref>, where sets are used in their development, instead of the more complicated domain. Unlike the traditional approaches, since abstract interpretation simulates a program execution, there is no distinction between intraprocedural and interprocedural analysis.
Reference: [Dij76] <author> E.W. Dijkstra. </author> <title> A Discipline of Programming. </title> <publisher> Prentice Hall, </publisher> <year> 1976. </year>
Reference-contexts: Because we expect such algorithms to be programmed, we prefer to have a more general framework that can be specialized to various simpler problems 1 Nondeterminism may also result from the execution of a nondeterministic construct such as guarded commands <ref> [Dij76] </ref>; however, that kind of constructs is not considered in this work. 9 10 Background than to have a special framework and later consider how to generalize it to deal with more complex problems. <p> Given a configuration , each enabled event a i in will result in a new configuration i , where T ( ; a i ) = i . We should point out that in our language, every process is determinate (there is no statement like guarded commands <ref> [Dij76] </ref> in the language): here there is at most one enabled transition in any configuration. The behavior of the system can be nondeterministic because there may be more than one enabled transition (by different processes) in a configuration.
Reference: [DS91] <author> Evelyn Duesterwald and Mary Lou Soffa. </author> <title> Concurrency Analysis in the Presence of Procedures Using a Data-Flow Framework. </title> <booktitle> In Proc. of the ACM Symposium on Testing, Analysis and Verification, </booktitle> <pages> pages 60-70, </pages> <year> 1991. </year>
Reference-contexts: The difficulty occurs when we try to adapt this graph to deal with more complex languages, such as those with dynamic allocation, pointers, and first class functions. An evidence of this difficulty is the distinction of intra-procedural analysis, inter-procedure analysis [BC86], and inter-module analysis (in the presence of concurrency) <ref> [DS91] </ref>. Some entities, such as heap objects or environments, do not even have syntactic correspondences in 3 It is like performing in-line procedure expansion first and then analyzing the results as a whole. Recursions are solved by fixpoint computation. 16 Background the program text. <p> Several kinds of representation graphs for parallel programs have been proposed, such as module interaction graphs <ref> [DS91] </ref>, task graphs [EGP89], co-graphs [BK89], concurrency history graphs [Tay83], task interaction graphs [LC89], synchronized control flow graphs [CS89], or parallel control flow graphs [SW91, GS92]. Most of them are proposed to detect race conditions or to formulate data flow equations. <p> These sets can be used to determine whether two statements can possibly be concurrent. The data flow formulation has been adapted in the work of Duesterwald and Soffa <ref> [DS91] </ref>, and Grunwald and Srinivasan [GS92]. Instead of using data flow analysis for determining concurrent statements, explicit con-currency graphs are constructed in Emrath, Ghosh, and Padua [EGP89], and Balasundaram and Kennedy [BK89]. One concern here is the pairing of signal and wait events as synchronization edges. <p> It makes use of the formulation of preserved sets defined in [CS89]. The work by Duesterwald and Soffa <ref> [DS91] </ref> presents a data flow framework for computing the happen-before and happen-after relations, in the presence of procedures. Their framework first constructs a module interaction graph; the work considers only rendezvous synchronization.
Reference: [EGP89] <author> Perry Emrath, Sanjoy Ghosh, and David Padua. </author> <title> Event Synchronization Analysis for Debugging Parallel Programs. </title> <booktitle> In Proceedings of the 1989 International Conference on Supercomputing, </booktitle> <year> 1989. </year>
Reference-contexts: Several kinds of representation graphs for parallel programs have been proposed, such as module interaction graphs [DS91], task graphs <ref> [EGP89] </ref>, co-graphs [BK89], concurrency history graphs [Tay83], task interaction graphs [LC89], synchronized control flow graphs [CS89], or parallel control flow graphs [SW91, GS92]. Most of them are proposed to detect race conditions or to formulate data flow equations. <p> an exact pairing is not available (e.g., there are multiple signal events that can trigger an wait event), it is still possible to add inter-thread constraints, for example, a constraint from the nearest common ancestors of the signals that might trigger the wait, to the wait, as the approach in <ref> [EGP89] </ref>. 10.3.2 Further Parallelization Instead of combining threads, a thread can be split into several threads to increase parallelism. The PPDG representation contains all the information necessary for executing a parallel program correctly (this is not true with other existing program representations). <p> One approach to the static methods is based on either the formulation of data flow equations [TO80, CS89], or graph manipulation <ref> [EGP89, BK89] </ref>, from a given program flow graph which may contain some fork/join nodes and synchronization edges constraining their execution orders. One of the earliest works to take this approach is by Taylor and Osterweil [TO80]. <p> The data flow formulation has been adapted in the work of Duesterwald and Soffa [DS91], and Grunwald and Srinivasan [GS92]. Instead of using data flow analysis for determining concurrent statements, explicit con-currency graphs are constructed in Emrath, Ghosh, and Padua <ref> [EGP89] </ref>, and Balasundaram and Kennedy [BK89]. One concern here is the pairing of signal and wait events as synchronization edges. In [EGP89], the task graph is constructed where synchronization edges from the nearest common ancestors of the signals that might trigger a wait to the wait are added iteratively. <p> Instead of using data flow analysis for determining concurrent statements, explicit con-currency graphs are constructed in Emrath, Ghosh, and Padua <ref> [EGP89] </ref>, and Balasundaram and Kennedy [BK89]. One concern here is the pairing of signal and wait events as synchronization edges. In [EGP89], the task graph is constructed where synchronization edges from the nearest common ancestors of the signals that might trigger a wait to the wait are added iteratively. In [BK89], the co-graph is constructed where two blocks that might be concurrent are connected. <p> Their framework first constructs a module interaction graph; the work considers only rendezvous synchronization. The happen-before and -after relations are computed through propagation of data flow equations for local control-flow analysis, synchronization analysis, and activation context analysis. Ghosh [Gho92], based on earlier work on detecting race conditions <ref> [EGP89] </ref>, develops data flow equations for scalar optimization of parallel programs, by constructing a concurrent control flow graph for programs with simple constructs. His work addresses only the constant propagation problem. The work of Cousot and Cousot [CC84] presents an invariance proof system for shared memory parallel programs.
Reference: [FOW87] <author> J. Ferrante, K. Ottenstein, and J. Warren. </author> <title> The Program Dependence Graph and Its Use in Optimization. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <year> 1987. </year>
Reference-contexts: There are several program representation graphs for sequential programs, including the control flow graph [ASU86], the data dependence graph [KKP + 81], or the program dependence graph <ref> [FOW87] </ref> which combines both graphs into a single one. To reduce the size of a representation graph, a node in the graph usually represents a basic block, i.e., a sequence of consecutive statements without branching or halt. <p> The latter will be discussed in the next section. The traditional approach, which originally targeted Fortran programs, relies on some intermediate program representation, such as the program dependence graph <ref> [FOW87] </ref>, that describes the control flow and data flow of a program, for deriving data dependences [KKP + 81] and control dependences [FOW87]; these dependences define an execution order that must be preserved in order to assure a correct execution. <p> The traditional approach, which originally targeted Fortran programs, relies on some intermediate program representation, such as the program dependence graph <ref> [FOW87] </ref>, that describes the control flow and data flow of a program, for deriving data dependences [KKP + 81] and control dependences [FOW87]; these dependences define an execution order that must be preserved in order to assure a correct execution.
Reference: [GAG + 92] <author> Kourosh Gharachorloo, Sarita V. Adve, Anoop Gupta, John L. Hennessy, and Mark D. Hill. </author> <title> Programming for Different Memory Consistency Models. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4) </volume> <pages> 399-407, </pages> <year> 1992. </year>
Reference-contexts: This semantics also underlies the sequential consistency model [Lam77]) in memory system design. Most modern machine architectures, e.g., the DASH multiprocessor [LLG + 92], attempt to make the machine look sequentially consistent to users, while hiding memory latency with a more relaxed consistency model such as release consistency <ref> [GLL + 90, GAG + 92] </ref>, because the sequential consistency model is easier to reason about. 2 Winskel and Nielsen [WN92] survey some fundamental concurrency models, classify them, and use category theory to formalize their relationships. <p> They then test whether a system of equations of an s-level cycle has a solution, similar to the data dependence test. [MP90] also gives eleven examples illustrating how a conventional compiler fails to generate correct code for parallel programs. The work by Gharachorloo et al., <ref> [GAG + 92] </ref>, proposes a programming model (PLpc, or properly labeled programs) which requires the programmer to explicitly categorize all memory requests in the program into shared/non-shared, competing/non-competing, and loop/non-loop accesses.
Reference: [GGS88] <author> Stephen Garland, John Guttag, and Jorgen Staunstrup. </author> <title> Verification of VLSI Circuits using LP. </title> <type> Technical Report DAIMI PB-258, </type> <institution> Computer Science Department, Aarhus University, Aarhus, Denmark, </institution> <month> July </month> <year> 1988. </year>
Reference-contexts: space explosion problem is inevitable in almost any analysis and verification method based on the examination of all possible states that a program can reach, for example, parallel program verification [Lip75, Ove81, Tay83], Petri-net reachability analysis [Val88], model checking [CGL92] (whether specific properties are satisfied by a system), circuit verification <ref> [GGS88] </ref>, and communication protocol verification [CPS90, LL92], some of which focus on finite state models, and use process algebra or temporal logic to specify a system.
Reference: [GHK + 80] <author> G. Gierz, K.H. Hofmann, K. Keimel, J.D. Lawson, M. </author> <title> Mislove, and D.S. Scott. </title>
Reference-contexts: Chapter 3 Domains This chapter first presents some basic definitions in domain theory, which are necessary for describing the program semantics developed in this thesis. Comprehensive treatment of this subject can be found in <ref> [GHK + 80, Sto77, Sco82, Sch86, GMS89] </ref>. The next section gives a brief introduction to powerdomains, which are often used to describe the semantics of nondeterministic or concurrent programs. <p> Most of material in this section is borrowed from [GMS89, Bur87, AH87, Bur91]. In particular, we will explain the the Hoare powerdomain, which is used in this work for both the purposes of correctness and to express concurrency. Then the Galois connection <ref> [GHK + 80] </ref> is described, which is used to establish the correctness of an abstract interpretation. <p> F Proposition 3.21 (1) fj:jg is continuous. (2) is continuous, and for f : D!E, S E ffi IPIPf = IPf ffi S (3) is continuous, and F Proof: See [Bur87]. 2 Moreover, F F X for finite complete lattices. 3.3 Galois Connections The material in this section is from <ref> [GHK + 80] </ref>. Definition 3.22 (Galois connection) Let S and T be two posets. <p> Both maps are required to be monotonic and to assure the correctness of an abstract interpretation. The correspondence between these two are established by showing the adjoined functions property (that is, the Galois connection <ref> [GHK + 80] </ref> described in Section 3.3) [CC77]: Property 8.1 (adjoined functions) Let D and D be posets.
References-found: 42

