URL: file://ftp.cc.gatech.edu/pub/ai/ram/git-cc-92-03.ps.Z
Refering-URL: http://www.cs.gatech.edu/faculty/ashwin/projects/introspective-multistrategy-learning.html
Root-URL: 
Email: E-mail: ashwin@cc.gatech.edu  
Phone: (404) 853-9372  
Title: Indexing, Elaboration and Refinement: Incremental Learning of Explanatory Cases  
Author: Ashwin Ram 
Note: Machine Learning, 10(3):201-248, 1993.  
Address: Atlanta, Georgia 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Abstract: This article describes how a reasoner can improve its understanding of an incompletely understood domain through the application of what it already knows to novel problems in that domain. Case-based reasoning is the process of using past experiences stored in the reasoner's memory to understand novel situations or solve novel problems. However, this process assumes that past experiences are well understood and provide good "lessons" to be used for future situations. This assumption is usually false when one is learning about a novel domain, since situations encountered previously in this domain might not have been understood completely. Furthermore, the reasoner may not even have a case that adequately deals with the new situation, or may not be able to access the case using existing indices. We present a theory of incremental learning based on the revision of previously existing case knowledge in response to experiences in such situations. The theory has been implemented in a case-based story understanding program that can (a) learn a new case in situations where no case already exists, (b) learn how to index the case in memory, and (c) incrementally refine its understanding of the case by using it to reason about new situations, thus evolving a better understanding of its domain through experience. This research complements work in case-based reasoning by providing mechanisms by which a case library can be automatically built for use by a case-based reasoning program. 
Abstract-found: 1
Intro-found: 1
Reference: [Barletta and Mark, 1988] <author> R. Barletta and W. Mark. </author> <title> Explanation-Based Indexing of Cases. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 541-546, </pages> <address> St. Paul, MN, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: This is a type of inductive category formation [Dietterich and Michalski, 1981]; however, the generalization process is constrained so that the features selected for generalization are those that are causally relevant to the explanations being indexed <ref> [Barletta and Mark, 1988; Flann and Dietterich, 1989] </ref>. XPs are indexed in memory using stereotypical descriptions of the EXPLAINS node.
Reference: [Bhatta and Ram, 1991] <author> S. Bhatta and A. Ram. </author> <title> Learning indices for schema selection. </title> <editor> In M. B. Fishman, editor, </editor> <booktitle> Proceedings of the Fourth Florida Artificial Intelligence Research Symposium, </booktitle> <pages> pages 226-231, </pages> <address> Cocoa Beach, FL, </address> <month> April </month> <year> 1991. </year> <institution> Florida AI Research Society. </institution>
Reference-contexts: This heuristic is independent of the actual content of the explanations, or the domain of applicability of the schemas, relying instead on the nature of causal relationships in the domain (e.g., <ref> [Bhatta and Ram, 1991] </ref>). Barletta and Mark's EBI algorithm identifies indices using a process similar to goal regression in explanation-based generalization (EBG) [Mitchell et al., 1986]. Any feature identified in this manner can be used as an index.
Reference: [Cox and Ram, 1991] <author> M. Cox and A. Ram. </author> <title> Using Introspective Reasoning to Select Learning Strategies. </title> <editor> In R. S. Michalski and G. Tecuci, editors, </editor> <booktitle> Proceedings of the First International Workshop on Multistrategy Learning, </booktitle> <pages> pages 217-230, </pages> <address> Harpers Ferry, WV, </address> <month> November </month> <year> 1991. </year> <institution> Center for Artificial Intelligence, George Mason University, Fairfax, VA. </institution>
Reference-contexts: We are developing learning algorithms that deal with different types of processing failures, and investigating the extent to which these learning algorithms can be integrated into a single multistrategy learning system <ref> [Cox and Ram, 1991; Ram and Cox, 1993] </ref>. In this article, we have presented explanation-based learning techniques for building and improving the case library for a case-based story understanding task within this general framework. Learning occurs incrementally when the understander encounters the following difficulties: 1.
Reference: [DeJong and Mooney, 1986] <author> G. F. DeJong and R. J. Mooney. </author> <title> Explanation-Based Learning: An Alternative View. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: This is in contrast to typical explanation-based learning systems in which new explanations are constructed by exhaustive backchaining through primitive inference rules if an applicable schema is not available (e.g., <ref> [DeJong and Mooney, 1986] </ref>). AQUA's approach relies on the efficiency and content assumptions, A-1 and A-2, discussed earlier. 4 Explanation-based refinement Let us start with the situation in which no specific XP is available, and a new case must be created by reasoning from abstract knowledge. <p> Explanation-based generalization programs can learn new explanation schemas through the generalization of causal features from a novel story (e.g., <ref> [DeJong and Mooney, 1986] </ref>). This technique provides a method of creating new XPs by generalizing the details of specific novel explanations. However, XPs can also be created by specializing or refining abstract explanations in memory to create situation-specific XPs for different stereotypical situations. <p> This identification relies on causal constraints similar to those used for the identification of relevant features in explanation-based learning (e.g., <ref> [Mitchell et al., 1986; DeJong and Mooney, 1986] </ref>), with the difference that the intent here is to produce generalizations that are as specific as possible within the causal constraints of the observed situation.
Reference: [DeJong, 1983] <author> G. F. DeJong. </author> <title> An Approach to Learning from Observation. </title> <editor> In R. S. Michalski, editor, </editor> <booktitle> Proceedings of the 1983 International Machine Learning Workshop, </booktitle> <pages> pages 171-176, </pages> <address> Monticello, IL, </address> <month> June </month> <year> 1983. </year> <institution> Department of Computer Science, University of Illinois, Urbana-Champaign. </institution>
Reference: [Dietterich and Michalski, 1981] <author> T. G. Dietterich and R. S. Michalski. </author> <title> Inductive Learning of Structural Descriptions: Evaluation Criteria and Comparative Review of Selected Methodologies. </title> <journal> Artificial Intelligence, </journal> <volume> 16 </volume> <pages> 257-294, </pages> <year> 1981. </year>
Reference-contexts: As described earlier, XPs are associated with stereotypical situations and people in memory. An understander needs to learn the stereotypical categories that serve as useful indices for volitional explanations. This is a type of inductive category formation <ref> [Dietterich and Michalski, 1981] </ref>; however, the generalization process is constrained so that the features selected for generalization are those that are causally relevant to the explanations being indexed [Barletta and Mark, 1988; Flann and Dietterich, 1989]. XPs are indexed in memory using stereotypical descriptions of the EXPLAINS node.
Reference: [Doyle, 1979] <author> J. Doyle. </author> <title> A Truth Maintenance System. </title> <journal> Artificial Intelligence, </journal> <volume> 12 </volume> <pages> 231-272, </pages> <year> 1979. </year>
Reference-contexts: AGE = TEENAGE AGE (hypothesized) RELIGION = SHIITE MOSLEM (hypothesized) GENDER = MALE (hypothesized) NATIONALITY = LEBANESE (hypothesized) Indexing XP-BLACKMAIL-SUICIDE-BOMBING in memory Category index = XP-GOAL-SACRIFICE Stereotype index = STEREOTYPE.79 Situation index = SUICIDE-BOMBING The label in (out) marks features that are known to be true (false) of this stereotype <ref> [Doyle, 1979] </ref>. These features are definitional of the stereotype. The label question marks features that are in but incomplete. In this case, (AVOIDANCE-GOAL (STATE)) refers to an unknown goal that needs to be filled in when the information comes in. This is represented as a goal with an unknown goal-object.
Reference: [Falkenhainer, 1988] <author> B. Falkenhainer. </author> <title> The Utility of Difference-Based Reasoning. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 530-535, </pages> <address> St. Paul, MN, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: AQUA learns from positive and negative hypotheses resulting from episodes of case-based reasoning using XPs. However, it does not perform any explicit comparison between its hypotheses, unlike Falken-hainer's system which exploits differences between similar situations to focus search and generate plausible hypotheses <ref> [Falkenhainer, 1988] </ref>. It would be instructive to use a difference-based reasoning method similar 38 to Falkenhainer's to improve the quality of the generalizations produced to be used as indices in AQUA's index learning algorithm.
Reference: [Flann and Dietterich, 1989] <author> N. S. Flann and T. G. Dietterich. </author> <title> A Study of Explanation-Based Methods for Inductive Learning. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 187-226, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: This is a type of inductive category formation [Dietterich and Michalski, 1981]; however, the generalization process is constrained so that the features selected for generalization are those that are causally relevant to the explanations being indexed <ref> [Barletta and Mark, 1988; Flann and Dietterich, 1989] </ref>. XPs are indexed in memory using stereotypical descriptions of the EXPLAINS node. <p> AQUA can also learn new indices to its XPs. A final difference between AQUA and SWALE is AQUA's ability to use XPs representing both stereotypical explanatory cases as well as abstract explanation schemas to build explanations for new situations. Explanation-based refinement is related to theory-based concept specialization <ref> [Flann and Dietterich, 1989; Mooney, ] </ref>, which involves the inductive specialization of a concept defined by a domain theory.
Reference: [Gupta, 1987] <author> A. Gupta. </author> <title> Explanation-Based Failure Recovery. </title> <booktitle> In Proceedings of the Sixth National Conference on Artificial Intelligence, </booktitle> <pages> pages 606-610, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1987. </year>
Reference-contexts: However, it is also possible to apply this technique to learn from negative examples (e.g., <ref> [Mostow and Bhatnagar, 1987; Gupta, 1987] </ref>). AQUA uses refuted hypotheses to infer features that should not be present in the newly built stereotype. These are features that, if present, would have led to the hypothesis being confirmed.
Reference: [Hammond, 1989] <author> K. J. Hammond, </author> <title> editor. </title> <booktitle> Proceedings of the Second Case-Based Reasoning Workshop, </booktitle> <address> Pensacola Beach, FL, </address> <month> May </month> <year> 1989. </year> <title> Defense Advanced Research Projects Agency, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: 1 Case-based learning Case-based reasoning programs deal with the issue of using past experiences or cases to understand, plan for, or learn from novel situations (e.g., see <ref> [Kolodner, 1988; Hammond, 1989] </ref>).
Reference: [Hobbs et al., 1990] <author> J. R. Hobbs, M. Stickel, D. Appelt, and P. Martin. </author> <title> Interpretation as Abduction. </title> <type> Technical Note 499, </type> <institution> SRI International, </institution> <year> 1990. </year>
Reference-contexts: Construction of such explanations is typically done by chaining together inference rules through a search process (e.g., [Rieger, 1975; Wilensky, 1981; Morris and O'Rorke, 1990]), through a weighted or cost-based search (e.g., <ref> [Hobbs et al., 1990; Stickel, 1990] </ref>), or through a case-based reasoning process in which previous explanations for similar situations are retrieved and adapted for the current situation (e.g., [Schank, 1986; Kass et al., 1986; Ram, 1989; Ram, 1990a]).
Reference: [Kass and Owens, 1988] <author> A. Kass and C. Owens. </author> <title> Learning New Explanations by Incremental Adaptation. </title> <booktitle> In Proceedings of the 1988 AAAI Spring Symposium on Explanation-Based Learning, </booktitle> <address> Stanford, CA, </address> <year> 1988. </year>
Reference-contexts: Learning may also occur through the learning of new indices for old cases, as the reasoner discovers new contexts in which its cases are applicable. As mentioned earlier, some case-based learning programs do include a learning step in which the results of case application are generalized (e.g., <ref> [Kass and Owens, 1988] </ref>). This gives these programs the ability to improve their case knowledge in an incremental fashion. However, in most programs this involves the 1 generalization of cases that embody correct and complete solutions to past or current problems.
Reference: [Kass et al., 1986] <author> A. Kass, D. Leake, and C. Owens. SWALE: </author> <booktitle> A Program That Explains. </booktitle> <pages> pages 232-254. </pages> <year> 1986. </year> <editor> In [Schank, </editor> <year> 1986]. </year>
Reference-contexts: inference rules through a search process (e.g., [Rieger, 1975; Wilensky, 1981; Morris and O'Rorke, 1990]), through a weighted or cost-based search (e.g., [Hobbs et al., 1990; Stickel, 1990]), or through a case-based reasoning process in which previous explanations for similar situations are retrieved and adapted for the current situation (e.g., <ref> [Schank, 1986; Kass et al., 1986; Ram, 1989; Ram, 1990a] </ref>). <p> Our main contribution to Schank's theory of explanation patterns is that the case-based explanation process in AQUA, while similar to that used by the SWALE program <ref> [Kass et al., 1986] </ref>, is formulated in a question-based framework. Our emphasis is on the questions that underly the creation, verification, and learning of explanations, and not on the creative adaptation process described by Kass et al. <p> We now discuss the main points of our theory in the context of other related work in case-based reasoning and machine learning. AQUA's case-based explanation process is similar to that used by SWALE <ref> [Kass et al., 1986] </ref>, but is formulated in a question-based framework that provides a basis for integrating explanation, natural language understanding, memory, and learning [Ram, 1989].
Reference: [Keller, 1988] <author> R. M. Keller. </author> <title> Defining Operationality for Explanation-Based Learning. </title> <journal> Artificial Intelligence, </journal> <volume> 35 </volume> <pages> 227-241, </pages> <year> 1988. </year>
Reference-contexts: Here, the theory of case-based explanation that specifies what kinds of indices are useful. This set of specifications may be viewed as the operationality criterion <ref> [Keller, 1988] </ref> for the explanation-based generalization process, and is in contrast to the use of purely syntactic criteria such as "generalize as far as 7 AQUA can still understand other blackmail situations that it has not learned about as yet, as it did while reading the story in this example.
Reference: [Kolodner, 1988] <editor> J. L. Kolodner, editor. </editor> <booktitle> Proceedings of a Workshop on Case-Based Reasoning, </booktitle> <address> Clearwater Beach, FL, </address> <month> May </month> <year> 1988. </year> <title> Defense Advanced Research Projects Agency, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: 1 Case-based learning Case-based reasoning programs deal with the issue of using past experiences or cases to understand, plan for, or learn from novel situations (e.g., see <ref> [Kolodner, 1988; Hammond, 1989] </ref>).
Reference: [Leake, 1989a] <author> D. Leake. </author> <title> Anomaly detection strategies for schema-based story understanding. </title> <booktitle> In Proceedings of the Eleventh Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 490-497, </pages> <address> Ann Arbor, MI, </address> <year> 1989. </year> <month> 41 </month>
Reference: [Leake, 1989b] <author> D. Leake. </author> <title> Evaluating Explanations. </title> <type> Ph.D. thesis, </type> <institution> Yale University, Department of Computer Science, </institution> <address> New Haven, CT, </address> <year> 1989. </year>
Reference: [Leake, 1989c] <author> D. B. Leake. </author> <title> The Effect of Explainer Goals on Case-Based Explanation. </title> <booktitle> In Proceedings of a Workshop on Case-Based Reasoning, </booktitle> <address> Pensacola Beach, FL, May 1989. </address> <publisher> Morgan Kaufmann, Inc. </publisher>
Reference-contexts: Ultimately, the evaluation of the desired explanation must be done with respect to the goals of the system, that is, with respect to the reasons for which the explanation is being produced in the first place (e.g., see <ref> [Leake, 1989c; Ram, 1990b; Ram and Leake, 1991] </ref>). The effect of this constraint on the evaluation of the learning algorithms is an open issue.
Reference: [Michalski, 1983] <author> R. S. Michalski. </author> <title> A Theory and Methodology of Inductive Learning. </title> <journal> Artificial Intelligence, </journal> <volume> 20 </volume> <pages> 111-161, </pages> <year> 1983. </year>
Reference-contexts: Heavy lines represent situation indices, which point from MOPs to XPs. Here, AQUA has just built a situation index from suicide-bombing to a copy of xp-blackmail. possible" or "always express the generalization as a disjunct of three conjuncts" that provides the constraints, or bias <ref> [Mitchell, 1980; Michalski, 1983] </ref>, on the generalization process in many empirical learning methods. 12 Learning character stereotype indices The main constraint on a theory of stereotype learning is that the kinds of stereotypes learned must be useful in retrieving explanations.
Reference: [Minton, 1988] <author> S. Minton. </author> <title> Learning effective search control knowledge: An explanation-based approach. </title> <type> Ph.D. thesis, </type> <institution> Carnegie-Mellon University, Computer Science Department, </institution> <address> Pittsburgh, PA, </address> <year> 1988. </year> <note> Technical Report CMU-CS-88-133. </note>
Reference-contexts: EBR is also related to the explanation-based specialization algorithm used in PRODIGY <ref> [Minton, 1988] </ref> to map problem-solving traces into explanations; however, EBR concentrates more on the adaptation of previously known abstract explanations (using mechanisms of substitution, internalization and elaboration) rather than the generalization of problem-solving traces. <p> Unlike Barletta and Mark's algorithm, however, we are focussing on the use of explanation-based methods for index refinement. We are also exploring better methods for evaluating our approach. For example, Minton and Carbonell's PRODIGY system sometimes slows down with learning <ref> [Minton, 1988] </ref>. While slowing down is not necessarily bad in itself, better evaluation methods are needed to evaluate the relative merits of producing simpler explanations in less time as opposed to better (and perhaps more complex) explanations that may take longer to compute.
Reference: [Mitchell et al., 1986] <author> T. M. Mitchell, R. Keller, and S. Kedar-Cabelli. </author> <title> Explanation-Based Generalization: </title>
Reference-contexts: The causal constraint used in this rule is similar to the identification and generalization of relevant features in explanation-based generalization through goal regression (e.g., <ref> [Mitchell et al., 1986] </ref>). <p> This identification relies on causal constraints similar to those used for the identification of relevant features in explanation-based learning (e.g., <ref> [Mitchell et al., 1986; DeJong and Mooney, 1986] </ref>), with the difference that the intent here is to produce generalizations that are as specific as possible within the causal constraints of the observed situation. <p> Barletta and Mark's EBI algorithm identifies indices using a process similar to goal regression in explanation-based generalization (EBG) <ref> [Mitchell et al., 1986] </ref>. Any feature identified in this manner can be used as an index.
References-found: 22

