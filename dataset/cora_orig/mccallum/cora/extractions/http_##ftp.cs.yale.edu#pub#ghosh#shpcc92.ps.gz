URL: http://ftp.cs.yale.edu/pub/ghosh/shpcc92.ps.gz
Refering-URL: http://ftp.cs.yale.edu/pub/ghosh/
Root-URL: http://www.cs.yale.edu
Email: email: ghosh@cs.yale.edu email: schultz-martin@cs.yale.edu  
Title: Portable Parallel Level-3 BLAS in Linda  
Author: Bhaskar Ghosh Martin H. Schultz 
Address: P.O.Box 2158, Yale Station P.O.Box 2158, Yale Station New Haven, CT 06520 New Haven, CT 06520  
Affiliation: Department of Computer Science Department of Computer Science Yale University Yale University  
Abstract: This paper describes an approach towards providing an efficient Level-3 BLAS library over a variety of parallel architectures using C-Linda. A blocked linear algebra program calling the sequential Level-3 BLAS can now run on both shared and distributed memory environments (which support Linda) by simply replacing each call by a call to the corresponding parallel Linda Level-3 BLAS . We give summarise some of the implementation and algorithmic issues related to the matrix multiply subroutine. All the various matrix algorithms being block-structured, we are particularly interested in parallel computers with hierarchical memory systems. Experimental data for our implementations show substantial speedups on shared memory, disjoint memory and networked configurations of processors. We also present the use of our parallel subroutines in blocked dense LU decomposition and present some preliminary experimental data. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Brochard and A. Freau, </author> <title> "Designing Algorithms on Hierarchical Memory Multiprocessors". </title> <type> RC 15271 (68213), </type> <institution> IBM Yorktown Heights. </institution> <year> 1989. </year>
Reference-contexts: The problem of implementing and tuning the Level-3 BLAS on specific machines to achieve good performance has been studied in detail for a wide variety of 1 BLAS is an acronym for Basic Linear Algebra Subroutines. machines in <ref> [1, 10, 4] </ref>. Dayde and Duff [5, 6] have reported on parallel Level-3 BLAS and their uses in various blocked forms of LU factorisation on shared memory machines with vector facilities, like the CRAY-2, IBM 3090 VF and Alliant FX/80.
Reference: [2] <author> N. Carriero and D. </author> <title> Gelernter , "How to write Parallel Programs". </title> <publisher> M.I.T. Press, </publisher> <address> Boston, </address> <year> 1990. </year>
Reference-contexts: Thus, for example, C with the addition of the Linda operations becomes the parallel programming language C-Linda. For an introduction to the tuple-space approach of parallel computation and to some general discussions of Linda implementations we refer the reader to <ref> [2, 3] </ref>. We will assume that the reader has minimal familiarity with the primitive operations in the Linda language. The algorithmic paradigm that we use in designing our algorithms is the oft-used Master-Worker Model [2, 3]. <p> of parallel computation and to some general discussions of Linda implementations we refer the reader to <ref> [2, 3] </ref>. We will assume that the reader has minimal familiarity with the primitive operations in the Linda language. The algorithmic paradigm that we use in designing our algorithms is the oft-used Master-Worker Model [2, 3]. In this model, a single process acts as the master process: spawning workers, maintaining overall control of the entire computation, distributing data and collecting results. The actual computation is done by a number of worker processes running independently and in parallel on different CPUs.
Reference: [3] <author> N. J. Carriero, </author> <title> "Implementation of Tuple Space Machines". </title> <address> YALEU/DCS/RR-567, </address> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: Thus, for example, C with the addition of the Linda operations becomes the parallel programming language C-Linda. For an introduction to the tuple-space approach of parallel computation and to some general discussions of Linda implementations we refer the reader to <ref> [2, 3] </ref>. We will assume that the reader has minimal familiarity with the primitive operations in the Linda language. The algorithmic paradigm that we use in designing our algorithms is the oft-used Master-Worker Model [2, 3]. <p> of parallel computation and to some general discussions of Linda implementations we refer the reader to <ref> [2, 3] </ref>. We will assume that the reader has minimal familiarity with the primitive operations in the Linda language. The algorithmic paradigm that we use in designing our algorithms is the oft-used Master-Worker Model [2, 3]. In this model, a single process acts as the master process: spawning workers, maintaining overall control of the entire computation, distributing data and collecting results. The actual computation is done by a number of worker processes running independently and in parallel on different CPUs. <p> This is also true for our Linda programs. In Linda programs, the set of tuples are divided into subsets of "similar" tuples 2 . When a tuple is accessed, it has to 2 We refer the reader to Nick Carriero's dissertation <ref> [3] </ref> for details on the compile time analysis and implementation details necessary for tuple space machines. be searched for and found in the data structure cor-responding to the subset it belongs to.
Reference: [4] <author> D. Chen, </author> <title> "Hierarchical Blocking and Data Flow Analysis for Numerical Linear Algebra.". </title> <booktitle> Conference Proceedings of Supercomputing, 1990. </booktitle> <address> New York City. </address>
Reference-contexts: The problem of implementing and tuning the Level-3 BLAS on specific machines to achieve good performance has been studied in detail for a wide variety of 1 BLAS is an acronym for Basic Linear Algebra Subroutines. machines in <ref> [1, 10, 4] </ref>. Dayde and Duff [5, 6] have reported on parallel Level-3 BLAS and their uses in various blocked forms of LU factorisation on shared memory machines with vector facilities, like the CRAY-2, IBM 3090 VF and Alliant FX/80.
Reference: [5] <author> M. Dayde and I Duff, </author> <title> "Level 3 BLAS in LU fac-torisation in LU factorisation on the CRAY-2 etc.". </title> <booktitle> The Intl. J. of Supercomputing Applications, </booktitle> <volume> 3 - 2, </volume> <month> Summer </month> <year> 1989. </year>
Reference-contexts: The problem of implementing and tuning the Level-3 BLAS on specific machines to achieve good performance has been studied in detail for a wide variety of 1 BLAS is an acronym for Basic Linear Algebra Subroutines. machines in [1, 10, 4]. Dayde and Duff <ref> [5, 6] </ref> have reported on parallel Level-3 BLAS and their uses in various blocked forms of LU factorisation on shared memory machines with vector facilities, like the CRAY-2, IBM 3090 VF and Alliant FX/80. <p> The LU factorisation algorithm can be expressed in blocked form and involves calls to Level-3 BLAS subroutines. Blocked LU factorisation is an extensively researched topic and we refer the readers to papers mentioned in <ref> [5, 6] </ref>. There can be six block forms of LU factorisa-tion corresponding to the six ways of reordering the loops that constitute the algorithm. <p> We implemented three block column forms (following the nomenclature of [7]) because we store the matrix in column order to allow calls to the Fortran Level-3 BLAS routines which are hidden inside our C-Linda block level primitives. These forms have been well described and analysed in <ref> [5] </ref>. In this chapter we will discuss only one blocked form called kji-saxpy without pivoting and use of our parallel subroutines. We quote the following algorithm from Dayde and Duff [5]. 3.1 KJI-SAXPY Algorithm. <p> These forms have been well described and analysed in <ref> [5] </ref>. In this chapter we will discuss only one blocked form called kji-saxpy without pivoting and use of our parallel subroutines. We quote the following algorithm from Dayde and Duff [5]. 3.1 KJI-SAXPY Algorithm. At the k th step of the elimination a block column of L and a block row of U are computed and the trailing submatrix M k is updated, as shown in Figure 3.
Reference: [6] <author> M. J. Dayde and I Duff, </author> <title> "Use of level 3 BLAS in LU factorization in a multiprocessor environment on three vector multiprocessors, the Alliant FX/80, the Cray-2 and the IBM 3090 VF". </title> <booktitle> ICS 1990 Proceedings. </booktitle>
Reference-contexts: The problem of implementing and tuning the Level-3 BLAS on specific machines to achieve good performance has been studied in detail for a wide variety of 1 BLAS is an acronym for Basic Linear Algebra Subroutines. machines in [1, 10, 4]. Dayde and Duff <ref> [5, 6] </ref> have reported on parallel Level-3 BLAS and their uses in various blocked forms of LU factorisation on shared memory machines with vector facilities, like the CRAY-2, IBM 3090 VF and Alliant FX/80. <p> The LU factorisation algorithm can be expressed in blocked form and involves calls to Level-3 BLAS subroutines. Blocked LU factorisation is an extensively researched topic and we refer the readers to papers mentioned in <ref> [5, 6] </ref>. There can be six block forms of LU factorisa-tion corresponding to the six ways of reordering the loops that constitute the algorithm.
Reference: [7] <author> J. J. Dongarra, J. Du Croz, I. Duff and S. Ham-marling, </author> <title> "A Set of Level 3 Basic Linear Algebra Subprograms". </title> <institution> Argonne National Laboratory, </institution> <month> August </month> <year> 1988. </year>
Reference-contexts: At the lowest level of computation each worker updates one or several block (s) of C by calling the single precision version of the widely available Fortran-77 Level-3 BLAS subroutine sgemm <ref> [7] </ref>. <p> There can be six block forms of LU factorisa-tion corresponding to the six ways of reordering the loops that constitute the algorithm. We implemented three block column forms (following the nomenclature of <ref> [7] </ref>) because we store the matrix in column order to allow calls to the Fortran Level-3 BLAS routines which are hidden inside our C-Linda block level primitives. These forms have been well described and analysed in [5].
Reference: [8] <author> K. Gallivan, R. J. Plemmons and A. H. </author> <title> Sameh , "Parallel Dense Linear Algebra algorithms". </title> <journal> Siam Review, </journal> <month> March </month> <year> 1990. </year>
Reference: [9] <author> B. Ghosh and Martin H. Schultz, </author> <title> "Portable parallel Level-3 BLAS : Algorithms, Performance and Use, </title> <institution> Technical Report (forthcoming) Yale University, Computer Science Department, </institution> <year> 1991. </year>
Reference-contexts: Current experimental data is presented. With the efficient parallel subroutines thus available we can speed up a blocked-LU factorisation scheme expressible in level-3 Blas primitives. In this paper we have tried to omit most details. Progress reports giving detailed accounts of our implementations and experiments can be found in <ref> [9] </ref>. 1.4 Background Linda consists of a small number of powerful operations that may be integrated into a conventional base language, yielding a dialect that supports parallel programming. Thus, for example, C with the addition of the Linda operations becomes the parallel programming language C-Linda. <p> The interaction of the size and shapes of matrix blocks with communication cost was crucial. We discuss one related observation below. The reader is referred to <ref> [9] </ref> for a discussion of other issues like data replication, handling of "large" tuples in relation to matrix blocking, and the use of smart block allocation techniques for reduction of contention amongst workers. * Blocking of result matrix C is important. <p> The total parallel time for square blocking still beat the block column form because of the latter's higher communication cost (across all three machines), but this fact was interesting enough to report. A quantitative analysis relating this observation to cache misses can be found in our detailed report <ref> [9] </ref>. <p> Square blocking was adopted on all three machines. Sub-blocking within a worker's node was employed only on the the Sparc network. Note that the efficiencies given for the Sparc network are not strictly valid because of the reasons given in <ref> [9] </ref> and should be expexpected to be slightly less. Efficiencies were calculated wrt unblocked and blocked versions of the sequential code. The un-blocked version was a single call to the Fortran-77 Level-3 BLAS routine sgemm . <p> The Linda cost was the major overhead of the parallel program and the hierarchical memory effects due to replication, copying and cache misses were secondary. Only on the SparcStations did the paging due to copying ever become a big issue. The detailed report <ref> [9] </ref> again discusses these issues and more in further detail. 3 Use of Level-3 BLAS: blocked LU factorisation We consider the use of Level-3 BLAS in the LU fac-torisation of dense matrices without pivoting. <p> But the speedup achieved was always within 3% to 5% of the predicted maximal speedup, where predicted speedup was calculated by making separate runs of Linda strsm and sgemm with the same parameters as those inside the blocked LU program. 4 Please refer to our detailed report <ref> [9] </ref> for the discussion on parallel triangular solve. Size of matrix procs speedup efficiency 400 4 2.03 .51 6 2.30 .38 8 2.47 .31 5 2.60 .52 7 2.98 .43 1200 4 2.51 .63 6 3.14 .52 8 3.57 .45 Table 2: Data for Sequent Symmetry. <p> However there is detailed analysis to be done regarding the relative influence of parallel cpu and memory availability here. To understand why the speedups cannot be any higher and do not scale well with p, we have tried to predict (in <ref> [9] </ref>) the maximal speedup possible given p, the problem size, blocking parameters and the I/O fraction 5 k. The observed speedup was always less than and very close to the maximal speedup predicted by this analysis. There are two main factors limiting the speedup that can be achieved. <p> Second, the copying overhead for such a Level-3 BLAS based LU decomposition grows with the number of blocks and is shown in <ref> [9] </ref> to be O (N 3 ) for N by N matrices. This limitation is unavoidable whenever we want to use any plug-compatible parallel BLAS in the commonly used blocked LU algorithms. 3.3 LU : Parallel Calls to parallel Level-3 BLAS .
Reference: [10] <author> M. Lam, E. Rothberg and M. Wolf, </author> <title> "The Cache Performance and Optimizations of Blocked Algorithms". </title> <booktitle> ASPLOS IV, </booktitle> <address> Palo Alto, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: The problem of implementing and tuning the Level-3 BLAS on specific machines to achieve good performance has been studied in detail for a wide variety of 1 BLAS is an acronym for Basic Linear Algebra Subroutines. machines in <ref> [1, 10, 4] </ref>. Dayde and Duff [5, 6] have reported on parallel Level-3 BLAS and their uses in various blocked forms of LU factorisation on shared memory machines with vector facilities, like the CRAY-2, IBM 3090 VF and Alliant FX/80.
Reference: [11] <author> C. Moler, </author> <title> "Matrix Computations with Fortran and Paging". </title> <journal> CACM 15, </journal> <volume> 4. </volume> <month> April </month> <year> 1972. </year>
Reference: [12] <author> A.C. McKellar and E.G.Jr. Coffman, </author> <title> "Organising Matrices and matrix operations for paged memory systems. </title> " <journal> CACM 12, </journal> <volume> 3. </volume> <month> March </month> <year> 1969. </year>
References-found: 12

