URL: http://vibes.cs.uiuc.edu/Publications/Papers/CM5IO.ps.gz
Refering-URL: http://vibes.cs.uiuc.edu/Publications/publications.htm
Root-URL: http://www.cs.uiuc.edu
Title: Performance of the CM-5 Scalable File System  
Author: Thomas T. Kwan Daniel A. Reed 
Address: Urbana, Illinois 61801  
Affiliation: National Center for Supercomputing Department of Computer Science Applications University of Illinois  
Abstract: Assessing the performance and software interactions of emerging parallel input/output systems is a critical first step in input/output software tuning. Moreover, understanding the system response to well-understood, synthetic input/output patterns is itself a prelude to analysis of more complex application input/output patterns. We have conducted a series of experiments to measure the performance of the CM-5's new Scalable Disk Array (SDA) and Scalable Parallel File System (SFS) using the file system interfaces provided by the data parallel CM Fortran and message passing CMMD programming models. The results of these experiments suggest that the CM-5's parallel input/output system is an improvement over its predecessor, the CM-2 Data Vault. However, network bandwidth can be a bottleneck for the data reordering phase of input/output operations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Batcher, K. E. </author> <title> Design of a Massively Parallel Processor. </title> <journal> IEEE Transactions on Computers C-29, </journal> <month> 9 (Sept. </month> <year> 1980), </year> <pages> 836-842. </pages>
Reference-contexts: A hardware solution mandates either sufficient interconnection network bandwidth for rapid reordering of data from its canonical storage format to the desired memory distribution or the design of dedicated data turning networks (e.g., as found in the Goodyear/NASA MPP <ref> [1] </ref>). Alternatively, rather than enforcing a fixed set of storage formats, exporting control of secondary storage data distributions to the application developer would obviate the need for complex data turning algorithms and would allow users to manage input/output data distributions just as they do in high-performance Fortran [6].
Reference: [2] <author> Best, M. </author> <type> Personal Communication. </type>
Reference-contexts: In Figure 3b, the maximum data rate is less than 1 megabyte/second for modest size arrays and quickly approaches an asymptote. Because the CMMD synchronous broadcast mode uses both the data and control networks to coordinate data transmission and synchronization of the processors <ref> [2] </ref>, and because the broadcast function of the CM-5's control network has a bandwidth of approximately 800 kilobytes/second [9], this limitation is a major cause for the asymptotes of Figure 3b. broadcast writes.
Reference: [3] <author> Bordawekar, R., Choudhary, A., and del Rosario, J. M. </author> <title> An Experimental Performance Evaluation of Touchstone Delta Concurrent File System. </title> <booktitle> In Proceedings of the International Conference on Supercomputing (July 1993), </booktitle> <pages> pp. 367-377. </pages>
Reference-contexts: Similarly, user control of file cache allocations and prefetch algorithms would allow them to balance the needs of multiple file request patterns and ameliorate the cache thrashing of Figures 4-5. 6 The data for the Delta are from <ref> [3] </ref>. 0 50 100 150 200 250 Array Size (megabytes) 0 10 20 30 Data Rate (megab ytes/second) * Two readers (same files) ffi Two readers (disjoint files) Single reader ? Reading with writer present * * * * * * ffi .. ... .. ... ... .. ... ... ..
Reference: [4] <author> Bordawekar, R., del Rosario, J. M., and Choudhary, A. </author> <title> Design and Evaluation of Primitives for Parallel I/O. </title> <booktitle> In Supercomputing 1993 (Nov 1993), </booktitle> <pages> pp. 452-461. </pages>
Reference-contexts: Intel is redressing this differential with the Parallel File System (PFS) on the In-tel Paragon XP/S [7]. PFS supports many of the same features as SFS, including disk striping, parity for fault tolerance, and distributed file caching. Recently, Bordawekar et al <ref> [4] </ref> proposed a two-phase access strategy to improve input/output performance on the Intel Delta and other parallel file systems.
Reference: [5] <author> Fineberg, S. A. </author> <title> Implementing the NHT-1 Application I/O Benchmark. </title> <booktitle> In Workshop on I/O in Parallel Computer Systems (April 1993). </booktitle>
Reference-contexts: On the CM-2, this limited the input/output performance to about 4.3 megabytes/second <ref> [5] </ref>. However, to achieve the highest possible input/output performance on the CM-5, users must still use a configuration-dependent file format. Additional insights into the parallel input/output performance of the CM-5 are possible by comparing our results with those obtained from other large parallel systems, notably the Intel Touchstone Delta.
Reference: [6] <author> HPFF. </author> <title> High-Performance Fortran Language Specfication, version 1.0. </title> <type> Tech. rep., </type> <institution> High Performance Fortran Forum, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Alternatively, rather than enforcing a fixed set of storage formats, exporting control of secondary storage data distributions to the application developer would obviate the need for complex data turning algorithms and would allow users to manage input/output data distributions just as they do in high-performance Fortran <ref> [6] </ref>.
Reference: [7] <institution> Intel Supercomputer Systems Division. </institution> <note> Paragon User's Guide, </note> <month> Oct </month> <year> 1993. </year>
Reference-contexts: However, the data rates for the read and write operations of the CM-5 are nearly any order of magnitude faster than the Delta. Intel is redressing this differential with the Parallel File System (PFS) on the In-tel Paragon XP/S <ref> [7] </ref>. PFS supports many of the same features as SFS, including disk striping, parity for fault tolerance, and distributed file caching. Recently, Bordawekar et al [4] proposed a two-phase access strategy to improve input/output performance on the Intel Delta and other parallel file systems.
Reference: [8] <author> Kwan, T. T. </author> <title> Performance Evaluation of the CM-5. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: The data rates do increase with the partition size. With a larger partition, the data in the array is distributed across more nodes, and more network 2 For additional details on the experiments and their results, see <ref> [8] </ref>. 0 50 100 150 200 250 300 350 Array Size (megabytes) 0 10 20 30 Data Rate (megab ytes/second) ffi 64 nodes * 256 nodes &gt; 512 nodes ffi ffi . ..................... ..................... ..................... .................... ..................... .... ... ... ... .... ... ... ... ... ... ... ... .... ...
Reference: [9] <author> Kwan, T. T., Totty, B. K., and Reed, D. A. </author> <title> Communication and Computation Performance of the CM-5. </title> <booktitle> In Supercomputing 1993 (Nov 1993), </booktitle> <pages> pp. 192-201. </pages>
Reference-contexts: Because the CMMD synchronous broadcast mode uses both the data and control networks to coordinate data transmission and synchronization of the processors [2], and because the broadcast function of the CM-5's control network has a bandwidth of approximately 800 kilobytes/second <ref> [9] </ref>, this limitation is a major cause for the asymptotes of Figure 3b. broadcast writes. In contrast to synchronous broadcast reads, where the data rate quickly approaches an asymptote, the performance of broadcast writes continues to increase with array size.
Reference: [10] <author> Leiserson, C. E., et al. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of Parallel Algorithms and Architectures Symposium (May 1992), </booktitle> <pages> pp. 272-285. </pages>
Reference-contexts: we briefly review the CM-5 architecture and the file system interfaces; see [11] for details on the operating system design. 2.1 Architecture The CM-5 is a MIMD, distributed memory system composed of computation and input/output nodes that are interconnected by a fat-tree data network and a binary tree control network <ref> [10] </ref>. The higher bandwidth data network is responsible for data motion, whereas the lower bandwidth control network is responsible for various global operations (e.g., broadcast and reduction). A computation node typically consists of a SPARC processor, four floating point vector units, four memory banks, and a network interface (NI) chip.
Reference: [11] <author> LoVerso, S. J., Isman, M., Nanopou-los, A., Nesheim, W., Milne, E. D., and Wheeler, R. sfs: </author> <title> A Parallel File System for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer Usenix Conference (1993), </booktitle> <pages> pp. 291-305. </pages>
Reference-contexts: Thinking Machines has recently begun shipping a new parallel file system for the CM-5 disk array that provides a wider variety of access methods than those on the older CM-2 Data Vault. A preliminary CM-5 performance study by LoVerso et al <ref> [11] </ref> reported the input/output data rate achievable via operating system calls. However, the CM-5 input/output library includes a rich set of file organization and access methods; these methods are those of most interest to application developers. <p> Below, we briefly review the CM-5 architecture and the file system interfaces; see <ref> [11] </ref> for details on the operating system design. 2.1 Architecture The CM-5 is a MIMD, distributed memory system composed of computation and input/output nodes that are interconnected by a fat-tree data network and a binary tree control network [10]. <p> As an example, an input/output node with a disk array, and a computation node. The raw data transfer rate of each SCSI channel is ten megabytes/second; the raw read and write data rates of a disk are, respectively, 2 megabytes/second and 1.8 megabytes/second <ref> [11] </ref>. The CM-5's Scalable Disk Array (SDA) is the union of the small disk arrays on the CM-5 input/output nodes. The SDA stripes file data across all disks in units of sixteen bytes (i.e., each successive group of sixteen bytes is placed on a different disk using a modulo mapping). <p> This sixteen byte increment was chosen because it is the same size as the payload of a data network packet. The Scalable File System (SFS) <ref> [11] </ref> logically sits atop the SDA and arbitrates input/output requests from applications. From a user perspective it is a Unix file system, albeit with parallel input/output interface extensions. 2.2 Scalable File System Interfaces Because the CM-5 supports multiple programming models, files can be accessed via interfaces in each. <p> For small transfer sizes (i.e., small arrays), the software overhead to retrieve file block information from the file system when the operation is initiated and then to update the file state at the end of the input/output operation dominates the access cost <ref> [11] </ref>. The data rates do increase with the partition size. <p> Finally, Figure 5 shows the effects of reader/writer interaction for CMMD file access. The interaction degrades the reader's performance, albeit not as severely as for CM Fortran. 5 Discussion It is instructive to compare the performance data of x4 with that obtained by LoVerso et al <ref> [11] </ref>, who recently reported the results of a series of operating system input/output experiments on the CM-5. <p> Thinking Machines is currently testing a software patch to be included in the CMOST 7.3 final release. 5 Although the hardware configurations used by <ref> [11] </ref> differ from those in the our study, they provide enough data to bound the expected data rates for the NCSA CM-5 configuration. the CM-2 unless users wrote files in the serial order format. On the CM-2, this limited the input/output performance to about 4.3 megabytes/second [5]. <p> . (a) Effect on the read data rate (b) Effect on the write data rate In summary, possible improvements to the CM-5 SFS include lowering the latency required for input/output of smaller arrays by moving part of the file system code into the operating system kernel of the computation nodes <ref> [11] </ref>, and avoiding the use of the control network when handling large data requests for the CMMD synchronous read operation.
Reference: [12] <author> Reed, D. A., Aydt, R. A., Noe, R. J., Roth, P. C., Shields, K. A., Schwartz, B. W., and Tavera, L. F. </author> <title> Scalable Performance Analysis: The Pablo Performance Analysis Environment. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference (1993), </booktitle> <editor> A. Skjellum, Ed., </editor> <publisher> IEEE Computer Society. </publisher>
Reference-contexts: In addition, we explore the effects of machine size on input/output performance and the potential performance variations due to interaction among multiple, concurrent input/output streams. To 1 This characterization effort is the major focus of our current work and builds on our Pablo performance instrumentation software <ref> [12] </ref>. gether, these baseline performance results can be used to gauge the input/output performance of input/output intensive applications and to aid in the understanding of any captured application input/output traces. The remainder of this paper is organized as follows.
Reference: [13] <institution> Thinking Machines Corporation. </institution> <note> CMMD Reference Manual Version, 3.0 Beta, </note> <month> Dec </month> <year> 1992. </year>
Reference-contexts: In this case, the CMMD input/output interface requires users to read or write files in increments that are multiples of sixteen bytes times the number of disks in the system. The CMMD library also provides four input/output modes for use with hardware-independent files. See <ref> [13] </ref> for additional details on the CMMD parallel input/output interface. * Local: Each node maintains its own file descriptor and file pointer, and the nodes all operate independently. * Global Independent: All the nodes must participate in the call to open the file.
Reference: [14] <institution> Thinking Machines Corporation. </institution> <note> CM-5 I/O System Programming Guide, Version 7.2, </note> <month> September </month> <year> 1993. </year>
Reference-contexts: Like the generic format, data written in this format can be read by programs executing in partitions of any size. In addition, the serial order format allows the reader to read arrays using shapes and distributions different from that of the writer. See <ref> [15, 14] </ref> for additional details on the CM Fortran parallel input/output interface. The fixed machine size format is most appropriate when the data will be re-read by the same code on partitions of the same size (e.g., checkpoint files).
Reference: [15] <author> Thinking Machines Corporation. </author> <title> CM Fortran Utility Library Reference Manual, Version 2.0 Beta, </title> <month> January </month> <year> 1993. </year>
Reference-contexts: Like the generic format, data written in this format can be read by programs executing in partitions of any size. In addition, the serial order format allows the reader to read arrays using shapes and distributions different from that of the writer. See <ref> [15, 14] </ref> for additional details on the CM Fortran parallel input/output interface. The fixed machine size format is most appropriate when the data will be re-read by the same code on partitions of the same size (e.g., checkpoint files).
References-found: 15

