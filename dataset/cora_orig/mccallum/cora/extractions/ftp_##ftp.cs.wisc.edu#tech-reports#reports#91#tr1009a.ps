URL: ftp://ftp.cs.wisc.edu/tech-reports/reports/91/tr1009a.ps
Refering-URL: http://www.cs.wisc.edu/math-prog/tech-reports/
Root-URL: 
Title: PARALLEL CONSTRAINT DISTRIBUTION IN CONVEX QUADRATIC PROGRAMMING  
Author: MICHAEL C. FERRIS 
Date: February 1991 Revised June 1992  
Abstract: We consider convex quadratic programs with large numbers of constraints. We distribute these constraints among several parallel processors and modify the objective function for each of these subproblems with Lagrange multiplier information from the other processors. New Lagrange multiplier information is aggregated in a master processor and the whole process is repeated. Linear convergence is established for strongly convex quadratic programs by formulating the algorithm in an appropriate dual space. The algorithm corresponds to a step of an iterative matrix splitting algorithm for a symmetric linear complementarity problem followed by a projection onto a subspace. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D.P. Bertsekas. </author> <title> Constrained Optimization and Lagrange Multiplier Methods. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: Furthermore, the computation in x3 suggests that the number of parallel iterations required for the solution of the subproblems is largely independent of the number of processors. The difference between this algorithm and standard augmented Lagrangian methods (see <ref> [1, 12, 13] </ref>) is that the multiplier update is carried out explicitly rather than with the traditional gradient updating scheme.
Reference: 2. <author> D.P. Bertsekas and P. Tseng. </author> <title> Partial proximal minimization for convex programming. </title> <type> Manuscript, </type> <month> November </month> <year> 1991. </year>
Reference-contexts: FERRIS the proposed parallel constraint distribution algorithm is equivalent to a step of an iterative matrix splitting method for a symmetric linear complementarity problem followed by a sub-space projection. Another point of view for splitting constraints is presented by Bertsekas and Tseng <ref> [2] </ref>. Their work appeared after the original version of this paper. A word about our notation now.
Reference: 3. <author> R.S. Dembo. </author> <title> The performance of NLPNET, a large-scale nonlinear network optimizer. </title> <journal> Mathematical Programming Study, </journal> <volume> 26 </volume> <pages> 245-248, </pages> <year> 1986. </year>
Reference-contexts: This would result in a large number of problems with network constraints and a single problem treating the linking constraints. Certainly, the network constraints could be exploited by a state-of-the-art nonlinear network code (for example <ref> [3] </ref>) and the coupling constraints are very simple and thus would also be easy to exploit. Secondly, for very large problems, a large number of processors is envisaged for solving the subproblems in parallel.
Reference: 4. <author> M.C. Ferris and O.L. Mangasarian. </author> <title> Parallel constraint distribution. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 1(4) </volume> <pages> 487-500, </pages> <year> 1991. </year>
Reference-contexts: We then solve each of these p subproblems independently, aggregate Lagrange multiplier information from the processors and repeat. The method we describe here is closely related to the one given in <ref> [4] </ref>. References to other constraint distribution algorithms can be found in that paper. The key to our approach lies in the precise form of the modified objective function to be optimized by each processor. <p> The constraint violation was also required to be less than this tolerance. We give two tables below for comparison. Table 1 gives the best results that were obtained using the algorithm described in <ref> [4] </ref> on the Sequent Symmetry S-81 for 5 small linear programs reformulated as in (31). The first three are homemade test problems, while the last two, AFIRO and ADLittle, are from the NETLIB collection [6]. In the tables, an empty column entry signifies that we did not perform the computation. <p> Note also that these results include a heuristic for calculating a step length. Table 2 gives the results for the algorithm outlined in this paper. We remark that this algorithm performs uniformly better than the one described in <ref> [4] </ref>. Furthermore, its implementation is somewhat simpler. Note the strong indication that these results give to the fact that the number of iterations is independent of the number of processors used. 4.
Reference: 5. <author> M. Frank and P. Wolfe. </author> <title> An algorithm for quadratic programming. </title> <journal> Naval Research Logistics Quarterly, </journal> <volume> 3 </volume> <pages> 95-110, </pages> <year> 1956. </year>
Reference-contexts: Thus B C is positive definite and so (19) holds. It remains to show that f is bounded below on positive orthant. The fact that f is bounded below is equivalent to a solution existing by <ref> [5] </ref>. Since M is symmetric and positive semidefinite any solution of minimize z0 f (z) solves LCP (M,q) and conversely. As shown above, any solution of LCP (M,q) leads to a solution of LCP (H,h) which is the dual of (1).
Reference: 6. <author> D.M. Gay. </author> <title> Electronic mail distribution of linear programming test problems. </title> <journal> COAL Newsletter, </journal> <volume> 13 </volume> <pages> 10-12, </pages> <year> 1985. </year>
Reference-contexts: Table 1 gives the best results that were obtained using the algorithm described in [4] on the Sequent Symmetry S-81 for 5 small linear programs reformulated as in (31). The first three are homemade test problems, while the last two, AFIRO and ADLittle, are from the NETLIB collection <ref> [6] </ref>. In the tables, an empty column entry signifies that we did not perform the computation. Note also that these results include a heuristic for calculating a step length. Table 2 gives the results for the algorithm outlined in this paper.
Reference: 7. <author> Z.-Q. Luo and P. Tseng. </author> <title> Error bound and convergence analysis of matrix splitting algorithms for the affine variational inequality problem. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 2(1), </volume> <year> 1992. </year>
Reference-contexts: We will invoke the following merit function to prove linear convergence in the dual space f (z) := 2 Our main theorem will require the following result due to Luo and Tseng <ref> [7, Theorem 2.1] </ref> which we state here for completeness. Proposition 2.3. <p> The proof of this theorem is modeled after the proof of Theorem 3.1 given in <ref> [7] </ref>. Theorem 2.4. Suppose that M is symmetric and positive semidefinite and that f, given by (23), is bounded from below on IR n + . Let fz i g be the iterates generated by the matrix splitting algorithm (18), (19), (20).
Reference: 8. <author> O.L. Mangasarian. </author> <title> Nonlinear Programming. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1969. </year>
Reference-contexts: Each subproblem is then solved and a point (x i+1 l ; s i+1 IR n+m l , l = 1; 2; 3, which satisfies the Karush Kuhn Tucker conditions <ref> [8] </ref> for subproblems (2) is obtained. We define t i+1 l a j ) + t i ; j 6= l (3) s i+1 1 2 6 l + k=1 t i+1 3 7 and jl = s i+1 This completes one iteration of the PCD algorithm.
Reference: 9. <author> O.L. Mangasarian. </author> <title> Normal solutions of linear programs. </title> <journal> Mathematical Programming Study, </journal> <volume> 22 </volume> <pages> 206-216, </pages> <year> 1984. </year>
Reference-contexts: In order to strongly convexify the objective we have used the least two-norm formulation <ref> [10, 9] </ref>, where for * 2 (0; *] for some * &gt; 0, the solution of minimize b T y + * y T y subject to A T y c (31) is the least two-norm solution of (30).
Reference: 10. <author> O.L. Mangasarian and R.R. Meyer. </author> <title> Nonlinear perturbation of linear programs. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 17(6) </volume> <pages> 745-752, </pages> <month> November </month> <year> 1979. </year>
Reference-contexts: In order to strongly convexify the objective we have used the least two-norm formulation <ref> [10, 9] </ref>, where for * 2 (0; *] for some * &gt; 0, the solution of minimize b T y + * y T y subject to A T y c (31) is the least two-norm solution of (30).
Reference: 11. <author> B.A. Murtagh and M.A. Saunders. </author> <title> MINOS 5.0 user's guide. </title> <type> Technical Report SOL 83.20, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1983. </year>
Reference-contexts: The PCD algorithm was implemented on the Sequent Symmetry S-81 shared memory multiprocessor. The subproblems were solved on each processor using MINOS 5.3 a more recent version of <ref> [11] </ref>. The explicit constraints in each subproblem remained fixed throughout the computation but the blocks were not chosen to satisfy the linear independence assumption. We have used the following scheme to update the augmented Lagrangian parameter, fl.
Reference: 12. <author> R.T. Rockafellar. </author> <title> Augmented Lagrange multiplier functions and duality in nonconvex programming. </title> <journal> SIAM Journal on Control, </journal> <volume> 12 </volume> <pages> 268-285, </pages> <year> 1974. </year>
Reference-contexts: Furthermore, the computation in x3 suggests that the number of parallel iterations required for the solution of the subproblems is largely independent of the number of processors. The difference between this algorithm and standard augmented Lagrangian methods (see <ref> [1, 12, 13] </ref>) is that the multiplier update is carried out explicitly rather than with the traditional gradient updating scheme.
Reference: 13. <author> R.T. Rockafellar. </author> <title> Augmented Lagrangians and applications of the proximal point algorithm in convex programming. </title> <journal> Mathematics of Operations Research, </journal> <volume> 1(2) </volume> <pages> 97-116, </pages> <year> 1976. </year> <institution> Computer Sciences Department, University of Wisconsin, </institution> <address> 1210 West Dayton Street, Madison, Wisconsin 53706 E-mail address: ferris@cs.wisc.edu </address>
Reference-contexts: Furthermore, the computation in x3 suggests that the number of parallel iterations required for the solution of the subproblems is largely independent of the number of processors. The difference between this algorithm and standard augmented Lagrangian methods (see <ref> [1, 12, 13] </ref>) is that the multiplier update is carried out explicitly rather than with the traditional gradient updating scheme.
References-found: 13

