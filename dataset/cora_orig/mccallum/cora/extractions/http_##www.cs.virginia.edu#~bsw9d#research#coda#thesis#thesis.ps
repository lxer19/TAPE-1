URL: http://www.cs.virginia.edu/~bsw9d/research/coda/thesis/thesis.ps
Refering-URL: http://www.cs.virginia.edu/~bsw9d/research.html
Root-URL: http://www.cs.virginia.edu
Title: Testing an Optimistically-Replicated Distributed File System  
Author: Brian S. White 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: May 5, 1998  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Adam, M., Hurfin M., Raynal, M., Plouzeau, N. </author> <title> Distributed Debugging Techniques. </title> <type> Technical Report IRIA2-1459, </type> <institution> Institut de Recherche en Informatique et Systems Aleatoires, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: Because each process in P is a sequential program (or a collection or sequential programs), all bugs inherent in sequential programs are subsumed by the distributed system [4]. The bugs in a distributed system may be caused by bugs in the process itself or bugs in the interprocess communication <ref> [1] </ref>. Bugs seen in distributed systems, but not in centralized, sequential systems, may be caused by: distribution, concur-rency, synchronization, and cooperative communication.
Reference: [2] <author> Briggs, J., Jamieson, G., Randall, G., Wand, I. </author> <title> Debugging Distributed Ada Programs. </title> <type> Technical Report YOR-233, </type> <institution> University of York, </institution> <month> June </month> <year> 1994. </year>
Reference: [3] <author> Cheung, W. H., Black, James P., Manning, </author> <title> Eric G A Study of Distributed Debugging. </title> <type> Technical Report WTCM 87-53, </type> <institution> University of Waterloo, </institution> <month> October </month> <year> 1987. </year>
Reference: [4] <author> Cheung, W., Manning, E. </author> <title> A Study of Distributed Debugging. </title> <type> Technical Report WTCM 88-44, </type> <institution> University of Waterloo, </institution> <month> October </month> <year> 1988. </year>
Reference-contexts: Because each process in P is a sequential program (or a collection or sequential programs), all bugs inherent in sequential programs are subsumed by the distributed system <ref> [4] </ref>. The bugs in a distributed system may be caused by bugs in the process itself or bugs in the interprocess communication [1]. Bugs seen in distributed systems, but not in centralized, sequential systems, may be caused by: distribution, concur-rency, synchronization, and cooperative communication. <p> Bugs seen in distributed systems, but not in centralized, sequential systems, may be caused by: distribution, concur-rency, synchronization, and cooperative communication. These bugs include message omission, unanticipated messages, out of order messages, deadlock, untimely process death, livelock, faulty synchronization, partition (due to communication failure), communication overload, and protocol faults <ref> [4] </ref>. 1.3 Distributed File Systems for Mobile Computing A distributed file system attempts to provide one name-space across a set of distributed nodes in order to facilitate file-sharing amongst those nodes.
Reference: [5] <author> Davies, N., Blair, G., Cherverst, K., Friday, A. </author> <title> A Network Emulator to Support the Development of Adaptive Applications. </title> <booktitle> In Proceedings of the Second Usenix Symposium on Mobile and Location Independent Computing. </booktitle> <address> Ann Arbor, Michigan. </address> <month> (April 10-11, </month> <year> 1995). </year>
Reference-contexts: Delayline [9] is another failure emulator that introduces packet delays and faults at the receiving host. This approach also involved recompilation and re-writing of low level modules and functions such as recv and send. The Lan-caster Emulator <ref> [5] </ref> takes a completely opposite approach. In this framework, packets are routed to a central distributor which enqueues packets, and hence may introduce a delay or drop the packet. This approach again modifies system calls. A recent product, in the same vein as the Lancaster Emulator, is WANspoof [11].
Reference: [6] <author> Ebling, M., Satyanarayanan, M. SynRGen: </author> <title> An Extensible File Reference Generator. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems (May 1994). </booktitle>
Reference-contexts: This pattern does not necessarily hold for other users or applications; databases are a good example of an application that do not exhibit such a tight spatial locality of reference. File systems are frequently designed around usage assumptions, particularly the degree and nature of locality of file reference <ref> [6] </ref>. SynRGens designers believed the ability to test file system performance based on different usage assumptions was useful. For this reason, pathname iterators were developed. Pathname iterators encapsulate locality information. <p> A volume configuration file, like that in figure 7, is interpreted by the mkvol preprocessor. mkvol translates the volume information encapsulated in the file into a C data structure that is accessed by micromodels through library routines <ref> [6] </ref>. SynRGen is exible and appears to strike a balance between realistic file system simulation and an effective debugging tool. It caters to the sophistication of its users.
Reference: [7] <author> Fagerstrom, J., Larsson, Y., Stromberg, L. </author> <title> Distributed Debugging - collected ideas. </title> <type> Technical Report LINI 86-21, </type> <institution> Linkoping University, </institution> <month> June </month> <year> 1986. </year>
Reference: [8] <author> Han, S., Rosenberg, H., Shin, K. DOCTOR: </author> <title> An IntegrateD SOftware Fault InjeCTiOn EnviRonment. </title> <type> Technical Report, MICE 192, </type> <institution> University of Michigan. </institution> <month> (December </month> <year> 1993). </year>
Reference-contexts: With the advent of distributed systems has come the need to test and emulate networks. Therefore, such emulators and simulators have become popular both in academia and industry. The fault injection methodology of DOCTOR <ref> [8] </ref> is similar in spirit to the filters employed by Snip and Filcon. DOCTOR allows for the injection of memory, processor, and communication faults. A different type of failure may be specified for each incoming and outgoing link. DOCTOR may drop, delay, or alter the body or header of packets.
Reference: [9] <author> Ingham, D., Parrington, G. Delayline: </author> <title> A Wide Area Network Emulation Tool. </title> <booktitle> Computing Systems. </booktitle> <year> (1994). </year>
Reference-contexts: This framework involves tight integration of many processes, such as the FIA and ECM, and also involves changes to low-level system functionality. This approach is both much more controlled and much more complicated and intrusive than Snip and Filcon. Delayline <ref> [9] </ref> is another failure emulator that introduces packet delays and faults at the receiving host. This approach also involved recompilation and re-writing of low level modules and functions such as recv and send. The Lan-caster Emulator [5] takes a completely opposite approach.
Reference: [10] <author> Kistler, J. </author> <title> Disconnected Operation in a Distributed File System. </title> <type> Ph.D. thesis, </type> <institution> School of Computer Science, Car-negie Mellon University, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: File system calls are intercepted by Venus and serviced locally (from its cache) if possible. As a last resort, Venus fetches file data and attributes from Vice via a remote procedure call (RPC) mechanism <ref> [10, 15] </ref>. Coda was designed to exploit the weak connectivity inherent in wide-area networks (WANs). As such, a Coda client can mimic connectivity despite disconnection from Vice. The Coda architects employed two primary techniques to combat network failure: server replication and disconnected operation [19]. <p> Consider a transaction T which overwrites every update by a previous transaction T. Since Ts updates will not be visible, the associated update records may be discarded. Identity subsequences, a transaction whose initial and final values are identical, are also discarded <ref> [10] </ref>. Where resolution is Codas means of reconciling conicts between servers, reintegration is the process whereby clients reconcile disconnected updates with servers. This activity is initiated by Venus on a per-volume basis, as connectivity changes permit and consists of transferring the update logs (described above) to Coda servers. <p> This activity is initiated by Venus on a per-volume basis, as connectivity changes permit and consists of transferring the update logs (described above) to Coda servers. Reintegration is a three-step process explained in detail in <ref> [10] </ref>. However, the actual transfer of state is accomplished at the server, 6 where a thread iterates over the update log, executing each transaction. This sequence of operations does not occur until a set of preconditions has been met at the client. <p> Further, all in-progress transactions must finish and the user who performed transactions in the update log must have sufficient authentication before reintegration proceeds <ref> [10] </ref>. In the original design, reintegration was used only when a volume was disconnected from its AVSG. During connection, updates were synchronously sent in parallel to all members of the AVSG. Unfortunately, such a procedure consumes significant bandwidth, which is at a premium during a weak connection.
Reference: [11] <author> Kram, P. </author> <title> (kram@transarc.com) Emulating Wide Area Networks to Test Distributed Systems. </title> <booktitle> To be presented at The Eleventh Annual IBM Testing Symposium. </booktitle> <address> Toronto, Canada. </address> <month> (April 27-28, </month> <year> 1998). </year>
Reference-contexts: In this framework, packets are routed to a central distributor which enqueues packets, and hence may introduce a delay or drop the packet. This approach again modifies system calls. A recent product, in the same vein as the Lancaster Emulator, is WANspoof <ref> [11] </ref>. As its name implies, WANspoof is a network emulator that attempts to control properties of LAN links in order to provide the appearance of WAN-like connectivity and persistence.
Reference: [12] <author> Kumar, P., Satyanarayanan, M. </author> <title> Log-Based Directory Resolution in the Coda File System. </title> <booktitle> In Proceedings of the Second International Conference on Parallel and Distributed Information Systems. </booktitle> <address> San Diego, California. (Jan-uary 20-22, </address> <year> 1993). </year>
Reference-contexts: Update is a frequent operation, so its performance is a critical concern. In order to reduce the latency seen by the user, control is returned after COP1. Server throughput is increased by piggybacking during COP2 <ref> [19, 12] </ref>. This has unfortunate repercussions on testing. Consider a test which updates a file and verifies that update by comparing the files version vector before and after the update. If the update protocol functions correctly, the test would expect to see the version vector incremented for each connected server. <p> The overall goal of this test framework is to recreate specific test cases, to provide a means of easily instantiating such cases, and to allow for regression testing. Each of this can be facilitated through the use of the make utility <ref> [12] </ref>. A test area consists of a Makerules file that describes generic targets, a utilities directory containing executables needed by the tests, a makefile in each directory which specifies that directorys sub-directories and any specific rules needed for that test, and the tests sub-directories themselves.
Reference: [13] <author> Larsson, Y. </author> <title> A Testbed Environment for Debugging Distributed Systems. </title> <type> Technical Report LINI 88-23, </type> <institution> Linkop-ing University, </institution> <month> August </month> <year> 1988. </year>
Reference: [14] <author> McCarthy, A. </author> <title> Unit and Regression Testing. </title> <journal> Dr. </journal> <note> Dobbs Journal (February, </note> <year> 1997). </year>
Reference: [15] <author> Mummert, L. </author> <title> Exploiting Weak Connectivity in a Distributed File System. </title> <type> Ph.D. thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: File system calls are intercepted by Venus and serviced locally (from its cache) if possible. As a last resort, Venus fetches file data and attributes from Vice via a remote procedure call (RPC) mechanism <ref> [10, 15] </ref>. Coda was designed to exploit the weak connectivity inherent in wide-area networks (WANs). As such, a Coda client can mimic connectivity despite disconnection from Vice. The Coda architects employed two primary techniques to combat network failure: server replication and disconnected operation [19]. <p> to be secure and connected via a local-area network (LAN), while no assumptions are made on clients (e.g. their connection to Vice may be via a relatively slow serial line). 2.2 Server Replication Coda utilizes replicated servers to provide greater file availability by decreasing the probability that data is unavailable <ref> [15] </ref>. <p> When the network is then healed and a client attempts to access a file in the volume, Coda must ensure that the most recent version is obtained. This is accomplished via a read-status-from-all, read-data-from-one, 5 write-all file access pattern <ref> [15] </ref>. Under such a system, a client reads data from a preferred server, which may be selected at random from the AVSG. The client then contacts other members of the AVSG to ensure that the data fetched from the preferred server is not stale. <p> Any updates to cached data are logged locally. When Venus receives a weak connection to Vice, it enters the write disconnected state. Weak connection is characterized by limited bandwidth, such as that characterized by limited bandwidth, intermittence, or expense <ref> [15] </ref>. The write disconnected state is a cross between the two previous states. As in the hoarding state, cache misses are serviced and callbacks are utilized to maintain coherence. However, as in the emulating state, updates are logged locally and later propagated asynchronously. <p> Venus maintains at least 18 threads, while a server requires at least 15. Threads whose activity has been touched upon in this thesis are listed in the tables below. A more thorough examination is presented in <ref> [15] </ref>. In order to provide efficient and seamless behavior (especially in the face of disconnection and weak connections), the threads interact in complex and asynchronous manners. Returning control from an operation while it completes in the background is critical in minimizing latency. <p> The s is a version stamp, which is incremented when any file in the volume is updated. A client may only obtain a callback on a volume when its version stamp matches that of the server <ref> [15] </ref>. The server maintains a list of callbacks. In this diagram, a &lt;host, file/volume name&gt; pair describe a callback, where the server promises to keep host up-to-date on the status of the associated file or volume. The nodes are assumed to be strongly-connected, unless separated by a dashed line.
Reference: [16] <author> Pressman, R. </author> <title> Software Engineering: A Practitioners Approach. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1997. </year>
Reference-contexts: The focus then is on black-box testing, which examines some fundamental aspect of the system with little regard for the internal logical structure of the software. This should be contrasted with white-box testing, or close examination of procedural detail <ref> [16] </ref>. Because it reects the design of the system and only requires examining state visible to the user, end-to-end testing appears to be the correct approach to such testing. End-to-end testing is concerned with system behavior at the endpoints (i.e. does the input match the expected output).
Reference: [17] <author> Okasaki, M. </author> <note> Snip man page. </note>
Reference-contexts: Snip, or stochastic failure inducer, injects failures into a network using the failure library described above. Such failures are generated probabilistically based on the grammar denoted in Table 3 <ref> [17] </ref>. A FailureSequence describes a step-wise set of network configurations to establish. A FailureSequence may be specified as an unconditional failure statement (Failure) or via a probability distribution (&lt; ProbFailure &gt;). A probability distribution consists of a probability and an associated failure statement.
Reference: [18] <author> Satyanarayanan, M., Ebling, M., Raiff, J. </author> <title> Coda File System: User & System Administrators Manual. </title> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address> <month> December, </month> <year> 1995. </year> <month> 40 </month>
Reference-contexts: The delay can be used to degrade network latency and is used to simulate networks slower than the host network. This delay applies regardless of the probability. Filters may be marked as applicable to incoming, outgoing, or incoming and outgoing packets <ref> [18] </ref>. It is important to note that filters may actually enable connectivity, as opposed to retarding it. For example, imagine an all-inclusive bi-directional filter with speed 0 is inserted at poulenc. Now, no packet received by poulenc will be passed on to the application layer. <p> Though the previous description of filters is sufficient for the needs of this work, they provide more fine-grained control of an applications view of the network. Above the logical contents of a filter were described, here the actual parameters are specified <ref> [18] </ref>: host name or IP address (in a.b.c.d form) of machine whose traffic should be perturbed. -1 may be used as a wildcard in specifying any component of the IP address. For example, -1.-1.-1.-1 applies to any host. color a number between 0 and 254, or -1.
Reference: [19] <author> Satyanarayanan, M., Kistler, J.., Kumar, P., Okasaki, M., Siegel, E., Steere, .D. Coda: </author> <title> A Highly Available File System for a Distributed Workstation Environment. </title> <note> In IEEE Transactions on Computers (April 1990). </note>
Reference-contexts: As primary goals, Coda aims to extend AFS to provide constant data availability and an integration with portable computers <ref> [19] </ref>. As in the AFS model, Coda adheres to the client/server paradigm. The client consists of a user-level process named Venus and a kernel-level MiniCache; a set of Coda servers is referred to as Vice. <p> Coda was designed to exploit the weak connectivity inherent in wide-area networks (WANs). As such, a Coda client can mimic connectivity despite disconnection from Vice. The Coda architects employed two primary techniques to combat network failure: server replication and disconnected operation <ref> [19] </ref>. <p> The servers in the AVSG then shoulder the burden of reconciling their replicas. This process, executed between servers, is referred to as resolution. Coda version vectors (CVVs) are used to detect write-write conicts on individual files and directories <ref> [19] </ref>. A CVV is a vector of integers, at least one entry per server in a VSG. Each server maintains its own copy of a CVV. <p> So, if foo is updated and this update is seen by all three servers, the resulting CVV is: [5 4 5 0 0 0 0 0]. When Coda detects a conict (done with CVVs as described in <ref> [19] </ref>), it attempts to automatically reconcile the data. Files are untyped byte streams that do not provide information pertinent to automation. Directories, on the other hand, have meaningful and understood structure. Coda can automatically resolve all but three types of conicts. <p> The first class, update/update conicts, are characterized by protection modifications to partitioned replicas in a directory. Remove/update conicts are characterized by an object being updated in one partition and removed in another. Name/name conicts arise when new objects with identical names are created in disparate partitions <ref> [19] </ref>. When Coda is unable to automate resolution of a file or directory, the user may use a manual repair tool to examine the con-icting items and select one to persist. 2.3 Disconnected Operation Disconnected operation is a means of masking disconnection by servicing file requests through a local cache. <p> These records may be reintegrated. Cache coherency and the status of updates on a particular server will play a role in testing. Cache consistency is maintained through callbacks, whereby servers notify clients of changes to their cached files <ref> [19] </ref>. Therefore, the coherence protocol is invalidation-based, as opposed to a validate-on-use strategy. Further, the granularity of cache coherence is at the session level. A more strict and consistent read/write level of coherence would require significantly more client/server interaction. <p> Update is a frequent operation, so its performance is a critical concern. In order to reduce the latency seen by the user, control is returned after COP1. Server throughput is increased by piggybacking during COP2 <ref> [19, 12] </ref>. This has unfortunate repercussions on testing. Consider a test which updates a file and verifies that update by comparing the files version vector before and after the update. If the update protocol functions correctly, the test would expect to see the version vector incremented for each connected server.
Reference: [20] <author> Satyanarayanan, M., Noble, B. </author> <title> The Role of Trace Modulation in Building Mobile Computing Systems. </title> <booktitle> In Proceedings of the 6th Workshop on Hot Topics in Operating Systems (May 1997). </booktitle>
Reference-contexts: Also, as above, large volumes of data are produced and their maintenance and storage is a concern. It should be noted that means of using the above techniques in a distributed fashion do exist. In fact, both output debugging and tracing <ref> [20] </ref> are currently used in debugging Coda. Nevertheless, most of the above techniques are inconsistent with the goals of this thesis. <p> Trace modulation provides application-transparent emulation of a target network on a LAN. This is done by modifying the network layer in the operating systems protocol stack so that it drops or delays packets according to a list of performance parameters modeling a network <ref> [20] </ref>. This approach seems to have a number of deficiencies in addressing the thesis statement. The primary concern lies in the generation of traces. The method of recording a trace does not seem to be consistent with our goals. <p> Trace-based tests strive to simulate actual applications by first obtaining a finger print (or trace) of system calls from a representative execution of that application and then executing them <ref> [20] </ref>. The first section of this chapter will examine an approach that lies somewhere in between. SynRGen attempts to simulate user behavior by following a pre-existing model of that behavior.
Reference: [21] <author> Yang, Z., Marsland, T. </author> <title> Global Snapshots for Distributed Debugging: An Overview. </title> <type> Technical Report ALB 92-03, </type> <institution> University of Alberta, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: A distributed system, D, is defined by the tuple (P, C), where P is a finite set of processes and C is a finite set of communication channels <ref> [21] </ref>. Because each process in P is a sequential program (or a collection or sequential programs), all bugs inherent in sequential programs are subsumed by the distributed system [4].
References-found: 21

