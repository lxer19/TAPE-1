URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1993/GIT-CC-93-10.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.93.html
Root-URL: 
Email: e-mail: rjohn@cc.gatech.edu  
Title: Causal Memory: Implementation, Programming Support and Experiences  
Author: Ranjit John Mustaque Ahamad 
Keyword: Distributed Shared Memory, Weakly ordered systems, Causal memory  
Web: GIT-CC-93-10  
Address: Atlanta, GA 30332-0280 USA  
Affiliation: College of Computing Georgia Institute of Technology  
Abstract: Weakly ordered systems which use synchronization information have been proposed to reduce the frequency of communication between processors. We have implemented a weakly ordered system based on the Causal memory model. We provide language and runtime support which allow programs to run efficiently on Causal memory. Actual implementation results show a significant reduction in the number of messages when compared to a system maintaining a consistent shared memory. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering anew definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year> <note> DRAFT June 21, 1993 23 </note>
Reference-contexts: Release Consistency allows remote memory accesses to be propagated asynchronously, as long as they complete by the end of the critical section. Such systems guarantee sequentially consistent behavior only for programs which do not have data races <ref> [1] </ref>. The Munin system [9] implements Release Consistency in software, by delaying propagating the changes made inside a critical section till the the lock is released. Entry Consistency [10] and Lazy Release Consistency [20] reduce communication further by propagating changes only to the processor which acquires the lock. <p> Synchronization is achieved through explicit language provided DRAFT June 21, 1993 7 mechanisms, such as semaphores, locks and barriers, or implicitly by busy-waiting. Dubois et al. [16] define a weakly ordered system where dependency conditions between processes are limited to the synchronization variables. Adve & Hill <ref> [1] </ref> formalize this by defining a happens-before relation, which orders two operations on different processes, only if, there is a synchronization operation between them. <p> Weakly ordered systems require that the synchronization operations are made explicit to the memory system. Inter-processor dependences can be then limited to the synchronization operations <ref> [1, 20, 10] </ref>. Parallel and distributed programs achieve parallelism by forking computation onto different processors. Domain decomposition is a commonly used method for developing parallel programs, where a "parent" process initializes the domains, and then forks "child" processes on different processors, each working on a different partition.
Reference: [2] <author> Sarita V. Adve and Mark D. Hill. </author> <title> A unified formalization of four shared-memory models. </title> <type> Technical Report 1051, </type> <institution> University of Wisconsin, Madison, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Compile time support is used to track data items which have been modified. Extra programming effort may be required to identify just the right data to be associated with a synchronization variable. There have been several hardware implementations of weakly ordered systems and are described in <ref> [18, 2, 28, 11, 17] </ref>. 6 Conclusions We have shown that a causal DSM can be efficiently implemented. With adequate programming support, causal DSM significantly reduces the frequency of communication between processors. Also, scalable systems can be built since accesses to data do not require expensive global synchronization.
Reference: [3] <author> Y. Afek, G. Brown, and M. Merritt. </author> <title> A lazy cache algorithm. </title> <booktitle> In Proceedings of the SPAA, </booktitle> <pages> pages 209-223, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: Kai Li's Ivy system [24] is based on a writer invalidate-readers protocol which implements atomic memory. Such implementations maintain the real time order by restricting a page (which is the smallest unit of sharing) to a single writer or multiple readers at the same time. In <ref> [13, 3] </ref> delayed invalidations and buffered writes are used to capture more efficient executions permitted by sequential consistency. Maintaining sequential consistency on a network of distributed machines, can be shown to limit performance and does not lend itself to scaling [25].
Reference: [4] <author> Mustaque Ahamad, James E. Burns, Phillip W. Hutto, and Gil Neiger. </author> <title> Causal memory. </title> <booktitle> In Proceedings of the 5th International Workshop on Distributed Algorithms, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: Section 4 discusses the programming support required to run applications efficiently on Causal memory. In Section 5 we describe how to program applications and give performance results. We talk about related work in Section 6 and conclude in Section 7. 2 Causal Memory In <ref> [4] </ref> we define correct execution on Causal memory by the possible set of values a read to a shared location can return.
Reference: [5] <author> Mustaque Ahamad, Phillip W. Hutto, and Ranjit John. </author> <title> Implementing and programming causal distributed shared memory. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: Entry Consistency [10] and Lazy Release Consistency [20] reduce communication further by propagating changes only to the processor which acquires the lock. We explore a weakly ordered memory system, based on Causal memory <ref> [5] </ref>. The Causal memory model requires that a read of a location return a value that is consistent with all causally related reads and writes to that location.
Reference: [6] <author> R. Ananthanarayan, Ranjit John, Ajay Mohindra, Mustaque Ahamad, and Umakishore Ra-machandran. </author> <title> An evaluation of state sharing techniques in distributed operating systems. </title> <type> Technical report, </type> <institution> College of Computing, Georgia Institute of Technology, Atlanta, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Once the program is shown to run correctly, we annotate the data. In this section, we program an iterative linear equation solver and a distributed calendar service. These programs are simple enough to illustrate how the data annotation is done. In <ref> [6] </ref>, we use NAS kernel applications, a traveling salesperson program and a matrix multiplication to compare performance on Causal memory with other implementations of DSM. DRAFT June 21, 1993 17 4.1 Linear Equation Solver on Causal Memory Large systems of linear equations often arise in many scientific and engineering applications.
Reference: [7] <author> R. Ananthanarayanan. </author> <title> Clouds C++ reference manual. </title> <type> Technical Report GIT-CC-91-07, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <year> 1991. </year>
Reference-contexts: As cache sizes grow this overhead could be significant. Currently we are investigating ways to reduce this cost. 3 Programming Support Programming on Causal memory requires language and runtime support. The Clouds Distributed-C++ <ref> [7] </ref> compiler and the runtime system have been extended to achieve the following. * The language provides the ability to define multiple user data segments within an object.
Reference: [8] <author> Gerard M. Baudet. </author> <title> The Design and Analysis of Algorithms for Asynchronous Multiprocessors. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <month> April </month> <year> 1978. </year>
Reference-contexts: Another approach is to perform the invalidations periodically to ensure that updates will eventually get propagated. The user can specify a time interval after which the page is unmapped. It DRAFT June 21, 1993 13 is useful for programming asynchronous algorithms <ref> [8] </ref> and certain distributed applications which tolerate stale values. 2.5 A note on Scalability The implementation we sketched requires at the maximum, three messages to satisfy a pagefault request. Unlike traditional implementations of atomic DSM, Causal memory does not require expensive multicasts or broadcasts.
Reference: [9] <author> J. K. Bennett, J. B. Carter, and W. Zwaenepoel. </author> <title> Adaptive software cache management for distributed shared memory architectures. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 125-135, </pages> <month> May </month> <year> 1990. </year> <note> DRAFT June 21, 1993 24 </note>
Reference-contexts: Release Consistency allows remote memory accesses to be propagated asynchronously, as long as they complete by the end of the critical section. Such systems guarantee sequentially consistent behavior only for programs which do not have data races [1]. The Munin system <ref> [9] </ref> implements Release Consistency in software, by delaying propagating the changes made inside a critical section till the the lock is released. Entry Consistency [10] and Lazy Release Consistency [20] reduce communication further by propagating changes only to the processor which acquires the lock. <p> We further elaborate on how this is achieved in the next section on programming support. * User annotations Pages which are only written into during initialization and never again, will not have their timestamps incremented. Such pages would get unnecessarily invalidated at each pagefault. The Munin system <ref> [9] </ref> has identified various classes of data access patterns which are characteristic of shared memory programs. Typical data access patterns identified are Private, WriteOnce and WriteShared among others. We provide language mechanisms, described in the next section, for a user to pass this information to the memory system. <p> The keyword segment specifies the smallest logical unit of consistency maintenance. Segments can be tagged as Static or Causal. In contrast to Munin <ref> [9] </ref>, this labeling for ordinary data does not imply multiple protocols. We use this information to reduce the unnecessary invalidations that would otherwise occur. Data which is used privately by a processor, globally initialized data and data which is written only once during an initialization phase is tagged as Static. <p> Simple message counting arguments are presented to show its superior performance to conventional atomic DSM's. Munin <ref> [9] </ref> implements release consistency in software by delaying propagating the changes made inside a critical section, till the lock is released. Modifications to shared data is tracked by using the page fault mechanism.
Reference: [10] <author> Brian N. Bershad and Matthew J. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Such systems guarantee sequentially consistent behavior only for programs which do not have data races [1]. The Munin system [9] implements Release Consistency in software, by delaying propagating the changes made inside a critical section till the the lock is released. Entry Consistency <ref> [10] </ref> and Lazy Release Consistency [20] reduce communication further by propagating changes only to the processor which acquires the lock. We explore a weakly ordered memory system, based on Causal memory [5]. <p> Weakly ordered systems require that the synchronization operations are made explicit to the memory system. Inter-processor dependences can be then limited to the synchronization operations <ref> [1, 20, 10] </ref>. Parallel and distributed programs achieve parallelism by forking computation onto different processors. Domain decomposition is a commonly used method for developing parallel programs, where a "parent" process initializes the domains, and then forks "child" processes on different processors, each working on a different partition. <p> DRAFT June 21, 1993 22 LRC uses a history based mechanism to record the modified data which has to be transmitted with the lock transfers. Entry Consistency <ref> [10] </ref> requires that all shared data be associated with a synchronization variable. When a processor acquires a synchronization variable, only modified data associated with the synchronization variable need to be transferred. Compile time support is used to track data items which have been modified.
Reference: [11] <author> R. Bisiani and M. Ravishankar. </author> <title> PLUS: A distributed shared memory system. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 115-124, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Compile time support is used to track data items which have been modified. Extra programming effort may be required to identify just the right data to be associated with a synchronization variable. There have been several hardware implementations of weakly ordered systems and are described in <ref> [18, 2, 28, 11, 17] </ref>. 6 Conclusions We have shown that a causal DSM can be efficiently implemented. With adequate programming support, causal DSM significantly reduces the frequency of communication between processors. Also, scalable systems can be built since accesses to data do not require expensive global synchronization.
Reference: [12] <author> Fabienne Boyer. </author> <title> A causal distributed shared memory based on external pagers. </title> <booktitle> In Proceedings of the 2nd Usenix Mach Symposium, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: The two values for the listener daemon case correspond to the cases where the if statement is false or true ( in which case, there is a write to the flag). 5 Related Work Boyer <ref> [12] </ref> describes an implementation of causal DSM on Mach using external pagers. Simple message counting arguments are presented to show its superior performance to conventional atomic DSM's. Munin [9] implements release consistency in software by delaying propagating the changes made inside a critical section, till the lock is released.
Reference: [13] <author> G. Brown. </author> <title> Asynchronous multicaches. </title> <journal> Distributed Computing, </journal> <volume> 4(1) </volume> <pages> 331-362, </pages> <year> 1990 1990. </year>
Reference-contexts: Kai Li's Ivy system [24] is based on a writer invalidate-readers protocol which implements atomic memory. Such implementations maintain the real time order by restricting a page (which is the smallest unit of sharing) to a single writer or multiple readers at the same time. In <ref> [13, 3] </ref> delayed invalidations and buffered writes are used to capture more efficient executions permitted by sequential consistency. Maintaining sequential consistency on a network of distributed machines, can be shown to limit performance and does not lend itself to scaling [25].
Reference: [14] <author> Nicholas Carriero and David Gelernter. </author> <title> How to Write Parallel Programs. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass. </address>
Reference-contexts: In these cases, although the reduction in the number of messages would be still significant, its effect on the total completion time will be reduced. 4.2 Distributed Calendar Consider the problem of implementing a distributed calendar service <ref> [14] </ref>. Every user in the system maintains an appointment calendar, which can be browsed by others. In addition, occasionally there could be a need to schedule a common meeting time for some group of people.
Reference: [15] <author> D. R. Cheriton. </author> <title> Problem-oriented shared memory: A decentralized approach to distributed systems design. </title> <booktitle> In Proceedings of the 6th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 190-197, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: Certain cooperative distributed applications, for example a distributed calendar service, may tolerate stale values. On the other hand, data used to achieve implicit synchronization may require writes to be immediately made visible. Other such examples are given in <ref> [15] </ref>. With adequate programming support, such application related information can be utilized by a causal memory system to enhance performance. 2.3 Synchronization and Forking Causal memory has been defined using the read/write model of shared memory. But, communication between processors is not limited to the read/write operations.
Reference: [16] <author> M. Dubois, C. Scheurich, and F. A. Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1986. </year>
Reference-contexts: In [13, 3] delayed invalidations and buffered writes are used to capture more efficient executions permitted by sequential consistency. Maintaining sequential consistency on a network of distributed machines, can be shown to limit performance and does not lend itself to scaling [25]. Dubois et al. <ref> [16] </ref> observe that parallel programs define their own consistency requirements through the means of synchronization operations. They define a weakly ordered system, where synchronization operations are made explicit to the memory system and consistency maintenance DRAFT June 21, 1993 2 is only performed at synchronization points. <p> Parallel and distributed programs, typically, define their consistency requirements through the use of synchronization operations. Synchronization is achieved through explicit language provided DRAFT June 21, 1993 7 mechanisms, such as semaphores, locks and barriers, or implicitly by busy-waiting. Dubois et al. <ref> [16] </ref> define a weakly ordered system where dependency conditions between processes are limited to the synchronization variables. Adve & Hill [1] formalize this by defining a happens-before relation, which orders two operations on different processes, only if, there is a synchronization operation between them.
Reference: [17] <author> Michel Dubois, Jin Chin Wang, Luiz A. Barroso, Kangwoo Lee, and Yung-Syau Chen. </author> <title> Delayed consistency and its effects on the miss rate of parallel programs. </title> <type> Technical report, </type> <institution> Dept. of Electrical Engineering Systems, USC, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Compile time support is used to track data items which have been modified. Extra programming effort may be required to identify just the right data to be associated with a synchronization variable. There have been several hardware implementations of weakly ordered systems and are described in <ref> [18, 2, 28, 11, 17] </ref>. 6 Conclusions We have shown that a causal DSM can be efficiently implemented. With adequate programming support, causal DSM significantly reduces the frequency of communication between processors. Also, scalable systems can be built since accesses to data do not require expensive global synchronization.
Reference: [18] <author> Kourosh Gharchorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared memory multipro DRAFT June 21, </title> <booktitle> 1993 25 cessors. In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: They define a weakly ordered system, where synchronization operations are made explicit to the memory system and consistency maintenance DRAFT June 21, 1993 2 is only performed at synchronization points. The DASH multiprocessor <ref> [18] </ref> is a weakly ordered system which implements a memory model called Release Consistency. Release Consistency allows remote memory accesses to be propagated asynchronously, as long as they complete by the end of the critical section. <p> Compile time support is used to track data items which have been modified. Extra programming effort may be required to identify just the right data to be associated with a synchronization variable. There have been several hardware implementations of weakly ordered systems and are described in <ref> [18, 2, 28, 11, 17] </ref>. 6 Conclusions We have shown that a causal DSM can be efficiently implemented. With adequate programming support, causal DSM significantly reduces the frequency of communication between processors. Also, scalable systems can be built since accesses to data do not require expensive global synchronization.
Reference: [19] <author> Maurice P. Herlihy and Jeannette M. Wing. </author> <title> Linearizability: A correctness condition for concurrent objec ts. </title> <journal> ACM Transactions on Programming Languages, </journal> <volume> 12(3) </volume> <pages> 463-492, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: We use a model that is similar to those used by Misra [27] and by Herlihy and Wing <ref> [19] </ref>. We define a system to be a finite set of processors that interact via a shared memory that consists of a finite set of locations. A processor's interaction with the memory is through a series of read and write operations on the memory.
Reference: [20] <author> Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th International Symposium of Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: Such systems guarantee sequentially consistent behavior only for programs which do not have data races [1]. The Munin system [9] implements Release Consistency in software, by delaying propagating the changes made inside a critical section till the the lock is released. Entry Consistency [10] and Lazy Release Consistency <ref> [20] </ref> reduce communication further by propagating changes only to the processor which acquires the lock. We explore a weakly ordered memory system, based on Causal memory [5]. <p> Weakly ordered systems require that the synchronization operations are made explicit to the memory system. Inter-processor dependences can be then limited to the synchronization operations <ref> [1, 20, 10] </ref>. Parallel and distributed programs achieve parallelism by forking computation onto different processors. Domain decomposition is a commonly used method for developing parallel programs, where a "parent" process initializes the domains, and then forks "child" processes on different processors, each working on a different partition. <p> Modifications to shared data is tracked by using the page fault mechanism. Dirty pages are compared with a copy of the original page and the modified data is propagated to all processors. Lazy Release Consistency <ref> [20] </ref> tracks the causal dependencies between writes, acquires and releases and propagates the writes by piggy-backing the modified data on lock transfer messages. DRAFT June 21, 1993 22 LRC uses a history based mechanism to record the modified data which has to be transmitted with the lock transfers.
Reference: [21] <author> Prince Kohli, Gil Neiger, and Mustaque Ahamad. </author> <title> A characterization of scalable shared memories. </title> <booktitle> In Proceedings of the 22nd International Conference of Parallel Processing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: We use a more generalized framework here, since this can be used to classify a range of different memory models <ref> [21] </ref>. 2.1 Formal Definition This section formally describes the system that underlies our implementation. We use a model that is similar to those used by Misra [27] and by Herlihy and Wing [19]. <p> Given this formalism we can define a variety of memory consistency models, by putting constraints on the allowable histories for any execution. <ref> [21] </ref> defines different memory models based on this formalism. Let H p+w be the history obtained from H by deleting all events of all read operations executed by processes other than p.
Reference: [22] <author> Leslie Lamport. </author> <title> Time, clocks and the ordering of events. </title> <journal> Communications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: A "happens before" relation in the sense defined by Lamport <ref> [22] </ref> can also be defined for shared memory. We denote this causal relation by H ; and it combines the ! relation with the read-by relation 7!.
Reference: [23] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> ACM TOCS, </journal> <volume> c-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: The DSM is an interface between the program and the memory which provides an ordering on the reads and writes, consistent with the memory model chosen. Ideally a distributed shared memory should provide all the consistency guarantees of a true shared memory. Lamport <ref> [23] </ref> defined a memory model called sequential consistency which provided such properties. Atomic memory [27] is a stronger memory model which requires that the memory maintain the real time order in which the reads and writes occur.
Reference: [24] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM TOCS, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Lamport [23] defined a memory model called sequential consistency which provided such properties. Atomic memory [27] is a stronger memory model which requires that the memory maintain the real time order in which the reads and writes occur. Kai Li's Ivy system <ref> [24] </ref> is based on a writer invalidate-readers protocol which implements atomic memory. Such implementations maintain the real time order by restricting a page (which is the smallest unit of sharing) to a single writer or multiple readers at the same time. <p> Programs are written without using the keyword segment. If data is not annotated, a centralized manager invalidation protocol similar to the one described in <ref> [24] </ref> is used. Once the program is shown to run correctly, we annotate the data. In this section, we program an iterative linear equation solver and a distributed calendar service. These programs are simple enough to illustrate how the data annotation is done. <p> DRAFT June 21, 1993 17 4.1 Linear Equation Solver on Causal Memory Large systems of linear equations often arise in many scientific and engineering applications. Li <ref> [24] </ref> investigated such an application and found that good speedups can be obtained on atomic DSM's. We show here that even better performance can be obtained on Causal memory. <p> The array x is partitioned so that there is no false sharing. We compare its performance on Causal memory with an atomic DSM implementation. The atomic DSM implementation uses a centralized manager writer-invalidate-readers protocol <ref> [24] </ref>. Multicast communication is not used for invalidating readers caching a dirty page; rather separate messages need to be sent. The data partitioning was done identically in both cases. The computation was done on Sun 3/60's.
Reference: [25] <author> Richard J. Lipton and Jonathan S. Sandberg. </author> <title> PRAM: A scalable shared memory. </title> <type> Technical Report CS-TR-180-88, </type> <institution> Princeton University, Department of Computer Science, </institution> <month> September </month> <year> 1988. </year>
Reference-contexts: In [13, 3] delayed invalidations and buffered writes are used to capture more efficient executions permitted by sequential consistency. Maintaining sequential consistency on a network of distributed machines, can be shown to limit performance and does not lend itself to scaling <ref> [25] </ref>. Dubois et al. [16] observe that parallel programs define their own consistency requirements through the means of synchronization operations. They define a weakly ordered system, where synchronization operations are made explicit to the memory system and consistency maintenance DRAFT June 21, 1993 2 is only performed at synchronization points.
Reference: [26] <author> F. Mattern. </author> <title> Time and global states of distributed systems. </title> <booktitle> In Proceedings of the International Workshop on Parallel and Distributed Algorithms, </booktitle> <year> 1989. </year> <note> DRAFT June 21, 1993 26 </note>
Reference-contexts: The keeper field indicates the node which has the latest copy of the page; if it is the data server supplies the page. The access field specifies the access privileges to the page on the local processor. We use vector timestamps <ref> [26] </ref> to capture the evolving causal relationships. A vector timestamp is associated with each page and each processor P i carries a timestamp V T i .
Reference: [27] <author> J. Misra. </author> <title> Axioms for memory access in asynchronous hardware systems. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 8(1) </volume> <pages> 142-153, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: Ideally a distributed shared memory should provide all the consistency guarantees of a true shared memory. Lamport [23] defined a memory model called sequential consistency which provided such properties. Atomic memory <ref> [27] </ref> is a stronger memory model which requires that the memory maintain the real time order in which the reads and writes occur. Kai Li's Ivy system [24] is based on a writer invalidate-readers protocol which implements atomic memory. <p> We use a more generalized framework here, since this can be used to classify a range of different memory models [21]. 2.1 Formal Definition This section formally describes the system that underlies our implementation. We use a model that is similar to those used by Misra <ref> [27] </ref> and by Herlihy and Wing [19]. We define a system to be a finite set of processors that interact via a shared memory that consists of a finite set of locations. A processor's interaction with the memory is through a series of read and write operations on the memory.
Reference: [28] <author> SUN. </author> <title> The SPARC Architecture Manual. </title> <institution> Sun Microsystems Inc., </institution> <note> No. 800-199-12, Version 8, January 1991. DRAFT June 21, </note> <year> 1993 </year>
Reference-contexts: Compile time support is used to track data items which have been modified. Extra programming effort may be required to identify just the right data to be associated with a synchronization variable. There have been several hardware implementations of weakly ordered systems and are described in <ref> [18, 2, 28, 11, 17] </ref>. 6 Conclusions We have shown that a causal DSM can be efficiently implemented. With adequate programming support, causal DSM significantly reduces the frequency of communication between processors. Also, scalable systems can be built since accesses to data do not require expensive global synchronization.
References-found: 28

