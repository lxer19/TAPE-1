URL: http://www.cs.virginia.edu/~alb/techrep/thesis.ps
Refering-URL: http://www.cs.virginia.edu/~alb/techrep/techrep.html
Root-URL: http://www.cs.virginia.edu
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> Wade G. Pemberton, Mark S. Dotterweich and Leigh B. Hawkins, </author> <title> ``An Overview of ATR Fusion Techniques'', </title> <booktitle> Proc. </booktitle> <address> DFS, </address> <year> 1987. </year>
Reference-contexts: If this fusion can be performed accurately the resulting estimates will in general be superior to those obtained from any single sensor report. A major component of many sensor data fusion schemes is correspondence processing <ref> [1] </ref>. If there exists a set of sensor reports from a set of distinct entities in the environment, then we would like to know which of the sensor reports correspond to the same entity. Correspondence processing is a procedure for making this determination. <p> These reports would then be fused together to provide an estimate for the entity's location. The problem of sensor data fusion arises in many different contexts, for example robotics [2][3] [4][5] and ATR fusion <ref> [1] </ref>. The choice of a sensor data fusion algorithm is to a large degree domain dependent.
Reference: 2. <author> Ren C. Luo and Min-Hsiung Lin, </author> <title> ``Multi-Sensor Integrated Intellegent Robot for Automated Assembly'', in Spatial Reasoning and Multi-Sensor Fusion, </title> <booktitle> Proceedings of the 1987 Workshop, </booktitle> <year> 1987, </year> <pages> 351-360. </pages>
Reference: 3. <author> Ren C. Luo, Min-Hsiung Lin and Ralph S.Scherp, </author> <title> Dynamic Multi-Sensor Data Fusion System for Intelligent Robots, </title> <journal> Vol. </journal> <volume> 4, </volume> <month> Aug. </month> <year> 1988. </year>
Reference: 4. <author> Hugh F. Durrant-Whyte, </author> <title> ``Sensor Models and Multi-Sensor Integration'', in Spatial Reasoning and Multi-Sensor Fusion, </title> <booktitle> Proceedings of the 1987 Workshop, </booktitle> <year> 1987, </year> <pages> 303-312. </pages>
Reference-contexts: When two multivariate Gaussians are determined to correlate, it is then necessary to fuse the two distributions into one. If the distributions are independent and unbiased, then the distributions may be updated by producing a maximum likelihood estimate as [5] <ref> [4] </ref> S new = [S 1 +S 2 ] -1 m new = S new [S 1 -1 m 2 ]. 1.1.2. Performance Measures for Correspondence Algorithms In this section, we discuss some of the issues involved in measuring the performance of correspondence processing algorithms.
Reference: 5. <author> James L. Crowley and Fano Ramparany, </author> <title> Mathematical Tools for Representing Uncertainty in Perception, </title> <year> 1987. </year>
Reference-contexts: When two multivariate Gaussians are determined to correlate, it is then necessary to fuse the two distributions into one. If the distributions are independent and unbiased, then the distributions may be updated by producing a maximum likelihood estimate as <ref> [5] </ref> [4] S new = [S 1 +S 2 ] -1 m new = S new [S 1 -1 m 2 ]. 1.1.2. Performance Measures for Correspondence Algorithms In this section, we discuss some of the issues involved in measuring the performance of correspondence processing algorithms.
Reference: 6. <author> Kenneth S. Miller, </author> <title> Multidimensional Gaussian Distributions, </title> <publisher> Wiley, </publisher> <year> 1964. </year>
Reference-contexts: Fusion of Gaussian Sensor Reports Some sensor report consist of the parameters of a multivariate Gaussian distribution. The multivariate Gaussian density is given by <ref> [6] </ref> g (m,S;x) = 1 hhhhhhhhhhhh e -1/2 (x-m) T S -1 (x-m) . Here m is the n dimensional mean vector of the distribution, and S is the nn covariance matrix.
Reference: 7. <author> K. Fukunaga, </author> <title> Introduction to Statistical Pattern Recognition, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: The measures we have considered are the divergence <ref> [7] </ref> 2 -1 +S 2 2 -1 S 2 +S 2 the Mahalanobis distance between the means with averaged covariance matrices [8] the Bhattacharyya distance [7] 2d T (S 1 +S 2 ) -1 d + 4ln | (S 1 +S 2 )/2 | hhhhhhhhhhhhhhh , and a measure we call <p> The measures we have considered are the divergence <ref> [7] </ref> 2 -1 +S 2 2 -1 S 2 +S 2 the Mahalanobis distance between the means with averaged covariance matrices [8] the Bhattacharyya distance [7] 2d T (S 1 +S 2 ) -1 d + 4ln | (S 1 +S 2 )/2 | hhhhhhhhhhhhhhh , and a measure we call the product [7][6] given by 2d T (S 1 +S 2 ) -1 d + 2ln4p 2 | S 1 +S 2 | .
Reference: 8. <author> Jack Sklansky and Gustav N. Wassel, </author> <title> Pattern Classifiers and Trainable Machines, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: The measures we have considered are the divergence [7] 2 -1 +S 2 2 -1 S 2 +S 2 the Mahalanobis distance between the means with averaged covariance matrices <ref> [8] </ref> the Bhattacharyya distance [7] 2d T (S 1 +S 2 ) -1 d + 4ln | (S 1 +S 2 )/2 | hhhhhhhhhhhhhhh , and a measure we call the product [7][6] given by 2d T (S 1 +S 2 ) -1 d + 2ln4p 2 | S 1 +S
Reference: 9. <author> David E. Rummelhart and James L. McClelland, </author> <title> Parallel Distributed Processing, </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Neural Networks Neural networks are models of parallel computation based loosely on the properties of physiological neurons in the brain. These models are attractive for many reasons <ref> [9] </ref>, including their ability to deal naturally with multiple simultaneous constraints, their ability to be translated into fast, parallel hardware implementations, and their tolerance to hardware faults. There are many different classes of neural networks. <p> This model is a Boltzmann machine used purely for constraint satisfaction, without learning synaptic weight values <ref> [9] </ref>. In networks with graded 10 response a similar effect can be achieved by slowly raising the gain of the input-output relation sigmoid [12]. 1.2.2. Kohonen's Self-Organizing Feature Maps Kohonen's self-organizing feature maps [16] [17] [18] are models of the formation of topographic maps in the brain.
Reference: 10. <author> J. J. </author> <title> Hopfield, ``Neural networks and physical systems with emergent collective computational abilities'', </title> <booktitle> Proc. </booktitle> <institution> Nat. Acad. Sci. </institution> <address> USA 79 (1982), </address> <pages> 2554-2558. </pages>
Reference-contexts: The network dynamics then cause the network to settle to a local minimum of the network's energy function, which hopefully represents a good solution to the optimization problem. This technique has been applied to the traveling salesman problem [12], content addressable memories <ref> [10] </ref>, analog to digital conversion [13], deconvolution decision problems [13], linear programming [13] and clustering problems [14]. There are still open questions, however, about how Hopfield networks scale to larger problem sizes [15].
Reference: 11. <author> J. J. </author> <title> Hopfield, ``Neurons with graded response have collective computational properties like those of two-state neurons'', </title> <booktitle> Proc. </booktitle> <institution> Natl. Acad. Sci. </institution> <month> 81 (May </month> <year> 1984), </year> <pages> 3088-3092. </pages>
Reference-contexts: Hopfield has shown <ref> [11] </ref> that for symmetric T, E = -1 hhh S S T ij V i V j - S I i V i + 1 hh S f -1 (v) dv is a Liapunov, or energy, function for the system. <p> These equations are Hopfield's equations of motion with time constants set equal to 1, equal resistance values, equal capacitance values, and with weights and inputs scaled by the capacitance value <ref> [11] </ref> [12]. The x ij 1 and x ij 2 neurons are assumed to simply form the scalar product of their input and weight vectors and pass this value through their input-output relations to the output.
Reference: 12. <author> J. J. Hopfield and D. W. Tank, </author> <title> ``Neural Computation of Decisions in Optimization Problems'', </title> <booktitle> Biological Cybernetics 52 (1985). </booktitle>
Reference-contexts: The network dynamics then cause the network to settle to a local minimum of the network's energy function, which hopefully represents a good solution to the optimization problem. This technique has been applied to the traveling salesman problem <ref> [12] </ref>, content addressable memories [10], analog to digital conversion [13], deconvolution decision problems [13], linear programming [13] and clustering problems [14]. There are still open questions, however, about how Hopfield networks scale to larger problem sizes [15]. <p> This model is a Boltzmann machine used purely for constraint satisfaction, without learning synaptic weight values [9]. In networks with graded 10 response a similar effect can be achieved by slowly raising the gain of the input-output relation sigmoid <ref> [12] </ref>. 1.2.2. Kohonen's Self-Organizing Feature Maps Kohonen's self-organizing feature maps [16] [17] [18] are models of the formation of topographic maps in the brain. One example of such a map is the retinotopic map from the retina to the visual cortex. <p> These equations are Hopfield's equations of motion with time constants set equal to 1, equal resistance values, equal capacitance values, and with weights and inputs scaled by the capacitance value [11] <ref> [12] </ref>. The x ij 1 and x ij 2 neurons are assumed to simply form the scalar product of their input and weight vectors and pass this value through their input-output relations to the output.
Reference: 13. <author> David W. Tank and John. J. </author> <title> Hopfield, ``Simple 'Neural' Optimization Networks: an A/D Converter, Signal Decision Circuit, and a Linear Programming Circuit'', </title> <journal> IEEE Trans. Circuits and Systems cas-33, </journal> <month> 5 (May </month> <year> 1986), </year> <pages> 533-541. </pages>
Reference-contexts: The network dynamics then cause the network to settle to a local minimum of the network's energy function, which hopefully represents a good solution to the optimization problem. This technique has been applied to the traveling salesman problem [12], content addressable memories [10], analog to digital conversion <ref> [13] </ref>, deconvolution decision problems [13], linear programming [13] and clustering problems [14]. There are still open questions, however, about how Hopfield networks scale to larger problem sizes [15]. <p> This technique has been applied to the traveling salesman problem [12], content addressable memories [10], analog to digital conversion <ref> [13] </ref>, deconvolution decision problems [13], linear programming [13] and clustering problems [14]. There are still open questions, however, about how Hopfield networks scale to larger problem sizes [15]. One problem with the use of Hopfield nets for optimization problems is that they only find a local minimum of their energy functions. <p> This technique has been applied to the traveling salesman problem [12], content addressable memories [10], analog to digital conversion <ref> [13] </ref>, deconvolution decision problems [13], linear programming [13] and clustering problems [14]. There are still open questions, however, about how Hopfield networks scale to larger problem sizes [15]. One problem with the use of Hopfield nets for optimization problems is that they only find a local minimum of their energy functions.
Reference: 14. <author> Behzad Kamgar-Parsi, J. A. Gualtieri and Judy E. Devaney, </author> <title> ``How to Cluster in Parallel with Neural Networks'', </title> <booktitle> Presented at the 2nd Symposium on Frontiers of Massively Parallel Computing. </booktitle>
Reference-contexts: This technique has been applied to the traveling salesman problem [12], content addressable memories [10], analog to digital conversion [13], deconvolution decision problems [13], linear programming [13] and clustering problems <ref> [14] </ref>. There are still open questions, however, about how Hopfield networks scale to larger problem sizes [15]. One problem with the use of Hopfield nets for optimization problems is that they only find a local minimum of their energy functions. <p> The rows of this matrix will therefore correspond to the reports c i out of C, and the columns will correspond to the clusters C i formed in C. This is shown in Figure 4.1, and is the same representation used in <ref> [14] </ref>. Note that the number of clusters that are formed, l, is bounded from above by the number of columns in the network. After the network converges, the neural outputs will represent the binary values for the v ij .
Reference: 15. <author> G. V. Wilson and G. S. Pawley, </author> <title> ``On the Stability of the Travelling Salesman Problem Algorithm of Hopfield and Tank'', </title> <booktitle> Biological Cybernetics 58 (1988), </booktitle> <pages> 63-70. 60 </pages>
Reference-contexts: This technique has been applied to the traveling salesman problem [12], content addressable memories [10], analog to digital conversion [13], deconvolution decision problems [13], linear programming [13] and clustering problems [14]. There are still open questions, however, about how Hopfield networks scale to larger problem sizes <ref> [15] </ref>. One problem with the use of Hopfield nets for optimization problems is that they only find a local minimum of their energy functions. By modifying the updating rule for two-state Hopfield neurons, however, simulated annealing can be applied to the minimization [9][12]. <p> Both of these numbers are imprac-tically high for realistic problem sizes. The reports could, however, be partitioned and the partitions processed in sequence. Another problem is that the clustering representation is highly degenerate, which may lead to poor network performance <ref> [15] </ref>. In Chapter 5 we consider spatial constraints on the possible match-ings which could greatly reduce both the number of neurons and the number of connection weights required, as well as reduce the high degeneracy in the problem representation. 45 4.3.
Reference: 16. <author> Teuvo Kohonen, </author> <title> ``Self-Organized Formation of Topologically Correct Feature Maps'', </title> <booktitle> Biological Cybernetics 43 (1982), </booktitle> <pages> 59-69. </pages>
Reference-contexts: This model is a Boltzmann machine used purely for constraint satisfaction, without learning synaptic weight values [9]. In networks with graded 10 response a similar effect can be achieved by slowly raising the gain of the input-output relation sigmoid [12]. 1.2.2. Kohonen's Self-Organizing Feature Maps Kohonen's self-organizing feature maps <ref> [16] </ref> [17] [18] are models of the formation of topographic maps in the brain. One example of such a map is the retinotopic map from the retina to the visual cortex. The retinal signals have an ordering to them defined by the neighborhood structure of receptor cells in the retina.
Reference: 17. <author> Teuvo Kohonen, </author> <title> ``Analysis of a Simple Self-Organizing Process'', </title> <booktitle> Biological Cybernetics 44 (1982), </booktitle> <pages> 135-140. </pages>
Reference-contexts: In networks with graded 10 response a similar effect can be achieved by slowly raising the gain of the input-output relation sigmoid [12]. 1.2.2. Kohonen's Self-Organizing Feature Maps Kohonen's self-organizing feature maps [16] <ref> [17] </ref> [18] are models of the formation of topographic maps in the brain. One example of such a map is the retinotopic map from the retina to the visual cortex. The retinal signals have an ordering to them defined by the neighborhood structure of receptor cells in the retina.
Reference: 18. <author> Teuvo Kohonen, </author> <title> Self-Organization and Associative Memory, </title> <publisher> Springer-Verlag, </publisher> <year> 1984. </year>
Reference-contexts: In networks with graded 10 response a similar effect can be achieved by slowly raising the gain of the input-output relation sigmoid [12]. 1.2.2. Kohonen's Self-Organizing Feature Maps Kohonen's self-organizing feature maps [16] [17] <ref> [18] </ref> are models of the formation of topographic maps in the brain. One example of such a map is the retinotopic map from the retina to the visual cortex. The retinal signals have an ordering to them defined by the neighborhood structure of receptor cells in the retina. <p> This type of topological mapping is ubiquitous in the brain. While these maps may be coarsely specified by developmental processes, they are thought to be refined by self-organizing processes based on local synaptic learning laws <ref> [18] </ref>. We will be concerned only with Kohonen's simplified version of the model, which is intended to illustrate the self-organizing process as simply as possible. We first discuss self-organizing feature maps, an unsupervised learning process, and then learning vector quantization, a supervised learning process related to the feature maps. <p> This causes point density of the weight vectors to approximate the probability density of the input vectors. Kohonen originally suggested that this "magnification factor" would be proportional to the probability density <ref> [18] </ref>. It was later shown, however, to be proportional to the probability density to the 2/3 power for one dimensional maps, and to be not expressible in simple closed form for the general case [19]. <p> One property of self-organizing feature maps which may make them very useful is their ability to capture the relationships among high dimensional pattern vectors in a low dimensional network topology <ref> [18] </ref>. They have also been demonstrated to recover the structure of an input environment, even when the observation vectors are nonlinear with mutually dependent components [18], and have recently been applied to find very good solutions to the traveling salesman problem [20]. <p> maps which may make them very useful is their ability to capture the relationships among high dimensional pattern vectors in a low dimensional network topology <ref> [18] </ref>. They have also been demonstrated to recover the structure of an input environment, even when the observation vectors are nonlinear with mutually dependent components [18], and have recently been applied to find very good solutions to the traveling salesman problem [20]. Learning vector quantization (LVQ) [21][22] is a supervised learning procedure closely related to the unsupervised procedure used to form feature maps.
Reference: 19. <author> H. Ritter and K. Schulten, </author> <title> ``On the Stationary State of Kohonen's Self-Organizing Sensory Mapping'', </title> <booktitle> Biological Cybernetics 54 (1986), </booktitle> <pages> 99-106. </pages>
Reference-contexts: Kohonen originally suggested that this "magnification factor" would be proportional to the probability density [18]. It was later shown, however, to be proportional to the probability density to the 2/3 power for one dimensional maps, and to be not expressible in simple closed form for the general case <ref> [19] </ref>. One property of self-organizing feature maps which may make them very useful is their ability to capture the relationships among high dimensional pattern vectors in a low dimensional network topology [18].
Reference: 20. <author> Bernard Angeniol, Jean-Yves Le Texier and Gael de La Croix Vauvois, </author> <title> ``Self-Organizing Feature Maps and the Travelling Salesman Problem'', </title> <booktitle> Neural Networks 1, 4 (1988), </booktitle> <pages> 289-293. </pages>
Reference-contexts: They have also been demonstrated to recover the structure of an input environment, even when the observation vectors are nonlinear with mutually dependent components [18], and have recently been applied to find very good solutions to the traveling salesman problem <ref> [20] </ref>. Learning vector quantization (LVQ) [21][22] is a supervised learning procedure closely related to the unsupervised procedure used to form feature maps. In LVQ, we are given a training set 13 of pattern vectors x i , along with their known classifications w i .
Reference: 21. <author> Teuvo Kohonen, </author> <title> ``An Introduction to Neural Computing'', </title> <booktitle> Neural Networks 1, 1 (1988), </booktitle> <pages> 3-16. </pages>
Reference: 22. <author> Teuvo Kohonen, </author> <title> ``Learning Vector Quantization'', </title> <booktitle> Abstracts of the First Annual INNS Meeting 1, supplement 1 (1988). </booktitle>
Reference-contexts: Kohonen has also described an improved LVQ, LVQ2, which makes use of a symmetric 14 window around the midplane between the winning unit and the next best unit <ref> [22] </ref>, and which only updates on misclassifications. LVQ has been found to be extremely effective in classifying speech data. It was found to be more effective than a parametric Bayes classifier for classifying spectral samples of Finnish phonemes [22]. <p> around the midplane between the winning unit and the next best unit <ref> [22] </ref>, and which only updates on misclassifications. LVQ has been found to be extremely effective in classifying speech data. It was found to be more effective than a parametric Bayes classifier for classifying spectral samples of Finnish phonemes [22]. CHAPTER 2 The General Problem As was mentioned earlier, the choice of a sensor data fusion algorithm is to a large degree domain dependent. In this chapter we first present a very general description of our problem.
Reference: 23. <author> Richard P. Lippmann, </author> <title> ``An Introduction to Computing with Neural Nets'', </title> <journal> IEEE ASSP Magazine, </journal> <month> April, </month> <year> 1987. </year>
Reference-contexts: The subnetwork shown in Figure 3.5 is based on a feedforward network for finding the maximum of two inputs that is described by Lippmann <ref> [23] </ref>. Other subnetworks are possible as long as they compute the maximum and pass the maximum value through to neuron o ij .
Reference: 24. <author> Robert Sedgewick, </author> <title> Algorithms, </title> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>

Reference: 1. <institution> Introduction ............................................................................................. 1 1.1 Sensor Data Fusion and Correspondence Processing .......................... 2 1.1.1 Fusion of Gaussian Sensor Reports ................................................... 3 1.1.2 Performance Measures for Correspondence Algorithms .................... 4 1.2 Neural Networks .................................................................................... 6 1.2.1 Hopfield Networks .............................................................................. 7 1.2.2 Kohonen's Self-Organizing Feature Maps .......................................... 10 </institution>
Reference-contexts: If this fusion can be performed accurately the resulting estimates will in general be superior to those obtained from any single sensor report. A major component of many sensor data fusion schemes is correspondence processing <ref> [1] </ref>. If there exists a set of sensor reports from a set of distinct entities in the environment, then we would like to know which of the sensor reports correspond to the same entity. Correspondence processing is a procedure for making this determination. <p> These reports would then be fused together to provide an estimate for the entity's location. The problem of sensor data fusion arises in many different contexts, for example robotics [2][3] [4][5] and ATR fusion <ref> [1] </ref>. The choice of a sensor data fusion algorithm is to a large degree domain dependent.
Reference: 2. <institution> The General Problem .............................................................................. 15 2.1 General Problem Description ................................................................ 15 2.2 The Problem Domain ............................................................................ 16 2.3 Problem Description Given the Domain ................................................. 17 </institution>
Reference: 3. <author> Case I: </author> <title> Restricted Sensor Function ......................................................... 20 3.1 Algorithm Description ............................................................................ 21 3.2 Simulation Results ................................................................................. 22 3.3 Neural Network Implementation ............................................................ 32 </title>
Reference: 4. <author> Case II: </author> <title> General Sensor Function ........................................................... 40 4.1 Algorithm Description ............................................................................ 40 4.2 Neural Network Description ................................................................... 42 4.3 Future Work .......................................................................................... 45 </title>
Reference-contexts: When two multivariate Gaussians are determined to correlate, it is then necessary to fuse the two distributions into one. If the distributions are independent and unbiased, then the distributions may be updated by producing a maximum likelihood estimate as [5] <ref> [4] </ref> S new = [S 1 +S 2 ] -1 m new = S new [S 1 -1 m 2 ]. 1.1.2. Performance Measures for Correspondence Algorithms In this section, we discuss some of the issues involved in measuring the performance of correspondence processing algorithms.
Reference: 5. <institution> Using a Pre-Mapping to Reduce the Numbers of Neurons ...................... 46 5.1 The Abstract Mapping ........................................................................... 47 5.1.1 Some Bounds for One-dimensional Mappings ................................... 49 5.1.2 Applications to the A1 Network ........................................................... 50 5.1.3 Applications to the A2 Network ........................................................... 53 5.2 Using a Feature Map to Approximate the Abstract Mapping ................. 55 </institution>
Reference-contexts: When two multivariate Gaussians are determined to correlate, it is then necessary to fuse the two distributions into one. If the distributions are independent and unbiased, then the distributions may be updated by producing a maximum likelihood estimate as <ref> [5] </ref> [4] S new = [S 1 +S 2 ] -1 m new = S new [S 1 -1 m 2 ]. 1.1.2. Performance Measures for Correspondence Algorithms In this section, we discuss some of the issues involved in measuring the performance of correspondence processing algorithms.
Reference: 6. <institution> Conclusions ............................................................................................. 58 References .................................................................................................. 59 </institution>
Reference-contexts: Fusion of Gaussian Sensor Reports Some sensor report consist of the parameters of a multivariate Gaussian distribution. The multivariate Gaussian density is given by <ref> [6] </ref> g (m,S;x) = 1 hhhhhhhhhhhh e -1/2 (x-m) T S -1 (x-m) . Here m is the n dimensional mean vector of the distribution, and S is the nn covariance matrix.
References-found: 30

