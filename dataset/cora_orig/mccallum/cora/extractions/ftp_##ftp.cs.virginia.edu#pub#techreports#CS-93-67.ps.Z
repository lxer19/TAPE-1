URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-93-67.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Note: This work was supported in part by a grant from Intel Supercomputer Division and by NSF contract MIP-9114110.  
Abstract: Uniprocessor SMC Performance on Vectors with Non-Unit Strides Sally A. McKee Computer Science Report No. CS-93-67 January 31, 1994 
Abstract-found: 1
Intro-found: 1
Reference: [Bud71] <author> Budnik, P., and Kuck, D., </author> <title> The Organization and Use of Parallel Memories, </title> <journal> IEEE Trans. Comput., </journal> <volume> 20, 12, </volume> <year> 1971. </year>
Reference-contexts: Aligning the vectors in a computation such that all memory banks are used will insure that the concurrency of the memory architecture is exploited. One way to achieve this is to use a stride that is relatively prime to the number of memory banks. Skewed storage <ref> [Bud71, Har87] </ref> or dynamic address transformations [Har89, Rau91] provide another means of increasing concurrency. gcd b stride,( ) Uniprocessor SMC Performance on Vectors with Non-Unit Strides 21 Very large vector strides hinder the SMCs ability to take advantage of page-mode, but given such a stride, the SMC will do the best
Reference: [DEC92] <institution> Alpha Architecture Handbook, Digital Equipment Corporation, </institution> <year> 1992. </year>
Reference-contexts: While not a common architectural feature, some commercial processors such as the Convex C-1 [Wal85] and Intel i860 [Int91] include such cache bypassing. Others, such as the DEC Alpha <ref> [DEC92] </ref>, provide a means of specifying some portions of memory as non-cacheable. 4.
Reference: [Don79] <author> Dongarra, J.J., et. al., </author> <title> Linpack Users Guide, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1979. </year>
Reference-contexts: For instance, daxpy denotes a computation involving a double-precision scalar times a vector plus a vector. Uniprocessor SMC Performance on Vectors with Non-Unit Strides 6 Our benchmark suite is depicted in Figure 2. Daxpy, copy, scale, and swap are from the BLAS (Basic Linear Algebra Subroutines) <ref> [Law79, Don79] </ref>. These vector and matrix computations occur frequently in scientific applications, and have been collected into a set of library routines that are highly optimized for various architectures. Hydro and tridiag are the first and fifth Livermore Loops [McM86], a set of kernels culled from important scientific computations.
Reference: [Don90] <author> Dongarra, J.J., DuCroz, J., Duff, I., and Hammerling, S., </author> <title> A set of Level 3 Basic Linear Algebra Subprograms, </title> <journal> ACM Trans. Math. Softw., </journal> <volume> 16 </volume> <pages> 1-17, </pages> <year> 1990. </year>
Reference-contexts: Note that although these computations do not reuse vector elements, they are often found in the inner loops of algorithms that do, as with vaxpy for matrix-vector multiply, and blocked algorithms such as those in the Level 3 BLAS <ref> [Don90] </ref>. 6. Results different memory systems with several SMC configurations. We present results for two different dynamic ordering policies, algorithms A1 and T1 from [McK93a].
Reference: [Gol93] <author> Golub, G., and Ortega, J.M., </author> <title> Scientific Computation: An Introduction with Parallel Computing, </title> <publisher> Academic Press, Inc., </publisher> <year> 1993. </year>
Reference-contexts: Vaxpy is a vector axpy computation that occurs in matrix-vector multiplication by diagonals; this algorithm is useful for the diagonally sparse matrices that arise frequently in the solution of parabolic or elliptic partial differential equations by finite element or finite difference methods <ref> [Gol93] </ref>. Note that although these computations do not reuse vector elements, they are often found in the inner loops of algorithms that do, as with vaxpy for matrix-vector multiply, and blocked algorithms such as those in the Level 3 BLAS [Don90]. 6. Results different memory systems with several SMC configurations.
Reference: [Har87] <author> Harper, D. T., Jump, J., </author> <title> Vector Access Performance in Parallel Memories Using a Skewed Storage Scheme, </title> <journal> IEEE Trans. Comput., </journal> <volume> 36, 12, </volume> <year> 1987. </year>
Reference-contexts: Aligning the vectors in a computation such that all memory banks are used will insure that the concurrency of the memory architecture is exploited. One way to achieve this is to use a stride that is relatively prime to the number of memory banks. Skewed storage <ref> [Bud71, Har87] </ref> or dynamic address transformations [Har89, Rau91] provide another means of increasing concurrency. gcd b stride,( ) Uniprocessor SMC Performance on Vectors with Non-Unit Strides 21 Very large vector strides hinder the SMCs ability to take advantage of page-mode, but given such a stride, the SMC will do the best
Reference: [Har89] <author> Harper, D. T., </author> <title> Address Transformation to Increase Memory Performance, </title> <booktitle> 1989 International Conference on Supercomputing. </booktitle>
Reference-contexts: One way to achieve this is to use a stride that is relatively prime to the number of memory banks. Skewed storage [Bud71, Har87] or dynamic address transformations <ref> [Har89, Rau91] </ref> provide another means of increasing concurrency. gcd b stride,( ) Uniprocessor SMC Performance on Vectors with Non-Unit Strides 21 Very large vector strides hinder the SMCs ability to take advantage of page-mode, but given such a stride, the SMC will do the best it can to maximize bandwidth.
Reference: [Hen90] <author> Hennessy, J., and Patterson, D., </author> <title> Computer Architecture: A Quantitative Approach, </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: 1. Introduction The growing disparity between processor speeds and memory speeds is well known <ref> [Kat89, Hen90] </ref>. Memory bandwidth is becoming the limiting performance factor for many applications particularly scientific computations.
Reference: [IEEE92] <author> High-speed DRAMs, </author> <title> Special Report, </title> <journal> IEEE Spectrum, </journal> <volume> vol. 29, no. 10, </volume> <month> October </month> <year> 1992. </year> <title> [Int91] i860 XP Microprocessor Data Book, </title> <publisher> Intel Corporation, </publisher> <year> 1991. </year>
Reference-contexts: The overhead time required to set up the new page makes servicing such an access significantly slower than one that hits the current page. Other modern devices offer similar features (e.g. nibble mode, static-column mode, or on-chip SRAM cache) or exhibit novel organizations (e.g. Rambus, Ramlink, or synchronous DRAM) <ref> [Ram92, IEEE92] </ref>. The order of requests strongly affects the performance of all these components. For multiple-module memory systems, the order of requests is important on yet another level: successive accesses to the same memory bank cannot be performed as quickly as accesses to different banks.
Reference: [Kat89] <author> Katz, R., and Hennessy, J., </author> <title> High Performance Microprocessor Architectures, </title> <institution> University of California, Berkeley, </institution> <note> Report No. UCB/CSD 89/529, </note> <month> August, </month> <year> 1989. </year>
Reference-contexts: 1. Introduction The growing disparity between processor speeds and memory speeds is well known <ref> [Kat89, Hen90] </ref>. Memory bandwidth is becoming the limiting performance factor for many applications particularly scientific computations.
Reference: [Law79] <editor> Lawson, et. al., </editor> <title> Basic Linear Algebra Subprograms for Fortran Usage, </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 5, 3, </volume> <year> 1979. </year>
Reference-contexts: For instance, daxpy denotes a computation involving a double-precision scalar times a vector plus a vector. Uniprocessor SMC Performance on Vectors with Non-Unit Strides 6 Our benchmark suite is depicted in Figure 2. Daxpy, copy, scale, and swap are from the BLAS (Basic Linear Algebra Subroutines) <ref> [Law79, Don79] </ref>. These vector and matrix computations occur frequently in scientific applications, and have been collected into a set of library routines that are highly optimized for various architectures. Hydro and tridiag are the first and fifth Livermore Loops [McM86], a set of kernels culled from important scientific computations.
Reference: [McK93a] <author> McKee, S.A, </author> <title> Hardware Support for Access Ordering: Performance of Some Design Options, </title> <institution> University of Virginia, Department of Computer Science, </institution> <type> Technical Report CS-93-08, </type> <month> August </month> <year> 1993. </year>
Reference-contexts: The hardware part of this solution is the Stream Memory Controller (SMC), the design and implementation of which is described in more detail in [McK93b]. We have conducted numerous simulation experiments to evaluate uniprocessor SMC performance for unit-stride vectors; results of these can be found in <ref> [McK93a] </ref>. Here we examine uniprocessor SMC performance for non-unit stride vectors. This report is organized as follows. Section 2 provides a brief introduction to access ordering in general, and Section 3 gives an overview of the design of the SMC. <p> Complete uniprocessor results for stride-one vectors, including a detailed description of each access-ordering heuristic, can be found in <ref> [McK93a] </ref>; highlights of these results are presented in [McK93b, McK93c]. Most performance figures in Section 6 are given as a percentage of peak bandwidth, i.e., the bandwidth necessary to allow the processor to perform a memory operation each cycle. <p> This 4:1 cost ratio is fairly representative of current technology. The DRAM page size used in the simulations discussed here is 4K doublewords (this is twice the page size used in <ref> [McK93a, McK93b, and McK93b] </ref>). This may seem large with respect to current technology, but this is of little import, for DRAM page size and vector stride are strongly related performance parameters. <p> Results different memory systems with several SMC configurations. We present results for two different dynamic ordering policies, algorithms A1 and T1 from <ref> [McK93a] </ref>.
Reference: [McK93b] <author> McKee, S.A., Klenke, R.H., Schwab, A.J., Wulf, Wm.A., Moyer, S.A., Hitchcock, C., Aylor, J.H., </author> <title> Experimental Implementation of Dynamic Access Ordering, </title> <institution> University of Virginia, TR CS-93-42, </institution> <month> August </month> <year> 1993. </year> <booktitle> In Proc. </booktitle> <address> HICSS-27, Maui, HI, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: The hardware part of this solution is the Stream Memory Controller (SMC), the design and implementation of which is described in more detail in <ref> [McK93b] </ref>. We have conducted numerous simulation experiments to evaluate uniprocessor SMC performance for unit-stride vectors; results of these can be found in [McK93a]. Here we examine uniprocessor SMC performance for non-unit stride vectors. This report is organized as follows. <p> The beneficial impact of access ordering on effective memory bandwidth together with the limitations inherent in implementing the technique statically motivate us to consider an implementation that reorders accesses dynamically at run time. What follows is an overview of the architecture proposed in <ref> [McK93b, McK93c] </ref>: see those documents for more details. Uniprocessor SMC Performance on Vectors with Non-Unit Strides 3 Our discussion is based on the simplified architecture of Figure 1. In this system, memory is interfaced to the processors through a controller labeled MSU for Memory Scheduling Unit. <p> Complete uniprocessor results for stride-one vectors, including a detailed description of each access-ordering heuristic, can be found in [McK93a]; highlights of these results are presented in <ref> [McK93b, McK93c] </ref>. Most performance figures in Section 6 are given as a percentage of peak bandwidth, i.e., the bandwidth necessary to allow the processor to perform a memory operation each cycle. <p> This 4:1 cost ratio is fairly representative of current technology. The DRAM page size used in the simulations discussed here is 4K doublewords (this is twice the page size used in <ref> [McK93a, McK93b, and McK93b] </ref>). This may seem large with respect to current technology, but this is of little import, for DRAM page size and vector stride are strongly related performance parameters.
Reference: [McK93c] <author> McKee, S.A., Moyer, S.A., Wulf, Wm.A., Hitchcock, C., </author> <title> Increasing Memory Bandwidth for Vector Computations, </title> <institution> University of Virginia, </institution> <note> TR Uniprocessor SMC Performance on Vectors with Non-Unit Strides 48 CS-93-34, August 1993. To appear in Proc. Conf. on Prog. </note> <author> Lang. and Sys. </author> <title> Arch., </title> <address> Zurich, Switzerland, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Memory bandwidth is becoming the limiting performance factor for many applications particularly scientific computations. Access ordering is one technique that can help bridge the processor-memory performance gap. [Moy93] develops and analyzes algorithms to perform access ordering statically at compile time. <ref> [McK93c] </ref> proposes a combined hardware/software scheme for implementing access ordering dynamically at run-time, and presents simulation results demonstrating its effectiveness. The hardware part of this solution is the Stream Memory Controller (SMC), the design and implementation of which is described in more detail in [McK93b]. <p> The beneficial impact of access ordering on effective memory bandwidth together with the limitations inherent in implementing the technique statically motivate us to consider an implementation that reorders accesses dynamically at run time. What follows is an overview of the architecture proposed in <ref> [McK93b, McK93c] </ref>: see those documents for more details. Uniprocessor SMC Performance on Vectors with Non-Unit Strides 3 Our discussion is based on the simplified architecture of Figure 1. In this system, memory is interfaced to the processors through a controller labeled MSU for Memory Scheduling Unit. <p> Complete uniprocessor results for stride-one vectors, including a detailed description of each access-ordering heuristic, can be found in [McK93a]; highlights of these results are presented in <ref> [McK93b, McK93c] </ref>. Most performance figures in Section 6 are given as a percentage of peak bandwidth, i.e., the bandwidth necessary to allow the processor to perform a memory operation each cycle.
Reference: [McK93d] <author> McKee, S.A., </author> <title> An Analytic Model of SMC Performance, </title> <institution> University of Virginia, TR CS-93-54, </institution> <month> November, </month> <year> 1994. </year>
Reference-contexts: The relationship turns out to be slightly more complicated than this, but is explained by the analytic model of <ref> [McK93d] </ref>. Let b be the number of interleaved memory banks, f be the depth of the FIFOs, v be the number of distinct vectors in the computation, and s be the number of streams. <p> A single Uniprocessor SMC Performance on Vectors with Non-Unit Strides 18 access vector constitutes one stream, whereas a read-modify-write vector counts as two. The model states that the average page-miss rate for each FIFO in a stride-1 computation involving at least two vectors is <ref> [McK93d] </ref>. This formula also applies to vectors with non-unit strides, provided that the number of vector elements residing in a DRAM page is significantly greater than the FIFO depth. <p> For single-vector computations, only the first access to each bank generates a DRAM page miss. The average page-miss rate per stream for a unit-stride vector is thus <ref> [McK93d] </ref>. For a non-unit stride vector, the average miss rate becomes . We can now use these miss rates to calculate SMC performance. <p> In practice, the SMC will be able to overlap memory latency with computation by prefetching read operands, thereby mitigating some of the unfavorable effects larger strides have on memory bandwidth. Finally, we have presented an extension to the analytic model of <ref> [McK93d] </ref> to explain SMC performance for non-unit strides. In particular, we have developed a performance model for when FIFO depth exceeds the number of data elements residing within a DRAM page. Uniprocessor SMC Performance on Vectors with Non-Unit Strides 22 Appendix different strides.
Reference: [McM86] <author> McMahon, F.H., </author> <title> The Livermore Fortran Kernels: A Computer Test of the Numerical Performance Range, </title> <institution> Lawrence Livermore National Laboratory, UCRL-53745, </institution> <month> December </month> <year> 1986. </year>
Reference-contexts: These vector and matrix computations occur frequently in scientific applications, and have been collected into a set of library routines that are highly optimized for various architectures. Hydro and tridiag are the first and fifth Livermore Loops <ref> [McM86] </ref>, a set of kernels culled from important scientific computations. The former is a hydrodynamics code fragment, and the latter is a tridiagonal elimination computation. Although the computations differ, their access patterns are identical, thus results for these benchmarks are presented together.
Reference: [Moy93] <author> Moyer, S.A., </author> <title> Access Ordering and Effective Memory Bandwidth, </title> <type> Ph.D. Dissertation, </type> <institution> Department of Computer Science, University of Virginia, </institution> <type> Technical Report CS-93-18, </type> <month> April </month> <year> 1993. </year>
Reference-contexts: 1. Introduction The growing disparity between processor speeds and memory speeds is well known [Kat89, Hen90]. Memory bandwidth is becoming the limiting performance factor for many applications particularly scientific computations. Access ordering is one technique that can help bridge the processor-memory performance gap. <ref> [Moy93] </ref> develops and analyzes algorithms to perform access ordering statically at compile time. [McK93c] proposes a combined hardware/software scheme for implementing access ordering dynamically at run-time, and presents simulation results demonstrating its effectiveness. <p> One way to do this is via access ordering, which we define as any technique for changing the order of memory requests to increase bandwidth. Here we are especially concerned with ordering a set of vector-like stream accesses. 3. The SMC <ref> [Moy93] </ref> develops algorithms and analyzes the performance benefits and limitations of doing compile-time access ordering. The beneficial impact of access ordering on effective memory bandwidth together with the limitations inherent in implementing the technique statically motivate us to consider an implementation that reorders accesses dynamically at run time. <p> Since these are representative of all results, they are discussed in detail here. Performance graphs for other benchmarks are similar, and are included in the Appendix. Non-SMC system performance numbers presented here were generated with the simulator of <ref> [Moy93] </ref>.
Reference: [Qui91] <author> Quinnell, R., </author> <title> High-speed DRAMs, </title> <type> EDN, </type> <month> May 23, </month> <year> 1991. </year>
Reference-contexts: For instance, nearly Uniprocessor SMC Performance on Vectors with Non-Unit Strides 2 all current DRAMs implement a form of page-mode operation <ref> [Qui91] </ref>. These devices behave as if implemented with a single on-chip cache line, or page (this should not be confused with a virtual memory page). A memory access falling outside the address range of the current DRAM page forces a new page to be accessed.
Reference: [Ram92] <institution> Architectural Overview, Rambus Inc., Mountain View, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: The overhead time required to set up the new page makes servicing such an access significantly slower than one that hits the current page. Other modern devices offer similar features (e.g. nibble mode, static-column mode, or on-chip SRAM cache) or exhibit novel organizations (e.g. Rambus, Ramlink, or synchronous DRAM) <ref> [Ram92, IEEE92] </ref>. The order of requests strongly affects the performance of all these components. For multiple-module memory systems, the order of requests is important on yet another level: successive accesses to the same memory bank cannot be performed as quickly as accesses to different banks.
Reference: [Rau91] <author> Rau, B. R., </author> <title> Pseudo-Randomly Interleaved Memory, </title> <booktitle> 18th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: One way to achieve this is to use a stride that is relatively prime to the number of memory banks. Skewed storage [Bud71, Har87] or dynamic address transformations <ref> [Har89, Rau91] </ref> provide another means of increasing concurrency. gcd b stride,( ) Uniprocessor SMC Performance on Vectors with Non-Unit Strides 21 Very large vector strides hinder the SMCs ability to take advantage of page-mode, but given such a stride, the SMC will do the best it can to maximize bandwidth.
Reference: [Wal85] <author> Wallach, S., </author> <title> The CONVEX C-1 64-bit Supercomputer, </title> <booktitle> Compcon Spring 85, </booktitle> <month> February </month> <year> 1985. </year>
Reference-contexts: Note that we assume the processor can perform non-caching loads and stores so that non-unit stride streams can be accessed without concomitantly accessing extraneous data and wasting bandwidth. While not a common architectural feature, some commercial processors such as the Convex C-1 <ref> [Wal85] </ref> and Intel i860 [Int91] include such cache bypassing. Others, such as the DEC Alpha [DEC92], provide a means of specifying some portions of memory as non-cacheable. 4.
References-found: 21

