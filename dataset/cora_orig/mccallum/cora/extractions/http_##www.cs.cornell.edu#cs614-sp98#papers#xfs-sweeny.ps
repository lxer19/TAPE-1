URL: http://www.cs.cornell.edu/cs614-sp98/papers/xfs-sweeny.ps
Refering-URL: http://www.cs.cornell.edu/cs614-sp98/Readings.html
Root-URL: 
Email: Email: office@usenix.org  
Title: Scalability in the XFS File System  
Phone: 1. Phone: 510 528-8649 2. FAX: 510 548-5738 3.  4.  
Author: Adam Sweeney 
Affiliation: Silicon Graphics  
Web: WWW URL: http://www.usenix.org  
Date: January 1996  
Note: The following paper was originally published in the Proceedings of the USENIX 1996 Annual Technical Conference San Diego, California,  For more information about USENIX Association contact:  
Abstract-found: 0
Intro-found: 1
Reference: [Anon85] <author> Anonymous, </author> <title> "A Measure of Transaction Processing Power," </title> <journal> Datamation, </journal> <volume> Vol. 31 No. 7, </volume> <pages> 112-118. </pages>
Reference-contexts: Database Sort Benchmark Results Using XFS, Silicon Graphics recently achieved record breaking performance on the Datamation sort <ref> [Anon85] </ref> and Indy MinuteSort [Nyberg94] benchmarks. The Datamation sort benchmark measures how fast the system can sort 100 megabytes of 100 byte records. The MinuteSort benchmark measures how much data the system can sort in one minute.
Reference: [Baker91] <author> Baker, M., Hartman, J., Kupfer, M., Shirriff, K., Ousterhout, J., </author> <title> "Measurements of a Distributed File System," </title> <booktitle> Proceedings of the 13th Symposium on Operating System Principles, </booktitle> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991, </year> <pages> 192-212. </pages>
Reference-contexts: Also, with delayed allocation, short lived files which can be buffered in memory are often never allocated any real disk blocks. The files are removed and purged from the file cache before they are pushed to disk. Such short lived files appear to be relatively common in Unix systems <ref> [Ousterhout85, Baker91] </ref>, and delayed allocation reduces both the number of metadata updates caused by such files and the impact of such files on file system fragmentation. Another benefit of delayed allocation is that files which are written randomly but have no holes can often be allocated contiguously.
Reference: [Chutani92] <author> Chutani, S., Anderson, O., et. al., </author> <title> "The Episode File System," </title> <booktitle> Proceedings of the 1992 Winter Usenix, </booktitle> <address> San Francisco, CA, </address> <year> 1992, </year> <pages> 43-60. </pages>
Reference-contexts: Inability to Support Large Directories Another area which has not been addressed by other Unix file systems is support for directories with more than a few thousand entries. While some, for example Episode <ref> [Chutani92] </ref> and VxFS [Veritas95], at least speed up searching for entries within a directory block via hashing, most file systems use directory structures which require a linear scan of the directory blocks in searching for a particular file. <p> The real number of files that will reside in a file system is rarely known at the time the file system is created. Being forced to choose makes the management of large file systems more difficult than it should be. Episode <ref> [Chutani92] </ref> and VxFS [Veritas95] both solve this problem by allowing the number of inodes in the file system to be increased dynamically. In summary, there are several problems with EFS and other file systems that we wanted to address in the design of XFS.
Reference: [Comer79] <author> Comer, D., </author> <title> "The Ubiquitous B-Tree," </title> <journal> Computing Surveys, </journal> <volume> Vol. 11, No. 2, </volume> <month> June </month> <year> 1979 </year> <month> 121-137. </month>
Reference-contexts: In this paper we describe the XFS file system with a focus on the mechanisms it uses to manage large file systems on large computer systems. The most notable mechanism used by XFS to increase the scalability of the file system is the pervasive use of B+ trees <ref> [Comer79] </ref>. B+ trees are used for tracking free extents in the file system rather than bitmaps. B+ trees are used to index directory entries rather than using linear lookup structures. B+ trees are used to manage file extent maps that overow the number of direct pointers kept in the inodes.
Reference: [Dimino94] <author> Dimino, L., Mediouni, R., Rengarajan, T., Rubino, M., Spiro, P., </author> <title> "Performance of DEC Rdb Version 6.0 on AXP Systems," </title> <journal> Digital Technical Journal, </journal> <volume> Vol. 6, No. 1, </volume> <month> Winter </month> <year> 1994 </year> <month> 23-35. </month>
Reference-contexts: It can be stored on a dedicated disk or nonvolatile memory device. Using non-volatile memory devices for the transaction log has proven very effective in high end OLTP systems <ref> [Dimino94] </ref>. It can be especially useful with XFS on an NFS server, where updates must be synchronous, in both increasing the throughput and decreasing the latency of metadata update operations. Exploiting Parallelism XFS is designed to run well on large scale shared memory multiprocessors.
Reference: [Ganger94] <author> Ganger, G., Patt, Y., </author> <title> "Metadata Update Performance in File Systems," </title> <booktitle> Proceedings of the First Usenix Symposium on Operating System Design and Implementation, </booktitle> <address> Monterey, CA, </address> <month> November, </month> <year> 1994, </year> <pages> 49-60. </pages>
Reference-contexts: Other schemes such as log structured file systems [Rosenblum92], shadow paging [Hitz94], and soft updates <ref> [Ganger94] </ref> have been proposed to solve this problem, but we feel that write ahead logging provides the best trade-off among exibility, performance, and reliability.
Reference: [Hagmann87] <author> Hagmann, R., </author> <title> "Reimplementing the Cedar File System Using Logging and Group Commit," </title> <booktitle> Proceedings of the 10th Symposium on Operating System Principles, </booktitle> <month> November, </month> <year> 1987. </year>
Reference-contexts: Others use in-memory hashing schemes layered over simple on-disk structures [Hitz94]. These in memory schemes work well to a point, but in very large directories they require a large amount of memory. This problem has been addressed in some non-Unix file systems, like NTFS [Custer94] and Cedar <ref> [Hagmann87] </ref>, by using B trees to index the entries in the directory. Inability to Support Large Numbers of Files While EFS and other file system can theoretically support very large numbers of files in a file system, in practice they do not. <p> XFS gains two things by writing the log asynchronously. First, multiple updates can be batched into a single log write. This increases the efficiency of the log writes with respect to the underlying disk array <ref> [Hagmann87, Rosenblum92] </ref>. Second, the performance of metadata updates is normally made independent of the speed of the underlying drives. This independence is limited by the amount of buffering dedicated to the log, but it is far better than the synchronous updates of older file systems.
Reference: [Hisgen93] <author> Hisgen, A., Birrell, A., Jerian, C., Mann, T., Swart, G., </author> <title> "New-Value Logging in the Echo Replicated File System," </title> <type> Research Report 104, </type> <institution> Systems Research Center, Digital Equipment Corporation, </institution> <year> 1993. </year>
Reference-contexts: To avoid these problems, XFS uses a write ahead logging scheme that enables atomic updates of the file system. This scheme is very similar to the one described very thoroughly in <ref> [Hisgen93] </ref>. XFS logs all structural updates to the file system metadata. This includes inodes, directory blocks, free extent tree blocks, inode allocation tree blocks, file extent map blocks, AG header blocks, and the superblock. XFS does not log user data.
Reference: [Hitz94] <author> Hitz, D., Lau, J., Malcolm, M., </author> <title> "File System Design for an NFS File Server Appliance," </title> <booktitle> Proceedings of the 1994 Winter Usenix, </booktitle> <address> San Francisco, CA, </address> <year> 1994, </year> <pages> 235-246. </pages>
Reference-contexts: The lookup and update performance of these unindexed formats degrades linearly with the size of the directory. Others use in-memory hashing schemes layered over simple on-disk structures <ref> [Hitz94] </ref>. These in memory schemes work well to a point, but in very large directories they require a large amount of memory. This problem has been addressed in some non-Unix file systems, like NTFS [Custer94] and Cedar [Hagmann87], by using B trees to index the entries in the directory. <p> Other schemes such as log structured file systems [Rosenblum92], shadow paging <ref> [Hitz94] </ref>, and soft updates [Ganger94] have been proposed to solve this problem, but we feel that write ahead logging provides the best trade-off among exibility, performance, and reliability.
Reference: [Kleiman86] <author> Kleiman, S., "Vnodes: </author> <title> an Architecture for Multiple File System types in Sun Unix," </title> <booktitle> Proceedings of the 1986 Summer Usenix, </booktitle> <month> Summer </month> <year> 1986. </year>
Reference-contexts: XFS supports all of the standard Unix file interfaces and is entirely POSIX and XPG4 compliant. It sits below the vnode interface <ref> [Kleiman86] </ref> in the IRIX kernel and takes full advantage of services provided by the kernel, includ-ing the buffer/page cache, the directory name lookup cache, and the dynamic vnode cache. XFS is modularized into several parts, each of which is responsible for a separate piece of the file system's functionality.
Reference: [McKusick84] <author> McKusick, M., Joy, W., Lefer, S., Fabry, R. </author> <title> "A Fast File System for UNIX," </title> <journal> ACM Transactions on Computer Systems Vol. </journal> <volume> 2, No. 3, </volume> <month> August </month> <year> 1984, </year> <pages> 181-197. </pages>
Reference-contexts: The problem was not the I/O performance of our hardware, but the limitations imposed by the old IRIX file system, EFS [SGI92]. EFS is similar to the Berkeley Fast File System <ref> [McKusick84] </ref> in structure, but it uses extents rather than individual blocks for file space allocation and I/O. <p> EFS and file systems based on the BSD Fast File System <ref> [McKusick84] </ref> falter in this area due to their dependence on a file system scavenger program to restore the file system to a consistent state after a crash. Running fsck over an 8 gigabyte file system with a few hundred thousand inodes today takes a few minutes.
Reference: [McVoy90] <author> McVoy, L., Kleiman, S., </author> <title> "Extent-like Performance from a UNIX File System," </title> <booktitle> Proceedings of the 1991 Winter Usenix, </booktitle> <address> Dallas, Texas, </address> <month> June </month> <year> 1991, </year> <pages> 33-43. </pages>
Reference-contexts: For larger application reads, XFS increases the read buffer size to match the application's request. This is very similar to the read clustering scheme in SunOS <ref> [McVoy90] </ref>, but it is more aggressive in using memory to improve I/O performance. While large read buffers satisfy the need for large request sizes, XFS uses multiple read ahead buffers to increase the parallelism in accessing the underlying disk array. <p> While large read buffers satisfy the need for large request sizes, XFS uses multiple read ahead buffers to increase the parallelism in accessing the underlying disk array. Traditional Unix systems have used only a single read ahead buffer at a time <ref> [McVoy90] </ref>. For sequential reads, XFS keeps outstanding two to three requests of the same size as the primary I/O buffer. The number varies because we try to keep three read ahead requests outstanding, but we wait until the process catches up a bit with the read ahead before issuing more. <p> Handling Write Requests To get good write performance, XFS uses aggressive write clustering <ref> [McVoy90] </ref>. Dirty file data is buffered in memory in chunks of 64 kilobytes, and when a chunk is chosen to be ushed from memory it is clustered with other contiguous chunks to form a larger I/O request.
Reference: [Nyberg94] <author> Nyberg, C., Barclay, T., Cvetanovic, Z., Gray, J., Lomet, D., "AlphaSort: </author> <title> A RISC Machine Sort," </title> <booktitle> Proceedings of the 1994 SIGMOD International Conference on Management of Data, </booktitle> <address> Minneapolis, </address> <year> 1994. </year>
Reference-contexts: Database Sort Benchmark Results Using XFS, Silicon Graphics recently achieved record breaking performance on the Datamation sort [Anon85] and Indy MinuteSort <ref> [Nyberg94] </ref> benchmarks. The Datamation sort benchmark measures how fast the system can sort 100 megabytes of 100 byte records. The MinuteSort benchmark measures how much data the system can sort in one minute.
Reference: [Ousterhout85] <author> Ousterhout, J., Da Costa, H., Harri-son, D., Kunze, J., Kupfer, M., Thompson, J., </author> <title> "A Trace-Driven Analysis of the UNIX 4.2 BSD File System," </title> <booktitle> Proceedings of the 10th Symposium on Operating System Principles, </booktitle> <address> Orcas Island, WA, </address> <month> December </month> <year> 1985, </year> <pages> 15-24. </pages>
Reference-contexts: Also, with delayed allocation, short lived files which can be buffered in memory are often never allocated any real disk blocks. The files are removed and purged from the file cache before they are pushed to disk. Such short lived files appear to be relatively common in Unix systems <ref> [Ousterhout85, Baker91] </ref>, and delayed allocation reduces both the number of metadata updates caused by such files and the impact of such files on file system fragmentation. Another benefit of delayed allocation is that files which are written randomly but have no holes can often be allocated contiguously.
Reference: [Ousterhout90] <author> Ousterhout, J. </author> <title> "Why Aren't Operating Systems Getting Faster As Fast as Hardware?" Proceedings of the 1990 Summer Usenix, </title> <address> Anaheim, CA, </address> <month> June, </month> <year> 1990, </year> <pages> 247-256. </pages>
Reference-contexts: Finally, we present performance results from running on real systems to demonstrate the success of the XFS design. 2. Why a New File System? The file system literature began predicting the coming of the "I/O bottleneck" years ago <ref> [Ousterhout90] </ref>, and we experienced it first hand at SGI. The problem was not the I/O performance of our hardware, but the limitations imposed by the old IRIX file system, EFS [SGI92].
Reference: [Rosenblum92] <author> Rosenblum, M., Ousterhout, J., </author> <title> "The Design and Implementation of a Log-Structured File System," </title> <journal> ACM Transactions on Computer Systems Vol 10, </journal> <volume> No. 1, </volume> <month> February </month> <year> 1992, </year> <pages> 26-52. </pages>
Reference-contexts: XFS uses a write ahead transaction log to gather all the writes of an update into a single disk I/O, and it writes the transaction log asynchronously in order to decouple the metadata update rate from the speed of the disks. Other schemes such as log structured file systems <ref> [Rosenblum92] </ref>, shadow paging [Hitz94], and soft updates [Ganger94] have been proposed to solve this problem, but we feel that write ahead logging provides the best trade-off among exibility, performance, and reliability. <p> XFS gains two things by writing the log asynchronously. First, multiple updates can be batched into a single log write. This increases the efficiency of the log writes with respect to the underlying disk array <ref> [Hagmann87, Rosenblum92] </ref>. Second, the performance of metadata updates is normally made independent of the speed of the underlying drives. This independence is limited by the amount of buffering dedicated to the log, but it is far better than the synchronous updates of older file systems.
Reference: [Sandberg85] <author> Sandberg, R., et al., </author> <title> "Design and Implementation of the Sun Network File System," </title> <booktitle> Proceedings of the 1985 Summer Usenix, </booktitle> <month> June, </month> <year> 1985, </year> <pages> 119-130. </pages>
Reference-contexts: This is because it provides us with the fast metadata updates and crash recovery we need without sacrificing our ability to efficiently support synchronous writing workloads, for example that of an NFS server <ref> [Sandberg85] </ref>, and without sacrificing our desire for large, contiguous file support. How-ev er, an in depth analysis of write ahead logging or the tradeoffs among these schemes is beyond the scope of this paper.
Reference: [Seltzer95] <author> Seltzer, M., Smith, K., Balakrishnan, H., Chang, J., McMains, S., Padmanabhan, V., </author> <title> "File System Logging Versus Clustering: A Performance Comparison," </title> <booktitle> Proceedings of the 1995 Usenix Technical Conference, </booktitle> <month> January </month> <year> 1995, </year> <pages> 249-264. </pages>
Reference-contexts: For other file systems, for example FFS, this has not been a problem up to this point, because they do not try very hard to allocate files contiguously. Not doing so, however, can have bad implications for the I/O performance of accessing files in those file systems <ref> [Seltzer95] </ref>. Inability to Support Large Directories Another area which has not been addressed by other Unix file systems is support for directories with more than a few thousand entries. <p> File systems with large files tend to make the opposite choice and use large block sizes in order to reduce external fragmentation of the file system and their files' extents. Av oiding File System Fragmentation The work by Seltzer and Smith <ref> [Seltzer95] </ref> shows that long term file system fragmentation can degrade the performance of FFS file systems by between 5% and 15%. This fragmentation is the result of creating and removing many files over time.
Reference: [SGI92] <institution> IRIX Advanced Site and Server Administration Guide, Silicon Graphics, Inc., </institution> <note> chapter 8, 241-288 </note>
Reference-contexts: Why a New File System? The file system literature began predicting the coming of the "I/O bottleneck" years ago [Ousterhout90], and we experienced it first hand at SGI. The problem was not the I/O performance of our hardware, but the limitations imposed by the old IRIX file system, EFS <ref> [SGI92] </ref>. EFS is similar to the Berkeley Fast File System [McKusick84] in structure, but it uses extents rather than individual blocks for file space allocation and I/O.

References-found: 19

