URL: ftp://stat.stanford.edu/pub/friedman/boost.ps.Z
Refering-URL: http://www.aic.nrl.navy.mil/~aha/research/machine-learning.html
Root-URL: http://www.cs.cmu.edu/~jr6b
Email: fjhf,trevorg@stat.stanford.edu  tibs@utstat.toronto.edu  
Title: Additive Logistic Regression: a Statistical View of Boosting  
Author: Jerome Friedman Trevor Hastie Robert Tibshirani 
Address: Hall, Stanford University, Stanford California 94305;  Toronto;  
Affiliation: Department of Statistics, Sequoia  Department of Public Health Sciences, and Department of Statistics, University of  
Date: July 23, 1998  
Abstract: Boosting (Freund & Schapire 1996, Schapire & Singer 1998) is one of the most important recent developments in classification methodology. The performance of many classification algorithms often can be dramatically improved by sequentially applying them to reweighted versions of the input data, and taking a weighted majority vote of the sequence of classifiers thereby produced. We show that this seemingly mysterious phenomenon can be understood in terms of well known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to that of boosting. Direct multi-class generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multi-class generalizations of boosting in most situations, and far superior in some. We suggest a minor modification to boosting that can reduce computation, often by factors of 10 to 50. Finally, we apply these insights to produce an alternative formulation of boosting decision trees. This approach, based on best-first truncated tree induction, often leads to better performance, and can provide interpretable descriptions of the aggregate decision rule. It is also much faster computationally making it more suitable to large scale data mining applications. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L. </author> <year> (1996), </year> <note> `Bagging predictors', Machine Learning 26 </note> . 
Reference-contexts: For some reason, it seems that AdaBoost is immune to overfitting. classification task, using a adaptation of CART TM (Breiman, Friedman, Ol-shen & Stone 1984) as the base classifier. This adaptation grows fixed-size trees in a "best-first" manner (see Section 7). Included in the figure is the bagged tree <ref> (Breiman 1996) </ref> which averages trees grown on bootstrap resampled versions of the training data. Bagging is purely a variance-reduction technique, and since trees tend to have high variance, bagging often produces good results. <p> As this example illustrates, very large reductions in computation for boosting can be achieved by this simple trick. A variety of other examples (not shown) exhibit similar behavior with all boosting methods. Note that other committee approaches to classification such as bagging <ref> (Breiman 1996) </ref> and randomized trees (Dietterich 1998), while admitting parallel implementations, cannot take advantage of this approach to reduce computation. 39 9 Concluding remarks In order to understand a learning procedure statistically it is necessary to identify two important aspects: its structural model and its error model. <p> The concepts developed in this paper suggest that there is very little, if any, connection between (deterministic) weighted boosting and other (randomized) ensemble methods such as bagging <ref> (Breiman 1996) </ref> and randomized trees (Dietterich 1998). In the language least squares regression, the latter are purely "variance" reducing procedures intended to mitigate instability, especially that associated with (large) decision trees. Boosting on the other hand seems fundamentally different.
Reference: <author> Breiman, L. </author> <year> (1997), </year> <title> Prediction games and arcing algorithms, </title> <type> Technical Report Technical Report 504, </type> <institution> Statistics Department, University of Cali-fornia, Berkeley. </institution> <note> Submitted to Neural Computing. </note>
Reference: <author> Breiman, L., Friedman, J., Olshen, R. & Stone, C. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Wadsworth, </publisher> <address> Belmont, California. </address>
Reference-contexts: Interestingly, the test error seems to consistently decrease and then level off as more classifiers are added, rather than ultimately increase. For some reason, it seems that AdaBoost is immune to overfitting. classification task, using a adaptation of CART TM <ref> (Breiman, Friedman, Ol-shen & Stone 1984) </ref> as the base classifier. This adaptation grows fixed-size trees in a "best-first" manner (see Section 7). Included in the figure is the bagged tree (Breiman 1996) which averages trees grown on bootstrap resampled versions of the training data. <p> Equivalently, the Newton update solves the weighted least squares criterion min E w (x) 1 y fl p (x) f (x) (33) The population algorithm described here translates immediately to an implementation on data when E (jx) is replaced by a regression method, such as regression trees <ref> (Breiman et al. 1984) </ref>. <p> The datasets are summarized in Table 1. The test error rates are shown in Table 2 for the smaller datasets, and in Table 3 for the larger ones. The vowel, sonar, satimage and letter datasets come with a pre-specified test set. The waveform data is simulated, as described in <ref> (Breiman et al. 1984) </ref>. For the others, 5-fold cross-validation was used to estimate the test error. It is difficult to discern trends on the small data sets (Table 2) because all but quite large observed differences in performance could be attributed to sampling fluctuations.
Reference: <author> Buja, A., Hastie, T. & Tibshirani, R. </author> <year> (1989), </year> <title> `Linear smoothers and additive models (with discussion)', </title> <journal> Annals of Statistics 17, </journal> <pages> 453-555. </pages>
Reference-contexts: In the right hand side, all the latest versions of the functions f k are used in forming the partial residuals. The backfitting cycles are repeated until convergence. Under fairly general conditions, backfitting can be shown to converge to the minimizer of E (yF (x)) 2 <ref> (Buja et al. 1989) </ref>. 2.2 Extended Additive Models More generally, one can consider additive models whose elements ff m (x)g M 1 are functions of potentially all of the input features x.
Reference: <author> Cover, T. & Hart, P. </author> <year> (1967), </year> <title> `Nearest neighbor pattern classification', </title> <journal> Proc. IEEE Trans. Inform. Theory pp. </journal> <pages> 21-27. </pages>
Reference-contexts: In the Machine-Learning literature this is explained in terms of VC dimension of the ensemble compared to that of each weak learner. * Classifiers are hurt less by overfitting than other function estimators (e.g. the famous risk bound of the 1-nearest-neighbor classifier <ref> (Cover & Hart 1967) </ref>). 41 Whatever the explanation, the empirical evidence is strong; the introduc-tion of boosting by Schapire, Freund and colleagues has brought an exciting and important set of new ideas to the table.
Reference: <author> Dietterich, T. </author> <year> (1998), </year> <title> `An experimental comparison of three methods for constructing ensembles of decision trees: bagging, boosting, and randomization', </title> <booktitle> Machine Learning ?, 1-22. </booktitle>
Reference-contexts: As this example illustrates, very large reductions in computation for boosting can be achieved by this simple trick. A variety of other examples (not shown) exhibit similar behavior with all boosting methods. Note that other committee approaches to classification such as bagging (Breiman 1996) and randomized trees <ref> (Dietterich 1998) </ref>, while admitting parallel implementations, cannot take advantage of this approach to reduce computation. 39 9 Concluding remarks In order to understand a learning procedure statistically it is necessary to identify two important aspects: its structural model and its error model. <p> The concepts developed in this paper suggest that there is very little, if any, connection between (deterministic) weighted boosting and other (randomized) ensemble methods such as bagging (Breiman 1996) and randomized trees <ref> (Dietterich 1998) </ref>. In the language least squares regression, the latter are purely "variance" reducing procedures intended to mitigate instability, especially that associated with (large) decision trees. Boosting on the other hand seems fundamentally different.
Reference: <author> Freund, Y. </author> <year> (1995), </year> <title> `Boosting a weak learning algorithm by majority', </title> <booktitle> Information and Computation 121(2), </booktitle> <pages> 256-285. </pages>
Reference-contexts: This theory (Schapire 1990) has evolved in the machine learning community, initially based on the concepts of PAC learning (Kearns & Vazirani 1994), and later from game theory <ref> (Freund 1995, Breiman 1997) </ref>. Early versions of boosting "weak learners" (Schapire 1990) are far simpler than those described here, and the theory is more precise. The bounds and the theory associated with the AdaBoost algorithms are interesting, but tend to be too loose to be of practical importance.
Reference: <author> Freund, Y. & Schapire, R. </author> <year> (1996), </year> <title> Experiments with a new boosting algorithm, </title> <booktitle> in `Machine Learning: Proceedings of the Thirteenth International Conference', </booktitle> <pages> pp. 148-156. </pages> <note> 42 Friedman, J. </note> <year> (1991), </year> <title> `Multivariate adaptive regression splines (with discus-sion)', </title> <journal> Annals of Statistics 19(1), </journal> <pages> 1-141. </pages>
Reference-contexts: 1 Introduction The starting point for this paper is an interesting procedure called "boosting", which is a way of combining or boosting the performance of many "weak" classifiers to produce a powerful "committee". Boosting was proposed in the machine learning literature <ref> (Freund & Schapire 1996) </ref> and has since received much attention. While boosting has evolved somewhat over the years, we first describe the most commonly used version of theAdaBoost procedure (Freund & Schapire 1996), which we call "Discrete" AdaBoost. Here is a concise description of AdaBoost in the two-class classification setting. <p> Boosting was proposed in the machine learning literature <ref> (Freund & Schapire 1996) </ref> and has since received much attention. While boosting has evolved somewhat over the years, we first describe the most commonly used version of theAdaBoost procedure (Freund & Schapire 1996), which we call "Discrete" AdaBoost. Here is a concise description of AdaBoost in the two-class classification setting. <p> This is done for a sequence of weighted samples, and then the final classifier is defined to be a linear combination of the classifiers from each stage. We describe the procedure in more detail in Algorithm 1 Discrete AdaBoost <ref> (Freund & Schapire 1996) </ref> 1. Start with weights w i = 1=N , i = 1; : : : ; N . 2.
Reference: <author> Friedman, J. </author> <year> (1996), </year> <title> Another approach to polychotomous classification, </title> <type> Technical report, </type> <institution> Stanford University. </institution>
Reference-contexts: Even if the decision boundaries separating all class pairs are relatively simple, pooling classes can produce complex decision boundaries that are difficult to approximate <ref> (Friedman 1996) </ref>. By considering all of the classes simultaneously, the symmetric multi-class model is better able to take advantage of simple pairwise boundaries when they exist (Hastie & Tibshirani 1998).
Reference: <author> Friedman, J. & Stuetzle, W. </author> <year> (1981), </year> <title> `Projection pursuit regression', </title> <journal> Journal of the American Statistical Association 76, </journal> <pages> 817-823. </pages>
Reference-contexts: More generally, each component f j is a function of a small, pre-specified subset of the input variables. The backfitting algorithm <ref> (Friedman & Stuetzle 1981, Buja, Hastie & Tibshirani 1989) </ref> is a convenient modular algorithm for fitting additive models.
Reference: <author> Hastie, T. & Tibshirani, R. </author> <year> (1990), </year> <title> Generalized Additive Models, </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: In particular, the linear logistic regression model (McCullagh & Nelder 1989, for example) and additive logistic regression model <ref> (Hastie & Tibshirani 1990) </ref> are popular. These models are usually fit by maximizing the binomial log-likelihood, and enjoy all the associated asymptotic optimality features of maximum likelihood estimation. A generalized version of backfitting (2), called "Local Scoring" in (Hastie & Tibshirani 1990), is used to fit the additive logistic model. <p> model (McCullagh & Nelder 1989, for example) and additive logistic regression model <ref> (Hastie & Tibshirani 1990) </ref> are popular. These models are usually fit by maximizing the binomial log-likelihood, and enjoy all the associated asymptotic optimality features of maximum likelihood estimation. A generalized version of backfitting (2), called "Local Scoring" in (Hastie & Tibshirani 1990), is used to fit the additive logistic model.
Reference: <author> Hastie, T. & Tibshirani, R. </author> <year> (1998), </year> <title> `Classification by pairwise coupling', </title> <note> Annals of Statistics . (to appear). </note>
Reference-contexts: By considering all of the classes simultaneously, the symmetric multi-class model is better able to take advantage of simple pairwise boundaries when they exist <ref> (Hastie & Tibshirani 1998) </ref>. As noted above, the pairwise boundaries induced by (38) and (39) are simple when viewed in the context of additive modeling, whereas the pooled boundaries are more complex; they cannot be well approximated by functions that are additive in the original predictor 30 variables.
Reference: <author> Hastie, T., Tibshirani, R. & Buja, A. </author> <year> (1994), </year> <title> `Flexible discriminant analysis by optimal scoring', </title> <journal> Journal of the American Statistical Association 89, </journal> <pages> 1255-1270. </pages>
Reference-contexts: One could transfer all the above regression machinery across to the classification domain by simply noting that E (1 (y=j) jx) = P (y = jjx), where 1 (y=j) is the 0=1 indicator variable representing class j. While this works fairly well in general, several problems have been noted <ref> (Hastie, Tibshirani & Buja 1994) </ref> for constrained regression methods. The estimates are typically not confined to [0; 1], and severe masking problems can occur. A notable exception is when trees are used as the regression method, and in fact this is the approach used by Breiman et al. (1984).
Reference: <author> Holte, R. </author> <year> (1993), </year> <title> `Very simple classification rules perform well on most commonly used datasets', </title> <booktitle> Machine Learning 11, </booktitle> <pages> 63-90. </pages>
Reference-contexts: The decision boundaries associated with these examples were deliberately chosen to be geometrically complex in an attempt to illicit performance differences among the methods being tested. Such complicated boundaries are not likely to often occur in practice. Many practical problems involve comparatively simple boundaries <ref> (Holte 1993) </ref>; in such cases performance differences will still be situation dependent, but correspondingly less pronounced. 6 Some experiments with data In this section we show the results of running the four fitting methods: LogitBoost, Discrete AdaBoost, Real AdaBoost, and Gentle AdaBoost on a collection of datasets from the UC-Irvine machine <p> There LogitBoost generally dominated, although often by a small margin. The inability of the real data examples to discriminate may reflect statistical difficulties in estimating subtle differences with small samples. Alternatively, it may be that the their underlying decision boundaries are all relatively simple <ref> (Holte 1993) </ref> so that all reasonable methods exhibit similar performance. 7 Additive Logistic Trees In most applications of boosting the base classifier is considered to be a primitive, repeatedly called by the boosting procedure as iterations proceed.
Reference: <author> Kearns, M. & Vazirani, U. </author> <year> (1994), </year> <title> An Introduction to Computational Learning Theory, </title> <publisher> MIT Press. </publisher>
Reference-contexts: Freund & Schapire (1996) and Schapire & Singer (1998) provide some theory to support their algorithms, in the form of upper bounds on generalization error. This theory (Schapire 1990) has evolved in the machine learning community, initially based on the concepts of PAC learning <ref> (Kearns & Vazirani 1994) </ref>, and later from game theory (Freund 1995, Breiman 1997). Early versions of boosting "weak learners" (Schapire 1990) are far simpler than those described here, and the theory is more precise.
Reference: <author> Mallat, S. & Zhang, Z. </author> <year> (1993), </year> <title> `Matching pursuits with time-frequency dictionaries', </title> <journal> IEEE Transactions on Signal Processing 41, </journal> <pages> 3397-3415. </pages>
Reference: <author> McCullagh, P. & Nelder, J. </author> <year> (1989), </year> <title> Generalized Linear Models, </title> <publisher> Chapman and Hall. </publisher>
Reference-contexts: In particular, the linear logistic regression model <ref> (McCullagh & Nelder 1989, for example) </ref> and additive logistic regression model (Hastie & Tibshirani 1990) are popular. These models are usually fit by maximizing the binomial log-likelihood, and enjoy all the associated asymptotic optimality features of maximum likelihood estimation.
Reference: <author> Schapire, R. </author> <year> (1990), </year> <title> `The strength of weak learnability', </title> <booktitle> Machine Learning 5(2), </booktitle> <pages> 197-227. </pages>
Reference-contexts: We describe the Schapire-Singer estimate for f m (x) in Section 3. Freund & Schapire (1996) and Schapire & Singer (1998) provide some theory to support their algorithms, in the form of upper bounds on generalization error. This theory <ref> (Schapire 1990) </ref> has evolved in the machine learning community, initially based on the concepts of PAC learning (Kearns & Vazirani 1994), and later from game theory (Freund 1995, Breiman 1997). Early versions of boosting "weak learners" (Schapire 1990) are far simpler than those described here, and the theory is more precise. <p> This theory <ref> (Schapire 1990) </ref> has evolved in the machine learning community, initially based on the concepts of PAC learning (Kearns & Vazirani 1994), and later from game theory (Freund 1995, Breiman 1997). Early versions of boosting "weak learners" (Schapire 1990) are far simpler than those described here, and the theory is more precise. The bounds and the theory associated with the AdaBoost algorithms are interesting, but tend to be too loose to be of practical importance.
Reference: <author> Schapire, R. & Singer, Y. </author> <year> (1998), </year> <title> Improved boosting algorithms using confidence-rated predictions, </title> <booktitle> in `Proceedings of the Eleventh Annual Conference on Computational Learning Theory'. </booktitle> <pages> 43 </pages>
Reference-contexts: All trees are grown "best-first" without pruning. The left-most iteration corresponds to a single tree. 4 best in our simulated examples in Fig. 1, especially with stumps, although we see with 100 node trees Discrete AdaBoost overtakes Real AdaBoost after 200 iterations. Real AdaBoost <ref> (Schapire & Singer 1998) </ref> 1. Start with weights w i = 1=N , i = 1; 2; : : : ; N . 2. <p> If the first split is on C | either a J -nary split if permitted, or else J 1 binary splits | then the sub-trees are identical to separate trees grown to each of the J groups. This will always be the case for the first tree. 20 AdaBoost.MH <ref> (Schapire & Singer 1998) </ref> The original N observations are expanded into N fi J pairs ((x i ; 1); y i1 ); ((x i ; 2); y i2 ); : : : ; ((x i ; J); y iJ ); i = 1; : : : ; N: 1. <p> In the multi-class case, the AdaBoost procedures maximize a separate Bernoulli likelihood for each class versus the others. This is a natural choice and is especially appropriate when observations can belong to more than one class <ref> (Schapire & Singer 1998) </ref>. In the more usual setting of a unique class label for each observation, the symmetric multinomial distribution is a more appropriate error model. We develop a multi-class LogitBoost procedure that maximizes the corresponding log-likelihood by quasi-Newton stepping.
References-found: 19

